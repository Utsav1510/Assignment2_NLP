{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e527a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\loyol\\onedrive\\documents\\au\\anaconda\\lib\\site-packages (2.32.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\loyol\\onedrive\\documents\\au\\anaconda\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\loyol\\onedrive\\documents\\au\\anaconda\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\loyol\\onedrive\\documents\\au\\anaconda\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\loyol\\onedrive\\documents\\au\\anaconda\\lib\\site-packages (from requests) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff34b1e",
   "metadata": {},
   "source": [
    "Below code was used to pull NLP requests \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b747ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [nlp]...\n",
      "✅ Total collected so far: 17\n",
      "Fetching page 2 for tag [nlp]...\n",
      "✅ Total collected so far: 37\n",
      "Fetching page 3 for tag [nlp]...\n",
      "✅ Total collected so far: 56\n",
      "Fetching page 4 for tag [nlp]...\n",
      "✅ Total collected so far: 66\n",
      "Fetching page 5 for tag [nlp]...\n",
      "✅ Total collected so far: 78\n",
      "Fetching page 6 for tag [nlp]...\n",
      "✅ Total collected so far: 96\n",
      "Fetching page 7 for tag [nlp]...\n",
      "✅ Total collected so far: 117\n",
      "Fetching page 8 for tag [nlp]...\n",
      "✅ Total collected so far: 145\n",
      "Fetching page 9 for tag [nlp]...\n",
      "✅ Total collected so far: 172\n",
      "Fetching page 10 for tag [nlp]...\n",
      "✅ Total collected so far: 200\n",
      "Fetching page 11 for tag [nlp]...\n",
      "✅ Total collected so far: 224\n",
      "Fetching page 12 for tag [nlp]...\n",
      "✅ Total collected so far: 240\n",
      "Fetching page 13 for tag [nlp]...\n",
      "✅ Total collected so far: 270\n",
      "Fetching page 14 for tag [nlp]...\n",
      "✅ Total collected so far: 293\n",
      "Fetching page 15 for tag [nlp]...\n",
      "✅ Total collected so far: 323\n",
      "Fetching page 16 for tag [nlp]...\n",
      "✅ Total collected so far: 353\n",
      "Fetching page 17 for tag [nlp]...\n",
      "✅ Total collected so far: 380\n",
      "Fetching page 18 for tag [nlp]...\n",
      "✅ Total collected so far: 410\n",
      "Fetching page 19 for tag [nlp]...\n",
      "✅ Total collected so far: 440\n",
      "Fetching page 20 for tag [nlp]...\n",
      "✅ Total collected so far: 467\n",
      "Fetching page 21 for tag [nlp]...\n",
      "✅ Total collected so far: 497\n",
      "Fetching page 22 for tag [nlp]...\n",
      "✅ Total collected so far: 524\n",
      "Fetching page 23 for tag [nlp]...\n",
      "✅ Total collected so far: 554\n",
      "Fetching page 24 for tag [nlp]...\n",
      "✅ Total collected so far: 584\n",
      "Fetching page 25 for tag [nlp]...\n",
      "✅ Total collected so far: 610\n",
      "Fetching page 26 for tag [nlp]...\n",
      "✅ Total collected so far: 637\n",
      "Fetching page 27 for tag [nlp]...\n",
      "✅ Total collected so far: 667\n",
      "Fetching page 28 for tag [nlp]...\n",
      "✅ Total collected so far: 697\n",
      "Fetching page 29 for tag [nlp]...\n",
      "✅ Total collected so far: 727\n",
      "Fetching page 30 for tag [nlp]...\n",
      "✅ Total collected so far: 757\n",
      "Fetching page 31 for tag [nlp]...\n",
      "✅ Total collected so far: 787\n",
      "Fetching page 32 for tag [nlp]...\n",
      "✅ Total collected so far: 817\n",
      "Fetching page 33 for tag [nlp]...\n",
      "✅ Total collected so far: 844\n",
      "Fetching page 34 for tag [nlp]...\n",
      "✅ Total collected so far: 874\n",
      "Fetching page 35 for tag [nlp]...\n",
      "✅ Total collected so far: 904\n",
      "Fetching page 36 for tag [nlp]...\n",
      "✅ Total collected so far: 934\n",
      "Fetching page 37 for tag [nlp]...\n",
      "✅ Total collected so far: 964\n",
      "Fetching page 38 for tag [nlp]...\n",
      "✅ Total collected so far: 994\n",
      "Fetching page 39 for tag [nlp]...\n",
      "✅ Total collected so far: 1024\n",
      "Fetching page 40 for tag [nlp]...\n",
      "✅ Total collected so far: 1054\n",
      "Fetching page 41 for tag [nlp]...\n",
      "✅ Total collected so far: 1084\n",
      "Fetching page 42 for tag [nlp]...\n",
      "✅ Total collected so far: 1114\n",
      "Fetching page 43 for tag [nlp]...\n",
      "✅ Total collected so far: 1144\n",
      "Fetching page 44 for tag [nlp]...\n",
      "✅ Total collected so far: 1173\n",
      "Fetching page 45 for tag [nlp]...\n",
      "✅ Total collected so far: 1201\n",
      "Fetching page 46 for tag [nlp]...\n",
      "✅ Total collected so far: 1231\n",
      "Fetching page 47 for tag [nlp]...\n",
      "✅ Total collected so far: 1261\n",
      "Fetching page 48 for tag [nlp]...\n",
      "✅ Total collected so far: 1291\n",
      "Fetching page 49 for tag [nlp]...\n",
      "✅ Total collected so far: 1321\n",
      "Fetching page 50 for tag [nlp]...\n",
      "✅ Total collected so far: 1351\n",
      "Fetching page 51 for tag [nlp]...\n",
      "✅ Total collected so far: 1381\n",
      "Fetching page 52 for tag [nlp]...\n",
      "✅ Total collected so far: 1411\n",
      "Fetching page 53 for tag [nlp]...\n",
      "✅ Total collected so far: 1441\n",
      "Fetching page 54 for tag [nlp]...\n",
      "✅ Total collected so far: 1471\n",
      "Fetching page 55 for tag [nlp]...\n",
      "✅ Total collected so far: 1501\n",
      "Fetching page 56 for tag [nlp]...\n",
      "✅ Total collected so far: 1531\n",
      "Fetching page 57 for tag [nlp]...\n",
      "✅ Total collected so far: 1561\n",
      "Fetching page 58 for tag [nlp]...\n",
      "✅ Total collected so far: 1591\n",
      "Fetching page 59 for tag [nlp]...\n",
      "✅ Total collected so far: 1621\n",
      "Fetching page 60 for tag [nlp]...\n",
      "✅ Total collected so far: 1651\n",
      "Fetching page 61 for tag [nlp]...\n",
      "✅ Total collected so far: 1681\n",
      "Fetching page 62 for tag [nlp]...\n",
      "✅ Total collected so far: 1711\n",
      "Fetching page 63 for tag [nlp]...\n",
      "✅ Total collected so far: 1741\n",
      "Fetching page 64 for tag [nlp]...\n",
      "✅ Total collected so far: 1771\n",
      "Fetching page 65 for tag [nlp]...\n",
      "✅ Total collected so far: 1801\n",
      "Fetching page 66 for tag [nlp]...\n",
      "✅ Total collected so far: 1831\n",
      "Fetching page 67 for tag [nlp]...\n",
      "✅ Total collected so far: 1861\n",
      "Fetching page 68 for tag [nlp]...\n",
      "✅ Total collected so far: 1891\n",
      "Fetching page 69 for tag [nlp]...\n",
      "✅ Total collected so far: 1921\n",
      "Fetching page 70 for tag [nlp]...\n",
      "✅ Total collected so far: 1951\n",
      "Fetching page 71 for tag [nlp]...\n",
      "✅ Total collected so far: 1981\n",
      "Fetching page 72 for tag [nlp]...\n",
      "✅ Total collected so far: 2011\n",
      "Fetching page 73 for tag [nlp]...\n",
      "✅ Total collected so far: 2041\n",
      "Fetching page 74 for tag [nlp]...\n",
      "✅ Total collected so far: 2071\n",
      "Fetching page 75 for tag [nlp]...\n",
      "✅ Total collected so far: 2101\n",
      "Fetching page 76 for tag [nlp]...\n",
      "✅ Total collected so far: 2131\n",
      "Fetching page 77 for tag [nlp]...\n",
      "✅ Total collected so far: 2161\n",
      "Fetching page 78 for tag [nlp]...\n",
      "✅ Total collected so far: 2191\n",
      "Fetching page 79 for tag [nlp]...\n",
      "✅ Total collected so far: 2221\n",
      "Fetching page 80 for tag [nlp]...\n",
      "✅ Total collected so far: 2251\n",
      "Fetching page 81 for tag [nlp]...\n",
      "✅ Total collected so far: 2281\n",
      "Fetching page 82 for tag [nlp]...\n",
      "✅ Total collected so far: 2311\n",
      "Fetching page 83 for tag [nlp]...\n",
      "✅ Total collected so far: 2341\n",
      "Fetching page 84 for tag [nlp]...\n",
      "✅ Total collected so far: 2371\n",
      "Fetching page 85 for tag [nlp]...\n",
      "✅ Total collected so far: 2401\n",
      "Fetching page 86 for tag [nlp]...\n",
      "✅ Total collected so far: 2431\n",
      "Fetching page 87 for tag [nlp]...\n",
      "✅ Total collected so far: 2461\n",
      "Fetching page 88 for tag [nlp]...\n",
      "✅ Total collected so far: 2491\n",
      "Fetching page 89 for tag [nlp]...\n",
      "✅ Total collected so far: 2521\n",
      "Fetching page 90 for tag [nlp]...\n",
      "✅ Total collected so far: 2551\n",
      "Fetching page 91 for tag [nlp]...\n",
      "✅ Total collected so far: 2581\n",
      "Fetching page 92 for tag [nlp]...\n",
      "✅ Total collected so far: 2611\n",
      "Fetching page 93 for tag [nlp]...\n",
      "✅ Total collected so far: 2641\n",
      "Fetching page 94 for tag [nlp]...\n",
      "✅ Total collected so far: 2671\n",
      "Fetching page 95 for tag [nlp]...\n",
      "✅ Total collected so far: 2701\n",
      "Fetching page 96 for tag [nlp]...\n",
      "✅ Total collected so far: 2731\n",
      "Fetching page 97 for tag [nlp]...\n",
      "✅ Total collected so far: 2761\n",
      "Fetching page 98 for tag [nlp]...\n",
      "✅ Total collected so far: 2791\n",
      "Fetching page 99 for tag [nlp]...\n",
      "✅ Total collected so far: 2821\n",
      "Fetching page 100 for tag [nlp]...\n",
      "✅ Total collected so far: 2851\n",
      "Fetching page 101 for tag [nlp]...\n",
      "✅ Total collected so far: 2881\n",
      "Fetching page 102 for tag [nlp]...\n",
      "✅ Total collected so far: 2911\n",
      "Fetching page 103 for tag [nlp]...\n",
      "✅ Total collected so far: 2941\n",
      "Fetching page 104 for tag [nlp]...\n",
      "✅ Total collected so far: 2971\n",
      "Fetching page 105 for tag [nlp]...\n",
      "✅ Total collected so far: 2999\n",
      "Fetching page 106 for tag [nlp]...\n",
      "✅ Total collected so far: 3029\n",
      "Fetching page 107 for tag [nlp]...\n",
      "✅ Total collected so far: 3059\n",
      "Fetching page 108 for tag [nlp]...\n",
      "✅ Total collected so far: 3089\n",
      "Fetching page 109 for tag [nlp]...\n",
      "✅ Total collected so far: 3119\n",
      "Fetching page 110 for tag [nlp]...\n",
      "✅ Total collected so far: 3149\n",
      "Fetching page 111 for tag [nlp]...\n",
      "✅ Total collected so far: 3179\n",
      "Fetching page 112 for tag [nlp]...\n",
      "✅ Total collected so far: 3209\n",
      "Fetching page 113 for tag [nlp]...\n",
      "✅ Total collected so far: 3239\n",
      "Fetching page 114 for tag [nlp]...\n",
      "✅ Total collected so far: 3269\n",
      "Fetching page 115 for tag [nlp]...\n",
      "✅ Total collected so far: 3299\n",
      "Fetching page 116 for tag [nlp]...\n",
      "✅ Total collected so far: 3329\n",
      "Fetching page 117 for tag [nlp]...\n",
      "✅ Total collected so far: 3359\n",
      "Fetching page 118 for tag [nlp]...\n",
      "✅ Total collected so far: 3389\n",
      "Fetching page 119 for tag [nlp]...\n",
      "✅ Total collected so far: 3419\n",
      "Fetching page 120 for tag [nlp]...\n",
      "✅ Total collected so far: 3449\n",
      "Fetching page 121 for tag [nlp]...\n",
      "✅ Total collected so far: 3479\n",
      "Fetching page 122 for tag [nlp]...\n",
      "✅ Total collected so far: 3509\n",
      "Fetching page 123 for tag [nlp]...\n",
      "✅ Total collected so far: 3539\n",
      "Fetching page 124 for tag [nlp]...\n",
      "✅ Total collected so far: 3569\n",
      "Fetching page 125 for tag [nlp]...\n",
      "✅ Total collected so far: 3599\n",
      "Fetching page 126 for tag [nlp]...\n",
      "✅ Total collected so far: 3629\n",
      "Fetching page 127 for tag [nlp]...\n",
      "✅ Total collected so far: 3659\n",
      "Fetching page 128 for tag [nlp]...\n",
      "✅ Total collected so far: 3689\n",
      "Fetching page 129 for tag [nlp]...\n",
      "✅ Total collected so far: 3719\n",
      "Fetching page 130 for tag [nlp]...\n",
      "✅ Total collected so far: 3749\n",
      "Fetching page 131 for tag [nlp]...\n",
      "✅ Total collected so far: 3779\n",
      "Fetching page 132 for tag [nlp]...\n",
      "✅ Total collected so far: 3809\n",
      "Fetching page 133 for tag [nlp]...\n",
      "✅ Total collected so far: 3839\n",
      "Fetching page 134 for tag [nlp]...\n",
      "✅ Total collected so far: 3869\n",
      "Fetching page 135 for tag [nlp]...\n",
      "✅ Total collected so far: 3899\n",
      "Fetching page 136 for tag [nlp]...\n",
      "✅ Total collected so far: 3929\n",
      "Fetching page 137 for tag [nlp]...\n",
      "✅ Total collected so far: 3959\n",
      "Fetching page 138 for tag [nlp]...\n",
      "✅ Total collected so far: 3989\n",
      "Fetching page 139 for tag [nlp]...\n",
      "✅ Total collected so far: 4019\n",
      "Fetching page 140 for tag [nlp]...\n",
      "✅ Total collected so far: 4049\n",
      "Fetching page 141 for tag [nlp]...\n",
      "✅ Total collected so far: 4079\n",
      "Fetching page 142 for tag [nlp]...\n",
      "✅ Total collected so far: 4109\n",
      "Fetching page 143 for tag [nlp]...\n",
      "✅ Total collected so far: 4139\n",
      "Fetching page 144 for tag [nlp]...\n",
      "✅ Total collected so far: 4169\n",
      "Fetching page 145 for tag [nlp]...\n",
      "✅ Total collected so far: 4199\n",
      "Fetching page 146 for tag [nlp]...\n",
      "✅ Total collected so far: 4229\n",
      "Fetching page 147 for tag [nlp]...\n",
      "✅ Total collected so far: 4259\n",
      "Fetching page 148 for tag [nlp]...\n",
      "✅ Total collected so far: 4289\n",
      "Fetching page 149 for tag [nlp]...\n",
      "✅ Total collected so far: 4319\n",
      "Fetching page 150 for tag [nlp]...\n",
      "✅ Total collected so far: 4349\n",
      "Fetching page 151 for tag [nlp]...\n",
      "✅ Total collected so far: 4379\n",
      "Fetching page 152 for tag [nlp]...\n",
      "✅ Total collected so far: 4409\n",
      "Fetching page 153 for tag [nlp]...\n",
      "✅ Total collected so far: 4439\n",
      "Fetching page 154 for tag [nlp]...\n",
      "✅ Total collected so far: 4469\n",
      "Fetching page 155 for tag [nlp]...\n",
      "✅ Total collected so far: 4499\n",
      "Fetching page 156 for tag [nlp]...\n",
      "✅ Total collected so far: 4529\n",
      "Fetching page 157 for tag [nlp]...\n",
      "✅ Total collected so far: 4559\n",
      "Fetching page 158 for tag [nlp]...\n",
      "✅ Total collected so far: 4589\n",
      "Fetching page 159 for tag [nlp]...\n",
      "✅ Total collected so far: 4619\n",
      "Fetching page 160 for tag [nlp]...\n",
      "✅ Total collected so far: 4649\n",
      "Fetching page 161 for tag [nlp]...\n",
      "✅ Total collected so far: 4679\n",
      "Fetching page 162 for tag [nlp]...\n",
      "✅ Total collected so far: 4709\n",
      "Fetching page 163 for tag [nlp]...\n",
      "✅ Total collected so far: 4739\n",
      "Fetching page 164 for tag [nlp]...\n",
      "✅ Total collected so far: 4769\n",
      "Fetching page 165 for tag [nlp]...\n",
      "✅ Total collected so far: 4799\n",
      "Fetching page 166 for tag [nlp]...\n",
      "✅ Total collected so far: 4829\n",
      "Fetching page 167 for tag [nlp]...\n",
      "✅ Total collected so far: 4859\n",
      "Fetching page 168 for tag [nlp]...\n",
      "✅ Total collected so far: 4889\n",
      "Fetching page 169 for tag [nlp]...\n",
      "✅ Total collected so far: 4919\n",
      "Fetching page 170 for tag [nlp]...\n",
      "✅ Total collected so far: 4949\n",
      "Fetching page 171 for tag [nlp]...\n",
      "✅ Total collected so far: 4979\n",
      "Fetching page 172 for tag [nlp]...\n",
      "✅ Total collected so far: 5009\n",
      "Fetching page 173 for tag [nlp]...\n",
      "✅ Total collected so far: 5039\n",
      "Fetching page 174 for tag [nlp]...\n",
      "✅ Total collected so far: 5069\n",
      "Fetching page 175 for tag [nlp]...\n",
      "✅ Total collected so far: 5099\n",
      "Fetching page 176 for tag [nlp]...\n",
      "✅ Total collected so far: 5129\n",
      "Fetching page 177 for tag [nlp]...\n",
      "✅ Total collected so far: 5159\n",
      "Fetching page 178 for tag [nlp]...\n",
      "✅ Total collected so far: 5189\n",
      "Fetching page 179 for tag [nlp]...\n",
      "✅ Total collected so far: 5219\n",
      "Fetching page 180 for tag [nlp]...\n",
      "✅ Total collected so far: 5249\n",
      "Fetching page 181 for tag [nlp]...\n",
      "✅ Total collected so far: 5279\n",
      "Fetching page 182 for tag [nlp]...\n",
      "✅ Total collected so far: 5309\n",
      "Fetching page 183 for tag [nlp]...\n",
      "✅ Total collected so far: 5339\n",
      "Fetching page 184 for tag [nlp]...\n",
      "✅ Total collected so far: 5369\n",
      "Fetching page 185 for tag [nlp]...\n",
      "✅ Total collected so far: 5399\n",
      "Fetching page 186 for tag [nlp]...\n",
      "✅ Total collected so far: 5429\n",
      "Fetching page 187 for tag [nlp]...\n",
      "✅ Total collected so far: 5459\n",
      "Fetching page 188 for tag [nlp]...\n",
      "✅ Total collected so far: 5489\n",
      "Fetching page 189 for tag [nlp]...\n",
      "✅ Total collected so far: 5519\n",
      "Fetching page 190 for tag [nlp]...\n",
      "✅ Total collected so far: 5549\n",
      "Fetching page 191 for tag [nlp]...\n",
      "✅ Total collected so far: 5579\n",
      "Fetching page 192 for tag [nlp]...\n",
      "✅ Total collected so far: 5609\n",
      "Fetching page 193 for tag [nlp]...\n",
      "✅ Total collected so far: 5639\n",
      "Fetching page 194 for tag [nlp]...\n",
      "✅ Total collected so far: 5669\n",
      "Fetching page 195 for tag [nlp]...\n",
      "✅ Total collected so far: 5699\n",
      "Fetching page 196 for tag [nlp]...\n",
      "✅ Total collected so far: 5729\n",
      "Fetching page 197 for tag [nlp]...\n",
      "✅ Total collected so far: 5759\n",
      "Fetching page 198 for tag [nlp]...\n",
      "✅ Total collected so far: 5789\n",
      "Fetching page 199 for tag [nlp]...\n",
      "✅ Total collected so far: 5819\n",
      "Fetching page 200 for tag [nlp]...\n",
      "✅ Total collected so far: 5849\n",
      "Fetching page 1 for tag [nltk]...\n",
      "✅ Total collected so far: 5875\n",
      "Fetching page 2 for tag [nltk]...\n",
      "✅ Total collected so far: 5900\n",
      "Fetching page 3 for tag [nltk]...\n",
      "✅ Total collected so far: 5930\n",
      "Fetching page 4 for tag [nltk]...\n",
      "✅ Total collected so far: 5960\n",
      "Fetching page 5 for tag [nltk]...\n",
      "✅ Total collected so far: 5990\n",
      "Fetching page 6 for tag [nltk]...\n",
      "✅ Total collected so far: 6020\n",
      "Fetching page 7 for tag [nltk]...\n",
      "✅ Total collected so far: 6050\n",
      "Fetching page 8 for tag [nltk]...\n",
      "✅ Total collected so far: 6080\n",
      "Fetching page 9 for tag [nltk]...\n",
      "✅ Total collected so far: 6110\n",
      "Fetching page 10 for tag [nltk]...\n",
      "✅ Total collected so far: 6140\n",
      "Fetching page 11 for tag [nltk]...\n",
      "✅ Total collected so far: 6170\n",
      "Fetching page 12 for tag [nltk]...\n",
      "✅ Total collected so far: 6200\n",
      "Fetching page 13 for tag [nltk]...\n",
      "✅ Total collected so far: 6230\n",
      "Fetching page 14 for tag [nltk]...\n",
      "✅ Total collected so far: 6260\n",
      "Fetching page 15 for tag [nltk]...\n",
      "✅ Total collected so far: 6290\n",
      "Fetching page 16 for tag [nltk]...\n",
      "✅ Total collected so far: 6320\n",
      "Fetching page 17 for tag [nltk]...\n",
      "✅ Total collected so far: 6350\n",
      "Fetching page 18 for tag [nltk]...\n",
      "✅ Total collected so far: 6380\n",
      "Fetching page 19 for tag [nltk]...\n",
      "✅ Total collected so far: 6410\n",
      "Fetching page 20 for tag [nltk]...\n",
      "✅ Total collected so far: 6440\n",
      "Fetching page 21 for tag [nltk]...\n",
      "✅ Total collected so far: 6470\n",
      "Fetching page 22 for tag [nltk]...\n",
      "✅ Total collected so far: 6500\n",
      "Fetching page 23 for tag [nltk]...\n",
      "✅ Total collected so far: 6530\n",
      "Fetching page 24 for tag [nltk]...\n",
      "✅ Total collected so far: 6560\n",
      "Fetching page 25 for tag [nltk]...\n",
      "✅ Total collected so far: 6590\n",
      "Fetching page 26 for tag [nltk]...\n",
      "✅ Total collected so far: 6620\n",
      "Fetching page 27 for tag [nltk]...\n",
      "✅ Total collected so far: 6650\n",
      "Fetching page 28 for tag [nltk]...\n",
      "✅ Total collected so far: 6680\n",
      "Fetching page 29 for tag [nltk]...\n",
      "✅ Total collected so far: 6710\n",
      "Fetching page 30 for tag [nltk]...\n",
      "✅ Total collected so far: 6740\n",
      "Fetching page 31 for tag [nltk]...\n",
      "✅ Total collected so far: 6770\n",
      "Fetching page 32 for tag [nltk]...\n",
      "✅ Total collected so far: 6800\n",
      "Fetching page 33 for tag [nltk]...\n",
      "✅ Total collected so far: 6830\n",
      "Fetching page 34 for tag [nltk]...\n",
      "✅ Total collected so far: 6860\n",
      "Fetching page 35 for tag [nltk]...\n",
      "✅ Total collected so far: 6890\n",
      "Fetching page 36 for tag [nltk]...\n",
      "✅ Total collected so far: 6920\n",
      "Fetching page 37 for tag [nltk]...\n",
      "✅ Total collected so far: 6950\n",
      "Fetching page 38 for tag [nltk]...\n",
      "✅ Total collected so far: 6980\n",
      "Fetching page 39 for tag [nltk]...\n",
      "✅ Total collected so far: 7010\n",
      "Fetching page 40 for tag [nltk]...\n",
      "✅ Total collected so far: 7040\n",
      "Fetching page 41 for tag [nltk]...\n",
      "✅ Total collected so far: 7070\n",
      "Fetching page 42 for tag [nltk]...\n",
      "✅ Total collected so far: 7100\n",
      "Fetching page 43 for tag [nltk]...\n",
      "✅ Total collected so far: 7130\n",
      "Fetching page 44 for tag [nltk]...\n",
      "✅ Total collected so far: 7160\n",
      "Fetching page 45 for tag [nltk]...\n",
      "✅ Total collected so far: 7190\n",
      "Fetching page 46 for tag [nltk]...\n",
      "✅ Total collected so far: 7220\n",
      "Fetching page 47 for tag [nltk]...\n",
      "✅ Total collected so far: 7250\n",
      "Fetching page 48 for tag [nltk]...\n",
      "✅ Total collected so far: 7280\n",
      "Fetching page 49 for tag [nltk]...\n",
      "✅ Total collected so far: 7310\n",
      "Fetching page 50 for tag [nltk]...\n",
      "✅ Total collected so far: 7340\n",
      "Fetching page 51 for tag [nltk]...\n",
      "✅ Total collected so far: 7370\n",
      "Fetching page 52 for tag [nltk]...\n",
      "✅ Total collected so far: 7400\n",
      "Fetching page 53 for tag [nltk]...\n",
      "✅ Total collected so far: 7430\n",
      "Fetching page 54 for tag [nltk]...\n",
      "✅ Total collected so far: 7460\n",
      "Fetching page 55 for tag [nltk]...\n",
      "✅ Total collected so far: 7490\n",
      "Fetching page 56 for tag [nltk]...\n",
      "✅ Total collected so far: 7520\n",
      "Fetching page 57 for tag [nltk]...\n",
      "✅ Total collected so far: 7550\n",
      "Fetching page 58 for tag [nltk]...\n",
      "✅ Total collected so far: 7580\n",
      "Fetching page 59 for tag [nltk]...\n",
      "✅ Total collected so far: 7610\n",
      "Fetching page 60 for tag [nltk]...\n",
      "✅ Total collected so far: 7640\n",
      "Fetching page 61 for tag [nltk]...\n",
      "✅ Total collected so far: 7670\n",
      "Fetching page 62 for tag [nltk]...\n",
      "✅ Total collected so far: 7700\n",
      "Fetching page 63 for tag [nltk]...\n",
      "✅ Total collected so far: 7730\n",
      "Fetching page 64 for tag [nltk]...\n",
      "✅ Total collected so far: 7760\n",
      "Fetching page 65 for tag [nltk]...\n",
      "✅ Total collected so far: 7790\n",
      "Fetching page 66 for tag [nltk]...\n",
      "✅ Total collected so far: 7820\n",
      "Fetching page 67 for tag [nltk]...\n",
      "✅ Total collected so far: 7850\n",
      "Fetching page 68 for tag [nltk]...\n",
      "✅ Total collected so far: 7880\n",
      "Fetching page 69 for tag [nltk]...\n",
      "✅ Total collected so far: 7910\n",
      "Fetching page 70 for tag [nltk]...\n",
      "✅ Total collected so far: 7940\n",
      "Fetching page 71 for tag [nltk]...\n",
      "✅ Total collected so far: 7970\n",
      "Fetching page 72 for tag [nltk]...\n",
      "✅ Total collected so far: 7973\n",
      "Fetching page 73 for tag [nltk]...\n",
      "No items found for tag [nltk], page 73\n",
      "Fetching page 1 for tag [spacy]...\n",
      "✅ Total collected so far: 7995\n",
      "Fetching page 2 for tag [spacy]...\n",
      "✅ Total collected so far: 8024\n",
      "Fetching page 3 for tag [spacy]...\n",
      "✅ Total collected so far: 8054\n",
      "Fetching page 4 for tag [spacy]...\n",
      "✅ Total collected so far: 8078\n",
      "Fetching page 5 for tag [spacy]...\n",
      "✅ Total collected so far: 8108\n",
      "Fetching page 6 for tag [spacy]...\n",
      "✅ Total collected so far: 8138\n",
      "Fetching page 7 for tag [spacy]...\n",
      "✅ Total collected so far: 8168\n",
      "Fetching page 8 for tag [spacy]...\n",
      "✅ Total collected so far: 8198\n",
      "Fetching page 9 for tag [spacy]...\n",
      "✅ Total collected so far: 8228\n",
      "Fetching page 10 for tag [spacy]...\n",
      "✅ Total collected so far: 8258\n",
      "Fetching page 11 for tag [spacy]...\n",
      "✅ Total collected so far: 8288\n",
      "Fetching page 12 for tag [spacy]...\n",
      "✅ Total collected so far: 8318\n",
      "Fetching page 13 for tag [spacy]...\n",
      "✅ Total collected so far: 8348\n",
      "Fetching page 14 for tag [spacy]...\n",
      "✅ Total collected so far: 8378\n",
      "Fetching page 15 for tag [spacy]...\n",
      "✅ Total collected so far: 8408\n",
      "Fetching page 16 for tag [spacy]...\n",
      "✅ Total collected so far: 8438\n",
      "Fetching page 17 for tag [spacy]...\n",
      "✅ Total collected so far: 8468\n",
      "Fetching page 18 for tag [spacy]...\n",
      "✅ Total collected so far: 8498\n",
      "Fetching page 19 for tag [spacy]...\n",
      "✅ Total collected so far: 8528\n",
      "Fetching page 20 for tag [spacy]...\n",
      "✅ Total collected so far: 8558\n",
      "Fetching page 21 for tag [spacy]...\n",
      "✅ Total collected so far: 8588\n",
      "Fetching page 22 for tag [spacy]...\n",
      "✅ Total collected so far: 8618\n",
      "Fetching page 23 for tag [spacy]...\n",
      "✅ Total collected so far: 8648\n",
      "Fetching page 24 for tag [spacy]...\n",
      "✅ Total collected so far: 8678\n",
      "Fetching page 25 for tag [spacy]...\n",
      "✅ Total collected so far: 8708\n",
      "Fetching page 26 for tag [spacy]...\n",
      "✅ Total collected so far: 8738\n",
      "Fetching page 27 for tag [spacy]...\n",
      "✅ Total collected so far: 8768\n",
      "Fetching page 28 for tag [spacy]...\n",
      "✅ Total collected so far: 8798\n",
      "Fetching page 29 for tag [spacy]...\n",
      "✅ Total collected so far: 8828\n",
      "Fetching page 30 for tag [spacy]...\n",
      "✅ Total collected so far: 8858\n",
      "Fetching page 31 for tag [spacy]...\n",
      "✅ Total collected so far: 8888\n",
      "Fetching page 32 for tag [spacy]...\n",
      "✅ Total collected so far: 8918\n",
      "Fetching page 33 for tag [spacy]...\n",
      "✅ Total collected so far: 8948\n",
      "Fetching page 34 for tag [spacy]...\n",
      "✅ Total collected so far: 8978\n",
      "Fetching page 35 for tag [spacy]...\n",
      "✅ Total collected so far: 9008\n",
      "Fetching page 36 for tag [spacy]...\n",
      "✅ Total collected so far: 9038\n",
      "Fetching page 37 for tag [spacy]...\n",
      "✅ Total collected so far: 9068\n",
      "Fetching page 38 for tag [spacy]...\n",
      "✅ Total collected so far: 9098\n",
      "Fetching page 39 for tag [spacy]...\n",
      "✅ Total collected so far: 9107\n",
      "Fetching page 40 for tag [spacy]...\n",
      "No items found for tag [spacy], page 40\n",
      "Fetching page 1 for tag [transformers]...\n",
      "No items found for tag [transformers], page 1\n",
      "Fetching page 1 for tag [text-classification]...\n",
      "✅ Total collected so far: 9135\n",
      "Fetching page 2 for tag [text-classification]...\n",
      "✅ Total collected so far: 9158\n",
      "Fetching page 3 for tag [text-classification]...\n",
      "✅ Total collected so far: 9181\n",
      "Fetching page 4 for tag [text-classification]...\n",
      "✅ Total collected so far: 9211\n",
      "Fetching page 5 for tag [text-classification]...\n",
      "✅ Total collected so far: 9241\n",
      "Fetching page 6 for tag [text-classification]...\n",
      "✅ Total collected so far: 9271\n",
      "Fetching page 7 for tag [text-classification]...\n",
      "✅ Total collected so far: 9301\n",
      "Fetching page 8 for tag [text-classification]...\n",
      "✅ Total collected so far: 9331\n",
      "Fetching page 9 for tag [text-classification]...\n",
      "✅ Total collected so far: 9361\n",
      "Fetching page 10 for tag [text-classification]...\n",
      "✅ Total collected so far: 9391\n",
      "Fetching page 11 for tag [text-classification]...\n",
      "✅ Total collected so far: 9421\n",
      "Fetching page 12 for tag [text-classification]...\n",
      "✅ Total collected so far: 9451\n",
      "Fetching page 13 for tag [text-classification]...\n",
      "✅ Total collected so far: 9481\n",
      "Fetching page 14 for tag [text-classification]...\n",
      "✅ Total collected so far: 9511\n",
      "Fetching page 15 for tag [text-classification]...\n",
      "✅ Total collected so far: 9541\n",
      "Fetching page 16 for tag [text-classification]...\n",
      "✅ Total collected so far: 9571\n",
      "Fetching page 17 for tag [text-classification]...\n",
      "✅ Total collected so far: 9600\n",
      "Fetching page 18 for tag [text-classification]...\n",
      "No items found for tag [text-classification], page 18\n",
      "🎉 Done! Saved 9600 valid Q&A posts to stackoverflow_nlp_posts_with_answers.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"nlp\", \"nltk\", \"spacy\", \"transformers\", \"text-classification\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\"✅ Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\"🎉 Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c0190f",
   "metadata": {},
   "source": [
    "the beelow code was used to convert the NLP json to CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d118903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ CSV saved successfully as: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5f9e3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>tags</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>accepted_answer_body</th>\n",
       "      <th>link</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79549787</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>&lt;p&gt;I'm using spaCy with the pl_core_news_lg mo...</td>\n",
       "      <td>['python', 'nlp', 'spacy', 'presidio']</td>\n",
       "      <td>79552218</td>\n",
       "      <td>&lt;p&gt;The configuration file is missing the 'labe...</td>\n",
       "      <td>https://stackoverflow.com/questions/79549787</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79548202</td>\n",
       "      <td>GPT-2 and other models from huggingface -100 l...</td>\n",
       "      <td>&lt;p&gt;I understand the -100 label id is used so t...</td>\n",
       "      <td>['nlp', 'huggingface-transformers', 'pre-train...</td>\n",
       "      <td>79551169</td>\n",
       "      <td>&lt;p&gt;The author of the tutorial you mentioned se...</td>\n",
       "      <td>https://stackoverflow.com/questions/79548202</td>\n",
       "      <td>nlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79523269</td>\n",
       "      <td>Trouble getting importing gensim to work in colab</td>\n",
       "      <td>&lt;p&gt;I am trying to import gensim into colab.&lt;/p...</td>\n",
       "      <td>['numpy', 'nlp', 'dependencies', 'google-colab...</td>\n",
       "      <td>79523777</td>\n",
       "      <td>&lt;p&gt;You have to restart the session for the und...</td>\n",
       "      <td>https://stackoverflow.com/questions/79523269</td>\n",
       "      <td>numpy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79501178</td>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>&lt;p&gt;I am running the code found on this &lt;a href...</td>\n",
       "      <td>['python', 'nlp', 'large-language-model']</td>\n",
       "      <td>79501337</td>\n",
       "      <td>&lt;p&gt;I can't test it but ...&lt;/p&gt;\\n&lt;p&gt;I checked &lt;...</td>\n",
       "      <td>https://stackoverflow.com/questions/79501178</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79482283</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>&lt;p&gt;I am using presidio/langchain_experimental ...</td>\n",
       "      <td>['python', 'nlp', 'spacy', 'langchain', 'presi...</td>\n",
       "      <td>79495969</td>\n",
       "      <td>&lt;p&gt;After some test I was able to find the solu...</td>\n",
       "      <td>https://stackoverflow.com/questions/79482283</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9595</th>\n",
       "      <td>17490361</td>\n",
       "      <td>learning, validation, and testing classifier</td>\n",
       "      <td>&lt;p&gt;I'm working on Sentiment Analysis for text ...</td>\n",
       "      <td>['machine-learning', 'text-classification']</td>\n",
       "      <td>17494417</td>\n",
       "      <td>&lt;p&gt;In your example, I don't think there is a m...</td>\n",
       "      <td>https://stackoverflow.com/questions/17490361</td>\n",
       "      <td>machine-learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9596</th>\n",
       "      <td>16823609</td>\n",
       "      <td>Natural Language Processing - Converting Text ...</td>\n",
       "      <td>&lt;p&gt;So I've been working on a natural language ...</td>\n",
       "      <td>['java', 'nlp', 'svm', 'text-classification']</td>\n",
       "      <td>16824208</td>\n",
       "      <td>&lt;p&gt;I'm not sure what values your attributes ca...</td>\n",
       "      <td>https://stackoverflow.com/questions/16823609</td>\n",
       "      <td>java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9597</th>\n",
       "      <td>16694088</td>\n",
       "      <td>How can i classify text documents with using S...</td>\n",
       "      <td>&lt;p&gt;Almost all of the examples are based on num...</td>\n",
       "      <td>['svm', 'knn', 'document-classification', 'tex...</td>\n",
       "      <td>16694673</td>\n",
       "      <td>&lt;p&gt;The common approach is to use a bag of word...</td>\n",
       "      <td>https://stackoverflow.com/questions/16694088</td>\n",
       "      <td>svm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9598</th>\n",
       "      <td>16266842</td>\n",
       "      <td>MAXent classifier NLTK output understand</td>\n",
       "      <td>&lt;p&gt;I am trying to understand the &lt;code&gt;classif...</td>\n",
       "      <td>['python', 'machine-learning', 'nltk', 'text-c...</td>\n",
       "      <td>16310378</td>\n",
       "      <td>&lt;p&gt;It seems that you have two labels, &lt;code&gt;\"R...</td>\n",
       "      <td>https://stackoverflow.com/questions/16266842</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9599</th>\n",
       "      <td>3802232</td>\n",
       "      <td>Detecting random keyboard hits considering QWE...</td>\n",
       "      <td>&lt;p&gt;The &lt;a href=\"http://arxiv.org/abs/1210.5560...</td>\n",
       "      <td>['algorithm', 'n-gram', 'qwerty', 'text-classi...</td>\n",
       "      <td>3803486</td>\n",
       "      <td>&lt;p&gt;If two &lt;a href=\"http://en.wikipedia.org/wik...</td>\n",
       "      <td>https://stackoverflow.com/questions/3802232</td>\n",
       "      <td>algorithm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9600 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      question_id                                              title  \\\n",
       "0        79549787  Why does Presidio with spacy nlp engine not re...   \n",
       "1        79548202  GPT-2 and other models from huggingface -100 l...   \n",
       "2        79523269  Trouble getting importing gensim to work in colab   \n",
       "3        79501178        Store images instead of showing in a server   \n",
       "4        79482283  Presidio with Langchain Experimental does not ...   \n",
       "...           ...                                                ...   \n",
       "9595     17490361       learning, validation, and testing classifier   \n",
       "9596     16823609  Natural Language Processing - Converting Text ...   \n",
       "9597     16694088  How can i classify text documents with using S...   \n",
       "9598     16266842           MAXent classifier NLTK output understand   \n",
       "9599      3802232  Detecting random keyboard hits considering QWE...   \n",
       "\n",
       "                                                   body  \\\n",
       "0     <p>I'm using spaCy with the pl_core_news_lg mo...   \n",
       "1     <p>I understand the -100 label id is used so t...   \n",
       "2     <p>I am trying to import gensim into colab.</p...   \n",
       "3     <p>I am running the code found on this <a href...   \n",
       "4     <p>I am using presidio/langchain_experimental ...   \n",
       "...                                                 ...   \n",
       "9595  <p>I'm working on Sentiment Analysis for text ...   \n",
       "9596  <p>So I've been working on a natural language ...   \n",
       "9597  <p>Almost all of the examples are based on num...   \n",
       "9598  <p>I am trying to understand the <code>classif...   \n",
       "9599  <p>The <a href=\"http://arxiv.org/abs/1210.5560...   \n",
       "\n",
       "                                                   tags  accepted_answer_id  \\\n",
       "0                ['python', 'nlp', 'spacy', 'presidio']            79552218   \n",
       "1     ['nlp', 'huggingface-transformers', 'pre-train...            79551169   \n",
       "2     ['numpy', 'nlp', 'dependencies', 'google-colab...            79523777   \n",
       "3             ['python', 'nlp', 'large-language-model']            79501337   \n",
       "4     ['python', 'nlp', 'spacy', 'langchain', 'presi...            79495969   \n",
       "...                                                 ...                 ...   \n",
       "9595        ['machine-learning', 'text-classification']            17494417   \n",
       "9596      ['java', 'nlp', 'svm', 'text-classification']            16824208   \n",
       "9597  ['svm', 'knn', 'document-classification', 'tex...            16694673   \n",
       "9598  ['python', 'machine-learning', 'nltk', 'text-c...            16310378   \n",
       "9599  ['algorithm', 'n-gram', 'qwerty', 'text-classi...             3803486   \n",
       "\n",
       "                                   accepted_answer_body  \\\n",
       "0     <p>The configuration file is missing the 'labe...   \n",
       "1     <p>The author of the tutorial you mentioned se...   \n",
       "2     <p>You have to restart the session for the und...   \n",
       "3     <p>I can't test it but ...</p>\\n<p>I checked <...   \n",
       "4     <p>After some test I was able to find the solu...   \n",
       "...                                                 ...   \n",
       "9595  <p>In your example, I don't think there is a m...   \n",
       "9596  <p>I'm not sure what values your attributes ca...   \n",
       "9597  <p>The common approach is to use a bag of word...   \n",
       "9598  <p>It seems that you have two labels, <code>\"R...   \n",
       "9599  <p>If two <a href=\"http://en.wikipedia.org/wik...   \n",
       "\n",
       "                                              link               tag  \n",
       "0     https://stackoverflow.com/questions/79549787            python  \n",
       "1     https://stackoverflow.com/questions/79548202               nlp  \n",
       "2     https://stackoverflow.com/questions/79523269             numpy  \n",
       "3     https://stackoverflow.com/questions/79501178            python  \n",
       "4     https://stackoverflow.com/questions/79482283            python  \n",
       "...                                            ...               ...  \n",
       "9595  https://stackoverflow.com/questions/17490361  machine-learning  \n",
       "9596  https://stackoverflow.com/questions/16823609              java  \n",
       "9597  https://stackoverflow.com/questions/16694088               svm  \n",
       "9598  https://stackoverflow.com/questions/16266842            python  \n",
       "9599   https://stackoverflow.com/questions/3802232         algorithm  \n",
       "\n",
       "[9600 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"stackoverflow_nlp_posts_with_answers.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25010e4",
   "metadata": {},
   "source": [
    "the below code was used to get nltk requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a3c8443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [nltk]...\n",
      "✅ Total collected so far: 26\n",
      "Fetching page 2 for tag [nltk]...\n",
      "✅ Total collected so far: 51\n",
      "Fetching page 3 for tag [nltk]...\n",
      "✅ Total collected so far: 81\n",
      "Fetching page 4 for tag [nltk]...\n",
      "✅ Total collected so far: 111\n",
      "Fetching page 5 for tag [nltk]...\n",
      "✅ Total collected so far: 141\n",
      "Fetching page 6 for tag [nltk]...\n",
      "✅ Total collected so far: 171\n",
      "Fetching page 7 for tag [nltk]...\n",
      "✅ Total collected so far: 201\n",
      "Fetching page 8 for tag [nltk]...\n",
      "✅ Total collected so far: 231\n",
      "Fetching page 9 for tag [nltk]...\n",
      "✅ Total collected so far: 261\n",
      "Fetching page 10 for tag [nltk]...\n",
      "✅ Total collected so far: 291\n",
      "Fetching page 11 for tag [nltk]...\n",
      "✅ Total collected so far: 321\n",
      "Fetching page 12 for tag [nltk]...\n",
      "✅ Total collected so far: 351\n",
      "Fetching page 13 for tag [nltk]...\n",
      "✅ Total collected so far: 381\n",
      "Fetching page 14 for tag [nltk]...\n",
      "✅ Total collected so far: 411\n",
      "Fetching page 15 for tag [nltk]...\n",
      "✅ Total collected so far: 441\n",
      "Fetching page 16 for tag [nltk]...\n",
      "✅ Total collected so far: 471\n",
      "Fetching page 17 for tag [nltk]...\n",
      "✅ Total collected so far: 501\n",
      "Fetching page 18 for tag [nltk]...\n",
      "✅ Total collected so far: 531\n",
      "Fetching page 19 for tag [nltk]...\n",
      "✅ Total collected so far: 561\n",
      "Fetching page 20 for tag [nltk]...\n",
      "✅ Total collected so far: 591\n",
      "Fetching page 21 for tag [nltk]...\n",
      "✅ Total collected so far: 621\n",
      "Fetching page 22 for tag [nltk]...\n",
      "✅ Total collected so far: 651\n",
      "Fetching page 23 for tag [nltk]...\n",
      "✅ Total collected so far: 681\n",
      "Fetching page 24 for tag [nltk]...\n",
      "✅ Total collected so far: 711\n",
      "Fetching page 25 for tag [nltk]...\n",
      "✅ Total collected so far: 741\n",
      "Fetching page 26 for tag [nltk]...\n",
      "✅ Total collected so far: 771\n",
      "Fetching page 27 for tag [nltk]...\n",
      "✅ Total collected so far: 801\n",
      "Fetching page 28 for tag [nltk]...\n",
      "✅ Total collected so far: 831\n",
      "Fetching page 29 for tag [nltk]...\n",
      "✅ Total collected so far: 861\n",
      "Fetching page 30 for tag [nltk]...\n",
      "✅ Total collected so far: 891\n",
      "Fetching page 31 for tag [nltk]...\n",
      "✅ Total collected so far: 921\n",
      "Fetching page 32 for tag [nltk]...\n",
      "✅ Total collected so far: 951\n",
      "Fetching page 33 for tag [nltk]...\n",
      "✅ Total collected so far: 981\n",
      "Fetching page 34 for tag [nltk]...\n",
      "✅ Total collected so far: 1011\n",
      "Fetching page 35 for tag [nltk]...\n",
      "✅ Total collected so far: 1041\n",
      "Fetching page 36 for tag [nltk]...\n",
      "✅ Total collected so far: 1071\n",
      "Fetching page 37 for tag [nltk]...\n",
      "✅ Total collected so far: 1101\n",
      "Fetching page 38 for tag [nltk]...\n",
      "✅ Total collected so far: 1131\n",
      "Fetching page 39 for tag [nltk]...\n",
      "✅ Total collected so far: 1161\n",
      "Fetching page 40 for tag [nltk]...\n",
      "✅ Total collected so far: 1191\n",
      "Fetching page 41 for tag [nltk]...\n",
      "✅ Total collected so far: 1221\n",
      "Fetching page 42 for tag [nltk]...\n",
      "✅ Total collected so far: 1251\n",
      "Fetching page 43 for tag [nltk]...\n",
      "✅ Total collected so far: 1281\n",
      "Fetching page 44 for tag [nltk]...\n",
      "✅ Total collected so far: 1311\n",
      "Fetching page 45 for tag [nltk]...\n",
      "✅ Total collected so far: 1341\n",
      "Fetching page 46 for tag [nltk]...\n",
      "✅ Total collected so far: 1371\n",
      "Fetching page 47 for tag [nltk]...\n",
      "✅ Total collected so far: 1401\n",
      "Fetching page 48 for tag [nltk]...\n",
      "✅ Total collected so far: 1431\n",
      "Fetching page 49 for tag [nltk]...\n",
      "✅ Total collected so far: 1461\n",
      "Fetching page 50 for tag [nltk]...\n",
      "✅ Total collected so far: 1491\n",
      "Fetching page 51 for tag [nltk]...\n",
      "✅ Total collected so far: 1521\n",
      "Fetching page 52 for tag [nltk]...\n",
      "✅ Total collected so far: 1551\n",
      "Fetching page 53 for tag [nltk]...\n",
      "✅ Total collected so far: 1581\n",
      "Fetching page 54 for tag [nltk]...\n",
      "✅ Total collected so far: 1611\n",
      "Fetching page 55 for tag [nltk]...\n",
      "✅ Total collected so far: 1641\n",
      "Fetching page 56 for tag [nltk]...\n",
      "✅ Total collected so far: 1671\n",
      "Fetching page 57 for tag [nltk]...\n",
      "✅ Total collected so far: 1701\n",
      "Fetching page 58 for tag [nltk]...\n",
      "✅ Total collected so far: 1731\n",
      "Fetching page 59 for tag [nltk]...\n",
      "✅ Total collected so far: 1761\n",
      "Fetching page 60 for tag [nltk]...\n",
      "✅ Total collected so far: 1791\n",
      "Fetching page 61 for tag [nltk]...\n",
      "✅ Total collected so far: 1821\n",
      "Fetching page 62 for tag [nltk]...\n",
      "✅ Total collected so far: 1851\n",
      "Fetching page 63 for tag [nltk]...\n",
      "✅ Total collected so far: 1881\n",
      "Fetching page 64 for tag [nltk]...\n",
      "✅ Total collected so far: 1911\n",
      "Fetching page 65 for tag [nltk]...\n",
      "✅ Total collected so far: 1941\n",
      "Fetching page 66 for tag [nltk]...\n",
      "✅ Total collected so far: 1971\n",
      "Fetching page 67 for tag [nltk]...\n",
      "✅ Total collected so far: 2001\n",
      "Fetching page 68 for tag [nltk]...\n",
      "✅ Total collected so far: 2031\n",
      "Fetching page 69 for tag [nltk]...\n",
      "✅ Total collected so far: 2061\n",
      "Fetching page 70 for tag [nltk]...\n",
      "✅ Total collected so far: 2091\n",
      "Fetching page 71 for tag [nltk]...\n",
      "✅ Total collected so far: 2121\n",
      "Fetching page 72 for tag [nltk]...\n",
      "✅ Total collected so far: 2124\n",
      "Fetching page 73 for tag [nltk]...\n",
      "No items found for tag [nltk], page 73\n",
      "🎉 Done! Saved 2124 valid Q&A posts to stackoverflow_nlp_posts_with_answers_nltk.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [ \"nltk\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_nltk.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\"✅ Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\"🎉 Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9fae232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_nltk.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_nltk.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_nltk.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ CSV saved successfully as: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d314596",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nltk = pd.read_csv(\"stackoverflow_nlp_posts_with_answers_nltk.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f811fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>tags</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>accepted_answer_body</th>\n",
       "      <th>link</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79485382</td>\n",
       "      <td>nltk.NaiveBayesClassifier.classify() input par...</td>\n",
       "      <td>&lt;p&gt;I have the following trained classifier:&lt;/p...</td>\n",
       "      <td>['classification', 'nltk', 'naivebayes', 'pyth...</td>\n",
       "      <td>79485768</td>\n",
       "      <td>&lt;p&gt;Use the feature without the label: &lt;code&gt;{'...</td>\n",
       "      <td>https://stackoverflow.com/questions/79485382</td>\n",
       "      <td>classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79312133</td>\n",
       "      <td>Getting all leaf words (reverse stemming) into...</td>\n",
       "      <td>&lt;p&gt;On the same lines as the solution provided ...</td>\n",
       "      <td>['python', 'nlp', 'nltk']</td>\n",
       "      <td>79312987</td>\n",
       "      <td>&lt;p&gt;One solution using nested list comprehensio...</td>\n",
       "      <td>https://stackoverflow.com/questions/79312133</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79293919</td>\n",
       "      <td>Determining most popular words in the English ...</td>\n",
       "      <td>&lt;p&gt;Forgive me if my wording is awful, but I'm ...</td>\n",
       "      <td>['python', 'nlp', 'nltk', 'detection']</td>\n",
       "      <td>79294074</td>\n",
       "      <td>&lt;p&gt;You need a external dataset for this task. ...</td>\n",
       "      <td>https://stackoverflow.com/questions/79293919</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79229713</td>\n",
       "      <td>define equality predicate Lambda-Calculus nltk</td>\n",
       "      <td>&lt;p&gt;I am trying to define a Lambda-Calculus rep...</td>\n",
       "      <td>['python', 'nltk', 'grammar', 'lambda-calculus']</td>\n",
       "      <td>79230197</td>\n",
       "      <td>&lt;p&gt;By its syntactic definition, &lt;code&gt;are&lt;/cod...</td>\n",
       "      <td>https://stackoverflow.com/questions/79229713</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78884251</td>\n",
       "      <td>Unable to install wordnet with nltk 3.9.0 as i...</td>\n",
       "      <td>&lt;p&gt;It is not possible to import nltk, and the ...</td>\n",
       "      <td>['python', 'nltk', 'wordnet']</td>\n",
       "      <td>78884294</td>\n",
       "      <td>&lt;p&gt;This bug was introduced in nltk 3.9.0 (rele...</td>\n",
       "      <td>https://stackoverflow.com/questions/78884251</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                              title  \\\n",
       "0     79485382  nltk.NaiveBayesClassifier.classify() input par...   \n",
       "1     79312133  Getting all leaf words (reverse stemming) into...   \n",
       "2     79293919  Determining most popular words in the English ...   \n",
       "3     79229713     define equality predicate Lambda-Calculus nltk   \n",
       "4     78884251  Unable to install wordnet with nltk 3.9.0 as i...   \n",
       "\n",
       "                                                body  \\\n",
       "0  <p>I have the following trained classifier:</p...   \n",
       "1  <p>On the same lines as the solution provided ...   \n",
       "2  <p>Forgive me if my wording is awful, but I'm ...   \n",
       "3  <p>I am trying to define a Lambda-Calculus rep...   \n",
       "4  <p>It is not possible to import nltk, and the ...   \n",
       "\n",
       "                                                tags  accepted_answer_id  \\\n",
       "0  ['classification', 'nltk', 'naivebayes', 'pyth...            79485768   \n",
       "1                          ['python', 'nlp', 'nltk']            79312987   \n",
       "2             ['python', 'nlp', 'nltk', 'detection']            79294074   \n",
       "3   ['python', 'nltk', 'grammar', 'lambda-calculus']            79230197   \n",
       "4                      ['python', 'nltk', 'wordnet']            78884294   \n",
       "\n",
       "                                accepted_answer_body  \\\n",
       "0  <p>Use the feature without the label: <code>{'...   \n",
       "1  <p>One solution using nested list comprehensio...   \n",
       "2  <p>You need a external dataset for this task. ...   \n",
       "3  <p>By its syntactic definition, <code>are</cod...   \n",
       "4  <p>This bug was introduced in nltk 3.9.0 (rele...   \n",
       "\n",
       "                                           link             tag  \n",
       "0  https://stackoverflow.com/questions/79485382  classification  \n",
       "1  https://stackoverflow.com/questions/79312133          python  \n",
       "2  https://stackoverflow.com/questions/79293919          python  \n",
       "3  https://stackoverflow.com/questions/79229713          python  \n",
       "4  https://stackoverflow.com/questions/78884251          python  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nltk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "808eb5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2124, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nltk.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e01ec",
   "metadata": {},
   "source": [
    "the below code will be used to get spacy json \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e17df9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [spacy]...\n",
      "✅ Total collected so far: 22\n",
      "Fetching page 2 for tag [spacy]...\n",
      "✅ Total collected so far: 51\n",
      "Fetching page 3 for tag [spacy]...\n",
      "✅ Total collected so far: 81\n",
      "Fetching page 4 for tag [spacy]...\n",
      "✅ Total collected so far: 105\n",
      "Fetching page 5 for tag [spacy]...\n",
      "✅ Total collected so far: 135\n",
      "Fetching page 6 for tag [spacy]...\n",
      "✅ Total collected so far: 165\n",
      "Fetching page 7 for tag [spacy]...\n",
      "✅ Total collected so far: 195\n",
      "Fetching page 8 for tag [spacy]...\n",
      "✅ Total collected so far: 225\n",
      "Fetching page 9 for tag [spacy]...\n",
      "✅ Total collected so far: 255\n",
      "Fetching page 10 for tag [spacy]...\n",
      "✅ Total collected so far: 285\n",
      "Fetching page 11 for tag [spacy]...\n",
      "✅ Total collected so far: 315\n",
      "Fetching page 12 for tag [spacy]...\n",
      "✅ Total collected so far: 345\n",
      "Fetching page 13 for tag [spacy]...\n",
      "✅ Total collected so far: 375\n",
      "Fetching page 14 for tag [spacy]...\n",
      "✅ Total collected so far: 405\n",
      "Fetching page 15 for tag [spacy]...\n",
      "✅ Total collected so far: 435\n",
      "Fetching page 16 for tag [spacy]...\n",
      "✅ Total collected so far: 465\n",
      "Fetching page 17 for tag [spacy]...\n",
      "✅ Total collected so far: 495\n",
      "Fetching page 18 for tag [spacy]...\n",
      "✅ Total collected so far: 525\n",
      "Fetching page 19 for tag [spacy]...\n",
      "✅ Total collected so far: 555\n",
      "Fetching page 20 for tag [spacy]...\n",
      "✅ Total collected so far: 585\n",
      "Fetching page 21 for tag [spacy]...\n",
      "✅ Total collected so far: 615\n",
      "Fetching page 22 for tag [spacy]...\n",
      "✅ Total collected so far: 645\n",
      "Fetching page 23 for tag [spacy]...\n",
      "✅ Total collected so far: 675\n",
      "Fetching page 24 for tag [spacy]...\n",
      "✅ Total collected so far: 705\n",
      "Fetching page 25 for tag [spacy]...\n",
      "✅ Total collected so far: 735\n",
      "Fetching page 26 for tag [spacy]...\n",
      "✅ Total collected so far: 765\n",
      "Fetching page 27 for tag [spacy]...\n",
      "✅ Total collected so far: 795\n",
      "Fetching page 28 for tag [spacy]...\n",
      "✅ Total collected so far: 825\n",
      "Fetching page 29 for tag [spacy]...\n",
      "✅ Total collected so far: 855\n",
      "Fetching page 30 for tag [spacy]...\n",
      "✅ Total collected so far: 885\n",
      "Fetching page 31 for tag [spacy]...\n",
      "✅ Total collected so far: 915\n",
      "Fetching page 32 for tag [spacy]...\n",
      "✅ Total collected so far: 945\n",
      "Fetching page 33 for tag [spacy]...\n",
      "✅ Total collected so far: 975\n",
      "Fetching page 34 for tag [spacy]...\n",
      "✅ Total collected so far: 1005\n",
      "Fetching page 35 for tag [spacy]...\n",
      "✅ Total collected so far: 1035\n",
      "Fetching page 36 for tag [spacy]...\n",
      "✅ Total collected so far: 1065\n",
      "Fetching page 37 for tag [spacy]...\n",
      "✅ Total collected so far: 1095\n",
      "Fetching page 38 for tag [spacy]...\n",
      "✅ Total collected so far: 1125\n",
      "Fetching page 39 for tag [spacy]...\n",
      "✅ Total collected so far: 1134\n",
      "Fetching page 40 for tag [spacy]...\n",
      "No items found for tag [spacy], page 40\n",
      "🎉 Done! Saved 1134 valid Q&A posts to stackoverflow_nlp_posts_with_answers_spacy.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"spacy\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_spacy.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\"✅ Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\"🎉 Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b98a69",
   "metadata": {},
   "source": [
    "the below code was used to convert spacy json to CSV \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c05e4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_spacy.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_spacy.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_spacy.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ CSV saved successfully as: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b23917d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spacy = pd.read_csv(\"stackoverflow_nlp_posts_with_answers_spacy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33731c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>tags</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>accepted_answer_body</th>\n",
       "      <th>link</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79549787</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>&lt;p&gt;I'm using spaCy with the pl_core_news_lg mo...</td>\n",
       "      <td>['python', 'nlp', 'spacy', 'presidio']</td>\n",
       "      <td>79552218</td>\n",
       "      <td>&lt;p&gt;The configuration file is missing the 'labe...</td>\n",
       "      <td>https://stackoverflow.com/questions/79549787</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79482283</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>&lt;p&gt;I am using presidio/langchain_experimental ...</td>\n",
       "      <td>['python', 'nlp', 'spacy', 'langchain', 'presi...</td>\n",
       "      <td>79495969</td>\n",
       "      <td>&lt;p&gt;After some test I was able to find the solu...</td>\n",
       "      <td>https://stackoverflow.com/questions/79482283</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79330953</td>\n",
       "      <td>Lemma of puncutation in spacy</td>\n",
       "      <td>&lt;p&gt;I'm using spacy for some downstream tasks, ...</td>\n",
       "      <td>['python', 'spacy', 'lemmatization']</td>\n",
       "      <td>79331038</td>\n",
       "      <td>&lt;p&gt;I can confirm the issue with German, but wh...</td>\n",
       "      <td>https://stackoverflow.com/questions/79330953</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79292283</td>\n",
       "      <td>Attaching custom KB to Spacy &amp;quot;entity_link...</td>\n",
       "      <td>&lt;p&gt;I want to run an entity linking job using a...</td>\n",
       "      <td>['spacy', 'named-entity-recognition', 'entity-...</td>\n",
       "      <td>79293967</td>\n",
       "      <td>&lt;p&gt;What happens here is that this line&lt;/p&gt;\\n&lt;p...</td>\n",
       "      <td>https://stackoverflow.com/questions/79292283</td>\n",
       "      <td>spacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79159805</td>\n",
       "      <td>How can I share a complex spaCy NLP model acro...</td>\n",
       "      <td>&lt;p&gt;I'm working on a multiprocessing python app...</td>\n",
       "      <td>['nlp', 'multiprocessing', 'python-multiproces...</td>\n",
       "      <td>79162232</td>\n",
       "      <td>&lt;p&gt;I would strongly advise you not to treat NL...</td>\n",
       "      <td>https://stackoverflow.com/questions/79159805</td>\n",
       "      <td>nlp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                              title  \\\n",
       "0     79549787  Why does Presidio with spacy nlp engine not re...   \n",
       "1     79482283  Presidio with Langchain Experimental does not ...   \n",
       "2     79330953                      Lemma of puncutation in spacy   \n",
       "3     79292283  Attaching custom KB to Spacy &quot;entity_link...   \n",
       "4     79159805  How can I share a complex spaCy NLP model acro...   \n",
       "\n",
       "                                                body  \\\n",
       "0  <p>I'm using spaCy with the pl_core_news_lg mo...   \n",
       "1  <p>I am using presidio/langchain_experimental ...   \n",
       "2  <p>I'm using spacy for some downstream tasks, ...   \n",
       "3  <p>I want to run an entity linking job using a...   \n",
       "4  <p>I'm working on a multiprocessing python app...   \n",
       "\n",
       "                                                tags  accepted_answer_id  \\\n",
       "0             ['python', 'nlp', 'spacy', 'presidio']            79552218   \n",
       "1  ['python', 'nlp', 'spacy', 'langchain', 'presi...            79495969   \n",
       "2               ['python', 'spacy', 'lemmatization']            79331038   \n",
       "3  ['spacy', 'named-entity-recognition', 'entity-...            79293967   \n",
       "4  ['nlp', 'multiprocessing', 'python-multiproces...            79162232   \n",
       "\n",
       "                                accepted_answer_body  \\\n",
       "0  <p>The configuration file is missing the 'labe...   \n",
       "1  <p>After some test I was able to find the solu...   \n",
       "2  <p>I can confirm the issue with German, but wh...   \n",
       "3  <p>What happens here is that this line</p>\\n<p...   \n",
       "4  <p>I would strongly advise you not to treat NL...   \n",
       "\n",
       "                                           link     tag  \n",
       "0  https://stackoverflow.com/questions/79549787  python  \n",
       "1  https://stackoverflow.com/questions/79482283  python  \n",
       "2  https://stackoverflow.com/questions/79330953  python  \n",
       "3  https://stackoverflow.com/questions/79292283   spacy  \n",
       "4  https://stackoverflow.com/questions/79159805     nlp  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spacy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220abf75",
   "metadata": {},
   "source": [
    "the below code is used to pull text_classification json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "026a32d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [text-classification]...\n",
      "✅ Total collected so far: 28\n",
      "Fetching page 2 for tag [text-classification]...\n",
      "✅ Total collected so far: 51\n",
      "Fetching page 3 for tag [text-classification]...\n",
      "✅ Total collected so far: 74\n",
      "Fetching page 4 for tag [text-classification]...\n",
      "✅ Total collected so far: 104\n",
      "Fetching page 5 for tag [text-classification]...\n",
      "✅ Total collected so far: 134\n",
      "Fetching page 6 for tag [text-classification]...\n",
      "✅ Total collected so far: 164\n",
      "Fetching page 7 for tag [text-classification]...\n",
      "✅ Total collected so far: 194\n",
      "Fetching page 8 for tag [text-classification]...\n",
      "✅ Total collected so far: 224\n",
      "Fetching page 9 for tag [text-classification]...\n",
      "✅ Total collected so far: 254\n",
      "Fetching page 10 for tag [text-classification]...\n",
      "✅ Total collected so far: 284\n",
      "Fetching page 11 for tag [text-classification]...\n",
      "✅ Total collected so far: 314\n",
      "Fetching page 12 for tag [text-classification]...\n",
      "✅ Total collected so far: 344\n",
      "Fetching page 13 for tag [text-classification]...\n",
      "✅ Total collected so far: 374\n",
      "Fetching page 14 for tag [text-classification]...\n",
      "✅ Total collected so far: 404\n",
      "Fetching page 15 for tag [text-classification]...\n",
      "✅ Total collected so far: 434\n",
      "Fetching page 16 for tag [text-classification]...\n",
      "✅ Total collected so far: 464\n",
      "Fetching page 17 for tag [text-classification]...\n",
      "✅ Total collected so far: 493\n",
      "Fetching page 18 for tag [text-classification]...\n",
      "No items found for tag [text-classification], page 18\n",
      "🎉 Done! Saved 493 valid Q&A posts to stackoverflow_nlp_posts_with_answers_text-classification.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"text-classification\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_text-classification.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\"✅ Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\"🎉 Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35feb089",
   "metadata": {},
   "source": [
    "text-classification CSV Conversion below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f288a382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_text-classification.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_text-classification.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_text-classification.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ CSV saved successfully as: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13bb23af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>tags</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>accepted_answer_body</th>\n",
       "      <th>link</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79247672</td>\n",
       "      <td>Error in getting Captum text explanations for ...</td>\n",
       "      <td>&lt;p&gt;I have the following code that I am using t...</td>\n",
       "      <td>['machine-learning', 'pytorch', 'nlp', 'huggin...</td>\n",
       "      <td>79248379</td>\n",
       "      <td>&lt;p&gt;You need to slightly change the gradients c...</td>\n",
       "      <td>https://stackoverflow.com/questions/79247672</td>\n",
       "      <td>machine-learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79247594</td>\n",
       "      <td>euclidian distance from word to sentence after...</td>\n",
       "      <td>&lt;p&gt;I have dataframe with 1000 text rows.&lt;/p&gt;\\n...</td>\n",
       "      <td>['pandas', 'dataframe', 'nlp', 'text-classific...</td>\n",
       "      <td>79248087</td>\n",
       "      <td>&lt;p&gt;I am not convinced that the Euclidean dista...</td>\n",
       "      <td>https://stackoverflow.com/questions/79247594</td>\n",
       "      <td>pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79192127</td>\n",
       "      <td>How can I get the confidence variable from a C...</td>\n",
       "      <td>&lt;p&gt;I am using the CreateML tool to train a tex...</td>\n",
       "      <td>['machine-learning', 'text-classification', 'c...</td>\n",
       "      <td>79209144</td>\n",
       "      <td>&lt;p&gt;To best use the text classifiers, you shoul...</td>\n",
       "      <td>https://stackoverflow.com/questions/79192127</td>\n",
       "      <td>machine-learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79016929</td>\n",
       "      <td>Machine learning model predicts training label...</td>\n",
       "      <td>&lt;p&gt;I am trying to build a model to predict &amp;qu...</td>\n",
       "      <td>['python', 'machine-learning', 'text-classific...</td>\n",
       "      <td>79017406</td>\n",
       "      <td>&lt;p&gt;The problem is that you read the test data ...</td>\n",
       "      <td>https://stackoverflow.com/questions/79016929</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77879635</td>\n",
       "      <td>How to reset parameters from AutoModelForSeque...</td>\n",
       "      <td>&lt;p&gt;Currently to reinitialize a model for &lt;code...</td>\n",
       "      <td>['python', 'machine-learning', 'huggingface-tr...</td>\n",
       "      <td>77879847</td>\n",
       "      <td>&lt;p&gt;That is the purpose of &lt;a href=\"https://hug...</td>\n",
       "      <td>https://stackoverflow.com/questions/77879635</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                              title  \\\n",
       "0     79247672  Error in getting Captum text explanations for ...   \n",
       "1     79247594  euclidian distance from word to sentence after...   \n",
       "2     79192127  How can I get the confidence variable from a C...   \n",
       "3     79016929  Machine learning model predicts training label...   \n",
       "4     77879635  How to reset parameters from AutoModelForSeque...   \n",
       "\n",
       "                                                body  \\\n",
       "0  <p>I have the following code that I am using t...   \n",
       "1  <p>I have dataframe with 1000 text rows.</p>\\n...   \n",
       "2  <p>I am using the CreateML tool to train a tex...   \n",
       "3  <p>I am trying to build a model to predict &qu...   \n",
       "4  <p>Currently to reinitialize a model for <code...   \n",
       "\n",
       "                                                tags  accepted_answer_id  \\\n",
       "0  ['machine-learning', 'pytorch', 'nlp', 'huggin...            79248379   \n",
       "1  ['pandas', 'dataframe', 'nlp', 'text-classific...            79248087   \n",
       "2  ['machine-learning', 'text-classification', 'c...            79209144   \n",
       "3  ['python', 'machine-learning', 'text-classific...            79017406   \n",
       "4  ['python', 'machine-learning', 'huggingface-tr...            77879847   \n",
       "\n",
       "                                accepted_answer_body  \\\n",
       "0  <p>You need to slightly change the gradients c...   \n",
       "1  <p>I am not convinced that the Euclidean dista...   \n",
       "2  <p>To best use the text classifiers, you shoul...   \n",
       "3  <p>The problem is that you read the test data ...   \n",
       "4  <p>That is the purpose of <a href=\"https://hug...   \n",
       "\n",
       "                                           link               tag  \n",
       "0  https://stackoverflow.com/questions/79247672  machine-learning  \n",
       "1  https://stackoverflow.com/questions/79247594            pandas  \n",
       "2  https://stackoverflow.com/questions/79192127  machine-learning  \n",
       "3  https://stackoverflow.com/questions/79016929            python  \n",
       "4  https://stackoverflow.com/questions/77879635            python  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_classification = pd.read_csv(\"stackoverflow_nlp_posts_with_answers_text-classification.csv\")\n",
    "df_text_classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e9afe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [lemmatization]...\n",
      "✅ Total collected so far: 30\n",
      "Fetching page 2 for tag [lemmatization]...\n",
      "✅ Total collected so far: 60\n",
      "Fetching page 3 for tag [lemmatization]...\n",
      "✅ Total collected so far: 90\n",
      "Fetching page 4 for tag [lemmatization]...\n",
      "✅ Total collected so far: 120\n",
      "Fetching page 5 for tag [lemmatization]...\n",
      "✅ Total collected so far: 141\n",
      "Fetching page 6 for tag [lemmatization]...\n",
      "No items found for tag [lemmatization], page 6\n",
      "🎉 Done! Saved 141 valid Q&A posts to stackoverflow_nlp_posts_with_answers_lemmatization.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"lemmatization\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_lemmatization.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\"✅ Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\"🎉 Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "359a35df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_lemmatization.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_lemmatization.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_lemmatization.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ CSV saved successfully as: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69cc61f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>tags</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>accepted_answer_body</th>\n",
       "      <th>link</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79330953</td>\n",
       "      <td>Lemma of puncutation in spacy</td>\n",
       "      <td>&lt;p&gt;I'm using spacy for some downstream tasks, ...</td>\n",
       "      <td>['python', 'spacy', 'lemmatization']</td>\n",
       "      <td>79331038</td>\n",
       "      <td>&lt;p&gt;I can confirm the issue with German, but wh...</td>\n",
       "      <td>https://stackoverflow.com/questions/79330953</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79081924</td>\n",
       "      <td>With spaCy, how can I get all lemmas from a st...</td>\n",
       "      <td>&lt;p&gt;I have a pandas data frame with a column of...</td>\n",
       "      <td>['python', 'pandas', 'nlp', 'spacy', 'lemmatiz...</td>\n",
       "      <td>79086290</td>\n",
       "      <td>&lt;p&gt;There are many ways to speed up SpaCy proce...</td>\n",
       "      <td>https://stackoverflow.com/questions/79081924</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78489915</td>\n",
       "      <td>How to lemmatize text column in pandas datafra...</td>\n",
       "      <td>&lt;p&gt;I read csv file into pandas dataframe.&lt;/p&gt;\\...</td>\n",
       "      <td>['pandas', 'nlp', 'tokenize', 'lemmatization',...</td>\n",
       "      <td>78491545</td>\n",
       "      <td>&lt;p&gt;No, you don't necessarily have to tokenize ...</td>\n",
       "      <td>https://stackoverflow.com/questions/78489915</td>\n",
       "      <td>pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78278881</td>\n",
       "      <td>How to speed up the lemmatization of a Serie i...</td>\n",
       "      <td>&lt;p&gt;I got this line that lemmatize a serie of a...</td>\n",
       "      <td>['python', 'dataframe', 'optimization', 'serie...</td>\n",
       "      <td>78304621</td>\n",
       "      <td>&lt;p&gt;I found a way to speed up this. In my case,...</td>\n",
       "      <td>https://stackoverflow.com/questions/78278881</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78215873</td>\n",
       "      <td>Comparison between stemmiation and lemmatization</td>\n",
       "      <td>&lt;p&gt;Based on several research , i found followi...</td>\n",
       "      <td>['python', 'nltk', 'stemming', 'lemmatization']</td>\n",
       "      <td>78216510</td>\n",
       "      <td>&lt;p&gt;Here is an example of what parts of speech ...</td>\n",
       "      <td>https://stackoverflow.com/questions/78215873</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                              title  \\\n",
       "0     79330953                      Lemma of puncutation in spacy   \n",
       "1     79081924  With spaCy, how can I get all lemmas from a st...   \n",
       "2     78489915  How to lemmatize text column in pandas datafra...   \n",
       "3     78278881  How to speed up the lemmatization of a Serie i...   \n",
       "4     78215873   Comparison between stemmiation and lemmatization   \n",
       "\n",
       "                                                body  \\\n",
       "0  <p>I'm using spacy for some downstream tasks, ...   \n",
       "1  <p>I have a pandas data frame with a column of...   \n",
       "2  <p>I read csv file into pandas dataframe.</p>\\...   \n",
       "3  <p>I got this line that lemmatize a serie of a...   \n",
       "4  <p>Based on several research , i found followi...   \n",
       "\n",
       "                                                tags  accepted_answer_id  \\\n",
       "0               ['python', 'spacy', 'lemmatization']            79331038   \n",
       "1  ['python', 'pandas', 'nlp', 'spacy', 'lemmatiz...            79086290   \n",
       "2  ['pandas', 'nlp', 'tokenize', 'lemmatization',...            78491545   \n",
       "3  ['python', 'dataframe', 'optimization', 'serie...            78304621   \n",
       "4    ['python', 'nltk', 'stemming', 'lemmatization']            78216510   \n",
       "\n",
       "                                accepted_answer_body  \\\n",
       "0  <p>I can confirm the issue with German, but wh...   \n",
       "1  <p>There are many ways to speed up SpaCy proce...   \n",
       "2  <p>No, you don't necessarily have to tokenize ...   \n",
       "3  <p>I found a way to speed up this. In my case,...   \n",
       "4  <p>Here is an example of what parts of speech ...   \n",
       "\n",
       "                                           link     tag  \n",
       "0  https://stackoverflow.com/questions/79330953  python  \n",
       "1  https://stackoverflow.com/questions/79081924  python  \n",
       "2  https://stackoverflow.com/questions/78489915  pandas  \n",
       "3  https://stackoverflow.com/questions/78278881  python  \n",
       "4  https://stackoverflow.com/questions/78215873  python  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lemmantization = pd.read_csv(\"stackoverflow_nlp_posts_with_answers_lemmatization.csv\")\n",
    "df_lemmantization.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de2feb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Found 8520 existing question IDs to exclude.\n",
      "Fetching page 1 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 0\n",
      "Fetching page 2 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 0\n",
      "Fetching page 3 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 0\n",
      "Fetching page 4 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 0\n",
      "Fetching page 5 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 0\n",
      "Fetching page 6 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 0\n",
      "Fetching page 7 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 0\n",
      "Fetching page 8 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 0\n",
      "Fetching page 9 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 0\n",
      "Fetching page 10 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 0\n",
      "Fetching page 11 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 0\n",
      "Fetching page 12 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 0\n",
      "Fetching page 13 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 2\n",
      "Fetching page 14 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 2\n",
      "Fetching page 15 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 6\n",
      "Fetching page 16 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 9\n",
      "Fetching page 17 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 9\n",
      "Fetching page 18 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 14\n",
      "Fetching page 19 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 16\n",
      "Fetching page 20 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 16\n",
      "Fetching page 21 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 22\n",
      "Fetching page 22 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 22\n",
      "Fetching page 23 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 23\n",
      "Fetching page 24 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 29\n",
      "Fetching page 25 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 29\n",
      "Fetching page 26 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 29\n",
      "Fetching page 27 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 32\n",
      "Fetching page 28 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 37\n",
      "Fetching page 29 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 40\n",
      "Fetching page 30 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 48\n",
      "Fetching page 31 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 51\n",
      "Fetching page 32 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 53\n",
      "Fetching page 33 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 53\n",
      "Fetching page 34 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 57\n",
      "Fetching page 35 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 62\n",
      "Fetching page 36 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 75\n",
      "Fetching page 37 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 78\n",
      "Fetching page 38 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 91\n",
      "Fetching page 39 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 98\n",
      "Fetching page 40 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 107\n",
      "Fetching page 41 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 116\n",
      "Fetching page 42 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 123\n",
      "Fetching page 43 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 135\n",
      "Fetching page 44 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 135\n",
      "Fetching page 45 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 135\n",
      "Fetching page 46 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 146\n",
      "Fetching page 47 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 155\n",
      "Fetching page 48 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 159\n",
      "Fetching page 49 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 164\n",
      "Fetching page 50 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 170\n",
      "Fetching page 51 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 176\n",
      "Fetching page 52 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 186\n",
      "Fetching page 53 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 199\n",
      "Fetching page 54 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 202\n",
      "Fetching page 55 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 211\n",
      "Fetching page 56 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 223\n",
      "Fetching page 57 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 229\n",
      "Fetching page 58 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 236\n",
      "Fetching page 59 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 251\n",
      "Fetching page 60 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 266\n",
      "Fetching page 61 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 274\n",
      "Fetching page 62 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 280\n",
      "Fetching page 63 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 290\n",
      "Fetching page 64 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 305\n",
      "Fetching page 65 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 308\n",
      "Fetching page 66 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 312\n",
      "Fetching page 67 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 320\n",
      "Fetching page 68 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 338\n",
      "Fetching page 69 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 346\n",
      "Fetching page 70 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 362\n",
      "Fetching page 71 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 367\n",
      "Fetching page 72 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 379\n",
      "Fetching page 73 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 390\n",
      "Fetching page 74 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 406\n",
      "Fetching page 75 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 418\n",
      "Fetching page 76 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 426\n",
      "Fetching page 77 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 440\n",
      "Fetching page 78 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 445\n",
      "Fetching page 79 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 460\n",
      "Fetching page 80 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 470\n",
      "Fetching page 81 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 475\n",
      "Fetching page 82 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 483\n",
      "Fetching page 83 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 495\n",
      "Fetching page 84 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 495\n",
      "Fetching page 85 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 502\n",
      "Fetching page 86 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 505\n",
      "Fetching page 87 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 514\n",
      "Fetching page 88 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 515\n",
      "Fetching page 89 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 528\n",
      "Fetching page 90 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 540\n",
      "Fetching page 91 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 541\n",
      "Fetching page 92 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 552\n",
      "Fetching page 93 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 563\n",
      "Fetching page 94 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 565\n",
      "Fetching page 95 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 571\n",
      "Fetching page 96 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 577\n",
      "Fetching page 97 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 587\n",
      "Fetching page 98 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 606\n",
      "Fetching page 99 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 616\n",
      "Fetching page 100 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 636\n",
      "Fetching page 101 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 651\n",
      "Fetching page 102 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 660\n",
      "Fetching page 103 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 671\n",
      "Fetching page 104 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 681\n",
      "Fetching page 105 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 681\n",
      "Fetching page 106 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 688\n",
      "Fetching page 107 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 699\n",
      "Fetching page 108 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 710\n",
      "Fetching page 109 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 715\n",
      "Fetching page 110 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 727\n",
      "Fetching page 111 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 736\n",
      "Fetching page 112 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 742\n",
      "Fetching page 113 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 758\n",
      "Fetching page 114 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 772\n",
      "Fetching page 115 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 786\n",
      "Fetching page 116 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 792\n",
      "Fetching page 117 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 805\n",
      "Fetching page 118 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 823\n",
      "Fetching page 119 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 830\n",
      "Fetching page 120 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 845\n",
      "Fetching page 121 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 846\n",
      "Fetching page 122 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 859\n",
      "Fetching page 123 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 866\n",
      "Fetching page 124 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 868\n",
      "Fetching page 125 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 873\n",
      "Fetching page 126 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 876\n",
      "Fetching page 127 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 886\n",
      "Fetching page 128 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 892\n",
      "Fetching page 129 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 897\n",
      "Fetching page 130 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 909\n",
      "Fetching page 131 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 925\n",
      "Fetching page 132 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 936\n",
      "Fetching page 133 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 956\n",
      "Fetching page 134 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 968\n",
      "Fetching page 135 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 975\n",
      "Fetching page 136 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 991\n",
      "Fetching page 137 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1002\n",
      "Fetching page 138 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1011\n",
      "Fetching page 139 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1023\n",
      "Fetching page 140 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1034\n",
      "Fetching page 141 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1043\n",
      "Fetching page 142 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1051\n",
      "Fetching page 143 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1059\n",
      "Fetching page 144 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1065\n",
      "Fetching page 145 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1074\n",
      "Fetching page 146 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1092\n",
      "Fetching page 147 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1101\n",
      "Fetching page 148 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1114\n",
      "Fetching page 149 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1124\n",
      "Fetching page 150 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1143\n",
      "Fetching page 151 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1149\n",
      "Fetching page 152 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1158\n",
      "Fetching page 153 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1169\n",
      "Fetching page 154 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1183\n",
      "Fetching page 155 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1197\n",
      "Fetching page 156 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1211\n",
      "Fetching page 157 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1218\n",
      "Fetching page 158 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1232\n",
      "Fetching page 159 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1243\n",
      "Fetching page 160 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1259\n",
      "Fetching page 161 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1266\n",
      "Fetching page 162 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1272\n",
      "Fetching page 163 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1279\n",
      "Fetching page 164 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1301\n",
      "Fetching page 165 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1321\n",
      "Fetching page 166 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1329\n",
      "Fetching page 167 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1338\n",
      "Fetching page 168 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1349\n",
      "Fetching page 169 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1365\n",
      "Fetching page 170 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1385\n",
      "Fetching page 171 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1406\n",
      "Fetching page 172 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1410\n",
      "Fetching page 173 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1425\n",
      "Fetching page 174 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1438\n",
      "Fetching page 175 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1448\n",
      "Fetching page 176 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1466\n",
      "Fetching page 177 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1481\n",
      "Fetching page 178 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1486\n",
      "Fetching page 179 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1504\n",
      "Fetching page 180 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1523\n",
      "Fetching page 181 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1530\n",
      "Fetching page 182 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1547\n",
      "Fetching page 183 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1559\n",
      "Fetching page 184 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1577\n",
      "Fetching page 185 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1591\n",
      "Fetching page 186 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1609\n",
      "Fetching page 187 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1628\n",
      "Fetching page 188 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1636\n",
      "Fetching page 189 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1658\n",
      "Fetching page 190 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1680\n",
      "Fetching page 191 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1706\n",
      "Fetching page 192 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1724\n",
      "Fetching page 193 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1744\n",
      "Fetching page 194 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1773\n",
      "Fetching page 195 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1800\n",
      "Fetching page 196 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1830\n",
      "Fetching page 197 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1860\n",
      "Fetching page 198 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1890\n",
      "Fetching page 199 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1909\n",
      "Fetching page 200 for tag [nlp]...\n",
      "✅ Total new questions collected so far: 1939\n",
      "🎉 Done! Saved 1939 NEW Q&A posts to stackoverflow_nlp_posts_with_answers_second_set.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAG = \"nlp\"\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "TARGET_COUNT = 20000\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"\n",
    "\n",
    "# Files\n",
    "ORIGINAL_JSON = \"stackoverflow_nlp_posts_with_answers.json\"  # already collected\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_second_set.json\"\n",
    "\n",
    "# === Load existing question IDs to avoid duplicates ===\n",
    "try:\n",
    "    with open(ORIGINAL_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "        existing_data = json.load(f)\n",
    "        existing_ids = {q[\"question_id\"] for q in existing_data}\n",
    "except FileNotFoundError:\n",
    "    print(f\"⚠️ Original file {ORIGINAL_JSON} not found. Proceeding without filtering.\")\n",
    "    existing_ids = set()\n",
    "\n",
    "print(f\"🔍 Found {len(existing_ids)} existing question IDs to exclude.\")\n",
    "\n",
    "# === Initialize ===\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === FETCH QUESTIONS ===\n",
    "for page in range(1, MAX_PAGES + 1):\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    print(f\"Fetching page {page} for tag [{TAG}]...\")\n",
    "\n",
    "    url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "    params = {\n",
    "        \"page\": page,\n",
    "        \"pagesize\": PAGE_SIZE,\n",
    "        \"order\": \"desc\",\n",
    "        \"sort\": \"creation\",\n",
    "        \"tagged\": TAG,\n",
    "        \"site\": SITE,\n",
    "        \"filter\": \"withbody\",\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        data = response.json()\n",
    "    except ValueError:\n",
    "        print(\"⚠️ Invalid JSON response. Skipping this page.\")\n",
    "        continue\n",
    "\n",
    "    if \"items\" not in data or not data[\"items\"]:\n",
    "        print(f\"No items found on page {page}\")\n",
    "        break\n",
    "\n",
    "    # Filter for new, answered questions only\n",
    "    questions_with_answers = [\n",
    "        item for item in data[\"items\"]\n",
    "        if item.get(\"accepted_answer_id\") and item.get(\"question_id\") not in existing_ids\n",
    "    ]\n",
    "\n",
    "    # Batch accepted answer IDs\n",
    "    accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "    id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "    accepted_answers = {}\n",
    "    for batch in id_batches:\n",
    "        ids_str = \";\".join(batch)\n",
    "        ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "        ans_params = {\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "        ans_resp = requests.get(ans_url, params=ans_params)\n",
    "        if ans_resp.status_code == 200:\n",
    "            try:\n",
    "                ans_data = ans_resp.json()\n",
    "                for ans in ans_data.get(\"items\", []):\n",
    "                    accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "            except ValueError:\n",
    "                continue\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    # Store new unique Q&A pairs only\n",
    "    for item in questions_with_answers:\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        aid = item[\"accepted_answer_id\"]\n",
    "        abody = accepted_answers.get(aid)\n",
    "\n",
    "        if abody:\n",
    "            question_id = item.get(\"question_id\")\n",
    "            if question_id in existing_ids:\n",
    "                continue  # double-check\n",
    "\n",
    "            question_data = {\n",
    "                \"question_id\": question_id,\n",
    "                \"title\": item.get(\"title\"),\n",
    "                \"body\": item.get(\"body\"),\n",
    "                \"tags\": item.get(\"tags\"),\n",
    "                \"accepted_answer_id\": aid,\n",
    "                \"accepted_answer_body\": abody,\n",
    "                \"score\": item.get(\"score\")\n",
    "            }\n",
    "            all_questions.append(question_data)\n",
    "            valid_question_count += 1\n",
    "\n",
    "    print(f\"✅ Total new questions collected so far: {valid_question_count}\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO NEW FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\"🎉 Done! Saved {valid_question_count} NEW Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77026e50",
   "metadata": {},
   "source": [
    "nlp tag second set CSV Conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6230308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_second_set.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_second_set.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_second_set.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ CSV saved successfully as: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c44a34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined dataset saved to stackoverflow_nlp_combined.json with 10459 unique questions.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# === File Paths ===\n",
    "FILE_1 = \"stackoverflow_nlp_posts_with_answers.json\"\n",
    "FILE_2 = \"stackoverflow_nlp_posts_with_answers_second_set.json\"\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_combined.json\"\n",
    "\n",
    "# === Load both JSON files ===\n",
    "with open(FILE_1, \"r\", encoding=\"utf-8\") as f1:\n",
    "    data1 = json.load(f1)\n",
    "\n",
    "with open(FILE_2, \"r\", encoding=\"utf-8\") as f2:\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "# === Merge and remove duplicates based on question_id ===\n",
    "combined = {item[\"question_id\"]: item for item in data1}\n",
    "for item in data2:\n",
    "    qid = item[\"question_id\"]\n",
    "    if qid not in combined:\n",
    "        combined[qid] = item\n",
    "\n",
    "# === Convert back to list and save ===\n",
    "merged_data = list(combined.values())\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    json.dump(merged_data, f_out, indent=4)\n",
    "\n",
    "print(f\"✅ Combined dataset saved to {OUTPUT_FILE} with {len(merged_data)} unique questions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf2f6e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [stemming]...\n",
      "✅ Total collected so far: 30\n",
      "Fetching page 2 for tag [stemming]...\n",
      "✅ Total collected so far: 60\n",
      "Fetching page 3 for tag [stemming]...\n",
      "✅ Total collected so far: 90\n",
      "Fetching page 4 for tag [stemming]...\n",
      "✅ Total collected so far: 120\n",
      "Fetching page 5 for tag [stemming]...\n",
      "✅ Total collected so far: 150\n",
      "Fetching page 6 for tag [stemming]...\n",
      "✅ Total collected so far: 170\n",
      "Fetching page 7 for tag [stemming]...\n",
      "No items found for tag [stemming], page 7\n",
      "🎉 Done! Saved 170 valid Q&A posts to stackoverflow_nlp_posts_with_answers_stemming.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"stemming\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_stemming.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\"✅ Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\"🎉 Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e54ba4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_stemming.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_stemming.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_stemming.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ CSV saved successfully as: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52a8f333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [named-entity-recognition]...\n",
      "✅ Total collected so far: 16\n",
      "Fetching page 2 for tag [named-entity-recognition]...\n",
      "✅ Total collected so far: 46\n",
      "Fetching page 3 for tag [named-entity-recognition]...\n",
      "✅ Total collected so far: 76\n",
      "Fetching page 4 for tag [named-entity-recognition]...\n",
      "✅ Total collected so far: 106\n",
      "Fetching page 5 for tag [named-entity-recognition]...\n",
      "✅ Total collected so far: 136\n",
      "Fetching page 6 for tag [named-entity-recognition]...\n",
      "✅ Total collected so far: 166\n",
      "Fetching page 7 for tag [named-entity-recognition]...\n",
      "✅ Total collected so far: 196\n",
      "Fetching page 8 for tag [named-entity-recognition]...\n",
      "✅ Total collected so far: 226\n",
      "Fetching page 9 for tag [named-entity-recognition]...\n",
      "✅ Total collected so far: 256\n",
      "Fetching page 10 for tag [named-entity-recognition]...\n",
      "✅ Total collected so far: 286\n",
      "Fetching page 11 for tag [named-entity-recognition]...\n",
      "✅ Total collected so far: 316\n",
      "Fetching page 12 for tag [named-entity-recognition]...\n",
      "✅ Total collected so far: 339\n",
      "Fetching page 13 for tag [named-entity-recognition]...\n",
      "✅ Total collected so far: 369\n",
      "Fetching page 14 for tag [named-entity-recognition]...\n",
      "✅ Total collected so far: 399\n",
      "Fetching page 15 for tag [named-entity-recognition]...\n",
      "✅ Total collected so far: 429\n",
      "Fetching page 16 for tag [named-entity-recognition]...\n",
      "No items found for tag [named-entity-recognition], page 16\n",
      "🎉 Done! Saved 429 valid Q&A posts to stackoverflow_nlp_posts_with_answers_named-entity-recognition.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"named-entity-recognition\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_named-entity-recognition.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\"✅ Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\"🎉 Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff9e8ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_named-entity-recognition.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_named-entity-recognition.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_named-entity-recognition.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ CSV saved successfully as: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c264c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [language-detection]...\n",
      "✅ Total collected so far: 30\n",
      "Fetching page 2 for tag [language-detection]...\n",
      "✅ Total collected so far: 60\n",
      "Fetching page 3 for tag [language-detection]...\n",
      "No items found for tag [language-detection], page 3\n",
      "🎉 Done! Saved 60 valid Q&A posts to stackoverflow_nlp_posts_with_answers_language-detection.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"language-detection\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_language-detection.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\"✅ Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\"🎉 Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5a0d7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_language-detection.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_language-detection.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_language-detection.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ CSV saved successfully as: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9957d611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [text-summarization]...\n",
      "No items found for tag [text-summarization], page 1\n",
      "🎉 Done! Saved 0 valid Q&A posts to stackoverflow_nlp_posts_with_answers_text-summarization.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"text-summarization\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_text-summarization.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\"✅ Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\"🎉 Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afaa3d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [bert]...\n",
      "✅ Total collected so far: 10\n",
      "Fetching page 2 for tag [bert]...\n",
      "✅ Total collected so far: 30\n",
      "Fetching page 3 for tag [bert]...\n",
      "✅ Total collected so far: 45\n",
      "Fetching page 4 for tag [bert]...\n",
      "✅ Total collected so far: 73\n",
      "Fetching page 5 for tag [bert]...\n",
      "✅ Total collected so far: 99\n",
      "Fetching page 6 for tag [bert]...\n",
      "✅ Total collected so far: 129\n",
      "Fetching page 7 for tag [bert]...\n",
      "✅ Total collected so far: 153\n",
      "Fetching page 8 for tag [bert]...\n",
      "✅ Total collected so far: 183\n",
      "Fetching page 9 for tag [bert]...\n",
      "✅ Total collected so far: 210\n",
      "Fetching page 10 for tag [bert]...\n",
      "✅ Total collected so far: 239\n",
      "Fetching page 11 for tag [bert]...\n",
      "✅ Total collected so far: 269\n",
      "Fetching page 12 for tag [bert]...\n",
      "✅ Total collected so far: 299\n",
      "Fetching page 13 for tag [bert]...\n",
      "✅ Total collected so far: 329\n",
      "Fetching page 14 for tag [bert]...\n",
      "✅ Total collected so far: 357\n",
      "Fetching page 15 for tag [bert]...\n",
      "✅ Total collected so far: 387\n",
      "Fetching page 16 for tag [bert]...\n",
      "✅ Total collected so far: 417\n",
      "Fetching page 17 for tag [bert]...\n",
      "✅ Total collected so far: 447\n",
      "Fetching page 18 for tag [bert]...\n",
      "✅ Total collected so far: 463\n",
      "Fetching page 19 for tag [bert]...\n",
      "No items found for tag [bert], page 19\n",
      "🎉 Done! Saved 463 valid Q&A posts to stackoverflow_nlp_posts_with_answers_bert.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"bert\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_bert.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\"✅ Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\"🎉 Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2dceb9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_bert.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_bert.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_bert.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ CSV saved successfully as: {OUTPUT_CSV}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf54c335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [sentence-transformers]...\n",
      "✅ Total collected so far: 8\n",
      "Fetching page 2 for tag [sentence-transformers]...\n",
      "✅ Total collected so far: 36\n",
      "Fetching page 3 for tag [sentence-transformers]...\n",
      "✅ Total collected so far: 50\n",
      "Fetching page 4 for tag [sentence-transformers]...\n",
      "No items found for tag [sentence-transformers], page 4\n",
      "🎉 Done! Saved 50 valid Q&A posts to stackoverflow_nlp_posts_with_answers_sentence-transformers.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"sentence-transformers\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_sentence-transformers.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\"✅ Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\"🎉 Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30e02c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_sentence-transformers.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_sentence-transformers.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_sentence-transformers.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ CSV saved successfully as: {OUTPUT_CSV}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45c26a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 17\n",
      "Fetching page 2 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 32\n",
      "Fetching page 3 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 58\n",
      "Fetching page 4 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 75\n",
      "Fetching page 5 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 89\n",
      "Fetching page 6 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 100\n",
      "Fetching page 7 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 123\n",
      "Fetching page 8 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 151\n",
      "Fetching page 9 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 174\n",
      "Fetching page 10 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 192\n",
      "Fetching page 11 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 210\n",
      "Fetching page 12 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 236\n",
      "Fetching page 13 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 266\n",
      "Fetching page 14 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 296\n",
      "Fetching page 15 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 326\n",
      "Fetching page 16 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 356\n",
      "Fetching page 17 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 379\n",
      "Fetching page 18 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 405\n",
      "Fetching page 19 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 435\n",
      "Fetching page 20 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 465\n",
      "Fetching page 21 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 495\n",
      "Fetching page 22 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 525\n",
      "Fetching page 23 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 555\n",
      "Fetching page 24 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 585\n",
      "Fetching page 25 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 615\n",
      "Fetching page 26 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 645\n",
      "Fetching page 27 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 675\n",
      "Fetching page 28 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 705\n",
      "Fetching page 29 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 735\n",
      "Fetching page 30 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 765\n",
      "Fetching page 31 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 795\n",
      "Fetching page 32 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 825\n",
      "Fetching page 33 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 855\n",
      "Fetching page 34 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 885\n",
      "Fetching page 35 for tag [huggingface-transformers]...\n",
      "✅ Total collected so far: 915\n",
      "Fetching page 36 for tag [huggingface-transformers]...\n",
      "No items found for tag [huggingface-transformers], page 36\n",
      "🎉 Done! Saved 915 valid Q&A posts to stackoverflow_nlp_posts_with_answers_huggingface-transformers.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"huggingface-transformers\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_huggingface-transformers.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\"✅ Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\"🎉 Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d24f81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_huggingface-transformers.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_huggingface-transformers.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_huggingface-transformers.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ CSV saved successfully as: {OUTPUT_CSV}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77fbc567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [information-retrieval]...\n",
      "✅ Total collected so far: 17\n",
      "Fetching page 2 for tag [information-retrieval]...\n",
      "✅ Total collected so far: 47\n",
      "Fetching page 3 for tag [information-retrieval]...\n",
      "✅ Total collected so far: 77\n",
      "Fetching page 4 for tag [information-retrieval]...\n",
      "✅ Total collected so far: 107\n",
      "Fetching page 5 for tag [information-retrieval]...\n",
      "✅ Total collected so far: 137\n",
      "Fetching page 6 for tag [information-retrieval]...\n",
      "✅ Total collected so far: 167\n",
      "Fetching page 7 for tag [information-retrieval]...\n",
      "✅ Total collected so far: 197\n",
      "Fetching page 8 for tag [information-retrieval]...\n",
      "✅ Total collected so far: 227\n",
      "Fetching page 9 for tag [information-retrieval]...\n",
      "✅ Total collected so far: 257\n",
      "Fetching page 10 for tag [information-retrieval]...\n",
      "✅ Total collected so far: 287\n",
      "Fetching page 11 for tag [information-retrieval]...\n",
      "✅ Total collected so far: 317\n",
      "Fetching page 12 for tag [information-retrieval]...\n",
      "✅ Total collected so far: 347\n",
      "Fetching page 13 for tag [information-retrieval]...\n",
      "No items found for tag [information-retrieval], page 13\n",
      "🎉 Done! Saved 347 valid Q&A posts to stackoverflow_nlp_posts_with_answers_information=retrieval.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"information-retrieval\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_information=retrieval.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\"✅ Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\"🎉 Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28f173a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_information=retrieval.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_information=retrieval.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_information=retrieval.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ CSV saved successfully as: {OUTPUT_CSV}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f7607fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [openai-api]...\n",
      "✅ Total collected so far: 16\n",
      "Fetching page 2 for tag [openai-api]...\n",
      "✅ Total collected so far: 29\n",
      "Fetching page 3 for tag [openai-api]...\n",
      "✅ Total collected so far: 45\n",
      "Fetching page 4 for tag [openai-api]...\n",
      "✅ Total collected so far: 65\n",
      "Fetching page 5 for tag [openai-api]...\n",
      "✅ Total collected so far: 82\n",
      "Fetching page 6 for tag [openai-api]...\n",
      "✅ Total collected so far: 97\n",
      "Fetching page 7 for tag [openai-api]...\n",
      "✅ Total collected so far: 109\n",
      "Fetching page 8 for tag [openai-api]...\n",
      "✅ Total collected so far: 126\n",
      "Fetching page 9 for tag [openai-api]...\n",
      "✅ Total collected so far: 152\n",
      "Fetching page 10 for tag [openai-api]...\n",
      "✅ Total collected so far: 180\n",
      "Fetching page 11 for tag [openai-api]...\n",
      "✅ Total collected so far: 201\n",
      "Fetching page 12 for tag [openai-api]...\n",
      "✅ Total collected so far: 217\n",
      "Fetching page 13 for tag [openai-api]...\n",
      "✅ Total collected so far: 247\n",
      "Fetching page 14 for tag [openai-api]...\n",
      "✅ Total collected so far: 273\n",
      "Fetching page 15 for tag [openai-api]...\n",
      "✅ Total collected so far: 299\n",
      "Fetching page 16 for tag [openai-api]...\n",
      "✅ Total collected so far: 326\n",
      "Fetching page 17 for tag [openai-api]...\n",
      "✅ Total collected so far: 351\n",
      "Fetching page 18 for tag [openai-api]...\n",
      "✅ Total collected so far: 364\n",
      "Fetching page 19 for tag [openai-api]...\n",
      "✅ Total collected so far: 382\n",
      "Fetching page 20 for tag [openai-api]...\n",
      "✅ Total collected so far: 405\n",
      "Fetching page 21 for tag [openai-api]...\n",
      "✅ Total collected so far: 435\n",
      "Fetching page 22 for tag [openai-api]...\n",
      "✅ Total collected so far: 465\n",
      "Fetching page 23 for tag [openai-api]...\n",
      "✅ Total collected so far: 489\n",
      "Fetching page 24 for tag [openai-api]...\n",
      "✅ Total collected so far: 516\n",
      "Fetching page 25 for tag [openai-api]...\n",
      "✅ Total collected so far: 546\n",
      "Fetching page 26 for tag [openai-api]...\n",
      "✅ Total collected so far: 576\n",
      "Fetching page 27 for tag [openai-api]...\n",
      "✅ Total collected so far: 606\n",
      "Fetching page 28 for tag [openai-api]...\n",
      "✅ Total collected so far: 634\n",
      "Fetching page 29 for tag [openai-api]...\n",
      "No items found for tag [openai-api], page 29\n",
      "🎉 Done! Saved 634 valid Q&A posts to stackoverflow_nlp_posts_with_answers_openai-api.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"openai-api\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_openai-api.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\"✅ Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\"🎉 Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d113f993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_openai-api.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_openai-api.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_openai-api.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ CSV saved successfully as: {OUTPUT_CSV}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3a4b841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [topic-modeling]...\n",
      "✅ Total collected so far: 21\n",
      "Fetching page 2 for tag [topic-modeling]...\n",
      "✅ Total collected so far: 49\n",
      "Fetching page 3 for tag [topic-modeling]...\n",
      "✅ Total collected so far: 79\n",
      "Fetching page 4 for tag [topic-modeling]...\n",
      "✅ Total collected so far: 109\n",
      "Fetching page 5 for tag [topic-modeling]...\n",
      "✅ Total collected so far: 139\n",
      "Fetching page 6 for tag [topic-modeling]...\n",
      "✅ Total collected so far: 169\n",
      "Fetching page 7 for tag [topic-modeling]...\n",
      "✅ Total collected so far: 199\n",
      "Fetching page 8 for tag [topic-modeling]...\n",
      "✅ Total collected so far: 229\n",
      "Fetching page 9 for tag [topic-modeling]...\n",
      "✅ Total collected so far: 259\n",
      "Fetching page 10 for tag [topic-modeling]...\n",
      "✅ Total collected so far: 289\n",
      "Fetching page 11 for tag [topic-modeling]...\n",
      "No items found for tag [topic-modeling], page 11\n",
      "🎉 Done! Saved 289 valid Q&A posts to stackoverflow_nlp_posts_with_answers_topic-modeling.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"topic-modeling\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_topic-modeling.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\"✅ Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\"🎉 Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e0885b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_topic-modeling.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_topic-modeling.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_topic-modeling.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ CSV saved successfully as: {OUTPUT_CSV}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e29e2217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [question-answering]...\n",
      "✅ Total collected so far: 30\n",
      "Fetching page 2 for tag [question-answering]...\n",
      "✅ Total collected so far: 60\n",
      "Fetching page 3 for tag [question-answering]...\n",
      "✅ Total collected so far: 71\n",
      "Fetching page 4 for tag [question-answering]...\n",
      "No items found for tag [question-answering], page 4\n",
      "🎉 Done! Saved 71 valid Q&A posts to stackoverflow_nlp_posts_with_answers_question-answering.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"question-answering\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_question-answering.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\"✅ Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\"🎉 Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a0675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_question-answering.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_question-answering.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_question-answering.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\" CSV saved successfully as: {OUTPUT_CSV}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd798f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [tf-idf]...\n",
      "✅ Total collected so far: 30\n",
      "Fetching page 2 for tag [tf-idf]...\n",
      "✅ Total collected so far: 60\n",
      "Fetching page 3 for tag [tf-idf]...\n",
      "✅ Total collected so far: 90\n",
      "Fetching page 4 for tag [tf-idf]...\n",
      "✅ Total collected so far: 120\n",
      "Fetching page 5 for tag [tf-idf]...\n",
      "✅ Total collected so far: 150\n",
      "Fetching page 6 for tag [tf-idf]...\n",
      "✅ Total collected so far: 180\n",
      "Fetching page 7 for tag [tf-idf]...\n",
      "✅ Total collected so far: 210\n",
      "Fetching page 8 for tag [tf-idf]...\n",
      "✅ Total collected so far: 240\n",
      "Fetching page 9 for tag [tf-idf]...\n",
      "✅ Total collected so far: 270\n",
      "Fetching page 10 for tag [tf-idf]...\n",
      "✅ Total collected so far: 300\n",
      "Fetching page 11 for tag [tf-idf]...\n",
      "✅ Total collected so far: 330\n",
      "Fetching page 12 for tag [tf-idf]...\n",
      "✅ Total collected so far: 360\n",
      "Fetching page 13 for tag [tf-idf]...\n",
      "✅ Total collected so far: 390\n",
      "Fetching page 14 for tag [tf-idf]...\n",
      "✅ Total collected so far: 392\n",
      "Fetching page 15 for tag [tf-idf]...\n",
      "No items found for tag [tf-idf], page 15\n",
      "🎉 Done! Saved 392 valid Q&A posts to stackoverflow_nlp_posts_with_answers_tf-idf.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"tf-idf\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_tf-idf.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\" Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\" Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\" Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\" Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15f72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_tf-idf.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_tf-idf.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_tf-idf.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\" CSV saved successfully as: {OUTPUT_CSV}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e8c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [genism]...\n",
      "No items found for tag [genism], page 1\n",
      "🎉 Done! Saved 0 valid Q&A posts to stackoverflow_nlp_posts_with_answers_genism.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"genism\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_genism.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\" Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\" Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\" Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\" Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25261901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [text-mining]...\n",
      "✅ Total collected so far: 29\n",
      "Fetching page 2 for tag [text-mining]...\n",
      "✅ Total collected so far: 59\n",
      "Fetching page 3 for tag [text-mining]...\n",
      "✅ Total collected so far: 89\n",
      "Fetching page 4 for tag [text-mining]...\n",
      "✅ Total collected so far: 119\n",
      "Fetching page 5 for tag [text-mining]...\n",
      "✅ Total collected so far: 149\n",
      "Fetching page 6 for tag [text-mining]...\n",
      "✅ Total collected so far: 179\n",
      "Fetching page 7 for tag [text-mining]...\n",
      "✅ Total collected so far: 209\n",
      "Fetching page 8 for tag [text-mining]...\n",
      "✅ Total collected so far: 239\n",
      "Fetching page 9 for tag [text-mining]...\n",
      "✅ Total collected so far: 269\n",
      "Fetching page 10 for tag [text-mining]...\n",
      "✅ Total collected so far: 299\n",
      "Fetching page 11 for tag [text-mining]...\n",
      "✅ Total collected so far: 329\n",
      "Fetching page 12 for tag [text-mining]...\n",
      "✅ Total collected so far: 359\n",
      "Fetching page 13 for tag [text-mining]...\n",
      "✅ Total collected so far: 389\n",
      "Fetching page 14 for tag [text-mining]...\n",
      "✅ Total collected so far: 419\n",
      "Fetching page 15 for tag [text-mining]...\n",
      "✅ Total collected so far: 449\n",
      "Fetching page 16 for tag [text-mining]...\n",
      "✅ Total collected so far: 479\n",
      "Fetching page 17 for tag [text-mining]...\n",
      "✅ Total collected so far: 509\n",
      "Fetching page 18 for tag [text-mining]...\n",
      "✅ Total collected so far: 539\n",
      "Fetching page 19 for tag [text-mining]...\n",
      "✅ Total collected so far: 569\n",
      "Fetching page 20 for tag [text-mining]...\n",
      "✅ Total collected so far: 599\n",
      "Fetching page 21 for tag [text-mining]...\n",
      "✅ Total collected so far: 629\n",
      "Fetching page 22 for tag [text-mining]...\n",
      "✅ Total collected so far: 659\n",
      "Fetching page 23 for tag [text-mining]...\n",
      "✅ Total collected so far: 689\n",
      "Fetching page 24 for tag [text-mining]...\n",
      "✅ Total collected so far: 719\n",
      "Fetching page 25 for tag [text-mining]...\n",
      "✅ Total collected so far: 749\n",
      "Fetching page 26 for tag [text-mining]...\n",
      "✅ Total collected so far: 779\n",
      "Fetching page 27 for tag [text-mining]...\n",
      "No items found for tag [text-mining], page 27\n",
      "🎉 Done! Saved 779 valid Q&A posts to stackoverflow_nlp_posts_with_answers_text-mining.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"text-mining\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_text-mining.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\" Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\" Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\" Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\" Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_text-mining.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_text-mining.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_text-mining.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\" CSV saved successfully as: {OUTPUT_CSV}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e242470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [pos-tagging]...\n",
      "✅ Total collected so far: 30\n",
      "Fetching page 2 for tag [pos-tagging]...\n",
      "✅ Total collected so far: 60\n",
      "Fetching page 3 for tag [pos-tagging]...\n",
      "✅ Total collected so far: 90\n",
      "Fetching page 4 for tag [pos-tagging]...\n",
      "✅ Total collected so far: 120\n",
      "Fetching page 5 for tag [pos-tagging]...\n",
      "✅ Total collected so far: 150\n",
      "Fetching page 6 for tag [pos-tagging]...\n",
      "✅ Total collected so far: 180\n",
      "Fetching page 7 for tag [pos-tagging]...\n",
      "No items found for tag [pos-tagging], page 7\n",
      "🎉 Done! Saved 180 valid Q&A posts to stackoverflow_nlp_posts_with_answers_pos-tagging.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\"pos-tagging\"]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_pos-tagging.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\" Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\" Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\" Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\" Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0bc816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_pos-tagging.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_pos-tagging.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_pos-tagging.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\" CSV saved successfully as: {OUTPUT_CSV}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ddbde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [sequence-labeling]...\n",
      "No items found for tag [sequence-labeling], page 1\n",
      "Fetching page 1 for tag [dependency-parsing]...\n",
      "✅ Total collected so far: 30\n",
      "Fetching page 2 for tag [dependency-parsing]...\n",
      "No items found for tag [dependency-parsing], page 2\n",
      "Fetching page 1 for tag [word-embedding]...\n",
      "✅ Total collected so far: 50\n",
      "Fetching page 2 for tag [word-embedding]...\n",
      "✅ Total collected so far: 80\n",
      "Fetching page 3 for tag [word-embedding]...\n",
      "✅ Total collected so far: 110\n",
      "Fetching page 4 for tag [word-embedding]...\n",
      "✅ Total collected so far: 140\n",
      "Fetching page 5 for tag [word-embedding]...\n",
      "✅ Total collected so far: 170\n",
      "Fetching page 6 for tag [word-embedding]...\n",
      "✅ Total collected so far: 200\n",
      "Fetching page 7 for tag [word-embedding]...\n",
      "✅ Total collected so far: 230\n",
      "Fetching page 8 for tag [word-embedding]...\n",
      "✅ Total collected so far: 260\n",
      "Fetching page 9 for tag [word-embedding]...\n",
      "✅ Total collected so far: 290\n",
      "Fetching page 10 for tag [word-embedding]...\n",
      "✅ Total collected so far: 320\n",
      "Fetching page 11 for tag [word-embedding]...\n",
      "✅ Total collected so far: 350\n",
      "Fetching page 12 for tag [word-embedding]...\n",
      "✅ Total collected so far: 357\n",
      "Fetching page 13 for tag [word-embedding]...\n",
      "No items found for tag [word-embedding], page 13\n",
      "Fetching page 1 for tag [glove]...\n",
      "✅ Total collected so far: 381\n",
      "Fetching page 2 for tag [glove]...\n",
      "✅ Total collected so far: 402\n",
      "Fetching page 3 for tag [glove]...\n",
      "✅ Total collected so far: 432\n",
      "Fetching page 4 for tag [glove]...\n",
      "No items found for tag [glove], page 4\n",
      "Fetching page 1 for tag [core-nlp]...\n",
      "No items found for tag [core-nlp], page 1\n",
      "Fetching page 1 for tag [langchain]...\n",
      "✅ Total collected so far: 442\n",
      "Fetching page 2 for tag [langchain]...\n",
      "✅ Total collected so far: 455\n",
      "Fetching page 3 for tag [langchain]...\n",
      "✅ Total collected so far: 464\n",
      "Fetching page 4 for tag [langchain]...\n",
      "✅ Total collected so far: 472\n",
      "Fetching page 5 for tag [langchain]...\n",
      "✅ Total collected so far: 484\n",
      "Fetching page 6 for tag [langchain]...\n",
      "✅ Total collected so far: 496\n",
      "Fetching page 7 for tag [langchain]...\n",
      "✅ Total collected so far: 506\n",
      "Fetching page 8 for tag [langchain]...\n",
      "✅ Total collected so far: 520\n",
      "Fetching page 9 for tag [langchain]...\n",
      "✅ Total collected so far: 535\n",
      "Fetching page 10 for tag [langchain]...\n",
      "✅ Total collected so far: 556\n",
      "Fetching page 11 for tag [langchain]...\n",
      "✅ Total collected so far: 576\n",
      "Fetching page 12 for tag [langchain]...\n",
      "✅ Total collected so far: 595\n",
      "Fetching page 13 for tag [langchain]...\n",
      "✅ Total collected so far: 608\n",
      "Fetching page 14 for tag [langchain]...\n",
      "✅ Total collected so far: 630\n",
      "Fetching page 15 for tag [langchain]...\n",
      "✅ Total collected so far: 646\n",
      "Fetching page 16 for tag [langchain]...\n",
      "✅ Total collected so far: 667\n",
      "Fetching page 17 for tag [langchain]...\n",
      "✅ Total collected so far: 690\n",
      "Fetching page 18 for tag [langchain]...\n",
      "✅ Total collected so far: 711\n",
      "Fetching page 19 for tag [langchain]...\n",
      "✅ Total collected so far: 723\n",
      "Fetching page 20 for tag [langchain]...\n",
      "✅ Total collected so far: 743\n",
      "Fetching page 21 for tag [langchain]...\n",
      "✅ Total collected so far: 769\n",
      "Fetching page 22 for tag [langchain]...\n",
      "✅ Total collected so far: 795\n",
      "Fetching page 23 for tag [langchain]...\n",
      "No items found for tag [langchain], page 23\n",
      "Fetching page 1 for tag [few-shot-learning]...\n",
      "✅ Total collected so far: 802\n",
      "Fetching page 2 for tag [few-shot-learning]...\n",
      "No items found for tag [few-shot-learning], page 2\n",
      "Fetching page 1 for tag [zero-shot-learning]...\n",
      "No items found for tag [zero-shot-learning], page 1\n",
      "Fetching page 1 for tag [semantic-similarity]...\n",
      "No items found for tag [semantic-similarity], page 1\n",
      "🎉 Done! Saved 802 valid Q&A posts to stackoverflow_nlp_posts_with_answers_final-stretch.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\n",
    "    \"sequence-labeling\",\n",
    "    \"dependency-parsing\",\n",
    "    \"word-embedding\",\n",
    "    \"glove\",\n",
    "    \"core-nlp\",\n",
    "    \"langchain\",\n",
    "    \"few-shot-learning\",\n",
    "    \"zero-shot-learning\",\n",
    "    \"semantic-similarity\"\n",
    "]\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_final-stretch.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\" Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\" Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\" Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\" Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc8f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_final-stretch.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_final-stretch.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_final-stretch.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\" CSV saved successfully as: {OUTPUT_CSV}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a3c3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined CSV saved as 'combined_20_csvs.csv' with 14188 unique rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === List of all 20 CSV files ===\n",
    "csv_files = [\n",
    " \"stackoverflow_nlp_posts_with_answers.csv\",\"stackoverflow_nlp_posts_with_answers_bert.csv\",\"stackoverflow_nlp_posts_with_answers_final-stretch.csv\",\"stackoverflow_nlp_posts_with_answers_huggingface-transformers.csv\",\"stackoverflow_nlp_posts_with_answers_information=retrieval.csv\",\"stackoverflow_nlp_posts_with_answers_language-detection.csv\",\"stackoverflow_nlp_posts_with_answers_lemmatization.csv\",\"stackoverflow_nlp_posts_with_answers_named-entity-recognition.csv\",\"stackoverflow_nlp_posts_with_answers_nltk.csv\",\"stackoverflow_nlp_posts_with_answers_openai-api.csv\",\"stackoverflow_nlp_posts_with_answers_pos-tagging.csv\",\"stackoverflow_nlp_posts_with_answers_question-answering.csv\",\"stackoverflow_nlp_posts_with_answers_second_set.csv\",\"stackoverflow_nlp_posts_with_answers_sentence-transformers.csv\",\"stackoverflow_nlp_posts_with_answers_spacy.csv\",\"stackoverflow_nlp_posts_with_answers_stemming.csv\",\"stackoverflow_nlp_posts_with_answers_text-classification.csv\",\"stackoverflow_nlp_posts_with_answers_text-mining.csv\",\"stackoverflow_nlp_posts_with_answers_tf-idf.csv\",\"stackoverflow_nlp_posts_with_answers_topic-modeling.csv\"\n",
    "]\n",
    "\n",
    "# === Load and concatenate all CSVs ===\n",
    "all_dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "# === Drop duplicate rows by question_id if it exists ===\n",
    "if \"question_id\" in combined_df.columns:\n",
    "    combined_df = combined_df.drop_duplicates(subset=\"question_id\")\n",
    "\n",
    "# === Save combined CSV ===\n",
    "combined_df.to_csv(\"combined_20_csvs.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\" Combined CSV saved as 'combined_20_csvs.csv' with {len(combined_df)} unique rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "070c59b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>tags</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>accepted_answer_body</th>\n",
       "      <th>link</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79549787</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>&lt;p&gt;I'm using spaCy with the pl_core_news_lg mo...</td>\n",
       "      <td>['python', 'nlp', 'spacy', 'presidio']</td>\n",
       "      <td>79552218</td>\n",
       "      <td>&lt;p&gt;The configuration file is missing the 'labe...</td>\n",
       "      <td>https://stackoverflow.com/questions/79549787</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79548202</td>\n",
       "      <td>GPT-2 and other models from huggingface -100 l...</td>\n",
       "      <td>&lt;p&gt;I understand the -100 label id is used so t...</td>\n",
       "      <td>['nlp', 'huggingface-transformers', 'pre-train...</td>\n",
       "      <td>79551169</td>\n",
       "      <td>&lt;p&gt;The author of the tutorial you mentioned se...</td>\n",
       "      <td>https://stackoverflow.com/questions/79548202</td>\n",
       "      <td>nlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79523269</td>\n",
       "      <td>Trouble getting importing gensim to work in colab</td>\n",
       "      <td>&lt;p&gt;I am trying to import gensim into colab.&lt;/p...</td>\n",
       "      <td>['numpy', 'nlp', 'dependencies', 'google-colab...</td>\n",
       "      <td>79523777</td>\n",
       "      <td>&lt;p&gt;You have to restart the session for the und...</td>\n",
       "      <td>https://stackoverflow.com/questions/79523269</td>\n",
       "      <td>numpy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79501178</td>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>&lt;p&gt;I am running the code found on this &lt;a href...</td>\n",
       "      <td>['python', 'nlp', 'large-language-model']</td>\n",
       "      <td>79501337</td>\n",
       "      <td>&lt;p&gt;I can't test it but ...&lt;/p&gt;\\n&lt;p&gt;I checked &lt;...</td>\n",
       "      <td>https://stackoverflow.com/questions/79501178</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79482283</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>&lt;p&gt;I am using presidio/langchain_experimental ...</td>\n",
       "      <td>['python', 'nlp', 'spacy', 'langchain', 'presi...</td>\n",
       "      <td>79495969</td>\n",
       "      <td>&lt;p&gt;After some test I was able to find the solu...</td>\n",
       "      <td>https://stackoverflow.com/questions/79482283</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                              title  \\\n",
       "0     79549787  Why does Presidio with spacy nlp engine not re...   \n",
       "1     79548202  GPT-2 and other models from huggingface -100 l...   \n",
       "2     79523269  Trouble getting importing gensim to work in colab   \n",
       "3     79501178        Store images instead of showing in a server   \n",
       "4     79482283  Presidio with Langchain Experimental does not ...   \n",
       "\n",
       "                                                body  \\\n",
       "0  <p>I'm using spaCy with the pl_core_news_lg mo...   \n",
       "1  <p>I understand the -100 label id is used so t...   \n",
       "2  <p>I am trying to import gensim into colab.</p...   \n",
       "3  <p>I am running the code found on this <a href...   \n",
       "4  <p>I am using presidio/langchain_experimental ...   \n",
       "\n",
       "                                                tags  accepted_answer_id  \\\n",
       "0             ['python', 'nlp', 'spacy', 'presidio']            79552218   \n",
       "1  ['nlp', 'huggingface-transformers', 'pre-train...            79551169   \n",
       "2  ['numpy', 'nlp', 'dependencies', 'google-colab...            79523777   \n",
       "3          ['python', 'nlp', 'large-language-model']            79501337   \n",
       "4  ['python', 'nlp', 'spacy', 'langchain', 'presi...            79495969   \n",
       "\n",
       "                                accepted_answer_body  \\\n",
       "0  <p>The configuration file is missing the 'labe...   \n",
       "1  <p>The author of the tutorial you mentioned se...   \n",
       "2  <p>You have to restart the session for the und...   \n",
       "3  <p>I can't test it but ...</p>\\n<p>I checked <...   \n",
       "4  <p>After some test I was able to find the solu...   \n",
       "\n",
       "                                           link     tag  \n",
       "0  https://stackoverflow.com/questions/79549787  python  \n",
       "1  https://stackoverflow.com/questions/79548202     nlp  \n",
       "2  https://stackoverflow.com/questions/79523269   numpy  \n",
       "3  https://stackoverflow.com/questions/79501178  python  \n",
       "4  https://stackoverflow.com/questions/79482283  python  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c = pd.read_csv(\"combined_20_csvs.csv\")\n",
    "df_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "40501cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14188, 8)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360439a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [nlp-transformer]...\n",
      "No items found for tag [nlp-transformer], page 1\n",
      "Fetching page 1 for tag [sequence-to-sequence]...\n",
      "✅ Total collected so far: 30\n",
      "Fetching page 2 for tag [sequence-to-sequence]...\n",
      "No items found for tag [sequence-to-sequence], page 2\n",
      "Fetching page 1 for tag [text-generation]...\n",
      "✅ Total collected so far: 32\n",
      "Fetching page 2 for tag [text-generation]...\n",
      "No items found for tag [text-generation], page 2\n",
      "Fetching page 1 for tag [attention-mechanism]...\n",
      "No items found for tag [attention-mechanism], page 1\n",
      "Fetching page 1 for tag [sentence-similarity]...\n",
      "✅ Total collected so far: 59\n",
      "Fetching page 2 for tag [sentence-similarity]...\n",
      "✅ Total collected so far: 89\n",
      "Fetching page 3 for tag [sentence-similarity]...\n",
      "✅ Total collected so far: 98\n",
      "Fetching page 4 for tag [sentence-similarity]...\n",
      "No items found for tag [sentence-similarity], page 4\n",
      "Fetching page 1 for tag [text-embedding]...\n",
      "No items found for tag [text-embedding], page 1\n",
      "Fetching page 1 for tag [intent-classification]...\n",
      "No items found for tag [intent-classification], page 1\n",
      "Fetching page 1 for tag [chatbot]...\n",
      "✅ Total collected so far: 109\n",
      "Fetching page 2 for tag [chatbot]...\n",
      "✅ Total collected so far: 123\n",
      "Fetching page 3 for tag [chatbot]...\n",
      "✅ Total collected so far: 134\n",
      "Fetching page 4 for tag [chatbot]...\n",
      "✅ Total collected so far: 150\n",
      "Fetching page 5 for tag [chatbot]...\n",
      "✅ Total collected so far: 162\n",
      "Fetching page 6 for tag [chatbot]...\n",
      "✅ Total collected so far: 184\n",
      "Fetching page 7 for tag [chatbot]...\n",
      "✅ Total collected so far: 206\n",
      "Fetching page 8 for tag [chatbot]...\n",
      "✅ Total collected so far: 231\n",
      "Fetching page 9 for tag [chatbot]...\n",
      "✅ Total collected so far: 255\n",
      "Fetching page 10 for tag [chatbot]...\n",
      "✅ Total collected so far: 285\n",
      "Fetching page 11 for tag [chatbot]...\n",
      "✅ Total collected so far: 304\n",
      "Fetching page 12 for tag [chatbot]...\n",
      "✅ Total collected so far: 331\n",
      "Fetching page 13 for tag [chatbot]...\n",
      "✅ Total collected so far: 361\n",
      "Fetching page 14 for tag [chatbot]...\n",
      "✅ Total collected so far: 389\n",
      "Fetching page 15 for tag [chatbot]...\n",
      "✅ Total collected so far: 418\n",
      "Fetching page 16 for tag [chatbot]...\n",
      "✅ Total collected so far: 448\n",
      "Fetching page 17 for tag [chatbot]...\n",
      "✅ Total collected so far: 478\n",
      "Fetching page 18 for tag [chatbot]...\n",
      "✅ Total collected so far: 508\n",
      "Fetching page 19 for tag [chatbot]...\n",
      "✅ Total collected so far: 538\n",
      "Fetching page 20 for tag [chatbot]...\n",
      "✅ Total collected so far: 568\n",
      "Fetching page 21 for tag [chatbot]...\n",
      "✅ Total collected so far: 597\n",
      "Fetching page 22 for tag [chatbot]...\n",
      "✅ Total collected so far: 627\n",
      "Fetching page 23 for tag [chatbot]...\n",
      "✅ Total collected so far: 657\n",
      "Fetching page 24 for tag [chatbot]...\n",
      "✅ Total collected so far: 687\n",
      "Fetching page 25 for tag [chatbot]...\n",
      "✅ Total collected so far: 717\n",
      "Fetching page 26 for tag [chatbot]...\n",
      "✅ Total collected so far: 747\n",
      "Fetching page 27 for tag [chatbot]...\n",
      "✅ Total collected so far: 777\n",
      "Fetching page 28 for tag [chatbot]...\n",
      "✅ Total collected so far: 807\n",
      "Fetching page 29 for tag [chatbot]...\n",
      "✅ Total collected so far: 837\n",
      "Fetching page 30 for tag [chatbot]...\n",
      "✅ Total collected so far: 867\n",
      "Fetching page 31 for tag [chatbot]...\n",
      "✅ Total collected so far: 897\n",
      "Fetching page 32 for tag [chatbot]...\n",
      "✅ Total collected so far: 926\n",
      "Fetching page 33 for tag [chatbot]...\n",
      "✅ Total collected so far: 956\n",
      "Fetching page 34 for tag [chatbot]...\n",
      "✅ Total collected so far: 986\n",
      "Fetching page 35 for tag [chatbot]...\n",
      "✅ Total collected so far: 1016\n",
      "Fetching page 36 for tag [chatbot]...\n",
      "✅ Total collected so far: 1046\n",
      "Fetching page 37 for tag [chatbot]...\n",
      "✅ Total collected so far: 1076\n",
      "Fetching page 38 for tag [chatbot]...\n",
      "✅ Total collected so far: 1106\n",
      "Fetching page 39 for tag [chatbot]...\n",
      "✅ Total collected so far: 1136\n",
      "Fetching page 40 for tag [chatbot]...\n",
      "✅ Total collected so far: 1166\n",
      "Fetching page 41 for tag [chatbot]...\n",
      "✅ Total collected so far: 1196\n",
      "Fetching page 42 for tag [chatbot]...\n",
      "✅ Total collected so far: 1226\n",
      "Fetching page 43 for tag [chatbot]...\n",
      "✅ Total collected so far: 1256\n",
      "Fetching page 44 for tag [chatbot]...\n",
      "✅ Total collected so far: 1278\n",
      "Fetching page 45 for tag [chatbot]...\n",
      "No items found for tag [chatbot], page 45\n",
      "🎉 Done! Saved 1278 valid Q&A posts to stackoverflow_nlp_posts_with_answers_final-stretch_p2.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\n",
    "    \"nlp-transformer\",\n",
    "    \"sequence-to-sequence\",\n",
    "    \"text-generation\",\n",
    "    \"attention-mechanism\",\n",
    "    \"sentence-similarity\",\n",
    "    \"text-embedding\",\n",
    "    \"intent-classification\",\n",
    "    \"chatbot\"\n",
    "]\n",
    "\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_final-stretch_p2.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\" Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(f\" Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\" Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\" Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281e61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_final-stretch_p2.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_final-stretch_p2.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_final-stretch_p2.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\" CSV saved successfully as: {OUTPUT_CSV}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e202b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined CSV saved as 'combined_V1_csv.csv' with 15344 unique rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "csv_file_1 = \"combined_20_csvs.csv\"\n",
    "csv_file_2 = \"stackoverflow_nlp_posts_with_answers_final-stretch_p2.csv\"\n",
    "output_csv = \"combined_V1_csv.csv\"\n",
    "\n",
    "# === Load the CSVs ===\n",
    "df1 = pd.read_csv(csv_file_1)\n",
    "df2 = pd.read_csv(csv_file_2)\n",
    "\n",
    "# === Combine them ===\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# === Remove duplicates based on 'question_id' column ===\n",
    "if \"question_id\" in combined_df.columns:\n",
    "    combined_df = combined_df.drop_duplicates(subset=\"question_id\")\n",
    "\n",
    "# === Save the combined file ===\n",
    "combined_df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\" Combined CSV saved as '{output_csv}' with {len(combined_df)} unique rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561f53c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [text-matching]...\n",
      "No items found for tag [text-matching], page 1\n",
      "Fetching page 1 for tag [semantic-search]...\n",
      "✅ Total collected so far: 9\n",
      "Fetching page 2 for tag [semantic-search]...\n",
      "No items found for tag [semantic-search], page 2\n",
      "Fetching page 1 for tag [intent-recognition]...\n",
      "No items found for tag [intent-recognition], page 1\n",
      "Fetching page 1 for tag [slot-filling]...\n",
      "No items found for tag [slot-filling], page 1\n",
      "Fetching page 1 for tag [relation-extraction]...\n",
      "✅ Total collected so far: 9\n",
      "Fetching page 2 for tag [relation-extraction]...\n",
      "No items found for tag [relation-extraction], page 2\n",
      "Fetching page 1 for tag [multi-label-classification]...\n",
      "No items found for tag [multi-label-classification], page 1\n",
      "Fetching page 1 for tag [zero-shot-classification]...\n",
      "No items found for tag [zero-shot-classification], page 1\n",
      "Fetching page 1 for tag [co-reference-resolution]...\n",
      "No items found for tag [co-reference-resolution], page 1\n",
      "Fetching page 1 for tag [text-vectorization]...\n",
      "No items found for tag [text-vectorization], page 1\n",
      "Fetching page 1 for tag [rasa-nlu]...\n",
      "✅ Total collected so far: 23\n",
      "Fetching page 2 for tag [rasa-nlu]...\n",
      "✅ Total collected so far: 46\n",
      "Fetching page 3 for tag [rasa-nlu]...\n",
      "✅ Total collected so far: 74\n",
      "Fetching page 4 for tag [rasa-nlu]...\n",
      "✅ Total collected so far: 104\n",
      "Fetching page 5 for tag [rasa-nlu]...\n",
      "✅ Total collected so far: 134\n",
      "Fetching page 6 for tag [rasa-nlu]...\n",
      "✅ Total collected so far: 164\n",
      "Fetching page 7 for tag [rasa-nlu]...\n",
      "✅ Total collected so far: 187\n",
      "Fetching page 8 for tag [rasa-nlu]...\n",
      "No items found for tag [rasa-nlu], page 8\n",
      "🎉 Done! Saved 187 valid Q&A posts to stackoverflow_nlp_posts_with_answers_final-stretch_p3.json\n",
      "Fetching page 1 for tag [text-matching]...\n",
      "No items found for tag [text-matching], page 1\n",
      "Fetching page 1 for tag [semantic-search]...\n",
      "✅ Total collected so far: 9\n",
      "Fetching page 2 for tag [semantic-search]...\n",
      "No items found for tag [semantic-search], page 2\n",
      "Fetching page 1 for tag [intent-recognition]...\n",
      "No items found for tag [intent-recognition], page 1\n",
      "Fetching page 1 for tag [slot-filling]...\n",
      "No items found for tag [slot-filling], page 1\n",
      "Fetching page 1 for tag [relation-extraction]...\n",
      "✅ Total collected so far: 9\n",
      "Fetching page 2 for tag [relation-extraction]...\n",
      "No items found for tag [relation-extraction], page 2\n",
      "Fetching page 1 for tag [multi-label-classification]...\n",
      "No items found for tag [multi-label-classification], page 1\n",
      "Fetching page 1 for tag [zero-shot-classification]...\n",
      "No items found for tag [zero-shot-classification], page 1\n",
      "Fetching page 1 for tag [co-reference-resolution]...\n",
      "No items found for tag [co-reference-resolution], page 1\n",
      "Fetching page 1 for tag [text-vectorization]...\n",
      "No items found for tag [text-vectorization], page 1\n",
      "Fetching page 1 for tag [rasa-nlu]...\n",
      "✅ Total collected so far: 23\n",
      "Fetching page 2 for tag [rasa-nlu]...\n",
      "✅ Total collected so far: 46\n",
      "Fetching page 3 for tag [rasa-nlu]...\n",
      "✅ Total collected so far: 74\n",
      "Fetching page 4 for tag [rasa-nlu]...\n",
      "✅ Total collected so far: 104\n",
      "Fetching page 5 for tag [rasa-nlu]...\n",
      "✅ Total collected so far: 134\n",
      "Fetching page 6 for tag [rasa-nlu]...\n",
      "✅ Total collected so far: 164\n",
      "Fetching page 7 for tag [rasa-nlu]...\n",
      "✅ Total collected so far: 187\n",
      "Fetching page 8 for tag [rasa-nlu]...\n",
      "No items found for tag [rasa-nlu], page 8\n",
      "🎉 Done! Saved 187 valid Q&A posts to stackoverflow_nlp_posts_with_answers_final-stretch_p3.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\n",
    "    \"text-matching\",\n",
    "    \"semantic-search\",\n",
    "    \"intent-recognition\",\n",
    "    \"slot-filling\",\n",
    "    \"relation-extraction\",\n",
    "    \"multi-label-classification\",\n",
    "    \"zero-shot-classification\",\n",
    "    \"co-reference-resolution\",\n",
    "    \"text-vectorization\",\n",
    "    \"rasa-nlu\"\n",
    "]\n",
    "\n",
    "\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_final-stretch_p3.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\" Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\" Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\" Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\" Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_final-stretch_p3.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\" Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\" Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\" Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\" Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71170647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "csv_file_1 = \"combined_20_csvs.csv\"\n",
    "csv_file_2 = \"stackoverflow_nlp_posts_with_answers_final-stretch_p2.csv\"\n",
    "output_csv = \"combined_V1_csv.csv\"\n",
    "\n",
    "# === Load the CSVs ===\n",
    "df1 = pd.read_csv(csv_file_1)\n",
    "df2 = pd.read_csv(csv_file_2)\n",
    "\n",
    "# === Combine them ===\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# === Remove duplicates based on 'question_id' column ===\n",
    "if \"question_id\" in combined_df.columns:\n",
    "    combined_df = combined_df.drop_duplicates(subset=\"question_id\")\n",
    "\n",
    "# === Save the combined file ===\n",
    "combined_df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\" Combined CSV saved as '{output_csv}' with {len(combined_df)} unique rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760599e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved successfully as: stackoverflow_nlp_posts_with_answers_final-stretch_p3.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "INPUT_JSON = \"stackoverflow_nlp_posts_with_answers_final-stretch_p3.json\"\n",
    "OUTPUT_CSV = \"stackoverflow_nlp_posts_with_answers_final-stretch_p3.csv\"\n",
    "\n",
    "# === Load JSON Data ===\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === Add Link to Question ===\n",
    "df[\"link\"] = \"https://stackoverflow.com/questions/\" + df[\"question_id\"].astype(str)\n",
    "\n",
    "# === Extract First Tag ===\n",
    "df[\"tag\"] = df[\"tags\"].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === Reorder and Keep Specific Columns ===\n",
    "ordered_columns = [\n",
    "    \"question_id\",\n",
    "    \"title\",\n",
    "    \"body\",\n",
    "    \"tags\",\n",
    "    \"accepted_answer_id\",\n",
    "    \"accepted_answer_body\",\n",
    "    \"link\",\n",
    "    \"tag\"\n",
    "]\n",
    "\n",
    "df = df[ordered_columns]\n",
    "\n",
    "# === Save to CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\" CSV saved successfully as: {OUTPUT_CSV}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb8e68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined CSV saved as 'combined_V2_csv.csv' with 14343 unique rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "csv_file_1 = \"combined_20_csvs.csv\"\n",
    "csv_file_2 = \"stackoverflow_nlp_posts_with_answers_final-stretch_p3.csv\"\n",
    "output_csv = \"combined_V2_csv.csv\"\n",
    "\n",
    "# === Load the CSVs ===\n",
    "df1 = pd.read_csv(csv_file_1)\n",
    "df2 = pd.read_csv(csv_file_2)\n",
    "\n",
    "# === Combine them ===\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# === Remove duplicates based on 'question_id' column ===\n",
    "if \"question_id\" in combined_df.columns:\n",
    "    combined_df = combined_df.drop_duplicates(subset=\"question_id\")\n",
    "\n",
    "# === Save the combined file ===\n",
    "combined_df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\" Combined CSV saved as '{output_csv}' with {len(combined_df)} unique rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7888b058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14343, 8)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_V2_csv = pd.read_csv(\"combined_V2_csv.csv\")\n",
    "df_V2_csv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041fb05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 for tag [text-cleaning]...\n",
      "No items found for tag [text-cleaning], page 1\n",
      "Fetching page 1 for tag [text-normalization]...\n",
      "✅ Total collected so far: 12\n",
      "Fetching page 2 for tag [text-normalization]...\n",
      "No items found for tag [text-normalization], page 2\n",
      "Fetching page 1 for tag [language-translation]...\n",
      "✅ Total collected so far: 38\n",
      "Fetching page 2 for tag [language-translation]...\n",
      "✅ Total collected so far: 68\n",
      "Fetching page 3 for tag [language-translation]...\n",
      "✅ Total collected so far: 98\n",
      "Fetching page 4 for tag [language-translation]...\n",
      "✅ Total collected so far: 128\n",
      "Fetching page 5 for tag [language-translation]...\n",
      "✅ Total collected so far: 129\n",
      "Fetching page 6 for tag [language-translation]...\n",
      "No items found for tag [language-translation], page 6\n",
      "Fetching page 1 for tag [term-extraction]...\n",
      "No items found for tag [term-extraction], page 1\n",
      "Fetching page 1 for tag [ocr-to-text]...\n",
      "No items found for tag [ocr-to-text], page 1\n",
      "Fetching page 1 for tag [unsupervised-nlp]...\n",
      "No items found for tag [unsupervised-nlp], page 1\n",
      "Fetching page 1 for tag [sentence-segmentation]...\n",
      "No items found for tag [sentence-segmentation], page 1\n",
      "Fetching page 1 for tag [language-identification]...\n",
      "No items found for tag [language-identification], page 1\n",
      "Fetching page 1 for tag [document-similarity]...\n",
      "No items found for tag [document-similarity], page 1\n",
      "🎉 Done! Saved 129 valid Q&A posts to stackoverflow_nlp_posts_with_answers_final-stretch_p4.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# === SETTINGS ===\n",
    "TAGS = [\n",
    "    \"text-cleaning\",\n",
    "    \"text-normalization\",\n",
    "    \"language-translation\",\n",
    "    \"term-extraction\",\n",
    "    \"ocr-to-text\",\n",
    "    \"unsupervised-nlp\",\n",
    "    \"sentence-segmentation\",\n",
    "    \"language-identification\",\n",
    "    \"document-similarity\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "SITE = \"stackoverflow\"\n",
    "PAGE_SIZE = 100\n",
    "MAX_PAGES = 200\n",
    "OUTPUT_FILE = \"stackoverflow_nlp_posts_with_answers_final-stretch_p4.json\"\n",
    "API_KEY = \"rl_MdVXEpV7UgL1XMwe5e5BwoqEa\"  # Replace with your key\n",
    "TARGET_COUNT = 20000\n",
    "\n",
    "all_questions = []\n",
    "valid_question_count = 0\n",
    "\n",
    "# === START LOOPING THROUGH TAGS AND PAGES ===\n",
    "for tag in TAGS:\n",
    "    if valid_question_count >= TARGET_COUNT:\n",
    "        break\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        if valid_question_count >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for tag [{tag}]...\")\n",
    "\n",
    "        # Get questions\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": PAGE_SIZE,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": tag,\n",
    "            \"site\": SITE,\n",
    "            \"filter\": \"withbody\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\" Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\" Invalid JSON response. Skipping this page.\")\n",
    "            continue\n",
    "\n",
    "        if \"items\" not in data or not data[\"items\"]:\n",
    "            print(f\"No items found for tag [{tag}], page {page}\")\n",
    "            break\n",
    "\n",
    "        # Filter questions with accepted_answer_id\n",
    "        questions_with_answers = [\n",
    "            item for item in data[\"items\"]\n",
    "            if item.get(\"accepted_answer_id\")\n",
    "        ]\n",
    "\n",
    "        # Batch accepted_answer_ids (max 100 per API call)\n",
    "        accepted_ids = [str(item[\"accepted_answer_id\"]) for item in questions_with_answers]\n",
    "        id_batches = [accepted_ids[i:i+100] for i in range(0, len(accepted_ids), 100)]\n",
    "\n",
    "        accepted_answers = {}\n",
    "        for batch in id_batches:\n",
    "            ids_str = \";\".join(batch)\n",
    "            ans_url = f\"https://api.stackexchange.com/2.3/answers/{ids_str}\"\n",
    "            ans_params = {\n",
    "                \"site\": SITE,\n",
    "                \"filter\": \"withbody\",\n",
    "                \"key\": API_KEY\n",
    "            }\n",
    "            ans_response = requests.get(ans_url, params=ans_params)\n",
    "            if ans_response.status_code == 200:\n",
    "                try:\n",
    "                    ans_data = ans_response.json()\n",
    "                    for ans in ans_data.get(\"items\", []):\n",
    "                        accepted_answers[ans[\"answer_id\"]] = ans.get(\"body\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # avoid hitting batch rate limits\n",
    "\n",
    "        # Combine only valid Q&A pairs\n",
    "        for item in questions_with_answers:\n",
    "            if valid_question_count >= TARGET_COUNT:\n",
    "                break\n",
    "\n",
    "            aid = item[\"accepted_answer_id\"]\n",
    "            abody = accepted_answers.get(aid)\n",
    "\n",
    "            if abody:  # Only store if body exists\n",
    "                question_data = {\n",
    "                    \"question_id\": item.get(\"question_id\"),\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"body\": item.get(\"body\"),\n",
    "                    \"tags\": item.get(\"tags\"),\n",
    "                    \"accepted_answer_id\": aid,\n",
    "                    \"accepted_answer_body\": abody,\n",
    "                    \"score\": item.get(\"score\")\n",
    "                }\n",
    "                all_questions.append(question_data)\n",
    "                valid_question_count += 1\n",
    "\n",
    "        print(f\" Total collected so far: {valid_question_count}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_questions, f, indent=4)\n",
    "\n",
    "print(f\" Done! Saved {valid_question_count} valid Q&A posts to {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6097d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
