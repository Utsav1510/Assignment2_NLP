{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b22258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\loyol\\onedrive\\documents\\au\\anaconda\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\loyol\\onedrive\\documents\\au\\anaconda\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\loyol\\onedrive\\documents\\au\\anaconda\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\loyol\\onedrive\\documents\\au\\anaconda\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\loyol\\onedrive\\documents\\au\\anaconda\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675be80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Fetching page 1...\n",
      "ğŸ“¥ Fetching page 2...\n",
      "ğŸ“¥ Fetching page 3...\n",
      "ğŸ“¥ Fetching page 4...\n",
      "ğŸ“¥ Fetching page 5...\n",
      "ğŸ“¥ Fetching page 6...\n",
      "ğŸ“¥ Fetching page 7...\n",
      "ğŸ“¥ Fetching page 8...\n",
      "ğŸ“¥ Fetching page 9...\n",
      "ğŸ“¥ Fetching page 10...\n",
      "ğŸ“¥ Fetching page 11...\n",
      "ğŸ“¥ Fetching page 12...\n",
      "ğŸ“¥ Fetching page 13...\n",
      "ğŸ“¥ Fetching page 14...\n",
      "ğŸ“¥ Fetching page 15...\n",
      "ğŸ“¥ Fetching page 16...\n",
      "ğŸ“¥ Fetching page 17...\n",
      "ğŸ“¥ Fetching page 18...\n",
      "ğŸ“¥ Fetching page 19...\n",
      "ğŸ“¥ Fetching page 20...\n",
      "ğŸ“¥ Fetching page 21...\n",
      "ğŸ“¥ Fetching page 22...\n",
      "ğŸ“¥ Fetching page 23...\n",
      "ğŸ“¥ Fetching page 24...\n",
      "ğŸ“¥ Fetching page 25...\n",
      "ğŸ“¥ Fetching page 26...\n",
      "ğŸ“¥ Fetching page 27...\n",
      "ğŸ“¥ Fetching page 28...\n",
      "ğŸ“¥ Fetching page 29...\n",
      "ğŸ“¥ Fetching page 30...\n",
      "ğŸ“¥ Fetching page 31...\n",
      "ğŸ“¥ Fetching page 32...\n",
      "ğŸ“¥ Fetching page 33...\n",
      "ğŸ“¥ Fetching page 34...\n",
      "ğŸ“¥ Fetching page 35...\n",
      "ğŸ“¥ Fetching page 36...\n",
      "ğŸ“¥ Fetching page 37...\n",
      "ğŸ“¥ Fetching page 38...\n",
      "ğŸ“¥ Fetching page 39...\n",
      "ğŸ“¥ Fetching page 40...\n",
      "ğŸ“¥ Fetching page 41...\n",
      "ğŸ“¥ Fetching page 42...\n",
      "ğŸ“¥ Fetching page 43...\n",
      "ğŸ“¥ Fetching page 44...\n",
      "ğŸ“¥ Fetching page 45...\n",
      "ğŸ“¥ Fetching page 46...\n",
      "ğŸ“¥ Fetching page 47...\n",
      "ğŸ“¥ Fetching page 48...\n",
      "ğŸ“¥ Fetching page 49...\n",
      "ğŸ“¥ Fetching page 50...\n",
      "ğŸ“¥ Fetching page 51...\n",
      "ğŸ“¥ Fetching page 52...\n",
      "ğŸ“¥ Fetching page 53...\n",
      "ğŸ“¥ Fetching page 54...\n",
      "ğŸ“¥ Fetching page 55...\n",
      "ğŸ“¥ Fetching page 56...\n",
      "ğŸ“¥ Fetching page 57...\n",
      "ğŸ“¥ Fetching page 58...\n",
      "ğŸ“¥ Fetching page 59...\n",
      "ğŸ“¥ Fetching page 60...\n",
      "ğŸ“¥ Fetching page 61...\n",
      "ğŸ“¥ Fetching page 62...\n",
      "ğŸ“¥ Fetching page 63...\n",
      "ğŸ“¥ Fetching page 64...\n",
      "ğŸ“¥ Fetching page 65...\n",
      "ğŸ“¥ Fetching page 66...\n",
      "ğŸ“¥ Fetching page 67...\n",
      "ğŸ“¥ Fetching page 68...\n",
      "ğŸ“¥ Fetching page 69...\n",
      "ğŸ“¥ Fetching page 70...\n",
      "ğŸ“¥ Fetching page 71...\n",
      "ğŸ“¥ Fetching page 72...\n",
      "ğŸ“¥ Fetching page 73...\n",
      "ğŸ“¥ Fetching page 74...\n",
      "ğŸ“¥ Fetching page 75...\n",
      "ğŸ“¥ Fetching page 76...\n",
      "ğŸ“¥ Fetching page 77...\n",
      "ğŸ“¥ Fetching page 78...\n",
      "ğŸ“¥ Fetching page 79...\n",
      "ğŸ“¥ Fetching page 80...\n",
      "ğŸ“¥ Fetching page 81...\n",
      "ğŸ“¥ Fetching page 82...\n",
      "ğŸ“¥ Fetching page 83...\n",
      "ğŸ“¥ Fetching page 84...\n",
      "ğŸ“¥ Fetching page 85...\n",
      "ğŸ“¥ Fetching page 86...\n",
      "ğŸ“¥ Fetching page 87...\n",
      "ğŸ“¥ Fetching page 88...\n",
      "ğŸ“¥ Fetching page 89...\n",
      "ğŸ“¥ Fetching page 90...\n",
      "ğŸ“¥ Fetching page 91...\n",
      "ğŸ“¥ Fetching page 92...\n",
      "ğŸ“¥ Fetching page 93...\n",
      "ğŸ“¥ Fetching page 94...\n",
      "ğŸ“¥ Fetching page 95...\n",
      "ğŸ“¥ Fetching page 96...\n",
      "ğŸ“¥ Fetching page 97...\n",
      "ğŸ“¥ Fetching page 98...\n",
      "ğŸ“¥ Fetching page 99...\n",
      "ğŸ“¥ Fetching page 100...\n",
      "ğŸ“¥ Fetching page 101...\n",
      "ğŸ“¥ Fetching page 102...\n",
      "ğŸ“¥ Fetching page 103...\n",
      "ğŸ“¥ Fetching page 104...\n",
      "ğŸ“¥ Fetching page 105...\n",
      "ğŸ“¥ Fetching page 106...\n",
      "ğŸ“¥ Fetching page 107...\n",
      "ğŸ“¥ Fetching page 108...\n",
      "ğŸ“¥ Fetching page 109...\n",
      "ğŸ“¥ Fetching page 110...\n",
      "ğŸ“¥ Fetching page 111...\n",
      "ğŸ“¥ Fetching page 112...\n",
      "ğŸ“¥ Fetching page 113...\n",
      "ğŸ“¥ Fetching page 114...\n",
      "ğŸ“¥ Fetching page 115...\n",
      "ğŸ“¥ Fetching page 116...\n",
      "ğŸ“¥ Fetching page 117...\n",
      "ğŸ“¥ Fetching page 118...\n",
      "ğŸ“¥ Fetching page 119...\n",
      "ğŸ“¥ Fetching page 120...\n",
      "ğŸ“¥ Fetching page 121...\n",
      "ğŸ“¥ Fetching page 122...\n",
      "ğŸ“¥ Fetching page 123...\n",
      "ğŸ“¥ Fetching page 124...\n",
      "ğŸ“¥ Fetching page 125...\n",
      "ğŸ“¥ Fetching page 126...\n",
      "ğŸ“¥ Fetching page 127...\n",
      "ğŸ“¥ Fetching page 128...\n",
      "ğŸ“¥ Fetching page 129...\n",
      "ğŸ“¥ Fetching page 130...\n",
      "ğŸ“¥ Fetching page 131...\n",
      "ğŸ“¥ Fetching page 132...\n",
      "ğŸ“¥ Fetching page 133...\n",
      "ğŸ“¥ Fetching page 134...\n",
      "ğŸ“¥ Fetching page 135...\n",
      "ğŸ“¥ Fetching page 136...\n",
      "ğŸ“¥ Fetching page 137...\n",
      "ğŸ“¥ Fetching page 138...\n",
      "ğŸ“¥ Fetching page 139...\n",
      "ğŸ“¥ Fetching page 140...\n",
      "ğŸ“¥ Fetching page 141...\n",
      "ğŸ“¥ Fetching page 142...\n",
      "ğŸ“¥ Fetching page 143...\n",
      "ğŸ“¥ Fetching page 144...\n",
      "ğŸ“¥ Fetching page 145...\n",
      "ğŸ“¥ Fetching page 146...\n",
      "ğŸ“¥ Fetching page 147...\n",
      "ğŸ“¥ Fetching page 148...\n",
      "ğŸ“¥ Fetching page 149...\n",
      "ğŸ“¥ Fetching page 150...\n",
      "ğŸ“¥ Fetching page 151...\n",
      "ğŸ“¥ Fetching page 152...\n",
      "ğŸ“¥ Fetching page 153...\n",
      "ğŸ“¥ Fetching page 154...\n",
      "ğŸ“¥ Fetching page 155...\n",
      "ğŸ“¥ Fetching page 156...\n",
      "ğŸ“¥ Fetching page 157...\n",
      "ğŸ“¥ Fetching page 158...\n",
      "ğŸ“¥ Fetching page 159...\n",
      "ğŸ“¥ Fetching page 160...\n",
      "ğŸ“¥ Fetching page 161...\n",
      "ğŸ“¥ Fetching page 162...\n",
      "ğŸ“¥ Fetching page 163...\n",
      "ğŸ“¥ Fetching page 164...\n",
      "ğŸ“¥ Fetching page 165...\n",
      "ğŸ“¥ Fetching page 166...\n",
      "ğŸ“¥ Fetching page 167...\n",
      "ğŸ“¥ Fetching page 168...\n",
      "ğŸ“¥ Fetching page 169...\n",
      "ğŸ“¥ Fetching page 170...\n",
      "ğŸ“¥ Fetching page 171...\n",
      "ğŸ“¥ Fetching page 172...\n",
      "ğŸ“¥ Fetching page 173...\n",
      "ğŸ“¥ Fetching page 174...\n",
      "ğŸ“¥ Fetching page 175...\n",
      "ğŸ“¥ Fetching page 176...\n",
      "ğŸ“¥ Fetching page 177...\n",
      "ğŸ“¥ Fetching page 178...\n",
      "ğŸ“¥ Fetching page 179...\n",
      "ğŸ“¥ Fetching page 180...\n",
      "ğŸ“¥ Fetching page 181...\n",
      "ğŸ“¥ Fetching page 182...\n",
      "ğŸ“¥ Fetching page 183...\n",
      "ğŸ“¥ Fetching page 184...\n",
      "ğŸ“¥ Fetching page 185...\n",
      "ğŸ“¥ Fetching page 186...\n",
      "ğŸ“¥ Fetching page 187...\n",
      "ğŸ“¥ Fetching page 188...\n",
      "ğŸ“¥ Fetching page 189...\n",
      "ğŸ“¥ Fetching page 190...\n",
      "ğŸ“¥ Fetching page 191...\n",
      "ğŸ“¥ Fetching page 192...\n",
      "ğŸ“¥ Fetching page 193...\n",
      "ğŸ“¥ Fetching page 194...\n",
      "ğŸ“¥ Fetching page 195...\n",
      "ğŸ“¥ Fetching page 196...\n",
      "ğŸ“¥ Fetching page 197...\n",
      "ğŸ“¥ Fetching page 198...\n",
      "ğŸ“¥ Fetching page 199...\n",
      "ğŸ“¥ Fetching page 200...\n",
      "âœ… Done. Saved 246 new entries to nlp_questions_output.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Load existing CSV to avoid duplicates\n",
    "existing_df = pd.read_csv('combined_V2_csv.csv')\n",
    "existing_ids = set(existing_df['question_id'].astype(int))\n",
    "\n",
    "# API Setup\n",
    "API_KEY = 'rl_MdVXEpV7UgL1XMwe5e5BwoqEa'  # Replace with your real API key\n",
    "BASE_URL = \"https://api.stackexchange.com/2.3/questions\"\n",
    "SITE = \"stackoverflow\"\n",
    "TAG = \"nlp\"\n",
    "PAGESIZE = 100\n",
    "MAX_PAGES = 200\n",
    "\n",
    "# Output JSON file\n",
    "output_data = []\n",
    "\n",
    "# Pagination\n",
    "for page in range(1, MAX_PAGES + 1):\n",
    "    print(f\"Fetching page {page}...\")\n",
    "    params = {\n",
    "        'page': page,\n",
    "        'pagesize': PAGESIZE,\n",
    "        'order': 'desc',\n",
    "        'sort': 'activity',\n",
    "        'tagged': TAG,\n",
    "        'site': SITE,\n",
    "        'filter': 'withbody',\n",
    "        'key': API_KEY\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 502:  # Retry on bad gateway\n",
    "        print(\"502 Error. Retrying after 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"API Error: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "    items = response.json().get('items', [])\n",
    "    if not items:\n",
    "        print(\"No more items.\")\n",
    "        break\n",
    "\n",
    "    for item in items:\n",
    "        qid = item.get('question_id')\n",
    "        if not item.get('accepted_answer_id') or qid in existing_ids:\n",
    "            continue\n",
    "\n",
    "        # Fetch accepted answer body\n",
    "        answer_id = item['accepted_answer_id']\n",
    "        answer_url = f\"https://api.stackexchange.com/2.3/answers/{answer_id}\"\n",
    "        ans_params = {\n",
    "            'order': 'desc',\n",
    "            'sort': 'activity',\n",
    "            'site': SITE,\n",
    "            'filter': 'withbody',\n",
    "            'key': API_KEY\n",
    "        }\n",
    "\n",
    "        ans_resp = requests.get(answer_url, params=ans_params)\n",
    "        if ans_resp.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        ans_items = ans_resp.json().get('items', [])\n",
    "        if not ans_items:\n",
    "            continue\n",
    "\n",
    "        answer_body = ans_items[0].get('body', '')\n",
    "        if not answer_body:\n",
    "            continue\n",
    "\n",
    "        # All conditions met; collect required fields\n",
    "        entry = {\n",
    "            'question_id': qid,\n",
    "            'title': item.get('title'),\n",
    "            'body': item.get('body'),\n",
    "            'tags': item.get('tags'),\n",
    "            'accepted_answer_id': answer_id,\n",
    "            'accepted_answer_body': answer_body,\n",
    "            'score': item.get('score')\n",
    "        }\n",
    "        output_data.append(entry)\n",
    "\n",
    "    time.sleep(1.5)  # Avoid hitting rate limits\n",
    "\n",
    "# Save to JSON\n",
    "with open('nlp_questions_output.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Done. Saved {len(output_data)} new entries to nlp_questions_output.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecec17dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CSV file saved as 'nlp_questions_converted.csv'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Load JSON data\n",
    "with open('nlp_questions_output.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define CSV file path\n",
    "csv_file = 'nlp_questions_converted.csv'\n",
    "\n",
    "# Define headers (columns)\n",
    "headers = [\n",
    "    'question_id',\n",
    "    'title',\n",
    "    'body',\n",
    "    'tags',\n",
    "    'accepted_answer_id',\n",
    "    'accepted_answer_body',\n",
    "    'link',\n",
    "    'tag'\n",
    "]\n",
    "\n",
    "# Write to CSV\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for entry in data:\n",
    "        writer.writerow({\n",
    "            'question_id': entry.get('question_id'),\n",
    "            'title': entry.get('title'),\n",
    "            'body': entry.get('body'),\n",
    "            'tags': \", \".join(entry.get('tags', [])),\n",
    "            'accepted_answer_id': entry.get('accepted_answer_id'),\n",
    "            'accepted_answer_body': entry.get('accepted_answer_body'),\n",
    "            'link': f\"https://stackoverflow.com/q/{entry.get('question_id')}\",\n",
    "            'tag': \", \".join(entry.get('tags', []))  # 'tag' is repeated for clarity\n",
    "        })\n",
    "\n",
    "print(\"CSV file saved as 'nlp_questions_converted.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a460051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Combined file saved as 'combined_V3_output.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both CSV files\n",
    "csv1 = pd.read_csv('combined_V2_csv.csv')\n",
    "csv2 = pd.read_csv('nlp_questions_converted.csv')\n",
    "\n",
    "# Combine them\n",
    "combined = pd.concat([csv1, csv2], ignore_index=True)\n",
    "\n",
    "# Save the combined CSV\n",
    "combined.to_csv('combined_V3_output.csv', index=False)\n",
    "\n",
    "print(\"Combined file saved as 'combined_V3_output.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73e01f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14589, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"combined_V3_output.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18664ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Fetching page 1...\n",
      "ğŸ“¥ Fetching page 2...\n",
      "ğŸ“¥ Fetching page 3...\n",
      "ğŸ“¥ Fetching page 4...\n",
      "ğŸ“¥ Fetching page 5...\n",
      "ğŸ“¥ Fetching page 6...\n",
      "ğŸ“¥ Fetching page 7...\n",
      "ğŸ“¥ Fetching page 8...\n",
      "ğŸ“¥ Fetching page 9...\n",
      "ğŸ“¥ Fetching page 10...\n",
      "ğŸ“¥ Fetching page 11...\n",
      "ğŸ“¥ Fetching page 12...\n",
      "ğŸ“¥ Fetching page 13...\n",
      "ğŸ“¥ Fetching page 14...\n",
      "ğŸ“¥ Fetching page 15...\n",
      "ğŸ“¥ Fetching page 16...\n",
      "ğŸ“¥ Fetching page 17...\n",
      "ğŸ“¥ Fetching page 18...\n",
      "ğŸ“¥ Fetching page 19...\n",
      "ğŸ“¥ Fetching page 20...\n",
      "ğŸ“¥ Fetching page 21...\n",
      "ğŸ“¥ Fetching page 22...\n",
      "ğŸ“¥ Fetching page 23...\n",
      "ğŸ“¥ Fetching page 24...\n",
      "ğŸ“¥ Fetching page 25...\n",
      "ğŸ“¥ Fetching page 26...\n",
      "ğŸ“¥ Fetching page 27...\n",
      "ğŸ“¥ Fetching page 28...\n",
      "ğŸ“¥ Fetching page 29...\n",
      "ğŸ“¥ Fetching page 30...\n",
      "ğŸ“¥ Fetching page 31...\n",
      "ğŸ“¥ Fetching page 32...\n",
      "ğŸ“¥ Fetching page 33...\n",
      "ğŸ“¥ Fetching page 34...\n",
      "ğŸ“¥ Fetching page 35...\n",
      "ğŸ“¥ Fetching page 36...\n",
      "ğŸ“¥ Fetching page 37...\n",
      "ğŸ“¥ Fetching page 38...\n",
      "ğŸ“¥ Fetching page 39...\n",
      "ğŸ“¥ Fetching page 40...\n",
      "ğŸ“¥ Fetching page 41...\n",
      "ğŸ“¥ Fetching page 42...\n",
      "ğŸ“¥ Fetching page 43...\n",
      "ğŸ“¥ Fetching page 44...\n",
      "ğŸ“¥ Fetching page 45...\n",
      "ğŸ“¥ Fetching page 46...\n",
      "ğŸ“¥ Fetching page 47...\n",
      "ğŸ“¥ Fetching page 48...\n",
      "ğŸ“¥ Fetching page 49...\n",
      "ğŸ“¥ Fetching page 50...\n",
      "ğŸ“¥ Fetching page 51...\n",
      "ğŸ“¥ Fetching page 52...\n",
      "ğŸ“¥ Fetching page 53...\n",
      "ğŸ“¥ Fetching page 54...\n",
      "ğŸ“¥ Fetching page 55...\n",
      "ğŸ“¥ Fetching page 56...\n",
      "ğŸ“¥ Fetching page 57...\n",
      "ğŸ“¥ Fetching page 58...\n",
      "ğŸ“¥ Fetching page 59...\n",
      "ğŸ“¥ Fetching page 60...\n",
      "ğŸ“¥ Fetching page 61...\n",
      "ğŸ“¥ Fetching page 62...\n",
      "ğŸ“¥ Fetching page 63...\n",
      "ğŸ“¥ Fetching page 64...\n",
      "ğŸ“¥ Fetching page 65...\n",
      "ğŸ“¥ Fetching page 66...\n",
      "ğŸ“¥ Fetching page 67...\n",
      "ğŸ“¥ Fetching page 68...\n",
      "ğŸ“¥ Fetching page 69...\n",
      "ğŸ“¥ Fetching page 70...\n",
      "ğŸ“¥ Fetching page 71...\n",
      "ğŸ“¥ Fetching page 72...\n",
      "ğŸ“¥ Fetching page 73...\n",
      "ğŸ“¥ Fetching page 74...\n",
      "ğŸ“¥ Fetching page 75...\n",
      "ğŸ“¥ Fetching page 76...\n",
      "ğŸ“¥ Fetching page 77...\n",
      "ğŸ“¥ Fetching page 78...\n",
      "ğŸ“¥ Fetching page 79...\n",
      "ğŸ“¥ Fetching page 80...\n",
      "ğŸ“¥ Fetching page 81...\n",
      "ğŸ“¥ Fetching page 82...\n",
      "ğŸ“¥ Fetching page 83...\n",
      "ğŸ“¥ Fetching page 84...\n",
      "ğŸ“¥ Fetching page 85...\n",
      "ğŸ“¥ Fetching page 86...\n",
      "ğŸ“¥ Fetching page 87...\n",
      "ğŸ“¥ Fetching page 88...\n",
      "ğŸ“¥ Fetching page 89...\n",
      "ğŸ“¥ Fetching page 90...\n",
      "ğŸ“¥ Fetching page 91...\n",
      "ğŸ“¥ Fetching page 92...\n",
      "ğŸ“¥ Fetching page 93...\n",
      "ğŸ“¥ Fetching page 94...\n",
      "ğŸ“¥ Fetching page 95...\n",
      "ğŸ“¥ Fetching page 96...\n",
      "ğŸ“¥ Fetching page 97...\n",
      "ğŸ“¥ Fetching page 98...\n",
      "ğŸ“¥ Fetching page 99...\n",
      "ğŸ“¥ Fetching page 100...\n",
      "ğŸ“¥ Fetching page 101...\n",
      "ğŸ“¥ Fetching page 102...\n",
      "ğŸ“¥ Fetching page 103...\n",
      "ğŸ“¥ Fetching page 104...\n",
      "ğŸ“¥ Fetching page 105...\n",
      "ğŸ“¥ Fetching page 106...\n",
      "ğŸ“¥ Fetching page 107...\n",
      "ğŸ“¥ Fetching page 108...\n",
      "ğŸ“¥ Fetching page 109...\n",
      "ğŸ“¥ Fetching page 110...\n",
      "ğŸ“¥ Fetching page 111...\n",
      "ğŸ“¥ Fetching page 112...\n",
      "ğŸ“¥ Fetching page 113...\n",
      "ğŸ“¥ Fetching page 114...\n",
      "ğŸ“¥ Fetching page 115...\n",
      "ğŸ“¥ Fetching page 116...\n",
      "ğŸ“¥ Fetching page 117...\n",
      "ğŸ“¥ Fetching page 118...\n",
      "ğŸ“¥ Fetching page 119...\n",
      "ğŸ“¥ Fetching page 120...\n",
      "ğŸ“¥ Fetching page 121...\n",
      "ğŸ“¥ Fetching page 122...\n",
      "ğŸ“¥ Fetching page 123...\n",
      "ğŸ“¥ Fetching page 124...\n",
      "ğŸ“¥ Fetching page 125...\n",
      "ğŸ“¥ Fetching page 126...\n",
      "ğŸ“¥ Fetching page 127...\n",
      "ğŸ“¥ Fetching page 128...\n",
      "ğŸ“¥ Fetching page 129...\n",
      "ğŸ“¥ Fetching page 130...\n",
      "ğŸ“¥ Fetching page 131...\n",
      "ğŸ“¥ Fetching page 132...\n",
      "ğŸ“¥ Fetching page 133...\n",
      "ğŸ“¥ Fetching page 134...\n",
      "ğŸ“¥ Fetching page 135...\n",
      "ğŸ“¥ Fetching page 136...\n",
      "ğŸ“¥ Fetching page 137...\n",
      "ğŸ“¥ Fetching page 138...\n",
      "ğŸ“¥ Fetching page 139...\n",
      "ğŸ“¥ Fetching page 140...\n",
      "ğŸ“¥ Fetching page 141...\n",
      "ğŸ“¥ Fetching page 142...\n",
      "ğŸ“¥ Fetching page 143...\n",
      "ğŸ“¥ Fetching page 144...\n",
      "ğŸ“¥ Fetching page 145...\n",
      "ğŸ“¥ Fetching page 146...\n",
      "ğŸ“¥ Fetching page 147...\n",
      "ğŸ“¥ Fetching page 148...\n",
      "ğŸ“¥ Fetching page 149...\n",
      "ğŸ“¥ Fetching page 150...\n",
      "ğŸ“¥ Fetching page 151...\n",
      "ğŸ“¥ Fetching page 152...\n",
      "ğŸ“¥ Fetching page 153...\n",
      "ğŸ“¥ Fetching page 154...\n",
      "ğŸ“¥ Fetching page 155...\n",
      "ğŸ“¥ Fetching page 156...\n",
      "ğŸ“¥ Fetching page 157...\n",
      "ğŸ“¥ Fetching page 158...\n",
      "ğŸ“¥ Fetching page 159...\n",
      "ğŸ“¥ Fetching page 160...\n",
      "ğŸ“¥ Fetching page 161...\n",
      "ğŸ“¥ Fetching page 162...\n",
      "ğŸ“¥ Fetching page 163...\n",
      "ğŸ“¥ Fetching page 164...\n",
      "ğŸ“¥ Fetching page 165...\n",
      "ğŸ“¥ Fetching page 166...\n",
      "ğŸ“¥ Fetching page 167...\n",
      "ğŸ“¥ Fetching page 168...\n",
      "ğŸ“¥ Fetching page 169...\n",
      "ğŸ“¥ Fetching page 170...\n",
      "ğŸ“¥ Fetching page 171...\n",
      "ğŸ“¥ Fetching page 172...\n",
      "ğŸ“¥ Fetching page 173...\n",
      "ğŸ“¥ Fetching page 174...\n",
      "ğŸ“¥ Fetching page 175...\n",
      "ğŸ“¥ Fetching page 176...\n",
      "ğŸ“¥ Fetching page 177...\n",
      "ğŸ“¥ Fetching page 178...\n",
      "ğŸ“¥ Fetching page 179...\n",
      "ğŸ“¥ Fetching page 180...\n",
      "ğŸ“¥ Fetching page 181...\n",
      "ğŸ“¥ Fetching page 182...\n",
      "ğŸ“¥ Fetching page 183...\n",
      "ğŸ“¥ Fetching page 184...\n",
      "ğŸ“¥ Fetching page 185...\n",
      "ğŸ“¥ Fetching page 186...\n",
      "ğŸ“¥ Fetching page 187...\n",
      "ğŸ“¥ Fetching page 188...\n",
      "ğŸ“¥ Fetching page 189...\n",
      "ğŸ“¥ Fetching page 190...\n",
      "ğŸ“¥ Fetching page 191...\n",
      "ğŸ“¥ Fetching page 192...\n",
      "ğŸ“¥ Fetching page 193...\n",
      "ğŸ“¥ Fetching page 194...\n",
      "ğŸ“¥ Fetching page 195...\n",
      "ğŸ“¥ Fetching page 196...\n",
      "ğŸ“¥ Fetching page 197...\n",
      "ğŸ“¥ Fetching page 198...\n",
      "ğŸ“¥ Fetching page 199...\n",
      "ğŸ“¥ Fetching page 200...\n",
      "âœ… Done. Saved 8136 entries to nlp_questions_output_nodup.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "\n",
    "# API Setup\n",
    "API_KEY = 'rl_MdVXEpV7UgL1XMwe5e5BwoqEa'  # Replace with your actual API key\n",
    "BASE_URL = \"https://api.stackexchange.com/2.3/questions\"\n",
    "SITE = \"stackoverflow\"\n",
    "TAG = \"nlp\"\n",
    "PAGESIZE = 100\n",
    "MAX_PAGES = 200\n",
    "\n",
    "# Output JSON file\n",
    "output_data = []\n",
    "\n",
    "# Pagination\n",
    "for page in range(1, MAX_PAGES + 1):\n",
    "    print(f\"Fetching page {page}...\")\n",
    "    params = {\n",
    "        'page': page,\n",
    "        'pagesize': PAGESIZE,\n",
    "        'order': 'desc',\n",
    "        'sort': 'activity',\n",
    "        'tagged': TAG,\n",
    "        'site': SITE,\n",
    "        'filter': 'withbody',\n",
    "        'key': API_KEY\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 502:  # Retry on bad gateway\n",
    "        print(\"502 Error. Retrying after 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"API Error: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "    items = response.json().get('items', [])\n",
    "    if not items:\n",
    "        print(\"No more items.\")\n",
    "        break\n",
    "\n",
    "    for item in items:\n",
    "        qid = item.get('question_id')\n",
    "        if not item.get('accepted_answer_id'):\n",
    "            continue\n",
    "\n",
    "        # Fetch accepted answer body\n",
    "        answer_id = item['accepted_answer_id']\n",
    "        answer_url = f\"https://api.stackexchange.com/2.3/answers/{answer_id}\"\n",
    "        ans_params = {\n",
    "            'order': 'desc',\n",
    "            'sort': 'activity',\n",
    "            'site': SITE,\n",
    "            'filter': 'withbody',\n",
    "            'key': API_KEY\n",
    "        }\n",
    "\n",
    "        ans_resp = requests.get(answer_url, params=ans_params)\n",
    "        if ans_resp.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        ans_items = ans_resp.json().get('items', [])\n",
    "        if not ans_items:\n",
    "            continue\n",
    "\n",
    "        answer_body = ans_items[0].get('body', '')\n",
    "        if not answer_body:\n",
    "            continue\n",
    "\n",
    "        # Collect required fields\n",
    "        entry = {\n",
    "            'question_id': qid,\n",
    "            'title': item.get('title'),\n",
    "            'body': item.get('body'),\n",
    "            'tags': item.get('tags'),\n",
    "            'accepted_answer_id': answer_id,\n",
    "            'accepted_answer_body': answer_body,\n",
    "            'score': item.get('score')\n",
    "        }\n",
    "        output_data.append(entry)\n",
    "\n",
    "    time.sleep(1.5)  # Avoid hitting rate limits\n",
    "\n",
    "# Save to JSON\n",
    "with open('nlp_questions_output_nodup.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Done. Saved {len(output_data)} entries to nlp_questions_output_nodup.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a638a0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CSV file saved as 'nlp_questions_converted.csv'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Load JSON data\n",
    "with open('nlp_questions_output_nodup.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define CSV file path\n",
    "csv_file = 'nlp_questions_converted.csv'\n",
    "\n",
    "# Define headers (columns)\n",
    "headers = [\n",
    "    'question_id',\n",
    "    'title',\n",
    "    'body',\n",
    "    'tags',\n",
    "    'accepted_answer_id',\n",
    "    'accepted_answer_body',\n",
    "    'link',\n",
    "    'tag'\n",
    "]\n",
    "\n",
    "# Write to CSV\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for entry in data:\n",
    "        writer.writerow({\n",
    "            'question_id': entry.get('question_id'),\n",
    "            'title': entry.get('title'),\n",
    "            'body': entry.get('body'),\n",
    "            'tags': \", \".join(entry.get('tags', [])),\n",
    "            'accepted_answer_id': entry.get('accepted_answer_id'),\n",
    "            'accepted_answer_body': entry.get('accepted_answer_body'),\n",
    "            'link': f\"https://stackoverflow.com/q/{entry.get('question_id')}\",\n",
    "            'tag': \", \".join(entry.get('tags', []))\n",
    "        })\n",
    "\n",
    "print(\"CSV file saved as 'nlp_questions_converted.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f68506fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8136, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = pd.read_csv(\"nlp_questions_converted.csv\")\n",
    "df_2.head()\n",
    "df_2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62842e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Combined file saved as 'combined_V4_output.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both CSV files\n",
    "csv1 = pd.read_csv('combined_V3_output.csv')\n",
    "csv2 = pd.read_csv('nlp_questions_converted.csv')\n",
    "\n",
    "# Combine them\n",
    "combined = pd.concat([csv1, csv2], ignore_index=True)\n",
    "\n",
    "# Save the combined CSV\n",
    "combined.to_csv('combined_V4_output.csv', index=False)\n",
    "\n",
    "print(\"âœ… Combined file saved as 'combined_V4_output.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cefca047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22725, 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_f = pd.read_csv(\"combined_V4_output.csv\")\n",
    "df_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e2a65bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "147f142e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>tags</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>accepted_answer_body</th>\n",
       "      <th>link</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79549787</td>\n",
       "      <td>Why does Presidio with spacy nlp engine not re...</td>\n",
       "      <td>&lt;p&gt;I'm using spaCy with the pl_core_news_lg mo...</td>\n",
       "      <td>['python', 'nlp', 'spacy', 'presidio']</td>\n",
       "      <td>79552218</td>\n",
       "      <td>&lt;p&gt;The configuration file is missing the 'labe...</td>\n",
       "      <td>https://stackoverflow.com/questions/79549787</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79548202</td>\n",
       "      <td>GPT-2 and other models from huggingface -100 l...</td>\n",
       "      <td>&lt;p&gt;I understand the -100 label id is used so t...</td>\n",
       "      <td>['nlp', 'huggingface-transformers', 'pre-train...</td>\n",
       "      <td>79551169</td>\n",
       "      <td>&lt;p&gt;The author of the tutorial you mentioned se...</td>\n",
       "      <td>https://stackoverflow.com/questions/79548202</td>\n",
       "      <td>nlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79523269</td>\n",
       "      <td>Trouble getting importing gensim to work in colab</td>\n",
       "      <td>&lt;p&gt;I am trying to import gensim into colab.&lt;/p...</td>\n",
       "      <td>['numpy', 'nlp', 'dependencies', 'google-colab...</td>\n",
       "      <td>79523777</td>\n",
       "      <td>&lt;p&gt;You have to restart the session for the und...</td>\n",
       "      <td>https://stackoverflow.com/questions/79523269</td>\n",
       "      <td>numpy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79501178</td>\n",
       "      <td>Store images instead of showing in a server</td>\n",
       "      <td>&lt;p&gt;I am running the code found on this &lt;a href...</td>\n",
       "      <td>['python', 'nlp', 'large-language-model']</td>\n",
       "      <td>79501337</td>\n",
       "      <td>&lt;p&gt;I can't test it but ...&lt;/p&gt;\\n&lt;p&gt;I checked &lt;...</td>\n",
       "      <td>https://stackoverflow.com/questions/79501178</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79482283</td>\n",
       "      <td>Presidio with Langchain Experimental does not ...</td>\n",
       "      <td>&lt;p&gt;I am using presidio/langchain_experimental ...</td>\n",
       "      <td>['python', 'nlp', 'spacy', 'langchain', 'presi...</td>\n",
       "      <td>79495969</td>\n",
       "      <td>&lt;p&gt;After some test I was able to find the solu...</td>\n",
       "      <td>https://stackoverflow.com/questions/79482283</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                              title  \\\n",
       "0     79549787  Why does Presidio with spacy nlp engine not re...   \n",
       "1     79548202  GPT-2 and other models from huggingface -100 l...   \n",
       "2     79523269  Trouble getting importing gensim to work in colab   \n",
       "3     79501178        Store images instead of showing in a server   \n",
       "4     79482283  Presidio with Langchain Experimental does not ...   \n",
       "\n",
       "                                                body  \\\n",
       "0  <p>I'm using spaCy with the pl_core_news_lg mo...   \n",
       "1  <p>I understand the -100 label id is used so t...   \n",
       "2  <p>I am trying to import gensim into colab.</p...   \n",
       "3  <p>I am running the code found on this <a href...   \n",
       "4  <p>I am using presidio/langchain_experimental ...   \n",
       "\n",
       "                                                tags  accepted_answer_id  \\\n",
       "0             ['python', 'nlp', 'spacy', 'presidio']            79552218   \n",
       "1  ['nlp', 'huggingface-transformers', 'pre-train...            79551169   \n",
       "2  ['numpy', 'nlp', 'dependencies', 'google-colab...            79523777   \n",
       "3          ['python', 'nlp', 'large-language-model']            79501337   \n",
       "4  ['python', 'nlp', 'spacy', 'langchain', 'presi...            79495969   \n",
       "\n",
       "                                accepted_answer_body  \\\n",
       "0  <p>The configuration file is missing the 'labe...   \n",
       "1  <p>The author of the tutorial you mentioned se...   \n",
       "2  <p>You have to restart the session for the und...   \n",
       "3  <p>I can't test it but ...</p>\\n<p>I checked <...   \n",
       "4  <p>After some test I was able to find the solu...   \n",
       "\n",
       "                                           link     tag  \n",
       "0  https://stackoverflow.com/questions/79549787  python  \n",
       "1  https://stackoverflow.com/questions/79548202     nlp  \n",
       "2  https://stackoverflow.com/questions/79523269   numpy  \n",
       "3  https://stackoverflow.com/questions/79501178  python  \n",
       "4  https://stackoverflow.com/questions/79482283  python  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_f = pd.read_csv(\"combined_V4_output.csv\")\n",
    "df_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a074965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
