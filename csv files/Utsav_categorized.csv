question_id,title,body,tags,accepted_answer_id,accepted_answer_body,link,tag,creation_date,view_count,score,answer_count,is_answered,last_activity_date,last_edit_date,category
74497075,"extract two specified words from the dataframe and place them in a new column, then delete the rows","this is the dataframe:
data = {""company"" : [[""consensys""] , [""cognizant""], [""ibm""], [""ibm""], [""reddit, inc""], [""reddit, inc""], [""ibm""]],
""skills"" : [['services', 'scientist technical expertise', 'databases'], ['datacomputing tools experience', 'deep learning models', 'cloud services'], ['quantitative analytical projects', 'financial services', 'field experience'],
['filesystems server architectures', 'systems', 'statistical analysis', 'data analytics', 'workflows', 'aws cloud services'], ['aws services'], ['data mining statistics', 'statistical analysis', 'aws cloud', 'services', 'data discovery', 'visualization'], ['communication skills experience', 'services', 'manufacturing environment', 'sox compliance']]}

dff = pd.dataframe(data)
dff


i need to create a new column, and i want to start by taking specific
words out of the skills column.
the row that does not include those specific words should then be
deleted.
specific words: 'services', 'statistical analysis'

expected output:





company
skills
new_col




0
[consensys]
[services, scientist technical expertise, databases]
[services]


1
[ibm]
[filesystems server architectures, systems, statistical analysis, data analytics, workflows, aws cloud services]
[services, statistical analysis]


2
[reddit, inc]
[data mining statistics, statistical analysis, aws cloud, services, data discovery, visualization]
[statistical analysis]


3
[ibm]
['communication skills experience', 'services', 'manufacturing environment', 'sox compliance']
[services]




i tried quite a lot of code in an effort to extract a specific word from the one that was available on stack overflow, but i was unsuccessful.","['python', 'pandas', 'dataframe', 'nlp']",74497192,"word = ['services', 'statistical analysis']
s1 = df['skills'].apply(lambda x: [i for i in word if i in x])

output(s1):
0                          [services]
1                                  []
2                                  []
3              [statistical analysis]
4                                  []
5    [services, statistical analysis]
6                          [services]
name: skills, dtype: object

make s1 to new_col and boolean indexing
df.assign(new_col=s1)[lambda x: x['new_col'].astype('bool')]

result:
    company skills  new_col
0   [consensys] [services, scientist technical expertise, data...   [services]
3   [ibm]   [filesystems server architectures, systems, st...   [statistical analysis]
5   [reddit, inc]   [data mining statistics, statistical analysis,...   [services, statistical analysis]
6   [ibm]   [communication skills experience, services, ma...   [services]

i think you should make more simple example",https://stackoverflow.com/questions/74497075,python,19-11-2022 02:14,35.0,0.0,2.0,True,19-11-2022 05:12,19-11-2022 02:35,Data Wrangling
49564176,python (nltk) - more efficient way to extract noun phrases?,"i've got a machine learning task involving a large amount of text data. i want to identify, and extract, noun-phrases in the training text so i can use them for feature construction later on in the pipeline. 
i've extracted the type of noun-phrases i wanted from text but i'm fairly new to nltk, so i approached this problem in a way where i can break down each step in list comprehensions like you can see below. 
but my real question is, am i reinventing the wheel here? is there a faster way to do this that i'm not seeing?
import nltk
import pandas as pd

mydata = pd.read_excel(""\user\train_.xlsx"")
texts = mydata['message']

# defining a grammar & parser
np = ""np: {(<v\w+>|<nn\w?>)+.*<nn\w?>}""
chunkr = nltk.regexpparser(np)

tokens = [nltk.word_tokenize(i) for i in texts]

tag_list = [nltk.pos_tag(w) for w in tokens]

phrases = [chunkr.parse(sublist) for sublist in tag_list]

leaves = [[subtree.leaves() for subtree in tree.subtrees(filter = lambda t: t.label == 'np')] for tree in phrases]

flatten the list of lists of lists of tuples that we've ended up with, into
just a list of lists of tuples
leaves = [tupls for sublists in leaves for tupls in sublists]

join the extracted terms into one bigram
nounphrases = [unigram[0][1]+' '+unigram[1][0] in leaves]","['python-3.x', 'pandas', 'nlp', 'nltk', 'text-chunking']",49584275,"take a look at why is my nltk function slow when processing the dataframe?, there's no need to iterate through all rows multiple times if you don't need intermediate steps. 
with ne_chunk and solution from 

nltk named entity recognition to a python list and
how can i extract gpe(location) using nltk ne_chunk?

[code]:
from nltk import word_tokenize, pos_tag, ne_chunk
from nltk import regexpparser
from nltk import tree
import pandas as pd

def get_continuous_chunks(text, chunk_func=ne_chunk):
    chunked = chunk_func(pos_tag(word_tokenize(text)))
    continuous_chunk = []
    current_chunk = []

    for subtree in chunked:
        if type(subtree) == tree:
            current_chunk.append("" "".join([token for token, pos in subtree.leaves()]))
        elif current_chunk:
            named_entity = "" "".join(current_chunk)
            if named_entity not in continuous_chunk:
                continuous_chunk.append(named_entity)
                current_chunk = []
        else:
            continue

    return continuous_chunk

df = pd.dataframe({'text':['this is a foo, bar sentence with new york city.', 
                           'another bar foo washington dc thingy with bruce wayne.']})

df['text'].apply(lambda sent: get_continuous_chunks((sent)))

[out]:
0                   [new york]
1    [washington, bruce wayne]
name: text, dtype: object

to use the custom regexpparser :
from nltk import word_tokenize, pos_tag, ne_chunk
from nltk import regexpparser
from nltk import tree
import pandas as pd

# defining a grammar & parser
np = ""np: {(<v\w+>|<nn\w?>)+.*<nn\w?>}""
chunker = regexpparser(np)

def get_continuous_chunks(text, chunk_func=ne_chunk):
    chunked = chunk_func(pos_tag(word_tokenize(text)))
    continuous_chunk = []
    current_chunk = []

    for subtree in chunked:
        if type(subtree) == tree:
            current_chunk.append("" "".join([token for token, pos in subtree.leaves()]))
        elif current_chunk:
            named_entity = "" "".join(current_chunk)
            if named_entity not in continuous_chunk:
                continuous_chunk.append(named_entity)
                current_chunk = []
        else:
            continue

    return continuous_chunk


df = pd.dataframe({'text':['this is a foo, bar sentence with new york city.', 
                           'another bar foo washington dc thingy with bruce wayne.']})


df['text'].apply(lambda sent: get_continuous_chunks(sent, chunker.parse))

[out]:
0                  [bar sentence, new york city]
1    [bar foo washington dc thingy, bruce wayne]
name: text, dtype: object",https://stackoverflow.com/questions/49564176,python-3.x,29-03-2018 20:04,22648.0,12.0,4.0,True,16-01-2023 22:23,31-03-2018 04:35,Task-specific Help
33676526,pos-tagger is incredibly slow,"i am using nltk to generate n-grams from sentences by first removing given stop words. however, nltk.pos_tag() is extremely slow taking up to 0.6 sec on my cpu (intel i7).
the output:
['the first time i went, and was completely taken by the live jazz band and atmosphere, i ordered the lobster cobb salad.']
0.620481014252
[""it's simply the best meal in nyc.""]
0.640982151031
['you cannot go wrong at the red eye grill.']
0.644664049149

the code:
for sentence in source:

    nltk_ngrams = none

    if stop_words is not none:   
        start = time.time()
        sentence_pos = nltk.pos_tag(word_tokenize(sentence))
        print time.time() - start

        filtered_words = [word for (word, pos) in sentence_pos if pos not in stop_words]
    else:
        filtered_words = ngrams(sentence.split(), n)

is this really that slow or am i doing something wrong here?","['python', 'nlp', 'nltk', 'pos-tagger']",33677051,"use pos_tag_sents for tagging multiple sentences:
>>> import time
>>> from nltk.corpus import brown
>>> from nltk import pos_tag
>>> from nltk import pos_tag_sents
>>> sents = brown.sents()[:10]
>>> start = time.time(); pos_tag(sents[0]); print time.time() - start
0.934092998505
>>> start = time.time(); [pos_tag(s) for s in sents]; print time.time() - start
9.5061340332
>>> start = time.time(); pos_tag_sents(sents); print time.time() - start 
0.939551115036",https://stackoverflow.com/questions/33676526,python,12-11-2015 16:32,4397.0,8.0,4.0,True,04-01-2024 03:14,12-11-2015 16:59,Implementation Issues
67964793,how to append tokenized sentences as row to a csv,"i am trying to do sentence tokenization several .txt files from a path, and then append each tokenized sentence to a new row with the *.txt document id as csv.
there are several *txt files in the path (work_dir)
in the below example, the first column needs to be the file name (wltw_5_2016_02_29), and the next column tokenized sentence. such that, if there are 40 sentences in a document, i would expect 40 rows with the same file name in the first column and the second column sentences.  i also attached a picture to show how the csv output is expected.
import nltk
work_dir='/content/drive/my drive/deneme'
filename = 'wltw_5_2016_02_29.txt'
file = open(filename, 'rt')
text = file.read()
#file.close()
# split into sentences
from nltk import sent_tokenize
sentences = sent_tokenize(text)
print(sentences)
import csv

with open('writedata.csv', mode='w') as file:
    writer = csv.writer(file, delimiter=',', quotechar='""', quoting=csv.quote_minimal)
    writer.writerow((""filename"", ""sentence""))
    writer.writerow((filename, sentences))

i tried this approach but i couldnot manage it.  here

with the above code, it writes everything to the same row. however as seen in the above example, i want to write them to the same column by appending as row.","['python', 'csv', 'nlp', 'nltk', 'tokenize']",67973798,"i think my issue was with the sequence of the code:
here comes the working one, in case anyone has the same issue, feel free to use it:
import nltk, glob, csv
from nltk import sent_tokenize
files = glob.glob(""/content/drive/my drive/deneme/*.txt"")

with open('writedata.csv', mode='w') as new_file:
  writer = csv.writer(new_file, delimiter=',', quotechar='""', quoting=csv.quote_minimal)
  for filename in files:

    # take all sentences from a given file
    file = open(filename, 'rt')
    text = file.read()
    file.close()
 
    sentences = sent_tokenize(text)
    print(sentences)

    for sentence in sentences:
      writer.writerow((filename, sentence))",https://stackoverflow.com/questions/67964793,python,14-06-2021 04:28,423.0,0.0,1.0,True,14-06-2021 16:15,14-06-2021 05:11,Implementation Issues
36241051,apache lucene doesn&#39;t filter stop words despite the usage of stopanalyzer and stopfilter,"i have a module based on apache lucene 5.5 / 6.0 which retrieves keywords. everything is working fine except one thing ï¿½ï¿½ï¿½ lucene doesn't filter stop words.
i tried to enable stop word filtering with two different approaches.
approach #1:
tokenstream = new stopfilter(new asciifoldingfilter(new classicfilter(new lowercasefilter(stdtoken))), englishanalyzer.getdefaultstopset());
tokenstream.reset();

approach #2:
tokenstream = new stopfilter(new classicfilter(new lowercasefilter(stdtoken)), stopanalyzer.english_stop_words_set);
tokenstream.reset();

the full code is available here:

my questions:

why lucene doesn't filter stop words?

how can i enable the stop words filtering in lucene 5.5 / 6.0?","['java', 'apache', 'lucene', 'information-retrieval', 'stop-words']",36246089,"the pitfall was in the default lucene's stop words list, i expected, it is much more broader.
here is the code which by default tries to load the customized stop words list and if it's failed then uses the standard one:
chararrayset stopwordsset;

try {
    // use customized stop words list
    string stopwordsdictionary = fileutils.readfiletostring(new file(%path_to_file%));
    stopwordsset = wordlistloader.getwordset(new stringreader(stopwordsdictionary));
} catch (filenotfoundexception e) {
    // use standard stop words list
    stopwordsset = chararrayset.copy(standardanalyzer.stop_words_set);
}

tokenstream = new stopfilter(new asciifoldingfilter(new classicfilter(new lowercasefilter(stdtoken))), stopwordsset);
tokenstream.reset();",https://stackoverflow.com/questions/36241051,java,26-03-2016 21:22,2242.0,3.0,2.0,True,22-11-2022 16:47,22-11-2022 16:47,Implementation Issues
71279997,can&#39;t access indexed tuple,"i'm trying to traverse through a dictionary that essentially contains tuples and keys for tuples like this:
(101940039, 'yoel'): 0.0016034940264139383, 
(101940039, 'yossi'): 0.004810482079241815, 
(101940039, 'youngmen'): 0.0016034940264139383}

i need to access the value of the key, i.e., the string of the tuple. i tried many things, like converting to the dictionary, using key[0] just gives me ""'int' object is not subscribable""..
    def matching_score(k, tokens, tf_idf_score):
    print(""matching score"")
    query_weights = {}
    for word in tokens:
        for key, value in tf_idf_score.items():
            **if key in word**:
                try:
                    query_weights[key[0]] += tf_idf_score[key]
                except:
                    query_weights[key[0]] = tf_idf_score[key]
        
        query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=true)
    print("""")
    
    l = []
    
    for i in query_weights[:10]:
        l.append(i[0])
    
    print(l)","['python', 'tuples', 'key', 'tf-idf']",71280418,"first, this is a recreation of your data as a dictionary:
d1 = {(101940039, 'yoel'): 0.0016034940264139383, 
      (101940039, 'yossi'): 0.004810482079241815, 
      (101940039, 'youngmen'): 0.0016034940264139383}

with keys() it is possible to access the keys. at the same time, we want to convert them into a list.
list(d1.keys())

the result is a list of tuples.
[(101940039, 'yoel'), (101940039, 'yossi'), (101940039, 'youngmen')]
to access individual items in this nested list: first, use the index of the list to select the desired list, and second, use the index of the tuple to select the desired item within.
list(d1.keys())[0][1]

'yoel'
to get all the string elements of the key tuples:
for i in range(len(d1)):
    print(list(d1.keys())[i][1])

yoel
yossi
youngmen",https://stackoverflow.com/questions/71279997,python,26-02-2022 20:17,632.0,0.0,1.0,True,26-02-2022 21:28,26-02-2022 20:33,Implementation Issues
77178335,configure multitenancy with langchain and qdrant,"i'm creating a q&a chatbot and i'm using langchain and qdrant.
i'm trying to configure langchain to be able to use qdrant in a multitenant environment.
the doc from qdrant says that the best approach in my case is to use a ""partition by payload"" and use a group_id = oneclient inside the payload of each element of a collection, so that then it's possible to filter on that group_id (which in my case will be the client).
that's the link to the doc 
i'm using langchain and i have added to the documents that i'm saving inside qdrant a ""group_id"" metadata field.
i'd like to understand how to filter on group_id when i use langchain.
this is how i'm using langchain to retrieve the answer to a question:
qdrant = qdrant(
    client=qdrantclient(...),
    collection_name=""collection1"",
    embeddings=embeddings
)
prompt = ...
llm = chatopenai(...) 
qa_chain = retrievalqawithsourceschain.from_chain_type(
     llm=llm,
     chain_type=""stuff"",
     return_source_documents=true,
     retriever=qdrant.as_retriever(),
     chain_type_kwargs = {""prompt"": prompt}
 )
result = qa_chain({""question"": question})

the group_id will represent the client and it is known before the question.
any help is much appreciated, thanks.","['python', 'chatbot', 'langchain', 'qdrant', 'qdrantclient']",77314317,"i have found the answer. thanks for all the suggestions.
to filter on an attribute ""group_id"" which is the client_id, i'm adding a metadata group_id = client when i load some data with ""vectorestore.from_documents"" and i'm using the ""as_retriever"" function to pass the search filter and return only the sources with that group_id:
chain = retrievalqawithsourceschain.from_chain_type(
    llm=llm,
    chain_type=chain_type,
    max_tokens_limit=max_tokens_limit,
    return_source_documents=true,
    retriever=vectorstore.as_retriever(
        search_kwargs={'filter': {'group_id': client}}
    ),
    reduce_k_below_max_tokens=false,
    chain_type_kwargs = {""prompt"": prompt}
)",https://stackoverflow.com/questions/77178335,python,26-09-2023 08:37,1285.0,1.0,2.0,True,18-10-2023 07:37,27-09-2023 19:10,Implementation Issues
76546004,openai fine-tuning api: why&#160;would i use llamaindex&#160;or&#160;langchain instead of fine-tuning a model?,"i'm just getting started with working with llms, particularly openais and other oss models. there are a lot of guides on using llamaindex to create a store of all your documents and then query on them. i tried it out with a few sample documents, but discovered that each query gets super expensive quickly. i think i used a 50-page pdf document, and a summarization query cost me around 1.5usd per query. i see there's a lot of tokens being sent across, so i'm assuming it's sending the entire document for every query. given that someone might want to use thousands of millions of records, i can't see how something like llamaindex can really be that useful in a cost-effective manner.
on the other hand, i see openai allows you to train a chatgpt model. wouldn't that, or using other custom trained llms, be much cheaper and more effective to query over your own data? why would i ever want to set up llamaindex?","['openai-api', 'langchain', 'chatgpt-api', 'language-model', 'llama-index']",76558875,"tl;dr: use llamaindex or langchain to get an exact answer (i.e., a fact) to a specific question from existing data sources.
why choose llamaindex or langchain over fine-tuning a model?
the answer is simple, but you couldn't answer it yourself because you were only looking at the costs. there are other aspects as well, not just costs. take a look at the usability side of the question.
fine-tuning a model will give the model additional general knowledge, but the fine-tuned model will not give you an exact answer (i.e., a fact) to a specific question.
people train an openai model with some data, but when they ask it something related to the fine-tuning data, they are surprised that the model doesn't answer with the knowledge gained by fine-tuning. see an example explanation on the official openai forum by @juan_olano:

i fine-tuned a 70k-word book. my initial expectation was to have the
desired qa, and at that point i didnï¿½ï¿½ï¿½t know any better. but this
fine-tuning showed me the limits of this approach. it just learned the
style and stayed more or less within the corpus, but hallucinated a
lot.
then i split the book into sentences and worked my way through
embeddings, and now i have a very decent qa system for the book, but
for narrow questions. it is not as good for questions that need the
context of the entire book.

also, see the official openai documentation:

some common use cases where fine-tuning can improve results:

setting the style, tone, format, or other qualitative aspects
improving reliability at producing a desired output
correcting failures to follow complex prompts
handling many edge cases in specific ways
performing a new skill or task thatï¿½ï¿½ï¿½s hard to articulate in a prompt


llamaindex or langchain enable you to connect openai models with your existing data sources. for example, a company has a bunch of internal documents with various instructions, guidelines, rules, etc. llamaindex or langchain can be used to query all those documents and give an exact answer to an employee who needs an answer.
openai models (gpt-3, gpt-3.5, gpt-4, etc.) can't query their knowledge. querying requires calculating embedding vectors from a resource and then calculating cosine similarity, which openai moan't do. an openai model simply gives an answer based on the statistical probability of which word should follow the previous one.
i strongly suggest you read my previous answer regarding semantic search. you'll understand this answer better.",https://stackoverflow.com/questions/76546004,openai-api,24-06-2023 12:12,4977.0,4.0,1.0,True,12-06-2024 15:57,17-01-2024 09:04,Implementation Issues
20907909,stanford corenlp remove/stop red information print outs,"i'm using stanford's corenlp java api and while running it prints out information in red.
it just fills up the command lines when i don't want to see it.
is there anyway of disabling this feature?
example of the red info lines:
searching for resource: stanfordcorenlp.properties
searching for resource: edu/stanford/nlp/pipeline/stanfordcorenlp.properties
adding annotator tokenize
adding annotator ssplit
adding annotator pos
reading pos tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.2 sec].
adding annotator lemma
adding annotator ner
loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [3.0 sec].
loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [2.7 sec].
loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [2.0 sec].
initializing jollydayholiday for sutime
reading tokensregex rules from edu/stanford/nlp/models/sutime/defs.sutime.txt
reading tokensregex rules from edu/stanford/nlp/models/sutime/english.sutime.txt
jan 03, 2014 3:52:37 pm edu.stanford.nlp.ling.tokensregex.coremapexpressionextractor appendrules
info: ignoring inactive rule: temporal-composite-8:ranges
reading tokensregex rules from edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
adding annotator parse
loading parser from serialized file edu/stanford/nlp/models/lexparser/englishpcfg.ser.gz ... done [0.8 sec].
adding annotator dcoref
searching for resource: stanfordcorenlp.properties
searching for resource: edu/stanford/nlp/pipeline/stanfordcorenlp.properties
adding annotator tokenize
adding annotator ssplit
adding annotator pos
adding annotator lemma
adding annotator ner
adding annotator parse
adding annotator dcoref
searching for resource: stanfordcorenlp.properties
searching for resource: edu/stanford/nlp/pipeline/stanfordcorenlp.properties
adding annotator tokenize
adding annotator ssplit
adding annotator pos
adding annotator lemma
adding annotator ner
adding annotator parse
adding annotator dcoref","['java', 'nlp', 'stanford-nlp']",21406307,"depending on your program context, you can just drop all text from the error stream's output during corenlp execution.

// this is your print stream, store the reference
printstream err = system.err;

// now make all writes to the system.err stream silent 
system.seterr(new printstream(new outputstream() {
    public void write(int b) {
    }
}));

// your code here

// set everything bck to its original state afterwards
system.seterr(err);",https://stackoverflow.com/questions/20907909,java,03-01-2014 16:09,2215.0,6.0,1.0,True,19-07-2022 01:08,19-07-2022 01:08,Implementation Issues
66715423,distance between strings by similarity of sound,"is the a quantitative descriptor of similarity between two words based on how they sound/are pronounced, analogous to levenshtein distance?
i know soundex gives same id to similar sounding words, but as far as i undestood it is not a quantitative descriptor of difference between the words.
from jellyfish import soundex

print(soundex(""two""))
print(soundex(""to""))","['python', 'audio', 'nlp', 'linguistics']",66716881,"you could combine phonetic encoding and string comparison algorithm. as a matter of fact jellyfish supplies both.
setting up the libraries examples
from jellyfish import soundex, metaphone, nysiis, match_rating_codex,\
    levenshtein_distance, damerau_levenshtein_distance, hamming_distance,\
    jaro_similarity
from itertools import groupby
import pandas as pd
import numpy as np


datalist = ['two','too','to','fourth','forth','dessert',
            'desert','byrne','boern','smith','smyth','catherine','kathryn']

sounds_encoding_methods = [soundex, metaphone, nysiis, match_rating_codex]

let compare different phonetic encoding
report = pd.dataframe([datalist]).t
report.columns = ['word']
for i in sounds_encoding_methods:
    print(i.__name__)
    report[i.__name__]= report['word'].apply(lambda x: i(x))
print(report)
          soundex metaphone   nysiis match_rating_codex
word                                                   
two          t000        tw       tw                 tw
too          t000         t        t                  t
to           t000         t        t                  t
fourth       f630       fr0     fart               frth
forth        f630       fr0     fart               frth
dessert      d263      tsrt    dasad               dsrt
desert       d263      tsrt    dasad               dsrt
byrne        b650       brn     byrn               byrn
boern        b650       brn     barn                brn
smith        s530       sm0     snat               smth
smyth        s530       sm0     snyt              smyth
catherine    c365      k0rn  cataran              cthrn
kathryn      k365      k0rn   catryn             kthryn

you can see that phonetic encoding is doing a pretty good job making comparable the words. you could see different cases and prefer one or other depending on your case.
now i will just take the above and try to find the closest match using levenshtein_distance, but i could you any other too.
""""""select the closer by algorithm
for instance levenshtein_distance""""""
report2 = pd.dataframe([datalist]).t
report2.columns = ['word']

report.set_index('word',inplace=true)
report2 = report.copy()
for sounds_encoding in sounds_encoding_methods:
    report2[sounds_encoding.__name__] = np.nan
    matched_words = []
    for word in datalist:
        closest_list = []
        for word_2 in datalist:
            if word != word_2:
                closest = {}
                closest['word'] =  word_2
                closest['similarity'] = levenshtein_distance(report.loc[word,sounds_encoding.__name__],
                                     report.loc[word_2,sounds_encoding.__name__])
                closest_list.append(closest)

        report2.loc[word,sounds_encoding.__name__] = pd.dataframe(closest_list).\
            sort_values(by = 'similarity').head(1)['word'].values[0]

print(report2)
             soundex  metaphone     nysiis match_rating_codex
word                                                         
two              too        too        too                too
too              two         to         to                 to
to               two        too        too                too
fourth         forth      forth      forth              forth
forth         fourth     fourth     fourth             fourth
dessert       desert     desert     desert             desert
desert       dessert    dessert    dessert            dessert
byrne          boern      boern      boern              boern
boern          byrne      byrne      byrne              byrne
smith          smyth      smyth      smyth              smyth
smyth          smith      smith      smith              smith
catherine    kathryn    kathryn    kathryn            kathryn
kathryn    catherine  catherine  catherine          catherine

as from above you could clearly see that combinations between  phonetic encoding and string comparison algorithm can be very straight forward.",https://stackoverflow.com/questions/66715423,python,19-03-2021 20:50,2676.0,4.0,1.0,True,19-03-2021 23:42,19-03-2021 20:56,Implementation Issues
78591465,unexpected string validation error in langchain pydantic output parser,"i do not understand why the below use of the pydanticoutputparser is erroring.
the docs do not seem correct - if i follow this exactly (i.e. use with_structured_output exclusively, without an output parser) then the output is a dict, not pydantic class. so i thought i modified it consistently with so so answers e.g. this
from langchain.prompts import prompttemplate
from langchain_openai import chatopenai
from langchain.output_parsers import pydanticoutputparser

from uuid import uuid4
from pydantic import basemodel, field

class testsummary(basemodel):
    """"""represents a summary of the concept""""""

    id: str = field(default_factory=lambda: str(uuid4()), description=""unique identifier"")
    summary: str = field(description=""succinct summary"")
 
llm = chatopenai(model=""gpt-3.5-turbo"", temperature=0).with_structured_output(testsummary)
parser = pydanticoutputparser(pydantic_object=testsummary)
prompt = prompttemplate(
    template=""you are an ai summarizing long texts. text: {stmt}"",
    input_variables=[""stmt""]
)
runnable = prompt | llm | parser 
result = runnable.invoke({""stmt"": ""this is a really long piece of literature i'm too lazy to read""})

the error is
validationerror: 1 validation error for generation
text
  str type expected (type=type_error.str)

as discussed, if i omit the output parser, i get a dict:
runnable = prompt | llm #| parser 
result = runnable.invoke({""stmt"": ""this is a really long piece of literature i'm too lazy to read""})
type(result)
dict","['python', 'pydantic', 'langchain', 'large-language-model']",78592816,"output parsers in langchain receive a string, not structured data. they are used to do what you are already doing with with_structured_output, parse some input string into structured data, or possibly change its format.
from the documentation:

output parsers are classes that help structure language model responses. there are two main methods an output parser must implement:

""get format instructions"": a method which returns a string containing instructions for how the output of a language model should be formatted.
""parse"": a method which takes in a string (assumed to be the response from a language model) and parses it into some structure.


now you have the structured data, you just need to fill the model with it. 
runnable = prompt | llm
result_dict = runnable.invoke({""stmt"": ""this is a really long piece of literature i'm too lazy to read""})
result = testsummary.parse_obj(result_dict)",https://stackoverflow.com/questions/78591465,python,07-06-2024 10:53,4584.0,0.0,1.0,True,07-06-2024 15:27,07-06-2024 11:21,Implementation Issues
24647400,what is the best stemming method in python?,"i tried all the nltk methods for stemming but it gives me weird results with some words. 
examples
it often cut end of words when it shouldn't do it :

poodle => poodl
article articl

or doesn't stem very good : 

easily and easy are not stemmed in the same word
leaves, grows, fairly are not stemmed

do you know other stemming libs in python, or a good dictionary?
thank you","['python', 'nltk', 'stemming']",24648116,"python implementations of the porter, porter2, paice-husk, and lovins stemming algorithms for english are available in the stemming package",https://stackoverflow.com/questions/24647400,python,09-07-2014 07:12,93249.0,46.0,7.0,True,19-03-2022 23:36,09-07-2014 07:19,Conceptual Questions
67378194,loss is nan when using keras bert for classification,"i'm using keras-bert for classification. on some datasets, it runs well and calculates the loss, while on others the loss is nan.
the different datasets are similar in that they are augmented versions of the original one. working with keras-bert, the original data and some augmented versions of the data run well while the other augmented versions of data don't run well.
when i use a regular one-layer bilstm on the augmented versions of data that don't run well with keras-bert, it works out fine which means i can rule out the possibility of the data being faulty or containing spurious values that may affect the way the loss is calculated.
the data in working with has three classes.
i'm using bert based uncased
!wget -q 

can anyone give me pointers as to why the loss is nan?
inputs = model.inputs[:2]
dense = model.layers[-3].output
outputs = keras.layers.dense(3, activation='sigmoid', kernel_initializer=keras.initializers.truncatednormal(stddev=0.02),name = 'real_output')(dense)
decay_steps, warmup_steps = calc_train_steps(train_y.shape[0], batch_size=batch_size,epochs=epochs,)
#(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=lr)
model = keras.models.model(inputs, outputs)
model.compile(adamwarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=lr), loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])
sess = tf.compat.v1.keras.backend.get_session()
uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.compat.v1.report_uninitialized_variables ())])
init_op = tf.compat.v1.variables_initializer([v for v in tf.compat.v1.global_variables() if v.name.split(':')[0] in uninitialized_variables])
sess.run(init_op)
model.fit(train_x,train_y,epochs=epochs,batch_size=batch_size)

 train on 20342 samples
epoch 1/10
20342/20342 [==============================] - 239s 12ms/sample - loss: nan - sparse_categorical_accuracy: 0.5572
epoch 2/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
epoch 3/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2081
epoch 4/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
epoch 5/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
epoch 6/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
epoch 7/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
epoch 8/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2081
epoch 9/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
epoch 10/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
<tensorflow.python.keras.callbacks.history at 0x7f1caf9b0f90>

also, i'm running this on google colab with tensorflow 2.3.0 and keras 2.4.3
update

i looked the data that was causing this issue again and i realised that one of the target labels were missing. i might have mistakenly edited it. once i fixed it, the loss is nan problem dissappeared. however, i'll be awarding the 50 points to the answer i got because it got me to think better about my code. thanks.","['tensorflow', 'keras', 'deep-learning', 'bert-language-model']",67449580,"i noticed one issue in your code but i'm not sure if this the main cause; better if you can possibly provide some reproducible code.
in your above code snippet, you set sigmoid in your last layer activation with unit < 1 which indicate the problem dataset is probably multi-label and that's why the loss function should be binary_crossentropy but you set sparse_categorical_crossentropy which is typical uses multi-class problem and with integer labels.
outputs = keras.layers.dense(3, activation='sigmoid',
                   kernel_initializer=keras.initializers.truncatednormal(stddev=0.02),
                   name = 'real_output')(dense)

model = keras.models.model(inputs, outputs)
model.compile(adamwarmup(decay_steps=decay_steps, 
                            warmup_steps=warmup_steps, lr=lr),
                 loss='sparse_categorical_crossentropy',
                 metrics=['sparse_categorical_accuracy'])
  

so, if your problem data set is a multi-label with the last layer unit = 3, then the set-up should be more like
outputs = keras.layers.dense(3, activation='sigmoid',
                   kernel_initializer=keras.initializers.truncatednormal(stddev=0.02),
                   name = 'real_output')(dense)
model.compile(adamwarmup(decay_steps=decay_steps, 
                                warmup_steps=warmup_steps, lr=lr),
                     loss='binary_crossentropy',
                     metrics=['accuracy'])

but if the problem set is a multi-class problem and your target labels are integer (unit = 3) then the set-up should more like as follows:
outputs = keras.layers.dense(3, activation='softmax',
                   kernel_initializer=keras.initializers.truncatednormal(stddev=0.02),
                   name = 'real_output')(dense)
model.compile(adamwarmup(decay_steps=decay_steps, 
                                warmup_steps=warmup_steps, lr=lr),
                     loss='sparse_categorical_crossentropy',
                     metrics=['sparse_categorical_accuracy'])",https://stackoverflow.com/questions/67378194,tensorflow,04-05-2021 02:53,664.0,1.0,1.0,True,12-05-2021 02:14,12-05-2021 02:14,Implementation Issues
34870614,what does tf.nn.embedding_lookup function do?,"tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=none)

i cannot understand the duty of this function. is it like a lookup table? which means to return the parameters corresponding to each id (in ids)?
for instance, in the skip-gram model if we use tf.nn.embedding_lookup(embeddings, train_inputs), then for each train_input it finds the correspond embedding?","['python', 'tensorflow', 'deep-learning', 'word-embedding', 'nlp']",34877590,"embedding_lookup function retrieves rows of the params tensor. the behavior is similar to using indexing with arrays in numpy. e.g.
matrix = np.random.random([1024, 64])  # 64-dimensional embeddings
ids = np.array([0, 5, 17, 33])
print matrix[ids]  # prints a matrix of shape [4, 64] 

params argument can be also a list of tensors in which case the ids will be distributed among the tensors. for example, given a list of 3 tensors [2, 64], the default behavior is that they will represent ids: [0, 3], [1, 4], [2, 5]. 
partition_strategy controls the way how the ids are distributed among the list. the partitioning is useful for larger scale problems when the matrix might be too large to keep in one piece.",https://stackoverflow.com/questions/34870614,python,19-01-2016 07:14,79200.0,174.0,9.0,True,26-08-2022 13:05,13-07-2018 12:19,Implementation Issues
67256474,lost git changes on local copy,"i cloned a repo, then made some changes on the local copy,
i wanted to upload to a new branch.
so i run the following commands:
git add .
git checkout -b new-branch
git add .
git stash
git push origin new-branch

all of the sudden ,my changes are gone , the repo has the same structure as i cloned it.
how to retrieve my changes??","['git', 'github', 'information-retrieval']",67256640,"it sounds like you stashed your changes, which puts them in a temporary stack data structure. you can get them back by using git stash apply, which applies the top of the stack back to your files.
see git help stash for more info. in particular, checkout git stash list and git stash show for commands to see what's in the stash stack.
tbh, something like sourcetree can also be nice for quickly glancing through the stash to see what's there.",https://stackoverflow.com/questions/67256474,git,25-04-2021 17:44,47.0,2.0,1.0,True,25-04-2021 18:00,25-04-2021 17:46,Uncategorized
73879459,documenttermmatrix misses some words,"i am using documenttermmatrix to find a list of keywords in a long text. most of the words in my list are correctly found, but there are a couple that are missing. now, i would love to post here a minimal working example, but the problem is: there is one of the words (""insolvency"", so not a short word as in the problem here) in a document of 32 pages which is missed. now, this word is actually in page 7 of the text. but if i reduce my text with text <- text[7], then documenttermmatrix actually finds it! so i am not able to reproduce this with a minimal working example...
do you have any ideas?
below a sketch of my script:
library(fastpipe)
library(openxlsx)
library(tm)

`%>>%` <- fastpipe::`%>>%`

source(""cleantext.r"") # custom function to clean up the text from reports

keywords_xlsx <- read.xlsx(paste0(getwd(),""/keywords.xlsx""),
                           sheet = ""all"",
                           startrow = 1,
                           colnames = false,
                           skipemptyrows = true,
                           skipemptycols = true)

keywords <- keywords_xlsx[1] %>>%
  tolower(as.character(.[,1]))

# custom function to read pdfs
read <- readpdf(control = list(text = ""-layout""))

# extract text from pdf
report <- ""my_report.pdf""
document <- corpus(urisource(paste0(""./annual reports/"", report)), readercontrol = list(reader = read))
text <- content(document[[1]]) 

text <- cleantext(report, text) # this is a custom function to clean up the texts

# text <- text[7] # if i do this, my word is found! otherwise it is missed

# create a corpus  
text_corpus <- corpus(vectorsource(text))


matrix <- t(as.matrix(inspect(documenttermmatrix(text_corpus,
                                                 list(dictionary = keywords,
                                                      list(wordlengths=c(1, inf))
                                                 )
))))
  
  
words <- sort(rowsums(matrix),decreasing=true) 
df <- data.frame(word = names(words),freq=words)","['r', 'nlp', 'tm']",73881206,"the problem lies in your use of inspect. only use inspect to check if your code is working and to see if a dtm has any values. never use inspect inside functions / transformations, because inspect by default only shows the firs 10 rows and 10 columns of a document term matrix.
also if you want to transpose the outcome of a dtm, use termdocumentmatrix.
your last line should be:
mat <- as.matrix(termdocumentmatrix(text_corpus,
                                    list(dictionary = keywords,
                                         list(wordlengths=c(1, inf)))))

note that turning a dtm / tdm into a matrix will use a lot more memory than having the data inside a sparse matrix.",https://stackoverflow.com/questions/73879459,r,28-09-2022 09:52,38.0,1.0,1.0,True,28-09-2022 12:12,28-09-2022 10:13,Implementation Issues
75252308,why smote raise &quot;found input variables with inconsistent numbers of samples&quot;?,"i try to classify emotion from tweet with dataset of 4401 tweet, when i use smaller sample of data (around 15 tweet) everything just work fine, but when i use the full dataset it raise the error of
found input variables with inconsistent numbers of samples: [7, 3520]

the error happen when i try to oversampling the data using smote after transforming the data using countvectorizer.
this is the code where the error raise
# n-gram feature and term frequency
vectorizer = countvectorizer(ngram_range=(1,3))
x_train_tf = vectorizer.fit_transform(str(x_train).split('\n')).toarray()
x_test_tf = vectorizer.transform(str(x_test).split('\n')).toarray()
df_output = pd.dataframe(data =x_train_tf, columns = vectorizer.get_feature_names_out())
display(df_output)
# the print shape is (7 rows ï¿½ï¿½ 250 columns)

smote = smote(random_state=42, k_neighbors=5)
x_smote, y_smote = smote.fit_resample(x_train_tf, y_train)
print(""total train data smote : &;,x_smote.shape), print(""total train label smote : "",y_smote)

i did not understand why this is happening so some explanation could really help.
i already tried to solve it using answers from other similiar question but nothing have worked.
this is the full code
import nltk
import re
#nltk.download()
import string
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
from nltk import everygrams
from collections import counter
from sklearn import preprocessing
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from imblearn.over_sampling import smote
from sklearn.naive_bayes import gaussiannb
from sklearn.preprocessing import labelencoder
from sklearn.model_selection import train_test_split
from sastrawi.stemmer.stemmerfactory import stemmerfactory
from sklearn.feature_extraction.text import countvectorizer
from sklearn.feature_extraction.text import tfidfvectorizer
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix

dataset = pd.read_csv(""g:/ta/program/dataset/twitter_emotion_dataset.csv"", encoding='latin-1')
# preprocessing
dataset['case_folding_tweet'] = dataset['tweet'].str.casefold()
dataset['only_alphabet_tweet'] = [re.sub('[^a-za-z]+\s*', ' ', s) for s in dataset['case_folding_tweet']]
dataset['data_cleaning_tweet'] = dataset['only_alphabet_tweet'].str.replace(r'\b\w{1}\b','').str.replace(r'\s+', ' ')

slangword_dictionary = (""g:/ta/program/dataset/kamus_singkatan.csv"")

deslang = {}
list_slangword = open(slangword_dictionary).readlines()
for line in list_slangword:
    slang, unslang = line.strip().split(';')
    deslang[slang] = unslang
deslang[slang] = {r""\b{}\b"".format(k): v for k, v in deslang.items()}

dataset['data_cleaning_tweet'] = dataset['data_cleaning_tweet'].replace(deslang[slang], regex=true)
dataset['convert_slang_tweet'] = dataset['data_cleaning_tweet']

replace_dictionary = {'tidak ': 'tidak', 'bukan ': 'bukan', 'jangan ': 'jangan', 'belum ': 'belum'}
dataset['convert_negation_tweet'] = dataset['convert_slang_tweet'].replace(replace_dictionary, regex=true)
dataset['tokenization_tweet'] = dataset['convert_negation_tweet'].apply(word_tokenize) 
list_stopwords = set(stopwords.words(""indonesian""))
list_stopwords.add('username')
list_stopwords.add('url')
dataset['stopword_removal_tweet'] = dataset['tokenization_tweet'].apply(lambda x: [item for item in x if item not in list_stopwords])

factory = stemmerfactory()
stemmer = factory.create_stemmer()
dataset['stemmed_tweet'] = dataset['stopword_removal_tweet'].apply(lambda x: [stemmer.stem(y) for y in x]) 

# split data
x = dataset[""stemmed_tweet""].values
y = dataset[""label""].values
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state= 42)

# get n-gram and tf
vectorizer = countvectorizer(ngram_range=(1,3))
x_train_tf = vectorizer.fit_transform(str(x_train).split('\n')).toarray()
x_test_tf = vectorizer.transform(str(x_test).split('\n')).toarray()

# oversampling
smote = smote(random_state=42, k_neighbors=5)
x_smote, y_smote = smote.fit_resample(x_train_tf, y_train)

print(""total train data smote : "",x_smote.shape), print(""total train label smote : "",y_smote)

gnb_classifier = gaussiannb()
gnb_classifier.fit(x_smote, y_smote)
print(gnb_classifier)
y_pred = gnb_classifier.predict(x_test_tf)
print(""emotion predicted :"", y_pred)

link to the dataset","['python', 'machine-learning', 'classification', 'text-classification', 'countvectorizer']",75304406,"i fix the problem using the answer from this post answer
by joining all the train data column before vectorizing.
df_train = pd.dataframe(data=x_train)
df_test = pd.dataframe(data=x_test)

series = pd.series(df_train['stemmed_tweet'])
corpus = series.apply(lambda series: ' '.join(series))
vectorizer = countvectorizer(ngram_range=(1,3), lowercase=false)
x_train_tf = vectorizer.fit_transform(corpus).toarray()
x_test_tf = vectorizer.transform(str(df_test.values).split(""\n"")).toarray()",https://stackoverflow.com/questions/75252308,python,26-01-2023 22:30,143.0,2.0,2.0,True,01-02-2023 00:09,01-02-2023 00:02,Data Wrangling
77651770,how to load the pre-trained word embeddings in .npy files,"i'm trying to use the word embeddings pre-trained in histwords project by the stanford nlp team. but when i ran the document example.py from the github website, there was an error: modulenotfounderror: no module named 'representations.sequentialembedding'.
how can i solve this problem?
i've installed the ""representations"" module, but it doesn't work. the pre-trained word embeddings are of "".npy"" format, is there any python-based method for uploading them?","['python', 'nlp', 'word-embedding']",77654014,"to set it up you can do the following in your shell:
cd <path you want to store your project>
git clone 
cd histwords

# ----- set up python2.7 -----
## python2.7 via conda is quite easy
conda create -y -n <env_name> python=2.7
conda activate <env_name>

## else install/locate a python 2 version
### check python -v (maybe its python2.7)
### maybe you have python2 or python2.7 executables
### on linux you could look for ls /usr/bin/python* 
python2 -m venv <env_name>
source <env_name>/bin/activate # if you use linux
bin/scripts/activate # on windows

# ---- now with python2.7 ----
pip install -r requirements.txt

# ---- download & move your embedding from 
# to embeddings/<category> subfolders

python examples.py
# outputs the similarities",https://stackoverflow.com/questions/77651770,python,13-12-2023 07:34,368.0,0.0,1.0,True,13-12-2023 13:37,13-12-2023 12:38,Implementation Issues
76104878,whisper api from a recorded audio blob,"i am creating a transcriber using openai whisper api in nodejs and react. i want the user to be able to record an audio file in the browser and transcribe their recording. i am doing this by saving the buffer data of the audio blob they have recorded into an mp3 file, then using the createtranscription() api call i input the fs.createreadstream(recorded_audio_file.mp3) which outputs a 400 error. when i record an audio file using the windows recorder and input that file the api call works just fin.  here is my recorder component in react
import react, { usestate, useeffect, useref } from ""react"";

import microphone from ""./microphone/microphone"";
const tsst = () => {
  const base_url = process.env.react_app_server_url || ""

  const mediarecorder = useref(null);
  const [stream, setstream] = usestate(null);
  const [audiochunks, setaudiochunks] = usestate([]);
  const [audio, setaudio] = usestate(null);
  const [audiofile, setaudiofile] = usestate(null);
  const [transcribtion, settranscription] = usestate("""");
  const [audioblob, setaudioblob] = usestate("""");
  const [audiobuffer, setaudiobuffer] = usestate("""");

  useeffect(() => {
    const initializemediarecorder = async () => {
      if (""mediarecorder"" in window) {
        try {
            const streamdata = await navigator.mediadevices.getusermedia({ audio: true });
            setstream(streamdata);
        } catch (err) {
            console.log(err.message);
        }
      } else {
          console.log(""the mediarecorder api is not supported in your browser."");
      }
    }

    initializemediarecorder();
  }, [])

  const handlestartrecording = () => {
    const media = new mediarecorder(stream, { type: ""audio/mp3"" });

    mediarecorder.current = media;
    mediarecorder.current.start();

    let chunks = [];
    mediarecorder.current.ondataavailable = (e) => {
       chunks.push(e.data);
    };
    setaudiochunks(chunks);
  }
  const handlestoprecording = () => {
    mediarecorder.current.stop();
    mediarecorder.current.onstop = () => {
      const audioblob = new blob(audiochunks, { type: ""audio/mp3"" });
      const audiourl = url.createobjecturl(audioblob);

      setaudioblob(audioblob)
      setaudio(audiourl);
      setaudiochunks([]);

      let file = new file([audiourl], ""recorded_audio.mp3"",{type:""audio/mp3"", lastmodified:new date().gettime()});
      let container = new datatransfer();
      container.items.add(file);
      document.getelementbyid(""audiofile"").files = container.files;
      setaudiofile(container.files[0]);

      console.log(file);
    };
  }

  const handlesubmitrecording = async () => {
    try {
      // assuming you have an audio blob called 'audioblob'

      // convert the audio blob to a base64 string
      const reader = new filereader();
      reader.onloadend = async () => {
        const base64string = reader.result.split(',')[1]; // extract base64 data from the result
        const res = await fetch(`${base_url}/api/openai/transcriber`, {
          method: ""post"",
          headers: {
            ""content-type"": ""application/json"",
          },
          body: json.stringify({ audiobuffer: base64string, lang: ""en"" })
        })
        const data = await res.json();
        settranscription(data);
      };
      reader.readasdataurl(audioblob);

    } catch (error) {
      console.log(error);

    } finally {
    }
  }

    return (
      <div classname=""h-[calc(100vh-73px)] flex justify-center items-center"">
        <div classname=""w-[40%] flex justify-between items-center"">
          <div classname=""flex flex-col"">
            <microphone startfunction={ handlestartrecording } stopfunction={ handlestoprecording } />
            <button onclick={handlestartrecording} classname=""w-fit my-10 p-5 bg-gray-200 rounded-lg"">start recording</button>
            <button onclick={handlestoprecording} classname=""w-fit mb-10 p-5 bg-gray-200 rounded-lg"">stop recording</button>

            <audio classname=""mb-10"" src={audio && audio} controls></audio>
            <input id=""audiofile"" type=""file"" onchange={ (e) => {setaudiofile(e.target.files[0])}}/>
          </div>
          
          <div>
            <button classname=""p-10 bg-yellow-500 rounded-xl"" onclick={ handlesubmitrecording } >submit</button>
          </div>
        </div>

        <div classname=""w-[40%] flex justify-center items-center"">
          <textarea value={transcribtion} readonly classname=""w-[60%] aspect-square resize-none shadow-lg shadow-black""></textarea>
        </div>
      </div>
    );
};
export default tsst;

here is the api:
export const transcribe = async (req, res) => {
    // const lang = json.parse(req.body.json).lang;
    // const audiobuffer = req.file;
    const { audiobuffer, lang} = req.body;

    const audiobufferbase64 = buffer.from(audiobuffer, 'base64');

    const filename = ""test.mp3"";
    const foldername = `./audio/${filename}`

    const writablestream = fs.createwritestream(foldername); // replace with your desired file path and extension
    writablestream.write(audiobufferbase64);

    const readstream = fs.createreadstream(foldername);

    readstream.on('data', (data) => {
        console.log('read stream data:', data);
    });

    try {
        const whisperres = await openai.createtranscription(
            readstream,
            ""whisper-1"",
        )

        const chatresponse = whisperres.data.text;
        console.log(chatresponse)

        res.status(200).json({ chatresponse: chatresponse });
    } catch (error) {
        //console.log(error);
        res.status(500).json({ message: error });
    }
}

and here is the server call:
import express from ""express"";
import cors from ""cors"";
import * as dotenv from ""dotenv"";
import mongoose from ""mongoose"";
import multer from ""multer"";

import { dalle, chatgpt, summarize, translate, transcribe } from ""./api/openai.js"";
import { getimages, postimage } from ""./api/imageshowcase.js"";
import { login, signup } from ""./api/user.js"";

dotenv.config();

const app = express();
const upload = multer();
const storage = multer.memorystorage();
const uploadmiddleware = multer({ storage: storage });

app.use(cors());
app.use(express.json({limit: '50mb'}));

const atlasurl = process.env.mongodb_url;    
const port = process.env.port || 5000;

mongoose.connect(atlasurl)
    .then(() => app.listen(port, () => console.log(`successfully connected to port ${port}`)))
    .catch(error => console.log(""there was an error: "", error));

app.get(""/"", async (req, res) => {
    res.send(""server is running"");
})

app.post(""/api/openai/transcriber"",(req, res) => transcribe(req, res));


the saved mp3 file is working just fine.
the apikey is correct.
when i record my own mp3 using windows recorder and use the createreadstream of that it works just fine.
the saved file data is a buffer of the form 
i tried changing the way i save a file, using different formatting methods for the buffer, binary hex, base64. tried uploading the buffer directly to whisper api. tried using axios to post to the api url directly. tried making a promise out of the saving of the mp3 file and then createreadstream and a lot of other little changes. tried to make a readable out of the buffer directly. i view all the similar questions with answers with no avail.","['node.js', 'reactjs', 'openai-api', 'recorder', 'openai-whisper']",76499252,"just call transcribeaudio function in your try, catch of transcribe function.
also, make sure you are able to create the .mp3 file locally and try to play it. sometimes, the audio file is not correct which causes problems while executing the code.
try {
        const whisperres = await transcribeaudio(readstream);

        const chatresponse = whisperres.data.text;
        console.log(chatresponse)

        res.status(200).json({ chatresponse: chatresponse });
    } catch (error) {
        //console.log(error);
        res.status(500).json({ message: error });
    }

import formdata from ""form-data"";
import axios from 'axios'

const transcribeaudio = async (file) => {
  let data = new formdata();

  data.append(""file"", fs.createreadstream(file));
  data.append(""model"", ""whisper-1"");
  data.append(""language"", ""en"");

  let config = {
    method: ""post"",
    maxbodylength: infinity,
    url: ""
    headers: {
      authorization:
        `bearer ${process.env.openai_api_key}`,
      ""content-type"": ""multipart/form-data"",
      ...data.getheaders(),
    },
    data: data,
  };

  try {
    const response = await axios.request(config);
    const data = response.data;

    return { data };
  } catch (error) {
    return {};
  }
};",https://stackoverflow.com/questions/76104878,node.js,25-04-2023 19:46,4300.0,5.0,1.0,True,18-06-2023 06:09,08-05-2023 15:58,Implementation Issues
72700395,&#39;meanembeddingvectorizer&#39; object has no attribute &#39;transform&#39;,"hello i'm working with text classification.
i've a dataset with 2 columns one made of text and the other one is the label.
since i'm a beginner i'm following step by step a tutorial on w2vec trying to understand if it can work for my usecase but i keep getting this error.
this is my code
class meanembeddingvectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        self.dim = len(next(iter(word2vec.values())))
def fit(self, x, y):
        return self
def transform(self, x):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for words in x
        ])

train_df['clean_text_tok']=[nltk.word_tokenize(i) for i in train_df['clean_text']]
model = word2vec(train_df['clean_text_tok'],min_count=1)
w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))
modelw = meanembeddingvectorizer(w2v)
# converting text to numerical data using word2vec
x_train_vectors_w2v = modelw.transform(x_train_tok)
x_val_vectors_w2v = modelw.transform(x_test_tok)

the error i'm getting is :
dimension:  100
---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-127-289141692350> in <module>
      4 modelw = meanembeddingvectorizer(w2v)
      5 # converting text to numerical data using word2vec
----> 6 x_train_vectors_w2v = modelw.transform(x_train_tok)
      7 x_val_vectors_w2v = modelw.transform(x_test_tok)

attributeerror: 'meanembeddingvectorizer' object has no attribute 'transform'","['python', 'nlp', 'word2vec']",72705368,"if your meanembeddingvectorizer is defined in your code exactly as its shows here, the failure-to-indent the .fit() and .transform() functions means they're not part of the class, as you likely intended.
indenting those each an extra 4 spaces ï¿½ï¿½ï¿½ as was likely the intent of any source you copied this code from! ï¿½ï¿½ï¿½ will put them ""inside"" the meanembeddingvectorizer class, as class methods. then, objects of that class won't give the same ""no attribute"" error.
for example:""lang-py prettyprint-override"">class meanembeddingvectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        self.dim = len(next(iter(word2vec.values())))
    def fit(self, x, y):
        return self
    def transform(self, x):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for words in x
        ])",https://stackoverflow.com/questions/72700395,python,21-06-2022 12:00,1133.0,0.0,1.0,True,21-06-2022 18:34,21-06-2022 17:29,Conceptual Questions
61826824,can you train a bert model from scratch with task specific architecture?,"bert pre-training of the base-model is done by a language modeling approach, where we mask certain percent of tokens in a sentence, and we make the model learn those missing mask. then, i think in order to do downstream tasks, we add a newly initialized layer and we fine-tune the model.
however, suppose we have a gigantic dataset for sentence classification. theoretically, can we initialize the bert base architecture from scratch, train both the additional downstream task specific layer + the base model weights form scratch with this sentence classification dataset only, and still achieve a good result?","['machine-learning', 'nlp', 'bert-language-model']",61839719,"bert can be viewed as a language encoder, which is trained on a humongous amount of data to learn the language well. as we know, the original bert model was trained on the entire english wikipedia and book corpus, which sums to 3,300m words. bert-base has 109m model parameters. so, if you think you have large enough data to train bert, then the answer to your question is yes. 
however, when you said ""still achieve a good result"", i assume you are comparing against the original bert model. in that case, the answer lies in the size of the training data.
i am wondering why do you prefer to train bert from scratch instead of fine-tuning it? is it because you are afraid of the domain adaptation issue? if not, pre-trained bert is perhaps a better starting point.
please note, if you want to train bert from scratch, you may consider a smaller architecture. you may find the following papers useful.

well-read students learn better: on the importance of pre-training compact models
albert: a lite bert for self-supervised learning of language representations",https://stackoverflow.com/questions/61826824,machine-learning,15-05-2020 19:21,6813.0,6.0,2.0,True,01-02-2024 15:10,01-02-2024 15:10,Implementation Issues
77107744,use nlp (nltk) to identify groups of phrases in a python dataframe,"i have a table containing diagnosis information for a large group of patients. i would like to determine what the most common groupings of those diagnoses are, for example is it ""bloaty head syndrome"" and ""slack tongue"", or ""broken wind"", ""chronic nosehair"" and ""corrugated ankles""... or some other combination.
data is structured like so:
import pandas as pd
import numpy as np

# list of ids
ids = ['id1', 'id2', 'id3','id4','id5'] 

# list of sample sentences 
diagnosis = [""broken wind"",""chronic nosehair"",""corrugated ankles"",""discrete itching""]

# create dataframe
df = pd.dataframe({'id': ids})

# generate list of sentences for each id
df['diagnosis'] = df['id'].apply(lambda x: np.random.choice(diagnosis, 5).tolist())

# explode into separate rows
df = df.explode('diagnosis')

print(df)

for example if both id2 and id5 contain ""broken wind"" and chronic nosehair"" that would be 2 of that combination. if id1, id3 and id4 contain ""chronic nosehair"",""corrugated ankles"", and ""discrete itching"" that would be 3 of that combination.
with the goal of determining which combination is most common.
i'm wondering is there an nlp library such as nltk, or a method, that can be used to process data stored like this in a pandas dataframe? most of what i have been able to find so far is geared toward sentiment analysis or analyzing single words as opposed to phrases...","['python', 'pandas', 'machine-learning', 'nlp', 'nltk']",77112903,"i would offer that what you are trying to do here is not necessarily an nlp problem, but a much more general frequent pattern mining problem which is typically seen in recommendation.
you can find the most frequent diagnosis combinations of any size by using the fpgrowth algorithm in the mlxtend library and looking at the support for each symptom or combinations thereof:
import pandas as pd
from mlxtend.preprocessing import transactionencoder
from mlxtend.frequent_patterns import fpgrowth

# create list of diagnoses for each patient
x = df.groupby('id').apply(lambda x:list(x['diagnosis']))

# encode to wide dataframe with column for each symptom
te = transactionencoder()
te_ary = te.fit(x).transform(x)
te_df = pd.dataframe(te_ary, columns=te.columns_)

# calculate most frequent diagnosis co-occurrences
fp_df = fpgrowth(te_df, min_support=0.01, use_colnames=true)

# sort and show
fp_df.sort_values(by='support', ascending=false)

the resultant table is a list of tuples with the support, the percentage of ""transactions"" (here, patients) for which the combinations occur:
| support | itemsets                                                 |
| ------- | -------------------------------------------------------- |
| 0.8     | {'broken wind'}                                          |
| 0.6     | {'corrugated ankles'}                                    |
| 0.6     | {'chronic nosehair'}                                     |
| 0.6     | {'discrete itching'}                                     |
| 0.6     | {'corrugated ankles', 'broken wind'}                     |
| 0.4     | {'chronic nosehair', 'broken wind'}                      |
| 0.4     | {'discrete itching', 'chronic nosehair'}                 |
| 0.4     | {'discrete itching', 'broken wind'}                      |
| 0.2     | {'corrugated ankles', 'discrete itching'}                |
| 0.2     | {'discrete itching', 'corrugated ankles', 'broken wind'} |
| 0.2     | {'corrugated ankles', 'chronic nosehair'}                |
| 0.2     | {'chronic nosehair', 'discrete itching', 'broken wind'}  |
| 0.2     | {'chronic nosehair', 'corrugated ankles', 'broken wind'} |",https://stackoverflow.com/questions/77107744,python,14-09-2023 19:04,333.0,1.0,1.0,True,15-09-2023 15:27,15-09-2023 15:27,Implementation Issues
76752935,error while peforming tf-idfvectorizer() on the training values,"#model creation

x = df.drop(columns='v1', axis=1)
y = df['v1']

from sklearn.feature_extraction.text import tfidfvectorizer


x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)


pipe_gnb = pipeline([
    ('vect', tfidfvectorizer()),
    ('gnb', gaussiannb())
])


params_gnb = {
        # 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],
        # 'vect__max_df': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
        # 'vect__min_df': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
        # 'vect__max_features': [3000, 4000, 5000, none],
        # 'vect__binary': [true, false],
        'vect__sublinear_tf': [true, false]
}

gs_gnb = gridsearchcv(pipe_gnb, params_gnb, verbose=10, cv=5, n_jobs=-1, scoring='accuracy')
gs_gnb.fit(x_train, y_train)

i have been trying to do tfidfvectorizeration on 'v2' column, that is the messages column in the attached dataset but i have been getting the below error(mainly an error to convert the fiited ""x, y(train values)"" to a 'dense array').
code link->
dataset link->

error log->
fitting 5 folds for each of 2 candidates, totalling 10 fits
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
<ipython-input-23-f27f59ee7f63> in <cell line: 45>()
     43 
     44 gs_gnb = gridsearchcv(pipe_gnb, params_gnb, verbose=10, cv=5, n_jobs=-1, scoring='accuracy')
---> 45 gs_gnb.fit(x, y)
     46 
     47 print('best accuracy: ', gs_gnb.best_score_, end='\n')

3 frames
/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py in _warn_or_raise_about_fit_failures(results, error_score)
    365                 f""below are more details about the failures:\n{fit_errors_summary}""
    366             )
--> 367             raise valueerror(all_fits_failed_message)
    368 
    369         else:

valueerror: 
all the 10 fits failed.
it is very likely that your model is misconfigured.
you can try to debug the error by setting error_score='raise'.

below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
traceback (most recent call last):
  file ""/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py"", line 686, in _fit_and_score
    estimator.fit(x_train, y_train, **fit_params)
  file ""/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py"", line 405, in fit
    self._final_estimator.fit(xt, y, **fit_params_last_step)
  file ""/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py"", line 267, in fit
    return self._partial_fit(
  file ""/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py"", line 428, in _partial_fit
    x, y = self._validate_data(x, y, reset=first_call)
  file ""/usr/local/lib/python3.10/dist-packages/sklearn/base.py"", line 584, in _validate_data
    x, y = check_x_y(x, y, **check_params)
  file ""/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py"", line 1106, in check_x_y
    x = check_array(
  file ""/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py"", line 845, in check_array
    array = _ensure_sparse_format(
  file ""/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py"", line 522, in _ensure_sparse_format
    raise typeerror(
typeerror: a sparse matrix was passed, but dense data is required. use x.toarray() to convert to a dense numpy array.","['python', 'machine-learning', 'scikit-learn', 'nlp']",76755926,"you need to add a specific step after the ""tfidfvectorizer"" because the output is a sparse matrix. you can create a densetransformer from transformermixin and add it in the pipeline :
import numpy as np
from sklearn.base import transformermixin

class densetransformer(transformermixin):
    def fit(self, x, y=none, **fit_params):
        return self

    def transform(self, x, y=none, **fit_params):
        return np.array(x.todense())

you need to make two modifications in your code. first, you need to select only the ""v2"" as feature :
x = df['v2']
y = df['v1']

and you need to modify the pipeline :
pipe_gnb = pipeline([
    ('vect', tfidfvectorizer()),
    ('to_dense', densetransformer()), 
    ('gnb', gaussiannb()),
])",https://stackoverflow.com/questions/76752935,python,24-07-2023 09:07,51.0,0.0,1.0,True,24-07-2023 15:24,24-07-2023 10:41,Implementation Issues
70251993,invalid_argument: assertion failed: [predictions must be &lt;= 1] [condition x &lt;= y did not hold element-wise:],"i have the following model, and i want to make use of the standard metric functions to report on true/false positives, and true/false negatives.
from transformers import tfrobertaforsequenceclassification

model = tfrobertaforsequenceclassification.from_pretrained('roberta-base', num_labels=1)

optimizer = tf.keras.optimizers.adam(learning_rate=5e-5)
model.compile(
    optimizer=optimizer, 
    loss=tf.keras.losses.binarycrossentropy(from_logits=false),
    metrics = [
      'accuracy',
      tf.keras.metrics.truepositives(),
      tf.keras.metrics.truenegatives(),
      tf.keras.metrics.falsenegatives(),
      tf.keras.metrics.falsepositives()
    ]) # can also use any keras loss fn
history = model.fit(train_dataset.shuffle(1000).batch(16), epochs=10, batch_size=16, validation_data = test_dataset.batch(1))

but i am getting the following error, and not sure how to troubleshoot. how can it be that some predictions are greater than 1?
invalid_argument:  assertion failed: [predictions must be <= 1] [condition x <= y did not hold element-wise:] [x (tf_roberta_for_sequence_classification_5/classifier/out_proj/biasadd:0) = ] [[0.375979185][0.340960771][0.41201663]...] [y (cast_9/x:0) = ] [1]
     [[node assert_less_equal/assert/assertguard/assert
 (defined at /usr/local/lib/python3.7/dist-packages/keras/utils/metrics_utils.py:615)","['python', 'tensorflow', 'keras', 'deep-learning', 'nlp']",70257476,"this a known issue with these metrics due to their predefined thresholds and the fact that y_pred is not being squished between 0 and 1. check out this issue for more information. here is a simple working example based on the workaround posted in the linked issue.
from transformers import robertatokenizer, tfrobertaforsequenceclassification
import tensorflow as tf
import pandas as pd

class truepositives(tf.keras.metrics.truepositives):
    def __init__(self, from_logits=false, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._from_logits = from_logits

    def update_state(self, y_true, y_pred, sample_weight=none):
        if self._from_logits:
            super(truepositives, self).update_state(y_true, tf.nn.sigmoid(y_pred), sample_weight)
        else:
            super(truepositives, self).update_state(y_true, y_pred, sample_weight)


class falsepositives(tf.keras.metrics.falsepositives):
    def __init__(self, from_logits=false, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._from_logits = from_logits

    def update_state(self, y_true, y_pred, sample_weight=none):
        if self._from_logits:
            super(falsepositives, self).update_state(y_true, tf.nn.sigmoid(y_pred), sample_weight)
        else:
            super(falsepositives, self).update_state(y_true, y_pred, sample_weight)


class truenegatives(tf.keras.metrics.truenegatives):
    def __init__(self, from_logits=false, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._from_logits = from_logits

    def update_state(self, y_true, y_pred, sample_weight=none):
        if self._from_logits:
            super(truenegatives, self).update_state(y_true, tf.nn.sigmoid(y_pred), sample_weight)
        else:
            super(truenegatives, self).update_state(y_true, y_pred, sample_weight)


class falsenegatives(tf.keras.metrics.falsenegatives):
    def __init__(self, from_logits=false, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._from_logits = from_logits

    def update_state(self, y_true, y_pred, sample_weight=none):
        if self._from_logits:
            super(falsenegatives, self).update_state(y_true, tf.nn.sigmoid(y_pred), sample_weight)
        else:
            super(falsenegatives, self).update_state(y_true, y_pred, sample_weight)


d = {'text': ['you are fishy', 'fishy people are fishy'], 'label': [1, 0]}
train = pd.dataframe(data=d)
train_text = list(train['text'].values)
train_label = list(train['label'].values)

val = pd.dataframe(data=d)
val_text = list(val['text'].values)
val_label = list(val['label'].values)

tokenizer = robertatokenizer.from_pretrained('roberta-base')
model = tfrobertaforsequenceclassification.from_pretrained('roberta-base')

train_encodings = tokenizer(train_text, truncation=true, padding=true)
val_encodings = tokenizer(val_text, truncation=true, padding=true)

train_dataset = tf.data.dataset.from_tensor_slices((
    dict(train_encodings),
    train_label
))
val_dataset = tf.data.dataset.from_tensor_slices((
    dict(val_encodings),
    val_label
))
model = tfrobertaforsequenceclassification.from_pretrained('roberta-base', num_labels=1)

optimizer = tf.keras.optimizers.adam(learning_rate=5e-5)
model.compile(
    optimizer=optimizer, 
    loss=tf.keras.losses.binarycrossentropy(from_logits=false),
    metrics = [
      'accuracy',
      truepositives(from_logits=true),
      truenegatives(from_logits=true),
      falsenegatives(from_logits=true),
      falsepositives(from_logits=true)
    ]) # can also use any keras loss fn
history = model.fit(train_dataset.shuffle(2).batch(1), epochs=2, validation_data = val_dataset.batch(1))

epoch 1/2
2/2 [==============================] - 81s 6s/step - loss: 7.7125 - accuracy: 0.5000 - true_positives_16: 0.0000e+00 - true_negatives_15: 1.0000 - false_negatives_15: 1.0000 - false_positives_15: 0.0000e+00 - val_loss: 7.7125 - val_accuracy: 0.5000 - val_true_positives_16: 0.0000e+00 - val_true_negatives_15: 1.0000 - val_false_negatives_15: 1.0000 - val_false_positives_15: 0.0000e+00
epoch 2/2
2/2 [==============================] - 3s 1s/step - loss: 7.7125 - accuracy: 0.5000 - true_positives_16: 0.0000e+00 - true_negatives_15: 1.0000 - false_negatives_15: 1.0000 - false_positives_15: 0.0000e+00 - val_loss: 7.7125 - val_accuracy: 0.5000 - val_true_positives_16: 0.0000e+00 - val_true_negatives_15: 1.0000 - val_false_negatives_15: 1.0000 - val_false_positives_15: 0.0000e+00",https://stackoverflow.com/questions/70251993,python,06-12-2021 21:10,1242.0,1.0,1.0,True,08-12-2021 08:29,08-12-2021 08:29,Conceptual Questions
78122293,can my code be wrapped and served in databricks model serving?,"import mlflow.pyfunc
import mlflow
from langchain.agents import create_sql_agent
from langchain.agents.agent_toolkits import sqldatabasetoolkit
from langchain.sql_database import sqldatabase
from langchain import openai

llm = openai(temperature=0)

class ucbot():

    def __init__(self, llm):
        self.llm = llm
        self.toolkit = sqldatabasetoolkit(db=sqldatabase.from_databricks(catalog=""samples"", schema=""nyctaxi""), llm=llm)
        self.agent = create_sql_agent(llm=self.llm, toolkit=self.toolkit, verbose=true, top_k=1)

    def get_answer(self, question):
        return self.agent.run(question)
    
class mlflowucbot(mlflow.pyfunc.pythonmodel):
    def __init__(self, llm):
        self.llm = llm

    def predict(self, context, input):
        ucbot = ucbot(self.llm)
        return ucbot.get_answer(input)


# persist model to mlflow
with mlflow.start_run():
    mlflow.pyfunc.log_model(
        python_model=mlflowucbot(llm),
        extra_pip_requirements=['langchain', 'databricks-sql-connector', 'sqlalchemy', 'openai'],
        artifact_path='model',
        registered_model_name=""my_model"",
        input_example={""input"":""how many tables?""}
    )


the code is able to create a model and predict
when i try to create a model serve i get this error:
an error occurred while loading the model. no module named 'openai'
after adding openai in dependency i get the following error
an error occurred while loading the model. no module named 'openai.api_resources'","['python-3.x', 'databricks', 'openai-api', 'mlflow', 'databricks-unity-catalog']",78147915,"installed openai==0.27.8 and it is working now
also added this code
conda_env = mlflow.pyfunc.get_default_conda_env()

# define packages required by model
packages = ['langchain', 'langchain_community', 'databricks-sql-connector', 'sqlalchemy', 'openai==0.27.8']

# add required packages to environment configuration
conda_env['dependencies'][-1]['pip'] += packages

# persist model to mlflow
with mlflow.start_run():
    mlflow.pyfunc.log_model(
        python_model=mlflowucbot(llm),
        artifact_path='model',
        conda_env=conda_env,
        registered_model_name=""my_model"",
        signature=signature
    )",https://stackoverflow.com/questions/78122293,python-3.x,07-03-2024 14:54,170.0,0.0,1.0,True,12-03-2024 14:27,07-03-2024 15:21,Conceptual Questions
76749071,openai chat completions api error in wordpress php code: &quot;invalid url (post /v4/engines/davinci-codex/completions)&quot;,"i've asked chatgpt to generate a code to be incorporated in wordpress function.php to generate tags automatically for all my newly created or updated posts.
here is the code:
function add_tags_with_gpt($post_id) {
    // vï¿½ï¿½rifier si le contenu de l'article a rï¿½ï¿½ellement changï¿½ï¿½
    $post = get_post($post_id);
    $old_content = get_post_meta($post_id, '_old_content', true);
    $new_content = $post->post_content;
    if ($old_content === $new_content) {
        return; // le contenu n'a pas changï¿½ï¿½, donc on ne fait rien
    }
    update_post_meta($post_id, '_old_content', $new_content);

    // utiliser l'api openai pour gï¿½ï¿½nï¿½ï¿½rer des tags
    $ch = curl_init();
    curl_setopt($ch, curlopt_url, '
    curl_setopt($ch, curlopt_returntransfer, 1);
    curl_setopt($ch, curlopt_post, 1);
    curl_setopt($ch, curlopt_postfields, json_encode(array(
        'prompt' => 'generate tags forllowing content: '.$new_content,
        'max_tokens' => 60
    )));
    curl_setopt($ch, curlopt_ array(
        'authorization: bearer your_openai_api_key', // remplacez 'your_openai_api_key' par votre vï¿½ï¿½ritable clï¿½ï¿½ api
        'content-type: application/json'
    ));
    $response = curl_exec($ch);
    if (!$response) {
        // enregistrer l'erreur dans le fichier de log de dï¿½ï¿½bogage de wordpress
        error_log('erreur lors de la gï¿½ï¿½nï¿½ï¿½ration de tags: ' . curl_error($ch));
        return;
    }
    curl_close($ch);

    // enregistrer la rï¿½ï¿½ponse de l'api dans le fichier de log de dï¿½ï¿½bogage de wordpress
    error_log('rï¿½ï¿½ponse de l\'api openai : ' . $response);

    $response_data = json_decode($response, true);
    if(!isset($response_data['choices'][0]['text'])) {
        error_log('erreur: la rï¿½ï¿½ponse de l\'api ne contient pas de tags');
        return;
    }
    $tags = explode(',', $response_data['choices'][0]['text']);
    $tags = array_slice(toyer les tags
    $tags = array_map('sanitize_text_field', $tags);
    $tags = array_map('wp_strip_all_tags', $tags);
    $tags = array_filter($tags, function($tag) {
        return strlen($tag) > 2 && strlen($tag) <= 20; // exclure les tags de moins de 3 caractï¿½ï¿½res et de plus de 20 caractï¿½ï¿½res
    });

    // ajouter les tags ï¿½ï¿½ l'article
    wp_set_post_tags($post_id, $tags, true);
}
add_action('save_post', 'add_tags_with_gpt');

the problem is that the code doesn't work and returns this error message in debug.log:
[23-jul-2023 15:16:27 utc] rï¿½ï¿½ponse de l'api openai : {
  ""error"": {
    ""message"": ""invalid url (post /v4/engines/davinci-codex/completions)"",
    ""type"": ""invalid_request_error"",
    ""param"": null,
    ""code"": null
  }
}

[23-jul-2023 15:16:27 utc] erreur: la rï¿½ï¿½ponse de l'api ne contient pas de tags
[23-jul-2023 15:16:34 utc] php deprecated:  strnull to parameter #3 ($subject) of type array|string is deprecated in /home/***/webapps/***/wp-includes/formatting.php on line 4303

any idea what the issue is and how to fix it? the problem doesn't come from the api key that i pass from my openai account into the code.
many thanks!","['php', 'openai-api', 'chatgpt-api']",76757100,"all engines api endpoints are deprecated.

use the chat completions api endpoint.
change the url from this...


...to this.


there are 4 required parameters for the chat completions api:

model
messages

role
content



see my past answer for a working example in php using the gpt-3.5-turbo model.",https://stackoverflow.com/questions/76749071,php,23-07-2023 15:36,319.0,0.0,1.0,True,12-06-2024 16:52,12-06-2024 16:52,Implementation Issues
73112216,apply name-entity recognition on specific dataframe columns,"i have the following dataframe:
df = pd.dataframe({'source': ['paul', 'paul'],
                   'target': ['google', 'ferrari'],
                   'edge': ['works at', 'drive']
                   })

df
    source  target  edge
0   paul    google  works at
1   paul    ferrari drive

i want to apply name-entity recognition(ner) on the columns.
expected outcome:
    source  target        edge
0   person  organization  works at
1   person  car           drive

i tried the following function:
!python -m spacy download en_core_web_sm

import spacy
nlp = spacy.load('en_core_web_sm')

def ner(df):
    df['source_entities'] = df['source'].apply(lambda x: nlp(x).label_)
    df['target_entities'] = df['target'].apply(lambda x: nlp(x).label_)
    return df

but when i call the function ner(df) i get back an error:
attributeerror: 'spacy.tokens.doc.doc' object has no attribute 'label_'

any ideas on how to reach the expected outcome?","['python', 'python-3.x', 'dataframe', 'nlp', 'named-entity-recognition']",73142098,"you are trying to get label_ attribute from list as nlp(x) return list of object. because of which you are getting that error.
replace
def ner(df):
  df['source_entities'] = df['source'].apply(lambda x: nlp(x).label_)
  df['target_entities'] = df['target'].apply(lambda x: nlp(x).label_)
  return df

with
def ner(df):
  df['source_entities'] = df['source'].apply(lambda x: [ent.label_ for ent in nlp(x).ents])
  df['target_entities'] = df['target'].apply(lambda x: [ent.label_ for ent in nlp(x).ents])
  return df",https://stackoverflow.com/questions/73112216,python,25-07-2022 16:03,1299.0,2.0,2.0,True,27-07-2022 17:14,25-07-2022 16:23,Implementation Issues
73848586,how to lemmatize a single word in swift,"how do you get the stem form of a single word token? here is my code. it works for some words, but not others.
let text = ""people"" // works
// let text = ""geese"" // doesn't work
let tagger = nltagger(tagschemes: [.lemma])
tagger.string = text
let (tag, range) = tagger.tag(at: text.startindex, unit: .word, scheme: .lemma)
let stemform = tag?.rawvalue ?? string(text[range])

however, if i lemmatize the entire text it's able to find all the stem forms of words.
let text = ""this is text with plurals such as geese, people, and millennia.""
let tagger = nltagger(tagschemes: [.lemma])
tagger.string = text

var words: [string] = []
tagger.enumeratetags(in: text.startindex..<text.endindex, unit: .word, scheme: .lemma, options: [.omitwhitespace, .omitpunctuation]) { tag, range in
    let stemform = tag?.rawvalue ?? string(text[range])
    words += [stemform]
    return true
}

// this be text with plural such as goose person and millennium
words.joined(separator: "" "")

also, is it possible to reverse the process and find the plural version of a stem word?","['swift', 'string', 'nlp']",73848909,"if you set the language of the text before tagging it, it works:
tagger.string = text
tagger.setlanguage(.english, range: text.startindex..<text.endindex)
let (tag, range) = tagger.tag(at: text.startindex, unit: .word, scheme: .lemma)

without setting a language, the tagger guesses the language. apparently, just ""geese"" alone is too little information for it to guess that it is english. if you check dominantlanguage without setting the language explicitly, it is apparently dutch.",https://stackoverflow.com/questions/73848586,swift,26-09-2022 00:02,185.0,2.0,1.0,True,26-09-2022 01:36,26-09-2022 00:27,Implementation Issues
68083466,how to use spacy train to add entities to an existing custom ner model? (spacy v3.0),"i am currently implementing a custom ner model interface where a user can interact with a frontend application to add custom entities to train a spacy model.
i want to use spacy train (cli) to take an existing model (custom ner model) and add the keyword and entity specified by the user, to that model. (instead of training the whole model again). i can't find this anywhere in the documentation.
for example, let's say i have a model that is already trained for a custom entity of food. (pizza, pasta, bread, etcï¿½ï¿½ï¿½). now i want to take this existing model, and train it for a new entity called drinks with keywords like coca-cola, pepsi, juice, etcï¿½ï¿½ï¿½ using spacy train command for spacy v3.0.
the spacy train command that i am using currently is as follows:
> python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy

i load the model for prediction using:
> nlp1 = spacy.load(r""el-best"")

as of now, i was training the model for new entities manually. below is the code to find keywords in my training data and output a json format for training data (old format).
import re

keyword = [""outages"",""updates"",""negative star"",""worst""]
entity = [""problem"",""problem"",""complaint"",""complaint""]

train = []

for text in df.text:

    for n in range(0,len(keyword)):
    
        start_index = []
        end_index = []

        start_index = [m.start() for m in re.finditer(keyword[n], str(text))]

        if(start_index):

            end_index = [m+len(keyword[n]) for m in start_index]

            for i in range(0,len(start_index)):

                train.append((text,{""entities"": [(start_index[i],end_index[i],entity[n])]}))

train

after this, i converted my json format into .spacy format with below code.
from tqdm import tqdm
from spacy.tokens import docbin

db = docbin() # create a docbin object

for text, annot in tqdm(train): # data in previous format
    doc = nlp.make_doc(text) # create doc object from text
    ents = []
    for start, end, label in annot[""entities""]: # add character indexes
        span = doc.char_span(start, end, label=label, alignment_mode=""contract"")
        if span is none:
            print(""skipping entity"")
        else:
            ents.append(span)
    doc.ents = ents # label the text with the ents
    db.add(doc)

db.to_disk(""./train.spacy"")","['python', 'machine-learning', 'nlp', 'spacy', 'named-entity-recognition']",68093963,"i want to use spacy train (cli) to take an existing model (custom ner model) and add the keyword and entity specified by the user, to that model. (instead of training the whole model again). i can't find this anywhere in the documentation.

what you are describing is called ""online learning"" and the default spacy models don't support it. most modern neural ner methods, even outside of spacy, have no support for it at all.
you cannot fix this by using a custom training loop.
your options are to use rule-based matching, so you can only match things explicitly in a list, or to retrain models on the fly.
rule-based matching should be easy to set up but has the obvious issue that it can't learn things not explicitly in the list.
training things on the fly may sound like it'll take too long, but you can train a small model quite quickly. what you can do is train a small model for a small number of iterations while the user is working interactively, and after they've confirmed the model is more or less working correctly you can use the same training data for a larger model with longer training.",https://stackoverflow.com/questions/68083466,python,22-06-2021 12:21,1702.0,3.0,1.0,True,20-07-2021 19:11,20-07-2021 19:10,Implementation Issues
78262843,openai api error: &quot;configuration is not a constructor&quot;,"i'm trying to build a discord bot that generates images, but i faced a problem. i'm getting the following error:

configuration is not a constructor

i'm following a youtube tutorial. i have the following code:
const { slashcommandbuilder, embedbuilder} = require(`discord.js`);
const { configuration, openaiapi } = require(""openai"");

const configuration = new configuration({
  apikey: 'my key'
});
const openai = new openaiapi(configuration);

can anyone help me solve the problem?","['javascript', 'discord', 'discord.js', 'bots', 'openai-api']",78262873,"problem
the code you have works with the openai node.js sdk <v4, but you're using >=v4.
solution
the following is the correct initialization if you're using the openai node.js sdk >=v4:
import openai from ""openai"";

const client = new openai(
  {apikey: ""sk-xxxxxxxxxxxxxxxxxxxxxxxx""}
);

then you can use, for example, the images api as follows:
 import openai from ""openai"";

 const client = new openai(
  { apikey: ""sk-xxxxxxxxxxxxxxxxxx""}
 );

async function main() {
  const image = await client.images.generate({ model: ""dall-e-3"", prompt: ""a cute baby sea otter"" });

  console.log(image.data);
}

main();",https://stackoverflow.com/questions/78262843,javascript,02-04-2024 16:41,1629.0,0.0,1.0,True,24-12-2024 11:42,15-04-2024 09:57,Implementation Issues
17447045,java library for keywords extraction from input text,"i'm looking for a java library to extract keywords from a block of text.
the process should be as follows:
stop word cleaning -> stemming -> searching for keywords based on english linguistics statistical information - meaning if a word appears more times in the text than in the english language in terms of probability than it's a keyword candidate.
is there a library that performs this task?","['java', 'nlp', 'extract', 'keyword', 'stemming']",17453157,"here is a possible solution using apache lucene. i didn't use the last version but the 3.6.2 one, since this is the one i know the best. besides the /lucene-core-x.x.x.jar, don't forget to add the /contrib/analyzers/common/lucene-analyzers-x.x.x.jar from the downloaded archive to your project: it contains the language-specific analyzers (especially the english one in your case).
note that this will only find the frequencies of the input text words based on their respective stem. comparing these frequencies with the english language statistics shall be done afterwards (this answer may help by the way).

the data model
one keyword for one stem. different words may have the same stem, hence the terms set. the keyword frequency is incremented every time a new term is found (even if it has been already found - a set automatically removes duplicates).
public class keyword implements comparable<keyword> {

  private final string stem;
  private final set<string> terms = new hashset<string>();
  private int frequency = 0;

  public keyword(string stem) {
    this.stem = stem;
  }

  public void add(string term) {
    terms.add(term);
    frequency++;
  }

  @override
  public int compareto(keyword o) {
    // descending order
    return integer.valueof(o.frequency).compareto(frequency);
  }

  @override
  public boolean equals(object obj) {
    if (this == obj) {
      return true;
    } else if (!(obj instanceof keyword)) {
      return false;
    } else {
      return stem.equals(((keyword) obj).stem);
    }
  }

  @override
  public int hashcode() {
    return arrays.hashcode(new object[] { stem });
  }

  public string getstem() {
    return stem;
  }

  public set<string> getterms() {
    return terms;
  }

  public int getfrequency() {
    return frequency;
  }

}


utilities
to stem a word:
public static string stem(string term) throws ioexception {

  tokenstream tokenstream = null;
  try {

    // tokenize
    tokenstream = new classictokenizer(version.lucene_36, new stringreader(term));
    // stem
    tokenstream = new porterstemfilter(tokenstream);

    // add each token in a set, so that duplicates are removed
    set<string> stems = new hashset<string>();
    chartermattribute token = tokenstream.getattribute(chartermattribute.class);
    tokenstream.reset();
    while (tokenstream.incrementtoken()) {
      stems.add(token.tostring());
    }

    // if no stem or 2+ stems have been found, return null
    if (stems.size() != 1) {
      return null;
    }
    string stem = stems.iterator().next();
    // if the stem has non-alphanumerical chars, return null
    if (!stem.matches(""[a-za-z0-9-]+"")) {
      return null;
    }

    return stem;

  } finally {
    if (tokenstream != null) {
      tokenstream.close();
    }
  }

}

to search into a collection (will be used by the list of potential keywords):
public static <t> t find(collection<t> collection, t example) {
  for (t element : collection) {
    if (element.equals(example)) {
      return element;
    }
  }
  collection.add(example);
  return example;
}


core
here is the main input method:
public static list<keyword> guessfromstring(string input) throws ioexception {

  tokenstream tokenstream = null;
  try {

    // hack to keep dashed words (e.g. ""non-specific"" rather than ""non"" and ""specific"")
    input = input.replaceall(""-+"", ""-0"");
    // replace any punctuation char but apostrophes and dashes by a space
    input = input.replaceall(""[\\p{punct}&&[^'-]]+"", "" "");
    // replace most common english contractions
    input = input.replaceall(""(?:'(?:[tdsm]|[vr]e|ll))+\\b"", """");

    // tokenize input
    tokenstream = new classictokenizer(version.lucene_36, new stringreader(input));
    // to lowercase
    tokenstream = new lowercasefilter(version.lucene_36, tokenstream);
    // remove dots from acronyms (and ""'s"" but already done manually above)
    tokenstream = new classicfilter(tokenstream);
    // convert any char to ascii
    tokenstream = new asciifoldingfilter(tokenstream);
    // remove english stop words
    tokenstream = new stopfilter(version.lucene_36, tokenstream, englishanalyzer.getdefaultstopset());

    list<keyword> keywords = new linkedlist<keyword>();
    chartermattribute token = tokenstream.getattribute(chartermattribute.class);
    tokenstream.reset();
    while (tokenstream.incrementtoken()) {
      string term = token.tostring();
      // stem each term
      string stem = stem(term);
      if (stem != null) {
        // create the keyword or get the existing one if any
        keyword keyword = find(keywords, new keyword(stem.replaceall(""-0"", ""-"")));
        // add its corresponding initial token
        keyword.add(term.replaceall(""-0"", ""-""));
      }
    }

    // reverse sort by frequency
    collections.sort(keywords);

    return keywords;

  } finally {
    if (tokenstream != null) {
      tokenstream.close();
    }
  }

}


example
using the guessfromstring method on the java wikipedia article introduction part, here are the first 10 most frequent keywords (i.e. stems) that were found:
java         x12    [java]
compil       x5     [compiled, compiler, compilers]
sun          x5     [sun]
develop      x4     [developed, developers]
languag      x3     [languages, language]
implement    x3     [implementation, implementations]
applic       x3     [application, applications]
run          x3     [run]
origin       x3     [originally, original]
gnu          x3     [gnu]

iterate over the output list to know which were the original found words for each stem by getting the terms sets (displayed between brackets [...] in the above example).

what's next
compare the stem frequency / frequencies sum ratios with the english language statistics ones, and keep me in the loop if your managed it: i could be quite interested too :)",https://stackoverflow.com/questions/17447045,java,03-07-2013 11:43,25872.0,34.0,3.0,True,26-01-2022 18:11,03-07-2013 16:21,Implementation Issues
48989629,input parameter for model as string in text classification,"i am building document classification system using scikit-learn and it works fine. i am converting the model to core ml model format. but the model format excepts the input parameter as multiarraytype. i want make it to excepts string or array of string so that i can easily predict from ios application.i have tried following way:
from sklearn.linear_model import logisticregression

logreg = logisticregression()
logreg.fit(x_train_dtm, y_train)

#testing a value
docs_new = ['get exclusive prize offer']
docs_pred_class = nb.predict(count_vect.transform(docs_new))

#exporting to coremodel
import coremltools

coreml_model = coremltools.converters.sklearn.convert(logreg)
#print model
coreml_model

printing the coreml model gives following output:
 input {
     name: ""input""
     type {
     multiarraytype {
      shape: 7505
      datatype: double
    }
  }
}
output {
  name: ""classlabel""
  type {
    int64type {
    }
  }
}
output {
  name: ""classprobability""
  type {
    dictionarytype {
      int64keytype {
      }
    }
  }
  }
  predictedfeaturename: ""classlabel""
predictedprobabilitiesname: ""classprobability"" 

i checked the core ml model in github library, i can see there is different input and output. 

how can i achieve this, so that i can pass a simple parameter from ios app to make prediction.","['python', 'scikit-learn', 'text-classification', 'coreml', 'coremltools']",48991924,"it sounds like that other mlmodel you found uses a dictvectorizer to turn the strings into indexes (possibly followed by a onehotencoder). 
you can do this by making a pipeline in sklearn and converting that pipeline to core ml.",https://stackoverflow.com/questions/48989629,python,26-02-2018 13:24,858.0,1.0,1.0,True,05-10-2023 10:15,26-02-2018 13:48,Task-specific Help
45403390,lemmatizing italian sentences for frequency counting,"i would like to lemmatize some italian text in order to perform some frequency counting of words and further investigations on the output of this lemmatized content.
i am preferring lemmatizing than stemming because i could extract the word meaning from the context in the sentence (e.g. distinguish between a verb and a noun) and obtain words that exist in the language, rather than roots of those words that don't usually have a meaning.
i found out this library called pattern (pip2 install pattern) that should complement nltk in order to perform lemmatization of the italian language, however i am not sure the approach below is correct because each word is lemmatized by itself, not in the context of a sentence.
probably i should give pattern the responsibility to tokenize a sentence (so also annotating each word with the metadata regarding verbs/nouns/adjectives etc), then retrieving the lemmatized word, but i am not able to do this and i am not even sure it is possible at the moment?
also: in italian some articles are rendered with an apostrophe so for example ""l'appartamento"" (in english ""the flat"") is actually 2 words: ""lo"" and ""appartamento"". right now i am not able to find a way to split these 2 words with a combination of nltk and pattern so then i am not able to count the frequency of the words in the correct way.
import nltk
import string
import pattern

# dictionary of italian stop-words
it_stop_words = nltk.corpus.stopwords.words('italian')
# snowball stemmer with rules for the italian language
ita_stemmer = nltk.stem.snowball.italianstemmer()

# the following function is just to get the lemma
# out of the original input word (but right now
# it may be loosing the context about the sentence
# from where the word is coming from i.e.
# the same word could either be a noun/verb/adjective
# according to the context)
def lemmatize_word(input_word):
    in_word = input_word#.decode('utf-8')
    # print('something: {}'.format(in_word))
    word_it = pattern.it.parse(
        in_word, 
        tokenize=false,  
        tag=false,  
        chunk=false,  
        lemmata=true 
    )
    # print(""input: {} output: {}"".format(in_word, word_it))
    the_lemmatized_word = word_it.split()[0][0][4]
    # print(""returning: {}"".format(the_lemmatized_word))
    return the_lemmatized_word

it_string = ""ieri sono andato in due supermercati. oggi volevo andare all'ippodromo. stasera mangio la pizza con le verdure.""

# 1st tokenize the sentence(s)
word_tokenized_list = nltk.tokenize.word_tokenize(it_string)
print(""1) nltk tokenizer, num words: {} for list: {}"".format(len(word_tokenized_list), word_tokenized_list))

# 2nd remove punctuation and everything lower case
word_tokenized_no_punct = [string.lower(x) for x in word_tokenized_list if x not in string.punctuation]
print(""2) clean punctuation, num words: {} for list: {}"".format(len(word_tokenized_no_punct), word_tokenized_no_punct))

# 3rd remove stop words (for the italian language)
word_tokenized_no_punct_no_sw = [x for x in word_tokenized_no_punct if x not in it_stop_words]
print(""3) clean stop-words, num words: {} for list: {}"".format(len(word_tokenized_no_punct_no_sw), word_tokenized_no_punct_no_sw))

# 4.1 lemmatize the words
word_tokenize_list_no_punct_lc_no_stowords_lemmatized = [lemmatize_word(x) for x in word_tokenized_no_punct_no_sw]
print(""4.1) lemmatizer, num words: {} for list: {}"".format(len(word_tokenize_list_no_punct_lc_no_stowords_lemmatized), word_tokenize_list_no_punct_lc_no_stowords_lemmatized))

# 4.2 snowball stemmer for italian
word_tokenize_list_no_punct_lc_no_stowords_stem = [ita_stemmer.stem(i) for i in word_tokenized_no_punct_no_sw]
print(""4.2) stemmer, num words: {} for list: {}"".format(len(word_tokenize_list_no_punct_lc_no_stowords_stem), word_tokenize_list_no_punct_lc_no_stowords_stem))

# difference between stemmer and lemmatizer
print(
    ""for original word(s) '{}' and '{}' the stemmer: '{}' '{}' (count 1 each), the lemmatizer: '{}' '{}' (count 2)""
    .format(
        word_tokenized_no_punct_no_sw[1],
        word_tokenized_no_punct_no_sw[6],
        word_tokenize_list_no_punct_lc_no_stowords_stem[1],
        word_tokenize_list_no_punct_lc_no_stowords_stem[6],
        word_tokenize_list_no_punct_lc_no_stowords_lemmatized[1],
        word_tokenize_list_no_punct_lc_no_stowords_lemmatized[1]
    )
)

gives this output:
1) nltk tokenizer, num words: 20 for list: ['ieri', 'sono', 'andato', 'in', 'due', 'supermercati', '.', 'oggi', 'volevo', 'andare', ""all'ippodromo"", '.', 'stasera', 'mangio', 'la', 'pizza', 'con', 'le', 'verdure', '.']
2) clean punctuation, num words: 17 for list: ['ieri', 'sono', 'andato', 'in', 'due', 'supermercati', 'oggi', 'volevo', 'andare', ""all'ippodromo"", 'stasera', 'mangio', 'la', 'pizza', 'con', 'le', 'verdure']
3) clean stop-words, num words: 12 for list: ['ieri', 'andato', 'due', 'supermercati', 'oggi', 'volevo', 'andare', ""all'ippodromo"", 'stasera', 'mangio', 'pizza', 'verdure']
4.1) lemmatizer, num words: 12 for list: [u'ieri', u'andarsene', u'due', u'supermercato', u'oggi', u'volere', u'andare', u""all'ippodromo"", u'stasera', u'mangiare', u'pizza', u'verdura']
4.2) stemmer, num words: 12 for list: [u'ier', u'andat', u'due', u'supermerc', u'oggi', u'vol', u'andar', u""all'ippodrom"", u'staser', u'mang', u'pizz', u'verdur']
for original word(s) 'andato' and 'andare' the stemmer: 'andat' 'andar' (count 1 each), the lemmatizer: 'andarsene' 'andarsene' (count 2)


how to effectively lemmatize some sentences with pattern using their tokenizer? (assuming lemmas are recognized as nouns/verbs/adjectives etc.)
is there a python alternative to pattern to use for italian lemmatization with nltk?  
how to split articles that are bound to the next word using apostrophes?","['python-2.7', 'nlp', 'nltk', 'stemming', 'lemmatization']",45555676,"i'll try to answer your question, knowing that i don't know a lot about italian!
1) as far as i know, the main responsibility for removing apostrophe is the tokenizer, and as such the nltk italian tokenizer seems to have failed.
3) a simple thing you can do about it is call the replace method (although you probably will have to use the re package for more complicated pattern), an example:
word_tokenized_no_punct_no_sw_no_apostrophe = [x.split(""'"") for x in word_tokenized_no_punct_no_sw]
word_tokenized_no_punct_no_sw_no_apostrophe = [y for x in word_tokenized_no_punct_no_sw_no_apostrophe for y in x]

it yields:
['ieri', 'andato', 'due', 'supermercati', 'oggi', 'volevo', 'andare', 'all', 'ippodromo', 'stasera', 'mangio', 'pizza', 'verdure']

2) an alternative to pattern would be treetagger, granted it is not the easiest install of all (you need the python package and the tool itself, however after this part it works on windows and linux).
a simple example with your example above:
import treetaggerwrapper 
from pprint import pprint

it_string = ""ieri sono andato in due supermercati. oggi volevo andare all'ippodromo. stasera mangio la pizza con le verdure.""
tagger = treetaggerwrapper.treetagger(taglang=""it"")
tags = tagger.tag_text(it_string)
pprint(treetaggerwrapper.make_tags(tags))

the pprint yields:
[tag(word=u'ieri', pos=u'adv', lemma=u'ieri'),
 tag(word=u'sono', pos=u'ver:pres', lemma=u'essere'),
 tag(word=u'andato', pos=u'ver:pper', lemma=u'andare'),
 tag(word=u'in', pos=u'pre', lemma=u'in'),
 tag(word=u'due', pos=u'adj', lemma=u'due'),
 tag(word=u'supermercati', pos=u'nom', lemma=u'supermercato'),
 tag(word=u'.', pos=u'sent', lemma=u'.'),
 tag(word=u'oggi', pos=u'adv', lemma=u'oggi'),
 tag(word=u'volevo', pos=u'ver:impf', lemma=u'volere'),
 tag(word=u'andare', pos=u'ver:infi', lemma=u'andare'),
 tag(word=u""all'"", pos=u'pre:det', lemma=u'al'),
 tag(word=u'ippodromo', pos=u'nom', lemma=u'ippodromo'),
 tag(word=u'.', pos=u'sent', lemma=u'.'),
 tag(word=u'stasera', pos=u'adv', lemma=u'stasera'),
 tag(word=u'mangio', pos=u'ver:pres', lemma=u'mangiare'),
 tag(word=u'la', pos=u'det:def', lemma=u'il'),
 tag(word=u'pizza', pos=u'nom', lemma=u'pizza'),
 tag(word=u'con', pos=u'pre', lemma=u'con'),
 tag(word=u'le', pos=u'det:def', lemma=u'il'),
 tag(word=u'verdure', pos=u'nom', lemma=u'verdura'),
 tag(word=u'.', pos=u'sent', lemma=u'.')]

it also tokenized pretty nicely the all'ippodromo to al and ippodromo (which is hopefully correct) under the hood before lemmatizing. now we just need to apply the removal of stop words and punctuation and it will be fine.
the doc for installing the treetaggerwrapper library for python",https://stackoverflow.com/questions/45403390,python-2.7,30-07-2017 18:41,12964.0,11.0,3.0,True,05-10-2023 12:46,02-08-2017 08:54,Preprocessing Tasks
43912195,re-training spacy&#39;s ner v1.8.2 - training volume and mix of entity types,"i'm in the process of (re-) training spacy's named entity recognizer and have a couple of doubts that i hope a more experienced researcher/practitioner can help me figure out:

if a few hundred examples are considered 'a good starting point', then what would be a reasonable number to aim for? is 100 000 entity/label excessive?
if i introduce a new label, is it best if the number of the entities of that labeled are roughly the same (balanced) during training?
regarding the mixing in 'examples of other entity types':

do i just add random known categories/labels to my training set eg: ('the business standard published in its recent issue on crude oil and natural gas ...', [(4,21, 'org')], )?

can i use the same text for various labels? e.g. ('the business standard published in its recent issue on crude oil and natural gas ...', [(55,64, 'commodity')], )?





on a similar note let's assume i want spacyto also recognize a second commodity could i then just use the same sentence and label a different region e.g. ('the business standard published in its recent issue on crude oil and natural gas ...', [(69,80, 'commodity')], )? is that how it's supposed to be done?

what ratio between new and other (old) labels is considered reasonable



i'm working with python2.7 in ubuntu 16.04 using spacy 1.8.2","['python-2.7', 'nlp', 'named-entity-recognition', 'spacy']",44159573,"for a full answer by matthew honnibal check out issue 1054 on spacy's github page. below are the most important points as they relate to my questions:

question(q) 1: if a few hundred examples are considered 'a good starting point', then what would be a reasonable number to aim for? is 100 000 entity/label excessive?

answer(a): every machine learning problem will have a different examples/accuracy curve. you can get an idea for this by training with less data than you have, and seeing what the curve looks like. if you have 1,000 examples, then try training with 500, 750, etc, and see how that affects your accuracy.

q 2: if i introduce a new label, is it best if the number of the entities of that label are roughly the same (balanced) during training?

a: there's trade-off between making the gradients too sparse, and making the learning problem too unrepresentative of what the actual examples will look like.

q 3: regarding the mixing in 'examples of other entity types':

do i just add random known categories/labels to my training set:


a: no, one should annotate all the entities in that text, so the example above: ('the business standard published in its recent issue on crude oil and natural gas ...', [(4,21, 'org')], ) should be ('the business standard published in its recent issue on crude oil and natural gas ...', [(4,21, 'org'), (55,64, 'commodity'), (69,80, 'commodity')], )


can i use the same text for various labels?:


a: not in the way the examples were given. see previous answer.


what ratio between new and other (old) labels is considered reasonable?:


a: see answer q 2.

ps: double citations are direct quotes from the github issue answer.",https://stackoverflow.com/questions/43912195,python-2.7,11-05-2017 09:58,821.0,3.0,1.0,True,16-03-2025 17:02,16-03-2025 17:02,Implementation Issues
70484237,what to use in place of predict_classes() in a jupyter notebook with tensorflow? (nlp text generation),"i am trying to follow a tensorflow nlp tutorial to train a neural network to generate poetry/lyric-like outputs using my own compiled sources. i only know basic python, so this is definitely far above my level of competence. it seems that the tutorial is slightly outdated as i am receiving this error code:
attributeerror: 'sequential' object has no attribute 'predict_classes'

i understand that the attribute 'predict_classes' is deprecated and no longer used in current versions of tensorflow.
this was a line of code suggested in an answer but i don't understand how to include it into my code:
 = np.argmax(model.predict(x_test), axis=-1)
any help would be appreciated. here is the section of code that is giving me trouble, along with the error code.
i included a link to the full jupyter notebook as well.
seed_text = ""vernal sunlight""
next_words = 100
  
for _ in range(next_words):
    token_list = tokenizer.texts_to_sequences([seed_text])[0]
    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
    predicted = model.predict_classes(token_list, verbose=0)
    output_word = """"
    for word, index in tokenizer.word_index.items():
        if index == predicted:
            output_word = word
            break
    seed_text += "" "" + output_word
print(seed_text)
---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-16-0539a42e927b> in <module>()
      5         token_list = tokenizer.texts_to_sequences([seed_text])[0]
      6         token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
----> 7         predicted = model.predict_classes(token_list, verbose=0)
      8         output_word = """"
      9         for word, index in tokenizer.word_index.items():

attributeerror: 'sequential' object has no attribute 'predict_classes'

here is the link to the video i was following!
and here is the copy of the jupyter notebook!","['python', 'tensorflow', 'jupyter-notebook', 'nlp']",70484326,"you can find the predicted class by using argmax with the predicted tensor as a parameter. define predicted as the following :
predicted = np.argmax(model.predict(x), axis=-1)",https://stackoverflow.com/questions/70484237,python,26-12-2021 04:37,154.0,-1.0,1.0,True,28-12-2021 13:42,28-12-2021 13:42,Conceptual Questions
75792678,count words in a sentence controlling for negations,"i am trying to count the number of times some words occur in a sentence while controlling for negations. in the example below, i write a very basic code where i count the number of times ""w"" appear in ""txt"". yet, i fail to control for negations like ""don't"" and/or ""not"".
w = [""hello"", ""apple""]

for word in w:
    txt = ""i love apples, apple are my favorite fruit. i don't really like apples if they are too mature. i do not like apples if they are immature either.""
    print(txt.count(word))

the code should say that it finds ""apple"" only times and not 4. so, i would like to add: if, n words before or after the words in ""w"" there is a negation, then don't count, and otherwise.
n.b. negations here are words like ""don't"" and ""not"".
can anyone help me with this?
thanks a lot for your help!","['python', 'nlp']",75792902,"firstly, before you consider the negations/negatives, str.count might not be doing what you're expecting.
text = ""i love apples, apple are my favorite fruit. i don't really like apples if they are too mature. i do not like apples if they are immature either.""

text.count('apple') # outputs: 4

but if you do:
text = ""the thief grappled the pineapples and ran away with a basket of apples""

text.count('apple') # outputs: 3

if you want to count the words, you would need to do some tokenization first to change the string into a list of strings, e.g.
from collections import counter

import nltk
from nltk import word_tokenize

nltk.download('punkt')

text = ""the thief grappled the pineapples and ran away with a basket of apples""

counter(word_tokenize(text))['apple'] # output: 0
counter(word_tokenize(text))['apples'] # output: 1

then you would need to ask yourself does plural matters when you want to count the no. of times apple/apples occur? if so, then you would have to do some stemming or lemmatization, stemmers vs lemmatizers
this tutorial might be helpful: 

assuming that you adopt lemmas and tokenizers and consider whatever you need to define what is a ""word"" and how to count them, you have to define what is negation and what do you want to do with the counts ultimately?
lets go with

i want to break the text down into ""chunks"" or clauses that have positive and negative sentiment towards some object/nouns.

then you would have to define what does negative/positive means, in the simplest terms you might say

anything negation words that comes near the window of the focus noun we consider as ""negative"" and in any other case, positive.

and if we try to code up the simplest terms of quantifying negation as above, you would first, have to

identify the focus word, lets take the word apple and
then the window, lets say 5 words before and 5 words after.

in code:
import nltk
from nltk import word_tokenize, ngrams

text = ""i love apples, apple are my favorite fruit. i don't really like apples if they are too mature. i do not like apples if they are immature either.""

negative_words = [""don't"", ""do not"", ""not""]
# add all the forms of tokenized negative words
negative_words += [word_tokenize(w) for w in negative_words]

def count_negation(tokens):
    return sum(1 for word in tokens if word in negative_words)

for window in ngrams(word_tokenize(text), 5): 
  if ""apple"" in window or ""apples"" in window:
    print(count_negation(window), window)

[out]:
0 ('i', 'love', 'apples', ',', 'apple')
0 ('love', 'apples', ',', 'apple', 'are')
0 ('apples', ',', 'apple', 'are', 'my')
0 (',', 'apple', 'are', 'my', 'favorite')
0 ('apple', 'are', 'my', 'favorite', 'fruit')
0 ('do', ""n't"", 'really', 'like', 'apples')
0 (""n't"", 'really', 'like', 'apples', 'if')
0 ('really', 'like', 'apples', 'if', 'they')
0 ('like', 'apples', 'if', 'they', 'are')
0 ('apples', 'if', 'they', 'are', 'too')
1 ('i', 'do', 'not', 'like', 'apples')
1 ('do', 'not', 'like', 'apples', 'if')
1 ('not', 'like', 'apples', 'if', 'they')
0 ('like', 'apples', 'if', 'they', 'are')
0 ('apples', 'if', 'they', 'are', 'immature')

q: but isn't that kind of over-counting when i do not like apples get counted 3 times even though the sentence/clause appears once in the text?
yes, it is over-counting, so it goes back to the question of what is the ultimate goal of counting the negations?
if the ultimate goal is to have a sentiment classifier then i think lexical approaches might not be as good as state-of-the-art language models, like:
from transformers import autotokenizer, automodelforseq2seqlm

model_name = ""google/flan-t5-large""

tokenizer= autotokenizer.from_pretrained(model_name)
model = automodelforseq2seqlm.from_pretrained(model_name)

text = ""i love apples, apple are my favorite fruit. i don't really like apples if they are too mature. i do not like apples if they are immature either.""


prompt=f""""""do i like apples or not?
query:{text}
options:
 - yes, i like apples
 - no, i hate apples
""""""

input_ids = tokenizer(prompt, return_tensors=""pt"").input_ids
tokenize.decode(model.generate(input_ids)[0], skip_special_tokens=true)

[out]:
yes, i like apples

q: but what if i want to explain why the model assumes positive/negative sentiments towards apple? how can i do it without counting negations?
a: good point, it's an active research area to explain the outputs, so definitely, there's no clear answer yet but take a look at",https://stackoverflow.com/questions/75792678,python,20-03-2023 16:08,100.0,0.0,1.0,True,20-03-2023 16:50,20-03-2023 16:43,Implementation Issues
63705803,merge related words in nlp,"i'd like to define a new word which includes count values from two (or more) different words. for example:
words frequency
0   mom 250
1   2020    151
2   the 124
3   19  82
4   mother  81
... ... ...
10  london  6
11  life    6
12  something   6

i would like to define mother as mom + mother:
words frequency
0   mother  331
1   2020    151
2   the 124
3   19  82
... ... ...
9   london  6
10  life    6
11  something   6

this is a way to alternative define group of words having some meaning (at least for my purpose).
any suggestion would be appreciated.","['python', 'nlp', 'cluster-analysis', 'word2vec', 'wordnet']",63771196,"update 10-21-2020
i decided to build a python module to handle the tasks that i outlined in this answer. the module is called wordhoard and can be downloaded from pypi

i have attempted to use word2vec and wordnet in projects where i needed to determine the frequency of a keyword (e.g. healthcare) and the keyword's synonyms (e.g., wellness program, preventive medicine).  i found that most nlp libraries didn't produce the results that i needed, so i decided to build my own dictionary with custom keywords and synonyms.  this approached has worked for both analyzing and classification text in multiple projects.
i'm sure that someone that is versed in nlp technology might have a more robust solution, but the one below is similar ones that have worked for me time and time again.
i coded my answer to match the words frequency data you had in your question, but it can be modified to use any keyword and synonyms dataset.
import string

# python dictionary
# i manually created these word relationship - primary_word:synonyms
word_relationship = {""father"": ['dad', 'daddy', 'old man', 'pa', 'pappy', 'papa', 'pop'],
          ""mother"": [""mamma"", ""momma"", ""mama"", ""mammy"", ""mummy"", ""mommy"", ""mom"", ""mum""]}

# this input text is from various poems about mothers and fathers
input_text = 'the hand that rocks the cradle also makes the house a home. it is the prayers of the mother ' \
         'that keeps the family strong. when i think about my mum, i just cannot help but smile; the beauty of ' \
         'her loving heart, the easy grace in her style. i will always need my mom, regardless of my age. she ' \
         'has made me laugh, made me cry. her love will never fade. if i could write a story, it would be the ' \
         'greatest ever told. i would write about my daddy, for he had a heart of gold. for my father, my friend, ' \
         'this to me you have always been. through the good times and the bad, your understanding i have had.'

# converts the input text to lowercase and splits the words based on empty space.
wordlist = input_text.lower().split()

# remove all punctuation from the wordlist
remove_punctuation = [''.join(ch for ch in s if ch not in string.punctuation) 
for s in wordlist]

# list for word frequencies
wordfreq = []

# count the frequencies of a word
for w in remove_punctuation:
wordfreq.append(remove_punctuation.count(w))

word_frequencies = (dict(zip(remove_punctuation, wordfreq)))

word_matches = []

# loop through the dictionaries
for word, frequency in word_frequencies.items():
   for keyword, synonym in word_relationship.items():
      match = [x for x in synonym if word == x]
      if word == keyword or match:
        match = ' '.join(map(str, match))
        # append the keywords (mother), synonyms(mom) and frequencies to a list
        word_matches.append([keyword, match, frequency])

# used to hold the final keyword and frequencies
final_results = {}

# list comprehension to obtain the primary keyword and its frequencies
synonym_matches = [(keyword[0], keyword[2]) for keyword in word_matches]

# iterate synonym_matches and output total frequency count for a specific keyword
for item in synonym_matches:
  if item[0] not in final_results.keys():
    frequency_count = 0
    frequency_count = frequency_count + item[1]
    final_results[item[0]] = frequency_count
  else:
    frequency_count = frequency_count + item[1]
    final_results[item[0]] = frequency_count

 
print(final_results)
# output
{'mother': 3, 'father': 2}

other methods
below are some other methods and their out-of-box output.

nltk wordnet
in this example, i looked up the synonyms for the word 'mother.' note that wordnet does not have the synonyms 'mom' or 'mum' linked to the word mother.  these two words are within my sample text above.  also note that the word 'father' is listed as a synonym for 'mother.'
from nltk.corpus import wordnet

synonyms = []
word = 'mother'
for synonym in wordnet.synsets(word):
   for item in synonym.lemmas():
      if word != synonym.name() and len(synonym.lemma_names()) > 1:
        synonyms.append(item.name())

print(synonyms)
['mother', 'female_parent', 'mother', 'fuss', 'overprotect', 'beget', 'get', 'engender', 'father', 'mother', 'sire', 'generate', 'bring_forth']

pydictionary
in this example, i looked up the synonyms for the word 'mother' using pydictionary, which queries synonym.com. the synonyms in this example include the words 'mom' and 'mum.' this example also includes additional synonyms that wordnet did not generate.
but, pydictionary also produced a synonym list for 'mum.' which has nothing to do with the word 'mother.'  it seems that pydictionary pulled this list from the adjective section of the page instead of the noun section.  it's hard for a computer to distinguish between the adjective mum and the noun mum.
from pydictionary import pydictionary
dictionary_mother = pydictionary('mother')

print(dictionary_mother.getsynonyms())
# output 
[{'mother': ['mother-in-law', 'female parent', 'supermom', 'mum', 'parent', 'mom', 'momma', 'para i', 'mama', 'mummy', 'quadripara', 'mommy', 'quintipara', 'ma', 'puerpera', 'surrogate mother', 'mater', 'primipara', 'mammy', 'mamma']}]

dictionary_mum = pydictionary('mum')

print(dictionary_mum.getsynonyms())
# output 
[{'mum': ['incommunicative', 'silent', 'uncommunicative']}]

some of the other possible approaches are using the oxford dictionary api or querying thesaurus.com. both these methods also have pitfalls. for instance the oxford dictionary api requires an api key and a paid subscription based on query numbers. and thesaurus.com is missing potential synonyms that could be useful in grouping words.

synonyms: mom, parent, ancestor, creator, mommy, origin, predecessor, progenitor, source, child-bearer, forebearer, procreator

update
producing a precise synonym lists for each potential word in your corpus is hard and will require a multiple prong approach.  the code below using
wordnet and pydictionary to create a superset of synonyms.  like all the other answers, this combine methods also leads to some over counting of word frequencies. i've been trying to reduce this over-counting by combining key and value pairs within my final dictionary of synonyms.  the latter problem is much harder than i anticipated and might require me to open my own question to solve.  in the end, i think that based on your use case you need to determine, which approach works best and will likely need to combine several approaches.
thanks for posting this question, because it allowed me to look at other methods for solving a complex problem.
from string import punctuation
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from pydictionary import pydictionary

input_text = """"""the hand that rocks the cradle also makes the house a home. it is the prayers of the mother
         that keeps the family strong. when i think about my mum, i just cannot help but smile; the beauty of
         her loving heart, the easy grace in her style. i will always need my mom, regardless of my age. she
         has made me laugh, made me cry. her love will never fade. if i could write a story, it would be the
         greatest ever told. i would write about my daddy, for he had a heart of gold. for my father, my friend,
         this to me you have always been. through the good times and the bad, your understanding i have had.""""""


def normalize_textual_information(text):
   # split text into tokens by white space
   token = text.split()

   # remove punctuation from each token
   table = str.maketrans('', '', punctuation)
   token = [word.translate(table) for word in token]

   # remove any tokens that are not alphabetic
   token = [word.lower() for word in token if word.isalpha()]

   # filter out english stop words
   stop_words = set(stopwords.words('english'))

   # you could add additional stops like this
   stop_words.add('cannot')
   stop_words.add('could')
   stop_words.add('would')

   token = [word for word in token if word not in stop_words]

   # filter out any short tokens
   token = [word for word in token if len(word) > 1]
   return token


def generate_word_frequencies(words):
   # list to hold word frequencies
   word_frequencies = []

   # loop through the tokens and generate a word count for each token
   for word in words:
      word_frequencies.append(words.count(word))

   # aggregates the words and word_frequencies into tuples and coverts them into a dictionary
   word_frequencies = (dict(zip(words, word_frequencies)))

   # sort the frequency of the words from low to high
   sorted_frequencies = {key: value for key, value in 
   sorted(word_frequencies.items(), key=lambda item: item[1])}

 return sorted_frequencies


def get_synonyms_internet(word):
   dictionary = pydictionary(word)
   synonym = dictionary.getsynonyms()
   return synonym

 
words = normalize_textual_information(input_text)

all_synsets_1 = {}
for word in words:
  for synonym in wordnet.synsets(word):
    if word != synonym.name() and len(synonym.lemma_names()) > 1:
      for item in synonym.lemmas():
        if word != item.name():
          all_synsets_1.setdefault(word, []).append(str(item.name()).lower())

all_synsets_2 = {}
for word in words:
  word_synonyms = get_synonyms_internet(word)
  for synonym in word_synonyms:
    if word != synonym and synonym is not none:
      all_synsets_2.update(synonym)

 word_relationship = {**all_synsets_1, **all_synsets_2}

 frequencies = generate_word_frequencies(words)
 word_matches = []
 word_set = {}
 duplication_check = set()

 for word, frequency in frequencies.items():
    for keyword, synonym in word_relationship.items():
       match = [x for x in synonym if word == x]
       if word == keyword or match:
         match = ' '.join(map(str, match))
         if match not in word_set or match not in duplication_check or word not in duplication_check:
            duplication_check.add(word)
            duplication_check.add(match)
            word_matches.append([keyword, match, frequency])

 # used to hold the final keyword and frequencies
 final_results = {}

 # list comprehension to obtain the primary keyword and its frequencies
 synonym_matches = [(keyword[0], keyword[2]) for keyword in word_matches]

 # iterate synonym_matches and output total frequency count for a specific keyword
 for item in synonym_matches:
    if item[0] not in final_results.keys():
      frequency_count = 0
      frequency_count = frequency_count + item[1]
      final_results[item[0]] = frequency_count
 else:
    frequency_count = frequency_count + item[1]
    final_results[item[0]] = frequency_count

# do something with the final results",https://stackoverflow.com/questions/63705803,python,02-09-2020 12:44,10091.0,23.0,6.0,True,16-05-2021 11:49,06-09-2020 02:49,Implementation Issues
78317989,is it possible to fine-tune a pretrained word embedding model like vec2word?,"i'm working on semantic matching in my search engine system. i saw that word embedding can be used for this task. however, my dataset is very limited and small, so i don't think that training a word embedding model such as word2vec from scratch will yield good results. as such, i decided to fine-tune a pre-trained model with my data.
however, i can't find a lot of information, such as articles or documentation, about fine-tuning. some people even say that it's impossible to fine-tune a word embedding model.
this raises my question: is fine-tuning a pre-trained word embedding model possible and has anyone tried this before? currently, i'm stuck and looking for more information. should i try to train a word embedding model from scratch or are there other approaches?","['python', 'nlp', 'artificial-intelligence', 'word2vec', 'word-embedding']",78323755,"as has been pointed out before, there is no ""go-to"" way for fine-tuning word2vec type models.
i would suggest training your own model from scratch, combining your data with other available data from a similar domain. word2vec models are fairly quick to train and this would probably give you the best results. if you do not need static word-level embeddings, i would recommend considering contextualized embeddings, for example through the use of sentence-transformers or similar frameworks, which has a wide selection of already pre-trained models you can choose from. you can fine-tune these types of models on your specific data rather easily, and there are tons of resources online on how to do that.
for your use case, you can embed all the documents into dense vector representations using the abovementioned library, and then construct a searchable index over this semantic space. in order to match queries, all you have to do then is to embed the query using the same model and then retrieve the documents with the highest approximate inner product, often referred to as a mips search. an example library to take a look at would be faiss.",https://stackoverflow.com/questions/78317989,python,12-04-2024 17:57,1355.0,0.0,2.0,True,15-04-2024 16:46,12-04-2024 19:56,Implementation Issues
72196425,identifying near duplicate keywords and replacing them,"i have a dataframe like as shown below
id,name,year,output
1,test level,2021,1
2,test lvele,2022,1
2,dummy inc,2022,1
2,dummy pvt inc,2022,1
3,dasho ltd,2022,1
4,dasho pvt ltd,2021,0
5,delphi ltd,2021,1
6,delphi pvt ltd,2021,1

df = pd.read_clipboard(sep=',')

my objective is
a) to replace near duplicate strings using a common string.
for example - let's pick couple of strings from name column. we have dummy inc and dummy pvt inc. these both have to be replaced as dummy
i manually prepared a mapping df map_df like as below (but can't do this for big data)
  name,correct_name
  test level,test
  test lvele,test
  dummy inc,dummy
  dummy pvt inc,dummy
  dasho ltd,dasho
  dasho pvt ltd,dasho
  delphi ltd,delphi
  delphi pvt ltd,delphi

so, i tried the below
map_df = map_df.set_index(name)
df['name'] = df['name'].map(map_df) # but this doesn't work and throws error

is creating mapping table the only way or is there any nlp based approach?
i expect my output to be like as below
id,name,year,output
1,test,2021,1
2,test,2022,1
2,dummy,2022,1
2,dummy,2022,1
3,dasho,2022,1
4,dasho,2021,0
5,delphi,2021,1
6,delphi,2021,1","['python', 'pandas', 'dataframe', 'nlp', 'nltk']",72200732,"i suggest using a dict instead of a pandas.dataframe for map_df.
id,name,year,output
1,test level,2021,1
2,test lvele,2022,1
2,dummy inc,2022,1
2,dummy pvt inc,2022,1
3,dasho ltd,2022,1
4,dasho pvt ltd,2021,0
5,delphi ltd,2021,1
6,delphi pvt ltd,2021,1

df = pd.read_clipboard(sep=',')

map_dict = dict(s.strip().split(',') for s in '''  test level,test
  test lvele,test
  dummy inc,dummy
  dummy pvt inc,dummy
  dasho ltd,dasho
  dasho pvt ltd,dasho
  delphi ltd,delphi
  delphi pvt ltd,delphi'''.split('\n'))

df['name'] = df['name'].map(map_dict.get)

results:
df.to_clipboard(sep=',')

,id,name,year,output
0,1,test,2021,1
1,2,test,2022,1
2,2,dummy,2022,1
3,2,dummy,2022,1
4,3,dasho,2022,1
5,4,dasho,2021,0
6,5,delphi,2021,1
7,6,delphi,2021,1

if map_df is already a dataframe with two columns and you want to turn these two columns into a dict, this related question: how to create a dictionary of two pandas dataframe columns? suggests a few methods:
map_dict = dict(zip(map_df['name'], map_df['correct_name']))

map_dict = pd.series(map_df['correct_name'].values,index=map_df['name']).to_dict()

map_dict = map_df.set_index('name').to_dict()['correct_name']

map_dict = dict(map_df.to_records(index=false))",https://stackoverflow.com/questions/72196425,python,11-05-2022 06:36,75.0,1.0,2.0,True,31-05-2022 06:03,11-05-2022 06:56,Implementation Issues
7870554,extracting clause from a penn treebank-formatted text,"say i have a sentence:
after he had eaten the cheese, bill went to the grocery.

in my program, i get the following output:
---parse tree---
(root
  (s
    (sbar (in after)
      (s
        (np (prp he))
        (vp (vbd had)
          (vp (vbn eaten)
            (np (dt the) (nn cheese))))))
    (, ,)
    (np (nnp bill))
    (vp (vbd went)
      (pp (to to)
        (np (dt the) (nn grocery))))
    (. .)))

how would i merge the stuff not within a clause to become an independent clause? like this:
s clause {
    sbar clause {
         after he had eaten the cheese,
    }

    s clause {
        bill went to the grocery.
    }
}

i'm pretty sure that i'm not clear, but basically i want to extract the independent and dependent clauses of the sentence, and the subclauses of those clauses.","['nlp', 'stanford-nlp']",7883386,here is a demonstration code from the nltk guide (it doesn't explicitly show how to extract a clause):,https://stackoverflow.com/questions/7870554,nlp,24-10-2011 01:27,1525.0,4.0,1.0,True,19-01-2024 23:13,04-05-2012 18:57,Implementation Issues
68693030,doc2vec model not producing expected similarity scores,"i'm trying to compare two sentences and get the cosine similarity between them.
i have about 50 sentences, and i used genism's pre-trained doc2vec and trained the model on these 50 sentences to just tweak the weights a little bit. however, the cosine similarity between two sentences is not truly reflecting the similarity. for example, sentence1 is not in english close to sentence2 but their embeddings are very similar.
my question is, how do i go about generally comparing 2 sentences for similarities (as doc2vec is not working for me). it seems to be due to the low amount of training inputs to tweak the weights, but i wonder if there is another technique to achieve this task.
e.g. rough implementation so far
s1 = ""this is a sentence""
s2 = ""this is also a sentence""
...
s50 =""this is the last sentence

list = [s1,s2..s50]

tagged_data = [taggeddocument(words=word_tokenize(_d.lower()), tags=[
                                      str(i)]) for i, _d in enumerate(list)]
model = doc2vec(vector_size=vec_size,
                        alpha=alpha,
                        min_alpha=0.00025,
                        min_count=1,
                        dm=1)

model.build_vocab(tagged_data)

for epoch in range(max_epochs):
   print('iteration {0}'.format(epoch))
   model.train(tagged_data,
   total_examples=model.corpus_count,
   epochs=100)
   # decrease the learning rate
   model.alpha -= 0.0002
   # fix the learning rate, no decay
   model.min_alpha = model.alpha


i then loop through each sentence and perform model.infer_vector(sent_tokens) to get the embeddings. but as i said, they are not even close to being correct when using similarities.
if i am doing something wrong please let me know.","['python', 'nlp', 'doc2vec']",68694040,"there is no ""gensim's pre-trained doc2vec"", so if in fact you're using some pre-trained model from some 3rd party, you'd need to descriobe the source to know what's in play here. (however, your code seems to show a new model trained up from only 50 sentences.)
50 sentences is not enough to train doc2vec (or related algorithms like word2vec or fasttext). they need bulk data, with many sublty-varying, realistic usage examples of every word of any interest, to create useful vectors.
it is almost always a bad idea to use min_count=1 with doc2vec & similar algorithms, as they depend on the influence of multiple varied contexts for a word. if there's only 1 use, or a few, of a word then any vector learned for tha word is likely to be idiosyncratic to that appearance and not of generalizable usefulness. plus, the existence of many such rare words (in usual natural-language corpora) can mean such junk words serve as noise in the model to dilute and interfere-with the training of other words for which there are suitable examples. the models usually work better if you discard such infrequent words entirely - and that's why the default is min_count=5.
i've not seen any good write-up of someone doing tiny followup tuning, with a small amount of new data, on a pretrained doc2vec model ï¿½ï¿½ï¿½ so i wouldn't recommend attempting that to someone just starting out with doc2vec. (if it works at all, it'll require expert experimentation & tuning.)
it's also almost always a misguided & error-prone idea to be calling .train() more than once in a loop, and adjusting alpha/min_alpha outside the usual default and automatic management. see this answer for more details: 
if you train properly with a good-sized corpus, then check pairwise similarities of texts that the training data was representative-of, you should see more sensible similarity values.",https://stackoverflow.com/questions/68693030,python,07-08-2021 14:01,206.0,0.0,1.0,True,07-08-2021 20:22,07-08-2021 20:22,Implementation Issues
78622400,embedding token limit overpass by chunking concatenation and dimensionality reduction,"if you want to generate embeddings for documents using azure openai with ada-002 model then you should sent maximum 8192 tokens to this api. if one document has more than 8k tokens then in order to process it we should follow specific steps as per my investigation.

prepare document text, clean, normalize, remove-stop-words to be able to count tokens as azure openai ada-002 counts them.
tokenize document text into words by splitting on space ("" "")
if document's tokens are more than 8k then split it into more sub-documents with maximum 8k tokens
pass these 8k sub-documents from the azure openai ada-002 endpoint and get embeddings for each sub-document.
combine those float embeddings (by appending) into one single vector to represent the original document.
then in order to be able to find similar documents based on question, question vector and document vectors should have same length, so we need obviously to reduce dimensionality on documents which were spitted and them re-embedded into single vector.

as an example, if a document (10k tokens) is split into two sub documents (8k and 2k) each sub-document embedding will have 1536 dimensions and therefore the complete document will have 1536 x 2 = 3072. the question which is not exceed the 8k tokens will have 1536 and therefore cannot be compared with all documents.
so is there any way to reduce properly the dimensions of those documents of 3072 dims back to 1536 dims?
according to my research this can be done using pca, i have found the following example in c#, but here the data are [][] instead of []:
double[][] data = new double[][]
{
// ... your combined embedding vectors here
};

// create a new principal component analysis
var pca = new principalcomponentanalysis()
{
method = principalcomponentmethod.center,
whiten = false
};

// learn the pca model
pca.learn(data);

// transform the data into the reduced dimensionality space
double[][] reduceddata = pca.transform(data, 3); // reducing to 3 dimensions

any ideas?","['c#', 'token', 'openai-api', 'embedding', 'dimensions']",78666476,"found the answer, there are multiple approaches to approach this issue:
first, split the document in chunks (important notice is the way the document is spitted in chunks, we can split the document by sentences or per symbols or per fixed number of tokens), if we use specific number of tokens based on the model used, for example splitting the document into 256-tokens, or 512-tokens, or 1k-tokens, is good for ada-002 performance. then embed each chunk using selected model, for example ada-002, and then gather all embedded chunks of the document. in most of the cases token overlap in chunks can increase the quality of the solution.
dimensionality reduction can be implemented with multiple ways:
one, good approach, is that after all chunks of the document are embedded we can take the average of each dimension beside the chunks. if a chunk of ada-002 has 1532 dimensions, then we will have multiple chunks of 1532 dimensions. taking the average of each dimension we will have again same dimension vector.  this method is fast and easy to implement.
second, approach, is that after all chunks of the document are embedded we can combine the document embeddings. if a chunk of ada-002 has 1532 dimensions, then we will have 1532 x number_of_embeddings dimensions. then we can use pca to reduce dimensions back to original shape.
just tested splitting document in fixed number of tokens set to 1k, with token overlap to 100 and taking the average as reduction method, seems to work pretty well.",https://stackoverflow.com/questions/78622400,c#,14-06-2024 10:26,1007.0,0.0,1.0,True,27-06-2024 09:45,27-06-2024 09:45,Implementation Issues
5907296,java api for plural forms of english words,are there any java api(s) which will provide plural form of english words (e.g. cacti for cactus)?,"java, dictionary, nlp, lexical, pluralize",6288368,"wolfram|alpha return a list of inflection forms for a given word.
see this as an example:

and here is their api:",https://stackoverflow.com/q/5907296,"java, dictionary, nlp, lexical, pluralize",06-05-2011 05:49,22869.0,34.0,7.0,True,10-12-2022 17:55,10-12-2022 17:55,Uncategorized
2683506,wikipedia text download,"i am looking to download full wikipedia text for my college project. do i have to write my own spider to download this or is there a public dataset of wikipedia available online?
to just give you some overview of my project, i want to find out the interesting words of few articles i am interested in. but to find these interesting words, i am planning to apply tf/idf to calculate term frequency for each word and pick the ones with high frequency. but to calculate the tf, i need to know the total occurrences in whole of wikipedia.
how can this be done?","['text', 'wikipedia', 'web-crawler', 'information-retrieval']",2683522,"from wikipedia:  

wikipedia offers free copies of all available content to interested users. these databases can be used for mirroring, personal use, informal backups, offline use or database queries (such as for wikipedia:maintenance). all text content is multi-licensed under the creative commons attribution-sharealike 3.0 license (cc-by-sa) and the gnu free documentation license (gfdl). images and other files are available under different terms, as detailed on their description pages. for our advice about complying with these licenses, see wikipedia:copyrights.

seems that you are in luck too.  from the dump section:

as of 12 march 2010, the latest complete dump of the english-language wikipedia can be found at   this is the first complete dump of the english-language wikipedia to have been created since 2008.
please note that more recent dumps (such as the 20100312 dump) are incomplete.

so the data is only 9 days old :)
edit: new link as old is broken:",https://stackoverflow.com/questions/2683506,text,21-04-2010 13:56,44033.0,29.0,8.0,True,01-12-2023 13:45,13-04-2017 22:04,Implementation Issues
75313457,openai api: openai.api_key = os.getenv() not working,"i am just trying some simple functions in python with openai apis but running into an error:
i have a valid api secret key which i am using.
code:
>>> import os
>>> import openai
>>> openai.api_key = os.getenv(""i have placed the key here"")
>>> response = openai.completion.create(model=""text-davinci-003"", prompt=""say this is a test"", temperature=0, max_tokens=7)","['python', 'openai-api', 'chatgpt-api', 'gpt-3', 'gpt-4']",75313682,"option 1: openai api key not set as an environment variable
change this...
openai.api_key = os.getenv('sk-xxxxxxxxxxxxxxxxxxxx')
...to this.
openai.api_key = 'sk-xxxxxxxxxxxxxxxxxxxx'

option 2: openai api key set as an environment variable (recommended)
there are two ways to set the openai api key as an environment variable:

using an .env file (easier, but don't forget to create a .gitignore file) or
using windows environment variables.

way 1: using an .env file
change this...
openai.api_key = os.getenv('sk-xxxxxxxxxxxxxxxxxxxx')
...to this...
openai.api_key = os.getenv('openai_api_key')
also, don't forget to use the python-dotenv package. your final python file should look as follows:
# main.py

import os
from dotenv import load_dotenv
from openai import openai

# load environment variables from the .env file
load_dotenv()

# initialize openai client with the api key from environment variables
client = openai(
    api_key=os.getenv(""openai_api_key""),
)

it's crucial that you create a .gitignore file not to push the .env file to your github/gitlab and leak your openai api key!
# .gitignore

.env

way 2: using windows environment variables (source)
step 1: open system properties and select advanced system settings

step 2: select environment variables

step 3: select new
step 4: add your name/key value pair
variable name: openai_api_key

variable value: sk-xxxxxxxxxxxxxxxxxxxx

step 5: restart your computer (important!)
your final python file should look as follows:
# main.py

import os
from dotenv import load_dotenv
from openai import openai

# initialize openai client
# it will automatically use your openai api key set via windows environment variables
client = openai()",https://stackoverflow.com/questions/75313457,python,01-02-2023 16:44,41799.0,3.0,2.0,True,16-10-2024 12:07,12-08-2023 15:03,Implementation Issues
63157909,how to determine if two sentences talk about similar topics?,"i would like to ask you a question. is there any algorithm/tool which can allow me to do some association between words?
for example: i have the following group of sentences:
(1)
    ""my phone is on the table""
    ""i cannot find the charger"". # no reference on phone
(2) 
    ""my phone is on the table""
    ""i cannot find the phone's charger"". 

what i would like to do is to find a connection, probably a semantic connection, which can allow me to say that the first two sentences are talking about a topic (phone) as two terms (phone and charger) are common within it (in general). same for the second sentence.
i should have something that can connect phone to charger, in the first sentence.
i was thinking of using word2vec, but i am not sure if this is something that i can do with it.
do you have any suggestions about algorithms that i can use to determine similarity of topics (i.e. sentence which are formulated in a different way, but having same topic)?","['python', 'algorithm', 'nlp', 'sentence-similarity']",63159498,"in python i'm pretty sure you have a sequence matcher that you can usee
from difflib import sequencematcher

def similar(a, b):
    return sequencematcher(none, a, b).ratio()

if you want your own algorithm i would suggest a levenstains distance (it calculates how many operations you need to turn one string(sentance) into another. might be usefull.). i coded it myself in like this for two strings
    edits = [[x for x in range(len(str1) + 1)] for y in range(len(str2)+ 1)]
    for i in range(len(str2) + 1):
        edits[i][0] = i
    for i in range(1, len(str2) + 1):
        for j in range(1,  len(str1) + 1):
            if str2[i-1] == str1[j-1]:
                edits[i][j] = edits[i-1][j-1]
            else:
                edits[i][j] = 1 + min(edits[i-1][j-1], edits[i-1][j],
                                     edits[i][j-1])
    return edits[-1][-1]

[edit] for you, you want to compare if the sentances are about the similar topic. i would suggest any of the following algorithms (all are pretty easy)

jaccary similarity
k-means and hierarchical clustering dendrogram
cosine similarity",https://stackoverflow.com/questions/63157909,python,29-07-2020 16:11,3995.0,7.0,2.0,True,26-06-2023 07:18,29-07-2020 17:41,Implementation Issues
72619329,how does one extract the verb phrase in spacy?,"for example:

ultimate swirly ice cream scoopers are usually overrated when one considers all of the scoopers one could buy.

here i'd like to pluck:

subject: ""ultimate swirly ice cream scoopers""
adverbial clause: ""when one considers all of the scoopers one could buy""
verb phrase: ""are usually overrated""


i have the following functions for subject, object, and adverbial clause:
def get_subj(decomp):
    for token in decomp:
        if (""subj"" in token.dep_):
            subtree = list(token.subtree)
            start = subtree[0].i
            end = subtree[-1].i + 1
            return str(decomp[start:end])

def get_obj(decomp):
    for token in decomp:
        if (""dobj"" in token.dep_ or ""pobr"" in token.dep_):
            subtree = list(token.subtree)
            start = subtree[0].i
            end = subtree[-1].i + 1
            return str(decomp[start:end])

def get_advcl(decomp):
    for token in decomp:
        # print(f""pos: {token.pos_}; lemma: {token.lemma_}; dep: {token.dep_}"")
        if (""advcl"" in token.dep_):
            subtree = list(token.subtree)
            start = subtree[0].i
            end = subtree[-1].i + 1
            return str(decomp[start:end])

phrase = ""ultimate swirly ice cream scoopers are usually overrated when one considers all of the scoopers one could buy.""

nlp = spacy.load(""en_core_web_sm"")
decomp = nlp(phrase)

subj = get_subj(decomp)
obj = get_obj(decomp)
advcl = get_advcl(decomp)

print(""subj: "", subj)
print(""obj: "", obj)
print(""advcl: "", advcl)

output:
subj:  ultimate swirly ice cream scoopers
obj:  all of the scoopers
advcl:  when one considers all of the scoopers one could buy


however, the actual depenency type .dep_ for the final word of the vp, ""are usually overrated"", is ""root"".
so, the subtree technique fails, as the subtree of root returns the entire sentence.","['nlp', 'nltk', 'stanford-nlp', 'spacy']",72624250,"you are wanting to construct something more like a ï¿½ï¿½ï¿½verb groupï¿½ï¿½ï¿½ where you keep with the root verb only certain close dependents like aux, cop, and advmod but not ones like nsubj, obj, or advcl<",https://stackoverflow.com/questions/72619329,nlp,14-06-2022 15:06,693.0,1.0,1.0,True,14-06-2022 23:01,14-06-2022 15:52,Uncategorized
37526550,removing stopwords from a user-defined corpus in r,"i have a set of documents:
documents = c(""she had toast for breakfast"",
 ""the coffee this morning was excellent"", 
 ""for lunch let's all have pancakes"", 
 ""later in the day, there will be more talks"", 
 ""the talks on the first day were great"", 
 ""the second day should have good presentations too"")

in this set of documents, i would like to remove the stopwords. i have already removed punctuation and converted to lower case, using:
documents = tolower(documents) #make it lower case
documents = gsub('[[:punct:]]', '', documents) #remove punctuation

first i convert to a corpus object:
documents <- corpus(vectorsource(documents))

then i try to remove the stopwords:
documents = tm_map(documents, removewords, stopwords('english')) #remove stopwords

but this last line results in the following error:
the_process_has_forked_and_you_cannot_use_this_corefoundation_functionality___you_must_exec() to debug.
this has already been asked here but an answer was not given. what does this error mean?
edit
yes, i am using the tm package.
here is the output of sessioninfo():
r version 3.0.2 (2013-09-25)
platform: x86_64-apple-darwin10.8.0 (64-bit)","['r', 'tm', 'topic-modeling']",37526926,"when i run into tm problems i often end up just editing the original text. 
for removing words it's a little awkward, but you can paste together a regex from tm's stopword list. 
stopwords_regex = paste(stopwords('en'), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
documents = stringr::str_replace_all(documents, stopwords_regex, '')

> documents
[1] ""     toast  breakfast""             "" coffee  morning  excellent""      
[3] "" lunch lets   pancakes""            ""later   day  will   talks""        
[5] "" talks   first day  great""         "" second day   good presentations """,https://stackoverflow.com/questions/37526550,r,30-05-2016 13:11,53112.0,5.0,4.0,True,03-07-2023 22:30,23-05-2017 11:54,Conceptual Questions
71915952,why does huggingface hang on list input for pipeline sentiment-analysis?,"with python 3.10 and latest version of huggingface.
for simple code likes this
from transformers import pipeline

input_list = ['how do i test my connection? (windows)', 'how do i change my payment method?', 'how do i contact customer support?']

classifier = pipeline('sentiment-analysis')
results = classifier(input_list)

the program hangs and returns error messages:
file "".......env/lib/python3.10/multiprocessing/spawn.py"", line 134, in _check_not_importing_main
    raise runtimeerror('''
runtimeerror: 
        an attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        this probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...


but replace the list input with a string, it works
from transformers import pipeline
classifier = pipeline('sentiment-analysis')
result = classifier('how do i test my connection? (windows)')",['huggingface-transformers'],71926664,"it needs to define a main function to run multitask that the list input depends on. following update works
from transformers import pipeline

def main():
    input_list = ['how do i test my connection? (windows)', 
    'how do i change my payment method?',
    'how do i contact customer support?']

    classifier = pipeline('sentiment-analysis')
    results = classifier(input_list)

if __name__ == '__main__':
    main()

the question is reduced to where to put freeze_support() in a python script?",https://stackoverflow.com/questions/71915952,huggingface-transformers,18-04-2022 19:00,406.0,0.0,1.0,True,19-04-2022 14:28,19-04-2022 12:08,Implementation Issues
60166302,"filtering entities based on the type &quot;person&quot;, &quot;org&quot; etc in spacy","after creating a nlp pipeline from spacy. passed the doc into the pipeline.
am trying to filter the entities based on the type of it.
for ent in doc.ents:
    print(ent.text)

what would be the code to filter just the person or org from the ents list",['spacy'],60167194,"for ent in doc.ents:
    if ent.label_ in [""person"", ""org""]:
        print(ent.text)

see also  for an overview of the relevant properties available from ner.",https://stackoverflow.com/questions/60166302,spacy,11-02-2020 10:02,1996.0,0.0,2.0,True,03-01-2022 11:24,11-02-2020 10:26,Uncategorized
74593644,how to fix no token found error while downloading hugging face?,"i am trying to test the hugging face's prithivida/parrot_paraphraser_on_t5 model but getting token not found error.
from parrot import parrot
import torch
import warnings
warnings.filterwarnings(""ignore"")
parrot = parrot(model_tag=""prithivida/parrot_paraphraser_on_t5"", use_gpu=false)

the error i am getting
oserror                                   traceback (most recent call last)
cell in [10], line 2
      1 #init models (make sure you init only once if you integrate this to your code)
----> 2 parrot = parrot(model_tag=""prithivida/parrot_paraphraser_on_t5"", use_gpu=false)

file ~/.local/lib/python3.10/site-packages/parrot/parrot.py:10, in parrot.__init__(self, model_tag, use_gpu)
      8 from parrot.filters import fluency
      9 from parrot.filters import diversity
---> 10 self.tokenizer = autotokenizer.from_pretrained(model_tag, use_auth_token=true)
     11 self.model     = automodelforseq2seqlm.from_pretrained(model_tag, use_auth_token=true)
     12 self.adequacy_score = adequacy()

file ~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:560, in autotokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    557     return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
    559 # next, let's try to use the tokenizer_config file to get the tokenizer class.
--> 560 tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
    561 if ""_commit_hash"" in tokenizer_config:
    562     kwargs[""_commit_hash""] = tokenizer_config[""_commit_hash""]

file ~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:412, in get_tokenizer_config(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)
    353 """"""
    354 loads the tokenizer configuration from a pretrained model tokenizer configuration.
    355 
   (...)
    409 tokenizer_config = get_tokenizer_config(""tokenizer-test"")
    410 ```""""""
    411 commit_hash = kwargs.get(""_commit_hash"", none)
--> 412 resolved_config_file = cached_file(
    413     pretrained_model_name_or_path,
    414     tokenizer_config_file,
    415     cache_dir=cache_dir,
    416     force_download=force_download,
    417     resume_download=resume_download,
    418     proxies=proxies,
    419     use_auth_token=use_auth_token,
    420     revision=revision,
    421     local_files_only=local_files_only,
    422     _raise_exceptions_for_missing_entries=false,
    423     _raise_exceptions_for_connection_errors=false,
    424     _commit_hash=commit_hash,
    425 )
    426 if resolved_config_file is none:
    427     logger.info(""could not locate the tokenizer configuration file, will try to use the model config instead."")

file ~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:409, in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)
    406 user_agent = 
    407 try:
    408     # load from url or cache if already cached
--> 409     resolved_file = hf_hub_download(
    410         path_or_repo_id,
    411         filename,
    412         subfolder=none if len(subfolder) == 0 else subfolder,
    413         revision=revision,
    414         cache_dir=cache_dir,
    415         user_agent=user_agent,
    416         force_download=force_download,
    417         proxies=proxies,
    418         resume_download=resume_download,
    419         use_auth_token=use_auth_token,
    420         local_files_only=local_files_only,
    421     )
    423 except repositorynotfounderror:
    424     raise environmenterror(
    425         f""{path_or_repo_id} is not a local folder and is not a valid model identifier ""
    426         ""listed on ' this is a private repository, make sure to ""
    427         ""pass a token having permission to this repo with `use_auth_token` or log in with ""
    428         ""`huggingface-cli login` and pass `use_auth_token=true`.""
    429     )

file ~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:124, in validate_hf_hub_args.<locals>._inner_fn(*args, **kwargs)
    119 if check_use_auth_token:
    120     kwargs = smoothly_deprecate_use_auth_token(
    121         fn_name=fn.__name__, has_token=has_token, kwargs=kwargs
    122     )
--> 124 return fn(*args, **kwargs)

file ~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1052, in hf_hub_download(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)
   1048         return pointer_path
   1050 url = hf_hub_url(repo_id, filename, repo_type=repo_type, revision=revision)
-> 1052 headers = build_hf_headers(
   1053     token=token,
   1054     library_name=library_name,
   1055     library_version=library_version,
   1056     user_agent=user_agent,
   1057 )
   1059 url_to_download = url
   1060 etag = none

file ~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:124, in validate_hf_hub_args.<locals>._inner_fn(*args, **kwargs)
    119 if check_use_auth_token:
    120     kwargs = smoothly_deprecate_use_auth_token(
    121         fn_name=fn.__name__, has_token=has_token, kwargs=kwargs
    122     )
--> 124 return fn(*args, **kwargs)

file ~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_headers.py:117, in build_hf_headers(token, is_write_action, library_name, library_version, user_agent)
     44 """"""
     45 build headers dictionary to send in a hf hub call.
     46 
   (...)
    114         if `token=true` but token is not saved locally.
    115 """"""
    116 # get auth token to send
--> 117 token_to_send = get_token_to_send(token)
    118 _validate_token_to_send(token_to_send, is_write_action=is_write_action)
    120 # combine headers

file ~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_headers.py:149, in get_token_to_send(token)
    147 if token is true:
    148     if cached_token is none:
--> 149         raise environmenterror(
    150             ""token is required (`token=true`), but no token found. you""
    151             "" need to provide a token or be logged in to hugging face with""
    152             "" `huggingface-cli login` or `huggingface_hub.login`. see""
    153             "" 
    154         )
    155     return cached_token
    157 # case implicit use of the token is forbidden by env variable

oserror: token is required (`token=true`), but no token found. you need to provide a token or be logged in to hugging face with `huggingface-cli login` or `huggingface_hub.login`. see 

i have the secret token downloaded but not sure where to pass and how?
the stack trace after updating the token inside class parrot in ~/.local/lib/python3.10/site-packages/parrot/parrot.py
traceback (most recent call last):
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/pp.py"", line 8, in <module>
    parrot = parrot(model_tag=""prithivida/parrot_paraphraser_on_t5"", use_gpu=false)
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/parrot/parrot.py"", line 10, in __init__
    self.tokenizer = autotokenizer.from_pretrained(model_tag, use_auth_token=true)
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py"", line 560, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py"", line 412, in get_tokenizer_config
    resolved_config_file = cached_file(
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/transformers/utils/hub.py"", line 409, in cached_file
    resolved_file = hf_hub_download(
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py"", line 124, in _inner_fn
    return fn(*args, **kwargs)
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/huggingface_hub/file_download.py"", line 1052, in hf_hub_download
    headers = build_hf_headers(
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py"", line 124, in _inner_fn
    return fn(*args, **kwargs)
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/huggingface_hub/utils/_headers.py"", line 117, in build_hf_headers
    token_to_send = get_token_to_send(token)
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/huggingface_hub/utils/_headers.py"", line 149, in get_token_to_send
    raise environmenterror(
oserror: token is required (`token=true`), but no token found. you need to provide a token or be logged in to hugging face with `huggingface-cli login` or `huggingface_hub.login`. see","['python-3.x', 'pytorch', 'huggingface-transformers']",74646499,"use generate token from  and past it
install python lib huggingface_hub
pip install huggingface_hub


python -c ""from huggingface_hub.hf_api import hffolder; hffolder.save_token('your_token_here')""

if you are using notebooke
from huggingface_hub import notebook_login
notebook_login()

past your genrated token",https://stackoverflow.com/questions/74593644,python-3.x,27-11-2022 20:36,26706.0,7.0,2.0,True,06-07-2024 22:23,30-11-2022 05:51,Implementation Issues
53772907,no such file or directory &#39;nltk_data/corpora/stopwords/english&#39; when using colab,"first of all i am using google colab for the work and
i have downloaded nltk stopwords for english with following:
nltk.download('stopwords')

the download was successful
[nltk_data] downloading package stopwords to /root/nltk_data...

but when i run stop = stopwords.words('english')
i am getting oserror: no such file or directory: '/root/nltk_data/corpora/stopwords/english'","['python', 'nlp', 'nltk', 'google-colaboratory']",53774017,"tl;dr
the english should be in lowercase =)
see:  
in code
# downloads the data.
import nltk
nltk.download('stopwords')


# using the stopwords.
from nltk.corpus import stopwords

# initialize the stopwords
stoplist = stopwords.words('english')",https://stackoverflow.com/questions/53772907,python,14-12-2018 02:56,11404.0,5.0,2.0,True,07-09-2024 13:18,14-12-2018 05:36,Implementation Issues
54334304,spacy: can&#39;t find model &#39;en_core_web_sm&#39; on windows 10 and python 3.5.3 :: anaconda custom (64-bit),"what is the difference between spacy.load('en_core_web_sm') and spacy.load('en')? this link explains different model sizes. but i am still not clear how spacy.load('en_core_web_sm') and spacy.load('en') differ
spacy.load('en') runs fine for me. but the spacy.load('en_core_web_sm') throws error
i have installed spacyas below. when i go to jupyter notebook and run command nlp = spacy.load('en_core_web_sm') i get the below error
---------------------------------------------------------------------------
oserror                                   traceback (most recent call last)
<ipython-input-4-b472bef03043> in <module>()
      1 # import spacy and load the language library
      2 import spacy
----> 3 nlp = spacy.load('en_core_web_sm')
      4 
      5 # create a doc object

c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder\lib\site-packages\spacy\__init__.py in load(name, **overrides)
     13     if depr_path not in (true, false, none):
     14         deprecation_warning(warnings.w001.format(path=depr_path))
---> 15     return util.load_model(name, **overrides)
     16 
     17 

c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder\lib\site-packages\spacy\util.py in load_model(name, **overrides)
    117     elif hasattr(name, 'exists'):  # path or path-like to model data
    118         return load_model_from_path(name, **overrides)
--> 119     raise ioerror(errors.e050.format(name=name))
    120 
    121 

oserror: [e050] can't find model 'en_core_web_sm'. it doesn't seem to be a shortcut link, a python package or a valid path to a data directory.

how i installed spacy ---
(c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder) c:\users\nikhizzz>conda install -c conda-forge spacy
fetching package metadata .............
solving package specifications: .

package plan for installation in environment c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder:

the following new packages will be installed:

    blas:           1.0-mkl
    cymem:          1.31.2-py35h6538335_0    conda-forge
    dill:           0.2.8.2-py35_0           conda-forge
    msgpack-numpy:  0.4.4.2-py_0             conda-forge
    murmurhash:     0.28.0-py35h6538335_1000 conda-forge
    plac:           0.9.6-py_1               conda-forge
    preshed:        1.0.0-py35h6538335_0     conda-forge
    pyreadline:     2.1-py35_1000            conda-forge
    regex:          2017.11.09-py35_0        conda-forge
    spacy:          2.0.12-py35h830ac7b_0    conda-forge
    termcolor:      1.1.0-py_2               conda-forge
    thinc:          6.10.3-py35h830ac7b_2    conda-forge
    tqdm:           4.29.1-py_0              conda-forge
    ujson:          1.35-py35hfa6e2cd_1001   conda-forge

the following packages will be updated:

    msgpack-python: 0.4.8-py35_0                         --> 0.5.6-py35he980bc4_3 conda-forge

the following packages will be downgraded:

    freetype:       2.7-vc14_2               conda-forge --> 2.5.5-vc14_2

proceed ([y]/n)? y

blas-1.0-mkl.t 100% |###############################| time: 0:00:00   0.00  b/s
cymem-1.31.2-p 100% |###############################| time: 0:00:00   1.65 mb/s
msgpack-python 100% |###############################| time: 0:00:00   5.37 mb/s
murmurhash-0.2 100% |###############################| time: 0:00:00   1.49 mb/s
plac-0.9.6-py_ 100% |###############################| time: 0:00:00   0.00  b/s
pyreadline-2.1 100% |###############################| time: 0:00:00   4.62 mb/s
regex-2017.11. 100% |###############################| time: 0:00:00   3.31 mb/s
termcolor-1.1. 100% |###############################| time: 0:00:00 187.81 kb/s
tqdm-4.29.1-py 100% |###############################| time: 0:00:00   2.51 mb/s
ujson-1.35-py3 100% |###############################| time: 0:00:00   1.66 mb/s
dill-0.2.8.2-p 100% |###############################| time: 0:00:00   4.34 mb/s
msgpack-numpy- 100% |###############################| time: 0:00:00   0.00  b/s
preshed-1.0.0- 100% |###############################| time: 0:00:00   0.00  b/s
thinc-6.10.3-p 100% |###############################| time: 0:00:00   5.49 mb/s
spacy-2.0.12-p 100% |###############################| time: 0:00:10   7.42 mb/s

(c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder) c:\users\nikhizzz>python -v
python 3.5.3 :: anaconda custom (64-bit)

(c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder) c:\users\nikhizzz>python -m spacy download en
collecting en_core_web_sm==2.0.0 from 
  downloading  (37.4mb)
    100% |################################| 37.4mb ...
installing collected packages: en-core-web-sm
  running setup.py install for en-core-web-sm ... done
successfully installed en-core-web-sm-2.0.0

    linking successful
    c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder\lib\site-packages\en_core_web_sm
    -->
    c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder\lib\site-packages\spacy\data\en

    you can now load the model via spacy.load('en')


(c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder) c:\users\nikhizzz>","['python', 'python-3.x', 'nlp', 'spacy']",54409674,"the answer to your misunderstanding is a unix concept, softlinks which we could say that in windows are similar to shortcuts. let's explain this.
when you spacy download en, spacy tries to find the best small model that matches your spacy distribution. the small model that i am talking about defaults to en_core_web_sm which can be found in different variations which correspond to the different spacy versions (for example spacy, spacy-nightly have en_core_web_sm of different sizes).
when spacy finds the best model for you, it downloads it and then links the name en to the package it downloaded, e.g. en_core_web_sm. that basically means that whenever you refer to en you will be referring to en_core_web_sm. in other words, en after linking is not a ""real"" package, is just a name for en_core_web_sm.
however, it doesn't work the other way. you can't refer directly to en_core_web_sm because your system doesn't know you have it installed. when you did spacy download en you basically did a pip install. so pip knows that you have a package named en installed for your python distribution, but knows nothing about the package en_core_web_sm. this package is just replacing package en when you import it, which means that package en is just a softlink to en_core_web_sm.
of course, you can directly download en_core_web_sm, using the command: python -m spacy download en_core_web_sm, or you can even link the name en to other models as well. for example, you could do python -m spacy download en_core_web_lg and then python -m spacy link en_core_web_lg en. that would make
en a name for en_core_web_lg, which is a large spacy model for the english language.",https://stackoverflow.com/questions/54334304,python,23-01-2019 19:24,473947.0,178.0,34.0,True,15-03-2025 14:57,13-03-2025 21:33,Conceptual Questions
56161468,why is part-of-speech tag for adjectives &#39;jj&#39;?,"what is the etymology for jj tag denoting pos for adjectives?  i am unable to find any references online. there are several resources listing all the tags, but none describing the reason.",['nlp'],78987349,"i am a ""still-living colleague"" of henry kucera and others, maybe i should print that on a t-shirt! i worked on the lob corpus tagging, the british english equivalent/copy of the us brown corpus of american english.  our tagged lob corpus user manual gives some explnation, see section ""an
overview""  
""... the tags consist of a base, which is very often followed by 'suffixes' marking subclass and/or inflection ..."" base is first letter of tag: n for nouns, v for verbs, a for article/determiner, etc.  as ""a"" was taken, we used j for adjective, r for adverb.  appendix 4 list of tags 
lisrs the subclasses in lob tagset: jj, jjb, jjr, jjt, jnp.
admittedly this dos not explain ""double j"" - why not simply use tags j, jb, jr, jt, jnp? i think we decided all tags must have at least 2 letters, hence jj rather tha just j for standard adjective, and then variants attrbute-only comaprative and superlativ also started jj
eric atwell,",https://stackoverflow.com/questions/56161468,nlp,16-05-2019 05:35,1248.0,3.0,2.0,True,09-10-2024 16:22,02-06-2019 20:21,Conceptual Questions
77933640,how to calculate the weighted sum of last 4 hidden layers using roberta?,"the table from this paper that explains various approaches to obtain the embedding, i think these approaches are also applicable to roberta too:

i'm trying to calculate the weighted sum of last 4 hidden layers using roberta to obtain token embedding, but i don't know if this is the correct way to do, this is the code i have tried:
from transformers import robertatokenizer, robertamodel
import torch

tokenizer = robertatokenizer.from_pretrained('roberta-base')
model = robertamodel.from_pretrained('roberta-base')
caption = ['this is a yellow bird', 'example caption']

tokens = tokenizer(caption, return_tensors='pt', padding=true)

input_ids = tokens['input_ids']
attention_mask = tokens['attention_mask']

output = model(input_ids, attention_mask, output_hidden_states=true)

states = output.hidden_states
token_emb = torch.stack([states[i] for i in [-4, -3, -2, -1]]).sum(0).squeeze()","['python', 'machine-learning', 'deep-learning', 'nlp', 'huggingface-transformers']",77952168,"first, lets do some digging from the og bert code, 
if we just do a quick search for ""sum"" on the github repo, we find this 
  # the transformer performs sum residuals on all layers so the input needs
  # to be the same as the hidden size.
  if input_width != hidden_size:
    raise valueerror(""the width of the input tensor (%d) != hidden size (%d)"" %
                     (input_width, hidden_size))

then a quick search on stackoverflow reveals how to get intermediate layers' output of pre-trained bert model in huggingface transformers library?

now, lets validate if your code logic works by working backwards a little:
from transformers import robertatokenizer, robertamodel
import torch

tokenizer = robertatokenizer.from_pretrained('roberta-base')
model = robertamodel.from_pretrained('roberta-base')
caption = ['this is a yellow bird', 'example caption']

tokens = tokenizer(caption, return_tensors='pt', padding=true)

input_ids = tokens['input_ids']
attention_mask = tokens['attention_mask']

output = model(input_ids, attention_mask, output_hidden_states=true)

then:
>>>len(output.hidden_states)

out:
13

why 13?
12 encoder (hidden) layer output + final pooler output
import torchinfo
torchinfo.summary(model)

[out]:
================================================================================
layer (type:depth-idx)                                  param #
================================================================================
robertamodel                                            --
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertaembeddings: 1-1                                --
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½embedding: 2-1                                   38,603,520
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½embedding: 2-2                                   394,752
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½embedding: 2-3                                   768
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½layernorm: 2-4                                   1,536
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½dropout: 2-5                                     --
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertaencoder: 1-2                                   --
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½modulelist: 2-6                                  --
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-1                           7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½=                    7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-5                           7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-6                           7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-7                           7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-8                           7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-9                           7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-10                          7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-11                          7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-12                          7,087,872
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertapooler: 1-3                                    --
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½linear: 2-7                                      590,592
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½tanh: 2-8       =====================================

to validate that the last layer output is the last layer in the hidden_states:
assert(
  true for x in 
    torch.flatten(
      output[0] == output.hidden_states[-1]
    )
)

lets check if the size for each layer's output matches:
first_hidden_shape = output.hidden_states[0].shape

for x in output.hidden_states:
  assert x.shape == first_hidden_shape

checks out!
>>>first_hidden_shape

[out]:
torch.size([2, 7, 768])

why [2, 7, 768]?
it's (batch_size, sequence_length, hidden_size)

2 sentences = batch size of 2
7 longest sequence length = no. of tokens (i.e. 5 in the case of your longest example sentence + <s> and </s> from len(input_ids[0]))
768 outputs = fixed for all hidden layers output

bï¿½ï¿½ddu aï¿½ï¿½eins! (wait a minute!), does that mean i've sequence_length * 768 outputs for each batch? and if my batches are not equal lengths, the output size are different?
yes that is correct! and to get some sense of ""equality"" for all inputs, it'll be good to pad/truncate all outputs to a fixed length if you're still going to use the feature-based bert approaches.
soooo, is my torch.stack approach right?
yes, it seems so, but it depends on whether you consider the pooler output to be last or second to last.
if second to last:
torch.stack(output.hidden_states[-5:-1]).sum(0)

if you consider the pooler to be the last:
torch.stack(output.hidden_states[-4:]).sum(0)

minor nitpicking, you can access the output.hidden_states through slices because it's a tuple object. next you won't need to she stacked output because the the outer most layer tensor is non-empty.
this is a special case for stack, in nlp where the 1st dimension is batch size and 2nd is token length, so summing the hidden dimensions up ends up the same when you're not explicitly stating which dimension you stack.
to be a little more explicit:
# 2nd dimension is where our hidden states are 
# and that's where we want to do our sum too.
torch.stack(output.hidden_states[-4:], dim=2).sum(2)

but in practice, you can do this to comfort yourself:
assert(
 true for x in torch.flatten(
    torch.stack(output.hidden_states[-4:]).sum(0)
    == 
    torch.stack(output.hidden_states[-4:], dim=2).sum(2)
  )
)

interesting, what about ""concat last four hidden""?
in the case of concat you'll need to be explicit when in the dimensions
>>> torch.cat(output.hidden_states[-4:], dim=2).shape

[out]:
torch.size([2, 7, 3072])

but note, you still ends with sequence_length * hidden_size * 4, which makes batches with unequal lengths a pain.
since you've covered almost everything on the table, what about the ""embeddings"" output?
this is the interesting part, it's actually not accessible through the model(inputs_ids) directly, you'll need to do this:
model.embeddings(input_ids)

finally, why didn't you just answer ""yes, you are right""?
if i did, would that convince you more than you proving the above for yourself?",https://stackoverflow.com/questions/77933640,python,03-02-2024 20:34,329.0,2.0,1.0,True,07-02-2024 04:52,03-02-2024 20:52,Implementation Issues
71598259,typeerror: len() of unsized object,"i am trying random forest classifier from sklearn, when i want to print the classifier report, it is give me an error.
this was the code :
randomforestmodel = randomforestclassifier()
randomforestmodel.fit(train_vectors, data_train['label'])
predict_rfmodel = randomforestmodel.predict(test_vectors)

print(""classification with randomforest"")
print(metrics.classification_report(test_vectors, predict_rfmodel))

and the error was like this :
    classification with randomforest
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
<ipython-input-34-f976cec884e4> in <module>()
      1 print(""classification with randomforest"")
----> 2 print(metrics.classification_report(test_vectors, predict_rfmodel))

2 frames
/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py in classification_report(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)
   2108     """"""
   2109 
-> 2110     y_type, y_true, y_pred = _check_targets(y_true, y_pred)
   2111 
   2112     if labels is none:

/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py in _check_targets(y_true, y_pred)
     83     """"""
     84     check_consistent_length(y_true, y_pred)
---> 85     type_true = type_of_target(y_true)
     86     type_pred = type_of_target(y_pred)
     87 

/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py in type_of_target(y)
    308 
    309     # invalid inputs
--> 310     if y.ndim > 2 or (y.dtype == object and len(y) and not isinstance(y.flat[0], str)):
    311         return ""unknown""  # [[[1, 2]]] or [obj_1] and not [""label_1""]
    312 

typeerror: len() of unsized object","['scikit-learn', 'classification', 'random-forest', 'text-classification']",71601330,"you're providing the test instances features (test_vectors) instead of the true test instances labels to classification_report.
as per the documentation, the first parameter should be:

y_true: 1d array-like, or label indicator array / sparse matrix.
ground truth (correct) target values.",https://stackoverflow.com/questions/71598259,scikit-learn,24-03-2022 07:05,1612.0,1.0,1.0,True,24-03-2022 11:05,24-03-2022 07:28,Task-specific Help
75723102,how does gensim calculate sentence embeddings when using a pretrained fasttext model?,"according to this answer, sentence similarity for fasttext is calculated with one of two ways (depending if the embeddings are created superviser or unsupervised)

the mean of the normalized word vectors (unsupervised)
the mean of the word vectors (supervised)

but i cannot make either of those give the same answer as the sentence embedding
from gensim.models import fasttext
import numpy as np

wv = fasttext.load_facebook_vectors(""transtotag/cc.da.300.bin"")

w1 = wv[""til""]
norm_w1 = np.linalg.norm(wv[""til""], ord=2)
s1 = w1/norm_w1

w2 = wv[""skat""]
norm_w2 = np.linalg.norm(wv[""skat""], ord=2)
s2 = w2/norm_w2

w3 = wv[""til skat""]

# using ""raw"" embeddings
((w1+w2)/2-w3).max() #0.25
((w1+w2)-w3).max() # 0.5

# using normalized embeddings
((s1+s2)/2-w3).max() # 0.18
((s1+s2)-w3).max() # 0.37


i even tried to add the eos (as stated in the answer) aswell
nl = wv[""</s>""]
norm_nl = np.linalg.norm(wv[""</s>""],2)
snl = nl/norm_nl

w3 = wv[""til skat""]

((s1+s2+snl)/3-w3).max() #0.12

if we look in the source code, then wv[] just returns vstack([self.get_vector(key) for key in key_or_keys]) i.e it treats til skat a single word.
i cannot find anyting about how sentence embeddings are created in the docs aswell.","['gensim', 'word-embedding', 'fasttext']",75724874,"in gensim, you should use get_sentence_vector method, which was recently added.
please read the docs and notice that this method expects a list of words specified by string or int ids.",https://stackoverflow.com/questions/75723102,gensim,13-03-2023 14:19,906.0,0.0,2.0,True,17-03-2023 17:39,13-03-2023 14:36,Implementation Issues
1077600,converting words to numbers in php,"i am trying to convert numerical values written as words into integers. for example,
iphone has two hundred and thirty thousand seven hundred and eighty three apps

would become
iphone has 230783 apps

is there any library or function that does this?","php, nlp, numbers",1077651,"there are lots of pages discussing the conversion from numbers to words. not so many for the reverse direction. the best i could find was some pseudo-code on ask yahoo. see  for a nice algorithm:

well, overall you are doing two things: finding tokens (words that translates to numbers) and applying grammar. in short, you are building a parser for a very limited language.
the tokens you would need are: 
power: thousand, million, billion
  hundred: hundred
  ten: twenty, thirty... ninety
  unit: one, two, three, ... nine,
  special: ten, eleven, twelve, ... nineteen
(drop any ""and""s as they are meaningless. break hyphens into two tokens. that is sixty-five should be processed as ""sixty"" ""five"")
once you've tokenized your string, move from right to left.

grab all the tokens from the right until you hit a power or the whole string.
parse the tokens after the stop point for these patterns:
special
  ten
  unit
  ten unit
  unit hundred
  unit hundred special
  unit hundred ten
  unit hundred unit
  unit hundred ten unit
(this assumes that ""seventeen hundred"" is not allowed in this grammar)
this gives you the last three digits of your number.
if you stopped at the whole string you are done.
if you stopped at a power, start again at step 1 until you reach a higher power or the whole string.",https://stackoverflow.com/q/1077600,"php, nlp, numbers",03-07-2009 02:55,25206.0,22.0,6.0,True,10-12-2022 18:39,10-12-2022 18:39,Implementation Issues
75373129,openai gpt-3 api error: &quot;this model&#39;s maximum context length is 2049 tokens&quot;,"i have two issues relating to the response result from openai completion.
the following result doesn't return back the full text when i give a content of 500 words and prompt with ""fix grammar mistakes"" (is tokens issue?)

the second issue is when the text sometimes have some double quotes or single quotes. it messes with the json format. so i delete any type of quotes from the content (i am not sure if it's the best solution, but i may prefer doing it in javascript, not php).
curl_setopt($ch, curlopt_postfields, ""{\n  \""model\"": \""text-davinci-001\"",\n  \""prompt\"": \"""" . $open_ai_prompt  . "":nn"" . $content_text  . ""\"",\n  \""temperature\"": 0,\n  \""top_p\"": 1.0,\n  \""frequency_penalty\"": 0.0,\n  \""presence_penalty\"": 0.0\n}"");


""message"": ""we could not parse the json body of your request. (hint:
this likely means you aren't using your http library correctly. the
openai api expects a json payload, but what was sent was not valid
json.","['javascript', 'php', 'json', 'openai-api', 'gpt-3']",75373214,"regarding token limits
first of all, i think you don't understand how tokens work: 500 words is more than 500 tokens. use the tokenizer to calculate the number of tokens.
as stated in the official openai article:

depending on the model used, requests can use up to 4097 tokens shared
between prompt and completion. if your prompt is 4000 tokens, your
completion can be 97 tokens at most.
the limit is currently a technical limitation, but there are often
creative ways to solve problems within the limit, e.g. condensing your
prompt, breaking the text into smaller pieces, etc.

switch text-davinci-001 for a gpt-3 model because the token limits are higher.
gpt-3 models:


regarding double quotes in json
you can escape double quotes in json by using \ in front of double quotes like this:
""this is how you can escape \""double quotes\"" in json.""

but... this is more of a quick fix. for proper solution, see @adyson's comment above:

don't build your json by hand like that. make a php object / array
with the correct structure, and then use json_encode() to turn it into
valid json, it will automatically handle any escaping etc which is
needed, and you can also use the options to tweak certain things about
the output - check the php documentation.


edit 1
you need to set the max_tokens parameter higher. otherwise, the output will be shorter than your input. you will not get the whole fixed text back, but just a part of it.

edit 2
now you set the max_tokens parameter too high! if you set max_tokens = 5000, this is too much even for the most capable gpt-3 model (i.e., text-davinci-003). the prompt and the completion together can be 4097 tokens.
you can figure this out if you take a look at the error you got:
""error"": {""message"": ""this model's maximum context length is 4097 tokens, however you requested 6450 tokens (1450 in your prompt; 5000 for the completion). please reduce your prompt; or completion length.""}",https://stackoverflow.com/questions/75373129,javascript,07-02-2023 12:07,3087.0,0.0,2.0,True,10-06-2024 12:17,15-07-2023 22:21,Implementation Issues
74473271,attributeerror: &#39;numpy.float64&#39; object has no attribute &#39;cpu&#39;,"i am trying to run bert and train a model using pytorch.
i am not sure why i am getting this error after finishing the first epoch.
i am using this code link
history = defaultdict(list)
best_accuracy = 0

for epoch in range(epochs):
    
    # show details 
    print(f""epoch {epoch + 1}/{epochs}"")
    print(""-"" * 10)
    
    train_acc, train_loss = train_epoch(
        model,
        train_data_loader,
        loss_fn,
        optimizer,
        device,
        scheduler,
        len(df_train)
    )
    
    print(f""train loss {train_loss} accuracy {train_acc}"")
    
    # get model performance (accuracy and loss)
    val_acc, val_loss = eval_model(
        model,
        val_data_loader,
        loss_fn,
        device,
        len(df_val)
    )
    
    print(f""val   loss {val_loss} accuracy {val_acc}"")
    print()
    
    history['train_acc'].append(train_acc.cpu())
    history['train_loss'].append(train_loss.cpu())
    history['val_acc'].append(val_acc.cpu())
    history['val_loss'].append(val_loss.cpu())
    
    # if we beat prev performance
    if val_acc > best_accuracy:
        torch.save(model.state_dict(), 'best_model_state.bin')
        best_accuracy = val_acc

here is the output and the error message
image
it is a first time for me to work with pytorch. any ideas how to fix the error>","['python', 'pytorch', 'bert-language-model']",74473473,"i checked kaggle link and i see that there is no cpu() reference as you have posted in your code. it should simply be:
history['train_acc'].append(train_acc)
history['train_loss'].append(train_loss)
history['val_acc'].append(val_acc)
history['val_loss'].append(val_loss)",https://stackoverflow.com/questions/74473271,python,17-11-2022 09:47,1418.0,1.0,1.0,True,17-11-2022 10:01,17-11-2022 09:50,Implementation Issues
54323427,how to fill in the blank using bidirectional rnn and pytorch?,"i am trying to fill in the blank using a bidirectional rnn and pytorch. 
the input will be like: the dog is _____, but we are happy he is okay.
the output will be like: 
1. hyper (perplexity score here) 
2. sad (perplexity score here) 
3. scared (perplexity score here)

i discovered this idea here: 
import torch, torch.nn as nn
from torch.autograd import variable

text = ['bos', 'how', 'are', 'you', 'eos']
seq_len = len(text)
batch_size = 1
embedding_size = 1
hidden_size = 1
output_size = 1

random_input = variable(
    torch.floattensor(seq_len, batch_size, embedding_size).normal_(), requires_grad=false)

bi_rnn = torch.nn.rnn(
    input_size=embedding_size, hidden_size=hidden_size, num_layers=1, batch_first=false, bidirectional=true)

bi_output, bi_hidden = bi_rnn(random_input)

# stagger
forward_output, backward_output = bi_output[:-2, :, :hidden_size], bi_output[2:, :, hidden_size:]
staggered_output = torch.cat((forward_output, backward_output), dim=-1)

linear = nn.linear(hidden_size * 2, output_size)

# only predict on words
labels = random_input[1:-1]

# for language models, use cross-entropy :)
loss = nn.mseloss()
output = loss(linear(staggered_output), labels)

i am trying to reimplement the code above found at the bottom of the blog post. i am new to pytorch and nlp, and can't understand what the input and output to the code is.
question about the input: i am guessing the input are the few words that are given. why does one need beginning of sentence and end of sentence tags in this case? why don't i see the input being a corpus on which the model is trained like other classic nlp problems? i would like to use the enron email corpus to train the rnn.
question about the output: i see the output is a tensor. my understanding is the tensor is a vector, so maybe a word vector in this case. how can you use the tensor to output the words themselves?","['python', 'nlp', 'pytorch']",54719164,"as this question is rather open-ended i will start from the last parts, moving towards the more general answer to the main question posed in the title.
quick note: as pointed in the comments by @qusai alothman, you should find a better resource on the topic, this one is rather sparse when it comes to necessary informations.
additional note: full code for the process described in the last section would take way too much space to provide as an exact answer, it would be more of a blog post. i will highlight possible steps one should take to create such a network with helpful links as we go along.
final note: if there is anything dumb down there below (or you would like to expand the answer in any way or form, please do correct me/add info by posting a comment below).
question about the input
input here is generated from the random normal distribution and has no connection to the actual words. it is supposed to represent word embeddings, e.g. representation of words as numbers carrying semantic (this is important!) meaning (sometimes depending on the context as well (see one of the current state of the art approaches, e.g. bert)).
shape of the input
in your example it is provided as:
seq_len, batch_size, embedding_size,
where

seq_len - means length of a single sentence (varies across your
dataset), we will get to it later.
batch_size - how many sentences
should be processed in one step of forward pass (in case of
pytorch it is the forward method of class inheriting from
torch.nn.module)
embedding_size - vector with which one word is represented (it
might range from the usual 100/300 using word2vec up to 4096 or
so using the more recent approaches like the bert mentioned
above)

in this case it's all hard-coded of size one, which is not really useful for a newcomer, it only outlines the idea that way.
why does one need beginning of sentence and end of sentence tags in this case?
correct me if i'm wrong, but you don't need it if your input is separated into sentences. it is used if you provide multiple sentences to the model, and want to indicate unambiguously the beginning and end of each (used with models which depend on the previous/next sentences, it seems to not be the case here). those are encoded by special tokens (the ones which are not present in the entire corpus), so neural network ""could learn"" they represent end and beginning of sentence (one special token for this approach would be enough).
if you were to use serious dataset, i would advise to split your text using libraries like spacy or nltk (the first one is a pleasure to use imo), they do a really good job for this task.
you dataset might be already splitted into sentences, in those cases you are kind of ready to go.
why don't i see the input being a corpus on which the model is trained like other classic nlp problems?
i don't recall models being trained on the corpuses as is, e.g. using strings. usually those are represented by floating-points numbers using:

simple approaches, e.g. bag of
words or
tf-idf
more sophisticated ones, which provide some information about word
relationships (e.g. king is more semantically related to queen
than to a, say, banana). those were already linked above, some
other noticeable might be
glove or
elmo and tons of other creative
approaches.

question about the output
one should output indices into embeddings, which in turn correspond to words represented by a vector (more sophisticated approach mentioned above).
each row in such embedding represents a unique word and it's respective columns are their unique representations (in pytorch, first index might be reserved for the words for which a representation is unknown [if using pretrained embeddings], you may also delete those words, or represent them as aj average of sentence/document, there are some other viable approaches as well).
loss provided in the example
# for language models, use cross-entropy :)
loss = nn.mseloss()

for this task it makes no sense, as mean squared error is a regression metric, not a classification one.
we want to use one for classification, so softmax should be used for multiclass case (we should be outputting numbers spanning [0, n], where n is the number of unique words in our corpus).
pytorch's crossentropyloss already takes logits (output of last layer without activation like softmax) and returns loss value for each example. i would advise this approach as it's numerically stable (and i like it as the most minimal one).
i am trying to fill in the blank using a bidirectional rnn and pytorch
this is a long one, i will only highlight steps i would undertake in order to create a model whose idea represents the one outlined in the post.
basic preparation of dataset
you may use the one you mentioned above or start with something easier like 20 newsgroups from scikit-learn.
first steps should be roughly this:

scrape the metadata (if any) from your dataset (those might be html tags, some headers etc.)
split your text into sentences using a pre-made library (mentioned above)

next, you would like to create your target (e.g. words to be filled) in each sentence.
each word should be replaced by a special token (say <target-token>) and moved to target.
example:

sentence: neural networks can do some stuff.

would give us the following sentences and it's respective targets:

sentence: <target-token> networks can do some stuff. target: neural
sentence: neural <target-token> can do some stuff. target: networks
sentence: neural networks <target-token> do some stuff. target: can
sentence: neural networks can <target-token> some stuff. target: do
sentence: neural networks can do <target-token> stuff. target: some
sentence: neural networks can do some <target-token>. target: some
sentence: neural networks can do some stuff <target-token> target: .

you should adjust this approach to the problem at hand by correcting typos if there are any, tokenizing, lemmatizing and others, experiment!
embeddings
each word in each sentence should be replaced by an integer, which in turn points to it embedding.
i would advise you to use a pre-trained one. spacy provides word vectors, but another interesting approach i would highly recommend is in the open source library flair.
you may train your own, but it would take a lot of time + a lot of data for unsupervised training, and i think it is way beyond the scope of this question.
data batching
one should use pytorch's torch.utils.data.dataset and torch.utils.data.dataloader.
in my case, a good idea is was to provide custom collate_fn to dataloader, which is responsible for creating padded batches of data (or represented as torch.nn.utils.rnn.packedsequence already).
important: currently, you have to sort the batch by length (word-wise) and keep the indices able to ""unsort"" the batch into it's original form, you should remember that during implementation. you may use torch.sort for that task. in future versions of pytorch, there is a chance, one might not have to do that, see this issue.
oh, and remember to shuffle your dataset using dataloader, while we're at it.
model
you should create a proper model by inheriting from torch.nn.module. i would advise you to create a more general model, where you can provide pytorch's cells (like gru, lstm or rnn), multilayered and bidirectional (as is described in the post).
something along those lines when it comes to model construction:
import torch


class filler(torch.nn.module):
    def __init__(self, cell, embedding_words_count: int):
        self.cell = cell
        # we want to output vector of n
        self.linear = torch.nn.linear(self.cell.hidden_size, embedding_words_count)

    def forward(self, batch):
        # assuming batch was properly prepared before passing into the network
        output, _ = self.cell(batch)
        # batch shape[0] is the length of longest already padded sequence
        # batch shape[1] is the length of batch, e.g. 32
        # here we create a view, which allows us to concatenate bidirectional layers in general manner
        output = output.view(
            batch.shape[0],
            batch.shape[1],
            2 if self.cell.bidirectional else 1,
            self.cell.hidden_size,
        )

        # here outputs of bidirectional rnns are summed, you may concatenate it
        # it makes up for an easier implementation, and is another often used approach
        summed_bidirectional_output = output.sum(dim=2)
        # linear layer needs batch first, we have to permute it.
        # you may also try with batch_first=true in self.cell and prepare your batch that way
        # in such case no need to permute dimensions
        linear_input = summed_bidirectional_output.permute(1, 0, 2)
        return self.linear(embedding_words_count)

as you can see, information about shapes can be obtained in a general fashion. such approach will allow you to create a model with how many layers you want, bidirectional or not (batch_first argument is problematic, but you can get around it too in a general way, left it out for improved clarity), see below:
model = filler(
    torch.nn.gru(
        # size of your embeddings, for bert it could be 4096, for spacy's word2vec 300
        input_size=300,
        hidden_size=100,
        num_layers=3,
        batch_first=false,
        dropout=0.4,
        bidirectional=true,
    ),
    # how many unique words are there in your dataset
    embedding_words_count=10000,
)

you may pass torch.nn.embedding into your model (if pretrained and already filled), create it from numpy matrix or plethora of other approaches, it's highly dependent how your structure your code exactly. still, please, make your code more general, do not hardcode shapes unless it's totally necessary (usually it's not).
remember it's only a showcase, you will have to tune and fix it on your own.
this implementation returns logits and no softmax layer is used. if you wish to calculate perplexity, you may have to add it in order to obtain a correct probability distribution across all possible vectors.
btw: here is some info on concatenation of bidirectional output of rnn.
model training
i would highly recommend pytorch ignite as it's quite customizable, you can log a lot of info using it, perform validation and abstract cluttering parts like for loops in training.
oh, and split your model, training and others into separate modules, don't put everything into one unreadable file.
final notes
this is the outline of how i would approach this problem, you may have more fun using attention networks instead of merely using the last output layer as in this example, though you shouldn't start with that.
and please check pytorch's 1.0 documentation and do not follow blindly tutorials or blog posts you see online as they might be out of date really fast and quality of the code varies enormously. for example torch.autograd.variable is deprecated as can be seen in the link.",https://stackoverflow.com/questions/54323427,python,23-01-2019 09:00,2384.0,8.0,1.0,True,11-12-2021 09:40,13-02-2019 18:52,Implementation Issues
52047163,use natural language processing to to split bad &amp; good comments from an employee survey,"so bit of a long shot here, and i apologize for the lack of information. however, i'm struggling to even know where to look now. 
so i'm trying to split good and bad comments from a made-up survey of employees at a random company. all i have is a dataframe consisting of the comment an employee has made along with their managers id code. the idea is to try and see how many good and/or bad comments are associated with a manager via their id.
import pandas as pd 
trial_text=pd.read_csv(""trial.csv"")
trial_text.head()

   managercode              comment
0        ab123  great place to work
1        ab123  need more training
2        ab123  hate working here
3        ab124  always late home
4        ab124  manager never listens

i've used nltk quite a lot for data sets that include a lot more information so anything nltk based won't be a problem. like i say, with what i have, ""google"" has far too much information that i don't know where to begin (or that is useful)! if there's anyone that might just have a suggestion that could put me on track that would be great!
thanks","['python', 'nlp', 'nltk']",52056089,"you need sentiment analysis. i don't think you will get amazing results with an off-the-shelf model though, because your responses are quite short and quite domain specific. in case you want to try anyway, here is an example of how to use the vader model with nltk:
from nltk.sentiment.vader import sentimentintensityanalyzer
sid = sentimentintensityanalyzer()
sid.polarity_scores('great place to work')
>>> {'neg': 0.0, 'neu': 0.423, 'pos': 0.577, 'compound': 0.6249}
sid.polarity_scores('manager never listens')
>>> {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}

as you can see, your mileage may vary.
if you have lots of responses (thousands), a more viable strategy would be to manually label a sample of e.g. a few tens to a few hundred and to train your own sentiment classifier. here are some good tutorials of how to do this with either nltk or sklearn",https://stackoverflow.com/questions/52047163,python,27-08-2018 21:23,384.0,0.0,3.0,True,20-09-2024 11:43,29-08-2018 07:23,Implementation Issues
61158024,typeerror: lemmatize() missing 1 required positional argument: &#39;word,"i have an array of for each row in a csv file as followed:
[['thxx'], ['too', 'late', 'now', 'dumbass'], ['you', 'ï¿½ï¿½ï¿½', 're', 'so', 'dumb', '?', '?'], ['thxxx'], ['i', 'ï¿½ï¿½ï¿½', 'd', 'be', 'fucked']]

when i try to pass this on to the lemmatizer like this:
from nltk.stem import wordnetlemmatizer
lemmatized_words = [wordnetlemmatizer.lemmatize(word) for word in tokened_text]
print(lemmatized_words)

i get the following error:
typeerror: lemmatize() missing 1 required positional argument: 'word'

why is that?
as a side question: do i need to do this before passing this for vectorization? i am building an machine learning model and saw the function countvectorizer in sci kit learn but could not find any information that it does lemmatization and so on beforehand a","['machine-learning', 'nlp', 'nltk']",61158876,"there are some things wrong in your code:

wordnetlemmatizer is a class, you need to instanciate it first 
tokened_text is a nested list, hence you need a nested list-comprehension to preserve the structure. also lemmatize is expecting a string.

here's how you could do this:
from nltk.stem import wordnetlemmatizer

wnl = wordnetlemmatizer()

lemmatized_words = [[wnl.lemmatize(word) for word in l] for l in tokened_text]",https://stackoverflow.com/questions/61158024,machine-learning,11-04-2020 14:03,6963.0,1.0,4.0,True,11-08-2021 09:58,11-04-2020 14:08,Implementation Issues
77792137,how to fix the learning-rate for huggingface&#180;s trainer?,"i'm training model with the following parameters:
seq2seqtrainingarguments(
    output_dir                   = ""./out"", 
    overwrite_output_dir         = true,
    do_train                     = true,
    do_eval                      = true,
    
    per_device_train_batch_size  = 2, 
    gradient_accumulation_steps  = 4,
    per_device_eval_batch_size   = 8, 
    
    learning_rate                = 1.25e-5,
    warmup_steps                 = 1,
    
    save_total_limit             = 1,
       
    evaluation_strategy          = ""epoch"",
    save_strategy                = ""epoch"",
    logging_strategy             = ""epoch"",  
    num_train_epochs             = 5,   
    
    gradient_checkpointing       = true,
    fp16                         = true,    
        
    predict_with_generate        = true,
    generation_max_length        = 225,
          
    report_to                    = [""tensorboard""],
    load_best_model_at_end       = true,
    metric_for_best_model        = ""wer"",
    greater_is_better            = false,
    push_to_hub                  = false,
)

i assume that warmup_steps=1 fixes the learning rate.
however, after finished training i'm looking on the file trainer_state.json, and it seems that the learning rate is not fixed.
here are the values of learning_rate and step:
learning_rate,     steps
1.0006 e-05       1033
7.5062 e-06       2066
5.0058 e-06       3099
2.5053 e-06       4132
7.2618 e-09       5165

it seems that the learning rate is not fixed on 1.25e-5 (after step 1). what am i missing? how to i fix the learning rate.","['machine-learning', 'deep-learning', 'huggingface-transformers', 'huggingface-trainer', 'learning-rate']",77793731,"a warm-up is in general an increase of the learning rate. it starts at 0 and then increases linearly over 1(here) step to the specified learning rate of 1.25e-5.
afterwards by default a linear (in other cases a cosine) learning-rate scheduler decays your learning-rate.
to disable the decay add lr_scheduler_type='constant'.
if i recall correctly, this also disables the warmup.
if you want warmup and afterwards a constant rate use constant_with_warmup instead.
edit: valid scheduler types are defined in trainer_utils.py, in the class schedulertype:
class schedulertype(explicitenum):
    """"""
    scheduler names for the parameter `lr_scheduler_type` in [`trainingarguments`].
    by default, it uses ""linear"". internally, this retrieves `get_linear_schedule_with_warmup` scheduler from [`trainer`].
    scheduler types:
       - ""linear"" = get_linear_schedule_with_warmup
       - ""cosine"" = get_cosine_schedule_with_warmup
       - ""cosine_with_restarts"" = get_cosine_with_hard_restarts_schedule_with_warmup
       - ""polynomial"" = get_polynomial_decay_schedule_with_warmup
       - ""constant"" =  get_constant_schedule
       - ""constant_with_warmup"" = get_constant_schedule_with_warmup
       - ""inverse_sqrt"" = get_inverse_sqrt_schedule
       - ""reduce_lr_on_plateau"" = get_reduce_on_plateau_schedule
       - ""cosine_with_min_lr"" = get_cosine_with_min_lr_schedule_with_warmup
       - ""warmup_stable_decay"" = get_wsd_schedule
    """"""

    linear = ""linear""
    cosine = ""cosine""
    cosine_with_restarts = ""cosine_with_restarts""
    polynomial = ""polynomial""
    constant = ""constant""
    constant_with_warmup = ""constant_with_warmup""
    inverse_sqrt = ""inverse_sqrt""
    reduce_on_plateau = ""reduce_lr_on_plateau""
    cosine_with_min_lr = ""cosine_with_min_lr""
    warmup_stable_decay = ""warmup_stable_decay""",https://stackoverflow.com/questions/77792137,machine-learning,10-01-2024 09:14,4614.0,1.0,2.0,True,16-02-2025 22:20,11-06-2024 07:42,Implementation Issues
68080447,error calling adapt in textvectorization keras,"i have the following code, with a custom standardization definition.
def custom_standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    regex = tf.strings.regex_replace(lowercase, r'[^\w]', ' ')
    regex = tf.strings.regex_replace(regex, ' +', ' ')

    return tf.strings.split(regex)

vectorize_layer = tf.keras.layers.experimental.preprocessing.textvectorization(
    standardize=custom_standardization,
    max_tokens=50000,
    output_mode=""int"",
    output_sequence_length=100,
)

but when i call adapt, like this, i got the next error
vectorize_layer.adapt(['the cat'])
# error:
invalidargumenterror: expected 'tf.tensor(false, shape=(), dtype=bool)' to be true. summarized data: b'the given axis (axis = 2) is not squeezable!'

according to their explication,

when using a custom callable for split, the data received by the callable will have the 1st dimension squeezed out - instead of [[""string to split""], [""another string to split""]], the callable will see [""string to split"", ""another string to split""]. the callable should return a tensor with the first dimension containing the split tokens - in this example, we should see something like [[""string"", ""to"", ""split""], [""another"", ""string"", ""to"", ""split""]]. this makes the callable site natively compatible with tf.strings.split().

blockquote source
but i can't see where the error is
edit: i've done some research in my code
when i pass an array like ['the other day was raining', 'please call me later'], the function custom_standardization() returns something like this
[['the', 'other', 'day', 'was', 'raining'], ['pleasse', 'call', 'me', 'later']]

so it seems that it is not respecting to have same shape. why it changes thought?","['python', 'tensorflow', 'keras', 'nlp']",68944712,"i referred the document you shared earlier. following was mentioned for custom standardize

when using a custom callable for standardize, the data received by
the callable will be exactly as passed to this layer. the callable
should return a tensor of the same shape as the input.

so i changed replaced the return tf.strings.split(regex) with return regex (as splitting is changing the shape here). please try like this.
import tensorflow as tf

def custom_standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    regex = tf.strings.regex_replace(lowercase, r'[^\w]', ' ')
    regex = tf.strings.regex_replace(regex, ' +', ' ')

    return regex

vectorize_layer = tf.keras.layers.experimental.preprocessing.textvectorization(
    standardize=custom_standardization,
    max_tokens=50000,
    output_mode=""int"",
    output_sequence_length=100,
)

#checking input shape and output shape are shape or not 
input = tf.constant([[""foo !  @ qux  #bar""], [""qux baz""]])
print(input)
print(custom_standardization(input))

vectorize_layer.adapt([""foo qux bar""])

providing gist for reference.",https://stackoverflow.com/questions/68080447,python,22-06-2021 08:51,89.0,0.0,1.0,True,09-09-2021 21:25,22-06-2021 09:09,Tool Setup/Errors
76187256,"importerror: urllib3 v2.0 only supports openssl 1.1.1+, currently the &#39;ssl&#39; module is compiled with libressl 2.8.3","after pip install openai, when i try to import openai, it shows this error:

the 'ssl' module of urllib3 is compile with libressl not openssl

i just followed a tutorial on a project about using api of openai. but when i get to the first step which is the install and import openai, i got stuck. and i tried to find the solution for this error but i found nothing.
here is the message after i try to import openai:
python 3.9.6 (default, mar 10 2023, 20:16:38)
[clang 14.0.3 (clang-1403.0.22.14.1)] on darwin
type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

>>> import openai

traceback (most recent call last):
  file ""<stdin>"", line 1, in <module>
  file ""/users/yule/library/python/3.9/lib/python/site-packages/openai/__init__.py"", line 19, in <module>
    from openai.api_resources import (
  file ""/users/mic/library/python/3.9/lib/python/site-packages/openai/api_resources/__init__.py"", line 1, in <module>
    from openai.api_resources.audio import audio  # noqa: f401
  file ""/users/mic/library/python/3.9/lib/python/site-packages/openai/api_resources/audio.py"", line 4, in <module>
    from openai import api_requestor, util
  file ""/users/mic/library/python/3.9/lib/python/site-packages/openai/api_requestor.py"", line 22, in <module>
    import requests
  file ""/users/mic/library/python/3.9/lib/python/site-packages/requests/__init__.py"", line 43, in <module>
    import urllib3
  file ""/users/mic/library/python/3.9/lib/python/site-packages/urllib3/__init__.py"", line 38, in <module>
    raise importerror(
importerror: urllib3 v2.0 only supports openssl 1.1.1+, currently the 'ssl' module is compiled with libressl 2.8.3. see: 

i tried to --upgrade the urllib3, but it is still not working. the result is:
pip3 install --upgrade urllib3
defaulting to user installation because normal site-packages is not writeable
requirement already satisfied: urllib3 in ./library/python/3.9/lib/python/site-packages (2.0.2)","['python', 'openai-api', 'urllib3']",76187415,"the reason why the error message mentioned openssl 1.1.1+ and libressl 2.8.3 is that urllib3 v2.0 (the version you've installed) requires openssl 1.1.1+ to work properly, as it relies on some new features of openssl 1.1.1.
the issue is that the version of the 'ssl' module that is currently installed in your environment is compiled with libressl 2.8.3, which is not compatible with urllib3 v2.0.
to use urllib3 v2.0, you need an 'ssl' module compiled with openssl 1.1.1 or later, by trying:
brew install openssl@1.1

or you could use an older version of urllib3 that is compatible suc. for example urllib3 v1.26.6, which does not have a strict openssl version requirement.
you can force the version installing with this command:
pip install urllib3==1.26.6",https://stackoverflow.com/questions/76187256,python,06-05-2023 05:11,414804.0,175.0,19.0,True,05-12-2024 12:05,14-08-2023 00:20,Tool Setup/Errors
70069026,how to use files in the answer api of openai,"as finally openai opened the gpt-3 related api publicly,
i am playing with it to explore and discover his potential.
i am trying the answer api, the simple example that is in the documentation:

i upload the .jsonl file as indicated, and i can see it succesfully uploaded with the openai.file.list() api.
when i try to use it, unfortunately, i always get the same error:
>>> openai.file.create(purpose='answers', file=open('example.jsonl') )
<file file id=file-xxx at 0x7fbc9eca5e00> json: {
  ""bytes"": 140,
  ""created_at"": 1637597242,
  ""filename"": ""example.jsonl"",
  ""id"": ""file-xxx"",
  ""object"": ""file"",
  ""purpose"": ""answers"",
  ""status"": ""uploaded"",
  ""status_details"": null
}

#use the file in the api:
openai.answer.create(
    search_model=""ada"", 
    model=""curie"", 
    question=""which puppy is happy?"", 
    file=""file-xxx"", 
    examples_context=""in 2017, u.s. life expectancy was 78.6 years."", 
    examples=[[""what is human life expectancy in the united states?"", ""78 years.""]], 
    max_rerank=10,
    max_tokens=5,
    stop=[""\n"", ""<|endoftext|>""]
)
<some exception, then>
openai.error.invalidrequesterror: file is still processing.  check back later.


i have waited several hours, and i do not think this content deserve such a long wait...
do you know if it is a normal behaviour, or if i miss something?
thanks","['python', 'openai-api', 'gpt-3']",70157536,"after a few hours (the day after) the file metadata status changed from uploaded to processed and the file could be used in the answer api as stated in the documentation.
i think this need to be better documented in the original openai api reference.",https://stackoverflow.com/questions/70069026,python,22-11-2021 16:19,6853.0,4.0,1.0,True,15-01-2023 17:50,15-01-2023 17:50,Implementation Issues
77373522,how does an instance of pytorch&#39;s `nn.linear()` process a tuple of tensors?,"in the annotated transformer's implementation of multi-head attention, three tensors (query, key, value) are all passed to a nn.linear(d_model, d_model):
# some class definition ...
self.linears = clones(nn.linear(d_model, d_model), 4) # deep-copied list of nn.linear-modules concatenated via nn.modulelist
# more code ...
query, key, value = [
  lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
  for lin, x in zip(self.linears, (query, key, value))
]

my question: what happens at lin(x), when an instance of nn.linear() is called on the tuple (query, key, value)? is the tuple somehow concatenated to a tensor? if so, how - on which dimension are the tensors concatenated?","['python', 'machine-learning', 'pytorch', 'nlp', 'transformer-model']",77373814,"self.linears = clones(nn.linear(d_model, d_model), 4) # deep-copied list of nn.linear-modules concatenated via nn.modulelist
# more code ...
query, key, value = [
  lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
  for lin, x in zip(self.linears, (query, key, value))
]

actually, the nn.linear does not process input as a tuple of a q,k,v.
in your code, the result similar like this
out_q = self.linears[0](q)
out_k = self.linears[1](k)
out_v = self.linears[2](v)

when you use zip(iterable a, iterable b)
so you will get the pairs (a[0], b[0]) (a[1], b[1]) ,... independently
or more specific
query = self.linears[0](query)
key = self.linears[1](key)
value = self.linears[2](value)",https://stackoverflow.com/questions/77373522,python,27-10-2023 10:43,178.0,0.0,1.0,True,27-10-2023 13:33,27-10-2023 13:33,Implementation Issues
70051704,efficient way to find an approximate string match and replacing with predefined string,"i need to build a ner system (named entity recognition). for simplicity, i am doing it by using approximate string matching as input can contain typos and other minor modifications. i have come across some great libraries like: fuzzywuzzy or even faster rapidfuzz. but unfortunately i didn't find a way to return the position where the match occurs. as, for my purpose i not only need to find the match, but also i need to know where the match happened. as for ner, i need to replace those matches with some predefined string.
for example, if any one of the line is found in input string i want to replace them with the string company_name:
google
microsoft
facebook
international business machine

like, input: s/he works at google will be transformed to s/he works at company_name.
you can safely assume that, all the input and the pattern to match are already preprocessed and most importantly they are in lower-case now. so, there is no problem with case-sensitivity.
currently, i have approached with a sliding window technique. and a sliding window is passed over the input string from left to right and this window has exactly the size of the pattern we want to match. for example, when i want to match with international business machine, i run a sliding window of size 3 from left to right and try to find the best match by observing each 3 consecutive tokens at the same time with a stride of 1. i do believe, it is not the best way to do it, also it cannot find the best match.
so, what is the efficient way to find the best possible match along with the quantification on the found match (how much they are similar) and the position of the match(es), such that we can replace them with a given fixed string (if the calculated similarity is not less than a threshold)? obviously, a single input may contain multiple portions to be replaced, each of them will be replaced separately, like: google and microsoft are big companies will become company_name and company_name are big companies etc.
edit: fixed link to rapidfuzz","['python', 'nlp', 'named-entity-recognition', 'fuzzy-search', 'fuzzywuzzy']",70091430,"it seems modules fuzzywuzzy and rapidfuzz don't have function for this. you could try to use process.extract() or process.extractone() but it would need to split text in smaller parts (ie. words) and check every part separatelly. for longer words like international business machine it would need to split in part with 3 words - so it would need even more work.

i think you need rather module fuzzysearch
import fuzzysearch

words = ['google', 'microsoft', 'facebook', 'international business machine']

text = 'google and microsoft are big companies like international business machine'

print(' text:', text)
print('---')
    
for word in sorted(words, key=len, reverse=true):
    print(' word:', word)
    
    results = fuzzysearch.find_near_matches(word, text, max_l_dist=1)
    print('found:', results)
    
    for item in reversed(results):
        text = text[:item.start] + 'company' + text[item.end:]
    print(' text:', text)
    
    print('---')

result:
 text: google and microsoft are big companies like facebook international business machine
---
 word: international business machine
found: [match(start=53, end=83, dist=0, matched='international business machine')]
 text: google and microsoft are big companies like facebook company
---
 word: microsoft
found: [match(start=11, end=20, dist=1, matched='microsoft')]
 text: google and company are big companies like facebook company
---
 word: facebook
found: [match(start=42, end=50, dist=0, matched='facebook')]
 text: google and company are big companies like company company
---
 word: google
found: [match(start=0, end=6, dist=1, matched='google')]
 text: company and company are big companies like company company

if it finds many results for one word then it is better to start replacing at last position to keep other words in the same place. and this is why i use reversed().
i would start also with the longest word/name so later it still can search shorter words like business. and this is why i use sorted(..., key=len, reverse=true)
but i'm not sure if it works exactly as you want. maybe it will have problem when words are more incorrect.

edit:
i tried to use fuzzywuzzy for this and created this version but only for names with single word. for international business machine it would need some other idea.
it split full text into words and compare words. later replace word wich have ration > 80
words = ['google', 'microsoft', 'facebook', 'international business machine']

text = 'google and microsoft are big companies like international business machine'

# ---

import fuzzywuzzy.fuzz as fuzz
#import fuzzywuzzy.process

new_words = []

for part in text.split():

    matches = []

    for word in words:
        result = fuzz.token_sort_ratio(part, word)
        matches.append([result, part, word])
        #print([result, part, word])

    matches = sorted(matches, reverse=true)

    if matches and matches[0][0] > 80:
        new_words.append('company')
    else:
        new_words.append(matches[0][1])
        
print("" "".join(new_words))

result:
[100, 'google', 'google']
[27, 'google', 'microsoft']
[29, 'google', 'facebook']
[17, 'google', 'international business machine']
[0, 'and', 'google']
[0, 'and', 'microsoft']
[18, 'and', 'facebook']
[12, 'and', 'international business machine']
[27, 'microsoft', 'google']
[100, 'microsoft', 'microsoft']
[35, 'microsoft', 'facebook']
[15, 'microsoft', 'international business machine']
[22, 'are', 'google']
[17, 'are', 'microsoft']
[36, 'are', 'facebook']
[12, 'are', 'international business machine']
[22, 'big', 'google']
[17, 'big', 'microsoft']
[18, 'big', 'facebook']
[12, 'big', 'international business machine']
[27, 'companies', 'google']
[33, 'companies', 'microsoft']
[24, 'companies', 'facebook']
[26, 'companies', 'international business machine']
[40, 'like', 'google']
[15, 'like', 'microsoft']
[17, 'like', 'facebook']
[18, 'like', 'international business machine']
[21, 'international', 'google']
[27, 'international', 'microsoft']
[19, 'international', 'facebook']
[60, 'international', 'international business machine']
[14, 'business', 'google']
[24, 'business', 'microsoft']
[12, 'business', 'facebook']
[42, 'business', 'international business machine']
[15, 'machine', 'google']
[25, 'machine', 'microsoft']
[40, 'machine', 'facebook']
[38, 'machine', 'international business machine']
company and company are big companies like international business machine


edit:
second version which check also names with many words
all_names = ['google', 'microsoft', 'facebook', 'international business machine']

text = 'google and microsoft are big companies like international business machine'

# ---

import fuzzywuzzy.fuzz as fuzz


for name in all_names:

    length = len(name.split(' ')) # how many words has name 
    print('name length:', length, '|', name)

    words = text.split()  # split text into words

    # compare name with all words in text
    
    matches = []
    
    for index in range(0, len(words)-length+1):
        # join words if name has more then 1 word
        part = "" "".join(words[index:index+length])
        #print('part:', part)
        
        result = fuzz.token_sort_ratio(part, name)
        matches.append([result, name, part, [index, index+length]])

        print([result, name, part, [index, index+length]])
        
    # reverse to start at last position
    matches = list(reversed(matches))

    max_match = max(x[0] for x in matches)
    print('max match:', max_match)

    # replace
    if max_match > 80:
        for match in matches:
            if  match[0] == max_match:
                idx = match[3]  
                words = words[:idx[0]] + ['company'] + words[idx[1]:]

    text = "" "".join(words)
    print('text:', text)
    print('---')

result:
ame length: 1 | google
[100, 'google', 'google', [0, 1]]
[0, 'google', 'and', [1, 2]]
[27, 'google', 'microsoft', [2, 3]]
[22, 'google', 'are', [3, 4]]
[22, 'google', 'big', [4, 5]]
[27, 'google', 'companies', [5, 6]]
[40, 'google', 'like', [6, 7]]
[21, 'google', 'international', [7, 8]]
[14, 'google', 'business', [8, 9]]
[15, 'google', 'machine', [9, 10]]
max match: 100
text: company and microsoft are big companies like international business machine
---
name length: 1 | microsoft
[25, 'microsoft', 'company', [0, 1]]
[0, 'microsoft', 'and', [1, 2]]
[100, 'microsoft', 'microsoft', [2, 3]]
[17, 'microsoft', 'are', [3, 4]]
[17, 'microsoft', 'big', [4, 5]]
[33, 'microsoft', 'companies', [5, 6]]
[15, 'microsoft', 'like', [6, 7]]
[27, 'microsoft', 'international', [7, 8]]
[24, 'microsoft', 'business', [8, 9]]
[25, 'microsoft', 'machine', [9, 10]]
max match: 100
text: company and company are big companies like international business machine
---
name length: 1 | facebook
[27, 'facebook', 'company', [0, 1]]
[18, 'facebook', 'and', [1, 2]]
[27, 'facebook', 'company', [2, 3]]
[36, 'facebook', 'are', [3, 4]]
[18, 'facebook', 'big', [4, 5]]
[24, 'facebook', 'companies', [5, 6]]
[17, 'facebook', 'like', [6, 7]]
[19, 'facebook', 'international', [7, 8]]
[12, 'facebook', 'business', [8, 9]]
[40, 'facebook', 'machine', [9, 10]]
max match: 40
text: company and company are big companies like international business machine
---
name length: 3 | international business machine
[33, 'international business machine', 'company and company', [0, 3]]
[31, 'international business machine', 'and company are', [1, 4]]
[31, 'international business machine', 'company are big', [2, 5]]
[34, 'international business machine', 'are big companies', [3, 6]]
[38, 'international business machine', 'big companies like', [4, 7]]
[69, 'international business machine', 'companies like international', [5, 8]]
[88, 'international business machine', 'like international business', [6, 9]]
[100, 'international business machine', 'international business machine', [7, 10]]
max match: 100
text: company and company are big companies like company


edit:
version with fuzzywuzzy.process
this time i don't have positions and i simply use standard text.replace(item[0], 'company').
i think in most situations it will work correctly and it doesn't need better method.
this time i check it on text with mistakes:
'gogle and mikro-soft are big companies like fasebok and internat. businnes machin'


all_names = ['google', 'microsoft', 'facebook', 'international business machine']

text = 'google and microsoft are big companies like facebook and international business machine'

# text with mistakes
text = 'gogle and mikro-soft are big companies like fasebok and internat. businnes machin'

# ---

import fuzzywuzzy.process
#import fuzzywuzzy.fuzz

for name in sorted(all_names, key=len, reverse=true):
    lenght = len(name.split())

    words = text.split()
    words = ["" "".join(words[i:i+lenght]) for i in range(0, len(words)-lenght+1)]
    #print(words)

    #result = fuzzywuzzy.process.extractbests(name, words, scorer=fuzzywuzzy.fuzz.token_sort_ratio, score_cutoff=80)
    result = fuzzywuzzy.process.extractbests(name, words, score_cutoff=80)
    print(name, result)

    for item in result:
        text = text.replace(item[0], 'company')

print(text)",https://stackoverflow.com/questions/70051704,python,21-11-2021 03:54,7532.0,1.0,1.0,True,10-10-2023 12:37,10-10-2023 12:37,Conceptual Questions
75176667,openai gpt-3 api error: &quot;cannot specify both model and engine&quot;,"so i'm working on some python code that works with chatgpt3. what it does is it sends a request with a prompt and then gets the reply, but i keep getting errors. the error is
traceback (most recent call last):
  file ""main.py"", line 16, in <module>
    print(response_json['choices'][0]['text'])
keyerror: 'choices'

here is my code:
import json
import requests
import os
data = {
    ""prompt"": ""what is the meaning of life?"",
    ""model"": ""text-davinci-002""
}

response = requests.post("" json=data, headers={
    ""content-type"": ""application/json"",
    ""authorization"": f""bearer {apikey}"",
})

response_json = json.loads(response.text)

print(response_json['choices'][0]['text'])


i do have an api key that is valid and the json code i don't get the json code.
{'error': {'message': 'cannot specify both model and engine', 'type': 'invalid_request_error', 'param': none, 'code': none}}

i have tried different api keys and that didn't work. i even looked up all the different models for chatgpt and it still doesn't work","['python', 'json', 'python-3.x', 'openai-api', 'gpt-3']",75182746,"all engines api endpoints are deprecated.

change the url from this...


...to this.


if you run test.py the openai api will return a completion. you'll get a different completion because the temperature parameter is not set to 0. i got the following completion:

the meaning of life is to find out and fulfil the purpose and meaning...

test.py
import json
import requests
import os

data = {
    ""prompt"": ""what is the meaning of life?"",
    ""model"": ""text-davinci-003""
}

response = requests.post("" json=data, headers={
    ""content-type"": ""application/json"",
    ""authorization"": f""bearer {apikey}""
})

response_json = json.loads(response.text)

print(response_json[""choices""][0][""text""])",https://stackoverflow.com/questions/75176667,python,19-01-2023 18:19,8006.0,1.0,2.0,True,12-08-2023 21:49,13-03-2023 13:49,Conceptual Questions
9538425,"is there a database, api, or parsable text for getting verb conjugations?","this isn't directly a programming question, so i apologize in advance. i've been working on a grammar-free random sentence generator for a typing game i'd like to make, and i've been having a difficult time finding any parsable (or callable) data for getting verb conjugations. ultimately, if i can't find anything like this, i'm going to have to go through the dictionary i've created and add first-person singular and plural, second-person singular and plural, third-person singular and plural, simple past, past participle, and present participle forms for every irregular verb.
this wouldn't be a problem in many languages, but there are so many irregular english verbs that this could take a long, long time to do manually. i'm not against the worse option, but i want to make sure i'm not going to be wasting obscene hours doing it myself when there is some database i can use instead.
i've seen  and spoken with the creator, but he doesn't release his exact dictionary (just the classes for it). i've also seen sites like  which would be great for scraping, but that's a bit of a pain as well.
this question has been asked here before ( verb conjugations database ), but the question was left unanswered, and the asker alluded to solving the problem but never said what the solution was.",['nlp'],9543779,"morphadorner (java) has a simple verb conjugator (with online demo).
but if you are interested with an exhaustive listing you can check lexical tools' inflection variants. after downloading lexical tools, you will be importing the data to your database server. then you can just query the database using their library (java).
simplenlg also has this feature, and is very much related to lexical tool.",https://stackoverflow.com/questions/9538425,nlp,02-03-2012 18:24,10023.0,13.0,1.0,True,15-01-2025 04:58,23-05-2017 12:33,Data Wrangling
77533488,nlp pre-processing on two columns in data frame gives error,"i have the following data frame:
gmedatedf.head(2)





title
score
id
url
comms_num
body
timestamp




it's not about the money, it's about sending a...
55.0
l6ulcx

6.0
nan
2021-01-28 21:37:41


math professor scott steiner says the numbers ...
110.0
l6uibd

23.0
nan
2021-01-28 21:32:10




i have the following function to pre-process the text (with the proper libraries imported and so on):
def preprocess_text(text):
  # tokenize words
  tokens = word_tokenize(text.lower())

  # remove stopwords and non-alphabetic words, and lemmatize
  processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]

  return processed_tokens

then calling it on specific column:
gmedatedf.loc[:, 'body'] = gmedatedf['body'].fillna('nan').astype(str)
gmedatedfprocessed = gmedatedf['body'].apply(preprocess_text)

that works properly as expected. however, when iï¿½ï¿½ï¿½try to do it on two columns, like so:
gmedatedf.loc[:, 'title','body'] = gmedatedf['title', 'body'].fillna('nan').astype(str)
gmedatedfprocessed = gmedatedf['title', 'body'].apply(preprocess_text)
>
iï¿½ï¿½ï¿½get the following error:
   3802                 return self._engine.get_loc(casted_key)
   3803             except keyerror as err:
-> 3804                 raise keyerror(key) from err
   3805             except typeerror:
   3806                 # if we have a listlike key, _check_indexing_error will raise

keyerror: ('title', 'body')

iï¿½ï¿½ï¿½ve looked around, asked chatgptï¿½ï¿½ï¿½for some help, but iï¿½ï¿½ï¿½canï¿½ï¿½ï¿½t figure it out.
please, bear with me as iï¿½ï¿½ï¿½m still learning the basics of python.
why canï¿½ï¿½ï¿½t iï¿½ï¿½ï¿½give it a listlike key? and why is it a listlike key only when iï¿½ï¿½ï¿½have the two columns? when i call it only on gmedatedf.loc[:, 'body'] it is some kind of listlike key, no? so why would it not work otherwise?
iï¿½ï¿½ï¿½m confused, and donï¿½ï¿½ï¿","['python', 'pandas', 'dataframe', 'nlp', 'nltk']",77538239,"as the error suggests, using gmedatedf['title', 'body'] attempts to find a column in the dataframe under the following key: ('title', 'body'). no column in your dataframe is called that, therefore the code fails.
if you wish to select multiple columns at once, you need to provide them in a list, like so: gmedatedf[['title', 'body']]. for more information, head to the documentation page on data selection from a dataframe.
given your specific example, you will need to fix the data selection, and then use some string vectorisation, something like:
gmedatedfprocessed[['title', 'body']] = gmedatedf[['title', 'body']].apply(lambda x: preprocess_text(x.str))",https://stackoverflow.com/questions/77533488,python,22-11-2023 22:54,131.0,0.0,1.0,True,23-11-2023 16:02,23-11-2023 08:33,Implementation Issues
22159351,handling same words but from different documents,"i'm making a python class which calculates the tfidf weight of each word in a document. now in my dataset i have 50 documents. in these documents many words intersect, thus having multiple same word features but with different tfidf weight. so the question is how do i sum up all the weights into one singular weight?","['python', 'machine-learning', 'text-classification', 'tf-idf']",22159481,"first, let's get some terminology clear.  a term is a word-like unit in a corpus.  a token is a term at a particular location in a particular document.  there can be multiple tokens that use the same term.  for example, in my answer, there are many tokens that use the term ""the"".  but there is only one term for ""the"".
i think you are a little bit confused.  tf-idf style weighting functions specify how to make a per term score out of the term's token frequency in a document and the background token document frequency in the corpus for each term in a document.  tf-idf converts a document into a mapping of terms to weights.  so more tokens sharing the same term in a document will increase the corresponding weight for the term, but there will only be one weight per term.  there is no separate score for tokens sharing a term inside the doc.",https://stackoverflow.com/questions/22159351,python,03-03-2014 22:58,794.0,0.0,1.0,True,04-01-2024 20:05,04-01-2024 20:05,Implementation Issues
5107371,document analysis and tagging,"let's say i have a bunch of essays (thousands) that i want to tag, categorize, etc.  ideally, i'd like to train something by manually categorizing/tagging a few hundred, and then let the thing loose.
what resources (books, blogs, languages) would you recommend for undertaking such a task?  part of me thinks this would be a good fit for a bayesian classifier or even latent semantic analysis, but i'm not really familiar with either other than what i've found from a few ruby gems.
can something like this be solved by a bayesian classifier?  should i be looking more at semantic analysis/natural language processing?  or, should i just be looking for keyword density and mapping from there?","machine-learning, nlp, classification, bayesian, tagging",5191602,"wow, that's a pretty huge topic you are venturing into :)
there is definitely a lot of books and articles you can read about it but i will try to provide a short introduction. i am not a big expert but i worked on some of this stuff.
first you need to decide whether you are want to classify essays into predefined topics/categories (classification problem) or you want the algorithm to decide on different groups on its own (clustering problem). from your description it appears you are interested in classification.
now, when doing classification, you first need to create enough training data. you need to have a number of essays that are separated into different groups. for example 5 physics essays, 5 chemistry essays, 5 programming essays and so on. generally you want as much training data as possible but how much is enough depends on specific algorithms. you also need verification data, which is basically similar to training data but completely separate. this data will be used to judge quality (or performance in math-speak) of your algorithm.
finally, the algorithms themselves. the two i am familiar with are bayes-based and tf-idf based. for bayes, i am currently developing something similar for myself in ruby, and i've documented my experiences in my blog. if you are interested, just read this -  and if you have any follow up questions i will try to answer.
the tf-idf is a short for termfrequence - inversedocumentfrequency. basically the idea is for any given document to find a number of documents in training set that are most similar to it, and then figure out it's category based on that. for example if document d is similar to t1 which is physics and t2 which is physics and t3 which is chemistry, you guess that d is most likely about physics and a little chemistry.
the way it's done is you apply the most importance to rare words and no importance to common words. for instance 'nuclei' is rare physics word, but 'work' is very common non-interesting word. (that's why it's called inverse term frequency). if you can work with java, there is a very very good lucene library which provides most of this stuff out of the box. look for api for 'similar documents' and look into how it is implemented. or just google for 'tf-idf' if you want to implement your own",https://stackoverflow.com/q/5107371,"machine-learning, nlp, classification, bayesian, tagging",24-02-2011 16:20,2054.0,2.0,2.0,True,27-02-2025 12:57,27-02-2025 12:57,Conceptual Questions
73257704,get contrastive_logits_per_image with flava model using huggingface library,"i have used a code of flava model from this link:


but i am getting the following error:
'flavamodeloutput' object has no attribute 'contrastive_logits_per_image'

i tried using flavaforpretraining model instead, so updated code was :
from pil import image
import requests
from transformers import flavaprocessor, flavaforpretraining

model = flavaforpretraining.from_pretrained(""facebook/flava-full"")
processor = flavaprocessor.from_pretrained(""facebook/flava-full"")

url = ""
image = image.open(requests.get(url, stream=true).raw)

inputs = processor(text=[""a photo of a cat""], images=image, return_tensors=""pt"", padding=true, return_codebook_pixels = true)

inputs.update(
    {
        ""input_ids_masked"": inputs.input_ids,
    }
)

outputs = model(**inputs)

logits_per_image = outputs.contrastive_logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities

but i'm still getting this as error:
/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py:714: futurewarning: the `device` argument is deprecated and will be removed in v5 of transformers.
  ""the `device` argument is deprecated and will be removed in v5 of transformers."", futurewarning

---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-44-bdb428b8184a> in <module>()
----> 1 outputs = model(**inputs)

2 frames
/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1128         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1129                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1130             return forward_call(*input, **kwargs)
   1131         # do not call functions when jit is used
   1132         full_backward_hooks, non_full_backward_hooks = [], []

/usr/local/lib/python3.7/dist-packages/transformers/models/flava/modeling_flava.py in forward(self, input_ids, input_ids_masked, pixel_values, codebook_pixel_values, attention_mask, token_type_ids, bool_masked_pos, position_ids, image_attention_mask, skip_unmasked_multimodal_encoder, mlm_labels, mim_labels, itm_labels, output_attentions, output_hidden_states, return_dict, return_loss)
   1968             if mim_labels is not none:
   1969                 mim_labels = self._resize_to_2d(mim_labels)
-> 1970                 bool_masked_pos = self._resize_to_2d(bool_masked_pos)
   1971                 mim_labels[bool_masked_pos.ne(true)] = self.ce_ignore_index
   1972 

/usr/local/lib/python3.7/dist-packages/transformers/models/flava/modeling_flava.py in _resize_to_2d(self, x)
   1765 
   1766     def _resize_to_2d(self, x: torch.tensor):
-> 1767         if x.dim() > 2:
   1768             x = x.view(x.size(0), -1)
   1769         return x

attributeerror: 'nonetype' object has no attribute 'dim'

can anyone provide suggestions with what's going wrong?","['python-3.x', 'image-processing', 'huggingface-transformers', 'bert-language-model', 'multimodal']",73284664,"flava's author here.
can you please add the following arguments to your processor call:
return_codebook_pixels=true, return_image_mask=true

here is an example colab if you want to see how to call flava model:",https://stackoverflow.com/questions/73257704,python-3.x,06-08-2022 06:40,347.0,1.0,1.0,True,08-08-2022 22:25,08-08-2022 14:19,Implementation Issues
76470759,transfer learning distillation model loss not decreasing,"currently i'm trying to reproduce paper ""a deep transfer learning method for cross-lingual natural language inference"" (bandyopadhyay et al., lrec 2022) for cross-lingual natural language inference task. but, the model i'm trying to reproduce is not learning any parameters which demonstrated by the model's loss not decreasing.
the dataset i'm using is indonli with hypothesis sentences translated into javanese. but, as you can read on the paper, you can also use xnli for this task.
for this experiment, i'm using pytorch, huggingface transformers, pandas, numpy, and wandb for logging.
first, i construct my dataset as follows:
class compdataset(dataset):
    def __init__(self, df_teacher, df_student):
        self.df_data_teacher = df_teacher
        self.df_data_student = df_student
        
    def __getitem__(self, index):
        # teacher
        sentence_teacher_1 = self.df_data_teacher.loc[index, 'premise']
        sentence_teacher_2 = self.df_data_teacher.loc[index, 'hypothesis']
        
        encoded_dict_teacher = tokenizer.encode_plus(
            sentence_teacher_1,
            sentence_teacher_2,
            add_special_tokens = true,
            max_length = max_len,
            truncation='longest_first',
            padding = 'max_length',
            return_attention_mask = true,
            return_tensors = 'pt'
        )
        
        padded_token_list_teacher = encoded_dict_teacher['input_ids'][0]
        att_mask_teacher = encoded_dict_teacher['attention_mask'][0]
        tok_type_id_teacher = encoded_dict_teacher['token_type_ids'][0]
        
        target_teacher = torch.tensor([self.df_data_teacher.loc[index, 'label']])
        lt_target_teacher = torch.longtensor(target_teacher)
        onehot_encoded_lbl_teacher = f.one_hot(lt_target_teacher, num_classes=3) # 3 classes: entails, neutral, contradict
        
        # student
        sentence_student_1 = self.df_data_student.loc[index, 'premise']
        sentence_student_2 = self.df_data_student.loc[index, 'hypothesis']
        
        encoded_dict_student = tokenizer.encode_plus(
            sentence_student_1,
            sentence_student_2,
            add_special_tokens = true,
            max_length = max_len,
            truncation='longest_first',
            padding = 'max_length',
            return_attention_mask = true,
            return_tensors = 'pt'
        )
        
        padded_token_list_student = encoded_dict_student['input_ids'][0]
        att_mask_student = encoded_dict_student['attention_mask'][0]
        tok_type_id_student = encoded_dict_student['token_type_ids'][0]
        
        target_student = torch.tensor([self.df_data_student.loc[index, 'label']])
        lt_target_student = torch.longtensor(target_student)
        onehot_encoded_lbl_student = f.one_hot(lt_target_student, num_classes=3) # 3 classes: entails, neutral, contradict
        
        output = {
            ""input_ids_teacher"": padded_token_list_teacher, 
            ""attention_mask_teacher"": att_mask_teacher,
            ""token_type_ids_teacher"": tok_type_id_teacher,
            ""lbl_teacher"": onehot_encoded_lbl_teacher,
            ""input_ids_student"": padded_token_list_student, 
            ""attention_mask_student"": att_mask_student,
            ""token_type_ids_student"": tok_type_id_student,
            ""lbl_student"": onehot_encoded_lbl_student
        }
        
        return output
    
    def __len__(self):
        return len(self.df_data_teacher)

then, i build the transformers' dataset & dataloader. the df_train_t and df_train_student being dataframe for teacher dataset (indonesian premise-indonesian hypothesis) and student dataset (indonesian premise-javanese hypothesis).
train_data_cmp = compdataset(df_train_t, df_train_student)
valid_data_cmp = compdataset(df_valid_t, df_valid_student)
test_data_cmp = compdataset(df_test_t, df_test_student)

train_dataloader = dataloader(train_data_cmp, batch_size = batch_size)
valid_dataloader = dataloader(valid_data_cmp, batch_size = batch_size)
test_dataloader = dataloader(test_data_cmp, batch_size = batch_size)

after that, i try to build the model using the schematic and algorithm of transfer learning method provided on the paper. as you can see on the code below, i tried to freeze the mbert model for teacher, and update only the student model parameters.
class transferlearningpaper(pretrainedmodel):
    def __init__(self, config, lambda_kld, learningrate_student, batchnorm_epsilon = 1e-5):
        super(transferlearningpaper, self).__init__(config)
        
        self.bert_model_teacher = bertmodel.from_pretrained(
            model_teacher_type, # using already pretrained mbert in ina language
            num_labels = 3,
            output_hidden_states=true
        )
        
        # freeze teacher mbert parameters
        for params_teacher in self.bert_model_teacher.parameters():
            params_teacher.requires_grad = false
    
        self.bert_model_student = bertmodel.from_pretrained(
            mbert_type,
            num_labels = 3,
            output_hidden_states=true
        )
        
        self.optimizer_student = adamw(
            self.bert_model_student.parameters(), 
            lr=learningrate_student
        )
        
        self.linear = nn.linear(config.hidden_size, 3)  # linear layer
        self.batchnorm = nn.batchnorm1d(config.hidden_size, eps=batchnorm_epsilon)
        self.softmax = nn.softmax(dim=1)  # softmax activation
        
        self.cross_entropy = nn.crossentropyloss()
        self.kld = nn.kldivloss(reduction='batchmean')
        
        # initialize the weights of the linear layer
        self.linear.weight.data.normal_(mean=0.0, std=0.02)
        self.linear.bias.data.zero_()
        
        self.lambda_kld = lambda_kld
    
    def forward(self, input_ids_teacher, attention_mask_teacher, token_type_ids_teacher, lbl_teacher, input_ids_student, attention_mask_student, token_type_ids_student, lbl_student):
        # assume the label is already one-hot encoded
        
        self.bert_model_teacher.eval()
        self.bert_model_student.eval()
        
        with torch.no_grad():
            outputs_teacher = self.bert_model_teacher(
                input_ids=input_ids_teacher, 
                attention_mask=attention_mask_teacher, 
                token_type_ids=token_type_ids_teacher
            )
            outputs_student = self.bert_model_student(
                input_ids=input_ids_student, 
                attention_mask=attention_mask_student, 
                token_type_ids=token_type_ids_student
            )
        
            # take cls token of the last hidden state
            pooled_output_teacher = outputs_teacher[0][:, 0, :]
            pooled_output_student = outputs_student[0][:, 0, :]
        
        batchnormed_logits = self.batchnorm(pooled_output_student)
        linear_output = self.linear(batchnormed_logits) # the output's logits
        softmax_linear_output = f.log_softmax(linear_output, dim=1)
        
        lbl_student = lbl_student[:,0,:].float()
        lbl_teacher = lbl_teacher[:,0,:].float()
        softmax_linear_output = softmax_linear_output.float()
        
        cross_entropy_loss = self.cross_entropy(softmax_linear_output, lbl_student)
        total_kld = self.kld(f.log_softmax(pooled_output_student, dim=1), f.softmax(pooled_output_teacher, dim=1))
        
        joint_loss = cross_entropy_loss + (self.lambda_kld * total_kld )
        
        return {""loss"": joint_loss, ""logits"": softmax_linear_output}
    
    def update_param_student_model(self, loss):
        # doing customized backpropagation for student's model
        self.bert_model_student.train()
        
        self.optimizer_student.zero_grad()
        loss.backward()
        self.optimizer_student.step()

then, i instantiate the model and its configurations and hyperparameters:
config = pretrainedconfig(
    problem_type = ""single_label_classification"",
    id2label = {
        ""0"": ""entail"",
        ""1"": ""neutral"",
        ""2"": ""contradiction""
    },
    label2id = {
        ""entail"": 0,
        ""neutral"": 1,
        ""contradiction"": 2
    },
    num_labels = 3,
    hidden_size = 768,
    name_or_path = ""indojavanesenli-transfer-learning"",
    finetuning_task = ""indonesian-javanese natural language inference""
)
print(config)
transferlearning_model = transferlearningpaper(
    config = config,
    lambda_kld = 0.011, # antara 0.01-0.5
    learningrate_student = student_lrate,
    batchnorm_epsilon = batch_norm_epsilon
)
transferlearning_model = transferlearning_model.to(device)

after that, i create functions to train and validate my model:
def train(the_model, train_data):
    the_model.train()
    
    batch_loss = 0
    
    for batch, data in enumerate(train_data):
        input_ids_teacher = data[""input_ids_teacher""].to(device)
        attention_mask_teacher = data[""attention_mask_teacher""].to(device)
        token_type_ids_teacher = data[""token_type_ids_teacher""].to(device)
        lbl_teacher = data[""lbl_teacher""].to(device)
        input_ids_student = data[""input_ids_student""].to(device)
        attention_mask_student = data[""attention_mask_student""].to(device)
        token_type_ids_student = data[""token_type_ids_student""].to(device)
        lbl_student = data[""lbl_student""].to(device)
        
        output = the_model(
            input_ids_teacher = input_ids_teacher, 
            attention_mask_teacher = attention_mask_teacher, 
            token_type_ids_teacher = token_type_ids_teacher, 
            lbl_teacher = lbl_teacher, 
            input_ids_student = input_ids_student, 
            attention_mask_student = attention_mask_student, 
            token_type_ids_student = token_type_ids_student, 
            lbl_student = lbl_student
        )
        
        loss_model = output[""loss""]
        batch_loss += loss_model
        wandb.log({""train/loss"": loss_model})
        
        # backpropagation
        the_model.update_param_student_model(loss_model)
    
    training_loss = batch_loss / batch_size
    
    return training_loss

def validate(the_model, valid_data):
    the_model.eval()
    
    batch_loss = 0
    
    with torch.no_grad():
        for batch, data in enumerate(valid_data):
            input_ids_teacher = data[""input_ids_teacher""].to(device)
            attention_mask_teacher = data[""attention_mask_teacher""].to(device)
            token_type_ids_teacher = data[""token_type_ids_teacher""].to(device)
            lbl_teacher = data[""lbl_teacher""].to(device)
            input_ids_student = data[""input_ids_student""].to(device)
            attention_mask_student = data[""attention_mask_student""].to(device)
            token_type_ids_student = data[""token_type_ids_student""].to(device)
            lbl_student = data[""lbl_student""].to(device)

            output = the_model(
                input_ids_teacher = input_ids_teacher, 
                attention_mask_teacher = attention_mask_teacher, 
                token_type_ids_teacher = token_type_ids_teacher, 
                lbl_teacher = lbl_teacher, 
                input_ids_student = input_ids_student, 
                attention_mask_student = attention_mask_student, 
                token_type_ids_student = token_type_ids_student, 
                lbl_student = lbl_student
            )

            logits = output[""logits""].cpu().detach().numpy()
            packed_val = logits, lbl_student.cpu().detach().numpy()
            metrics = compute_metrics(packed_val)

            loss_model = output[""loss""]
            batch_loss += loss_model
            wandb.log({
                ""eval/loss"": loss_model, 
                ""eval/f1_score"": metrics[""f1_score""], 
                ""eval/accuracy"": metrics[""accuracy""],
                ""eval/precision"": metrics[""precision""],
                ""eval/recall"": metrics[""recall""]
            })
    
        eval_loss = batch_loss / batch_size
    
    return eval_loss, metrics

def training_sequence(the_model, train_data, valid_data, epochs):
    track_train_loss = []
    track_val_loss = []
    
    t = trange(epochs, colour=""green"", position=0, leave=true)
    for ep in t:
        training_loss = train(the_model, train_data)
        valid_loss, _ = validate(the_model, valid_data)
        
        track_train_loss.append(training_loss)
        track_val_loss.append(valid_loss)
        
        t.set_description(f""epoch [{ep + 1}/{epochs}] - train loss: {training_loss:.2f} valid loss: {valid_loss:.2f}"")
        
        if valid_loss < min(track_val_loss) or ep + 1 == 1:
            the_model.save_pretrained(
                save_directory = model_path + ""indojavanesenli-transfer-learning""
            )
            
        wandb.log({
            ""train_loss/epoch"": training_loss,
            ""validation_loss/epoch"": valid_loss
        })
        
    return {
        ""training_loss"": track_train_loss,
        ""validation_loss"": track_val_loss
    }

finally, i train my model by using:
training_result = training_sequence(transferlearning_model, train_dataloader, valid_dataloader, num_epochs)

but the problem is, during training, the model not updating the student's model parameters as you can see on fig.1 below.

figure 1. model loss not decreasing
fyi, this is the configuration variable i use for the code above:
tokenizer_type = 'bert-base-multilingual-cased'
mbert_type = 'bert-base-multilingual-cased'
model_teacher_type = 'jalaluddin94/nli_mbert' # this is an already fine-tuned mbert on the indonesian language
model_path = 'd:/training/machine learning/nlp/nli/indo-javanese-nli/researchedmodels/'

student_lrate = 2e-5
max_len = 512
num_epochs = 25
batch_size = 12
batch_norm_epsilon = 1e-5
lambda_l2 = 3e-5","['pytorch', 'nlp', 'artificial-intelligence', 'huggingface-transformers', 'transfer-learning']",76601976,"i have figured it out.
it's because of with torch.no_grad(): on the forward pass of the model.
the bert student parameters wouldn't be updated because it lost its gradient.",https://stackoverflow.com/questions/76470759,pytorch,14-06-2023 06:41,114.0,0.0,1.0,True,03-07-2023 05:13,14-06-2023 06:47,Implementation Issues
76393971,bert ner model start and end position none after fine-tuning,"i have fine-tuned a bert ner model to my dataset. the base model that i am fine-tuning is ï¿½ï¿½ï¿½dslim/bert-base-nerï¿½ï¿½ï¿½. i have been successfully able to train the model using the following script as refren"" rel=""nofollow noreferrer"">
the code which does the prediction:
from transformers import pipeline, berttokenizer

tokenizer = berttokenizer.from_pretrained('dslim/bert-base-ner', return_offsets_mapping=true, is_split_into_words=true)
model = bertfortokenclassification.from_pretrained('dslim/bert-base-ner')

pipe = pipeline(task=""ner"", model=model.to(""cpu""), tokenizer=tokenizer, grouped_entities=true)
pipe(""this is a abc corp. ltd"")

the prediction form the base model contained the start and end position of the word in the original text like:
{ï¿½ï¿½ï¿½entity_groupï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½orgï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½scoreï¿½ï¿½ï¿½: 0.9992545247077942, ï¿½ï¿½ï¿½wordï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½aï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½startï¿½ï¿½ï¿½: 10, ï¿½ï¿½ï¿½endï¿½ï¿½ï¿½: 11}
{ï¿½ï¿½ï¿½entity_groupï¿½ï¿½ï¿½ýýendýýý: 22}

while the prediction from the re-trained model is:
{ýýýentity_groupýýý: ýýýorgýýý, ýýýscoreýýý: 0.747031033039093, ýýýwordýýý: ýýý##7ýýý, ýýýstartýýý: none, ýýýendýýý: none},
{ýýýentity_groupýýý: ýýýorgýýý, ýýýscoreýýý: 0.9055356582005819, ýýýwordýýý: ýýýgames , incýýý, ýýýstartýýý: none, ýýýendýýý: none}

i am passing the position ids to the model during the training process. i looked at the model training parameters but, could not find a way to pass start and end position of the words to model training process. i have the start and end position of the tokenized words.","['nlp', 'huggingface-transformers', 'bert-language-model', 'named-entity-recognition']",76399667,"the pipeline can not return positions when you pass a ""slow""-tokenizer. use a ""fast""-tokenizer to get the positions as well:
from transformers import pipeline, berttokenizer, berttokenizerfast, bertfortokenclassification

fast_t = berttokenizerfast.from_pretrained('dslim/bert-base-ner')
slow_t = berttokenizer.from_pretrained('dslim/bert-base-ner')
model = bertfortokenclassification.from_pretrained('dslim/bert-base-ner')


text= ""this is a abc corp. ltd""

slow_p = pipeline(task=""ner"", model=model, tokenizer=slow_t, device=""cpu"", aggregation_strategy=""simple"")
print(slow_p(text))
fast_p = pipeline(task=""ner"", model=model, tokenizer=fast_t, device=""cpu"", aggregation_strategy=""simple"")
print(fast_p(text))

output:
[{'entity_group': 'org', 'score': 0.9992956, 'word': 'a', 'start': none, 'end': none}, {'entity_group': 'org', 'score': 0.97245616, 'word': '##bc corp . ltd', 'start': none, 'end': none}]
[{'entity_group': 'org', 'score': 0.9992956, 'word': 'a', 'start': 10, 'end': 11}, {'entity_group': 'org', 'score': 0.97245616, 'word': '##bc corp. ltd', 'start': 11, 'end': 23}]",https://stackoverflow.com/questions/76393971,nlp,02-06-2023 23:23,559.0,1.0,1.0,True,04-06-2023 09:13,03-06-2023 15:34,Implementation Issues
76571812,multiclass text classification using hugging face models,"i am trying to do sentiment analysis on customer feedback and for that i am using hugging face models (required). the issue is that all the responses i am getting are either positive or negative , i haven't gotten a neutral response.
this is how my dataset looks like
import pandas as pd
from transformers import autotokenizer, automodelforsequenceclassification
import numpy as np


# example dataframe
df = pd.dataframe({'text': ['this movie is great!','neutral','happy this movie!' ,'i feel bored.', 'the weather is nice.',np.nan]})

    # function to predict sentiment
    def predict_sentiment(text):
        # load tokenizer and model
        if pd.isna(text):
            return 'n/a'  # return a default value for nan
        tokenizer = autotokenizer.from_pretrained(""bert-base-uncased"")
        model = automodelforsequenceclassification.from_pretrained(""textattack/bert-base-uncased-imdb"")
        tokens = tokenizer.encode_plus(text, padding=true, truncation=true, return_tensors=""pt"")
        outputs = model(**tokens)
        predicted_class = outputs.logits.argmax().item()
        sentiment_classes = ['negative','positive', 'neutral']
        predicted_sentiment = sentiment_classes[predicted_class]
        return predicted_sentiment
    
    # apply sentiment prediction on dataframe column
    df['predicted_sentiment'] = df['text'].apply(predict_sentiment)

             text                      predicted_sentiment
0   this movie is great!                  positive
1   neutral                               positive
2   happy this movie!                     positive
3   i feel bored.                         negative
4   the weather is nice.                  positive
5   nan                                     n/a

now, if i switch the lables like this ['negative','neutral','positive'] i only get results
             text                      predicted_sentiment
0   this movie is great!                  neutral
1   neutral                               neutral
2   happy this movie!                     neutral
3   i feel bored.                         negative
4   the weather is nice.                  neutral
5   nan                                     n/a

whereas the results should be
         text                      predicted_sentiment
0   this movie is great!                  positive
1   neutral                               neutral
2   happy this movie!                     positive
3   i feel bored.                         negative
4   the weather is nice.                  positive
5   nan                                     n/a","['python', 'nlp', 'huggingface-transformers', 'huggingface']",76580210,"the main reason why you're not getting the neutral label is because the model that you're using (textattack/bert-base-uncased-imdb, huggingface link here) has been trained on the imdb dataset which is binary classification dataset. in other words, the model only has two final layers and will only output a prediction between two outputs, positive or negative. you can see more info about the dataset here. as you can clearly see it's a binary classification dataset so the model trained on it does not allow for 3 different outputs.
if you pass it three or more labels (as you did with ['negative','neutral','positive']) it will only consider the first two and ignore the others. the model is set such that the first label is ""negative"" and the second is ""positive"", irrespective of what you set. so your ""neutral"" is a ""positive"" label as it takes the second position. even if you set the labels to random words such as ['hello','there'] then your ""hello"" label will signify a negative score (first place) and ""there"" a positive score (second place). be careful because if you set your labels to ['positive','negative'] then a ""positive"" review (first item) will effectively be ""negative"" one. this is just how the model you're using is set up. hope this makes sense.",https://stackoverflow.com/questions/76571812,python,28-06-2023 09:28,883.0,1.0,2.0,True,29-06-2023 10:33,28-06-2023 09:36,Task-specific Help
76474907,how to visulaize or print the output from each layer of pytorch saved model specifically for vision_transformer?,"what i have done?
i have trained a vit using pytorch from torchvision.models import vision_transformer as vits specifically model = vits.vit_b_16(pretrained=false, num_classes=10).to(device) and saved the whole model using torch.save(model, ""vit_mnist_model.pth"") in the separate python file i have loaded the model and also test it on single image from the dataset it is working fine.
what i want to do?
as transformer use the attention mechanism. i want to visualize the patches self attention is focusing most in the prediction of the image. to do that i want to pass the same image to the vit and get the output from the each encoder block. further, my plan is to visualize these patches that are output from from each block using matplot.
code i am using till now
import torch
from torchvision import datasets, transforms
from torch.utils.data import dataloader
import torch.nn as nn
from torchsummary import summary

# set device
device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

# load mnist test dataset and apply transformations
transform = transforms.compose([
    transforms.resize(224),
    transforms.grayscale(num_output_channels=3),
    transforms.totensor(),
    transforms.normalize((0.5,), (0.5,))
])

test_dataset = datasets.mnist(root='./data', train=false, download=true, transform=transform)

# create data loader
batch_size = 1  # set batch size to 1 to process one image at a time
test_loader = dataloader(test_dataset, batch_size=batch_size, shuffle=false)


# load the saved model state dictionary
model =torch.load('vit_mnist_model.pth')

# set the model in evaluation mode
model.eval()

# get a single image from the test data at position 5
index = 5
image, label = test_dataset[index]
image = image.unsqueeze(0).to(device)  # add a batch dimension and move to the device

# print the summary of the model
summary(model, input_size=(3, 224, 224), print_summary=true)

print(image.shape)

for name, module in model.named_modules():
    print(name)

# pass the image through the model up to the 'encoder_layer_11'
output = image
for name, module in model.named_children():
    output = module(output)
    if name == 'encoder':
        for encoder_name, encoder_module in module.named_children():
            output = encoder_module(output)
            if encoder_name == 'layer11':
                encoder_layer_output = output
                break

print(f""output shape of 'encoder_layer_11': {encoder_layer_output.shape}"")
print(encoder_layer_output)

error i am getting
traceback (most recent call last):
  file ""/home/blggpu2/projects/visiontransformer/visualize_vit.py"", line 54, in <module>
    output = module(output)
  file ""/home/blggpu2/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  file ""/home/blggpu2/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/vision_transformer.py"", line 155, in forward
    torch._assert(input.dim() == 3, f""expected (batch_size, seq_length, hidden_dim) got {input.shape}"")
  file ""/home/blggpu2/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/__init__.py"", line 853, in _assert
    assert condition, message
assertionerror: expected (batch_size, seq_length, hidden_dim) got torch.size([1, 768, 14, 14])

i don't understand why it is giving an error when i pass the image using the for loop. i am just passing the image to the input layer and whatever it giving output i am transferring that output to the next layer.
update
i used the below code to get the output for specific layers. however, it is a bit confusing, first of all, according to my knowledge transformer breaks the image into patches but when i return the output of the 'conv_proj' it returns [1, 768, 14, 14] meaning there are 768 patches of size 14 x 14. but when i plot them they show the whole 768 images not just a patch. further 'encoder.layers.encoder_layer_11' returns [1, 197, 768] which doesn't make any sense to me as it cant be plotted.
for name, module in model.named_modules():
    print(name)
return_nodes = {
    
    ""conv_proj"": ""layer1"",
    ""encoder.layers.encoder_layer_11"": ""layer2"",
   

}
model2 = create_feature_extractor(model, return_nodes=return_nodes)
intermediate_outputs = model2(image)


print(intermediate_outputs['layer1'].shape)
print(intermediate_outputs['layer2'].shape)

out1= intermediate_outputs['layer1'][0,0,:,:]
normalized_output = out1.cpu().detach().numpy()

plt.imshow(normalized_output, cmap='gray')
plt.colorbar()
plt.show()

output
torch.size([1, 768, 14, 14])
torch.size([1, 197, 768])","['python', 'pytorch', 'huggingface-transformers']",76475442,"as shown in this other question, the order of the elements in model.named_children() is given by the order of appearance in the __init__ method of the model, which means that without access to this method, the order of appearance of the different layers is not guaranteed.
moreover, even if you are certain that modules will appear in the correct order, your loop will still probably cause an error, because some parts of the model will be used twice. in your loop, when the module named encoder appears, the current value of output is processed and saved to the same variable with output = encoder(output). after, children layers of the encoder are applied to the current value of output, which have already been processed by the encoder. the effect of that will be, in the best case if the shapes and layers are not compatible, to raise an error like you got, and in the worst case if shapes and layers are compatible, to give you an answer as a tensor, but which would not be what you wanted, since the data would have gone through the encoder once and then through some parts of the encoder again, resulting on uninterpretable results.
update
first, about the conv_proj output :
this is the opposite, you have 14*14=196 patches, each of which being summarized by a 768-sized representation. this representation is not really vizualizable, since each feature could be independant from the other. if you want something that can be seen as an image, you could check the kernel of the convolution layer. this will give you a [k, k, n] tensor, with k being the kernel size of the convolution, and n the number of filters (likely to be 768, unless there are other layers after the convolutions). for interpretation, you could visualize each [k, k] array as ""how important is each pixel for this filter"". values can be positive or negative, depending on whether the pixel has a positive or negative effect on this filter.
then about the transformer layer :
the dimension you get is [b, t, n], where b is batch size (1 in your case), t is some kind of ""time"" dimension and n is the size of the representation. since an image does not have a time dimension, one is constructed by defining an order between the patches, so you find again the 196 patches you already had from the convolution layer, with one patch added by the transformer layer, which gives the t=196 dimension. finally n is the size of the embedding, which is kept at 768. as for the previous one, this representation is not really meant to be visualized, something that can be interesting to watch is the attention maps from the transformer layers. it is a [t,t,h]-sized tensor, where t is still the ""time"" dimension (197) and h is the number of heads in this layer. each of these t*t tensors are describing the importance of each patch relatively to the other to construct the output of the layer, and so for each attention head you have. you can find some details in this link from pytorch doc, the output is named attn_output_weights and you might have to tweak a bit your implementation to get these.",https://stackoverflow.com/questions/76474907,python,14-06-2023 14:56,2222.0,1.0,1.0,True,08-03-2024 17:40,08-03-2024 17:40,Implementation Issues
37793118,load pretrained glove vectors in python,i have downloaded pretrained glove vector file from the internet. it is a .txt file. i am unable to load and access it. it is easy to load and access a word vector binary file using gensim but i don't know how to do it when it is a text file format.,"['python-2.7', 'vector', 'nlp']",38230349,"glove model files are in a word - vector format. you can open the textfile to verify this. here is a small snippet of code you can use to load a pretrained glove file:
import numpy as np

def load_glove_model(file):
    print(""loading glove model"")
    glove_model = {}
    with open(file,'r') as f:
        for line in f:
            split_line = line.split()
            word = split_line[0]
            embedding = np.array(split_line[1:], dtype=np.float64)
            glove_model[word] = embedding
    print(f""{len(glove_model)} words loaded!"")
    return glove_model

you can then access the word vectors by simply using the glovemodel variable.
print(glovemodel['hello'])",https://stackoverflow.com/questions/37793118,python-2.7,13-06-2016 15:01,90957.0,50.0,14.0,True,29-12-2022 02:41,29-12-2022 02:41,Implementation Issues
69837764,find index of word in sentence with information from phrase,"i need the index of word in sentence. but sometimes there are repetitions of the words. the phrase information would then be helpful. or the previous or next row in the word column.
basically, i just need to identify the word in the utterance, e.g. if the word is 'seaside', i want to know which 'seaside' it is in the sentence. i have extra information from the phrase that can help with this identification. the order of their appearance in the dataframe can also help.
what i have right now is this:




file_id
phrase
word
sentence
word_indices




a
i am
i
i am a happy bird. i sing every day. i eat worms.
[0, 5, 9]


b
the seaside is
the
she is by the seaside. the seaside is  packed.
[3, 5]


b
the seaside is
seaside
she is by the seaside. the seaside is  packed.
[4, 6]


b
the seaside is
is
she is by the seaside. the seaside is  packed.
[1, 7]


c
nobody knows
nobody
nobody knows what is going on. she can find nobody
[0, 9]


c
find nobody
nobody
nobody knows what is going on. she can find nobody
[0, 9]


d
it is such a sunny day
sunny
it is such a sunny day ah i am so happy when it's sunny such a sunny day is the best
[4, 13, 16]




but what i want to get is what is in the target column.




file_id
phrase
word
sentence
word_indices
target




a
i am
i
i am a happy bird. i sing every day. i eat worms.
[0, 5, 9]
[0]


b
the seaside is
the
she is by the seaside. the seaside is  packed.
[3, 5]
[5]


b
the seaside is
seaside
she is by the seaside. the seaside is  packed.
[4, 6]
[6]


b
the seaside is
is
she is by the seaside. the seaside is  packed.
[1, 7]
[7]


c
nobody knows
nobody
nobody knows what is going on. she can find nobody
[0, 9]
[0]


c
find nobody
nobody
nobody knows what is going on. she can find nobody
[0, 9]
[9]


d
it is such a sunny day
sunny
it is such a sunny day ah i am so happy when it's sunny such a sunny day is the best
[4, 13, 16]
[4]




i found a similar question here: find index of words in matched text
but unfortunately, this is in java and i need an answer using python.
thanks a lot!","['python', 'pandas', 'indexing', 'nlp', 'nltk']",69838364,"i would break it down into two steps. find the number of words leading up to the phrase in the sentence and then the word index number of the word in the phrase: something like below:
def get_index_of_word_in_sentence(word, phrase, sentence):
    index1 = sentence.index(phrase)
    word_num1 = len(sentence[:index1].split())
    word_num2 = phrase.split().index(word)
    return word_num1 + word_num2

df[""target""] = df.apply(lambda x: get_index_of_word_in_sentence(x[""word""], x[""phrase""], x[""sentence""]), axis=1)",https://stackoverflow.com/questions/69837764,python,04-11-2021 10:28,353.0,0.0,1.0,True,04-11-2021 11:50,04-11-2021 11:46,Uncategorized
18446408,adding custom stopwords in r tm,"i have a corpus in r using the tm package. i am applying the removewords function to remove stopwords
tm_map(abs, removewords, stopwords(""english"")) 

is there a way to add my own custom stop words to this list?","['r', 'text-mining', 'stop-words', 'corpus', 'tm']",18446631,"stopwords just provides you with a vector of words, just combine your own ones to this.
tm_map(abs, removewords, c(stopwords(""english""),""my"",""custom"",""words""))",https://stackoverflow.com/questions/18446408,r,26-08-2013 14:22,42266.0,17.0,6.0,True,11-03-2021 12:44,20-07-2016 02:24,Preprocessing Tasks
78775926,how can i fix my flask server&#39;s 405 error that includes openai api?,"i'm trying to add an api to my webpage and have never used any flask server before, i have never used javascript too so this is a completely brand new learning experience.
my problem is that i keep receiving a 405 error code saying that the method is not allowed. i keep on using the post method but it isn't working, i am banking that my issue may be with my html code more than my flask server because the code is extremely generic and simple.
import openai
from flask import flask, request, jsonify

app = flask(__name__)

openai.api_key = '**my key is in here**'

@app.route('/', methods=['post'])
def chat():
    data = request.get_json()
    message = data.get('message')
    
    response = openai.completion.create(
        model=""gpt-3.5-turbo"",  
        prompt=message,
        max_tokens=50
    )

    return {'response': response.choices[0].text.strip()}

if __name__ == '__main__':
    app.run(port=5000)

async function sendmessage() {
            const message = document.getelementbyid('message').value;
            document.getelementbyid('chat-box').innerhtml += `<div>you: ${message}</div>`;
            
            const response = await fetch('/', {
                method: ""post"",
                body: json.stringify({ message }),
                headers: {
                    'content-type': 'application/json',
                },
            });

            const data = await response.json();
            document.getelementbyid('chat-box').innerhtml += `<div>bot: ${data.reply}</div>`;
            document.getelementbyid('message').value = '';
        }

i tried changing up the structure of the code, i uninstalled flask and reinstalled it again. i've also extensively used chatgpt to try and come up with better code but it just kept taking me in circles. i'm hoping someone can help with this. i even tried a simple server that just said hello world which worked, but i truly think the issue might be with my javascript. also, i am a beginner and this is supposed to be one of my first coding projects so please take it easy on me if possible. thanks.","['javascript', 'python', 'flask', 'openai-api']",78776107,"you have to add a route for '/' to serve the html file. i also fixed the way you call the openai api because you're using a deprecated one.
import openai
from flask import flask, request, jsonify, render_template

chat_client = openai(api_key='....')


@app.route('/')
def index():
    return render_template('index.html')

@app.route('/', methods=['post'])
def chat():
    data = request.get_json()
    message = data.get('message')
    
    response = chat_client.chat.completions.create(
        model=""gpt-3.5-turbo"",
        messages=[
            {""role"": ""user"", ""content"": message}
        ],
        max_tokens=50
    )

    return jsonify({'reply': response.choices[0].message.content.strip()})

if __name__ == '__main__':
    app.run(port=5000, debug=true)

sample index.html that i tested with
<!doctype html>
<html lang=""en"">
<head>
    <meta charset=""utf-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    <title>chat app</title>
    <script>
        async function sendmessage() {
            const message = document.getelementbyid('message').value;
            document.getelementbyid('chat-box').innerhtml += `<div>you: ${message}</div>`;
            
            const response = await fetch('/', {
                method: ""post"",
                body: json.stringify({ message }),
                headers: {
                    'content-type': 'application/json',
                },
            });

            const data = await response.json();
            document.getelementbyid('chat-box').innerhtml += `<div>bot: ${data.reply}</div>`;
            document.getelementbyid('message').value = '';
        }
    </script>
</head>
<body>
    <div id=""chat-box""></div>
    <input type=""text"" id=""message"" placeholder=""type your message here..."">
    <button onclick=""sendmessage()"">send</button>
</body>
</html>


directory structure
- app.py
- templates/
  - index.html",https://stackoverflow.com/questions/78775926,javascript,21-07-2024 17:40,136.0,0.0,1.0,True,22-07-2024 08:19,22-07-2024 06:36,Implementation Issues
75935259,"expected file to have jsonl format, where every line is a json dictionary. openai createfile for fine tune","i created file with name mydata.jsonl and i put on it these lines
    {
        ""prompt"": ""aa"",
        ""completion"": ""bb""
    }
    {
        ""prompt"": ""cc"",
        ""completion"": ""dd""
    }

then in index.js i did this function
    const { configuration, openaiapi } = require(""openai"");
    const fs = require(""fs"");
    
    const configuration = new configuration({
        apikey: process.env.openai_api_key,
    });
    async function getairesponse(topic) {
        const openai = new openaiapi(configuration);
        const filename = 'example.txt';
        const filecontents = 'this is some example text.';
        const response = await openai.createfile(
            fs.createreadstream(""mydata.jsonl""),
            ""fine-tune""
        );
        console.log(response)
    }
    getairesponse(""ee"");

when i run my code i got error
    expected file to have jsonl format, where every line is a json dictionary. line 1 is not a dictionary (hint: line starts with: ""{..."").

i can't get where is the exactly the error","['openai-api', 'fine-tuning']",75935581,"the jsonl format is where the each json object is sperated by a newline characater.
so your output should be:
{ ""prompt"": ""aa"", ""completion"": ""bb"" }
{ ""prompt"": ""cc"", ""completion"": ""dd"" }

i.e. one line per json object",https://stackoverflow.com/questions/75935259,openai-api,05-04-2023 02:44,7795.0,4.0,2.0,True,04-05-2023 12:10,05-04-2023 19:48,Data Wrangling
77465301,trying adding embeddings in azure cognitive search leads to error &quot;the property &#39;content&#39; does not exist on type &#39;search.documentfields&#39;.&quot;,"i am extracting text from pdf documents and load it to azure cognitive search for a rag approach. unfortunately this does not work. i am receiving the error message
 () the request is invalid. details: the property 'content' does not exist on type 'search.documentfields'. make sure to only use property names that are defined by the type.
code: 
message: the request is invalid. details: the property 'content' does not exist on type 'search.documentfields'. make sure to only use property names that are defined by the type.

what i want to do is

extract text from pdf via pymupdf - works
upload it to azure vector search as embeddings with vectors and metdata `filename``
query this through chatgpt model

beside the error i want to add to this document object the metadata information filename but also dont know how to extend this ...
my code:
!pip install cohere tiktoken
!pip install openai==0.28.1
!pip install pymupdf
!pip install azure-storage-blob azure-identity
!pip install azure-search-documents --pre --upgrade
!pip install langchain

import fitz
import time
import uuid
import os
import openai

from pil import image
from io import bytesio
from ipython.display import display

from azure.identity import defaultazurecredential
from azure.storage.blob import blobserviceclient, blobclient, containerclient

from langchain.embeddings import openaiembeddings
from langchain.text_splitter import recursivecharactertextsplitter

from langchain.chat_models import azurechatopenai
from langchain.vectorstores import azuresearch
from langchain.docstore.document import document
from langchain.document_loaders import directoryloader
from langchain.document_loaders import textloader
from langchain.text_splitter import tokentextsplitter
from langchain.chains import conversationalretrievalchain
from langchain.prompts import prompttemplate

from google.colab import drive

openai_api_base = ""
openai_api_key = ""xxx""
openai_api_version = ""2023-05-15""

openai.api_type = ""azure""
openai.api_key = openai_api_key
openai.api_base = openai_api_base
openai.api_version = openai_api_version

azure_cognitive_search_service_name = ""
azure_cognitive_search_api_key = ""xxx""
azure_cognitive_search_index_name = ""test""

llm = azurechatopenai(deployment_name=""gpt35"", openai_api_key=openai_api_key, openai_api_base=openai_api_base, openai_api_version=openai_api_version)
embeddings = openaiembeddings(deployment_id=""ada002"", chunk_size=1, openai_api_key=openai_api_key, openai_api_base=openai_api_base, openai_api_version=openai_api_version)

acs = azuresearch(azure_search_endpoint=azure_cognitive_search_service_name,
                  azure_search_key = azure_cognitive_search_api_key,
                  index_name = azure_cognitive_search_index_name,
                  embedding_function = embeddings.embed_query)
    
def generate_tokens(s, f):
  text_splitter = recursivecharactertextsplitter(chunk_size=1000, chunk_overlap=100)
  splits = text_splitter.split_text(s)
  i = 0

  documents = []
  for split in splits:
    metadata = {}
    metadata[""index""] = i
    metadata[""file_source""] = f
    i = i+1

    new_doc = document(page_content=split, metadata=metadata)
    documents.append(new_doc)
    #documents = text_splitter.create_documents(splits)

  print (documents)

  return documents


drive.mount('/content/drive')
folder = ""/content/drive/.../pdf/""

page_content = ''
doc_content = ''
    
for filename in os.listdir(folder):
    file_path = os.path.join(folder, filename)
    if os.path.isfile(file_path):
        print(f""processing file: {file_path}"")

        doc = fitz.open(file_path)
        for page in doc: # iterate the document pages
          page_content += page.get_text() # get plain text encoded as utf-8    
          d = generate_tokens(doc_content)

          # the following line throws the error
          # how can i add the chunks + filename to 
          # azure cognitive search?

          doc_content += page_content
          d = generate_tokens(doc_content, file_path)

          acs.add_documents(documents=d)
    
        print(metadatas)
        print(""----------"")
        print(doc_content)
        count = len(doc_content.split())
        print(""number of tokens: "", count)


                         traceback (most recent call last)
<ipython-input-11-d9eaff7ee027> in <cell line: 10>()
     31           all_texts.extend(d)
     32 
---> 33           acs.add_documents(documents=d)
     34 
     35           metadatas = [{""source"": f""{i}-pl""} for i in range(len(all_texts))]

7 frames
/usr/local/lib/python3.10/dist-packages/azure/search/documents/_generated/operations/_documents_operations.py in index(self, batch, request_options, **kwargs)
   1249             map_error(status_code=response.status_code, response=response, error_map=error_map)
   1250             error = self._deserialize.failsafe_deserialize(_models.searcherror, pipeline_response)
-> 1251             raise  model=error)
   1252 
   1253         if response.status_code == 200:

 () the request is invalid. details: the property 'content' does not exist on type 'search.documentfields'. make sure to only use property names that are defined by the type.
code: 
message: the request is invalid. details: the property 'content' does not exist on type 'search.documentfields'. make sure to only use property names that are defined by the type.

this is my index in azure cognitive search index:","['python', 'azure', 'azure-cognitive-search', 'langchain', 'azure-openai']",77465478,"i have solved it now. you had to create the necessary fields in azure cognitive search. these are

the field content_vector seems to hold the vectors. the json definition of the field is
{
  ""name"": ""content_vector"",
  ""type"": ""collection(edm.single)"",
  ""searchable"": true,
  ""filterable"": false,
  ""retrievable"": true,
  ""sortable"": false,
  ""facetable"": false,
  ""key"": false,
  ""indexanalyzer"": null,
  ""searchanalyzer"": null,
  ""analyzer"": null,
  ""normalizer"": null,
  ""dimensions"": 1536,
  ""vectorsearchconfiguration"": ""vector-config-1699712748580"",
  ""synonymmaps"": []
}

and",https://stackoverflow.com/questions/77465301,python,11-11-2023 13:44,2524.0,1.0,1.0,True,11-11-2023 14:32,11-11-2023 14:16,Conceptual Questions
76580372,problems with openai authentification,"i've got some problems with the authentification to openai in my python code. it seems like, openai doesn't accept my key. i did a new on and tried it with other ones before. i always get the same issues. i just copied and pasted the key, same for the organization. there isn't a typo. there is another problem, but i don't know, why the authentifitaion failed.
here is my code:
i used it in the same way like the openai website 
import os
import openai
openai.organization = ""org-cxk0suzbnyzzclcasbcfav64""
openai.api_key = os.getenv(""sk-**********************************************9q"")
openai.model.list()

it produces the follwing error:
openai.error.authenticationerror: no api key provided. you can set your api key in code using 'openai.api_key = <api-key>', or you can set the environment variable openai_api_key=<api-key>). if your api key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <path>'. you can generate api keys in the openai web interface. see  for details, or email support@openai.com if you have any questions.


did anyone had the some issues before? are there any solutions?","['python', 'authentication', 'api-key', 'openai-api']",76580533,"your environment variables are like a dictionary, and the code to retrieve should match the sample code:
openai.api_key = os.getenv(""openai_api_key"")

however, you have to set the environment keys first.
from your code, it looks like you just want to hard code the value, in which case your code should look like:
import openai
openai.organization = ""org-cxk0suzbnyzzclcasbcfav64""
openai.api_key = ""sk-**********************************************9q""
openai.model.list()",https://stackoverflow.com/questions/76580372,python,29-06-2023 10:53,1214.0,0.0,1.0,True,29-06-2023 11:17,29-06-2023 10:54,Implementation Issues
70051086,python sklearn tfidfvectorizer arguments error,"i've been using the sklearn tfidfvectorizer but suddenly it has been throwing an error:
typeerror: __init__() takes 1 positional argument but 2 positional arguments 
(and 4 keyword-only arguments) were given

the argument i'm giving is:
tfidf_vectorizer = tfidfvectorizer(x_train, ngram_range=(1,2), max_df=0.9, min_df=5, token_pattern=r'(\s+)' )

where x_train is a list of strings such as:
 'done earlier siesta',
 'sunday mass us family greatful opportunity',
 'wet wet wet frustrated outside',
 'tired headache headache',
 'friends creative talented inspired friendship love creatives',
 'grateful lucky beaches sunshine hubby family pets awesome sunday',
 'latest artwork',
 'two headache sick tired sore'

im confused as to why it would say i'm giving two positional arguments when im only inputting one which is the x_train list. even when i simplify the statement down to:
tfidfvectorizer(x_train)

it still gives the same error saying i gave two positional arguments.
i'm using sklearn 1.0.1 but i tried reverting it to 1.0.0 and it still have the same error
could the error be in the list that i am passing in?","['python', 'machine-learning', 'scikit-learn', 'nlp', 'tfidfvectorizer']",70051388,"library and their implementation did change. if we look at the version 0.23.1 we get a warning which states that it needs to pass with keyword args.
tfidvect=tfidfvectorizer(x_train)
futurewarning: pass input=['done earlier siesta', 'sunday mass us family greatful opportunity', 'wet wet wet frustrated outside', 'tired headache headache', 'friends creative talented inspired friendship love creatives', 'grateful lucky beaches sunshine hubby family pets awesome sunday', 'latest artwork', 'two headache sick tired sore'] as keyword args. from version 0.25 passing these as positional arguments will result in an error
  warnings.warn(""pass {} as keyword args. from version 0.25 ""

so fast forward to 1.0.1, the same call will be like:
tfidvect1_01=tfidfvectorizer(input=x_train) # input positional argument

@ambrayers added.
an alternative way is, create the object and then fit_transform , refer the example in official documentation
vectorizer = tfidfvectorizer()  
x_train = vectorizer.fit_transform(x_train)",https://stackoverflow.com/questions/70051086,python,21-11-2021 01:06,1526.0,0.0,1.0,True,21-11-2021 22:31,21-11-2021 10:31,Tool Setup/Errors
77413194,how to increase the code view box size of chatgpt web interface,"it is frustrating how small the code viewing area is in chatgpt and to make the matters worse it has a fixed width that does not change with the size of the browser window.
update: guys use the extension in the answer below. it is a better solution than mine.
a temporary solution is this bookmarklet to maximize all the elements:
create a bookmark and put this in the url. then run by selecting the bookmark on chatgpt ui.
javascript:(function() { 
  document.queryselectorall('div[class*=""group w-full""] > div > div')
    .foreach((element) => { element.classname = ""flex flex-1""; }); 
})();

is there a better solution than this?",['openai-api'],78908485,"there's a chrome extension now -  its pretty clean.
ps: not built by me.",https://stackoverflow.com/questions/77413194,openai-api,02-11-2023 22:21,3042.0,4.0,4.0,True,06-09-2024 20:24,06-09-2024 20:24,Implementation Issues
73106002,udpipe_annotate() in r labels the same word differently if followed by punctuation,"i'm doing a standard topic modelling task on nouns in newspaper articles using udpipe to annotate the article content. using the function udpipe_annotate() i noticed that words together with the following punctuation mark sometimes were labelled as upos = noun. thus when i run the topic model function - lda() from library topicmodels - the most common words for a topic might include, say, 'product' and 'product.', the latter including the punctuation mark. they should be seen as the same word. how can i remedy this and remove the punctuation?
another issue is when words before a punctuation were labelled as upos = punct. e.g. 'energy' and 'energy,' were labelled differently. thus i have to specify that i want to include punct in the analysis, and even then i run into the same problem as above of the algorithm treating this as two different words.
is this a problem with the udpipe annotation or is there an easy fix to this problem?
edit: adding code example using first two sentences of wikipedia article on norway in norwegian:
text <- c('norge, offisielt kongeriket norge, er et nordisk, europeisk land og en selvstendig stat vest pï¿½ï¿½ den skandinaviske halvï¿½ï¿½y. geografisk sett er landet langt og smalt.', 'pï¿½ï¿½ den langstrakte kysten mot nord-atlanteren befinner norges vidkjente fjorder seg.', 'kongeriket norge omfatter hovedlandet (fastlandet med tilliggende ï¿½ï¿½yer innenfor grunnlinjen), jan mayen og svalbard.')

id <- c(1:3)

df <- data.frame(text, id)

ud_model <- udpipe_download_model(language = ""norwegian-bokmaal"")
ud_model <- udpipe_load_model(ud_model$file_model)
x <- udpipe_annotate(ud_model, x = df$text, doc_id = df$id)
x_df = data.frame(x)

showing example of the problematic outputs (the rest (adj, verb, etc) are fine i think):
head(x_df[x_df$upos=='noun',5:8], 5)
output:""s-table-container"">



token_id
token
lemma
upos




1
norge,
norge,
noun


4
norge,
norge,
noun


9
land
land
noun


13
stat
stat
noun


18
halvï¿½ï¿½y.
halvï¿½ï¿½y.
noun




head(x_df[x_df$upos=='punct',5:8])
the words with token_id 1,4,and 18 are not correct.
output:




token_id
token
lemma
upos




7
nordisk,
$nordisk,
punct


10
grunnlinjen),
$grunnlinjen),
punct




here, udpipe is finding the punctuation but it also includes the preceding word.
edit2: the problem does not occur for me with the french or english language models. nor does it seem to appear on the norwegian-nynorsk version.","['r', 'nlp', 'annotations', 'punctuation', 'udpipe']",73109347,"looks like there is an issue with the norwegian-bokmaal ud 2.5 model. looking at the ud treebank for norwegian bokmal they are already on version 2.10.
if you use either norwegian-nynorks it works correctly or norwegian-bokmaal ud 2.4 model.
# switch to older model
ud_model <- udpipe_download_model(language = ""norwegian-bokmaal"", 
                                  udpipe_model_repo = ""jwijffels/udpipe.models.ud.2.4"")

# nynorsk works as well
ud_model <- udpipe_download_model(language = ""norwegian-nynorsk"")

you can, of course, get version 2.10, but then you have to train your udpipe model yourself. more info about this in the model building vignette.",https://stackoverflow.com/questions/73106002,r,25-07-2022 08:17,156.0,2.0,1.0,True,25-07-2022 12:34,25-07-2022 10:51,Data Wrangling
73484411,"can i plot roc curve for multiclass text classification problem, without using onevsrestclassifier?","i have a pickle file that when loaded returns a trained randomforest classifier. i want to plot the roc curve for the classes, but from what i read online, the classifier must be wrapped in scikit learn's onevsrestclassifier. the problem is that since i already have the trained model i cannot wrap it in it to fit the model again.
so i would like to know if there is some workaround to plot the roc curve. from my trained model i have y_test, y_proba. i also have x_test values.

the shape of my y_proba examples is: (6715, 5)



the shape of y_test is (6715, 5)


this is the output of the code @dx2-66 suggested:","['python', 'scikit-learn', 'text-classification', 'roc', 'multiclass-classification']",73485714,"i assume your y_test is single column with class id, and your y_proba has as much columns as there are classes (at least that's what you'd usually get from predict_proba().
how about this? it should yield you ovr-style curves:
from sklearn.metrics import roc_curve
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt

classes = range(y_proba.shape[1])

for i in classes:
    fpr, tpr, _ = roc_curve(label_binarize(y_test, classes=classes)[:,i], y_proba[:,i])
    plt.plot(fpr, tpr, alpha=0.7)
    plt.legend(classes)

update: solution for non-monotonic class labels:
classes = sorted(list(y_test['label'].unique()))

plt.plot([0, 1], linestyle='--')

for i in range(len(classes)):
    fpr, tpr, _ = roc_curve(label_binarize(y_test, classes=classes)[:,i], y_proba.values[:,i])
    plt.plot(fpr, tpr, alpha=0.7)
    plt.legend(['baseline']+classes) # fixed the baseline legend",https://stackoverflow.com/questions/73484411,python,25-08-2022 08:38,290.0,0.0,1.0,True,25-08-2022 14:54,25-08-2022 13:06,Task-specific Help
34351978,nltk regex chunker not capturing defined grammar patterns with wildcards,"i am trying to chunk a sentence using nltk's pos tags as regular expressions. 2 rules are defined to identify phrases, based on the tags of words in the sentence.
mainly, i wanted to capture the chunk of one or more verbs followed by an optional determiner and then one or more nouns at the end. this is the first rule in definition. but it is not getting captured as phrase chunk.
import nltk

## defining the pos tagger 
tagger = nltk.data.load(nltk.tag._pos_tagger)


## a single sentence - input text value
textv=""this has allowed the device to start, and i then see glitches which is not nice.""
tagged_text = tagger.tag(textv.split())

## defining grammar rules for  phrases
actphgrammar = r""""""
     ph: {<vb*>+<dt>?<nn*>+}  # verbal phrase - one or more verbs followed by optional determiner, and one or more nouns at the end
     {<rb*><vb*|jj*|nn*\$>} # adverbial phrase - adverb followed by adjective / noun or verb
     """"""

### parsing the defined grammar for  phrases
actp = nltk.regexpparser(actphgrammar)

actphrases = actp.parse(tagged_text)

the input to the chunker, tagged_text is as below.

tagged_text
out[7]:
[('this', 'dt'),
('has', 'vbz'),
('allowed', 'vbn'),
('the', 'dt'),
('device', 'nn'),
('to', 'to'),
('start,', 'nnp'),
('and', 'cc'),
('i', 'prp'),
('then', 'rb'),
('see', 'vb'),
('glitches', 'nns'),
('which', 'wdt'),
('is', 'vbz'),
('not', 'rb'),
('nice.', 'nnp')]

in the final output, only the adverbial phrase ('then see'), that is matching the second rule is being captured.
i expected the verbal phrase ('allowed the device') to match with the first rule and get captured as well, but its not.

actphrases out[8]: tree('s', [('this', 'dt'), ('has', 'vbz'),
('allowed', 'vbn'), ('the', 'dt'), ('device', 'nn'), ('to', 'to'),
('start,', 'nnp'), ('and', 'cc'), ('i', 'prp'), tree('ph', [('then',
'rb'), ('see', 'vb')]), ('glitches', 'nns'), ('which', 'wdt'), ('is',
'vbz'), ('not', 'rb'), ('nice.', 'nnp')])

nltk version used is 2.0.5 (python 2.7)
any help or suggestion would be greatly appreciated.","['python', 'regex', 'nlp', 'nltk', 'text-chunking']",34361516,"close but minor changes to your regex will get you your desired output. when you want to get a wildcard using regexpparser grammar, you should use .* instead of *, e.g. vb.* instead of vb*:
>>> from nltk import word_tokenize, pos_tag, regexpparser
>>> text = ""this has allowed the device to start, and i then see glitches which is not nice.""
>>> tagged_text = pos_tag(word_tokenize(text))    
>>> g = r""""""
... vp: {<vb.*><dt><nn.*>}
... """"""
>>> p = regexpparser(g); p.parse(tagged_text)
tree('s', [('this', 'dt'), ('has', 'vbz'), tree('vp', [('allowed', 'vbn'), ('the', 'dt'), ('device', 'nn')]), ('to', 'to'), ('start', 'vb'), (',', ','), ('and', 'cc'), ('i', 'prp'), ('then', 'rb'), ('see', 'vbp'), ('glitches', 'nns'), ('which', 'wdt'), ('is', 'vbz'), ('not', 'rb'), ('nice', 'jj'), ('.', '.')])

note that you're catching the tree(advp, [('then', 'rb'), ('see', 'vb')]),  because the tags are exactly rb and vb. so the wildcard in your grammar (i.e. `""""""advp: {}"""""") in this scenario is ignored.
also, if it's two different types of phrases, it's more advisable to use 2 labels not one. and (i think) end of string after wildcard is sort of redundant, so it's better to:
g = r""""""
vp:{<vb.*><dt><nn.*>} 
advp: {<rb.*><vb.*|jj.*|nn.*>}
""""""",https://stackoverflow.com/questions/34351978,python,18-12-2015 09:07,2246.0,5.0,1.0,True,21-06-2024 17:36,21-06-2024 17:36,Conceptual Questions
13892638,extracting the relationship between entities in stanford corenlp,"i want to extract the complete relationship between two entities using stanford corenlp (or maybe other tools).
for example:

windows is more popular than linux.
this tool requires java.
football is the most popular game in the world.

what is the fastest way? and what is the best practice for that?
thanks in advance","['nlp', 'stanford-nlp']",13924721,"you are probably looking for dependency relations between nouns. stanford parser provides such output. have a look here. you can combine what pete said (i.e. the pos graph) with the dependency graph to identify what relationship (for example, direct object or nominal subject, etc.) a pair of nouns (or noun phrases) share.",https://stackoverflow.com/questions/13892638,nlp,15-12-2012 13:30,8493.0,13.0,5.0,True,16-08-2023 12:11,15-12-2012 13:36,Uncategorized
76926213,nextjs with &quot;openai api&quot; got 401 error at endpoint createchatcompletion() but 200 ok at createcompletion(),"in short
nextjs api router failed to call openai api endpoint createchatcompletion() while pure node.js code can succeed.
detail
i use nextjs 's api router as coded here to call ""openai api"" endpoints using node.js package openai
i got 401 at calling endpoint createchatcompletion() view code - though succeeded 200 with createcompletion() view code

i double check with pure node.js code, and can succeed calling createchatcompletion(), strange...
what am i missing?
p.s.
my search in stack overflow and google search result with no helpful found","['next.js', 'openai-api', 'nextjs-api-router']",76933499,"as far as i know, nextjs define env in next.config.js
sample
  // ... other nextjs config
  env: {
    apiapp_host: ' 
    // ...
  }",https://stackoverflow.com/questions/76926213,next.js,18-08-2023 03:06,303.0,0.0,1.0,True,21-08-2023 00:06,21-08-2023 00:06,Conceptual Questions
77487437,open ai gpt api: how to get completion in between (fill-in-the-middle)?,"in this announcement:  an insert mode that lets you have completion in the middle of a text is introduced. how can i have it through the open ai gpt models' completion api?
for instance in the case of a code completion (which github copilot does currently):
def say_hello():
    print('hi', name)

should be completed with name argument inside the parentheses:
def say_hello(name):
    print('hi', name)","['openai-api', 'gpt-3']",77487438,"as insertion instruction guide explains, we can use suffix parameter to provide the text which is located after where you want to insert the new completion.
prompt = ""def say_hello(""
suffix = """"""):
  print('hi', name)""""""

response = client.completions.create(
    model=""gpt-3.5-turbo-instruct"",
    prompt=prompt,
    suffix=suffix,
    max_tokens=32
)
print(response.choices[0].text) # should be 'name'",https://stackoverflow.com/questions/77487437,openai-api,15-11-2023 11:45,1204.0,0.0,1.0,True,16-11-2023 09:48,16-11-2023 09:48,Implementation Issues
78712878,size mismatch for embed_out.weight: copying a param with shape torch.size([0]) from checkpoint - huggingface pytorch,"i want to finetune an llm. i am able to successfully finetune llm. but when reload the model after save, gets error. below is the code
import argparse
import numpy as np
import torch
from datasets import load_dataset
from transformers import autotokenizer, automodelforcausallm

from trl import dpotrainer, dpoconfig
def preprocess_data(item):
    return {
        'prompt': 'instruct: ' + item['prompt'] + '\n',
        'chosen': 'output: ' + item['chosen'],
        'rejected': 'output: ' + item['rejected']
    }        

def main():
    parser = argparse.argumentparser()
    parser.add_argument(""--epochs"", type=int, default=1)
    parser.add_argument(""--beta"", type=float, default=0.1)
    parser.add_argument(""--batch_size"", type=int, default=4)
    parser.add_argument(""--lr"", type=float, default=1e-6)
    parser.add_argument(""--seed"", type=int, default=2003)
    parser.add_argument(""--model_name"", type=str, default=""eleutherai/pythia-14m"")
    parser.add_argument(""--dataset_name"", type=str, default=""jondurbin/truthy-dpo-v0.1"")
    parser.add_argument(""--local_rank"", type=int, default=0)

    args = parser.parse_args()

    # determine device based on local_rank
    device = torch.device(""cuda"", args.local_rank) if torch.cuda.is_available() else torch.device(""cpu"")


    tokenizer = autotokenizer.from_pretrained(args.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    model = automodelforcausallm.from_pretrained(args.model_name).to(device)
    ref_model = automodelforcausallm.from_pretrained(args.model_name).to(device)

    dataset = load_dataset(args.dataset_name, split=""train"")
    dataset = dataset.map(preprocess_data)

    # split the dataset into training and validation sets
    dataset = dataset.train_test_split(test_size=0.1, seed=args.seed)
    train_dataset = dataset['train']
    val_dataset = dataset['test']

    training_args = dpoconfig(
        learning_rate=args.lr,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        logging_steps=10,
        remove_unused_columns=false,
        max_length=1024,
        max_prompt_length=512,
        fp16=true        
    )

    

    # verify and print embedding dimensions before finetuning
    print(""base model embedding dimension:"", model.config.hidden_size)

    model.train()
    ref_model.eval()

    dpo_trainer = dpotrainer(
        model,
        ref_model,
        beta=args.beta,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        args=training_args,
    )

    dpo_trainer.train()
    # evaluate
    evaluation_results = dpo_trainer.evaluate()
    print(""evaluation results:"", evaluation_results)

    save_model_name = 'finetuned_model'
    model.save_pretrained(save_model_name)

if __name__ == ""__main__"":
    main()

error i was getting as below
    return model_class.from_pretrained(
    file ""/.local/lib/python3.10/site-packages/transformers/modeling_utils.py"", line 3838, in from_pretrained
        ) = cls._load_pretrained_model(
    file ""/.local/lib/python3.10/site-packages/transformers/modeling_utils.py"", line 4349, in _load_pretrained_model
        raise runtimeerror(f""error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}"")
        runtimeerror: error(s) in loading state_dict for gptneoxforcausallm:
            size mismatch for gpt_neox.embed_in.weight: copying a param with shape torch.size([0]) from checkpoint, the shape in current model is torch.size([50304, 128]).
            size mismatch for embed_out.weight: copying a param with shape torch.size([0]) from checkpoint, the shape in current model is torch.size([50304, 128]).
            you may consider adding `ignore_mismatched_sizes=true` in the model `from_pretrained` method.

after finetuning,  model works perfectly. but after reloading the saved trained model its not working. any idea why gets this error when reloading the model ?","['pytorch', 'huggingface-transformers', 'large-language-model', 'huggingface']",78722271,"instead of
model.save_pretrained(save_model_name)

try this
dpo_trainer.save_model(save_model_name)",https://stackoverflow.com/questions/78712878,pytorch,05-07-2024 18:29,514.0,0.0,1.0,True,08-07-2024 17:46,08-07-2024 17:44,Uncategorized
18707401,natural language processing for smart homes,"i'm writing up a smart home software for my bachelor's degree, that will only simulate the actual house, but i'm stuck at the nlp part of the project. the idea is to have the client listen to voice inputs (already done), transform it into text (done) and send it to the server, which does all the heavy lifting / decision making.
so all my inputs will be fairly short (like ""please turn on the porch light""). based on this, i want to take the decision on which object to act, and how to act. so i came up with a few things to do, in order to write up something somewhat efficient.

get rid of unnecessary words (in the previous example ""please"" and ""the"" are words that don't change the meaning of what needs to be done; but if i say ""turn off my lights"", ""my"" does have a fairly important meaning).
deal with synonyms (""turn on lights"" should do the same as ""enable lights"" -- i know it's a stupid example). i'm guessing the only option is to have some kind of a dictionary (xml maybe), and just have a list of possible words for one particular object in the house.
detecting the verb and subject. ""turn on"" is the verb, and ""lights"" is the subject. i need a good way to detect this.
general implementation. how are these things usually developed in terms of algorithms? i only managed to find one article about nlp in smart homes, which was very vague (and had bad english). any links welcome.","['algorithm', 'nlp']",18707691,"for your project i would suggest you to go through stanford parser

from your problem definition i guess you don't need anything other then verbs and nouns. sp generates pos(part of speech tags) that you can use to prune the words that you don't require.

for this i can't think of any better option then what you have in mind right now.

for this again you can use grammatical dependency structure from sp and i am pretty much sure that it is good enough to tackle this problem.

this is where your research part lies. i guess you can find enough patterns using gd and pos tags to come up with an algorithm for your problem. i hardly doubt that any algorithm would be efficient enough to handle every set of input sentence(structured+unstructured) but something that is more that 85% accurate should be good enough for you.",https://stackoverflow.com/questions/18707401,algorithm,09-09-2013 21:35,1062.0,8.0,3.0,True,19-05-2023 02:41,19-05-2023 02:40,Implementation Issues
74804449,splitting sentences from a .txt file to .csv using nltk,"i have a corpus of newspaper articles in a .txt file, and i'm trying to split the sentences from it to a .csv in order to annotate each sentence.
i was told to use nltk for this purpose, and i found the following code for sentence splitting:
import nltk

from nltk.tokenize import sent_tokenize

sent_tokenize(""here is my first sentence. and that's a second one."")

however, i'm wondering:

how does one use a .txt file as an input for the tokenizer (so that i don't have to just copy and paste everything), and
how does one output a .csv file instead of just printing the sentences in my terminal.","['csv', 'nlp', 'nltk', 'txt', 'sentence']",74806058,"reading a .txt file & tokenizing its sentences
assuming the .txt file is located in the same folder as your python script, you can read a .txt file and tokenize the sentences using nltk as shown below:
from nltk import sent_tokenize

with open(""myfile.txt"") as file:
    textfile = file.read()

tokentextlist = sent_tokenize(textfile)
print(tokentextlist)
# output: ['here is my first sentence.', ""and that's a second one.""]


writing a list of sentence tokens to .csv file
there are a number of options for writing a .csv file. pick whichever is more convenient (e.g. if you already have pandas loaded, use the pandas option).
to write a .csv file using the pandas module:
import pandas as pd

df = pd.dataframe(tokentextlist)
df.to_csv(""mycsvfile.csv"", index=false, header=false)

to write a .csv file using the numpy module:
import numpy as np

np.savetxt(""mycsvfile.csv"", tokentextlist, delimiter="","", fmt=""%s"")

to write a .csv file using the csv module:
import csv

with open('mycsvfile.csv', 'w', newline='') as file:
    write = csv.writer(file, lineterminator='\n')
    # write.writerows([tokentextlist])
    write.writerows([[token] for token in tokentextlist]) # for pandas style output",https://stackoverflow.com/questions/74804449,csv,14-12-2022 21:40,307.0,1.0,1.0,True,15-12-2022 13:13,15-12-2022 13:13,Data Wrangling
76994550,no region has availability azure openai,"on the microsoft website it says that only the north usa zone is congested, but i tried it with:
australia,
east us,
east us2
and none of them work.
for each test i created a new resource imagining it to be this problem.
error

some of the tests","['azure', 'openai-api', 'azure-openai']",76994762,"sorry to disappoint you, it seems like its turned off for now, for all regions. fine-tuning is currently unavailable to new customers. this is mentioned in features overview
you can see fine-tuning models in the documentation is set to n/a


doc:",https://stackoverflow.com/questions/76994550,azure,28-08-2023 16:31,999.0,1.0,1.0,True,15-10-2023 16:06,15-10-2023 16:06,Uncategorized
72350021,spacy matcher isn&#39;t always matching,"i can't figure out why the matcher isn't working. this works:
test = [""14k""]

nlp = spacy.blank(""en"")
matcher = matcher(nlp.vocab)

matcher.add(""test"", [[{""norm"": ""14k""}]])

docs = []
for doc in nlp.pipe(test):
    matches = matcher(doc)
    print(matches)

but if i change 14k to 14k in both my matcher and text, the matcher finds nothing.  why? i just want to understand the difference and why this doesn't work and how i could go about troubleshooting this myself in the future.  i've looked at the docs:

and can't figure out where i'm going wrong.  i changed ""norm"" to orth and text and it still hasn't found it. thank you for any help.
edit
ok, so i did:
for ent in doc:
   print(ent)

and for the lowercase version, spacy was catorgizing it all as one ent, but when i uppercased the k, spacy says it two different ents. with this knowledge i did, matcher.add(""test"", [[{""orth"": ""14""}, {""orth"":""k""}]]) and it worked.
i still want to know why.  why does spacy think 14k is one ""word"" but 14k is two ""words""?","['python-3.x', 'spacy', 'spacy-3']",72357752,"it looks like you may be running into issues with differences in tokenization for this kind of sequence. in particular note that things that look like temperatures (so number + [fck]) may get special treatment. this may seem odd but it usually results in better compatibility with existing corpora.
you can find out why an input is tokenized a particular way using tokenizer.explain() like so:
import spacy

nlp = spacy.blank(""en"")
print(nlp.tokenizer.explain(""14k""))
print(""..."")
print(nlp.tokenizer.explain(""14k""))

that gives the output:
[('token', '14'), ('suffix', 'k')]
...
[('token', '14k')]

you can read more about this at the tokenizer.explain docs.",https://stackoverflow.com/questions/72350021,python-3.x,23-05-2022 14:11,552.0,1.0,1.0,True,24-05-2022 05:49,23-05-2022 14:41,Conceptual Questions
77622170,how can i execute openai&#39;s api with changed proxy in a better way?,"according to the way which openai officially offered, i can't execute the api because of some geographic reasons. although i think i have changed the proxies, it still cause error as apiconnectionerror and 429.
it is so weird because i believe i only send one request,
and when i do the same options in java before with okhttp (execute openai api),
it is 429 too. so weird!
import requests
import openai

proxies = {
# my proxy host
    ""
    ""
}
requests.session().proxies.update(proxies)

openai.api_key = ""myapikey""

completion = openai.chat.completions.create(
    model=""gpt-3.5"",
    messages=[{""role"": ""user""},
              {""content"": ""tell me about math""}]
)

print(completion)

so after i changed the way, and it succeeds finally.
here is code.
import requests
# openai api url
url = ""
headers = {
    ""authorization"": ""bearer myapikey"",
    ""content-type"": ""application/json""
}
proxies = {
# my proxy host
    "" ""
    "" ""
}
data = {
    ""prompt"": ""tell me about math"",
    ""max_tokens"": 60
}
response = requests.post(url, json=data, headers=headers, verify=false)

in this traditional way, i can use it successfull, but i still want to improve it,and i also have heard if you execute openai's api in this way, your apikey will be banned.
can anybody help me with this, like improve the executing way adding some other codes?
or how can i execute it with openai's officially package? i have stuckwith n it for while, waiting for your answer please. :)
that's the official fail record:

{'id': 'cmpl-8tcp9evm1poau8pbmybpfabjtbkaz', 'object': 'text_completion', 'created': 1701971499, 'model': 'gpt-3.5-turbo-instruct', 'choices': [{'text': '\n\nmath, also known as mathematics, is the study of numbers, quantity, and space. it is a fundamental subject in education and plays a crucial role in various fields such as science, engineering, and finance.\n\nthe study of math involves learning about mathematical concepts, theories, and techniques to solve problems', 'index': 0, 'logprobs': none, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 4, 'completion_tokens': 60, 'total_tokens': 64}}

that's the success record in traditional way.
traceback (most recent call last):
  file ""c:\user\pycharmprojects\pythonproject\main.py"", line 42, in <module>
    completion = openai.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""c:\user\pycharmprojects\pythonproject\venv\lib\site-packages\openai\_utils\_utils.py"", line 301, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  file ""c:\users\pycharmprojects\pythonproject\venv\lib\site-packages\openai\resources\chat\completions.py"", line 598, in create
    return self._post(
           ^^^^^^^^^^^
  file ""c:\users\pycharmprojects\pythonproject\venv\lib\site-packages\openai\_base_client.py"", line 1096, in post
    return cast(responset, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""c:\users\pycharmprojects\pythonproject\venv\lib\site-packages\openai\_base_client.py"", line 856, in request
    return self._request(
           ^^^^^^^^^^^^^^
  file ""c:\users\pycharmprojects\pythonproject\venv\lib\site-packages\openai\_base_client.py"", line 894, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  file ""c:\user\pycharmprojects\pythonproject\venv\lib\site-packages\openai\_base_client.py"", line 966, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  file ""c:\users\pycharmprojects\pythonproject\venv\lib\site-packages\openai\_base_client.py"", line 894, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  file ""c:\users\pycharmprojects\pythonproject\venv\lib\site-packages\openai\_base_client.py"", line 966, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  file ""c:\users\pycharmprojects\pythonproject\venv\lib\site-packages\openai\_base_client.py"", line 908, in _request
    raise self._make_status_error_from_response(err.response) from none
openai.ratelimiterror: error code: 429 - {'error': {'message': 'you exceeded your current quota, please check your plan and billing details.', 'type': 'insufficient_quota', 'param': none, 'code': 'insufficient_quota'}}","['python', 'python-requests', 'proxy', 'openai-api']",77658673,"these are the officially supported openai countries:

the client docs have an example of configuring a proxy:
import 
from openai import openai

client = openai(
    # or use the `openai_base_url` env var
    base_url=""
    
        proxies=""
        transport=
    ),
)",https://stackoverflow.com/questions/77622170,python,07-12-2023 17:57,2725.0,-1.0,1.0,True,14-12-2023 08:19,07-12-2023 19:47,Implementation Issues
58417374,how to load the saved tokenizer from pretrained model,"i fine-tuned a pretrained bert model in pytorch using huggingface transformer. all the training/validation is done on a gpu in cloud.
at the end of the training, i save the model and tokenizer like below:
best_model.save_pretrained('./saved_model/')
tokenizer.save_pretrained('./saved_model/')

this creates below files in the saved_model directory:
config.json
added_token.json
special_tokens_map.json
tokenizer_config.json
vocab.txt
pytorch_model.bin

now, i download the saved_model directory in my computer and want to load the model and tokenizer. i can load the model like below
model = torch.load('./saved_model/pytorch_model.bin',map_location=torch.device('cpu'))
but how do i load the tokenizer? i am new to pytorch and not sure because there are multiple files. probably i am not saving the model in the right way?","['machine-learning', 'pytorch', 'huggingface-transformers']",58427691,"if you look at the syntax, it is the directory of the pre-trained model that you are supposed to pass. hence, the correct way to load tokenizer must be:
tokenizer = berttokenizer.from_pretrained(<path to the directory containing pretrained model/tokenizer>)

in your case:
tokenizer = berttokenizer.from_pretrained('./saved_model/')

./saved_model here is the directory where you'll be saving your pretrained model and tokenizer.",https://stackoverflow.com/questions/58417374,machine-learning,16-10-2019 15:57,30477.0,12.0,2.0,True,28-11-2023 07:13,11-07-2021 19:00,Implementation Issues
76411359,openai api: how do i migrate from text-davinci-003 to gpt-3.5-turbo in nodejs?,"how do i migrate from text-davinci-003 to gpt-3.5-turbo?
what i tried to do is the following:
changing this...
model: ""text-davinci-003""

...to this.
model: ""gpt-3.5-turbo""

also, changing this...
const api_url = ""

...to this.
const api_url = ""

the problem is that it does not work. the code i will be giving is the unmodified code, so that anyone can help me what to change.
why i wanted this upgrade?
i was irritated by text-davinci-003's completion. like sending ""hello"" gives me an entire letter not a greeting.
live sample (via github pages):

github repository:","['openai-api', 'gpt-3', 'chatgpt-api', 'text-davinci-003']",76412710,"you're using the gpt-3.5-turbo model.
there are three main differences between the chat completions api (i.e., the gpt-3.5 api) and the completions api (i.e., the gpt-3 api).

api endpoint

completions api: 
chat completions api: 


the prompt parameter (completions api) is replaced by the messages parameter (chat completions api)
response access

completions api: response.choices[0].text.trim()
chat completions api: response.choices[0].message.content.trim()



try this:
const getchatresponse = async (incomingchatdiv) => {
    const api_url = "" /* changed */
    const pelement = document.createelement(""p"");

    // define the properties and data for the api request
    const requestoptions = {
        method: ""post"",
        headers: {
            ""content-type"": ""application/json"",
            ""authorization"": `bearer ${api_key}`
        },
        body: json.stringify({
            model: ""gpt-3.5-turbo"",
            messages: [{role: ""user"", content: `${usertext}`}], /* changed */
            max_tokens: 2048,
            temperature: 0.2,
            n: 1,
            stop: null
        })
    }

    // send post request to api, get response and set the reponse as paragraph element text
    try {
        const response = await (await fetch(api_url, requestoptions)).json();
        pelement.textcontent = response.choices[0].message.content.trim(); /* changed */
    } catch (error) { // add error class to the paragraph element and set error text
        pelement.classlist.add(""error"");
        pelement.textcontent = ""oops! something went wrong while retrieving the response. please try again."";
    }

    // remove the typing animation, append the paragraph element and save the chats to local storage
    incomingchatdiv.queryselector("".typing-animation"").remove();
    incomingchatdiv.queryselector("".chat-details"").appendchild(pelement);
    localstorage.setitem(""all-chats-thedoggybrad"", chatcontainer.innerhtml);
    chatcontainer.scrollto(0, chatcontainer.scrollheight);
}",https://stackoverflow.com/questions/76411359,openai-api,06-06-2023 03:56,1326.0,0.0,1.0,True,14-09-2023 15:02,07-06-2023 10:50,Implementation Issues
63312859,how to change huggingface transformers default cache directory?,"the default cache directory lacks disk capacity, i need to change the configuration of the default cache directory. how can i do that?",['huggingface-transformers'],63314437,"you can specify the cache directory whenever you load a model with .from_pretrained by setting the parameter cache_dir. you can also set a default location by exporting an environment variable hf_home each time before you use the library (i.e. before importing it!).
python example:
import os
os.environ['hf_home'] = '/blabla/cache/'

bash example:
export hf_home=/blabla/cache/

windows example:
set hf_home=e:\huggingface_cache

google colab example (export via os works fine but not the bash variant. an alternative are the magic commands):
%env hf_home=/blabla/cache/

transformers <v4.0.0
use the variable transformers_cache instead of hf_home. you can also use it in v4.0.0 <= transformers <= v5.0.0 but starting from v4.36.0 you will see the following warning:

futurewarning: using transformers_cache is deprecated and will be
removed in v5 of transformers. use hf_home instead.",https://stackoverflow.com/questions/63312859,huggingface-transformers,08-08-2020 07:28,229148.0,135.0,8.0,True,07-04-2025 13:56,09-02-2025 22:26,Implementation Issues
76982659,how to get attentions part from the output of a bert model?,"i am using bert-model for  query expansion and i am trying to extract the keywords from the document i have
tokenizer = berttokenizer.from_pretrained(""bert-base-uncased"")
model = bertmodel.from_pretrained(""bert-base-uncased"")
sentence=""this is a sentence""
tokens = tokenizer.tokenize(sentence)
print(tokens,""-tokens"")
input_ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(\[input_ids\])
print(input_ids)
with torch.no_grad():
output = model(input_ids)
attention_scores = output.attentions
print(attention_scores)   #this prints none

this is my code i am using a simple sentence and trying to extract keywords from it in order to do that i need attention of the output part of bert-model
i tried with different tokenizer methods(tokenize,encode,encode_plus) to tokenize and tried with
different bert variants (bert-large-uncased)
i want to extract the attention part form the model output but i am not able to do that
i get none in that place i am not able to get any value in the attention part of  the output","['python', 'nlp', 'huggingface-transformers', 'bert-language-model', 'word-embedding']",76983149,"you can't really extract keywords from the code you provided. your output has no attribute called attentions, that's why output.attentions is returning none, hence the error you're facing.
i benchmarked some transformer models for keyword extraction last year for my university project. i will provide you with a solution from there. this script will return you the extracted keywords with their score as a dictionary.
solution

install the dependencies

pip install nltk
pip install spacy
pip install torch
pip install transformers


necessary imports

from transformers import automodel, autotokenizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.stem.wordnet import wordnetlemmatizer
import re
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import countvectorizer
import spacy

nlp = spacy.load('en_core_web_sm')
nltk.download('stopwords')
nltk.download('wordnet')
stop_words = set(stopwords.words('english'))


utility functions

def pre_process(text):
    # lowercase
    text=text.lower()
    #remove tags
    text=re.sub(""&lt;/?.*?&gt;"","" &lt;&gt; "",text)
    # remove special characters and digits
    text=re.sub(""(\\d|\\w)+"","" "",text)
    ##convert to list from string
    text = text.split()
    # remove stopwords
    text = [word for word in text if word not in stop_words]
    # remove words less than three letters
    text = [word for word in text if len(word) >= 3]
    #lemmatize
    lmtzr = wordnetlemmatizer()
    text = [lmtzr.lemmatize(word) for word in text]
    return ' '.join(text)

def get_candidates(text):
    # group of words
    n_gram_range = (1, 1)

    # extract candidate words/phrases
    count = countvectorizer(ngram_range=n_gram_range, stop_words=""english"").fit([text])
    all_candidates = count.get_feature_names_out()
    
    doc = nlp(text)
    noun_phrases = set(chunk.text.strip() for chunk in doc.noun_chunks)

    # only pass the noun/adjective keywords for extraction
    noun_adj = set()
    for token in doc:
        if token.pos_ == ""noun"":
            noun_adj.add(token.text)
        if token.pos_ == ""adj"":
            noun_adj.add(token.text)

    all_words = noun_adj.union(noun_phrases)
    candidates = list(filter(lambda candidate: candidate in all_words, all_candidates))
    return candidates


keyword extraction method

# the keyword extraction method
def keyword_extract(string, model_name):

    # obtaining candidate keywords and document representations
    model = automodel.from_pretrained(model_name)
    tokenizer = autotokenizer.from_pretrained(model_name)

    text=pre_process(string)
    candidates=get_candidates(text)
    candidate_tokens = tokenizer(candidates, padding=true, return_tensors=""pt"")
    candidate_embeddings = model(**candidate_tokens)[""pooler_output""]

    # determination of keywords:
    text_tokens = tokenizer([text], padding=true, return_tensors=""pt"", truncation=true, max_length=512)
    text_embedding = model(**text_tokens)[""pooler_output""]

    candidate_embeddings = candidate_embeddings.detach().numpy()
    text_embedding = text_embedding.detach().numpy()

    distances = cosine_similarity(text_embedding, candidate_embeddings)
    keywords = {i:j for i,j in zip(candidates, distances[0])}

    # sort based on attention score and return the dictionary
    return dict(sorted(keywords.items(), key=lambda x:x[1], reverse=true))

# put the model name here
model_name = ""bert-base-uncased""

my_sentence = ""hello there! this is an example sentence and this is the code that i'm using to extract keywords from a sentence""

keywords = keyword_extract(my_sentence, model_name)
print(keywords)

output
output returned the keywords sorted according to their score
{'keywords': 0.97554314, 'example': 0.94142365, 'extract': 0.885766, 'code': 0.83722556, 'sentence': 0.76782566}",https://stackoverflow.com/questions/76982659,python,26-08-2023 11:00,560.0,0.0,1.0,True,27-08-2023 17:45,27-08-2023 17:45,Implementation Issues
61633485,extract noun phrases with stanza and corenlpclient,"i am trying to extract noun phrases from sentences using stanza(with stanford corenlp). this can only be done with the corenlpclient module in stanza. 
# import client module
from stanza.server import corenlpclient
# construct a corenlpclient with some basic annotators, a memory allocation of 4gb, and port number 9001
client = corenlpclient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse'], memory='4g', endpoint='

here is an example of a sentence, and i am using the tregrex function in client to get all the noun phrases. tregex function returns a dict of dicts in python. thus i needed to process the output of the tregrex before passing it to the tree.fromstring function in nltk to correctly extract the noun phrases as strings. 
pattern = 'np'
text = ""albert einstein was a german-born theoretical physicist. he developed the theory of relativity.""
matches = client.tregrex(text, pattern) ``

hence, i came up with the method stanza_phrases which has to loop through the dict of dicts which is the output of tregrex and correctly format for tree.fromstring in nltk.
def stanza_phrases(matches):
  nps = []
  for match in matches:
    for items in matches['sentences']:
      for keys,values in items.items():
        s = '(root\n'+ values['match']+')'
        nps.extend(extract_phrase(s, pattern))
  return set(nps)

generates a tree to be used by nltk 
from nltk.tree import tree
def extract_phrase(tree_str, label):
    phrases = []
    trees = tree.fromstring(tree_str)
    for tree in trees:
        for subtree in tree.subtrees():
            if subtree.label() == label:
                t = subtree
                t = ' '.join(t.leaves())
                phrases.append(t)

    return phrases

here is my output:
{'albert einstein', 'he', 'a german-born theoretical physicist', 'relativity',  'the theory', 'the theory of relativity'}

is there a way i can make this more code efficient with less number of lines (especially, stanza_phrases and extract_phrase methods)","['python', 'nlp', 'stanford-nlp', 'stanford-stanza']",61648173,"from stanza.server import corenlpclient

# get noun phrases with tregex
def noun_phrases(_client, _text, _annotators=none):
    pattern = 'np'
    matches = _client.tregex(_text,pattern,annotators=_annotators)
    print(""\n"".join([""\t""+sentence[match_id]['spanstring'] for sentence in matches['sentences'] for match_id in sentence]))

# english example
with corenlpclient(timeout=30000, memory='16g') as client:
    englishtext = ""albert einstein was a german-born theoretical physicist. he developed the theory of relativity.""
    print('---')
    print(englishtext)
    noun_phrases(client,englishtext,_annotators=""tokenize,ssplit,pos,lemma,parse"")

# french example
with corenlpclient(properties='french', timeout=30000, memory='16g') as client:
    frenchtext = ""je suis john.""
    print('---')
    print(frenchtext)
    noun_phrases(client,frenchtext,_annotators=""tokenize,ssplit,mwt,pos,lemma,parse"")",https://stackoverflow.com/questions/61633485,python,06-05-2020 11:00,2898.0,4.0,2.0,True,16-01-2023 20:58,19-05-2020 08:34,Preprocessing Tasks
71768061,huggingface transformers classification using num_labels 1 vs 2,"question 1)
the answer to this question suggested that for a binary classification problem i could use num_labels as 1 (positive or not) or 2 (positive and negative). is there any guideline regarding which setting is better? it seems that if we use 1 then probability would be calculated using sigmoid function and if we use 2 then probabilities would be calculated using softmax function.
question 2)
in both cases are my y labels going to be same? each data point will have 0 or 1 and not one hot encoding? for example, if i have 2 data points then y would be 0,1 and not [0,0],[0,1]
i have very unbalanced classification problem where class 1 is present only 2% of times. in my training data i am oversampling
question 3)
my data is in pandas dataframe and i am converting it to a dataset and creating y variable using below. how should i cast my y column - label if i am planning to use num_labels=1?
`train_dataset=dataset.from_pandas(train_df).cast_column(""label"", classlabel(num_classes=2, names=['neg', 'pos'], names_file=none, id=none))`","['python', 'classification', 'huggingface-transformers']",72510500,"well, it probably is kind of late. but i want to point out one thing, according to the hugging face code, if you set num_labels = 1, it will actually trigger the regression modeling, and the loss function will be set to mseloss(). you can find the code here.
also, in their own tutorial, for a binary classification problem (imdb, positive vs. negative), they set num_labels = 2.
from transformers import automodelforsequenceclassification, trainingarguments, trainer
model = automodelforsequenceclassification.from_pretrained(""distilbert-base-uncased"", num_labels=2)

here is the link.",https://stackoverflow.com/questions/71768061,python,06-04-2022 13:53,6986.0,5.0,2.0,True,05-06-2022 19:26,08-04-2022 17:47,Implementation Issues
71189665,change value in pandas dataframe using iteration,"i have a training data set of the following format:
print(data.head(5))
#output



           0  1
0  a b c d e  1
1  a b c d e  1
2  a b c d e  1
3  a b c d e  1
4  a b c d e  1

it is a text classification task and i am trying to split the text ""a b c d e"" in to a python list. i tried iteration:
data #the dataset
len_data = len(data)
for row_num in range(len_data):
    data.loc[row_num, 0] = data.loc[row_num, 0].split("" "")

however this doesn't work and returned the error must have equal len keys and value when setting with an iterable. could someone help me with this problem? many thanks!","['python', 'pandas', 'dataframe', 'nlp']",71189717,"use str.split:
df[0] = df[0].str.split()
print(df)

# output
                 0  1
0  [a, b, c, d, e]  1
1  [a, b, c, d, e]  1
2  [a, b, c, d, e]  1
3  [a, b, c, d, e]  1
4  [a, b, c, d, e]  1

setup:
data = {0: {0: 'a b c d e', 1: 'a b c d e'}, 1: {0: 1, 1: 1}}
df = pd.dataframe(data)",https://stackoverflow.com/questions/71189665,python,19-02-2022 22:15,79.0,1.0,1.0,True,19-02-2022 23:51,19-02-2022 23:51,Implementation Issues
70064477,how to handle lemmatizertrainer &#39;utfdataformatexception: encoded string too long&#39;?,"i am using opennlp to train a model for lemmatization of german words. therefore i use the opennlp cli and the training set of ud_german-hdt which can be downloaded here
the training itself works fine (just need a little bit of ram) but the cli fails to write the model because of an utfdataformatexception: encoded string too long exception.
the cli command i am using: opennlp lemmatizertrainerme.conllu -params params.txt -lang de -model de-lemmatizer.bin -data ud_german-hdt/de_hdt-ud-train.conllu -encoding utf-8
stacktrace:
writing lemmatizer model ... failed
error during writing model file 'de-lemmatizer.bin'
encoded string too long: 383769 bytes
java.io.utfdataformatexception: encoded string too long: 383769 bytes
        at java.base/java.io.dataoutputstream.writeutf(dataoutputstream.java:364)
        at java.base/java.io.dataoutputstream.writeutf(dataoutputstream.java:323)
        at opennlp.tools.ml.maxent.io.binarygismodelwriter.writeutf(binarygismodelwriter.java:71)
        at opennlp.tools.ml.maxent.io.gismodelwriter.persist(gismodelwriter.java:97)
        at opennlp.tools.ml.model.genericmodelwriter.persist(genericmodelwriter.java:75)
        at opennlp.tools.util.model.modelutil.writemodel(modelutil.java:71)
        at opennlp.tools.util.model.genericmodelserializer.serialize(genericmodelserializer.java:36)
        at opennlp.tools.util.model.genericmodelserializer.serialize(genericmodelserializer.java:29)
        at opennlp.tools.util.model.basemodel.serialize(basemodel.java:597)
        at opennlp.tools.cmdline.cmdlineutil.writemodel(cmdlineutil.java:182)
        at opennlp.tools.cmdline.lemmatizer.lemmatizertrainertool.run(lemmatizertrainertool.java:77)
        at opennlp.tools.cmdline.cli.main(cli.java:256)

has somebody encountered this problem and has a solution?","['java', 'nlp', 'opennlp', 'lemmatization']",74253692,"recently, i've written a patch to cure opennlp-1366. the related pr  documents the problem and solution in detail.
in this context, the upcoming opennlp version 2.0.1 will bring the cure for the problem reported in the op. updating to the aforementioned version will resolve the crashing during writing trained model files.
note: 
i verified that the patch works with ud_german-hdt, ud_german-gsd, and other treebanks for the german language.",https://stackoverflow.com/questions/70064477,java,22-11-2021 10:41,473.0,1.0,1.0,True,30-10-2022 14:05,30-10-2022 14:05,Implementation Issues
78525417,&quot;deadline&quot; error when embedding video with google vertex ai multimodal embedding modal,"i am currently using vertex ai's multimodal embedding model (
i was able to get the image and text examples running just fine using the python sdk and post, however, when i try to run the following video example (from the above link), i get the following error
    import vertexai
    
    from vertexai.vision_models import multimodalembeddingmodel, video
    
    project_id = 'project_name'
    location = 'us-central1'
    
    vertexai.init(project=project_id, location=location)
    
    # document metadata
    video_path = 'gs://test-public-bucket-123/dog_jumping_short.mp4' # public small 1 mb 7 second video file
    description = 'dogs jumping'
    
    model = multimodalembeddingmodel.from_pretrained(""multimodalembedding@001"")
    
    video = video.load_from_file(video_path)
    
    embeddings = model.get_embeddings(
        video=video
    )
    
    # video embeddings are segmented based on the video_segment_config.
    print(""video embeddings:"")
    for video_embedding in embeddings.video_embeddings:
        print(
            f""video segment: {video_embedding.start_offset_sec} - {video_embedding.end_offset_sec}""
        )
        print(f""embedding: {video_embedding.embedding}"")
    
    print(f""text embedding: {embeddings.text_embedding}"")

the error i get when running this sample code is related to deadline
traceback (most recent call last):
  file ""/users/me/code/multi-modal-test/multi-modal/lib/python3.12/site-packages/google/api_core/grpc_helpers.py"", line 76, in error_remapped_callable
    return callable_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""/users/me/code/multi-modal-test/multi-modal/lib/python3.12/site-packages/grpc/_channel.py"", line 1181, in __call__
    return _end_unary_response_blocking(state, call, false, none)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""/users/me/code/multi-modal-test/multi-modal/lib/python3.12/site-packages/grpc/_channel.py"", line 1006, in _end_unary_response_blocking
    raise _inactiverpcerror(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._inactiverpcerror: <_inactiverpcerror of rpc that terminated with:
        status = statuscode.unauthenticated
        details = ""video embedding failed with the following error: deadline""
        debug_error_string = ""unknown:error received from peer ipv4:142.250.81.234:443 {grpc_message:""video embedding failed with the following error: deadline"", grpc_status:16, created_time:""2024-05-23t15:53:15.429704-04:00""}""

note that i have been able to embed contextual_text and image just fine with the same authentication, so i am fairly certain this has nothing to do with authentication even though the error says so.
i have also tried to post using the following curl, but i get the same error response deadline
    curl -x post \ 
     -h ""authorization: bearer $(gcloud auth print-access-token)"" \
     -h ""content-type: application/json; charset=utf-8"" \
     -d @request.json \
     ""

i have the following apis enabled (note that according to docs, i only need to have vertex ai api enabled)

these are my iam permissions
- members:
  - user:me@gmail.com
  role: roles/aiplatform.admin
- members:
  - user:me@gmail.com
  role: roles/aiplatform.user
- members:
  - user:me@gmail.com
  role: roles/ml.admin
- members:
  - user:me@gmail.com
  role: roles/owner
- members:
  - user:me@gmail.com
  role: roles/storage.admin
- members:
  - user:me@gmail.com
  role: roles/visionai.admin
etag: bwyajsgtzok=
version: 1

i am not using a service account, but a user account.
anyone know what i can do here?","['google-cloud-platform', 'word-embedding', 'google-cloud-vertex-ai', 'video-embedding']",78618078,"this might come from a discrepancy between the ""current project"" of gcloud, and the ""application defaults credentials"". this can be confusing because the exact order of these commands matter.
to make sure, could you try in this order:
1.
gcloud config set project my_project_id

(doc)
2.
gcloud auth application-default login

(doc)

run your python program (or your curl command)",https://stackoverflow.com/questions/78525417,google-cloud-platform,23-05-2024 20:27,485.0,2.0,4.0,True,15-08-2024 12:29,05-06-2024 17:49,Implementation Issues
71099545,failedpreconditionerror: table not initialized,"i am trying to create an nlp neural-network using the following code:
imports:
import zipfile
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import textvectorization,embedding,input,globalaveragepooling1d,dense
from sklearn.model_selection import train_test_split

download and unzip dataset:
# download data (same as from kaggle)
!wget ""

# unzip data
zip_ref = zipfile.zipfile(""nlp_getting_started.zip"", ""r"")
zip_ref.extractall()
zip_ref.close()

split data into train and test datasets:
    train_df = pd.read_csv(""train.csv"")
    
    train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df['text'], train_df['target'],train_size=.8)
    average_output_sequence_length = 15

                                           

create the neural-network:
input = input(shape=(1,),dtype='string')
x = textvectorization(max_tokens=10000,
                      ngrams=5,
                      standardize='lower_and_strip_punctuation',
                      output_mode='int',
                      output_sequence_length = average_output_sequence_length)(input)
x = embedding(input_dim=22,embeddings_initializer='uniform',output_dim=128, name= 'embeding_layer')(x)
x = globalaveragepooling1d()(x)
output = dense(1,activation='sigmoid')(x)

model = tf.keras.model(input,output)

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'] )

model.fit(x=train_sentences,y=train_labels,epochs=6,validation_data=(val_sentences,val_labels))

unfortunately, when i run the code, i face with the following error:
epoch 1/6
---------------------------------------------------------------------------
failedpreconditionerror                   traceback (most recent call last)
<ipython-input-52-991547d73612> in <module>()
     13 model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'] )
     14 
---> 15 model.fit(x=train_sentences,y=train_labels,epochs=6,validation_data=(val_sentences,val_labels))

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     57     ctx.ensure_initialized()
     58     tensors = pywrap_tfe.tfe_py_execute(ctx._handle, device_name, op_name,
---> 59                                         inputs, attrs, num_outputs)
     60   except core._notokstatusexception as e:
     61     if name is not none:

failedpreconditionerror:  table not initialized.
     [[node model_6/text_vectorization_7/string_lookup_7/none_lookup/lookuptablefindv2
 (defined at /usr/local/lib/python3.7/dist-packages/keras/layers/preprocessing/index_lookup.py:669)
]] [op:__inference_train_function_5066]

errors may have originated from an input operation.

update:
i got it to work when i change the way that i use the functional code:
create textvectorization function:
text_vectorization_layer =  textvectorization(max_tokens=10000,
                                              ngrams=5,
                                              standardize='lower_and_strip_punctuation',
                                              output_mode='int',
                                              output_sequence_length = average_output_sequence_length
                                              )

and then finally create the neural-network:
input = input(shape=(1,),dtype='string')
x = text_vectorization_layer(input)
x = embedding(input_dim=22,embeddings_initializer='uniform',output_dim=128, name= 'embeding_layer')(x)
x = globalaveragepooling1d()(x)
output = dense(1,activation='sigmoid')(x)

model = tf.keras.model(input,output)

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'] )

model.fit(x=train_sentences,y=train_labels,epochs=6,validation_data=(val_sentences,val_labels))

question:
but, i still do not understand why these change fix the issue ?
in other word, why the:
x = textvectorization(input)

and the
x = textvectorization(max_tokens=10000,
                      ngrams=5,
                      standardize='lower_and_strip_punctuation',
                      output_mode='int',
                      output_sequence_length = average_output_sequence_length)(input)

are not equal ?","['python', 'tensorflow', 'keras', 'deep-learning', 'nlp']",71099954,"the textvectorization layer is a preprocessing layer that needs to be instantiated before being called. also as the docs explain:

the vocabulary for the layer must be either supplied on construction
or learned via adapt().

another important information can be found here:

crucially, these layers are non-trainable. their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by ""adapting"" them on data

furthermore, it is important to note, that the textvectorization layer uses an underlying stringlookup layer that also needs to be initialized beforehand. otherwise, you will get the failedpreconditionerror: table not initialized as you posted.",https://stackoverflow.com/questions/71099545,python,13-02-2022 09:55,2121.0,1.0,1.0,True,13-02-2022 11:58,13-02-2022 11:34,Conceptual Questions
72917714,combinations of words that co-occur most often across strings,"here's the problem...
i have a list of strings:
strings = ['one two three four', 'one two four five', 'four one two', 'three four']

i'm trying to find combinations of words that co-occur in two or more strings.
and here's the output i'm trying to get...

[one, two, four] - 3 times
[three, four] - 2 times
[one, two] - 3 times
[two, four] - 3 times

the combinations could be any length of two or more words.
here's what i've already looked at - though i'm not having much luck finding anything i can bootstrap for my needs : (

text processing to find co-occurences of strings

efficient way of extracting co-occurence values of specific word pairs from python counter() results","['python', 'pandas', 'nlp']",72917799,"you can compute the powersets with minimum 2 combinations and count the combinations:
from itertools import chain, combinations
from collections import counter

# 
def powerset(iterable, min=2):
    ""powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)""
    s = list(iterable)
    return chain.from_iterable(combinations(s, r) for r in range(min, len(s)+1))

c = counter(chain.from_iterable(set(powerset(s.split()))
            for s in strings))

# keep counts of 2 or more
out = {k: v for k, v in c.items() if v >= 2}

output:
{('three', 'four'): 2, 
 ('two', 'four'): 2, 
 ('one', 'two', 'four'): 2, 
 ('one', 'four'): 2, 
 ('one', 'two'): 3}

keep order
use:
c = counter(chain.from_iterable(tuple(powerset(s.split()))
            for s in strings))",https://stackoverflow.com/questions/72917714,python,08-07-2022 22:53,790.0,2.0,1.0,True,08-07-2022 23:42,08-07-2022 23:14,Implementation Issues
75874218,fails to generate image using openai,"i am trying to take a text from discord user and then generating an image of that. i made a code for that but openai shows 400 error.
error:
error: request failed with status code 400
    at createerror (/users/anshtyagi/documents/backup/s/node_modules/openai/node_modules/axios/lib/core/createerror.js:16:15)
    at settle (/users/anshtyagi/documents/backup/s/node_modules/openai/node_modules/axios/lib/core/settle.js:17:12)
    at incomingmessage.handlestreamend (/users/anshtyagi/documents/backup/s/node_modules/openai/node_modules/axios/lib/adapters/
    at incomingmessage.emit (node:events:538:35)
    at endreadablent (node:internal/streams/readable:1345:12)
    at processticksandrejections (node:internal/process/task_queues:83:21)

code:
const { configuration, openaiapi } = require('openai')
const configuration = new configuration({
    apikey: 'sk-aqe0ero77gonnsbr2n7'
})
const openai = new openaiapi(configuration);

let text = interaction.options.getstring(`text`)
const response = await openai.createimage({
  prompt: text,
  n: 1,
  size: '1024x1024',
});
let imageurl = response.data.data[0].url;
const embed = new embedbuilder()
  .settitle('generated image')
  .setimage(imageurl);
interaction.editreply(embed)
  .catch(error => {
    interaction.editreply('failed to generate image');
  });

i have taking prompt as text from discord slash command options
i don't know what is this error for please help me fix this.","['javascript', 'openai-api']",76564074,that was a issue with openai which is now resolved it will work now.,https://stackoverflow.com/questions/75874218,javascript,29-03-2023 07:16,686.0,0.0,1.0,True,27-06-2023 10:46,29-03-2023 15:59,Uncategorized
60290640,embedding layer in keras: vocab size +1,"from a number of examples i have seen, when we use text_tokenizer from keras, when specifying the input size for the input layer, we use vocab size +1.  this naturally yields an embedding space with +1 'rows'.  
for example, i fit a simple model to estimate the embedding vectors for a vocab of size 3 = i like turtles.  the embedding space has length 5 per word in our vocabulary.
the embedding weights are:
0.01209533  0.034303080 -0.04666784 0.02803965  -0.03691160
-0.01302978 -0.030584216    -0.02506201 0.04771456  0.01906699
0.02800793  0.042204402 0.05223191  -0.01184921 0.02000498
0.02692273  -0.008792922    0.01560913  -0.02783649 0.02692282


my question:  i assume that the first ""row"" in our matrix is the 0-based vector, such that rows 2, 3, and 4 would be associated with ""i"", ""like"", and ""turtles"" respectively.   
is this the case?  i want to ensure that i align my vocabulary properly, but i haven't been able to pin down any documentation to confirm this assumption.","['r', 'keras', 'tensorflow2.0', 'word-embedding']",60294856,"i understand that you are wanting to extract the embedding for each word, but i think the real question is: what is the output the tokenizer is producing.
also, that tokenizer is a bit of a mess. you'll see what i mean below.
because the tokenizer will filter words (assuming a non-trivial vocabulary), i don't want to assume that the words are stored in the order in which they are found. so here i programmatically determine the vocabulary using word_index. i then explicitly check what words are tokenized after filtering for the most frequently used words. (word_index remembers all words; i.e. the pre-filtered values.)
import tensorflow as tf
from tensorflow.keras.preprocessing.text import tokenizer
corpus = 'i like turtles'
num_words = len(corpus.split())
oov = 'oov'
tokenizer = tokenizer(num_words=num_words + 2, oov_token=oov)
tokenizer.fit_on_texts(corpus.split())
print(f'word_index: {tokenizer.word_index}')
print(f'vocabulary: {tokenizer.word_index.keys()}')
text = [key for key in tokenizer.word_index.keys()]
print(f'keys: {text}: {tokenizer.texts_to_sequences(text)}')

text = 'i like turtles'.split()
print(f'{text}: {tokenizer.texts_to_sequences(text)}')

text = 'i like marshmallows'.split() 
print(f'{text}: {tokenizer.texts_to_sequences(text)}')

this produces the following output:
word_index: {'oov': 1, 'i': 2, 'like': 3, 'turtles': 4}
vocabulary: dict_keys(['oov', 'i', 'like', 'turtles'])
keys: ['oov', 'i', 'like', 'turtles']: [[1], [2], [3], [4]]
['i', 'like', 'turtles']: [[2], [3], [4]]
['i', 'like', 'marshmallows']: [[2], [3], [1]]

however, if you specify oov_token, the output looks like this:
{'oov': 1, 'i': 2, 'like': 3, 'turtles': 4}

notice how i had to specify num_words=num_words + 2 instead of the expected '+1'.
that's because we're explicitly defining an oov token, which gets added to the vocabulary, which is a bit nuts imo.
if you specify an oov token and you set num_words=num_words + 1 (as documented), then 'i like turtles' gets the same encoding as 'i like marshmallows'. also nuts.
hopefully, you now have to tools to know what the tokenizer is feeding the encoding layer. then hopefully, it'll be trivial to correlate the tokens with their embeddings.
please let us know what you find. :) 
(for more on the madness, check out this stackoverflow post.)",https://stackoverflow.com/questions/60290640,r,18-02-2020 22:57,3515.0,4.0,2.0,True,26-11-2022 21:29,19-02-2020 04:13,Conceptual Questions
66712753,how to use languagedetector() from spacy_langdetect package?,"i'm trying to use the spacy_langdetect package and the only example code i can find is (
import spacy
from spacy_langdetect import languagedetector
nlp = spacy.load(""en_core_web_sm"")
nlp.add_pipe(languagedetector(), name='language_detector', last=true)
text = 'this is an english text.'
doc = nlp(text)
print(doc._.language)

it's throwing error:

nlp.add_pipe now takes the string name of the registered component factory, not a callable component.

so i tried using the below for adding to my nlp pipeline
language_detector = languagedetector()
nlp.add_pipe(""language_detector"")

but this gives error:

can't find factory for 'language_detector' for language english (en). this usually happens when spacy calls nlp.create_pipe with a custom component name that's not registered on the current language class. if you're using a transformer, make sure to install 'spacy-transformers'. if you're using a custom component, make sure you've added the decorator @language.component (for function components) or @language.factory (for class components).
available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer

i don't fully understand how to add it since it's not really a custom component.","['python', 'spacy']",66727355,"with spacy v3.0 for components not built-in such as languagedetector, you will have to wrap it into a function prior to adding it to the nlp pipe. in your example, you can do the following:
import spacy
from spacy.language import language
from spacy_langdetect import languagedetector

def get_lang_detector(nlp, name):
    return languagedetector()

nlp = spacy.load(""en_core_web_sm"")
language.factory(""language_detector"", func=get_lang_detector)
nlp.add_pipe('language_detector', last=true)
text = 'this is an english text.'
doc = nlp(text)
print(doc._.language)

for built-in components (i.e. tagger, parser, ner, etc.), see:",https://stackoverflow.com/questions/66712753,python,19-03-2021 17:17,17146.0,16.0,2.0,True,14-03-2023 19:22,14-03-2023 19:22,Implementation Issues
53612938,spacy nlp word.pos returns digits instead of pos tags,"i am using spacy library for pos tagging but when i run this code, it returns numbers in the place of the pos tags:
import spacy
from spacy.lang.fr.examples import sentences

nlp = spacy.load('en')
mystring = "" i am missing my lovely family a lot.""
exuu = nlp(mystring)
for word in exuu: 
  print(word.text, word.pos)

here is how the output looks like:
102
i 94
am 99
missing 99
my 83
dear 83
family 91
a 89
lot 91
. 96","['python', 'python-3.x', 'nlp', 'spacy', 'part-of-speech']",54382451,"you are reading the ""wrong"" attribute. word.pos returns the pos tag id, not the pos tag string. to do what you want, just replace word.pos with word.pos_.
the following code will work fine:
import spacy
from spacy.lang.fr.examples import sentences
nlp = spacy.load('en')
mystring = "" i am missing my lovely family a lot.""
exuu = nlp(mystring)
for word in exuu: 
  print(word.text, word.pos_)",https://stackoverflow.com/questions/53612938,python,04-12-2018 12:23,235.0,3.0,1.0,True,08-07-2021 02:35,08-07-2021 02:35,Data Wrangling
73127139,equivalent to tokenizer() in transformers 2.5.0?,"i am trying to convert the following code to work with transformers 2.5.0. as written, it works in version 4.18.0, but not 2.5.0.
# converting pretrained bert classification model to regression model
# i.e. extracting base model and swapping out heads

from transformers import berttokenizer, bertmodel, bertconfig, bertformaskedlm, bertforsequenceclassification, autoconfig, automodelfortokenclassification
import torch
import numpy as np

old_model = bertforsequenceclassification.from_pretrained(""textattack/bert-base-uncased-yelp-polarity"")
model = bertforsequenceclassification.from_pretrained(""bert-base-uncased"", num_labels=1) 
model.bert = old_model.bert

# ensure that model parameters are equivalent except for classifier head layer
for param_name in model.state_dict():
    if 'classifier' not in param_name:
        sub_param, full_param = model.state_dict()[param_name], old_model.state_dict()[param_name] # type: torch.tensor, torch.tensor
        assert (sub_param.cpu().numpy() == full_param.cpu().numpy()).all(), param_name


tokenizer = berttokenizer.from_pretrained(""textattack/bert-base-uncased-yelp-polarity"")
inputs = tokenizer(""hello, my dog is cute"", return_tensors=""pt"")

with torch.no_grad():
    logits = model(**inputs).logits

output_value = np.array(logits)[0][0]
print(output_value)

tokenizer is not callable with transformers 2.5.0, resulting the following:
typeerror                                 traceback (most recent call last)
<ipython-input-1-d83f0d613f4b> in <module>
     19 
     20 
---> 21 inputs = tokenizer(""hello, my dog is cute"", return_tensors=""pt"")
     22 
     23 with torch.no_grad():

typeerror: 'berttokenizer' object is not callable

however, attempting to replace tokenizer() with tokenizer.tokenize() results in the following:
typeerror                                 traceback (most recent call last)
<ipython-input-2-1d431131eb87> in <module>
     21 
     22 with torch.no_grad():
---> 23     logits = model(**inputs).logits
     24 
     25 output_value = np.array(logits)[0][0]

typeerror: bertforsequenceclassification object argument after ** must be a mapping, not list

any help would be greatly appreciated.

solution
using tokenizer.encode_plus() as suggested by @cronoik:
tokenized = tokenizer.encode_plus(""hello, my dog is cute"", return_tensors=""pt"")

with torch.no_grad():
    logits = model(**tokenized)

output_value = np.array(logits)[0]
print(output_value)","['pytorch', 'tokenize', 'huggingface-transformers', 'bert-language-model', 'huggingface-tokenizers']",73127802,"sadly their documentation for the old versions is broken, but you can use encode_plus as shown in the following (he oldest available documentation of encode_plus is from 2.10.0):
import torch
from transformers import berttokenizer


t = berttokenizer.from_pretrained(""textattack/bert-base-uncased-yelp-polarity"")
tokenized = t.encode_plus(""hello, my dog is cute"", return_tensors='pt')
print(tokenized)

output:
{'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]]), 
'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 
'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}",https://stackoverflow.com/questions/73127139,pytorch,26-07-2022 16:55,395.0,1.0,1.0,True,26-07-2022 20:00,26-07-2022 19:40,Implementation Issues
77820916,openai api error: &quot;modulenotfounderror: no module named &#39;openai.error&#39;&quot;,"i am having an issue installing the openai.error package in python. this is what i am trying to install:
from flask import flask, render_template, request, jsonify
from flask_cors import cors
import openai
from openai.error import openaierror
import re
import os

after installing the openai package, i thought the openai.error package would install automatically. however, it gives me this error:
traceback (most recent call last):
  file ""c:\users\user\downloads\examiner.production\ielts_examiner_private\app.py"", line 4, in <module>
    from openai.error import openaierror
modulenotfounderror: no module named 'openai.error'","['python', 'flask', 'openai-api']",77820930,"you imported the openaierror class from the openai incorrectly.
change this...
from openai.error import openaierror

...to this.
from openai import openaierror",https://stackoverflow.com/questions/77820916,python,15-01-2024 15:54,6531.0,2.0,1.0,True,16-01-2024 14:58,15-01-2024 18:59,Tool Setup/Errors
3158132,verbally format a number in python,"how do pythonistas print a number as words, like the equivalent of the common lisp code:
[3]> (format t ""~r"" 1e25)
nine septillion, nine hundred and ninety-nine sextillion, nine hundred and ninety-nine quintillion, seven hundred and seventy-eight quadrillion, one hundred and ninety-six trillion, three hundred and eight billion, three hundred and sixty-one million, two hundred and sixteen thousand","python, formatting, nlp, language-comparisons",3158344,"no in python core, but there is 3rd party library num2words
>>> from num2words import num2words
>>> num2words(1e25)
'ten septillion, one billion, seventy-three million, seven hundred and forty-one thousand, eight hundred and twenty-four'

>>> num2words(10000000000000000000000000)
'ten septillion'

(note that 1e25 is not converted to integer precisely, neither in your example)",https://stackoverflow.com/q/3158132,"python, formatting, nlp, language-comparisons",01-07-2010 13:17,3913.0,9.0,3.0,True,05-11-2023 04:21,02-07-2010 06:52,Data Wrangling
62360325,determining best fit distributions by sse - python 3.8,"i am trying to come up with a way to determine the ""best fit"" between the following distributions:
gaussian, multinomial, bernoulli. 
i have a large pandas df, where each column can be thought of as a distribution of numbers. what i am trying to do, is for each column, determine the distribution of the above list as the best fit.
i noticed this question which asks something familiar, but these all look like discrete distribution tests, not continuous. i know scipy has metrics for a lot of these, but i can't determine how to to properly place the inputs. my thought would be:

for each column, save the data in a temporary np array
generate gaussian, multinomial, bernoulli distributions, perform a sse test to determine the distribution that gives the ""best fit"", and move on to the next column.

an example dataset (arbitrary, my dataset is 29888 x 73231) could be:
| could | couldnt | coupl | cours | death | develop | dialogu | differ | direct | director | done |
|:-----:|:-------:|:-----:|:-----:|:-----:|:-------:|:-------:|:------:|:------:|:--------:|:----:|
|   0   |    0    |   0   |   1   |   0   |    1    |    1    |    0   |    0   |     0    |   0  |
|   0   |    2    |   1   |   0   |   0   |    1    |    0    |    2   |    0   |     0    |   1  |
|   0   |    0    |   0   |   0   |   0   |    0    |    0    |    0   |    1   |     1    |   2  |
|   1   |    0    |   0   |   0   |   0   |    1    |    0    |    1   |    0   |     0    |   0  |
|   0   |    0    |   0   |   0   |   0   |    1    |    1    |    1   |    1   |     0    |   0  |
|   0   |    0    |   0   |   1   |   0   |    0    |    0    |    0   |    0   |     0    |   1  |
|   0   |    0    |   0   |   0   |   2   |    1    |    0    |    1   |    0   |     0    |   2  |
|   0   |    0    |   0   |   0   |   0   |    1    |    0    |    0   |    2   |     0    |   1  |
|   0   |    0    |   0   |   0   |   0   |    2    |    0    |    0   |    0   |     0    |   0  |
|   0   |    0    |   0   |   1   |   0   |    0    |    5    |    0   |    0   |     0    |   3  |
|   1   |    1    |   0   |   0   |   1   |    2    |    0    |    0   |    1   |     0    |   0  |
|   1   |    1    |   0   |   0   |   0   |    4    |    0    |    0   |    1   |     0    |   1  |
|   0   |    0    |   0   |   0   |   1   |    0    |    0    |    0   |    0   |     0    |   0  |
|   0   |    0    |   0   |   0   |   0   |    0    |    1    |    0   |    0   |     0    |   0  |
|   0   |    0    |   0   |   0   |   0   |    1    |    0    |    3   |    0   |     0    |   1  |
|   2   |    0    |   0   |   0   |   0   |    0    |    0    |    0   |    1   |     0    |   2  |
|   0   |    0    |   1   |   0   |   0   |    0    |    0    |    0   |    0   |     0    |   2  |
|   1   |    1    |   0   |   0   |   1   |    0    |    0    |    1   |    1   |     0    |   2  |
|   0   |    0    |   0   |   0   |   0   |    1    |    0    |    0   |    0   |     0    |   1  |
|   0   |    1    |   0   |   3   |   0   |    0    |    0    |    1   |    1   |     0    |   0  |

i have some basic code now, which was edited from this question, which attempts this:
import warnings
import numpy as np
import pandas as pd
import scipy.stats as st
import statsmodels as sm
import matplotlib
import matplotlib.pyplot as plt

matplotlib.rcparams['figure.figsize'] = (16.0, 12.0)
matplotlib.style.use('ggplot')

# create models from data
def best_fit_distribution(data, bins=200, ax=none):
    """"""model data by finding best fit distribution to data""""""
    # get histogram of original data
    y, x = np.histogram(data, bins=bins, density=true)
    x = (x + np.roll(x, -1))[:-1] / 2.0

    # distributions to check
    distributions = [        
        st.norm, st.multinomial, st.bernoulli
    ]

    # best holders
    best_distribution = st.norm
    best_params = (0.0, 1.0)
    best_sse = np.inf

    # estimate distribution parameters from data
    for distribution in distributions:

        # try to fit the distribution
        try:
            # ignore warnings from data that can't be fit
            with warnings.catch_warnings():
                warnings.filterwarnings('ignore')

                # fit dist to data
                params = distribution.fit(data)

                # separate parts of parameters
                arg = params[:-2]
                loc = params[-2]
                scale = params[-1]

                # calculate fitted pdf and error with fit in distribution
                pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)
                sse = np.sum(np.power(y - pdf, 2.0))

                # if axis pass in add to plot
                try:
                    if ax:
                        pd.series(pdf, x).plot(ax=ax)
                    end
                except exception:
                    pass

                # identify if this distribution is better
                if best_sse > sse > 0:
                    best_distribution = distribution
                    best_params = params
                    best_sse = sse

        except exception:
            print(""error on: {}"".format(distribution))
            pass

        #print(""distribution: {} | sse: {}"".format(distribution, sse))

    return best_distribution.name, best_sse

for col in df.columns:
    nm, pm = best_fit_distribution(df[col])
    print(nm)
    print(pm)

however, i get:
error on: <scipy.stats._multivariate.multinomial_gen object at 0x000002e3ccfa9f40>
error on: <scipy.stats._discrete_distns.bernoulli_gen object at 0x000002e3ccef4040>
norm
(4.4, 7.002856560004639)

my expected output would be something like, for each column:
gaussian sse: <val> | multinomial sse: <val> | bernoulli sse: <val>
update
catching the error yields:
error on: <scipy.stats._multivariate.multinomial_gen object at 0x000002e3ccfa9f40>
'multinomial_gen' object has no attribute 'fit'
error on: <scipy.stats._discrete_distns.bernoulli_gen object at 0x000002e3ccef4040>
'bernoulli_gen' object has no attribute 'fit'

why am i getting errors? i think it is because multinomial and bernoulli do not have fit methods. how can i make a fit method, and integrate that to get the sse?? the target output of this function or program would be, for agaussian, multinomial, bernoulli' distributions, what is the average sse, per column in the df, for each distribution type (to try and determine best-fit by column).
update 06/15:
i have added a bounty.
update 06/16:
the larger intention, as this is a piece of a larger application, is to discern, over the course of a very large dataframe, what the most common distribution of tfidf values is. then, based on that, apply a naive bayes classifier from sklearn that matches that most-common distribution. scikit-learn.org/stable/modules/naive_bayes.html contains details on the different classifiers. therefore, what i need to know, is which distribution is the best fit across my entire dataframe, which i assumed to mean, which was the most common amongst the distribution of tfidf values in my words. from there, i will know which type of classifier to apply to my dataframe. in the example above, there is a column not shown called class which is a positive or negative classification. i am not looking for input to this, i am simply following the instructions i have been given by my lead.","['python', 'numpy', 'scipy', 'nlp', 'statistics']",62365555,"i summarize the question as: given a list of nonnegative integers, can we fit a probability distribution, in particular a gaussian, multinomial, and  bernoulli, and compare the quality of the fit?
for discrete quantities, the correct term is probability mass function: p(k) is the probability that a number picked is exactly equal to the integer value k. a bernoulli distribution can be parametrized by a p parameter: be(k, p) where 0 <= p <= 1 and k can only take the values 0 or 1. it is a special case of the binomial distribution b(k, p, n) that has parameters 0 <= p <= 1 and integer n >= 1. (see the linked wikipedia article for an explanation of the meaning of p and n) it is related to the bernoulli distribution as be(k, p) = b(k, p, n=1). the trinomial distribution t(k1, k2, p1, p2, n) is parametrized by p1, p2, n and describes the probability of pairs (k1, k2). for example, the set {(0,0), (0,1), (1,0), (0,1), (0,0)} could be pulled from a trinomial distribution. binomial and trinomial distributions are special cases of multinomial distributions; if you have data occuring as quintuples such as (1, 5, 5, 2, 7), they could be pulled from a multinomial (hexanomial?) distribution m6(k1, ..., k5, p1, ..., p5, n). the question specifically asks for the probability distribution of the numbers of a single column, so the only multinomial distribution that fits here is the binomial one, unless you specify that the sequence [0, 1, 5, 2, 3, 1] should be interpreted as [(0, 1), (5, 2), (3, 1)] or as [(0, 1, 5), (2, 3, 1)]. but the question does not specify that numbers can be accumulated in pairs or triplets.
therefore, as far as discrete distributions go, the pmf for one list of integers is of the form p(k) and can only be fitted to the binomial distribution, with suitable n and p values. if the best fit is obtained for n=1, then it is a bernoulli distribution.
the gaussian distribution is a continuous distribution g(x, mu, sigma), where mu (mean) and sigma (standard deviation) are parameters. it tells you that the probability of finding x0-a/2 < x < x0+a/2 is equal to g(x0, mu, sigma)*a, for a << sigma. strictly speaking, the gaussian distribution does not apply to discrete variables, since the gaussian distribution has nonzero probabilities for non-integer x values, whereas the probability of pulling a non-integer out of a distribution of integers is zero. typically, you would use a gaussian distribution as an approximation for a binomial distribution, where you set a=1 and set p(k) = g(x=k, mu, sigma)*a.
for sufficiently large n, a binomial distribution and a gaussian will appear similar according to
b(k, p, n) =  g(x=k, mu=p*n, sigma=sqrt(p*(1-p)*n)).

if you wish to fit a gaussian distribution, you can use the standard scipy function scipy.stats.norm.fit. such fit functions are not offered for the discrete distributions such as the binomial. you can use the function scipy.optimize.curve_fit to fit non-integer parameters such as the p parameter of the binomial distribution. in order to find the optimal integer n value, you need to vary n, fit p for each n, and pick the n, p combination with the best fit.
in the implementation below, i estimate n and p from the relation with the mean and sigma value above and search around that value. the search could be made smarter, but for the small test datasets that i used, it's fast enough. moreover, it helps illustrate a point; more on that later. i have provided a function fit_binom, which takes a histogram with actual counts, and a function fit_samples, which can take a column of numbers from your dataframe.
""""""binomial fit routines.

author: han-kwang nienhuys (2020)
copying: cc-by-sa, cc-by, bsd, gpl, lgpl.
 
""""""

import numpy as np
from scipy.stats import binom, poisson
from scipy.optimize import curve_fit
import matplotlib.pyplot as plt

class binompmf:
    """"""wrapper so that integer parameters don't occur as function arguments.""""""
    def __init__(self, n):
        self.n = n
    def __call__(self, ks, p):
        return binom(self.n, p).pmf(ks)

def fit_binom(hist, plot=true, weighted=true, f=1.5, verbose=false):
    """"""fit histogram to binomial distribution.
    
    parameters:

    - hist: histogram as int array with counts, array index as bin.
    - plot: whether to plot
    - weighted: whether to fit assuming poisson statistics in each bin.
      (recommended: true).
    - f: try to fit n in range n0/f to n0*f where n0 is the initial estimate.
      must be >= 1.
    - verbose: whether to print messages.
    
    return: 
        
    - histf: fitted histogram as int array, same length as hist.
    - n: binomial n value (int)
    - p: binomial p value (float)
    - rchi2: reduced chi-squared. this number should be around 1.
      large values indicate a bad fit; small values indicate
      ""too good to be true"" data.
    """""" 
   
    hist = np.array(hist, dtype=int).ravel() # force 1d int array
    pmf = hist/hist.sum() # probability mass function
    nk = len(hist)
    if weighted:
        sigmas = np.sqrt(hist+0.25)/hist.sum()
    else:
        sigmas = np.full(nk, 1/np.sqrt(nk*hist.sum()))
    ks = np.arange(nk)
    mean = (pmf*ks).sum()
    variance = ((ks-mean)**2 * pmf).sum()
    
    # initial estimate for p and search range for n
    nest = max(1, int(mean**2 /(mean-variance) + 0.5))
    nmin = max(1, int(np.floor(nest/f)))
    nmax = max(nmin, int(np.ceil(nest*f)))
    nvals = np.arange(nmin, nmax+1)
    num_n = nmax-nmin+1
    verbose and print(f'initial estimate: n={nest}, p={mean/nest:.3g}')

    # store fit results for each n
    pvals, sses = np.zeros(num_n), np.zeros(num_n)
    for n in nvals:
        # fit and plot
        p_guess = max(0, min(1, mean/n))
        fitparams, _ = curve_fit(
            binompmf(n), ks, pmf, p0=p_guess, bounds=[0., 1.],
            sigma=sigmas, absolute_sigma=true)
        p = fitparams[0]
        sse = (((pmf - binompmf(n)(ks, p))/sigmas)**2).sum()
        verbose and print(f'  trying n={n} -> p={p:.3g} (initial: {p_guess:.3g}),'
                          f' sse={sse:.3g}')
        pvals[n-nmin] = p
        sses[n-nmin] = sse
    n_fit = np.argmin(sses) + nmin
    p_fit = pvals[n_fit-nmin]
    sse = sses[n_fit-nmin]    
    chi2r = sse/(nk-2) if nk > 2 else np.nan
    if verbose:
        print(f'  found n={n_fit}, p={p_fit:.6g} sse={sse:.3g},'
              f' reduced chi^2={chi2r:.3g}')
    histf = binompmf(n_fit)(ks, p_fit) * hist.sum()

    if plot:    
        fig, ax = plt.subplots(2, 1, figsize=(4,4))
        ax[0].plot(ks, hist, 'ro', label='input data')
        ax[0].step(ks, histf, 'b', where='mid', label=f'fit: n={n_fit}, p={p_fit:.3f}')
        ax[0].set_xlabel('k')
        ax[0].axhline(0, color='k')
        ax[0].set_ylabel('counts')
        ax[0].legend()
        
        ax[1].set_xlabel('n')
        ax[1].set_ylabel('sse')
        plotfunc = ax[1].semilogy if sses.max()>20*sses.min()>0 else ax[1].plot
        plotfunc(nvals, sses, 'k-', label='sse over n scan')
        ax[1].legend()
        fig.show()
        
    return histf, n_fit, p_fit, chi2r

def fit_binom_samples(samples, f=1.5, weighted=true, verbose=false):
    """"""convert array of samples (nonnegative ints) to histogram and fit.
    
    see fit_binom() for more explanation.
    """"""
    
    samples = np.array(samples, dtype=int)
    kmax = samples.max()
    hist, _ = np.histogram(samples, np.arange(kmax+2)-0.5)
    return fit_binom(hist, f=f, weighted=weighted, verbose=verbose) 

def test_case(n, p, nsamp, weighted=true, f=1.5):
    """"""run test with n, p values; nsamp=number of samples.""""""
    
    print(f'test case: n={n}, p={p}, nsamp={nsamp}')
    ks = np.arange(n+1) # bins
    pmf = binompmf(n)(ks, p)
    hist = poisson.rvs(pmf*nsamp)
    fit_binom(hist, weighted=weighted, f=f, verbose=true)

if __name__ == '__main__':
    plt.close('all')
    np.random.seed(1)
    weighted = true
    test_case(10, 0.2, 500, f=2.5, weighted=weighted)
    test_case(10, 0.3, 500, weighted=weighted)
    test_case(10, 0.8, 10000, weighted)
    test_case(1, 0.3, 100, weighted) # equivalent to bernoulli distribution
    fit_binom_samples(binom(15, 0.5).rvs(100), weighted=weighted)

in principle, the most best fit will be obtained if you set weighted=true. however, the question asks for the minimum sum of squared errors (sse) as a metric; then, you can set weighted=false.
it turns out that it is difficult to fit a binomial distribution unless you have a lot of data. here are tests with realistic (random-generated) data for n, p combinations (10, 0.2), (10, 0.3), (10, 0.8), and (1, 0.3), for various numbers of samples. the plots also show how the weighted sse changes with n.

typically, with 500 samples, you get a fit that looks ok by eye, but which does not recover the actual n and p values correctly, although the product n*p is quite accurate. in those cases, the sse curve has a broad minimum, which is a giveaway that there are several reasonable fits.
the code above can be adapted for different discrete distributions. in that case, you need to figure out reasonable initial estimates for the fit parameters. for example: poisson: the mean is the only parameter (use the reduced chi2 or sse to judge whether it's a good fit).
if you want to fit a combination of m input columns to a (m+1)-dimensional multinomial , you can do a binomial fit on each input column and store the fit results in arrays nn and pp (each an array with shape (m,)). transform these into an initial estimate for a multinomial:
n_est = int(nn.mean()+0.5)
pp_est = pp*nn/n_est
pp_est = np.append(pp_est, 1-pp_est.sum())

if the individual values in the nn array vary a lot, or if the last element of pp_est is negative, then it's probably not a multinomial.
you want to compare the residuals of multiple models; be aware that a model that has more fit parameters will tend to produce lower residuals, but this does not necessarily mean that the model is better.
note: this answer underwent a large revision.",https://stackoverflow.com/questions/62360325,python,13-06-2020 13:21,2296.0,4.0,2.0,True,22-01-2023 08:41,19-06-2020 03:12,Implementation Issues
66775231,can anybody tell me the regex that matches twenty (20) day and 28 days but not in 27 days,"i am writing a python script to match the duration of an activity. there are 2 choices- x days/months/years or  (x) days/months/years.
i wrote a regex \w*\s*['(']*\d{1,4}[')']*\s*\w{3,6} and the sentence is
ujjwal in 28 days and 40 months and 2 years or twenty (20) day

i want to match only 28 days, 40 months, 2 years and twenty (20) day.
but my regex is matching in 28 days, and 40 months, and 2 years.
please help me.","['python-3.x', 'regex', 'nlp']",66775327,"it is probably easier to be more specific with your regex, trying to match either a word before digits in parentheses or just digits:
(?:\w+\s+\(\d+\)|\b\d+)

followed by a space and one of the date type words:
\s+(?:year|month|day)s?

in python:
import re

text = 'ujjwal in 28 days and 40 months and 2 years or twenty (20) day'
print(re.findall(r'(?:\w+\s+\(\d+\)|\b\d+)\s+(?:year|month|day)s?', text))

output:
['28 days', '40 months', '2 years', 'twenty (20) day']",https://stackoverflow.com/questions/66775231,python-3.x,24-03-2021 05:27,47.0,1.0,1.0,True,24-03-2021 07:27,24-03-2021 07:27,Uncategorized
76293427,langchain pipeline vram usage when loading model,"i'm trying to load 6b 128b 8bit llama based model from file (note the model itself is an example, i tested others and got similar problems), the pipeline is completely eating up my 8gb of vram:


my code:
from langchain.llms import huggingfacepipeline
from langchain import prompttemplate, llmchain

import torch
from transformers import llamatokenizer, llamaforcausallm, llamaconfig, pipeline

torch.cuda.set_device(torch.device(""cuda:0""))

path = './models/wizardlm-7b-gptq-4bit-128g'
config = llamaconfig.from_json_file(f'{path}/config.json')
base_model = llamaforcausallm(config=config).half()

torch.cuda.empty_cache()
tokenizer = llamatokenizer.from_pretrained(
    pretrained_model_name_or_path=path,
    low_cpu_mem_usage=true,
    local_files_only=true
)
torch.cuda.empty_cache()

pipe = pipeline(
    ""text-generation"",
    model=base_model,
    tokenizer=tokenizer,
    batch_size=1,
    device=0,
    max_length=100,
    temperature=0.6,
    top_p=0.95,
    repetition_penalty=1.2
)

how can i make the pipeline initiation consume less vram?
gpu: amdï¿½ï¿½ radeon rx 6600 (8gb vram, rocm 5.4.2 & torch)
i want to mention that i ged to load the same model on other frameworks like ""koboldai"" or ""text-generation-webui"" so i know it should be possible.
to load the model ""wizardlm-7b-gptq-4bit-128g"" downloaded from huggingface and run it using with langchain on python.
pip list output:
    package                  version
------------------------ ----------------
accelerate               0.19.0
aiofiles                 23.1.0
aiohttp                  3.8.4
aiosignal                1.3.1
altair                   5.0.0
anyio                    3.6.2
argilla                  1.7.0
async-timeout            4.0.2
attrs                    23.1.0
backoff                  2.2.1
beautifulsoup4           4.12.2
bitsandbytes             0.39.0
certifi                  2022.12.7
cffi                     1.15.1
chardet                  5.1.0
charset-normalizer       2.1.1
chromadb                 0.3.23
click                    8.1.3
clickhouse-connect       0.5.24
cmake                    3.25.0
colorclass               2.2.2
commonmark               0.9.1
compressed-rtf           1.0.6
contourpy                1.0.7
cryptography             40.0.2
cycler                   0.11.0
dataclasses-json         0.5.7
datasets                 2.12.0
deprecated               1.2.13
dill                     0.3.6
duckdb                   0.8.0
easygui                  0.98.3
ebcdic                   1.1.1
et-xmlfile               1.1.0
extract-msg              0.41.1
fastapi                  0.95.2
ffmpy                    0.3.0
filelock                 3.9.0
fonttools                4.39.4
frozenlist               1.3.3
fsspec                   2023.5.0
gradio                   3.28.3
gradio_client            0.2.5
greenlet                 2.0.2
h11                      0.14.0
hnswlib                  0.7.0
                 0.16.3
                0.5.0
                    0.23.3
huggingface-hub          0.14.1
idna                     3.4
imapclient               2.3.1
jinja2                   3.1.2
joblib                   1.2.0
jsonschema               4.17.3
kiwisolver               1.4.4
langchain                0.0.171
lark-parser              0.12.0
linkify-it-py            2.0.2
lit                      15.0.7
llama-cpp-python         0.1.50
loralib                  0.1.1
lxml                     4.9.2
lz4                      4.3.2
markdown                 3.4.3
markdown-it-py           2.2.0
markupsafe               2.1.2
marshmallow              3.19.0
marshmallow-enum         1.5.1
matplotlib               3.7.1
mdit-py-plugins          0.3.3
mdurl                    0.1.2
monotonic                1.6
mpmath                   1.2.1
msg-parser               1.2.0
msoffcrypto-tool         5.0.1
multidict                6.0.4
multiprocess             0.70.14
mypy-extensions          1.0.0
networkx                 3.0
nltk                     3.8.1
numexpr                  2.8.4
numpy                    1.24.1
nvidia-cublas-cu11       11.10.3.66
nvidia-cuda-cupti-cu11   11.7.101
nvidia-cuda-nvrtc-cu11   11.7.99
nvidia-cuda-runtime-cu11 11.7.99
nvidia-cudnn-cu11        8.5.0.96
nvidia-cufft-cu11        10.9.0.58
nvidia-curand-cu11       10.2.10.91
nvidia-cusolver-cu11     11.4.0.1
nvidia-cusparse-cu11     11.7.4.91
nvidia-nccl-cu11         2.14.3
nvidia-nvtx-cu11         11.7.91
olefile                  0.46
oletools                 0.60.1
openai                   0.27.7
openapi-schema-pydantic  1.2.4
openpyxl                 3.1.2
orjson                   3.8.12
packaging                23.1
pandas                   1.5.3
pandoc                   2.3
pcodedmp                 1.2.6
pdfminer.six             20221105
pillow                   9.3.0
pip                      23.0.1
plumbum                  1.8.1
ply                      3.11
posthog                  3.0.1
psutil                   5.9.5
pyarrow                  12.0.0
pycparser                2.21
pydantic                 1.10.7
pydub                    0.25.1
pygments                 2.15.1
pygpt4all                1.1.0
pygptj                   2.0.3
pyllamacpp               2.3.0
pypandoc                 1.11
pyparsing                2.4.7
pyrsistent               0.19.3
python-dateutil          2.8.2
python-docx              0.8.11
python-dotenv            1.0.0
python-magic             0.4.27
python-multipart         0.0.6
python-pptx              0.6.21
pytorch-triton-rocm      2.0.1
pytz                     2023.3
pytz-deprecation-shim    0.1.0.post0
pyyaml                   6.0
red-black-tree-mod       1.20
regex                    2023.5.5
requests                 2.28.1
responses                0.18.0
rfc3986                  1.5.0
rich                     13.0.1
rtfde                    0.0.2
scikit-learn             1.2.2
scipy                    1.10.1
semantic-version         2.10.0
sentence-transformers    2.2.2
sentencepiece            0.1.99
setuptools               66.0.0
six                      1.16.0
sniffio                  1.3.0
soupsieve                2.4.1
sqlalchemy               2.0.15
starlette                0.27.0
sympy                    1.11.1
tabulate                 0.9.0
tenacity                 8.2.2
threadpoolctl            3.1.0
tokenizers               0.13.3
toolz                    0.12.0
torch                    2.0.1+rocm5.4.2
torchaudio               2.0.2+rocm5.4.2
torchvision              0.15.2+rocm5.4.2
tqdm                     4.65.0
transformers             4.30.0.dev0
triton                   2.0.0
typer                    0.9.0
typing_extensions        4.4.0
typing-inspect           0.8.0
tzdata                   2023.3
tzlocal                  4.2
uc-micro-py              1.0.2
unstructured             0.6.6
urllib3                  1.26.13
uvicorn                  0.22.0
uvloop                   0.17.0
watchfiles               0.19.0
websockets               11.0.3
wheel                    0.38.4
wikipedia                1.4.0
wrapt                    1.14.1
xlsxwriter               3.1.0
xxhash                   3.2.0
yarl                     1.9.2
zstandard                0.21.0","['python', 'pytorch', 'huggingface-transformers', 'langchain']",76336277,"i assume you are trying to load this model: thebloke/wizardlm-7b-gptq. this model can not be loaded directly with the transformers library as it was 4bit quantized, but you can load it with autogptq:
pip install auto-gptq

import torch
from transformers import llamatokenizer, pipeline
from auto_gptq import autogptqforcausallm, basequantizeconfig


quantize_config = basequantizeconfig(**{""bits"": 4, ""damp_percent"": 0.01, ""desc_act"": true, ""group_size"": 128})

model_id = 'thebloke/wizardlm-7b-gptq'

# i downloaded the model from the hub due to name conflicts
m = autogptqforcausallm.from_quantized(""/tmp/blabla/"",  device=""cuda:0"", quantize_config=quantize_config, use_safetensors=true)


t = llamatokenizer.from_pretrained(
    pretrained_model_name_or_path=model_id,
)

pipe = pipeline(
    ""text-generation"",
    model=m,
    tokenizer=t,
    batch_size=1,
    device=0,
    max_length=100,
    temperature=0.6,
    top_p=0.95,
    repetition_penalty=1.2
)

pipe(""please give me an life changing advise."")

output:
[{'generated_text': 'please give me an life changing advise.\ni am a 28 year old woman and i have been struggling with anxiety for the past few years now. it has affected my personal and professional life greatly. i have tried various therapies, medications etc but nothing seems to work long term. recently, i started practicing meditation regularly and it has helped me immensely in reducing my anxiety levels. however, i still struggle with social situations and public speaking. can'}]",https://stackoverflow.com/questions/76293427,python,20-05-2023 03:42,1844.0,1.0,1.0,True,25-05-2023 21:11,23-05-2023 21:17,Tool Setup/Errors
53718660,memory does not refresh automatically in recast.ai,"i have created a entity called #user-name and have set that as a requirement.
now, for the first time when the entity is detected in the conversation - say, ""i am john"" , then the memory is set to john. on subsequent encounter of the same entity with different value - ""i am dave"", the memory remains unchanged.
i have seen the edit memory option, which provides 1. reset memory 2. set to a value . for the option 2, it does not provide a way to set to the value of #user-name, instead only provides option to enter static values.
how can i update the memory every time the value of the entity changes ??
edit
hi, i am attaching some screenshots to show what's exactly going wrong.

i have a entity named '#user_name' that saves the user name in a  memory variable .

i make the following conversation -


the json payload after the conversation is as follows. this works perfectly-


i update the conversation again by providing a new user name.


this triggers the entity just fine. you can see the entity being detected properly.


however, the memory value remains the same.



what i wanted was the memory variable to replace 'dev' with 'john'.","['memory', 'nlp', 'chatbot', 'sap-conversational-ai']",54336851,"remember that: 
memory <> intent
you can set memory in the message section or update automatically using for example a requirement in this case every time the skill is trigged it will replace the value in the memory id 
edit: because the set memory field expect a json you can't use memory as you want, but if you reset that memory id shomewhere relevant in the chat (in my sample i delete it right after saying hi xxx) so when the skill is trigged again it will ""replace"" it with the new value
in the requirement i set the golden entity #person to variable ""name"" and if is missing i ask her name.
sample image",https://stackoverflow.com/questions/53718660,memory,11-12-2018 06:36,557.0,2.0,2.0,True,02-07-2023 12:57,02-07-2023 12:57,Implementation Issues
40207422,binary numbers instead of one hot vectors,"while doing logistic regression, it is common practice to use one hot vectors as desired result. so, no of classes = no of nodes in output layer. we don't use index of word in vocabulary(or a class number in general) because that may falsely indicate closeness of two classes. but why can't we use binary numbers instead of one-hot vectors?
i.e if there are 4 classes, we can represent each class as 00,01,10,11 resulting in log(no of classes) nodes in output layer.","['machine-learning', 'nlp', 'computer-vision', 'neural-network']",40208528,"it is fine if you encode with binary. but you probably need to add another layer (or a filter) depending on your task and model. because your encoding now implicates invalid shared features due to the binary representation.
for example, a binary encoding for input (x = [x1, x2]):
'apple' = [0, 0]
'orange' = [0, 1]
'table' = [1, 0]
'chair' = [1, 1]

it means that orange and chair share same feature x2. now with predictions for two classes y:
'fruit' = 0
'furniture' = 1

and linear optimization model (w = [w1, w2] and bias b) for labeled data sample:
(argmin w) loss = y - (w1 * x1 + w2 * x2 + b)

whenever you update w2 weights for chair as furniture you get an undesirable update as if predicting orange as furniture as well.
in this particular case, if you add another layer u = [u1, u2], you can probably solve this issue:
(argmin u,w) loss = y - (u1 * (w1 * x1 + w2 * x2 + b) +
                         u2 * (w1 * x1 + w2 * x2 + b) +
                         b2)

ok, why not avoid this miss representation, by using one-hot encoding. :)",https://stackoverflow.com/questions/40207422,machine-learning,23-10-2016 20:19,4560.0,7.0,2.0,True,04-02-2021 13:41,23-10-2016 20:28,Implementation Issues
2005084,how to specify two fields in lucene queryparser?,"i read how to incorporate multiple fields in queryparser? but i didn't get it.
at the moment i have a very strange construction like:
parser = new queryparser(""bodytext"", analyzer)
parser2 = new queryparser(""title"", analyzer)
query = parser.parse(strsuchbegriff)
query2 = parser.parse(strsuchbegriff)

what can i do for something like:
parser = new querparser (""bodytext"" , ""title"",analyzer)
query =parser.parse(strsuchbegriff) 

so the parser looks for the searching word in the field ""bodytext"" an in the field ""title"".","['java', 'parsing', 'lucene', 'lucene.net', 'information-retrieval']",2036886,"there are 3 ways to do this.
the first way is to construct a query manually, this is what queryparser is doing internally. this is the most powerful way to do it, and means that you don't have to parse the user input if you want to prevent access to some of the more exotic features of queryparser:
indexreader reader = indexreader.open(""<lucene dir>"");
searcher searcher = new indexsearcher(reader);

booleanquery booleanquery = new booleanquery();
query query1 = new termquery(new term(""bodytext"", ""<text>""));
query query2 = new termquery(new term(""title"", ""<text>""));
booleanquery.add(query1, booleanclause.occur.should);
booleanquery.add(query2, booleanclause.occur.should);
// use booleanclause.occur.must instead of booleanclause.occur.should
// for and queries
hits hits = searcher.search(booleanquery);

the second way is to use multifieldqueryparser, this behaves like queryparser, allowing access to all the power that it has, except that it will search over multiple fields.
indexreader reader = indexreader.open(""<lucene dir>"");
searcher searcher = new indexsearcher(reader);

analyzer analyzer = new standardanalyzer();
multifieldqueryparser queryparser = new multifieldqueryparser(
                                        new string[] {""bodytext"", ""title""},
                                        analyzer);

hits hits = searcher.search(queryparser.parse(""<text>""));

the final way is to use the special syntax of queryparser see here.
indexreader reader = indexreader.open(""<lucene dir>"");
searcher searcher = new indexsearcher(reader);    

analyzer analyzer = new standardanalyzer();
queryparser queryparser = new queryparser(""<default field>"", analyzer);
// <default field> is the field that queryparser will search if you don't 
// prefix it with a field.
string special = ""bodytext:"" + text + "" or title:"" + text;

hits hits = searcher.search(queryparser.parse(special));

your other option is to create new field when you index your content called bodytextandtitle, into which you can place the contents of both bodytext and title, then you only have to search one field.",https://stackoverflow.com/questions/2005084,java,05-01-2010 09:30,49529.0,73.0,2.0,True,07-05-2022 06:26,22-05-2020 17:09,Implementation Issues
72775559,resize_token_embeddings on the a pertrained model with different embedding size,"i would like to ask about the way to change the embedding size of the trained model.
i have a trained model models/bert-pretrain-1-step-5000.pkl.
now i am adding a new token [tra]to the tokeniser and try to use the resize_token_embeddings to the pertained one.
from pytorch_pretrained_bert_inset import bertmodel #berttokenizer 
from transformers import autotokenizer
from torch.nn.utils.rnn import pad_sequence
import tqdm

tokenizer = autotokenizer.from_pretrained(""bert-base-uncased"")
model_bert = bertmodel.from_pretrained('bert-base-uncased', state_dict=torch.load('models/bert-pretrain-1-step-5000.pkl', map_location=torch.device('cpu')))

#print(tokenizer.all_special_tokens) #--> ['[unk]', '[sep]', '[pad]', '[cls]', '[mask]']
#print(tokenizer.all_special_ids)    #--> [100, 102, 0, 101, 103]

num_added_toks = tokenizer.add_tokens(['[tra]'], special_tokens=true)
model_bert.resize_token_embeddings(len(tokenizer))  # --> embedding(30523, 768)
print('[tra] token id: ', tokenizer.convert_tokens_to_ids('[tra]'))  # --> 30522

but i encountered the error:
attributeerror: 'bertmodel' object has no attribute 'resize_token_embeddings'

i assume that it is because the model_bert(bert-pretrain-1-step-5000.pkl) i had has the different embedding size.
i would like to know if there is any way to fit the embedding size of my modified tokeniser and the model i would like to use as the initial weights.
thanks a lot!!","['pytorch', 'huggingface-transformers', 'bert-language-model', 'word-embedding', 'huggingface-tokenizers']",72847700,"resize_token_embeddings is a huggingface transformer method. you are using the bertmodel class from pytorch_pretrained_bert_inset which does not provide such a method. looking at the code, it seems like they have copied the bert code from huggingface some time ago.
you can either wait for an update from inset (maybe create a github issue) or write your own code to extend the word_embedding layer:
from torch import nn 

embedding_layer = model.embeddings.word_embeddings

old_num_tokens, old_embedding_dim = embedding_layer.weight.shape

num_new_tokens = 1

# creating new embedding layer with more entries
new_embeddings = nn.embedding(
        old_num_tokens + num_new_tokens, old_embedding_dim
)

# setting device and type accordingly
new_embeddings.to(
    embedding_layer.weight.device,
    dtype=embedding_layer.weight.dtype,
)

# copying the old entries
new_embeddings.weight.data[:old_num_tokens, :] = embedding_layer.weight.data[
    :old_num_tokens, :
]

model.embeddings.word_embeddings = new_embeddings",https://stackoverflow.com/questions/72775559,pytorch,27-06-2022 16:38,14393.0,3.0,1.0,True,03-07-2022 15:22,03-07-2022 15:22,Implementation Issues
79395156,why langchain-weaviate is not installing?,"i was trying to create rag with weaviate local server by langchain documentation but bumped into next error while downloading library
    pip install langchain-weaviate
collecting langchain-weaviate
  using cached langchain_weaviate-0.0.3-py3-none-any.whl.metadata (2.7 kb)
requirement already satisfied: langchain-core<0.4,>=0.1.33 in d:\ds\venv\lib\site-packages (from langchain-weaviate) (0.3.31)
collecting numpy<2.0.0,>=1.26.2 (from langchain-weaviate)
  using cached numpy-1.26.4.tar.gz (15.8 mb)
  installing build dependencies ... done
  getting requirements to build wheel ... done
  installing backend dependencies ... done
  preparing metadata (pyproject.toml) ... done
collecting simsimd<5.0.0,>=3.6.1 (from langchain-weaviate)
  using cached simsimd-4.4.0.tar.gz (33 kb)
  installing build dependencies ... done
  getting requirements to build wheel ... error
  error: subprocess-exited-with-error

  ï¿½ï¿½ getting requirements to build wheel did not run successfully.
  ï¿½t code: 1
  ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½> [20 lines of output]
      traceback (most recent call last):
        file ""d:\ds\venv\lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 389, in <module>
          main()
          ~~~~^^
        file ""d:\ds\venv\lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 373, in main
          json_out[""return_val""] = hook(**hook_input[""kwargs""])
                                   ~~~~^^^^^^^^^^^^^^^^^^^^^^^^
        file ""d:\ds\venv\lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 143, in get_requires_for_build_wheel
          return hook(config_settings)
        file ""c:\users\user\appdata\local\temp\pip-build-env-z_l8tv83\overlay\lib\site-packages\setuptools\build_meta.py"", line 334, in get_requires_for_build_wheel       
          return self._get_build_requires(config_settings, requirements=[])
                 ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        file ""c:\users\user\appdata\local\temp\pip-build-env-z_l8tv83\overlay\lib\site-packages\setuptools\build_meta.py"", line 304, in _get_build_requires
          self.run_setup()
          ~~~~~~~~~~~~~~^^
        file ""c:\users\user\appdata\local\temp\pip-build-env-z_l8tv83\overlay\lib\site-packages\setuptools\build_meta.py"", line 320, in run_setup
          exec(code, locals())
          ~~~~^^^^^^^^^^^^^^^^
        file ""<string>"", line 6, in <module>
      filenotfounderror: [errno 2] no such file or directory: 'version'
      [end of output]

  note: this error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

ï¿½ï¿½ getting requirements to build wheel did not run successfully.
ï¿½ï¿½ï¿½ exit code: 1
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½> see above for output.

note: this error originates from a subprocess, and is likely not a problem with pip.
ps d:\ds\rag\backend>


 long to insert here.
tried to install simsimd and then install langchain-weaviate but it didn't help, actually made it even worse, before an error wasn't so long, haven't found anything about that in internet maybe somebody had the same problem?
would appreciate your help!","['python', 'langchain', 'vector-database', 'weaviate']",79404061,"duda from weaviate here.
edit: version 0.0.4 of langchain-weaviate was released and it fixes this issue.
there seems to be an update that was not yet released to pypi that is preventing on using simsimd on python3.13, as per this issue:

meanwhile, if for development, you can install it directly from source with
pip install -e ""git+

or you can run python3.12
hope we can have that released soon.
thanks!",https://stackoverflow.com/questions/79395156,python,28-01-2025 20:50,89.0,0.0,1.0,True,05-02-2025 12:17,04-02-2025 00:13,Tool Setup/Errors
67010607,val_accuracy does not increase,"currently i'm trying to train a keras sequential network with pooled output from bert. the fine tuned bertforsequence classification yields good results, but using the pooled_output in a neural network does not work as intented. as input data i got 10.000 values, each consisting of the 768 floats that my bert-model provides. i'm trying to do a simple binary classification, so i also got the labels with 1 and 0's.

as you can see my data has a good number of examples for both classes. after shuffling them, i do a normal train test split and create/fit my model with:
model = sequential()
model.add(dense(1536, input_shape=(768,), activation='relu'))
model.add(dense(1536, activation='relu'))
model.add(dense(1536, activation='relu'))
model.add(dense(1, activation='sigmoid'))

opt = adam(learning_rate=0.0001)
model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])

#normally with early stopping so quite a few epochs
history = model.fit(train_features, train_labels, epochs=800, batch_size=68, verbose=1, 
validation_split=0.2, callbacks=[])

during training the loss decreases and my accuracy increases as expected. but the val_loss increases and the val_accuracy stays the same! sure i'm overfitting, but i would expect that the val_accuracy increases, at least for a few epochs and then decreaes when i'm overfitting.


has anyone an idea what i'm doing wrong? perhaps 10.000 values aren't enough to generalize?","['python', 'tensorflow', 'keras', 'nlp', 'bert-language-model']",67032063,"it was not just a mislabeling in my validation set, but in my whole data.
i take a sample of 100000 entries
train_df = train_df.sample(frac=1).reset_index(drop=true)
train_df = train_df.iloc[0:100000]

and delete some values
train_df = train_df[train_df['label'] != '-']

after that i set a few values using train_df.at in a loop, but some indices don't exist because i deleted them. train_df.at only throws warnings so i did not see this. also i mixed .loc and .iloc so in my case i selected .iloc[2:3] but the index 2 does not exist, so it return index 3 wich is on position 2. after that i make my changes and train_df.at fails at inserting on position 2, but my loop goes on. the next iteration .iloc returns index 4 on position 3. my loop then puts the data on index 3 - from now on all my labels are one position off.",https://stackoverflow.com/questions/67010607,python,08-04-2021 19:24,3185.0,0.0,2.0,True,10-04-2021 07:59,08-04-2021 20:01,Implementation Issues
67997713,modulenotfounderror: no java install detected. please install java to use language-tool-python,"i would like to check the number if issues in a given sentence.
my code is
import language_tool_python
tl = language_tool_python.languagetool('en-us')

txt = ""good mooorning sirr and medam my namee anderen i am from amerecia !""
m = tl.check(txt)
len(m)

instead of returning the number i am getting error message as shown below.
modulenotfounderror                       traceback (most recent call last)
<ipython-input-1-1c4c9134d6f4> in <module>
      1 import language_tool_python
----> 2 tool = language_tool_python.languagetool('en-us')
      3 
      4 text = ""your the best but their are allso  good !""
      5 matches = tool.check(text)

e:\anaconda\lib\site-packages\language_tool_python\server.py in __init__(self, language, mothertongue, remote_server, newspellings, new_spellings_persist)
     43             self._update_remote_server_config(self._url)
     44         elif not self._server_is_alive():
---> 45             self._start_server_on_free_port()
     46         if language is none:
     47             try:

e:\anaconda\lib\site-packages\language_tool_python\server.py in _start_server_on_free_port(self)
    212             self._url = ' self._port)
    213             try:
--> 214                 self._start_local_server()
    215                 break
    216             except servererror:

e:\anaconda\lib\site-packages\language_tool_python\server.py in _start_local_server(self)
    222     def _start_local_server(self):
    223         # before starting local server, download language tool if needed.
--> 224         download_lt()
    225         err = none
    226         try:

e:\anaconda\lib\site-packages\language_tool_python\download_lt.py in download_lt(update)
    142     ]
    143 
--> 144     confirm_java_compatibility()
    145     version = latest_version
    146     filename = filename.format(version=version)

e:\anaconda\lib\site-packages\language_tool_python\download_lt.py in confirm_java_compatibility()
     73         # found because of a pathext-related issue
     74         # (
---> 75         raise modulenotfounderror('no java install detected. please install java to use language-tool-python.')
     76 
     77     output = subprocess.check_output([java_path, '-version'],

modulenotfounderror: no java install detected. please install java to use language-tool-python.

when i run the code i get no java install detected
how to solve this issue?","['python', 'nlp', 'grammar']",67998073,"i think this is not an issue with the code itself when i run the code you provided
import language_tool_python
tl = language_tool_python.languagetool('en-us')

txt = ""good mooorning sirr and medam my namee anderen i am from amerecia !""
m = tl.check(txt)
len(m)

i get as result a number in this case
 out: 8

in the documentation of the language-tool-python is written:

by default, language_tool_python will download a languagetool server .jar and run that in the background to detect grammar errors locally. however, languagetool also offers a public http proofreading api that is supported as well. follow the link for rate-limiting details. (running locally won't have the same restrictions.)

so you will need java (jre and skd). also it's written in the requirements of the library:

prerequisites
python 3.5+
languagetool (java 8.0 or higher)
the installation process should take care of downloading languagetool (it may take a few minutes). otherwise, you can manually download languagetool-stable.zip and unzip it into where the language_tool_python package resides.

source:


python 2.7 - javaerror when using grammar-check 1.3.1 library

i hope i could help.",https://stackoverflow.com/questions/67997713,python,16-06-2021 06:50,6210.0,3.0,2.0,True,18-04-2024 09:03,16-06-2021 07:12,Implementation Issues
13392791,reading pos tag models in android,"i have tried doing pos tagging using opennlp pos models on a normal java application. now i would like to implement it on android platform. i am not sure what is the android requirement or restrictions as i am not able to read the models (binary file) and execute the pos tagging properly.
i tried getting the .bin file from external storage as well as putting it in an external libraries but still it couldn't work. these are my codes:
inputstream modelin = null;
posmodel model = null;

string path = environment.getexternalstoragedirectory().getpath() + ""/textsumit/en-pos-maxent.bin"";

modelin = new bufferedinputstream( new fileinputstream(path));
model = new posmodel(modelin);

the error i got:
11-15 06:39:35.072: w/system.err(565): opennlp.tools.util.invalidformatexception: the profile data stream has an invalid format!
11-15 06:39:35.177: w/system.err(565):  at opennlp.tools.dictionary.serializer.dictionaryserializer.create(dictionaryserializer.java:224)
11-15 06:39:35.177: w/system.err(565):  at opennlp.tools.postag.posdictionary.create(posdictionary.java:282)
11-15 06:39:35.182: w/system.err(565):  at opennlp.tools.postag.posmodel$posdictionaryserializer.create(posmodel.java:48)
11-15 06:39:35.182: w/system.err(565):  at opennlp.tools.postag.posmodel$posdictionaryserializer.create(posmodel.java:44)
11-15 06:39:35.182: w/system.err(565):  at opennlp.tools.util.model.basemodel.<init>(basemodel.java:135)
11-15 06:39:35.197: w/system.err(565):  at opennlp.tools.postag.posmodel.<init>(posmodel.java:93)
11-15 06:39:35.197: w/system.err(565):  at com.main.textsumit.summarizationactivity.postagwords(summarizationactivity.java:676)
11-15 06:39:35.205: w/system.err(565):  at com.main.textsumit.summarizationactivity.generatesummary(summarizationactivity.java:252)
11-15 06:39:35.205: w/system.err(565):  at com.main.textsumit.summarizationactivity.oncreate(summarizationactivity.java:127)

what is it that cause it not reading the model properly? and how should i resolve this? please help.
thank you.","['android', 'machine-learning', 'nlp', 'opennlp']",13564541,"for what it's worth, if this is still an issue: i had a similar issue attempting to use the pos model in a different context (non-android), and in my case it appeared to be the extraction failing from the bin file, not anything with the model itself.  it appears to be local to the tags.tagdict file in the archive (as suggested here  so if you don't need that currently (and i didn't for my simple scenarios) then try removing it from the archive.  (but leave the archive intact as it's expected to arrive in zip'd form.)",https://stackoverflow.com/questions/13392791,android,15-11-2012 06:55,1662.0,4.0,3.0,True,12-08-2022 16:32,20-06-2020 09:12,Implementation Issues
68918962,huggingface-transformers --- ner single sentence/sample prediction,"i am trying to predict with the ner model, as in the tutorial from huggingface (it contains only the training+evaluation part).
i am following this exact tutorial here : 
the training works flawlessly, but the problems that i have begin when i try to predict on a simple sample.
model_checkpoint = ""distilbert-base-uncased""
tokenizer = autotokenizer.from_pretrained(model_checkpoint)
loaded_model = automodel.from_pretrained('./my_model_own_custom_training.pth',
                                         from_tf=false)



input_sentence = ""john nash is a great mathematician, he lives in france""
tokenized_input_sentence = tokenizer([input_sentence],
                                     truncation=true, 
                                     is_split_into_words=false,
                                     return_tensors='pt')
predictions = loaded_model(tokenized_input_sentence[""input_ids""])[0]

predictions is of shape (1,13,768)
how can i arrive at the final result of the form [john <-> ï¿½ï¿½ï¿½ýý, ýýý france <-> ýýýb-locýýý], where b-per and b-loc are two ground truth labels, representing the tag for a person and location respectively?
the result of the prediction is:
torch.size([1, 13, 768])

if i write:
print(predictions.argmax(axis=2))
tensor([613, 705, 244, 620, 206, 206, 206, 620, 620, 620, 477, 693, 308])

i get the tensor above.
however i would have expected to get the tensor representing the ground truth [0ýýý8] labels from the ground truth annotations.
summary when loading the model :

loading configuration file ./my_model_own_custom_training.pth/config.json
model config distilbertconfig {
ýýýname_or_path"": ýýýdistilbert-base-uncasedýýý,
ýýýactivationýýý: ýýýgeluýýý,
ýýýarchitecturesýýý: [
ýýýdistilbertfortokenclassificationýýý
],
ýýýattention_dropoutýýý: 0.1,
ýýýdimýýý: 768,
ýýýdropoutýýý: 0.1,
ýýýhidden_dimýýý: 3072,
ýýýid2labelï¿½ï¿½ï¿½: {
ï¿½ï¿½ï¿½0ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_0ï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½1ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_1ï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½2ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_2ï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½3ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_3ï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½4ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_4ï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½5ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_5ï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½6ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_6ï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½7ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_7ï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½8ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_8ï¿½ï¿½ï¿½
},
ï¿½ï¿½ï¿½initializer_rangeï¿½ï¿½ï¿½: 0.02,
ï¿½ï¿½ï¿½label2idï¿½ï¿½ï¿½: {
ï¿½ï¿½ï¿½label_0ï¿½ï¿½ï¿½: 0,
ï¿½ï¿½ï¿½label_1ï¿½ï¿½ï¿½: 1,
ï¿½ï¿½ï¿½label_2ï¿½ï¿½ï¿½: 2,
ï¿½ï¿½ï¿½label_3ï¿½ï¿½ï¿½: 3,
ï¿½ï¿½ï¿½label_4ï¿½ï¿½ï¿½: 4,
ï¿½ï¿½ï¿½label_5ï¿½ï¿½ï¿½: 5,
ï¿½ï¿½ï¿½label_6ï¿½ï¿½ï¿½: 6,
ï¿½ï¿½ï¿½label_7ï¿½ï¿½ï¿½: 7,
ï¿½ï¿½ï¿½label_8ï¿½ï¿½ï¿","['python-3.x', 'deep-learning', 'pytorch', 'huggingface-transformers', 'huggingface-tokenizers']",68919603,"the answer is a bit trickier than expected[huge credits to niels rogge].
firstly, loading models in huggingface-transformers can be done in (at least) two ways:

automodel.from_pretrained('./my_model_own_custom_training.pth', from_tf=false)
automodelfortokenclassification.from_pretrained('./my_model_own_custom_training.pth', from_tf=false)

it seems that, according to the task at hand, different automodels subclasses need to be used. in this scenario i posted, it is the automodelfortokenclassification() that has to be used.
after that, a solution to obtain the predictions would be to do the following:
# forward pass
outputs = model(**encoding)
logits = outputs.logits

predictions = logits.argmax(-1)",https://stackoverflow.com/questions/68918962,python-3.x,25-08-2021 07:56,1008.0,2.0,1.0,True,09-08-2022 14:02,09-08-2022 14:02,Implementation Issues
76802096,runtimeerror when trying to extract text features from a bert model then using knn for classification,"i'm trying to use camembert model to just to extract text features. after that, i'm trying to use a knn classifier to classify the feature vectors as inputs.
this is the code i wrote
import torch
from transformers import autotokenizer, camembertmodel
from sklearn.neighbors import kneighborsclassifier

tokenizer = autotokenizer.from_pretrained(""camembert-base"")
model = camembertmodel.from_pretrained(""camembert-base"")

data = df.to_dict(orient='split')
data = dict(zip(data['index'], data['data']))

# collect all the input texts into a list of strings
input_texts = [str(text) for text in data.values()]

# tokenize all the input texts together
inputs = tokenizer(input_texts, return_tensors=""pt"", padding=true, truncation=true)

# get the model outputs for all the input texts
with torch.no_grad():
    outputs = model(**inputs)

# extract the last hidden states and convert them to a numpy array
last_hidden_states = outputs.last_hidden_state
input_features = last_hidden_states[:, 0, :].numpy()

# extract the labels from the data dictionary
input_labels = list(data.keys())

neigh = kneighborsclassifier(n_neighbors=3)
neigh.fit(input_features, input_labels)


however, i get this error
runtimeerror: [enforce fail at ..\c10\core\impl\alloc_cpu.cpp:72] data. defaultcpuallocator: not enough memory: you tried to allocate 19209424896 bytes.

the data i'm using in the dictionary has this form:
{
    'index': [row_index_1, row_index_2, ...],
    'columns': [column_name_1, column_name_2, ...],
    'data': [
        [cell_value_row_1_col_1, cell_value_row_1_col_2, ...],
        [cell_value_row_2_col_1, cell_value_row_2_col_2, ...],
        ...
    ]
}","['python', 'machine-learning', 'nlp', 'bert-language-model', 'knn']",76802422,"it seems that you are feeding all your data to the model at once and you don't have enough memory to do that. instead of doing that, you can invoke the model sentence by sentence or with small sentence batches, so that you keep the needed memory within the available system resources.",https://stackoverflow.com/questions/76802096,python,31-07-2023 09:02,139.0,2.0,1.0,True,01-08-2023 07:36,01-08-2023 07:36,Implementation Issues
77103010,how to combine conversationalretrievalchain with an agent?,"i have a chain which is defined as:
convr_qa = conversationalretrievalchain(retriever=customretriever, 
memory=memory, question_generator=question_generator_chain, 
combine_docs_chain=qa_chain, return_source_documents=true,  
return_generated_question=true, verbose=true )`

but now, i want to combine my chain with an agent, where agent can decide whether to retrieve or not depends on the user's intention.
i know there is ""conversational retrieval agent"" to handle this problem, but i have no idea how to combine my conversationalretrievalchain with an agent, as both question_generator_chain and qa_chain are important in my case, and i don't want to drop them.
thanks for your attention.
i have tried conversational retrieval agent in langchain document. but it is hard to customize for me.","['python', 'langchain']",77140368,"to handle with ""how to decide to retrieve or not when using conversationalretrievalchain"", i have a another solution rather than using ""conversational retrieval agent"", which is token-consuming and not robust.
a new llmchain called ""intention_detector"" is defined in my conversationalretrievalchain, taking user's question as input.
then it will decide:
intention = self.intention_detector.run(question=question)

return true or false. the prompt template of intention_detector behaves like:
prompt_few_shots_intention_detection = fewshotprompttemplate(
examples=_intention_detection_examples, 
example_prompt=_intention_detection_prompt_template, 
prefix=""please judge does user's query be related to knowledge in specific domain. return true or false."",
suffix=""input: {question}\noutput: "", 
input_variables=[""question""]

)
fewshots examples are given above depending on your demand.
then, another llmchain can handle with:
prompt_chat_history = chatprompttemplate.from_messages([
systemmessage(content=""refer to the chat_history and answer the latest question.""), 
messagesplaceholder(variable_name=""chat_history""),
humanmessageprompttemplate.from_template(""{question}""), 

])
therefore, our conversationalretrievalchain bahaves much more smart, like:
human: who are you?
ai(answer directly): i am an ai assistant. how can i help you?

and if you propose some professional issues:
human: is biyadi worth investing in?
(after retrieving the vectordb)
ai: refer to the infomation provided by gf securities, biyadi launches a lot new ......

and finally, we can end the conversation:
human: thanks for your help!
ai(answer directly): you are welcome!

here our conversationalretrievalchain can distinguish human's intention, choosing to retrieve the vectordb or not.",https://stackoverflow.com/questions/77103010,python,14-09-2023 08:22,1887.0,2.0,1.0,True,20-09-2023 07:51,14-09-2023 09:16,Implementation Issues
61269954,attribute error using neuralcoref in colab,"i'm trying to use the following spacy module in colab: 

i install the following packages: 
!pip install spacy
import spacy 
!pip show spacy

!git clone 
import neuralcoref

i get the following output after installing: 
name: spacy
version: 2.2.4
summary: industrial-strength natural language processing (nlp) in python
home-page: 
author: explosion
author-email: contact@explosion.ai
license: mit
location: /usr/local/lib/python3.6/dist-packages
requires: thinc, murmurhash, preshed, blis, srsly, cymem, setuptools, plac, requests, tqdm, numpy, wasabi, catalogue
required-by: fastai, en-core-web-sm
cloning into 'neuralcoref'...
remote: enumerating objects: 48, done.
remote: counting objects: 100% (48/48), done.
remote: compressing objects: 100% (44/44), done.
remote: total 739 (delta 14), reused 10 (delta 1), pack-reused 691
receiving objects: 100% (739/739), 67.86 mib | 30.25 mib/s, done.
resolving deltas: 100% (368/368), done.

i then follow the instructions on the website: 
nlp = spacy.load('en')
neuralcoref.add_to_pipe(nlp)

however, i get the following error: 
---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-8-fe99e1a1a10f> in <module>()
      1 nlp = spacy.load('en')
----> 2 neuralcoref.add_to_pipe(nlp)
      3 #coref = neuralcoref.neuralcoref(nlp.vocab)
      4 #nlp.add_pipe(coref, name='neuralcoref')

attributeerror: module 'neuralcoref' has no attribute 'add_to_pipe'

does anybody know how to fix this? 
edit
after (successfully) using the suggestion below, colab crashed on me when i tried to run the example provided (see details below). 
here is the code used: 
from google.colab import drive
drive.mount('/content/gdrive')

!pip install neuralcoref

import spacy
import neuralcoref

nlp = spacy.load('en') #ï¿½ï¿½this is the line where it crashes
neuralcoref.add_to_pipe(nlp)

doc1 = nlp('my sister has a dog. she loves him.')
print(doc1._.coref_clusters)

i've attached a screenshot with the original error message at the bottom left. 

edit 2
i got the code to work on colab when changing the order of installing the modules (not sure why). 
the following has worked for me now: 
from google.colab import drive
drive.mount('/content/gdrive')

!git clone 
!pip install -u spacy
!python -m spacy download en

import spacy
nlp = spacy.load('en')

%cd neuralcoref

!pip install -r requirements.txt
!pip install -e .

import neuralcoref
neuralcoref.add_to_pipe(nlp)

doc1 = nlp('my sister has a dog. she loves him.')
print(doc1._.coref_clusters)","['python-3.x', 'google-colaboratory', 'spacy']",61270808,"update:
since the previous helped solving the first problem but created another problem, i have updated the answer.
according to neuralcoref page, for our version of spacy, we need to manually install it from the source.
also, try each of the following blocks in new cell in colab, and restart runtime after installation.
mkdir temp

cd temp

!git clone 
!pip install -u spacy
!python -m spacy download en

cd neuralcoref

!pip install -r requirements.txt
!pip install -e .


import neuralcoref
import spacy

nlp = spacy.load('en')
neuralcoref.add_to_pipe(nlp)

doc1 = nlp('my sister has a dog. she loves him.')
print(doc1._.coref_clusters)",https://stackoverflow.com/questions/61269954,python-3.x,17-04-2020 10:56,5598.0,6.0,4.0,True,06-04-2023 15:36,17-04-2020 13:27,Tool Setup/Errors
73183103,clustering based on semantic similarity returning no values,"i have 'key_phrases' as a column in pandas dataframe df. the objective is to cluster them on semantic similarity. i am using sentencetransformer model.
 df['key phrases'] is as follows

                'key_phrases'

0              ['byd' 'daiwa capital markets analyst' 'nio' 'order flows'\n 'consumer preferences' 'cost pressures' 'raw materials'\n 'regulatory pressure' 'sales cannibalization' 'sales volume growth'\n 'vehicle batteries']
1              ['canada' 'canada' 'global carbon pricing challenge'\n 'major economies forum' 'climate finance commitment'\n 'developing countries' 'energy security' 'food security'\n 'international shipping' 'pollution pricing']
2              ['clean power plan' 'epa' 'environmental protection agency'\n 'supreme court' 'supreme court decision' 'virginia' 'west virginia'\n 'renewable energy' 'tax subsidies']
3              ['blueovalsk' 'ford' 'ford motor' 'kathleen valley' 'lg energy' 'liontown'\n 'liontown resources' 'sk innovation' 'sk on' 'tesla' 'battery metals'\n 'joint venture' 'lithium spodumene concentrate'\n 'lithium supply agreement']
4              ['emissions trading system' 'european commission' 'european parliament'\n 'icis' 'carbon border adjustment mechanism' 'carbon leakage']
5              ['digital industries' 'mg motor india' 'mindsphere'\n 'plant simulation software' 'siemens' 'carbon footprints'\n 'digitalisation' 'experience' 'intelligent manufacturing'\n 'production efficiency' 'strategic collaborations']
6              ['malaysia' 'mosti' 'ntis' 'national technology and innovation sandbox'\n 'national urbanisation policy' 'sunway innovation labs'\n 'sunway ilabs super accelerator' 'economic growth'\n 'memorandum of understanding' 'quality of life' 'safe environment'\n 'smart cities' 'smart city sandbox' 'urban management' 'urban population']
7              ['artificial intelligence' 'electricity and water authority'\n 'green mobility' 'grid automation' 'internet of things' 'smart dubai'\n 'smart energy solutions' 'smart grid' 'smart water'\n 'artificial intelligence' 'blockchain' 'connected services'\n 'energy storage' 'integrated systems' 'interoperability' 'smart city'\n 'smart grid' 'sustainability' 'water network']
8              ['artificial intelligence' 'clean energy strategy 2050'\n 'dubai electricity and water authority' 'green mobility'\n 'grid automation' 'internet of things' 'smart dubai'\n 'smart energy solutions' 'smart grid' 'smart water'\n 'zero carbon emissions strategy' 'artificial intelligence' 'blockchain'\n 'clean energy sources' 'connected services' 'energy storage'\n 'integrated systems' 'interoperability' 'smart city' 'smart grid'\n 'sustainability']

key_phrases_list_1 = df['key phrases'].tolist()
from sentence_transformers import sentencetransformer, util
import numpy as np

model = sentencetransformer('distilbert-base-nli-stsb-quora-ranking')    
#encoding is done with one simple step
embeddings = model.encode(key_phrases_list_1, show_progress_bar=true, convert_to_numpy=true)

then the following function is created:
def detect_clusters(embeddings, threshold=0.90, min_community_size=20):
    # compute cosine similarity scores
    cos_scores = util.pytorch_cos_sim(embeddings, embeddings)

    #we filter those scores according to the minimum community size we specified earlier
    # minimum size for a community
    top_k_values, _ = cos_scores.topk(k=min_community_size, largest=true)
    # filter for rows >= min_threshold
    extracted_communities = []
    for i in range(len(top_k_values)):
        if top_k_values[i][-1] >= threshold:
            new_cluster = []

    # only check top k most similar entries
            top_val_large, top_idx_large = cos_scores[i].topk(k=init_max_size, largest=true)
            top_idx_large = top_idx_large.tolist()
            top_val_large = top_val_large.tolist()
            
            if top_val_large[-1] < threshold:
                for idx, val in zip(top_idx_large, top_val_large):
                    if val < threshold:
                        break
                        new_cluster.append(idx)
            else:
                # iterate over all entries (slow)
                for idx, val in enumerate(cos_scores[i].tolist()):
                    if val >= threshold:
                        new_cluster.append(idx)
                        
            extracted_communities.append(new_cluster)

    unique_communities = []
    extracted_ids = set()
        
    for community in extracted_communities:
        add_cluster = true
        for idx in community:
            if idx in extracted_ids:
                add_cluster = false
                break
        if add_cluster:
            unique_communities.append(community)
            for idx in community:
                extracted_ids.add(idx)
    return unique_communities

then the function is called:
clusters = detect_clusters(embeddings, min_community_size=6, threshold=0.75)

i am getting no values in return. am i missing anything in the detect_clusters function.","['python-3.x', 'pandas', 'numpy', 'word-embedding', 'transformer-model']",73218870,"as the op asked for a solution where the number of clusters would be automatic selected it is easier to use something more robust like sklearn:
from sentence_transformers import sentencetransformer, util
import numpy as np
model = sentencetransformer('distilbert-base-nli-stsb-quora-ranking')
from sklearn.cluster import kmeans
from sklearn.metrics import silhouette_score
def choose_classifier(x):
    x1 = x / (x**2).sum(axis=-1, keepdims=true)
    vv = []
    cc = np.arange(2, len(x))
    for nclusters in cc:
        km_model = kmeans(nclusters).fit(x1)
        labels = km_model.labels_
        v = silhouette_score(x1, labels)
        vv.append(v)
    nclusters = cc[np.argmax(vv)]
    return kmeans(nclusters).fit(x1)


use it like this
phrases = [
    'i like ice cream',
    'i like cake',
    'you are so kind',
    'you are very intelligent'
]
embeddings = model.encode(phrases, show_progress_bar=true, convert_to_numpy=true)

classifier = choose_classifier(embeddings)

for i, (v, s) in enumerate(zip(embeddings, phrases)):
    print(classifier.predict(v[np.newaxis]), s)

[1] i like ice cream
[1] i like cake
[0] you are so kind
[0] you are very intelligent

gpu capable solution
at a first sight i couldn't grasp all you are doing in your code, but let me suggest you some simplified method. i use pytorch_kmeans, and i explore the fact that the squared euclidean distance is dot(a-b,a-b) = dot(a,a) + dot(b,b) - 2 * dot(a, b), and that cosine similarity is dot(a, b) / sqrt(dot(a,a) * dot(b,b)). so (1)
multiplying a or b by a scalar does not change cosine similarity, (2) if a and b have the same length, minimizing euclidean maximizes cosine similarity. given the set of vectors you want to cluster you can (1) normalize all of them, making them the same length, (2) compute the clusters that minimize euclidean distance. then you have the clusters that maximize cosine similarity.
pip install kmeans_pytorch

setup
since you didnt' give data i will generate an example myself
import torch;
# 2d example data
# generate some random data in three clusters
npc=10
x = torch.cat([
  (torch.randn((npc, 2)) + c) * (torch.rand((npc,1))**2+1)/2 
      for c in torch.tensor([[5,3], [-7,0], [-0, -7]])])

solution
this is the code
from kmeans_pytorch import kmeans
import torch
def detect_clusters(x, nclusters, tol=1e-6):
  x = torch.as_tensor(x)
  assert x.ndim == 2
  # project the points in a hypersphere
  x1 = x / torch.sqrt(torch.sum(x**2, axis=-1, keepdims=true))

  # run kmeans on the normalized points with euclidean distance
  cluster_id, c = kmeans(x1, nclusters, distance='euclidean', tol=tol)
  return cluster_id, c

example visualization
import matplotlib.pyplot as plt
import numpy as np
import torch;

#### the resutls ####
cluster_id, c = detect_clusters(x, 3)
# avoid distortion of the angles
plt.axes().set_aspect('equal')
# initial points
plt.plot(x[:,0], x[:,1], '.')
# reference circle
theta = torch.linspace(0, 2*np.pi, 1000)
plt.plot(torch.cos(theta), torch.sin(theta), '--k')
plt.plot(x1[:,0], x1[:,1], '.')
xlim = plt.xlim()
ylim = plt.ylim()
plt.xlim(xlim)
plt.ylim(ylim)

# draw lines in the directions given by the centroids
r = 20
for c in c:
    plt.plot([0, c[0]*r], [0, c[1]*r]);

plt.grid();



using with sentence embeddings
some example embeddings
from sentence_transformers import sentencetransformer, util
import numpy as np

model = sentencetransformer('distilbert-base-nli-stsb-quora-ranking')   


phrases = [
    'i like ice cream',
    'i like cake',
    'you are so kind',
    'you are very intelligent'
]
embeddings = model.encode(phrases, show_progress_bar=true, convert_to_numpy=true)

then you can pass the embeddings to the detect_cluster function i provided above
label, center = detect_clusters(torch.as_tensor(embeddings), 2)
for c, s in zip(label, phrases):
    print(f'[{c}] {s}')

that should give give you the sentences with the corresponding cluster
[0] i like ice cream
[0] i like cake
[1] you are so kind
[1] you are very intelligent",https://stackoverflow.com/questions/73183103,python-3.x,31-07-2022 11:56,1002.0,3.0,1.0,True,05-08-2022 17:56,03-08-2022 18:30,Implementation Issues
70446032,how does text encoding from tensorflow.keras.preprocessing.text.tokenizer differ from the old tfds.deprecated.text.tokentextencoder,"tfds.deprecated.text.tokentextencoder
in the deprecated encoding method with tfds.deprecated.text.tokentextencoder
we first create a vocab set of token
tokenizer = tfds.deprecated.text.tokenizer()
vocabulary_set = set()

#imdb_train --> imdb dataset from tensorflow_datasets
for example, label in imdb_train:
    some_tokens = tokenizer.tokenize(example.numpy())


then load it into the encoder
encoder = tfds.deprecated.text.tokentextencoder(vocabulary_set,
                                                   lowercase=true,
                                                   tokenizer=tokenizer)

afterward when performing encoding i notice the encoder will output a single integer, for example while debugging i found that the word ""the"" got encoded with 112
 token_id = encoder.encode(word)[0]
>> token_id = 112

but then when it comes to
tensorflow.keras.preprocessing.text.tokenizer
tokenizer = tensorflow.keras.preprocessing.text.tokenizer()
tokenizer.fit_on_texts(words)
token_id = tokenizer.texts_to_sequences(word) #word = the
>> token_id = [800,2085,936]

it produces a sequence of 3 integers, so now do i use all 3 numbers or should it be also correct if i take just 1 number in that sequence? i'm trying to use this encoded integer to create embedding matrix using glove embedding. the old deprecated one produces just one integer so it's easier to map, with integer sequence i'm not sure how to proceed","['python', 'tensorflow', 'keras', 'nlp', 'tensorflow2.0']",70447071,"maybe try something like this:
import tensorflow as tf

lines = ['you are a fish', 'this is a fish', 'where are the fishes']
tokenizer = tf.keras.preprocessing.text.tokenizer()
tokenizer.fit_on_texts(lines)
text_sequences = tokenizer.texts_to_sequences(lines)
text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences, padding='post')
vocab_size = len(tokenizer.word_index) + 1
print(tokenizer.word_index)
print(vocab_size)
print(tokenizer.texts_to_sequences(['fish'])[0])

{'are': 1, 'a': 2, 'fish': 3, 'you': 4, 'this': 5, 'is': 6, 'where': 7, 'the': 8, 'fishes': 9}
10
[3]

the index 0 is reserved for the padding token. and then to create the weight matrix with the glove model, try this:
import gensim.downloader as api
import numpy as np

model = api.load(""glove-twitter-25"")
embedding_dim = 25
weight_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in tokenizer.word_index.items():
  try:
    embedding_vector = model[word]
    weight_matrix[i] = embedding_vector
  except keyerror:
    weight_matrix[i] = np.random.uniform(-5, 5, embedding_dim)
print(weight_matrix.shape)
# (10, 25)",https://stackoverflow.com/questions/70446032,python,22-12-2021 08:16,1254.0,2.0,1.0,True,23-12-2021 07:49,23-12-2021 07:49,Implementation Issues
77455738,finding the nouns in a sentence given the context in python,"how to find the nouns in a sentence regarding the context? i am using the nltk library as follows:
text = 'i bought a vintage car.'
text = nltk.word_tokenize(text)
result = nltk.pos_tag(text)
result = [i for i in result if i[1] == 'nn']

#result = [('vintage', 'nn'), ('car', 'nn')]

the problem with this script is that it considers vintage as a noun, which can be true, but given the context, it is an adjective.
how can we achieve this task?
appendix: using textblob, we get ""vintage car"" as the noun:
!python -m textblob.download_corpora
from textblob import textblob
txt = ""i bought a vintage car.""
blob = textblob(txt)
print(blob.noun_phrases) #['vintage car']","['python', 'nlp', 'nltk', 'textblob']",77455927,"using spacy might solve your task. try this:
import spacy
nlp = spacy.load(""en_core_web_lg"")

def analyze(text):
    doc = nlp(text)
    for token in doc:
        print(token.text, token.pos_)

analyze(""i bought a vintage car."")
print()
analyze(""this old wine is a vintage."")

output
i pron
bought verb
a det
vintage adj <- correctly identified as adjective
car noun
. punct

this det
old adj
wine noun
is aux
a det
vintage noun  <- correctly identified as noun
. punct",https://stackoverflow.com/questions/77455738,python,09-11-2023 18:59,181.0,3.0,2.0,True,09-11-2023 19:43,09-11-2023 19:13,Data Wrangling
70607224,huggingface - &#39;optimum&#39; modulenotfounderror,"i want to run the 3 code snippets from this webpage.
i've made all 3 one post, as i am assuming it all stems from the same problem of optimum not having been imported correctly?
kernel: conda_pytorch_p36

installations:
pip install optimum

or
! pip install datasets transformers optimum[intel]

both provide same traceback:
requirement already satisfied: optimum in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.1.3)
requirement already satisfied: transformers>=4.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (4.15.0)
requirement already satisfied: coloredlogs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (15.0.1)
requirement already satisfied: torch>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (1.10.1)
requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (1.8)
requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch>=1.9->optimum) (3.10.0.0)
requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch>=1.9->optimum) (0.8)
requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (1.19.5)
requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (21.3)
requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (5.4.1)
requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (0.0.46)
requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (4.62.3)
requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (2021.4.4)
requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (2.25.1)
requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (0.2.1)
requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (0.10.3)
requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (4.5.0)
requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (3.0.12)
requirement already satisfied: humanfriendly>=9.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from coloredlogs->optimum) (10.0)
requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sympy->optimum) (1.2.1)
requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->transformers>=4.12.0->optimum) (2.4.7)
requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers>=4.12.0->optimum) (3.4.1)
requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers>=4.12.0->optimum) (2.10)
requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers>=4.12.0->optimum) (2021.5.30)
requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers>=4.12.0->optimum) (4.0.0)
requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers>=4.12.0->optimum) (1.26.5)
requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers>=4.12.0->optimum) (1.0.1)
requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers>=4.12.0->optimum) (8.0.1)
requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers>=4.12.0->optimum) (1.16.0)
note: you may need to restart the kernel to use updated packages.


from optimum.intel.lpot.quantization import lpotquantizerforsequenceclassification

# create quantizer from config 
quantizer = lpotquantizerforsequenceclassification.from_config(
    ""echarlaix/quantize-dynamic-test"",
    ""quantization.yml"",
    model_name_or_path=""textattack/bert-base-uncased-sst-2"",
)

model = quantizer.fit_dynamic()

traceback:
---------------------------------------------------------------------------
modulenotfounderror                       traceback (most recent call last)
<ipython-input-6-9dcf25f181ea> in <module>
----> 1 from optimum.intel.lpot.quantization import lpotquantizerforsequenceclassification
      2 
      3 # create quantizer from config
      4 quantizer = lpotquantizerforsequenceclassification.from_config(
      5     ""echarlaix/quantize-dynamic-test"",

modulenotfounderror: no module named 'optimum.intel.lpot'

from optimum.intel.lpot.pruning import lpotprunerforsequenceclassification

# create pruner from config 
pruner = lpotprunerforsequenceclassification.from_config(
    ""echarlaix/magnitude-pruning-test"",
    ""prune.yml"",
    model_name_or_path=""textattack/bert-base-uncased-sst-2"",
)

model = pruner.fit()

traceback:
---------------------------------------------------------------------------
modulenotfounderror                       traceback (most recent call last)
<ipython-input-7-e9872c164aee> in <module>
----> 1 from optimum.intel.lpot.pruning import lpotprunerforsequenceclassification
      2 
      3 # create pruner from config
      4 pruner = lpotprunerforsequenceclassification.from_config(
      5     ""echarlaix/magnitude-pruning-test"",

modulenotfounderror: no module named 'optimum.intel.lpot'

from optimum.graphcore import iputrainer
from optimum.graphcore.bert import bertipuconfig
from transformers import bertformaskedlm, berttokenizer
from poptorch.optim import adamw

# allocate model and tokenizer as usual
tokenizer = berttokenizer.from_pretrained(""bert-base-cased"")
model = bertformaskedlm.from_pretrained(""bert-base-cased"")

# trainer + poptorch custom configuration optional 
ipu_config = bertipuconfig()
trainer = iputrainer(model, trainings_args, config=ipu_config)
optimizer = adamw(model.parameters)

# this is hidden from the user, it will be handled by the trainer
with trainer.compile(some_data_loader) as model_f:
    for steps in range(10):  # !
        outputs = trainer.step(optimizer)    

# save the model and/or push to hub
model.save_pretrained(""..."")
model.push_to_hub(""..."")

traceback:
---------------------------------------------------------------------------
modulenotfounderror                       traceback (most recent call last)
<ipython-input-8-921e03245390> in <module>
----> 1 from optimum.graphcore import iputrainer
      2 from optimum.graphcore.bert import bertipuconfig
      3 from transformers import bertformaskedlm, berttokenizer
      4 from poptorch.optim import adamw
      5 

modulenotfounderror: no module named 'optimum.graphcore'

please let me know if there's anything else i can add to post.","['python', 'huggingface-transformers', 'quantization', 'modulenotfounderror', 'pruning']",70667079,"pointed out by a contributor of huggingface, on this git issue,

the library previously named lpot has been renamed to intel neural compressor (inc), which resulted in a change in the name of our subpackage from lpot to neural_compressor.
the correct way to import would now be from optimum.intel.neural_compressor.quantization import incquantizerforsequenceclassification
concerning the graphcore subpackage, you need to install it first with pip install optimum[graphcore]
furthermore you'll need to have access to an ipu in order to use it.


solution
! pip install datasets transformers optimum[graphcore]

instead of:
from optimum.intel.lpot.quantization import lpotquantizerforsequenceclassification
from optimum.intel.lpot.pruning import lpotprunerforsequenceclassification

from optimum.intel.neural_compressor.quantization import incquantizerforsequenceclassification
from optimum.intel.neural_compressor.pruning import incprunerforsequenceclassification",https://stackoverflow.com/questions/70607224,python,06-01-2022 12:39,11186.0,1.0,2.0,True,27-08-2024 18:36,06-01-2022 12:46,Tool Setup/Errors
195010,how can i split multiple joined words?,"i have an array of 1000 or so entries, with examples below:
wickedweather
liquidweather
driveourtrucks
gocompact
slimprojector

i would like to be able to split these into their respective words, as:
wicked weather
liquid weather
drive our trucks
go compact
slim projector

i was hoping a regular expression my do the trick.  but, since there is no boundary to stop on, nor is there any sort of capitalization that i could possibly key on, i am thinking, that some sort of reference to a dictionary might be necessary?  
i suppose it could be done by hand, but why - when it can be done with code! =)  but this has stumped me.  any ideas?","string, nlp",481773,"the viterbi algorithm is much faster. it computes the same scores as the recursive search in dmitry's answer above, but in o(n) time. (dmitry's search takes exponential time; viterbi does it by dynamic programming.)
import re
from collections import counter

def viterbi_segment(text):
    probs, lasts = [1.0], [0]
    for i in range(1, len(text) + 1):
        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)
                        for j in range(max(0, i - max_word_length), i))
        probs.append(prob_k)
        lasts.append(k)
    words = []
    i = len(text)
    while 0 < i:
        words.append(text[lasts[i]:i])
        i = lasts[i]
    words.reverse()
    return words, probs[-1]

def word_prob(word): return dictionary[word] / total
def words(text): return re.findall('[a-z]+', text.lower()) 
dictionary = counter(words(open('big.txt').read()))
max_word_length = max(map(len, dictionary))
total = float(sum(dictionary.values()))

testing it:
>>> viterbi_segment('wickedweather')
(['wicked', 'weather'], 5.1518198982768158e-10)
>>> ' '.join(viterbi_segment('itseasyformetosplitlongruntogetherblocks')[0])
'its easy for me to split long run together blocks'

to be practical you'll likely want a couple refinements:

add logs of probabilities, don't multiply probabilities. this avoids floating-point underflow.
your inputs will in general use words not in your corpus. these substrings must be assigned a nonzero probability as words, or you end up with no solution or a bad solution. (that's just as true for the above exponential search algorithm.) this probability has to be siphoned off the corpus words' probabilities and distributed plausibly among all other word candidates: the general topic is known as smoothing in statistical language models. (you can get away with some pretty rough hacks, though.) this is where the o(n) viterbi algorithm blows away the search algorithm, because considering non-corpus words blows up the branching factor.",https://stackoverflow.com/q/195010,"string, nlp",12-10-2008 02:37,37898.0,58.0,16.0,True,05-07-2023 05:27,06-05-2012 07:34,Implementation Issues
75624308,openai gpt-3 api errors: &#39;text&#39; does not exist ts(2339) &amp; &#39;prompt&#39; does not exist on type &#39;createchatcompletion&#39; ts(2345),"import openai from ""./zggpt"";

const query = async (prompt:string,  chatid:string, model:string) => {
    const res= await openai
    .createchatcompletion({
        model,
        prompt,
        temperature: 0.9,
        
        top_p:1,
       
        max_tokens:1000,
        frequency_penalty:0,
        presence_penalty:0,
    })
    .then((res) => res.data.choices[0].text)
    .catch((err)=>
    `zg was unable to find an answer for that!
     (error: ${err.message})`
     );
     return res;
};

export default query;

property 'text' does not exist on type 'createchatcompletionresponsechoicesinner'.ts(2339)
argument of type '{ model: string; prompt: string; temperature: number; top_p: number; max_tokens: number; frequency_penalty: number; presence_penalty: number; }' is not assignable to parameter of type 'createchatcompletionrequest'.
object literal may only specify known properties, and 'prompt' does not exist in type 'createchatcompletionrequest'.ts(2345)
even though i do everything as in the video, i get these errors.
i'm a beginner in coding, so i'm trying to make applications based on videos to learn.
thanks
the application responds without returning an error.
enter image description here","['next.js', 'openai-api', 'gpt-3']",75626340,"problem
you watched a tutorial which used the completions api endpoint where you need to provide the prompt and parameters to get a completion). in this case, this is the function which generates a completion:
openai.createcompletion()

whereas, you used the code from the tutorial, but used the chat completions api endpoint (you need to provide the chat message to get a completion). in this case, this is the function which generates a completion:
openai.createchatcompletion()

note: openai nodejs sdk v4 was released on august 16, 2023, and is a complete rewrite of the sdk. among other things, there are changes in method names. see the v3 to v4 migration guide.




api endpoint
nodejs function (sdk v3)
nodejs function (sdk v4)




chat completions api
openai.createchatcompletion
openai.chat.completions.create


completions api
openai.createcompletion
openai.completions.create




solution
ï¿½ï¿½ï¿½ if you have the openai nodejs sdk v3:
change this...
openai.createchatcompletion()

...to this.
openai.createcompletion()

ï¿½ï¿½ï¿½ if you have the openai nodejs sdk v4:
change this...
openai.chat.completions.create

...to this.
openai.completions.create

both errors will disappear.

my advice
however, you want to achieve a chat-like bot using a gpt-3 model. at the time the tutorial was recorded, thily way to do it. since 1 march 2023, the gpt-3.5-turbo model is available. i strongly suggest you to use it. see the official openai documentation.",https://stackoverflow.com/questions/75624308,next.js,03-03-2023 07:34,2142.0,0.0,2.0,True,14-09-2023 15:24,03-03-2023 11:23,Implementation Issues
79091722,unable to install `evals` python cli for openai,"failing to install: pip install evals. for complete logs please see this gist file here.
tools version:

python --version: python 3.9.6
pip --version: pip 24.2 from /users/apple/library/python/3.9/lib/python/site-packages/pip (python 3.9)

collecting keras<2.8,>=2.7.0rc0 (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals)
  using cached keras-2.7.0-py2.py3-none-any.whl.metadata (1.3 kb)
collecting tensorboard~=2.6 (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals)
  using cached tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kb)
  using cached tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kb)
  using cached tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kb)
  using cached tensorboard-2.11.2-py3-none-any.whl.metadata (1.9 kb)
collecting protobuf (from google-generativeai->evals)
  using cached protobuf-3.20.3-cp39-cp39-macosx_10_9_x86_64.whl.metadata (679 bytes)
collecting tensorboard~=2.6 (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals)
  using cached tensorboard-2.11.0-py3-none-any.whl.metadata (1.9 kb)
  using cached tensorboard-2.10.1-py3-none-any.whl.metadata (1.9 kb)
  using cached tensorboard-2.10.0-py3-none-any.whl.metadata (1.9 kb)
  using cached tensorboard-2.7.0-py3-none-any.whl.metadata (1.9 kb)
  using cached tensorboard-2.6.0-py3-none-any.whl.metadata (1.9 kb)
collecting tensorflow<3.0.0,>=2.4.0 (from spacy-universal-sentence-encoder->evals)
  using cached tensorflow-2.7.1-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.9 kb)
  using cached tensorflow-2.7.0-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.9 kb)
info: pip is still looking at multiple versions of tf-keras to determine which version is compatible with other requirements. this could take a while.
  using cached tensorflow-2.6.5-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.6.4-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.6.3-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.6.2-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.6.1-cp39-cp39-macosx_10_14_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.6.0-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.5.3-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.5.2-cp39-cp39-macosx_10_14_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.5.1-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.5.0-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kb)
collecting sqlalchemy<3,>=1.4 (from langchain->evals)
  using cached sqlalchemy-2.0.35-cp39-cp39-macosx_10_9_x86_64.whl.metadata (9.6 kb)
info: this is taking longer than usual. you might need to provide the dependency resolver with stricter constraints to reduce runtime. see  for guidance. if you want to abort this run, press ctrl + c.
  using cached sqlalchemy-2.0.34-cp39-cp39-macosx_10_9_x86_64.whl.metadata (9.6 kb)
  using cached sqlalchemy-2.0.33-cp39-cp39-macosx_10_9_x86_64.whl.metadata (9.6 kb)
error: exception:
traceback (most recent call last):
  file ""/users/apple/library/python/3.9/lib/python/site-packages/pip/_internal/cli/base_command.py"", line 105, in _run_wrapper
    status = _inner_run()
  file ""/users/apple/library/python/3.9/lib/python/site-packages/pip/_internal/cli/base_command.py"", line 96, in _inner_run
    return self.run(options, args)
  file ""/users/apple/library/python/3.9/lib/python/site-packages/pip/_internal/cli/req_command.py"", line 67, in wrapper
    return func(self, options, args)
  file ""/users/apple/library/python/3.9/lib/python/site-packages/pip/_internal/commands/install.py"", line 379, in run
    requirement_set = resolver.resolve(
  file ""/users/apple/library/python/3.9/lib/python/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 95, in resolve
    result = self._result = resolver.resolve(
  file ""/users/apple/library/python/3.9/lib/python/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 546, in resolve
    state = resolution.resolve(requirements, max_rounds=max_rounds)
  file ""/users/apple/library/python/3.9/lib/python/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 457, in resolve
    raise resolutiontoodeep(max_rounds)
pip._vendor.resolvelib.resolvers.resolutiontoodeep: 200000

as we can see in the end of below log, we see some error. please help me debug this, i'm new to python and tried to find the answer but couldn't figure it out after waiting to install this for like 1 hours twice. the error is like pip._vendor.resolvelib.resolvers.resolutiontoodeep: 200000.
please help. thanks in advance.
github respository issue by me on this python library: 
community post on openai:","['python', 'python-3.x', 'pip', 'openai-api']",79107808,"i updated my python to latest version from official python website on recommendation of @stark-jarvis, and that helped me. thanks @stark-jarvis.",https://stackoverflow.com/questions/79091722,python,15-10-2024 20:54,133.0,-1.0,2.0,True,20-10-2024 18:43,16-10-2024 10:25,Tool Setup/Errors
66685754,finding the dominant topic in each sentence in topic modeling,"one question that i can't find the answer for in r is that how i can find the dominant topic in nlp model for each sentence?
imagine i have data frame like this:
comment <- c(""outstanding renovation all improvements are topoftheline and done with energy efficiency in mind low monthly utilities even the interior"",
             ""solidly constructed lovingly maintained sf crest built"",
             ""one year since built new this well designed storey home"",
             ""beautiful street large bdm in the heart of lynn valley over sqft bathrooms"",
             ""rare to find legal beautiful upgr in port moody centre with a mountain view all bedroom units were nicely renovated"",
             ""fantastic opportunity to get value for the money excellent family home in desirable blueridge with legal selfcontained bachelor suite on the main floor great location close to swimming ice skating community"",
             ""original owner tired but rock solid perfect location half a block to norquay elementary school and short quiet blocks to slocan park and sky train station"")

id <- c(1,2,3,4,5,6,7)

data <- data.frame(id, comment)

i do preprocess as shown below:
text_cleaning_tokens <- data %>% 
  tidytext::unnest_tokens(word, comment)
text_cleaning_tokens$word <- gsub('[[:digit:]]+', '', text_cleaning_tokens$word)
text_cleaning_tokens$word <- gsub('[[:punct:]]+', '', text_cleaning_tokens$word)


text_cleaning_tokens <- text_cleaning_tokens %>% filter(!(nchar(word) == 1))%>% 
  anti_join(stop_words)

stemmed_token <- text_cleaning_tokens %>% mutate(word=wordstem(word))


tokens <- stemmed_token %>% filter(!(word==""""))
tokens <- tokens %>% mutate(ind = row_number())
tokens <- tokens %>% group_by(id) %>% mutate(ind = row_number()) %>%
  tidyr::spread(key = ind, value = word)
tokens [is.na(tokens)] <- """"
tokens <- tidyr::unite(tokens, clean_remark,-id,sep ="" "" )
tokens$clean_remark <- trimws(tokens$clean_remark)

the i ran fitldamodel function on this data and finally, found the best topics based on 2 groups:
             t_1            t_2
1         beauti          built
2          block           home
3          renov          legal
4       bathroom          locat
5            bdm       bachelor
6      bdm_heart  bachelor_suit
7  beauti_street  block_norquai
8    beauti_upgr       blueridg
9        bedroom blueridg_legal
10  bedroom_unit   built_design

now based on the result i have, i want to find the most dominant topic in each sentence in topic modelling. for example, i want to know that for comment 1 (""outstanding renovation all improvements are topoftheline and done with energy efficiency in mind low monthly utilities even the interior""), which topic (topic 1 or topic 2) is the most dominant?
can anyone help me with this question? do we have any package that can do this?","['r', 'nlp', 'text-mining', 'topic-modeling']",66687746,"it is pretty easy to work with quanteda and topicmodels. the former is for data management and quantitative analysis of textual data, the latter is for topic modeling inference.
here i take your comment object and transform it to a corpus and then to a dfm. i then convert it to be understandable by topicmodels.
the function lda() gives you all you need to easily extract information. in particular, with get_topics() you get the most probable topic for each document. if you instead want to see the document-topic-weights you can do so with ldamodel@gamma. you will see that get_topics() does exactly what you asked.
please, see if this works for you.
library(quanteda)
#> package version: 2.1.2
#> parallel computing: 2 of 16 threads used.
#> see  for tutorials and examples.
#> 
#> attaching package: 'quanteda'
#> the following object is masked from 'package:utils':
#> 
#>     view
library(topicmodels)


comment <- c(""outstanding renovation all improvements are topoftheline and done with energy efficiency in mind low monthly utilities even the interior"",
             ""solidly constructed lovingly maintained sf crest built"",
             ""one year since built new this well designed storey home"",
             ""beautiful street large bdm in the heart of lynn valley over sqft bathrooms"",
             ""rare to find legal beautiful upgr in port moody centre with a mountain view all bedroom units were nicely renovated"",
             ""fantastic opportunity to get value for the money excellent family home in desirable blueridge with legal selfcontained bachelor suite on the main floor great location close to swimming ice skating community"",
             ""original owner tired but rock solid perfect location half a block to norquay elementary school and short quiet blocks to slocan park and sky train station"")

mycorp <- corpus(comment)
docvars(mycorp, ""id"") <- 1l:7l

mydfm <- dfm(mycorp)

# convert the dfm to a document matrix for topicmodels
fortm <- convert(mydfm, to = ""topicmodels"")

mylda <- lda(fortm, k = 2)

dominant_topics <- get_topics(mylda)
dominant_topics
#> text1 text2 text3 text4 text5 text6 text7 
#>     2     2     2     2     1     1     1

dtw <- mylda@gamma
dtw
#>           [,1]      [,2]
#> [1,] 0.4870600 0.5129400
#> [2,] 0.4994974 0.5005026
#> [3,] 0.4980144 0.5019856
#> [4,] 0.4938985 0.5061015
#> [5,] 0.5037667 0.4962333
#> [6,] 0.5000727 0.4999273
#> [7,] 0.5176960 0.4823040

created on 2021-03-18 by the reprex package (v1.0.0)",https://stackoverflow.com/questions/66685754,r,18-03-2021 06:22,1040.0,0.0,2.0,True,07-10-2022 11:40,18-03-2021 06:56,Task-specific Help
52286330,inaccurate similarities results by doc2vec using gensim library,"i am working with gensim library to train some data files using doc2vec,  while trying to test the similarity of one of the files using the method model.docvecs.most_similar(""file"") , i always get all the results above 91% with almost no difference between them (which is not logic), because the files do not have similarities between them. so the results are inaccurate.
here is the code for training the model
model = gensim.models.doc2vec(vector_size=300, min_count=0, alpha=0.025, min_alpha=0.00025,dm=1)
model.build_vocab(it)
for epoch in range(100):
    model.train(it,epochs=model.iter, total_examples=model.corpus_count)
    model.alpha -= 0.0002
    model.min_alpha = model.alpha
model.save('doc2vecs.model')
model_d2v = gensim.models.doc2vec.doc2vec.load('doc2vecs.model')
sim = model_d2v.docvecs.most_similar('file1.txt')
print sim

 
**this is the output result**


[('file2.txt', 0.9279470443725586), ('file6.txt', 0.9258157014846802), ('file3.txt', 0.92499840259552), ('file5.txt', 0.9209873676300049), ('file4.txt', 0.9180108308792114), ('file7.txt', 0.9141069650650024)]


what am i doing wrong ? how could i improve the accuracy of results ?","['python', 'nlp', 'gensim', 'doc2vec']",52300508,"what is your it data, and how is it prepared? (for example, what does print(iter(it).next()) do, especially if you call it twice in a row?)
by calling train() 100 times, and also retaining the default model.iter of 5, you're actually making 500 passes over the data. and the first 5 passes will use train()s internal, effective alpha-management to lower the learning rate gradually to your declared min_alpha value. then your next 495 passes will be at your own clumsily-managed alpha rates, first back up near 0.025 and then lower each batch-of-5 until you reach 0.005.
none of that is a good idea. you can just call train() once, passing it your desired number of epochs. a typical number of epochs in published work  is 10-20. (a bit more might help with a small dataset, but if you think you need hundreds, something else is probably wrong with the data or setup.)
if it's a small amount of data, you won't get very interesting word2vec/doc2vec results, as these algorithms depend on lots of varied examples. published results tend to use training sets with tens-of-thousands to millions of documents, and each document at least dozens, but preferably hundreds, of words long. with tinier datasets, sometimes you can squeeze out adequate results by using more training passes, and smaller vectors. also using the simpler pv-dbow mode (dm=0) may help with smaller corpuses/documents. 
the values reported by most_similar() are not similarity ""percentages"". they're cosine-similarity values, from -1.0 to 1.0, and their absolute values are less important than the relative ranks of different results. so it shouldn't matter if there are a lot of results with >0.9 similarities ï¿½ï¿½ï¿½ as long as those documents are more like the query document than those lower in the rankings.
looking at the individual documents suggested as most-similar is thus the real test. if they seem like nonsense, it's likely there are problems with your data  or its preparation, or training parameters. 
for datasets with sufficient, real natural-language text, it's typical for higher min_count values to give better results. real text tends to have lots of low-frequency words that don't imply strong things without many more examples, and thus keeping them during training serves as noise making the model less strong.",https://stackoverflow.com/questions/52286330,python,12-09-2018 01:44,1327.0,2.0,2.0,True,01-10-2022 14:31,01-10-2022 14:31,Conceptual Questions
66473977,document layout analysis for text extraction,"i need to analyze the layout structure of different documents type like: pdf, doc, docx, odt etc.
my task is:
giving a document, group the text in blocks finding the correct boundaries of each.
i did some tests using apache tika, which is a good extractor, it is a very good tool but it often mess up the order of the block, let me explain a bit what i mean with order.
apache tika just extracts the text, so if my document has two columns, tika extracts the entire text of the first column and then the text of the second column, which is ok...but sometimes the text on the first column is related to the text  on the second, like a table that has row relation.
so i must take care of the positions of each block, so the problems are:

define the box boundaries, which is hard... i should understand if a sentence is starting a new block or not.

define the orientation, for example, giving a table the ""sentence"" should be the row, not the column.


so basically here i have to deal with the layout structure to correcly understand the block boundaries.
i give you a visual example:

a classical extractor returns:
2019
2018
2017
2016
2015
2014
oregon arts commission individual artist fellowship...

which is wrong (in my case) because the dates are related to the texts on the right.
this task is preparatory for other nlp analysis, so it is very important, because, for example doing, when i need to recognize the entities(ner) inside the text, and then identify their relations, working with the correct context is very important.
how to extract the text from the document and assembly related pieces of text (understanding the layout structure of the document) under the same block?","['python', 'machine-learning', 'nlp', 'artificial-intelligence', 'document-layout-analysis']",66550563,"this is but a partial solution to your issue, but it may simplify the task at hand.
this tool receives pdf files and converts them to text files. it works pretty fast and can run on bulks of files.
it creates an output text file for each pdf. the advantage of this tool over others is that the output texts are aligned with accordance to their original layout.
for example, this is a resume with complex layout:

the output for it is the following text file:
christopher                         summary
                                    senior web developer specializing in front end development.
morgan                              experienced with all stages of the development cycle for
                                    dynamic web projects. well-versed in numerous programming
                                    languages including html5, php oop, javascript, css, mysql.
                                    strong background in project management and customer
                                    relations.


                                    skill highlights
                                        ï¿½ï¿½ï¿½   project management          ï¿½ï¿½ï¿½   creative design
                                        ï¿½ï¿½ï¿½   strong decision maker       ï¿½ï¿½ï¿½   innovative
                                        ï¿½ï¿½ï¿½   complex problem             ï¿½ï¿½ï¿½   service-focused
               

                                    experience
contact
                                    web developer - 09/2015 to 05/2019
address:                            luna web design, new york
177 great portland street, london      ï¿½ï¿½ï¿½ cooperate with designers to create clean interfaces and
w5w 6pq                                   simple, intuitive interactions and experiences.
                                       ï¿½ï¿½ï¿½ develop project concepts and maintain optimal
phone:                                    workflow.
+44 (0)20 7666 8555
                                       ï¿½ï¿½ï¿½ work with senior developer to manage large, complex
                                          design projects for corporate clients.
email:
                                       ï¿½ï¿½ï¿½ complete detailed programming and development tasks
christoper.m@gmail.com
                                          for front end public and internal websites as well as
                                          challenging back-end                                       ï¿½ï¿½ï¿½ carry out quality assurance tests to discover errors and
linkedin.com/christopher.morgan
                                          optimize usability.

languages                           education
spanish ï¿½ï¿½ï¿½ c2
                                    bachelor of science: computer information systems - 2014
chinese ï¿½ï¿½ï¿½ a1
                                    columbia university, ny
german ï¿½ï¿½ï¿½ a2


hobbies                             certifications
                                    php framework (certificate): zend, codeigniter, symfony.
   ï¿½ï¿½ï¿½   writing
                                    programming languages: javascript, html5, php oop, css,
   ï¿½ï¿½ï¿½   sketching
                                    sql, mysql.
   ï¿½ï¿½ï¿½   photography
   ï¿½ï¿½ï¿½   design
-----------------------page 1 end-----------------------

now your task is reduced to finding the bulks within a text file, and using the spaces between words as alat finds the margin between to columns of text and yields rhs and lhs - the text stream of the right and left columns respectively.
import numpy as np
import matplotlib.pyplot as plt
import re

txt_lines = txt.split('\n')
max_line_index = max([len(line) for line in txt_lines])
padded_txt_lines = [line + "" "" * (max_line_index - len(line)) for line in txt_lines] # pad short lines with spaces
space_idx_counters = np.zeros(max_line_index)

for idx, line in enumerate(padded_txt_lines):
    if line.find(""-----------------------page"") >= 0: # reached end of page
        break
    space_idxs = [pos for pos, char in enumerate(line) if char == "" ""]
    space_idx_counters[space_idxs] += 1

padded_txt_lines = padded_txt_lines[:idx] #remove end page line

# plot histogram of spaces in each character column
plt.bar(list(range(len(space_idx_counters))), space_idx_counters)
plt.title(""number of spaces in each column over all lines"")
plt.show()

# find the separator column idx
separator_idx = np.argmax(space_idx_counters)
print(f""separator index: {separator_idx}"")
left_lines = []
right_lines = []

# separate two columns of text
for line in padded_txt_lines:
    left_lines.append(line[:separator_idx])
    right_lines.append(line[separator_idx:])

# join each bulk into one stream of text, remove redundant spaces
lhs = ' '.join(left_lines)
lhs = re.sub(""\s{4,}"", "" "", lhs)
rhs = ' '.join(right_lines)
rhs = re.sub(""\s{4,}"", "" "", rhs)

print(""************ left hand side ************"")
print(lhs)
print(""************ right hand side ************"")
print(rhs)

plot output:

text output:
separator index: 33
************ left hand side ************
christopher morgan contact address: 177 great portland street, london w5w 6pq phone: +44 (0)20 7666 8555 email: christoper.m@gmail.com linkedin: linkedin.com/christopher.morgan languages spanish ï¿½ï¿½ï¿½ c2 chinese ï¿½ï¿½ï¿½ a1 german ï¿½ï¿½ï¿½ a2 hobbies ï¿½ï¿½ï¿½   writing ï¿½ï¿½ï¿½   sketching ï¿½ï¿½ï¿½   photography ï¿½ï¿½ï¿½   design 
************ right hand side ************
   summary senior web developer specializing in front end development. experienced with all stages of the development cycle for dynamic web projects. well-versed in numerous programming languages including html5, php oop, javascript, css, mysql. strong background in project management and customer relations. skill highlights ï¿½ï¿½ï¿½   project management ï¿½ï¿½ï¿½   creative design ï¿½ï¿½ï¿½   strong decision maker ï¿½ï¿½ï¿½   innovative ï¿½ï¿½ï¿½   complex problem ï¿½ï¿½ï¿½   service-focused solver experience web developer - 09ate clean interfaces and simple, intuitive interactions and experiences. ï¿½ï¿½ï¿½ develop project concepts and maintain optimal workflow. ï¿½ï¿½ï¿½ work with senior developer to manage large, complex design projects for corporate clients. ï¿½ï¿½ï¿½ complete detailed programming and development tasks for front end public and internal websites as well as challenging back-end server code. ï¿½ï¿½ï¿½ carry out quality assurance tests to discover errors and optimize usability. education bachelor of science: computer information systems - 2014 columbia university, ny certifications php framework (certificate): zend, codeigniter, symfony. programming languages: javascript, html5, php oop, css, sql, mysql. 

the next step would be to generalize this script to work on multi-page documents, remove redundant signs, etc",https://stackoverflow.com/questions/66473977,python,04-03-2021 11:20,5819.0,6.0,5.0,True,15-07-2022 12:53,15-07-2022 12:53,Implementation Issues
77570838,number of tokens exceeded maximum limit,"i am using the llama2 quantized model from huggingface and loading it using ctransformers from langchain. when i run the query, i got the below warning
number of tokens (512) exceeded maximum context length (512)
below is my code:
from langchain.llms import ctransformers
llm = ctransformers(model='models_k/llama-2-7b-chat.ggmlv3.q2_k.bin',
                      model_type='llama',
                      config={'max_new_tokens': 512,
                              'temperature': 0.01}
                      )

b_inst, e_inst = ""[inst]"", ""[/inst]""
b_sys, e_sys = ""<<sys>>\n"", ""\n<</sys>>\n\n""

default_system_prompt=""""""\
you are a helpful, respectful and honest assistant. always answer as helpfully as possible. 
please ensure that your responses are socially unbiased and positive in nature.

if a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. 
if you don't know the answer to a question, please don't share false information.""""""

instruction = db_schema + "" based on the database schema provided to you \n convert the following text from natural language to sql query: \n\n {text} \n only display the sql query""

system_prompt = b_sys + default_system_prompt + e_sys

template = b_inst + system_prompt + instruction + e_inst

prompt = prompttemplate(template=template, input_variables=[""text""])
llm_chain=llmchain(prompt=prompt, llm=llm)
print(llm_chain.run(""list the names and prices of electronic products that cost less than $500.""))

can anyone tell me why am i getting this error? do i have to change the settings?","['langchain', 'large-language-model', 'llama', 'ctransformers']",77576262,"you can fix this by the suggestion: context length.
code like here:
llm = ctransformers(model='models_k/llama-2-7b-chat.ggmlv3.q2_k.bin',
                      model_type='llama',
                      config={'max_new_tokens': 600,
                              'temperature': 0.01,
                              'context_length': 700}
                      )",https://stackoverflow.com/questions/77570838,langchain,29-11-2023 11:39,5109.0,4.0,1.0,True,30-11-2023 18:47,30-11-2023 18:47,Implementation Issues
73299450,notfittederror: the tf-idf vectorizer is not fitted,"i've trained a sentiment analysis classifier using tripadvisor's textual reviews datasets. it can predict the input textual reviews' rating based on sentiment. everything is ok with the training and testing.
however, when i loaded the classifier in a new .ipynb file and tried to use a review for prediction, i get
 notfittederror: the tf-idf vectorizer is not fitted** arises. 

this is the detailed error:
---------------------------------------------------------------------------
notfittederror                            traceback (most recent call last)
/var/folders/rn/vqtp35xn15zd9d5scq3rxsth0000gn/t/ipykernel_71297/777236349.py in <module>
----> 1 prediction(test_str,hotelmodel1000)

/var/folders/rn/vqtp35xn15zd9d5scq3rxsth0000gn/t/ipykernel_71297/1165328373.py in prediction(text, model)
      4     cw = clean_string(text)
      5     cw = tokenize(cw)
----> 6     cw = tfidf_vectorizer.transform([cw])
      7     result = model.predict(cw)
      8     print(""expected rating:"",int(result))

~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py in transform(self, raw_documents, copy)
   1869             tf-idf-weighted document-term matrix.
   1870         """"""
-> 1871         check_is_fitted(self, msg='the tf-idf vectorizer is not fitted')
   1872 
   1873         # fixme remove copy parameter support in 0.24

~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py in inner_f(*args, **kwargs)
     71                           futurewarning)
     72         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})
---> 73         return f(**kwargs)
     74     return inner_f
     75 

~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py in check_is_fitted(estimator, attributes, msg, all_or_any)
   1018 
   1019     if not attrs:
-> 1020         raise notfittederror(msg % {'name': type(estimator).__name__})
   1021 
   1022 

notfittederror: the tf-idf vectorizer is not fitted


here is my code to predict:
hotelmodel = pickle.load(open('./models/tripadvisorhotels_svm_model1000(2).pickle','rb'))
test_str = input('')
prediction(test_str,hotelmodel)

here is prediction() i called:
tfidf_vectorizer = tfidfvectorizer(max_features=5000,ngram_range=(2,2))

def prediction(text,model):
    cw = clean_string(text)
    cw = tokenize(cw)
    cw = tfidf_vectorizer.transform([cw])
    result = model.predict(cw)
    print(""expected rating:"",int(result)) 
    print(""\nthe confidence of the prediction is:"",model.predict_proba(cw)[0][int(result)-1])","['python', 'machine-learning', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']",73301712,"as mentioned in the comment,
you have correctly loaded the trained model from pickle file.
hotelmodel = pickle.load(open('./models/tripadvisorhotels_svm_model1000(2).pickle','rb'))

it can do the prediction because you saved the fitted version.
similarly, tfidf_vectorizer also need the fitted version.
you have to pickle the tfidf_vectorizer fitted version, then load from pickle to use it.
if you are using the svm based model, keep an eye with vectorizer length for fine tuning.",https://stackoverflow.com/questions/73299450,python,10-08-2022 00:32,2983.0,-1.0,1.0,True,10-08-2022 19:24,10-08-2022 19:24,Tool Setup/Errors
78690284,oobabooga-textgen-web-ui how to get authorization to view model list from port 5000 via the ooba&#39;s api-key in python,"i wish to extract and print out a list of llm models from the oobabooga-text-gen-web-ui in python.
some context first before i head over to my problem.
for those familiar with what ooba is its essentially a gradio web ui for large language models.
i downloaded and loaded a few llm models onto this web ui. the web ui uses  to display the web user interface on port 7860.
but if i enable the openai and api extensions, and also edit the cmd_flags.txt within the ooba folder into something like:
--listen --api --api-key ""enter-your-fake-api-key-here"" 

the extensions will mimic an open ai api key by connecting to ooba from a network via port 5000
here come the problem...
this is my code to view the model list from ooba :
import requests

url = ""

#model list
headers = {
    ""content-type"": ""application/json""
}

response = requests.get(f'{url}/internal/model/list',
                        headers=headers,
                        verify=false)
print(response.json())

the output should look something like this:
{'model_names': ['l3-8b-stheno-v3.2', 'l3-8b-stheno-v3.2_exl2_8h_8bpw', 'l3-8b-stheno-v3.2_q8_0.gguf', 'mixtao-7bx2-moe-v8.1_q8_0.gguf']}

but instead i got this:
{'detail': 'unauthorized'}

after some fiddling around i found that if i leave cmd_flags.txt blank the code works as intended and i get the model lists but i dont have access to an api key since its not enable on ooba.
if i do enable it with cmd_flags.txt and typing:
--listen --api --api-key ""enter-your-fake-api-key-here"" 

i'll have access to the openai api key but the code to extract the model list will return as :
{'detail': 'unauthorized'}
i need the api key enabled from ooba cuz i plan to use the openai.client feature for model interactions.
how do i keep the configuration that enables the fake open ai api key but also allows me to extract the model list?","['python', 'openai-api', 'large-language-model']",78699532,"1 day later... found the answer.
turns out if i want the api key to be enabled while also being able to view the model list from ooba via the api, i need to add an 'authorization': f'bearer {api_key}' in the header.
the code should look something like this:
import requests

api_key = ""enter-your-fake-api-key-here""
url = ""

#model list
headers = {
    ""content-type"": ""application/json"",
    ""authorization"": f""bearer {api_key}""
}

response = requests.get(f'{url}/internal/model/list',
                        headers=headers,
                        verify=false)
print(response.json())",https://stackoverflow.com/questions/78690284,python,01-07-2024 02:42,957.0,1.0,1.0,True,02-07-2024 23:18,02-07-2024 23:14,Implementation Issues
77178058,how to set safety parameters for text generation model in google cloud vertex ai?,"i am working on a research project where i need to summarize news articles using the google palm2 text generation model. i have encountered an issue with certain news articles in my dataset where i'm getting empty responses along with safety attributes that block the output. here is the code i'm using:
from vertexai.language_models import textgenerationmodel
parameters = {  # default values
    'max_output_tokens': 256,
    'temperature': 0.0,
    'top_p': 1.0,
    'top_k': 40,
}
prompt = ""...""
model = textgenerationmodel.from_pretrained('text-bison@001')
response = model.predict(
    prompt,
    **parameters,
)

the following is an example prediction:
prediction(predictions=[{'content': '', 'citationmetadata': none, 'safetyattributes': {'blocked': true, 'errors': [253.0]}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=none)

the issue seems to be related to safety parameters preventing the model from generating a summary for certain news articles. i've been trying to find documentation on how to configure these safety parameters using the python api, but i could not locate the relevant information.
could someone please provide guidance on how to set the safety parameters for the textgenerationmodel? any help or pointers to documentation would be greatly appreciated. thank you!","['python', 'machine-learning', 'nlp', 'google-cloud-vertex-ai', 'large-language-model']",77181428,"i'm not sure about vertex ai but you can set the safety_settings of the palm model (from google generative ai) by the following:
import google.generativeai as palm

completion = palm.generate_text(
    model=model,
    prompt=prompt,
    safety_settings=[
        {
            ""category"": safety_types.harmcategory.harm_category_derogatory,
            ""threshold"": safety_types.harmblockthreshold.block_none,
        },
        {
            ""category"": safety_types.harmcategory.harm_category_violence,
            ""threshold"": safety_types.harmblockthreshold.block_none,
        },
    ]
) 

you should checkout this guide to get complete details of the safety catalogue and how to set threshold for each category as there are multiple categories and different threshold levels.
note: to use the palm api from generative ai, you'd need to install it first via:
pip install -q google-generativeai

and then set an api key which you'll get from here:
import google.generativeai as palm
palm.configure(api_key='your_api_key')

and then to access the same text-bison-001 model:
models = [m for m in palm.list_models() if 'generatetext' in m.supported_generation_methods]
model = models[0].name # use this model on the first code snippet
print(model) # prints 'models/text-bison-001'",https://stackoverflow.com/questions/77178058,python,26-09-2023 07:58,2558.0,3.0,1.0,True,27-09-2023 09:03,27-09-2023 09:03,Implementation Issues
67949858,deal with dictionaries in python,"i am a begginer in nlp and i want to do a preprocessing in a dataset which has the following form
dataset = {'key1': [{ 'x1': '...', 'x2': '...', 'x3': '...' }], 'key2': [...], ....}

first of all i want to create a list which contains all the values of 'x2', such as
dataset_list = [[value of 'x2' of key1], [value of 'x2' of key2], ...]

could you please help you know can i do it?","['python', 'dictionary', 'nlp']",67949879,"each key of your dictionary has a list which contains a dict.

first, you access the list using dataset[key]

then, you access the dict using dataset[key][0], the first element of the list.

lastly, to access the value of x2 in this dictionary, you use dataset[key][0]['x2'].


putting this together into a list comprehension, you would get this:
dataset_list = [dataset[k][0]['x2'] for k in dataset]",https://stackoverflow.com/questions/67949858,python,12-06-2021 14:38,54.0,-1.0,1.0,True,12-06-2021 14:49,12-06-2021 14:49,Implementation Issues
66467748,spacy how do i make a matcher which is noun-noun without white space within it?,"i tried to make a matcher which could detect words like
'all-purpose'
i was trying to make a pattern like
pattern=[{'pos':'noun'}, {'orth':'-'},{'pos':'noun'}]
however, i realized that it only find the matches like
'all - purpose' with white space between tokens instead of 'all-purpose'.
how could i make a matcher like this?
it has to be a generalized pattern like noun-noun instead of
specific words like 'barak obama' as in the example in spacy documentation
best,","['nlp', 'tokenize', 'spacy']",66468894,"what exactly are you trying to match? using en_core_web_sm, ""all-purpose"" is three tokens and all has the adv pos tag for me. so that might be the issue with your match pattern. if you just want hyphenated words this might be a better match:
pattern = [{'is_alpha': true}, {'orth':'-'}, {'is_alpha': true}]

more generally, you are correct that your pattern will only match three tokens, though that doesn't require white space - it depends on how the tokenizer works. for example, that's has no spaces but is two tokens.
if you are finding hyphenated words that occur as one token and want to match them, you can use regular expressions in matcher rules. here's an example ofhow that would work from the docs:
pattern = [{""text"": {""regex"": ""deff?in[ia]tely""}}]

in your case it could just look like this:
pattern = [{""text"": {""regex"": ""-""}}]",https://stackoverflow.com/questions/66467748,nlp,04-03-2021 02:31,721.0,2.0,1.0,True,06-09-2022 09:31,06-09-2022 09:31,Implementation Issues
71680285,how to deploy a question answering bert model as a chat bot on ms teams,"i have a text2sql model (editsql:  which i have configured to take a sentence as input and return a sql query as output.
now, i want to deploy this program as a chat bot application in microsoft teams.
i understand there's microsoft bot framework that enables publishing a bot and the 3 options are described here.

however, i am not finding any of them suitable for my use case since i need to deploy a question-answering bot where the questions from users need to be sent to an external server like aws and the response from aws (could be an excel file) needs to be sent back to the user. multiple questions can be the part of a conversation, so the chat client should be able to mark start and end of a conversation.
my problem:

what are the basic steps of exposing a ml model via a server so that it can be queried in production.
what are the tools that will allow me to make a client on teams and a server for this model on aws?

please let me know if i should add more information on this.
thanks","['microsoft-teams', 'chatbot', 'bert-language-model']",71682612,"as you've seen, there are a bunch of tools/approaches to creating bots in the microsoft world, for teams or otherwise. underneath, these all use the bot framework, but you can develop directly (i.e. write code), or use a higher-level tool like bot framework composer - the choice is yours depending on your own internal skills. if you want to work with code directly, here are a bunch of bot samples, in multiple languages:  . for isntance, here is an example of integrating the microsoft qnamaker service into your bot: 
basically, if you go the development approach, your bot is just a web service. once it receives the message, it can call out to any other service behind the scenes. that means it can receive a message, call out to an aws service, receive the response, and send a reply to the user.
for multiple questions as part of a 'set' of chats, bot framework provides an idea called ""dialogs"" that should work for you.",https://stackoverflow.com/questions/71680285,microsoft-teams,30-03-2022 15:45,551.0,1.0,1.0,True,30-03-2022 18:51,30-03-2022 16:29,Implementation Issues
60833301,train huggingface&#39;s gpt2 from scratch : assert n_state % config.n_head == 0 error,"i am trying to use a gpt2 architecture for musical applications and consequently need to train it from scratch. after a bit of googling i found that the issue #1714 from huggingface's github already had ""solved"" the question. when i try the to run the propose solution :
from transformers import gpt2config, gpt2model

numlayer = 4
numhead = 4
sizereduction = 10 #the factor by which we reduce the size of the velocity argument.
velsize = int(np.floor(127/sizereduction)) + 1 
seqlen=40 #size of data sequences.
embedsize = 5 

config = gpt2config(vocab_size = velsize, n_positions = seqlen, n_embd = embedsize, n_layer = numlayer, n_ctx = seqlen, n_head = numhead)  
model = gpt2model(config)

i get the following error : 
traceback (most recent call last):

  file ""<ipython-input-7-b043a7a2425f>"", line 1, in <module>
    runfile('c:/users/cnelias/desktop/phd/swing project/code/script/gpt2.py', wdir='c:/users/cnelias/desktop/phd/swing project/code/script')

  file ""c:\users\cnelias\anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 786, in runfile
    execfile(filename, namespace)

  file ""c:\users\cnelias\anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  file ""c:/users/cnelias/desktop/phd/swing project/code/script/gpt2.py"", line 191, in <module>
    model = gpt2model(config)

  file ""c:\users\cnelias\anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 355, in __init__
    self.h = nn.modulelist([block(config.n_ctx, config, scale=true) for _ in range(config.n_layer)])

  file ""c:\users\cnelias\anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 355, in <listcomp>
    self.h = nn.modulelist([block(config.n_ctx, config, scale=true) for _ in range(config.n_layer)])

  file ""c:\users\cnelias\anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 223, in __init__
    self.attn = attention(nx, n_ctx, config, scale)

  file ""c:\users\cnelias\anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 109, in __init__
    assert n_state % config.n_head == 0

what does it mean and how can i solve it ?
also more generally, is there a documentation on how to do a forward call with the gpt2 ? can i define my own train() function or do i have to use the model's build-in function ? am i forced to use a dataset to do the training or can i feed it individual tensors ? 
i looked for it but couldn't find answer to these on the doc, but maybe i missed something.
ps : i already read the blogpost fron huggingface.co, but it omits too much informations and details to be usefull for my application.","['python', 'nlp', 'huggingface-transformers', 'transformer-model', 'gpt-2']",60833512,"i think the error message is pretty clear:

assert n_state % config.n_head == 0

tracing it back through the code, we can see

n_state = nx  # in attention: n_state=768

which indicates that n_state represents the embedding dimension (which is generally 768 by default in bert-like models). when we then look at the gpt-2 documentation, it seems the parameter specifying this is n_embd, which you are setting to 5. as the error indicates, the embedding dimension has to be evenly divisible through the number of attention heads, which were specified as 4. so, choosing a different embedding dimension as a multiple of 4 should solve the problem. of course, you can also change the number of heads to begin with, but it seems that odd embedding dimensions are not supported.",https://stackoverflow.com/questions/60833301,python,24-03-2020 14:42,950.0,0.0,1.0,True,29-11-2020 12:10,29-11-2020 12:10,Implementation Issues
78788751,what is the best practice to calculate global frequency of list of elements with exact orders in python within multiple pandas dataframe?,"let's say i have the following datafarme df1 corresponding to user1:
+-------------------+-------+--------+-------+-------+----------+----------------+
|      models       |  mae  |  mse   | rmse  | mape  | rï¿½ï¿½ score |  runtime [ms]  |
+-------------------+-------+--------+-------+-------+----------+----------------+
| linearregression  | 4.906 | 27.784 | 5.271 | 0.405 |  -6.917  | 0:00:43.387145 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|   random forest   | 2.739 | 10.239 |  3.2  | 0.231 |  -1.917  | 0:28:11.761681 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|      xgboost      | 2.826 | 10.898 | 3.301 | 0.234 |  -2.105  | 0:03:58.883474 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|   mlpregressor    | 5.234 | 30.924 | 5.561 | 0.43  |  -7.812  | 0:01:44.252276 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|        svr        | 5.061 | 29.301 | 5.413 | 0.417 |  -7.349  | 0:04:52.754769 |
+---------------+-------+--------+-------+-------+----------+----------------+
| catboostregressor | 2.454 | 8.823  | 2.97  | 0.201 |  -1.514  | 0:19:36.925169 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|   lgbmregressor   | 2.76  | 10.204 | 3.194 | 0.231 |  -1.907  | 0:04:51.223103 |
+-------------------+-------+--------+-------+-------+----------+----------------+


+-------------------+----------------------------------------------------------------------------------------------------------+
|      rank         |                                                          mae                                             | 
+-------------------+----------------------------------------------------------------------------------------------------------+
| top models(sorted)| [""catboostregressor"",""randomforest"",""lgbmregressor"", ""xgboost"",""linearregression"",""svr"",""mlpregressor""]  |  
+-------------------+----------------------------------------------------------------------------------------------------------+

i have following datafarme df2 corresponding to user2:
+-------------------+-------+--------+-------+-------+----------+----------------+
|      models       |  mae  |  mse   | rmse  | mape  | rï¿½ï¿½ score |  runtime [ms]  |
+-------------------+-------+--------+-------+-------+----------+----------------+
| linearregression  | 4.575 | 24.809 | 4.981 | 0.377 |  -6.079  | 0:00:45.055854 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|   random forest   | 2.345 | 8.065  | 2.84  | 0.199 |  -1.301  | 0:10:55.468473 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|      xgboost      | 2.129 | 7.217  | 2.686 | 0.179 |  -1.059  | 0:01:01.575033 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|   mlpregressor    | 4.414 | 23.477 | 4.845 | 0.363 |  -5.699  | 0:00:31.231719 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|        svr        | 4.353 | 22.826 | 4.778 | 0.357 |  -5.513  | 0:02:12.258870 |
+---------------+-------+--------+-------+-------+----------+----------------+
| catboostregressor | 2.281 | 7.671  | 2.77  | 0.189 |  -1.189  | 0:08:16.526615 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|   lgbmregressor   | 2.511 |  9.18  | 3.03  | 0.212 |  -1.619  | 0:15:25.084937 |
+-------------------+-------+--------+-------+-------+----------+----------------+


+-------------------+----------------------------------------------------------------------------------------------------------+
|      rank         |                                                          mae                                             | 
+-------------------+----------------------------------------------------------------------------------------------------------+
| top models(sorted)| [""xgboost"",""catboostregressor"",""randomforest"",""lgbmregressor"",""linearregression"",""svr"",""mlpregressor""]  |  
+-------------------+----------------------------------------------------------------------------------------------------------+

let's say i have more datafarmes df1000 corresponding to user1000.
problem statement: i want to count how often each ranking order occurs across all users (for a given metric). (and then, sort the ranking orders by their counts, and, additionally, compute the percentage of how often each particular ranking order occurs (based on the counts).)
i want to rank models result (sorted over a specific column (e.g. mae ) iteratively and return the frequency of top models over all dfs (df1 till df1000). so this is not something i can easily reach using the:
df[""category""].value_counts()

we are interested in computing absolute\relative frequencies in final ranked table in expected output.
so definitely i need to transform and add the list of sorted models' names that'd be a list of strings.
possible transformation or aggregation stages from my understanding:

take each df and create the list of sorted model names based desired column or metric:
['model2','model7', 'model6', 'model5', 'model4', 'model3', 'model1' ]
including the name of users in the final transformed dataframe could also be useful (however i did not mention it in the following table in the expected output.)
computing absolute\relative frequencies and return as counts and freq(%) in final table

expected output:
+-------------------+----------------------------------------------------------------------------------------------------------+--------+---------+
|      rank         |                                                    mae                                                  |counts  |freq(%)  |
+-------------------+----------------------------------------------------------------------------------------------------------+--------+---------+
| top models(sorted)| [""catboostregressor"",""randomforest"",""lgbmregressor"", ""xgboost"",""linearregression"",""svr"",""mlpregressor""]   | 70     |   65%   |
| top models(sorted)| [""xgboost"",""catboostregressor"",""randomforest"",""lgbmregressor"",""linearregression"",""svr"",""mlpregressor""]   | 20     |   12%   |
| top models(sorted)|                                             ....                                                  | ....     |   ....  |
....
+-------------------+----------------------------------------------------------------------------------------------------------+--------+---------+

i also was thinking maybe i can use natural language processing (nlp) methods called tf-idf to handle this problem using:
# import required module
from sklearn.feature_extraction.text import tfidfvectorizer


potentially related posts i have checked:

how can i compute a histogram (frequency table) for a single series?
count the frequency that a value occurs in a dataframe column
efficient way to get frequency of elements in a pandas column of lists
calculate frequency of item in list
get the frequency of individual items in a list of each row of a column in a dataframe
count the frequency of elements in list of lists in python
what's the best alternative to using lists as elements in a pandas dataframe?
pandas - create dataframe with counts and frequency of elements
python: calculate pmf for list in pandas dataframe
frequency plot of a pandas dataframe
python & pandas - how to calculate frequency under conditions in columns in dataframe?","['python', 'pandas', 'dataframe', 'frequency', 'tf-idf']",78789737,"does this work?
import pandas as pd
from collections import counter

# i've left out the other metrics because they are irrelevant to your question, but you can add them
data1 = {
    'models': ['linearregression', 'random forest', 'xgboost', 'mlpregressor', 'svr', 'catboostregressor', 'lgbmregressor'],
    'mae': [4.906, 2.739, 2.826, 5.234, 5.061, 2.454, 2.76]
}
data2 = {
    'models': ['linearregression', 'random forest', 'xgboost', 'mlpregressor', 'svr', 'catboostregressor', 'lgbmregressor'],
    'mae': [4.575, 2.345, 2.129, 4.414, 4.353, 2.281, 2.511]
}
data3 = {
    'models': ['linearregression', 'random forest', 'xgboost', 'mlpregressor', 'svr', 'catboostregressor', 'lgbmregressor'],
    'mae': [4.575, 2.345, 2.129, 4.414, 4.353, 2.281, 2.511]
}


df1 = pd.dataframe(data1)
df2 = pd.dataframe(data2)
df3 = pd.dataframe(data3)

# add your 1000 data frames here if you want
dataframes = [df1, df2, df3]

concatenated_models_list = []

for df in dataframes:
    df_sorted = df.sort_values(by='mae', ascending=false)
    # concatenate the sorted-by-mae model names into one string, so that
    # you can group by it as a key later
    concatenated_models = ','.join(df_sorted['models'].tolist())
    concatenated_models_list.append(concatenated_models)

# union everything into a single dataframe
union_df = pd.dataframe(concatenated_models_list, columns=['top models(sorted)'])

grouped_df = union_df['top models(sorted)'].value_counts().reset_index()
grouped_df.columns = ['top models(sorted)', 'count']

grouped_df['freq(%)'] = (grouped_df['count'] / len(dataframes)) * 100

grouped_df


output
top models(sorted)  count   freq(%)
linearregression,mlpregressor,svr,lgbmregresso...   2   66.666667
mlpregressor,svr,linearregression,xgboost,lgbm...   1   33.333333",https://stackoverflow.com/questions/78788751,python,24-07-2024 13:47,159.0,1.0,2.0,True,05-08-2024 10:00,05-08-2024 01:01,Conceptual Questions
79111733,"how to derive attributes/labels from short plain text descriptions? (ner, llm, ?)","how to derive attributes/labels from short plain text descriptions? (ner, llm, ?)
i have short product descriptions that iï¿½ï¿½ï¿½d like to transform into structured attributes.
example:
input:
ï¿½ï¿½ï¿½la lecciaia cabernet sauvignon 2017 ï¿½ï¿½ï¿½ red ï¿½ï¿½ï¿½ 750mlï¿½ï¿½ï¿½

output:
year = 2017

color = red

weight = 750

weight unit = ml

if everything was in this format it would be trivial to write a regular expression and be done with it, but there are many different formats and nuances. it is increasingly cumbersome to hard-code logic for each format. trying to create a generic solution i immediately run into issues with a ï¿½ï¿½ï¿½basicï¿½ï¿½ï¿½ approach:

there are several different data providers, and each has its own format. for the example above, another provider might use ï¿½ï¿½ï¿½(red) 2017 la lecciaia cabernet sauvignon 750 mlï¿½ï¿½ï¿½. even for a given provider, there may be multiple formats atrictly followed.

there are many ways of expressing particular components. as an example, weight might be expressed as any one of these: ï¿½ï¿½ï¿½1.5lï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½1 1/2 litersï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½1500mlï¿½ï¿½ï¿½, etc.

parts of the description may be confused for target components. there may be a white wine from a brand called ï¿½ï¿½ï¿½red head vineyardï¿½ï¿½ï¿½. a weight of ï¿½ï¿½ï¿½2000 mlï¿½ï¿½ï¿½ may be confused for a year, etc. iï¿½ï¿½ï¿½m only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues.

iï¿½ï¿½ï¿½d consider this more of a ï¿½ï¿½ï¿½nice to haveï¿½ï¿½ï¿½ but would be useful to be able to parse out even more detail like the algo would be smart enough to know that ï¿½ï¿½ï¿½la lecciaiaï¿½ï¿½ï¿½ is the brand and ï¿½ï¿½ï¿½cabernet sauvignonï¿½ï¿½ï¿½ is the grape variety. assuming this would take more up front work and harder to get right but if thereï¿½ï¿½ï¿½s a straigpose function that can accept a description from any format. i have little experience with nlp/artificial intelligence but suspect there are useful tools/algos i can leverage. i have 1,000+ example records that i could potentially use to train a model. something that can run locally would be preferred but not absolutely necessary.
iï¿½ï¿½ï¿½m not looking for a specific implementation but for guidance from anyone whoï¿½ï¿½ï¿½s worked on a similar problem. open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies.
appreciate any insight into approaches or suggested learning resources.

i've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical","['nlp', 'artificial-intelligence', 'large-language-model', 'named-entity-recognition']",79113907,"llm would work nicely for this.  i'v done similar tasks before and it worked nicely with minimal training.  just keep in mind that any of the statistical methods nlp / llm / ner will never be 100% accurate,  but for practical purposes i find llms to be more accurate then a custom soup of regular expressions.
for you task i would use a framework like langchain,  and the following prompt (note you might need to work on your prompt a bit this just an example).  when run with a model it will create an xml output which would be trivial to parse.  you can modify the prompt to create different type of outputs. but, personally i find xml working very well for me.
you are an ai language model designed to parse wine bottle descriptions into structured data. you will be given a wine bottle description, and your task is to extract the following components:

- **year**: the vintage year of the wine.
- **color**: the color of the wine (e.g., red, white, rosï¿½ï¿½).
- **weight**: the volume of the wbottle expressed as a number (e.g., 750, 1500).
- **weight unit**: the unit of measurement for the weight (e.g., ml, ml, l, liters).
- **brand**: the brand or producer of the wine.
- **grape variety**: the variety of grape used (e.g., cabernet sauvignon, merlot).

**instructions:**

- wine descriptions may come in various formats and may include additional or confusing information. carefully analyze the description to accurately extract the components.
- be cautious of potential ambiguities. for example:
  - a brand name may include words like ""red"" or ""white"" (e.g., ""red head vineyard"") which should not be confused with the wine color.
  - large numbers may represent weight (e.g., ""1500 ml"") rather than a year.
- **do not assume information not present in the description.** if a component is missing, you may leave the corresponding tag empty or omit it.

**output format:**

provide the extracted information in xml format, using the following structure:

<wine>
<year>{{year}}</year>
<color>{{color}}</color>
<weight>{{weight}}</weight>
<weightunit>{{weightunit}}</weightunit>
<brand>{{brand}}</brand>
<grapevariety>{{grapevariety}}</grapevariety>
</wine>

**examples:**

  1. **input:**

 `la lecciaia cabernet sauvignon 2017 ï¿½ï¿½ï¿½ red ï¿½ï¿½ï¿½ 750ml`

 **output:**



```xml
   <wine>
     <year>2017</year>
     <color>red</color>
     <weight>750</weight>
     <weightunit>ml</weightunit>
     <brand>la lecciaia</brand>
     <grapevariety>cabernet sauvignon</grapevariety>
   </wine>
   ```

   
   `red head vineyard chardonnay 2020 1.5l`

   **output:**

   <wine>
     <year>2020</year>
     <color></color>
     <weight>1.5</weight>
     <weightunit>l</weightunit>
     <brand>red head vineyard</brand>
     <grapevariety>chardonnay</gra;
   </wine>

 

    **task:**
    
    given the following wine description, extract the components and provide the output in xml format as specified.
    
    {win_description}

keep in mind that llms are not cheap to run.  but for this tasks given ambiguousness of the domain it is most likely the best choice.  for this particular task it would be 1/1000 of a penny per label using openai service.  you might find a cheaper model / provider.  however when working with llm it is very important to ensure accuracy first,  then optimize for costs.
the whole thing will probably take 1-2 hours to build for the intermediate llm developer.  if you are learning it may vary.  but this is a perfect project to learn about llms",https://stackoverflow.com/questions/79111733,nlp,21-10-2024 20:54,186.0,0.0,1.0,True,22-10-2024 15:18,22-10-2024 11:29,Implementation Issues
65246703,"how does max_length, padding and truncation arguments work in huggingface&#39; berttokenizerfast.from_pretrained(&#39;bert-base-uncased&#39;)?","i am working with text classification problem where i want to use the bert model as the base followed by dense layers. i want to know how does the 3 arguments work? for example, if i have 3 sentences as:
'my name is slim shade and i am an aspiring ai engineer',
'i am an aspiring ai engineer',
'my name is slim'

so what will these 3 arguments do? what i think is as follows:

max_length=5 will keep all the sentences as of length 5 strictly
padding=max_length will add a padding of 1 to the third sentence
truncate=true will truncate the first and second sentence so that their length will be strictly 5.

please correct me if i am wrong.
below is my code which i have used.
! pip install transformers==3.5.1

from transformers import berttokenizerfast

tokenizer = berttokenizerfast.from_pretrained('bert-base-uncased')

tokens = tokenizer.batch_encode_plus(text,max_length=5,padding='max_length', truncation=true)
  
text_seq = torch.tensor(tokens['input_ids'])
text_mask = torch.tensor(tokens['attention_mask'])","['python', 'deep-learning', 'pytorch', 'bert-language-model', 'huggingface-tokenizers']",65255500,"what you have assumed is almost correct, however, there are few differences.
max_length=5, the max_length specifies the length of the tokenized text. by default, bert performs word-piece tokenization. for example the word ""playing"" can be split into ""play"" and ""##ing"" (this may not be very precise, but just to help you understand about word-piece tokenization), followed by adding [cls] token at the beginning of the sentence, and [sep] token at the end of sentence. thus, it first tokenizes the sentence, truncates it to max_length-2 (if truncation=true), then prepend [cls] at the beginning and [sep] token at the end.(so a total length of max_length)
padding='max_length', in this example it is not very evident that the 3rd example will be padded, as the length exceeds 5 after appending [cls] and [sep] tokens. however, if you have a max_length of 10. the tokenized text corresponds to [101, 2026, 2171, 2003, 11754, 102, 0, 0, 0, 0], where 101 is id of [cls] and 102 is id of [sep] tokens. thus, padded by zeros to make all the text to the length of max_length
likewise, truncate=true will ensure that the max_length is strictly adhered, i.e, longer sentences are truncated to max_length only if truncate=true",https://stackoverflow.com/questions/65246703,python,11-12-2020 06:26,60899.0,27.0,1.0,True,24-11-2022 22:00,24-11-2022 22:00,Implementation Issues
73205546,"spacy, how not to remove &quot;not&quot; when cleaning the text with space","i use this spacy code to later apply it on my text, but i need the negative words to stay in the text like ""not"".
nlp = spacy.load(""en_core_web_sm"") 

def my_tokenizer(sentence): 
    return [token.lemma_ for token in tqdm(nlp(sentence.lower()), leave = false) if token.is_stop == false and token.is_alpha == true and  token.lemma_ ] 

whit this when i apply i get this as a result :
[hello, earphone, work]

however the original sentence was
hello,my earphones are still not working.

so, i would like to see the following sentence: [earphone, still, not, work]
thank you","['python', 'spacy', 'stop-words']",73208162,"""not"" is actually a stop word and in your code if a token is removed if it's a stopword. you can see this either by looking at the list of spacy stopwords 
""not"" in spacy.lang.en.stop_words.stop_words

or by looping over the tokens of your doc object
for tok in nlp(text.lower()):
  print(tok.text, tok.is_stop, tok.lemma_)

#hello false hello
#, false ,
#my true my
#earphones false earphone
#are true be
#still true still
#not true not
#working false work
#. false .

solution
to solve this, you should remove the target words such as ""not"" from the list of stop_words. you can do it this way: 
# spacy.lang.en.stop_words.stop_words.remove(""not"")
# or for multiple words use this
to_del_elements = {""not"", ""no""}
nlp.defaults.stop_words = nlp.defaults.stop_words - to_del_elements

then you can rerun your code and you'll get your expected results:
import spacy
#spacy.lang.en.stop_words.stop_words.remove(""not"")
to_del_elements = {""not"", ""no""}
nlp.defaults.stop_words = nlp.defaults.stop_words - to_del_elements
nlp = spacy.load(""en_core_web_sm"") 
def my_tokenizer(sentence): 
    return [token.lemma_ for token in tqdm(nlp(sentence.lower()), leave = false) if token.is_stop == false and token.is_alpha == true and  token.lemma_ ] 

sentence = ""hello,my earphones are still not working. no way they will work""
results = my_tokenizer(sentence)
print(results)

#['hello', 'earphone', 'not', 'work', 'no', 'way', 'work']",https://stackoverflow.com/questions/73205546,python,02-08-2022 09:57,645.0,4.0,1.0,True,03-08-2022 21:26,02-08-2022 09:59,Preprocessing Tasks
73078231,how to get all stop words from spacy and don&#39;t get any errors? typeerror: argument of type &#39;module&#39; is not iterable,"how to get all stop words from spacy.lang.en and don't get any errors?
from spacy.lang.en import stop_words as stop_words


def tokenize(sentence):
    sentence = nlp(sentence)
    # lemmatizing
    sentence = [ word.lemma_.lower().strip() if word.lemma_ != ""-pron-"" else word.lower_ for word in sentence ]
    # removing stop words
    sentence = [ word for word in sentence if word not in stop_words and word not in punctuations ]        
    return sentence

tokenize(""hallo ik ben leyla en "") and then i get 

then i got the following error and this is the error that i got
typeerror: argument of type 'module' is not iterable","['python', 'nlp', 'data-science', 'spacy']",73078527,"make sure stop_words and punctuations be a list or set and for getting a set of all stop_words from from spacy.lang.en import stop_words you can use stop_words.stop_words or as an alternative solution you can use nlp.defaults.stop_words.
import spacy
from string import punctuation
from spacy.lang.en import stop_words


nlp = spacy.load('en_core_web_sm')

stop_words = stop_words.stop_words
# print(stop_words)
# as an alternative solution
# stop_words = nlp.defaults.stop_words


punctuations = list(punctuation)
print(punctuations)
# ['!', '""', '#', '$', '%', '&', ""'"", '(', ')', '*', '+', '', '', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\', ']', '^', '_', '`', '{', '|', '}', '~']


def tokenize(sentence):
    sentence = nlp(sentence)
    # lemmatizing
    sentence = [ word.lemma_.lower().strip() if word.lemma_ != ""-pron-"" else word.lower_ for word in sentence ]
    # removing stop words
    sentence = [ word for word in sentence if word not in stop_words and word not in punctuations ]        
    return sentence


>>> tokenize(""hallo ik ben leyla en "")
['hallo', 'ik', 'ben', 'leyla', 'en']",https://stackoverflow.com/questions/73078231,python,22-07-2022 09:37,8343.0,4.0,1.0,True,12-01-2023 16:04,22-07-2022 10:28,Implementation Issues
76490589,valueerror when using model.fit even with the vectors being aligned,"i am attempting to build a naive bayes model for text classification.
here is a sample of the data i'm working with:
df_some_observations = filtered_training.sample(frac=0.0001)
df_some_observations.to_dict()

the output looks like this:
{'intitulï¿½ï¿½ (ce champ doit respecter la nomenclature suivante : code action ï¿½ï¿½ï¿½ libellï¿½ï¿½)_x': {40219: 'aegua00268 format oper scad htbhta fonction avance',
  16820: 'aeedf50490 sort conflit facon construct',
  24771: '4022mps192 prepar a lhabilit electr boho indic v personnel non elec',
  34482: '3095mceg73 affirmezvous relat professionnel bas ref 7114'},
 'nï¿½ï¿½ud parent au niveau n y compris moi-mï¿½ï¿½me.1': {40219: 'distribu electricit rel reseau electricit ecr exploit conduit reseau electricit',
  16820: 'ct competent transvers rhu ressourc humain for pilotag gestion format',
  24771: 'ss sant securit prevent prf prevent risqu professionnel hcp habilit certif perm prevent risqu meti',
  34482: 'nan'},
 'thï¿½ï¿½me de formation (chemin complet)': {40219: 'distribu electricit rel reseau electricit ecr exploit conduit reseau electricit',
  16820: 'ct competent transvers rhu ressourc humain for pilotag gestion format',
  24771: 'ss sant securit prevent prf prevent risqu professionnel hvent risqu meti',
  34482: 'in ingenier esp equip sous pression'},
 'description du champ supplï¿½ï¿½mentaire : objectifs de la formation': {40219: 'nan',
  16820: 'nan',
  24771: 'prepar a lhabilit electr boho indic v autoris special lissu cet format stagiair doit connaitr risqu electr savoir sen proteg doit etre capabl deffectu oper simpl dexploit suiv certain methodolog',
  34482: 'nan'},
 'objectifs': {40219: 'nan', 16820: 'nan', 24771: 'nan', 34482: 'nan'},
 'programme de formation': {40219: 'nan',
  16820: 'nan',
  24771: 'notion elementair delectricit sensibilis risqu electr prevent risqu electr publiqu utec 18 510 definit oper lenviron intervent tbt b appareillag electr bt materiel protect individuel collect manoeuvr mesurag essais verif outillag electr portat a main mis situat coffret didact',
  34482: 'nan'},
 'populations concernï¿½ï¿½es': {40219: 'nan',
  16820: 'nan',
  24771: 'personnel electricien effectu oper dordr electr',
  34482: 'nan'},
 'prï¿½ï¿½requis': {40219: 'nan',
  16820: 'nan',
  2nnel non electricien effectu oper simpl remplac fusibl rearm disjoncteur rel thermiqu',
  34482: 'nan'},
 ""description du champ supplï¿½ï¿½mentaire : commanditaire de l'action"": {40219: 'nan',
  16820: 'nan',
  24771: 'nan',
  34482: 'nan'},
 ""organisme dispensant l'action"": {40219: 'local sei',
  16820: 'intern edf',
  24771: 'intern edf',
  34482: 'intern edf'},
 'durï¿½ï¿½e thï¿½ï¿½orique (h)': {40219: 14.0, 24771: 11.0, 34482: 14.0},
 'coï¿½ï¿½t de la catï¿½ï¿½gorie coï¿½ï¿½t pï¿½ï¿½dagogique': {40219: 0.0,
  16820: 0.0,
  24771: 0.0,
  34482: 0.0},
 'coï¿½ï¿½t de la catï¿½ï¿½gorie coï¿½ï¿½t logistique': {40219: 0.0,
  16820: 0.0,
  24771: 0.0,
  34482: 0.0},

i started by splitting the data after removing some unnecessary columns:
(my target variable i""lang-py prettyprint-override"">df_training = filtered_training.sample(frac=0.8, random_state=42) 
df_test = filtered_training.drop(df_training.index)
x_train = df_training.iloc[:,:14]
y_train = df_training.iloc[:,15]
x_test = df_test.iloc[:,:14]
y_test = df_test.iloc[:,15]

when building the model with:
model = make_pipeline(tfidfvectorizer(), multinomialnb())
model.fit(x_train, y_train)
predicted_categories = model.predict(x_test)

i receive the following error when executing model.fit(x_train, y_train):
valueerror: found input variables with inconsistent numbers of samples: [14, 35478]

additional information that may be helpful:
np.shape(x_train) #(35478, 14)
np.shape(y_train) #(35478,)
np.shape(x_test) #(8870, 14)
np.shape(y_test) #(8870,)","['python', 'machine-learning', 'nlp', 'valueerror', 'naivebayes']",76514426,"i think that the main problem that tfidfvectorizer is able to work with one-dimensional text data only (as i see it from here). that's why when it tries to convert several columns with text data it tries to do it for column names for some reason.
in your case i see 2 ways how to solve this problem:

if you want to apply tfidfvectorizer for each column individually, it would be better to do it like this for example:

column_transformer = columntransformer([(x, tfidfvectorizer(), x) for x in x_train.columns]) # make sure that all columns contains text data
model = make_pipeline(column_transformer, multinomialnb())
model.fit(x_train, y_train)
predicted_categories = model.predict(x_test)


but if you want to apply one vocabulary for your columns, then i would recomment to do it like this:

nex_x_train = x_train.iloc[:,0]
for x in x_train.columns[1:]:
    nex_x_train = nex_x_train + ' ' + x_train[x]

nex_x_test = x_test.iloc[:,0]
for x in x_test.columns[1:]:
    nex_x_test = nex_x_test + ' ' + x_test[x]
    
model = make_pipeline(tfidfvectorizer(), multinomialnb())
model.fit(nex_x_train, y_train)
predicted_categories = model.predict(nex_x_test)",https://stackoverflow.com/questions/76490589,python,16-06-2023 13:17,163.0,4.0,1.0,True,20-06-2023 11:54,19-06-2023 11:29,Implementation Issues
66513144,pseudo labelling on text classification python,"i'm not good at machine learning. can someone tell me how to doing text classification with pseudo labeling in python? i never know the right implementation, i have searched everywhere in internet, but i give up as found anything :'( i just found the implementation for numeric datasets, but i found no implementation for text classification (vectorized text).. so i wrote this syntax, but i don't know whether my code is correct or not. am i doing wrong? please help me guys, i really need your help.. :'(
this is my datasets if you wanna try. i want to classify 'label' from 'content'
my steps are:

split data 0.75 unlabeled, 0.25 labeled
from 0.25 labeld i split: 0.75 train labeled, and 0.25 test labeled
make vectorizer for train, test and unlabeled datasets
build first model from train labeled, then labelling the unlabeled datasets
concatting train labeled data with prediction of unlabeled that have >0.99 (pseudolabeled), and make the second model
remove pseudolabeled from unabeled datasets
predict the remaining unlabeled from second model, then iterate step 3 until the probability of predicted pseudolabeled <0.99.

this is my code:
performing pseudo labelling on text classification
from sklearn.naive_bayes import multinomialnb

# initiate iteration counter
iterations = 0

# containers to hold f1_scores and # of pseudo-labels
train_f1s = []
test_f1s = []
pseudo_labels = []

# assign value to initiate while loop
high_prob = [1] 

# loop will run until there are no more high-probability pseudo-labels
while len(high_prob) > 0:
    
    # set the vector transformer (from data train)
    columntransformer = columntransformer([
    ('tfidf',tfidfvectorizer(stop_words=none, max_features=100000),
     'content')
    ],remainder='drop')

    def transforms(series):
        before_vect = pd.dataframe({'content':series})
        vector_transformer = columntransformer.fit(pd.dataframe({'content':x_train}))
        return vector_transformer.transform(before_vect)

    x_train_df = transforms(x_train);
    x_test_df = transforms(x_test);
    x_unlabeled_df = transforms(x_unlabeled)
    
    # fit classifier and make train/test predictions
    nb = multinomialnb()
    nb.fit(x_train_df, y_train)
    y_hat_train = nb.predict(x_train_df)
    y_hat_test = nb.predict(x_test_df)

    # calculate and print iteration # and f1 scores, and store f1 scores
    train_f1 = f1_score(y_train, y_hat_train)
    test_f1 = f1_score(y_test, y_hat_test)
    print(f""iteration {iterations}"")
    print(f""train f1: {train_f1}"")
    print(f""test f1: {test_f1}"")
    train_f1s.append(train_f1)
    test_f1s.append(test_f1)
   
    # generate predictions and probabilities for unlabeled data
    print(f""now predicting labels for unlabeled data..."")

    pred_probs = nb.predict_proba(x_unlabeled_df)
    preds = nb.predict(x_unlabeled_df)
    prob_0 = pred_probs[:,0]
    prob_1 = pred_probs[:,1]

    # store predictions and probabilities in dataframe
    df_pred_prob = pd.dataframe([])
    df_pred_prob['preds'] = preds
    df_pred_prob['prob_0'] = prob_0
    df_pred_prob['prob_1'] = prob_1
    df_pred_prob.index = x_unlabeled.index
    
    # separate predictions with > 99% probability
    high_prob = pd.concat([df_pred_prob.loc[df_pred_prob['prob_0'] > 0.99],
                           df_pred_prob.loc[df_pred_prob['prob_1'] > 0.99]],
                          axis=0)
    
    print(f""{len(high_prob)} high-probability predictions added to training data."")
    
    pseudo_labels.append(len(high_prob))

    # add pseudo-labeled data to training data
    x_train = pd.concat([x_train, x_unlabeled.loc[high_prob.index]], axis=0)
    y_train = pd.concat([y_train, high_prob.preds])      
    
    # drop pseudo-labeled instances from unlabeled data
    x_unlabeled = x_unlabeled.drop(index=high_prob.index)
    
    print(f""{len(x_unlabeled)} unlabeled instances remaining.\n"")
    
    # update iteration counter
    iterations += 1

i think i'm doing something wrong.. because when i see the f1 scores it is decreasing. please help me guys :'( i'm stressed.
f1 scores image
=================edit=================
so i've search on journal, then i think that i've got misunderstanding about the concept of data splitting in pseudo-labelling.
i initially thought that, the steps starts from splitting the data into labeled and unlabeled data, then from that labeled data, it was splitted into train and test.
but after surfing and searching, i found in this journal that my steps is incorrect. this journal says that the steps pseudo-labeling should start from splitting the data into train and test sets at first, and then from that train sets, data is splited to labeled and unlabeled datasets.
according to that journal, it reach the best result when splitting data into 90% of train sets and 10% of test sets. then, from that 90% train set, it is splitted into 20% labeled data and 80% unlabeled data sets. this journal trying evidence range from 0.7 till 0.9 as boundary to drop the pseudo labeling, and on that proportion of splitting, the best evidence threshold value is 0.74. so i fix my steps with that new proportion and 0.74 threshold, and i finally got the f1 scores is increasing. here are my steps:

split data 0.9 train, 0.1 test sets (i labeled the test sets, so i can measure the f1 scores)
from 0.9 train, i split: 0.2 labeled, and 0.8 unlabeled data
making vectorizer for x value of labeled train, test and unlabeled training datasets
build first model from labeled train, then labeling the unlabeled training datasets. then measure the f-1 scores according to the test sets (that already labeled).
concatting train labeled data with prediction of unlabeled that have probability > 0.74 (threshold based on journal). we call this new data as pseudo-labelled, likened to the actual label), and make the second model from new train data sets.
remove selected pseudo-labelled from unlabeled datasets
use the second model to predict the remaining of unlabeled data, then iterate step 3 until there are no probability of predicted pseudo-labelled>0.74
so the last model is the final.

my syntax is still the same, i just changing the split proportion and i finally got my f1 scores increasing through 4 iterations: my new f1 scores.
am i doing something right? thank you for all of your attention guys.. so much thank you..","['python', 'text-classification', 'semisupervised-learning']",66532861,"i'm not good at machine learning.

overall i would say that you are quite good at machine learning: semi-supervised learning is an advanced type of problem and i think your solution is quite good. at least the general principle seems correct, but it's difficult to say for sure (i don't have time to analyze the code in detail sorry). a few comments:

one thing which might be improvable is the 0.74 threshold: this value certainly depends on the data, so you could do your own experiment by trying different threshold values and selecting the one which works best with your data.
preferably it would be better to keep a final test set aside and use a separate validation set during the iterations. this would avoid the risk of data leakage.
i'm not sure about the stop condition for the loop. it might be ok but it might be worth trying other options:

simply iterate a fixed number of times (for instance 10 times).
the stop condition could be based on ""no more f1-score improvement"" (i.e. stabilization of the performance), but it's a bit more advanced.



it's pretty good anyway, my comments are just ideas if you want to improve further. note that it's been a long time since i've work with semi-supervised, i'm not sure i remember everything very well ;)",https://stackoverflow.com/questions/66513144,python,07-03-2021 04:17,1447.0,3.0,1.0,True,08-03-2021 15:46,08-03-2021 09:15,Task-specific Help
68149998,fast filtering of sentences in spacy,"i'm using spacy to divide a text into sentences, match a regex pattern on each sentence, and use some logic based on the results of the match. i started with a naive approach such as:
nlp = spacy.load(""en_core_web_trf"")
regex = re.compile(r'\b(foo|bar)\b')

for text in texts_list:
  doc = nlp(text)
  for sent in doc.sents:
    if re.search(regex, str(s)):
    [...]
    else:
    [...]

and it was very slow. then i used a pipe:
for doc in nlp.pipe(texts_list, disable=['tagger', 'ner', 'attribute_ruler', 'lemmatizer'], n_process=4):
  for sent in doc.sents:
    if re.search(regex, str(s)):
    [...]
    else:
    [...]

but it's still slow. am i missing something?","['python', 'nlp', 'spacy', 'transformer-model', 'sentence']",68153440,"a transformer model is overkill for splitting sentences and will be very slow. instead, a good option is the fast senter from an sm model:
import spacy
nlp = spacy.load(""en_core_web_sm"", disable=[""tok2vec"", ""tagger"", ""parser"", ""attribute_ruler"", ""lemmatizer"", ""ner""])
nlp.enable_pipe(""senter"")
for doc in nlp.pipe(texts, n_process=4):
    ...

the senter should work pretty well if your sentences end with punctuation. if you have a lot of run-on sentences without final punctuation, then the parser might do a better job. to run only the parser, keep the tok2vec and parser components from the original pipeline and don't enable the senter. the parser will be ~5-10x slower than the senter.
if you need this to be even faster, you can use the rule-based sentencizer (start from a blank en model), which is typically a bit worse than the senter because it only splits on the provided punctuation symbols.",https://stackoverflow.com/questions/68149998,python,27-06-2021 10:04,1860.0,1.0,1.0,True,27-06-2021 16:55,27-06-2021 10:26,Implementation Issues
76676065,cannot get jnius to work with chaquopy/android studio,"i am making a simple python script that utilizes openai and jnius(or as it's now known 'pyjnius) libraries with the intention of integrating with android studio.
i have the mainactivity.xml and everything working perfectly, internet access setup in the manifest etc.
my only issue is when asking chatgpt a question (in the emulator while running the app) i am getting a response in the app ui that the module jnius is not found.
i have pyjnius installed on my dev machine and this shows in visual studio codes interpreter and the like.
i have tried doing a pip install in the build.gradle but android studio wont install jnius that way (however it works perfectly for the openai library).
e.g.
python{
            pip{
                install ""jnius""
            }
        }

in my script.py in src/main/python i have this for my imports:
from jnius import autoclass
import openai

am i simply using jnius(pyjnius) incorrectly? or is it not compatible for use with chaquopy/android studio?
error as seen in emulator","['python', 'android-studio', 'openai-api', 'chaquopy']",76677299,"pyjnius is not compatible with chaquopy. you should use chaquopy's own api instead.
for simple cases, you may be able to simply replace from jnius import autoclass with from java import jclass as autoclass.",https://stackoverflow.com/questions/76676065,python,13-07-2023 05:00,191.0,0.0,1.0,True,13-07-2023 08:18,13-07-2023 06:48,Implementation Issues
65220447,add new named entity to spacy&#39;s en_core_web_sm model?,"i'm following the example here on training a new entity type:

it works fine when i don't pass an existing model to it, and will correctly create a new model, which recognizes my new named entity e.g.
 python.exe train-new-entity-type.py

it also works fine when i pass an existing model to it (created by running it once before), and will correctly load the model in the dir/my_model dir, which still recognizes my new named entity e.g.
 python.exe train-new-entity-type.py -m dir/my_model

however, i want to train a new entity type, and add that to spacy's existing model, so that spacy will recognize it's own supported named entities as well as my new entity type, so i tried:
 python.exe train-new-entity-type.py -m en_core_web_sm

however, this didn't seem to work. spacy's own supported named entities were recognized but they were not correct (vs just using the en_core_web_sm on its own without adding my new entity type to it), and my new entity type was no longer recognized at all.
am i doing something wrong? is this possible (adding named entities to en_core_web_sm)?","['python', 'spacy']",65231010,"read about the ""catastrophic forgetting"" problem when updating an existing model: 
it can be tricky to update an existing model, so it might be easier to train a separate model for your new entity type and add the ner component to the en_core_web_sm pipeline with a custom name. the main thing to watch out for is that you need to make sure the models are loaded with the same vocab so that you don't run into problems with the string store:
import spacy
nlp = spacy.load(""en_core_web_sm"")
custom_nlp = spacy.load(""my_model"", vocab=nlp.vocab)
nlp.add_pipe(custom_nlp.get_pipe(""ner""), name=""my_ner"", before=""ner"")

where you add it in the pipeline (before/after the existing ner) will determine which entity spans have priority, since the ner component won't modify existing entity spans.",https://stackoverflow.com/questions/65220447,python,09-12-2020 16:05,1846.0,1.0,1.0,True,10-12-2020 08:33,09-12-2020 17:58,Data Wrangling
78744294,scikit-learn&#39;s feature_names_in method,"a number of scikit-learn's classes have a feature_names_in method, which would be a real time saver if i could understand it better.  specifically, assume your x is a nested list of strings [['a', 'b', 'c'], ['a', 'b', 'd']] and your y is a list of labels ['blue', 'green'].  now, assume you are doing feature selection using, for example, the selectkbest class in scikit.  assume you choose the chi2 univariate approach and ask for the top 2 features (i.e., k=2) and you get your k_best_object.  now, that k_best_object has a method associated with it called feature_names_in which would be really helpful if it returned the ""names"" of the top 2 features.  the problem is that the documentation says that this method is only available when the features are entirely strings.  that would be fine, except for the fact that i haven't been able to get selectkbest (or other scikit classes) to work on strings. instead, i have only been able to get them to work by converting the x values into a numpy array of floats using tfidvectorizer (either count or tf-idf).  so, my question is... how would this method ever be used?  if it's only viable when all x input values are strings, but the only x it will take is floats, then how does this method ever apply?

to illustrate with code, if you try this:
x_t = [['land','building','cat'],['land','building','dog']]
y_t = ['blue', 'green']
chi_select_object_test = selectkbest(chi2, k=100)
chi_select_object_test.fit(x_t,y_t)

it won't work because the data consists of strings, not numbers. you get this error: valueerror: dtype='numeric' is not compatible with arrays of bytes/strings.convert your data to numeric values explicitly instead.
but, if you convert x_t to numbers using, for example, tfid vectorizer(), the class will work:

x_t = ['land building camp','land building dog']
tfidvectorizer_t = tfidfvectorizer(analyzer='word',stop_words= 'english')
x_t = tfidvectorizer_t.fit_transform(x_t)
y_t = ['blue', 'green']
chi_select_object_test = selectkbest(chi2, k=1)
chi_select_object_test.fit(x_t,y_t)

but, then, when you try and access the feature names attribute:
chi_select_object_test.feature_names_in_

you receive the error that: 'selectkbest' object has no attribute 'feature_names_in_'","['numpy', 'scikit-learn', 'tf-idf']",78747386,"i believe what you want to do is pass a pandas.dataframe to selectkbest. the dataframe includes column names that then become the feature names. in the end you can get the best according to the metric you passed by using get_feature_names_out.
in my silly example i generate a dataframe with 3 random columns and tell selectkbest that i want to predict if the 3rd column is bigger than 0.5. obviously we expect it to give us the 3rd column then.
import pandas as pd
import numpy as np
from sklearn.feature_selection import selectkbest, chi2

df = pd.dataframe(np.random.random((100,3)), columns=['a', 'b','c'])

selector = selectkbest(chi2, k=1)
selector.fit(df, df['c'] > 0.5)

selector.get_feature_names_out()

and indeed it returns
array(['c'], dtype=object)

finally feature_names_in is now set to array(['a', 'b', 'c'], dtype=object)  since those are the names of the features we put into the feature selector.",https://stackoverflow.com/questions/78744294,numpy,13-07-2024 15:40,152.0,2.0,1.0,True,14-07-2024 20:16,13-07-2024 21:45,Data Wrangling
56979376,error: could not find a version that satisfies the requirement preshed==2.0.1,"i am trying to install spacy on windows. i am running python 3.6.  when i run
pip install -u spacy
i get the following error:
looking in indexes: 
collecting spacy
  downloading                               /packages/spacy/2.1.4/spacy-2.1.4.tar.gz (29.8mb)
  installing build dependencies: started
  installing build dependencies: finished with status 'error'
  error: complete output from command 'c:\fast\python\3.6.4\python.exe' 'c:\fast\python\3.6.4\lib\site-packages\pip' install --ignore-installed --no-user --prefix 'c:\users\r419957\appdata\local\temp\pip-build-env-8u72uukr\overlay' --no-warn-script-location --no-binary :none: --only-binary :none: -i  -- setuptools 'wheel>0.32.0.<0.33.0' cython 'cymem>=2.0.2,<2.1.0' 'preshed>=2.0.1,<2.1.0' 'murmurhash>=0.28.0,<1.1.0' thinc==7.0.0.dev6:
  error: looking in indexes: 
  collecting setuptools
    downloading  (575kb)
  collecting wheel>0.32.0.<0.33.0
    downloading 
  collecting cython
    downloading  (1.6mb)
  collecting cymem<2.1.0,>=2.0.2
    downloading 
  collecting preshed<2.1.0,>=2.0.1
    error: could not find a version that satisfies the requirement preshed<2.1.0,>=2.0.1 (from versions: 1.0.0, 1.0.1)
  error: no matching distribution found for preshed<2.1.0,>=2.0.1
  ----------------------------------------
error: command ""'c:\fast\python\3.6.4\python.exe' 'c:\fast\python\3.6.4\lib\site-packages\pip' install --ignore-installed --no-user --prefix 'c:\users\r419957\appdata\local\temp\pip-build-env-8u72uukr\overlay' --no-warn-script-location --no-binary :none: --only-binary :none: -i  -- setuptools 'wheel>0.32.0.<0.33.0' cython 'cymem>=2.0.2,<2.1.0' 'preshed>=2.0.1,<2.1.0' 'murmurhash>=0.28.0,<1.1.0' thinc==7.0.0.dev6"" failed with error code 1 in none

i then tried to download preshed by itself by running
pip install preshed
this however does not install the version i need which is 2.0.1
when i run
pip install preshed==2.0.1
i get the following error
error: could not find a version that satisfies the requirement preshed==2.0.1 (from versions: 1.0.0, 1.0.1)
error: no matching distribution found for preshed==2.0.1

any help figuring out what is going wrong would be appreciated. thanks.","['python', 'python-3.x', 'pip', 'version', 'spacy']",56984255,"you have pip set up to look for packages in your private repository (my-artifact-repo.com) which is missing the package. either upload the preshed package (and its eventual dependencies) to the private repo, or install preshed from pypi:
$ pip install preshed --index-url=",https://stackoverflow.com/questions/56979376,python,10-07-2019 22:21,4286.0,0.0,1.0,True,22-02-2023 16:52,22-02-2023 16:52,Tool Setup/Errors
76744966,frequency distribution is not returning words but letters,"i am trying to find what words appear the most often.
but each time i run freqdist it does not return the most common words but letters.
freqdist({' ': 496, 'e': 306, 't': 205, 'a': 182, 's': 181, 'n': 160, 'o': 146, 'r': 142, 'i': 118, 'l': 110, ...})
here is my code:
    newdf['tokens1'] = newdf['review'].apply(word_tokenize) newdf['tokens1'] = newdf['tokens1'].apply(str)
for i in range(newdf.shape[1]):
    # add each comment.
    review_comments = review_comments + newdf['tokens1'][i]

from nltk.probability import freqdist
fdist = freqdist(review_comments)
fdist

returns
freqdist({' ': 496, 'e': 306, 't': 205, 'a': 182, 's': 181, 'n': 160, 'o': 146, 'r': 142, 'i': 118, 'l': 110, ...})","['python', 'pandas', 'nltk', 'frequency-distribution']",76744996,"you need first yo use nltk.word_tokenize:
from nltk.tokenize import word_tokenize
tokens = nltk.word_tokenize(review_comments)
fdist = freqdist(tokens)
fdist",https://stackoverflow.com/questions/76744966,python,22-07-2023 17:08,119.0,0.0,1.0,True,22-07-2023 17:16,22-07-2023 17:09,Implementation Issues
78420651,create bio format to a sentence from a json file - to train ner model,"i have a json file that'll be used as data for a ner model.
it has a sentence and the relevant entities in that specific sentence.
i want to create a function that will generate a bio-labeled string for each sentence according to the entities
for example the following object from the json file
{
      ""request"": ""i want to fly to new york on the 13.3"",
      ""entities"": [
        {""start"": 16, ""end"": 23, ""text"": ""new york"", ""category"": ""destination""},
        {""start"": 32, ""end"": 35, ""text"": ""13.3"", ""category"": ""date""}
      ]
} 

""i want to fly to new york on the 13.3""
the corresponding bio label will be
""o o o o o b-destination i-destination o o b-date""
where b-category is the beginning of that category
i-category stands for inside and o for outside.
i'm looking for a python code to iterate on each object in the json file that will generate a bio-label for it.
change the json format if necessary","['python', 'machine-learning', 'named-entity-recognition']",78421095,"this is just a quick implementation for the above task, and many optimizations are possible, which can be explored later, but at first glace here is the function:
def bio_converter(r, entities):
    to_replace = {} # needed to maintain all the ner to be replaced
    for i in entities:
        sub = r[i['start']+1:i['end']+2].split(' ') # 1 indexed values in entities
        if len(sub) > 1:
            vals = [f""b-{i['category']}""] + ([f""i-{i['category']}""] * (len(sub)-1))
        else:
            vals = [f""b-{i['category']}""]

        to_replace = to_replace | dict(zip(sub,vals))

    r = r.split(' ')
    r = [to_replace[i] if i in to_replace else 'o' for i in r ]
    return ' '.join(r)

js = {
        ""request"": ""i want to fly to new york on the 13.3"",
        ""entities"": [
          {""start"": 16, ""end"": 23, ""text"": ""new york"", ""category"": ""destination""},
          {""start"": 32, ""end"": 35, ""text"": ""13.3"", ""category"": ""date""}
        ]
      }
bio_converter(js['request'], js['entities'])

should output:
o o o o o b-destination i-destination o o b-date",https://stackoverflow.com/questions/78420651,python,02-05-2024 16:56,127.0,0.0,1.0,True,03-05-2024 05:41,03-05-2024 05:41,Implementation Issues
71702731,"job type(full time , part time) detection with machine learning model in python","i have a dataset of jobs where i have columns ""title"" ,""description"" , ""city"" etc. and ""best jobs"" column. output of the dataset is ""best jobs"" where i have two outputs(yes , no) yes mean jobs are part time and no , mean job is full time. i want to train any machine learning model. firstly i want to train the model x or feature columns will be title , description etc. and label will be ""best jobs"". but i do not know how to train the model on string columns. please help me in this.
import numpy as np
import pandas as pd
import os, sys
from sklearn.preprocessing import minmaxscaler
from xgboost import xgbclassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

df = pd.read_csv(""machinelearning-new-best-gar-jobs.csv"", engine = 'python',encoding='mac_roman')
df.head()\


df['job description'].replace('  ', np.nan, inplace=true) df=df.dropna(subset=['job description']) df.isnull().sum()


then i will convert the label (bestjobs) to integer 1 and 0
df['bestjobs'] = (df['bestjobs']=='yes').astype(int)  # changing yes to 1 and no to 0 print(df['bestjobs'].value_counts())


i want to know which model should i apply to get it done.","['python', 'python-3.x', 'machine-learning', 'nlp']",71785552,"i think you probably can only use two columns ""job description"" and ""best job"" to train the model. then it becomes a text classification problem, like classifying movie reviews as either positive or negative. then you can preprocess the job description text and use a neural network to train your model.
the basic idea is that you may only need a few required features to train your model instead of processing all of the feature data you got. you can refer to this blog  (text preprocessing for nlp).
hope it is helpful for you!",https://stackoverflow.com/questions/71702731,python,01-04-2022 06:35,194.0,-1.0,1.0,True,07-04-2022 16:19,02-04-2022 10:25,Preprocessing Tasks
74279532,limit the number of repetitive consecutive characters in a string,"i'm preprocessing tweets and need to set the maximum limit of the number of consecutive occurrences of ""@user"" to 3 times. for example, a tweet like this:

this tweet contains hate speech @user@user@user@user@user about a target group @user@user

after processing, should look like this:

this tweet contains hate speech @user@user@user about a target group @user@user

i was able to achieve the desired result with a while loop, however, i'm wondering if someone knows how to do it a simpler way. thanks!
tweets = [""this tweet contains hate speech @user@user@user@user@user about a target group @user@user""]

k = ""@user""
limit = 3
i = 0
for tweet in tweets: 
    tweet = tweet.split(' ')

    while i < len(tweet):
        if tweet[i].count(k) > limit:
            tweet[i] = k*int(limit)
            tweet = "" "".join(str(item) for item in tweet)
        i +=1

print(tweet)
# output: this tweet contains hate speech @user@user@user about a target group @user@user","['python', 'nlp', 'data-preprocessing']",74279686,"you can just use re to replace 4 or more occurrences of @user with three:
tweet = ""this tweet contains hate speech @user@user@user@user@user about a target group @user@user""
re.sub(r'(@user){4,}', r'@user@user@user', tweet)",https://stackoverflow.com/questions/74279532,python,01-11-2022 17:18,111.0,1.0,2.0,True,02-11-2022 06:19,02-11-2022 06:19,Implementation Issues
70922447,valueerror: the first argument to `layer.call` must always be passed,"i was trying to build a model with the sequential api (it has already worked for me with the functional api). here is the model that i'm trying to built in sequential api:
from tensorflow.keras import layers
model_1 = tf.keras.sequential([
    layers.input(shape=(1,), dtype='string'),
    text_vectorizer(),
    embedding(),
    layer.globalaveragepooling1d(),
    layers.dense(1, activation='sigmoid')
], name=""model_1_dense"")

error:
----> 4     text_vectorizer(),
      5     embedding(),
      6     layer.globalaveragepooling1d(),
valueerror: the first argument to `layer.call` must always be passed.

here is how text_vectorizer layer look like:
max_vocab_length = 10000
max_length = 15

text_vectorizer = textvectorization(max_tokens=max_vocab_length,
                                    output_mode=""int"",
                                    output_sequence_length=max_length)","['python', 'tensorflow', 'keras', 'deep-learning', 'nlp']",70922839,"the text_vectorizer layer should be passed to your model without parentheses. try something like this:
import tensorflow as tf

max_vocab_length = 10000
max_length = 15

text_vectorizer = tf.keras.layers.textvectorization(max_tokens=max_vocab_length,
                                    output_mode=""int"",
                                    output_sequence_length=max_length)

text_dataset = tf.data.dataset.from_tensor_slices([""foo"", ""bar"", ""baz""])
text_vectorizer.adapt(text_dataset.batch(64))
model_1 = tf.keras.sequential([
    tf.keras.layers.input(shape=(1,), dtype='string'),
    text_vectorizer,
    tf.keras.layers.embedding(max_vocab_length, 50),
    tf.keras.layers.globalaveragepooling1d(),
    tf.keras.layers.dense(1, activation='sigmoid')
], name=""model_1_dense"")

print(model_1(tf.constant([['foo']])))

tf.tensor([[0.48518932]], shape=(1, 1), dtype=float32)",https://stackoverflow.com/questions/70922447,python,31-01-2022 07:26,9561.0,2.0,1.0,True,07-05-2022 02:07,31-01-2022 09:24,Tool Setup/Errors
73244442,huggingface trainer() cannot report to wandb,"i am trying to set trainer with arguments report_to to wandb, refer to this docs
with config:
training_args = trainingarguments(
    output_dir=""test_trainer"",
    evaluation_strategy=""steps"",
    learning_rate=config.learning_rate,
    num_train_epochs=config.epochs,
    weight_decay=config.weight_decay,
    logging_dir=config.logging_dir,
    report_to=""wandb"",
    save_total_limit=1,
    per_device_train_batch_size=config.batch_size,
    per_device_eval_batch_size=config.batch_size,
    fp16=true,
    load_best_model_at_end=true,
    seed=42
)

yet when i set trainer with:
trainer = trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics
)

it shows:
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
<ipython-input-68-b009351ab52d> in <module>
      4     train_dataset=train_dataset,
      5     eval_dataset=eval_dataset,
----> 6     compute_metrics=compute_metrics
      7 )

~/.virtualenvs/transformers_lab/lib/python3.7/site-packages/transformers/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)
    286                 ""you should subclass `trainer` and override the `create_optimizer_and_scheduler` method.""
    287             )
--> 288         default_callbacks = default_callbacks + get_reporting_integration_callbacks(self.args.report_to)
    289         callbacks = default_callbacks if callbacks is none else default_callbacks + callbacks
    290         self.callback_handler = callbackhandler(

~/.virtualenvs/transformers_lab/lib/python3.7/site-packages/transformers/integrations.py in get_reporting_integration_callbacks(report_to)
    794         if integration not in integration_to_callback:
    795             raise valueerror(
--> 796                 f""{integration} is not supported, only {', '.join(integration_to_callback.keys())} are supported.""
    797             )
    798     return [integration_to_callback[integration] for integration in report_to]

valueerror: w is not supported, only azure_ml, comet_ml, mlflow, tensorboard, wandb are supported.

have anyone got same error before?","['python', 'huggingface-transformers', 'wandb']",73258966,"although the documentation states that the report_to parameter can receive both list[str] or str i have always used a list with 1! element for this purpose.
therefore, even if you report only to wandb, the solution to your problem is to replace:
 report_to = 'wandb'

with
report_to = [""wandb""]",https://stackoverflow.com/questions/73244442,python,05-08-2022 03:48,2268.0,1.0,1.0,True,09-08-2022 08:29,09-08-2022 08:29,Implementation Issues
78145421,how to ensure that the langchain generates the correct output and is not random?,"i am using the below code and for the same question, it return different results, is there any way to fix that?
from langchain.chains import create_sql_query_chain
from langchain_openai import chatopenai
from langchain_community.utilities import sqldatabase
import os

def return_query(question)
   db = sqldatabase.from_uri(os.getenv(""postgres_url""))
   llm = chatopenai(model=""gpt-3.5-turbo"", temperature=0)
   chain = create_sql_query_chain(llm, db)
   response = chain.invoke({""question"": question})
   return response

example my question is ""create table student"" and i get the below responses on re-trying the same code:

response1: this table does not exist in the provided database schema.
response2: select * from information_schema.tables where table_name = 'student' limit 1;
response3: this question cannot be answered directly using the existing tables provided in the database schema. to create a new table named ""student"", you can use the following sql query:

create table student (
    id serial primary key,
    name text not null,
    email text not null,
    age integer,
    major text
);","['langchain', 'large-language-model']",78145784,"firstly, i'd like to point out that large language models (llms) produce non-deterministic outputs, meaning that for every query, you may get different results. this is not a bug, it is feature.
the two most crucial terms for achieving deterministic outcomes with llms are the ""prompt"" and the ""function calling"".
regarding your question, check the source code:


the sqldatabase class provides a get_table_info method that can be
used to get column information as well as sample data from the table.

this functionality does not extend to ""write"" operations. if you want your llm to produce the same result when you query ""create table"", you can use the create_sql_agent method found at  this method enables the creation of a tool (function calling in langchain) with a function that can generate a table in the database with the necessary parameters provided.",https://stackoverflow.com/questions/78145421,langchain,12-03-2024 07:56,686.0,-1.0,1.0,True,12-03-2024 09:01,12-03-2024 08:43,Implementation Issues
79502752,getting cuda out of memory when importing microsoft/orca-2-13b from hugging faces,"i am using ubuntu 24.04.1 on an aws ec2 instance g5.8xlarge.
i am receiving the following error message:
outofmemoryerror: allocation on device 

code:
import os
os.environ[""pytorch_cuda_alloc_conf""] = ""backend:cudamallocasync""
import torch
torch.cuda.empty_cache()
import transformers
    
if torch.cuda.is_available():
    torch.set_default_device(""cuda"")
    
device = torch.device(""cuda"")
    
model = transformers.automodelforcausallm.from_pretrained(""microsoft/orca-2-13b"", device_map=device)

full error:
/home/ubuntu/anaconda3/envs/ai/lib/python3.12/site-packages/torch/cuda/__init__.py:734: userwarning: can't initialize nvml
  warnings.warn(""can't initialize nvml"")

loadingï¿½ï¿½ï¿½checkpointï¿½ï¿½ï¿½shards:ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½33%
ï¿½ï¿½ï¿½2/6ï¿½ï¿½ï¿½[00:04<00:06,ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½1.72s/it]

/home/ubuntu/anaconda3/envs/ai/lib/p34: userwarning: can't initialize nvml
  warnings.warn(""can't initialize nvml"")

---------------------------------------------------------------------------
outofmemoryerror                          traceback (most recent call last)
cell in[5], line 6
      2     torch.set_default_device(""cuda"")
      4 device = torch.device(""cuda"")
----> 6 model = transformers.automodelforcausallm.from_pretrained(""microsoft/orca-2-13b"", device_map=device)
      8 # 
      9 # please use the slow tokenizer since fast and slow tokenizer produces different tokens
     10 tokenizer = transformers.autotokenizer.from_pretrained(
     11         ""microsoft/orca-2-13b"",
     12         use_fast=true,
     13     )

file ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564, in _baseautomodelclass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    562 elif type(config) in cls._model_mapping.keys():
    563     model_class = _get_model_class(config, cls._model_mapping)
--> 564     return model_class.from_pretrained(
    565         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    566     )
    567 raise valueerror(
    568     f""unrecognized configuration class {config.__class__} for this kind of automodel: {cls.__name__}.\n""
    569     f""model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.""
    570 )

file ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:262, in restore_default_torch_dtype.<locals>._wrapper(*args, **kwargs)
    260 old_dtype = torch.get_default_dtype()
    261 try:
--> 262     return func(*args, **kwargs)
    263 finally:
    264     torch.set_default_dtype(old_dtype)

file ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:4319, in pretrainedmodel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)
   4309     if dtype_orig is not none:
   4310         torch.set_default_dtype(dtype_orig)
   4312     (
   4313         model,
   4314         missing_keys,
   4315         unexpected_keys,
   4316         mismatched_keys,
   4317         offload_index,
   4318         error_msgs,
-> 4319     ) = cls._load_pretrained_model(
   4320         model,
   4321         state_dict,
   4322         loaded_state_dict_keys,  # xxx: rename?
   4323         resolved_archive_file,
   4324         pretrained_model_name_or_path,
   4325         ignore_mismatched_sizes=ignore_mismatched_sizes,
   4326         sharded_metadata=sharded_metadata,
   4327         _fast_init=_fast_init,
   4328         low_cpu_mem_usage=low_cpu_mem_usage,
   4329         device_map=device_map,
   4330         offload_folder=offload_folder,
   4331         offload_state_dict=offload_state_dict,
   4332         dtype=torch_dtype,
   4333         hf_quantizer=hf_quantizer,
   4334         keep_in_fp32_modules=keep_in_fp32_modules,
   4335         gguf_path=gguf_path,
   4336         weights_only=weights_only,
   4337     )
   4339 # make sure token embedding weights are still tied if needed
   4340 model.tie_weights()

file ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:4897, in pretrainedmodel._load_pretrained_model(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)
   4895     else:
   4896         fixed_state_dict = cls._fix_state_dict_keys_on_load(state_dict)
-> 4897         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
   4898             model_to_load,
   4899             fixed_state_dict,
   4900             start_prefix,
   4901             expected_keys,
   4902             device_map=device_map,
   4903             offload_folder=offload_folder,
   4904             offload_index=offload_index,
   4905             state_dict_folder=state_dict_folder,
   4906             state_dict_index=state_dict_index,
   4907             dtype=dtype,
   4908             hf_quantizer=hf_quantizer,
   4909             is_safetensors=is_safetensors,
   4910             keep_in_fp32_modules=keep_in_fp32_modules,
   4911             unexpected_keys=unexpected_keys,
   4912         )
   4913         error_msgs += new_error_msgs
   4914 else:
   4915     # sharded checkpoint or whole but low_cpu_mem_usage==true

file ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:896, in _load_state_dict_into_meta_model(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)
    893         param_device = ""cpu"" if is_local_dist_rank_0() else ""meta""
    895     # for backward compatibility with older versions of `accelerate` and for non-quantized params
--> 896     set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
    897 else:
    898     hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)

file ~/anaconda3/envs/ai/lib/python3.12/site-packages/accelerate/utils/modeling.py:330, in set_module_tensor_to_device(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)
    328             module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)
    329 elif isinstance(value, torch.tensor):
--> 330     new_value = value.to(device)
    331 else:
    332     new_value = torch.tensor(value, device=device)

file ~/anaconda3/envs/ai/lib/python3.12/site-packages/torch/utils/_device.py:104, in devicecontext.__torch_function__(self, func, types, args, kwargs)
    102 if func in _device_constructors() and kwargs.get('device') is none:
    103     kwargs['device'] = self.device
--> 104 return func(*args, **kwargs)

outofmemoryerror: allocation on device","['machine-learning', 'pytorch', 'huggingface-transformers', 'huggingface']",79504165,"you can check out information on the specific model here. but you can see it requires 52.1ï¿½ï¿½ï¿½gb of vram (gpu memory).
based on this table we see that you have 24gb of gpu memory. so it won't be able to fit. if you aren't able to get more gpu memory, you can look into quantized models.
you can check out the models on huggingface that have quantized versions, the gpu memory required, and the best use case.",https://stackoverflow.com/questions/79502752,machine-learning,12-03-2025 06:05,59.0,0.0,1.0,True,12-03-2025 16:00,12-03-2025 11:32,Implementation Issues
78347434,problem setting up llama-2 in google colab - cell-run fails when loading checkpoint shards,"i'm trying to use llama 2 chat (via hugging face) with 7b parameters in google colab (python 3.10.12). i've already obtain my access token via meta. i simply use the code in hugging face on how to implement the model along with my access token. here is my code:
!pip install transformers
 
from transformers import automodelforcausallm, autotokenizer
import torch

token = ""---token copied from hugging face and pasted here---""

tokenizer = autotokenizer.from_pretrained(""meta-llama/llama-2-7b-chat-hf"", token=token)
model = automodelforcausallm.from_pretrained(""meta-llama/llama-2-7b-chat-hf"", token=token)

it starts downloading the model but when it reaches loading checkpoint shards: it just stops running and there is no error:","['python', 'huggingface-transformers', 'large-language-model', 'llama']",78347722,"the issue is with colab instance running out of ram. based on your comments you are using basic colab instance with 12.7 gb cpu ram.
for llama model you'll need:

for the float32 model about 25 gb (but you'll need both cpu ram and same 25 gb gpu ram);
for the bfloat16 model around 13 gb (and still not enough to fit basic colab cpu instance, given that you'll also need to store intermediate calculations from the model);

check this link for the details on the required resources:
huggingface.co/nousresearch/llama-2-7b-chat-hf/discussions/3
also if you want only to do inference (predictions) on the model i would recommend to use it's quantized 4bit or 8bit versions. both can be ran on cpu and don't need a lot of memory.",https://stackoverflow.com/questions/78347434,python,18-04-2024 12:28,1431.0,3.0,1.0,True,20-10-2024 15:01,18-04-2024 12:34,Implementation Issues
72936945,pass elements of a list in a function,"i have a function that is able to create triples and relationships from text. however, when i create a list of a column that contains text and pass it through the function, it only processes the first row, or item of the list. therefore, i am wondering how the whole list can be processed within this function. maybe a for loop would work?
the following line contains the list
rez_dictionary = {'decent little reader, poor tablet',
 'ok for what it is',
 'too heavy and poor weld quality,',
 'difficult mount',
 'just got it installed'}

from transformers import pipeline

triplet_extractor = pipeline('text2text-generation', model='babelscape/rebel-large', tokenizer='babelscape/rebel-large')

# we need to use the tokenizer manually since we need special tokens.
extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(rez_dictionary, return_tensors=true, return_text=false)[0][""generated_token_ids""]])

print(extracted_text[0])

if anyone has a suggestion, i am looking forward for it.
would it also be possible to get the output adjusted to the following format:
# function to parse the generated text and extract the triplets
def extract_triplets(text):
    triplets = []
    relation, subject, relation, object_ = '', '', '', ''
    text = text.strip()
    current = 'x'
    for token in text.replace(""<s>"", """").replace(""<pad>"", """").replace(""</s>"", """").split():
        if token == ""<triplet>"":
            current = 't'
            if relation != '':
                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
                relation = ''
            subject = ''
        elif token == ""<subj>"":
            current = 's'
            if relation != '':
                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
            object_ = ''
        elif token == ""<obj>"":
            current = 'o'
            relation = ''
        else:
            if current == 't':
                subject += ' ' + token
            elif current == 's':
                object_ += ' ' + token
            elif current == 'o':
                relation += ' ' + token
    if subject != '' and relation != '' and object_ != '':
        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
    return triplets
extracted_triplets = extract_triplets(extracted_text[0])
print(extracted_triplets)","['python', 'huggingface-transformers']",72972107,"you are removing the other entries of rez_dictionary inside the batch_decode:
triplet_extractor(rez_dictionary, return_tensors=true, return_text=false)[0][""generated_token_ids""]

use a list comprehension instead:
from transformers import pipeline

rez = ['decent little reader, poor tablet',
 'ok for what it is',
 'too heavy and poor weld quality,',
 'difficult mount',
 'just got it installed']

triplet_extractor = pipeline('text2text-generation', model='babelscape/rebel-large', tokenizer='babelscape/rebel-large')

model_output = triplet_extractor(rez, return_tensors=true, return_text=false)

extracted_text = triplet_extractor.tokenizer.batch_decode([x[""generated_token_ids""] for x in model_output])
print(""\n"".join(extracted_text))

output:
<s><triplet> decent little reader <subj> poor tablet <obj> different from <triplet> poor tablet <subj> decent little reader <obj> different from</s>
<s><triplet> ok for what it is <subj> film <obj> instance of</s>
<s><triplet> too heavy and poor <subj> weld quality <obj> subclass of</s>
<s><triplet> difficult mount <subj> mount <obj> subclass of</s>
<s><triplet> 2008 summer olympics <subj> 2008 <obj> point in time</s>

regarding the extension of the op's question, op wanted to know how to run the function extract_triplets. op can simply do that via a for-loop:
for text in extracted_text:
  print(extract_triplets(text))

output:
[{'head': 'decent little reader', 'type': 'different from', 'tail': 'poor tablet'}, {'head': 'poor tablet', 'type': 'different from', 'tail': 'decent little reader'}]
[{'head': 'ok for what it is', 'type': 'instance of', 'tail': 'film'}]
[{'head': 'too heavy and poor', 'type': 'subclass of', 'tail': 'weld quality'}]
[{'head': 'difficult mount', 'type': 'subclass of', 'tail': 'mount'}]
[{'head': '2008 summer olympics', 'type': 'point in time', 'tail': '2008'}]",https://stackoverflow.com/questions/72936945,python,11-07-2022 10:14,154.0,2.0,1.0,True,20-07-2022 09:50,20-07-2022 09:50,Implementation Issues
77639714,big query - case statements and apostrophe&#39;s,"afternoon, was wondering if anyone is able to assist.
whilst writing case statements to flag particular phrases in netezza, if we came across verbatim which contained an apostrophe i would get this round this with the below example. however im struggling to recreate this on bq and wondered if anyone has a solution?
verbatim example : doesn't have an account
netezza case statement example : when lower(a.case_detail) like lower('%doesn''t have an account%')
thanks in advance for any helpful suggestions :)
currently not found a solution","['sql', 'google-cloud-platform', 'google-bigquery', 'nlp']",77639946,"you have at least two options:

escape the ' using backslash \: '%doesn\'t have an account%'
use double quotes to start and end the string literal: ""%doesn't have an account%""",https://stackoverflow.com/questions/77639714,sql,11-12-2023 12:48,106.0,0.0,1.0,True,11-12-2023 13:30,11-12-2023 12:49,Implementation Issues
71577525,huggingface sequence classification unfreezing layers,"i am using longformer for sequence classification - binary problem
i have downloaded required files
# load model and tokenizer and define length of the text sequence
model = longformerforsequenceclassification.from_pretrained('allenai/longformer-base-4096',
                                                           gradient_checkpointing=false,
                                                           attention_window = 512)
tokenizer = longformertokenizerfast.from_pretrained('allenai/longformer-base-4096', max_length = 1024)

then as shown here i ran the below code
for name, param in model.named_parameters():
     print(name, param.requires_grad)


longformer.embeddings.word_embeddings.weight true
longformer.embeddings.position_embeddings.weight true
longformer.embeddings.token_type_embeddings.weight true
longformer.embeddings.layernorm.weight true
longformer.embeddings.layernorm.bias true
longformer.encoder.layer.0.attention.self.query.weight true
longformer.encoder.layer.0.attention.self.query.bias true
longformer.encoder.layer.0.attention.self.key.weight true
longformer.encoder.layer.0.attention.self.key.bias true
longformer.encoder.layer.0.attention.self.value.weight true
longformer.encoder.layer.0.attention.self.value.bias true
longformer.encoder.layer.0.attention.self.query_global.weight true
longformer.encoder.layer.0.attention.self.query_global.bias true
longformer.encoder.layer.0.attention.self.key_global.weight true
longformer.encoder.layer.0.attention.self.key_global.bias true
longformer.encoder.layer.0.attention.self.value_global.weight true
longformer.encoder.layer.0.attention.self.value_global.bias true
longformer.encoder.layer.0.attention.output.dense.weight true
longformer.encoder.layer.0.attention.output.dense.bias true
longformer.encoder.layer.0.attention.output.layernorm.weight true
longformer.encoder.layer.0.attention.output.layernorm.bias true
longformer.encoder.layer.0.intermediate.dense.weight true
longformer.encoder.layer.0.intermediate.dense.bias true
longformer.encoder.layer.0.output.dense.weight true
longformer.encoder.layer.0.output.dense.bias true
longformer.encoder.layer.0.output.layernorm.weight true
longformer.encoder.layer.0.output.layernorm.bias true
longformer.encoder.layer.1.attention.self.query.weight true
longformer.encoder.layer.1.attention.self.query.bias true
longformer.encoder.layer.1.attention.self.key.weight true
longformer.encoder.layer.1.attention.self.key.bias true
longformer.encoder.layer.1.attention.self.value.weight true
longformer.encoder.layer.1.attention.self.value.bias true
longformer.encoder.layer.1.attention.self.query_global.weight true
longformer.encoder.layer.1.attention.self.query_global.bias true
longformer.encoder.layer.1.attention.self.key_global.weight true
longformer.encoder.layer.1.attention.self.key_global.bias true
longformer.encoder.layer.1.attention.self.value_global.weight true
longformer.encoder.layer.1.attention.self.value_global.bias true
longformer.encoder.layer.1.attention.output.dense.weight true
longformer.encoder.layer.1.attention.output.dense.bias true
longformer.encoder.layer.1.attention.output.layernorm.weight true
longformer.encoder.layer.1.attention.output.layernorm.bias true
longformer.encoder.layer.1.intermediate.dense.weight true
longformer.encoder.layer.1.intermediate.dense.bias true
longformer.encoder.layer.1.output.dense.weight true
longformer.encoder.layer.1.output.dense.bias true
longformer.encoder.layer.1.output.layernorm.weight true
longformer.encoder.layer.1.output.layernorm.bias true
longformer.encoder.layer.2.attention.self.query.weight true
longformer.encoder.layer.2.attention.self.query.bias true
longformer.encoder.layer.2.attention.self.key.weight true
longformer.encoder.layer.2.attention.self.key.bias true
longformer.encoder.layer.2.attention.self.value.weight true
longformer.encoder.layer.2.attention.self.value.bias true
longformer.encoder.layer.2.attention.self.query_global.weight true
longformer.encoder.layer.2.attention.self.query_global.bias true
longformer.encoder.layer.2.attention.self.key_global.weight true
longformer.encoder.layer.2.attention.self.key_global.bias true
longformer.encoder.layer.2.attention.self.value_global.weight true
longformer.encoder.layer.2.attention.self.value_global.bias true
longformer.encoder.layer.2.attention.output.dense.weight true
longformer.encoder.layer.2.attention.output.dense.bias true
longformer.encoder.layer.2.attention.output.layernorm.weight true
longformer.encoder.layer.2.attention.output.layernorm.bias true
longformer.encoder.layer.2.intermediate.dense.weight true
longformer.encoder.layer.2.intermediate.dense.bias true
longformer.encoder.layer.2.output.dense.weight true
longformer.encoder.layer.2.output.dense.bias true
longformer.encoder.layer.2.output.layernorm.weight true
longformer.encoder.layer.2.output.layernorm.bias true
longformer.encoder.layer.3.attention.self.query.weight true
longformer.encoder.layer.3.attention.self.query.bias true
longformer.encoder.layer.3.attention.self.key.weight true
longformer.encoder.layer.3.attention.self.key.bias true
longformer.encoder.layer.3.attention.self.value.weight true
longformer.encoder.layer.3.attention.self.value.bias true
longformer.encoder.layer.3.attention.self.query_global.weight true
longformer.encoder.layer.3.attention.self.query_global.bias true
longformer.encoder.layer.3.attention.self.key_global.weight true
longformer.encoder.layer.3.attention.self.key_global.bias true
longformer.encoder.layer.3.attention.self.value_global.weight true
longformer.encoder.layer.3.attention.self.value_global.bias true
longformer.encoder.layer.3.attention.output.dense.weight true
longformer.encoder.layer.3.attention.output.dense.bias true
longformer.encoder.layer.3.attention.output.layernorm.weight true
longformer.encoder.layer.3.attention.output.layernorm.bias true
longformer.encoder.layer.3.intermediate.dense.weight true
longformer.encoder.layer.3.intermediate.dense.bias true
longformer.encoder.layer.3.output.dense.weight true
longformer.encoder.layer.3.output.dense.bias true
longformer.encoder.layer.3.output.layernorm.weight true
longformer.encoder.layer.3.output.layernorm.bias true
longformer.encoder.layer.4.attention.self.query.weight true
longformer.encoder.layer.4.attention.self.query.bias true
longformer.encoder.layer.4.attention.self.key.weight true
longformer.encoder.layer.4.attention.self.key.bias true
longformer.encoder.layer.4.attention.self.value.weight true
longformer.encoder.layer.4.attention.self.value.bias true
longformer.encoder.layer.4.attention.self.query_global.weight true
longformer.encoder.layer.4.attention.self.query_global.bias true
longformer.encoder.layer.4.attention.self.key_global.weight true
longformer.encoder.layer.4.attention.self.key_global.bias true
longformer.encoder.layer.4.attention.self.value_global.weight true
longformer.encoder.layer.4.attention.self.value_global.bias true
longformer.encoder.layer.4.attention.output.dense.weight true
longformer.encoder.layer.4.attention.output.dense.bias true
longformer.encoder.layer.4.attention.output.layernorm.weight true
longformer.encoder.layer.4.attention.output.layernorm.bias true
longformer.encoder.layer.4.intermediate.dense.weight true
longformer.encoder.layer.4.intermediate.dense.bias true
longformer.encoder.layer.4.output.dense.weight true
longformer.encoder.layer.4.output.dense.bias true
longformer.encoder.layer.4.output.layernorm.weight true
longformer.encoder.layer.4.output.layernorm.bias true
longformer.encoder.layer.5.attention.self.query.weight true
longformer.encoder.layer.5.attention.self.query.bias true
longformer.encoder.layer.5.attention.self.key.weight true
longformer.encoder.layer.5.attention.self.key.bias true
longformer.encoder.layer.5.attention.self.value.weight true
longformer.encoder.layer.5.attention.self.value.bias true
longformer.encoder.layer.5.attention.self.query_global.weight true
longformer.encoder.layer.5.attention.self.query_global.bias true
longformer.encoder.layer.5.attention.self.key_global.weight true
longformer.encoder.layer.5.attention.self.key_global.bias true
longformer.encoder.layer.5.attention.self.value_global.weight true
longformer.encoder.layer.5.attention.self.value_global.bias true
longformer.encoder.layer.5.attention.output.dense.weight true
longformer.encoder.layer.5.attention.output.dense.bias true
longformer.encoder.layer.5.attention.output.layernorm.weight true
longformer.encoder.layer.5.attention.output.layernorm.bias true
longformer.encoder.layer.5.intermediate.dense.weight true
longformer.encoder.layer.5.intermediate.dense.bias true
longformer.encoder.layer.5.output.dense.weight true
longformer.encoder.layer.5.output.dense.bias true
longformer.encoder.layer.5.output.layernorm.weight true
longformer.encoder.layer.5.output.layernorm.bias true
longformer.encoder.layer.6.attention.self.query.weight true
longformer.encoder.layer.6.attention.self.query.bias true
longformer.encoder.layer.6.attention.self.key.weight true
longformer.encoder.layer.6.attention.self.key.bias true
longformer.encoder.layer.6.attention.self.value.weight true
longformer.encoder.layer.6.attention.self.value.bias true
longformer.encoder.layer.6.attention.self.query_global.weight true
longformer.encoder.layer.6.attention.self.query_global.bias true
longformer.encoder.layer.6.attention.self.key_global.weight true
longformer.encoder.layer.6.attention.self.key_global.bias true
longformer.encoder.layer.6.attention.self.value_global.weight true
longformer.encoder.layer.6.attention.self.value_global.bias true
longformer.encoder.layer.6.attention.output.dense.weight true
longformer.encoder.layer.6.attention.output.dense.bias true
longformer.encoder.layer.6.attention.output.layernorm.weight true
longformer.encoder.layer.6.attention.output.layernorm.bias true
longformer.encoder.layer.6.intermediate.dense.weight true
longformer.encoder.layer.6.intermediate.dense.bias true
longformer.encoder.layer.6.output.dense.weight true
longformer.encoder.layer.6.output.dense.bias true
longformer.encoder.layer.6.output.layernorm.weight true
longformer.encoder.layer.6.output.layernorm.bias true
longformer.encoder.layer.7.attention.self.query.weight true
longformer.encoder.layer.7.attention.self.query.bias true
longformer.encoder.layer.7.attention.self.key.weight true
longformer.encoder.layer.7.attention.self.key.bias true
longformer.encoder.layer.7.attention.self.value.weight true
longformer.encoder.layer.7.attention.self.value.bias true
longformer.encoder.layer.7.attention.self.query_global.weight true
longformer.encoder.layer.7.attention.self.query_global.bias true
longformer.encoder.layer.7.attention.self.key_global.weight true
longformer.encoder.layer.7.attention.self.key_global.bias true
longformer.encoder.layer.7.attention.self.value_global.weight true
longformer.encoder.layer.7.attention.self.value_global.bias true
longformer.encoder.layer.7.attention.output.dense.weight true
longformer.encoder.layer.7.attention.output.dense.bias true
longformer.encoder.layer.7.attention.output.layernorm.weight true
longformer.encoder.layer.7.attention.output.layernorm.bias true
longformer.encoder.layer.7.intermediate.dense.weight true
longformer.encoder.layer.7.intermediate.dense.bias true
longformer.encoder.layer.7.output.dense.weight true
longformer.encoder.layer.7.output.dense.bias true
longformer.encoder.layer.7.output.layernorm.weight true
longformer.encoder.layer.7.output.layernorm.bias true
longformer.encoder.layer.8.attention.self.query.weight true
longformer.encoder.layer.8.attention.self.query.bias true
longformer.encoder.layer.8.attention.self.key.weight true
longformer.encoder.layer.8.attention.self.key.bias true
longformer.encoder.layer.8.attention.self.value.weight true
longformer.encoder.layer.8.attention.self.value.bias true
longformer.encoder.layer.8.attention.self.query_global.weight true
longformer.encoder.layer.8.attention.self.query_global.bias true
longformer.encoder.layer.8.attention.self.key_global.weight true
longformer.encoder.layer.8.attention.self.key_global.bias true
longformer.encoder.layer.8.attention.self.value_global.weight true
longformer.encoder.layer.8.attention.self.value_global.bias true
longformer.encoder.layer.8.attention.output.dense.weight true
longformer.encoder.layer.8.attention.output.dense.bias true
longformer.encoder.layer.8.attention.output.layernorm.weight true
longformer.encoder.layer.8.attention.output.layernorm.bias true
longformer.encoder.layer.8.intermediate.dense.weight true
longformer.encoder.layer.8.intermediate.dense.bias true
longformer.encoder.layer.8.output.dense.weight true
longformer.encoder.layer.8.output.dense.bias true
longformer.encoder.layer.8.output.layernorm.weight true
longformer.encoder.layer.8.output.layernorm.bias true
longformer.encoder.layer.9.attention.self.query.weight true
longformer.encoder.layer.9.attention.self.query.bias true
longformer.encoder.layer.9.attention.self.key.weight true
longformer.encoder.layer.9.attention.self.key.bias true
longformer.encoder.layer.9.attention.self.value.weight true
longformer.encoder.layer.9.attention.self.value.bias true
longformer.encoder.layer.9.attention.self.query_global.weight true
longformer.encoder.layer.9.attention.self.query_global.bias true
longformer.encoder.layer.9.attention.self.key_global.weight true
longformer.encoder.layer.9.attention.self.key_global.bias true
longformer.encoder.layer.9.attention.self.value_global.weight true
longformer.encoder.layer.9.attention.self.value_global.bias true
longformer.encoder.layer.9.attention.output.dense.weight true
longformer.encoder.layer.9.attention.output.dense.bias true
longformer.encoder.layer.9.attention.output.layernorm.weight true
longformer.encoder.layer.9.attention.output.layernorm.bias true
longformer.encoder.layer.9.intermediate.dense.weight true
longformer.encoder.layer.9.intermediate.dense.bias true
longformer.encoder.layer.9.output.dense.weight true
longformer.encoder.layer.9.output.dense.bias true
longformer.encoder.layer.9.output.layernorm.weight true
longformer.encoder.layer.9.output.layernorm.bias true
longformer.encoder.layer.10.attention.self.query.weight true
longformer.encoder.layer.10.attention.self.query.bias true
longformer.encoder.layer.10.attention.self.key.weight true
longformer.encoder.layer.10.attention.self.key.bias true
longformer.encoder.layer.10.attention.self.value.weight true
longformer.encoder.layer.10.attention.self.value.bias true
longformer.encoder.layer.10.attention.self.query_global.weight true
longformer.encoder.layer.10.attention.self.query_global.bias true
longformer.encoder.layer.10.attention.self.key_global.weight true
longformer.encoder.layer.10.attention.self.key_global.bias true
longformer.encoder.layer.10.attention.self.value_global.weight true
longformer.encoder.layer.10.attention.self.value_global.bias true
longformer.encoder.layer.10.attention.output.dense.weight true
longformer.encoder.layer.10.attention.output.dense.bias true
longformer.encoder.layer.10.attention.output.layernorm.weight true
longformer.encoder.layer.10.attention.output.layernorm.bias true
longformer.encoder.layer.10.intermediate.dense.weight true
longformer.encoder.layer.10.intermediate.dense.bias true
longformer.encoder.layer.10.output.dense.weight true
longformer.encoder.layer.10.output.dense.bias true
longformer.encoder.layer.10.output.layernorm.weight true
longformer.encoder.layer.10.output.layernorm.bias true
longformer.encoder.layer.11.attention.self.query.weight true
longformer.encoder.layer.11.attention.self.query.bias true
longformer.encoder.layer.11.attention.self.key.weight true
longformer.encoder.layer.11.attention.self.key.bias true
longformer.encoder.layer.11.attention.self.value.weight true
longformer.encoder.layer.11.attention.self.value.bias true
longformer.encoder.layer.11.attention.self.query_global.weight true
longformer.encoder.layer.11.attention.self.query_global.bias true
longformer.encoder.layer.11.attention.self.key_global.weight true
longformer.encoder.layer.11.attention.self.key_global.bias true
longformer.encoder.layer.11.attention.self.value_global.weight true
longformer.encoder.layer.11.attention.self.value_global.bias true
longformer.encoder.layer.11.attention.output.dense.weight true
longformer.encoder.layer.11.attention.output.dense.bias true
longformer.encoder.layer.11.attention.output.layernorm.weight true
longformer.encoder.layer.11.attention.output.layernorm.bias true
longformer.encoder.layer.11.intermediate.dense.weight true
longformer.encoder.layer.11.intermediate.dense.bias true
longformer.encoder.layer.11.output.dense.weight true
longformer.encoder.layer.11.output.dense.bias true
longformer.encoder.layer.11.output.layernorm.weight true
longformer.encoder.layer.11.output.layernorm.bias true
classifier.dense.weight true
classifier.dense.bias true
classifier.out_proj.weight true
classifier.out_proj.bias true

my questions

why for all layers param.requires_grad is true? shouldnt it be false at least for classifier. layers? aren't we training them?
does param.requires_grad==true mean that particular layer is freezed? i am confused with wording requires_grad. does it mean freezed?
if i want to train some of the previous layers as shown here , should i use below code?

    for name, param in model.named_parameters():
         if name.startswith(""...""): # choose whatever you like here
            param.requires_grad = false

considering it takes a lot of time to train, is there specific recommendation regarding layers that i should train? to begin with i am planning to train -

all layers starting with  longformer.encoder.layer.11. and
`classifier.dense.weight` 
`classifier.dense.bias` 
`classifier.out_proj.weight` 
`classifier.out_proj.bias`


do i need add any additional layers such as dropout or is that already taken care by longformerforsequenceclassification.from_pretrained? i am not seeing any dropout layers in the above output and that's why asking the question

#------------------
update 1
how could i know which layers are frozen by using below code from the answer given by @joe32140 ? my guess is everything except last 4 layers from my output shown in my original question gets frozen. but is there any easier way to check?
for param in model.base_model.parameters():
    param.requires_grad = false","['python', 'pytorch', 'classification', 'huggingface-transformers']",71579242,"requires_grad==true means that we will compute the gradient of this tensor, so the default setting is we will train/finetune all layers.
you can only train the output layer by freezing the encoder with

for param in model.base_model.parameters():
    param.requires_grad = false


yes, dropout is used in huggingface output layer implementation. see here: 

as for update 1, yes, base_model refers to layers excluding the output classification head. however, it's actually two layers instead of four where each layer has a weight and a bias tensors.",https://stackoverflow.com/questions/71577525,python,22-03-2022 18:55,337.0,0.0,1.0,True,22-03-2022 23:48,22-03-2022 23:02,Implementation Issues
75686316,changing the output of text_tokens function in r,"i have a question redarding text mining with the corpus package and the function text_tokens(). i want to use the function for stemming and deleting stop words. i have a huge amount of data (almost 1.000.000 comments) where i want to use it for. but i've problems with the output, the function text_tokens produces. so here is a basic example of my data and code:
library(tidyverse)
library(corpus)
library(stopwords)

text <- data.frame(comment_id = 1:2,
                   comment_content = c(""hallo mein name ist aaron"",""vielen lieben dank fï¿½ï¿½r das video""))


tmp <- text_tokens(text$comment_content, 
                   text_filter(stemmer = ""de"",drop = stopwords(""german"")))

my problem now is, that i want a data.frame as output with the comment_id in the first column and word_token in the column. so the output i would like to have looks as followed:
df <- data.e(comment_id = c(1,1,1,2,2,2),
                 comment_tokens = c(""hallo"",""nam"",""aaron"",""lieb"",""dank"",""video""))


i tried different do.calls (cbind/rbind), but they don't give me the result i need. so what is the function i'm looking for, is it map() from the tidyverse?
thank you in advance.
cheers,
aaron","['r', 'nlp', 'stemming']",75686412,"here's an option  using imap_dfr from purrr:
library(corpus)
library(dplyr)
library(purrr)

text <- data.frame(comment_id = 1:2,
                   comment_content = c(""hallo mein name ist aaron"",""vielen lieben dank fï¿½ï¿½r das video""))


tmp <- text_tokens(text$comment_content, 
                   text_filter(stemmer = ""de"",drop = corpus::stopwords_de)) %>% 
  purrr::imap_dfr(function(x, y) {
  tibble(
    comment_id = y,
    comment_tokens = x
  )
})

tmp
#> # a tibble: 6 ï¿½ï¿½ 2
#>   comment_id comment_tokens
#>        <int> <chr>         
#> 1          1 hallo         
#> 2          1 nam           
#> 3          1 aaron         
#> 4          2 lieb          
#> 5          2 dank          
#> 6          2 video

or if you prefer using an anonymous function:
tmp <- text_tokens(text$comment_content, text_filter(stemmer = ""de"",drop = corpus::stopwords_de)) %>% 
  purrr::imap_dfr(~ tibble(comment_id = .y, comment_tokens /code>",https://stackoverflow.com/questions/75686316,r,09-03-2023 14:58,69.0,0.0,1.0,True,10-03-2023 12:52,10-03-2023 12:52,Implementation Issues
76740367,how to resolve the error importerror: cannot import name &#39;generationconfig&#39; from &#39;transformers&#39;,"i am using paperspace for programming. i want to import automodelforcausallmwithvaluehead, ppoconfig, ppotrainer  from trl but i  i encountered an error.  i installed the required libraries such as torch==1.9.0 or 2.0.1,
transformers==4.18.0, and trl. after importing the libraries as shown below:
import pandas as pd
import os
import torch
from transformers import gpt2tokenizer

from trl import automodelforcausallmwithvaluehead, ppoconfig, ppotrainer 

i got the error:
importerror: cannot import name 'generationconfig' from 'transformers' (/usr/local/lib/python3.9/dist-packages/transformers/__init__.py)
please be aware that the code functions correctly when using google colab. however, when running it on paperspace, i encountered an error. it seems that the version of pytorch on paperspace is 1.12.1, which is not suitable for my requirements. i attempted to create a virtual environment to install a different version of pytorch, but unfortunately, the version remains 1.12.1. although i managed to update pytorch to higher versions like 2.0.1, i encountered a conflicting dependency error:
error: pip's dependency resolver does not currently take into account all the packages that are installed. this behavior is the source of the following dependency conflicts.
torchvision 0.13.1+cu116 requires torch==1.12.1, but you have torch 2.0.1 which is incompatible.
torchaudio 0.12.1+cu116 requires torch==1.12.1, but you have torch 2.0.1 which is incompatible.

even with the latest version of pytorch, the error persists, and i am still unable to resolve the issue.","['pytorch', 'huggingface-transformers']",77197021,"it looks like torchvision and torchaudio are not compatible with your current torch version. try to install pytorch's stable version (2.0.1) using this command:
pip3 install torch torchvision torchaudio --index-url 

or generate the desired combination from 
see also my answer to a similar question.",https://stackoverflow.com/questions/76740367,pytorch,21-07-2023 18:22,2938.0,1.0,2.0,True,09-04-2024 02:15,09-04-2024 02:15,Implementation Issues
78382913,how to know which words are encoded with unknown tokens in huggingface berttokenizer?,"i use the following code to count how many % of words are encoded to unknown tokens.
paragraph_chinese = '...' # it is a long paragraph from a text file.
from transformers import autotokenizer, berttokenizer
tokenizer_bart = berttokenizer.from_pretrained(""fnlp/bart-base-chinese"")
encoded_chinese_bart = tokenizer_bart.encode(paragraph_chinese)
unk_token_id_bart = tokenizer_bart.convert_tokens_to_ids([""[unk]""])
len_paragraph_chinese   = len(paragraph_chinese)

unk_token_cnt_chinese_bart   = encoded_chinese_bart.count(unk_token_id_bart[0])
print(""bart unknown token count in chinese paragraph:"", unk_token_cnt_chinese_bart, ""("" + str(unk_token_cnt_chinese_bart * 100 / len_paragraph_chinese) + ""%)"")
print(type(tokenizer_bart))

which prints:
bart unknown token count in chinese paragraph: 1 (0.015938795027095953%)
<class 'transformers.models.bert.tokenization_bert.berttokenizer'>

my question is: i noticed there is one unknown token. how can i know which word causes this unknown token?
p.s. i tried print(encoded_chinese_bart), but it is a list of token ids.
using transformers 4.28.1","['huggingface-transformers', 'huggingface-tokenizers']",78387343,"when you use the berttokenizerfast instead of the ""slow"" version, you will get a batchencoding object that gives you access to several convenient methods that allow you to map a token back to the original string.
the following code uses the token_to_chars method:
from transformers import berttokenizerfast

# just an example
paragraph_chinese = 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ koï¿½ï¿½ka ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ koï¿½ï¿½ka ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½' 

tokenizer_bart = berttokenizerfast.from_pretrained(""fnlp/bart-base-chinese"")
encoded_chinese_bart = tokenizer_bart(paragraph_chinese)
unk_token_id_bart = tokenizer_bart.unk_token_id
len_paragraph_chinese   = len(paragraph_chinese)

unk_token_cnt_chinese_bart   = encoded_chinese_bart.input_ids.count(unk_token_id_bart)
print(f'bart unknown token count in chinese paragraph: {unk_token_cnt_chinese_bart} ({unk_token_cnt_chinese_bart * 100 / len_paragraph_chinese}%)')

#find all indices
unk_indices = [i for i, x in enumerate(encoded_chinese_bart.input_ids) if x == unk_token_id_bart]
for unk_i in unk_indices:
  start, stop = encoded_chinese_bart.token_to_chars(unk_i)
  print(f""at {start}:{stop}: {paragraph_chinese[start:stop]}"")

original:
<ýka
at 17:22: koýýka",https://stackoverflow.com/questions/78382913,huggingface-transformers,25-04-2024 07:22,702.0,3.0,2.0,True,20-07-2024 02:06,25-04-2024 09:11,Implementation Issues
46541255,mitie library for nlp,i am trying to understand how mitie is integrated with rasa. i wanted to know what exactly the mitie file total_word_feature_extractor.dat contain? i dont find any good documentation about this.,"['python', 'neural-network', 'nlp', 'rasa-nlu']",46544157,"if you poke around deep enough in the mitie repo's on github you can find your answer. for example here is a bit of information about what goes into that file.

as for what's inside, yes, it's a variant of word2vec based on the two step cca method from this paper:  i also upgraded it to include something that is similar to the cca method but works on out of sample words by analyzing their morphology to produce a word vector. this significantly improved the results on datasets containing lots of words not in the original dictionary.

as far as how mitie integrates into rasa, it is one of a few backend choices for rasa. it provides a few pipeline components that can do both intent classification and ner. both of which use an svm and use the total_word_feature_extractor.dat to provide the individual word vectors.",https://stackoverflow.com/questions/46541255,python,03-10-2017 09:23,1119.0,1.0,1.0,True,07-03-2025 15:32,07-03-2025 15:32,Implementation Issues
74953426,creating and computing percentage of co-occurrence based on keywords,"i have the following data set:
df <- data.frame (text  = c(""house sky blue"",
                            ""house sky green"",
                            ""house sky red"",
                            ""house sky yellow"",
                            ""house sky green"",
                            ""house sky glue"",
                            ""house sky green""))

i'd like to find the percentage of co-occurrence of some terms of tokens. for example, out of all documents, where can i find the token ""house"" and at the same time how many of them also include the term ""green""?
in out data we have 7 documents that have the term house and 3 out of those 7 p=(100*3/7) also include the term green, it would be so nice to see also what terms or tokens appear within some p threshold along side the token ""house"".
i have used these two functions:
textstat_collocations(tokens)

> textstat_collocations(tokens)
  collocation count count_nested length   lambda        z
1   house sky     7            0      2 5.416100 2.622058
2   sky green     3            0      2 2.456736 1.511653

fun textstat_simil

textstat_simil(dfm(tokens),margin=""features"")

textstat_simil object; method = ""correlation""
       house sky   blue  green    red yellow   glue
house    nan nan    nan    nan    nan    nan    nan
sky      nan nan    nan    nan    nan    nan    nan
blue     nan nan  1.000 -0.354 -0.167 -0.167 -0.167
green    nan nan -0.354  1.000 -0.354 -0.354 -0.354
red      nan nan -0.167 -0.354  1.000 -0.167 -0.167
yellow   nan nan -0.167 -0.354 -0.167  1.000 -0.167
glue     nan nan -0.167 -0.354 -0.167 -0.167  1.000

but they do not seem to give my desired output also i wonder why the correlation btw green and house is nan for the textsats_simil fun
my desired output would show the following info:
feature=""house""
 percentage of co-occurrence 

green = 3/7
blue= 1/7
red = 1/7
yellow = 1/7
glue = 1/7

in the quetda docs i can't seem to find a function that can give me my desired output, although i know there must be a way around since i find this library to be so fast and complete.","['r', 'text-mining', 'quanteda']",74988297,"one way to do this is using the fcm() to get document-level co-occurrences for a target feature.  below, i show how to do this using fcm(), fcm_remove() to remove the target feature, then a loop to get the desired printed output.
library(""quanteda"")
#> package version: 3.2.4
#> unicode version: 14.0
#> icu version: 70.1
#> parallel computing: 10 of 10 threads used.
#> see  for tutorials and examples.

df <- data.frame(text = c(""house sky blue"",
                          ""house sky green"",
                          ""house sky red"",
                          ""house sky yellow"",
                          ""house sky green"",
                          ""house sky glue"",
                          ""house sky green""))
corp <- corpus(df)

coocc_fract <- function(corp, feature) {
   # create a document-level co-occurrence matrix
   fcmat <- fcm(dfm(tokens(corp), tolower = false), context = ""document"")
   # select for the given feature
   fcmat <- fcm_remove(fcmat, feature)
   cat(""feature=\"""", feature, ""\""\n"", sep = """")
   cat("" percentage of co-occurrence\n\n"")
   for (f in featnames(fcmat)) {
       # skip zeroes
       freq <- as.character(fcmat[1, f])
       if (freq != ""0"") {
           cat(f, "" = "", as.character(fcmat[1, f]), ""/"", ndoc(corp), 
               ""\n"", sep = """")
       }
   }
}

this produces this output:
coocc_fract(corp, feature = ""house"")
#> feature=""house""
#>  percentage of co-occurrence
#> 
#> blue = 1/7
#> green = 3/7
#> red = 1/7
#> yellow = 1/7
#> glue = 1/7

created on 2023-01-02 with reprex v2.0.2",https://stackoverflow.com/questions/74953426,r,29-12-2022 16:06,157.0,0.0,2.0,True,19-05-2024 22:53,19-05-2024 22:53,Implementation Issues
73802610,"is there an api for the azure language studio, especially for custom question answering?","i am currently working with the language studio by azure. for now, i am wondering if there is an api for this, especially for the custom question answering that i can work with on my current project.","['azure', 'azure-cognitive-services', 'nlp-question-answering', 'language-studio']",73803199,"language studio is just a demonstrator. everything is made thanks to apis.
for example for question answering:",https://stackoverflow.com/questions/73802610,azure,21-09-2022 14:34,1036.0,0.0,1.0,True,21-01-2023 20:14,21-01-2023 20:14,Uncategorized
61938628,convert from prodigy&#39;s jsonl format for labeled ner to spacy&#39;s training format?,"i am new to prodigy and spacy as well as cli coding. i'd like to use prodigy to label my data for an ner model, and then use spacy in python to create models. 
prodigy outputs in sqlite format. spacy takes in this other kind of format, not sure what to call it: 
train_data = [
    (
        ""horses are too tall and they pretend to care about your feelings"",
        {""entities"": [(0, 6, label)]},
    ),
    (""do they bite?"", {""entities"": []}),
    (
        ""horses are too tall and they pretend to care about your feelings"",
        {""entities"": [(0, 6, label)]},
    ),
    (""horses pretend to care about your feelings"", {""entities"": [(0, 6, label)]}),
    (
        ""they pretend to care about your feelings, those horses"",
        {""entities"": [(48, 54, label)]},
    ),
    (""horses?"", {""entities"": [(0, 6, label)]}),
]

how can i convert from one to the other? it seems like this should be easy, but i cannot find it anywhere. 
i have no problem loading in the dataset, just converting.","['sqlite', 'nlp', 'spacy', 'named-entity-recognition', 'prodigy']",61941227,prodigy should export this training format with data-to-spacy as of version 1.9:,https://stackoverflow.com/questions/61938628,sqlite,21-05-2020 16:00,2456.0,0.0,1.0,True,14-04-2022 18:27,14-04-2022 18:27,Data Wrangling
70720126,oserror: e053 could not read config.cfg spacy on colab,"i want to use spacytextblob in google colab, when i use the formal installation, i got the below error.
oserror: [e053] could not read config.cfg from /usr/local/lib/python3.7/dist-packages/en_core_web_sm/en_core_web_sm-2.2.5/config.cfg

what i do, first run this block:
import spacy
!pip install spacytextblob

then based on the output i restart the runtime. then run this code:
from spacytextblob.spacytextblob import spacytextblob
nlp = spacy.load('en_core_web_sm')
nlp.add_pipe(""spacytextblob"")

but at the first line, i got the error.
if i don't install spacytextblob the default version of  spacy on colab it's 2.2.4 but after installation the  spacy version it's 3.2.1
there is an answer here for the same error but i can't use it.
i guess the problem it's from the spacy version which changed after installing the spacytextblob but how can i fix it?","['python', 'spacy']",70720293,"i solve the problem by using this  installation guide
!pip install spacytextblob
!python -m textblob.download_corpora
!python -m spacy download en_core_web_sm",https://stackoverflow.com/questions/70720126,python,15-01-2022 09:05,3549.0,2.0,1.0,True,15-01-2022 11:14,15-01-2022 10:56,Tool Setup/Errors
76451997,langchain: reduce size of tokens being passed to openai,"i am using langchain to create embeddings and then ask a question to those embeddings like so:
embeddings: openaiembeddings = openaiembeddings(disallowed_special=())
db = deeplake(
    dataset_path=deeplake_url,
    read_only=true,
    embedding_function=embeddings,
)
retriever: vectorstoreretriever = db.as_retriever()
model = chatopenai(model_name=""gpt-3.5-turbo"") 
qa = conversationalretrievalchain.from_llm(model, retriever=retriever)
result = qa({""question"": question, ""chat_history"": chat_history})

but i am getting the following error:
file ""/xxxxx/openai/api_requestor.py"", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.invalidrequesterror: this model's maximum context length is 4097 tokens. however, your messages resulted in 13918 tokens. please reduce the length of the messages.

the chat_history is empty and the question is quite small.
how can i reduce the size of tokens being passed to openai?
i'm assuming the response from the embeddings is too large being passed to openai. it might be easy enough to just figure out how to truncate the data being sent to openai.","['openai-api', 'langchain', 'py-langchain', 'deeplake', 'activeloop']",76452214,"summary
when you initiate the conversationalretrievalchain object, pass in a max_tokens_limit amount.
qa = conversationalretrievalchain.from_llm(
        model, retriever=retriever, max_tokens_limit=4000
    )

this will automatically truncate the tokens when asking openai / your llm.
longer explainer
in the base.py of conversationalretrievalchain there is a function that is called when asking your question to deeplake/openai:
    def _get_docs(self, question: str, inputs: dict[str, any]) -> list[document]:
        docs = self.retriever.get_relevant_documents(question)
        return self._reduce_tokens_below_limit(docs)

which reads from the deeplake vector database, and adds that as context to your doc's text that you upload to openai.
the _reduce_tokens_below_limit reads from the class instance variable max_tokens_limit to truncate the size of the input docs.",https://stackoverflow.com/questions/76451997,openai-api,11-06-2023 18:41,20988.0,3.0,2.0,True,15-09-2023 13:47,11-06-2023 19:03,Conceptual Questions
75152760,removing rows that contain above a certain share of upper-case letters in r,"i have a large dataframe that consists of company identifiers and extracted phrases from newspapers. it is very messy, and i want to clean it by conditional row removing.

for this i want to remove rows that have more then 50% upper-case letters.
i have found this code from a post which will remove me rows with all upper-case letters:
data <- data[!grepl(""^[a-z]+(?:[ -][a-z]+)*$"", data$text), ]

how can i express it as a share of the total word or letter count?","['r', 'string', 'nlp', 'grepl']",75153069,"you could do this with regular expressions, but the stringi function stri_count_charclass provide a highly optimized version for detecting categories of characters. the package manual documents the list of unicode general categories, here we use string l for all letters, and lu for uppercase letters.
something like this should accomplish what you need:
library(stringi)

data <- data.frame(text = c(""foo"",
                            ""bar"",
                            ""baz""))

data[which(stri_count_charclass(data[[""text""]],""[\\p{lu}]"") / stri_count_charclass(data[[""text""]],""[\\p{l}]"") < 0.5),]
# [1] ""foo""

one note: i updated my answer here since i failed to point out a powerful feature of stringi in my original response. my instinctive reaction was to use [a-z] and [a-z] to signify lower and upper case characters, respectively. however, using unicode general categories allows the solution to work well for non-ascii characters as well.
x = c(""foo"",
      ""bar"",
      ""baz"",
      ""ï¿½ï¿½ï¿½oo"",
      ""ï¿½ï¿½ï¿½ï¿½ï¿½r"",
      ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"")
stri_count_charclass(x,""[a-z]"")/stri_count_charclass(x,""[[a-z][a-z]]"")
[1] 0.3333333 0.6666667 1.0000000 0.0000000 0.0000000       nan

stri_count_charclass(x,""[\\p{lu}]"")/stri_count_charclass(x,""[\\p{l}]"")
[1] 0.3333333 0.6666667 1.0000000 0.3333333 0.",https://stackoverflow.com/questions/75152760,r,17-01-2023 22:13,43.0,2.0,1.0,True,19-01-2023 14:03,19-01-2023 14:03,Implementation Issues
70426487,how to find a pattern inside a pattern when start and end is known?,"i have a pattern that has a starting and ending pattern like:
start = '\n\\[\n'
end = '\n\\]\n'

my string is:
'the above mentioned formal formula is\nthat of\n\\[\n\\oplus \\bigoplus_{(5)} \\widehat{c_{(5)}} a_{5} g(2)\n\\]\na. tobacco\nb. tulip\nc. soybean\nd. sunhemp'

i want to find:
\n\\oplus \\bigoplus_{(5)} \\widehat{c_{(5)}} a_{5} g(2)'

if i use:
re.findall(r'\s*\\+\n\\[\n(.*?)\\+\n\\]\n', mystring)

r'\s*\\+\[(.*?)\\+\]' # did not work either

then it gives me an empty result. what am  i doing wrong here?","['python', 'nlp', 'python-re']",70426840,"this works for me:
mystring = 'the above mentioned formal formula is\nthat of\n\\[\n\\oplus \\bigoplus_{(5)} \\widehat{c_{(5)}} a_{5} g(2)\n\\]\na. tobacco\nb. tulip\nc. soybean\nd. sunhemp'

expected_result = '\n\\oplus \\bigoplus_{(5)} \\widehat{c_{(5)}} a_{5} g(2)'

import codecs
import re

matches = re.findall(r'\\n\\\\\[(\\n.*)\\n\\\\\]\\n', repr(mystring))

results = [codecs.decode(match, 'unicode_escape') for match in matches]

results
['\n\\oplus \\bigoplus_{(5)} \\widehat{c_{(5)}} a_{5} g(2)']

results[0] == expected_result
true",https://stackoverflow.com/questions/70426487,python,20-12-2021 18:33,70.0,1.0,2.0,True,21-12-2021 08:13,20-12-2021 18:42,Implementation Issues
71496028,valueerror: class encoding field is specified without a type,"dim_red = truncatedsvd(n_components=2)
data_red = dim_red.fit_transform(tfidf)

scatter = alt.chart(data_red,title=""dimensionality reduction"",height=400).mark_circle().encode(
    x='principal component 1', y='principal component 2', color=alt.color(
        'class', scale=alt.scale(scheme='blues')),tooltip=[""class""]).interactive()

st.altair_chart(scatter)`","['python', 'nlp', 'altair']",71499667,"this error will generally arise when the encoding names (here 'principal component 1' and/or 'principal component 2') do not match the names of any columns in the dataframe passed to the chart. check the names of the columns in the dataframe, and make sure you're reproducing them correctly.",https://stackoverflow.com/questions/71496028,python,16-03-2022 11:08,1165.0,0.0,1.0,True,24-11-2022 12:40,16-03-2022 13:30,Tool Setup/Errors
63413414,is there a way to get the location of the substring from which a certain token has been produced in bert?,"i am feeding sentences to a bert model (hugging face library). these sentences get tokenized with a pretrained tokenizer. i know that you can use the decode function to go back from tokens to strings.
string = tokenizer.decode(...)

however, the reconstruction is not perfect. if you use an uncased pretrained model, the uppercase letters get lost. also, if the tokenizer splits a word into 2 tokens, the second token will start with '##'. for example, the word 'coronavirus' gets split into 2 tokens: 'corona' and '##virus'.
so my question is: is there a way to get the indices of the substring from which every token is created?
for example, take the string ""tokyo to report nearly 370 new coronavirus cases, setting new single-day record"". the 9th token is the token corresponding to 'virus'.
['[cls]', 'tokyo', 'to', 'report', 'nearly', '370', 'new', 'corona', '##virus', 'cases', ',', 'setting', 'new', 'single', '-', 'day', 'record', '[sep]']

i want something that tells me that the token '##virus' comes from the 'virus' substring in the original string, which is located between the indices 37 and 41 of the original string.
sentence = ""tokyo to report nearly 370 new coronavirus cases, setting new single-day record""
print(sentence[37:42]) # --> outputs 'virus","['tokenize', 'bert-language-model', 'huggingface-transformers', 'huggingface-tokenizers']",63422347,"as far as i know their is no built-in method for that, but you can create one by yourself:
import re
from transformers import berttokenizer

tokenizer = berttokenizer.from_pretrained('bert-base-uncased')

sentence = ""tokyo to report nearly 370 new coronavirus cases, setting new single-day record""

b = []
b.append(([101],))
for m in re.finditer(r'\s+', sentence):
  w = m.group(0)
  t = (tokenizer.encode(w, add_special_tokens=false), (m.start(), m.end()-1))

  b.append(t)

b.append(([102],))

b

output:
[([101],),
 ([5522], (0, 4)),
 ([2000], (6, 7)),
 ([3189], (9, 14)),
 ([3053], (16, 21)),
 ([16444], (23, 25)),
 ([2047], (27, 29)),
 ([21887, 23350], (31, 41)),
 ([3572, 1010], (43, 48)),
 ([4292], (50, 56)),
 ([2047], (58, 60)),
 ([2309, 1011, 2154], (62, 71)),
 ([2501], (73, 78)),
 ([102],)]",https://stackoverflow.com/questions/63413414,tokenize,14-08-2020 13:09,3211.0,2.0,2.0,True,05-12-2021 14:04,14-08-2020 13:28,Implementation Issues
71548193,how to use stemming algorithm for a list of words in python,"i have a word list:
'aws', 
'jquery', 
'jquery', 
'sliding', 
'jquery', 
'jquery', 
'manipulating', 
'us!'

i removed common words and need to apply stemming to make the word list more clear.
my code to remove common words:
raw2 = second_headers corpus = common_word_corpus  #my personal word corpus added here

corpus = [w.lower() for w in corpus]  
processed_h2_tag = [w for w in raw2.split(' ') if w.lower() not in corpus] 

print(processed_h2_tag)","['python', 'nlp']",71548298,"how about this?
# download wordnet
import nltk
nltk.download('wordnet')

# import these modules
from nltk.stem import wordnetlemmatizer
from nltk.corpus import wordnet 
nltk.download('wordnet')

lemmatizer = wordnetlemmatizer()

# choose some words to be stemmed
words = ['aws', 
'jquery', 
'jquery', 
'sliding', 
'jquery', 
'jquery', 
'manipulating', 
'manipulateing', 
'manipulate', 
'us!']
 
for w in words:
    print(w, "" : "", lemmatizer.lemmatize(w.lower(), pos=wordnet.verb))


output:
aws  :  aws
jquery  :  jquery
jquery  :  jquery
sliding  :  slide
jquery  :  jquery
jquery  :  jquery
manipulating  :  manipulate
manipulateing  :  manipulate
manipulate  :  manipulate
us!  :  us!",https://stackoverflow.com/questions/71548193,python,20-03-2022 15:41,48.0,0.0,1.0,True,01-04-2022 16:42,21-03-2022 08:23,Implementation Issues
66193575,why is the vocab size of byte level bpe smaller than unicode&#39;s vocab size?,"i recently read gpt2 and the paper says:

this would result in a base vocabulary of over 130,000 before any multi-symbol tokens are added.  this is prohibitively large compared to the 32,000 to 64,000 token vocabularies often used with bpe. in contrast, a byte-level version of bpe only requires a base vocabulary of size 256.

i really don't understand the words. the number of characters that unicode represents is 130k but how can this be reduced to 256? where's the rest of approximately 129k characters? what am i missing? does byte-level bpe allow duplicating of representation between different characters?
i don't understand the logic. below are my questions:

why the size of vocab is reduced? (from 130k to 256)
what's the logic of the bbpe (byte-level bpe)?


detail question
thank you for your answer but i really don't get it. let's say we have 130k unique characters. what we want (and bbpe do) is to reduce this basic (unique) vocabulary. each unicode character can be converted 1 to 4 bytes by utilizing utf-8 encoding. the original paper of bbpe says (neural machine translation with byte-level subwords):

representing text at the level of bytes and using the 256 bytes set as vocabulary is a potential solution to this issue.

each byte can represent 256 characters (bits, 2^8), we only need 2^17 (131072) bits for representing the unique unicode characters. in this case, where did the 256 bytes in the original paper come from? i don't know both the logic and how to derive this result.
i arrange my questions again, more detail:

how does bbpe work?
why the size of vocab is reduced? (from 130k to 256 bytes)

anyway, we always need 130k space for a vocab. what's the difference between representing unique characters as unicode and bytes?



since i have little knowledge of computer architecture and programming, please let me know if there's something i missed.
sincerely, thank you.","['unicode', 'utf-8', 'nlp']",66193820,"unicode code points are integers in the range 0..1,114,112, of which roughly 130k are in use at the moment. every unicode code point corresponds to a character, like ""a"" or ""ï¿½ï¿½"" or ""ï¿½ï¿½ï¿½"", which is handy to work with in many cases (but there are a lot of complicated details, eg. combining marks).
when you save text data to a file, you use one of the utfs (utf-8, utf-16, utf-32) to convert code points (integers) to bytes. for utf-8 (the most popular file encoding), each character is represented by 1, 2, 3, or 4 bytes (there's some inner logic to discriminate single- and multi-byte characters).
so when the base vocabulary are bytes, this means that rare characters will be encoded with multiple bpe segments.
example
let's consider a short example sentence like ï¿½ï¿½ï¿½thatï¿½ï¿½ï¿½s great ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½.
with a base vocabulary of all unicode characters, the bpe model starts off with something like this:
t      5   2019
s      73
       20
g      67
r      72
e      65
a      61
t      74
       20
ï¿½ï¿½ï¿½ï¿½   1f44d

(the first column is the character, the second its codepoint in hexadecimal notation.)
if you first encode this sentence with utf-8, then this sequence of bytes is fed to bpe instead:
t      54
h      68
a      61
t      74
ï¿½ï¿½ï¿½      e2
ï¿½ï¿½ï¿½      80
ï¿½ï¿½ï¿½      99
s      73
       20
g      67
r      72
e      65
a      61
t      74
       20
ï¿½ï¿½ï¿½      f0
ï¿½ï¿½ï¿½      9f
ï¿½ï¿½ï¿½      91
ï¿½ï¿½ï¿½      8d

the typographic apostrophe ""ï¿½ï¿½ï¿½"" and the thumbs-up emoji are represented by multiple bytes.
with either input, the bpe segmentation (after training) may end with something like this:
th|at|ï¿½ï¿½ï¿½s|great|ï¿½ï¿½ï¿½ï¿½

(this is a hypothetical segmentation, but it's possible that capitalised ï¿½ï¿½ï¿½thatï¿½ï¿½ï¿½ is too rare to be represented as a single segment.)
ýýs, only one merge step is required for code-point input, but three steps for byte input.
with byte input, the bpe segmentation is likely to end up with sub-character segments for rare characters.
the down-stream language model will have to learn to deal with that kind of input.",https://stackoverflow.com/questions/66193575,unicode,14-02-2021 08:17,4747.0,7.0,4.0,True,17-01-2025 22:44,15-02-2021 16:46,Implementation Issues
69845992,"date pattern for whatsapp chat text file that has 24 hour format, split() error: too many values to unpack","i have a whatsapp chat text file from ios, where it has a 24hour format

[07/04/2018, 14:11:22] mike: ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½

i want to create a dataframe from the text. i've tried different date patterns e.g.
def datetimeios(s):
    pattern = '^\[([0-9]+)(\/)([0-9]+)(\/)([0-9]+), ([0-9]+):([0-9]+):([0-9])?\] ' #(am|pm|am|pm)?
    result = re.match(pattern, s)
    if result:
        return true
    return false 

but they're not working. if i add + [:([0-9]+) ]on the seconds i get split() error: too many values to"" rel=""nofollow noreferrer"">","['python', 'pandas', 'date', 'nlp']",69846118,"try with:
x = re.search(r""^\[([0-9]+)(\/)([0-9]+)(\/)([0-9]+), ([0-9]+):([0-9]+):([0-9]+)]"", s)

print(x.group())

your output:
>>> [07/04/2018, 14:11:22]

have a look at this example.",https://stackoverflow.com/questions/69845992,python,04-11-2021 21:22,584.0,0.0,1.0,True,05-11-2021 08:04,05-11-2021 08:04,Tool Setup/Errors
76784484,cannot import name amazonkendraretriever from langchain,"i cannot import 'amazonkendraretriever' from langchain due to the following errors

---------------------------------------------------------------------------
importerror                               traceback (most recent call last)
cell in[3], line 1
----> 1 from langchain.retrievers import amazonkendraretriever

importerror: cannot import name 'amazonkendraretriever' from 'langchain.retrievers' (/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/langchain/retrievers/__init__.py)

my python version is 3.10.10. i have tried fixing it by pip installing langchain==0.0.201 based on some discussion on a github thread, but that doesn't fix the issue. kindly assist.","['langchain', 'amazon-kendra']",76839928,i changed the kernel from conda_pytorchp310 to conda_python3 and that fixed the issue. running the code from a sagemaker jupyterlab notebook.,https://stackoverflow.com/questions/76784484,langchain,28-07-2023 02:22,705.0,1.0,1.0,True,03-09-2023 23:48,03-09-2023 23:48,Implementation Issues
79532570,"use of training, validation and test set in huggingface seq2seqtrainer","i have the following dataset, which has 3 splits (train, validation and test). the data are parallel corpus of 2 languages.
datasetdict({
    train: dataset({
        features: ['translation'],
        num_rows: 109942
    })
    validation: dataset({
        features: ['translation'],
        num_rows: 6545
    })
    test: dataset({
        features: ['translation'],
        num_rows: 13743
    })
})

for my seq2seqtrainer, i supply the dataset as follows:
trainer = seq2seqtrainer(
    model = model,
    args = training_args,
    train_dataset = tokenized_dataset['train'],
    eval_dataset = tokenized_dataset['validation'],
    tokenizer = tokenizer,
    data_collator = data_collator,
    compute_metrics = compute_metrics,
)

is it correct to put the validation split in eval_dataset? in the documentation, it says:

the dataset to use for evaluation. if it is a dataset, columns not accepted by the model.forward() method are automatically removed. if it is a dictionary, it will evaluate on each dataset prepending the dictionary key to the metric name.

or should i put the test split in eval_dataset? in either way, is that true that one of the splits is not used?","['python', 'machine-learning', 'dataset', 'huggingface-transformers']",79533126,"i am going to focus on the code side here. for a deeper theoretical explanation of why we need (or should have) training, validation and test set, see what is the difference between test set and validation set?.
for training, using the validation set is correct, the way you already do:
trainer = seq2seqtrainer(
    model = model,
    args = training_args,
    train_dataset = tokenized_dataset['train'],
    eval_dataset = tokenized_dataset['validation'],
    tokenizer = tokenizer,
    data_collator = data_collator,
    compute_metrics = compute_metrics,
)

after training, you can use .predict() or .evaluate(), with your test set.
if you want only the metrics, and not the outputs, you can use .evaluate():
metrics = trainer.evaluate(tokenized_dataset['test'])

if you want the outputs as well as the metrics (or maybe just the outputs), you can use .predict():
preds = trainer.predict(tokenized_dataset['test'])
print(preds.predictions)
print(preds.metrics)",https://stackoverflow.com/questions/79532570,python,25-03-2025 02:22,42.0,0.0,1.0,True,25-03-2025 15:18,25-03-2025 15:18,Conceptual Questions
36064946,text summarization in r language,"i have long text file using help of r language i want to summarize text in at least 10 to 20 line or in small sentences.
how to summarize text in at least 10 line with r language ?","['r', 'text', 'text-mining', 'summarization']",36065535,"you may try this (from the lsafun package):
genericsummary(d,k=1)

whereby 'd' specifies your text document and 'k' the number of sentences to be used in the summary. (further modifications are shown in the package documentation).
for more information:",https://stackoverflow.com/questions/36064946,r,17-03-2016 15:26,10453.0,4.0,4.0,True,31-10-2023 16:24,17-03-2016 16:23,Implementation Issues
76041048,"&quot;invalid input image - format must be in [&#39;rgba&#39;, &#39;la&#39;, &#39;l&#39;], got rgb.&quot; ios swift","i am using the openai ""create image edit"" api link. my mask image is maskimage.png.
after calling the api, when the response comes, i have got the error:

invalid input image - format must be in ['rgba', 'la', 'l'], got rgb.""

how can i change the png image format as ['rgba', 'la', 'l'] in ios swift?
i have found a solution in javascript, how to validate an image before sending to dall e api with front-end javascript. but i can not find a solution in ios.","['ios', 'swift', 'openai-api']",76056584,"all you have to do is add opacity/alpha to the uiimage/pngdata.
extension uiimage {
    var hasalpha: bool {
        guard let alphainfo = self.cgimage?.alphainfo else {return false}
        return alphainfo != cgimagealphainfo.none &&
            alphainfo != cgimagealphainfo.noneskipfirst &&
            alphainfo != cgimagealphainfo.noneskiplast
    }
}

is a quick way to check if a uiimage has alpha.
you can add alpha with
extension uiimage {

    func imagewithalpha(alpha: cgfloat) throws -> uiimage {
        uigraphicsbeginimagecontextwithoptions(size, false, scale)
        draw(at: cgpointzero, blendmode: .normal, alpha: alpha)
        guard let newimage = uigraphicsgetimagefromcurrentimagecontext() else{
            uigraphicsendimagecontext()
            throw imageerror.unabletoaddaplhatoimage
        }
        uigraphicsendimagecontext()
        return newimage
    }
    enum imageerror: localizederror{
        case unabletoaddaplhatoimage
    }
}

1 being a dark image and 0 being a transparent image, openai is looking for 0.
the method above will change the entire uiimage, if you use this the entire image will be replaced.
a mask will only have sections of an image that have a value of 0.
the png you are showing doesn't show any pixels as zero.
in short, with the code above if image.hasalpha == false you will get the error you are talking about when you submit.
if if image.hasalpha == true then it is ok with openai.
in javascript you can use jimp to simplify the whole thing, you can do all the checks in just a few lines.
dall e api error: ""invalid input image - format must be in ['rgba', 'la', 'l'], got rgb.""",https://stackoverflow.com/questions/76041048,ios,18-04-2023 04:13,1434.0,-1.0,1.0,True,19-04-2023 16:11,18-04-2023 23:11,Implementation Issues
75943880,what is the most efficient way to identify text similarity between items in large lists of strings in python?,"the following piece of code achieves the results i'm trying to achieve. there is a list of strings called 'lemmas' that contains the accepted forms of a specific class of words. the other list, called 'forms' contains a lot of spelling variations of words found in a large amount of texts from different periods and different dialects of a specific language. for each one of the words in 'forms', i want to get the string in 'lemmas' that is the closest match.
the script, as i said, seems to work well with some test lists i've constructed. the problem i have, though, is that when i use the real lists, which are rather large, it takes forever to produce the results. in fact, i have had to stop the execution of the program because it was taking already more than two hours and the computer was becoming very slow and i couldn't do anything else.
what could i do to make this more efficient? how would i have to modify the code using other python tools or libraries to make this faster? thanks in advance.
    import textdistance
    from textdistance import hamming
    from textdistance import cosine
    from textdistance import jaro_winkler
    import heapq
    
    # 'lemmas' is a list containing a huge amount of words, basically dictionary entries
    # 'forms' is a huge list of spelling variations of words found in hundreds of texts
    
    distances = {}
    
    processed_pairs = set() # keep track of processed pairs
    for lemma in lemmas:
        if lemma is none:
            continue
        lemma_lower = lemma.lower()
        for form in forms:
            if form is none:
                continue        
            form_lower = form.lower()
            pair = (lemma_lower, form_lower) # create a tuple with the lowercase pair
            if pair not in processed_pairs: # check if the pair has been processed before
                processed_pairs.add(pair)
                if textdistance.hamming.normalized_similarity(lemma_lower, form_lower) > 0.34 and textdistance.jaro_winkler(lemma_lower, form_lower) > 0.7 and textdistance.cosine(lemma_lower, form_lower) > 0.5:             
                    dist = hamming.normalized_similarity(lemma_lower, form_lower)
                    distances.setdefault(form_lower, []).append((dist, lemma_lower))
    
    # find the closest pairs
    closest_pairs = {}
    for form, dist_lemmas in distances.items():
        closest_pairs[form] = heapq.nsmallest(2, dist_lemmas)
    
    with open(root / 'potential_lemmas.txt', 'w') as f:
        for form, pairs in closest_pairs.items():
            for dist, lemma in pairs:
                f.write(f""{form} ï¿½ï¿½ï¿½  {lemma}: {dist}\n"")
             


edit:
in the end, the solution that worked the best was an integration of @kyle f hartzenberg's proposal with @jamie_b suggestion of using joblib to parallelize (see comments after code, though):
from itertools import zip_longest
from bisect import insort
from joblib import parallel, delayed
import line_pr

profile = line_profiler.lineprofiler()

emmas = ['gran', 'vermell', 'groc', 'atens', 'do', 'done', 'purpose', 'can', 'be', 'use', 'for', 'cannon', 'amuse', 'useful', 'user', 'become', 'downtown', 'develop', 'fulminate', 'deduce', 'de', 'bezant']

forms = ['preriarenos', 'marinara', 'grand', 'gran', 'grans', 'grands', 'grandeses', 'grandullons', 'grand', 'grandissisimus', 'gran', 'grans', 'grands', 'grandeses', 'grandullons', 'grandullon', 'grandullones', 'uermell', 'uermells', 'vermell', 'vermells', 'vermella', 'vermelles', 'varmellï¿½ï¿½ssimes', 'uarmellï¿½ï¿½ssimes', 'uermellï¿½ï¿½ssimes', 'uarnellï¿½ï¿½ssimes', 'varmellï¿½ï¿½ssima', 'uermella', 'uarmella', 'uarnella', 'varnella', 'uarnellas', 'varnellas', 'varmella', 'uermelles', 'grog', 'grogues', 'done', 'done', 'doing', 'purposeful', 'canonical', 'becareful', 'being', 'berate', 'best', 'bezant', 'full', 'fulmination', 'predict', 'downgrade', 'down', 'developing', 'deduct', 'deducing']

distances = {}

@delayed
def calc_distances(form, lemmas_low):
   
    for lemma in lemmas_low:
        char_matches = [c1 != c2 for c1, c2 in zip_longest(lemma, form)]
        dist = 1 - (sum(char_matches)/len(char_matches))
        if dist > 0.25:
            insort(form_distances, (dist, lemma))
    return (form, form_distances)

@profile
def profile_distance_calcs():
    lemmas_low = [lemma.lower() for lemma in lemmas]
    forms_low = [form.lower() for form in forms]
    results = parallel(n_jobs=-1, prefer=""threads"")(calc_distances(form, lemmas_low) for form in forms_low)
    for form, form_distances in results:
        distances[form] = form_distances

    with open(""potential_lemmas_hamming-like.txt"", ""w"") as f:
        for form, form_distances in distances.items():
            for dist, lemma in reversed(form_distances[-2:]):
                f.write(f""{form} ï¿½ï¿½ï¿½  {lemma}: {dist}\n"")

if __name__ == ""__main__"":
    profile_distance_calcs()
    profile.print_stats()


this was a huge improvemen everything i had tried before. besides the test with the short lists in the example, i ran it with the actual lists containing around 190,000 strings and the processing time was 118 minutes. while i'm pretty sure this could be improved (one might look for ways to do it using some kind of vectorization - someone suggested using arrays from numpy or ai oriented libraries), for the time being, this is quite manageable. there is still a problem that doesn't have to do with efficiency.
i mention this in my comment to @jqurious below but i will explain it here in more detail. running the script above with the test list, one gets results like the following:
berate ï¿½ï¿½ï¿½  bezant: 0.5
berate ï¿½ï¿½ï¿½  become: 0.5

from a linguistic point of view, any english speaker would know that these pairs of words are not related (ok, unless you know about the history of the language and know that be- used to be a productive prefix). what i'm trying to do with this script is to determin be the appropriate lemma (the dictionary form or representative word) for all the variants of a particular word found in the texts of a corpus.
this is a diachronic corpus containing many texts from many different authors and from many different dialects of a language writen over a period of more than 5 centuries. a 'u' could often be used instead of 'v' or a 'y' instead of an 'i'. an 'h' can als be often be missing from a word that is spelt with 'h' even in the same text by the same author. the variation is huge and yet even a modern speaker of the languate can usually detect whether the words are related quite easily. of course, the speaker of the language is knowledgeable about the word structure and the morphology and so can immediately see that, for instance, 'uermellï¿½ï¿½ssima' is related to 'vermell' despite the fact that a lot of characters are different.
using kyle's suggestion with the actual lists, i got results like the following:
beato ï¿½ï¿½ï¿½  beat: 0.8
beatriï¿½iu: 0.5714285714285714
beatriï¿½ï¿½ ï¿½ï¿½ï¿½  teatral: 0.5714285714285714
beatte ï¿½ï¿½ï¿½  beats: 0.6666666666666667
beatus ï¿½ï¿½ï¿½  certus: 0.6666666666666667
beatï¿½ï¿½ssim ï¿½ï¿½ï¿½  nequï¿½ï¿½ssim: 0.6666666666666667
beatï¿½ï¿½ssim ï¿½ï¿½ï¿½  gravï¿½ï¿½ssim: 0.6666666666666667

even if you don't know the language (medieval catalan in case anybody is interested), you can see how this is very wrong (using other algorithms like the levenshtein or the cosine distance it is just hopeless). the lemmas 'beat' or 'beats' should ideally be the ones selected as being the ""closest"" in all these cases. yet the algorithm does what it does.
perhaps i haven't looked hard enough, but with all the work in nlp, i'm surprised there aren't other algorithms that could do better in this kind of scenario. i know this deviates a little bit from the main point in the original question but if anybody can give me some","['python', 'text', 'nlp', 'cosine-similarity', 'hamming-distance']",75946347,"the following solution is based on your original code (hamming distance) which offers an (almost) order of magnitude speed-up (~89.41%), averaged across five runs of each, as measured by line-profiler. using this solution as a base for parallel processing may get you closer to the total processing times you are after.
to use line-profiler, pip install line-profiler and then run kernprof -l -v test.py after adding @profile and calling the function to be profiled from __main__.
from itertools import zip_longest
from bisect import insort

lemmas = [""do"", ""done"", ""purpose"", ""can"", ""be"", ""use"", ""for"", ""cannon"", ""amuse"", ""useful"", ""user"", ""become"", ""downtown"", ""develop"", ""fulminate"", ""deduce"", ""de"", ""bezant""]
forms = [""done"", ""done"", ""doing"", ""purposeful"", ""canonical"", ""becareful"", ""being"", ""berate"", ""best"", ""bezant"", ""full"", ""fulmination"", ""predict"", ""downgrade"", ""down"", ""developing"", ""deduct"", ""deducing""]
distances = {}

@profile
def profile_distance_calcs():
    lemmas_low = [lemma.lower() for lemma in lemmas]
    forms_low = [form.lower() for form in forms]
    for form in forms_low:
        form_distances = []
        for lemma in lemmas_low:
            char_matches = [c1 != c2 for c1, c2 in zip_longest(lemma, form)]
            dist = 1 - (sum(char_matches)/len(char_matches))
            if dist > 0.25:
                insort(form_distances, (dist, lemma))
        distances[form] = form_distances

    with open(""potential_lemmas_hamming.txt"", ""w"") as f:
        for form, form_distances in distances.items():
            for dist, lemma in reversed(form_distances[-2:]):
                f.write(f""{form} ï¿½ï¿½ï¿½  {lemma}: {dist}\n"")

if __name__ == ""__main__"":
    profile_distance_calcs()

from the time profile breakdown below (total time: 0.00122992 s), you can get an idea of where the slow-downs are coming from.
the main culprit is (obviously) the distance computation which is why i switched the textdistance.hamming.normalized_similarity for a much more efficient (barebones) manual calculation of the same thing based on the textdistancehamming and hamming.normalized_similarity source code. i also believe using bisect.insort and maintaining a sorted list while inserting is faster than inserting all elements and then running heapq.nlargest.
line #      hits         time  per hit   % time  line contents
==============================================================
    10                                           @profile
    11                                           def profile_distance_calcs():
    12         1          7.9      7.9      0.6      lemmas_low = [lemma.lower() for lemma in lemmas]
    13         1          7.0      7.0      0.6      forms_low = [form.lower() for form in forms]
    14        18          1.8      0.1      0.1      for form in forms_low:
    15        18          2.0      0.1      0.2          form_distances = []
    16       324         33.4      0.1      2.7          for lemma in lemmas_low:
    17       324        844.5      2.6     68.7              char_matches = [c1 != c2 for c1, c2 in zip_longest(lemma, form)]
    18       324        155.6      0.5     12.7              dist = 1 - (sum(char_matches)/len(char_matches))
    19       285         44.4      0.2      3.6              if dist > 0.25:
    20        39         12.3      0.3      1.0                  insort(form_distances, (dist, lemma))
    21        18          4.7      0.3      0.4          distances[form] = form_distances
    22
    23         1         52.5     52.5      4.3      with open(""potential_lemmas_hamming.txt"", ""w"") as f:
    24        17          4.2      0.2      0.3          for form, form_distances in distances.items():
    25        26         11.5      0.4      0.9              for dist, lemma in reversed(form_distances[-2:]):
    26        26         48.3      1.9      3.9                  f.write(f""{form} ï¿½ï¿½ï¿½  {lemma}: {dist}\n"")

original code speed profile
here is your original code for comparison. i modified some aspects of it, the main difference is the use of heapq.nlargest as i believe you were after the 2 most similar lemmas for each form and not the 2 least similar which heapq.nsmallest provided.
from textdistance import hamming, cosine, jaro_winkler
import heapq

lemmas = [""do"", ""done"", ""purpose"", ""can"", ""be"", ""use"", ""for"", ""cannon"", ""amuse"", ""useful"", ""user"", ""become"", ""downtown"", ""develop"", ""fulminate"", ""deduce"", ""de"", ""bezant""]
forms = [""done"", ""done"", ""doing"", ""purposeful"", ""canonical"", ""becareful"", ""being"", ""berate"", ""best"", ""bezant"", ""full"", ""fulmination"", ""predict"", ""downgrade"", ""down"", ""developing"", ""deduct"", ""deducing""]
distances = {}
processed_pairs = set() # keep track of processed pairs

@profile
def profile_distance_calcs():
    for lemma in lemmas:
        if lemma is none:
            continue
        lemma_lower = lemma.lower()
        for form in forms:
            if form is none:
                continue        
            form_lower = form.lower()
            pair = (lemma_lower, form_lower)
            if pair not in processed_pairs:
                processed_pairs.add(pair)
                dist = hamming.normalized_similarity(lemma_lower, form_lower)
                if dist > 0.25: 
                    distances.setdefault(form_lower, []).append((dist, lemma_lower))

    # find the closest pairs
    closest_pairs = {}
    for form, dist_lemmas in distances.items():
        closest_pairs[form] = heapq.nlargest(2, dist_lemmas)

    with open(""potential_lemmas_orig.txt"", ""w"") as f:
        for form, pairs in closest_pairs.items():
            for dist, lemma in pairs:
                f.write(f""{form} ï¿½ï¿½ï¿½  {lemma}: {dist}\n"")

if __name__ == ""__main__"":
    profile_distance_calcs()

time profile breakdown for the original code (total time: 0.0114992 s):
line #      hits         time  per hit   % time  line contents
==============================================================
    11                                           @profile
    12                                           def profile_distance_calcs():
    13        18          2.4      0.1      0.0      for lemma in lemmas:
    14        18          1.9      0.1      0.0          if lemma is none:
    15                                                       continue
    16        18          6.4      0.4      0.1          lemma_lower = lemma.lower()
    17       324         38.8      0.1      0.3          for form in forms:
    18       324         32.6      0.1      0.3              if form is none:
    19                                                           continue
    20       324        108.2      0.3      0.9              form_lower = form.lower()
    21       324         46.9      0.1      0.4              pair = (lemma_lower, form_lower)
    22       306         60.2      0.2      0.5              if pair not in processed_pairs:
    23       306         92.0      0.3      0.8                  processed_pairs.add(pair)
    24       306      10828.9     35.4     94.2                  dist = hamming.normalized_similarity(lemma_lower, form_lower)
    25       270         47.5      0.2      0.4                  if dist > 0.25:
    26        36         24.1      0.7      0.2                      distances.setdefault(form_lower, []).append((dist, lemma_lower))
    27
    28                                               # find the closest pairs
    29         1          0.2      0.2      0.0      closest_pairs = {}
    30        16          4.3      0.3      0.0      for form, dist_lemmas in distances.items():
    31        16         72.7      4.5      0.6          closest_pairs[form] = heapq.nlargest(2, dist_lemmas)
    32
    33         1         72.3     72.3      0.6      with open(""potential_lemmas_orig.txt"", ""w"") as f:
    34        16          4.2      0.3      0.0          for form, pairs in closest_pairs.items():
    35        26          6.5      0.3      0.1              for dist, lemma in pairs:
    36        26         49.0      1.9      0.4                  f.write(f""{form} ï¿½ï¿½ï¿½  {lemma}: {dist}\n"")

measuring natural language similarity
measuring the similarity between two pieces of natural language text is a non-trivial task. attempting to gauge spelling/morphological/semantic similarity based purely on rudimentary character-based metrics (e.g. hamming distance, levenshtein distance etc.) won't suffice as these metrics fail to capture complex linguistic patterns (hence why neural network methods are commonly used to pick up these patterns in large bodies of text). with that being said, one can begin to add their own ""rules"" to calculate more ""accurate"" similarity scores. for example, the code below modifies the normalised hamming similarity computation to track how many consecutive characters matd then scales the ""similarity score"" accordingly. there is obviously scope for fine-tuning and/or increasing the complexity/number of rules used, but with more complexity comes slower processing times. this custom function avoids the issue of results like beatte ï¿½ï¿½ï¿½ beats: 0.667 and beatus ï¿½ï¿½ï¿½ certus: 0.667, instead scoring them as beatte ï¿½ï¿½ï¿½ beats 0.79167 and beatus ï¿½ï¿½ï¿½ certu""lang-py prettyprint-override"">def custom_hamming_norm_sim(stra, strb, scale=0.5):
    max_str_len = max(len(stra), len(strb))
    max_score_per_char = 1 / max_str_len
    penalty = 1
    score = 0
    for c1, c2 in zip_longest(stra, strb):
        if c1 != c2:
            penalty = penalty * scale
            score += max_score_per_char * penalty
        else:
            p = penalty / scale
            if p < max_score_per_char:
                penalty = p
            score += max_score_per_char * penalty
    return score


@profile
def profile_distance_calcs():
    lemmas_low = [lemma.lower() for lemma in lemmas]
    forms_low = [form.lower() for form in forms]
    for form in forms_low:
        form_distances = []
        for lemma in lemmas_low:
            dist = custom_hamming_norm_sim(lemma, form)
            if dist > 0.25:
                insort(form_distances, (dist, lemma))
        distances[form] = form_distances

    with open(""potential_lemmas_hamming.txt"", ""w"") as f:
        for form, form_distances in distances.items():
            for dist, lemma in reversed(form_distances[-2:]):
                f.write(f""{form} ï¿½ï¿½ï¿½  {lemma}: {dist}\n"")

if __name__ == ""__main__"":
    profile_distance_calcs()
<",https://stackoverflow.com/questions/75943880,python,05-04-2023 20:54,3214.0,3.0,2.0,True,18-04-2023 12:32,09-04-2023 10:33,Implementation Issues
79154295,llama-3.1-nemotron-70b-instruct not working with langchain chatnvidia,"userwarning: found nvidia/llama-3.1-nemotron-70b-instruct in
available_models, but type is unknown and inference may fail.
warnings.warn(

python code to call model
## core lc chat interface
from langchain_nvidia_ai_endpoints import chatnvidia
llm = chatnvidia(model=""nvidia/llama-3.1-nemotron-70b-instruct"")
result = llm.invoke(""write a ballad about langchain."")
print(result.content)","['python', 'langchain']",79177231,"you can use chatopenai to invoke:
from langchain_openai import chatopenai
from openai import openai
import os
chatopenai(
    client=openai(
        base_url = ""
        api_key=os.getenv(""nvidia_api_key""),
    ).chat.completions,
).invoke(messages, model=""nvidia/llama-3.1-nemotron-70b-instruct"")

nvidia example",https://stackoverflow.com/questions/79154295,python,04-11-2024 05:35,205.0,0.0,2.0,True,13-11-2024 13:15,13-11-2024 13:15,Implementation Issues
77082604,error inserting spacy.tokens.span.span into pandas dataframe,"using scispacy, trying to use the hearst patterns feature, which returns a spacy.tokens.span.span object. when trying to get the result into a datafame i get an error,  object is treated as several words and not as a single object.
following the example -
import spacy
from scispacy.hyponym_detector import hyponymdetector

nlp = spacy.load(""en_core_sci_sm"")
nlp.add_pipe(""hyponym_detector"", last=true, config={""extended"": false})

doc = nlp(""keystone plant species such as fig trees are good for the soil."")

print(doc._.hearst_patterns)
>>> [('such_as', keystone plant species, fig trees)]
print(type(doc_hp[0][1]))
>>> <class 'spacy.tokens.span.span'>

doc_hp = doc._.hearst_patterns
dict = {
    ""hp_connector"": doc_hp[0][0],
    ""hp_entity_1"":doc_hp[0][1],
    ""hp_entity_2"":doc_hp[0][2],
}

df = pd.dataframe.from_dict(dict)

throws an error:
traceback (most recent call last):
  file ""extract_hearst_patterns.py"", line 42, in <module>
    df = pd.dataframe.from_dict(dict)
  file ""/venv/lib/python3.9/site-packages/pandas/core/frame.py"", line 1760, in from_dict
    return cls(data, index=index, columns=columns, dtype=dtype)
  file ""/venv/lib/python3.9/site-packages/pandas/core/frame.py"", line 709, in __init__
    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)
  file ""/venv/lib/python3.9/site-packages/pandas/core/internals/construction.py"", line 481, in dict_to_mgr
    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)
  file ""/venv/lib/python3.9/site-packages/pandas/core/internals/construction.py"", line 115, in arrays_to_mgr
    index = _extract_index(arrays)
  file ""/venv/lib/python3.9/site-packages/pandas/core/internals/construction.py"", line 655, in _extract_index
    raise valueerror(""all arrays must be of the same length"")
valueerror: all arrays must be of the same length","['python', 'dataframe', 'nlp', 'spacy', 'spacy-3']",77171874,"this ended up working for me -
doc_hp = doc._.hearst_patterns
for pattern in doc_hp:
    patten_dict = get_pattern_dict(full_sent, pattern)
    patten_dict = {
        ""hp object"": [patten],
        ""hp_connector"": str(patten[0]),
        ""hp_entity_1"": patten[1].text,
        ""hp_entity_2"": patten[2].text,
    }
    list_of_pattern_dicts.append(patten_dict)
df = pd.dataframe.from_dict(list_of_pattern_dicts)",https://stackoverflow.com/questions/77082604,python,11-09-2023 14:41,54.0,-1.0,1.0,True,25-09-2023 10:30,25-09-2023 10:22,Tool Setup/Errors
78237430,integrating gpt-4 with team foundation server for data insights,"i'm exploring options to integrate gpt-4, the latest version of openai's powerful language model, with team foundation server (tfs). my objective is to leverage gpt-4's capabilities to gain insights from the data stored within our tfs environment.
i'm wondering if there are any apis or existing integrations available that facilitate this process. specifically, i'm interested in extracting data from tfs and feeding it into gpt-4 for analysis and generating insights.
could anyone provide guidance on whether such an api or integration exists? if not, are there any alternative approaches or workarounds that could achieve similar results?
any advice, resources, or experiences related to integrating gpt-4 with tfs for data analysis and insights?
i tried exporting the data from tfs and then feeding the same data to chat gpt, but exported data doe s not contain all the required details and also exported data is very huge, so it crosses the prompt limit. my expectation is to have an api, which can directly be integrated with tfs server. like we can integrate 365 copilot with office and github co - pilot with codebase.","['artificial-intelligence', 'openai-api', 'gpt-3', 'gpt-4']",78238614,"there isn't a direct integration between openai's gpt models and team foundation server, not that i am aware off. but to utilize gpt-4 for gaining insights from tfs data, you would generally need to build a custom integration.
you will use the tfs/azure rest api to programatically retrieve the data you are interested in. then the data will be processed and cleaned so unnecessary data will not be sent to gpt. once it is done you can send the data to gpt api using gpt-4-0125-preview which has context window of 128000 tokens can generate output upto 4096 tokens",https://stackoverflow.com/questions/78237430,artificial-intelligence,28-03-2024 10:19,99.0,-1.0,1.0,True,18-04-2024 11:37,18-04-2024 11:37,Implementation Issues
64977817,gerund form of a word in python,"i'd like to get the gerund form of a string. i have not found a straightforward way to invoke a library to get the gerund.
i applied the rules for words ending in 'ing`, but because i am getting some errors due to exceptions. then, i am checking against the cmu words to ensure the generated gerund word is correct. the code looks as follows:
import cmudict
import re

ing= 'ing'
vowels = ""aeiou""
consonants = ""bcdfghjklmnpqrstvwxyz""
words=['lead','take','hit','begin','stop','refer','visit']
cmu_words= cmudict.words()
g_w = []

for word in words:
    if word[-1] == 'e':
        if word[:-1] + ing in cmu_words:
            g_w.append(word[:-1] + ing)             
    elif count_syllables(word) == 1 and word[-2] in vowels and word[-1] in consonants:
        if word.__len__()>2 and word[-3] in vowels:
            if word + ing in cmu_words:
                g_w.append(word + ing)                 
        else:
            if word + word[-1] + ing in cmu_words:
                g_w.append(word + word[-1] + ing)
    elif count_syllables(word)>1 and word[-2] in vowels and word[-1] in consonants:
        if word + word[-1]+ ing in cmu_words:
            g_w.append(word + word[-1]+ ing)            
        else:
            if word + ing in cmu_words:
                g_w.append(word + ing) 
    
print(g_w)

the rules are as follow:
when a verb ends in ""e"", drop the ""e"" and add ""-ing"". for example: ""take + ing = taking"".
when a one-syllable verb ends in vowel + consonant, double the final consonant and add ""-ing"". for example: ""hit + ing = hitting"".
when a verb ends in vowel + consonant with stress on the final syllable, double the consonant and add ""-ing"". for example: ""begin + ing = beginning"".
do not double the consonant of words with more than one syllable if the stress is not on the final

is there a more efficient way to get the gerunds of a string if exists?
thanks","['python', 'nlp', 'nltk', 'porter-stemmer']",64978015,"maybe this is what you are looking for. library called pyinflect

a python module for word inflections that works as a spacy extension. to use standalone, import the method getallinflections and/or getinflection and call them directly. the method getinflection takes a lemma and a penn treebank tag and returns a tuple of the specific inflection(s) associated with it.

there is a variety of tags available for getting inflections including the 'vbg' tag (verb, gerund) you are looking for.
pos_type = 'a'
* jj      adjective
* jjr     adjective, comparative
* jjs     adjective, superlative
* rb      adverb
* rbr     adverb, comparative
* rbs     adverb, superlative

pos_type = 'n'
* nn      noun, singular or mass
* nns     noun, plural

pos_type = 'v'
* vb      verb, base form
* vbd     verb, past tense
* vbg     verb, gerund or present participle
* vbn     verb, past participle
* vbp     verb, non-3rd person singular present
* vbz     verb, 3rd person singular present
* md      modal

here is a sample implementation.
#!pip install pyinflect
from pyinflect import getinflection

words = ['lead','take','hit','begin','stop','refer','visit']
[getinflection(i, 'vbg') for i in words]

[('leading',),
 ('taking',),
 ('hitting',),
 ('beginning',),
 ('stopping', 'stoping'),
 ('referring',),
 ('visiting',)]

note: the authors have setup a more sophisticated and benchmarked library which does both lemmatization and inflections called lemminflect. do check this out if you want something more reliable than the above library. the syntax is pretty much the same as above.",https://stackoverflow.com/questions/64977817,python,23-11-2020 22:56,1528.0,3.0,1.0,True,08-01-2023 01:27,08-01-2023 01:27,Implementation Issues
49917033,built-in function to get the frequency of one word with spacy?,"i'm looking for faster alternatives to nltk to analyze big corpora and do basic things like calculating frequencies, pos tagging etc... spacy seems great and easy to use in many ways, but i can't find any built-in function to count the frequency of a specific word for example. i've looked at the spacy documentation, but i can't find a straightforward way to do it. am i missing something?
what i would like would be the nltk equivalent of:
tokens.count(""word"") #where tokens is the tokenized text in which the word is to be counted

in nltk, the above code would tell me that in my text, the word ""word"" appears x number of times.
note that i've come by the count_by function, but it doesn't seem to do what i'm looking for.","['python', 'nlp', 'spacy']",60714164,"i use spacy for frequency counts in corpora quite often. this is what i usually do:
import spacy
nlp = spacy.load(""en_core_web_sm"")

list_of_words = ['run', 'jump', 'catch']

def word_count(string):
    words_counted = 0
    my_string = nlp(string)

    for token in my_string:
        # actual word
        word = token.text
        # lemma
        lemma_word = token.lemma_
        # part of speech
        word_pos = token.pos_
        if lemma_word in list_of_words:
            words_counted += 1
            print(lemma_word)
    return words_counted


sentence = ""i ran, jumped, and caught the ball.""
words_counted = word_count(sentence)
print(words_counted)",https://stackoverflow.com/questions/49917033,python,19-04-2018 09:07,7349.0,4.0,3.0,True,14-02-2023 23:57,19-04-2018 09:16,Implementation Issues
8097383,implementing a top down parser in c#,"i am student and i want to implement a top-down parser in my c# language translation project. for example, if i need to construct a parser tree for the sentence ""my name is husni and i am a student"", how can i do it with c#?","['c#', 'artificial-intelligence', 'nlp']",8097485,after the book you can also find interesting to read about a compiler generator as antlr that can help you to write the compiler ( also in c# ) and browsing the ast even visually.,https://stackoverflow.com/questions/8097383,c#,11-11-2011 17:02,4087.0,1.0,4.0,True,28-01-2022 15:58,28-01-2022 15:58,Implementation Issues
68797316,python: extract non-english words and iterate it over a dataframe,"i have a table of about 30,000 rows and need to extract non-english words from a column named dummy_df  from a dummy_df dataframe. i need to put the non-english words in an adjacent column named non_english.
a dummy data is as thus:
dummy_df = pandas.dataframe({'outcome':    [""i want to go to church"",  ""i love matauranga"", ""take me to  oranga tamariki""]})

my idea is to extract non-english words from a sentence, and then iterate the process over a dataframe. i was able to accurately extract non-english words from a sentence with this code:
import nltk
nltk.download('words')
from nltk.corpus import words

words = set(nltk.corpus.words.words())

sent = ""i love matauranga""
"" "".join(w for w in nltk.wordpunct_tokenize(sent) \
         if not w.lower() in words or not w.isalpha())

the result of the above code is 'matauranga' which is perfectly correct.
but when i try to iterate the code over a dataframe using this code:
import nltk
nltk.download('words')
from nltk.corpus import words

def no_english(text):
  words = set(nltk.corpus.words.words())
  "" "".join(w for w in nltk.wordpunct_tokenize(text['outcome']) \
         if not w.lower() in words or not w.isalpha())

dummy_df['non_english'] = dummy_df.apply(no_english, axis = 1)
print(dummy_df)

i got an undesirable result in that the non_english column has none value instead of the desired non-english words (see below):
                       outcome non_english
0       i want to go to church        none
1            i love matauranga        none
2  take me to  oranga tamariki        none
3                                     none

instead, the desired result should be:
                       outcome        non_english
0       i want to go to church        
1            i love matauranga        matauranga
2  take me to  oranga tamariki        oranga tamariki","['python', 'pandas', 'text', 'nlp', 'nltk']",68797328,"you are missing the return in your function:
import nltk
nltk.download('words')
from nltk.corpus import words

def no_english(text):
    words = set(nltk.corpus.words.words())
    return "" "".join(w for w in nltk.wordpunct_tokenize(text['outcome']) \
           if not w.lower() in words or not w.isalpha())

dummy_df['non_english'] = dummy_df.apply(no_english, axis = 1)
print(dummy_df)

output:
                       outcome      non_english
0       i want to go to church                 
1            i love matauranga       matauranga
2  take me to  oranga tamariki  oranga tamariki",https://stackoverflow.com/questions/68797316,python,16-08-2021 04:08,437.0,1.0,1.0,True,25-05-2022 11:22,25-05-2022 11:22,Data Wrangling
61085302,can inverted index have multiple words in one entry?,"in information retrieval, the inverted index has entries which are the words of corpus, and each word has a posting list which is the list of documents it appears in.
if stemming is applied, index entry would be a stem, so multiple words may finally map to the same entry if they share the same stem. for example:
without stemming:
(slowing) --> [d1, d5, d9,...]

(slower) --> [d9, d10, d20,...]

(slow) --> [d2,...]

with stemming:
(slow) --> [d1, d2, d5, d9, , d10, d20...]

i want to avoid stemming, and instead would like to make each entry in my inverted index as a bag of words (inflections) such as (slow, slows, slowing, slowed, slower, slowest). for example:
(slow, slows, slowing, slowed, slower, slowest) --> [d1, d2, d5, d9, , d10, d20...]

would that be possible and feasible or not?","['information-retrieval', 'stemming', 'inverted-index']",61098980,"short answer:
simply avoid stemming to suit your need of not considering slow and slows to be a match.
long answer:
question: 
i want to avoid stemming, and instead would like to make each entry in my inverted index as a bag of words (inflections) such as (slow, slows, slowing, slowed, slower, slowest).

let me try to clear some confusion that you have about inverted lists. it is the documents that are stored in the postings for each term (not the terms themselves). 
the words are typically stored in a in-memory dictionary (implemented with a hash-table or a trie) containing pointers to the postings (list of documents which contain that particular term) stored and loaded on the fly from secondary storage.
a simple example (without showing document weights):

(information) --> [d1, d5, d9,...]
(informative) --> [d9, d10, d20,...]
(retrieval) --> [d1, d9, d17,...]
..

so, if you don't want to apply stemming, that's fine... in fact, the above example shows an unstemmed index, where the words information and informative appear in their non-conflated forms. in a conflated term index (with a stemmer or a lemmatizer), you would substitute the different forms with an equivalent representation (say inform). in that case, the index will be:

(inform) --> [d1, d5, d9, d10, d20...]. --- union of the different forms
(retrieval) --> [d1, d9, d17,...]
..

so, this conflated representation matches all possible forms of the word information, e.g. informative, informational etc.
longer answer
now let's say you want to achieve the best of both worlds, i.e. a representation which allows this conflation to be done in a user controlled way, e.g. wrapping a word around quotes to denote requiring an exact match (""slow""vs.slowin the query), or some indicator to include synonyms for a query term for semantic search (e.g.syn(slow)` to include synonyms of the word slow).
for this, you need to maintain separate postings for the non-conflated words and maintain additional equivalence indicating pointers between a set of equivalent (stem relation/synonym relation/ semantic relation etc.) terms.
coming back to our example, you would have something like:
(e1)-->(information) --> [d1, d5, d9,...]
 |---->(informative) --> [d9, d10, d20,...]
 |---->(data) --> [d20, d23, d25,...]


(e2)-->(retrieval) --> [d1, d9, d17,...]
 |---->(search) --> [d20, d30, d31,...]

..

here, i have shown two examples of equivalence classes (concept representations) of two sets of terms information, data... and retrieval, search....
depending on the query syntax, it would then be possible at the retrieval time to facilitate exact search or relaxed search (based on inflections/synonyms etc.)",https://stackoverflow.com/questions/61085302,information-retrieval,07-04-2020 16:56,751.0,0.0,1.0,True,27-04-2022 15:09,27-04-2022 15:09,Implementation Issues
64685243,getting sentence embedding from huggingface feature extraction pipeline,"how do i get an embedding for the whole sentence from huggingface's feature extraction pipeline?
i understand how to get the features for each token (below) but how do i get the overall features for the sentence as a whole?
feature_extraction = pipeline('feature-extraction', model=""distilroberta-base"", tokenizer=""distilroberta-base"")
features = feature_extraction(""i am sentence"")","['machine-learning', 'nlp', 'huggingface-transformers', 'spacy-transformers']",64714245,"to explain more on the comment that i have put under stackoverflowuser2010's answer, i will use ""barebone"" models, but the behavior is the same with the pipeline component.
bert and derived models (including distilroberta, which is the model you are using in the pipeline) agenerally indicate the start and end of a sentence with special tokens (mostly denoted as [cls] for the first token) that usually are the easiest way of making predictions/generating embeddings over the entire sequence. there is a discussion within the community about which method is superior (see also a more detailed answer by stackoverflowuser2010 here), however, if you simply want a ""quick"" solution, then taking the [cls] token is certainly a valid strategy.
now, while the documentation of the featureextractionpipeline isn't very clear, in your example we can easily compare the outputs, specifically their lengths, with a direct model call:
from transformers import pipeline, autotokenizer

# direct encoding of the sample sentence
tokenizer = autotokenizer.from_pretrained('distilroberta-base')
encoded_seq = tokenizer.encode(""i am sentence"")

# your approach
feature_extraction = pipeline('feature-extraction', model=""distilroberta-base"", tokenizer=""distilroberta-base"")
features = feature_extraction(""i am sentence"")

# compare lengths of outputs
print(len(encoded_seq)) # 5
# note that the output has a weird list output that requires to index with 0.
print(len(features[0])) # 5

when inspecting the content of encoded_seq, you will notice that the first token is indexed with 0, denoting the beginning-of-sequence token (in our case, the embedding token). since the output lengths are the same, you could then simply access a preliminary sentence embedding by doing something like
sentence_embedding = features[0][0]",https://stackoverflow.com/questions/64685243,machine-learning,04-11-2020 17:52,16444.0,4.0,4.0,True,29-12-2023 11:26,06-11-2020 03:53,Conceptual Questions
78609056,openai assistants api: how do i pass the assistant to the api?,"i am trying to pass my assistant to the openai assistants api, but i get the following error when doing so:

message: 'the model my model id does not exist or you do not have
access to it.',
type: 'invalid_request_error',
param: null,
code: 'model_not_found'

code:
try {
    const fetch = await import('node-fetch').then(mod => mod.default);

    const response = await fetch(' {
        method: 'post',
        headers: {
            'content-type': 'application/json',
            'authorization': `bearer ${apikey}`
        },
        body: json.stringify({
            model: 'my model id',
            messages: conversation
        })
    });
}

if i change the model parameter to gpt-3.5-turbo it works, and my app can interface with the standard model.
i know the api key is correct and it has all permissions. the assistant id i'm trying to pass to the model parameter is also correct. for testing purposes, both are defined in the same file.","['javascript', 'openai-api', 'openai-assistants-api']",78612505,"problem
the openai assistants api doesn't use the chat completions api endpoint (i.e.,  using the openai assistants api is fundamentally different (i.e., more complex) than using any other apis, like the completions api or chat completions api.
solution
you don't pass the assistant by using the model parameter. you pass the assistant by using the assistant_id parameter (see step 4 below).
here are the steps you need to follow to get a response from the assistant:
step 1: create an assistant
post 

step 2: create a thread
post 

step 3: add the user's question to the thread
post 

step 4: run the assistant
post 

step 5: periodically retrieve the run to check its status to see if it has moved to completed
get 

step 6: retrieve the assistant's answer
get 

also, i've made a youtube tutorial on how to use the assistants api and posted the code on my github profile.",https://stackoverflow.com/questions/78609056,javascript,11-06-2024 17:44,890.0,2.0,1.0,True,05-08-2024 10:24,12-06-2024 13:15,Implementation Issues
35596031,gensim word2vec: find number of words in vocabulary,"after training a word2vec model using python gensim, how do you find the number of words in the model's vocabulary?","['python', 'neural-network', 'nlp', 'gensim', 'word2vec']",35641434,"in recent versions, the model.wv property holds the words-and-vectors, and can itself can report a length ï¿½ï¿½ï¿½ the number of words it contains. so if w2v_model is your word2vec (or doc2vec or fasttext) model, it's enough to just do:
vocab_len = len(w2v_model.wv)

if your model is just a raw set of word-vectors, like a keyedvectors instance rather than a full word2vec/etc model, it's just:
vocab_len = len(kv_model)

other useful internals in gensim 4.0+ include model.wv.index_to_key, a plain list of the key (word) in each index position, and model.wv.key_to_index, a plain dict mapping keys (words) to their index positions.
in pre-4.0 versions, the vocabulary was in the vocab field of the word2vec model's wv property, as a dictionary, with the keys being each token (word). so there it was just the usual python for getting a dictionary's length:
len(w2v_model.wv.vocab)

in very-old gensim versions before 0.13 vocab appeared directly on the model. so way back then you would use w2v_model.vocab instead of w2v_model.wv.vocab.
but if you're still using anything from before gensim 4.0, you should definitely upgrade! there are big memory & performance improvements, and the changes required in calling code are relatively small ï¿½ï¿½ï¿½ some rgs & moves, covered in the 4.0 migration notes.",https://stackoverflow.com/questions/35596031,python,24-02-2016 07:39,97358.0,55.0,5.0,True,23-05-2023 06:04,26-02-2019 18:28,Implementation Issues
70161048,python - how to loop through each index position in a list?,"given a list [[[""source1""], [""target1""], [""alignment1""]], [""source2""], [""target2""], [""alignment2""]], ...] , i want to extract the words in the source that align with the words in the target.
for example, in the english-german sentence pair the hat is on the table . - der hut liegt auf dem tisch ., i want to print the following:
the - der
hat - hut
is - liegt
on - auf
the - dem
table - tisch
. - . 

so i have written the following:
en_de = [
[['the', 'hat', 'is', 'on', 'the', 'table', '.'], ['der', 'hut', 'liegt', 'auf', 'dem', 'tisch', '.'], '0-0 1-1 2-2 3-3 4-4 5-5 6-6'], 
[['the', 'picture', 'is', 'on', 'the', 'wall', '.'], ['das', 'bild', 'hï¿½ï¿½ngt', 'an', 'der', 'wand', '.'], '0-0 1-1 2-2 3-3 4-4 5-5 6-6'], 
[['the', 'bottle', 'is', 'under', 'the', 'sink', '.'], ['die', 'flasche', 'ist', 'under', 'dem', 'waschbecken', '.'], '0-0 1-1 2-2 3-3 4-4 5-5 6-6']
]

for group in en_d   src_sent = group[0]
    tgt_sent = group[1]
    aligns = group[2]

    split_aligns = aligns.split()

    hyphen_split = [align.split(""-"") for align in split_aligns]

    align_index = hyphen_split[0]

    print(src_sent[int(align_index[0])],""-"", tgt_sent[int(align_index[1])])

this prints, as expected, the words in index position 0 of src_sent and tgt_sent:
the - der
the - das
the - die

now, i don't know how i can print the words of all index positions of src_sent and tgt_sent. obviously, i could manually update align_index to a new index position for each position in the sentence pair, but on the full dataset, some sentences will have up to 25 index positions.
is there a way to possibly for-loop through each index position?
when i try:
align_index = hyphen_split[0:]
print(src_sent[int(align_index[0])],""-"", tgt_sent[int(align_index[1])])

i get a typeerror:  int() argument must be a string, a bytes-like object or a number, not 'list'
it's clear that align_index can't be a list, but i'm not sure how to convert it into something that will do what i want it to do.
any advice or help would be greatly appreciated. thank you in advance.","['python', 'for-loop', 'nlp', 'linguistics']",70161318,"you are forgetting to loop over your hyphen_split list:
for group in en_de:
    src_sent = group[0]
    tgt_sent = group[1]
    aligns = group[2]

    split_aligns = aligns.split()

    hyphen_split = [align.split(""-"") for align in split_aligns]

    for align_index in hyphen_split:
        print(src_sent[int(align_index[0])],""-"", tgt_sent[int(align_index[1])])

see the last two lines, updated from your code.",https://stackoverflow.com/questions/70161048,python,29-11-2021 20:39,650.0,1.0,2.0,True,29-11-2021 21:06,29-11-2021 20:58,Implementation Issues
33374010,does word2vec has a hidden layer?,"when i am reading one of papers of tomas mikolov: 
i have one concern on the continuous bag-of-words model sectionï¿½ï¿½ï¿½

the first proposed architecture is similar to the feedforward nnlm, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged).

i find some people mention that there is a hidden layer in word2vec model, but from my understanding, there is only one projection layer in that model. does this projection layer do the same work as hidden layer?
the another question is that how to project input data into the projection layer?","['machine-learning', 'nlp', 'neural-network', 'word2vec']",33786063,"from the original paper, section 3.1, it is clear that there is no hidden layer:

""the first proposed architecture is similar to the feedforward nnlm
  where the non-linear hidden layer is removed and the projection layer is shared for all words"".

with respect to your second question (what does sharing the projection layer means), it means that you consider only one single vector, which is the centroid of the vectors of all the words in context. thus, instead of having n-1 word vectors as input, you consider only one vector. this is why it is called continuous bag of words (because word order is lost within the context of size n-1).",https://stackoverflow.com/questions/33374010,machine-learning,27-10-2015 16:57,3105.0,11.0,1.0,True,06-03-2025 03:22,06-03-2025 03:22,Implementation Issues
72516835,word frequency over time : how to count the word frequency by date?,"i have a data frame look like this :




date
text




201901
thank you for helping me


201902
you  are amazing


201902
for helping with this




my aim is to calculate the word frequency in each line, and eventually look like this:




date
thank
you
for
helping
me
are
amazing
with
this
for




201901
1
1
1
1
1
0
0
0
0
0


201902
0
1
1
1
0
1
1
1
1
1




the actual data set is like this frame, but contains millions of text lines. so i was wondering how to automate this process using r, without typing all those texts lines.","['r', 'nlp', 'word-frequency']",72516955,"using r and tidyverse:
df <- data.frame(date = c(201901, 201902, 201902),
                 text = c(""thank you for helping me"", ""you are amazing"", ""for helping with this""))

library(tidyverse)

if you want your data as a table of counts
df %>% 
            separate_rows(text, sep = "" "") %>% 
            mutate(text = tolower(text)) %>% 
            table()

output:
text
date     amazing are for helping me thank this with you
  201901       0   0   1       1  1     1    0    0   1
  201902       1   1   1       1  0     0    1    1   1

if you want your output as a tibble
df %>% 
        separate_rows(text, sep = "" "") %>% 
        mutate(text = tolower(text)) %>% 
        table() %>% 
        as_tibble() %>% 
        pivot_wider(names_from = text, values_from = n)

output:
# a tibble: 2 x 10
  date   amazing   are `for` helping    me thank  this  with   you
  <chr>    <int> <int> <int>   <int> <int> <int> <int> <int> <int>
1 201901       0     0     1       1     1     1     0     0     1
2 201902       1     1     1       1     0     0     1     1     1

edit: to transform everything to lowercase as your desired output and to show you the output
edit2: to show you that you can also get your data as a tibble to further work with it",https://stackoverflow.com/questions/72516835,r,06-06-2022 11:14,243.0,-1.0,2.0,True,06-06-2022 11:57,06-06-2022 11:55,Implementation Issues
78959794,"how to convert the result from openai call, convert it into json and write to .txt file?","i am very new to python and only know the basics, so basically i am calling openai and getting a response in return and want to write that response in a .txt file.
i want to convert the response in json before writing in the file. my response is already in json format but weird when print it shows json format with json {}  with it, this is my script
def get_json(image_file, category):
    with open(image_file, ""rb"") as image:
        response = openai_client.chat.completions.create(
            model=""gpt-4-vision-preview"",
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": [
                        {""type"": ""text"", ""text"": f""analyze this image and provide the following attributes: color theme, font style, and a short description of about 4-7 words. categorize it as {category}. return the result as a json object.""},
                        {""type"": ""image_url"", ""image_url"": {""url"": f""data:image/jpeg;base64,{base64.b64encode(image.read()).decode()}""}},
                    ],
                }
            ],
            temperature=1,
            max_tokens=4095,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0,
        ) 
        return response.choices[0].message.content
     
with open(file, 'a') as file:
    for filename in os.listdir(images_folder):
        filepath = os.path.join(images_folder, filename)
        result =get_json(filepath, 'hero')
        file.write(result + '\n')
        json_result = json.loads(result)
        print(json_result)

this is the result i am getting
enter image description here
i want to remove the text '''json'''
tried to convert it into json by json.loads(result) but getting the following error:-
raise jsondecodeerror(""expecting value"", s, err.value) from none
json.decoder.jsondecodeerror: expecting value: line 1 column 1 (char 0)","['python', 'json', 'openai-api']",78959861,"you are asking it to return a json object in you prompt, ""return the result as a json object."" that it why! if you give it the same prompt using the website, you will notice that the response is nicely formatted, that is because of those ""```json ...content ```"" markdown formatting.
you can solve it using two methods:

explicitly replace ""```json"" and ""```"" with empty string:

import json

# for example, this is the content in response:
response = '```json{""color_theme"": ""mint green and black"",""font_style"": ""sans-serif"",""short_description"": ""web publishing platform"",""category"": ""hero""}```'

# replace and assign back to original content
response = response.replace(""```json"", """")
response = response.replace(""```"", """")

# don't forget to convert to json as it is a string right now:
json_result = json.loads(response)


using slicing:

import json

# for example, this is the content in response:
response = '```json{""color_theme"": ""mint green and black"",""font_style"": ""sans-serif"",""short_description"": ""web publishing platform"",""category"": ""hero""}```'

# ""```json"" is 7 character long, but slicing count start from 0. ""{"" is at 7th character.
# ""```"" is 3 character long (at the end).
response = response[7:-3]

# don't forget to convert to json as it is a string right now:
json_result = json.loads(response)


and for writing to the .txt file, you can use json.dump() as follows:
import json

response = '```json{""color_theme"": ""mint green and black"",""font_style"": ""sans-serif"",""short_description"": ""web publishing platform"",""category"": ""hero""}```'

response = response[7:-3]
response = json.loads(response)

# write to response.txt file (overwriting it).
with open(""response.txt"", ""w"") as file:
    json.dump(response, file)",https://stackoverflow.com/questions/78959794,python,07-09-2024 09:22,1758.0,0.0,1.0,True,08-09-2024 19:34,08-09-2024 19:34,Implementation Issues
39109743,adding new text to sklearn tfidif vectorizer (python),"is there a function to add to the existing corpus? i've already generated my matrix, i'm looking to periodically add to the table without re-crunching the whole sha-bang
e.g;
articlelist = ['here is some text blah blah','another text object', 'more foo for your bar right now']
tfidf_vectorizer = tfidfvectorizer(
                        max_df=.8,
                        max_features=2000,
                        min_df=.05,
                        preprocessor=prep_text,
                        use_idf=true,
                        tokenizer=tokenize_text
                    )
tfidf_matrix = tfidf_vectorizer.fit_transform(articlelist)

#### adding a new article to existing set?
bigger_tfidf_matrix = tfidf_vectorizer.fit_transform(['the last article i wanted to add'])","['python', 'scikit-learn', 'tf-idf']",39114555,"you can access the vocabulary_ attribute of your vectoriser directly, and you can access the idf_ vector via _tfidf._idf_diag, so it would be possible to monkey-patch something like this:
import re 
import numpy as np
from scipy.sparse.dia import dia_matrix
from sklearn.feature_extraction.text import tfidfvectorizer

def partial_fit(self, x):
    max_idx = max(self.vocabulary_.values())
    for a in x:
        #update vocabulary_
        if self.lowercase: a = a.lower()
        tokens = re.findall(self.token_pattern, a)
        for w in tokens:
            if w not in self.vocabulary_:
                max_idx += 1
                self.vocabulary_[w] = max_idx

        #update idf_
        df = (self.n_docs + self.smooth_idf)/np.exp(self.idf_ - 1) - self.smooth_idf
        self.n_docs += 1
        df.resize(len(self.vocabulary_))
        for w in tokens:
            df[self.vocabulary_[w]] += 1
        idf = np.log((self.n_docs + self.smooth_idf)/(df + self.smooth_idf)) + 1
        self._tfidf._idf_diag = dia_matrix((idf, 0), shape=(len(idf), len(idf)))

tfidfvectorizer.partial_fit = partial_fit
articlelist = ['here is some text blah blah','another text object', 'more foo for your bar right now']
vec = tfidfvectorizer()
vec.fit(articlelist)
vec.n_docs = len(articlelist)
vec.partial_fit(['the last text i wanted to add'])
vec.transform(['the last text i wanted to add']).toarray()

# array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
#          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
#          0.        ,  0.        ,  0.27448674,  0.        ,  0.43003652,
#          0.43003652,  0.43003652,  0.43003652,  0.43003652]])",https://stackoverflow.com/questions/39109743,python,23-08-2016 20:00,3421.0,15.0,2.0,True,20-07-2023 08:36,24-08-2016 03:41,Uncategorized
79160811,python: compare html tags in ro folder with their corresponding tags in en folder and displays in output the unique tags from both files,"in short, i have two files, one in romanian, the other has been translated into english. in the ro file there are some tags that have not been translated into en. so i want to display in an html output all the tags in en that have corresponding tags in ro, but also those tags in ro that do not appear in en.
i have this files:
   ro_file_path = r'd:\3\ro\incotro-vezi-tu-privire.html'
   en_file_path = r'd:\3\en\where-do-you-see-look.html'
   output =  d:\3\output\where-do-you-see-look.html 

task: compare the 3 tags below, in both files.
<p class=""text_obisnuit"">(.*?)</p>
<p class=""text_obisnuit2"">(.*?)</p>
<p class=""text_obisnuit""><span class=""text_obisnuit2"">(.*?)</span>(.*?)</p>

requirements:

all tags are enclosed between: <!-- start article --> and <!-- final article -->
count the tags in ro and count the tags in en, and compare.
then count the words in the tags in ro and compare with the number of words in the tags in en.
compares the html tags in ro with the html tags in en, in order, and displays in output the unique tags from both files

ro       d:\3\ro\incotro-vezi-tu-privire.html
<!-- articol start --> 
<p class=""text_obisnuit""><span class=""text_obisnuit2"">stiu ca este dificil sa conduci la inceput, </span>dar dupa 4-5 luni inveti.</p> 
<p class=""text_obisnuit2"">imi place sa merg la scoala si sa invat, mai ales in timpul saptamanii.</p> 
<p class=""text_obisnuit"">sunt un bun conducator auto, dar am facut si greseli din care am invatat.</p> 
<p class=""text_obisnuit"">ï¿½ï¿½n fond, cele scrise de mine, sunt adevarate.</p> 
<p class=""text_obisnuit""&gtesc sa conduc masina.</p> 
<p class=""text_obisnuit""><span class=""text_obisnuit2"">ma iubesti?</p> 
<p class=""text_obisnuit""><span class=""text_obisnuit2"">stiu ca este dificil sa conduci la inceput, </span>dar dupa 4-5 luni inveti.</p> 
<p class=""text_obisnuit"">totul se repetï¿½ï¿½, chiar ï¿½ï¿½i ochii care nu se vad.</p> 
<p class=""text_obisnuit2"">bee servesc o cafea 2 mai buna</p> 
<!-- articol final -->

   

en    d:\3\en\where-do-you-see-look.html
<!-- articol start -->
<p class=""text_obisnuit2"">i like going to school and learning, especially during the week.</p>
<p class=""text_obisnuit"">i'm a good driver, but i've also made mistakes that i've learned from.</p>
<p class=""text_obisnuit"">basically, what i wrote is true.</p>
<p class=""text_obisnuigt;i love driving.</p>
<p class=""text_obisnuit""><span class=""text_obisinuit2"">i know it's difficult to drive at first, </span> but after 4-5 months you learn.</p>
<p class=""text_obisnuit"">everything is repeated, even the eyes that can't see.</p>
<!-- articol final -->

expected output:  d:\3\output\where-do-you-see-look.html
<!-- articol start -->
<p class=""text_obisnuit""><span class=""text_obisnuit2"">stiu ca este dificil sa conduci la inceput, </span> dar dupa 4-5 luni inveti.</p> 
<p class=""text_obisnuit2"">i like going to school and learning, especially during the week.</p>
<p class=""text_obisnuit"">i'm a good driver, but i've also made mistakes that i've learned from.</p>
<p class=""text_obisnuit"">basically, what i wrote is true.</p>
<p class=""text_obisnuit""><span class=""text_obisnuit2"">ma iubesti?</p> 
<p class=""text_obisnuit"">i love driving.</p>
<p class=""text_obisnuit""><span class=""text_obisinuit2"">i know it's difficult to drive at first, </span> but after 4-5 months you learn.</p>
<p class=""text_obisnuit"">everything is repeated, even the eyes that can't see.</p>
<p class=""text_obisnuit2"">bee servesc o cafea 2 mai buna</p> 
<!-- articol final -->

python code must compares the html tags in ro with the html tags in en and displays in output the unique tags in both files, taking into account that most of the tags in ro have their corresponding translation in the tags in en. but the idea of ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½the code is that the code also finds those html tags in ro that were omitted from being translated into en.
here's how i came up with the solution in python code. i followed a simple calculation.ong>first method:
first, you have to count all the tags in ro, then all the tags in en.
then you have to memorize each type of tag in ro, but then also in en.
then you have to count the words in each tag in ro and the words in each tag in en.
don't forget that there can be 2 identical tags, but on different lines, just like in ro.
then you have to statistically calculate the result. how much are the tags in ro minus the tags in en?
the second method, to verify the output, is to print the screen. compare the entire ro part and the entire en part separately through ocr, then line by line, see which tags in ro are plus compared to the tags in en
python code:
import re
import os

def extract_tags(content):
    start = content.find('<!-- articol start -->')
    end = content.find('<!-- articol final -->')
    if start == -1 or end == -1:
        raise valueerror(""marcajele 'articol start' sau 'articol final' lipsesc."")

    section_content = content[start:end]
    pattern = re.compile(r'<p class=""text_obisnuit(?:2)?"">(?:<span class=""text_obisnuit2"">)?.*?</p>', re.dotall)
    tags = []

    for idx, match in enumerate(pattern.finditer(section_content), 1):
        tag = match.group(0)
        text = re.sub(r'<[^>]+>', '', tag).strip()

        if '<span class=""text_obisnuit2"">' in tag or '<span class=""text_obisinuit2"">' in tag:
            tag_type = 'span'
        elif 'class=""text_obisnuit2""' in tag:
            tag_type = 'text_obisnuit2'
        else:
            tag_type = 'text_obisnuit'

        tags.append({
            'index': idx,
            'tag': tag,
            'text': text,
            'type': tag_type,
            'word_count': len(text.split())
        })
    return tags

def find_matching_pairs(ro_tags, en_tags):
    matched_indices = set()
    used_en = set()

    for i, ro_tag in enumerate(ro_tags):
        for j, en_tag in enumerate(en_tags):
            if j in used_en:
                continue

            if ro_tag['type'] == en_tag['type']:
                word_diff = abs(ro_tag['word_count'] - en_tag['word_count'])
                if word_diff <= 3:
                    matched_indices.add(i)
                    used_en.add(j)
                    break
    return matched_indices

def fix_duplicates(output_content, ro_content):
    """"""corecteazï¿½ï¿½ poziï¿½ï¿½ia tag-urilor duplicate""""""
    ro_tags = extract_tags(ro_content)
    output_tags = extract_tags(output_content)

    # gï¿½ï¿½sim tag-urile care apar ï¿½ï¿½n ro ï¿½ï¿½i output
    for ro_idx, ro_tag in enumerate(ro_tags):
        for out_idx, out_tag in enumerate(output_tags):
            if ro_tag['tag'] == out_tag['tag'] and ro_idx != out_idx:
                # am gï¿½ï¿½sit un tag care apare ï¿½ï¿½n poziï¿½ï¿½ii diferite
                # verificï¿½ï¿½m dacï¿½ï¿½ este cazul de duplicat care trebuie mutat
                          out_lines = output_content.split('\n')

                if ro_tag['tag'] in ro_lines[ro_idx+1] and out_tag['tag'] in out_lines[out_idx+1]:
                    # mutï¿½ï¿½m tag-ul la poziï¿½ï¿½ia corectï¿½ï¿½
                    out_lines.remove(out_tag['tag'])
                    out_lines.insert(ro_idx+1, out_tag['tag'])
                    output_content = '\n'.join(out_lines)
                    break

    return output_content

def generate_output(ro_tags, en_tags, original_content):
    start = original_content.find('<!-- articol start -->')
    end = original_content.find('<!-- articol final -->')
    if start == -1 or end == -1:
        raise valueerror(""marcajele 'articol start' sau 'articol final' lipsesc."")

    output_content = original_content[:start + len('<!-- articol start -->')] + ""\n""
    matched_indices = find_matching_pairs(ro_tags, en_tags)
    en_index = 0

    for i, ro_tag in enumerate(ro_tags):
        if i in matched_indices:
         ntent += en_tags[en_index]['tag'] + ""\n""
            en_index += 1
        else:
            output_content += ro_tag['tag'] + ""\n""

    while en_index < len(en_tags):
        output_content += en_tags[en_index]['tag'] + ""\n""
        en_index += 1

    output_content += original_content[end:]
    return output_content

def main():
    try:
        ro_file_path = r'd:\3\ro\incotro-vezi-tu-privire.html'
        en_file_path = r'd:\3\en\where-do-you-see-look.html'
        output_file_path = r'd:\3\output\where-do-you-see-look.html'

        with open(ro_file_path, 'r', encoding='utf-8') as ro_file:
            ro_content = ro_file.read()
        with open(en_file_path, 'r', encoding='utf-8') as en_file:
            en_content = en_file.read()

        ro_tags = extract_tags(ro_content)
        en_tags = extract_tags(en_content)

        # generï¿½ï¿½m primul output
        initial_output = generate_output(ro_tags, en_tags, en_content)

        # corectï¿½ï¿½m poziï¿½ï¿½iile tag-uate
        final_output = fix_duplicates(initial_output, ro_content)

        with open(output_file_path, 'w', encoding='utf-8') as output_file:
            output_file.write(final_output)

        print(f""output-ul a fost generat la {output_file_path}"")

    except exception as e:
        print(f""eroare: {str(e)}"")

if __name__ == ""__main__"":
    main()

my python code is almost perfect, but not perfect. the problem occurs when i introduce other tags in ro, such as:
<!-- articol start --> 
<p class=""text_obisnuit"">laptopul meu este de culoare neagra.</p>
<p class=""text_obisnuit2"">imi place sa merg la scoala si sa invat, mai ales in timpul saptamanii.</p> 
<p class=""text_obisnuit"">sunt un bun conducator auto, dar am facut si greseli din care am invatat.</p> 
<p class=""text_obisnuit""><span class=""text_obisnuit2"">stiu ca este dificil sa conduci la inceput, </span>dar dupa 4-5 luni inveti.</p>
<p class=""text_obisnuit"">ï¿½ï¿½n fond, cele scrise de mine, sunt adevarate.</p> 
<p class=""text_obisnuit"">iubesc sa conduc masina.</p> 

<p class=""text_obisnuit""><span class=""text_obisnuit2"">stiu ca este dificil sa conduci la inceput, </span>dar dupa 4-5 luni inveti.</p>
<p class=""text_obisnuit"">totul se repetï¿½ï¿½, chiar ï¿½ï¿½i ochii care nu se vad.</p> 

<!-- articol final -->
</","['python', 'python-3.x', 'openai-api', 'claude']",79191744,"second, and the best solution.
finally i solved the problem, but not with chatgpt or claude. no other ai could find the solution, because it didn't know how to think about the solution.
in fact, to find the solution to this problem, you had to assign some identifiers to each tag, and do multiple searches.
chatgpt or claude, or other ais, will have to seriously consider this type of solution for such problems.
here are the specifications, the way i thought about solving the problem. it's a different way of thinking about doing parsings.

python code made by a friend of mine. i think the solution, he made the code:
from bs4 import beautifulsoup
import re

def count_words(text):
    """"""numï¿½ï¿½rï¿½ï¿½ cuvintele dintr-un text.""""""
    return len(text.strip().split())

def get_greek_identifier(word_count):
    """"""determinï¿½ï¿½ identificatorul grecesc bazat de cuvinte.""""""
    if word_count < 7:
        return 'ï¿½ï¿½'
    elif word_count <= 14:
        return 'ï¿½ï¿½'
    else:
        return 'ï¿½ï¿½'

def get_tag_type(tag):
    """"""determinï¿½ï¿½ tipul tagului (a, b, sau c).""""""
    if tag.find('span'):
        return 'a'
    elif 'text_obisnuit2' in tag.get('class', []):
        return 'b'
    return 'c'

def analyze_tags(content):
    """"""analizeazï¿½ï¿½ tagurile ï¿½ï¿½i returneazï¿½ï¿½ informaï¿½ï¿½ii despre fiecare tag.""""""
    soup = beautifulsoup(content, 'html.parser')
    tags_info = []

    article_content = re.search(r'<!-- articol start -->(.*?)<!-- articol final -->',
                              content, re.dotall)

    if article_content:
        content = article_content.group(1)
        soup = beautifulsoup(content, 'html.parser')

    for i, tag in enumerate(soup.find_all('p', recursive=false)):
        text_content = tag.get_text(strip=true)
        tag_ word_count = count_words(text_content)
        greek_id = get_greek_identifier(word_count)

        tags_info.append({
            'number': i + 1,
            'type': tag_type,
            'greek': greek_id,
            'content': str(tag),
            'text': text_content
        })

    return tags_info

def compare_tags(ro_tags, en_tags):
    """"""comparï¿½ï¿½ tagurile ï¿½ï¿½i gï¿½ï¿½seï¿½ï¿½te diferenï¿½ï¿½ele.""""""
    wrong_tags = []
    i = 0
    j = 0

    while i < len(ro_tags):
        ro_tag = ro_tags[i]
        if j >= len(en_tags):
            wrong_tags.append(ro_tag)
            i += 1
            continue

        en_tag = en_tags[j]

        if ro_tag['type'] != en_tag['type']:
            wrong_tags.append(ro_tag)
            i += 1
            continue

        i += 1
        j += 1

    return wrong_tags

def format_results(wrong_tags):
    """"""formateazï¿½ï¿½ rezultatele pentru afiï¿½ï¿½are ï¿½ï¿½i salvare.""""""
    type_counts ype_content = {'a': [], 'b': [], 'c': []}

    for tag in wrong_tags:
        type_counts[tag['type']] += 1
        type_content[tag['type']].append(tag['content'])

    # creï¿½ï¿½m rezultatul formatat
    result = []

    # prima linie cu sumarul
    summary_parts = []
    for tag_type in ['a', 'b', 'c']:
        if type_counts[tag_type] > 0:
            summary_parts.append(f""{type_counts[tag_type]} taguri de tipul ({tag_type})"")
    result.append(""in ro exista in plus fata de en urmatoarele: "" + "" si "".join(summary_parts))

    # detaliile pentru fiecare tip de tag
    for tag_type in ['a', 'b', 'c']:
        if type_counts[tag_type] > 0:
            result.append(f""\n{type_counts[tag_type]}({tag_type}) adica asta {'taguri' if type_counts[tag_type] > 1 else 'tag'}:"")
            for content in type_content[tag_type]:
                result.append(content)
            result.append("""")  # linie goalï¿½ï¿½ pentru separare

    return ""\n"".ult)

def merge_content(ro_tags, en_tags, wrong_tags):
    """"""combinï¿½ï¿½ conï¿½ï¿½inutul ro ï¿½ï¿½i en, inserï¿½ï¿½nd tagurile wrong ï¿½ï¿½n poziï¿½ï¿½iile lor originale.""""""
    merged_tags = []

    # creï¿½ï¿½m un dicï¿½ï¿½ionar pentru tagurile wrong indexat dupï¿½ï¿½ numï¿½ï¿½rul lor original
    wrong_dict = {tag['number']: tag for tag in wrong_tags}

    # parcurgem poziï¿½ï¿½iile ï¿½ï¿½i decidem ce tag sï¿½ï¿½ punem ï¿½ï¿½n fiecare poziï¿½ï¿½ie
    current_en_idx = 0
    for i in range(max(len(ro_tags), len(en_tags))):
        position = i + 1

        # verificï¿½ï¿½m dacï¿½ï¿½ aceastï¿½ï¿½ poziï¿½ï¿½ie este pentru un tag wrong
        if position in wrong_dict:
            merged_tags.append(wrong_dict[position]['content'])
        elif current_en_idx < len(en_tags):
            merged_tags.append(en_tags[current_en_idx]['content'])
            current_en_idx += 1

    return merged_tags

def save_results(merged_content, results, output_path):
    """"""s""""
    final_content = '<!-- rezultate analiza -->\n'
    final_content += '<!-- articol start -->\n'

    # adaugï¿½ï¿½ conï¿½ï¿½inutul combinat
    for tag in merged_content:
        final_content += tag + '\n'

    final_content += '<!-- articol final -->\n'
    final_content += '<!-- final rezultate analiza -->\n'

    # adaugï¿½ï¿½ rezultatele analizei
    final_content += results

    # salveazï¿½ï¿½ ï¿½ï¿½n fiï¿½ï¿½ier
    with open(output_path, 'w', encoding='utf-8') as file:
        file.write(final_content)

# citeï¿½ï¿½te fiï¿½ï¿½ierele
with open(r'd:/3/ro/incotro-vezi-tu-privire.html', 'r', encoding='utf-8') as file:
    ro_content = file.read()

with open(r'd:/3/en/where-do-you-see-look.html', 'r', encoding='utf-8') as file:
    en_content = file.read()

# defineï¿½ï¿½te calea pentru fiï¿½ï¿½ierul de output
output_path = r'd:/3/output/where-do-you-see-look.html'

# analizeazï¿½ï¿½ tagurile
ro_tags = analyze_tags(ro_content)
en_tags = analyze_tags(en_content)

# gags(ro_tags, en_tags)

# formateazï¿½ï¿½ rezultatele
results = format_results(wrong_tags)

# genereazï¿½ï¿½ conï¿½ï¿½inutul combinat
merged_content = merge_content(ro_tags, en_tags, wrong_tags)

# afiï¿½ï¿½eazï¿½ï¿½ rezultatele ï¿½ï¿½n consolï¿½ï¿½
print(results)

# salveazï¿½ï¿½ rezultatele ï¿½ï¿½n fiï¿½ï¿½ierul de output
save_results(merged_conte",https://stackoverflow.com/questions/79160811,python,05-11-2024 22:01,53.0,0.0,2.0,True,15-11-2024 09:51,08-11-2024 07:14,Implementation Issues
77360174,map bert token indices to spacy token indices,"iï¿½ï¿½ï¿½m trying to make bertï¿½ï¿½ï¿½s (bert-base-uncased) tokenization token indices (not ids, token indices) map to spacyï¿½ï¿½ï¿½s tokenization token indices. in the following example, my approach doesnï¿½ï¿½ï¿½t work becos spacyï¿½ï¿½ï¿½s tokenization behaves a bit more complex than i anticipated. thoughts on solving this?
import spacy
from transformers import berttokenizer
tokenizer = berttokenizer.from_pretrained(""bert-base-uncased"")
nlp = spacy.load(""en_core_web_sm"")

sent = nlp(""britain's railways cost ï¿½ï¿½20.7bn during the 2020-21 financial year, with ï¿½ï¿½2.5bn generated through fares and other income, ï¿½ï¿½1.3bn through other sources and ï¿½ï¿½16.9bn from government, figures released by the regulator the office of rail and road (orr) on november 30 revealed."")
# get spacy word index to bert token indice mapping
wd_to_tok_map = [wd.i for wd in sent for el in tokenizer.encode(wd.text, add_special_tokens=false)]
len(sent) # 55
len(wd__ids = tokenizer.encode(sent.text, add_special_tokens=false)
len(input_ids) # 65

i can print both tokenizations and look for perfect text matches, but the problem i run into is what if a word repeats twice in the tokenization? looking for a word match will return two indices at different sections of the sentence.
[el.text for el in sent]
['britain', ''s', 'railways', 'cost', 'ï¿½ï¿½', '20.7bn', 'during', 'the', '2020', '-', '21', 'financial', 'year', ',', 'with', 'ï¿½ï¿½','2.5bn','generated','through', 'fares', 'and','other', 'income', ',', 'ï¿½ï¿½', '1.3bn', 'through', 'other', 'sources', 'and', 'ï¿½ï¿½', '16.9bn', 'from', 'government', ',', 'figures', 'released', 'by', 'the', 'regulator', 'the', 'office', 'of', 'rail', 'and', 'road', '(', 'orr', ')', 'on', 'november', '30', 'revealed', '.']

[tokenizer.ids_to_tokens[el] for el in input_ids]
['britain',''', 's', 'railways', 'cost', 'ï¿½ï¿½2', '##0', '.', '7', '##bn', 'during', 'the', '2020', '-', '21', 'financial', 'year', ',', '5', '##bn', 'generated', 'through', 'fares', 'and', 'other', 'income', ',', 'ï¿½ï¿½1', '.', '3', '##bn', 'through', 'other', 'sources', 'and', 'ï¿½ï¿½1', '##6', '.', '9', '##bn', 'from', 'government', ',', 'figures', 'released', 'by', 'the', 'regulator', 'the', 'office', 'of', 'rail', 'and', 'road', '(', 'orr', ')', 'on', 'november', '30', 'revealed', '.']

decode() doesnï¿½ï¿½ï¿½t seem to give me what i want, as iï¿½ï¿½ï¿½m aft","['python', 'mapping', 'spacy', 'tokenize', 'bert-language-model']",77361610,"use a fast tokenizer to get the character offsets directly from the transformer tokenizer with return_offsets_mapping=true, and then map those to the spacy tokens however you'd like:
from transformers import autotokenizer
tokenizer = autotokenizer.from_pretrained(""bert-base-uncased"")
text = ""britain's railways cost ï¿½ï¿½20.7bn""
output = tokenizer([text], return_offsets_mapping=true)

print(output[""input_ids""])
# [[101, 3725, 1005, 1055, 7111, 3465, 21853, 2692, 1012, 1021, 24700, 102]]

print(tokenizer.convert_ids_to_tokens(output[""input_ids""][0]))
# ['[cls]', 'britain', ""'"", 's', 'railways', 'cost', 'ï¿½ï¿½2', '##0', '.', '7', '##bn', '[sep]']

print(output[""offset_mapping""])
# [[(0, 0), (0, 7), (7, 8), (8, 9), (10, 18), (19, 23), (24, 26), (26, 27), (27, 28), (28, 29), (29, 31), (0, 0)]]
</code",https://stackoverflow.com/questions/77360174,python,25-10-2023 13:58,203.0,0.0,1.0,True,25-10-2023 17:10,25-10-2023 14:47,Implementation Issues
77646675,trying to create vectors and chunked data using azure cognitive search/ azure ai search,"i am trying to create a search solution using azure ai search/cognitive search. i need to chunk the data, so that the retrieved text is limited and more relevant. i also need to implement a hybrid search over the data hence trying out the embedding creation as well.
i tried using splittext and azureopenaiembedding skillsets however they are not getting indexed. my intention is to use ocr ,key phrase extraction etc and get them collated using merge skill and then do chunking and embeddings over it. also i would like to add incremental indexing [updating the indexes without redoing the whole indexing] as an additional feature.
i am not sure where i am going wrong. any help will be highly appreciated.
the following is what i tried:
def create_skillset(skillset_name, uri, headers):
    print(""trying out updated 2 set ------------------------->"")
    skillset = {
    ""description"": ""skillset created from the portal. skillsetname: azureblob-skillsetn; contentfield: merged_content; enrichmentgranularity: document; knowledgestorestorageaccount: ;"",
    ""skills"": [
        {
        ""@odata.type"": ""#microsoft.skills.text.v3.entityrecognitionskill"",
        ""name"": ""#1"",
        ""context"": ""/document/merged_content"",
        ""categories"": [
            ""product"",
            ""phonenumber"",
            ""person"",
            ""quantity"",
            ""organization"",
            ""ipaddress"",
            ""url"",
            ""email"",
            ""event"",
            ""skill"",
            ""location"",
            ""persontype"",
            ""address"",
            ""datetime""
        ],
        ""defaultlanguagecode"": ""en"",
        ""inputs"": [
            {
            ""name"": ""text"",
            ""source"": ""/document/merged_content""
            }
        ],
        ""outputs"": [
            {
            ""name"": ""persons"",
            ""targetname"": ""people""
            },
            {
            ""name"": ""organizations"",
            ""targetname"": ""organizations""
            },
            {
            ""name"": ""locations"",
            ""targetname"": ""locations""
            }
        ]
        },
        {
        ""@odata.type"": ""#microsoft.skills.text.keyphraseextractionskill"",
        ""name"": ""#2"",
        ""context"": ""/document/merged_content"",
        ""defaultlanguagecode"": ""en"",
        ""inputs"": [
            {
            ""name"": ""text"",
            ""source"": ""/document/merged_content""
            }
        ],
        ""outputs"": [
            {
            ""name"": ""keyphrases"",
            ""targetname"": ""keyphrases""
            }
        ]
        },
        {
          ""@odata.type"": ""#microsoft.skills.text.splitskill"",
          ""textsplitmode"": ""pages"",
          ""maximumpagelength"": 500,
          ""pageoverlaplength"": 50,
          ""maximumpagestotake"": 1,
          ""defaultlanguagecode"": ""en"",
          ""context"": ""/document"",
          ""inputs"": [
            {
              ""name"": ""text"",
              ""source"": ""/document/merged_text""
            }
          ],
          ""outputs"": [
            {
              ""name"": ""textitems"",
              ""targetname"": ""pages""
            }
          ]
        },
        {
        ""@odata.type"": ""#microsoft.skills.text.translationskill"",
        ""name"": ""#3"",
        ""context"": ""/document/merged_content"",
        ""defaulttolanguagecode"": ""en"",
        ""suggestedfrom"": ""en"",
        ""inputs"": [
            {
            ""name"": ""text"",
            ""source"": ""/document/merged_content""
            }
        ],
        ""outputs"": [
            {
            ""name"": ""translatedtext"",
            ""targetname"": ""translated_text""
            }
        ]
        },
        {
        ""@odata.type"": ""#microsoft.skills.text.mergeskill"",
        ""name"": ""#4"",
        ""context"": ""/document"",
        ""insertpretag"": "" "",
        ""insertposttag"": "" "",
        ""inputs"": [
            {
            ""name"": ""text"",
            ""source"": ""/document/content""
            },
            {
            ""name"": ""itemstoinsert"",
            ""source"": ""/document/normalized_images/*/text""
            },
            {
            ""name"": ""offsets"",
            ""source"": ""/document/normalized_images/*/contentoffset""
            }
        ],
        ""outputs"": [
            {
            ""name"": ""mergedtext"",
            ""targetname"": ""merged_content""
            }
        ]
        },
        {
        ""@odata.type"": ""#microsoft.skills.text.azureopenaiembeddingskill"",
        ""description"": ""connects a deployed embedding model."",
        ""resourceuri"": ""
        ""deploymentid"": ""text-embedding-ada-002"",
        ""inputs"": [
          {
            ""name"": ""text"",
            ""source"": ""/document/merged_content""
          }
        ],
        ""outputs"": [
          {
            ""name"": ""embedding""
          }
          ]
         },
        {
        ""@odata.type"": ""#microsoft.skills.vision.ocrskill"",
        ""name"": ""#5"",
        ""context"": ""/document/normalized_images/*"",
        ""lineending"": ""space"",
        ""defaultlanguagecode"": ""en"",
        ""inputs"": [
            {
            ""name"": ""image"",
            ""source"": ""/document/normalized_images/*""
            }
        ],
        ""outputs"": [
            {
            ""name"": ""text"",
            ""targetname"": ""text""
            },
            {
            ""name"": ""layouttext"",
            ""targetname"": ""layouttext""
            }
        ]
        },
        {
        ""@odata.type"": ""#microsoft.skills.vision.imageanalysisskill"",
        ""name"": ""#6"",
        ""context"": ""/document/normalized_images/*"",
        ""defaultlanguagecode"": ""en"",
        ""visualfeatures"": [
            ""tags"",
            ""description""
        ],
        ""details"": [],
        ""inputs"": [
            {
            ""name"": ""image"",
            ""source"": ""/document/normalized_images/*""
            }
        ],
        ""outputs"": [
            {
            ""name"": ""tags"",
            ""targetname"": ""imagetags""
            },
            {
            ""name"": ""description"",
            ""targetname"": ""imagecaption""
            }
        ]
        }
    ],
    ""cognitiveservices"": {
        ""@odata.type"": ""#microsoft.azure.search.cognitiveservicesbykey"",
        ""description"": """",
        ""key"": """"
    }
    }
    resp = requests.put(uri, headers=headers, data=json.dumps(skillset), verify=false)
    logging.info(f""message: {resp.status_code}\n""
                 f""message: {resp.ok}\n"")

def create_index(index_name, uri, headers):
    index = {
    ""fields"": [
        {
        ""name"": ""id"",
        ""type"": ""edm.string"",
        ""key"": true,
        ""searchable"": true,
        ""filterable"": true,
        ""facetable"": true,
        ""sortable"": true
        },
        {
        ""name"": ""metadata_storage_name"",
        ""type"": ""edm.string"",
        ""searchable"": true,
        ""filterable"": false,
        ""facetable"": false,
        ""sortable"": false
        },
        {
        ""name"": ""imagecaption"",
        ""type"": ""collection(edm.string)"",
        ""searchable"": true,
        ""filterable"": false,
        ""facetable"": false,
        ""sortable"": false
        },
        {
        ""name"": ""imagetags"",
        ""type"": ""collection(edm.string)"",
        ""searchable"": true,
        ""filterable"": false,
        ""facetable"": false,
        ""sortable"": false
        },
        {
        ""name"": ""translated_text"",
        ""type"": ""edm.string"",
        ""searchable"": true,
        ""filterable"": false,
        ""facetable"": false,
        ""sortable"": false
        },
        {
        ""name"": ""keyphrases"",
        ""type"": ""collection(edm.string)"",
        ""searchable"": true,
        ""filterable"": false,
        ""facetable"": false,
        ""sortable"": false
        },
        {
        ""name"": ""merged_content"",
        ""type"": ""edm.string"",
        ""searchable"": true,
        ""filterable"": true,
        ""facetable"": true,
        ""sortable"": true
        },
        {
        ""name"": ""pages"",
        ""type"": ""edm.string"",
        ""searchable"": true,
        ""filterable"": true,
        ""facetable"": true,
        ""sortable"": true
        },

    ]
    }
    resp = requests.put(uri, headers=headers, data=json.dumps(index), verify=false)
    logging.info(f""message: {resp.status_code}\n""
             f""message: {resp.ok}\n"")

def create_indexer(indexer_name, datasource_name, index_name, skillset_name, uri, headers):
    indexer = {
    ""name"": indexer_name,
    ""datasourcename"" : datasource_name,
    ""targetindexname"" : index_name,
    ""skillsetname"" : skillset_name,
    ""fieldmappings"" : [
        {
        ""sourcefieldname"" : ""metadata_storage_path"",
        ""targetfieldname"" : ""id"",
        ""mappingfunction"" : {""name"": ""base64encode""}
        },
        {
        ""sourcefieldname"" : ""metadata_storage_name"",
        ""targetfieldname"" : ""metadata_storage_name"",
        }
    ],
    ""outputfieldmappings"" :
    [
        {
        ""sourcefieldname"" : ""/document/merged_content"",
        ""targetfieldname"" : ""merged_content""
        },
        {
        ""sourcefieldname"" : ""/document/pages"",
        ""targetfieldname"" : ""pages""
        },
        {
        ""sourcefieldname"": ""/document/merged_content/translated_text"",
        ""targetfieldname"": ""translated_text""
        },
        {
        ""sourcefieldname"" : ""/document/merged_content/keyphrases"",
        ""targetfieldname"" : ""keyphrases""
        },
        {
        ""sourcefieldname"": ""/document/normalized_images/*/imagetags/*/name"",
        ""targetfieldname"": ""imagetags""
        },
        {
        ""sourcefieldname"": ""/document/normalized_images/*/imagecaption"",
        ""targetfieldname"": ""imagecaption""
        }
    ],
    ""parameters"":
    {
        ""maxfaileditems"": 4,
        ""maxfaileditemsperbatch"": 4,
        ""configuration"":
        {
        ""datatoextract"": ""contentandmetadata"",
        ""parsingmode"": ""default"",
        ""firstlinecontainsheaders"": true,
        ""delimitedtextdelimiter"": "","",
        ""imageaction"": ""generatenormalizedimages""
        }
    }
    }
    resp = requests.put(uri, headers=headers, data=json.dumps(indexer), verify=false)
    logging.info(f""message: {resp.status_code}\n""
                 f""message: {resp.ok}\n"")


*** i am calling these functions like this ***
# delete already exisitng datasource
    uri = f""
    resp = requests.delete(uri, headers=headers, verify=false)

    uri = f""
    # delete already existing indexer
    resp = requests.delete(uri, headers=headers, verify=false)
    con_str=""hjgjhgkj""
    blob_service_client = blobserviceclient.from_connection_string(con_str)


    container_client = blob_service_client.get_container_client(container_name)
    uri = f""
    create_datasource(datasource_name, uri, headers,container_name)
    time.sleep(15)

    uri = f""
    #delete already existing skillset
    resp = requests.delete(uri, headers=headers, verify=false)
    create_skillset(skillset_name, uri, headers)

    uri = f""
    # delete already existing index
    resp = requests.delete(uri, headers=headers, verify=false)
    create_index(index_name, uri, headers)
    uri = f""
    create_indexer(indexer_name, datasource_name, index_name, skillset_name, uri, headers)
    uri = f""
    resp = requests.get(uri, headers=headers, verify=false)


the error i get is as follows:
{""error"":{""code"":""invalidrequestparameter"",""message"":""the request is invalid. details: skillset : one or more skills are invalid. details: unexpected properties found on skill."",""details"":[{""code"":""invalidskillset"",""message"":""one or more skills are invalid. details: unexpected properties found on skill. parameters: skillset""}]}}

the spittext and openai embeddings are the two new skillsets that i added, rest of it are working, i followed the azure documentation while creating these skillsets. please advice on where am i going wrong.","['azure', 'openai-api', 'azure-cognitive-search', 'azure-openai', 'vertex-ai-search']",77655453,"the issue is that, the following 2 parameters are not available, even though it is thus mentioned in azure documentation. take these out and that will fix the splittext skill issue:
""pageoverlaplength"": 50,

""maximumpagestotake"": 1,
for azureopenaiembeddingskill, it is available only on 2023-10-01-preview api. you just need to change the api parameter while creating the skillset, it should work.",https://stackoverflow.com/questions/77646675,azure,12-12-2023 14:01,1262.0,0.0,1.0,True,13-12-2023 17:18,13-12-2023 02:48,Uncategorized
72842851,how to use cosine similarity within triplet loss,"the triplet loss is defined as follows:
l(a, p, n) = max(ï¿½ï¿½ï¿½f(a) - f(p)ï¿½ï¿½ï¿½ï¿½ï¿½ - ï¿½ï¿½ï¿½f(a) - f(n)ï¿½ï¿½ï¿½ï¿½ï¿½ + margin, 0)

where a=anchor, p=positive, and n=negative are the data samples in the loss, and margin is the minimum distance between the anchor and positive/negative samples.
i read somewhere that (1 - cosine_similarity) may be used instead of the l2 distance.
note that i am using tensorflow - and the cosine similarity loss is defined that when it is a negative number between -1 and 0, 0 indicates orthogonality and values closer to -1 indicate greater similarity. the values closer to 1 indicate greater dissimilarity. so, it is the opposite of cosine similarity metric.
any suggestions on how to write my triplet loss with cosine similarity?
edit
all good stuff in the answers (comments and answers). based on ok for me:
 self.margin = 1
 self.loss = tf.keras.losses.cosinesimilarity(axis=1)
 ap_distance = self.loss(anchor, positive)
 an_distance = self.loss(anchor, negative)
 loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)

i would like to eventually use the tensorflow addon loss as @pygeek pointed out but i haven't figured out how to pass the data yet.
note
to use it standalone - one must do something like this:
cosine_similarity = tf.keras.metrics.cosinesimilarity()
cosine_similarity.reset_state()
cosine_similarity.update_state(anch_prediction, other_prediction)
similarity = cosine_similarity.result().numpy() 

resources
pytorch cosine embedding layer
tensorflow cosine similarity implmentation
tensorflow triplet loss hard/soft margin","['machine-learning', 'deep-learning', 'nlp', 'artificial-intelligence', 'loss-function']",72879131,"first of all, cosine_distance = 1 - cosine_similarity. the distance and similarity are different. this is not correctly mentioned in some of the answers!
secondly, you should look at the tensorflow code on how the cosine similarity loss is implemented  which is different from pytorch!!
finally, i suggest you use existing loss: you should replace the || ... ||^2 with tf.losses.cosinedistance(...).",https://stackoverflow.com/questions/72842851,machine-learning,02-07-2022 22:37,5042.0,2.0,2.0,True,13-11-2022 19:28,13-11-2022 19:28,Implementation Issues
78347352,how to make conversationalretrievalchain to include metadata in the prompt using langchain with chromadb to make the llm aware of metadata?,"im trying to do a bot that answer questions from a chromadb , i have stored multiple pdf files with metadata like the filename and candidate name , my problem is when i use conversational retrieval chain the llm model just receive page_content without the metadata , i want the llm model to be aware of the page_content with its metadata like filename and candidate name
here is my code
conversation_chain=conversationalretrievalchain.from_llm(
        llm=llm,
        retriever=selfqueryretriever.from_llm(llm,vectorstore,document_content_description,metadata_field_info),
       
        memory=memory,
        verbose=true,
       
        
    )

and here is my attribute info
metadata_field_info = [
    attributeinfo(
        name=""filename"",
        description=""the name of the resumee"",
        type=""string"",

        
    ),
    attributeinfo(
        name=""candidatename"",
        description=""the name of the candidate"",
        type=""string""
    )

]","['python', 'openai-api', 'langchain', 'large-language-model']",78369897,"i did fix this issue by including document_prompt
    document_combine_prompt = prompttemplate(
     input_variables=[""candidatename"",""page_content""],
     template= """"""
        page_content: {page_content}
        candidatename:{candidatename}
        """"""
)
    conversation_chain=conversationalretrievalchain.from_llm(
        llm=llm,
        retriever=selfqueryretriever.from_llm(llm,vectorstore,document_content_description,metadata_field_info,verbose=true),
        
        memory=memory,
        verbose=true,
        return_source_documents=true,
        combine_docs_chain_kwargs={""prompt"": custom_prompt,
                                   ""document_prompt"":document_combine_prompt
                                   }
    )",https://stackoverflow.com/questions/78347352,python,18-04-2024 12:12,1233.0,0.0,1.0,True,23-04-2024 04:35,19-04-2024 06:25,Implementation Issues
77521121,validationerror: 1 validation error for sqldatabasetoolkit llm value is not a valid dict,"trying to conenct postgresql with langchain.llm used - azureopenai
from langchain.llms import azureopenai

llms = azureopenai( temperature=0,deployment_name=""gpt3turbo"".......)

toolkit = sqldatabasetoolkit(db=db,llm=llms)

error:
validationerror: 1 validation error for sqldatabasetoolkit
llm
  value is not a valid dict (type=type_error.dict)

tried different versions of langchain","['python', 'pip', 'openai-api', 'langchain', 'azure-openai']",77521354,"see if all the steps are correct from the beginning.
install packages
pip install langchain 
pip install openai
pip install psycopg2


next create a python file called main.py and import the following:
from langchain.agents import create_sql_agent 
from langchain.agents.agent_toolkits import sqldatabasetoolkit 
from langchain.sql_database import sqldatabase 
from langchain.llms.openai import openai 
from langchain.agents import agentexecutor 
from langchain.agents.agent_types import agenttype
from langchain.chat_models import chatopenai


connect to database:
pg_uri = f""postgresql+psycopg2://{username}:{password}@{host}:{port}/{mydatabase}""

set database:
db = sqldatabase.from_uri(pg_uri)

once you have your openai_api_key:
openai_api_key = ""your openai key""


define your llm model:
gpt = openai(temperature=0, openai_api_key=openai_api_key, model_name='gpt-3.5-turbo')


toolkit should be:
toolkit = sqldatabasetoolkit(db=db, llm=gpt)

information on agent types : 

credit to @dishenwang for the complete article:",https://stackoverflow.com/questions/77521121,python,21-11-2023 07:54,1348.0,2.0,1.0,True,26-05-2024 21:17,26-05-2024 21:17,Conceptual Questions
73318795,build vocab in doc2vec,"i have a list of abstracts and articles approx 500 in csv each paragraph contains approx 800 to 1000 words whenever i build vocab and print with words giving none and how i can improve results?
    lst_doc = doc.translate(str.maketrans('', '', string.punctuation))

    target_data = word_tokenize(lst_doc)

    train_data = list(read_data())

    model = gensim.models.doc2vec.doc2vec(vector_size=50, min_count=2, epochs=40)

    train_vocab = model.build_vocab(train_data)

    print(train_vocab)

   {train = model.train(train_data, total_examples=model.corpus_count, 
   epochs=model.epochs) }

output:
none","['machine-learning', 'nlp', 'word2vec', 'doc2vec']",73324980,"a call to build_vocab() only builds the vocabulary inside the model, for further usage. that function call doesn't return anything, so your train_vocab variable will be python none.
so, the behavior you're seeing is as expected, and you should say more about what your ultimate aims are, and what you'd want to see as steps towards those aims, if you're stuck.
if you want to see reporting of the progress of your calls to build_vocab() or train(), you can set the logging level to info. this is always a usually a good idea working to learn a new library: even if initially the copious info shown is hard to understand, by reviewing it you'll start to see the various internal steps, and internal counts/timings/etc, that hint whehter things are doing well or poorly.
you can also examine the state of the model and its various internal properties after the code has run.
for example, the model.wv property contains, after build_vocab(), a gensim keyedvectors structure holding all the untrained ready-for-training vectors. you can ask for its length (len(model.wv) or examine the discovered active list of words (model.wv.index_to_key).
other comments:

it's not clear your 1st two lines ï¿½ï¿½ï¿½ assigning into lst_doc and target_data ï¿½ï¿½ï¿½ affect anything further, since it's unclear what read_data() might be doing to fill the train_corpus.

often low min_count values worsen results, by including more words that have so few usage examples that they're little more than noise during training.

only 500 documents is rather small compared to most published work showing impressive results with this algorithm, which uses tens-of-thousands of documents (if not millions). so, keep in mind that results on such a smaay be unrepresentative of what's possible with a larger corpus - in terms of quality, optimal parameters, etc.",https://stackoverflow.com/questions/73318795,machine-learning,11-08-2022 10:01,449.0,0.0,1.0,True,11-08-2022 17:55,11-08-2022 11:03,Implementation Issues
65541788,how to reduce the inference time of helsinki-nlp/opus-mt-es-en (translation model) from transformer,"currently helsinki-nlp/opus-mt-es-en model takes around 1.5sec for inference from transformer. how can that be reduced?
also when trying to convert it to onxx runtime getting this error:

valueerror: unrecognized configuration class <class 'transformers.models.marian.configuration_marian.marianconfig'> for this kind of automodel: automodel.
model type should be one of retribertconfig, mt5config, t5config, distilbertconfig, albertconfig, camembertconfig, xlmrobertaconfig, bartconfig, longformerconfig, robertaconfig, layoutlmconfig, squeezebertconfig, bertconfig, openaigptconfig, gpt2config, mobilebertconfig, transfoxlconfig, xlnetconfig, flaubertconfig, fsmtconfig, xlmconfig, ctrlconfig, electraconfig, reformerconfig, funnelconfig, lxmertconfig, bertgenerationconfig, debertaconfig, dprconfig, xlmprophetnetconfig, prophetnetconfig, mpnetconfig, tapasconfig.

is it possible to convert this to onxx runtime?","['pytorch', 'huggingface-transformers', 'machine-translation']",65699717,"the opus models are originally trained with marian which is a highly optimized toolkit for machine translation written fully in c++. unlike pytorch, it does have the ambition to be a general deep learning toolkit, so it can focus on mt efficiency. the marian configurations and instructions on how to download the models are at 
the opus-mt models for huggingface's transformers are converted from the original marian models are meant more for prototyping and analyzing the models rather than for using them for translation in a production-like setup.
running the models in marian will certainly much faster than in python and it is certainly much easier than hacking transformers to run with onxx runtime. marian also offers further tricks to speed up the translation, e.g., by model quantization, which is however at the expense of the translation quality.
with both marian and tranformers, you can speed things up if you use gpu or if you narrow the beam width during decoding (attribute num_beams in the generate method in transformers).",https://stackoverflow.com/questions/65541788,pytorch,02-01-2021 17:06,3826.0,7.0,2.0,True,10-01-2022 13:56,02-01-2021 17:14,Implementation Issues
78905614,why doesn&#39;t permuting positional encodings in bert affect the output as expected?,"i am working on a jupyter notebook about transformers. in the section on positional encodings, i want to demonstrate that the transformer relies entirely on positional encoding to understand the order of the sequence. i previously learned from another question i posted that this concept only applies to models that don't use masked attention, like gpt-2. however, when i attempted the same approach with a bert model (which uses cross-attention) to predict a [mask] token, i encountered unexpected results.
what i expected to happen:

no permutation should cause the model to predict a different token, i.e., distribution a should be consistent over the vocabulary.
permuting only the input ids should return distribution b.
permuting only the positional embeddings should return distribution b.
permuting both the input ids and positional embeddings should return distribution a.

what actually happens:
sometimes the results align with my expectations, but other times, permuting one aspect (either the input ids or positional embeddings) leads to different outcomes, even though occasionally, they produce the same result.
my question is: is there something else in hugging face's bert model that might be influenced by position, beyond just the positional encoding?
for completeness, i have included the full code from this part of the notebook below, so it can be tried out directly. the important part happens in masked_prediction.
import torch
import ipywidgets as widgets
from ipython.display import display
from transformers import bertformaskedlm, autotokenizer
import matplotlib.pyplot as plt
import torch.nn.functional as f

# surpress renaming warnings
logging.getlogger(""transformers.modeling_utils"").setlevel(logging.error)
warnings.simplefilter(""ignore"", futurewarning)

tokenizer = autotokenizer.from_pretrained(""bert-base-uncased"")

input_ids = torch.tensor([[]])
tokens = []
permutation = []

output = widgets.output()

def permute_columns(matrix, permutation=none):
    n = len(permutation)
    first_n_columns = matrix[:, :n]
    permuted_columns = first_n_columns[:, permutation]
    remaining_columns = matrix[:, n:]
    new_matrix = torch.hstack((permuted_columns, remaining_columns))
    return new_matrix

def update_permutation(ordered_tags):
    global permutation
    fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]]
    
    permutation = [tokens.index(tag) for tag in fixed_tokens]
    

def tokenize(text):
    global input_ids, tokens
    input_ids = tokenizer(text, return_tensors=""pt"").input_ids
    tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]]
    
    if len(tokens) > 2:
        reorderable_tokens = tokens[1:-1]
    else:
        reorderable_tokens = []
    
    with output:
        output.clear_output(wait=true)
        tags_input.allowed_tags = reorderable_tokens
        tags_input.value = reorderable_tokens
        update_permutation(tags_input.value)

def on_tags_change(change):
    if len(change['new']) != len(tags_input.allowed_tags):
        tags_input.value = tags_input.allowed_tags  # restore original value


def masked_prediction(input_ids, permutation, permute_input, permute_encoding):
    
    with output:
        output.clear_output(wait=true)  # clear previous outputs
        
        if input_ids.numel() == 0:
            print(""you can't use an empty sequence for prediction"")
            return
        
        model = bertformaskedlm.from_pretrained(""bert-base-uncased"")
        
        if permute_encoding:
            model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.t, permutation).t
        if permute_input:
            input_ids = permute_columns(input_ids, permutation)
            
        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=false)
            
        with torch.no_grad():
            outputs = model(input_ids)
            
        logits = outputs.logits

        top_k = 5

        mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1]
        print(decoded_text, mask_token_indices, permutation)
        num_masks = len(mask_token_indices)
        if num_masks == 0:
            print(""you need to include a [mask] token for prediction"")
            return

        fig, axs = plt.subplots(1, num_masks, figsize=(15, 6))
        
        if num_masks == 1:
            axs = [axs]

        for i, idx in enumerate(mask_token_indices):
            mask_token_logits = logits[0, idx, :]

            softmax_probs = f.softmax(mask_token_logits, dim=0)

            top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0)

            predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids]
            predicted_confidences = top_token_probs.tolist()

            axs[i].bar(predicted_tokens, predicted_confidences, color='blue')
            axs[i].set_xlabel('predicted tokens')
            axs[i].set_ylabel('confidence')
            axs[i].set_title(f'masked token at position {idx.item()}')
            axs[i].set_ylim(0, 1)

        plt.show()

def on_predict_button_click(b):
    masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value)

text_input = widgets.text(placeholder='write text here to encode.', description='input:')
text_input.observe(lambda change: tokenize(change['new']), names='value')
tags_input = widgets.tagsinput(value=[], allowed_tags=[], allow_duplicates=false)

# observe changes in tags order to update the permutation and prevent deletion
tags_input.observe(on_tags_change, names='value')
tags_input.observe(lambda change: update_permutation(change['new']), names='value')

# create checkboxes for permute_input and permute_encoding
permute_input_checkbox = widgets.checkbox(value=false, description='permute inputs')
permute_encoding_checkbox = widgets.checkbox(value=false, description='permute encodings')

# create a button to trigger the prediction
predict_button = widgets.button(description=""run prediction"")
predict_button.on_click(on_predict_button_click)

# display the widgets
display(text_input)
display(tags_input)
display(permute_input_checkbox)
display(permute_encoding_checkbox)
display(predict_button)
display(output)","['python', 'pytorch', 'nlp', 'huggingface-transformers']",78906902,"the model inputs have token ids and position ids. there are four scenarios to consider:

baseline. correct order for tokens and positions
permute position ids only
permute token ids only
permute position ids and token ids

you are correct that scenario 1 and 4 should produce the same results. however you are incorrect in assuming that permuting tokens or positions separately should give the same result. consider:
# given:
tokens = [0, 1, 2]
positions = [0, 1, 2]
permutation = [2, 0, 1]

# ex1: permute tokens but not positions
[2, 0, 1] # permuted tokens
[0, 1, 2] # standard positions

# ex2: permute positions but not tokens
[0, 1, 2] # standard tokens
[2, 0, 1] # permuted positions

in ex1, the model is told that token 2 occurs at position 0. in ex2, the model is told that token 2 occurs at position 1. even though we used the same permutation, the mapping of tokens to positions is different. this results in different model outputs.
the reason you sometimes see these results line up is because you can (through random chance) sample a permutation that results in token/position embeddings lining up the same way (or mostly the same way) when permuting just one of them. this is luck - the average case produces different results.
it is simple to test this. huggingface models take a position_ids input parameter. we can use this to test permutations of the input ids without messing with the weight matrices.
to test this, we'll create input data, permute as needed, compute logits and compare logits.
when comparing logits, we will permute or depermute as needed to compare on a token to token basis. for example if token i in scenario 1 is permuted to token j in scenario 3, we want to compare logits i from scenario 1 to logits j in scenario 3.
import torch
from transformers import bertformaskedlm, autotokenizer

def get_logits(inputs):
    with torch.no_grad():
        outputs = model(**inputs)  
        logits = outputs.logits
    return logits

def permute_inputs(inputs, permutation, permute_ids=true, permute_positions=true):
    outputs = {}
    for k,v in inputs.items():
        if k=='position_ids' and permute_positions:
            outputs[k] = v[permutation]
        elif k!='position_ids' and permute_ids:
            outputs[k] = v[:,permutation]
        else:
            outputs[k] = v
            
    return outputs

# load tokenizer/model
tokenizer = autotokenizer.from_pretrained(""bert-base-uncased"")
model = bertformaskedlm.from_pretrained(""bert-base-uncased"")
model.eval() # remember to set model to eval

# create input ids and position ids
inputs = tokenizer('input text test sequence', return_tensors='pt')

inputs['position_ids'] = torch.tensor(list(range(inputs['input_ids'].shape[1])))

# create permutation tensor
permutation = torch.randperm(inputs['input_ids'].shape[1])

# compute scenario data
data = {
    's1' : { # scenario 1 - baseline
        'inputs' : inputs,
        'permuted_ids' : false
    },
    's2' : { # scenario 2 - permute positions only
        'inputs' : permute_inputs(inputs, permutation, permute_ids=false, permute_positions=true),
        'permuted_ids' : false
    },
    's3' : { # scenario 3 - permute token ids only
        'inputs' : permute_inputs(inputs, permutation, permute_ids=true, permute_positions=false),
        'permuted_ids' : true
    },
    's4' : { # scenario 4 - permute tokens and positions
        'inputs' : permute_inputs(inputs, permutation),
        'permuted_ids' : true
    }
}

# compute logits
for k,v in data.items():
    v['logits'] = get_logits(v['inputs'])

comparisons = [
    ['s1', 's2'],
    ['s1', 's3'],
    ['s1', 's4'],
    ['s2', 's3'],
    ['s2', 's4'],
    ['s3', 's4'],
]

# compare scenarios 
for sa, sb in comparisons:
    data_a = data[sa]
    data_b = data[sb]
    
    logits_a = data_a['logits']
    logits_b = data_b['logits']
    
    if data_a['permuted_ids'] == data_b['permuted_ids']:
        # either both logits are permuted or both logits are unpermuted
        # so we can compare directly
        val = (logits_a - logits_b).abs().mean()
    elif data_a['permuted_ids'] and (not data_b['permuted_ids']):
        # if `a` is permuted but `b` is not, we permute `b` to make tokens line up
        val = (logits_a - logits_b[:,permutation]).abs().mean()
    else:
        # otherwise we permute `b` to make tokens line up
        val = (logits_a[:,permutation] - logits_b).abs().mean()
        
    print(f""comparison {sa}, {sb}: {val.item():.6f}"")

the code should produce an output like:
comparison s1, s2: 1.407895
comparison s1, s3: 1.583560
comparison s1, s4: 0.000003
comparison s2, s3: 1.750883
comparison s2, s4: 1.407894
comparison s3, s4: 1.583560

run the code a bunch of times. you will find that the s1, s4 comparison always has a small deviation. this is because permuting tokens and positions together always produces the same result, ignoring small deviations caused by numeric issues.
you will find the s2, s3 comparison generally has a large deviation, but sometimes has a small deviation. as discussed, this is due to getting a lucky permutation where positions and ids mostly line up.",https://stackoverflow.com/questions/78905614,python,23-08-2024 11:12,80.0,1.0,1.0,True,23-08-2024 20:26,23-08-2024 20:26,Implementation Issues
77812092,how to resolve the error that occurs in the process of integrating the openai model made directly on the flutter app? (error: invalid_request_error),"i am a student who is interested in flutter and gpt api and developing my flutter app named 'fit buddy'.
i want to create a function that connects my own gpt model to the flutter app via api to send and receive exercise routine information using post and modify it to the appropriate exercise routine based on user response via gpt.
however, during the process of configuring the program, the error message 'invalid_request_error' appeared and the problem that the information processing did not proceed normally from the gpt was confirmed. the schema used in the gpt add action is as follows and the prompt in the flutter app is as follows. could you tell me what the problem is and how to solve it?
flutter app prompt example :
 prompt: 'model: gpt-3.5-turbo, exercisetype: leg curl, reps: 15, sets: 3, weight: 40, duration: '15 mins', userinput: 'gain weight'

schema for my gpt program:
{
    ""openapi"": ""3.0.0"",
    ""info"": {
        ""title"": ""fit buddy user routine modulator"",
        ""description"": ""generate and revise exercise routines based on user input."",
        ""version"": ""1.0.0""
    },
    ""servers"": [
        {
            ""url"": ""
        }
    ],
    ""paths"": {
        ""/gpt-3.5-turbo/completions"": {
            ""post"": {
                ""operationid"": ""reviseexerciseroutine"",
                ""requestbody"": {
                    ""required"": true,
                    ""content"": {
                        ""application/json"": {
                            ""schema"": {
                                ""type"": ""object"",
                                ""properties"": {
                                    ""model"": {
                                        ""type"": ""string"",
                                        ""example"": ""gpt-3.5-turbo""
                                    },
                                    ""prompt"": {
                                        ""type"": ""string""
                                    },
                                    `reps...`
                                },
                                ""required"": [
                                    ""exercisetype"",
                                    ""reps"",
                                    ""sets"",
                                    ""weight"",
                                    ""userinput""
                                ]
                            }
                        }
                    }
                },
                `responses`
                `components / security part`
            }
        }
    }
}

debug and flutter app tests under various conditions have been conducted, and the codes developed so far are as described above.","['flutter', 'openai-api', 'large-language-model', 'chatgpt-api']",77812105,"you're trying to use the wrong openai api endpoint. all engines api endpoints were deprecated a very, very long time ago.
change this...


...to this.


then, set the model parameter to gpt-3.5-turbo. also, don't forget to set the messages parameter. both are required for the chat completions api.
see the official openai documentation.",https://stackoverflow.com/questions/77812092,flutter,13-01-2024 15:59,127.0,0.0,1.0,True,14-01-2024 11:32,14-01-2024 11:32,Implementation Issues
76866751,i don&#39;t understand how the prompts work in llama_index,"i have been trying to query a pdf file in my local directory using llm, i have downloaded the llm model i'm using in my local system (gpt4all-13b-snoozy.ggmlv3.q4_0.bin) and trying to use langchain and hugging face's instructor-large model for embedding purpose, i was able to set the service_context and then building index but i'm not able to query , i keeping getting this error regarding prompt..

valueerror: argument prompt is expected to be a string. instead found <class 'llama_index.prompts.base.prompt'>. if you want to run the llm on multiple prompts, use generate instead.

i'm just starting to learn how to use llm, hope the community helps me....
error message part1
error message part2
from llama_index import vectorstoreindex, simpledirectoryreader
from instructorembedding import instructor
from llama_index import prompthelper, servicecontext
from llama_index import langchainembedding
from langchain.chat_models import chatopenai
from langchain.embeddings import huggingfaceembeddings
from langchain.llms import openllm
# from langchain.chat_models.human import humaninputchatmodel
from langchain import prompttemplate, llmchain
from langchain.llms import gpt4all
from langchain.callbacks.streaming_stdout import streamingstdoutcallbackhandler

documents = simpledirectoryreader(r'c:\users\avish.wagde\documents\work_avish\llm_trials\instructor_large').load_data()

model_id = 'hkunlp/instructor-large'

model_path = ""..\models\gpt4all-13b-snoozy.ggmlv3.q4_0.bin""

callbacks = [streamingstdoutcallbackhandler()]

# verbose is required to pass to the callback manager
llm = gpt4all(model = model_path, callbacks=callbacks, verbose=true)

embed_model = langchainembedding(huggingfaceembeddings(model_name = model_id))

# define prompt helper
# set maximum input size
max_input_size = 4096
# set number of output tokens
num_output = 256
# set maximum chunk overlap
max_chunk_overlap = 0.2

prompt_helper = prompthelper(max_input_size, num_output, max_chunk_overlap)

service_context = servicecontext.from_defaults(chunk_size= 1024, llm_predictor=llm, prompt_helper=prompt_helper, embed_model=embed_model)

index = vectorstoreindex.from_documents(documents, service_context= service_context)

query_engine = index.as_query_engine()

response = query_engine.query(""what is apple's finnacial situation"")
print(response)


i have been going through, the source code of the library as the error message guides but i couldn't find the problemï¿½ï¿½ï¿½","['langchain', 'huggingface', 'large-language-model', 'llama-index', 'vector-database']",76868784,"the code you have written here is a little old/erroneous. but the main error is the service context setup with llm_predictor=llm. you can just pass the llm in directly as a kwarg.
using the latest version (v0.7.22 at the time of writing) i would re-write your service context like so:
service_context = servicecontext.from_defaults(
    chunk_size= 1024, 
    llm=llm, # this is updated
    prompt_helper=prompt_helper, 
    embed_model=embed_model
)

source: 
for reference, if you pass in an llm from langchain like this, the service context detects this and wraps it with our langchain wrapper for you:
from llama_index.llms import langchainllm

llm = langchainllm(langchain_llm)

this is useful to know, since other parts of llama-index (agents, chat engines, etc.) my expect an llm object as the input, and won't wrap it for you.",https://stackoverflow.com/questions/76866751,langchain,09-08-2023 10:13,11231.0,4.0,1.0,True,09-08-2023 14:33,09-08-2023 11:27,Tool Setup/Errors
69551405,sparknlp&#39;s nercrfapproach with custom labels,"i am trying to train a sparknlp nercrfapproach model with a dataset in conll format that has custom labels for product entities (like i-prod, b-prod etc.). however, when using the trained model to make predictions, i get only ""o"" as the assigned label for all tokens. when using the same model trained on the conll data from the sparknlp workshop example, the classification works fine.
(cf. 
so, the question is: does nercrfapproach rely on the standard tag set for ner labels used by the conll data? or can i use it for any custom labels and, if yes, do i need to specify these somehow? my assumption was that the labels are inferred from the training data.
cheers,
martin
update: the issue might not be related to the labels after all. i tried to replace my custom labels with conll standard labels and i am still not getting the expected classification results.","['named-entity-recognition', 'johnsnowlabs-spark-nlp']",69565969,"as it turns out, this issue was not caused by the labels, but rather by the size of the dataset. i was using a rather small dataset for development purposes. not only was this dataset quite small, but also heavily imbalanced, with a lot more ""o"" labels than the other labels. fixing this by using a dataset of 10x the original size (in terms of sentences), i am able to get meaningful results, even for my custom labels.",https://stackoverflow.com/questions/69551405,named-entity-recognition,13-10-2021 07:33,366.0,0.0,2.0,True,18-11-2022 05:02,14-10-2021 06:01,Implementation Issues
74885225,cast features to classlabel,"i have a dataset with type dictionary which i converted to dataset:
ds = datasets.dataset.from_dict(bio_dict)
the shape now is:
dataset({
    features: ['id', 'text', 'ner_tags', 'input_ids', 'attention_mask', 'label'],
    num_rows: 8805
})

when i use the train_test_split function of datasets i receive the following error:
train_testvalid = ds.train_test_split(test_size=0.5, shuffle=true, stratify_by_column=""label"")


valueerror: stratifying by column is only supported for classlabel
column, and column label is sequence.

how can i change the type to classlabel so that stratify works?","['python', 'huggingface-transformers', 'huggingface-datasets']",75106464,"you should apply the following class_encode_column function:
ds = ds.class_encode_column(""label"")",https://stackoverflow.com/questions/74885225,python,22-12-2022 07:19,4368.0,8.0,1.0,True,13-01-2023 08:16,22-12-2022 16:05,Implementation Issues
75013624,early stopping based on bleu in fairseq,"my goal is to use bleu as early stopping metric while training a translation model in fairseq.
following the documentation, i am adding the following arguments to my training script:
--eval-bleu --eval-bleu-args --eval-bleu-detok --eval-bleu-remove-bpe

i am getting the following error:
fairseq-train: error: unrecognized arguments: --eval-bleu --eval-bleu-args --eval-bleu-detok --eval-bleu-remove-bpe

system information:

fairseq version: 0.10.2
torch: 1.10.1+cu113

more details:
when i am trying to finetune m2m100 model, i am getting error as:
keyerror: 'bleu'
when using following:
cuda_visible_devices=0,1,2,3 fairseq-train \
    $path_2_data --ddp-backend=no_c10d \
    --best-checkpoint-metric bleu \
    --maximize-best-checkpoint-metric \
    --max-tokens 2048 --no-epoch-checkpoints \
    --finetune-from-model $pretrained_model \
    --save-dir $checkpoint --task translation_multi_simple_epoch \
    --encoder-normalize-before \
    --langs 'af,am,ar,ast,az,ba,be,bg,bn,br,bs,ca,ceb,cs,cy,da,de,el,en,es,et,fa,ff,fi,fr,fy,ga,gd,gl,gu,ha,he,hi,hr,ht,hu,hy,id,ig,ilo,is,it,ja,jv,ka,kk,km,kn,ko,lb,lg,ln,lo,lt,lv,mg,mk,ml,mn,mr,ms,my,ne,nl,no,ns,oc,or,pa,pl,ps,pt,ro,ru,sd,si,sk,sl,so,sq,sr,ss,su,sv,sw,ta,th,tl,tn,tr,uk,ur,uz,vi,wo,xh,yi,yo,zh,zu' \
    --lang-pairs $lang_pairs \
    --decoder-normalize-before --sampling-method temperature \
    --sampling-temperature 1.5 --encoder-langtok src \
    --decoder-langtok --criterion label_smoothed_cross_entropy \
    --label-smoothing 0.2 --optimizer adam --adam-eps 1e-06
    --adam-betas '(0.9, 0.98)' --lr-scheduler inverse_sqrt \
    --lr 3e-05 --warmup-updates 2500 --max-update 400000 \
    --dropout 0.3 --attention-dropout 0.1 \
    --weight-decay 0.0 --update-freq 2 --save-interval 1 \
    --save-interval-updates 5000 --keep-interval-updates 10 \
    --seed 222 --log-format simple --log-interval 2 --patience 5  \
    --arch transformer_wmt_en_de_big --encoder-layers 24 \
    --decoder-layers 24 --encoder-ffn-embed-dim 8192 \
    --decoder-ffn-embed-dim 8192 --encoder-layerdrop 0.05 \
    --decoder-layerdrop 0.05 --share-decoder-input-output-embed \
    --share-all-embeddings --fixed-dictionary $fix_dict --fp16 \
    --skip-invalid-size-inputs-valid-test","['deep-learning', 'nlp', 'machine-translation', 'bleu', 'fairseq']",75015379,"the task that you are using translation_multi_simple_epoch does not have these arguments; they are specific for translation task.
note that some of the arguments that you are using require values.

--eval-bleu-args expects a path to a configuration json for sacrebleu.  if you want to you the default 4-gram bleu, you should skip this.

--eval-bleu-detok expects a specification of how you want to detokenize the model output. the default value is space which does not do anything.


for more details, see the documentation of the translation task in fairseq.",https://stackoverflow.com/questions/75013624,deep-learning,05-01-2023 03:40,499.0,1.0,1.0,True,05-01-2023 08:12,05-01-2023 08:07,Preprocessing Tasks
71284177,how to remove words from a sentence that carry no positive or negative sentiment?,"im trying a sentiment analysis based approach on youtube comments, but the comments many times have words like mrbeast, tiger/'s, lion/'s, pewdiepie, james, etc which do not add any feeling in the sentence. i've gone through nltk's average_perception_tagger but it didn't work well as it gave the results as
my input:
""mrbeast james lion tigers bad sad clickbait fight nice good""

words that i need in my sentence:
""bad sad clickbait fight nice good""

what i got using average_perception_tagger:
[('mrbeast', 'nn'),
 ('james', 'nns'),
 ('lion', 'jj'),
 ('tigers', 'nns'),
 ('bad', 'jj'),
 ('sad', 'jj'),
 ('clickbait', 'nn'),
 ('fight', 'nn'),
 ('nice', 'rb'),
 ('good', 'jj')]


so as you can see if i remove mrbeast i.e nn the words like clickbait, fight will also get removed which than ultimately remove expressions from that sentence.","['python', 'machine-learning', 'nlp', 'sentiment-analysis']",73273796,"there are multiple ways of doing this like

you can create a set of positive and negative words and for each word in your grammar you can check if it exists in your set, if it does you should keep the word, else delete it. this however would first require all positive and negative words dataset.

you can use something like textblob which can give you the sentiment score of a word or a sentence. so with a cutoff sentiment score you can filter out the words that you don't need.",https://stackoverflow.com/questions/71284177,python,27-02-2022 10:59,1112.0,-2.0,2.0,True,19-06-2024 11:53,27-02-2022 21:13,Implementation Issues
75173490,how can i check similarity in meaning and not just having same words between two texts with spacy,"i'm trying to compare two different textsï¿½ï¿½ï¿½one coming from a curriculum vitae (cv) and the other from a job announcement.
after cleaning the texts, i'm trying to compare them to detect if a job announcement is more linked to a specific cv.
i am trying to do this using similarity matching in spacy via the following code:
similarity = pdf_text.similarity(final_text_from_annonce)

this works well, but i'm getting strange results from two different cvs for the same job announcement. specifically, i get the same similarity score (~0.6), however, one should clearly be higher than the other.
i checked on spacy website and i found this very important sentence:

vector averaging means that the vector of multiple tokens is insensitive to the order of the words. two documents expressing the same meaning with dissimilar wording will return a lower similarity score than two documents that happen to contain the same words while expressing different meanings.

so, what do i need to use or code to make spacy compare my two texts based on their meaning instead of the occurrence of words?
i am expecting a parameter for the similarity function of spacy, or another function that will compare my both texts and calculate a similarity score based on the meaning of the texts and not if the same words are used.","['python', 'nlp', 'spacy', 'similarity', 'semantics']",75193330,"the spacy library by default will use the average of the word embeddings of words in a sentence to determine semantic similarity. this can be thought of as a naive sentence embedding approach. such an approach could work, but if you were to use it is recommended that you first filter non-meaningful words (e.g. common words) to prevent them from undesirably influencing the final sentence embeddings.
the alternative (and more reliable) solution is to use a different pipeline within spacy that has been designed to use sentence embeddings created specifically with a dedicated sentence encoder (e.g. the universal sentence encoder (use) [1] by cer et al.). martino mensio created a package called spacy-universal-sentence-encoder that makes use of this model. install it via the following command in your command prompt:
pip install spacy-universal-sentence-encoder

then you can compute the semantic similarity between sentences as follows:
import spacy_universal_sentence_encoder

# load one of the models: ['en_use_md', 'en_use_lg', 'xx_use_md', 'xx_use_lg']
nlp = spacy_universal_sentence_encoder.load_model('en_use_lg')

# create two documents
doc_1 = nlp('hi there, how are you?')
doc_2 = nlp('hello there, how are you doing today?')

# use the similarity method to compare the full documents (i.e. sentences)
print(doc_1.similarity(doc_2))  # output: 0.9356049733134972
# or make the comparison using a predefined span of the second document 
print(doc_1.similarity(doc_2[0:7])) # output: 0.9739387861159459

as a side note, when you run the nlp = spacy_universal_sentence_encoder.load_model('en_use_lg') command for the first time, you may have to do so with administrator rights to allow tensorflow to create the models folder in c:\program files\python310\lib\site-packages\spacy_universal_sentence_encoder and download the appropriate model. if you don't, it is possible that there will be a permissiondeniederror and the code will not run.
references
[1] cer, d., yang, y., kong, s.y., hua, n., limtiaco, n., john, r.s., constant, n., guajardo-cespedes, m., yuan, s., tar, c. and sung, y.h., 2018. universal sentence encoder. arxiv preprint arxiv:1803.11175.",https://stackoverflow.com/questions/75173490,python,19-01-2023 14:06,1656.0,2.0,1.0,True,27-01-2023 09:53,27-01-2023 09:53,Conceptual Questions
76601293,calculating similarity score in contexto.me clone,"i am currently trying to clone the popular browser game contexto.me and i am having trouble with as to how to calculate the similarity score between two words (the target word and the user inputted guess word). i am able to get the cosine similarity between the two words, but as to how to properly quantify the score into a clean integer like in the game, i am confused as to how it is done.
for example, if the target word is 'helicopter' and i guess the word plane, contexto will return something like a similarity score of 13, but if i guess a word like 'king' contexto will return a score of '2000' for instance.
target_word = ""helicopter""
glove = torchtext.vocab.glove(name=""6b"", dim=100)


@app.route('/', methods=[""get"", ""post""])
def getsimscore():
    if request.method == ""post"":
        text = request.form.get(""word"")
        new_text = singularize(text)
        sim_score = ((torch.cosine_similarity(glove[target_word].unsqueeze(0), glove[new_text].unsqueeze(0))).numpy()[0])
        print(sim_score)
    return render_template('homepage.html', messagetext='sample text', gamenum=1, guessnum=1, wordaccuracy=999)

this is my code so far with sim_score printing to be ~0.77 for the input 'truck' and ~0.29 for the input 'king' (closer to 1 the more similar the word is to the target word).","['python', 'python-3.x', 'nlp', 'stanford-nlp', 'torch']",76601396,"for example, if the target word is 'helicopter' and i guess the word plane, contexto will return something like a similarity score of 13, but if i guess a word like 'king' contexto will return a score of '2000' for instance.

this metric is typically called ""rank,"" and you can calculate it with the following algorithm.

compute the similarity score of every word the user can enter.
sort this list.
given a specific score, find what position it appears on the list. if the score appears at index 0, then it is rank 1. if it appears at index 4, then  it is rank 5, and so on.

for speed, steps 1 and 2 can be computed ahead of time, if you want.",https://stackoverflow.com/questions/76601293,python,03-07-2023 00:44,441.0,0.0,1.0,True,03-07-2023 01:32,03-07-2023 00:51,Implementation Issues
47725035,lemmatization with apache lucene,"i'm developing a text analysis project using apache lucene. i need to lemmatize some text (transform the words to their canonical forms). i've already written the code that makes stemming. using it, i am able to convert the following sentence

the stem is the part of the word that never changes even when morphologically inflected; a lemma is the base form of the word. for example, from ""produced"", the lemma is ""produce"", but the stem is ""produc-"". this is because there are words such as production

into

stem part word never chang even when morpholog inflect lemma base form word exampl from produc lemma produc stem produc becaus word product

however, i need to get the base forms of the words: example instead of exampl, produce instead of produc, and so on.
i am using lucene because it has analyzers for many languages (i need at least english and russian). i know about stanford nlp library, but it has no russian language support.
so is there any way to do lemmatization for several languages like i do stemming using lucene?
the simplified version of my code responsible for stemming:
//using apache tika to identify the language
languageidentifier identifier = new languageidentifier(text);
//getting analyzer according to the language (eg, englishanalyzer for 'en')
analyzer analyzer = getanalyzer(identifier.getlanguage());
tokenstream stream = analyzer.tokenstream(""field"", text);
stream.reset();
while (stream.incrementtoken()) {
    string stem = stream.getattribute(chartermattribute.class).tostring();
    // doing something with the stem
    system.out.print(stem+ "" "");
}
stream.end();
stream.close();

update: i found the library that does almost what i need (for english and russian languages) and uses apache lucene (although in its own way), it's definitely worth exploring.","['java', 'lucene', 'nlp', 'stemming', 'lemmatization']",62033221,"in case someone still needs it, i decided to return to this question and illustrate how to use the russianmorphology library i found earlier to do lemmatization for english and russian languages.
first of all, you will need these dependencies (besides the lucene-core):
<!-- if you need russain -->
<dependency>
    <groupid>org.apache.lucene.morphology</groupid>
    <artifactid>russian</artifactid>
    <version>1.1</version>
</dependency>

<!-- if you need english-->
<dependency>
    <groupid>org.apache.lucene.morphology</groupid>
    <artifactid>english</artifactid>
    <version>1.1</version>
</dependency>

<dependency>
    <groupid>org.apache.lucene.morphology</groupid>
    <artifactid>morph</artifactid>
    <version>1.1</version>
</dependency>

then, make sure you import the right analyzer:
import org.apache.lucene.morphology.english.englishanalyzer;
import org.apache.lucene.morphology.russian.russiananalyzer;

these analyzers, unlike standard lucene analyzers, use morphologyfilter which converts each word into a set of its normal forms.
so if you use the following code
string text = ""the stem is the part of the word that never changes even when morphologically inflected; a lemma is the base form of the word. for example, from \""produced\"", the lemma is \""produce\"", but the stem is \""produc-\"". this is because there are words such as production"";
analyzer analyzer = new englishanalyzer();
tokenstream stream = analyzer.tokenstream(""field"", text);
stream.reset();
while (stream.incrementtoken()) {
    string lemma = stream.getattribute(chartermattribute.class).tostring();
    system.out.print(lemma + "" "");
}
stream.end();
stream.close();

it will print

the stem be the part of the word that never change even when
morphologically inflected inflect a lemma be the base form of the word
for example from produced produce the lemma be produce but the stem be
produc this be because there are be word such as production

and for the russian text
string text = ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½. ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½. ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½reýýýýýýýýýýýýýýýýýýýý

yo may notice that some words have more that one base form, e.g. inflected is converted to [inflected, inflect]. if you don't like this behaviour, you would have to change the implementation of the org.apache.lucene.morphology.analyzer.morhpologyfilter (if you are interested in how exactly to do it, let me know and i'll elaborate on this).
hope it helps, good luck!",https://stackoverflow.com/questions/47725035,java,09-12-2017 03:29,4280.0,10.0,2.0,True,28-01-2022 21:37,31-12-2020 07:15,Implementation Issues
77446419,callback in llm chain doesn&#39;t get executed,"the following code do not do what it is supposed to do:
from langchain.callbacks.base import basecallbackhandler
from langchain import prompttemplate
from langchain.chains import llmchain
from langchain.llms import vertexai


class mycustomhandler(basecallbackhandler):
    def on_llm_end(self, event, context):
        print(f""prompt: {event.prompt}"")
        print(f""response: {event.response}"")


llm = vertexai(
            model_name='text-bison@001',
            max_output_tokens=1024,
            temperature=0.3,
            verbose=false)
prompt = prompttemplate.from_template(""1 + {number} = "")
handler = mycustomhandler()
chain = llmchain(llm=llm, prompt=prompt, callbacks=[handler])
response = chain.run(number=2)
print(response)

based on this documentation and this tutorial, the code should execute the custom handler callback on_llm_end but in fact it doesn't.
can anyone please tell me why?","['python', 'langchain', 'google-cloud-vertex-ai', 'py-langchain']",77769356,"i did some research and found the solution.
you need to pass callback parameter to llm itself. in your case you need to change the code as below
callback_handler  = mycustomhandler()
llm = vertexai(
               model_name='text-bison@001',
               max_output_tokens=1024,
               temperature=0.3,
               callbacks=[callback_handler]
               verbose=false)

secondly change the implementation of on_llm_end as below
class mycustomhandler(basecallbackhandler):
    def on_llm_end(self, response, **kwargs):
        print(f""response: {response}"")

this should fix the problem.",https://stackoverflow.com/questions/77446419,python,08-11-2023 14:26,2785.0,1.0,4.0,True,08-01-2024 12:17,18-11-2023 01:56,Implementation Issues
37524799,what is the best way to do natural language processing in rails app?,"i have a rails app. i need to implement automatic text categorization algorithms and possibly more nlp capabilities in app. i believe ruby does not have good nlp tools available as python has. i am using a separate resque server for process background jobs. i believe i have following 

run python scripts using resque jobs
run a flask application on a separate server which can either talk to resque job or can automatically update the app database with processed results. 
use ruby tools mentioned in this thread
any other suggestions welcome

please let me know what is the best way to do it. are there any similar working examples?","['ruby-on-rails', 'nlp', 'nltk']",37526239,"i had the same problem a few months ago. after a bit of research and testing this is the solution i implemented 
run several python processes as many as one machine can hold. and use as many machines as you need.
use zeromq to communicate between the web servers and the machines running python processes
do not use http to communicate because the overhead is significant and it will be very slow compared to zeromq. you will also not need an as complex handler with zeromq as you would with http
take care to expose zeromq sockets to internal networks only, or you would need to set up authentication on each python server
another option is to just use one of the many available nlp apis, if don't need any corpus based algorithms (such as pos tagging, sentiment analysis, etc).",https://stackoverflow.com/questions/37524799,ruby-on-rails,30-05-2016 11:41,580.0,1.0,1.0,True,27-02-2023 15:47,27-02-2023 15:47,Conceptual Questions
70123519,valueerror: unknown url type: &#39;languagetool-3.2.zip&#39;,"i am trying to install pycontractions library which is dependent on language-check library. so when installing language-check i am getting the below error
valueerror: unknown url type: 'languagetool-3.2.zip

i have python-3.10 and java-8 installed. when i am using pip install language-check, i get urllib.error. http error 403: forbidden so, i downloaded language tool manually(as instructed in github link) and run the following command:
pip install git+

i tried all the possible ways available here but still couldnt resolve it. its getting installed on mac with no issues but i am a windows user. so i need to install it in my windows desktop.
error:
collecting git+
  cloning  to c:\users\skamble\appdata\local\temp\pip-req-build-c78lrf2e
  running command git clone --filter=blob:none -q  'c:\users\skamble\appdata\local\temp\pip-req-build-c78lrf2e'
  resolved  to commit 58e419833ef28a9193fcaa21193616a8a14504a9
  preparing metadata (setup.py) ... done
using legacy 'setup.py install' for language-check, since package 'wheel' is not installed.
installing collected packages: language-check
    running setup.py install for language-check ... error
    error: command errored out with exit status 1:
     command: 'c:\users\skamble\appdata\local\programs\python\python310\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'c:\\users\\skamble\\appdata\\local\\temp\\pip-req-build-c78lrf2e\\setup.py'""'""'; __file__='""'""'c:\\users\\skamble\\appdata\\local\\temp\\pip-req-build-c78lrf2e\\setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.stringio('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'c:\users\skamble\appdata\local\temp\pip-record-msctmso8\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\users\skamble\appdata\local\programs\python\python310\include\language-check'
         cwd: c:\users\skamble\appdata\local\temp\pip-req-build-c78lrf2e\
    complete output (23 lines):
    traceback (most recent call last):
      file ""<string>"", line 1, in <module>
      file ""c:\users\skamble\appdata\local\temp\pip-req-build-c78lrf2e\setup.py"", line 595, in <module>
        sys.exit(main())
      file ""c:\users\skamble\appdata\local\temp\pip-req-build-c78lrf2e\setup.py"", line 590, in main
        run_setup_hooks(config)
      file ""c:\users\skamble\appdata\local\temp\pip-req-build-c78lrf2e\setup.py"", line 561, in run_setup_hooks
        language_tool_hook(config)
      file ""c:\users\skamble\appdata\local\temp\pip-req-build-c78lrf2e\setup.py"", line 584, in language_tool_hook
        download_lt()
      file ""c:\users\skamble\appdata\local\temp\pip-req-build-c78lrf2e\download_lt.py"", line 131, in download_lt
        with closing(urlopen(url)) as u:
      file ""c:\users\skamble\appdata\local\programs\python\python310\lib\urllib\request.py"", line 216, in urlopen
        return opener.open(url, data, timeout)
      file ""c:\users\skamble\appdata\local\programs\python\python310\lib\urllib\request.py"", line 503, in open
        req = request(fullurl, data)
      file ""c:\users\skamble\appdata\local\programs\python\python310\lib\urllib\request.py"", line 322, in __init__
        self.full_url = url
      file ""c:\users\skamble\appdata\local\programs\python\python310\lib\urllib\request.py"", line 348, in full_url
        self._parse()
      file ""c:\users\skamble\appdata\local\programs\python\python310\lib\urllib\request.py"", line 377, in _parse
        raise valueerror(""unknown url type: %r"" % self.full_url)
    valueerror: unknown url type: 'languagetool-3.2.zip'
    ----------------------------------------
error: command errored out with exit status 1: 'c:\users\skamble\appdata\local\programs\python\python310\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'c:\\users\\skamble\\appdata\\local\\temp\\pip-req-build-c78lrf2e\\setup.py'""'""'; __file__='""'""'c:\\users\\skamble\\appdata\\local\\temp\\pip-req-build-c78lrf2e\\setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.stringio('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'c:\users\skamble\appdata\local\temp\pip-record-msctmso8\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\users\skamble\appdata\local\programs\python\python310\include\language-check' check the logs for full command output.","['python-3.x', 'url', 'pip', 'nlp', 'urllib']",70161691,i resolved it. used git+ and it worked.,https://stackoverflow.com/questions/70123519,python-3.x,26-11-2021 11:13,289.0,0.0,1.0,True,29-11-2021 21:44,26-11-2021 22:17,Implementation Issues
78739644,how to specify the langsmith project name for each chain?,"according to the langsmith documentation you need to set the langchain_project environment variable to specify the project name in langsmith.
however, if we want to execute multiple chains within a single service but under different project names, how do we specify the project name at runtime per-chain?
we think we have a solution, but it required looking into the langchain code, so we are concerned this is not the idomatic way to do it and it might cause issues when their api changes.
this is what we think we should do.

instantiate the chain.
instantiate a custom langchain tracer but with a specific project name defined at runtime.
replace the chain's callbacks with the tracer.

        chain = llmchain(llm=self._chat_model, prompt=prompt, verbose=true)
        tracer = langchaintracer(project_name=""whatever"")
        chain.callbacks = callbackmanager(handlers=[tracer])

thank you in advance for any help.","['langchain', 'langsmith']",78813387,"you can specify project name in the @traceable decorator as an argument.
@traceable(
    run_type=""llm"",
    project_name=""my project""
)
def my_llm_call(
    ...
) -> str:

you can also put it as a part of langsmith_extra when calling the traced function. this takes precedence over the traceable project:
my_llm_call(
   ...,
   langsmith_extra={""project_name"": ""my project 2""},
)",https://stackoverflow.com/questions/78739644,langchain,12-07-2024 09:40,514.0,0.0,1.0,True,30-07-2024 19:24,12-07-2024 12:33,Implementation Issues
79375287,gpu utilization almost always 0 during training hugging face transformer,"i am fine-tuning a donut cord-v2 model with my invoice data which is around 360 gb in size when preprocessed and saved on disk as a dataset. i am following this notebook almost exactly, except i have 6 training epochs instead of 3.
i am training on single nvidia h100 sxm gpu / intel xeonï¿½ï¿½ gold 6448y / 128 gb ram.
whenever i start training, and inspect cpu and gpu utilization using htop and nvidia-smi, i see that cpu is at 10-12% utilization, used by python, gpu memory is almost 90% filled constantly, but gpu utilization is almost always 0. if i keep refreshing the output of nvidia-smi, once every 10-12 seconds the utilization will jump to 100% and then go back to 0 immediately. i cant help but feel ther eis a bottleneck between my cpu and gpu, where cpu attempts to constantly process data and send it to gpu, gpu processes it very fast, and just idles, awaiting for the next batch from cpu. i load already pre-processed dataset from disk like so:
from datasets import load_from_disk
processed_dataset = load_from_disk(r""/dataset/dataset_final"")

my processor configas follows:
from transformers import donutprocessor

new_special_tokens = [] # new tokens which will be added to the tokenizer
task_start_token = ""<s>""  # start of task token
eos_token = ""</s>"" # eos token of tokenizer

processor = donutprocessor.from_pretrained(""naver-clova-ix/donut-base-finetuned-cord-v2"")

# add new special tokens to tokenizer
processor.tokenizer.add_special_tokens({""additional_special_tokens"": new_special_tokens + [task_start_token] + [eos_token]})

# we update some settings which differ from pretraining; namely the size of the images + no rotation required
processor.feature_extractor.size = [1200,1553] # should be (width, height)
processor.feature_extractor.do_align_long_axis = false

my model config is:
import torch
from transformers import visionencoderdecodermodel, visionencoderdecoderconfig

#print(torch.cuda.is_available())

# load model from huggingface.co
model = visionencoderdecodermodel.from_pretrained(""naver-clova-ix/donut-base-finetuned-cord-v2"")

# resize embedding layer to match vocabulary size
new_emb = model.decoder.resize_token_embeddings(len(processor.tokenizer))
print(f""new embedding size: {new_emb}"")
# adjust our image size and output sequence lengths
model.config.encoder.image_size = processor.feature_extractor.size[::-1] # (height, width)
model.config.decoder.max_length = len(max(processed_dataset[""train""][""labels""], key=len))

# add task token for decoder to start
model.config.pad_token_id = processor.tokenizer.pad_token_id
model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['<s>'])[0]

and my training code is:
import gc
gc.collect()

torch.cuda.empty_cache()


from transformers import seq2seqtrainingarguments, seq2seqtrainer

import logging
logging.basicconfig(level=logging.info)

# arguments for training
training_args = seq2seqtrainingarguments(
    output_dir=r""/trained"",  # specify a local directory to save the model
    num_train_epochs=6,
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    weight_decay=0.01,
    fp16=true,
    logging_steps=50,
    save_total_limit=2,
    evaluation_strategy=""no"",
    save_strategy=""epoch"",
    predict_with_generate=true,
    report_to=""none"",
    # disable push to hub
    push_to_hub=false
   
)

# create trainer
trainer = seq2seqtrainer(
    model=model,
    args=training_args,
    train_dataset=processed_dataset[""train""],
)


# start training
trainer.train()

the estimated time to complete the training with 6 epochs, with 360 gb dataset, is 54 hours. when i run the same exact code on my pc that has intel i9 11900kf / rtx 3050, i see gpu utilization constantly at 100%. is there a bottleneck in my code? why does cpu keep processing so much on already preprocessed dataset? cuda 12.6
edit:
does it make sense to change the dataloader_num_workers parameter of seq2seqtrainer to >0 value, since my ram and cpu core count allows it? (and since cpu utilization is at 10-12% max.)","['python', 'machine-learning', 'huggingface-transformers']",79378532,"you seem to have an io bottleneck. it means the data cannot be transfered fast enough and your gpu ends up waiting for the data most of the time.
you can verify that claim by checking the status of the python workers in htop.
you do not seem to have a cpu bottleneck because your cpu isn't fully used.
this often happens on vms when the data is being transfered using old protocols like nfs.
if the vm you're using has a local disk, you can try copying the data there before the training, and point your huggingface dataset to that local path.
this could also be due to a suboptimal configuration of the data loading process. you might want to give this a read.
you might not be seeing this issue on your pc because:

your gpu is slower than an h100 hence takes more time to process a single batch. as a result, your system has more time to load the next batch.
your data is stored in your local disk and therefore the time to load the data is much smaller.

and yes, please increase your number of workers, it can drastically improve the performance.",https://stackoverflow.com/questions/79375287,python,21-01-2025 17:09,236.0,1.0,1.0,True,22-01-2025 16:34,21-01-2025 20:11,Data Wrangling
47687797,when to remove stop words when using bigram_measures like pmi?,"i need to verify an overall approach to dealing with bigram stop words that are returned from bigram_measures such as pmi. why deal with these stop words? well, they're noise and donï¿½ï¿½ï¿½t add any additional value past a certain point.
i've seen several specific examples of how to use bigram_measures. however, i'm wondering when it's best to remove stop word in the overall process of cleaning data, expansion, lemmatizing/stemming, etc.
and yes, i am using a corpus that is sufficiently large. i remember the size of your corpus will also affect the quality of the bigram_measures result.
based on the accepted answer in this post (nltk - counting frequency of bigram) it seems that stop words could be removed after pmi or other bigram_measures are used on the corpus.

""imagine that if filtering collocations was simply deleting them, then there were many probability measures such as liklihood ratio or the pmi itself (that compute probability of a word relative to other words in a corpus) which would not function properly after deleting words from random positions in the given corpus. by deleting some collocations from the given list of words, many potential functionalities and computations would be disabled...""

therefore, i believe the best process is:

clean the text and remove garbage chars like html tags, etc.
expand contractions (e.g.: they're -> they are)
lemmatize or stem to normalize the words
calculate bigrams using bigram_measures like pmi. you can calculate bigrams using other methods, but this is what i'm using.
apply a frequency filter like ""apply_freq_filter(n)"" to get the bigrams that occur above your threshold. note this will still return some bigrams with stop words mixed in with valuable bigrams.
check to see if both words are stop words. if yes, then don't include that bigram in the final results but leave them in the corpus for the reasons quoted above.

is this a correct overall approach to dealing with bigram stop words mixed in with valuable bigrams?","['python', 'nlp', 'nltk']",48491681,"one approach is to:

clean the text
expand contractions
lemmatize
remove stop words
run pmi or other measure to score n-grams.

source: text analytics with python, pg 224.
my purpose in providing the source above is to show where i received this answer from rather than providing some ungrounded answer.",https://stackoverflow.com/questions/47687797,python,07-12-2017 04:49,3459.0,4.0,1.0,True,29-10-2023 18:33,29-10-2023 18:32,Preprocessing Tasks
78213463,running chatgpt programmatically - how to continue conversation without re-submitting all past messages?,"one can obtain a chatgpt response to a prompt using the following example:
from openai import openai

client = openai()  # requires key in open_ai_key environment variable

completion = client.chat.completions.create(
  model=""gpt-3.5-turbo"",
  messages=[
    {""role"": ""system"", ""content"": ""you are a poetic assistant, skilled in explaining complex programming concepts with creative flair.""},
    {""role"": ""user"", ""content"": ""compose a poem that explains the concept of recursion in programming.""}
  ]
)

print(completion.choices[0].message.content)

how can one continue the conversation? i've seen examples saying you just add a new message to the list of messages and re-submit:
# continue the conversation by including the initial messages and adding a new one
continued_completion = client.chat.completions.create(
    model=""gpt-3.5-turbo"",
    messages=[
        {""role"": ""system"", ""content"": ""you are a poetic assistant, skilled in explaining complex programming concepts with creative flair.""},
        {""role"": ""user"", ""content"": ""compose a poem that explains the concept of recursion in programming.""},
        {""role"": ""assistant"", ""content"": initial_completion.choices[0].message.content},  # include the initial response
        {""role"": ""user"", ""content"": ""can you elaborate more on how recursion can lead to infinite loops if not properly handled?""}  # new follow-up prompt
    ]
)

but i would imagine this means processing the previous messages all over again at every new prompt, which seems quite wasteful. is that really the only way? isn't there a way to keep a ""session"" of some sort that keeps chatgpt's internal state and just processes a newly given prompt?","['python', 'openai-api', 'langchain', 'chatgpt-api']",78214798,"from here

conversation summary
now let's take a look at using a slightly more complex type of memory

conversationsummarymemory. this type of memory creates a summary of the conversation over time. this can be useful for condensing
information from the conversation over time. conversation summary
memory summarizes the conversation as it happens and stores the
current summary in memory. this memory can then be used to inject the
summary of the conversation so far into a prompt/chain. this memory is
most useful for longer conversations, where keeping the past message
history in the prompt verbatim would take up too many tokens.


in a conversational context where you want to maintain continuity and provide context to openai's model, you need to send some form of conversation history. this history helps the model understand the ongoing dialogue and generate responses that are contextually relevant.",https://stackoverflow.com/questions/78213463,python,24-03-2024 05:11,1460.0,2.0,1.0,True,25-03-2024 03:00,24-03-2024 22:13,Implementation Issues
76768113,permissionerror while executing langchain with azureopenai,"i'm working with azureopenai and langchain, constantly getting hit by permissionerror. this mostly could be due to the proxy, but can someone please check the code --
from langchain.llms import openai, azureopenai
from langchain.prompts import prompttemplate
from langchain.chains import llmchain

llm = azureopenai(openai_api_type="""", openai_api_base="""", deployment_name="""", model_name="""", openai_api_key="""", openai_api_version="""")

template = """"""""
translate the following text from {source_lang} to {dest_lang}: {source_text}
""""""

prompt_name = prompttemplate(input_variables=[""source_lang"", ""dest_lang"", ""source_text""], template=template)
chain = llmchain(llm=llm, prompt=prompt_name)

chain.predict(source_lang=""english"", dest_lang=""spanish"", source_text=""how are you?"")

chain(inputs={""source_lang"": ""english"", ""dest_lang"": ""spanish"", ""source_text"": ""how are you""})

i also tried the additional openai_proxy parameter without much luck.","['azure', 'openai-api', 'langchain', 'azure-openai']",76815315,"your code runs smoothly! below my llm config. other variables are set with .env
llm=azurechatopenai(
    deployment_name=azure_openai_chatgpt_deployment,
    temperature=0.0,
)",https://stackoverflow.com/questions/76768113,azure,26-07-2023 04:50,627.0,1.0,1.0,True,31-08-2023 17:06,26-07-2023 06:30,Implementation Issues
75650840,openai chat completions api error 400: &quot;bad request&quot; (migrating from gpt-3 api to gpt-3.5 api),"i'm trying to call the chat completions api that was just released, but i'm getting a bad request error.

    var body = new
                    {
                        model = ""gpt-3.5-turbo"",
                        messages = data
                    };

                    string jsonmessage = jsonconvert.serializeobject(body);

  using ( client = new 
                    {
                        servicepointmanager.securityprotocol = securityprotocoltype.tls12;

                         requestmessage = new
                         ""
                        {
                            content = new stringcontent(jsonmessage, encoding.utf8, ""application/json"")
                        };

                        string api_key = pageextension_currentuser.community.caichatgptapikey.length > 30 ? pageextension_currentuser.community.caichatgptapikey : genesis.generic.readappsettingsvalue(""chatgptapikey"");
                        requestmessage.headers.add(""authorization"", $""bearer {api_key}"");

                         response = client.sendasync(requestmessage).result;
                        if (response.statuscode == 
                        {
                            string responsedata = response.content.readasstringasync().result;
                            dynamic responseobj = jsonconvert.deserializeobject(responsedata);
                            string choices = responseobj.choices[0].text;
                           
                    }


there is the code from their api documentation:
curl  \
  -h 'content-type: application/json' \
  -h 'authorization: bearer your_api_key' \
  -d '{
  ""model"": ""gpt-3.5-turbo"",
  ""messages"": [{""role"": ""user"", ""content"": ""hello!""}]
}'

here is another example:
openai.chatcompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
        {""role"": ""system"", ""content"": ""you are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""who won the world series in 2020?""},
        {""role"": ""assistant"", ""content"": ""the los angeles dodgers won the world series in 2020.""},
        {""role"": ""user"", ""content"": ""where was it played?""}
    ]
)

can anyone see why i'm getting the following error?
{statuscode: 400, reasonphrase: 'bad request', version: 1.1, content: system.net. headers:
{
  connection: keep-alive
  access-control-allow-origin: *
  openai-organization: user-lmjzqj7ba2bggaekkhr68aqn
  openai-processing-ms: 141
  openai-version: 2020-10-01
  strict-transport-security: max-age=15724800; includesubdomains
  x-request-id: 9eddf8bb8dcc106ca11d44ad7f8bbecc
  date: mon, 06 mar 2023 12:49:46 gmt
  content-length: 201
  content-type: application/json
}}



{method: post, requesturi: ' version: 1.1, content: system.net. headers:
{
  authorization: bearer sk-ihuxxxxxxxxxxxxxxxxxx[just removed my api key]xxxxxxxxxxxxxxx
  content-type: application/json; charset=utf-8
  content-length: 79
}}","['post', 'openai-api', 'chatgpt-api']",75650860,"you're using the gpt-3.5-turbo model.
there are three main differences between the chat completions api (i.e., the gpt-3.5 api) and the completions api (i.e., the gpt-3 api):

api endpoint

completions api: 
chat completions api: 


the prompt parameter (completions api) is replaced by the messages parameter (chat completions api)
response access

completions api: response.getjsonarray(""choices"").getjsonobject(0).getstring(""text"")
chat completions api: response.getjsonarray(""choices"").getjsonobject(0).getstring(""message"")




problem 1: you're using the wrong api endpoint
change this (completions api)...


...to this (chat completions api).



problem 2: make sure the json for the messages parameter is valid

curl: ""messages"": [{""role"": ""user"", ""content"": ""hello!""}]

python: messages = [{""role"": ""user"", ""content"": ""hello!""}]

nodejs: messages: [{role: ""user"", content: ""hello world""}]



problem 3: you're accessing the response incorrectly
note: openai nodejs sdk v4 was released on august 16, 2023, and is a complete rewrite of the sdk. among other things, there are changes in extracting the message content. see the v3 to v4 migration guide.
change this...
string choices = responseobj.choices[0].text;

...to this.
string choices = responseobj.choices[0].message.content;


problem 4: you didn't set the content-type header
add this:
requestmessage.headers.add(""content-type"", ""application/json"");

note: ""application/json, utf-8"" won't work, as @srishti mentioned in the comment below.",https://stackoverflow.com/questions/75650840,post,06-03-2023 12:26,24081.0,6.0,4.0,True,12-06-2024 17:20,12-06-2024 17:20,Implementation Issues
76434311,how to get the logits of the model with a text classification pipeline from huggingface?,"i need to use pipeline in order to get the tokenization and inference from the distilbert-base-uncased-finetuned-sst-2-english model over my dataset.
my data is a list of sentences, for recreation purposes we can assume it is:
texts = [""this is the first sentence"", ""of my data."", ""in fact, thats not true,"", ""but we are going to assume it"", ""is""]
before using pipeline, i was getting the logits from the model outputs like this:
with torch.no_grad():
     logits = model(**tokenized_test).logits

now i have to use pipeline, so this is the way i'm getting the model's output:
 selected_model = ""distilbert-base-uncased-finetuned-sst-2-english""
 tokenizer = autotokenizer.from_pretrained(selected_model)
 model = automodelforsequenceclassification.from_pretrained(selected_model, num_labels=2)
 classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
 print(classifier(text))

which gives me:
[{'label': 'positive', 'score': 0.9746173024177551}, {'label': 'negative', 'score': 0.5020197629928589}, {'label': 'negative', 'score': 0.9995120763778687}, {'label': 'negative', 'score': 0.9802979826927185}, {'label': 'positive', 'score': 0.9274746775627136}]
and i cant get the 'logits' field anymore.
is there a way to get the logits instead of the label and score? would a custom pipeline be the best and/or easiest way to do it?","['python', 'huggingface-transformers', 'sentiment-analysis', 'huggingface', 'large-language-model']",76435401,"when you use the default pipeline, the postprocess function will usually take the softmax, e.g.
from transformers import autotokenizer, automodelforsequenceclassification

tokenizer = autotokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')
model = automodelforsequenceclassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')


text = ['hello this is a test',
 'that transforms a list of sentences',
 'into a list of list of sentences',
 'in order to emulate, in this case, two batches of the same lenght',
 'to be tokenized by the hf tokenizer for the defined model']

classifier(text, batch_size=2, truncation=""only_first"")

[out]:
[{'label': 'negative', 'score': 0.9379090666770935},
 {'label': 'positive', 'score': 0.9990271329879761},
 {'label': 'negative', 'score': 0.9726701378822327},
 {'label': 'negative', 'score': 0.9965035915374756},
 {'label': 'negative', 'score': 0.9913086891174316}]

so what you want is to overload the postprocess logic by inheriting from the pipeline.
to check which pipeline the classifier inherits do this:
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
type(classifier)

[out]:
transformers.pipelines.text_classification.textclassificationpipeline

now that you know the parent class of the task pipeline you want to use, now you can do this and still enjoy the perks of the precoded batching from textclassificationpipeline:
from transformers import textclassificationpipeline

class mariotheplumber(textclassificationpipeline):
    def postprocess(self, model_outputs):
        best_class = model_outputs[""logits""]
        return best_class

pipe = mariotheplumber(model=model, tokenizer=tokenizer)

pipe(text, batch_size=2, truncation=""only_first"")

[out]:
[tensor([[ 1.5094, -1.2056]]),
 tensor([[-3.4114,  3.5229]]),
 tensor([[ 1.8835, -1.6886]]),
 tensor([[ 3.0780, -2.5745]]),
 tensor([[ 2.5383, -2.1984]])]",https://stackoverflow.com/questions/76434311,python,08-06-2023 17:26,4458.0,8.0,1.0,True,12-06-2023 00:41,08-06-2023 20:15,Implementation Issues
76633913,"if an fst transition is based on a given context, how can it be called as &#39;non deterministic&#39;?","i am going through the paper 'speech recognition with weighted finite-state transducers' (hbka.pdf - 
at page 9, and figure 6
i do not understand how 6 a) figure be considered for 'non-determinism'.
[here ae/k t represents the triphonic model for ae with left context k and right context t. this transition is considered as non-deterministic. why?]
the differences between transitions that are termed 'non-deterministic' and 'deterministic' are

nondeterministic transitions allow (epsilon transitions) a transition to happen between states without any input symbol consumption and/or any output symbol generation.

this is especially useful where the transducer is supposed to output english word morphological description from the input english word, like in cities -> city -pl. here there are transitions between states where you input and output the same alphabet. but when you input i or e you need not output anything and in the final transition for input s you would output y -pl
so, i understand the need for epsilon transitions. i also understand that non-deterministic transitions are when multiple transitions for same input label can exist. this could lead to ambiguity and hence supports the name 'non-deterministic'.

deterministic transitions are the very opposite of the above. no multiple transitions for same input label; hence every transition is unique. no epsilon transitions.

with this limited knowing, i could not decipher why a transition could ever be called 'non-deterministic' when you have provided the context. here ae/k t represents the triphonic model for ae with left context k and right context t. this transition is considered as non-deterministic. why?
the main idea of providing context is to remove the ambiguity.","['nlp', 'speech-recognition', 'speech-to-text', 'state-machine', 'kaldi']",76633959,"the text to the right of the colon : is the output of the transition, not its input; so it doesn't help determine which transition to take, but rather, you need to take the right transition in order to get the right output.",https://stackoverflow.com/questions/76633913,nlp,07-07-2023 04:16,83.0,0.0,1.0,True,07-07-2023 04:30,07-07-2023 04:19,Uncategorized
78921595,does the answer quality of openai gpt4o (from api) change over time?,"i'm creating an application that uses gpt-4 (via the openai api) for visual question answering.
the problem is that tests using this module, which previously passed, have started to fail constantly due to a decline in the quality of the answers. is this expected behavior for gpt-4 (or the openai api)?","['openai-api', 'chatgpt-api', 'gpt-4', 'chat-gpt-4']",78936797,"if you use the gpt-4o as a model identifier in your application, then yes, the underlying model might change.
let's take gpt-4o as an example:

as i write this, it points to gpt-4o-2024-05-13
but soon it will point to the newer gpt-4o-2024-08-06

it's a good question whether the quality of the answer can decrease in newer models. companies use various metrics to ensure it doesn't happen. and the blind tests rating of ai models generally show that people rank newer models higher.
at the same time, it's easy to find anecdotal opinions that some previous models were better. at first, my impression too was that gpt-4 was better than the newer gpt-4o, but when i did a blind test, i preferred answers from gpt-4o.
if you prefer to stick to tested legacy model version, you can still use more specific id like gpt-4o-2024-05-13.",https://stackoverflow.com/questions/78921595,openai-api,28-08-2024 05:01,361.0,1.0,1.0,True,01-09-2024 07:31,28-08-2024 07:54,Implementation Issues
70449122,change last layer on pretrained huggingface model,"i want to re-finetuned a transformer model but i get an unknown error when i tried to train the model.
i can't change the ""num_labels"" on loading the model.
so, i tried to change it manually
model_name = ""mrm8488/flaubert-small-finetuned-movie-review-sentiment-analysis""
tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforsequenceclassification.from_pretrained(model_name).to('cuda')

num_labels = 3
model.sequence_summary.summary = torch.nn.linear(in_features=model.sequence_summary.summary.in_features, out_features=num_labels, bias=true)




trainer = trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train['train'],
    eval_dataset=tokenized_test['train'],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    #data_collator=data_collator,
)

trainer.train()

the error
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
<ipython-input-93-8139f38c5ec6> in <module>()
     20 )
     21 
---> 22 trainer.train()

7 frames
/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)
   2844     if size_average is not none or reduce is not none:
   2845         reduction = _reduction.legacy_get_string(size_average, reduce)
-> 2846     return torch._c._nn.cross_entropy_loss(input, target, weight, _reduction.get_enum(reduction), ignore_index, label_smoothing)
   2847 
   2848 

valueerror: expected input batch_size (24) to match target batch_size (16).","['python', 'pytorch', 'torch', 'huggingface-transformers']",70450157,"so, there is a solution for this
just add ignore_mismatched_sizes=true when loading the model as:
model = automodelforsequenceclassification.from_pretrained(model_name,num_labels=3, ignore_mismatched_sizes=true).to('cuda')",https://stackoverflow.com/questions/70449122,python,22-12-2021 12:33,3401.0,2.0,2.0,True,25-12-2021 18:14,25-12-2021 18:14,Task-specific Help
66895997,pattern extract using regex in python,"i am trying to use regex for word extraction in python, since i am beginner and not experienced in regex i want you to help me, i have this string :
deadline for nsf-bsf programs in elementary particle physics ï¿½ï¿½ï¿½ theory; particle astrophysics and cosmology ï¿½ï¿½ï¿½ theory; quantum information science (nsf deadline is dec. 14)

and i want the output to be a list of area or research in this word, so the output should be:
[elementary particle physics, particle astrophysics and cosmology, quantum information science]

could any one give regular expression to identify this pattern using re.findall().
thanks in a","['python', 'regex', 'nlp']",66896188,"assuming that (1) ' in ' is the indicator that the words you're interested in are are starting, and that (2) all areas are separated by ';' and that (3) all areas end with  - theory or something in parenthesis, we can get the list you're looking for. note: these are the assumptions that must be consistent across all input sources if the below code is to work as expected.
import re

src = ""deadline for nsf-bsf programs in elementary particle physics - theory; "" \
      ""particle astrophysics and cosmology - theory; "" \
      ""quantum information science (nsf deadline is dec. 14)""

_, out = src.split(' in ')
out = [re.split(r'( - theory)|\(.*\)', o)[0].strip() for o in out.split(';')]

print(out)

out:
['elementary particle physics',
 'particle astrophysics and cosmology',
 'quantum information science']",https://stackoverflow.com/questions/66895997,python,31-03-2021 22:13,87.0,-1.0,1.0,True,24-11-2021 05:58,31-03-2021 22:29,Implementation Issues
71512301,"error: could not build wheels for spacy, which is required to install pyproject.toml-based projects","hi guys, i am trying to install spacy model == 2.3.5 but i am getting this error, please help me!","['python', 'python-3.x', 'pip', 'nlp', 'spacy']",74540622,"i had the similar error while executing pip install -r requirements.txt but for aiohttp module:
socket.c -o build/temp.linux-armv8l-cpython-311/aio
aio fatal error: 'longintrepr.h' file not found
#include ""longintrepr.h""                                   
          ^~~~~~~                        1 error generated.
error: command '/data/data/com.termux/files/usr/bin/arm-linux-androideabi-clang' 
failed with exit code 1
[end of output]
note: this error originates from a subprocess, and is likely not a problem with pip.
error: failed building wheel for aiohttp
failed to build aiohttp
error: could not build wheels for aio which is required to install
pyproject.toml-based projects

just in case i will leave here solution to my error. this error is specific to python 3.11 version. on python with 3.10.6 version installation went fine.
to solve it i needed to update requirements.txt.
not working versions of modules with python 3.11:
aio
yarl==1.4.2
frozenlist==1.3.0

working versions:
aio
yarl==1.8.1
frozenlist==1.3.1

links to the corresponding issues with fixes:",https://stackoverflow.com/questions/71512301,python,17-03-2022 12:29,47328.0,5.0,4.0,True,27-05-2024 06:20,27-05-2024 06:20,Tool Setup/Errors
73234626,how to keep structure of text after feeding it to a pipeline for ner,"i've build an ner (named entity recognition) model, based on a huggingface existing model and that i fine-tuned to recognize my custom entities. the text i want to run my model on is in a txt file.
the code of how i use the model:
from transformers import pipeline

# loading the fine-tuned model
ner_pipeline =  pipeline('token-classification', model=""./my-model.model/"", tokenizer=""./my-model.model/"", ignore_labels=[])

with open(my_file, 'r', encoding=""utf8"") as f:
  lines = f.readlines()
  joined_lines = ' '.join(lines)

  result = ner_pipeline(joined_lines, aggregation_strategy='first')
  text = """"
      
  for group in result:
     if group[""entity_group""] != 'o':
        #ï¿½ï¿½substitute the entity with its tag
        text += group[""entity_group""]+ "" ""
     else:
        text += group[""word""] + "" ""

basically what i do is substituting the entities recognized with the entity tag, and leave the rest of the text as is.
with my code, the final text is filled with the content exactly as i want it, but the structure is lost. while doing ' '.join(lines) i'm basically throwing away the \nde>s inside the text, that however i would like to keep in my reconstructed text.
i've tried feeding the pipeline with single sentences (each of the f.readlines()) end not the full joined text, but the results are far worse. the model works a lot better predicting on the whole text.
does anyone knows a way how i could keep or retrieve the structure of the original text? thanks.","['python', 'nlp', 'tokenize', 'huggingface-transformers', 'named-entity-recognition']",73259456,"the groups have a start and end index that tell you which part of the input string each label corresponds to. i.e., you can pass the text as a whole, with the newlines intact (ner_pipeline(f.read(), ...)) and subsequently replace substrings.
here's a working, minimal reproducible example. the only thing to note here is that we replace from right to left (result[::-1]) so we don't mess up the indices of subsequent labels by changing the length of the string when replacing.
from nltk.corpus import brown # for example data
from transformers import pipeline

ner_pipeline =  pipeline('token-classification')

# equivalent to f.read()
text = '\n'.join(' '.join(sent) for sent in brown.sents()[:100])

result = ner_pipeline(lines_joined, aggregation_strategy='first')

def replace_at(label, start, end, txt):
    """"""replace substring of txt from start to end with label""""""
    return ''.join((txt[:start], label, txt[end:]))

# substitution
for group in result[::-1]:
    ent = group[""entity_group""]
    if ent != 'org': # for testing since there's no 'o' in the default model
        text = replace_at(ent, group['start'], group['end'], text)

sentences = text.split('\n')

example input/output (first line):
""the fulton county grand jury said friday an investigation of atlanta's recent primary election produced `` no evidence '' that any irregularities took place .""

after processing:
""the fulton county grand jury said friday an investigation of loc's recent primary election produced `` no evidence '' that any irregularities took place .""
                                                              ^^^",https://stackoverflow.com/questions/73234626,python,04-08-2022 10:51,646.0,1.0,1.0,True,06-08-2022 11:46,04-08-2022 13:06,Implementation Issues
76160057,openai chat completions api: how do i customize answers from gpt-3.5 or gpt-4 models if i can&#39;t fine-tune them?,"we have seen some companies use gpt-3.5 or gpt-4 models to train their own data and provide customized answers. but gpt-3.5 and gpt-4 models are not available for fine-tuning.
i've seen the document from openai about this issue, but i had seen openai only allow fine-tuning davinci, for example.
how do i customize answers from a gpt-3.5 or gpt-4 model if i can't fine-tune them?","['openai-api', 'chatgpt-api', 'gpt-4']",76161653,"they don't fine-tune the gpt-3.5 or gpt-4 models.
you have two options.

option 1: using llamaindex or langchain
what they do is use llamaindex (formerly gpt-index) or langchain. both of them enable you to connect openai models with your existing data sources.

option 2: using the openai embeddings api endpoint
see my past answer. also, as @peter_the_oak mentioned, you can use pinecone to store embedding vectors. pinecone is designed specifically for handling this type of data.",https://stackoverflow.com/questions/76160057,openai-api,03-05-2023 02:27,2401.0,3.0,2.0,True,20-02-2024 15:00,20-02-2024 15:00,Implementation Issues
58900022,get the topics or the keywords of a sentence,"good morning all
does anyone of you know a tool or an api or something that takes a sentence as input and as output, it gives the topics or keywords of this sentence?
i tried textrazor in the online demo it works well like you can see in the screenshot 

but when i used as a library in my python code it always gives me a blank list even for the sentence used in the demo
this is my code in python:
import textrazor
import ssl
textrazor.api_key =""bdd69bdc3f91045cdb6d4261d39df34d887278602cb8f60401b7eb0b""
client = textrazor.textrazor(extractors=[""entities"", ""topics""])
client.set_cleanup_mode(""cleanhtml"")
client.set_classifiers([""textrazor_newscodes""])
sentence = ""adam hill,b it's super bowl sunday  pastors. get your jesus jukes ready! guilt is an awesome motivator! #sarcasm""
response = client.analyze(sentence)
print(sentence)
print(len(response.topics()))
entities = list(response.entities())
print(len(entities))
for topic in response.topics():
    if topic.score > 0.3:
        print (topic.label)

it gives me zero for entities and topics length
someone proposed for me to use opennlp but i didn't get how to extract topics and keywords if anyone of you has any tutorial or clarification please help me  
and thank you in advance","['python', 'nlp', 'data-analysis']",58900447,"you have to remove the line client.set_cleanup_mode(""cleanhtml""). then it should work just fine.
as i understand the cleanup_mode, it treads your text as an html. as your example text is not html it won't find any raw text in between html tags.",https://stackoverflow.com/questions/58900022,python,17-11-2019 11:33,842.0,0.0,1.0,True,29-12-2023 15:01,29-12-2023 15:01,Preprocessing Tasks
72814534,error when passing argument through function for converting pandas dataframe of tweets into corpus files,"i want to prepare my text data that is in a pandas dataframe for sentiment analysis with nltk. for that, i'm using code for a function that converts each row of a pandas dataframe into a corpus.
import nltk
# convert each row of the pandas dataframe of tweets into corpus files
def createcorpusfromdataframe(corpusfolder,df):
    for index, r in df.iterrows():
        date=r['date']
        tweet=r['text']
        place=r['place']
        fname=str(date)+'_'+'.txt'
        corpusfile=open(corpusfolder+'/'+fname,'a')
        corpusfile.write(str(tweet) +"" "" +str(date))
        corpusfile.close()
createcorpusfromdataframe(myfolder,mydf)

the problem is i keep getting the message that
nameerror: name 'myfolder' is not defined

even though i have a folder called 'myfolder' in the same path directory of jupyter notebook that my code is in?
update:
i can see now that the issue was simply that i needed to pass the folder name as a string. now that i've done that and amended my code. the problem i have now is that the contents of the text file created with the function are not being written into a corpus and the type of variable being created is a 'nonetype'.
import nltk
# convert each row of the pandas dataframe of tweets into corpus files
def createcorpusfromdataframe(corpusfolder,df):
    for index, r in df.iterrows():
        id=r['date']
        tweet=r['text']
        #place=r['place']
        #fname=str(date)+'_'+'.txt'
        fname='tweets'+'.txt'
        corpusfile=open(corpusfolder+'/'+fname,'a')
        corpusfile.write(str(tweet) +"" "")
        corpusfile.close()
corpus df = createcorpusfromdataframe('myfolder',mydf)
type(corpusdf)
nonetype","['python', 'pandas', 'function', 'nltk']",72814664,"problem
you are passing myfolder as a variable to your function which you have not defined in your code and hence it raises a nameerror.
solution
just replace it with 'myfolder' [pass it as a string].
createcorpusfromdataframe('myfolder',mydf)",https://stackoverflow.com/questions/72814534,python,30-06-2022 10:52,47.0,0.0,1.0,True,30-06-2022 16:07,30-06-2022 16:07,Conceptual Questions
75503124,bitwise operation on a dynamic data structure,"i am implementing a simple document indexer for information retrieval. now i need to implement an incidence matrix, that should be able to be extended dynamically (not satisfied with just a static array or smth).
and to make boolean search possible i have to be able to perform bitwise operations on the rows of the matrix. however, i have not come up with a fast solution. the question is data structure for each row of the matrix.
if it were just a std::vector<bool>, is it possible to do fast bitwise operations on it? or is there any other data structure, like bitarray from c#, applicable in the situation?","['c++', 'bitwise-operators', 'information-retrieval', 'adjacency-matrix', 'boolean-search']",75503280,"if fast is your goal, look into using largest int available on your system (likely - uint64_t) and do a simple bitwise operations on that. if your matrix is wider that 64 - use an std::array of those. then check if your compiler generates simd instructions from your code. if not - consider using intrinsics",https://stackoverflow.com/questions/75503124,c++,19-02-2023 20:10,109.0,1.0,1.0,True,19-02-2023 20:36,19-02-2023 20:24,Implementation Issues
73124816,split data frame of comments into multiple rows,"i have a data frame with long comments and i want to split them into indiviual sentences using spacy sentencizer.
comments = pd.read_excel('comments.xlsx', sheet_name = 'sheet1')  
comments
>>>
         reviews
    0    one of the rare films where every discussion leaving the theater is about how much you 
         just had, instead of an analysis of its quotients.
    1    gorgeous cinematography, insane flying action sequences, thrilling, emotionally moving, 
         and a sequel that absolutely surpasses its predecessor. well-paced, executed & has that 
         re-watchability factor.


i loaded the model like this
import spacy
nlp = spacy.load(""en_core_news_sm"")

and using sentencizer
from spacy.lang.en import english
nlp = english()
nlp.add_pipe('sentencizer')
data = comments.reviews.apply(lambda x : list( nlp(x).sents))

but when i check the sentence is in just one row like this
[one of the rare films where every discussion leaving the theater is about how much you just had.,
 instead of an analysis of its quotients.]

thanks a lot for any help. i'm new using nlp tools in data frame.","['python', 'dataframe', 'lambda', 'nlp', 'spacy']",73175632,"currently, data is a series whose rows are lists of sentences, or actually, lists of spacy's span objects. you probably want to obtain the text of these sentences and to put each sentence on a different row.
comments = [{'reviews': 'this is the first sentence of the first review. and this is the second.'},
            {'reviews': 'this is the first sentence of the second review. and this is the second.'}]

comments = pd.dataframe(comments) # building your input dataframe

+----+--------------------------------------------------------------------------+
|    | reviews                                                                  |
|----+--------------------------------------------------------------------------|
|  0 | this is the first sentence of the first review. and this is the second.  |
|  1 | this is the first sentence of the second review. and this is the second. |
+----+--------------------------------------------------------------------------+

now let's define a function which, given a string, returns the list of its sentences as texts (strings).
def obtain_sentences(s):
    doc = nlp(s)
    sents = [sent.text for sent in doc.sents]
    return sents

the function can be applied to the comments dataframe to produce a new dataframe containing sentences.
data = comments.copy()
data['reviews'] = comments.apply(lambda x: obtain_sentences(x['reviews']), axis=1)
data = data.explode('reviews').reset_index(drop=true)
data

i used explode to transform the elements of the lists of sentences into rows.
and this is the obtained output!
+----+--------------------------------------------------+
|    | reviews                                          |
|----+--------------------------------------------------|
|  0 | this is the first sentence of the first review.  |
|  1 | and this is the second.                          |
|  2 | this is the first sentence of the second review. |
|  3 | and this is the second.                          |
+----+--------------------------------------------------+",https://stackoverflow.com/questions/73124816,python,26-07-2022 14:02,169.0,1.0,1.0,True,30-07-2022 12:31,26-07-2022 14:09,Conceptual Questions
76435173,categorize rows per their similarity in python,"i am here to look for input for a data manipulation problem related to natural language processing.
to make life easier, i am using a mock dataset posted several years ago from how to group text data based on document similarity?.
import pandas as pd
from difflib import sequencematcher

df = pd.dataframe({'questions': ['what are you doing?','what are you doing tonight?','what are you doing now?','what is your name?','what is your nick name?','what is your full name?','shall we meet?',
                             'how are you doing?' ]})

def similarity_score(s1, s2):
    return sequencematcher(none, s1, s2).ratio()

def similarity(x,df):
    sim_score = []
    for i in df['questions']:
        sim_score.append(similarity_score(x,i))
    return sim_score

df['similarity'] = df['questions'].apply(lambda x : similarity(x, df)).astype(str)
print(df)

the output is as following
questions  \
0          what are you doing?   
1  what are you doing tonight?   
2      what are you doing now?   
3           what is your name?   
4      what is your nick name?   
5      what is your full name?   
6               shall we meet?   
7           how are you doing?   

                                          similarity  
0  [1.0, 0.8260869565217391, 0.9047619047619048, ...  
1  [0.8260869565217391, 1.0, 0.84, 0.533333333333...  
2  [0.9047619047619048, 0.84, 1.0, 0.585365853658...  
3  [0.6486486486486487, 0.5333333333333333, 0.585...  
4  [0.5714285714285714, 0.52, 0.5217391304347826,...  
5  [0.5714285714285714, 0.52, 0.5652173913043478,...  
6  [0.36363636363636365, 0.34146341463414637, 0.3...  
7  [0.8108108108108109, 0.6666666666666666, 0.731...  

the logic is that i go through each row in the data frame to compare it to all over rows (including itself) in order to compute their similarity. i then store the similarity score as a list in another column called ""similarity"".
next, i want to categorize the questions in the first column. if the similarity score > 0.9, then those rows should be assigned to the same group. how can i achieve this?","['python', 'pandas', 'nlp', 'aggregate', 'grouping']",76436371,"a solution is to iterate row-wise over your similarity scores, create a binary mask based on some threshold, and then use the binary mask to only extract those questions who meet the threshold.
note that this solution presumes that the ""groups"" you desire are the questions themselves (i.e. for each question, you want a list of similar questions associated with it). i made up similarity scores for the rest of the array to create this minimal example.
solution
import pandas as pd

orig_data = {
    ""questions"": [
        ""what are you doing?"",
        ""what are you doing tonight?"",
        ""what are you doing now?"",
        ""what is your name?"",
        ""what is your nick name?"",
        ""what is your full name?"",
        ""shall we meet?"",
        ""how are you doing?"",
    ],
    ""similarity"": [
        [1.0, 0.826, 0.905, 0.234, 0.544, 0.673, 0.411, 0.45],
        [0.826, 1.0, 0.84, 0.533, 0.444, 0.525, 0.641, 0.62],
        [0.905, 0.84, 1.0, 0.585, 0.861, 0.685, 0.455, 0.65],
        [0.649, 0.533, 0.585, 1.0, 0.901, 0.902, 0.642, 0.234],
        [0.571, 0.52, 0.522, 0.901, 1.0, 0.905, 0.753, 0.786],
        [0.571, 0.52, 0.565, 0.902, 0.903, 1.0, 0.123, 0.586],
        [0.364, 0.341, 0.3, 0.674, 0.584, 0.421, 1.0, 0.544],
        [0.811, 0.667, 0.731, 0.345, 0.764, 0.242, 0.55, 1.0],
    ],
}

df = pd.dataframe(orig_data)

results = []
for idx, sim_row in enumerate(df[""similarity""]):
    bin_mask = [true if score > 0.9 else false for score in sim_row]
    curr_q = df[""questions""][idx]
    sim_quests = [q for q, b in zip(df[""questions""], bin_mask) if b and q != curr_q]
    results.append(sim_quests)

df[""similar-questions""] = results
print(df)

output
                     questions  ...                                  similar-questions
0          what are you doing?  ...                          [what are you doing now?]
1  what are you doing tonight?  ...                                                 []
2      what are you doing now?  ...                              [what are you doing?]
3           what is your name?  ...  [what is your nick name?, what is your full na...
4      what is your nick name?  ...      [what is your name?, what is your full name?]
5      what is your full name?  ...      [what is your name?, what is your nick name?]
6               shall we meet?  ...                                                 []
7           how are you doing?  ...                                                 []",https://stackoverflow.com/questions/76435173,python,08-06-2023 19:35,226.0,1.0,1.0,True,08-06-2023 23:56,08-06-2023 23:53,Conceptual Questions
74922924,how to add threshold limit to tf-idf values in a sparse matrix,"i am using sklearn.feature_extraction.text, tfidftransformer to get the tf_idf values for my corpus.
this is how my code looks like
    x = dataset[:,0]
    y = dataset[:,1]

    for index, item in enumerate(x):
        reqjson = json.loads(item, object_pairs_hook=ordereddict)
        x[index] = json.dumps(reqjson, separators=(',', ':'))
    count_vect = countvectorizer()
    x_train_counts = count_vect.fit_transform(x)


    tfidf_transformer = tfidftransformer()
    x_train_tfidf = (tfidf_transformer.fit_transform(x_train_counts))

    #(58720, 167216) is the size of my sparse matrix


    for i in range (0,58720):
        for j in range (0,167216):
            print(i,j)
            if x_train_tfidf[i,j]>0.35:
                x_train_tfidf[i,j]=0

as you can see that i want to filter out tf-idf values which more than 0.35 so that i can reduce my feature set and make my model more time efficient but using a for loop just makes worse. i have looked into the documentation of tfidftransformer but cannot find a way to make it any better. any ideas or tips? thank you.","['python', 'scikit-learn', 'nlp', 'tf-idf']",74923600,"it sounds like this question is trying to ignore frequent words.
the tfidfvectorizer (not tfidftransformer) implementation includes a max_df parameter for:

when building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).

in the following example, word1 and word3 occur in >50% of documents, so setting max_df=0.5 means the resulting array only includes word2:
from sklearn.feature_extraction.text import tfidfvectorizer

raw_data = [
    ""word1 word2 word3"",
    ""word1 word1 word1"",
    ""word2 word2 word3"",
    ""word1 word1 word3"",
]

vect = tfidfvectorizer(max_df=0.5)
x = vect.fit_transform(raw_data)

print(vect.get_feature_names_out())
print(x.todense())

['word2']
[[1.]
 [0.]
 [1.]
 [0.]]",https://stackoverflow.com/questions/74922924,python,26-12-2022 18:14,702.0,2.0,1.0,True,26-12-2022 19:57,26-12-2022 18:20,Implementation Issues
79330283,can&#39;t compile marian nmt,"i'm using endeavouros. i'm trying to compile marian with these instructions:  but it fails.
the error message seemingly indicates a conflict between the code and c++20. but in all the cmakelists.txt files of the repo, there is the line set (cmake_cxx_standard 11).
these are the steps that i followed:
git clone 
mkdir marian/build
cd marian/build
cmake ..
make -j4

this is the result i had:
ï¿½ï¿½ï¿½ make -j4
[  1%] built target 3rd_party_installs
[  1%] built target marian_version
[  6%] built target sentencepiece_train-static
[ 19%] built target libyaml-cpp
[ 25%] built target sqlitecpp
[ 25%] built target pathie-cpp
[ 32%] built target zlib
[ 35%] built target intgemm
[ 35%] built target faiss
[ 53%] built target sentencepiece-static
[ 55%] built target spm_decode
[ 55%] built target spm_normalize
[ 55%] built target spm_encode
[ 55%] building cxx object src/cmakefiles/marian.dir/common/aliases.cpp.o
[ 55%] building cxx object src/cmakefiles/marian.dir/common/fastopt.cpp.o
[ 56%] built target spm_train
[ 57%] built target spm_export_vocab
[ 57%] building cxx object src/cmakefiles/marian.dir/common/utils.cpp.o
[ 58%] building cxx object src/cmakefiles/marian.dir/common/logging.cpp.o
in file included from /data/tools/marian/src/3rd_spdlog/details/spdlog_impl.h:12,
                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,
                 from /data/tools/marian/src/common/logging.h:5,
                 from /data/tools/marian/src/common/definitions.h:3,
                 from /data/tools/marian/src/common/fastopt.h:3,
                 from /data/tools/marian/src/common/fastopt.cpp:1:
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  138 |     registry_t<mutex>() {}
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ï¿½ï¿½ï¿½< >ï¿½ï¿½ï¿½
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  139 |     registry_t<mutex>(const registry_t<mutex>&) = delete;
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/re:22: note: remove the ï¿½ï¿½ï¿½< >ï¿½ï¿½ï¿½
in file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,
                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,
                 from /data/tools/marian/src/common/logging.h:5,
                 from /data/tools/marian/src/common/utils.cpp:2:
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  138 |     registry_t<mutex>() {}
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ï¿½ï¿½ï¿½< >ï¿½ï¿½ï¿½
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  139 |     registry_t<mutex>(const registry_t<mutex>&) = delete;
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove tle included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,
                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,
                 from /data/tools/marian/src/common/logging.h:5,
                 from /data/tools/marian/src/common/logging.cpp:1:
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  138 |     registry_t<mutex>() {}
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ï¿½ï¿½ï¿½< >ï¿½ï¿½ï¿½
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  139 |     registry_t<mutex>(const registry_t<mutex>&) = delete;
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ï¿½ï¿½ï¿½< >ï¿½ï¿½ï¿½
in file included from rd_party/spdlog/details/spdlog_impl.h:12,
                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,
                 from /data/tools/marian/src/common/logging.h:5,
                 from /data/tools/marian/src/common/definitions.h:3,
                 from /data/tools/marian/src/common/cli_wrapper.h:6,
                 from /data/tools/marian/src/common/config_parser.h:4,
                 from /data/tools/marian/src/common/aliases.cpp:1:
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  138 |     registry_t<mutex>() {}
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ï¿½ï¿½ï¿½< >ï¿½ï¿½ï¿½
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  139 |     registry_t<mutex>(const registry_t<mutex>&) = delet                    ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ï¿½ï¿½ï¿½< >ï¿½ï¿½ï¿½
cc1plus: all warnings being treated as errors
make[2]: *** [src/cmakefiles/marian.dir/build.make:93: src/cmakefiles/marian.dir/common/fastopt.cpp.o] error 1
make[2]: *** waiting for unfinished jobs....
cc1plus: all warnings being treated as errors
make[2]: *** [src/cmakefiles/marian.dir/build.make:121: src/cmakefiles/marian.dir/common/utils.cpp.o] error 1
cc1plus: all warnings being treated as errors
make[2]: *** [src/cmakefiles/marian.dir/build.make:79: src/cmakefiles/marian.dir/common/aliases.cpp.o] error 1
cc1plus: all warnings being treated as errors
make[2]: *** [src/cmakefiles/marian.dir/build.make:135: src/cmakefiles/marian.dir/common/logging.cpp.o] error 1
make[1]: *** [cmakefiles/makefile2:374: src/cmakefiles/marian.dir/all] error 2
make: *** [makefile:156: all] error 2

pleas","['gcc', 'cmake', 'nlp', 'g++']",79332711,"the diagnostic that your build is tripping, wtemplate-id-cdtor, was introduced
with gcc 14.1. it is a warning, not an error, but your build promotes all warnings to
errors, so it breaks your build.
although your build specifies -std=c++11 in src/3rd_party/spdlog/cmakelists.txt, which
generates the failure, g++-14 emits wtemplate-id-cdtor to warn you that the code would be
illegal under the more recent standard c++20 (and later). then the warning is made an error.
the warning is made an error by the compile option -werror. this option is included in the list
of compile options all_warnings, which is created in the top-level marian/cmakelists.txt
at line 227 et seq:
# these are used in src/cmakelists.txt on a per-target basis
list(append all_warnings -wall; -werror; -wextra; -wno-unused-result; -wno-deprecated;
-wno-pragmas; -wno-unused-parameter; -wno-unused-function;
-wno-unused-value; -wno-unknown-pragmas; -wno-sign-compare;
-wno-missing-field-initializers;)

and then applied as compile options for the marian library target in src/cmakelists.txt
at line 133:
target_compile_options(marian private ${all_warnings})

whence the options are operative for the failing compilation of src/cmakefiles/marian.dir/common/logging.cpp.
this failure is a bug in the marian repo which you should report to the maintainers, as
it does not seem to have been reported already. the head revision v1.12.0 is more than a year older than gcc 14.
pending a fix, you seem to have three interim options to get your build done. either:

make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence:
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  139 |     registry_t<mutex>(const registry_t<mutex>&) = delete;
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ï¿½ï¿½ï¿½< >ï¿½ï¿½ï¿½



e.g. make it registry_t(const registry_t<mutex>&) = delete; in this occurrence.
or:

locally disable -wtemplate-id-cdtor at each occurrence, e.g:
#pragma gcc diagnostica gcc diagnostic ignored ""-wtemplate-id-cdtor""
registry_t<mutex>(const registry_t<mutex>&) = delete;
#pragma gcc diagnostic pop



or:

remove -werror from the all_warnings list in marian/cmakelists.txt so that wtemplate-id-cdtor remains just a warning. this may result in other diagnostics being demoted from errors to warnings (their default status).

i haven't tested any of these options as i'd need to go to the trouble of installing cuda.",https://stackoverflow.com/questions/79330283,gcc,05-01-2025 06:04,77.0,4.0,1.0,True,07-01-2025 09:00,05-01-2025 06:12,Implementation Issues
78005051,modulenotfounderror: no module named &#39;llama_index.graph_stores&#39;,"i am trying to use the nebulagraphstore class from llama_index via from llama_index.graph_stores.nebula import nebulagraphstore as suggested by the llama_index documentation, but the following error occurred:
modulenotfounderror                       traceback (most recent call last)
cell in[2], line 1
----> 1 from llama_index.graph_stores.nebula import nebulagraphstore

modulenotfounderror: no module named 'llama_index.graph_stores'

i tried updating llama_index (version 0.10.5) with pip install -u llama-index but it doesn't work. how can i resolve this?","['python', 'langchain', 'large-language-model', 'llama-index', 'nebula-graph']",78031871,"according to latest doc of llama-index, all graph-store module are not included in llama-index core packages and needs to install it by pip:
%pip install llama-index-llms-openai
%pip install llama-index-embeddings-openai
%pip install llama-index-graph-stores-nebula
%pip install llama-index-llms-azure-openai",https://stackoverflow.com/questions/78005051,python,16-02-2024 03:32,16969.0,2.0,2.0,True,21-04-2024 04:53,16-02-2024 19:08,Tool Setup/Errors
74921920,i can this specific error while download nltk,"import nltk
nltk.download('punkt')

showing error:
[nltk_data] error loading punkt: <urlopen error [winerror 10060] a
[nltk_data]     connection attempt failed because the connected party
[nltk_data]     did not properly respond after a period of time, or
[nltk_data]     established connection failed because connected host
[nltk_data]     has failed to respond>

i have pip installed nltk and tried downloading the full nltk version for my learning.
# i used this key to download full version
import nltk
nltk.download()

showing the error: winerror 10060","['python', 'python-3.x', 'windows', 'nltk']",75072632,changing the network to another wifi connection worked for me. the previous connection had some sort of blockers for me. hope this helps.,https://stackoverflow.com/questions/74921920,python,26-12-2022 15:53,634.0,1.0,1.0,True,10-01-2023 16:00,26-12-2022 20:55,Tool Setup/Errors
73729113,extracting n-grams from txt only returns the first lines,"i'm a total newbie in ml and everything in it.
i have a ~15k log and my goal is to extract 3 to 8-grams from it. the code i'm using is partially adopted from this question.

    df = pd.read_fwf(r'c:\path\to\my\log.txt')
    vect = sklearn.feature_extraction.text.countvectorizer(ngram_range=(3,8))
    vect.fit(df)
    for w in vect.get_feature_names_out():
    print(w)


the code actually works, but i'm not able to ""iterate"" over the txt. the result of the execution only returns the first x n-grams extracted from the first 2-3 lines of the log. how can i read and extract all the n-grams from the document?
extra question: since the final goal is to extract the n-grams and build a tf-idf model on them, does the fact that my log is a txt instead of csv represent a problem? i have variable-lenght lines so csv is not feasible i guess.","['python', 'scikit-learn', 'tf-idf']",73729200,"use a for loop on a file object to read it line-by-line. use with open(...) to let a context manager ensure that the file is closed after reading:
with open(""log.txt"") as infile:
    for line in infile:
        print(line)",https://stackoverflow.com/questions/73729113,python,15-09-2022 09:59,38.0,0.0,1.0,True,15-09-2022 10:04,15-09-2022 10:02,Implementation Issues
72772448,spacy - adding multiple patterns to a single ner using entity ruler,"so this is my problem in spacy rule based matching.
i have a txt group say
text = ('wan, flex, havelock st, wan, premium, fibre, 15a,  uk, fletcher inc, fletcher, princeton street, fendalton road, bealey avenue)
doc = nlp3(text)
for ent in doc.ents:
print(ent, '|', ent.label_)
#this provides me a result where : wan, wan are classified as persons and fibre as an org
#now when i build my custom pattern using entity ruler
**nlp3 = spacy.load(""en_core_web_sm"")
ruler = nlp3.add_pipe(""entity_ruler"", before=""ner"")**
#list of entities and patterns
patterns = [{""label"": ""product"", ""pattern"": [{""lower"": ""wan""}, {""lower"": ""fibre""}, {""lower"": ""flex""},{""lower"": ""premium""},{""lower"": ""standard""},{""lower"": ""service""}]}]
ruler.add_patterns(patterns)
nlp3.pipe_names
even after this when i run i get wan classified as person (while i wish to see wan, wan, fibre classified as product). what am i doing wrong in adding patterns here. and is there a way i can add multiple patterns in a single dictionary to a label. any help in this regard is appreciated.","['pattern-matching', 'spacy', 'named-entity-recognition']",72781191,"each pattern you add to the ruler is one sequence of tokens. so you aren't matching each of those terms individually, you're matching all of them in a row, without punctuation. you should add them as separate patterns, something like this:
words = (""wan"", ""fibre"", ...)
patterns = []
for word in words:
    patterns.append({""label"":""product"", ""pattern"":[{""lower"":word}]})

couple of other things:

you may need to set overwrite_ents = true to get the results you want, see here.
if your actual input looks like ""wan, flex, havelock st, wan, premium, ..."", that's not the normal prose the spacy models were trained on, and they may not work very well.",https://stackoverflow.com/questions/72772448,pattern-matching,27-06-2022 12:54,1523.0,1.0,1.0,True,28-06-2022 05:34,27-06-2022 12:59,Preprocessing Tasks
5479333,what are the available tools to summarize or simplify text?,"is there any library, preferably in python but at least open source, that can summarize and or simplify natural-language text?","python, nlp, text-processing",5479497,"i'm not sure if there is currently any libraries that do this, as text summarization, or at least understandable text summarization isn't something that will be easily accomplished by a simple plug & play library.
here are a few links that i managed to find regarding projects / resources that are related to text summarization to get you started:

the lemur project
python natural language toolkit
o'reilly's book on natural language processing in python
google resource on natural language processing
tutorial : how to create a keyword summary of text in python

hope that helps :)",https://stackoverflow.com/q/5479333,"python, nlp, text-processing",29-03-2011 21:46,31599.0,36.0,7.0,True,26-12-2022 21:07,26-12-2022 21:07,Implementation Issues
76952054,the requested module &#39;openai&#39; does not provide an export named &#39;configuration&#39; error,"i'm trying to build an ai image generating website using mern and i got this error:

the requested module 'openai' does not provide an export named
'configuration'.
file:///c:/users/rashmika%20satish/ai_website/server/routes/dalleroutes.js:3
import {configuration, openaiapi} from 'openai';
^^^^^^^^^^^^^ syntaxerror: the requested module 'openai' does not provide an export named 'configuration'
at modulejob._instantiate (node:internal/modules/esm/module_job:124:21)
at async modulejob.run (node:internal/modules/esm/module_job:190:5)
node.js v18.15.0 [nodemon] app crashed - waiting for file changes
before starting...

this is the dalleroutes.js page:
import express from 'express';
import * as dotenv from 'dotenv';
import {configuration, openaiapi} from 'openai';



dotenv.config();

const router = express.router();

this is the index.js page:
import express from 'express'
import *  as dotenv from 'dotenv';
import cors from 'cors';

import connectdb from './mongodb/connect.js';

import postroutes from './routes/postroutes.js';
import dalleroutes from './routes/dalleroutes.js';

dotenv.config();

const app = express();
app.use(cors());
app.use(express.json({limit: '50mb'}));

app.use('/api/v1/post', postroutes);
app.use('/api/v1/dalle', dalleroutes);

app.get('/', async(req, res)=>{
    res.send('hello from createai');
})

const startserver = async () =>{


    try{
        connectdb(process.env.mongodb_url);
        app.listen(8080, () => console.log('server has started on port 
    }catch(error){
         console.log(error);
    }
    

}
startserver();

this is the postroutes.js page
import express from 'express';
import * as dotenv from 'dotenv';
import {v2 as cloudinary} from 'cloudinary';

import post from '../mongodb/models/post.js';

dotenv.config();

const router = express.router();","['javascript', 'reactjs', 'express', 'mern', 'openai-api']",76962380,"i've got this same error. i'm assuming that you're following the jsm tutorial to create the app. after a lot of searching, i finally found a similar discussion on the openai forum 5 days back, and it seems like it's a version change - configuring the api key has been simplified in v4.
follow the forum here, if you're interested: 
here's the github guide on migrating from v3 to v4: 
in short, just run npm exec openai migrate and it should automatically migrate and change the code in your codebase to the latest version and should fix this version issue.",https://stackoverflow.com/questions/76952054,javascript,22-08-2023 09:47,14343.0,4.0,8.0,True,27-03-2024 05:46,22-08-2023 10:04,Tool Setup/Errors
65370140,unable to import process_tweets from utils,"thanks for looking into this, i have a python program for which i need to have process_tweet and build_freqs for some nlp task, nltk is installed already and utils wasn't so i installed it via pip install utils but the above mentioned two modules apparently weren't installed, the error i got is standard one here,
importerror: cannot import name 'process_tweet' from
'utils' (c:\python\lib\site-packages\utils\__init__.py)

what have i done wrong or is there anything missing?
also i referred this stackoverflow answer but it didn't help.","['python', 'nlp', 'nltk', 'sentiment-analysis']",66375357,"you can easily access any source code with ??, for example in this case: process_tweet?? (the code above from deeplearning.ai nlp course custome utils library):
def process_tweet(tweet):
""""""process tweet function.
input:
    tweet: a string containing a tweet
output:
    tweets_clean: a list of words containing the processed tweet

""""""
stemmer = porterstemmer()
stopwords_english = stopwords.words('english')
# remove stock market tickers like $ge
tweet = re.sub(r'\$\w*', '', tweet)
# remove old style retweet text ""rt""
tweet = re.sub(r'^rt[\s]+', '', tweet)
# remove hyperlinks
tweet = re.sub(r' '', tweet)
# remove hashtags
# only removing the hash # sign from the word
tweet = re.sub(r'#', '', tweet)
# tokenize tweets
tokenizer = tweettokenizer(preserve_case=false, strip_handles=true,
                           reduce_len=true)
tweet_tokens = tokenizer.tokenize(tweet)

tweets_clean = []
for word in tweet_tokens:
    if (word not in stopwords_english and  # remove stopwords
            word not in string.punctuation):  # remove punctuation
        # tweets_clean.append(word)
        stem_word = stemmer.stem(word)  # stemming word
        tweets_clean.append(stem_word)",https://stackoverflow.com/questions/65370140,python,19-12-2020 13:06,6394.0,1.0,5.0,True,18-06-2023 12:28,19-12-2020 13:12,Preprocessing Tasks
73777328,splitting string made out of dataframe row wise,"i'm trying to tokenize the words within dataframe which looks like
  a            b       c          d            e           f
0 orange     robot   x eyes   discomfort   striped tee    nan
1 orange     robot  blue beams   grin      vietnam jacket nan
2 aquamarine robot   3d          bored        cigarette   nan   
     

after removing all the special characters the dataframe became a string like this
df_str = df.to_string(header=false)
    
import re

normalised_text = bayc_features_str.lower()
text = re.sub(r""[^\a-za-z0-9 ]"","""", normalised_text)

print(text)

    1    orange   robot   x eyes   discomfort   striped tee   nan
    2    orange   robot   blue beams   grin   vietnam jacket  nan
    3    aquamarine  robot   3d       bored       cigarette    nan   

so when i tokenize this string, with below code
def tokenize(obj):
    if obj is none:
        return none
    elif isinstance(obj, str): 
        return word_tokenize(obj)
    elif isinstance(obj, list):
        return [tokenize(i) for i in obj
    else:
        return obj

tokenized_text = (tokenize(text))

i get the output
['orange', 'robot', 'x', 'eyes', 'discomfort', 'striped', 'tee', nan,'orange', 'robot', 'blue', 'beams', 'grin', 'vietnam', 'jacket', nan,'aquamarine', 'robot', '3d', 'bored', 'cigarette', nan, 'sea', 'captains', 'hat']

which is quite different from the output i expected
[['orange'], ['robot'], ['x', 'eyes'], ['discomfort'], ['striped', 'tee'], nan]
[['orange'], ['robot'], ['blue', 'beams'], ['grin'], ['vietnam', 'jacket'], nan]
[['aquamarine'], ['robot'], ['3d'], ['bored', 'cigarette'], nan, ['sea', 'captains', 'hat']]

any ideas on how can i get the output i expected?
any help would be greatly appreciated!","['python', 'string', 'list', 'nlp', 'tokenize']",73778975,"don't convert dataframe to string but work with every text in dataframe separatelly.
use.applymap(function) to execute function on every text (on every cell in dataframe).
new_df = df.applymap(tokenize)

result = new_df.values.tolist()


minimal working example:
import pandas as pd
from nltk.tokenize import word_tokenize

data = {
    'background': ['orange', 'orange', 'aqua'], 
    'fur': ['robot', 'robot', 'robot'], 
    'eyes': ['x eyes', 'blue beams', '3d'],
    'mouth': ['discomfort', 'grin', 'bored cigarette'],
    'clothes': ['striped tee', 'vietman jacket', none],
    'hat': [none, none, ""sea captain's hat""],
}

df = pd.dataframe(data)

print(df.to_string())  # `to_string()` to display full dataframe without `...`

# ----------------------------------------

def tokenize(obj):
    if obj is none:
        return none
    elif isinstance(obj, str): 
        return word_tokenize(obj)
    elif isinstance(obj, list):
        return [tokenize(i) for i in obj]
    else:
        return obj

new_df = df.applymap(tokenize)

result = new_df.values.tolist()

print(result)

result:
  background    fur        eyes            mouth         clothes                hat
0     orange  robot      x eyes       discomfort     striped tee               none
1     orange  robot  blue beams             grin  vietman jacket               none
2       aqua  robot          3d  bored cigarette            none  sea captain's hat

[
  [['orange'], ['robot'], ['x', 'eyes'], ['discomfort'], ['striped', 'tee'], none], 
  [['orange'], ['robot'], ['blue', 'beams'], ['grin'], ['vietman', 'jacket'], none], 
  [['aqua'], ['robot'], ['3d'], ['bored', 'cigarette'], none, ['sea', 'captain', ""'s"", 'hat']]
]",https://stackoverflow.com/questions/73777328,python,19-09-2022 17:41,88.0,-1.0,1.0,True,20-09-2022 18:01,20-09-2022 18:01,Implementation Issues
76982260,huggingface transformer evaluation process is too slow,"i used the huggingface transformers library to train a bert model for sequence classification.
the training process is good on gpu, but the evaluation process(which is running gpu) is too slow. for example, when i just have a sanity check for just 20 short text inputs, the evaluation runtime is about 160 seconds per step.
here's the snippet code:
def compute_metrics(eval_pred):
    accuracy_metric = evaluate.load(""accuracy"")
    f1_metric = evaluate.load(""f1"", average=""macro"")

    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)
    f1_score = f1_metric.compute(predictions=predictions, references=labels, average=""macro"")

    return {**accuracy, **f1_score}

model = automodelforsequenceclassification.from_pretrained(
        base_model_path,
        num_labels=num_labels,
        id2label=id2label,
        label2id=label2id
    )

    training_args = trainingarguments(
        output_dir=""."",
        learning_rate=lr,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=n_epoch,
        weight_decay=weight_decay,
        evaluation_strategy=""steps"",
        eval_steps=eval_steps,
        logging_strategy=""steps"",
        logging_steps=logging_steps,
        save_strategy=""steps"",
        save_steps=saving_steps,
        load_best_model_at_end=true,
        report_to=[""tensorboard""],
    )

    trainer = trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train_ds,
        eval_dataset=tokenized_valid_ds,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    trainer.train()

the properties of the environment:
transformers              4.29.2
python                    3.10.9

and the configuration of training is like the following:
len(train_data) ~= 36k
len(valid_data) ~= 2k
len(test_data) ~= 2k

model_name = 'bert-base-uncased'

per_device_train_batch_size=16
per_device_eval_batch_size=16
num_train_epochs=30


p.s.: the length of all data is small(less than ten tokens).
can anyone suggest a solution to reduce the time overhead of the evaluation process?","['machine-learning', 'huggingface-transformers', 'bert-language-model', 'training-data', 'huggingface']",77027142,"so i finally got the problem. it's related to evaluate.load() calls inside the compute_metrics function. it seems this method has a significant overhead in time, so it shouldn't be inside some functions e.g. compute_metrics which are called many times. i moved out two load() methods of compute_metrics function and it works quickly now.",https://stackoverflow.com/questions/76982260,machine-learning,26-08-2023 09:06,3150.0,1.0,1.0,True,30-09-2023 18:48,26-08-2023 12:59,Implementation Issues
28339622,is there a corpus of english words in nltk?,"is there any way to get the list of english words in python nltk library?
i tried to find it but the only thing i have found is wordnet from nltk.corpus. but based on documentation, it does not have what i need (it finds synonyms for a word).
i know how to find the list of this words by myself (this answer covers it in details), so i am interested whether i can do this by only using nltk library.",['nltk'],28339791,"yes, from nltk.corpus import words
and check using:
>>> ""fine"" in words.words()
true

reference: section 4.1 (wordlist corpora), chapter 2 of natural language processing with python.",https://stackoverflow.com/questions/28339622,nltk,05-02-2015 08:48,67056.0,39.0,2.0,True,09-01-2023 15:59,09-01-2023 15:59,Uncategorized
73946165,how to avoid double-extraction of patterns in spacy?,"i'm using an incident database to identify the causes of accidents. i have defined a pattern and a function to extract the matching patterns. however, sometimes this function creates overlapping results. i saw in a previous post that we can use for span in spacy.util.filter_spans(spans):
to avoid repetition of answers. but i don't know how to rewrite the function with this. i will be grateful for any help you can provide.
pattern111 = [{'dep':'compound','op':'?'},{'dep':'nsubj'}]
def get_relation111(x):
    doc = nlp(x)
    matcher = matcher(nlp.vocab)
    relation= []

    matcher.add(""matching_111"", [pattern111], on_match=none)

    matches = matcher(doc)
  
    for match_id, start, end in matches:
        matched_span = doc[start: end]
        relation.append(matched_span.text)
    return relation","['dataframe', 'function', 'spacy', 'repeat', 'matcher']",73947115,"filter_spans can be used on any list of spans. this is a little weird because you want a list of strings, but you can work around it by saving a list of spans first and only converting to strings after you've filtered.
def get_relation111(x):
    doc = nlp(x)
    matcher = matcher(nlp.vocab)
    relation= []

    matcher.add(""matching_111"", [pattern111], on_match=none)

    matches = matcher(doc)
  
    for match_id, start, end in matches:
        matched_span = doc[start: end]
        relation.append(matched_span)
    # xxx just add this line
    relation = [ss.text for ss in filter_spans(relation)]
    return relation",https://stackoverflow.com/questions/73946165,dataframe,04-10-2022 09:57,101.0,1.0,1.0,True,04-10-2022 11:21,04-10-2022 10:16,Implementation Issues
73875337,nn.lstm doesn&#39;t seem to learn anything or not updating properly,"i was trying out a simple lstm use case form pytorch, with the following model.
class simplelstm(nn.module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super().__init__()
        
        self.embedding = nn.embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.lstm(batch_first=true, input_size=embedding_dim, num_layers=1, hidden_size=hidden_dim, bidirectional=true)
        self.linear = nn.linear(hidden_dim*2, 1)
        self.sigmoid = nn.sigmoid()
        
    def forward(self, x):   # nxd, padded to same length with 0s in n-sized batch
        x = self.embedding(x)
        output, (final_hidden_state, final_cell_state) = self.lstm(x)
        x = self.linear(output[:,-1,:])
        x=self.sigmoid(x)
        return x

it is a binary classification, with bceloss (combined with the sigmoid output layer). unfortunately, loss is stuck at 0.6969 (i.e. it is not learning anything).
i've tried using final_hidden_state, output[:,0,:] feeding into the linear layer, but so far no dice.
everything else (optimizer, loss criterion, train loop, val loop) already works because i tried the exact same setup with a basic nn using nn.embedding, nn.linear, and nn.sigmoid only, and could get to good loss decrease and high accuracy. in the simplelstm, the only thing i added is the nn.lstm.","['python', 'machine-learning', 'pytorch', 'nlp', 'lstm']",73888500,"typically final_hidden_state is passed to linear, not output. use it.
add 1-2 more linear layers after the lstm.
try lower lr, especially when embeddings are not pre-trained.
better yet, try loading pre-trained embeddings.",https://stackoverflow.com/questions/73875337,python,28-09-2022 01:38,545.0,1.0,1.0,True,28-09-2022 22:36,28-09-2022 22:34,Implementation Issues
74173869,bert transformer model gives an error for multiclass classification,"i am trying to train a sentiment analysis model with 5 classes (1-very negative, 2-negative, 3-neutral, 4-positive, 5-very positive) with the bert model.
from transformers import berttokenizer, tfbertforsequenceclassification
from transformers import inputexample, inputfeatures
        
model = tfbertforsequenceclassification.from_pretrained(""bert-base-cased"")
tokenizer = berttokenizer.from_pretrained(""bert-base-cased"")
        
model.compile(optimizer=tf.keras.optimizers.adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), 
              loss=tf.keras.losses.sparsecategoricalcrossentropy(from_logits=true), 
              metrics=[tf.keras.metrics.sparsecategoricalaccuracy('accuracy')])
    
model.fit(train_data, epochs=2, validation_data=validation_data)

but i get the following error (just the last part of the error message)
node: 'sparse_categorical_crossentropy/sparsesoftmaxcrossentropywithlogits/sparsesoftmaxcrossentropywithlogits'
received a label value of 5 which is outside the valid range of [0, 2).  label values: 3 4 5 2 2 4 4 3 4 5 5 4 5 5 4 4 4 3 4 4 5 5 5 4 4 5 3 5 4 4 3 5
         [[{{node sparse_categorical_crossentropy/sparsesoftmaxcrossentropywithlogits/sparsesoftmaxcrossentropywithlogits}}]] [op:__inference_train_function_31614]

can somebody tell me what i am doing wrong here?","['bert-language-model', 'softmax', 'cross-entropy']",74178380,"the tfbertforsequenceclassification object needs to create a so-called classification head. the classification head is a cool name for a single nn layer that projects the [cls] token representation into a vector with one item for each possible target class.
when you initialize the model by calling from_pretrained, you can specify num_labels, which is a number of target labels (see an example in transformers documentation). if you do not specify it, the number of target classes will be inferred from the first training batch by taking the maximum class id in the batch. if you are not lucky and the first batch only contains lower label ids, it initializes a smaller classification head and fails when a batch with higher ids comes.
note also, that the class numbers start from zero. if you use labels 1-5, the model will have an additional 0th class that will not be used. if you want to keep the numbers 1-5, your num_labels will be 6.",https://stackoverflow.com/questions/74173869,bert-language-model,23-10-2022 18:47,169.0,0.0,1.0,True,24-10-2022 08:26,24-10-2022 08:17,Implementation Issues
44827930,evaluation in a spacy ner model,"i am trying to evaluate a trained ner model created using spacy lib.
 normally for these kind of problems you can use f1 score (a ratio between precision and recall). i could not find in the documentation an accuracy function for a trained ner model. 
i am not sure if it's correct but i am trying to do it with the following way(example) and using f1_score from sklearn:
from sklearn.metrics import f1_score
import spacy
from spacy.gold import goldparse


nlp = spacy.load(""en"") #load ner model
test_text = ""my name is john"" # text to test accuracy
doc_to_test = nlp(test_text) # transform the text to spacy doc format

# we create a golden doc where we know the tagged entity for the text to be tested
doc_gold_text= nlp.make_doc(test_text)
entity_offsets_of_gold_text = [(11, 15,""person"")]
gold = goldparse(doc_gold_text, entities=entity_offsets_of_gold_text)

# bring the data in a format acceptable for sklearn f1 function
y_true = [""person"" if ""person"" in x else 'o' for x in gold.ner]
y_predicted = [x.ent_type_ if x.ent_type_ !='' else 'o' for x in doc_to_test]
f1_score(y_true, y_predicted, average='macro')`[1]
> 1.0

any thoughts are or insights are useful.","['python', 'spacy']",44841535,"you can find different metrics including f-score, recall and precision in spacy/scorer.py.
this example shows how you can use it:
import spacy
from spacy.gold import goldparse
from spacy.scorer import scorer

def evaluate(ner_model, examples):
    scorer = scorer()
    for input_, annot in examples:
        doc_gold_text = ner_model.make_doc(input_)
        gold = goldparse(doc_gold_text, entities=annot)
        pred_value = ner_model(input_)
        scorer.score(pred_value, gold)
    return scorer.scores

# example run

examples = [
    ('who is shaka khan?',
     [(7, 17, 'person')]),
    ('i like london and berlin.',
     [(7, 13, 'loc'), (18, 24, 'loc')])
]

ner_model = spacy.load(ner_model_path) # for spacy's pretrained use 'en_core_web_sm'
results = evaluate(ner_model, examples)

the scorer.scores returns multiple scores. when running the example, the result looks like this: (note the low scores occuring because the examples classify london and berlin as 'loc' while the model classifies them as 'gpe'. you can figure this out by looking at the ents_per_type.)
{'uas': 0.0, 'las': 0.0, 'las_per_type': {'attr': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'root': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'compound': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'nsubj': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'dobj': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'cc': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'conj': {'p': 0.0, 'r': 0.0, 'f': 0.0}}, 'ents_p': 33.33333333333333, 'ents_r': 33.33333333333333, 'ents_f': 33.33333333333333, 'ents_per_type': {'person': {'p': 100.0, 'r': 100.0, 'f': 100.0}, 'loc': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'gpe': {'p': 0.0, 'r': 0.0, 'f': 0.0}}, 'tags_acc': 0.0, 'token_acc': 100.0, 'textcat_score': 0.0, 'textcats_per_cat': {}}

the example is taken from a spacy example on github (link does not work anymore). it was last tested with spacy 2.2.4.",https://stackoverflow.com/questions/44827930,python,29-06-2017 14:27,26383.0,29.0,5.0,True,20-01-2023 00:49,19-03-2018 15:20,Implementation Issues
58384286,calculate tf-idf using sklearn for variable-n-grams in python,"problem:
using scikit-learn to find the number of hits of variable n-grams of a particular vocabulary.
explanation.
i got examples from here.
imagine i have a corpus and i want to find how many hits (counting) has a vocabulary like the following one:
myvocabulary = [(window=4, words=['tin', 'tan']),
                (window=3, words=['electrical', 'car'])
                (window=3, words=['elephant','banana'])

what i call here window is the length of the span of words in which the words can appear. as follows:
'tin tan' is hit (within 4 words)
'tin dog tan' is hit (within 4 words)
'tin dog cat tan is hit (within 4 words)
'tin car sun eclipse tan' is not hit. tin and tan appear more than 4 words away from each other.
i just want to count how many times (window=4, words=['tin', 'tan']) appears in a text and the same for all the other ones and then add the result to a pandas in order to calculate a tf-idf algorithm.
i could only find something like this:
from sklearn.feature_extraction.text import tfidfvectorizer
tfidf = tfidfvectorizer(vocabulary = myvocabulary, stop_words = 'english')
tfs = tfidf.fit_transform(corpus.values())

where vocabulary is a simple list of strings, being single words or several words.
besides from scikitlearn:
class sklearn.feature_extraction.text.countvectorizer
ngram_range : tuple (min_n, max_n)

the lower and upper boundary of the range of n-values for different n-grams to be extracted. all values of n such that min_n <= n <= max_n will be used.
does not help neither.
any ideas?","['python', 'text', 'scikit-learn', 'tf-idf', 'n-gram']",58439141,"i am not sure if this can be done using countvectorizer or tfidfvectorizer. i have written my own function for doing this as follows:
import pandas as pd
import numpy as np
import string 

def contained_within_window(token, word1, word2, threshold):
  word1 = word1.lower()
  word2 = word2.lower()
  token = token.translate(str.maketrans('', '', string.punctuation)).lower()
  if (word1 in token) and word2 in (token):
      word_list = token.split("" "")
      word1_index = [i for i, x in enumerate(word_list) if x == word1]
      word2_index = [i for i, x in enumerate(word_list) if x == word2]
      count = 0
      for i in word1_index:
        for j in word2_index:
          if np.abs(i-j) <= threshold:
            count=count+1
      return count
  return 0


sample:

corpus = [
    'this is the first document. and this is what i want',
    'this document is the second document.',
    'and this is the third one.',
    'is this the first document?',
    'i like coding in sklearn',
    'this is a very good question'
]

df = pd.dataframe(corpus, columns=[""test""])

your df will look like this:
    test
0   this is the first document. and this is what i...
1   this document is the second document.
2   and this is the third one.
3   is this the first document?
4   i like coding in sklearn
5   this is a very good question

now you can apply contained_within_window as follows:
sum(df.test.apply(lambda x: contained_within_window(x,word1=""this"", word2=""document"",threshold=2)))

and you get:
2

you can just run a for loop for checking different instances.
and you this to construct your pandas df  and apply tfidf on it, which is straight forward.",https://stackoverflow.com/questions/58384286,python,14-10-2019 21:29,667.0,2.0,1.0,True,23-11-2022 13:06,23-11-2022 13:06,Implementation Issues
76045605,using a custom trained huggingface tokenizer,"iï¿½ï¿½ï¿½ve trained a custom tokenizer using a custom dataset using this code thatï¿½ï¿½ï¿½s on the documentation. is there a method for me to add this tokenizer to the hub and to use it as the other tokenizers by calling the autotokenizer.from_pretrained() function? if i canï¿½ï¿½ï¿½t do that how can i use the tokenizer to train a custom model from scratch?
thanks for your help!!!
here's the code below:
from tokenizers import tokenizer
from tokenizers.models import bpe
tokenizer = tokenizer(bpe(unk_token=""[unk]""))

from tokenizers.trainers import bpetrainer
trainer = bpetrainer(special_tokens=[""[unk]"", ""[cls]"", ""[sep]"", ""[pad]"", ""[mask]""])

from tokenizers.pre_tokenizers import whitespace
tokenizer.pre_tokenizer = whitespace()

folder = 'dataset_unicode'
files = [f""/content/drive/mydrive/{folder}/{split}.txt"" for split in [""test"", ""train"", ""valid""]]
tokenizer.train(files, trainer)

from tokenizers.processors import templateprocessing
tokenizer.post_procesteprocessing(
    single=""[cls] $a [sep]"",
    pair=""[cls] $a [sep] $b:1 [sep]:1"",
    special_tokens=[
        (""[cls]"", tokenizer.token_to_id(""[cls]"")),
        (""[sep]"", tokenizer.token_to_id(""[sep]"")),
    ],
)

# i've tried saving it like this but it doesn't work as i expect it:
tokenizer.save(""data/tokenizer-custom.json"")","['python', 'huggingface-transformers', 'huggingface-tokenizers', 'huggingface', 'huggingface-hub']",76058017,"the autotokenizer expects a few files in the directory:
awesometokenizer/
    tokenizer_config.json
    special_tokens_map.json
    tokenizer.json

but the default tokenizer.tokenizer.save() function only saves the vocab file in awesometokenizer/tokenizer.json, open up the json file and compare the ['model']['vocab'] keys to your json from data/tokenizer-custom.json.
the simplest way to let autotokenizer load .from_pretrained is to follow the answer that @cronoik posted in the comment, using pretrainedtokenizerfast, i.e. adding a few lines to your existing code:
from tokenizers import tokenizer
from tokenizers.models import bpe
from tokenizers.trainers import bpetrainer
from tokenizers.pre_tokenizers import whitespace
from tokenizers.processors import templateprocessing

from transformers import pretrainedtokenizerfast  # <---- add this line.



trainer = bpetrainer(special_tokens=[""[unk]"", ""[cls]"", ""[sep]"", ""[pad]"", ""[mask]""])

tokenizer = tokenizer(bpe(unk_token=""[unk]""))
tokenizer.pre_tokenizer = whitespace()

files = [""big.txt""]  # e.g. training with 
tokenizer.train(files, trainer)

tokenizer.post_processor = templateprocessing(
    single=""[cls] $a [sep]"",
    pair=""[cls] $a [sep] $b:1 [sep]:1"",
    special_tokens=[
        (""[cls]"", tokenizer.token_to_id(""[cls]"")),
        (""[sep]"", tokenizer.token_to_id(""[sep]"")),
    ],
)

# add these lines:
#     |
#     |
#     v
awesome_tokenizer = pretrainedtokenizerfast(tokenizer_object=tokenizer)
awesome_tokenizer.save_pretrained(""awesome_tokenizer"")

then you can load the trained tokenizer:
from transformers import autotokenizer

auto_loaded_tokenizer = autotokenizer.from_pretrained(
    ""awesome_tokenizer"", 
    local_files_only=true
)


note: tokenizers though can be pip installed, is a library in rust with python bindings",https://stackoverflow.com/questions/76045605,python,18-04-2023 14:05,2358.0,5.0,2.0,True,29-02-2024 08:33,19-04-2023 19:03,Implementation Issues
71726244,is possible to get dependency/pos information for entities in spacy?,"i am working on extracting entities from scientific text (i am using scispacy) and later i will want to extract relations using hand-written rules. i have extracted entities and their character span successfully, and i can also get the pos and dependency tags for tokens and noun chunks. so i am comfortable with the two tasks separately, but i want to bring the two together and i have been stuck for a while.
the idea is that i want to be able to write rules such as: (just an example) if in a sentence/clause there are two entities where the first one is a 'drug/chemical' + is the subject, and the second one is a 'disease' + is an object --> (then) infer 'treatment' relation between the two.
if anyone has any hints on how to approach this task, i would really appreciate it. thank you!
s.
what i am doing to extract entities:
doc = nlp(text-with-more-than-one-sent)
for ent in doc.ents:
`... (get information about the ent e.g. its character span)`

getting dependency information (for noun chunks and for tokens):
for chunk in doc.noun_chunks:
    print(f""text: {chunk.text}, root text: {chunk.root.text}, root dep: {chunk.root.dep_}, root head text: {chunk.root.head.text}, pos: {chunk.root.head.pos_}"")
_
for token in doc:
    print(f""text: {token.text}, dep label: {token.dep_}, head text: {token.head.text}, head pos: {token.head.pos_}, children: {[child for child in token.children]}"")","['nlp', 'spacy', 'named-entity-recognition', 'dependency-parsing']",71732367,"you can use the merge_entities mini-component to convert entities to single tokens, which would simplify what you're trying to do. there's also a component to merge noun chunks similarly.",https://stackoverflow.com/questions/71726244,nlp,03-04-2022 13:11,771.0,1.0,1.0,True,04-04-2022 05:21,03-04-2022 13:24,Implementation Issues
61133531,how does spacy generate vectors for phrases?,"medium and large vocabularies of spacy can generate vectors for words and phrases.  let's consider the following example:
import spacy
    
nlp = spacy.load(""en_core_web_md"")
tokens = nlp(""apple cat sky"")
    
print(tokens.text, tokens.vector[:3], tokens.vector_norm) # only the first three components of the vector 
    
for token in tokens:
    print(token.text, token.vector[:3], token.vector_norm)

output:
apple cat sky [-0.06734333  0.03672066 -0.13952099] 4.845729844425328
apple [-0.36391  0.43771 -0.20447] 7.1346846
cat [-0.15067  -0.024468 -0.23368 ] 6.6808186
sky [ 0.31255  -0.30308   0.019587] 6.617719

it is clear that the vocabulary contains vectors for each word, but how are the vectors for the entire phase generated? as one can see it is not just simple sum of vectors.","['nlp', 'spacy', 'word2vec']",61135858,"by default, the vector of a doc is the average of the vectors of the tokens, cf 

models that come with built-in word vectors make them available as the token.vector attribute. doc.vector and span.vector will default to an average of their token vectors.",https://stackoverflow.com/questions/61133531,nlp,10-04-2020 03:19,6917.0,5.0,2.0,True,24-04-2023 06:37,24-04-2023 06:37,Uncategorized
45495190,initializing out of vocabulary (oov) tokens,"i am building tensorflow model for nlp task, and i am using pretrained glove 300d word-vector/embedding dataset.
obviously some tokens can't be resolved as embeddings, because were not included into training dataset for word vector embedding model, e.g. rare names.
i can replace those tokens with vectors of 0s, but rather than dropping this information on the floor, i prefer to encode it somehow and include to my training data.
say, i have 'raijin' word, which can't be resolved as embedding vector, what would be the best way to encode it consistently with glove embedding dataset? what is the best approach to convert it to 300d vector?
thank you.","['tensorflow', 'embedding', 'word-embedding', 'oov']",45496869,"instead of assigning all the out of vocabulary tokens to a common unk vector (zeros), it is better to assign them a unique random vector. at-least this way when you find the similarity between them with any other word, each of them will be unique and the model can learn something out of it. in the unk case, they will all be same and so all the unk words will be treated as having the same context.
i tried this approach and got a 3% accuracy improvement on the quora duplicate question pair detection dataset using an lstm model.",https://stackoverflow.com/questions/45495190,tensorflow,03-08-2017 21:58,8789.0,3.0,2.0,True,05-03-2024 09:33,05-03-2024 09:33,Task-specific Help
74123446,tenserflow issue when tokenizing sentences,"i followed a tutorial about tokenizing sentences using tensorflow, here's the code i'm trying:
from tensorflow.keras.preprocessing.text import tokenizer #api for tokenization

t = tokenizer(num_words=4) #meant to catch most imp _
listofsentences=['apples are fruits', 'an orange is a tasty fruit', 'fruits are tasty!']
t.fit_on_texts(listofsentences) #processes words

print(t.word_index)
print(t.texts_to_sequences(listofsentences)) #arranges tokens, returns nested list

the first print statement shows a dictionary as expected:
{'are': 1, 'fruits': 2, 'tasty': 3, 'apples': 4, 'an': 5, 'orange': 6, 'is': 7, 'a': 8, 'fruit': 9}

but the last line outputs a list that misses many words:
[[1, 2], [3], [2, 1, 3]]

please let me know what i'm doing wrong and how to get the expected list:
[[4,1,2],[5,6,7,8,3,9],[2,1,3]]","['python', 'tensorflow', 'nlp', 'tokenize']",74125318,"to specify an unlimited amount of tokens use:
t = tokenizer(num_words=none)

output:
{'are': 1, 'fruits': 2, 'tasty': 3, 'apples': 4, 'an': 5, 'orange': 6, 'is': 7, 'a': 8, 'fruit': 9}
[[4, 1, 2], [5, 6, 7, 8, 3, 9], [2, 1, 3]]",https://stackoverflow.com/questions/74123446,python,19-10-2022 10:00,42.0,1.0,1.0,True,19-10-2022 12:27,19-10-2022 10:14,Implementation Issues
77068488,how to efficiently convert a markdown table to a dataframe in python?,"i need to convert a markdown table into a pandas dataframe. i've managed to do this using the pd.read_csv function with '|' as the separator, but it seems like there's some additional cleanup required. specifically, i need to remove the row containing '-----', which is used for table separation, and i also want to get rid of the last column.
here's a simplified example of what i'm doing:
import pandas as pd
from io import stringio

# the text containing the table
text = """"""
| some title | some description             | some number |
|------------|------------------------------|-------------|
| dark souls | this is a fun game           | 5           |
| bloodborne | this one is even better      | 2           |
| sekiro     | this one is also pretty good | 110101      |
""""""

# use stringio to create a file-like object from the text
text_file = stringio(text)

# read the table using pandas read_csv with '|' as the separator
df = pd.read_csv(text_file, sep='|', skipinitialspace=true)

# remove leading/trailing whitespace from column names
df.columns = df.columns.str.strip()

# remove the index column
df = df.iloc[:, 1:]

is there a more elegant and efficient way to convert a markdown table into a dataframe without needing to perform these additional cleanup steps? i'd appreciate any suggestions or insights on improving this process.","['python', 'nlp', 'markdown']",77068592,"like this
import re
import pandas as pd

text = """"""
| some title | some description             | some number |
|------------|------------------------------|-------------|  
| dark souls | this is a fun game           | 5           |
| bloodborne | this one is even better      | 2           |
| sekiro     | this one is also pretty good | 110101      |
""""""

pattern = r""\| ([\w\s]+) \| ([\w\s]+) \| ([\w\s]+) \|""

# use the findall function to extract all rows that match the pattern
matches = re.findall(pattern, text)

# extract the header and data rows
header = matches[0]
data = matches[1:]

# create a pandas dataframe using the extracted header and data rows
df = pd.dataframe(data, columns=header)

# optionally, convert numerical columns to appropriate types
df['some number'] = df['some number'].astype(int)

print(df)",https://stackoverflow.com/questions/77068488,python,08-09-2023 16:21,6123.0,9.0,2.0,True,09-09-2023 08:08,09-09-2023 08:08,Implementation Issues
73571053,how to know the topic from trained data (or predict the topic of new data) using trained topic modelling using octis?,"i've trained an lda for topic modelling using octis. but i don't know how to see the predicted topic for each data input or how to apply/predict my trained model to new data.
this is the code and the output of the trained model:
input:
# custom dataset
from octis.dataset.dataset import dataset
dataset = dataset()
dataset.load_custom_dataset_from_folder(""/content/new"")

# create model
model = lda(num_topics=5, alpha=0.1)

# train the model 
output = model.train_model(dataset)

output:
>> output
{'topic-word-matrix': array([[0.00030817, 0.00646953, 0.00338882, ..., 0.00030812, 0.00030813,
         0.00030812],
        [0.00041419, 0.00248425, 0.0004141 , ..., 0.0004141 , 0.0004141 ,
         0.0004141 ],
        [0.0002584 , 0.00025837, 0.00025836, ..., 0.00025836, 0.00025836,
         0.00025836],
        [0.00044957, 0.0004495 , 0.00044949, ..., 0.00269659, 0.00269643,
         0.00269655],
        [0.00238244, 0.0003972 , 0.00039719, ..., 0.00039719, 0.00039719,
         0.00039719]], dtype=float32),
 'topics': [['vaksin',
   'sertifikat',
   'aplikasi',
   'pertama',
   'alhamdulillah',
   'pedulilindungi',
   'ada',
   'padahal',
   'terimakasih',
   'nik'],
  ['aplikasi',
   'vaksin',
   'sertifikat',
   'guna',
   'data',
   'udh',
   'bikin',
   'pake',
   'login',
   'nik'],
  ['aplikasi',
   'vaksin',
   'sertifikat',
   'ada',
   'tanggal',
   'tgl',
   'di',
   'belum',
   'coba',
   'sangat'],
  ['covid',
   'vaksin',
   'jadi',
   'hp',
   'ada',
   'aplikasi',
   'mau',
   'kalau',
   'sakit',
   'salah'],
  ['aplikasi',
   'bahasa',
   'lahir',
   'bisa',
   'nx',
   'sertifikat',
   'pakai',
   'di',
   'vaksin',
   'indonesia']],
 'topic-document-matrix': array([[0.06667246, 0.98769081, 0.00190489, 0.00165357, 0.98805857,
         0.00392223, 0.00210558, 0.00219824, 0.0029861 , 0.00170955,
         0.00215115, 0.00160036, 0.00210585, 0.00210572, 0.0030779 ,
         0.00289892, 0.00289916, 0.00307764, 0.00317504, 0.00307748,
         0.00183547, 0.10921329, 0.00160071, 0.98933005, 0.00219851,
         0.49730667, 0.98768848, 0.00194217, 0.00194207, 0.99120653,
         0.00160038, 0.00363727, 0.23678468, 0.98545253, 0.00168113,
         0.0016811 , 0.99349433, 0.00229977, 0.00339057, 0.98769081,
         0.00190489, 0.00165355, 0.98805857, 0.00392223, 0.00210558,
         0.00219824, 0.00298613, 0.00170955, 0.00215115, 0.00160037],
        [0.06667251, 0.00307732, 0.00190493, 0.99338686, 0.00298535,
         0.00392244, 0.00210581, 0.00219835, 0.00298549, 0.00170954,
         0.00215096, 0.00160031, 0.00210574, 0.00210569, 0.00307775,
         0.00289909, 0.0028996 , 0.00307763, 0.0031751 , 0.98769003,
         0.00183529, 0.88495934, 0.00160057, 0.00266775, 0.0021984 ,
         0.00224775, 0.00307769, 0.99223095, 0.99223143, 0.00219834,
         0.64113957, 0.98545176, 0.00219818, 0.00363676, 0.00168091,
         0.00168108, 0.00162644, 0.00229943, 0.00339064, 0.00307732,
         0.00190493, 0.99338692, 0.00298535, 0.00392238, 0.00210581,
         0.00219834, 0.00298549, 0.00170954, 0.00215096, 0.00160031],
        [0.06667244, 0.00307741, 0.00190496, 0.0016533 , 0.00298539,
         0.98431122, 0.99157733, 0.99120724, 0.9880569 , 0.00170964,
         0.00215104, 0.99359852, 0.002106  , 0.99157727, 0.00307766,
         0.00289921, 0.9884032 , 0.0681117 , 0.98729986, 0.00307752,
         0.00183526, 0.0019427 , 0.0016006 , 0.00266733, 0.06295873,
         0.49595022, 0.00307819, 0.00194241, 0.00194218, 0.00219866,
         0.35405946, 0.00363706, 0.75662065, 0.00363694, 0.00168095,
         0.99327528, 0.00162655, 0.00229944, 0.00339072, 0.00307741,
         0.00190496, 0.0016533 , 0.00298541, 0.98431122, 0.99157733,
         0.99120724, 0.9880569 , 0.00170964, 0.00215105, 0.99359864],
        [0.06667253, 0.00307718, 0.00190488, 0.00165313, 0.00298533,
         0.00392208, 0.00210552, 0.00219805, 0.00298561, 0.99316174,
         0.00215088, 0.00160023, 0.00210566, 0.00210558, 0.98768842,
         0.9884038 , 0.00289898, 0.00307778, 0.00317501, 0.00307729,
         0.0018353 , 0.00194239, 0.99359751, 0.00266747, 0.93044597,
         0.00224771, 0.0030776 , 0.00194221, 0.00194201, 0.00219822,
         0.00160026, 0.0036369 , 0.00219826, 0.00363692, 0.99327594,
         0.00168124, 0.00162628, 0.00229925, 0.98643756, 0.00307718,
         0.00190488, 0.00165313, 0.00298533, 0.00392208, 0.00210552,
         0.00219805, 0.00298561, 0.99316174, 0.00215088, 0.00160023],
        [0.73331004, 0.00307727, 0.99238032, 0.00165311, 0.00298531,
         0.00392205, 0.00210574, 0.00219817, 0.00298594, 0.00170954,
         0.99139601, 0.00160052, 0.99157673, 0.00210574, 0.00307827,
         0.00289894, 0.00289911, 0.92265522, 0.00317498, 0.00307761,
         0.99265867, 0.00194224, 0.00160059, 0.00266738, 0.00219837,
         0.00224767, 0.00307806, 0.00194227, 0.0019423 , 0.00219831,
         0.00160033, 0.00363697, 0.00219824, 0.00363684, 0.00168108,
         0.00168125, 0.00162644, 0.99080211, 0.00339047, 0.00307727,
         0.99238032, 0.00165311, 0.00298531, 0.00392205, 0.00210574,
         0.00219817, 0.00298594, 0.00170954, 0.99139601, 0.00160043]]),
 'test-topic-document-matrix': array([], dtype=float64)}

my goal is to at least know the topic for each input data (it'll be great if i can predict new data by the trained model too!)

i used trial data, so the result is still not that great, but my main objective is to understand how to do topic modelling with octis
num topic = 5 (if this helps)
octis framework is new, it was published last year! (2021)ï¿½ï¿½
</","['python', 'machine-learning', 'nlp', 'lda', 'topic-modeling']",76714445,"topics in the training data
the topics that the model has found are represented by the top 10 words in that topic.
these can be found in output['topics'].
so your first topic would be represented by the words: ['vaksin','sertifikat','aplikasi','pertama','alhamdulillah','pedulilindungi','ada','padahal','terimakasih','nik'].
to know which topics are found in which document, you should look at output['topic-document-matrix'].
the first list in this list represents the distribution of topics in the first document of your training data. example: the first document mostly consists of topic 2 (because of the value 0.98769081)
prediction on new documents
unfortunately, this is not possible using octis. octis is exclusively a package for optimizing and comparing topic models. it is possible to define a test set, to see how models perform on unseen data. however, octis is not suitable for developing production topic models. if that is your goal, take a look at gensim. (this is the package that octis uses behind the scenes.)",https://stackoverflow.com/questions/73571053,python,01-09-2022 14:27,314.0,0.0,1.0,True,18-07-2023 15:35,02-09-2022 13:16,Implementation Issues
73569160,valueerror(&#39;[e999] unable to merge the doc objects because they do not all share the same `vocab`.&#39;),"i want to combine the results of two different spacy.languages but will receive the following error:

valueerror('[e999] unable to merge the doc objects because they do not
all share the same vocab.')

example code:
import spacy
from spacy.tokens import doc

nlp_1 = spacy.blank(""en"")
ruler = nlp_1.add_pipe(""entity_ruler"")
ruler.add_patterns([{""label"": ""org"", ""pattern"": ""apple""}, ])
doc_1 = nlp_1('apple')

nlp_2 = spacy.blank(""en"")
ruler = nlp_2.add_pipe(""entity_ruler"")
ruler.add_patterns([{""label"": ""per"", ""pattern"": ""peter""}, ])
doc_2 = nlp_2('peter')

print(doc.from_docs([doc_1, doc_2]))
# valueerror: [e999] unable to merge the doc objects because they do not all share the same `vocab`.

question:
how do i fix this, e.g. share the vocabs between both nlp objects?
why would i want that?
lets say i want to analyse a mail. it is one document, but likelyhood that a number in the adress field is postal code is much higher than in the footer where it is probably a phone number. therefore depending on the field i want to apply different ""languages"" but which share the same vocab and then combine them into one doc for the mail.","['python', 'spacy', 'spacy-3']",73569856,"provide the existing vocab from the first model for any subsequent models when they're loaded:
nlp_2 = spacy.blank(""en"", vocab=nlp_1.vocab)",https://stackoverflow.com/questions/73569160,python,01-09-2022 12:12,87.0,1.0,1.0,True,02-09-2022 14:13,02-09-2022 14:13,Implementation Issues
72979157,how can i test my openai fine-tuned model against question answering benchmarks?,"i think the documentation only explains how to use the model through an api but that does not allow much flexibility nor automation. for example, i do not know how to test my model against some popular benchmarks from huggingface.",['openai-api'],73083718,"the general flow of fine tuning open ai models consists of creating an account, having a valid api key and then uploading the data for fine tuning using the cli tool, as described here: 
then to test against question answering benchmarks, like squad you simply dowload the dataset, create a script that takes the questions (see below json snippet) and feeds to your model by calling the api as described here (using curl): 
""question"": ""what century did the normans first gain their separate identity?"",
""id"": ""56ddde6b9a695914005b962c"",
""answers"": [
    {
        ""text"": ""10th century"",
        ""answer_start"": 671
    },
    {
        ""text"": ""the first half of the 10th century"",
        ""answer_start"": 649
    },
    {
        ""text"": ""10th"",
        ""answer_start"": 671
    },
    {
        ""text"": ""10th"",
        ""answer_start"": 671
    }
],
""is_impossible"": false",https://stackoverflow.com/questions/72979157,openai-api,14-07-2022 10:38,742.0,1.0,1.0,True,15-02-2023 12:23,15-02-2023 12:23,Data Wrangling
29788047,keep tfidf result for predicting new content,"i am using sklearn on python to do some clustering. i've trained 200,000 data, and code below works well.
corpus = open(""token_from_xml.txt"")
vectorizer = countvectorizer(decode_error=""replace"")
transformer = tfidftransformer()
tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))
km = kmeans(30)
kmresult = km.fit(tfidf).predict(tfidf)

but when i have new testing content, i'd like to cluster it to existed clusters i'd trained. so i'm wondering how to save idf result, so that i can do tfidf for the new testing content and make sure the result for new testing content have same array length.
thanks in advance.
update
i may need to save ""transformer"" or ""tfidf"" variable to file(txt or others), if one of them contains the trained idf result.
update
for example. i have the training data:
[""a"", ""b"", ""c""]
[""a"", ""b"", ""d""]

and do tfidf, the result will contains 4 features(a,b,c,d)
when i test:
[""a"", ""c"", ""d""]

to see which cluster(already made by k-means) it belongs to. tfidf will only give the result with 3 features(a,c,d), so the clustering in k-means will fall. (if i test [""a"", ""b"", ""e""], there may have other problems.)
so how to store the features list for testing data (even more, store it in file)?","['python', 'machine-learning', 'scikit-learn', 'tf-idf']",29793628,"i successfully saved the feature list by saving vectorizer.vocabulary_, and reuse by countvectorizer(decode_error=""replace"",vocabulary=vectorizer.vocabulary_)
codes below:
corpus = np.array([""aaa bbb ccc"", ""aaa bbb ddd""])
vectorizer = countvectorizer(decode_error=""replace"")
vec_train = vectorizer.fit_transform(corpus)
#save vectorizer.vocabulary_
pickle.dump(vectorizer.vocabulary_,open(""feature.pkl"",""wb""))

#load it later
transformer = tfidftransformer()
loaded_vec = countvectorizer(decode_error=""replace"",vocabulary=pickle.load(open(""feature.pkl"", ""rb"")))
tfidf = transformer.fit_transform(loaded_vec.fit_transform(np.array([""aaa ccc eee""])))

that works. tfidf will have same feature length as trained data.",https://stackoverflow.com/questions/29788047,python,22-04-2015 04:55,41058.0,27.0,5.0,True,15-10-2024 09:35,15-10-2024 09:35,Implementation Issues
78128206,return sentences from list of sentences using user specified keyword,"i got a list of sentences (roughly 20000) stored in excel file named list.xlsx and  sheet named sentence under column name named sentence.
my intention is to get words from user and return those sentences where in those exact words matches.
i am currently able to do so with the code i developed using spacy. but it takes lot of time to check and return output.
is there any other time saving way of achieving this by any other means.
i see in geany notepad or libre calc wherein its search function return sentences in a jiffy.
how?
kindly help.
import pandas as pd
import spacy

# load the english language model in spacy
nlp = spacy.load(""en_core_web_sm"")

# function to extract sentences containing the keyword
def extract_sentences_with_keyword(text, keyword):
    doc = nlp(text)
    sentences = [sent.text for sent in doc.sents if keyword in sent.text.lower()]
    return sentences

i = input(""enter keyword(s):"")

# read the excel file
file_path = ""list.xlsx""
sheet_name = ""sentence""  # update with your sheet name
column_name = ""sentence""   # update with the column containing text data

data = pd.read_excel(file_path, sheet_name=sheet_name)



# iterate over the rows and extract sentences with the keyword
keyword = i  # update with the keyword you want to search for
for index, row in data.iterrows():
    text = row[column_name]
    sentences = extract_sentences_with_keyword(text, keyword)
    
    if sentences:
        for sentence in sentences:
            print(sentence)
        print(""\n"")","['python', 'spacy', 'text-processing']",78141386,"you can use sqlite with a full text index. i tried the following proof of concept code with a 6 mb text file and it is very fast. you of course need to adjust the code for your needs, using spacy for sentence splitting as you did above might be a decent option:
import sqlite3
import re

conn = sqlite3.connect(':memory:')
cursor = conn.cursor()

cursor.execute('create virtual table fts_sentences using fts5(content)')

def load_and_split_file(file_path):
    sentence_endings = r'[.!?]\s+|\s*$'
    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()
        sentences = re.split(sentence_endings, text)
        return sentences

def insert_sentences(sentences):
    for sentence in sentences:
        cursor.execute('insert into fts_sentences (content) values (?)', (sentence,))
    conn.commit()

def search_word(word):
    cursor.execute('select content from fts_sentences where fts_sentences match ?', (word,))
    return cursor.fetchall()

file_path = 'big.txt' 
sentences = load_and_split_file(file_path)
insert_sentences(sentences)

while true:
    word_to_search = input('enter a word to search for: ')
    matching_sentences = search_word(word_to_search)

    for sentence in matching_sentences:
        print(sentence[0])

your code using spacy is also very slow because you do not disable any pipelines, so it also performs stuff like part of speech detection, which you do not need for your use case. for details you can look here: 
quoting from the docs (you might need to disable more or less pipelines):
import spacy

texts = [
    ""net income was $9.4 million compared to the prior year of $2.7 million."",
    ""revenue exceeded twelve billion dollars, with a loss of $1b."",
]

nlp = spacy.load(""en_core_web_sm"")
for doc in nlp.pipe(texts, disable=[""tok2vec"", ""tagger"", ""parser"", ""attribute_ruler"", ""lemmatizer""]):
    # do something with the doc here
    print([(ent.text, ent.label_) for ent in doc.ents])",https://stackoverflow.com/questions/78128206,python,08-03-2024 14:02,117.0,0.0,1.0,True,11-03-2024 14:29,08-03-2024 15:45,Implementation Issues
78892056,how to disable the warning message for g4f version deprecation?,"i am using this code to get my response out of the model :
from g4f.client import client

client = client()

response = client.chat.completions.create(
    model=""gpt-3.5-turbo"",
    messages=[{""role"": ""user"", ""content"": ""hello""}],
)

this code is run through subprocess.popen() call like so:
p = subprocess.popen(['c:\\python38\\python.exe','-wignore', 'c:\\users\\user\\proj\\projname\\chatgpt.py'],
                             stdin=subprocess.pipe, stdout=subprocess.pipe, shell=true, env=env)

but the call to client.chat.completions.create() generates this warning message before actually returning the model response:
new g4f version: 0.3.2.4 (current: 0.3.2.2) | pip install -u g4f

my question is how to suppress that warning message from being generated by the mentioned call?","['python', 'warnings', 'openai-api']",78892118,"you can suppress it using the warnings module:
import warnings
warnings.filterwarnings(""ignore"")

or, you can resolve the issue by upgrading the g4f package to the latest version via pip install -u g4f

you can also use sys.stdout to redirect standard output temporarily.
for example:
# save the current stdout
old_stdout = sys.stdout

# redirect all output that would normally go to stdout (e.g., the console) into a buffer instead
sys.stdout = io.stringio()

# call the g4f function that generates the warning here
...
...

# restore stdout to its original state
sys.stdout = old_stdout",https://stackoverflow.com/questions/78892056,python,20-08-2024 11:01,304.0,0.0,1.0,True,23-08-2024 13:00,20-08-2024 13:54,Implementation Issues
53118666,spacy - convert token type into list,"i have few elements which i got after performing operation in spacy having type
input -
li = ['india', 'australia', 'brazil']
for i in li:
    print(type(i))

output:

<class 'spacy.tokens.token.token'>
<class 'spacy.tokens.token.token'>
<class 'spacy.tokens.token.token'>

i want to make all elements in list with str type for iteration.
expected output -
li = ['india', 'australia', 'brazil']
for i in li:
    print(type(i))

output

<class 'str'>
<class 'str'>
<class 'str'>

please suggest some optimized way..","['python-3.x', 'list', 'token', 'spacy']",53119749,"spacy token has a attribute called text.
here's a complete example:
import spacy
nlp = spacy.load('en_core_web_sm')
t = (u""india australia brazil"")
li = nlp(t)
for i in li:
    print(i.text)

or if you want the list of tokens as list of strings:
list_of_strings  = [i.text for i in li]",https://stackoverflow.com/questions/53118666,python-3.x,02-11-2018 12:30,13206.0,5.0,2.0,True,20-06-2022 05:47,20-06-2020 09:12,Data Wrangling
58735715,getting a spacy error: no module named &#39;spacy.pipeline.pipes&#39;; &#39;spacy.pipeline&#39; is not a package,"i'm trying to test a model that is working in another machine, but when i try to import it to my notebook, i get this error: 
modulenotfounderror: no module named 'spacy.pipeline.pipes'; 'spacy.pipeline' is not a package
we have installed:
spacy 2.0.18 (frozen version, not updatable whatsoever) 
and i'm importing:
import spacy
import thinc
import unidecode
import nltk
from spacy.vocab    import vocab
from spacy.language import language
from spacy.lang.pt  import portuguese
from spacy.lang.en  import english
from spacy.pipeline import entityrecognizer
ner = entityrecognizer(nlp.vocab)
nlp = language(vocab())
nlp = portuguese()
# load ner model
ner_model = pickle.load( open(""/ner_model_v022_epoch=706_loss=09o76364626.pkl"", ""rb"" ) )

and i get the following error:
---------------------------------------------------------------------------
modulenotfounderror                       traceback (most recent call last)
<ipython-input-12-83d4770d3e3e> in <module>

---> 40 ner_model = pickle.load( open(""/ner_model_v022_epoch=706_loss=09o76364626.pkl"", ""rb"" ) )

modulenotfounderror: no module named 'spacy.pipeline.pipes'; 'spacy.pipeline' is not a package

any ideas why this might be happening? already installed everything again from 0 but keeps giving me the same error.
any help will be greatly appreciated.","['nlp', 'spacy', 'named-entity-recognition']",60910286,i had this problem come up and found that switching my spacy version from spacy==2.0.18 to spacy==2.1.4 worked! went back through their releases and spacy.pipeline.pipes isn't present until v2.1.0a8,https://stackoverflow.com/questions/58735715,nlp,06-11-2019 17:37,5713.0,3.0,3.0,True,01-01-2022 07:12,07-11-2019 15:05,Tool Setup/Errors
52021855,nltk linguistic tree traversal and extract noun phrase (np),"i created a custom classifier based chunker: digdug_classifier, which chunks the following sentence:
sentence = ""there is high signal intensity evident within the disc at t1.""

to create these chunks:
(s
  (np there/ex)
  (vp is/vbz)
  (np high/jj signal/jj intensity/nn evident/nn)
  (pp within/in)
  (np the/dt disc/nn)
  (pp at/in)
  (np t1/nnp)
  ./.)

i need to create a list of just the np from the above, like this:
np = ['there', 'high signal intensity evident', 'the disc', 't1']

i wrote the following code:
output = []
for subtree in digdug_classifier.parse(pos_tags): 
    try:
        if subtree.label() == 'np': output.append(subtree)
    except attributeerror:
        output.append(subtree)
print(output)

but that gives me this answer instead:
[tree('np', [('there', 'ex')]), tree('np', [('high', 'jj'), ('signal', 'jj'), ('intensity', 'nn'), ('evident', 'nn')]), tree('np', [('the', 'dt'), ('disc', 'nn')]), tree('np', [('t1', 'nnp')]), ('.', '.')]

what can i do to get the desired answer?","['python', 'tree', 'nltk', 'chunking']",52023337,"first, see how to traverse an nltk tree object?
specific to your question of extraction np:
>>> from nltk import tree
>>> parse_tree = tree.fromstring(""""""(s
...   (np there/ex)
...   (vp is/vbz)
...   (np high/jj signal/jj intensity/nn evident/nn)
...   (pp within/in)
...   (np the/dt disc/nn)
...   (pp at/in)
...   (np t1/nnp)
...   ./.)"""""")

# iterating through the parse tree and 
# 1. check that the subtree is a tree type and 
# 2. make sure the subtree label is np
>>> [subtree for subtree in parse_tree if type(subtree) == tree and subtree.label() == ""np""]
[tree('np', ['there/ex']), tree('np', ['high/jj', 'signal/jj', 'intensity/nn', 'evident/nn']), tree('np', ['the/dt', 'disc/nn']), tree('np', ['t1/nnp'])]

# to access the item inside the tree object, 
# use the .leaves() function
>>> [subtree.leaves() for subtree in parse_tree if type(subtree) == tree and subtree.label() == ""np""]
[['there/ex'], ['high/jj', 'signal/jj', 'intensity/nn', 'evident/nn'], ['the/dt', 'disc/nn'], ['t1/nnp']]

# to get the string representation of the leaves
# use "" "".join()
>>> [' '.join(subtree.leaves()) for subtree in parse_tree if type(subtree) == tree and subtree.label() == ""np""]
['there/ex', 'high/jj signal/jj intensity/nn evident/nn', 'the/dt disc/nn', 't1/nnp']


# to just get the leaves' string, 
# iterate through the leaves and split the string and
# keep the first part of the ""/""
>>> ["" "".join([leaf.split('/')[0] for leaf in subtree.leaves()]) for subtree in parse_tree if type(subtree) == tree and subtree.label() == ""np""]
['there', 'high signal intensity evident', 'the disc', 't1']",https://stackoverflow.com/questions/52021855,python,25-08-2018 22:55,1528.0,1.0,2.0,True,30-10-2022 12:16,26-08-2018 05:03,Implementation Issues
77940314,cannot change training arguments when resuming from a checkpoint,"i noticed when resuming the training of a model from a checkpoint changing properties like save_steps and per_device_train_batch_size has no affect. i'm wondering if there's something syntactically wrong here or technically the config of the model checkpoint overrides everything?
import transformers
from datetime import datetime

tokenizer.pad_token = tokenizer.eos_token

learning_rate = 5e-5  
warmup_steps = 100

gradient_accumulation_steps = 2  

trainer = transformers.trainer(
    model=model,
    callbacks=[upload_checkpoint_callback],
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_val_dataset,
    args=transformers.trainingarguments(
        output_dir=output_dir,
        warmup_steps=warmup_steps,
        per_device_train_batch_size=8,
        gradient_checkpointing=true,
        gradient_accumulation_steps=gradient_accumulation_steps,
        max_steps=5000,
        learning_rate=learning_rate,
        logging_steps=10,
        fp16=true,
        optim=""paged_adamw_8bit"",
        logging_dir=""/content/logs"",       
        save_strategy=""steps"",      
        save_steps=10,              
        evaluation_strategy=""steps"", 
        eval_steps=10,               
        load_best_model_at_end=true,
        report_to=""wandb"",           
        run_name=f""{run_name}-{datetime.now().strftime('%y-%m-%d-%h-%m')}""          # name of the w&b run (optional)
    ),
    data_collator=transformers.datacollatorforlanguagemodeling(tokenizer, mlm=false),
)

model.config.use_cache = false
trainer.train(resume_from_checkpoint=""/content/latest_checkpoint/"")","['huggingface-transformers', 'huggingface-trainer']",77949365,the transformers library does not have the ability to change training arguments when resuming from a checkpoint.,https://stackoverflow.com/questions/77940314,huggingface-transformers,05-02-2024 10:45,1108.0,0.0,1.0,True,06-02-2024 16:32,05-02-2024 10:54,Uncategorized
75130116,getting 400 bad request from open ai api using python flask,"i want to get response using flask from openai api. whether i am getting status 400 bad request from browser through 
bad request
the browser (or proxy) sent a request that this server could not understand.
also i am checking this from postman
from flask import flask, request, render_template
import requests

app = flask(__name__)

@app.route('/')
def index():
    return 'welcome to chatgpt app!'

@app.route('/chat', methods=['get', 'post'])
def chat():
    user_input = request.form['text']
    # use openai's api to generate a response from chatgpt
    response = generate_response_from_chatgpt(user_input)
    return response

def generate_response_from_chatgpt(user_input):
    api_key = ""your_api_key""
    url = ""
    headers = {
        ""content-type"": ""application/json"",
        ""authorization"": f""bearer {api_key}""
    }
    data = {
        ""prompt"": user_input,
        ""engine"": ""davinci""
    }
    response = requests.post(url, headers=headers, json=data)
    return response.json()[""choices""][0][""text""]


if __name__ == '__main__':
    app.run()","['flask', 'flask-restful', 'openai-api', 'gpt-3']",75130180,"it would be best if you check the openai documentation to make sure you are using the correct endpoint and data format in your request.
also, you should check your api key, if it is correct and if you have reached the limit of requests.
also, it's worth noting that the code you provided is missing the import statement for flask. you will need to add the following line at the top of your file:
from flask import flask, request
also, i see that you're using request.form['text'] but you should check if the request is a get or post request.
if request.method == 'post':
    user_input = request.form['text']
else:
    user_input = request.args.get('text')

this is to avoid a keyerror being raised when the request is a get request.",https://stackoverflow.com/questions/75130116,flask,16-01-2023 03:56,1430.0,-2.0,1.0,True,24-01-2023 18:21,24-01-2023 18:21,Implementation Issues
77640823,how to make azure openai service use default system message in api calls?,"i have created an azure openai service and set a default system message. i want to create a chatbot by sending api calls to the endpoint and integrating it into an existing react app as a component.
my aim is to allow other team members to modify and fine tune the system message and examples. however, whenever i call the openai service through api calls, i must always send the system message and examples with the request. it seems that the api responses disregard the system message unless i send it with the request.
i assumed that the default system message gets saved to the deployment and i would not have to send it on every request. is there a way to configure the azure openai service to always use the default system message that is configured in the azure openai studio?","['openai-api', 'azure-openai']",77641567,"the current 'state of the art' does not support that, as it works on the basis of stateless interaction. what you can do is place a system prompt in a variable and include it in the openai sdk call, or you can build a facade class around it. this makes it part of a class's default prompt, especially useful if you're leveraging it from multiple places, ensuring it's not 'forgotten'.",https://stackoverflow.com/questions/77640823,openai-api,11-12-2023 16:01,941.0,0.0,1.0,True,11-12-2023 18:14,11-12-2023 16:08,Implementation Issues
76950609,what is the difference between openai and chatopenai in langchain?,"i read the langchain quickstart.
there is a demo inside:
from langchain.llms import openai
from langchain.chat_models import chatopenai

llm = openai()
chat_model = chatopenai()

llm.predict(""hi!"")
>>> ""hi""

chat_model.predict(""hi!"")
>>> ""hi""

i searched the rest of the document and also online, but didn't find any info for the difference between openai and chatopenai.
based on from langchain.llms import openai, openai is a large language model (llm) which is also chat related.
so is openai more general-purpose, while chatopenai more chat focused?
what is the difference between openai class and chatopenai class in langchain? could someone clarify?","['openai-api', 'langchain']",76955946,"tl;dr
based on my research,

openai class includes more generic machine learning task attributes such as frequency_penalty, presence_penalty, logit_bias, allowed_special, disallowed_special, best_of.

chatopenai class provides more chat-related methods, such as completion_with_retry, get_num_tokens_from_messages to make it more user-friendly when build chatbot related applications.



class inheritance
upon reviewing the source code, here's what i've discovered.
listed below are the class inheritances for both the openai and chatopenai classes, along with their respective class attributes and methods.
openai
openai ï¿½ï¿½ï¿½ baseopenai ï¿½ï¿½ï¿½ basellm ï¿½ï¿½ï¿½ baselanguagemodel

chatopenai
chatopenai ï¿½ï¿½ï¿½ basechatmodel ï¿½ï¿½ï¿½ baselanguagemodel

comparison
let's begin our comparison, moving from the fourth column to the first column.
fourth column
both classes ultimately inherit from the base class baselanguagemodel.
third column
basellm and basechatmodel are very similar with slightly difference:

for openai's basellm, it includes additional methods:

batch(self, inputs, config=none, max_concurrency=none, **kwargs)
abatch (self, inputs, config=none, max_concurrency=none,**kwargs)


for chatopenai's basechatmodel, it includes an extra method:

_combine_llm_outputs(self, llm_outputs)



second column
the second column contains the baseopenai class, which primarily exists due to the presence of higher-level classes openai and azureopenai. however, they all share the same class attributes and methods.
first column
at the top-level class (first column):

openai class includes more generic machine learning task attributes such as frequency_penalty, presence_penalty, logit_bias, allowed_special, disallowed_special, best_of.

chatopenai class provides more chat-related methods, such as completion_with_retry, get_num_tokens_from_messages to make it more user-friendly when build chatbot related applications.",https://stackoverflow.com/questions/76950609,openai-api,22-08-2023 06:30,31111.0,30.0,2.0,True,29-09-2024 18:56,23-08-2023 19:37,Conceptual Questions
1833252,java stanford nlp: part of speech labels?,"the stanford nlp, demo'd here, gives an output like this:
colorless/jj green/jj ideas/nns sleep/vbp furiously/rb ./.

what do the part of speech tags mean? i am unable to find an official list. is it stanford's own system, or are they using universal tags? (what is jj, for instance?)
also, when i am iterating through the sentences, looking for nouns, for instance, i end up doing something like checking to see if the tag .contains('n'). this feels pretty weak. is there a better way to programmatically search for a certain part of speech?","java, nlp, stanford-nlp, part-of-speech",1833718,"the penn treebank project. look at the part-of-speech tagging ps.
jj is adjective. nns is noun, plural. vbp is verb present tense. rb is adverb.
that's for english. for chinese, it's the penn chinese treebank. and for german it's the negra corpus.


cc coordinating conjunction 
cd cardinal number 
dt determiner 
ex existential there 
fw foreign word 
in preposition or subordinating conjunction 
jj adjective 
jjr adjective, comparative 
jjs adjective, superlative 
ls list item marker 
md modal 
nn noun, singular or mass 
nns noun, plural 
nnp proper noun, singular 
nnps proper noun, plural 
pdt predeterminer 
pos possessive ending 
prp personal pronoun 
prp$ possessive pronoun 
rb adverb 
rbr adverb, comparative 
rbs adverb, superlative 
rp particle 
sym symbol 
to to 
uh interjection 
vb verb, base form 
vbd verb, past tense 
vbg verb, gerund or present participle 
vbn verb, past participle 
vbp verb, nonï¿½ï¿½3rd person singular present 
vbz verb, 3rd person singular present 
wdt whï¿½ï¿½determiner 
wp whï¿½ï¿½pronoun 
wp$ possessive whï¿½ï¿½pronoun 
wrb whï¿½ï¿½adverb",https://stackoverflow.com/q/1833252,"java, nlp, stanford-nlp, part-of-speech",02-12-2009 14:30,105441.0,186.0,9.0,True,14-12-2023 12:33,11-08-2015 03:27,Implementation Issues
3926891,trying to use hpsg pet parser,"i'm trying to use the pet parser, but the given documentation for usage is insufficient. can anyone point me to a good article or tutorial on using pet? does it support utf-8?","['parsing', 'utf-8', 'nlp', 'pos-tagger']",5129891,"to use the pet parser, first you have to load a grammar for the language of interest. the grammar must be authored in the tdl language, as used in the delph-in consortium (wiki here). large, compatible grammars are available for several languages, including english, japanese, and german. there are also smaller grammars available, and you can write your own.
for this--and for working with these grammars--your best bet is ann copestake's book, ""implementing typed feature structure grammars"" (csli 2002). the book provides a thorough introduction to tdl and grammars such as these which function via the unification of typed feature structures. the grammars support bidirectional mapping between syntax (surface strings) and semantics (""meaning,"" represented according to copestake's mrs--minimal recursion semantics). note that these are precision grammars, which means that they are generally less tolerant of ungrammatical inputs than statistical systems.
the english resource grammar (erg) is a large grammar of english which has broad, general-domain coverage. it's open source and you can download it from the website. an online demo, powered by the pet parser, can be found here.
the pet parser runs in two steps. the first, called flop produces a ""compiled"" version of the grammar. the second step is the actual parsing, which uses the cheap program. you will need to obtain these two pet binaries for your linux machine, or build them yourself. this step may not be easy if you're not familiar with building software on linux. pet does not run on windows (or mac, to my knowledge).
running flop is easy. just go to your /erg directory, and type:
$ flop english.tdl

this will produce the english.grm file. now you can parse sentences by running cheap:
$ echo the child has the flu. | cheap --mrs english.grm

this example produces a single semantic representation of the sentence in mrs (minimal recursion semantics) format:
 [ ltop: h1
   index: e2 [ e sf: prop tense: pres mood: indicative prog: - perf: - ]
   rels: <
          [ _the_q_rel<-1:-1>
            lbl: h3
            arg0: x6 [ x pers: 3 num: sg ind: + ]
            rstr: h5
            body: h4 ]
          [ ""_child_n_1_rel""<-1:-1>
            lbl: h7
            arg0: x6 ]
          [ ""_have_v_1_rel""<-1:-1>
            lbl: h8
            arg0: e2
            arg1: x6
            arg2: x9 [ x pers: 3 num: sg ] ]
          [ _the_q_rel<-1:-1>
            lbl: h10
            arg0: x9
            rstr: h12
            body: h11 ]
          [ ""_flu_n_1_rel""<-1:-1>
            lbl: h13
            arg0: x9 ] >
   hcons: < h5 qeq h7 h12 qeq h13 > ]

copestake's book explains the specific syntax and linguistic formalism used in grammars that are compatible with pet. it also serves as a user's manual for the open-source lkb system, which is a more interactive system that can also parse with these grammars. in addition to parsing, the lkb can do the reverse: generate sentences from mrs semantic representations. the lkb is currently only supported on linux/unix. there are actually a total of four delph-in compliant grammar processing engines, including lkb and pet.
for windows, there is agree, a multi-threaded parser/generator (and here) that i've developed for .net; it also supports both generation and parsing. if you need to work with the grammars interactively, you might want to consider using the lkb or agree in addition to--or instead of--pet. the interactive client front-ends for agree are mostly wpf-based, but the engine and a simple console client can run on any mono platform.
ace is another open-source delph-in compatible parsing and generation system which is designed for high performance, and it is available for linux and macos.
the lkb is written in lisp, whereas pet and ace are c/c++, so the latter are the faster parsers for production use. agree is also much faster than the lkb, but only becomes faster than pet when parsing complex sentences, where overheads from agree's lock-free concurrency become amortized.
[11/25/2011 edit: agree now supports generation as well as parsing]",https://stackoverflow.com/questions/3926891,parsing,13-10-2010 18:40,1406.0,8.0,2.0,True,07-11-2023 05:21,07-11-2023 05:21,Conceptual Questions
77152662,error [err_fr_max_body_length_exceeded]: request body larger than maxbodylength limit when sending a request to openai apu,"i'm using whisper.js code to get transcription of a wav file. however when running the code below this is the error i'm getting.
error [err_fr_max_body_length_exceeded]: request body larger than maxbodylength limit.
when i asked chatgpt for the error it said, 'indicates that the size of the audio file you're trying to send to openai's api is exceeding the allowed request body size' so i added the the following part in the code as per said by chatgpt:
const configuration = new configuration({
    apikey: ""your_api_key"",
    maxbodylength: infinity,
});


this is my code:
require('dotenv').config();
const { configuration, openaiapi } = require(""openai"");
const fs = require(""fs"");



const configuration = new configuration({
    apikey: ""your_api_key"",
    maxbodylength: infinity,
  });


const openai = new openaiapi(configuration);

async function createtranscription(audiofilename) {
    const resp = await openai.createtranscription(
      fs.createreadstream(audiofilename),
      ""whisper-1""
    );

    return resp.data.text;
}

async function main() {
    try {
        const audiofilename = '/users/rahulsharma/desktop/whisper/output_dir/stream.wav';
        const transcription = await createtranscription(audiofilename);
        console.log(transcription);
    } catch (e) {
        console.error(e);
    }
}

main();

does anyone how to get around this error?","['javascript', 'node.js', 'openai-api']",77156756,"you can pass some request options when creating the transcription. with the openai v3 package you need to specify all options you can pass according to the openai api reference, then you can pass on an object with maxbodylength.
await openai.createtranscription(
          fs.createreadstream(audiofilename),
          model, // ""whisper-1""
          prompt,
          responseformat,
          temperature,
          language,
          {
            maxbodylength: infinity,
          },
        );

if you don't want to specify prompt, responseformat, temperature, or language, you can just set them to undefined.
edit:
note that you only need to set maxbodylength when you use the openai library < version 4. before version 4 the library uses axios, starting with v4 the library uses fetch (well, node-fetch). maxbodylength is an option specific to axios, fetch doesn't have it and you don't need to do anything to be able to send large files to the whisper api.",https://stackoverflow.com/questions/77152662,javascript,21-09-2023 18:11,1105.0,1.0,1.0,True,22-09-2023 10:54,21-09-2023 18:17,Conceptual Questions
75091786,openai unity - post request not working properly (400 status),"i'm connecting gpt3 openai but i just cant manage to make a proper post request to it (i'm following some guides but for them it works...).
private ienumerator upload ( )
{
     form = new 
    form.addfield ( ""prompt"", prompt );
    form.addfield ( ""max_tokens"", maxtokens );
    form.addfield ( ""model"", model );
    form.addfield ( ""temperature"", temperature );

    using ( unitywebrequest wr = unitywebrequest.post ( "" form ) )
    {
        wr.setrequestheader ( ""authorization"", ""bearer "" + apikey );
        wr.setrequestheader ( ""content-type"", ""json"" );
        yield return wr.sendwebrequest ( );
        if ( wr.result != unitywebrequest.result.success )
        {
            debug.log ( ""error:\n"" + wr.error );
        }
        else
        {
            debug.log ( ""success:\n"" + wr.result + ""\nupload completed!);
        }
    }
}

my code is always returning me a bad request (a.k.a 400 bad request).","['unity-game-engine', 'httprequest', 'openai-api', 'gpt-3']",75092193,"remove the ""content-type"" from the headers. the content is not json, it's form data.
i.e.
using ( unitywebrequest wr = unitywebrequest.post ( "" form ) )
{
    wr.setrequestheader ( ""authorization"", ""bearer "" + apikey );
    //wr.setrequestheader ( ""content-type"", ""json"" );
    yield return wr.sendwebrequest ( );
    if ( wr.result != unitywebrequest.result.success )
    {
        debug.log ( ""error:\n"" + wr.error );
    }
    else
    {
        debug.log ( ""success:\n"" + wr.result + ""\nupload completed!);
    }
}",https://stackoverflow.com/questions/75091786,unity-game-engine,12-01-2023 04:58,546.0,4.0,1.0,True,30-01-2023 07:17,12-01-2023 06:45,Data Wrangling
76772986,how do i decode the output of a pytorch openaigptmodel?,"i am trying to decode the outputs of a pytorch openaigptmodel, but i can't see how to go about it, and i can't find any complete examples online.
i've found only this much:
from transformers import openaigpttokenizer, openaigptmodel
tokenizer = openaigpttokenizer.from_pretrained('openai-gpt')
model = openaigptmodel.from_pretrained('openai-gpt')

inputs = tokenizer(""how does a kite fly?"", return_tensors=""pt"")
outputs = model(**inputs)

outputs has an attribute last_hidden_state which is a torch.floattensor of shape (batch_size, sequence_length, hidden_size). i've tried grabbing the first vector of length hidden_size and calling tokenizer.decode(vector.tolist()), but i get:
'<unk><unk><unk><unk>'

i've also tried interpreting my last_hidden_state as a series of probabilities for each token in the lexicon with tokenizer.decode(torch.argmax(last_hidden_states, 2)[0].tolist()), but that also outputs nonsense:
'ï¿½ï¿½ ï¿½ï¿½ore ï¿½ï¿½have ï¿½ï¿½ï","['pytorch', 'torch', 'openai-api']",76774526,"what the model returns in this case isn't actually a representation of tokens.
there are many ways to return results from a large language model: logits predicting particular token ids, softmaxed percent probabilities of each possible token, a vector representing the embedding of a single token, etc.
given that you get batch x seq_len x hidden_size it appears that the vector of size ""hidden size"" is probably a vector representing the embedding of a single token. it's the ""lm_head"" that's used to convert a token embedding to a token id.
checking the doc for that model i see

the bare openai gpt transformer model outputting raw hidden-states without any specific head on top.

without any head, you're not getting tokens, just embeddings that can be used to predict tokens. you probably want something like openaigptlmheadmodel that has an lm head. the given example for that model appears to output logits, so maybe from that you might need to use argmax to select specific tokens. then once you have token ids you'd use something like tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(tokens))
you may be wondering why a user would want a model that returns logits instead of tokens. users don't always want to select the ""most likely"" predicted next token. by returning logits (or a probability distribution) the caller can sample from likely next tokens.",https://stackoverflow.com/questions/76772986,pytorch,26-07-2023 15:43,545.0,0.0,1.0,True,07-08-2023 21:07,07-08-2023 21:07,Implementation Issues
71622637,elastic: treat symbol and html encoded symbol the same during search,"my goal is to return the same results when searching by the symbol or html encoded version.  example queries:
# searching with symbol
get my-test-index/_search
{
  ""query"": {
    ""bool"": {
      ""must"": {
        ""simple_query_string"": {
          ""query"": ""helloï¿½ï¿½"",
          ""analyzer"": ""english_syn"",
          ""fields"": [
            ""allcontent""
          ]
        }
      }
    }
  }
}

# html symbol
get my-test-index/_search
{
  ""query"": {
    ""bool"": {
      ""must"": {
        ""simple_query_string"": {
          ""query"": ""hello&reg;"",
          ""analyzer"": ""english_syn"",
          ""fields"": [
            ""allcontent""
          ]
        }
      }
    }
  }
}

i've tried a couple different things.
adding synonyms but they still produced different resu
#######################################
# synonyms
# symbols
#######################################
ï¿½ï¿½ï¿½, &trade;
ï¿½ï¿½, &reg;

created a char_filter to replace special characters so they would at least be searching for ""hello"". but that comes with its own set of issues that is out of scope of what i am trying to achieve.
char_filter"": {
    ""specialcharactersfilter"": {
    ""type"": ""pattern_replace"",
    ""pattern"": ""[^a-za-z0-9]"",
    ""replacement"": "" ""
}

i appreciate any feedback for any new alternatives to achieve this goal. ideally a solution that covers more than ï¿½ï¿½ and","['elasticsearch', 'filter', 'analyzer', 'elasticsearch-query', 'stemming']",71625917,"what you are looking for is the html strip char filter, which works not only for two symbols but for a broad html characters.
working example
index mapping with html strip char filter
{
    ""settings"": {
        ""analysis"": {
            ""analyzer"": {
                ""my_analyzer"": {
                    ""tokenizer"": ""standard"",
                    ""char_filter"": [
                        ""html_strip""
                    ]
                }
            }
        }
    },
    ""mappings"": {
        ""properties"": {
            ""title"": {
                ""type"": ""text"",
                ""analyzer"": ""my_analyzer""
            }
        }
    }
}

index sample doc with just (ï¿½ï¿½ï¿½) in that document.
put 71622637/_doc/1

{
   ""title"" : ""ï¿½ï¿½ï¿½""
}


search encoded version
{
    ""query"" :{
        ""match"" : {
            ""title"" : ""&trade""
        }
    }
}

and search result

""hits"": [
            {
                ""_index"": ""71622637"",
                ""_id"": ""1"",
                ""_score"": 0.89701396,
                ""_source"": {
                    ""title"": ""ï¿½ï¿½ï¿½""
                }
            }
        ]

similar to this, search on trademark symbol
{
    ""query"" :{
        ""match"" : {
            ""title"" : ""ï¿½ï¿½ï¿½""
        }
    }
}

and search result

""hits"": [
            {
                ""_index"": ""71622637"",
                ""_id"": ""1"",
                ""_score"": 0.89701396,
                ""_source"": {
                    ""title"": ""ï¿½ï¿½        }
            }
        ]",https://stackoverflow.com/questions/71622637,elasticsearch,25-03-2022 20:16,409.0,1.0,1.0,True,26-03-2022 10:28,26-03-2022 10:28,Implementation Issues
74116535,"python/pandas/nltk: iterating through a dataframe, get value, transform it and add the new value to a new column","i scraped some data from google news into a dataframe:
dataframe:
df

title   link    pubdate     description     source  source_url
0   australian research finds cost-effective way t...      sat, 15 oct 2022 23:51:00 gmt   australian research finds cost-effective way t...   the guardian    
1   something new under the sun: floating solar pa...      tue, 18 oct 2022 11:49:11 gmt   something new under the sun: floating solar pa...   voice of america - voa news     
2   adapt solar panels for sub-saharan africa - na...      tue, 18 oct 2022 09:06:41 gmt   adapt solar panels for sub-saharan africanatur...   nature.com  
3   cost of living: the people using solar panels ...      wed, 05 oct 2022 07:00:00 gmt   cost of living: the people using solar panels ...   bbc     
4   business matters: solar panels on commercial p...      mon, 17 oct 2022 09:13:35 gmt   business matters: solar panels on commercial p...   insider media   
...     ...     ...     ...     ...     ...     ...

what i want to do now is basically to iterate through the ""link"" column and summarize every article with nltk and add the summary to a new column. here is an example:
article = article(df.iloc[4, 1]) #get the url from the link column
article.download()
article.parse()
article.nlp()
article = article.summary
print(article)

output:
north westgemma cornwall, head of sustainability of anderton gables, looks into the benefit of solar panels.
and, with the cost of solar panels continually dropping, it is becoming increasingly affordable for commercial property owners.
reduce your energy spendmost people are familiar with solar energy, but many are unaware of the significant financial savings that can be gained by installing solar panels in commercial buildings.
as with all things, there are pros and cons to weigh up when considering solar panels.
if youï¿½ï¿½ï¿½re considering solar panels for your property, contact one of the anderton gables team, who can advise you on the best course of action.

i tried a little bit, but i couldn't make it work...
thanks for your help","['python', 'pandas', 'dataframe', 'loops', 'nltk']",74118072,"this will be a very slow solution with a for loop, but it might work for a small dataset. iterating through all the links and then applying the transformations needed, and ultimately create a new column in the dataframe
summaries = []
for l in df['source_url'].values:
    article = article(l)
    article.download()
    article.parse()
    article.nlp()
    summaries.append(article.summary)
df['summaries'] = summaries

or you could define a custom function and the use pd.apply:
def get_description(x):
    art = article(x)
    art.download()
    art.parse()
    art.nlp()
    return art.summary

df['summary'] = df['source_url'].apply(get_description)",https://stackoverflow.com/questions/74116535,python,18-10-2022 19:33,41.0,0.0,1.0,True,19-10-2022 10:13,18-10-2022 19:39,Conceptual Questions
78301939,having trouble using openai api,"i am having trouble with this code. i want to implement ai using openai api in my react.js project but i cannot seem to get what the issue is. i ask it a question in the search bar in my project and it says ""no response from ai"". there is more to it but this is just what i think is having trouble.
//landingpage.js
import react, { usestate, useeffect } from 'react';
import { fasearch } from 'react-icons/fa';
import './app.css';
import { entryform } from './entryform';

function landingpage() {
  // states related to the healthy innovations features
  const [search, setsearch] = usestate('');
  const [showsuggestions, setshowsuggestions] = usestate(true);
  const [isloading, setisloading] = usestate(false);
  const [recipedetails, setrecipedetails] = usestate(null);
  const [showworkoutquestion, setshowworkoutquestion] = usestate(false);
  const [selectedsuggestion, setselectedsuggestion] = usestate(null);
  const [showworkoutplan, setshowworkoutplan] = usestate(false);
  const [showcaloriecalculator, setshowcaloriecalculator] = usestate(false);
  const [workoutsplit, setworkoutsplit] = usestate('');
  const [showcaloriequestion, setshowcaloriequestion] = usestate(false);
  const [chatinput, setchatinput] = usestate('');
  const [chathistory, setchathistory] = usestate([]);
  const [currenttitle, setcurrenttitle]= usestate(null)
  
  console.log(chathistory); // debugging: check the structure before rendering
  

  const createnewchat = () => {
    // clears the chat to start a new conversation
    setchatinput('');
    setcurrenttitle(null)
    // no need for setcurrenttitle in this context
  };

  const renderchathistory = () =>
  chathistory.map((chat, index) => (
      <div key={index} classname=""chat-history"">
          <p>role: {chat.role}</p>
          {/* check if chat.content is a string; if not, handle it appropriately */}
          <p>message: {chat.content}</p>
      </div>
  ));

  const handlesearchchange = (e) => {
    const inputvalue = e.target.value;
    setchatinput(inputvalue); // update chatinput instead of search state.
    setshowsuggestions(inputvalue.length > 0); // show suggestions if there's input
  };

  const renderdynamicrecommendations = () => {
    // filter suggestions based on search input
    const filteredsuggestions = staticsuggestions.filter(suggestion =>
      suggestion.tolowercase().includes(search.tolowercase())
    ); 

    return (
      <ul>
        {filteredsuggestions.map((suggestion, index) => (
          <li key={index} onclick={() => handleselectsuggestion(suggestion)} style={{ cursor: 'pointer' }}>
            {suggestion}
          </li>
        ))}
      </ul>
    );
  };

  const server_url = ""
  // get messages function and other logic remain the same, ensure you're using chatinput for input value management
  // adjusting the getmessages function to handle server response correctly
  const getmessages = async () => {
    if (!chatinput.trim()) return; // avoid sending empty messages
    setisloading(true);
  
    try {
      const response = await fetch(' {
        method: 'post',
        headers: { 'content-type': 'application/json' },
        body: json.stringify({ message: chatinput })
      });
  
      if (!response.ok) {
        throw new error(`http error! status: ${response.status}`);
      }
  
      const data = await response.json();
      const airesponse = data.choices && data.choices.length > 0
        ? data.choices[0].message
        : ""no response from ai.""; 
      // update chat history
      setchathistory(prev => [...prev, { role: 'user', content: chatinput }, { role: 'ai', content: airesponse }]);
      setchatinput(''); // clear the input field
    } catch (error) {
      console.error('fetch error:', error);
      setchathistory(prev => [...prev, { role: 'user', content: chatinput }, { role: 'ai', content: ""error communicating with ai."" }]);
    } finally {
      setisloading(false);
    }
  };


//server.js 

const port = 8000
const express = require('express')
const cors = require('cors')
require('dotenv').config()
const app = express()
app.use(express.json())
app.use(cors())

const api_key = process.env.api_key

app.post('/completions', async (req, res) => {
    const options = {
        method: ""post"",
        headers: {
            ""authorization"": `bearer ${api_key}`, 
            ""content-type"": ""application/json"" 
        },
        body: json.stringify({
            model: ""gpt-3.5-turbo"",
            messages: [{role: ""user"", content: req.body.message}],
            max_tokens: 100,
        })
    };
    try {
        const response = await fetch(' options);
        const data = await response.json();

        if (data.choices && data.choices.length > 0 && data.choices[0].message) {
            // adjust this path according to the actual structure of openai's response
            res.json({ message: data.choices[0].message.content });
        } else {
            throw new error(""invalid response structure from openai api."");
        }
    } catch (error) {
        console.error(""server error:"", error);
        res.status(500).json({ message: ""failed to get response from ai."" });
    }
});

app.listen(port, () => console.log('your server is running on port'+ port))

.env file: api_key = ""api key""
i have tried changing varablies and also seing if i have everything downloaded which i do.","['reactjs', 'node.js', 'openai-api']",78303163,"the backend returns a response in a format different from what the frontend expects.
from server.js
  if (data.choices && data.choices.length > 0 && data.choices[0].message) {
    res.json({ message: data.choices[0].message.content });
  } else {
    throw new error(""invalid response structure from openai api."");
  }

this will produce json response { message: ""response from openai"" }
however on the frontend act as if backend return raw response straight from the openai api
   const data = await response.json();
   const airesponse = data.choices && data.choices.length > 0
     ? data.choices[0].message
     : ""no response from ai.""; 

here is a fix of the frontend code to match response shape from the backend:
   const { message } = await response.json();
   const airesponse = message || ""no response from ai."";",https://stackoverflow.com/questions/78301939,reactjs,10-04-2024 02:59,561.0,1.0,1.0,True,10-04-2024 08:48,10-04-2024 04:31,Implementation Issues
76435499,extracting text and tables in semi-structured .txt,"i have a .txt file that serves as the codebook for a large dataset that looks similar to this
==============================                                                
var v960922                                                                    
              numeric                                                         
                                                                              
         admin.48                                                             
                                                                              
         summary - post mode assignment and administration                    
         -----------------------------------------------------------          
                                                                              
              post mode in this variable refers to beginning mode             
              (question admin.47).                                            
                                                                              
        749      1.   assigned to personal, administered as                   
                      personal iw                                             
          7      2.   assigned to personal, administered as                   
                      telephone iw                                            
         28      3.   assigned to telephone, administered as                  
                      personal iw                                             
        750      4.   assigned to telephone, administered as                  
                      telephone iw                                            
                                                                              
                 0.   inap, no post iw                                        
                                                                              
============================== 

i would like to be able to convert this structure into a data frame to help with cleaning and labeling the dataset for use later. my ideal end result would be a table like this

| var name | freqeuncies | value labels
| -------- | --------    | ---------------------------------------------------
| v960922  |        749  | 1. assigned to personal, administered as personal iw
| v960922  |          7  | 2. assigned to personal, administered as telephone iw
| v960922  |         28  | 3. assigned to telephone, administered as personal iw
| v960922  |        750  | 4. assigned to telephone, administered as telephone iw
| v960922  |         na  | 0. inap, no post iw
     

repeating for each of the variables included in the txt file. each variable in the file follows a similar structure but has variations in the number of values or length of the summary for instance.
my main strategy so far has been to read in the txt file with readlines and then use str_subset to break off lines of the text that meet the criteria i need with the goal of then appending these together to create a data frame.
nes <- readlines(""nes1996var.txt"")
 
vars <- str_subset(nes, ""^var"", )
vars


numbers <- str_subset(nes,""\\d?\\."")
numbers

the first instance of just grabbing variable names worked okay since i ended up with a vector of all the variables like i wanted.
however, trying to pull the tables has been trickier. i've seen other threads on stackoverflow suggest to filter off of the rows that start with numbers, but in the text file there's a lot of deadspace before the numbers so i can't pull just the rows that begin with numbers because technically there aren't any.
so instead i've pulled all the rows that have any numbers at all that are then followed by a period, hoping to catch on the value labels formatting. this was better but not perfect, both because it captured a lot of rows from summaries that included years or other numbers and the fact that some of the rows in the tables actually go over and fill in the second row, meaning sometimes the necessary text got cut off.
even after that i couldn't find a way to separate the frequency number from the value label strings since they were placed on the same row.
is there a more efficient/effective method of achieving what i want? i'm somewhat experienced with r but i am also still learning a lot if that helps also.
edit: the solution provided by dave did what i needed once i made a few tweaks. here is the code that worked for me in case anyone happens to be in a similar situation.
rl <- readlines(.txt file path)


## trim the white space from the front and back of each string 
## this will put the frequencies as the first characters in their lines. 
rl <- trimws(rl)

## find the variable delimiters
delims <- grep(""=============================="", rl)

## initialize the output as a list
out <- vector(mode=""list"", length=length(delims)-1)
    ## loop over the delimiters
for (i in 1:(length(delims) - 1)) {
  ## find the text between adjacent delimiters and call that vbl
  vbl <- rl[(delims[i] + 1):(delims[(i + 1)] - 1)]
  ## capture the varname as the stuff after ""var "" in the first row of vbl
  varname <- gsub(""var (.*)"", ""\\1"", vbl[1])
  ## identify the lines that start with a number
  resps <- grep(""^\\d"", vbl)
  
  if (length(resps) > 0) {
    ## identify the closest blank line to the last last response value and treat 
    ## that as the delimiter for the end of the last response category
    blanks <- which(vbl == """")
    resps <- c(resps, blanks[min(which(blanks > max(resps)))])
    ## grab the frequencies and remove the last one because the last one should be blank
    freqs <- gsub(""^(\\d+).*"", ""\\1"", vbl[resps])
    ## thanks to use padding out resps with the blank line after the last response category
    freqs <- freqs[-length(freqs)]
    ## for each identified response, paste together the text between the identified response row 
    ## and everything that comes before the next identifies response row.
    vlabs <- sapply(1:(length(resps) - 1), function(j) {
      paste(vbl[resps[j]:(resps[(j + 1)] - 1)], collapse = "" "")
    })
    ## remove the frequencies and white space from the start of the variable labels
    ## trim the white space around variable labels as well
    vlabs <- trimws(gsub(""^\\d+\\s+(.*)"", ""\\1"", vlabs))
    ## collect all the information in one place
    out[[i]] <- data.frame(`var name` = varname, frequencies = freqs, `value labels` = vlabs)
  } else {
    out[[i]] <- data.frame(`var name` = character(0), frequencies = character(0), `value labels` = character(0))
  }
}","['r', 'text-mining', 'txt']",76435627,"here's an example.  comments through identify what each piece of code does.  my assumption is that the delisting rows of equals signs separate each variable.
rl <- readlines(textconnection(""==============================                                                
var v960922                                                                    
              numeric                                                         
                                                                              
         admin.48                                                             
                                                                              
         summary - post mode assignment and administration                    
         -----------------------------------------------------------          
                                                                              
              post mode in this variable refers to beginning mode             
              (question admin.47).                                            
                                                                              
        749      1.   assigned to personal, administered as                   
                      personal iw                                             
          7      2.   assigned to personal, administered as                   
                      telephone iw                                            
         28      3.   assigned to telephone, administered as                  
                      personal iw                                             
        750      4.   assigned to telephone, administered as                  
                      telephone iw                                            
                                                                              
                 0.   inap, no post iw                                        
                                                                              
============================== ""))

## trim the white space from the front and back of each string 
## this will put the frequencies as the first characters in their lines. 
rl <- trimws(rl)

## find the variable delimiters
delims <- grep(""=============================="", rl)

## initialize the output as a list
out <- vector(mode=""list"", length=length(delims)-1)

## loop over the delimiters
for(i in 1:(length(delims)-1)){
  ## find the text between adjacent delimiters and call that vbl
  vbl <- rl[(delims[i]+1):(delims[(i+1)]-1)]
  ## capture the varname as the stuff after ""var "" in the first row of vbl
  varname <- gsub(""var (.*)"", ""\\1"", vbl[1])
  ## identify the lines that start with a number 
  resps <- grep(""^\\d"", vbl)
  ## identify the closest blank line to the last last response value and treat 
  ## that as the delimiter for the end of the last response category
  blanks <- which(vbl == """")
  resps <- c(resps, blanks[min(which(blanks > max(resps)))])
  ## grab the frequencies and remove the last one because the last one should be blank 
  freqs <- gsub(""^(\\d+).*"", ""\\1"", vbl[resps])
  ## thanks to use padding out resps with the blank line after the last response category
  freqs <- freqs[-length(freqs)]
  ## for each identified response, paste together the text between the identified response row 
  ## and everything that comes before the next identifies response row. 
  vlabs <- sapply(1:(length(resps)-1), function(i){
    paste(vbl[resps[i]:(resps[(i+1)]-1)], collapse="" "")
  })
  ## remove the frequencies and white space from the start of the variable labels
  ## trim the white space around variable labels as well
  vlabs <- trimws(gsub(""^\\d+\\s+(.*)"", ""\\1"", vlabs))
  ## collect all the information in one place
  out[[i]] <- data.frame(`var name` = varname, 
                    frequencies = freqs, 
                    `value labels` = vlabs)  
  
}
## make all the variables into a markdown table
lapply(out, knitr::kable)
#> [[1]]
#> 
#> 
#> |var.name |frequencies |value.labels                                             |
#> |:--------|:-----------|:--------------------------------------------------------|
#> |v960922  |749         |1.   assigned to personal, administered as personal iw   |
#> |v960922  |7           |2.   assigned to personal, administered as telephone iw  |
#> |v960922  |28          |3.   assigned to telephone, administered as personal iw  |
#> |v960922  |750         |4.   assigned to telephone, administered as telephone iw |
#> |v960922  |0           |0.   inap, no post iw                                    |

created on 2023-06-08 with reprex v2.0.2",https://stackoverflow.com/questions/76435499,r,08-06-2023 20:31,157.0,0.0,2.0,True,15-06-2023 16:34,15-06-2023 16:34,Implementation Issues
73021827,bi-gram probability,"trying to find the probability of a phrase using bi-gram
filename.txt
# how many times bigram occurs
bg_count = bigrams.count(('word1', 'word2'))

# probabilty of bigram in text p(word1 word2)
bg_count/number_of_bigrams","['python', 'nlp', 'artificial-intelligence', 'probability', 'n-gram']",73026566,"in a bigram langauge model:
p(w1,w2,w3,...wn) = p(w1)*p(w2|w1)*p(w3|w2).....*p(wn-1|wn)
so p(life, might) = p(life)*p(might|life) where

p(life) = count(life)/number of unigrams
p(might|life) = count(life, might)/count(life)

code to calculate p(life might) using bigram model:
p_life = s.count(""life"")/len(s)
p_might_given_life = bigrams.count(('life', 'might'))/s.count('life')
p_life_might = p_life * p_might_given_life
print (p_life_might)

output:
0.0024752475247524753

log probabilities
since probabilities <=1 and it is not safe to multiply many small numbers, we usually use the log probabilities since it converts multiplication to additions. and since the log is a monotone increasing function, the comparison between different log probabilities will be the same as comparing the actual probabilities.
log(p_life_might) = log(p_life * p_might_given_life)
 = log(p_life) + log(p_might_given_life)
code:
print (math.log(p_life)+math.log(p_might_given_life))

output:
-6.0014148779611505",https://stackoverflow.com/questions/73021827,python,18-07-2022 11:46,342.0,-1.0,1.0,True,18-07-2022 18:44,18-07-2022 18:44,Implementation Issues
72929507,extracting mean number of words in verb phrases,"so i have a bit of a silly question, but being fairly new to python, i can't seem to find the answer to it myself. i extracted verb phrases using spacy's matcher. now, i'm hoping to get the mean number of words in the extracted verb phrases for each person's text and stored them in a new dataframe column. in order to do this, i'm trying to create a function that i'll then apply to said dataframe column.
i created this function:
def get_length_phrases(column):
    for phrase in column:
        phrase_length = len(phrase)
        mean_length = np.mean(phrase_length)
    return mean_length

the problem is, when i apply it to the column in which the verb phrases are stored, i get an output that looks like this :
0      1.0
1      1.0
2      1.0
3      1.0
4      1.0
      ... 
235    1.0
236    1.0
237    1.0
238    1.0
239    1.0
name: verb_phrases_length, length: 240, dtype: float64

the problem is, there is way more than one word per phrase, so clearly, i'm doing something wrong, but can't seem to figure out what... statistics.mean doesn't work either...","['python', 'spacy']",72929586,"np.mean() takes an array (or similar) as an argument. as far as i can tell (correct me if i'm wrong) you are getting the mean of the length of each phase, which is just one number, and the mean of one number will be that number.
from numpy docs:

parameters:
a:array_like -
array containing numbers whose mean is desired. if a is not an array, a conversion is attempted.

you are going to want to save each length to a list, and then give that to np.mean()
def get_length_phrases(column):
    phrase_lengths = []
    for phrase in column:
        phrase_lengths.append(len(phrase))
    mean_length = np.mean(phrase_lengths)
    return mean_length

if, at this point, you are still getting 1.0, it is likely an issue with getting the phrases, not this function.",https://stackoverflow.com/questions/72929507,python,10-07-2022 15:06,68.0,3.0,1.0,True,10-07-2022 15:53,10-07-2022 15:53,Implementation Issues
70546666,search for particular parts of speech (e.g. nouns) and print them along with a preceding word,"i have a text which is made up of a list of basic sentences, such as ""she is a doctor"", ""he is a good person"", and so forth. i'm trying to write a program which will return only the nouns and the preceding pronoun (e.g. she, he, it). i need them to print as a pair, for example (she, doctor) or (he, person). i'm using spacy as this will allow me to work with similar texts in french and german as well.
this is the closest thing i've found elsewhere on this site as to what i need. what i've been trying so far is to produce a list of nouns in the text and then search the text for nouns in the list, and print the noun and the word 3 places before it (since this is the pattern for most of the sentences, and most is good enough for my purposes). this is what i've got for creating the list:


def spacy_tag(text):
  text_open = codecs.open(text, encoding='latin1').read()
  parsed_text = nlp_en(text_open)
  tokens = list([(token, token.tag_) for token in parsed_text])
  list1 = []
  for token, token.tag_ in tokens:
    if token.tag_ == 'nn':
      list1.append(token)
  return(list1)



however, when i try to do anything with it, i get an error message. i've tried using enumerate but i couldn't get that to work either. this is the current code i have for searching the text for the words in the list (i haven't gotten around to adding the part which should print the word several places beforehand as i'm still stuck on the searching part):


def spacy_search(text, list):
  text_open = codecs.open(text, encoding='latin1').read()
  for word in text_open:
   if word in list:
     print(word)



the error i get is at line 4, ""if word in list:"", and it says ""typeerror: argument 'other' has incorrect type (expected spacy.tokens.token.token, got str)""
is there a more efficient way of printing a prp, nn pair using spacy? and alternatively, how can i amend my code to work so it searches the text for the nouns in the list? (it doesn't need to be a particularly elegant solution, it just needs to produce a result).","['python', 'spacy', 'pos-tagger']",70562037,"here is a clean way to implement your intended approach.
# put your nouns of interest here
noun_list = [""doctor"", ...]

def find_stuff(text):
    doc = nlp(text)
    if len(doc) < 4: return none # too short
    
    for tok in doc[3:]:
        if tok.pos_ == ""noun"" and tok.text in noun_list and doc[tok.i-3].pos_ == ""pron"":
            return (doc[tok.i-3].text, tok.text)

as the other answer mentioned, your approach here is wrong though. you want the subject and object (or predicate nominative) of the sentence. you should use the dependencymatcher for that. here's an example:
from spacy.matcher import dependencymatcher
import spacy

nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""she is a good person"")

pattern = [
  # anchor token: verb, usually ""is""
  {
    ""right_id"": ""verb"",
    ""right_attrs"": {""pos"": ""aux""}
  },
  # verb -> pronoun
  {
    ""left_id"": ""verb"",
    ""rel_op"": "">"",
    ""right_id"": ""pronoun"",
    ""right_attrs"": {""dep"": ""nsubj"", ""pos"": ""pron""}
  },
  # predicate nominatives have ""attr"" relation
  {
    ""left_id"": ""verb"",
    ""rel_op"": "">"",
    ""right_id"": ""target"",
    ""right_attrs"": {""dep"": ""attr"", ""pos"": ""noun""}
  }
]

matcher = dependencymatcher(nlp.vocab)
matcher.add(""prednom"", [pattern])
matches = matcher(doc)

for match_id, (verb, pron, target) in matches:
    print(doc[pron], doc[verb], doc[target])

you can check dependency relations using displacy. you can learn more about what they are in the jurafsky and martin book.",https://stackoverflow.com/questions/70546666,python,01-01-2022 02:17,602.0,1.0,2.0,True,03-01-2022 05:37,01-01-2022 22:54,Implementation Issues
70563462,is there a way to use pre-trained embedding with tf-idf in tensorflow?,"i am using the default and basic implementation of text classification as:

 

  tokenizer = tokenizer(num_words=vocab_size, filters = filters)
  tokenizer.fit_on_texts(list(train_x))
  train_x = tokenizer.texts_to_sequences(train_x)
  val_x = tokenizer.texts_to_sequences(val_x)
  train_x = pad_sequences(train_x, maxlen=maxlen)
  val_x = pad_sequences(val_x, maxlen=maxlen)

 def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32') # for loading embedding

  embeddings_index = dict(get_coefs(*o.split("" "")) for o in open(embedding_file))
  all_embs = np.stack(embeddings_index.values())
  emb_mean,emb_std = all_embs.mean(), all_embs.std()
  embed_dim = all_embs.shape[1]

  word_index = tokenizer.word_index
  vocab_size = min(vocab_size, len(word_index))

  embedding_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, embed_dim)) # vocab_size was nb_words
  for word, i in word_index.items():
      if i >= vocab_size: continue
      embedding_vector = embeddings_index.get(word)
      if embedding_vector is not none: embedding_matrix[i] = embedding_vector


it works fine but is there a way to texts_to_matrix, where there are options like binart, tfidf, count etc. is it possible that i can use them with existing embeddings?
one possible way could be to use a multiple input model and then concatenate two inputs at one place. apart from that, is there any?","['python', 'tensorflow', 'keras', 'deep-learning', 'nlp']",70565057,"the most common approach is to multiply each word vector by its corresponding tf_idf score. one often sees this approach in academic papers. you could do something like this:
create tfidf scores:
import tensorflow as tf
import numpy as np
import gensim.downloader as api
from sklearn.feature_extraction.text import tfidfvectorizer
import collections

def td_idf_word2weight(text):
    print(""creating tfidfvectorizer..."")
    tfidf = tfidfvectorizer(preprocessor=' '.join)
    tfidf.fit(text)

    # if a word was never seen - it is considered to be at least as infrequent as any of the known words
    max_idf = max(tfidf.idf_)
    return collections.defaultdict(
        lambda: max_idf,
        [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])

text = [['she let the balloon float up into the air with her hopes and dreams'],
        ['the old rusted farm equipment surrounded the house predicting its demise'],
        ['he was so preoccupied with whether or not he could that he failed to stop to consider if he should']]

td_idf = td_idf_word2weight(text)

text = np.concatenate(text)
tokenizer = tf.keras.preprocessing.text.tokenizer()
tokenizer.fit_on_texts(text)
text_sequences = tokenizer.texts_to_sequences(text)
text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences, padding='post')
vocab_size = len(tokenizer.word_index) + 1
print(td_idf.items())
print(vocab_size)

creating tfidfvectorizer...
dict_items([('she', 1.6931471805599454), ('let', 1.6931471805599454), ('the', 1.2876820724517808), ('balloon', 1.6931471805599454), ('float', 1.6931471805599454), ('up', 1.6931471805599454), ('into', 1.6931471805599454), ('air', 1.6931471805599454), ('with', 1.2876820724517808), ('her', 1.6931471805599454), ('hopes', 1.6931471805599454), ('and', 1.6931471805599454), ('dreams', 1.6931471805599454), ('old', 1.6931471805599454), ('rusted', 1.6931471805599454), ('farm', 1.6931471805599454), ('equipment', 1.6931471805599454), ('surrounded', 1.6931471805599454), ('house', 1.6931471805599454), ('predicting', 1.6931471805599454), ('its', 1.6931471805599454), ('demise', 1.6931471805599454), ('he', 1.6931471805599454), ('was', 1.6931471805599454), ('so', 1.6931471805599454), ('preoccupied', 1.6931471805599454), ('whether', 1.6931471805599454), ('or', 1.6931471805599454), ('not', 1.6931471805599454), ('could', 1.6931471805599454), ('that', 1.6931471805599454), ('failed', 1.6931471805599454), ('to', 1.6931471805599454), ('stop', 1.6931471805599454), ('consider', 1.6931471805599454), ('if', 1.6931471805599454), ('should', 1.6931471805599454)])
38

create tf_idf-weighted embeddings matrix:
model = api.load(""glove-twitter-25"")
embedding_dim = 25
weight_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in tokenizer.word_index.items():
  try:
    embedding_vector = model[word] * td_idf[word]
    weight_matrix[i] = embedding_vector 
  except keyerror:
    weight_matrix[i] = np.random.uniform(-5, 5, embedding_dim)
print(weight_matrix.shape)

(38, 25)",https://stackoverflow.com/questions/70563462,python,03-01-2022 08:59,1355.0,0.0,1.0,True,03-01-2022 13:52,03-01-2022 13:52,Implementation Issues
78840242,how to return used context to answer using langchain in python,"i have built a rag system like this:
loader = pypdfloader(pdf_file_name)
raw_documents = loader.load()

text_splitter = recursivecharactertextsplitter(chunk_size=1000, chunk_overlap=200)
documents = text_splitter.split_documents(raw_documents)
print(documents[-1])

document(
   metadata={'source': '/appraisal.pdf', 'page': 37},
   page_content='file no.\nproperty address\ncity county state zip code\nclient10828\nborrower or owner john smith & kitty smith\n29 dream st\ndreamtown sc 99999\nsouthern first bank\nbb appraisals, llc'
)

compressor = coherererank(
    top_n=top_n,
    model=""rerank-english-v3.0"",
    cohere_api_key=""""
)

retriever = vectorstore.as_retriever(
    search_type=""similarity"", 
    search_kwargs={""k"": top_n}
)

compression_retriever = contextualcompressionretriever(
    base_compressor=compressor, base_retriever=retriever
)

def format_docs(docs):
    return ""\n\n"".join(doc.page_content for doc in docs)

response_schemas = [
    responseschema(name=""price"", description=""price"", type=""float""),
    responseschema(name=""unit"", description=""unit"", type=""int""),
]
output_parser = structuredoutputparser.from_response_schemas(response_schemas)

rag_prompt = prompttemplate(
    input_variables=[""context"",""question""],
    template=template,
    partial_variables={""format_instructions"": output_parser.get_format_instructions()},
)

rag_chain = (
    {""context"": compression_retriever | format_docs, ""question"": runnablepassthrough()}
    | rag_prompt
    | llm
    | output_parser
)

query = ""what is the price? how many units?""

response = rag_chain.invoke(query, config={""configurable"": {""session_id"": ""abc123""}},)

but then my response is a json with my price and unit as keys only. and i would like to be able to have a ""context"" variable that stores the paragraphs used in my document that the algo relied upon to answer the questions.
any idea how i could do that please?","['python', 'langchain', 'large-language-model']",78840830,"there are two ways to do this

for pictorial rep. of the information, as to which document the llm used,  you would need to visit langchain_smith, you must also understand some methodologies in rag - like rag-fusion, this will help you create a rag-fusion used by the llm to get the documents it used to retrieve the information.

i am not sure about this, but you can try this function, pipe it with your compression_retriever chain. the idea of this function is for you to pass it along with the llm or the retriever as the case maybe, so as to make retrieving the docs easier


def unique_union_of_documents(docs: list) -> list[any]:
    """"""
    get the unique union of the documents
    args:
    docs: the documents to be processed

    returns:
    list: the unique union of the documents""""""

    doc_news = [json.dumps(doc.page_content) for _ in docs]
    # find the unique union of the documents
    unique_union = list(set(doc_news))

    return [json.loads(doc) for doc in unique_union]

you can now call this function after your variable response",https://stackoverflow.com/questions/78840242,python,06-08-2024 16:57,162.0,1.0,1.0,True,07-08-2024 08:43,07-08-2024 07:53,Implementation Issues
75119911,lstm named entity recognition model - shape are incompatible or logits/labels have different dimensions - tensorflow 2.9,"i am working on nlp lstm named entity extraction model but running into different errors below are more details about error. i am running this code in jupiter notebook
tensorflow version 2.9
both input and output are of length 50
input sentence : [123 88 170 221 132 52 105 32 211 91 126 211 24 221 134 154 221 162
215 80 144 101 61 136 68 133 40 200 133 40 218 131 139 199 124 74
184 92 213 185 221 221 221 221 221 221 221 221 221 221]
output sentece label: [ 7 7 7 7 0 7 6 2 7 5 1 7 7 7 7 7 7 7 7 10 7 7 7 7
3 8 7 3 8 7 7 7 7 7 7 7 7 6 2 7 7 7 7 7 7 7 7 7
7 7]
added upto 5 layers to train the model
here is the model:
model = tf.keras.sequential([

tf.keras.layers.embedding(num_words, 50, input_length=50),

tf.keras.layers.bidirectional(tf.keras.layers.lstm(64, return_sequences=true)),

tf.keras.layers.bidirectional(tf.keras.layers.lstm(32)),

tf.keras.layers.dropout(0.5),

tf.keras.layers.dense(64, activation=ï¿½ï¿½ï¿½reluï¿½ï¿½ï¿½),

tf.keras.layers.dense(num_tags, activation=ï¿½ï¿½ï¿½softmaxï¿½ï¿½ï¿½)
])

if i use loss function as ï¿½ï¿½ï¿½categorical_crossentropyï¿½ï¿½ï¿½ , i get this error:
valueerror: shapes (none, 50) and (none, 11) are incompatible
if i use loss function as ï¿½ï¿½ï¿½sparse_categorical_crossentropyï¿½ï¿½ï¿½ , i get this error:
logits and labels must have the same first dimension, got logits shape [13,11] and labels shape [650]
[[{{node sparse_categorical_crossentropy/sparsesoftmaxcrossentropywithlogits/sparsesoftmaxcrossentropywithlogits}}]]
i tried adding input shape as first layer but still no luck
tf.keras.layers.input(shape=(max_len,))
can anyone help , how to solve this. tried  different approaches but no luck
here is model summary
layer (type)                output shape              param #   
=============================================================            11100     
                                                                 
 bidirectional_35 (bidirecti  (none, 50, 128)          58880     
 onal)                                                           
                                                                 
 bidirectional_36 (bidirecti  (none, 64)               41216     
 onal)                                                           
                                                                 
 dropout_17 (dropout)        (none, 64)                0         
                                                                 
 dense_35 (dense)            (none, 64)                4160      
                                                                 
 dense_36 (dense)            (none, 11)                715       
                                                                 
=================================================================
total params: 116,071
trainable params: 116,071
non-trainable params: 0
_________________________________________________________________","['keras', 'nlp', 'lstm', 'tensorflow2.0', 'named-entity-recognition']",75127524,"i think you have a problem in 2 last dense layers. when run on a sequence of 50 numbers, you will get 'num_tags' numbers as output (11).
but you want to get 'num_tags' outputs at each step of the sequence, not at the end. to achieve this, you can use timedistributed layer:
tf.keras.layers.timedistributed(tf.keras.layers.dense(64, activation=ï¿½ï¿½ï¿½reluï¿½ï¿½ï¿½)),
tf.keras.layers.timedistributed(tf.keras.layers.dense(num_tags, activation=ï¿½ï¿½ï¿½softmaxï¿½ï¿½ï¿½))

then you can use ï¿½ï¿½ï¿½sparse_categorical_crossentropyï¿½ï¿½ï¿½ loss function since your labels are ints.",https://stackoverflow.com/questions/75119911,keras,14-01-2023 17:25,91.0,2.0,1.0,True,15-01-2023 18:40,14-01-2023 17:31,Task-specific Help
78697835,deepspeed : attributeerror: &#39;dummyoptim&#39; object has no attribute &#39;step&#39;,"i want to use deepspeed for training llms along with huggingface trainer. but when i use deepspeed  along with trainer i get error ""attributeerror: 'dummyoptim' object has no attribute 'step'"". below is my code
import argparse
import numpy as np
import torch
from datasets import load_dataset
from transformers import autotokenizer, automodelforcausallm

from trl import dpotrainer, dpoconfig
def preprocess_data(item):
    return {
        'prompt': 'instruct: ' + item['prompt'] + '\n',
        'chosen': 'output: ' + item['chosen'],
        'rejected': 'output: ' + item['rejected']
    }        

def main():
    parser = argparse.argumentparser()
    parser.add_argument(""--epochs"", type=int, default=1)
    parser.add_argument(""--beta"", type=float, default=0.1)
    parser.add_argument(""--batch_size"", type=int, default=4)
    parser.add_argument(""--lr"", type=float, default=1e-6)
    parser.add_argument(""--seed"", type=int, default=2003)
    parser.add_argument(""--model_name"", type=str, default=""eleutherai/pythia-14m"")
    parser.add_argument(""--dataset_name"", type=str, default=""jondurbin/truthy-dpo-v0.1"")
    parser.add_argument(""--local_rank"", type=int, default=0)

    args = parser.parse_args()

    # determine device based on local_rank
    device = torch.device(""cuda"", args.local_rank) if torch.cuda.is_available() else torch.device(""cpu"")


    tokenizer = autotokenizer.from_pretrained(args.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    model = automodelforcausallm.from_pretrained(args.model_name).to(device)
    ref_model = automodelforcausallm.from_pretrained(args.model_name).to(device)

    dataset = load_dataset(args.dataset_name, split=""train"")
    dataset = dataset.map(preprocess_data)

    # split the dataset into training and validation sets
    dataset = dataset.train_test_split(test_size=0.1, seed=args.seed)
    train_dataset = dataset['train']
    val_dataset = dataset['test']

    training_args = dpoconfig(
        learning_rate=args.lr,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        logging_steps=10,
        remove_unused_columns=false,
        max_length=1024,
        max_prompt_length=512,
        deepspeed=""ds_config.json""       
    )

    

    # verify and print embedding dimensions before finetuning
    print(""base model embedding dimension:"", model.config.hidden_size)

    model.train()
    ref_model.eval()

    dpo_trainer = dpotrainer(
        model,
        ref_model,
        beta=args.beta,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        args=training_args,
    )

    dpo_trainer.train()
    # evaluate
    evaluation_results = dpo_trainer.evaluate()
    print(""evaluation results:"", evaluation_results)

    save_model_name = 'finetuned_model'
    model.save_pretrained(save_model_name)

if __name__ == ""__main__"":
    main()

the config file used is the below one
{
""zero_optimization"": {
        ""stage"": 3,
        ""offload_optimizer"": {
            ""device"": ""cpu"",
            ""pin_memory"": true
        },
        ""offload_param"": {
            ""device"": ""cpu"",
            ""pin_memory"": true
        },
        ""overlap_comm"": true,
        ""contiguous_gradients"": true,
        ""sub_group_size"": 1e9,
        ""reduce_bucket_size"": ""auto"",
        ""stage3_prefetch_bucket_size"": ""auto"",
        ""stage3_param_persistence_threshold"": ""auto"",
        ""stage3_max_live_parameters"": 1e9,
        ""stage3_max_reuse_distance"": 1e9,
        ""stage3_gather_16bit_weights_on_model_save"": true
    },
""bf16"": {
    ""enabled"": ""auto""
},
""fp16"": {
    ""enabled"": ""auto"",
    ""loss_scale"": 0,
    ""initial_scale_power"": 32,
    ""loss_scale_window"": 1000,
    ""hysteresis"": 2,
    ""min_loss_scale"": 1
},

""gradient_accumulation_steps"": ""auto"",
""gradient_clipping"": ""auto"",
""train_batch_size"": ""auto"",
""train_micro_batch_size_per_gpu"": ""auto"",
""wall_clock_breakdown"": false,
""flops_profiler"": {
    ""enabled"": false,
    ""detailed"": false
},
""optimizer"": {
    ""type"": ""lamb"",
    ""params"": {
    ""lr"": ""auto"",
    ""betas"": [0.9, 0.999],
    ""eps"": ""auto"",
    ""weight_decay"": ""auto""
    }
},
""zero_allow_untested_optimizer"": true
}

the code works with out deepspeed. i have torch=2.3.1, deepspeed                 =0.14.5, trl=0.9.4 and cuda version: 12.5.
appreciate any hint on this !","['python', 'huggingface-transformers', 'large-language-model', 'huggingface-trainer', 'deepspeed']",78713256,"from accelerate.utils import distributedtype

training_args.distributed_state.distributed_type = distributedtype.deepspeed

adding this solves the issue",https://stackoverflow.com/questions/78697835,python,02-07-2024 14:53,635.0,0.0,1.0,True,08-07-2024 17:50,08-07-2024 17:50,Tool Setup/Errors
74961297,failed to connect to tensorflow master: tpu worker may not be ready or tensorflow master address is incorrect,"i signed up for the tensor research cloud (trc) program for the third time in two years. now i barely created a preemptible v3-8 tpu. before that, i could efficiently allocate five non-preemptible v3-8 tpus. even with this allocation (preemptible and non-preemptible), the tpu is listed as ready and healthy. however, when i try to access it from the pretraining script, i run into this error that i have never encountered before:
failed to connect to the tensorflow master. the tpu worker may not be ready (still scheduling), or the tensorflow master address is incorrect.

i know that the tensorflow master address is correct, and i have checked that the tpu is healthy and ready. i have also double-checked that my code is correctly creating the tensorflow session and specifying the tpu address.
what could be causing this error message, and how can i troubleshoot and fix it?
i also tried this code from  note that i'm not using colab but using google cloud platform.
resolver = tf.distribute.cluster_resolver.tpuclusterresolver(tpu='pretrain-1')
tf.config.experimental_connect_to_cluster(resolver)
# this is the tpu initialization code that has to be at the beginning.
tf.tpu.experimental.initialize_tpu_system(resolver)
print(""all devices: "", tf.config.list_logical_devices('tpu'))

any i'm stuck at:
info:tensorflow:initializing the tpu system: pretrain-1

however, i expected something like this:
info:tensorflow:deallocate tpu buffers before initializing tpu system.
2022-12-20 13:08:56.187870: e tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuinit: cuda_error_no_device: no cuda-capable device is detected
info:tensorflow:deallocate tpu buffers before initializing tpu system.
info:tensorflow:initializing the tpu system: grpc://10.99.59.162:8470
info:tensorflow:initializing the tpu system: grpc://10.99.59.162:8470
info:tensorflow:finished initializing tpu system.
info:tensorflow:finished initializing tpu system.
all devices:  [logicaldevice(name='/job:worker/replica:0/task:0/device:tpu:0', device_type='tpu'), logicaldevice(name='/job:worker/replica:0/task:0/device:tpu:1', device_type='tpu'), logicaldevice(name='/job:worker/replica:0/task:0/device:tpu:2', device_type='tpu'), logicaldevice(name='/job:worker/replica:0/task:0/device:tpu:3', device_type='tpu'), logicaldevice(name='/job:worker/replica:0/task:0/device:tpu:4', device_type='tpu'), logicaldevice(name='/job:worker/replica:0/task:0/device:tpu:5', device_type='tpu'), logicaldevice(name='/job:worker/replica:0/task:0/device:tpu:6', device_type='tpu'), logicaldevice(name='/job:worker/replica:0/task:0/device:tpu:7', device_type='tpu')]

edit: i successfully accessed the tpu with the same configurations from a new tensor research cloud (trc) account. however, the problem is still ongoing with the previous trc account. i suspect it might be a problem with the google cloud platform (gcp) configuration.","['google-cloud-platform', 'google-compute-engine', 'bert-language-model', 'pre-trained-model', 'tpu']",74974689,"i solved the problem by deleting all tpus and vm instances and then disabling and reenabling all apis.
the issue might be related to the vpn connection to a gpu cluster during enabling services.",https://stackoverflow.com/questions/74961297,google-cloud-platform,30-12-2022 12:01,551.0,0.0,2.0,True,01-01-2023 12:48,01-01-2023 12:48,Data Wrangling
59858705,is it possible to add date picker in rasa chatbot?,"i am trying to make a chatbot using rasa for a website where the user will be asked their birth date like the photo i have attached.  
is it possible to add date picker in rasa? i have already read the documentation but confused about how to customize it for website.","['nlp', 'chatbot', 'rasa-nlu', 'rasa-core', 'rasa']",60169173,"date picker or other accessories are possible to add to rasa and you can include them to your assistant by specifying custom output payloads. however, for that to work, you will need the specific accessory to be implemented in your client first.
for example, slack has an block datepicker which can then be incorporated to rasa as follows:
responses:
  utter_take_bet:
  - custom:
      blocks:
      - type: section
        text:
          text: ""make a bet on when the world will end:""
          type: mrkdwn
        accessory:
          type: datepicker
          initial_date: '2019-05-21'
          placeholder:
            type: plain_text
            text: select a date

which chat widget are you using for integrating rasa on your website?",https://stackoverflow.com/questions/59858705,nlp,22-01-2020 11:27,1940.0,0.0,1.0,True,09-02-2023 06:19,06-02-2020 10:11,Implementation Issues
75879812,using a single api key or generating temporary keys for each user?,"i'm building a flutter app that requires the openai gpt3 api, and i'm not sure how to implement it. currently, i'm using a single api key that's stored in a .env file and accessed via the flutter_dotenv package. i'm wondering whether it's best to use this one api key for all users of the app, or whether i should implement an api gateway and generate temporary api keys for each user.
while i don't anticipate reaching the request limit for my single api key after i release the app, i'm uncertain about the best approach. what are the potential downsides to using a single api key for all users, and what are the benefits of generating temporary keys for each user? would an api gateway be necessary for my use case?","['flutter', 'openai-api', 'api-key']",75884936,"are you concerned about security or people abusing your tool and hurting your openai api limits?
security
if you are concerned about security, keep your api key secret, and make sure it does not leak to any frontend or public repo.
you can even use secret manager solutions like doppler, aws secret manager, or 1password for developers.
cost/api limits
you may want reduce the risk of someone harming your system and potentially costing you thousands of dollars by making a lot of api requests.
one solution is to track on your side how many calls are made over a period of time per user.
ex: one user can generate 15 completions per period of 24 hours.
if you offer a paying plan for your service, this is an incentive for people to upgrade.
to fight abuse, openai has also implemented end-user ids.
you can add a user parameter to your api requests. it will identify which user is responsible for which api call, and eventually, you can shut it down.
here you can read more in the doc.",https://stackoverflow.com/questions/75879812,flutter,29-03-2023 16:23,1442.0,-2.0,2.0,True,31-10-2024 11:30,31-10-2024 11:30,Implementation Issues
78104294,azureopenai upload a file from memory,"i am building an assistant and i would like to give it a dataset to analyze. i understand that i can upload a file that an assistant can use with the following code:
from openai import azureopenai
import pandas as pd

client = azureopenai(**credentials_here)

pd.dataframe({
    ""a"": [1, 2, 3, 4, 5],
    ""b"": [6, 7, 8, 9, 10],
    ""c"": [11, 12, 13, 14, 15],
}).to_csv('data.csv', index=false)

file = client.files.create(
    file=open(
        ""data.csv"",
        ""rb"",
    ),
    purpose=""assistants"",
)

i would prefer to upload the file from a data structure in memory. how can i upload a data from memory using the azureopenai client?
i read that openai allows users to provide bytes-like objects so i hoped i could do this with pickle.dumps
import pickle
df = pd.dataframe({
    ""a"": [1, 2, 3, 4, 5],
    ""b"": [6, 7, 8, 9, 10],
    ""c"": [11, 12, 13, 14, 15],
})

file = client.files.create(
    file=pickle.dumps(df),
    purpose=""assistants""
)

this snippet does not throw an error using the openai client. i get the below through the azureopenai client.
openai.badrequesterror: error code: 400 - {'error': {'message': ""invalid file format. supported formats: ['c', 'cpp', 'csv', 'docx', 'html', 'java', 'json', 'md', 'pdf', 'php', 'pptx', 'py', 'rb', 'tex', 'txt', 'css', 'jpeg', 'jpg', 'js', 'gif', 'png', 'tar', 'ts', 'xlsx', 'xml', 'zip']"", 'type': 'invalid_request_error', 'param': none, 'code': none}}","['python', 'openai-api', 'azure-openai', 'openai-assistants-api']",78110840,"it looks like azureopenai does accept bytes encoded objects from io.bytesio. so one easy way to do this for a dataframe is to use io.bytesio on the string representation of a dataframe.
import io
df = pd.dataframe({
    ""a"": [1, 2, 3, 4, 5],
    ""b"": [6, 7, 8, 9, 10],
    ""c"": [11, 12, 13, 14, 15],
})

in_memory_df = io.bytesio(df.to_csv().encode())

file = client.files.create(
    file=in_memory_df,
    purpose=""assistants""
)

tuples of (file_name, bytes_contents, file_type) are also accepted so this code snippet is also valid and more explicit.
file = client.files.create(
    file=('name_dataset_here.csv', in_memory_df, 'text/csv'),
    purpose=""assistants""
)",https://stackoverflow.com/questions/78104294,python,04-03-2024 22:06,650.0,0.0,1.0,True,05-03-2024 22:18,05-03-2024 13:29,Data Wrangling
69195950,problem with inputs when building a model with tfbertmodel and autotokenizer from huggingface&#39;s transformers,"i'm trying to build the model illustrated in this picture:

i obtained a pre-trained bert and respective tokenizer from huggingface's transformers in the following way:
from transformers import autotokenizer, tfbertmodel
model_name = ""dbmdz/bert-base-italian-xxl-cased""
tokenizer = autotokenizer.from_pretrained(model_name)
bert = tfbertmodel.from_pretrained(model_name)

the model will be fed a sequence of italian tweets and will need to determine if they are ironic or not.
i'm having problems building the initial part of the model, which takes the inputs and feeds them to the tokenizer in order to get a representation i can feed to bert.
i can do it outside of the model-building context:
my_phrase = ""ciao, come va?""
# an equivalent version is tokenizer(my_phrase, other parameters)
bert_input = tokenizer.encode(my_phrase, add_special_tokens=true, return_tensors='tf', max_length=110, padding='max_length', truncation=true) 
attention_mask = bert_input > 0
outputs = bert(bert_input, attention_mask)['pooler_output']

but i'm having troubles building a model that does this. here is the code for building such a model (the problem is in the first 4 lines ):
def build_classifier_model():
  text_input = tf.keras.layers.input(shape=(), dtype=tf.string, name='text')
  encoder_inputs = tokenizer(text_input, return_tensors='tf', add_special_tokens=true, max_length=110, padding='max_length', truncation=true)
  outputs = bert(encoder_inputs)
  net = outputs['pooler_output']
  
  x = tf.keras.layers.bidirectional(tf.keras.layers.lstm(64, return_sequences=true, dropout=0.1, recurrent_dropout=0.1))(net)
  x = tf.keras.layers.concatenate(axis=-1)([x, input_layer])
  x = tf.keras.layers.maxpooling1d(20)(x)
  x = tf.keras.layers.spatialdropout1d(0.4)(x)
  x = tf.keras.layers.flatten()(x)
  x = tf.keras.layers.dense(128, activation=""relu"")(x)
  x = tf.keras.layers.dropout(0.25)(x)
  x = tf.keras.layers.dense(2, activation='softmax')(x)

  model = tf.keras.model(inputs=text_input, outputs = x) 
  
  return model

and when i call the function for creating this model i get this error:

text input must of type str (single example), list[str] (batch or single pretokenized example) or list[list[str]] (batch of pretokenized examples).

one thing i thought was that maybe i had to use the tokenizer.batch_encode_plus function which works with lists of strings:
class bertpreprocessinglayer(tf.keras.layers.layer):
  def __init__(self, tokenizer, maxlength):
    super().__init__()
    self._tokenizer = tokenizer
    self._maxlength = maxlength
  
  def call(self, inputs):
    print(type(inputs))
    print(inputs)
    tokenized = tokenizer.batch_encode_plus(inputs, add_special_tokens=true, return_tensors='tf', max_length=self._maxlength, padding='max_length', truncation=true)
    return tokenized

def build_classifier_model():
  text_input = tf.keras.layers.input(shape=(), dtype=tf.string, name='text')
  encoder_inputs = bertpreprocessinglayer(tokenizer, 100)(text_input)
  outputs = bert(encoder_inputs)
  net = outputs['pooler_output']
  # ... same as above

but i get this error:

batch_text_or_text_pairs has to be a list (got <class 'keras.engine.keras_tensor.kerastensor'>)

and beside the fact i haven't found a way to convert that tensor to a list with a quick google search, it seems weird that i have to go in and out of tensorflow in this way.
i've also looked up on the huggingface's documentation but there is only a single usage example, with a single phrase, and what they do is analogous at my ""out of model-building context"" example.
edit:
i also tried with lambdas in this way:
tf.executing_eagerly()

def tokenize_tensor(tensor):
  t = tensor.numpy()
  t = np.array([str(s, 'utf-8') for s in t])
  return tokenizer(t.tolist(), return_tensors='tf', add_special_tokens=true, max_length=110, padding='max_length', truncation=true)

def build_classifier_model():
  text_input = tf.keras.layers.input(shape=(1,), dtype=tf.string, name='text')
  
  encoder_inputs = tf.keras.layers.lambda(tokenize_tensor, name='tokenize')(text_input)
  ...
  
  outputs = bert(encoder_inputs)

but i get the following error:

'tensor' object has no attribute 'numpy'

edit 2:
i also tried the approach suggested by @mdaoust of wrapping everything in a tf.py_function and got this error.
def py_func_tokenize_tensor(tensor):
  return tf.py_function(tokenize_tensor, [tensor], tout=[tf.int32, tf.int32, tf.int32])


eager_py_func() missing 1 required positional argument: 'tout'

then i defined tout as the type of the value returned by the tokenizer:
transformers.tokenization_utils_base.batchencoding
and got the following error:

expected datatype for argument 'tout' not <class
'transformers.tokenization_utils_base.batchencoding'>

finally i unpacked the value in the batchencoding in the following way:
def tokenize_tensor(tensor):
  t = tensor.numpy()
  t = np.array([str(s, 'utf-8') for s in t])
  dictionary = tokenizer(t.tolist(), return_tensors='tf', add_special_tokens=true, max_length=110, padding='max_length', truncation=true)
  #unpacking
  input_ids = dictionary['input_ids']
  tok_type = dictionary['token_type_ids']
  attention_mask = dictionary['attention_mask']
  return input_ids, tok_type, attention_mask

and get an error in the line below:
...
outputs = bert(encoder_inputs)


valueerror: cannot take the length of shape with unknown rank.","['tensorflow', 'keras', 'huggingface-transformers', 'bert-language-model', 'huggingface-tokenizers']",69336070,"for now i solved by taking the tokenization step out of the model:
def tokenize(sentences, tokenizer):
    input_ids, input_masks, input_segments = [],[],[]
    for sentence in sentences:
        inputs = tokenizer.encode_plus(sentence, add_special_tokens=true, max_length=128, pad_to_max_length=true, return_attention_mask=true, return_token_type_ids=true)
        input_ids.append(inputs['input_ids'])
        input_masks.append(inputs['attention_mask'])
        input_segments.append(inputs['token_type_ids'])        
        
    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')

the model takes two inputs which are the first two values returned by the tokenize funciton.
def build_classifier_model():
   input_ids_in = tf.keras.layers.input(shape=(128,), name='input_token', dtype='int32')
   input_masks_in = tf.keras.layers.input(shape=(128,), name='masked_token', dtype='int32') 

   embedding_layer = bert(input_ids_in, attention_mask=input_masks_in)[0]
...
   model = tf.keras.model(inputs=[input_ids_in, input_masks_in], outputs = x)

   for layer in model.layers[:3]:
     layer.trainable = false
   return model

i'd still like to know if someone has a solution which integrates the tokenization step inside the model-building context so that an user of the model can simply feed phrases to it to get a prediction or to train the model.",https://stackoverflow.com/questions/69195950,tensorflow,15-09-2021 15:28,6885.0,8.0,6.0,True,08-09-2022 11:11,26-09-2021 11:26,Implementation Issues
73975817,how do a put a different classifier on top of bertforsequenceclassification?,"i have a huggingface model:
model_name = 'bert-base-uncased'
model = bertforsequenceclassification.from_pretrained(model_name, num_labels=1).to(device)

how can i change the default classifier head? since it's only a single linearclassifier. i found this issue in the huggingface github which said:

you can also replace self.classifier with your own model.
model = bertforsequenceclassification.from_pretrained(""bert-base-multilingual-cased"")
model.classifier = new_classifier

where new_classifier is any pytorch model that you want.

however, i can't figure out how the structure of the new_classifier should look like (in particular the inputs and outputs so it can handle batches).","['machine-learning', 'pytorch', 'huggingface-transformers', 'huggingface']",73976116,"by looking at the source code of bertforsequenceclassification here, you can see that the classifier is simply a linear layer that project the bert output from hidden_size dimension to num_labels dimension. suppose you want to change the linear classifier to a two layer mlp with relu activation, you can do the following:
new_classifier = nn.sequential(
      nn.linear(config.hidden_size, config.hidden_size *2),
      nn.relu(),
      nn.linear(config.hidden_size * 2, config.num_labels)
    )
model.classifier = new_classifier

the requirement of the structure of your new classifier is its input dimension and output dimension need to be config.hidden_size dimension and config.num_labels accordingly. the structure of the classifier doesn't rely on the batch size, and module like nn.linear takes (*, h_dimension) dimension as input so you don't need to specify the batch size when creating the new classifier.",https://stackoverflow.com/questions/73975817,machine-learning,06-10-2022 14:47,384.0,0.0,1.0,True,06-10-2022 17:22,06-10-2022 17:22,Implementation Issues
76863889,how does one fix an interleaved data set from only sampling one data set?,"the following
from datasets import load_dataset
from datasets import interleave_datasets

# preprocess each dataset
c4 = load_dataset(""c4"", ""en"", split=""train"", streaming=true) 
wikitext = load_dataset(""wikitext"", ""wikitext-103-v1"", split=""train"", streaming=true)

# interleave the preprocessed datasets  
datasets = [c4, wikitext]
for dataset in datasets:
  print(dataset.description)
interleaved = interleave_datasets(datasets, probabilities=[0.5, 0.5])
print(interleaved)

only samples from one data set, why?
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
counts=100

colab: 

cross:

hf discord: 
hf discuss:","['python', 'huggingface-transformers', 'huggingface', 'huggingface-datasets']",76868861,"the interleave_datasets function works correctly here, it's your conclusion that is incorrect. what happens is that when two datasets are interleaved, their features are combined.
these are the features of c4 and wikitext:
print(c4.column_names)

>>> ['text', 'timestamp', 'url']

print(wikitext.column_names)

>>> ['text']

when you combine the datasets, all examples in the new dataset will have features ['text', 'timestamp', 'url'], even if they come from wikitext dataset. since wikitext dataset does not have features timestamp and url, these will be none.
dummy example:
from datasets import dataset, interleave_datasets
d1 = dataset.from_dict({
  'feature_1': ['a', 'b', 'c']
})
d2 = dataset.from_dict({
  'feature_2': [1, 2, 3]
})

dataset = interleave_datasets([d1, d2], probabilities=[0.5, 0.5], seed=42)
print('features:', dataset.column_names)

for e in dataset:
  print(e)

output:
features: ['feature_1', 'feature_2']
{'feature_1': none, 'feature_2': 1}
{'feature_1': 'a', 'feature_2': none}
{'feature_1': none, 'feature_2': 2}
{'feature_1': none, 'feature_2': 3}",https://stackoverflow.com/questions/76863889,python,09-08-2023 00:37,105.0,0.0,1.0,True,09-08-2023 14:40,09-08-2023 00:43,Implementation Issues
56653159,why is the value of tf-idf different from idf_?,"why is the value of the vectorized corpus different from the value obtained through the idf_ attribute? should not the idf_ attribute just return the inverse document frequency (idf) in the same way it appears in the corpus vectorized? 
from sklearn.feature_extraction.text import tfidfvectorizer
corpus = [""this is very strange"",
          ""this is very nice""]
vectorizer = tfidfvectorizer()
corpus = vectorizer.fit_transform(corpus)

print(corpus)

corpus vectorized:
  (0, 2)    0.6300993445179441
  (0, 4)    0.44832087319911734
  (0, 0)    0.44832087319911734
  (0, 3)    0.44832087319911734
  (1, 1)    0.6300993445179441
  (1, 4)    0.44832087319911734
  (1, 0)    0.44832087319911734
  (1, 3)    0.44832087319911734

vocabulary and idf_ values:
print(dict(zip(vectorizer.vocabulary_, vectorizer.idf_)))

output:
{'this': 1.0, 
 'is': 1.4054651081081644, 
 'very': 1.4054651081081644, 
 'strange': 1.0, 
 'nice': 1.0}

vocabulary index:
print(vectorizer.vocabulary_)

output:
{'this': 3, 
 'is': 0, 
 'very': 4, 
 'strange': 2, 
 'nice': 1}

why is the idf value of the word this is 0.44 in the corpus and 1.0 when obtained by idf_?","['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']",56653559,"this is because of l2 normalization, which is applied by default in tfidfvectorizer().
if you set the norm param as none, you will get the same values as idf_.

>>> vectorizer = tfidfvectorizer(norm=none)

#output

  (0, 2)    1.4054651081081644
  (0, 4)    1.0
  (0, 0)    1.0
  (0, 3)    1.0
  (1, 1)    1.4054651081081644
  (1, 4)    1.0
  (1, 0)    1.0
  (1, 3)    1.0

also, your way of computing the feature's corresponding idf values is wrong because dict does not preserve the order.
you could use the following method:
 >>>> print(dict(zip(vectorizer.get_feature_names(), vectorizer.idf_)))
      
     {'is': 1.0,
      'nice': 1.4054651081081644, 
      'strange': 1.4054651081081644, 
      'this': 1.0, 
      'very': 1.0}",https://stackoverflow.com/questions/56653159,python,18-06-2019 16:08,547.0,3.0,1.0,True,16-03-2023 06:32,18-06-2019 17:40,Implementation Issues
48573174,how to combine tfidf features with other features,"i have a classic nlp problem, i have to classify a news as fake or real.
i have created two sets of features:
a) bigram term frequency-inverse document frequency
b) approximately 20 features associated to each document obtained using pattern.en ( as subjectivity of the text, polarity, #stopwords, #verbs, #subject, relations grammaticals etc ...
which is the best way to combine the tfidf features with the other features for a single prediction?
thanks a lot to everyone.","['machine-learning', 'nlp', 'text-analysis']",48573688,"not sure if your asking technically how to combine two objects in code or what to do theoretically after so i will try and answer both.
technically your tfidf is just a matrix where the rows are records and the columns are features. as such to combine you can append your new features as columns to the end of the matrix. probably your matrix is a sparse matrix (from scipy) if you did this with sklearn so you will have to make sure your new features are a sparse matrix as well (or make the other dense).
that gives you your training data, in terms of what to do with it then it is a little more tricky. your features from a bigram frequency matrix will be sparse (im not talking data structures here i just mean that you will have a lot of 0s), and it will be binary. whilst your other data is dense and continuous. this will run in most machine learning algorithms as is although the prediction will probably be dominated by the dense variables. however with a bit of feature engineering i have built several classifiers in the past using tree ensambles that take a combination of term-frequency variables enriched with some other more dense variables and give boosted results (for example a classifier that looks at twitter profiles and classifies them as companies or people). usually i found better results when i could at least bin the dense variables into binary (or categorical and then hot encoded into binary) so that they didn't dominate.",https://stackoverflow.com/questions/48573174,machine-learning,01-02-2018 23:02,9771.0,13.0,2.0,True,11-07-2022 17:57,02-02-2018 01:55,Implementation Issues
73764895,can&#39;t run the spacy spancat (spancategorizer) model?,"i am trying to train the spancat model without luck.
i am getting:
valueerror: [e143] labels for component 'spancat' not initialized. this can be fixed by calling add_label, or by providing a representative batch of examples to the component's 'initialize' method.
i did convert my ner ents to spans:
def main(loc: path, lang: str, span_key: str):
    """"""
    set the ner data into the doc.spans, under a given key.
    the spancategorizer component uses the doc.spans, so that it can work with
    overlapping or nested annotations, which can't be represented on the
    per-token level.
    """"""
    nlp = spacy.blank(lang)
    docbin = docbin().from_disk(loc)
    docs = list(docbin.get_docs(nlp.vocab))
    for doc in docs:
        doc.spans[span_key] = list(doc.ents)
    docbin(docs=docs).to_disk(loc)

here is my config file:
[paths]
train = null
dev = null
vectors = null
init_tok2vec = null

[system]
gpu_allocator = null
seed = 444

[nlp]
lang = ""en""
pipeline = [""tok2vec"",""spancat""]
batch_size = 1000
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {""@tokenizers"":""spacy.tokenizer.v1""}

[components]

[components.spancat]
factory = ""spancat""
max_positive = null
scorer = {""@scorers"":""spacy.spancat_scorer.v1""}
spans_key = ""sc""
threshold = 0.5

[components.spancat.model]
@architectures = ""spacy.spancategorizer.v1""

[components.spancat.model.reducer]
@layers = ""spacy.mean_max_reducer.v1""
hidden_size = 128

[components.spancat.model.scorer]
@layers = ""spacy.linearlogistic.v1""
no = null
ni = null

[components.spancat.model.tok2vec]
@architectures = ""spacy.tok2veclistener.v1""
width = ${components.tok2vec.model.encode.width}
upstream = ""*""

[components.spancat.suggester]
@misc = ""spacy.ngram_suggester.v1""
sizes = [1,2,3]

[components.tok2vec]
factory = ""tok2vec""

[components.tok2vec.model]
@architectures = ""spacy.tok2vec.v2""

[components.tok2vec.model.embed]
@architectures = ""spacy.multihashembed.v2""
width = ${components.tok2vec.model.encode.width}
attrs = [""norm"",""prefix"",""suffix"",""shape""]
rows = [5000,1000,2500,2500]
include_static_vectors = true

[components.tok2vec.model.encode]
@architectures = ""spacy.maxoutwindowencoder.v2""
width = 256
depth = 8
window_size = 1
maxout_pieces = 3

[corpora]

[corpora.dev]
@readers = ""spacy.corpus.v1""
path = ${paths.dev}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[corpora.train]
@readers = ""spacy.corpus.v1""
path = ${paths.train}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[training]
dev_corpus = ""corpora.dev""
train_corpus = ""corpora.train""
max_epochs = 70
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_steps = 20000
eval_frequency = 200
frozen_components = []
annotating_components = []
before_to_disk = null

[training.batcher]
@batchers = ""spacy.batch_by_words.v1""
discard_oversize = false
tolerance = 0.2
get_length = null

[training.batcher.size]
@schedules = ""compounding.v1""
start = 100
stop = 1000
compound = 1.001
t = 0.0

[training.logger]
@loggers = ""spacy.consolelogger.v1""
progress_bar = false

[training.optimizer]
@optimizers = ""adam.v1""
beta1 = 0.9
beta2 = 0.999
l2_is_weight_decay = true
l2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001
learn_rate = 0.001

[training.score_weights]
spans_sc_f = 1.0
spans_sc_p = 0.0
spans_sc_r = 0.0

[pretraining]

[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}
vocab_data = null
lookups = null
before_init = null
after_init = null

[initialize.components]

[initialize.tokenizer]

i am using the ""sc"" key. please advise how to solve it.","['python-3.x', 'spacy', 'named-entity-recognition', 'spacy-3']",73784715,"i have solved it using the following function, but one should address the spans span(doc, start, end, label) according to the project/text for their task. it worked for me because all the text (a few words in my case) are labeled with a label and this is my need.
def convert_to_docbin(input, output_path=""./train.spacy"", lang='en'):
    """""" convert a pair of text annotations into docbin then save """"""
    # load a new spacy model:
    nlp = spacy.blank(lang)
    # create a docbin object:
    db = docbin()
    for text, annotations in input: # data in previous format
        doc = nlp(text)
        ents = []
        spans = []
        for start, end, label in annotations: # add character indexes
            spans.append(span(doc, 0, len(doc), label=label))
            span = doc.char_span(start, end, label=label)
            ents.append(span)
        doc.ents = ents # label the text with the ents
        group = spangroup(doc, name=""sc"", spans=spans)
        doc.spans[""sc""] = group
        db.add(doc)
    db.to_disk(output_path)",https://stackoverflow.com/questions/73764895,python-3.x,18-09-2022 17:16,663.0,1.0,1.0,True,20-09-2022 09:47,18-09-2022 17:50,Implementation Issues
56545363,how to find matching patterns in strings irrespective of order?,"i am trying to match patterns between two strings. for example, i have
pattern_search = ['education four year'] 
string1 = 'it is mandatory to have at least of four years of professional education'
string2 = 'need to have education four years with professional degree'

i am trying a way to say true when i try to find match between pattern_search and string1 & string2.
when i am using regex library match/search/findall doesn't help me. in string i have the all the words required but not in order, in string2 i have one extra word with added plural.
currently i am splitting the strings checking with each word in pattern_search with each word in string1 & 2 after preprocessing, is there any way to find match between the sentences?","['regex', 'python-3.x', 'string', 'nlp']",56546751,"you should take a nice look at the difflib library, specifically the get_close_matches function which returns words that are ""close enough"" to fill that requirement of words that may not exactly match. be sure to adjust your threshold (cutoff=) accordingly.
from difflib import get_close_matches
from re import sub

pattern_search = 'education four year'
string1 = 'it is mandatory to have at least of four years of professional education'
string2 = 'need to have education four years with professional degree'
string3 = 'we have four years of military experience'

def match(string, pattern):
  pattern = pattern.lower().split()
  words = set(sub(r""[^a-z0-9 ]"", """", string.lower()).split())  # sanitize input
  return all(get_close_matches(word, words, cutoff=0.8) for word in pattern)

print(match(string1, pattern_search))  # true
print(match(string2, pattern_search))  # true
print(match(string3, pattern_search))  # false

if you want to make pattern_search a list of patterns, then you should probably loop through the match function.",https://stackoverflow.com/questions/56545363,regex,11-06-2019 13:53,1147.0,3.0,3.0,True,19-05-2022 16:53,13-06-2019 13:01,Implementation Issues
77482126,"openai api error: &quot;you tried to access openai.model, but this is no longer supported in openai\&gt;=1.0.0&quot;","using visual studio code and pycharm, after install openai (pip install openai) a strange error is bugging me - please help.
if for example i write:
import openai

openai.api_key = ""sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""

lista_de_modelos = openai.model.list()
print(lista_de_modelos)

it fails and i get an this error:
ps c:\\proyectovs_python\> & ""c:/users/kitkatuser/appdata/local/programs/python/python312/python.exe"" ""c:/proyectovs_python/import os.py""
traceback (most recent call last):
file ""c:\\proyectovs_python\\import os.py"", line 5, in \<module\>
lista_de_modelos = openai.model.list()
^^^^^^^^^^^^^^^^^
file ""c:\\users\\kitkatuser\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\openai_utils_proxy.py"", line 22, in __getattr__
return getattr(self.__get_proxied__(), attr)
^^^^^^^^^^^^^^^^^^^^^^
file ""c:\\users\\kitkatuser\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\openai_utils_proxy.py"", line 43, in __get_proxied__  
return self.__load__()
^^^^^^^^^^^^^^^
file ""c:\\users\\kitkatuser\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\openai\\lib_old_api.py"", line 33, in __load__
raise apiremovedinv1(symbol=self.\_symbol)
openai.lib.\_old_api.apiremovedinv1:

you tried to access openai.model, but this is no longer supported in openai\>=1.0.0 - see the readme at  for the api.

you can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface.

alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

a detailed migration guide is available here: 

ps c:\\proyectovs_python\>

what iï¿½ï¿½m doing wrong? why i can't access openai, i've try several keys, the same program and process to install works well with other friends. using pycharm showmilar. i'm used several programs to try but always similar response! i don't find a solution or similar problems! i'm really confused!please help","['python', 'visual-studio-code', 'error-handling', 'configuration', 'openai-api']",77482219,"problem
the method name you're trying to use doesn't work with the openai python sdk version 1.0.0 or newer.
the old sdk (i.e., version 0.28) works with the following method name:
client.model.list

the new sdk (i.e., version 1.0.0 or newer) works with the following method name:
client.models.list

note: be careful because the api is case-sensitive (i.e., client.models.list will not work with the new sdk version).
solution
try this:
import os
from openai import openai
client = openai()
openai.api_key = os.getenv('openai_api_key')

client.models.list()",https://stackoverflow.com/questions/77482126,python,14-11-2023 16:06,3099.0,0.0,2.0,True,22-06-2024 00:40,22-11-2023 16:56,Tool Setup/Errors
74432598,typeerror: openai__webpack_imported_module_1__.openaiapi is not a constructor,"i'm trying to recreate an idea using openai api with react.
i'm using the official openai documentation here but got stuck and i'm hoping someone is able to help.
the idea is to have an simple text-input and a button on a page. reciving the user's prompt and sending the information to the openai api.
onclick to the button i'm handling everything within the handlesubmit function which looks like this:
const config = new configuration({
    apikey: ""api_key_is_here"",
})

const handlesubmit = async (e) => {
    e.preventdefault()
    setstate('loading...')
    
    const openai = new openaiapi(config)
    const res = await openai.createimage({
      prompt: prompt,
      n: 1,
      size: ""256ï¿½ï¿½256"",
    })

    const url = res.data.data[0].url
    console.log(url)
    console.log('clicked: ' + prompt)
}

which clicking the button and calling the function handlesubmit now i'm getting an error
uncaught typeerror: openai__webpack_imported_module_1__.openaiapi is not a constructor

besides putting api key directly into the source code instead of env variables (testing and playing around locally) nothing seems out of the ordernary to me.
thank you for any input!
edit:
imports:
import { useeffect, usestate } from ""react""
import { configuration, openaiapi } from ""openai""

versions:
""openai"": ""^3.1.0""
""react"": ""^18.2.0""","['reactjs', 'openai-api']",74432688,"it could be webpack complaining about how openai package is imported.
could you show us the lines of code that import.
also it seems from a glance at the documentation that it is supposed to be:
openaiapi and not openaiapi

case matters :d",https://stackoverflow.com/questions/74432598,reactjs,14-11-2022 13:42,3120.0,0.0,1.0,True,25-11-2023 03:53,14-11-2022 13:50,Tool Setup/Errors
78712265,problems retrieving the embeddings data form openai api batch embedding job,"i have to embed over 300,000 products description for a multi-classification project. i split the descriptions onto chunks of 34,337 descriptions to be under the batch embeddings limit size.
a sample of my jsonl file for batch processing:
{""custom_id"": ""request-0"", ""method"": ""post"", ""url"": ""/v1/embeddings"", ""body"": {""model"": ""text-embedding-ada-002"", ""input"": ""base l\u00edquida maybelline superstay 24 horas full coverage cor 220 natural beige 30ml"", ""encoding_format"": ""float""}}
{""custom_id"": ""request-1"", ""method"": ""post"", ""url"": ""/v1/embeddings"", ""body"": {""model"": ""text-embedding-ada-002"", ""input"": ""sand\u00e1lia havaianas top animals cinza/gelo 39/40"", ""encoding_format"": ""float""}}

my jsonl file has 34,337 lines.
i've susscesfully uploaded the file:
file 'batch_emb_file_1.jsonl' uploaded succesfully:
 fileobject(id='redacted for work compliance', bytes=6663946, created_at=1720128016, filename='batch_emb_file_1.jsonl', object='file', purpose='batch', status='processed', status_details=none)

and ran the embedding job:
batch job created successfully:
 batch(id='redacted for work compliance', completion_window='24h', created_at=1720129886, endpoint='/v1/embeddings', input_file_id='redacted for work compliance', object='batch', status='validating', cancelled_at=none, cancelling_at=none, completed_at=none, error_file_id=none, errors=none, expired_at=none, expires_at=1720216286, failed_at=none, finalizing_at=none, in_progress_at=none, metadata={'description': 'batch job for embedding large quantity of product descriptions', 'initiated_by': 'marcio', 'project': 'product classification', 'date': '2024-07-04 21:51', 'comments': 'this is the 1 batch job of embeddings'}, output_file_id=none, request_counts=batchrequestcounts(completed=0, failed=0, total=0))

the work was completed:
client.batches.retrieve(batch_job_1.id).status
'completed'

client.batches.retrieve('redacted for work compliance'), returns:
batch(id='redacted for work compliance', completion_window='24h', created_at=1720129886, endpoint='/v1/embeddings', input_file_id='redacted for work compliance', object='batch', status='completed', cancelled_at=none, cancelling_at=none, completed_at=1720135956, error_file_id=none, errors=none, expired_at=none, expires_at=1720216286, failed_at=none, finalizing_at=1720133521, in_progress_at=1720129903, metadata={'description': 'batch job for embedding large quantity of product descriptions', 'initiated_by': 'marcio', 'project': 'product classification', 'date': '2024-07-04 21:51', 'comments': 'this is the 1 batch job of embeddings'}, output_file_id='redacted for work compliance', request_counts=batchrequestcounts(completed=34337, failed=0, total=34337))

but when i try to get the content using output_file_id string
client.files.content(value of output_file_id), returns:
<openai._legacy_response. at 0x79ae81ec7d90>

i have tried:
client.files.content(value of output_file_id).content but this kills my kernel
what am i doing wrong? also i believe i am under utilizing batch embeddings. the 90,000 limits conflicts with batch queue limit of 'text-embedding-ada-002' model which is: 3,000,000
could someone help?","['batch-processing', 'openai-api', 'openaiembeddings']",78752532,"retrieving the embedding data from batch file is a bit trick, this tutorial breaks it down set by set link
after getting the output_file_id, you need to:
output_file =client.files.content(output_files_id).text

embedding_results = []
for line in output_file.split('\n')[:-1]:
            data =json.loads(line)
            custom_id = data.get('custom_id')
            embedding = data['response']['body']['data'][0]['embedding']
            embedding_results.append([custom_id, embedding])


embedding_results = pd.dataframe(embedding_results, columns=['custom_id', 'embedding'])

in my case, this retrieves the embedding data from the batch job file",https://stackoverflow.com/questions/78712265,batch-processing,05-07-2024 15:35,530.0,0.0,1.0,True,16-07-2024 02:57,05-07-2024 15:43,Data Wrangling
74192948,attributeerror: &#39;list&#39; object has no attribute &#39;ents&#39; in building ner using bert,"i'm trying to build a ner model using bert-base-ner for a tweets dataset and ending up getting this error . please help
this is what i have done
from transformers import autotokenizer, automodelfortokenclassification
from transformers import pipeline

tokenizer = autotokenizer.from_pretrained(""dslim/bert-base-ner"")
model = automodelfortokenclassification.from_pretrained(""dslim/bert-base-ner"")

nlp = pipeline(""ner"", model=model, tokenizer=tokenizer)

# ---------

def all_ents(v):
        return [(ent.text, ent.label_) for ent in nlp(v).ents]

df1['entities'] = df['text'].apply(lambda v: all_ents(v))

df1.head()

attributeerror: 'list' object has no attribute 'ents'

thank you for the help","['python', 'pandas', 'bert-language-model', 'named-entity-recognition']",74196722,"it seems you mix code from different modules.
.ents exists in module spacy but not in transformers
#import spacy
import en_core_web_sm

nlp = en_core_web_sm.load()

doc = nlp('hello world of python. have a nice day')

print([(x.text, x.label_) for x in doc.ents])

in transformers you should use directly nlp(v) but it gives directory with ent[""entity""], ent[""score""], ent[""index""], ent[""word""], ent[""start""], ent[""end""]
from transformers import autotokenizer, automodelfortokenclassification
from transformers import pipeline

tokenizer = autotokenizer.from_pretrained(""dslim/bert-base-ner"")
model = automodelfortokenclassification.from_pretrained(""dslim/bert-base-ner"")

nlp = pipeline(""ner"", model=model, tokenizer=tokenizer)

# ---------

import pandas as pd

df = pd.dataframe({
    'text': ['hello world of python. have a nice day']
})

# ---------

def all_ents(v):
    #print(nlp(v))
    return [(ent['word'], ent['entity']) for ent in nlp(v)]

df['entities'] = df['text'].apply(all_ents)

#df1['entities'] = df['text'].apply(lambda v: [(ent['word'], ent['entity']) for ent in nlp(v)])

print(df['entities'].head())",https://stackoverflow.com/questions/74192948,python,25-10-2022 10:53,301.0,0.0,1.0,True,25-10-2022 16:18,25-10-2022 16:18,Implementation Issues
69609401,suppress huggingface logging warning: &quot;setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.&quot;,"in huggingface, every time i call a pipeline() object, i get a warning:
`""setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.""

how do i suppress this warning without suppressing all logging warnings? i want other warnings, but i don't want this one.","['huggingface-transformers', 'huggingface-tokenizers']",71397707,"the warning comes for any text generation task done by huggingface. this is explained here, and you can see the code here.
avoid that warning by manually setting the pad_token_id (e.g., to match the tokenizer or the eos_token_id).
set the pad_token_id in the generation_config with:
model.generation_config.pad_token_id = tokenizer.pad_token_id

alternatively, if you only need to make a single call to generate:

when you call
model.generate(**encoded_input)

just change it to
model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id)",https://stackoverflow.com/questions/69609401,huggingface-transformers,17-10-2021 23:40,96819.0,60.0,5.0,True,16-12-2024 00:11,18-10-2021 08:05,Conceptual Questions
78439451,create_sql_agent with azureopenai?,"i have put together a script that works just fine using openai api. i am now trying to switch it over to azureopenai yet it seems i am running into an issue with the create_sql_agent(). can you use create_sql_agent with azureopenai model gpt-35-turbo-1106? could it be an issue with my api_version within azureopenai()? the error i receive is ""typeerror: completions. create() got an unexpected keyword argument 'tools'"" which i think could also be the option using 'openai-tools' as my agent_type?
code
import os
from langchain_openai import azureopenai
from langchain.agents import create_sql_agent
from langchain.agents.agent_toolkits import sqldatabasetoolkit
from langchain.sql_database import sqldatabase
from dotenv import load_dotenv
from langchain.agents import agentexecutor

from langchain_core.prompts.chat import (
    chatprompttemplate,
    humanmessageprompttemplate,
    systemmessageprompttemplate,
    aimessageprompttemplate,
    messagesplaceholder,
)

path = (os.getcwd()+'\creds.env')

load_dotenv(path)  

db = sqldatabase.from_uri(
    f""postgresql://{os.environ.get('user')}:{os.environ.get('password')}@{os.environ.get('host')}:{os.environ.get('port')}/{os.environ.get('database')}"")

llm = azureopenai(azure_endpoint=my_endpoint,
                  deployment_name=my_deployment_name,
                  model_name='gpt-35-turbo', # should it be 'gpt-35-turbo-1106'?
                 temperature = 0,
                 api_key = my_key,
                 api_version = '2023-07-01-preview') #my api_version correct? uncertain which one

toolkit = sqldatabasetoolkit(db=db, llm=llm)

prefix = """"""
you are an agent designed to interact with a sql database.
given an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.
unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.
you can order the results by a relevant column to return the most interesting examples in the database.
never query for all the columns from a specific table, only ask for the relevant columns given the question.
you have access to tools for interacting with the database.
only use the below tools. only use the information returned by the below tools to construct your final answer.
you must double-check your query before executing it. if you get an error while executing a query, rewrite the query and try again.

do not make any dml statements (insert, update, delete, drop, cascade, etc.) to the database.

if the question does not seem related to the database, just return ""i don't know"" as the answer.

if asked about a person do not return an 'id' but return a first name and last name.

""""""

suffix = """""" i should look at the tables in the database to see what i can query.  then i should query the schema of the most relevant tables.
""""""

messages = [
                systemmessageprompttemplate.from_template(prefix),
                humanmessageprompttemplate.from_template(""{input}""),
                aimessageprompttemplate.from_template(suffix),
                messagesplaceholder(variable_name=""agent_scratchpad""),
            ]


agent_executor = create_sql_agent(llm,
                                  toolkit=toolkit,
                                  agent_type='openai-tools', #does this work with azure?
                                  prompt=prompt,
                                  verbose=false)


print(agent_executor.invoke(""what are the names of the tables""))

error
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
cell in[69], line 1
----> 1 print(agent_executor.invoke(""what are the names of the tables""))

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain\chains\base.py:163, in chain.invoke(self, input, config, **kwargs)
    161 except baseexception as e:
    162     run_manager.on_chain_error(e)
--> 163     raise e
    164 run_manager.on_chain_end(outputs)
    166 if include_run_info:

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain\chains\base.py:153, in chain.invoke(self, input, config, **kwargs)
    150 try:
    151     self._validate_inputs(inputs)
    152     outputs = (
--> 153         self._call(inputs, run_manager=run_manager)
    154         if new_arg_supported
    155         else self._call(inputs)
    156     )
    158     final_outputs: dict[str, any] = self.prep_outputs(
    159         inputs, outputs, return_only_outputs
    160     )
    161 except baseexception as e:

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain\agents\agent.py:1432, in agentexecutor._call(self, inputs, run_manager)
   1430 # we now enter the agent loop (until it returns something).
   1431 while self._should_continue(iterations, time_elapsed):
-> 1432     next_step_output = self._take_next_step(
   1433         name_to_tool_map,
   1434         color_mapping,
   1435         inputs,
   1436         intermediate_steps,
   1437         run_manager=run_manager,
   1438     )
   1439     if isinstance(next_step_output, agentfinish):
   1440         return self._return(
   1441             next_step_output, intermediate_steps, run_manager=run_manager
   1442         )

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain\agents\agent.py:1138, in agentexecutor._take_next_step(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)
   1129 def _take_next_step(
   1130     self,
   1131     name_to_tool_map: dict[str, basetool],
   (...)
   1135     run_manager: optional[callbackmanagerforchainrun] = none,
   1136 ) -> union[agentfinish, list[tuple[agentaction, str]]]:
   1137     return self._consume_next_step(
-> 1138         [
   1139             a
   1140             for a in self._iter_next_step(
   1141                 name_to_tool_map,
   1142                 color_mapping,
   1143                 inputs,
   1144                 intermediate_steps,
   1145                 run_manager,
   1146             )
   1147         ]
   1148     )

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain\agents\agent.py:1138, in <listcomp>(.0)
   1129 def _take_next_step(
   1130     self,
   1131     name_to_tool_map: dict[str, basetool],
   (...)
   1135     run_manager: optional[callbackmanagerforchainrun] = none,
   1136 ) -> union[agentfinish, list[tuple[agentaction, str]]]:
   1137     return self._consume_next_step(
-> 1138         [
   1139             a
   1140             for a in self._iter_next_step(
   1141                 name_to_tool_map,
   1142                 color_mapping,
   1143                 inputs,
   1144                 intermediate_steps,
   1145                 run_manager,
   1146             )
   1147         ]
   1148     )

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain\agents\agent.py:1166, in agentexecutor._iter_next_step(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)
   1163     intermediate_steps = self._prepare_intermediate_steps(intermediate_steps)
   1165     # call the llm to see what to do.
-> 1166     output = self.agent.plan(
   1167         intermediate_steps,
   1168         callbacks=run_manager.get_child() if run_manager else none,
   1169         **inputs,
   1170     )
   1171 except outputparserexception as e:
   1172     if isinstance(self.handle_parsing_errors, bool):

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain\agents\agent.py:514, in runnablemultiactionagent.plan(self, intermediate_steps, callbacks, **kwargs)
    506 final_output: any = none
    507 if self.stream_runnable:
    508     # use streaming to make sure that the underlying llm is invoked in a
    509     # streaming
   (...)
    512     # because the response from the plan is not a generator, we need to
    513     # accumulate the output into final output and return that.
--> 514     for chunk in self.runnable.stream(inputs, config={""callbacks"": callbacks}):
    515         if final_output is none:
    516             final_output = chunk

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\runnables\base.py:2875, in runnablesequence.stream(self, input, config, **kwargs)
   2869 def stream(
   2870     self,
   2871     input: input,
   2872     config: optional[runnableconfig] = none,
   2873     **kwargs: optional[any],
   2874 ) -> iterator[output]:
-> 2875     yield from self.transform(iter([input]), config, **kwargs)

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\runnables\base.py:2862, in runnablesequence.transform(self, input, config, **kwargs)
   2856 def transform(
   2857     self,
   2858     input: iterator[input],
   2859     config: optional[runnableconfig] = none,
   2860     **kwargs: optional[any],
   2861 ) -> iterator[output]:
-> 2862     yield from self._transform_stream_with_config(
   2863         input,
   2864         self._transform,
   2865         patch_config(config, run_name=(config or {}).get(""run_name"") or self.name),
   2866         **kwargs,
   2867     )

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\runnables\base.py:1880, in runnable._transform_stream_with_config(self, input, transformer, config, run_type, **kwargs)
   1878 try:
   1879     while true:
-> 1880         chunk: output = context.run(next, iterator)  # type: ignore
   1881         yield chunk
   1882         if final_output_supported:

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\runnables\base.py:2826, in runnablesequence._transform(self, input, run_manager, config)
   2817 for step in steps:
   2818     final_pipeline = step.transform(
   2819         final_pipeline,
   2820         patch_config(
   (...)
   2823         ),
   2824     )
-> 2826 for output in final_pipeline:
   2827     yield output

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\runnables\base.py:1283, in runnable.transform(self, input, config, **kwargs)
   1280 final: input
   1281 got_first_val = false
-> 1283 for chunk in input:
   1284     if not got_first_val:
   1285         final = adapt_first_streaming_chunk(chunk)  # type: ignore

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\runnables\base.py:4728, in runnablebindingbase.transform(self, input, config, **kwargs)
   4722 def transform(
   4723     self,
   4724     input: iterator[input],
   4725     config: optional[runnableconfig] = none,
   4726     **kwargs: any,
   4727 ) -> iterator[output]:
-> 4728     yield from self.bound.transform(
   4729         input,
   4730         self._merge_configs(config),
   4731         **{**self.kwargs, **kwargs},
   4732     )

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\runnables\base.py:1300, in runnable.transform(self, input, config, **kwargs)
   1293             raise typeerror(
   1294                 f""failed while trying to add together ""
   1295                 f""type {type(final)} and {type(chunk)}.""
   1296                 f""these types should be addable for transform to work.""
   1297             )
   1299 if got_first_val:
-> 1300     yield from self.stream(final, config, **kwargs)

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\language_models\llms.py:458, in basellm.stream(self, input, config, stop, **kwargs)
    451 except baseexception as e:
    452     run_manager.on_llm_error(
    453         e,
    454         response=llmresult(
    455             generations=[[generation]] if generation else []
    456         ),
    457     )
--> 458     raise e
    459 else:
    460     run_manager.on_llm_end(llmresult(generations=[[generation]]))

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\language_models\llms.py:442, in basellm.stream(self, input, config, stop, **kwargs)
    440 generation: optional[generationchunk] = none
    441 try:
--> 442     for chunk in self._stream(
    443         prompt, stop=stop, run_manager=run_manager, **kwargs
    444     ):
    445         yield chunk.text
    446         if generation is none:

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_openai\llms\base.py:262, in baseopenai._stream(self, prompt, stop, run_manager, **kwargs)
    260 params = {**self._invocation_params, **kwargs, ""stream"": true}
    261 self.get_sub_prompts(params, [prompt], stop)  # this mutates params
--> 262 for stream_resp in self.client.create(prompt=prompt, **params):
    263     if not isinstance(stream_resp, dict):
    264         stream_resp = stream_resp.model_dump()

file ~\appdata\local\programs\python\python311\lib\site-packages\openai\_utils\_utils.py:277, in required_args.<locals>.inner.<locals>.wrapper(*args, **kwargs)
    275             msg = f""missing required argument: {quote(missing[0])}""
    276     raise typeerror(msg)
--> 277 return func(*args, **kwargs)

typeerror: completions.create() got an unexpected keyword argument 'tools'","['python', 'azure', 'openai-api', 'langchain', 'azure-openai']",78440581,"your model name and api version should be fine. however, you should be using the chat model type. the azurechatopenai class is used for this.
update your code:
from langchain.chat_models import azurechatopenai

# ...

llm = azurechatopenai(azure_endpoint=my_endpoint,
                  deployment_name=my_deployment_name,
                  model_name='gpt-35-turbo',
                  temperature = 0,
                  api_key = my_key,
                  api_version = '2023-07-01-preview')

when you create the sql agent, use the agenttype enumerator, and zero shot to tell the agent not to use memory.
from langchain.agents import agenttype, create_sql_agent

# ...

agent_executor = create_sql_agent(llm=llm,
                                  toolkit=toolkit,
                                  agent_type=agenttype.zero_shot_react_description,
                                  prompt=prompt,
                                  verbose=false)",https://stackoverflow.com/questions/78439451,python,06-05-2024 23:03,952.0,1.0,1.0,True,07-05-2024 06:45,07-05-2024 02:58,Implementation Issues
74914461,"spacy add_alias, typeerror","mwe
from spacy.kb import knowledgebase
import spacy 


#kb.add_entity already called.
nlp = spacy.blank(""en"")
kb = knowledgebase(vocab=nlp.vocab, entity_vector_length=96)
name = ""test""
qid = 1 # type(qid) => int
kb.add_alias(alias=name.lower(), entities=[qid], probabilities=[1])

produces the error at the last line: typeerror: an integer is required
a previous so post suggested that the same error arose in another context (importing spacy) because the version of srsly was greater than 2. using their solution of downgrading to v1.0.1 of srsly merely switched the error to module srsly has no attribute read_yaml.
i am using spacy 3.4.4 and srsly 2.4.5.
update
a fuller stack trace points to line 228 in spacy/kb.pyx:
  for entity, prob in zip(entities, probabilities):
            entity_hash = self.vocab.strings[entity] #this gives the error
            if not entity_hash in self._entry_index:
                raise valueerror(errors.e134.format(entity=entity))

            entry_index = <int64_t>self._entry_index.get(entity_hash)
            entry_indices.push_back(int(entry_index))
            probs.push_back(float(prob))","['python', 'cython', 'spacy', 'spacy-3']",74960152,"that looks like a bug. in the api docs knowledgebase.add_alias has type iterable[union[str, int]] for entities but the code above (the actual error is actually one line below) only works for str and not int values. (the marked line should have self.vocab.strings.as_int(entity).)
that said, the value 1 is probably not going to be the right value here no matter what and the simplest solution is to use strings instead like ""1"" or ""q1"", which should currently work as expected. you also need to add the entity before adding aliases (this snippet is not going to work even with a string value).",https://stackoverflow.com/questions/74914461,python,25-12-2022 16:14,133.0,0.0,1.0,True,31-12-2022 11:33,31-12-2022 11:33,Implementation Issues
77340227,add log entry on model export actions,"i have enabled log entries in the django admin
class customlogentryadmin(admin.modeladmin):
    list_display = [
        ""action_time"",
        ""user"",
        ""content_type"",
        ""object_id"",
        ""object_repr"",
        ""action_flag"",
        ""change_message"",
    ]
    # list filter
    list_filter = [""action_time"", ""user"", ""content_type""]

    # search
    search_fields = [""user__username"", ""object_repr""]


admin.site.register(logentry, customlogentryadmin)

and i have another model whose admin.py code is like this
class regadmin(exportactionmixin, admin.modeladmin):
    resource_class = regadminresource

    def has_view_permission(self, request, obj=none):
        return true

    def has_module_permission(self, request):
        return true


by default all change, addition and deletion entries are logged but i also want to log an entry when any export action is performed on it. chatgpt suggests that i should something like this
    # in admin class 
    def export_action(self, request, *args, **kwargs):
        # log the export action
        logentry.objects.create(
            user_id=request.user.id,
            content_type_id=contenttype.objects.get_for_model(self.model).id,
            object_id=none,
            object_repr=str(self.model),
            action_flag=1,  # assuming 1 stands for the action flag of 'change'
            change_message=""export action triggered."",
        )
        return super().export_action(request, *args, **kwargs)

but this function is not triggered when an export action is performed. i confirmed by adding a print statement.
how should i do this?","['python', 'django', 'import', 'export-to-csv', 'openai-api']",77413332,"you are quite close, the only problem is that the actions have reference to the one defined in the exportactionmixin, so you can override whatever you want, it is not going to refer to your method. you can however override that to to get the right action, so:
from django.utils.translation import gettext as _


class regadmin(exportactionmixin, admin.modeladmin):
    # …
    
    def get_actions(self, request):
        actions = super().get_actions(request)
        actions.update(
            export_admin_action=(
                regadmin.export_admin_action,
                'export_admin_action',
                _('export selected %(verbose_name_plural)s'),
            )
        )
        return actions
update: i've opened a pull requestï¿½ï¿½[github] to fix this behavior.<",https://stackoverflow.com/questions/77340227,python,22-10-2023 13:54,117.0,1.0,1.0,True,26-12-2023 14:30,26-12-2023 14:30,Conceptual Questions
75849546,problem tokenizing with huggingface&#39;s library when fine tuning bloom,"i have a problem with my tokenizer function. to be honest i am quiet lost, since i do not really understand whats happening inside the transformer library. here is what i wanted to do:
i would like to fine tune the bloom model to a conversation bot. now when tokenizing i dont really understand whats happening and therefore how the data is supposed to look. all examples i find online are with plain text but none of them touch the topic of conversation training with a dataset.
in huggingface's example they simply put ['text']at the end of their tokenizer function. since i dont have the feature text, but ['dialog'] i thought replacing it here would work. but apparently, it does not.
i would really appreciate if someone could say a few words of what exactly went wrong in my code and how to fix it. since i want to train varies models over the next months, explaining the mistake would help a lot in future.
here is my code and below the exact error as well as my notebook:
import torch
import random
import numpy as np
from transformers import autotokenizer, automodelforcausallm, trainer, trainingarguments
import datasets

# laden des modells und des tokenizers
model_name = ""bigscience/bloom-560m""
tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforcausallm.from_pretrained(model_name)

# laden des datasets
dataset = datasets.load_dataset('conv_ai_2')

# tokenisieren des datasets
def tokenize_function(examples):
    return tokenizer(examples[""dialog""])

tokenized_dataset = dataset.map(tokenize_function, batched=true, num_proc=4)

# aufteilen in trainings- und validierungsset
train_dataset = tokenized_dataset['train']
val_dataset = tokenized_dataset['valid']

# trainingsargumente
training_args = trainingarguments(
    output_dir='./results',
    evaluation_strategy = ""epoch"",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    logging_steps=500,
    save_steps=500,
    seed=42,
    learning_rate=5e-5,
    report_to=""none""
)

# trainer-objekt
trainer = trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# finetuning des modells
trainer.train()

# generieren einer antwort
def generate_response(input_text, model, tokenizer):
    input_ids = tokenizer.encode(input_text, return_tensors='pt')
    chat_history_ids = model.generate(
        input_ids=input_ids,
        max_length=1000,
        do_sample=true,
        top_p=0.9,
        top_k=50
    )
    return tokenizer.decode(chat_history_ids[0], skip_special_tokens=true)

# testen des conversational bots
while true:
    user_input = input(""you: "")
    response = generate_response(user_input, model, tokenizer)
    print(""bot: "" + response)


error:
---------------------------------------------------------------------------
remotetraceback                           traceback (most recent call last)
remotetraceback: 
""""""
traceback (most recent call last):
  file ""/usr/local/lib/python3.9/dist-packages/multiprocess/pool.py"", line 125, in worker
    result = (true, func(*args, **kwds))
  file ""/usr/local/lib/python3.9/dist-packages/datasets/utils/py_utils.py"", line 1349, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  file ""/usr/local/lib/python3.9/dist-packages/datasets/arrow_dataset.py"", line 3329, in _map_single
    batch = apply_function_on_filtered_inputs(
  file ""/usr/local/lib/python3.9/dist-packages/datasets/arrow_dataset.py"", line 3210, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  file ""<ipython-input-18-25d239b4d59f>"", line 17, in tokenize_function
    return tokenizer(examples[""dialog""])
  file ""/usr/local/lib/python3.9/dist-packages/datasets/formatting/formatting.py"", line 280, in __getitem__
    value = self.data[key]
keyerror: 'dialog'
""""""

the above exception was the direct cause of the following exception:

keyerror                                  traceback (most recent call last)
<ipython-input-18-25d239b4d59f> in <module>
     17     return tokenizer(examples[""dialog""])
     18 
---> 19 tokenized_dataset = dataset.map(tokenize_function, batched=true, num_proc=4)
     20 
     21 # aufteilen in trainings- und validierungsset

13 frames
/usr/local/lib/python3.9/dist-packages/datasets/formatting/formatting.py in __getitem__()
    278 
    279     def __getitem__(self, key):
--> 280         value = self.data[key]
    281         if key in self.keys_to_format:
    282             value = self.format(key)

keyerror: 'dialog'","['python', 'nlp', 'artificial-intelligence', 'huggingface-transformers']",75855859,"in the original tokenize_function, you were directly tokenizing the ""dialog"" key from the examples. however, this didn't ensure that the dimensions of the input and label tensors were consistent. this mismatch in dimensions was causing the error you encountered during training i converted each dialog entry into a single string by joining the ""text"" key values in each dialog. then i tokenize the dialog strings with proper truncation, padding, and a specified maximum length. this creates tokenized input tensors with consistent dimensions. then i shift the input_ids by one position. this means that the model will learn to predict the next token in the sequence. i also clone the shifted input_ids to avoid modifying the original tensor in place.
def tokenize_function(examples):
    dialog_texts = [' '.join([entry[""text""] for entry in dialog]) for dialog in examples[""dialog""]]
    tokenized = tokenizer(dialog_texts, truncation=true, padding='max_length', max_length=128, return_tensors=""pt"")  
    tokenized[""labels""] = tokenized.input_ids[:, 1:].clone()
    tokenized.input_ids = tokenized.input_ids[:, :-1]
    tokenized[""labels""] = torch.cat([tokenized.labels, torch.full((tokenized.labels.size(0), 1), tokenizer.pad_token_id, dtype=torch.long)], dim=1)

    return tokenized",https://stackoverflow.com/questions/75849546,python,26-03-2023 17:54,987.0,2.0,1.0,True,28-03-2023 13:59,27-03-2023 12:25,Implementation Issues
77560044,spacy displacy output using anvil.works server,"i am attempting to display entities using spacy's displacy feature.
the output of my render is being shown in my jupyter notebook code cell with my anvil.server.wait_forever() code.
here is an example of the code cell output i'm getting.
i would rather have the output appear here.
i have already tried using displacy.server instead of displacy.render and tried auto_port.
here is my code:
def visualize_entities_in_sentences(self, doc_id):
    """"""visualize entities in the sentences of a document.

    :param doc_id: the id of the document to visualize
    :type doc_id: str
    """"""
    doc = self.get_document(doc_id)
    sentences = list(doc.sents)
    labels = displacy.render(sentences, style=""ent"", page=false, minify=true)
    return labels

and my callable for the anvil implementation:
@anvil.server.callable
def get_visualize_entities_in_sentences(doc_id):
    """"""""get the document markdown for a document in my_corpus with entity labels visualized.

    :param doc_id: a document id
    :type doc_id: str
    :returns: markdown
    :rtype: str
    """"""    
    return my_corpus.visualize_entities_in_sentences(doc_id)","['python', 'jupyter-notebook', 'nlp', 'spacy', 'displacy']",77562648,try adding jupyter=false to displacy.render to skip the jupyter auto-detection.,https://stackoverflow.com/questions/77560044,python,27-11-2023 21:29,90.0,0.0,2.0,True,29-11-2023 01:52,28-11-2023 13:25,Uncategorized
70456489,convert list to string with conditions,"i have a dataframe that looks like:
x <- tibble(
  experiment_id = rep(c('1a','1b'),each=5),
  keystroke = rep(c('a','shift','b','space','e'),2)
)

i know i can concatenate a list into a string using str_c or str_flatten and only keep certain values like below:
> y <- c('b','a','space','d')
> y[y %in% letters]
[1] ""b"" ""a"" ""d""

but when i try the same thing in a grouped pipe:
x_out <- x %>%
  group_by(experiment_id) %>%
  mutate(
    grp = cumsum(lag(keystroke=='space',default=0))) %>% 
    group_by(grp, .add=true) %>%
      mutate(within_keystrokes = list(keystroke),
             within_word = within_keystrokes[within_keystrokes %in% letters]
             ) %>% 
  ungroup()

i get the error:
error: problem with `mutate()` input `within_word`.
x input `within_word` can't be recycled to size 2.
ï¿½ï¿½ï¿½ input `within is `within_keystrokes[within_keystrokes %in% letters]`.
ï¿½ï¿½ï¿½ input `within_word` must be size 2 or 1, not 0.
ï¿½ï¿½ï¿½ the error occurred in group 1: experiment_id = ""1a"", grp = 0.

i r"" answer and tried using ifelse but still ran into errors.
any insight into what i'm doing wrong?
edit: expected output sorry for not including this. i would expect the final df to look like:
    x <- tibble(
      experiment_id = rep(c('1a','1b'),each=5),
      keystroke = rep(c('a','shift','b','space','e'),2),
      within_keystrokes = list(list('a','shift','b','space'), 
                          list('a','shift','b','space'), 
                          list('a','shift','b','space'), 
                          list('a','shift','b','space'),
                          'e',
                          list('a','shift','b','space'), 
                          list('a','shift','b','space'), 
                          list('a','shift','b','space'), 
                          list('a','shift','b','space'),
                          'e'),
      within_word = rep(list('ab','ab','ab','ab','e'),2)
)","['r', 'nlp', 'tidyverse']",70456568,"you almost solved your issue. you could use
library(dplyr)
library(stringr)

x %>%
  group_by(experiment_id, grp = cumsum(lag(keystroke == ""space"", default = 0))) %>% 
  mutate(
    within_keystrokes = list(keystroke),
    within_word = list(str_c(keystroke[keystroke %in% letters], collapse = """"))
    )

to get
# a tibble: 10 x 4
   experiment_id keystroke within_keystrokes within_word
   <chr>         <chr>     <list>            <list>     
 1 1a            a         <list [4]>        <chr [1]>  
 2 1a            shift     <list [4]>        <chr [1]>  
 3 1a            b         <list [4]>        <chr [1]>  
 4 1a            space     <list [4]>        <chr [1]>  
 5 1a            e         <chr [1]>         <chr [1]>  
 6 1b            a         <list [4]>        <chr [1]>  
 7 1b            shift     <list [4]>        <chr [1]>  
 8 1b            b         <list [4]>        <chr [1]>  
 9 1b            space     <list [4]>        <chr [1]>  
10 1b            e         <chr [1]>         <chr [1]> 

if you don't want within_word to be a list, just remove the list() function.",https://stackoverflow.com/questions/70456489,r,23-12-2021 00:28,58.0,0.0,1.0,True,23-12-2021 19:50,23-12-2021 02:24,Implementation Issues
62785916,spacy replace token,"i am trying to replace a word without destroying the space structure in the sentence. suppose i have the sentence text = ""hi this is my dog."". and i wish to replace dog with simba. following the answer from  i did:
import spacy
nlp = spacy.load(""en_core_web_lg"")
from spacy.tokens import doc

doc1 = nlp(""hi this is my dog."")
new_words = [token.text if token.text!=""dog"" else ""simba"" for token in doc1]
doc(doc1.vocab, words=new_words)
# hi this is my simba . 

notice how there was an extra space at the end before the full stop (it ought to be hi this is my simba.). is there a way to remove this behaviour. happy for a general python string processing answer too.","['python', 'spacy']",62787267,"thanks to @lora-johns i found this answer. so without going down the matcher route, i think this might be a simpler answer:
new_words = [(token.idx, len(""dog"")) for token in doc1 if token.text.lower()==""dog""]
# reverse order of replacement words from end to start
new_words = sorted(new_words, key=lambda x:-x[0])
for i, l in new_words: 
    text = text[:i] +  ""simba"" + text[i+l:]",https://stackoverflow.com/questions/62785916,python,08-07-2020 00:47,8803.0,4.0,9.0,True,17-05-2022 14:43,08-07-2020 03:27,Uncategorized
75744401,can someone explain how to create a ptb dataset and/or train my own model using stanfordnlp?,"i'm learning about sentiment analysis and i can't seem to find anything online that outlines how to create a ptb dataset. i'm using stanfordnlp with java. i've downloaded the test, dev and validate data that they used and i can't get my head around how these have been outlined:
test.txt:
(3 (2 (2 the) (2 rock)) (4 (3 (2 is) (4 (2 destined) (2 (2 (2 (2 (2 to) (2 (2 be) (2 (2 the) (2 (2 21st) (2 (2 (2 century) (2 's)) (2 (3 new) (2 (2 ``) (2 conan)))))))) (2 '')) (2 and)) (3 (2 that) (3 (2 he) (3 (2 's) (3 (2 going) (3 (2 to) (4 (3 (2 make) (3 (3 (2 a) (3 splash)) (2 (2 even) (3 greater)))) (2 (2 than) (2 (2 (2 (2 (1 (2 arnold) (2 schwarzenegger)) (2 ,)) (2 (2 jean-claud) (2 (2 van) (2 damme)))) (2 or)) (2 (2 steven) (2 segal))))))))))))) (2 .)))

i figure that numbers are aligned to sentiment value but i'm still not sure how it works.
tldr; i'm trying to develop my own model for news analysis and have seen that the stanfordnlp model has been trained on movie reviews which is leading to poor sentiment analysis so, i thought to attempt to develop my own but i can't find anything online that teaches what each element is or how to even do this.
at best; outlined on this page: 
is the dataset available and the code to train.
models can be retrained using the following command using the ptb format dataset:

java -mx8g edu.stanford.nlp.sentiment.sentimenttraining -numhid 25 -trainpath train.txt -devpath dev.txt -train -model model.ser.gz

i have the data that i need to parse ready.","['java', 'machine-learning', 'stanford-nlp']",75745771,"okay.. so i've done some more digging and have started to finally understand (some what) as how to create a dataset tree and will try to break it down for anyone who stumbles upon this post with the same troubles as i've been having.
step 1.

find your data. (in my case it's news articles about the uk housing
market)

uk renters: are you living with someone youï¿½ï¿½ï¿½ve fallen out with?
uk property asking prices stagnating, lifting hopes of softer landing for housing market

step 2.

annotate your data

2 uk renters: are you living with someone youï¿½ï¿½ï¿½ve fallen out with?
1 fallen out with
1 fallen out
2 uk renters
2 living with someone
3 fallen
2 :
2 ?
2 living with
2 someone

3 uk property asking prices stagnating, lifting hopes of softer landing for housing market
2 uk property
3 asking prices stagnating
2 asking prices
4 lifting hopes
2 hopes
4 lifting hopes of softer landing
3 softer landing for housing mark market
2 lifting
2 landing
2 , 

annotation meanings
very positive= 4
positive = 3
neutral = 2
negative = 1
very negative = 0

structure
2 uk renters: are you living with someone youï¿½ï¿½ï¿½ve fallen out with?
   //overall sentiment

1 fallen out with
   // negative

1 fallen out
   // negative

2 uk renters
   // neutral

...etc..


save the annotated data to a .txt (sample.txt)

step 3:

locate your stanford-corenlp-4.5.2.jar

example  ~/.m2/repository/edu/stanford/nlp/stanford-corenlp/4.5.2



step 4:

open bash and run

java -cp ""*"" -mx5g edu.stanford.nlp.sentiment.buildbinarizeddataset -input /c/users/rusku/desktop/stanfordnpl/russample/sample.txt
replace the above data location



step 5:

result

(2 (2 (2 (2 uk) (2 renters)) (2 :)) (2 (2 (2 (2 are) (2 you)) (2 (2 living) (2 (2 with) (2 (2 someone) (2 (2 you) (2 (2 ï¿½ï¿½ï¿½ve) (1 (1 (3 fallen) (2 out)) (2 with)))))))) (2 ?)))
(3 (3 (2 (3 uk) (3 property)) (2 (3 asking) (3 prices))) (3 (3 (3 stagnating) (3 (2 ,) (4 (2 lifting) (2 hopes)))) (3 (3 of) (3 (3 (3 softer) (2 landing)) (3 (3 for) (2 (3 housing) (3 market)))))))

resource: train stanford corenlp about the sentiment of domain-specific phrases
this is as far as i've currently gotten.
hope this helps.",https://stackoverflow.com/questions/75744401,java,15-03-2023 11:58,165.0,0.0,1.0,True,15-03-2023 17:45,15-03-2023 15:10,Implementation Issues
79118378,how to save and load spacy encodings in a polars dataframe,"i want to use spacy to generate embeddings of text stored in a polars dataframe and store the results in the same dataframe. next, i want to save this dataframe to the disk and be able to load again as a polars dataframe. the backtransformation from pandas to polars results in an error.
this is the error message:
arrowinvalid: could not convert hello with type spacy.tokens.doc.doc: did not recognize python value type when inferring an arrow data type
here is my code:
from io import stringio
import polars as pl
import pandas as pd
import spacy


nlp = spacy.load(""de_core_news_sm"")
json_str = '[{""foo"":""hello"",""bar"":6},{""foo"":""what a lovely day"",""bar"":7},{""foo"":""nice to meet you"",""bar"":8}]'


#initalize and store dataframe
df = pl.read_json(stringio(json_str))
df = df.with_columns(pl.col(""foo"").map_elements(lambda x: nlp(x)).alias(""encoding""))
df.to_pandas().to_pickle('pickled_df.pkl')

#load dataframe
df_loaded_pd = pd.read_pickle('pickled_df.pkl')
df_loaded_pl = pl.from_pandas(df_loaded_pd)


these are the package versions i used:
# name                    version                   build  channel
pandas                    2.2.3           py312hf9745cd_1    conda-forge
polars                    1.9.0           py312hfe7c9be_0    conda-forge
spacy                     3.7.2           py312h6db74b5_0  
spacy-curated-transformers 0.2.2                    pypi_0    pypi
spacy-legacy              3.0.12             pyhd8ed1ab_0    conda-forge
spacy-loggers             1.0.5              pyhd8ed1ab_0    conda-forge

thank you for your help!","['python', 'dataframe', 'spacy', 'python-polars']",79124658,"serializing and deserializing
spacy objects within a polars dataframe can be stored by using spacys native docbin class. the following code generates doc objects, saves them locally, and successfully loads them afterwards.
from io import stringio
from spacy.tokens import docbin
import polars as pl
import spacy

nlp = spacy.load(""de_core_news_md"")
json_str = '[{""foo"":""hello"",""bar"":6},{""foo"":""what a lovely day"",""bar"":7},{""foo"":""nice to meet you"",""bar"":8}]'
doc = nlp(""some text"")

#serialize polars dataframe
df = pl.read_json(stringio(json_str))
df = df.with_columns(pl.col(""foo"").map_elements(lambda x: docbin(docs=[nlp(x)]).to_bytes()).alias('binary_embbeding'))
df.write_parquet('saved.pq')

#deserialize polars dataframe
df_loaded = pl.read_parquet('saved.pq')
df_loaded = df_loaded.with_columns(pl.col('binary_embbeding').map_elements(lambda x: list(docbin().from_bytes(x).get_docs(nlp.vocab))[0]).alias(""spacy_embedding""))

#calculate similarity
df_loaded.with_columns(pl.col(""spacy_embedding"").map_elements(lambda x: doc.similarity(x), return_dtype=pl.float64).alias('score'))


applying functions to deserialized spacy objects
serializing and deserializing spacys objects with native polars functions (such as df.write_parquet()) heavily depends on the used model. in the above case the similarity calculation only works when utilizing spacys language model that contain wordvectors.
nlp = spacy.load(""de_core_news_sm"") # line 20 does not works
nlp = spacy.load(""de_core_news_md"") # line 20 works
nlp = spacy.load(""de_core_news_lg"") # line 20 works
nlp = spacy.load(""de_dep_news_trf"") # line 20 does not works",https://stackoverflow.com/questions/79118378,python,23-10-2024 14:31,174.0,4.0,1.0,True,25-10-2024 07:26,23-10-2024 14:50,Implementation Issues
36610179,how to get the dependency tree with spacy?,"i have been trying to find how to get the dependency tree with spacy but i can't find anything on how to get the tree, only on how to navigate the tree.","['python', 'spacy']",36612605,"it turns out, the tree is available through the tokens in a document.
would you want to find the root of the tree, you can just go though the document:
def find_root(docu):
    for token in docu:
        if token.head is token:
            return token

to then navigate the tree, the tokens have api to get through the children",https://stackoverflow.com/questions/36610179,python,13-04-2016 21:47,57621.0,73.0,9.0,True,23-12-2022 15:27,14-05-2018 15:16,Implementation Issues
77011579,error &quot;&#39;tuple&#39; object has no attribute &#39;page_content&#39;&quot; when using weaviate.add_documents,"i have the following piece of code:
if file.filename.lower().endswith('.pdf'):
                        pdf = ep.pdfload(file_path)  # this is the loader from langchain
                        doc = pdf.load()
                        archivo = crear_archivo(doc, file)

inside crear_archivo function i am splitting the document and sending it to the weaviate.add_documents:
   cliente = db.newvect()  # this one creates the weaviate.client
    text_splitter = charactertextsplitter(chunk_size=1000, chunk_overlap=0)
    docs = text_splitter.split_documents(document)

    embeddings = openaiembeddings()
    return weaviate.add_documents(docs, embeddings, client=client, weaviate_url=envvect.host, by_text=false,      index_name=""langchain"") 
# using this instead of from_documents since i don't want to initialize a new vectorstore         

# some more logic to save the doc to another database

whenever i try to run the code it breaks during the weaviate.add_documents() function prompting the following error:
'tuple' object has no attribute 'page_content'.
i tried to check the type of docs, but that doesn't seem wrong since it returns a list[document] which is the same type the function accepts.
how can i make it work? i kind of followed this approach but the difference is i am loading files such as pdf, txt etc.","['python', 'langchain', 'py-langchain', 'weaviate']",77031161,"(tambiï¿½ï¿½n soy nuevo acï¿½ï¿½)
the error you're getting is likely because docs isn't a document object.
afaik, a ""document"" in langchain is a list of document objects. if you run type(docs[0]) you should get langchain.schema.document.document. this document object is a dictionary with two keys: one is page_content: which accepts string values, and the second key is metadata: which only accepts dictionaries. {page_content: str, metadata: dict}. not very well explained in langchain's documentation.
my suggestions to tackle your problem:

make sure that the document you're splitting here docs = text_splitter.split_documents(document), is effectively a langchain document object. use print(document), and you should see this in the first line: [document(page_content='your text etc... and at the end of the output, you ee ...end of your text', metadata={'...
if document isn't a langchain document, you'll need to check how you created it.
if document is a langchain document, try weaviate.from_documents() instead.

hope this helps, y un abrazo!",https://stackoverflow.com/questions/77011579,python,30-08-2023 21:20,5320.0,2.0,1.0,True,03-09-2023 06:31,30-08-2023 22:13,Conceptual Questions
71223907,how to change allennlp bert based semantic role labeling to roberta in allennlp,"currently i'm able to train a semantic role labeling model using the config file below. this config file is based on the one provided by allennlp and works for the default bert-base-uncased model and also gronlp/bert-base-dutch-cased.
{
  ""dataset_reader"": {
    ""type"": ""srl_custom"",
    ""bert_model_name"": ""gronlp/bert-base-dutch-cased""
  },
  ""data_loader"": {
    ""batch_sampler"": {
      ""type"": ""bucket"",
      ""batch_size"": 32
    }
  },
  ""train_data_path"": ""./data/srl/sonar_1_srl/manual500/"",
  ""validation_data_path"": ""./data/srl/sonar_1_srl/manual500/"",
  ""model"": {
    ""type"": ""srl_bert"",
    ""embedding_dropout"": 0.1,
    ""bert_model"": ""gronlp/bert-base-dutch-cased""
  },
  ""trainer"": {
    ""optimizer"": {
      ""type"": ""huggingface_adamw"",
      ""lr"": 5e-5,
      ""correct_bias"": false,
      ""weight_decay"": 0.01,
      ""parameter_groups"": [
        [
          [
            ""bias"",
            ""layernorm.bias"",
            ""layernorm.weight"",
            ""layer_norm.weight""
          ],
          {
            ""weight_decay"": 0.0
          }
        ]
      ]
    },
    ""learning_rate_scheduler"": {
      ""type"": ""slanted_triangular""
    },
    ""checkpointer"": {
      ""keep_most_recent_by_count"": 2
    },
    ""grad_norm"": 1.0,
    ""num_epochs"": 3,
    ""validation_metric"": ""+f1-measure-overall""
  }
}

swapping the values of bert_model_name and bert_model parameters from gronlp/bert-base-dutch-cased to roberta-base won't work out of the box since the srl datareader only supports the berttokenizer and not the robertatokenizer. so i changed the config file to the following:
{
  ""dataset_reader"": {
    ""type"": ""srl_custom"",
    ""token_indexers"": {
      ""tokens"": {
        ""type"": ""pretrained_transformer"",
        ""model_name"": ""roberta-base""
      }
    }
  },
  ""data_loader"": {
    ""batch_sampler"": {
      ""type"": ""bucket"",
      ""batch_size"": 32
    }
  },
  ""train_data_path"": ""./data/srl/sonar_1_srl/manual500/"",
  ""validation_data_path"": ""./data/srl/sonar_1_srl/manual500/"",
  ""model"": {
    ""type"": ""srl_bert"",
    ""embedding_dropout"": 0.1,
    ""bert_model"": ""roberta-base""
  },
  ""trainer"": {
    ""optimizer"": {
      ""type"": ""huggingface_adamw"",
      ""lr"": 5e-5,
      ""correct_bias"": false,
      ""weight_decay"": 0.01,
      ""parameter_groups"": [
        [
          [
            ""bias"",
            ""layernorm.bias"",
            ""layernorm.weight"",
            ""layer_norm.weight""
          ],
          {
            ""weight_decay"": 0.0
          }
        ]
      ]
    },
    ""learning_rate_scheduler"": {
      ""type"": ""slanted_triangular""
    },
    ""checkpointer"": {
      ""keep_most_recent_by_count"": 2
    },
    ""grad_norm"": 1.0,
    ""num_epochs"": 15,
    ""validation_metric"": ""+f1-measure-overall""
  }
}

however, this is still not working. i'm receiving the following error:
2022-02-22 16:19:34,122 - info - allennlp.training.gradient_descent_trainer - training
  0%|          | 0/1546 [00:00<?, ?it/s]2022-02-22 16:19:34,142 - info - allennlp.data.samplers.bucket_batch_sampler - no sorting keys given; trying to guess a good one
2022-02-22 16:19:34,142 - info - allennlp.data.samplers.bucket_batch_sampler - using ['tokens'] as the sorting keys
  0%|          | 0/1546 [00:00<?, ?it/s]
2022-02-22 16:19:34,526 - critical - root - uncaught exception
traceback (most recent call last):
  file ""c:\program files\python39\lib\runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, none,
  file ""c:\program files\python39\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  file ""c:\users\denbe\appdata\roaming\python\python39\scripts\allennlp.exe\__main__.py"", line 7, in <module>
    sys.exit(run())
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\__main__.py"", line 39, in run
    main(prog=""allennlp"")
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\commands\__init__.py"", line 119, in main
    args.func(args)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\commands\train.py"", line 111, in train_model_from_args
    train_model_from_file(
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\commands\train.py"", line 177, in train_model_from_file
    return train_model(
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\commands\train.py"", line 258, in train_model
    model = _train_worker(
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\commands\train.py"", line 508, in _train_worker
    metrics = train_loop.run()
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\commands\train.py"", line 581, in run
    return self.trainer.train()
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\training\gradient_descent_trainer.py"", line 771, in train
    metrics, epoch = self._try_train()
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\training\gradient_descent_trainer.py"", line 793, in _try_train
    train_metrics = self._train_epoch(epoch)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\training\gradient_descent_trainer.py"", line 510, in _train_epoch
    batch_outputs = self.batch_outputs(batch, for_training=true)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\training\gradient_descent_trainer.py"", line 403, in batch_outputs
    output_dict = self._pytorch_model(**batch)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\torch\nn\modules\module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp_models\structured_prediction\models\srl_bert.py"", line 141, in forward
    bert_embeddings, _ = self.bert_model(
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\torch\nn\modules\module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\transformers\models\bert\modeling_bert.py"", line 989, in forward
    embedding_output = self.embeddings(
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\torch\nn\modules\module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\transformers\models\bert\modeling_bert.py"", line 215, in forward
    token_type_embeddings = self.token_type_embeddings(token_type_ids)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\torch\nn\modules\module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\torch\nn\modules\sparse.py"", line 156, in forward
    return f.embedding(
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\torch\nn\functional.py"", line 1916, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
indexerror: index out of range in self

i don't fully understand whats going wrong and couldn't find any documentation on how to change the config file to load in a 'custom' bert/roberta model (one thats not mentioned here). i'm running the default allennlp train config.jsonnet command to start training. allennlp train config.jsonnet --dry-run produces no errors however.
thanks in advance!
thijs
edit:
i've now swapped out and inherited the ""srl_bert"" for a custom ""srl_roberta"" class to make use of the robertamodel. this however still produces the same error.
edit2: i'm now using the autotokenizer as suggested by dirk groeneveld. it looks like changing the srlreader class to support roberta based models involves way more changes like swapping berts wordpiece tokenizer to roberta's bpe tokenizer. is there an easy way to adapt the srlreader class or is it better to write a new robertasrlreader from scratch?
i've inherited the srlreader class and changed this line to the following:
self.bert_tokenizer = autotokenizer.from_pretrained(bert_model_name)

it produces the following error since roberta tokenization differs from bert:
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp_models\structured_prediction\dataset_readers\srl.py"", line 255, in text_to_instance
    wordpieces, offsets, start_offsets = self._wordpiece_tokenize_input(
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp_models\structured_prediction\dataset_readers\srl.py"", line 196, in _wordpiece_tokenize_input
    word_pieces = self.bert_tokenizer.wordpiece_tokenizer.tokenize(token)
attributeerror: 'robertatokenizerfast' object has no attribute 'wordpiece_tokenizer'","['bert-language-model', 'allennlp', 'roberta-language-model', 'srl']",71246269,"the easiest way to resolve this is to patch srlreader so that it uses pretrainedtransformertokenizer (from allennlp) or autotokenizer (from huggingface) instead of berttokenizer. srlreader is an old class, and was written against an old version of the huggingface tokenizer api, so it's not so easy to upgrade.
if you want to submit a pull request in the allennlp project, i'd be happy to help you get it merged into allennlp!",https://stackoverflow.com/questions/71223907,bert-language-model,22-02-2022 15:24,447.0,2.0,1.0,True,24-02-2022 12:34,24-02-2022 12:34,Implementation Issues
75158430,error &#39;img&#39; when applying increment with keras and transformers for image classification,"i would like to apply vit for image classification. but i have one problem and i don't know as resolve it. my error is this ""keyerror: 'img'"". the error is shown when i apply the last comand, and i don't know where is my error. the image within dataset are in .png, but i don't think that this was mistake.
below there is the script:
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_addons as tfa
import os
import cv2
import matplotlib.pyplot as plt
from transformers import vitfeatureextractor, vitforimageclassification
from transformers import trainingarguments, trainer
from tensorflow import keras 
from tensorflow.keras import layers
from datasets import load_metric
from pil import image as img
from ipython.display import image, display
from datasets import load_dataset 
import torch
dataset = load_dataset(""imagefolder"", data_dir=""datasets"")

dataset
example = dataset[""train""][10]
example
dataset[""train""].features
example['image']
example['image'].resize((200, 200))
example['label']
dataset[""train""].features[""label""]


img_class_labels = dataset[""train""].features[""label""].names

from transformers import vitfeatureextractor
from tensorflow import keras
from tensorflow.keras import layers


model_id = ""google/vit-base-patch16-224-in21k""
feature_extractor = vitfeatureextractor.from_pretrained(model_id)

# learn more about data augmentation here: 
data_augmentation = keras.sequential(
    [
        layers.resizing(feature_extractor.size, feature_extractor.size),
        layers.rescaling(1./255),
        layers.randomflip(""horizontal""),
        layers.randomrotation(factor=0.02),
        layers.randomzoom(
            height_factor=0.2, width_factor=0.2
        ),
    ],
    name=""data_augmentation"",
)
# use keras image data augementation processing
def augmentation(examples):
    # print(examples[""img""])
    examples[""pixel_values""] = [data_augmentation(image) for image in examples[""img""]]
    return examples


# basic processing (only resizing)
def process(examples):
    examples.update(feature_extractor(examples['img'], ))
    return examples

# we are also renaming our label col to labels to use `.to_tf_dataset` later
dataset_ds = dataset[""train""].rename_column(""label"", ""labels"")

processed_dataset = dataset_ds.map(augmentation, batched=true)
processed_dataset","['python', 'image', 'keras', 'classification', 'huggingface-transformers']",75158976,"i guess the error is here:
def augmentation(examples):
    # print(examples[""img""])
    examples[""pixel_values""] = [data_augmentation(image) for image in examples[""img""]]
    return examples

you are trying to access 'examples' dictionary using 'img' key. from some code above it looks like the key should be 'image':
    examples[""pixel_values""] = [data_augmentation(image) for image in examples[""image""]]",https://stackoverflow.com/questions/75158430,python,18-01-2023 11:18,109.0,1.0,1.0,True,18-01-2023 15:57,18-01-2023 15:57,Implementation Issues
65083581,how to compute mean/max of huggingface transformers bert token embeddings with attention mask?,"i'm using the huggingface transformers bert model, and i want to compute a summary vector (a.k.a. embedding) over the tokens in a sentence, using either the mean or max function. the complication is that some tokens are [pad], so i want to ignore the vectors for those tokens when computing the average or max.
here's an example. i initially instantiate a berttokenizer and a bertmodel:
import torch
import transformers
from transformers import autotokenizer, automodel

transformer_name = 'bert-base-uncased'

tokenizer = autotokenizer.from_pretrained(transformer_name, use_fast=true)

model = automodel.from_pretrained(transformer_name)

i then input some sentences into the tokenizer and get out input_ids and attention_mask. notably, an attention_mask value of 0 means that the token was a [pad] that i can ignore.
sentences = ['deep learning is difficult yet very rewarding.',
             'deep learning is not easy.',
             'but is rewarding if done right.']
tokenizer_result = tokenizer(sentences, max_length=32, padding=true, return_attention_mask=true, return_tensors='pt')

input_ids = tokenizer_result.input_ids
attention_mask = tokenizer_result.attention_mask

print(input_ids.shape) # torch.size([3, 11])

print(input_ids)
# tensor([[  101,  2784,  4083,  2003,  3697,  2664,  2200, 10377,  2075,  1012,  102],
#         [  101,  2784,  4083,  2003,  2025,  3733,  1012,   102,     0,     0,    0],
#         [  101,  2021,  2003, 10377,  2075,  2065,  2589,  2157,  1012,   102,   0]])

print(attention_mask.shape) # torch.size([3, 11])

print(attention_mask)
# tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
#         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],
#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])

now, i call the bert model to get the 768-d token embeddings (the top-layer hidden states).
model_result = model(input_ids, attention_mask=attention_mask, return_dict=true)

token_embeddings = model_result.last_hidden_state
print(token_embeddings.shape) # torch.size([3, 11, 768])

so at this point, i have:

token embeddings in a [3, 11, 768] matrix: 3 sentences, 11 tokens, 768-d vector for each token.
attention mask in a [3, 11] matrix: 3 sentences, 11 tokens. a 1 value indicates non-[pad].

how do i compute the mean / max over the vectors for the valid, non-[pad] tokens?
i tried using the attention mask as a mask and then called torch.max(), but i don't get the right dimensions:
masked_token_embeddings = token_embeddings[attention_mask==1]
print(masked_token_embeddings.shape) # torch.size([29, 768] <-- wrong. should be [3, 11, 768]

pooled = torch.max(masked_token_embeddings, 1)
print(pooled.values.shape) # torch.size([29]) <-- wrong. should be [3, 768]

what i really want is a tensor of shape [3, 768]. that is, a 768-d vector for each of the 3 sentences.","['machine-learning', 'pytorch', 'bert-language-model', 'huggingface-transformers']",65084300,"for max, you can multiply with attention_mask:
pooled = torch.max((token_embeddings * attention_mask.unsqueeze(-1)), axis=1)

for mean, you can sum along the axis and divide by attention_mask along that axis:
mean_pooled = token_embeddings.sum(axis=1) / attention_mask.sum(axis=-1).unsqueeze(-1)",https://stackoverflow.com/questions/65083581,machine-learning,01-12-2020 01:38,6251.0,8.0,3.0,True,07-09-2022 17:39,01-12-2020 01:49,Implementation Issues
77433096,notimplementederror: loading a dataset cached in a localfilesystem is not supported,"i try to load a dataset using the datasets python module in my local python notebook. i am running a python 3.10.13 kernel as i do for my virtual environment.
i cannot load the datasets i am following from a tutorial. here's the error:
---------------------------------------------------------------------------
notimplementederror                       traceback (most recent call last)
/users/ari/downloads/00-fine-tuning.ipynb celda 2 line 3
      1 from datasets import load_dataset
----> 3 data = load_dataset(
      4     ""jamescalam/agent-conversations-retrieval-tool"",
      5     split=""train""
      6 )
      7 data

file ~/documents/fastapi_language_tutor/env/lib/python3.10/site-packages/datasets/load.py:2149, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)
   2145 # build dataset for splits
   2146 keep_in_memory = (
   2147     keep_in_memory if keep_in_memory is not none else is_small_dataset(builder_instance.info.dataset_size)
   2148 )
-> 2149 ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory)
   2150 # rename and cast features to match task schema
   2151 if task is not none:
   2152     # to avoid issuing the same warning twice

file ~/documents/fastapi_language_tutor/env/lib/python3.10/site-packages/datasets/builder.py:1173, in datasetbuilder.as_dataset(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)
   1171 is_local = not is_remote_filesystem(self._fs)
   1172 if not is_local:
-> 1173     raise notimplementederror(f""loading a dataset cached in a {type(self._fs).__name__} is not supported."")
   1174 if not os.path.exists(self._output_dir):
   1175     raise filenotfounderror(
   1176         f""dataset {self.dataset_name}: could not find data in {self._output_dir}. please make sure to call ""
   1177         ""builder.download_and_prepare(), or use ""
   1178         ""datasets.load_dataset() before trying to access the dataset object.""
   1179     )

notimplementederror: loading a dataset cached in a localfilesystem is not supported.

how do i resolve this? i don't understand how this error is applicable, given that the dataset is something i am fetching and thus cannot be cached in my localfilesystem in the first place.","['python', 'python-3.x', 'pip', 'openai-api', 'huggingface-datasets']",77433141,"try doing:
pip install -u datasets


this error stems from a breaking change in fsspec. it has been fixed in the latest datasets release (2.14.6). updating the installation with pip install -u datasets should fix the issue.

git link : 

if you are using fsspec, then do:
pip install fsspec==2023.9.2

there is a problem with fsspec==2023.10.0
git link : 


edit: looks like it broken again in 2.17 and 2.18 downgrading to 2.16 should work.",https://stackoverflow.com/questions/77433096,python,06-11-2023 17:29,34028.0,24.0,2.0,True,09-05-2024 20:10,09-05-2024 20:10,Tool Setup/Errors
78501938,how to stop generating response in openai library for python?,"i am using completions in openai library for python. something like this:
self.__response = self.client.chat.completions.create(
    model='gpt-4',
    messages=messages,
    stream=true
)

after this i just loop through chunks:
    for chunk in self.__response:
        text = chunk.choices[0].delta.content
        # processing text here

is it enough to just do break inside the loop to prevent server generating response and wasting tokens if i see that the response is not meeting my expectations? or probably there is correct way to achieve this?","['python', 'openai-api']",78501966,"you are charged for all the tokens (words or parts of words) the api generates, even if you don't process them. so, breaking the loop early stops you from processing more tokens but doesn't stop you from being charged for them.
you can limit that using 'max_tokens', that'll save you from the cost, but in that case, you'll be forever stuck with the lower max_tokens response, even if that particular response is the desired one.",https://stackoverflow.com/questions/78501938,python,19-05-2024 07:58,498.0,0.0,1.0,True,19-05-2024 08:21,19-05-2024 08:06,Implementation Issues
72442701,how can i find repeated string segments in python?,"so i have some medium-length string - somewhere between a few words and a few sentences. sometimes, a substring in the text is repeated twice in a row. i need to write automatic code to identify the repeated part. or at least flag it with a high probability.
what i know:

the repeated substring is a series of a few whole words (and punctuation marks). a repeat will not happen in the middle of a word.
the repeat is of a variable length. it can be a few words to a few sentences itself. but it's always at least a few words long. i would like to avoid flagging single word repetitions if possible.
when a repeat happens, it's always repeated exactly once, and right after the previous appearence. right after the previous appearence. (<- example)
i need to run this check on about a million different strings, so the code has to be somewhat efficient at least (not the brute force check-every-option approach).

i've been struggling with this for a while now. would really appreciate your help.","['python', 'string', 'nlp']",72443424,"since the repetition of one word is a subclass of a multiple-word repetition, it's already helpful to match single words or word-like sequences. here is the regular expression i tried on your question in an editor with regex search:
(\<\w.{3,16}\w\>).{2,}\1

this is the first repetition found

the repeat is of a variable length. it can be a few words to a few sentences itself. but it's always at least a few words long. i would like to avoid flagging single word repetitions if possible.

but it next finds repeat in repeating. so we have to tune the limits.
the part (\<\w.{3,16}\w\>) means

from word start (including a character)
3 to 16 arbitrary characters
before word end (including a character)

in other words, one or more word with a total character count of 5 to 18.
the part .{2,}\1 means

at least two characters
no upper limit
captured match

here, the lower limit can be higher. an upper limit should be tried, especially on longer text.
i'd think that starting with finding short character sequences which repeat, then refine by looking for longer sequences that repeat in the result of the first step (plus additional characters at the end).
it's also a matter of preprocessing. i'd guess that repeating multiple-word sequences should be missed if line breaks (instead of space occur) on different places.
to automate this further, you may switch to python's re module.",https://stackoverflow.com/questions/72442701,python,31-05-2022 05:37,503.0,2.0,1.0,True,31-05-2022 07:10,31-05-2022 05:49,Tool Setup/Errors
68870383,type errors with bert example,"i'm new to bert qa model & was trying to follow the example found in this article. the problem is when i run the code attached to the example it produces a type error as follows typeerror: argmax(): argument 'input' (position 1) must be tensor, not str.
here is the code that i've tried running :
import torch
from transformers import bertforquestionanswering
from transformers import berttokenizer

#model
model = bertforquestionanswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

#tokenizer
tokenizer = berttokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

question = '''sample question""'''

paragraph = '''sample paragraph'''
            
encoding = tokenizer.encode_plus(text=question,text_pair=paragraph, add_special=true)

inputs = encoding['input_ids']  #token embeddings
sentence_embedding = encoding['token_type_ids']  #segment embeddings
tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens

start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))

start_index = torch.argmax(start_scores)

end_index = torch.argmax(end_scores)

answer = ' '.join(tokens[start_index:end_index+1])

the issue appears at line 13 of this code where i'm trying to get the maximum element in start_scores saying that this is not a tensor. when i tried printing this variable it showed ""start_logits"" as a string. does anyone know a solution to this issue?","['python', 'bert-language-model']",68871099,"so after referring to the bert documentation we identified that the model output object contains multiple properties not only start & end scores. thus, we applied the following changes to the code.

outputs = model(input_ids=torch.tensor([inputs]),token_type_ids=torch.tensor([sentence_embedding]))

start_index = torch.argmax(outputs.start_logits)

end_index = torch.argmax(outputs.end_logits)

answer = ' '.join(tokens[start_index:end_index+1])

always refer to the documentation first :""d",https://stackoverflow.com/questions/68870383,python,21-08-2021 05:28,426.0,0.0,1.0,True,21-08-2021 07:45,21-08-2021 05:38,Implementation Issues
77427999,custom spacy tagger to tag all words that are in a dictionary,"i'm trying spacy to extract specific information from a text.
so i need to configure a custom tokenizer to identify them and a custom tagger to label all the words that are in an external dictionary in json format.
the tokenizer worked on several attempts, but the labeler has been having problems when processing simple text.
i hope that the label i will add to the words is a custom pos-tag ""unm"" and that i can attribute it to token.pos_ like all other labels ""noun"", ""verb"", etc.
import requests

#keywords dictionary
dictionary = requests.get(
    ""

    
#creating the custom tagger
doc.set_extension('pos_tag', default=none, force=true)

@language.factory(""keyword_pos_tagger"")
class keywordpostagger:
   def __init__(self, name, nlp, keywords, pos_tag):
       self.keywords = keywords
       self.pos_tag = pos_tag
       #doc.set_extension('pos_tag', default=none, force=true)

   def __call__(self, doc):
       for token in doc:
           if token.text in self.keywords:
               token._.pos_tag = self.pos_tag
       return doc

nlp = spacy.load('pt_core_news_md')


keywords = ('mï¿½ï¿½', 'm2', '(w/k)', 'ï¿½ï¿½c')
pos_tag = 'unm' # substitua por seu rï¿½ï¿½tulo pos

keyword_pos_tagger = keywordpostagger(nlp, 'keyword_pos_tagger', keywords, pos_tag)

config = {""nlp"": nlp, ""keywords"": keywords, ""pos_tag"": pos_tag}

nlp.add_pipe('keyword_pos_tagger', config = config)

<main.keywordpostagger at 0x78d568e4cee0>
and when i use the custom tagger:
doc = nlp('a temperatura tem 159ï¿½ï¿½c ou 20 ï¿½ï¿½c. tambï¿½ï¿½m precisa ter 20m de largura e 14 mï¿½ï¿½ de ï¿½ï¿½rea, caso contrï¿½ï¿½rio terï¿½ï¿½ 1 kelvin (w/k)')
for token in doc:
   print(token.text, token._.pos_tag)

it returns this error
-------------------------------------------------------------                 traceback (most recent call last)
<ipython-input-5-3c241e1c89fd> in <cell line: 1>()
----> 1 doc = nlp('a temperatura tem 159ï¿½ï¿½c ou 20 ï¿½ï¿½c. tambï¿½ï¿½m precisa ter 20m de largura e 14 mï¿½ï¿½ de ï¿½ï¿½rea, caso contrï¿½ï¿½rio terï¿½ï¿½ 1 kelvin (w/k)')
      2 for token in doc:
      3    print(token.text, token._.pos_tag)

4 frames
/usr/local/lib/python3.10/dist-packages/spacy/tokens/underscore.py in __setattr__(self, name, value)
     74     def __setattr__(self, name: str, value: any):
     75         if name not in self._extensions:
---> 76             raise attributeerror(errors.e047.format(name=name))
     77         default, method, getter, setter = self._extensions[name]
     78         if setter is not none:

attributeerror: [e047] can't assign a value to unregistered extension attribute 'pos_tag'. did you forget to call the `set_exten","['python', 'nlp', 'spacy', 'pos-tagger']",77429721,"you need to provide the config settings in the add_pipe method through a config dict. in your code, the keyword_pos_tagger variable is a stranded component that's not actually added to the nlp pipeline. it shares the same vocab and you could use it for unit testing, but otherwise you can't add it to a pipeline when it's created like this.
nlp.add_pipe(""keyword_pos_tagger"", config={""keywords"": keywords, ""pos_tag"": pos_tag})


edited to expand answer:
# tested with spacy==3.7.2
import spacy
from spacy.language import language
from spacy.tokens import token

# creating the custom tagger
token.set_extension(""pos_tag"", default=none, force=true)


@language.factory(""keyword_pos_tagger"")
class keywordpostagger:
    def __init__(self, name, nlp, keywords, pos_tag):
        self.keywords = keywords
        self.pos_tag = pos_tag

    def __call__(self, doc):
        for token in doc:
            if token.text in self.keywords:
                token._.pos_tag = self.pos_tag
        return doc


nlp = spacy.load(""pt_core_news_md"")

keywords = (""mï¿½ï¿½"", ""m2"", ""(w/k)"", ""ï¿½ï¿½c"")
pos_tag = ""unm""  # substitua por seu rï¿½ï¿½tulo pos

config = {""keywords"": keywords, ""pos_tag"": pos_tag}

nlp.add_pipe(""keyword_pos_tagger"", config=config)

doc = nlp(
    ""a temperatura tem 159ï¿½ï¿½c ou 20 ï¿½ï¿½c. tambï¿½ï¿½m precisa ter 20m de largura e 14 mï¿½ï¿½ rï¿½ï¿½ 1 kelvin (w/k)""
)
assert doc[16]._.pos_tag == ""unm""
</p",https://stackoverflow.com/questions/77427999,python,05-11-2023 23:07,184.0,1.0,1.0,True,06-11-2023 14:17,06-11-2023 14:02,Implementation Issues
69028332,spacy 3 - lemma_ returned will be empty string,"i normalize ten thousands of docs using spacy 3.

to speed up the process, i try this way,

nlp = spacy.load('en_core_web_sm')
docs = nlp.tokenizer.pipe(doc_list)
return [[word.lemma_ for word in doc if word.is_punct == false and word.is_stop == false] for doc, _ in doc_list]

but all lemma_ returned would be empty string.

so i directly use nlp(doc) like the following, but it's too slow.

a = [[word.lemma_ for word in nlp(doc) if word.is_punct == false and word.is_stop == false] for doc in doc_list]

how can i do this properly?","['python', 'spacy']",69052065,"the difference is in how you are creating the docs.

in the first example you use nlp.tokenizer.pipe() - this will only run the tokenizer on all your docs but not the lemmatizer. so, all you get is your docs split into tokens but the lemma_ attribute is not set.
in the second example you use nlp(doc) this will run all the default components (which are ['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']. since the lemmatizer is part of the pipeline the lemma_ attribute is set. but, it slower because you are running all the components, even the ones you don't need.

what you should be doing:
import spacy

# exclude components not required when loading the spacy model.
nlp = spacy.load(""en_core_web_sm"", exclude=[""tok2vec"", ""parser"", ""ner"", ""attrbute_ruler""]) 

# extract lemmas as required.
a = [[word.lemma_ for word in nlp(doc) if word.is_punct == false and word.is_stop == false] for doc in doc_list]",https://stackoverflow.com/questions/69028332,python,02-09-2021 10:17,945.0,2.0,1.0,True,11-10-2021 13:39,11-10-2021 13:39,Implementation Issues
76080243,what is the correct way to send a long string by an http stream in expressjs/nestjs?,"i'm using nestjs to write a forwarding service for the openai chat completion api. i want to do a transformation for the original stream and then forward stream to the client side.
the code is like below, and it's inside a nestjs controller
const completion = await openai.createchatcompletion(
  {
    model: 'gpt-3.5-turbo',
    messages: messages,
    n: 1,
    stream: true,
    max_tokens: 4000,
  },
  { responsetype: 'stream' },
);

class transformerstream extends transform {
  _transform(chunk, encoding, callback) {
    // if i directly forward the chunk like this, the client can receive chunk by chunk
    this.push(chunk)
    // however, if i use string, the client can't receive chunk by chunk.
    // my original code is to transform the chunk to string and do some transformation, to simplify the question, just use 'data: ping\n' here
    this.push('data: ping\n', 'utf8')
    callback()
  }
}

const transformer = new transformerstream()
completion.data.pipe(transformer).pipe(res)

and i'm using axios to request the api from the client side, and i'm trying to receive it chunk by chunk using ondownloadprogress
axios.post('/api/chat', body, {
  responsetype: 'stream',
  ondownloadprogress: progress => {
    console.log(progress)
  }
} )

in summary, when i directly send the buffer chunk from the openai api, the progress can be logged several times.
but when i send the string, it can only be logged once.","['node.js', 'express', 'http', 'openai-api', 'http-streaming']",76089098,"it might be due to the difference between the length of the original chunk and the length of the string you are trying to write to the stream.
you can consider setting the following headers in your nestjs controller:

transfer-encoding: chunked
x-content-type-options: nosniff

sample code:
res.setheader('transfer-encoding', 'chunked');
res.setheader('x-content-type-options', 'nosniff');

transfer-encoding tells the browser to start processing the data instead of waiting for all the content to be loaded first
x-content-type-options tells the browser to respect the content-type specified by your header instead of trying to guess based on the head of the content returned. based on my test with latest chrome browser, it seems like the initial 1024 bytes are ""blocked"" before browser correctly identified the content-type.
you can read more about the behaviour here: what is ""x-content-type-options=nosniff""?
reference:",https://stackoverflow.com/questions/76080243,node.js,22-04-2023 14:48,2188.0,0.0,1.0,True,24-04-2023 15:51,24-04-2023 15:51,Conceptual Questions
76674599,can i make use charset-normalizer 3.0.0 instead of 3.2.0 in build.gradle so that i do not get aiohttp related library error in android studio?,"i am using chaquopy to integrate python into android. i am making a fairly simple app the uses the openai and webvtt libraries; hence my src/main/script.py contains ""import open ai"" and ""import webvtt"" at the beginning of the script. i also have in my build.gradle the block:
android {

    namespace 'com.example.python_integration'
    compilesdk 33

    defaultconfig {


        python{
            pip{
                install ""numpy""
            }
        }
        python{
             pip{
                install ""openai""
            }
        }
        python{
            pip{
                install ""tk""
            }
        }
        python{
            pip{
                install ""webvtt_py""
            }
        }

when i run the app in an emulator i get a warning ""aiohttp 3.8.1 has requirement charset-normalizer<3.0,>=2.0, but you'll have charset-normalizer 3.2.0 which is incompatible.""
i tried adding an implementation to the gradle:
implementation 'com.github.ousret:charset_normalizer:3.0.0'
as well as trying
implementation 'com.github.mizosoft.charset-normalizer:charset-normalizer:3.0.0'
but i still get the error/warning. is this warning significant? is there another block of code i need to add so that the gradle uses the previous charset-normalizer version?
the app still runs but i am still editing my script.py so i have no way of knowing whether this is a critical error or not at this stage, i am currently just making sure i can get all of the libraries i need to work from my original python script.
sometimes when i build the app in the emulator i don't even get this error in the build log so it is a little frustrating that it randomly keeps popping up.
maybe i am wildly misunderstanding the situation?","['python', 'android', 'openai-api', 'chaquopy', 'webvtt']",76677277,"the warning may or may not be critical, depending on what your app does. but you can probably work around it by adding a direct requirement on the indicated version:
install ""charset-normalizer<3.0,>=2.0""

the reason you don't see the warning every time you build your app is that it only re-runs pip when you change the pip block, or other relevant settings like the python version.
additional notes:

you can include more than one install line in a python { pip } block, so you don't need to copy the entire block multiple times.
the gradle dependencies block is completely independent from your python requirements, so don't bother editing that.",https://stackoverflow.com/questions/76674599,python,12-07-2023 21:36,965.0,1.0,1.0,True,13-07-2023 08:16,13-07-2023 00:50,Tool Setup/Errors
60867353,using lime for bert transformer visualization results in memory error,"situation:  i am currently working on visualizing the results of a huggingface transformers machine learning model i have been building using the lime package following this tutorial. 
complication: my code is set up and runs well until i create the lime explainer object. at this point i get a memory error.
question: what am i doing wrong? why am i running into a memory error?
code: here is my code (you should be able to just copy-paste this into google colab and run the whole thing)
########################## load packages ######################
# install new packages in our environment
!pip install lime
!pip install wget
!pip install transformers

# import general libraries
import sklearn
import sklearn.ensemble
import sklearn.metrics
import numpy as np
import pandas as pd

# import libraries specific to this notebook
import lime
import wget
import os
from __future__ import print_function
from transformers import featureextractionpipeline, bertmodel, berttokenizer, bertconfig
from lime.lime_text import limetextexplainer

# let the notebook know to plot inline
%matplotlib inline

########################## load data ##########################
# get url
url = '

# download the file (if we haven't already)
if not os.path.exists('./cola_public_1.1.zip'):
    wget.download(url, './cola_public_1.1.zip')

# unzip the dataset (if we haven't already)
if not os.path.exists('./cola_public/'):
    !unzip cola_public_1.1.zip

# load the dataset into a pandas dataframe.
df_cola = pd.read_csv(""./cola_public/raw/in_domain_train.tsv"", delimiter='\t', 
                      header=none, names=['sentence_source', 'label', 
                                          'label_notes', 'sentence'])

# only look at the first 50 observations for debugging
df_cola = df_cola.head(50)

###################### train test split ######################
# apply the train test split
x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(
    df_cola.sentence, df_cola.label, test_size=0.2, random_state=42
)

###################### create lime classifier ######################
# create a function to extract vectors from a single sentence
def vector_extractor(sentence):

    # create a basic bert model, config and tokenizer for the pipeline
    configuration = bertconfig()
    configuration.max_len = 64
    tokenizer = berttokenizer.from_pretrained('bert-base-uncased',
                                              do_lower_case=true, 
                                              max_length=64,
                                              pad_to_max_length=true)
    model = bertmodel.from_pretrained('bert-base-uncased',config=configuration)

    # create the pipeline
    vector_extractor = featureextractionpipeline(model=model,
                                                 tokenizer=tokenizer,
                                                 device=0)

    # the pipeline gives us all tokens in the final layer - we want the cls token
    vector = vector_extractor(sentence)
    vector = vector[0][0]

    # return the vector
    return vector

# adjust the format of our sentences (from pandas series to python list)
x_train = x_train.values.tolist()
x_test = x_test.values.tolist()

# first we vectorize our train features for the classifier
x_train_vectorized = [vector_extractor(x) for x in x_train]

# create and fit the random forest classifier
rf = sklearn.ensemble.randomforestclassifier(n_estimators=100)
rf.fit(x_train_vectorized, y_train)

# define the lime_classifier function
def lime_classifier(sentences): 

    # turn all the sentences into vectors
    vectors = [vector_extractor(x) for x in sentences]

    # get predictions for all 
    predictions = rf.predict_proba(vectors)

    # return the probabilies as a 2d-array
    return predictions  

########################### apply lime ##########################
# create the general explainer object
explainer = limetextexplainer()

# ""fit"" the explainer object to a specific observation
exp = explainer.explain_instance(x_test[1], 
                                 lime_classifier, 
                                 num_features=6)","['python', 'machine-learning', 'huggingface-transformers', 'lime']",60989317,"ended up solving this by re-implementing along the lines of this github post: 

my code is now very different from the above - probably makes sense if you look to the github post for guidance if you're running into similar issues.",https://stackoverflow.com/questions/60867353,python,26-03-2020 12:32,4150.0,2.0,1.0,True,18-12-2022 20:45,18-12-2022 20:45,Implementation Issues
73251309,how to feed big data into pipeline of huggingface for inference,"model = ""bert-base-uncased""

# load the model
model_name = model + '-text-classification'

from transformers import automodelforsequenceclassification, autotokenizer

load_model = automodelforsequenceclassification.from_pretrained(model_name)
load_tokenizer = autotokenizer.from_pretrained(model_name)
from transformers import pipeline
my_pipeline  = pipeline(""text-classification"", model=load_model, 
                                                tokenizer=load_tokenizer)
a = list(df_0.limit(10000).topandas()[""lines""])
my_pipeline(a)

error message:
token indices sequence length is longer than the specified maximum sequence length for this model (1081 > 512). running this sequence through the model will result in indexing errors
--------------------------------------------------------------------------- runtimeerror                              traceback (most recent call last) input in [26], in <cell line: 1>()
----> 1 b = my_pipeline(a)

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:138, in textclassificationpipeline.__call__(self, *args, **kwargs)
    104 def __call__(self, *args, **kwargs):
    105     """"""
    106     classify the text(s) given as inputs.
    107     (...)
    136         if `top_k` is used, one such dictionary is returned per label.
    137     """"""
--> 138     result = super().__call__(*args, **kwargs)
    139     if isinstance(args[0], str) and isinstance(result, dict):
    140         # this pipeline is odd, and return a list when single item is run
    141         return [result]

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/pipelines/base.py:1032, in pipeline.__call__(self, inputs, num_workers, batch_size, *args,
**kwargs)    1028 if can_use_iterator:    1029     final_iterator = self.get_iterator(    1030         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params    1031     )
-> 1032     outputs = [output for output in final_iterator]    1033     return outputs    1034 else:

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/pipelines/base.py:1032, in <listcomp>(.0)    1028 if can_use_iterator:    1029     final_iterator = self.get_iterator(    1030         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params    1031     )
-> 1032     outputs = [output for output in final_iterator]    1033     return outputs    1034 else:

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:111, in pipelineiterator.__next__(self)
    108     return self.loader_batch_item()
    110 # we're out of items within a batch
--> 111 item = next(self.iterator)
    112 processed = self.infer(item, **self.params)
    113 # we now have a batch of ""inferred things"".

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:112, in pipelineiterator.__next__(self)
    110 # we're out of items within a batch
    111 item = next(self.iterator)
--> 112 processed = self.infer(item, **self.params)
    113 # we now have a batch of ""inferred things"".
    114 if self.loader_batch_size is not none:
    115     # try to infer the size of the batch

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/pipelines/base.py:959, in pipeline.forward(self, model_inputs, **forward_params)
    957     with inference_context():
    958         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)
--> 959         model_outputs = self._forward(model_inputs, **forward_params)
    960         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(""cpu""))
    961 else:

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:163, in textclassificationpipeline._forward(self, model_inputs)
    162 def _forward(self, model_inputs):
--> 163     return self.model(**model_inputs)

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in module._call_impl(self, *input, **kwargs)    1126 # if we don't have any hooks, we want to skip the rest of the logic in    1127 # this function, and just call forward.    1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks    1129         or
_global_forward_hooks or _global_forward_pre_hooks):
-> 1130     return forward_call(*input, **kwargs)    1131 # do not call functions when jit is used    1132 full_backward_hooks, non_full_backward_hooks = [], []

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1556, in bertforsequenceclassification.forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)    1548 r""""""    1549 labels (`torch.longtensor` of shape `(batch_size,)`, *optional*):    1550     labels for computing the sequence classification/regression loss. indices should be in `[0, ...,    1551     config.num_labels - 1]`. if `config.num_labels == 1` a regression loss is computed (mean-square loss), if    1552     `config.num_labels > 1` a classification loss is computed (cross-entropy).    1553 """"""    1554 return_dict = return_dict if return_dict is not none else self.config.use_return_dict
-> 1556 outputs = self.bert(    1557     input_ids,    1558     attention_mask=attention_mask,    1559     token_type_ids=token_type_ids,    1560     position_ids=position_ids,  1561     head_mask=head_mask,    1562     inputs_embeds=inputs_embeds, 1563     output_attentions=output_attentions,    1564     output_hidden_states=output_hidden_states,    1565     return_dict=return_dict,    1566 )    1568 pooled_output = outputs[1]  1570 pooled_output = self.dropout(pooled_output)

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in module._call_impl(self, *input, **kwargs)    1126 # if we don't have any hooks, we want to skip the rest of the logic in    1127 # this function, and just call forward.    1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks    1129         or
_global_forward_hooks or _global_forward_pre_hooks):
-> 1130     return forward_call(*input, **kwargs)    1131 # do not call functions when jit is used    1132 full_backward_hooks, non_full_backward_hooks = [], []

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1011, in bertmodel.forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)    1004 # prepare head mask if needed    1005 # 1.0 in head_mask indicate we keep the head    1006 # attention_probs has shape bsz x n_heads x n x n    1007 # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]    1008 # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]    1009 head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)
-> 1011 embedding_output = self.embeddings(    1012     input_ids=input_ids,    1013     position_ids=position_ids,    1014    token_type_ids=token_type_ids,    1015     inputs_embeds=inputs_embeds,    1016     past_key_values_length=past_key_values_length,    1017 )    1018 encoder_outputs = self.encoder(    1019     embedding_output,    1020  attention_mask=extended_attention_mask,    (...)    1028     return_dict=return_dict,    1029 )    1030 sequence_output = encoder_outputs[0]

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in module._call_impl(self, *input, **kwargs)    1126 # if we don't have any hooks, we want to skip the rest of the logic in    1127 # this function, and just call forward.    1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks    1129         or
_global_forward_hooks or _global_forward_pre_hooks):
-> 1130     return forward_call(*input, **kwargs)    1131 # do not call functions when jit is used    1132 full_backward_hooks, non_full_backward_hooks = [], []

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:241, in bertembeddings.forward(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)
    239 if self.position_embedding_type == ""absolute"":
    240     position_embeddings = self.position_embeddings(position_ids)
--> 241     embeddings += position_embeddings
    242 embeddings = self.layernorm(embeddings)
    243 embeddings = self.dropout(embeddings)

df_0 is spark dataframe, which include huge data. my question is how to feed this dataframe into the pipeline with entire data, or with batch size.",['huggingface-transformers'],73255205,"the error you get (please always post the full error stacktrace in the future), is not caused by the size of a, it is caused by one of the texts exceeding the length your model can handle. your model can handle up to 512 tokens and you need to truncate your input otherwise:
from transformers import pipeline
my_pipeline  = pipeline(""text-classification"", model=""distilbert-base-uncased-finetuned-sst-2-english"")

te = ""this is a long text ""*1024
print(te)
print(len(my_pipeline.tokenizer.tokenize(te)))
my_pipeline(te, truncation=true)

output:
this is a long text this is a long text this is a long text this is a long text this is a long text ...
5120
[{'label': 'negative', 'score': 0.9979830980300903}]

the pipeline object will process a list with one sample at a time. you can try to speed up the classification by specifying a batch_size, however, note that it is not necessarily faster and depends on the model and hardware:
te_list = [te]*10
my_pipeline(te_list, batch_size=5, truncation=true,)",https://stackoverflow.com/questions/73251309,huggingface-transformers,05-08-2022 14:26,4433.0,2.0,1.0,True,04-04-2025 23:03,04-04-2025 23:03,Implementation Issues
53309192,similarity between two lists of documents,"i need to find the similarity between two lists of the short texts in python.
texts can be 1-4 word long. the length of the lists can be 10k each. so, i need to effectively calculate 10k*10k=100m similarity scores.
i didn't find how to do this effectively in spacy. maybe other packages can do this?
i assume the words are represented by a vector (300d), but any other options are also ok.
this task can be done in a cycle, but there should be a more effective way for sure. this task fits the tensorflow, pytorch, and similar packages, but i'm not familiar with details of these packages.","['tensorflow', 'nlp', 'similarity', 'spacy', 'sentence-similarity']",71697660,the solution was to use something like spotify annoy which uses approximate nearest neighbours method. there are some other libraries to do the nearest neighbour search.,https://stackoverflow.com/questions/53309192,tensorflow,14-11-2018 21:45,1774.0,0.0,2.0,True,06-12-2023 04:48,31-03-2022 18:40,Implementation Issues
72628556,&#39;maskedlmoutput&#39; object has no attribute &#39;view&#39;,"i wrote this:
def forward(self, x):
    x = self.bert(x)
    
    x = x.view(x.shape[0], -1)
    x = self.fc(self.dropout(self.bn(x)))
    return x

but it doesn't work well, and the error is 'maskedlmoutput' object has no attribute 'view'.
i'm considering the input might not be 'tensor' type, so i change it as below:
def forward(self, x):
        x = torch.tensor(x)     # this part
        x = self.bert(x)
        
        x = x.view(x.shape[0], -1)
        x = self.fc(self.dropout(self.bn(x)))
        return x

but it still gets wrong, same error 'maskedlmoutput' object has no attribute 'view'.
could someone tell me how to fix this?  much thanks.
whole error information here:
 ------------------------------------------------------------------------
    attributeerror                            traceback (most recent call last)
    input in [5], in <cell line: 8>()
          6 optimizer = optim.adam(bert_punc.parameters(), lr=learning_rate_top)
          7 criterion = nn.crossentropyloss()
    ----> 8 bert_punc, optimizer, best_val_loss = train(bert_punc, optimizer, criterion, epochs_top, 
          9     data_loader_train, data_loader_valid, save_path, punctuation_enc, iterations_top, best_val_loss=1e9)
    
    input in [3], in train(model, optimizer, criterion, epochs, data_loader_train, data_loader_valid, save_path, punctuation_enc, iterations, best_val_loss)
         17 inputs.requires_grad = false
         18 labels.requires_grad = false
    ---> 19 output = model(inputs)
         20 loss = criterion(output, labels)
         21 loss.backward()
    
    file ~\anaconda3\lib\site-packages\torch\nn\modules\module.py:1110, in module._call_impl(self, *input, **kwargs)
       1106 # if we don't have any hooks, we want to skip the rest of the logic in
       1107 # this function, and just call forward.
       1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
       1109         or _global_forward_hooks or _global_forward_pre_hooks):
    -> 1110     return forward_call(*input, **kwargs)
       1111 # do not call functions when jit is used
       1112 full_backward_hooks, non_full_backward_hooks = [], []
    
    file ~\anaconda3\lib\site-packages\torch\nn\parallel\data_parallel.py:166, in dataparallel.forward(self, *inputs, **kwargs)
        163     kwargs = ({},)
        165 if len(self.device_ids) == 1:
    --> 166     return self.module(*inputs[0], **kwargs[0])
        167 replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
        168 outputs = self.parallel_apply(replicas, inputs, kwargs)
    
    file ~\anaconda3\lib\site-packages\torch\nn\modules\module.py:1110, in module._call_impl(self, *input, **kwargs)
       1106 # if we don't have any hooks, we want to skip the rest of the logic in
       1107 # this function, and just call forward.
       1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
       1109         or _global_forward_hooks or _global_forward_pre_hooks):
    -> 1110     return forward_call(*input, **kwargs)
       1111 # do not call functions when jit is used
       1112 full_backward_hooks, non_full_backward_hooks = [], []
    
    file d:\bertpunc-original\model.py:21, in bertpunc.forward(self, x)
         18 x = torch.tensor(x)
         19 x = self.bert(x)
    ---> 21 x = x.view(x.shape[0], -1)
         22 x = self.fc(self.dropout(self.bn(x)))
         23 return x
    
    attributeerror: 'maskedlmoutput' object has no attribute 'view'","['python', 'tensorflow', 'pytorch', 'huggingface-transformers', 'bert-language-model']",72628798,"i think so this should help you solve the error. 
the output after self.bert(x) is an object of the class maskedlmoutput.",https://stackoverflow.com/questions/72628556,python,15-06-2022 08:59,1886.0,0.0,1.0,True,15-06-2022 09:16,15-06-2022 09:10,Tool Setup/Errors
18902608,generating the plural form of a noun,"given a word, which may or may not be a singular-form noun, how would you generate its plural form?
based on this nltk tutorial and this informal list on pluralization rules, i wrote this simple function:
def plural(word):
    """"""
    converts a word to its plural form.
    """"""
    if word in c.plurale_tantums:
        # defective nouns, fish, deer, etc
        return word
    elif word in c.irregular_nouns:
        # foot->feet, person->people, etc
        return c.irregular_nouns[word]
    elif word.endswith('fe'):
        # wolf -> wolves
        return word[:-2] + 'ves'
    elif word.endswith('f'):
        # knife -> knives
        return word[:-1] + 'ves'
    elif word.endswith('o'):
        # potato -> potatoes
        return word + 'es'
    elif word.endswith('us'):
        # cactus -> cacti
        return word[:-2] + 'i'
    elif word.endswith('on'):
        # criterion -> criteria
        return word[:-2] + 'a'
    elif word.endswith('y'):
        # community -> communities
        return word[:-1] + 'ies'
    elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:
        return word + 'es'
    elif word.endswith('an'):
        return word[:-2] + 'en'
    else:
        return word + 's'

but i think this is incomplete. is there a better way to do this?","['python', 'nlp', 'wordnet', 'linguistics']",19018986,"the pattern-en package offers pluralization
>>> import pattern.text.en
>>> pattern.text.en.pluralize(""dog"")
'dogs'

note also that in order to run the import above successfully, you may have to first execute the following (at least the first time):
>>> import nltk
>>> nltk.download('omw-1.4')",https://stackoverflow.com/questions/18902608,python,19-09-2013 18:44,29817.0,14.0,5.0,True,03-06-2024 02:39,20-09-2013 13:35,Implementation Issues
3541371,php: how do i detect if an input string is arabic,is there a way to detect the language of the data being entered via the input field?,"['php', 'language-detection']",3542914,"hmm i may offer an improved version of dimakrasun's function:
functoin is_arabic($string) {
    if($string === 'arabic') {
         return true;
    }
    return false;
}

okay, enough joking!
pekkas suggestion to use the google translate api is a good one! but you are relying on an external service which is always more complicated etc.
i think rushyos approch is good! its just not that easy.
i wrote the following function for you but its not tested, but it should work...
    <?
function uniord($u) {
    // i just copied this function fron the php.net comments, but it should work fine!
    $k = mb_convert_encoding($u, 'ucs-2le', 'utf-8');
    $k1 = ord(substr($k, 0, 1));
    $k2 = ord(substr($k, 1, 1));
    return $k2 * 256 + $k1;
}
function is_arabic($str) {
    if(mb_detect_encoding($str) !== 'utf-8') {
        $str = mb_convert_encoding($str,mb_detect_encoding($str),'utf-8');
    }

    /*
    $str = str_split($str); <- this function is not mb safe, it splits by bytes, not characters. we cannot use it
    $str = preg_split('//u',$str); <- this function woulrd probably work fine but there was a bug reported in some php version so it pslits by bytes and not chars as well
    */
    preg_match_all('/.|\n/u', $str, $matches);
    $chars = $matches[0];
    $arabic_count = 0;
    $latin_count = 0;
    $total_count = 0;
    foreach($chars as $char) {
        //$pos = ord($char); we cant use that, its not binary safe 
        $pos = uniord($char);
        echo $char ."" --> "".$pos.php_eol;

        if($pos >= 1536 && $pos <= 1791) {
            $arabic_count++;
        } else if($pos > 123 && $pos < 123) {
            $latin_count++;
        }
        $total_count++;
    }
    if(($arabic_count/$total_count) > 0.6) {
        // 60% arabic chars, its probably arabic
        return true;
    }
    return false;
}
$arabic = is_arabic('ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½. ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½'); 
var_dump($arabic);
?>

final thoughts:
as you see i added for example a latin counter, the range is just a dummy number b ut this way you could detect charsets (hebrew, latin, arabic, hindi, chinese, etc...) 
you may also want to all the character sets and see which one of course the most... 
and finally you should consider chopping your string off after 200 chars or something. this should be enough to tell what character set is used.
and you have to do some error handling! like division by zero, empty string etc etc! don't forget that please... any questions? comment!
if you want to detect the language of a string, you should split into words and check for the words in some pre-defined tables. you don't need a complete dictionary, just the most common words and it should work fine. tokenization/normalization is a must as well! there are libraries for that anyway and this is not what you asked for :) just wanted to mention it",https://stackoverflow.com/questions/3541371,php,22-08-2010 11:53,27408.0,19.0,10.0,True,21-01-2022 09:07,03-11-2010 00:09,Implementation Issues
69364068,"loading tf.keras model, valueerror: the two structures don&#39;t have the same nested structure","i created a tf.keras model that has bert and i want to train and save it for further use.
loading this model is a big issue cause i keep getting error: valueerror: the two structures don't have the same nested structure.
i simplified the model a lot, to see where is the problem exactly. the code is pretty simple:
bert = tfbertmodel.from_pretrained(""bert-base-german-cased"")

model_name = ""model""
txt12_input_ids = tf.keras.layers.input(shape=(max_length,),  name='txt12_input_ids', dtype='int32')
txt12_mask      = tf.keras.layers.input(shape=(max_length,),  name='txt12_mask', dtype='int32')
txt12_outputs = bert(txt12_input_ids, txt12_mask).pooler_output

model_k = tf.keras.model(inputs=(txt12_input_ids,  txt12_mask), outputs=txt12_outputs, name=model_name)
model_k.compile(optimizer=adam(1e-5), loss=""binary_crossentropy"", metrics=""accuracy"")


model_k.save(dir_path+'prob')
model_2 = tf.keras.models.load_model(dir_path+'prob')

some notes before you start replying:

i did specified dtype.

no, i don't want to save just weights.

i tried to use tf.keras.models.save_model(model_k, dir_path+'prob') instead and it gives the same error.


and the last thing, i work with tf version: 2.6.0. does anyone knows how to solve it?
full error message:
valueerror: the two structures don't have the same nested structure.

first structure: type=tuple str=(({'input_ids': tensorspec(shape=(none, 5), dtype=tf.int32, name='input_ids/input_ids')}, none, none, none, none, none, none, none, none, false), {})

second structure: type=tuple str=((tensorspec(shape=(none, 120), dtype=tf.int32, name='input_ids'), tensorspec(shape=(none, 120), dtype=tf.int32, name='attention_mask'), none, none, none, none, none, none, none, false), {})

more specifically: substructure ""type=dict str={'input_ids': tensorspec(shape=(none, 5), dtype=tf.int32, name='input_ids/input_ids')}"" is a sequence, while substructure ""type=tensorspec str=tensorspec(shape=(none, 120), dtype=tf.int32, name='input_ids')"" is not
entire first structure:
(({'input_ids': .}, ., ., ., ., ., ., ., ., .), {})
entire second structure:
((., ., ., ., ., ., ., ., ., .), {})","['python', 'tensorflow', 'keras', 'huggingface-transformers', 'bert-language-model']",69451206,check out the issue here on github. it should help you to solve the problem with the shape.,https://stackoverflow.com/questions/69364068,python,28-09-2021 14:57,3893.0,4.0,3.0,True,21-01-2025 07:29,29-09-2021 14:37,Tool Setup/Errors
76070777,powerbi custom visual with chatgpt,"i am developing a custom visual into power bi using typescript. i have an input of type text for user input prompt and an input of type text for chatgpt answer. the idea is that the user can ask anything about report's data or any report's visual and get an answer. the visual at the current stage looks like this:

behind the scenes the user prompt is sent to azure-openai service and is being processed by chatgpt deployment to get the response. the only part which is missing is to be able to pass also the report's data. i have seen a similar video doing this with powerautomate visual, here is the video: 

in this video we are able to pass though report's data though power automate visual into user prompt in order to be analyzed together with the question on our data:

i managed to do the same by passing visual's data in a structured json format along with prompt and seems to working, but the question is if it is possible to get report's data though typescript into the custom visual without having the dataset on the visual it self?
i tried already a library called powerbi client inside my custom visual but with any use of this library the visual stop working (i think this can be used only with powerbi embedded):




based on this article is not possible to use a custom visual and access data on page or report scope level: 
any ideas?","['typescript', 'powerbi', 'openai-api', 'powerbi-custom-visuals', 'chatgpt-api']",78388995,"after a lot of research, at the moment, this is not support and cannot be done though powerbi, as we cannot access the whole reports data, so i changed the direction by creating a langchain server inside a kubernetes cluster accessing directly the respective database which exists also inside the cluster. this service is capable of converting questions to sql then take the sql query, execute it inside infrastructure and push again the result to chatgpt to generate a html visual.",https://stackoverflow.com/questions/76070777,typescript,21-04-2023 07:13,649.0,1.0,1.0,True,26-04-2024 07:54,02-05-2023 09:39,Data Wrangling
72380308,split player and chat from chat log (text-mining),"i have a chat log which includes 4 players (a, b, c, d) and their chats in one row in my data frame (across many groups). i want to split each phrase into its own row and identify the speaker of that phrase in a separate column.
i have attempted many things using the following packages but haven't been able to succeed.
psych
dplyr
splitstackshape
tidytext
stringr
tidyr
the data frame is not a txt.document, but i'm thinking it needs to be?
for example this is what the chat log looks like. this is all in one row in my dataset.
[1] ""ï¿½ï¿½ï¿½*** d has joined the chat ***""                                                                                                                                         
  [2] ""ï¿½ï¿½ï¿½*** b has joined the chat ***""                                                                                                                                         
  [3] ""ï¿½ï¿½ï¿½*** a has joined the chat ***""                                                                                                                       
  [4] ""d: hi""                                                                                                                                                                  
  [5] ""b: hello!""                                                                                                                                                              
  [6] ""a: hi!""                                                                                                                                                                 
  [7] ""d: i think oxygen is most important""                                                                                                                                    
  [8] ""a: i do too""                                                                                                                                                            
  [9] ""ï¿½ï¿½ï¿½*** c has joined the chat ***""                                                                                                                                         
 [10] ""b: agreed, that was my #1""                                                                                                                                              
 [11] ""a: i didnt at first but then on second guess""                                                                                                                           
 [12] ""a: oxygen then water""                                                                                                                                                   
 [13] ""c: hi hi""                                                              

i want the following (to have these columns where each row is a new phrase)




player id
phrase




a
hi!


b
hello!




i want to eventually use this to count # of words/characters per player","['r', 'text-mining']",72382010,"library(dplyr)
library(tidyr)

d %>%
  t() %>%
  as.data.frame(""v1"") %>%
  filter(!grepl(""***"", v1, fixed = true)) %>%
  separate(v1, into = c(""playerid"", ""phrase""), sep = "": "") %>%
  mutate(count = nchar(phrase))

result:
#>   playerid                                    phrase count
#> 1        d                                        hi     2
#> 2        b                                    hello!     6
#> 3        a                                       hi!     3
#> 4        d          i think oxygen is most important    32
#> 5        a                                  i do too     8
#> 6        b                    agreed, that was my #1    22
#> 7        a i didnt at first but then on second guess    41
#> 8        a                         oxygen then water    17
#> 9        c                                     hi hi     5

you could use add this to the dplyr chain to count the number of characters per player:
group_by(playerid) %>%
summarize(total = sum(count))

#>   playerid total
#>   <chr>    <int>
#> 1 a           69
#> 2 b           28
#> 3 c            5
#> 4 d           34

data:
d <- structure(c("" *** d has joined the chat ***"", "" *** b has joined the chat ***"", 
                 "" *** a has joined the chat ***"", ""d: hi"", ""b: hello!"", ""a: hi!"", 
                 ""d: i think oxygen is most important"", ""a: i do too"", "" *** c has joined the chat ***"", 
                 ""b: agreed, that was my #1"", ""a: i didnt at first but then on second guess"", 
                 ""a: oxygen then water"", ""c: hi hi""), .dim = c(1l, 13l))

created on 2022-05-25 by the reprex package (v2.0.1)",https://stackoverflow.com/questions/72380308,r,25-05-2022 15:19,76.0,0.0,1.0,True,25-05-2022 17:57,25-05-2022 17:38,Implementation Issues
70496137,can we calculate feature importance in huggingface bert?,"we can fit a linearregression model on the regression dataset and retrieve the coeff_ property that contains the coefficients found for each input variable. these coefficients can provide the basis for a crude feature importance score. this assumes that the input variables have the same scale or have been scaled prior to fitting a model.

what about bert? can we get coef_ variable from the model and use it to calculate feature importance like linearregression model in text classification task?","['nlp', 'huggingface-transformers', 'bert-language-model']",70500609,captum is a prominent tool (from  pytorch/facebook) for interpreting transformers and you can get a score for the attention the model pays to specific tokens at specific layers. see a tutorial here:  or here,https://stackoverflow.com/questions/70496137,nlp,27-12-2021 13:29,2649.0,3.0,1.0,True,27-12-2021 21:30,27-12-2021 18:27,Uncategorized
65527427,extract named entities using spacy and python lambda,"i am using following code to extract named entities using lambda.
df['place'] = df['text'].apply(lambda x: [entity.text for entity in nlp(x).ents if entity.label_ == 'gpe'])

and
df['text'].apply(lambda x: ([entity.text for entity in nlp(x).ents if entity.label_ == 'gpe'] or [''])[0])

for a few hundred records it can extract results. but when it comes to thousands of records. it takes pretty much forever. can someone help me to optimize this line of code?","['python', 'nlp', 'spacy', 'named-entity-extraction']",65529502,"you may improve by:

calling nlp.pipe on the whole list of documents
disabling unnecessary pipes.

try:
import spacy
nlp = spacy.load(""en_core_web_md"", disable = [""tagger"",""parser""])

df = pd.dataframe({""text"":[""this is a text about germany"",""this is another about trump""]})

texts = df[""text""].to_list()
ents = []
for doc in nlp.pipe(texts):
    for ent in doc.ents:
        if ent.label_ == ""gpe"":
            ents.append(ent)
            
print(ents)


[germany]",https://stackoverflow.com/questions/65527427,python,01-01-2021 05:21,835.0,0.0,1.0,True,01-01-2021 17:19,01-01-2021 17:19,Data Wrangling
76000137,openai embeddings api: how embeddings work?,"there are quite a few tutorials on embeddings in openai. i can't understand how they work.
referring to  , an embedding is a vector or list. a string is passed to an embedding model and the model returns a number (in simplest terms). i can use this number(s).
if i use a simple string to get its embeddings, i get a massive list
result = get_embedding(""i live in space"", engine = ""textsearchcuriedoc001mc"")

result when printed
[5.4967957112239674e-05,
 -0.01301578339189291,
 -0.002223075833171606,
 0.013594076968729496,
 -0.027540158480405807,
 0.008867159485816956,
 0.009403547272086143,
 -0.010987567715346813,
 0.01919262297451496,
 0.022209804505109787,
 -0.01397960539907217,
 -0.012806257233023643,
 -0.027908924967050552,
 0.013074451126158237,
 0.024942029267549515,
 0.0200139675289392 , ..... -> truncated this much, much, much longer list 

question 1 - how is this massive list correlated with my 4-word text?
question 2 -
i create embeddings of the text i want to use in query. note that it is exactly the same as the text of original content i live in space
queryembedding = get_embedding(
        'i live in space',
        engine=""textsearchcuriequery001mc""
    )
queryembedding

when i run cosine similarity , the value is 0.42056650555103214.
similarity = cosine_similarity(embeddings_of_i_live,queryembedding)
similarity

i get value 0.42056650555103214
shouldn't the value be 1 to indicate identical value?","['azure', 'openai-api', 'azure-openai']",76003658,"q1:

how is this massive list correlated with my 4-word text?

a1: let's say you want to use the openai text-embedding-ada-002 model. no matter what your input is, you will always get a 1536-dimensional embedding vector (i.e., there are 1536 numbers inside). you are probably familiar with 3-dimensional space (i.e., x, y, z). well, this is a 1536-dimensional space, which is very hard to imagine. why are there exactly 1536 numbers inside the embedding vector? because the text-embedding-ada-002 model has an output dimension of 1536. it's pre-defined.

q2:

i create embeddings of the text i want to use in the query. note that it
is exactly the same as the text of the original content: i live in space.
when i run cosine similarity, the value is 0.42056650555103214.
should the value be 1 to indicate an identical value?

a2: yes, the value should be 1 if you calculate cosine similarity between two identical texts. see an example here.
for an example of semantic search based on embeddings, see this answer.",https://stackoverflow.com/questions/76000137,azure,12-04-2023 21:56,6159.0,4.0,1.0,True,04-07-2023 07:10,21-04-2023 15:50,Conceptual Questions
39850826,how can i convert pcfg in cnf for this grammar?,"given the following probabilistic context-free grammar - 
1.np -> adj n [0.6]
2.np -> n     [0.4] 
3.n  -> cat   [0.2] 
4.n  -> dog   [0.8]

what will be the cnf??","['nlp', 'stanford-nlp', 'cnf']",42428525,"given pcfg in cnf is given below.
1.np -> adj n [0.6]
2.np -> cat   [0.08] 
3.np -> dog   [0.32] 

because you need to get the same probability for the result by applying both the original and the converted set of rules (in cnf).",https://stackoverflow.com/questions/39850826,nlp,04-10-2016 11:13,1024.0,1.0,2.0,True,16-01-2021 02:13,23-02-2017 23:55,Implementation Issues
1787110,what is the difference between lemmatization vs stemming?,"when do i use each ?
also...is the nltk lemmatization dependent upon parts of speech?
wouldn't it be more accurate if it was?","['nlp', 'nltk', 'lemmatization']",1787121,"short and dense: 

the goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.
however, the two words differ in their flavor. stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .

from the nltk docs:

lemmatization and stemming are special cases of normalization. they identify a canonical representative for a set of related word forms.",https://stackoverflow.com/questions/1787110,nlp,24-11-2009 00:48,169254.0,207.0,15.0,True,26-02-2024 04:36,16-12-2021 09:03,Conceptual Questions
65674941,how do i leverage spark&#39;s pipelines to find phrases in strings then add feature category?,"i would like to search my text column in a pyspark data frame for phrases. here is an example to show you what i mean.
sentencedata = spark.createdataframe([
(0, ""hi i heard about spark""),
(4, ""i wish java could use case classes""),
(11, ""logistic regression models are neat"")], 
[""id"", ""sentence""])

if the sentence contains ""heard about spark"" then categoryspark=1 and categoryheard=1.
if the sentence contains ""java or regression"" then categorycool=1.
i have about 28 booleans (or maybe better if i use regex) to check for.
sentencedata.withcolumn('categorycool',sentencedata['sentence'].rlike('java | regression')).show()

returns:
+---+--------------------+------------+
| id|            sentence|categorycool|
+---+--------------------+------------+
|  0|hi i heard about ...|       false|
|  4|i wish java could...|        true|
| 11|logistic regressi...|        true|
+---+--------------------+------------+

this is what i want, but i'd like to add it to a pipeline as a transformation step.","['apache-spark', 'pyspark', 'nlp', 'feature-extraction']",65675709,"i found this nice medium article and this s.o. answer which i combined to answer my own question! i hope someone finds this helpful someday.
    from pyspark.ml.pipeline import transformer
    from pyspark.ml import pipeline
    from pyspark.sql.types import *
    from pyspark.ml.util import identifiable
    
    sentencedata = spark.createdataframe([
        (0, ""hi i heard about spark""),
        (4, ""i wish java could use case classes""),
        (11, ""logistic regression models are neat"")
    ], [""id"", ""sentence""])
    
    class onesearchmultilabelextractor(transformer):
        def __init__(self, rlikesearch, outputcols, inputcol = 'fulltext'):
            self.inputcol = inputcol
            self.outputcols = outputcols
            self.rlikesearch = rlikesearch
            self.uid = str(identifiable())
        def copy(extra):
            defaultcopy(extra)
        def check_input_type(self, schema):
            field = schema[self.inputcol]
            if (field.datatype != stringtype()):
                raise exception('onesearchmultilabelextractor input type %s did not match input type stringtype' % field.datatype)
        def check_output_type(self):
            if not (isinstance(self.outputcols,list)):
                raise exception('onesearchmultilabelextractor output columns must be a list')
        def _transform(self, df):
            self.check_input_type(df.schema)
            self.check_output_type()
            df = df.withcolumn(""searchresult"", df[self.inputcol].rlike(self.rlikesearch)).cache()
            for outputcol in self.outputcols:
                df = df.withcolumn(outputcol, df[""searchresult""])
            return df.drop(""searchresult"")
            
    dex = coolextractor(inputcol='sentence',rlikesearch='java | regression',outputcols=['coolcategory'])
    featurespipeline =  pipeline(stages=[dex])
    featpip = featurespipeline.fit(sentencedata)
    featpip.transform(sentencedata).show()",https://stackoverflow.com/questions/65674941,apache-spark,11-01-2021 21:42,162.0,0.0,1.0,True,13-01-2021 17:56,12-01-2021 17:09,Implementation Issues
74875388,how do i extract the text of a single page with pypdf2?,"i have a document library which consists of several hundred pdf documents. i am attempting to export the first page of each pdf document. below is my script which extracts the page. it saves each page as an individual pdf. however, the files which are exported seem to be exporting in unreadable or damaged format.
is there something missing from my script?
import os
from pypdf2 import pdfreader, pdfwriter

# get the file names in the directory
input_directory = ""fund_docs_sample""
entries = os.listdir(input_directory)
output_directory = ""first pages""
outputs = os.listdir(output_directory)

for output_file_name in entries:
    reader = pdfreader(input_directory + ""/"" + output_file_name)
    page = reader.pages[0]
    first_page = ""\n"" + page.extract_text() + ""\n""

    with open(output_file_name, ""wb"") as outputstream:
        pdf_writer = pdfwriter(output_file_name + first_page)","['python', 'data-science', 'text-mining', 'pypdf']",74882928,"you're missing pdf_writer.write(outputstream)
do you want to write a text file (containing the extracted text) or a pdf file (containing the first page of the pdf)?
you seem to overwrite the files of the input
output_directory is not used at all

after reading the comments, you likely want this:
from pathlib import path
from pypdf2 import pdfreader

# get the file names in the directory
input_directory = path(""fund_docs_sample"")
output_directory = path(""first pages"")

for input_file_path in input_directory.glob(""*.pdf""):
    print(input_file_path)
    reader = pdfreader(input_file_path)
    page = reader.pages[0]
    first_page_text = ""\n"" + page.extract_text() + ""\n""
    
    # create the output text file path
    output_file_path = output_directory / f""{input_file_path.name}.txt""
    
    # write the text to the output file
    with open(output_file_path, ""w"") as output_file:
        output_file.write(first_page_text)",https://stackoverflow.com/questions/74875388,python,21-12-2022 11:29,1441.0,0.0,2.0,True,23-12-2022 11:20,23-12-2022 11:20,Implementation Issues
69564293,pyspark - counting particular words in sentences,"i have a pyspark dataframe with a column that contains textual content.
i am trying to count the number of sentences that contain an exclamation mark '!' along with the word ""like"" and ""want"".
for example: the column with a row that contains the following sentences:
i don't like to sing!
i like to go shopping!
i want to go home!
i like fast food. 
you don't want to!
what does he want?

the desired output i'm hoping to achieve would like something like this (only counts the sentences that contain ""like"" or ""want"" and ""!""):
+----+-----+
|word|count|
+----+-----+
|like|   2 |
|want|   2 |
+----+-----+

can someone help me with writing a udf that can do this? this is what i have written so far, but i can't seem to get it to work.
nltk.tokenize import sent_tokenize

def convert_a_sentence(a_string):
    sentence = lower(nltk.sent_tokenize(a_string))
    return sentence

df = df.withcolumn('a_sentence', convert_a_sentence(df['text']))

df.select(explode('a_sentence').alias('found')).filter(df['a_sentence'].isin('like', 'want', '!').groupby('found').count().collect()","['python', 'apache-spark', 'pyspark', 'data-science', 'nltk']",69564772,"if all you want is uni-gram (i.e 1 token), you can just split the sentence by space, then explode, group by, count then filter what you wants
(df
    .withcolumn('words', f.split('sentence', ' '))
    .withcolumn('word', f.explode('words'))
    .groupby('word')
    .agg(
        f.count('*').alias('word_cnt')
    )
    .where(f.col('word').isin(['like', 'want']))
    .show()
)

# output
# +----+--------+
# |word|word_cnt|
# +----+--------+
# |want|       2|
# |like|       3|
# +----+--------+

note #1: you can apply filter before groupby, with contains function
note #2: if you ever want to do n-gram instead of ""hacking"" like above, you can consider using sparkml package with tokenizer
from pyspark.ml.feature import tokenizer

tokenizer = tokenizer(inputcol='sentence', outputcol=""words"")
tokenized = tokenizer.transform(df)

# output
# +----------------------+----------------------------+
# |sentence              |words                       |
# +----------------------+----------------------------+
# |i don't like to sing! |[i, don't, like, to, sing!] |
# |i like to go shopping!|[i, like, to, go, shopping!]|
# |i want to go home!    |[i, want, to, go, home!]    |
# |i like fast food.     |[i, like, fast, food.]      |
# |you don't want to!    |[you, don't, want, to!]     |
# |what does he want?    |[what, does, he, want?]     |
# +----------------------+----------------------------+

or ngram
from pyspark.ml.feature import ngram

ngram = ngram(n=2, inputcol=""words"", outputcol=""ngrams"")
ngramed = ngram.transform(tokenized)

# output
# +----------------------+----------------------------+----------------------------------------+
# |col                   |words                       |ngrams                                  |
# +----------------------+----------------------------+----------------------------------------+
# |i don't like to sing! |[i, don't, like, to, sing!] |[i don't, don't like, like to, to sing!]|
# |i like to go shopping!|[i, like, to, go, shopping!]|[i like, like to, to go, go shopping!]  |
# |i want to go home!    |[i, want, to, go, home!]    |[i want, want to, to go, go home!]      |
# |i like fast food.     |[i, like, fast, food.]      |[i like, like fast, fast food.]         |
# |you don't want to!    |[you, don't, want, to!]     |[you don't, don't want, want to!]       |
# |what does he want?    |[what, does, he, want?]     |[what does, does he, he want?]          |
# +----------------------+----------------------------+----------------------------------------+",https://stackoverflow.com/questions/69564293,python,14-10-2021 02:07,434.0,0.0,2.0,True,14-10-2021 03:44,14-10-2021 03:01,Implementation Issues
76056193,tokenclassificationchunkpipeline is throwing error: &#39;batchencoding&#39; object is not an iterator,"following this huggingface anonymisation tutorial.
using pytorch 2.0.0 and transformers-4.28.1
running the code as it is, i get an error over the custom pipeline:
def anonymize(text):
    ents = pipe(text) # this errors out
    ...
typeerror: 'batchencoding' object is not an iterator

i realise it's a tokenizer issue,
class tokenclassificationchunkpipeline(tokenclassificationpipeline):
def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)

def preprocess(self, sentence, offset_mapping=none):
    model_inputs = self.tokenizer(
        sentence,
        return_tensors=""pt"",
        truncation=true,
        return_special_tokens_mask=true,
        return_offsets_mapping=true,
        return_overflowing_tokens=true,  # return multiple chunks
        max_length=self.tokenizer.model_max_length,
        padding=true
    )
    if offset_mapping:
        model_inputs[""offset_mapping""] = offset_mapping

    model_inputs[""sentence""] = sentence

    return model_inputs

this model_inputs is a

<class 'transformers.tokenization_utils_base.batchencoding'>

how can i make an iterator batchencoding object?
else, is there another way?
for full code, please visit the tutorial link above.","['pytorch', 'nlp', 'huggingface-transformers', 'torch', 'named-entity-recognition']",76058924,"not sure why the pipeline was coded that way in the blogpost, but here's a working version:
import torch
from transformers import autotokenizer, automodelfortokenclassification
from transformers.pipelines.token_classification import tokenclassificationpipeline

model_checkpoint = ""davlan/bert-base-multilingual-cased-ner-hrl""

tokenizer = autotokenizer.from_pretrained(model_checkpoint)
model = automodelfortokenclassification.from_pretrained(model_checkpoint)


class tokenclassificationchunkpipeline(tokenclassificationpipeline):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def preprocess(self, sentence, offset_mapping=none, **preprocess_params):
        tokenizer_params = preprocess_params.pop(""tokenizer_params"", {})
        truncation = true if self.tokenizer.model_max_length and self.tokenizer.model_max_length > 0 else false
        inputs = self.tokenizer(
            sentence,
            return_tensors=""pt"",
            truncation=true,
            return_special_tokens_mask=true,
            return_offsets_mapping=true,
            return_overflowing_tokens=true,  # return multiple chunks
            max_length=self.tokenizer.model_max_length,
            padding=true
        )
        #inputs.pop(""overflow_to_sample_mapping"", none)
        num_chunks = len(inputs[""input_ids""])

        for i in range(num_chunks):
            if self.framework == ""tf"":
                model_inputs = {k: tf.expand_dims(v[i], 0) for k, v in inputs.items()}
            else:
                model_inputs = {k: v[i].unsqueeze(0) for k, v in inputs.items()}
            if offset_mapping is not none:
                model_inputs[""offset_mapping""] = offset_mapping
            model_inputs[""sentence""] = sentence if i == 0 else none
            model_inputs[""is_last""] = i == num_chunks - 1
            yield model_inputs

    def _forward(self, model_inputs):
        # forward
        special_tokens_mask = model_inputs.pop(""special_tokens_mask"")
        offset_mapping = model_inputs.pop(""offset_mapping"", none)
        sentence = model_inputs.pop(""sentence"")
        is_last = model_inputs.pop(""is_last"")

        overflow_to_sample_mapping = model_inputs.pop(""overflow_to_sample_mapping"")

        output = self.model(**model_inputs)
        logits = output[""logits""] if isinstance(output, dict) else output[0]


        model_outputs = {
            ""logits"": logits,
            ""special_tokens_mask"": special_tokens_mask,
            ""offset_mapping"": offset_mapping,
            ""sentence"": sentence,
            ""overflow_to_sample_mapping"": overflow_to_sample_mapping,
            ""is_last"": is_last,
            **model_inputs,
        }

        # we reshape outputs to fit with the postprocess inputs
        model_outputs[""input_ids""] = torch.reshape(model_outputs[""input_ids""], (1, -1))
        model_outputs[""token_type_ids""] = torch.reshape(model_outputs[""token_type_ids""], (1, -1))
        model_outputs[""attention_mask""] = torch.reshape(model_outputs[""attention_mask""], (1, -1))
        model_outputs[""special_tokens_mask""] = torch.reshape(model_outputs[""special_tokens_mask""], (1, -1))
        model_outputs[""offset_mapping""] = torch.reshape(model_outputs[""offset_mapping""], (1, -1, 2))

        return model_outputs


pipe = tokenclassificationchunkpipeline(model=model, tokenizer=tokenizer, aggregation_strategy=""simple"")

pipe(""bernard works at bnp paribas in paris."")


[out]:
[{'entity_group': 'per',
  'score': 0.9994497,
  'word': 'bernard',
  'start': 0,
  'end': 7},
 {'entity_group': 'org',
  'score': 0.9997708,
  'word': 'bnp paribas',
  'start': 17,
  'end': 28},
 {'entity_group': 'loc',
  'score': 0.99906,
  'word': 'paris',
  'start': 32,
  'end': 37}]

for reference, take a look at how the preproces() and the _forward() functions are coded in the tokenclassificationpipeline class, 
the preprocess should return a generator, that's why the _forward is expecting a generator and complains typeerror: 'batchencoding' object is not an iterator.",https://stackoverflow.com/questions/76056193,pytorch,19-04-2023 15:17,1134.0,1.0,1.0,True,19-04-2023 21:26,19-04-2023 21:26,Implementation Issues
8897593,how to compute the similarity between two text documents?,i want to take two documents and determine how similar they are. any programming language if fine but i prefer python.,"['python', 'nlp']",8897648,"the common way of doing this is to transform the documents into tf-idf vectors and then compute the cosine similarity between them. any textbook on information retrieval (ir) covers this. see esp. introduction to information retrieval, which is free and available online.
computing pairwise similarities
tf-idf (and similar text transformations) are implemented in the python packages gensim and scikit-learn. in the latter package, computing cosine similarities is as easy as
from sklearn.feature_extraction.text import tfidfvectorizer

documents = [open(f).read() for f in text_files]
tfidf = tfidfvectorizer().fit_transform(documents)
# no need to normalize, since vectorizer will return normalized tf-idf
pairwise_similarity = tfidf * tfidf.t

or, if the documents are plain strings,
>>> corpus = [""i'd like an apple"", 
...           ""an apple a day keeps the doctor away"", 
...           ""never compare an apple to an orange"", 
...           ""i prefer scikit-learn to orange"", 
...           ""the scikit-learn docs are orange and blue""]                                                                                                                                                                                                   
>>> vect = tfidfvectorizer(min_df=1, stop_words=""english"")                                                                                                                                                                                                   
>>> tfidf = vect.fit_transform(corpus)                                                                                                                                                                                                                       
>>> pairwise_similarity = tfidf * tfidf.t 

though gensim may have more options for this kind of task.
see also this question.
[disclaimer: i was involved in the scikit-learn tf-idf implementation.]
interpreting the results
from above, pairwise_similarity is a scipy sparse matrix that is square in shape, with the number of rows and columns equal to the number of documents in the corpus.
>>> pairwise_similarity                                                                                                                                                                                                                                      
<5x5 sparse matrix of type '<class 'numpy.float64'>'
    with 17 stored elements in compressed sparse row format>

you can convert the sparse array to a numpy array via .toarray() or .a:
>>> pairwise_similarity.toarray()                                                                                                                                                                                                                            
array([[1.        , 0.17668795, 0.27056873, 0.        , 0.        ],
       [0.17668795, 1.        , 0.15439436, 0.        , 0.        ],
       [0.27056873, 0.15439436, 1.        , 0.19635649, 0.16815247],
       [0.        , 0.        , 0.19635649, 1.        , 0.54499756],
       [0.        , 0.        , 0.16815247, 0.54499756, 1.        ]])

let's say we want to find the document most similar to the final document, ""the scikit-learn docs are orange and blue"".  this document has index 4 in corpus.  you can find the index of the most similar document by taking the argmax of that row, but first you'll need to mask the 1's, which represent the similarity of each document to itself.  you can do the latter through np.fill_diagonal(), and the former through np.nanargmax():
>>> import numpy as np     
                                                                                                                                                                                                                                  
>>> arr = pairwise_similarity.toarray()     
>>> np.fill_diagonal(arr, np.nan)                                                                                                                                                                                                                            
                                                                                                                                                                                                                 
>>> input_doc = ""the scikit-learn docs are orange and blue""                                                                                                                                                                                                  
>>> input_idx = corpus.index(input_doc)                                                                                                                                                                                                                      
>>> input_idx                                                                                                                                                                                                                                                
4

>>> result_idx = np.nanargmax(arr[input_idx])                                                                                                                                                                                                                
>>> corpus[result_idx]                                                                                                                                                                                                                                       
'i prefer scikit-learn to orange'

note: the purpose of using a sparse matrix is to save (a substantial amount of space) for a large corpus & vocabulary.  instead of converting to a numpy array, you could do:
>>> n, _ = pairwise_similarity.shape                                                                                                                                                                                                                         
>>> pairwise_similarity[np.arange(n), np.arange(n)] = -1.0
>>> pairwise_similarity[input_idx].argmax()                                                                                                                                                                                                                  
3",https://stackoverflow.com/questions/8897593,python,17-01-2012 15:51,316592.0,287.0,14.0,True,14-03-2025 00:25,14-03-2025 00:25,Implementation Issues
72912929,huggingface - finetuning in tensorflow with custom datasets,"i have been battling with my own implementation on my dataset with a different transformer model than the tutorial, and i have been getting this error attributeerror: 'nonetype' object has no attribute 'dtype', when i was starting to train my model. i have been trying to debug for hours, and then i have tried the tutorial from hugging face as it can be found here  running this exact code, so i could identify my mistake, also leads to the same error.
!wget 
!tar -xf aclimdb_v1.tar.gz

from pathlib import path
def read_imdb_split(split_dir):
    split_dir = path(split_dir)
    texts = []
    labels = []
    for label_dir in [""pos"", ""neg""]:
        for text_file in (split_dir/label_dir).iterdir():
            texts.append(text_file.read_text())
            labels.append(0 if label_dir is ""neg"" else 1)

    return texts, labels

train_texts, train_labels = read_imdb_split('aclimdb/train')
test_texts, test_labels = read_imdb_split('aclimdb/test')

from sklearn.model_selection import train_test_split
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)

from transformers import distilberttokenizerfast
tokenizer = distilberttokenizerfast.from_pretrained('distilbert-base-uncased')

train_encodings = tokenizer(train_texts, truncation=true, padding=true)
val_encodings = tokenizer(val_texts, truncation=true, padding=true)
test_encodings = tokenizer(test_texts, truncation=true, padding=true)

import tensorflow as tf

train_dataset = tf.data.dataset.from_tensor_slices((
    dict(train_encodings),
    train_labels
))
val_dataset = tf.data.dataset.from_tensor_slices((
    dict(val_encodings),
    val_labels
))
test_dataset = tf.data.dataset.from_tensor_slices((
    dict(test_encodings),
    test_labels
))

from transformers import tfdistilbertforsequenceclassification

model = tfdistilbertforsequenceclassification.from_pretrained('distilbert-base-uncased')

optimizer = tf.keras.optimizers.adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn
model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)

my goal will be to perform multi-label text classification on my own custom dataset, which unfortunately i cannot share for privacy reasons. if anyone could point out what is wrong with this implementation, will be highly appreciated.","['tensorflow', 'huggingface-transformers', 'transfer-learning', 'huggingface-tokenizers', 'fine-tuning']",72918008,"there seems to be an error, when you are passing the loss parameter.
model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn

you don't need to pass the loss parameter, if you want to use the model's built-in loss function.
i was able to train the model with your provided source code by changing mentioned line to:
model.compile(optimizer=optimizer)

or by passing a loss function
loss_fn = tf.keras.losses.sparsecategoricalcrossentropy(from_logits=true)

model.compile(optimizer=optimizer, loss=loss_fn)

transformers version: 4.20.1
hope it helps.",https://stackoverflow.com/questions/72912929,tensorflow,08-07-2022 14:28,854.0,0.0,1.0,True,26-06-2024 17:34,09-07-2022 05:20,Implementation Issues
70783834,how can i categorize tweets with google cloud natural language api - if possible?,"i am trying to use google cloud natural language api to classify/categorize tweets in order to filter out tweets that are not relevant to my audience (weather related). i can understand it must be tricky for an ai solution to make a classification on a short amount of text but i would imagine it would at least have a guess on text like this:

wind chills of zero to -5 degrees are expected in northwestern
arkansas into north-central arkansas extending into portions of
northern oklahoma during the 6-9am window . #arwx #okwx

i have tested several tweets but only very few get a categorization, the rest gets no result (or ""no categories found. try a longer text input."" if i try it through the gui).
is it pointless to hope for this to work? or, is it possible to decrease the threshold for the categorization? an ""educated guess"" from the nlp-solution would be better than no filter at all. is there an alternate solution (outside training my own nlp-model)?
edit: in order to clarify:
i am, in the end, using the google cloud platform natural language api in order to classify tweets. in order to test it i am using the gui (linked above). i can see that quite few of the tweets i test (in the gui) gets a categorization from gcp nlp, i.e. the category is empty.
the desired state i want is for gcp nlp to provide a category guess of a tweet text, rather than providing an empty result. i assume the nlp model removes any results with a confidence less than x%. it would be interesting to know if that threshold could be configured.
i assume the categorization of tweets must have been done before, and if there is any other way to solve this?
edit 2: classifytweet-code:
async function classifytweet(tweettext) {
   const language = require('@google-cloud/language');
   const client = new language.languageserviceclient({projectid, keyfilename});
   //const tweettext = ""some light snow dusted the ground this morning, adding to the intense snow fall of yesterday. here at my warwick station the numbers are in, new snow 19.5cm and total depth 26.6cm. a very good snow event. photos to be posted. #onstorm #canwarnon4464 #cocorahson525""
   const document = {
      content: tweettext,
      type: 'plain_text',
   };   
   const [classification] = await client.classifytext({document});
   
   console.log('categories:');
   classification.categories.foreach(category => {
     console.log(`name: ${category.name}, confidence: ${category.confidence}`);
   });
   
   return classification.categories
}","['google-cloud-platform', 'nlp', 'google-natural-language']",70897988,"i have dig on the current state of cloud natural language and my answer to your principal question will be that at the current state of the natural language classify text is not possible. although, a workaround would be if you base your categories on the output you get from analyzing the text from your inputs.
consider that we are not using a custom model for this and just using the options that cloud natural language offers, one tentative approach on this matter will be as follows:
to start, i have updated the code from the official samples to our needs to explain a bit further on this:
from google.cloud import language_v1 
from google.cloud.language_v1 import enums 


def sample_cloud_natural_language_text(text_content):
    """""" 
    args:
      text_content the text content to analyze. must include at least 20 words.
    """"""

    client = language_v1.languageserviceclient()
    type_ = enums.document.type.plain_text

    language = ""en""
    document = {""content"": text_content, ""type"": type_, ""language"": language}


    print(""=====classify text====="")
    response = client.classify_text(document)
    for category in response.categories:
        print(u""category name: {}"".format(category.name))
        print(u""confidence: {}"".format(category.confidence))


    print(""=====analyze text====="")
    response = client.analyze_entities(document)
    for entity in response.entities:
        print(f"">>>>> entity {entity.name}"")  
        print(u""entity type: {}"".format(enums.entity.type(entity.type).name))
        print(u""salience score: {}"".format(entity.salience))

        for metadata_name, metadata_value in entity.metadata.items():
            print(u""{}: {}"".format(metadata_name, metadata_value))

        for mention in entity.mentions:
            print(u""mention text: {}"".format(mention.text.content))
            print(u""mention type: {}"".format(enums.entitymention.type(mention.type).name))


if __name__ == ""__main__"":
    #text_content = ""that actor on tv makes movies in hollywood and also stars in a variety of popular new tv shows.""
    text_content=""wind chills of zero to -5 degrees are expected in northwestern arkansas into north-central arkansas extending into portions of northern oklahoma during the 6-9am window""
    
    sample_cloud_natural_language_text(text_content)

output
=====classify text=====
=====analyze text=====
>>>>> entity wind chills
entity type: other
salience score: 0.46825599670410156
mention text: wind chills
mention type: common
>>>>> entity degrees
entity type: other
salience score: 0.16041776537895203
mention text: degrees
mention type: common
>>>>> entity northwestern arkansas
entity type: organization
salience score: 0.07702474296092987
mid: /m/02vvkn4
wikipedia_url: 
mention text: northwestern arkansas
mention type: proper
>>>>> entity north
entity type: location
salience score: 0.07702474296092987
mention text: north
mention type: proper
>>>>> entity arkansas
entity type: location
salience score: 0.07088913768529892
mid: /m/0vbk
wikipedia_url: 
mention text: arkansas
mention type: proper
>>>>> entity window
entity type: other
salience score: 0.06348973512649536
mention text: window
mention type: common
>>>>> entity oklahoma
entity type: location
salience score: 0.04747137427330017
wikipedia_url: 
mid: /m/05mph
mention text: oklahoma
mention type: proper
>>>>> entity portions
entity type: other
salience score: 0.03542650490999222
mention text: portions
mention type: common
>>>>> entity 6
entity type: number
salience score: 0.0
value: 6
mention text: 6
mention type: type_unknown
>>>>> entity 9
entity type: number
salience score: 0.0
value: 9
mention text: 9
mention type: type_unknown
>>>>> entity -5
entity type: number
salience score: 0.0
value: -5
mention text: -5
mention type: type_unknown
>>>>> entity zero
entity type: number
salience score: 0.0
value: 0
mention text: zero
mention type: type_unknown

as you can see, classify text do not helps a lot (the result its empty). its when we start to analyze text that we can get some values. we can use that to build or own categories. the trick (and hard-work too) will be to make the pool of key words that will fit each category (a category built by us) that we can use to set the data that we are analyzing. about categorization, we can check the current list of available categories made by google to have an idea of what categories should look like.
i don't think there is a feature to lower the bar yet implemented with current builds but its something than can be requested to google as a feature.",https://stackoverflow.com/questions/70783834,google-cloud-platform,20-01-2022 09:37,611.0,2.0,1.0,True,31-01-2022 12:11,31-01-2022 12:11,Conceptual Questions
77546477,how to build a model and train it with tensorflow keras sub classing,"i have written a custom encoder and decoder layers that implements the architecture described in the attention is all you need paper. everything works fine until i trying compiling it, i get one error. if i run it with a sample data it compiles but then when i call the fit method to train the model it throws another error. i'm going to provide the blocks that i might be implementing incorrectly and let me know if more code is needed to debug.
tf version: 2.14.0
multi-head sub layer and positional encoding layer:
class mhasublayer(layer):
  def __init__(self, units, **kwargs):
    super().__init__()
    self.mha = multiheadattention(key_dim=units, **kwargs)
    self.inner_dense = timedistributed(dense(2048, activation='relu'))
    self.outer_dense = timedistributed(dense(units))
    self.layernorm_mha = layernormalization()
    self.layernorm_ff = layernormalization()
    self.add = add()

  def call(self, x, context, **kwargs):
    ### calculate attention output
    attn_out, attn_scores = self.mha(query=x, value=context, return_attention_scores=true, **kwargs)

    attn_resid_cnxt = self.add([x, attn_out])  ## residual connection
    attn_layer_norm = self.layernorm_mha(attn_resid_cnxt) 

    attn_scores = tf.reduce_mean(attn_scores, axis=1)
    self.last_attention_weights = attn_scores

    ### pass the attention output to the dense layer
    dense_out = self.outer_dense(self.inner_dense(attn_layer_norm))
    dense_resid_cnxt = self.add([attn_layer_norm, dense_out])  ### feed forward residual connection

    dense_layer_norm = self.layernorm_ff(dense_resid_cnxt)
    return dense_layer_norm

class positionalencodinglayer(layer):
  def __init__(self, **kwargs):
    super().__init__()
    self.add = add()

  def get_positional_encodings(self, x):
    seq_len = x.shape[0]
    d = x.shape[1]
    
    p = np.zeros((seq_len, d))

    for k in range(seq_len):
        for i in np.arange(int(d/2)):
            denominator = np.power(10000, 2*i/d)
            p[k, 2*i] = np.sin(k/denominator)
            p[k, 2*i+1] = np.cos(k/denominator)
    return p
  
  def call(self, x):

    # pos_enc = []
    pos_enc = tf.map_fn(fn=self.get_positional_encodings, elems=x)

    # for n, elm in enumerate(x):
    #   p = self.get_positional_encodings(elm)
    #   pos_enc.append(p)
    
    # pos_enc = tf.convert_to_tensor(pos_enc)

    pos_embeddings = self.add([x, pos_enc])
    return pos_embeddings

encoder-decoder block:
class encoder(layer):
  def __init__(self, units, embed_input_dim, name='encoder', **kwargs):
    super().__init__()

    ### encoder input embedding and layer
    self.embedding = embedding(input_dim=embed_input_dim, output_dim=units, name='en_embed_layer')
    self.pos_embedding = positionalencodinglayer(name='en_positional_embed_layer')

    ### encoder multi-head self attention sub layer
    self.mha_sub_layer1 = mhasublayer(units, num_heads=8, name='en_mha_layer_1')
    self.mha_sub_layer2 = mhasublayer(units, num_heads=8, name='en_mha_layer_2')
    self.mha_sub_layer3 = mhasublayer(units, num_heads=8, name='en_mha_layer_3')
    self.mha_sub_layer4 = mhasublayer(units, num_heads=8, name='en_mha_layer_4')
    self.mha_sub_layer5 = mhasublayer(units, num_heads=8, name='en_mha_layer_5')
    self.mha_sub_layer6 = mhasublayer(units, num_heads=8, name='en_mha_layer_6')

    ### encoder mha dropout layer
    self.dropout =  dropout(rate=0.1, name='en_dropout_pos_enc')
    self.dropout1 = dropout(rate=0.1, name='en_dropout_layer1')
    self.dropout2 = dropout(rate=0.1, name='en_dropout_layer2')
    self.dropout3 = dropout(rate=0.1, name='en_dropout_layer3')
    self.dropout4 = dropout(rate=0.1, name='en_dropout_layer4')
    self.dropout5 = dropout(rate=0.1, name='en_dropout_layer5')
    self.dropout6 = dropout(rate=0.1, name='en_dropout_layer6')

  def call(self, x):
    embedding_output = self.embedding(x)

    positional_embedding = self.pos_embedding(embedding_output)
    postitional_embedding = self.dropout(positional_embedding)

    ### first mha sub-layer
    sub_layer1_out = self.mha_sub_layer1(positional_embedding, positional_embedding)
    sub_layer1_out = self.dropout1(sub_layer1_out)

    ### second mha sub-layer
    sub_layer2_out = self.mha_sub_layer2(sub_layer1_out, sub_layer1_out)
    sub_layer2_out = self.dropout2(sub_layer2_out)

    ### third mha sub-layer
    sub_layer3_out = self.mha_sub_layer3(sub_layer2_out, sub_layer2_out)
    sub_layer3_out = self.dropout3(sub_layer3_out)

    ### fourth mha sub-layer
    sub_layer4_out = self.mha_sub_layer4(sub_layer3_out, sub_layer3_out)
    sub_layer4_out = self.dropout4(sub_layer4_out)

    ### fifth mha sub-layer
    sub_layer5_out = self.mha_sub_layer5(sub_layer4_out, sub_layer4_out)
    sub_layer5_out = self.dropout5(sub_layer5_out)

    ### sixth mha sub-layer
    sub_layer6_out = self.mha_sub_layer6(sub_layer5_out, sub_layer5_out)
    sub_layer6_out = self.dropout6(sub_layer6_out)

    return sub_layer6_out

class decoder(layer):
  def __init__(self, units, embed_input_dim, name='decoder', **kwargs):
    super().__init__()
    ### decoder input embedding layer
    self.embedding = embedding(input_dim=embed_input_dim, output_dim=units, name='de_embed_layer')
    self.pos_embedding = positionalencodinglayer(name='de_positional_embed_layer')

    ### decoder multi-head attention sub layer
    self.mha_sub_layer1 = mhasublayer(units, num_heads=8, name='de_mha_layer_1')
    self.mha_sub_layer2 = mhasublayer(units, num_heads=8, name='de_mha_layer_2')
    self.mha_sub_layer3 = mhasublayer(units, num_heads=8, name='de_mha_layer_3')
    self.mha_sub_layer4 = mhasublayer(units, num_heads=8, name='de_mha_layer_4')
    self.mha_sub_layer5 = mhasublayer(units, num_heads=8, name='de_mha_layer_5')
    self.mha_sub_layer6 = mhasublayer(units, num_heads=8, name='de_mha_layer_6')

    ### decoder mha droput layer
    self.dropout =  dropout(rate=0.1, name='de_dropout_pos_enc')
    self.dropout1 = dropout(rate=0.1, name='de_dropout_layer1')
    self.dropout2 = dropout(rate=0.1, name='de_dropout_layer2')
    self.dropout3 = dropout(rate=0.1, name='de_dropout_layer3')
    self.dropout4 = dropout(rate=0.1, name='de_dropout_layer4')
    self.dropout5 = dropout(rate=0.1, name='de_dropout_layer5')
    self.dropout6 = dropout(rate=0.1, name='de_dropout_layer6')

    ### dense output layer
    self.output_dense_layer = timedistributed(dense(1), name=""output_layer"")

  def call(self, x, en_context):
    embedding_output = self.embedding(x)
    positional_embedding = self.pos_embedding(embedding_output)
    postitional_embedding = self.dropout(positional_embedding)

    ### first mha sub-layer
    sub_layer1_out = self.mha_sub_layer1(positional_embedding, positional_embedding)
    sub_layer1_out = self.dropout1(sub_layer1_out)

    ### second mha sub-layer
    sub_layer2_out = self.mha_sub_layer2(sub_layer1_out, en_context)
    sub_layer2_out = self.dropout2(sub_layer2_out)

    ### third mha sub-layer
    sub_layer3_out = self.mha_sub_layer3(sub_layer2_out, en_context)
    sub_layer3_out = self.dropout3(sub_layer3_out)

    ### fourth mha sub-layer
    sub_layer4_out = self.mha_sub_layer4(sub_layer3_out, en_context)
    sub_layer4_out = self.dropout4(sub_layer4_out)

    ### fifth mha sub-layer
    sub_layer5_out = self.mha_sub_layer5(sub_layer4_out, en_context)
    sub_layer5_out = self.dropout5(sub_layer5_out)

    ### sixth mha sub-layer
    sub_layer6_out = self.mha_sub_layer6(sub_layer5_out, en_context)
    sub_layer6_out = self.dropout6(sub_layer6_out)

    ### output dense layer
    output = self.output_dense_layer(sub_layer6_out)
    output = tf.round(tf.abs(output))
    return output

sample data:
np.random.seed(42)
trainx = np.random.randint(0, high=250, size=(5,12))
trainxt_in = np.random.randint(0, high=250, size=(5,3))
trainy = np.random.randint(0, high=250, size=(5,3,1))

modelling block:
training shape: ((1616304, 12), (1616304, 3), (1616304, 3, 1))

## the model sub-class

class trxster(model):
  def __init__(self, units, en_embed_dim, de_embed_dim, name='trxster', **kwargs):
    super().__init__()
    self.encoder = encoder(units, en_embed_dim)
    self.decoder = decoder(units, de_embed_dim)

  def call(self, inputs):
    context_vec, target_in = inputs
    context = self.encoder(context_vec)
    preds = self.decoder(target_in, context)
    return preds

forecastor = trxster(hsize, embed_dim, embed_dim)
forecastor.build(((12, 1),(3, 1)))
forecastor.summary()

error-1:

typeerror: error converting shape to a tensorshape: dimension value
must be integer or none or have an index method, got value '(12,
1)' with type '<class 'tuple'>'.

if run the model with an example:
hsize = 512
embed_dim = 268

forecastor = trxster(hsize, embed_dim, embed_dim)
forecastor((trainx, trainxt_in))

model: ""trxster_11""
_________________________________________________________________
 layer (type)                output shape              param #   
=================================================================
 encoder_13 (encoder)        multiple                  63156224  
                                                                 
 decoder_13 (decoder)        multiple                  63156737  
                                                                 
=================================================================
total params: 126312961 (481.85 mb)
trainable params: 126312961 (481.85 mb)
non-trainable params: 0 (0.00 byte)
_________________________________________________________________

### fit the model
batch_size = 64
epochs = 100
steps = trainx.shape[0]//batch_size
warmup_steps = steps//25

class mylrschedule(tf.keras.optimizers.schedules.learningrateschedule):
  def __init__(self, d_model, warmup_steps):
    self.d_model = d_model
    self.warmup_steps = warmup_steps

  def __call__(self, step):
    step_num = step.numpy()
    self.lr = []

    denom = self.d_model**(-0.5)
    numer = min(step_num**(-0.5), step_num*(self.warmup_steps**(-1.5)))
    lrate = np.divide(numer, denom)
    self.lr.append(lrate)

    return lrate

opt = tf.keras.optimizers.adam(learning_rate=mylrschedule(hsize, warmup_steps), beta_1=0.9, beta_2=0.98, epsilon=1e-8)

### configure trxster
checkpoint_filepath = './training_ckpt'

cb = [tf.keras.callbacks.earlystopping(patience=10, 
                                        monitor='val_loss',
                                        restore_best_weights=true),
      tf.keras.callbacks.modelcheckpoint(
                                        filepath=checkpoint_filepath,
                                        save_weights_only=true,
                                        monitor='val_loss',
                                        mode='min',
                                        verbose=1,
                                        save_best_only=true)]

loss = tf.keras.losses.meansquarederror()
metrics = [tf.keras.metrics.accuracy(), tf.keras.losses.meanabsoluteerror()]

forecastor.compile(optimizer=opt,
                   loss='mean_squared_error',
                   metrics=['acc','mean_absolute_error'])

history = forecastor.fit((trainx, trainxt_in), trainy,
                          batch_size=batch_size,
                          steps_per_epoch=steps,
                          epochs=1,
                          validation_data=((valx, valxt_in), valy),
                          callbacks=cb)

error-2: providing few lines of error trace:

valueerror: no gradients provided for any variable:
(['trxster_11/encoder_13/en_embed_layer/embeddings:0',
'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/query/kernel:0',
'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/query/bias:0',
'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/key/kernel:0',
'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/key/bias:0',
'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/value/kernel:0',
'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/value/bias:0',
'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/attention_output/kernel:0',
'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/attention_output/bias:0',
'trxster_11/encoder_13/mha_sub_layer_157/time_distributed_314/kernel:0'

every examples i see tells me that it should work just the way i have written but it isn't so.","['python-3.x', 'tensorflow', 'keras', 'deep-learning', 'nlp']",77554281,"i figured it out! gradient calculations fail when there is a tensorflow function in the graph which is the case in my network where i have applied tf.round and tf.abs in the output layer of the decoder. that was failing the gradient calculations. i removed them and it the model trains as expected. here is the link to the issue 
decoder:
class decoder(layer):
  def __init__(self, units, embed_input_dim, name='decoder', **kwargs):
    super().__init__()
    ### decoder input embedding layer
    self.embedding = embedding(input_dim=embed_input_dim, output_dim=units, name='de_embed_layer')
    self.pos_embedding = positionalencodinglayer(name='de_positional_embed_layer')

    ### decoder multi-head attention sub layer
    self.mha_sub_layer1 = mhasublayer(units, num_heads=8, name='de_mha_layer_1')
    self.mha_sub_layer2 = mhasublayer(units, num_heads=8, name='de_mha_layer_2')
    self.mha_sub_layer3 = mhasublayer(units, num_heads=8, name='de_mha_layer_3')
    self.mha_sub_layer4 = mhasublayer(units, num_heads=8, name='de_mha_layer_4')
    self.mha_sub_layer5 = mhasublayer(units, num_heads=8, name='de_mha_layer_5')
    self.mha_sub_layer6 = mhasublayer(units, num_heads=8, name='de_mha_layer_6')

    ### decoder mha droput layer
    self.dropout =  dropout(rate=0.1, name='de_dropout_pos_enc')
    self.dropout1 = dropout(rate=0.1, name='de_dropout_layer1')
    self.dropout2 = dropout(rate=0.1, name='de_dropout_layer2')
    self.dropout3 = dropout(rate=0.1, name='de_dropout_layer3')
    self.dropout4 = dropout(rate=0.1, name='de_dropout_layer4')
    self.dropout5 = dropout(rate=0.1, name='de_dropout_layer5')
    self.dropout6 = dropout(rate=0.1, name='de_dropout_layer6')

    ### dense output layer
    self.output_dense_layer = timedistributed(dense(1), name=""output_layer"")

  def call(self, x, en_context):
    embedding_output = self.embedding(x)
    positional_embedding = self.pos_embedding(embedding_output)
    postitional_embedding = self.dropout(positional_embedding)

    ### first mha sub-layer
    sub_layer1_out = self.mha_sub_layer1(positional_embedding, positional_embedding)
    sub_layer1_out = self.dropout1(sub_layer1_out)

    ### second mha sub-layer
    sub_layer2_out = self.mha_sub_layer2(sub_layer1_out, en_context)
    sub_layer2_out = self.dropout2(sub_layer2_out)

    ### third mha sub-layer
    sub_layer3_out = self.mha_sub_layer3(sub_layer2_out, en_context)
    sub_layer3_out = self.dropout3(sub_layer3_out)

    ### fourth mha sub-layer
    sub_layer4_out = self.mha_sub_layer4(sub_layer3_out, en_context)
    sub_layer4_out = self.dropout4(sub_layer4_out)

    ### fifth mha sub-layer
    sub_layer5_out = self.mha_sub_layer5(sub_layer4_out, en_context)
    sub_layer5_out = self.dropout5(sub_layer5_out)

    ### sixth mha sub-layer
    sub_layer6_out = self.mha_sub_layer6(sub_layer5_out, en_context)
    sub_layer6_out = self.dropout6(sub_layer6_out)

    ### output dense layer
    output = self.output_dense_layer(sub_layer6_out)
    return output",https://stackoverflow.com/questions/77546477,python-3.x,25-11-2023 02:49,63.0,1.0,1.0,True,27-11-2023 02:46,25-11-2023 16:44,Implementation Issues
72611335,what are the differences between fine tuning and few shot learning?,"i am trying to understand the concept of fine-tuning and few-shot learning.
i understand the need for fine-tuning. it is essentially tuning a pre-trained model to a specific downstream task. however, recently i have seen a plethora of blog posts stating zero-shot learning, one-shot learning and few-shot learning.

how are they different from fine-tuning? it appears to me that few-shot learning is a specialization of fine-tuning. what am i missing here?

can anyone please help me?","['machine-learning', 'deep-learning', 'artificial-intelligence', 'fine-tuning', 'few-shot-learning']",72709377,"fine tuning - when you already have a model trained to perform the task you want but on a different dataset, you initialise using the pre-trained weights and train it on target (usually smaller) dataset (usually with a smaller learning rate).
few shot learning - when you want to train a model on any task using very few samples. e.g., you have a model trained on different but related task and you (optionally) modify it and train for target task using small number of examples.
for example:
fine tuning - training a model for intent classification and then fine tuning it on a different dataset.
few shot learning - training a language model on large text dataset and modifying it (usually last (few) layer) to classify intents by training on small labelled dataset.
there could be many more ways to do few shot learning. for 1 more example, training a model to classify images where some classes have very small (or 0 for zero shot and 1 for one shot) number of training samples. here in inference, classifying these rare classes (rare in training) correctly becomes the aim of few shot learning.",https://stackoverflow.com/questions/72611335,machine-learning,14-06-2022 03:54,10467.0,8.0,4.0,True,08-02-2025 00:54,22-06-2022 02:55,Task-specific Help
76420235,laravel openai client doesn&#39;t work with list result,"when i try request throw openai laravel library to model gpt-3.5-turbo and result must be list, library throw exception
warning  undefined array key ""choices"" in 

vendor/openaiphp/client/src/responses/completions/createresponse.php on line 45.

typeerror  array_map(): argument #2 ($array) must be of type array, null given.


i tried this request for test
use openai\laravel\facades\openai;

openai::completions()->create([
                'model' => 'gpt-3.5-turbo',
                'prompt' => 'top 3 reachest peaople',
            ]);","['php', 'laravel', 'openai-api', 'chatgpt-api']",76635758,the problem was related to the openai model. i changed the model to text-davinci-003 and it worked.,https://stackoverflow.com/questions/76420235,php,07-06-2023 05:53,798.0,-1.0,1.0,True,07-07-2023 09:39,08-06-2023 06:33,Uncategorized
90580,word frequency algorithm for natural language processing,"without getting a degree in information retrieval, i'd like to know if there exists any algorithms for counting the frequency that words occur in a given body of text.  the goal is to get a ""general feel"" of what people are saying over a set of textual comments.  along the lines of wordle.
what i'd like:

ignore articles, pronouns, etc ('a', 'an', 'the', 'him', 'them' etc)
preserve proper nouns
ignore hyphenation, except for soft kind

reaching for the stars, these would be peachy:

handling stemming & plurals (e.g. like, likes, liked, liking match the same result)
grouping of adjectives (adverbs, etc) with their subjects (""great service"" as opposed to ""great"", ""service"")

i've attempted some basic stuff using wordnet but i'm just tweaking things blindly and hoping it works for my specific data.  something more generic would be great.","algorithm, nlp, word-frequency",90846,"you'll need not one, but several nice algorithms, along the lines of the following.

ignoring pronouns is done via a stoplist.
preserving proper nouns? you mean, detecting named entities, like hoover dam and saying ""it's one word"" or compound nouns, like programming language? i'll give you a hint: that's tough one, but there exist libraries for both. look for ner (named entitiy recognition) and lexical chunking. opennlp is a java-toolkit that does both.
ignoring hyphenation? you mean, like at line breaks? use regular expressions and verify the resulting word via dictionary lookup.
handling plurals/stemming: you can look into the snowball stemmer. it does the trick nicely.
""grouping"" adjectives with their nouns is generally a task of shallow parsing. but if you are looking specifically for qualitative adjectives (good, bad, shitty, amazing...) you may be interested in sentiment analysis. lingpipe does this, and a lot more.

i'm sorry, i know you said you wanted to kiss, but unfortunately, your demands aren't that easy to meet. nevertheless, there exist tools for all of this, and you should be able to just tie them together and not have to perform any task yourself, if you don't want to. if you want to perform a task yourself, i suggest you look at stemming, it's the easiest of all.
if you go with java, combine lucene with the opennlp toolkit. you will get very good results, as lucene already has a stemmer built in and a lot of tutorial. the opennlp toolkit on the other hand is poorly documented, but you won't need too much out of it. you might also be interested in nltk, written in python.
i would say you drop your last requirement, as it involves shallow parsing and will definetly not impove your results.
ah, btw. the exact term of that document-term-frequency-thing you were looking for is called tf-idf. it's pretty much the best way to look for document frequency for terms. in order to do it properly, you won't get around using multidimenional vector matrices.
... yes, i know. after taking a seminar on ir, my respect for google was even greater. after doing some stuff in ir, my respect for them fell just as quick, though.",https://stackoverflow.com/q/90580,"algorithm, nlp, word-frequency",18-09-2008 06:49,22656.0,33.0,8.0,True,11-03-2025 10:08,22-06-2015 13:56,Conceptual Questions
74969153,"how to display response with proper spacing, line-break and formatting in reactjs?","i am taking response from openai and store it using react usestate
const [response, setresponse] = usestate();

try {
  const res = await axios.post(' body, config)
  const completions = res.data.choices[0].text
  console.log(completions)
  setresponse(completions)
  setloading(false)
} catch (err) {
  console.error(err)
  setloading(false)
}

when i console the response it appears in this way

and when display using this code..
{loading ? (
        <div classname='block p-2.5 h-full w-full text-sm text-gray-900 bg-gray-50 rounded-lg border border-gray-300  dark:bg-gray-700 dark:border-gray-600  dark:text-white '>loading...</div>
      ) : (
        <div classname=""block p-3 h-full w-full text-sm text-gray-900 bg-gray-50 rounded-lg border border-gray-300  dark:bg-gray-700 dark:border-gray-600  dark:text-white "" >
          {response}
        </div>
      )}

it will appears like this:","['reactjs', 'react-hooks', 'openai-api']",74969447,an div element does not parse spaces or carriage return. as a solution for your problem i recommend to use <pre> tag or to use css property white-space (white-space: pre;),https://stackoverflow.com/questions/74969153,reactjs,31-12-2022 11:16,1258.0,-1.0,1.0,True,31-12-2022 12:17,31-12-2022 11:37,Implementation Issues
72983957,regex: searching for words that starts with @ or @,"i want to create a regex in python that find words that start with @ or @.
i have created the following regex, but the output contains one extra space in each string as you can see
regex = r'\s@\/?[\w\.\-]{2,}'
exp = 'george want@to play @.hdgska football @dddada'
re.findall(regex, exp)
output: [' @.hdgska', ' @dddada']

however, the output that i want to have is the following
output: ['@.hdgska', '@dddada']

i would be grateful if you could help me!
edit:
@the fourth bird, thank you so much for your help. there is one more thing that i don't know how to deal with. in case that we have this string
s = ""george want@to play @.hdgska football @dddada@snhfbjskjs""

the output is
['@.hdgska', '@dddada']

however, the output that i want should be one this
'@.hdgska'","['python', 'regex', 'nlp']",72983995,"in your pattern you are actually matching the leading \s and after the @ there can be an optional / with \/? but it should optionally start with a dot.
you could match for example an optional dot, and then 2 or more times the allowed characters in the character class.
at the left of the @ sign, either assert a non word boundary or assert a whitespace boundary.
note that you don't have to escape the dot and the hyphen in the character class.
\b@\.?[\w.-]{2,}

regex demo
another option:
(?<!\s)@\.?[\w.-]{2,}

regex demo
example
import re
 
pattern = r""(?<!\s)@\.?\w+[\w.-]{2,}""
s = ""george want@to play @.hdgska football @dddada""
print(re.findall(pattern, s))

output
['@.hdgska', '@dddada']",https://stackoverflow.com/questions/72983957,python,14-07-2022 16:41,53.0,0.0,1.0,True,15-07-2022 11:07,15-07-2022 11:07,Uncategorized
40941761,i am having trouble downloading nltk&#39;s punkt tokenizer,"i'm trying to download punkt, but i'm getting the following error...
>>> import nltk
>>> nltk.download('punkt')
>>> [nltk_data] error loading punkt: <urlopen error [ssl] unknown error
>>> [nltk_data]     (_ssl.c:590)>
>>> false
>>> 

can someone please help i've been trying for days...","['python', 'nltk']",40944901,"i guess the downloader script is broken. as a temporal workaround can manually download the punkt tokenizer from here and then place the unzipped folder in the corresponding location. the default folders for each os are:

windows: c:\nltk_data\tokenizers
osx: /usr/local/share/nltk_data/tokenizers
unix: /usr/share/nltk_data/tokenizers

i am not sure but you may find this post helpful.",https://stackoverflow.com/questions/40941761,python,02-12-2016 22:22,20115.0,3.0,5.0,True,29-12-2024 19:44,03-12-2016 06:21,Implementation Issues
76387002,python inheritance - interfaces/classes,"from langchain.schema import basememory

class chatmemory(basememory):
   def __init__(self, user_id: uuid, type: str):
    self.user_id = user_id
    self.type = type

   # implemented abstract methods

class anothermem(chatmemory):
    def __init__(self, user_id: uuid, type: str):
        super().__init__(user_id, type)

this seems simple enough - but i get an error: valueerror: ""anothermem"" object has no field ""user_id"". what am i doing wrong?
note that basememory is an interface.","['python', 'inheritance', 'langchain']",76387031,"it looks like basememory from langchain is defined as a pydantic model, which has strict rules for defining instance attributes. instead of using a constructor method, use the standard pydantic syntax of declaring expected instance attributes on the child class.
import uuid
from langchain.schema import basememory


class chatmemory(basememory):
    user_id: uuid.uuid
    type: str

    ...

class anothermem(chatmemory):
    pass


print(anothermem(user_id=uuid.uuid4(), type=""foo"").user_id)

d952d62e-79e0-4cf4-a786-d23d880f96a2",https://stackoverflow.com/questions/76387002,python,02-06-2023 03:50,103.0,2.0,1.0,True,02-06-2023 03:58,02-06-2023 03:57,Conceptual Questions
68959472,no vector when using spacy.load(&#39;en_core_web_trf&#39;)?,"after running this
nlp = spacy.load('en_core_web_lg')
has_vector = nlp('test text').has_vector
# ...
...has_vector == true

but after running this
nlp = spacy.load('en_core_web_trf')
has_vector = nlp('test text').has_vector
# ...
...has_vector == false

what am i missing?","['nlp', 'spacy', 'spacy-3']",72681901,"as per the docs here ( though, you can get the vector using
nlp('test text')._.trf_data.tensors[-1]",https://stackoverflow.com/questions/68959472,nlp,27-08-2021 21:19,2078.0,3.0,2.0,True,20-06-2022 03:21,29-08-2021 00:07,Data Wrangling
76720413,how can i install chatgpt seo plugin in chatgpt?,"i learn about chatgpt plugin in internet and below link.

but i don't know how can i download and install this plugin?

seo plugin
core seo ai plugin
aiprm
seo assistant
bramework

also please let me know, if it's safe to use this plugin in our seo.
i don't get any download link of core seo ai plugin.","['plugins', 'openai-api']",76817442,"if you go into the aiprm website  there is a link to install chrome extension.
chrome extension link

this only provides you prompt and help you to add those prompts to the chatgpt page so it's fully safe.
if any website or plugin asks for your api key to use chatgpt functionality then stay away from those applications.",https://stackoverflow.com/questions/76720413,plugins,19-07-2023 10:38,259.0,0.0,1.0,True,01-03-2024 07:22,01-03-2024 07:22,Implementation Issues
78122648,openai api: how do i get a list of all available openai models?,"can anyone find the curl command to get a list of all available openai models? i've been looking for like 10 minutes and can't find it.

edit: i got the answer, and i see the problem. it is one of those docs where everything is on one page.",['openai-api'],78122662,"python
from openai import openai
import os
client = openai(
    api_key = os.getenv('openai_api_key')
)
models = client.models.list()
for model in models:
    print(model.id)

node.js
const openai = require(""openai"");

const client = new openai({
  apikey: process.env.openai_api_key,
});

async function main() {
  const list = await client.models.list();

  for await (const model of list) {
    console.log(model);
  }
}

main();

curl
curl  \
  -h ""authorization: bearer $openai_api_key""

see the official openai documentation.",https://stackoverflow.com/questions/78122648,openai-api,07-03-2024 15:44,18888.0,4.0,1.0,True,23-03-2025 15:18,22-03-2024 12:20,Implementation Issues
73574217,re:sub expected string or byte-like object error,"class rereplacer(object):
   def __init__(self, pattern = r_patterns):
      self.pattern = [(re.compile(regex), repl) for (regex, repl) in patterns]

   def replace(self, text):
      s = text
      for (pattern, repl) in self.pattern:
         s = re.sub(pattern, repl, s)
      return s

i have got this code which replaces certain words with their replacements. when i call the method replace of class replacer,
rep=replacer()
rep.replace(""i like oranges"") 

it works perfectly fine with strings but gives an error with list or nested lists.
error- (re.sub) expected string or bytes-like object.
is there a way (except for converting the list to string) in order to make the function work of list of sentences? thanks in advance.
seems like predefined re.sub takes string as argument. should i split the words of the list?","['python', 'regex', 'nlp', 'nltk', 'python-re']",73574390,"if text is a list (or some other iterable type), loop over it and perform the replacements, and return a list of results.
from collections.abc import iterable

def replace(self, text):
    if isinstance(text, string):
        for (pattern, repl) in self.pattern:
            text = re.sub(pattern, repl, text)
        return text
    elif isinstance(text, iterable):
        return [self.replace(i) for i in text]
    else:
        raise valueerror('text must be a string or iterable collection')",https://stackoverflow.com/questions/73574217,python,01-09-2022 18:53,97.0,-3.0,1.0,True,04-09-2022 17:40,04-09-2022 17:40,Tool Setup/Errors
74857383,unnecessary words included in the word cloud created using r programming,"i am trying to create some word cloud in r, which i am managing well so far with the exception of one little problem. i don't know where these words/symbols are coming from, but the following words are also getting displayed in my word cloud:

""language""
""en""
""=""

and i can't seem to remove them.these words/symbols are not part of the original text and don't know why and how they are getting displayed in my word cloud and really need help in understanding how i can remove such unwanted words and why they are there. below, i am attaching a screen shot of my word cloud for clarity, and added blue arrows to show where those words/symbols are located in the word cloud. i am also attaching my lines of code and the text i used for creating the word cloud. any help is much appreciated and many thanks.

    the_txt <-  ""
  - the wealthiest country\n
  - the highest proportion of wealthy population (population aged 40-49)\n
  - the highest numbers of ""rich business men and women"" and ""rich soil and land""\n
  - the country with the highest ""employed populstion"" and ""self employed"" numbers

  ""

    mydata <- corpus(vectorsource(the_txt))


mydata <- mydata %>%
    tm_map(removenumbers) %>%
    tm_map(removepunctuation) %>%
    tm_map(stripwhitespace)

mydata <- tm_map(mydata, content_transformer(tolower))

mydata <- tm_map(mydata, removenumbers)

mydata <- tm_map(mydata, removewords, stopwords(""english""))

mydata <- tm_map(mydata, stemdocument)


as.character(mydata[[1]])

minfreq_trigram<-1

token_delim <- "" \\t\\r\\n.!?,;\""()""

tritoken <- ngramtokenizer(my data, weka_control(min=1,max=3, delimiters = token_delim))

three_word <- data.frame(table(tritoken))

sort_three <- three_word[order(three_word$freq, decreasing=true),]

set.seed(1234)

wordcloud(sort_three$tritoken, sort_three$freq, 
              random.order=false, scale = c(3,0.4),
              min.freq = minfreq_trigram,
              colors = brewer.pal(8,""dark2""),
              max.words=200)","['r', 'nlp', 'word-cloud']",74861996,"> as.character(mydata)
[1] ""wealthiest countri highest proport wealthi popul popul age highest number rich busi men women rich soil land countri highest employ populst self employ number""
[2] ""list(language = \""en\"")""                                                                                                                                       
[3] ""list()"" 

you checked mydata[[1]] , explicitly looking at a part of mydata, but the rest has content, that you fed into ngramtokenizer and ultimitaly the wordcloud.
if you want to pass mydata[[1]]] instead of mydata i would think that would work out for you, and is a straightforward approach. i think the recommended approach is to use content()
i.e.
mycontent <- content(mydata)

to get the character vector out",https://stackoverflow.com/questions/74857383,r,20-12-2022 00:24,108.0,0.0,1.0,True,20-12-2022 16:06,20-12-2022 16:06,Implementation Issues
76759792,how do i do function calling in azure openai using the javascript sdk,"i want to be able to call functions based on the user input, i could do this with the openai library but can't find a way to do so in the azure openai library
below is the code from azure openai in python that is able to do what i want to accomplish
as such i want to replicate this code using the javascript sdk link
import openai 
openai.api_type = ""azure"" 
openai.api_base = "" 
openai.api_version = ""2023-07-01-preview"" 
openai.api_key = os.getenv(""openai_api_key"") 
response = openai.chatcompletion.create(             
    engine=""gpt-35-turbo-xxx"",             
    model=""gpt-35-turbo-0613-xxxx""             
    messages=messages,             
    functions=functions,             
    function_call=""auto"",         
)

i tried this
const response = await openai.getchatcompletions(
deploymentid,       
messages,    
{  function_call: functions }

and
const response = await openai.getchatcompletions(
deploymentid,       
messages,    
functions )

i couldn't find anything on the documentation for this as well","['azure', 'openai-api', 'azure-openai']",77094357,"the updated azure openai javascript documentation now incorporates support for functions and function calls.
you can find the relevant documentation at the following link, 
it is recommended that you ensure your azure openai library is updated to the most recent version.(",https://stackoverflow.com/questions/76759792,azure,25-07-2023 05:55,1783.0,2.0,1.0,True,17-01-2024 22:19,26-07-2023 08:48,Implementation Issues
36966019,how aretf-idf calculated by the scikit-learn tfidfvectorizer,"i run the following code to convert the text matrix to tf-idf matrix.
text = ['this is a string','this is another string','tfidf computation calculation','tfidf is the product of tf and idf']

from sklearn.feature_extraction.text import tfidfvectorizer
vectorizer = tfidfvectorizer(max_df=1.0, min_df=1, stop_words='english',norm = none)
                    
x = vectorizer.fit_transform(text)
x_vocab = vectorizer.get_feature_names_out()
x_mat = x.todense()
x_idf = vectorizer.idf_

i get the following output
x_vocab =
[u'calculation',
 u'computation',
 u'idf',
 u'product',
 u'string',
 u'tf',
 u'tfidf']

and x_mat =
  ([[ 0.        ,  0.        ,  0.        ,  0.        ,  1.51082562,
      0.        ,  0.        ],
    [ 0.        ,  0.        ,  0.        ,  0.        ,  1.51082562,
      0.        ,  0.        ],
    [ 1.91629073,  1.91629073,  0.        ,  0.        ,  0.        ,
      0.        ,  1.51082562],
    [ 0.        ,  0.        ,  1.91629073,  1.91629073,  0.        ,
      1.91629073,  1.51082562]])

now i dont understand how these scores are computed. my idea is that for the text[0], score for only 'string' is computed and there is a score in the 5th coloumn. but as tf_idf is the product of term frequency which is 2 and idf which is log(4/2) is 1.39 and not 1.51 as shown in the matrix. how is the tf-idf score calculated in scikit-learn.","['nlp', 'scikit-learn', 'tf-idf']",36972265,"tf-idf is done in multiple steps by scikit learn's tfidfvectorizer, which in fact uses tfidftransformer and inherits countvectorizer.
let me summarize the steps it does to make it more straightforward:

tfs are calculated by countvectorizer's fit_transform()
idfs are calculated by tfidftransformer's fit()
tfidfs are calculated by tfidftransformer's transform()

you can check the source code here.
back to your example. here is the calculation that is done for the tfidf weight for the 5th term of the vocabulary, 1st document (x_mat[0,4]):
first, the tf for 'string', in the 1st document:
tf = 1

second, the idf for 'string', with smoothing enabled (default behavior):
df = 2
n = 4
idf = ln(n + 1 / df + 1) + 1 = ln (5 / 3) + 1 = 1.5108256238

and finally, the tfidf weight for (document 0, feature 4):
tfidf(0,4) = tf * idf = 1 * 1.5108256238 = 1.5108256238

i noticed you choose not to normalize the tfidf matrix. keep in mind normalizing the tfidf matrix is a common and usually recommended approach, since most models will require the feature matrix (or design matrix) to be normalized.
tfidfvectorizer will l-2 normalize the output matrix by default, as a final step of the calculation. having it normalized means it will have only weights between 0 and 1.",https://stackoverflow.com/questions/36966019,nlp,01-05-2016 11:16,14203.0,21.0,3.0,True,10-06-2024 23:20,10-06-2024 23:20,Implementation Issues
68471586,training epochs interpretation during spacy ner training,"i was training my ner model with transformers, and am not really sure why the training stopped at some point, or why did it even go with so many batches. this is how my configuration file looks like (relevant part):
[training]
train_corpus = ""corpora.train""
dev_corpus = ""corpora.dev""
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_epochs = 2
max_steps = 0
eval_frequency = 200
frozen_components = []
before_to_disk = null

[training.batcher]
@batchers = ""spacy.batch_by_words.v1""
discard_oversize = false
tolerance = 0.2
get_length = null

[training.batcher.size]
@schedules = ""compounding.v1""
start = 100
stop = 1000
compound = 1.001
t = 0.0

[training.optimizer]
@optimizers = ""adam.v1""
beta1 = 0.9
beta2 = 0.999
l2_is_weight_decay = true
l2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001
learn_rate = 0.00005

and this is the training log:
============================= training pipeline =============================
ï¿½ï¿½ï¿½ pipeline: ['transformer', 'ner']
ï¿½ï¿½ï¿½ initial learn rate: 5e-05
e    #       loss trans...  loss ner  ents_f  ents_p  ents_r  score 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0         398.75     40.97    2.84    3.36    2.46    0.03
  0     200         906.30   1861.38   94.51   94.00   95.03    0.95
  0     400         230.06   1028.51   98.10   97.32   98.89    0.98
  0     600          90.22   1013.38   98.99   98.40   99.58    0.99
  0     800          80.64   1131.73   99.02   98.25   99.81    0.99
  0    1000          98.50   1260.47   99.50   99.16   99.85    1.00
  0    1200          73.32   1414.91   99.49   99.25   99.73    0.99
  0    1400          84.94   1529.75   99.70   99.56   99.85    1.00
  0    1600          55.61   1697.55   99.75   99.63   99.87    1.00
  0    1800          80.41   1936.64   99.75   99.63   99.87    1.00
  0    2000         115.39   2125  99.69   99.87    1.00
  0    2200          63.06   2395.48   99.80   99.75   99.85    1.00
  0    2400         104.14   2574.36   99.87   99.79   99.96    1.00
  0    2600          86.07   2308.35   99.88   99.79   99.97    1.00
  0    2800          81.05   1853.15   99.90   99.87   99.93    1.00
  0    3000          52.67   1462.61   99.96   99.93   99.99    1.00
  0    3200          57.99   1154.62   99.94   99.91   99.97    1.00
  0    3400         110.74    847.50   99.90   99.85   99.96    1.00
  0    3600          90.49    621.99   99.90   99.91   99.90    1.00
  0    3800          51.03    378.93   99.87   99.78   99.97    1.00
  0    4000          93.40    274.80   99.95   99.93   99.97    1.00
  0    4200         138.98    203.28   99.91   99.87   99.96    1.00
  0    4400         106.16    127.60   99.70   99.75   99.64    1.00
  0    4600          70.28     87.25   99.95   99.94   99.96    1.00
ï¿½ï¿½ï¿½ saved pipeline to output directory
training/model-last


i was trying to traiodel for 2 epochs (max_epochs=2), and my train file has around 123591 examples, and dev file has 2522 examples.
my question is:

since my minimum batch size is 100, i expect my training to end before the 2400th eval batch, right? because 2400th batch evaluated implies i have a minimum of 2400*100 = 240000, and it would actually be even more than that, since my batch size is increasing. so why did it go all the way to # 4600?

the training ended automatically, but the e still reads the 0th epoch. why is that?


edit: in continuation to my 2nd bullet point, i'm curious to know why did the training went all the way upto 4600 batches, because 4600 batches at minimum means 4600*100 = 460000 examples, and i gave 123591  examples for train, so i'm clearly well above and over the 1st epoch, but e still reads as 0.","['nlp', 'spacy', 'named-entity-recognition', 'spacy-3']",68479425,"there's an entry for this in the faq, but to summarize:

max_steps is the maximum iterations. (not ""evaluation iterations"", but batches.)
max_epochs is the maximum number of epochs.
if training goes for patience batches without improvement it will stop. that is what stopped your training.

it seems like your model has already gotten a perfect score so i'm not sure why early stopping is a problem in this case, but that's what's happening.",https://stackoverflow.com/questions/68471586,nlp,21-07-2021 14:42,5789.0,4.0,3.0,True,21-07-2022 14:54,21-07-2021 21:43,Data Wrangling
76734099,openai chat completions api: how do i use a function to store conversation memory?,"i am trying to make a chatbot using openai function calling. i have taken the basic example of getting the current weather condition, which was given in the documentation.
what i want to implement is to have a memory with it.
i tried to append into the message, but what i want is when i have a new message, so instead of calling the function, how can it get the response from memory if it's already asked?
my code is like this:
def get_current_weather(location, unit=""fahrenheit""):
    print(""it ran>>>>>>>>>>"")
    weather_info = {
        ""location"": location,
        ""temperature"": ""72"",
        ""unit"": unit,
        ""forecast"": [""sunny"", ""windy""],
    }
    return json.dumps(weather_info)


messages = []


def run_conversation(input_message):
    messages.append({""role"": ""user"", ""content"": f""{input_message}""})
    functions = [
        {
            ""name"": ""get_current_weather"",
            ""description"": ""get the details about a drug/medication"",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""definition"": {
                        ""type"": ""string"",
                        ""description"": ""the city and state, e.g. san francisco, ca"",
                    },
                    ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
                },
                ""required"": [""location""],
            },
        }
    ]
    print(""message 1"", messages)
    response = openai.chatcompletion.create(
        model=""gpt-3.5-turbo-0613"",
        messages=messages,
        functions=functions,
        # function_call=""auto"",
    )
    response_message = response[""choices""][0][""message""]
    print(""response msg"", response_message)

    if response_message.get(""function_call""):
        available_functions = {""get_current_weather"": get_current_weather}
        function_name = response_message[""function_call""][""name""]
        function_to_call = available_functions[function_name]
        function_args = json.loads(response_message[""function_call""][""arguments""])
        function_response = function_to_call(
            location=function_args.get(""location""),
            unit=function_args.get(""unit""),
        )

        # messages.append(response_message)
        messages.append(
            {""role"": ""function"", ""name"": function_name, ""content"": function_response}
        )
        print(""message 2"", messages)
        second_response = openai.chatcompletion.create(
            model=""gpt-3.5-turbo-0613"",
            messages=messages,
        )
        print(""second response"", second_response['choices'][0]['message'].to_dict())
        messages.append(second_response['choices'][0]['message'].to_dict())
        print(""message 3"", messages)
        return second_response

it always runs the function even if i ask the same question
output:
response msg {
  ""role"": ""assistant"",
  ""content"": null,
  ""function_call"": {
    ""name"": ""get_current_weather"",
    ""arguments"": ""{\n  \""definition\"": \""boston, ma\"",\n  \""unit\"": \""celsius\""\n}""
  }
}
it ran>>>>>>>>>>
message 2 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}]
second response {'role': 'assistant', 'content': 'the temperature in boston is 72 degrees celsius.'}
message 3 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degree
s celsius.'}]
message 1 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degree
s celsius.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}]
response msg {
  ""role"": ""assistant"",
  ""content"": null,
  ""function_call"": {
    ""name"": ""get_current_weather"",
    ""arguments"": ""{\""location\"": \""new york\""}""
  }
}
it ran>>>>>>>>>>
message 2 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degree
s celsius.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""locati
on"": ""new york"", ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}]
second response {'role': 'assistant', 'content': 'the temperature in new york is 72 degrees. please note that i did not specify the temperature unit, as i
t is missing in the response.'}
message 3 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degree
s celsius.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""locati
on"": ""new york"", ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in new york is 72
 degrees. please note that i did not specify the temperature unit, as it is missing in the response.'}]
message 1 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degree
s celsius.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""locati
on"": ""new york"", ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in new york is 72
 degrees. please note that i did not specify the temperature unit, as it is missing in the response.'}, {'role': 'user', 'content': 'what is the temperatu
re in boston?'}]
response msg {
  ""role"": ""assistant"",
  ""content"": null,
  ""function_call"": {
    ""name"": ""get_current_weather"",
    ""arguments"": ""{\""definition\"": \""boston\""}""
  }
}
it ran>>>>>>>>>>
message 2 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degree
s celsius.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""locati
message 3 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degree
s celsius.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""locati
on"": ""new york"", ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in new york is 72
 degrees. please note that i did not specify the temperature unit, as it is missing in the response.'}, {'role': 'user', 'content': 'what is the temperatu
re in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"": null, ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny
"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degrees. please note that the unit of temperature is missing in the resp
onse.'}]
message 1 [{'role': 'user', 'content': 'what is the temperature in boston?'}]
response msg {        
  ""role"": ""assistant"",
  ""content"": null,    
  ""function_call"": {
    ""name"": ""get_current_weather"",
    ""arguments"": ""{\n  \""definition\"": \""boston, ma\""\n}""
  }
}
it ran>>>>>>>>>>
message 2 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}]
second response {'role': 'assistant', 'content': 'the temperature in boston is 72 degrees fahrenheit.'}
message 3 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degrees fah
renheit.'}]
message 1 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degrees fah
renheit.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}]
response msg {
  ""role"": ""assistant"",
  ""content"": null,
  ""function_call"": {
    ""name"": ""get_current_weather"",
    ""arguments"": ""{\""location\"": \""new york\"", \""unit\"": \""celsius\""}""
  }
}
it ran>>>>>>>>>>
message 2 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degrees fah
renheit.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location
"": ""new york"", ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}]
second response {'role': 'assistant', 'content': 'the temperature in new york is 72 degrees celsius.'}
message 3 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degrees fah
renheit.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location
"": ""new york"", ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in new york is
 72 degrees celsius.'}]
message 1 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degrees fah
renheit.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location
"": ""new york"", ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in new york is
 72 degrees celsius.'}, {'role': 'user', 'content': 'what is the temperature in boston?'}]
response msg {
  ""role"": ""assistant"",
  ""content"": null,
  ""function_call"": {
    ""name"": ""get_current_weather"",
    ""arguments"": ""{\""location\"": \""boston\""}""
  }
is 72 degrees.'}]","['python', 'openai-api', 'chatgpt-api', 'gpt-4']",76778206,"using the openai api function to store conversation memory is not the right approach because of the possibility that the model may generate invalid json or hallucinate parameters.
people are already having problems with that.
if you take a look at the official openai documentation, you can see that openai transparently states:

the basic sequence of steps for function calling is as follows:

call the model with the user query and a set of functions defined in the functions parameter.
the model can choose to call a function; if so, the content will be a stringified json object adhering to your custom schema (note: the
model may generate invalid json or hallucinate parameters).
parse the string into json in your code, and call your function with the provided arguments if they exist.
call the model again by appending the function response as a new message, and let the model summarize the results back to the user.",https://stackoverflow.com/questions/76734099,python,20-07-2023 22:33,2361.0,-1.0,1.0,True,12-08-2023 15:37,12-08-2023 15:37,Implementation Issues
77491173,is it possible use &quot;gpt-4-vision-preview&quot; with batching?,"i am trying to use the ""gpt-4-vision-preview"" model with the batching option (since the limits are very low at the moment).
this is my messages object (not sure if it's correct but i tried to follow the docs).
            let messages = batch.map(doc => {
            const imageurl =`someurl`;
            const question = doc.questao;
            const answers = doc.respostas;
            let options = object.keys(answers).map(key => `${key}: ${answers[key]}`).join('\n');

            return {
                role: ""user"",
                content: [
                    {
                        type: ""text"",
                        text: `${question} \n ${options} \n ${questionexplanation}`
                    },

                    {
                        type: ""image_url"",
                        image_url: {
                            url: imageurl
                        }
                    }
                ]
            };
        });

and this is how i make the request.
const response = await openai.chat.completions.create({
            model: ""gpt-4-vision-preview"",
            max_tokens: 4000, // adjust if needed
            messages: messages
        });

i did not see anywhere in the docs saying if it was possible or not.","['node.js', 'openai-api']",77514874,"in terms of batching, it's only possible to pass multiple images with one text message at the moment.
where your messages would look like this
 const prompt_messages = [
  {
    role: ""user"",
    content: [
      {
        type: ""text"",
        text: ""<prompt message>"",
      },
      ...batch.map((doc) => ({
        type: ""image_url"",
        image_url: {
          url: ""<image_url>""
          detail: ""low"",
        },
      })),
    ],
  },

optional: by passing detail: ""low"", you specify the low-res 512px x 512px version of the image to the model which will represent the image with a budget of 65 tokens.",https://stackoverflow.com/questions/77491173,node.js,15-11-2023 22:13,534.0,0.0,1.0,True,20-11-2023 09:52,16-11-2023 07:58,Implementation Issues
74074350,tensorflow expected 2 inputs but received 1 input tensor,"hey guys so i'm building a model based on the roberta-base and at the end when i try to fit the model i get a error saying: valueerror: layer model_39 expects 2 input(s), but it received 1 input tensors. inputs received: [<tf.tensor 'iteratorgetnext:0' shape=(16, 128) dtype=float64>]
i'm using tf.data.dataset to make the dataset:
def map_dataset(ids, masks, labels):
    return {'input_ids': ids, 'input_mask': masks}, labels

# create dataset
dataset = tf.data.dataset.from_tensor_slices((ids, mask, labels))
dataset.map(map_dataset)
dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=true)

supposedly dataset is generating 2 inputs properly but for some reason fit is refusing to work and i'm not sure why.
full code:
len_seq = 128
batch_size = 16
test_train_split = 0.9
transformer = 'roberta-base'

# load roberta model
base_model = tfautomodel.from_pretrained('roberta-base')
for layer in base_model.layers:
    layer.trainable = false

# define input layers
input_ids = tf.keras.layers.input(shape=(len_seq,), name='input_ids', dtype='int32')
input_mask = tf.keras.layers.input(shape=(len_seq,), name='input_mask', dtype='int32')

# define hidden layers
embedding = base_model([input_ids, input_mask])[1]
layer = tf.keras.layers.dense(len_seq * 2, activation='relu')(embedding)
layer = tf.keras.layers.dense(len_seq, activation='relu')(layer)

# define output
output = tf.keras.layers.dense(1, activation='softmax', name='output')(layer)

model = tf.keras.model(inputs=[input_ids, input_mask], outputs=[output])

model.compile(
    optimizer = adam(learning_rate=1e-3, decay=1e-4),
    loss = categoricalcrossentropy(),
    metrics = [
        categoricalaccuracy('accuracy')
    ]
)

# load data
df = pd.read_csv('train-processed.csv')
df = df.head(100)
samples_count = len(df)

# tokenize data
tokenizer = autotokenizer.from_pretrained(transformer)
tokens = tokenizer(
    df['first_phrase'].tolist(),
    max_length=len_seq,
    truncation=true,
    padding='max_length',
    add_special_tokens=true,
    return_tensors='tf'
)
ids = tokens['input_ids']
mask = tokens['attention_mask']

def map_dataset(ids, masks, labels):
    return {'input_ids': ids, 'input_mask': masks}, labels

# create dataset
dataset = tf.data.dataset.from_tensor_slices((ids, mask, labels))
dataset.map(map_dataset)
dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=true)

# split data intro train and test
train_size = int((samples_count / batch_size) * test_train_split)
train = dataset.take(train_size)
test = dataset.skip(train_size)

# train model
history = model.fit(
    train,
    validation_data=test,
    epochs=2
)

inside dataset -> <batchdataset shapes: ((16, 128), (16, 128), (16, 5)), types: (tf.float64, tf.float64, tf.float64)>
inside train -> <takedataset shapes: ((16, 128), (16, 128), (16, 5)), types: (tf.float64, tf.float64, tf.float64)>
data example:

any help appreciated. i'm new to transformers so please feel free to point any extra considerations.","['python', 'tensorflow', 'bert-language-model']",74079275,"so i managed to fix this as far as i know with the help of @djinn.
i did remove dataset api and instead built my own datasets manually using the following code:
# split into training and validation sets
train_size = int(samples_count * test_train_split)
train = [ids[:train_size], mask[:train_size]]
train_labels = labels[:train_size]
test = [ids[train_size:], mask[train_size:]], labels[train_size:]
# train model
history = model.fit(
    train, train_labels,
    validation_data=test,
    epochs=10
)

this seems to be working and fit() accepted this data, but feel free to point out if this is wrong or could be made differently.",https://stackoverflow.com/questions/74074350,python,14-10-2022 20:03,973.0,0.0,1.0,True,15-10-2022 12:03,14-10-2022 20:31,Implementation Issues
77615264,why does opennlp cli output &quot;slf4j: failed to load class &quot;org.slf4j.impl.staticloggerbinder&quot; on windows?,"based on apache opennlp documentation, i downloaded binary version of opennlp, then set java_home and opennlp_home. when i run opennlp command it faced to below exception:
slf4j: failed to load class ""org.slf4j.impl.staticloggerbinder"".
slf4j: defaulting to no-operation (nop) logger implementation
slf4j: see  for further details.

i used windows operating system and tried this command on different systems with different versions of java, but always this exception is shown. it is too odd, because i've searched and no one faced this issued before.","['java', 'nlp', 'slf4j', 'opennlp']",77630535,"you found a bug. congratulation! i created opennlp-1527 for it.
meanwhile, you can override the content of the opennlp.bat file with
@echo off

rem #   licensed to the apache software foundation (asf) under one
rem #   or more contributor license agreements.  see the notice file
rem #   distributed with this work for additional information
rem #   regarding copyright ownership.  the asf licenses this file
rem #   to you under the apache license, version 2.0 (the
rem #   ""license""); you may not use this file except in compliance
rem #   with the license.  you may obtain a copy of the license at
rem #
rem #    
rem #
rem #   unless required by applicable law or agreed to in writing,
rem #   software distributed under the license is distributed on an
rem #   #  ""as is"" basis, without warranties or conditions of any
rem #   kind, either express or implied.  see the license for the
rem #   specific language governing permissions and limitations
rem #   under the license.

rem # note:  do not output anything in this script file, any output
rem #        may be inadvertantly placed in any output files if
rem #        output redirection is used.
setlocal

if ""%java_cmd%"" == """" (
    if ""%java_home%"" == """" (
        set java_cmd=java 
    ) else (
        rem # keep java_home to short-name without spaces
        for %%a in (""%java_home%"") do set java_cmd=%%~sfa\bin\java
    )
)

rem remove heap variable
set heap=
if not ""%java_heap%"" == """" (
    set heap=""-xmx%java_heap%""
)

rem #  should work with windows xp and greater.  if not, specify the path to where it is installed.
if ""%opennlp_home%"" == """" (
    set opennlp_home=%~sp0..
) else (
    rem # keep opennlp_home to short-name without spaces
    for %%a in (""%opennlp_home%"") do set opennlp_home=%%~sfa
)

echo environment
echo java_home=%java_home%
echo opennlp_home=%opennlp_home% 

rem add lib directory to the classpath
set classpath=""%opennlp_home%\lib\*""

echo classpath=%classpath%

%java_cmd% %heap% ""-dlog4j.configurationfile=%opennlp_home%\conf\log4j2.xml"" -cp %classpath% opennlp.tools.cmdline.cli %*

endlocal

this will correctly append the classes contained in the lib folder to the classpath and the cli will work as expected.",https://stackoverflow.com/questions/77615264,java,06-12-2023 17:47,174.0,1.0,1.0,True,09-12-2023 07:47,09-12-2023 07:15,Tool Setup/Errors
78030052,delete files downloaded by outlines python library,"import outlines
model = outlines.models.transformers('mistralai/mistal-7b-v0.1')

i had run the above code on my m1 macbook to try mistral llm, because of that 16 gb size of weights were downloaded on my system. now i cant find where they were downloaded. i need to delete those files from system to free up my space. please help me locate those files. uninstalling the library also didnt help free up my space.","['python', 'langchain', 'large-language-model', 'mistral-7b']",78030509,"it appears to be downloaded to the directory ~/.cache/huggingface/hub.
you can run cd ~/.cache/huggingface/hub in the terminal to change to this directory, or run open ~/.cache/huggingface/hub to open finder in this directory.
versions used: osx 13.6.1, python 3.10.10, outlines 0.0.32, and transformers 4.37.2.",https://stackoverflow.com/questions/78030052,python,20-02-2024 20:09,135.0,0.0,2.0,True,20-02-2024 22:17,20-02-2024 21:23,Implementation Issues
78824299,openai api returns null when i retrieve the fine-tuned model,"i'm fine-tuning a model and generating actions from text. i create a train.jsonl file, upload it, and fine-tune the model. however, when i try to get the name of the model i just created, it returns null.
i fine-tune a model like this:
const model = await openai.finetuning.jobs.create({
  training_file: process.env.file_id,
  model: 'babbage-002',
})

then i try to retrieve the fine-tuned model like this:
const response = await openai.finetuning.jobs.retrieve(
    process.env.fine_tune_id,
)

but this is the response i get from the openai api:
data:  {
  object: 'fine_tuning.job',
  id: 'ftjob-nsfvxzjttsfr5jcqqtfedtco',
  model: 'babbage-002',
  created_at: 1722546992,
  fine_tuned_model: null,
  organization_id: 'org-gljhkxwkbqrlovhk0762ucml',
  result_files: [],
  status: 'running',
  validation_file: null,
  training_file: 'file-lh1c4vv1hdiv7lxxugh9mil9',
  hyperparameters: { n_epochs: 9, batch_size: 1, learning_rate_multiplier: 16 },
  trained_tokens: null,
  error: {},
  user_provided_suffix: null,
  seed: 1564492262,
  estimated_finish: null,
  integrations: []
}","['node.js', 'openai-api', 'fine-tuning']",78825035,"the fine-tuning job hasn't finished yet.
the fine-tuning flow is the following:

create a fine-tuning job.
fine-tuning is in progress.
retrieve the fine-tuning job.

try to run the following code to see if the fine-tuning is still in progress.
import openai from ""openai"";

const client = new openai();

async function main() {
  const list = await client.finetuning.jobs.list();

  for await (const finetune of list) {
    console.log(finetune);
  }
}

main();",https://stackoverflow.com/questions/78824299,node.js,02-08-2024 07:40,76.0,0.0,1.0,True,21-09-2024 15:52,21-09-2024 15:51,Implementation Issues
77469097,how can i process a pdf using openai&#39;s apis (gpts)?,"the web interface for chatgpt has an easy pdf upload. is there an api from openai that can receive pdfs?
i know there are 3rd party libraries that can read pdf but given there are images and other important information in a pdf, it might be better if a model like gpt 4 turbo was fed the actual pdf directly.
i'll state my use case to add more context. i intent to do rag. in the code below i handle the pdf and a prompt. normally i'd append the text at the end of the prompt. i could still do that with a pdf if i extract its contents manually.
the following code is taken from here  is this how i'm supposed to do it?
# upload a file with an ""assistants"" purpose
file = client.files.create(
  file=open(""example.pdf"", ""rb""),
  purpose='assistants'
)

# create an assistant using the file id
assistant = client.beta.assistants.create(
  instructions=""you are a personal math tutor. when asked a math question, write and run code to answer the question."",
  model=""gpt-4-1106-preview"",
  tools=[{""type"": ""code_interpreter""}],
  file_ids=[file.id]
)

there is an upload endpoint as well, but it seems the intent of those endpoints are for fine-tuning and assistants. i think the rag use case is a normal one and not necessarily related to assistants.","['python', 'machine-learning', 'pdf', 'openai-api', 'chat-gpt-4']",78924474,"as of today (openai.__version__==1.42.0) using openai assistants + gpt-4o allows to extract content of (or answer questions on) an input pdf file foobar.pdf stored locally, with a solution along the lines of
from openai import openai
from openai.types.beta.threads.message_create_params import (
    attachment,
    attachmenttoolfilesearch,
)
import os

filename = ""foobar.pdf""
prompt = ""extract the content from the file provided without altering it. just output its exact content and nothing else.""

client = openai(api_key=os.environ.get(""my_openai_key""))

pdf_assistant = client.beta.assistants.create(
    model=""gpt-4o"",
    description=""an assistant to extract the contents of pdf files."",
    tools=[{""type"": ""file_search""}],
    name=""pdf assistant"",
)

# create thread
thread = client.beta.threads.create()

file = client.files.create(file=open(filename, ""rb""), purpose=""assistants"")

# create assistant
client.beta.threads.messages.create(
    thread_id=thread.id,
    role=""user"",
    attachments=[
        attachment(
            file_id=file.id, tools=[attachmenttoolfilesearch(type=""file_search"")]
        )
    ],
    content=prompt,
)

# run thread
run = client.beta.threads.runs.create_and_poll(
    thread_id=thread.id, assistant_id=pdf_assistant.id, timeout=1000
)

if run.status != ""completed"":
    raise exception(""run failed:"", run.status)

messages_cursor = client.beta.threads.messages.list(thread_id=thread.id)
messages = [message for message in messages_cursor]

# output text
res_txt = messages[0].content[0].text.value
print(res_txt)

the prompt can of course be replaced with the desired user request and i assume that the openai key is stored in a env var named my_openai_key.
limitations:

it's not (yet) possible to enforce json structure (other than with instructions in the prompt). this solution is inspired by 

this relies on text content in the pdf (i.e. searchable text content), and the queries won't be able to access e.g. image content in the pdf.",https://stackoverflow.com/questions/77469097,python,12-11-2023 13:25,50796.0,32.0,5.0,True,27-03-2025 11:51,27-03-2025 11:51,Implementation Issues
79007155,does prompt_token usage affect my billing when using azure openai models with your own data?,"i have set azure openai on my data, chat with azure openai models using your own data. my goal was to reduce token usage in each request.
however, i have noticed additional prompt_token usage, even when i am sending user content without any prompt. for example, if i only send the text hello there, it results in a total of 2628 tokens, whereas it should only be 24. if a longer text (7 words) is provided without any prompt, it results in a total of approximately 3.4k tokens.
example:
[{'role': 'system', 'content': ''}, {'role': 'user', 'content': 'hello there'}]
total_tokens: {'completion_tokens': 24, 'prompt_tokens': 2604, 'total_tokens': 2628}

----------------------------------------------------

[{'role': 'system', 'content': ''},
{'role': 'user', 'content': 'i worked overtime what should i do?'}]
total_tokens: {'completion_tokens': 52, 'prompt_tokens': 3334, 'total_tokens': 3386}

as you can see, under total token i am seeing prompt_token usage close the ~3.5k.
where does prompt_token usage come from since i do not provide any prompt or system message? isn't the whole purpose of using azure openai models with your own data to reduce token usage? for each request, additional 3.5k token is very expensive. will it affect my billing, where would prompt_token be considered as part of input tokens?

pricing/details/cognitive-services/openai-service/, states that input (per 1,000 tokens) is $0.0025, than as i understand for 4,000 tokens, the cost should be $0.01.","['openai-api', 'chatgpt-api', 'azure-openai', 'azure-ai']",79016386,"there is a bit of magic going on when they ""use your data""; it is essentially retrieval augmented generation or rag. basically there's a few more steps than simply your prompt text.
they explain it fairly well here:

in total, there are two calls made to the model:


for processing the intent: the token estimate for the intent prompt includes those for the user question, conversation history, and the instructions sent to the model for intent generation.


for generating the response: the token estimate for the generation prompt includes those for the user question, conversation history, the retrieved list of document chunks, role information, and the instructions sent to it for generation.",https://stackoverflow.com/questions/79007155,openai-api,20-09-2024 14:18,429.0,1.0,1.0,True,20-10-2024 17:36,20-09-2024 15:03,Conceptual Questions
74790060,view takes 1 positional argument but 2 were given,"trying to make a post request to openai with the input:
{""write hello world""}

but getting the error:
typeerror: view.__init__() takes 1 positional argument but 2 were given

here is my view:
def get_help(user_input):
    response = openai.completion.create(
        engine=""text-davinci-002"",
        prompt=""user_input"",
        temperature=0.5,
        max_tokens=1024,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0
    )
    return response[""choices""][0][""text""]

@api_view(['post'])
class receive_response(view):
    def post(self, request):
        user_input = request.post[""user_input""]
        response = get_help(user_input)
        return 

and my urls.py:
urlpatterns = [
    path(""get"", get_help, name=""get_help""),
    path(""post"", receive_response, name=""post""),
]","['python', 'django', 'openai-api']",74790220,"your problem is on this line: class receive_response(view):
(why does receive_response inherit from view?)
essentially what is happening is:

post request is received
request_received object is initialized (with the args of the post - of which there must be two)
since it inherits from view (and no __init__() is specified, the parent class' __init__() is passed the same inputs
since view.__init__() accepts a single input value, but it received two, you get your error: typeerror: view.__init__() takes 1 positional argument but 2 were given

add a def __init__(self, v1, v2): declaration to your class, and debug it to see what v1/v2 are (and decide which/what to pass to super().__init__() (view's constructor)",https://stackoverflow.com/questions/74790060,python,13-12-2022 19:34,3369.0,0.0,1.0,True,14-12-2022 05:17,14-12-2022 05:17,Conceptual Questions
74100762,iterate over vector of vectors of strings without using for loops in julia,"given a vector of vectors of strings, like:
sentences = [ [""julia"", ""is"", ""1000x"", ""faster"", ""than"", ""python!""], 
              [""julia"", ""reads"", ""beautiful!""], 
              [""python"", ""has"", ""600"", ""times"", ""more"", ""libraries""] 
]

i'm trying to filter out some tokens in each of them, without losing the outer vector structure (i.e., without flattening the vector down to a single list of tokens).
so far i've achieved this using a classic for loop:
number_of_alphabetical_tokens = []
number_of_long_tokens = []
total_tokens = []

for sent in sentences
    append!(number_of_alphabetical_tokens, length([token for token in sent if all(isletter, token)]))
    append!(number_of_long_words, length([token for token in sent if length(token) > 2]))
    append!(total_tokens, length(sent))
end

collect(zip(number_of_alphabetical_tokens, number_of_long_words, total_tokens))

output: (edited as per @shayan observation)
3-element vector{tuple{any, any, any}}:
 (4, 5, 6)
 (2, 3, 3)
 (5, 6, 6)

this gets the job done, but it takes more time than i'd like (i have 6000+ documents, with thousands of sentences each...), and it looks a bit like an antipattern.
is there a way of doing this with comprehensions or broadcasting (or any more performant method)?","['loops', 'vector', 'nlp', 'julia', 'array-broadcasting']",74101904,"there's no reason to avoid loops for performance reasons in julia. loops are fast, and vectorized code is just loops in disguise.
here's an example of doing this with loops, and some reductions, like all and count:
function wordstats(sentences)
    out = vector{ntuple{3, int}}(undef, length(sentences))
    for (i, sent) in pairs(sentences)
        a = count(all(isletter, word) for word in sent)
        b = count(length(word)>2 for word in sent)
        c = length(sent)
        out[i] = (a, b, c)
    end
    return out
end

the above code is not optimized, for example, counting words longer than 2 can be improved, but it runs in approximately 700ns on my laptop, which is much faster than the vectorized solution.
edit: here's basically the same code, but using the map do syntax (so you don't have to figure out the return type):
function wordstats2(sentences)
    map(sentences) do sent
        a = count(all(isletter, word) for word in sent)
        b = count(length(word)>2 for word in sent)
        c = length(sent)
        return (a, b, c)
    end
end",https://stackoverflow.com/questions/74100762,loops,17-10-2022 17:03,557.0,1.0,2.0,True,20-10-2022 12:17,17-10-2022 23:31,Implementation Issues
79293919,determining most popular words in the english dictionary within a dictionary of words,"forgive me if my wording is awful, but i'm trying to figure out how to determine the most used words in the english language from a set of words in a dictionary i've made. i've done some research on nltk but can't seem to find a function within it (or any other library for that matter) that will help me do what i need to do.
for example:
a sentence ""i enjoy a cold glass of water on a hot day"" would return ""water"" because it's the most used word in day to day conversation from the sentence. essentially i need a returned value of the most frequently used word in conversations.
i figure i'll likely have to involve ai, but any time i've tried to use ai i wind up copy and pasting code because i just don't understand it, so i'm trying to avoid going that route
any and all help is welcome and appreciated.
for context, i decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.","['python', 'nlp', 'nltk', 'detection']",79294074,"you need a external dataset for this task. you can try dataset such as google n gram dataset.
here is the breakdown of the problem statement:

input: ""i enjoy a cold glass of water on a hot day"". output: ""water"".
split the sentences into words list.


example: [""i"", ""enjoy"", ""a"", ""cold"", ""glass"", ""of"", ""water"", ""on"",
""a"", ""hot"", ""day""]


first loop in through all the word of the sentences. so let say you are at first word ""i"".
now you will look the same word ""i"" in external dataset and will look for the frequency of that word.
let say the word ""i"" in external dataset is repeated 5000000 times
repeat this task for all the word.
now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data.
frequency in the below example is random value not exact value.


{
    ""i"": 5000000,
    ""enjoy"": 50000,
    ""a"": 10000000,
    ""cold"": 30000,
    ""glass"": 100000,
    ""of"": 8000000,
    ""water"": 1200000,
    ""on"": 6000000,
    ""hot"": 700000,
    ""day"": 400000
}



pick the word with highest frequency.

note: you can try any big corpus as external data. using big corpus will have most of the english word which is used in conversation. and even if the frequency is not mentioned then you can create that yourself",https://stackoverflow.com/questions/79293919,python,19-12-2024 10:24,73.0,0.0,2.0,True,19-12-2024 11:17,19-12-2024 10:41,Implementation Issues
77836174,how can i add a progress bar/status when creating a vector store with langchain?,"creating a vector store with the python library langchain may take a while. how can i add a progress bar?

example of code where a vector store is created with langchain:
import pprint
from langchain_community.vectorstores import faiss
from langchain_community.embeddings import huggingfaceembeddings
from langchain.docstore.document import document

model = ""sentence-transformers/multi-qa-minilm-l6-cos-v1""
embeddings = huggingfaceembeddings(model_name = model)

def main():
    doc1 = document(page_content=""the sky is blue."",    metadata={""document_id"": ""10""})
    doc2 = document(page_content=""the forest is green"", metadata={""document_id"": ""62""})
    docs = []
    docs.append(doc1)
    docs.append(doc2)

    for doc in docs:
        doc.metadata['summary'] = 'hello'

    pprint.pprint(docs)
    db = faiss.from_documents(docs, embeddings)
    db.save_local(""faiss_index"")
    new_db = faiss.load_local(""faiss_index"", embeddings)

    query = ""which color is the sky?""
    docs = new_db.similarity_search_with_score(query)
    print('retrieved docs:', docs)
    print('metadata of the most relevant document:', docs[0][0].metadata)

if __name__ == '__main__':
    main()

tested with python 3.11 with:
pip install langchain==0.1.1 langchain_openai==0.0.2.post1 sentence-transformers==2.2.2 langchain_community==0.0.13 faiss-cpu==1.7.4

the vector store is created with db = faiss.from_documents(docs, embeddings).","['python', 'progress-bar', 'langchain', 'faiss']",77839038,"langchain does not natively support any progress bar for this at the moment with release of 1.0.0
i also had similar case, so instead of sending all the documents, i send independent document for ingestion and tracked progress at my end. this was helpful for me.
you can do the ingestion in the following way
    with tqdm(total=len(docs), desc=""ingesting documents"") as pbar:
        for d in docs:
            if db:
                db.add_documents([d])
            else:
                db = faiss.from_documents([d], embeddings)
            pbar.update(1)  


from what i checked from langchain code  they are making call to add_texts as well, so no major operation is being performed here other than parsing.
i had simple documents, and i didn't observe much difference. probably others who has tried on huge documents can add if it adds latency in their usecase.
below is your updated code
import pprint
from tqdm import tqdm
from langchain_community.vectorstores import faiss
from langchain_community.embeddings import huggingfaceembeddings
from langchain.docstore.document import document

model = ""sentence-transformers/multi-qa-minilm-l6-cos-v1""
embeddings = huggingfaceembeddings(model_name = model)

def main():
    doc1 = document(page_content=""the sky is blue."",    metadata={""document_id"": ""10""})
    doc2 = document(page_content=""the forest is green"", metadata={""document_id"": ""62""})
    docs = []
    docs.append(doc1)
    docs.append(doc2)

    for doc in docs:
        doc.metadata['summary'] = 'hello'

    db = none
    with tqdm(total=len(docs), desc=""ingesting documents"") as pbar:
        for d in docs:
            if db:
                db.add_documents([d])
            else:
                db = faiss.from_documents([d], embeddings)
            pbar.update(1)  

    # pprint.pprint(docs)
    # db = faiss.from_documents(docs, embeddings)
    db.save_local(""faiss_index"")
    new_db = faiss.load_local(""faiss_index"", embeddings)

    query = ""which color is the sky?""
    docs = new_db.similarity_search_with_score(query)
    print('retrieved docs:', docs)
    print('metadata of the most relevant document:', docs[0][0].metadata)

if __name__ == '__main__':
    main()",https://stackoverflow.com/questions/77836174,python,17-01-2024 23:59,5710.0,6.0,2.0,True,14-08-2024 06:12,18-01-2024 05:43,Implementation Issues
27750608,error installing nltk supporting packages : nltk.download(),"i have installed the nltk package. following that i am trying to download the supporting packages using nltk.download() and am getting error:
[errno 11001] getaddrinfo 

my machine / software details are:
os: windows 8.1
python: 3.3.4
nltk package: 3.0
below are the commands run in python:
python 3.3.4 (v3.3.4:7ff62415e426, feb 10 2014, 18:13:51) [msc v.1600 64 bit (amd64)] on win32
type ""copyright"", ""credits"" or ""license()"" for more information.

import nltk

nltk.download()
showing info 
true

nltk.download(""all"")
[nltk_data] error loading all: <urlopen error [errno 11001]
[nltk_data]     getaddrinfo failed>
false


it looks like it is going to   whereas it should ideally try to get the data from 
on another machine when we type  in the browser, it redirects to  i am not understanding why the redirection is not happening on my laptop.
i feel that this might be the issue.
i have added the command prompt screenshot.","['python', 'python-3.x', 'nltk']",27764910,"got the solution. the issue in my case was that when the nltk downloader started it had the server index as - 
this needs to be changed to - 
you can change this by going into the nltk downloader window and the file->change server index.
regards,
bonson",https://stackoverflow.com/questions/27750608,python,03-01-2015 00:51,67274.0,9.0,11.0,True,05-01-2024 10:31,15-09-2023 15:16,Tool Setup/Errors
29151329,arabic lemmatization and stanford nlp,"i try to make lemmatization, ie identifying the lemma and possibly the arabic root of a verb, for example:
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ==> lemma (infinitive of the verb) ==> ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ==> root (triliteral root / jidr thoulathi)
==> ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½
do you think stanford","['nlp', 'stanford-nlp', 'lexical-analysis', 'stemming', 'lemmatization']",29219800,"the stanford arabic segmenter can't do true lemmatization. however, it is possible to train a new model to do something like stemming:

ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½+ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ +ï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½+ ï¿½ï¿""ýýýýýý"" is not a true lemma), you might be better off with a tool like madamira (
elaboration: the stanford arabic segmenter produces its output character-by-character using only these operations (implemented in edu.stanford.nlp.international.arabic.process.iobutils):

split a word between two characters
transform lil- (ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½) into li+ al- (ï¿½ï¿½+ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½)
transform ta (ï¿½ï¿½) or ha (ï¿½ï¿½) into ta marbuta (ï¿½ï¿½)
transform ya (ï¿½ï¿½) or alif (ï¿½ï¿½) into alif maqsura (ï¿½ï¿½)
transform alif maqsura (ï¿½ï¿½) into ya (ï¿½ï¿½)

so lemmatizing ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ to ï¿½ï¿½+ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ would require implementing an extra rule, i.e., to insert an alif after ya or ta. lemmatization of certain irregular forms would be completely impossible (for example, ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½).
the version of the stanford segmenter available for download also only breaks off proebank or a similarly rich source of arabic text with morphological segmentation annotated, it is possible to train your own model to remove all morphological affixes, which is closer to lemmatization:
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½""ýýýýýýýý"" is not a real arabic word, but the segmenter should at least consistently produce ""ýýýýýýýý"" for ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ,ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ,ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½, etc. if this is acceptable, you would need to change the atb preprocessing script to instead use the morphological segmentation annotations. you could do this by replacing the script called 
then follow the instructions for ""training the segmenter"" in the readme.",https://stackoverflow.com/questions/29151329,nlp,19-03-2015 17:33,5327.0,5.0,2.0,True,06-01-2025 09:17,19-03-2015 21:13,Implementation Issues
78925963,unexpected value passed to langchain tool argument,"i'm trying to create a simple example tool that creates new user accounts in a hypothetical application when instructed to do so via a user prompt. the llm being used is llama3.1:8b via ollama.
so far what i've written works, but it's very unreliable.
the reason why it's unreliable is because when langchain calls on my tool, it provides unexpected/inconsistent values to the user creation tool's single username argument.
sometime the argument will be a proper username and other times it will be a username with the value ""username="" prefixed to the username (eg: ""username=jdoe"" rather than simply ""jdoe"").
also, if i ask for multiple users to be created, sometimes langchain will correctly invoke the tool multiple times while other times, it will invoke the tool once with a string in the format of an array (eg: ""['jdoe','jsmith']"")
my questions are:

is the issue i'm encountering due to the limitations of langchain or the llama3.1:8b model that i'm using? or is the issue something else?
is there a way to get langchain to more reliably call my user creation tool with a correctly formatted username?
are there are other useful tips/recommendations that you can provide for a beginner like me?

below is my code:
from dotenv import load_dotenv
from langchain.agents import agentexecutor, create_react_agent
from langchain.tools import tool
from langchain_core.prompts import prompttemplate
from langchain_ollama.chat_models import chatollama

load_dotenv()


# define the tool to create a user account
mock_user_db = [""jdoe"", ""jrogers"", ""jsmith""]


def create_user_tool(username: str):
    print(""username provided for creation: "" + username)
    if username in mock_user_db:
        return f""user {username} already exists.""
    mock_user_db.append(username)
    return f""user {username} created successfully.""


# define the tool to delete a user account
def delete_user_tool(username: str):
    print(""username provided for deletion: "" + username)
    if username not in mock_user_db:
        return f""user {username} does not exist.""
    
    mock_user_db.remove(username)
    return f""user {username} deleted successfully.""


def list_users_tool(ignore) -> list:
    return mock_user_db


# wrap these functions as langchain tools
create_user = tool(
    name=""create user"",
    func=create_user_tool,
    description=""creates a new user account in the company hr system.""
)

delete_user = tool(
    name=""delete user"",
    func=delete_user_tool,
    description=""deletes an existing user account in company hr system.""
)

list_users = tool(
    name=""list users"",
    func=list_users_tool,
    description=""lists all user accounts in company hr system.""
)

# initialize the language model
llm = chatollama(model=""llama3.1:latest"", temperature=0)

# create the agent using the tools
tools = [create_user, delete_user, list_users]

# get the prompt to use
#prompt = hub.pull(""hwchase17/react"") # does not work with ollama/llama3:8b
prompt = hub.pull(""hwchase17/react-chat"") # kinda works with ollama/llama3:8b

agent = create_react_agent(llm, tools, prompt)

# create an agent executor by passing in the agent and tools
agent_executor = agentexecutor(agent=agent, tools=tools, handle_parsing_errors=true)

print(agent_executor.invoke({""input"": ""please introduce yourself.""})['output'])

while true:
    user_prompt = input(""prompt: "")
    agent_response = agent_executor.invoke({""input"": user_prompt})
    print(agent_response['output'])","['python', 'langchain', 'large-language-model']",79022969,"prompt engineering (what you are attempting here), is far from an exact science.  however, there are ways you can clarify the schema of the tool.
one example (from their docs) is getting it to parse your docstrings:
@tool(parse_docstring=true)
def create_user(username: str):
    """"""creates a user

        args:
            username: username of the user to be created. the exact string of the username, no longer than 20 characters long
    """"""
    ... # rest of your code here

see docs here
but even more reliable would be to create your schema with pydantic (great tool in general), again, from their docs:

class create_user(basemodel):
    """"""creates a user""""""

    username: str = field(..., description=""username of the user to be created. the exact string of the username, no longer than 20 characters long""

in general, the more detail you provide, regarding the shape and nature of the tools and the data, the better results you can expect.
you may also want to consider setting your temperature to 0, so you get repeatable responses for any given prompt, which should help with debugging, but you need to test with a higher range of prompts to ensure reliable behaviour",https://stackoverflow.com/questions/78925963,python,29-08-2024 02:57,794.0,4.0,1.0,True,23-12-2024 12:57,23-12-2024 12:57,Implementation Issues
70880940,combine two regexp grammars in nltk,"i'm defining a noun phrase using grammar in nltk. the example provided by nltk is:
grammar = ""np: {<dt>?<nnp>*<nn>}""

then if i have a sentence like: show me the paris hospitals, the library can detect the noun phrase:
>>> s
'show me the paris hospitals'
>>> grammar = ""np: {<dt>?<nnp>*<nns>}""
>>> nltk.regexpparser(grammar).parse(nltk.pos_tag(nltk.word_tokenize(s)))
tree('s', [('show', 'vb'), ('me', 'prp'), tree('np', [('the', 'dt'), ('paris', 'nnp'), ('hospitals', 'nns')])])

now, the sentence can be written in another way: show me the hospitals of paris, and hence i need to change the grammar to:
>>> grammar = ""np: {<dt>?<nns><in><nnp>}""
>>> s = ""show me the hospitals in paris""
>>> nltk.regexpparser(grammar).parse(nltk.pos_tag(nltk.word_tokenize(s)))
tree('s', [('show', 'vb'), ('me', 'prp'), tree('np', [('the', 'dt'), ('hospitals', 'nns'), ('in', 'in'), ('paris', 'nnp')])])

how do i combine the two grammars in a unique one? i couldn't figure out the or condition for the two grammars.","['python', 'nlp', 'nltk', 'grammar']",70885691,"you can just define two np rules in one grammar:
grammar = """"""
np: {<dt>?<nnp>*<nns>}
np: {<dt>?<nns><in><nnp>}
""""""

or using | as the wanted or condition:
grammar = ""np: {<dt>?<nnp>*<nns>|<dt>?<nns><in><nnp>}""

full example:
import nltk

sentence_1 = 'show me the paris hospitals'
sentence_2 = ""show me the hospitals in paris""

grammar_1 = """"""
np: {<dt>?<nnp>*<nns>}
np: {<dt>?<nns><in><nnp>}
""""""
parser_1 = nltk.regexpparser(grammar_1)

grammar_2 = ""np: {<dt>?<nnp>*<nns>|<dt>?<nns><in><nnp>}""
parser_2 = nltk.regexpparser(grammar_2)

for s in sentence_1, sentence_2:
    tokens = nltk.word_tokenize(s)
    pos_tags = nltk.pos_tag(tokens)
    print(parser_1.parse(pos_tags))
    print(parser_2.parse(pos_tags))

# outputs the same for both parsers:
# (s show/vb me/prp (np the/dt paris/nnp hospitals/nns))
# (s show/vb me/prp (np the/dt paris/nnp hospitals/nns))
# (s show/vb me/prp (np the/dt hospitals/nns) in/in paris/nnp)
# (s show/vb me/prp (np the/dt hospitals/nns) in/in paris/nnp)

(link to the documentation)",https://stackoverflow.com/questions/70880940,python,27-01-2022 15:10,211.0,1.0,1.0,True,27-01-2022 21:28,27-01-2022 21:26,Conceptual Questions
77282461,huggingface bettertransformer in `with` context - cannot disable after context,"i am writing a custom with context manager to temporarily make the model a bettertransformer model while calling trainer.evaluate().
i evaluated before, in, and after the with context. i noticed that the evaluation after the with context still uses bettertransformer. this is a problem because the trainer.train() call afterwards will also use bettertransformer, resulting in poor training due to padding.
how do i create a custom with context that only uses bettertransformer inside the context, not afterwards?
please find the mwe gist here.
i created a custom context manager:
class bettertransformercontext:
    """"""temporarily replace a model with a bettertransformer model.""""""

    def __init__(self, model):
        self.model = model
        self.original_model = none

    def __enter__(self):
        self.original_model = self.model
        self.model = bettertransformer.transform(self.model)
        return self.model

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.model = self.original_model
        # self.model = bettertransformer.reverse(self.model)  # note: same result

the output is as follows. evaluating without bettertransformer handles approximately 100 it/s, with bettertransformer handles approximately 115 it/s. as you can see, evaluating after the context still results in 115 it/s.
========== without optimum (-> should be slow) ==========
bt before context:  false
100%|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý| 204/204 [00:01<00:00, 103.09it/s]
0.3161764705882353
========== with optimum (-> should be fast) ==========
bt in context:  true
100%|ýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý| 204/204 [00:01<00:00, 116.68it/s]
0.3161764705882353
========== without optimum (-> should be slow) ==========
bt after context:  true
100%|ýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý| 204/204 [00:01<00:00, 116.53it/s]
0.3161764705882353","['python', 'huggingface-transformers', 'with-statement', 'contextmanager', 'huggingface-trainer']",77302404,"i found a solution by using a custom context manager on the trainer object, as opposed to applying it on a model object.
the custom context manager is as follows:
class bettertransformertrainercontext:
    """"""context manager to wrap trainer.model with bettertransformer.""""""
    def __init__(self, trainer):
        self.trainer = trainer

    def __enter__(self):
        self.trainer.model = bettertransformer.transform(
            self.trainer.model, keep_original_model=true
        )
        return self.trainer

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.trainer.model = bettertransformer.reverse(self.trainer.model)

it can be used as follows:
print(""="" * 10, ""with optimum (-> should be fast)"", ""="" * 10)
with bettertransformertrainercontext(trainer) as _optimum_trainer:
    eval_accuracy = _optimum_trainer.evaluate()[""eval_accuracy""]
    print(eval_accuracy)

i hope this might be helpful to someone else.",https://stackoverflow.com/questions/77282461,python,12-10-2023 16:48,112.0,0.0,1.0,True,16-10-2023 13:31,12-10-2023 16:51,Implementation Issues
74836900,valueerror: [e966] `nlp.add_pipe` when changing the sentence segmentaion rule of spacy model,"i am using python 3.9.7 and the spacy library and want to change the way the model segments a given sentence. here is a sentence and the segmentation rule i created as an example:
import spacy
nlp=spacy.load('en_core_web_sm')

doc2=nlp(u'""management is doing the  right things; leadership is doing the right things."" -peter drucker')

def set_custom_boundaries(doc):
    for token in doc[:-1]:
        if token.text=="";"":
            doc[token.i +1].is_sent_start=true
    return doc

nlp.add_pipe(set_custom_boundaries, before='parser')

however, this produces the error message below:
valueerror                                traceback (most recent call last)
c:\users\seydou~1\appdata\local\temp/ipykernel_21000/1705623728.py in <module>
----> 1 nlp.add_pipe(set_custom_boundaries, before='parser')

~\anaconda3\lib\site-packages\spacy\language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)
    777             bad_val = repr(factory_name)
    778             err = errors.e966.format(component=bad_val, name=name)
--> 779             raise valueerror(err)
    780         name = name if name is not none else factory_name
    781         if name in self.component_names:

valueerror: [e966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. expected string, but got <function set_custom_boundaries at 0x000002520a59cca0> (name: 'none').

- if you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.
    
- if you passed in a component like `textcategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.
    
- if you're using a custom component: add the decorator `@language.component` (for function components) or `@language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@language.component('your_name')`. you can then run `nlp.add_pipe('your_name')` to add it to the pipeline.

i looked at some solutions online, however, i could not solve the problem as a beginner in python. how does one use their own custom segmentation rule in the spacy pipeline?","['python', 'nlp', 'spacy']",74837826,"the syntax of
nlp.add_pipe with a custom function is given here.  you must (1) declare the component function with a 'decorator' and (2) pass the name of the component/function as a string. so it should be something like this:
@language.component(""set_custom_boundaries"")
def set_custom_boundaries(doc):
    for token in doc[:-1]:
        if token.text=="";"":
            doc[token.i +1].is_sent_start=true
    return doc



nlp.add_pipe(""set_custom_boundaries"", before='parser')

note: your function is doing a strange sentence segmentation, it won't work in general. for example it won't work if a sentence ends with '.', '...', or '!', etc.",https://stackoverflow.com/questions/74836900,python,17-12-2022 19:27,429.0,2.0,1.0,True,18-12-2022 23:56,18-12-2022 23:56,Implementation Issues
76757241,i&#39;m trying to turn this python script into a web app with flask,"i am trying to turn my script into a web app, it uses wordpress and chatgpt to generate articles based on keywords and what you type in the input boxes and allows the user to instantly upload to drafts on their wordpress site..
i am running into an issue:
updated
# app.py
from flask import flask, render_template, request, flash
from flask_wtf import flaskform
from wtforms import stringfield, textareafield, submitfield
from wtforms.validators import inputrequired
import openai
import requests
import random

app = flask(__name__)
app.config['secret_key'] = 'your_secret_key'

# set your openai api key
openai.api_key = """"


class inputform(flaskform):
    api_key = stringfield('openai api key', validators=[inputrequired()])
    input_text = textareafield('input text', validators=[inputrequired()])
    keywords = textareafield('keywords (comma-separated)', validators=[inputrequired()])
    wp_base_url = stringfield('wordpress base url', validators=[inputrequired()])
    wp_username = stringfield('wordpress username', validators=[inputrequired()])
    wp_password = stringfield('wordpress password', validators=[inputrequired()])
    submit = submitfield('submit')


# array holding wordpress author ids
authorlist = [""1"", ""2""]

# you can call the ""gpt-3.5-turbo"" model
modelengine = ""gpt-3.5-turbo""

# wordpress information
wp_base_url = """"
wp_username = """"
wp_password = """"


# post creator function
def create_post(inputtitlesent, outputtext):
    randomauthor = random.choice(authorlist)

    post_status = ""draft""
    headers = {
        ""content-type"": ""application/x-
    }
    post = {
        ""title"": inputtitlesent,
        ""content"": outputtext,
        ""status"": post_status,
        ""author"": randomauthor,
        ""categories:"": ""6""
    }
    url = wp_base_url + ""/wp-json/wp/v2/posts""
    response = requests.post(url, data=post, headers=headers, auth=(wp_username, wp_password))
    return response


# post title creator function
def create_title(outputtext):
    response = openai.chatcompletion.create(
        model=modelengine,
        messages=[{""role"": ""user"", ""content"": f""write a title for this article:\n\n {outputtext}""}],
        n=1
    )
    create_title = response.choices[0].message.content
    create_title = create_title.replace('""', '')
    return create_title


@app.route('/', methods=['get', 'post'])
def index():
    global wp_base_url, wp_username, wp_password  # move the global declaration here

    form = inputform()

    if form.validate_on_submit():
        # get user input from the form
        data = request.get_json()
        api_key = data.get('api_key')
        input_text = data.get('input_text')
        keywords = data.get('keywords')
        wp_base_url = data.get('wp_base_url')
        wp_username = data.get('wp_username')
        wp_password = data.get('wp_password')

        try:
            response = openai.chatcompletion.create(
                model=modelengine,
                messages=[{""role"": ""user"", ""content"": ""testing api key""}],
                n=1
            )

            if 'choices' not in response or not response['choices']:
                flash(""invalid api key. please check and try again."", ""error"")
                return render_template('index.html', form=form, data=none)

            # ... (rest of the code remains the same)
        except requests.exceptions.requestexception as e:
            flash(""error occurred while connecting to the openai api. please check your internet connection and try again."", ""error"")
            return render_template('index.html', form=form, data=none)

        except exception as e:
            flash(""an unexpected error occurred. please try again later."", ""error"")
            return render_template('index.html', form=form, data=none)

    return render_template('index.html', form=form, data=none)


if __name__ == '__main__':
    app.run(debug=true)

the requests  are getting sent to the server on debug but it doesn't run the script when pressing submit and the inputs doesn't render, how can i fix this?
index.html (updated):
<!doctype html>
<html>
<head>
    <title>openai wordpress web app</title>
</head>
<body>
    <h1>input your information:</h1>
    <form id=""inputform"" method=""post"">
        {{ form.csrf_token }}
        {{ form.api_key.label }} {{ form.api_key(id='api_key', size=60) }}<br>
        {{ form.input_text.label }} {{ form.input_text(id='input_text', rows=10, cols=60) }}<br>
        {{ form.keywords.label }} {{ form.keywords(id='keywords', rows=3, cols=60) }}<br>
        {{ form.wp_base_url.label }} {{ form.wp_base_url(id='wp_base_url', size=60) }}<br>
        {{ form.wp_username.label }} {{ form.wp_username(id='wp_username', size=60) }}<br>
        {{ form.wp_password.label }} {{ form.wp_password(id='wp_password', size=60) }}<br>
        {{ form.submit() }}
    </form>

    {% if data %}
        <h1>generated posts:</h1>
        {% for post in data %}
            <h2>{{ post.title }}</h2>
            <p>{{ post.content }}</p>
            <hr>
        {% endfor %}
    {% endif %}

    <script>
        document.getelementbyid('inputform').addeventlistener('submit', function (event) {
            event.preventdefault(); // prevent the form from submitting normally
            var api_key = document.getelementbyid('api_key').value;
            var input_text = document.getelementbyid('input_text').value;
            var keywords = document.getelementbyid('keywords').value;
            var wp_base_url = document.getelementbyid('wp_base_url').value;
            var wp_username = document.getelementbyid('wp_username').value;
            var wp_password = document.getelementbyid('wp_password').value;

            var xhr = new xml
            xhr.open('post', '/');
            xhr.setrequestheader('content-type', 'application/json');
            xhr.onreadystatechange = function () {
                if (xhr.readystate === xml {
                    if (xhr.status === 200) {
                        // request successful, update the page if necessary
                        var response = json.parse(xhr.responsetext);
                        if (response.error) {
                            alert('error: ' + response.error);
                        } else {
                            alert('data processed successfully!');
                            // optionally, you can update the page with the generated posts here
                        }
                    } else {
                        // handle error here
                        alert('error occurred. please try again later.');
                    }
                }
            };

            var data = json.stringify({
                api_key: api_key,
                input_text: input_text,
                keywords: keywords,
                wp_base_url: wp_base_url,
                wp_username: wp_username,
                wp_password: wp_password
            });

            xhr.send(data);
        });
    </script>
</body>
</html>","['python', 'flask', 'openai-api']",76759240,"edir your index.html :
    <!doctype html>
<html lang=""en"">
<head>
    <title>openai wordpress web app</title>
</head>
<body>
    <h1>input your information:</h1>
    <form id=""inputform"" method=""post"" action=""{{ url_for('index') }}"">
        {{ form.csrf_token }}
        {{ form.api_key.label }} {{ form.api_key(id='api_key', size=60) }}<br>
        {{ form.input_text.label }} {{ form.input_text(id='input_text', rows=10, cols=60) }}<br>
        {{ form.keywords.label }} {{ form.keywords(id='keywords', rows=3, cols=60) }}<br>
        {{ form.wp_base_url.label }} {{ form.wp_base_url(id='wp_base_url', size=60) }}<br>
        {{ form.wp_username.label }} {{ form.wp_username(id='wp_username', size=60) }}<br>
        {{ form.wp_password.label }} {{ form.wp_password(id='wp_password', size=60) }}<br>
        {{ form.submit() }}
    </form>

    {% if data %}
        <h1>generated posts:</h1>
        {% for post in data %}
            <h2>{{ post.title }}</h2>
            <p>{{ post.content }}</p>
            <hr>
        {% endfor %}
    {% endif %}


</body>
</html>

and it will be you flask wtf form :
from flask_wtf import flaskform
from wtforms import stringfield, textareafield, submitfield
from wtforms.validators import  datarequired

class inputform(flaskform):
    api_key = stringfield('openai api key', validators=[datarequired()])
    input_text = textareafield('input text', validators=[datarequired()])
    keywords = textareafield('keywords (comma-separated)', validators=[datarequired()])
    wp_base_url = stringfield('wordpress base url', validators=[datarequired()])
    wp_username = stringfield('wordpress username', validators=[datarequired()])
    wp_password = stringfield('wordpress password', validators=[datarequired()])
    submit = submitfield('submit')

and tour index route will be you can just add you rest of your code problem was getting information right ?
@app.route('/', methods=['get', 'post'])
def index():
    print(request.method) # testing request method coming from your index.html form
    form = inputform()
    if request.method == 'post' and form.validate_on_submit():
    # get user input from the for
        api_key = form.api_key.data
        input_text = form.input_text.data
        keywords = form.keywords.data
        wp_base_url = form.wp_base_url.data
        wp_username = form.wp_username.data
        wp_password = form.wp_password.data
        for item in form: # for testing your results
            print(item.data)
        # rest of your code 

return render_template('index.html', form=form, data=none)",https://stackoverflow.com/questions/76757241,python,24-07-2023 18:43,118.0,1.0,1.0,True,26-07-2023 06:38,25-07-2023 19:36,Implementation Issues
78001556,error while installing sentence-transformers,"get the following error while installing sentence-transformers on windows 11 (using the latest version of python and pip). can someone please help with this? checked many other similar posts, but none of those solutions work.
c:\users\abc\ai\llama\jupyterproj\stlit>py -m pip install sentence-transformers
collecting sentence-transformers
  using cached sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kb)
collecting transformers<5.0.0,>=4.32.0 (from sentence-transformers)
  using cached transformers-4.37.2-py3-none-any.whl.metadata (129 kb)
requirement already satisfied: tqdm in c:\users\abc\appdata\local\programs\python\python312\lib\site-packages (from sentence-transformers) (4.66.2)
requirement already satisfied: torch>=1.11.0 in c:\users\abc\appdata\local\programs\python\python312\lib\site-packages (from sentence-transformers) (2.2.0)
requirement already satisfied: numpy in c:\users\abc\appdata\local\programs\python\python312\lib\site-packages (from sentence-transformers) (1.26.4)
collecting scikit-learn (from sentence-transformers)
  using cached scikit_learn-1.4.0-1-cp312-cp312-win_amd64.whl.metadata (11 kb)
collecting scipy (from sentence-transformers)
  using cached scipy-1.12.0-cp312-cp312-win_amd64.whl.metadata (60 kb)
collecting nltk (from sentence-transformers)
  using cached nltk-3.8.1-py3-none-any.whl (1.5 mb)
collecting sentencepiece (from sentence-transformers)
  using cached sentencepiece-0.1.99.tar.gz (2.6 mb)
  preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  ï¿½ï¿½ python setup.py egg_info did not run successfully.
  ï¿½ï¿½ï¿½ exit code: 1
  ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½> [17 lines of output]
      traceback (most recent call last):
        file ""<string>"", line 2, in <module>
        file ""<pip-setuptools-caller>"", line 34, in <module>
        file ""c:\users\abc\appdata\local\temp\pip-install-3n9shirh\sentencepiece_fc383392079e43b6a8c226f0484c0928\setup.py"", line 126, in <moduocess.check_call([
        file ""c:\users\abc\appdata\local\programs\python\python312\lib\subprocess.py"", line 408, in check_call
          retcode = call(*popenargs, **kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
        file ""c:\users\abc\appdata\local\programs\python\python312\lib\subprocess.py"", line 389, in call
          with popen(*popenargs, **kwargs) as p:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
        file ""c:\users\abc\appdata\local\programs\python\python312\lib\subprocess.py"", line 1026, in __init__
          self._execute_child(args, executable, preexec_fn, close_fds,
        file ""c:\users\abc\appdata\local\programs\python\python312\lib\subprocess.py"", line 1538, in _execute_child
          hp, ht, pid, tid = _winapi.createprocess(executable, args,
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      filenotfounderror: [winerror 2] the system cannot find the file specified
      [end of output]

note: this error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed
ï¿½ï¿½ encountered error while generating package metadata.
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½> see above for output.
note: this is an issue with the package mentioned above, not pip.
hint: see above f","['python', 'python-3.x', 'pip', 'sentence-transformers']",78001637,"you need to use python 3.11 to install sentence-transformers
it is dependent on pytorch


currently, pytorch on windows only supports python 3.8-3.11; python 2.x is not supported.



we recommend python 3.8 or higher, pytorch 1.11.0 or higher and transformers v4.32.0 or higher. the code does not work with python 2.7",https://stackoverflow.com/questions/78001556,python,15-02-2024 14:26,1299.0,1.0,1.0,True,27-05-2024 06:21,27-05-2024 06:21,Implementation Issues
53316174,using pre-trained word embeddings - how to create vector for unknown / oov token?,"i wan't to add pre-trained embeddings to a model. but as it seems there is no out-of-vocabulary (oov) token resp. no vector for unseen words existent. 
so what can i do to handle oov-tokens i come across? i have some ideas, but none of them seem to be very good:

i could just create a random vector for this token, but ideally i'd like the vector to within the logic of the existing model. if i just create it randomly i'm afraid the vector accidentally could be very similar to a very frequent word like 'the', 'for', 'that' etc. which is not my intention.
or should i just initialize the vector with plain zeros instead?
another idea would be averaging the token over other existing vectors. but averaging on what vectors then? on all? this doesn't seem to be very conclusive either.
i also thought about trying to train this vector. however this doesn't come very handy if i want to freeze the rest of the embedding during training. 

(a general solution is appreciated, but i wanted to add that i'm using pytorch - just in case pytorch already comes with a handy solution to this problem.)
so what would be a good and easy strategy to create such a vector?","['neural-network', 'deep-learning', 'nlp', 'pytorch', 'word-embedding']",53329859,"there are multiple ways you can deal with it. i don't think i can cite references about which works better.
non-trainable option:

random vector as embedding
you can use an all-zero vector for oov.
you can use the mean of all the embedding vectors, that way you avoid the risk of being away from the actual distribution.
also embeddings generally come with ""unk"" vectors learned during the training you can use that.

trainable option:
you can declare a separate embedding vector for oov and make it trainable keeping other embedding fixed. you might have to over-write the forward method of embedding lookup for this. you can declare a new trainable variable and in the forward pass use this vector as embedding for oov instead of doing a look-up.

addressing the comments of op:
i am not sure which of the three non-trainable methods may work better and i am not sure if there is some work about this. but method 4) should be working better.
for trainable option, you can create a new embedding layer as below.
class embeddings_new(torch.nn.module): 
    def __init__(self, dim, vocab): 
        super().__init__() 
        self.embedding = torch.nn.embedding(vocab, dim) 
        self.embedding.weight.requires_grad = false
        # vector for oov 
        self.oov = torch.nn.parameter(data=torch.rand(1,dim)) 
        self.oov_index = -1 
        self.dim = dim 

    def forward(self, arr): 
        n = arr.shape[0] 
        mask =  (arr==self.oov_index).long() 
        mask_ = mask.unsqueeze(dim=1).float() 
        embed =(1-mask_)*self.embedding((1-mask)*arr) + mask_*(self.oov.expand((n,self.dim))) 
        return embed 

usage:
model = embeddings_new(10,20000)    
out = model.forward(torch.tensor([-1,-1, 100, 1, 0]))
# dummy loss
loss = torch.sum(a**2)
loss.backward()",https://stackoverflow.com/questions/53316174,neural-network,15-11-2018 09:26,3856.0,4.0,1.0,True,11-04-2022 17:56,15-11-2018 11:06,Implementation Issues
78837277,resourcenotfound error when connecting to azure openai using latest javascript sdk (version 2.0.0-beta.1),"i am trying to connect to azure openai using the latest version of sdk (version 2.0.0-beta.1) but no matter what i do, i am getting resource not found error). when i use the previous version (1.0.0-beta.12) of the sdk, everything works well.
these are the settings in my environment file:
azure_openai_endpoint=""
azure_openai_api_key=""xxxxyyyyxxxxyyyyxxxxyyyyxxxxyyyy""
azure_openai_chat_completion_model_deployment_id=""gpt-4o""
azure_openai_api_version=""2024-04-01-preview""

and this is how i am creating my client:
azureopenaiclient = new azureopenai({
    baseurl: process.env.azure_openai_endpoint,
    // apikey: process.env.azure_openai_api_key, (tried without commenting it as well)
    deployment: process.env.azure_openai_chat_completion_model_deployment_id,
    apiversion: process.env.azure_openai_api_version,
});

and this is my code:
const messages: openai.chatcompletionmessageparam[] = [
    { role: 'system', content: systemmessage },
    { role: 'user', content: usermessage },
    ];
const result = await azureopenaiclient.chat.completions.create(
    {
        messages,
        model: '',
        response_format: { type: jsonoutput ? 'json_object' : 'text' },
    },
    {},
    );
let response = '';
for await (const choice of result.choices) {
    response += choice.message?.content;
}

i even tried changing the model to my deployment id above but the result is the same.
i am pretty sure i am missing something really trivial but i am not able to figure it out. can anyone tell me what i am doing wrong here? thanks.
update
here's the stack trace:
notfounderror: 404 resource not found
    at apierror.generate (webpack-internal:///(action-browser)/../../node_modules/openai/error.mjs:67:20)
    at azureopenai.makestatuserror (webpack-internal:///(action-browser)/../../node_modules/openai/core.mjs:304:65)
    at azureopenai.makerequest (webpack-internal:///(action-browser)/../../node_modules/openai/core.mjs:347:30)
    at process.processticksandrejections (node:internal/process/task_queues:95:5)
....rest is the list of my files","['typescript', 'azure', 'openai-api', 'azure-openai']",78841338,"the issue was that i was trying to set baseurl property when i was instantiating azureopenai. the correct property is endpoint.
here's the github issue that solved my problem:",https://stackoverflow.com/questions/78837277,typescript,06-08-2024 04:40,529.0,2.0,1.0,True,06-08-2024 23:25,06-08-2024 04:58,Tool Setup/Errors
70996277,why flair does&#39;t recognize the entire location name of simple sentence?,"i'm tying to to detect simple location with ner algorithm, and i'm getting semi-correct results:
from flair.data   import sentence
from flair.models import sequencetagger

tagger   = sequencetagger.load('ner')
text     = 'jackson leaves at north carolina'
sentence = sentence(text)

tagger.predict(sentence)
for entity in sentence.get_spans('ner'):
    print(entity)

output:
span [1]: ""jackson""   [ï¿½ï¿½ï¿½ labels: per (0.9996)]
span [5]: ""carolina""   [ï¿½ï¿½ï¿½ labels: loc (0.7363)]

i was expecting to receive ""north carolina"".

can flair detect full location description? what do we need for it?
is there any ner algorithm that cat detect full location description?","['deep-learning', 'nlp', 'named-entity-recognition', 'flair']",72038013,"flair can detect full location description. the reason for your issue is that the 'north' is not capitalized.
if you run
from flair.data   import sentence
from flair.models import sequencetagger

tagger   = sequencetagger.load('ner')
text     = 'jackson leaves at north carolina'
sentence = sentence(text)

tagger.predict(sentence)
for entity in sentence.get_spans('ner'):
    print(entity)

you'll get
span[0:1]: ""jackson"" ï¿½ï¿½ï¿½ per (0.9997)
span[3:5]: ""north carolina"" ï¿½ï¿½ï¿½ loc (0.9246)
</",https://stackoverflow.com/questions/70996277,deep-learning,05-02-2022 08:09,501.0,0.0,1.0,True,28-04-2022 04:21,05-02-2022 21:14,Implementation Issues
78084538,openai assistants api: how do i upload a file and use it as a knowledge base?,"my goal is to create a chatbot that i can provide a file to that holds a bunch of text, and then use the openai assistants api to actually use the file when querying my chatbot. i will use the gpt-3.5-turbo model to answer the questions.
the code i have is the following:
file_response = client.files.create(
   file=open(""website_content.txt"", ""rb""),
   purpose=""assistants""
)

query_response = client.assistants.query(
   assistant_id=""my_assistant_id"", 
   input=""tell me about xxx?"",
   files=[file_response['id']] 
)

however, this is not working, for what i think could be a few things. for one, i don't fully understand the way it is supposed to work, so i was looking for some guidance. i have already created an assistant via the dashboard, but now i want to just upload a file and then query it. do i have to use something else, like ""threads"" via the api, or no?
how do i do this?","['python', 'artificial-intelligence', 'openai-api', 'chatgpt-api', 'openai-assistants-api']",78103896,"note: the code below works with the openai assistants api v1. in april 2024, the openai assistants api v2 was released. see the migration guide.

i created a customer support chatbot and made a youtube tutorial about it.
the process is as follows:
step 1: upload a file with an ""assistants"" purpose
my_file = client.files.create(
  file=open(""knowledge.txt"", ""rb""),
  purpose='assistants'
)

step 2: create an assistant
my_assistant = client.beta.assistants.create(
    model=""gpt-3.5-turbo-1106"",
    instructions=""you are a customer support chatbot. use your knowledge base to best respond to customer queries."",
    name=""customer support chatbot"",
    tools=[{""type"": ""retrieval""}]
)

step 3: create a thread
my_thread = client.beta.threads.create()

step 4: add a message to a thread
my_thread_message = client.beta.threads.messages.create(
  thread_id=my_thread.id,
  role=""user"",
  content=""what can i buy in your online store?"",
  file_ids=[my_file.id]
)

step 5: run the assistant
my_run = client.beta.threads.runs.create(
  thread_id=my_thread.id,
  assistant_id=my_assistant.id,
)

step 6: periodically retrieve the run to check on its status to see if it has moved to completed
keep_retrieving_run = client.beta.threads.runs.retrieve(
    thread_id=my_thread.id,
    run_id=my_run.id
)

step 7: retrieve the messages added by the assistant to the thread once the run status is ""completed""
all_messages = client.beta.threads.messages.list(
    thread_id=my_thread.id
)

print(f""user: {my_thread_message.content[0].text.value}"")
print(f""assistant: {all_messages.data[0].content[0].text.value}"")

see the full code.
important note
the assistant might sometimes behave strangely. the assistants api is still in beta, and it seems that openai has trouble keeping it realiable, as discussed on the official openai forum.
the assistant might sometimes answer that it cannot access the files you uploaded. you might think you did something wrong, but if you run identical code later or the next day, the assistant will successfully access all files and give you an answer.
the weird responses i got were the following:

assistant: i currently do not have access to the file you uploaded.
could you provide some details about what you're selling or any
specific questions you have in mind?
assistant: i currently don't have the ability to directly access the
contents of the file you uploaded. however, if you can provide some
details or specific questions about the than happy to assist you in
finding the information you need.
assistant: i currently don't have visibility into the specific
contents of the file you've uploaded. could you provide more details
about the file or its contents so that i can assist you further?
assistant: i see you've uploaded a file. how can i assist you with
it?",https://stackoverflow.com/questions/78084538,python,29-02-2024 22:06,18398.0,2.0,1.0,True,09-07-2024 17:56,22-03-2024 12:26,Implementation Issues
77990896,"importerror: dependencies for instructorembedding not found, while it is installed","i already installed instructorembedding, but it keeps giving me the error, in jupyter notebook environment using python 3.12 (i also tried in 3.11). kernel restarting didn't help.
import torch
from langchain.embeddings import huggingfaceinstructembeddings

device = ""cuda:0"" if torch.cuda.is_available() else ""cpu""


embedding = huggingfaceinstructembeddings(model_name=""sentence-transformers/all-minilm-l6-v2"", model_kwargs={""device"": device})

error:
---------------------------------------------------------------------------
modulenotfounderror                       traceback (most recent call last)
file /opt/conda/lib/python3.11/site-packages/langchain_community/embeddings/huggingface.py:151, in huggingfaceinstructembeddings.__init__(self, **kwargs)
    150 try:
--> 151     from instructorembedding import instructor
    153     self.client = instructor(
    154         self.model_name, cache_folder=self.cache_folder, **self.model_kwargs
    155     )

file /opt/conda/lib/python3.11/site-packages/instructorembedding/__init__.py:1
----> 1 from .instructor import *

file /opt/conda/lib/python3.11/site-packages/instructorembedding/instructor.py:9
      8 from torch import tensor, device
----> 9 from sentence_transformers import sentencetransformer
     10 from sentence_transformers.models import transformer

modulenotfounderror: no module named 'sentence_transformers'

the above exception was the direct cause of the following exception:

importerror                               traceback (most recent call last)
cell in[2], line 10
      4 device = ""cuda:0"" if torch.cuda.is_available() else ""cpu""
      6 #loader = pypdfdirectoryloader(""aircraft_pdfs"")
      7 #docs = loader.load()
      8 #print(len(docs))  # length of all pages together
---> 10 embedding = huggingfaceinstructembeddings(model_name=""sentence-transformers/all-minilm-l6-v2"", model_kwargs={""device"": device})

file /opt/conda/lib/python3.11/site-packages/langchain_community/embeddings/huggingface.py:157, in huggingfaceinstructembeddings.__init__(self, **kwargs)
    153     self.client = instructor(
    154         self.model_name, cache_folder=self.cache_folder, **self.model_kwargs
    155     )
    156 except importerror as e:
--> 157     raise importerror(""dependencies for instructorembedding not found."") from e

importerror: dependencies for instructorembedding not found.

here is the output of pip freeze
transformers==4.37.2
torch==2.2.0
langchain==0.1.6
instructorembedding==1.0.1
...","['python', 'langchain', 'sentence-transformers']",77990923,"i think you would also need to install sentence-transformers. try installing it via:
pip install -u sentence-transformers==2.2.2

and then run your code. please make sure you install the version 2.2.2 otherwise you'll end up with this error:
typeerror: instructor._load_sbert_model() got an unexpected keyword argument 'token'

it seems the latest version of sentence-transformers has some compatibility issues.",https://stackoverflow.com/questions/77990896,python,13-02-2024 21:13,10112.0,2.0,2.0,True,09-03-2024 21:03,09-03-2024 21:03,Tool Setup/Errors
76883181,running sentence transformers at pythonanywhere,"i am trying to run a huggingface model for computing vector embeddings as explained here at pythonanywhere (it worked just fine locally on my laptop under ubuntu under wsl2).
the installation went fine:
pip install -u sentence-transformers

however, when i run the following code:
from sentence_transformers import sentencetransformer
import time

def ms_now():
    return int(time.time_ns() / 1000000)

class timer():
    def __init__(self):
        self.start = ms_now()
    
    def stop(self):
        return ms_now() - self.start

sentences = [""this is an example sentence each sentence is converted""] * 10

timer = timer()
model = sentencetransformer('sentence-transformers/all-minilm-l6-v2')
print(""model initialized"", timer.stop())
for _ in range(10):
    timer = timer()
    embeddings = model.encode(sentences)
    print(timer.stop())

i get the error:
traceback (most recent call last):
  file ""/home/drmeir/test/test.py"", line 17, in <module>
    model = sentencetransformer('sentence-transformers/all-minilm-l6-v2')
  file ""/home/drmeir/.local/lib/python3.9/site-packages/sentence_transformers/sentencetransformer.py"", line 95, in __init__
    modules = self._load_sbert_model(model_path)
  file ""/home/drmeir/.local/lib/python3.9/site-packages/sentence_transformers/sentencetransformer.py"", line 840, in _load_sbert_model
    module = module_class.load(os.path.join(model_path, module_config['path']))
  file ""/home/drmeir/.local/lib/python3.9/site-packages/sentence_transformers/models/transformer.py"", line 137, in load
    return transformer(model_name_or_path=input_path, **config)
  file ""/home/drmeir/.local/lib/python3.9/site-packages/sentence_transformers/models/transformer.py"", line 29, in __init__
    self._load_model(model_name_or_path, config, cache_dir)
  file ""/home/drmeir/.local/lib/python3.9/site-packages/sentence_transformers/models/transformer.py"", line 49, in _load_model
    self.auto_model = automodel.from_pretrained(model_name_or_path, config=config, cache_dir=cache_dir)
  file ""/home/drmeir/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py"", line 493, in from_pretrained
    return model_class.from_pretrained(
  file ""/home/drmeir/.local/lib/python3.9/site-packages/transformers/modeling_utils.py"", line 2903, in from_pretrained
    ) = cls._load_pretrained_model(
  file ""/home/drmeir/.local/lib/python3.9/site-packages/transformers/modeling_utils.py"", line 3061, in _load_pretrained_model
    id_tensor = id_tensor_storage(tensor) if tensor.device != torch.device(""meta"") else id(tensor)
runtimeerror: expected one of cpu, cuda, xpu, mkldnn, opengl, opencl, ideep, hip, msnpu, xla, vulkan device type at start of device string: meta

they have torch 1.8.1+cpu at pythonanywhere. on my laptop, it's 2.0.1.
what is the reason for the error and how can i get this to work?","['python', 'pytorch', 'huggingface-transformers', 'pythonanywhere', 'sentence-transformers']",76883714,"as mentioned in the comments, the meta device was added in the pytorch version 1.9. and the pythonanywhere comes with pytorch version 1.8.1.
downgrading transformers library to 4.6.0 which was released in may 12, 2021 (before torch 1.9 was released) solved this issue.",https://stackoverflow.com/questions/76883181,python,11-08-2023 11:34,792.0,2.0,1.0,True,11-08-2023 12:56,11-08-2023 12:56,Implementation Issues
71666450,how to get average pairwise cosine similarity per group in pandas,"i have a sample dataframe as below
df=pd.dataframe(np.array([['facebook', ""women tennis""], ['facebook', ""men basketball""], ['facebook', 'club'],['apple', ""vice president""], ['apple', 'swimming contest']]),columns=['firm','text'])


now i'd like to calculate the degree of text similarity within each firm using word embedding. for example, the average cosine similarity for facebook would be the cosine similarity between row 0, 1, and 2. the final dataframe should have a column ['mean_cos_between_items'] next to each row for each firm. the value will be the same for each company, since it is a within-firm pairwise comparison.
i wrote below code:
import gensim
from gensim import utils
from gensim.models import word2vec
from gensim.models import keyedvectors
from gensim.scripts.glove2word2vec import glove2word2vec
from sklearn.metrics.pairwise import cosine_similarity

 # map each word to vector space
    def represent(sentence):
        vectors = []
        for word in sentence:
            try:
                vector = model.wv[word]
                vectors.append(vector)
            except keyerror:
                pass
        return np.array(vectors).mean(axis=0)
    
    # get average if more than 1 word is included in the ""text"" column
    def document_vector(items):
        # remove out-of-vocabulary words
        doc = [word for word in items if word in model_glove.vocab]
        if doc:
            doc_vector = model_glove[doc]
            mean_vec=np.mean(doc_vector, axis=0)
        else:
            mean_vec = none
        return mean_vec
    
# get average pairwise cosine distance score 
def mean_cos_sim(grp):
   output = []
   for i,j in combinations(grp.index.tolist(),2 ): 
       doc_vec=document_vector(grp.iloc[i]['text'])
       if doc_vec is not none and len(doc_vec) > 0:      
           sim = cosine_similarity(document_vector(grp.iloc[i]['text']).reshape(1,-1),document_vector(grp.iloc[j]['text']).reshape(1,-1))
           output.append([i, j, sim])
       return np.mean(np.array(output), axis=0)

# save the result to a new column    
df['mean_cos_between_items']=df.groupby(['firm']).apply(mean_cos_sim)

however, i got below error:

could you kindly help? thanks!","['python', 'pandas', 'nlp']",71667247,"remove the .vocab here in model_glove.vocab, this is not supported in the current version of gensim any more: edit: also needs split() to iterate over words and not characters here.
# get average if more than 1 word is included in the ""text"" column
def document_vector(items):
    # remove out-of-vocabulary words
    doc = [word for word in items.split() if word in model_glove]
    if doc:
        doc_vector = model_glove[doc]
        mean_vec = np.mean(doc_vector, axis=0)
    else:
        mean_vec = none
    return mean_vec

here you iterate over tuples of indices when you want to iterate over the values, so drop the .index. also you put all values in output including the words (/indices) i and j, so if you want to get their average you would have to specify what exactly you want the average over. since you seem to not need i and j you can just put only the resulting sims in a list and then take the lists average:
# get pairwise cosine similarity score
def mean_cos_sim(grp):
    output = []
    for i, j in combinations(grp.tolist(), 2):
        if document_vector(i) is not none and len(document_vector(i)) > 0:
            sim = cosine_similarity(document_vector(i).reshape(1, -1), document_vector(j).reshape(1, -1))
            output.append(sim)
    return np.mean(output, axis=0)

here you try to add the results as a column but the number of rows is going to be different as the result dataframe only has one row per firm while the original dataframe has one per text. so you have to create a new dataframe (which you can optionally then merge/join with the original dataframe based on the firm column):
df = pd.dataframe(np.array(
    [['facebook', ""women tennis""], ['facebook', ""men basketball""], ['facebook', 'club'],
     ['apple', ""vice president""], ['apple', 'swimming contest']]), columns=['firm', 'text'])
df_grpd = df.groupby(['firm'])[""text""].apply(mean_cos_sim)

which overall will give you (edit: updated):
print(df_grpd)
> firm
  apple       [[0.53190523]]
  facebook    [[0.83989316]]
  name: text, dtype: object

edit:
i just noticed that the reason for the super high score is that this is missing a tokenization, see the changed part. without the split() this just compares character similarities which tend to be super high.",https://stackoverflow.com/questions/71666450,python,29-03-2022 17:33,1992.0,2.0,2.0,True,29-03-2022 20:51,29-03-2022 18:23,Implementation Issues
70976353,"after installing scrubadub_spacy package, spacy.load(&quot;en_core_web_sm&quot;) not working oserror: [e053] could not read config.cfg","i am getting the below error when i'm trying to run the following line of code to load en_core_web_sm in the azure machine learning instance.
i debugged the issue and found out that once i install scrubadub_spacy, that seems is the issue causing the error.
spacy.load(""en_core_web_sm"")

oserror                                   traceback (most recent call last)
<ipython-input-2-c6e652d70518> in <module>
     1 # load english tokenizer, tagger, parser and ner
----> 2 nlp = spacy.load(""en_core_web_sm"")

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/spacy/__init__.py in load(name, vocab, disable, exclude, config)
    50     """"""
    51     return util.load_model(
---> 52         name, vocab=vocab, disable=disable, exclude=exclude, config=config
    53     )
    54 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/spacy/util.py in load_model(name, vocab, disable, exclude, config)
   418             return get_lang_class(name.replace(""blank:"", """"))()
   419         if is_package(name):  # installed as package
--> 420             return load_model_from_package(name, **kwargs)  # type: ignore[arg-type]
   421         if path(name).exists():  # path to model data directory
   422             return load_model_from_path(path(name), **kwargs)  # type: ignore[arg-type]

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/spacy/util.py in load_model_from_package(name, vocab, disable, exclude, config)
   451     """"""
   452     cls = importlib.import_module(name)
--> 453     return cls.load(vocab=vocab, disable=disable, exclude=exclude, config=config)  # type: ignore[attr-defined]
   454 
   455 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/en_core_web_sm/__init__.py in load(**overrides)
    10 
    11 def load(**overrides):
---> 12     return load_model_from_init_py(__file__, **overrides)

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config)
   619         disable=disable,
   620         exclude=exclude,
--> 621         config=config,
   622     )
   623 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config)
   485     config_path = model_path / ""config.cfg""
   486     overrides = dict_to_dot(config)
--> 487     config = load_config(config_path, overrides=overrides)
   488     nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude)
   489     return nlp.from_disk(model_path, exclude=exclude, overrides=overrides)

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/spacy/util.py in load_config(path, overrides, interpolate)
   644     else:
   645         if not config_path or not config_path.exists() or not config_path.is_file():
--> 646             raise ioerror(errors.e053.format(path=config_path, name=""config.cfg""))
   647         return config.from_disk(
   648             config_path, overrides=overrides, interpolate=interpolate

oserror: [e053] could not read config.cfg from /anaconda/envs/azureml_py36/lib/python3.6/site-packages/en_core_web_sm/en_core_web_sm-2.3.1/config.cfg

i installed the packages using the below three lines codes from spacy
pip install -u pip setuptools wheel
pip install -u spacy
python -m spacy download en_core_web_sm

how should i fix this issue? thanks in advance.","['python', 'python-3.6', 'spacy', 'azure-machine-learning-service', 'oserror']",71004266,"taking the path from your error message:
en_core_web_sm-2.3.1/config.cfg

you have a model for v2.3, but it's looking for a config.cfg, which is only a thing in v3 of spacy. it looks like you upgraded spacy without realizing it.
there are two ways to fix this. one is to reinstall the model with spacy download, which will get a version that matches your current spacy version. if you are just starting something that is probably the best idea. based on the release date of scrubadub, it seems to be intended for use with spacy v3.
however, note that v2 and v3 are pretty different - if you have a project with v2 of spacy you might want to downgrade instead.",https://stackoverflow.com/questions/70976353,python,03-02-2022 18:19,508.0,2.0,1.0,True,06-02-2022 04:46,04-02-2022 18:12,Tool Setup/Errors
78912227,"typeerror: expected a runnable, callable or dict.instead got an unsupported type: &lt;class &#39;list&#39;&gt;","i am experimenting with llm development.
here is my code:
import langchain, pydantic, transformers
from langchain import huggingfacepipeline
from langchain_core.prompts import prompttemplate
from langchain_core.runnables.base import runnablesequence, runnablemap, runnablelambda

from langchain.callbacks import get_openai_callback
from pydantic import basemodel, field
from langchain.output_parsers import pydanticoutputparser
from transformers import pipeline

class medicalspecialty(basemodel):
    medical_specialty: str = field(description=""medical specialty the patient should go to"")
    urgent: bool = field(description=""the patient should go to the hospital immediately"")

parser = pydanticoutputparser(pydantic_object=medicalspecialty)

queries = [""i have ache in my chest and in my left arm. which medical specialty should i go to?""]

template = """"""
question: {question}
""""""
prompt = prompttemplate(template=template, input_variables=[""question""])


llm = huggingfacepipeline.from_model_id(
    model_id=""bigscience/bloom-1b7"",
    task=""text-generation"",
    model_kwargs={""max_length"": 1024},
    device=-1  # ensure it runs on cpu for macos m1
)

# wrap the prompt in a runnablelambda to make it a runnable
prompt_runnable = runnablelambda(lambda x: prompt.format(**x))

# define the sequence that includes the prompt and llm
sequence = runnablesequence([
    prompt_runnable,
    llm
])

with get_openai_callback() as cb:
    for query in queries:
        result = sequence.invoke({""question"": query})
        print(query)
        print(result)
        print(""===================================="")
    
    # print the costs of the requests
    print(cb)

unfortunately, after several iterations, i keep getting this error:
typeerror                                 traceback (most recent call last)
cell in[6], line 19
     16 prompt_runnable = runnablelambda(lambda x: prompt.format(**x))
     18 # define the sequence that includes the prompt and llm
---> 19 sequence = runnablesequence([
     20     prompt_runnable,
     21     llm
     22 ])
     24 with get_openai_callback() as cb:
     25     for query in queries:

file /opt/anaconda3/envs/llm/lib/python3.11/site-    packages/langchain_core/runnables/base.py:2632, in runnablesequence.__init__(self,     name, first, middle, last, *steps)
   2630         steps_flat.extend(step.steps)
   2631     else:
-> 2632         steps_flat.append(coerce_to_runnable(step))
   2633 if len(steps_flat) < 2:
   2634     raise valueerror(
   2635         f""runnablesequence must have at least 2 steps, got     {len(steps_flat)}""
   2636     )

file /opt/anaconda3/envs/llm/lib/python3.11/site-    packages/langchain_core/runnables/base.py:5554, in coerce_to_runnable(thing)
   5552     return cast(runnable[input, output], runnableparallel(thing))
   5553 else:
-> 5554     raise typeerror(
   5555         f""expected a runnable, callable or dict.""
   5556         f""instead got an unsupported type: {type(thing)}""
   5557     )

typeerror: expected a runnable, callable or dict.instead got an unsupported type:     <class 'list'>

please, someone help!","['python', 'langchain', 'runnable', 'large-language-model', 'huggingface']",78913307,"no need to use lambda for this simple prompt with one input.
try to use below and report back here on what happens
prompt_runnable = promptrunnable(prompt)",https://stackoverflow.com/questions/78912227,python,25-08-2024 20:40,934.0,0.0,1.0,True,26-08-2024 07:42,25-08-2024 20:51,Implementation Issues
77042911,how to use multiple ner pipes with the same spacy nlp object?,"i trained a custom ner model for specific entity types (say, drugs) that are different from those that come out of the box in the standard spacy models (org, person, etc.). is it possible to add the ner pipe from this custom model to another model that already contains the standard spacy ner pipe? i tried the following:
import spacy

custom_nlp = spacy.load('my_trained_model/model-best/') #trained with the gpu option from the spacy quickstart page
doc = custom_nlp('chantix is a drug')
print(doc.ents) # prints chantix as a drug, as expected

main_nlp = spacy.load('en_core_web_trf')
doc = custom_nlp('chantix is a drug')
print(doc.ents) # prints chantix as a product, as expected

main_nlp.add_pipe('ner', source=custom_nlp, name='custom_ner', before='ner')
print(main_nlp.pipe_names) # both custom_ner and ner are there in the expected order
doc = main_nlp('chantix is a drug')
print(doc.ents) # here hell breaks loose, basically any token becomes an entity","['spacy', 'named-entity-recognition']",77081284,"the problem is that your added custom_ner is listening to the transformer component from en_core_web_trf rather than the one from the custom_nlp pipeline, so it's not getting the right input and is producing nonsense.
you need to ""replace the listeners"" before you add the component to en_core_web_trf:
custom_nlp.replace_listeners(""transformer"", ""ner"", [""model.tok2vec""])
main_nlp.add_pipe('ner', source=custom_nlp, name='custom_ner', before='ner')

docs:",https://stackoverflow.com/questions/77042911,spacy,05-09-2023 08:28,240.0,0.0,1.0,True,11-09-2023 11:36,06-09-2023 09:44,Implementation Issues
77759685,how to return source documents when using langchain expression language (lcel)?,"most samples of using langchain's expression language (lcel) look like this:
chain = setup_and_retrieval | prompt | model | output_parser

how can i access the source_documents in a rag application when using this expression language?","['artificial-intelligence', 'langchain', 'py-langchain', 'retrieval-augmented-generation']",77759686,"this works well for me:
rag_chain = (
    runnablepassthrough.assign(source_documents=condense_question | retriever)
    | runnablepassthrough.assign(context=lambda inputs: format_docs(inputs[""source_documents""]) if inputs[""source_documents""] else """")
    | runnablepassthrough.assign(prompt=qa_prompt)
    | runnablepassthrough.assign(response=lambda inputs: llm(inputs[""prompt""].messages))
)

it's called like this:
response_dict = rag_chain.invoke({""question"": question, ""chat_history"": chat_history})
ai_msg = response_dict[""response""]
source_documents = response_dict[""source_documents""]

the way that helped me understand how to do it was this:

you initially pass a dictionary into the chain (in my case with the keys question and chat_history).
every time you use runnablepassthrough.assign, you can add stuff to that dictionary and then pass that on to the next step.
runnablepassthrough.assign always returns a dictionary.

this is what happens in my code example:

we use runnablepassthrough.assign to add a new source_documents key to the dictionary. its value is the result of calling the condense_question function (defined elsewhere) that builds and returns a condenser chain. its condensed result is passed into our retriever (also defined elsewhere).
we use runnablepassthrough.assign to add a new context key to the dictionary. its value is the result of calling a format_docs method (defined elsewhere) that combines the source_documents into a single context string.
we use runnablepassthrough.assign to add a new prompt key to the dictionary. its value is the result of calling qa_prompt, which is defined as qa_prompt = chatprompttemplate.from_messages(...).
we use runnablepassthrough.assign one more time to add a new response key to the dictionary. its value is the result of actually calling the llm with the messages from our prompt.
the chain returns a dictionary with all the keys we've added along the way. the response key contains the llm's response as an aimessage, and the source_documents key contains the source documents.

i'm sure this can be done in a more concise way, but this worked for me and i can understand it :)",https://stackoverflow.com/questions/77759685,artificial-intelligence,04-01-2024 16:12,2654.0,3.0,2.0,True,03-06-2024 13:50,04-01-2024 20:33,Implementation Issues
69091576,string comparison with bert seems to ignore &quot;not&quot; in sentence,"i implemented a string comparison method using sentencetransformers and bert like following
from sentence_transformers import sentencetransformer
from sklearn.metrics.pairwise import cosine_similarity

model = sentencetransformer('sentence-transformers/all-distilroberta-v1')

sentences = [
    ""i'm a good person"",
    ""i'm not a good person""
]

sentence_embeddings = model.encode(sentences)

cosine_similarity(
    [sentence_embeddings[0]],
    sentence_embeddings[1:]
)

notice how my sentence examples are very similar but with the opposite meaning. the problem is the cosine similarity returns 0.9, indicating that these two strings are very similar in context when i expected it to return something closer to zero, as they have the opposite meanings.
how can i adapt my code to return a more accurate result?","['nlp', 'bert-language-model', 'transformer-model', 'sentence-similarity', 'sentence-transformers']",69260955,"tl;dr: nli is all you need
first, the cosine similarity is reasonably high, because the sentences are similar in the following sense:

they are about the same topic (evaluation of a person)
they are about the same subject (""i"") and the same property (""being a good person"")
they have similar syntactic structure
they have almost the same vocabulary

so, from the formal point of view, they should be considered similar. moreover, from the practical point of view, they should often be considered similar. for example, if you google ""gmo are causing cancer"", you might find that the text with label ""gmo are not causing cancer"" is relevant.
second, if you want to measure logical connection between sentences, cosine similarity of embeddings is just not expressive enough. this is because embeddings contain lots of semantic stylistic, lexical and syntactic information, but they are fixed-size (768-dimensional, in your case), so they cannot contain complete information about the meaning of both sentences. so you need another model with the following properties:

it encodes both texts simultaneously, so it compares the texts themselves, not just their fixed-size embeddings
it is explicitly trained to evaluate logical connection between sentences

the task of assesing logical connection between texts is called natural language inference (nli), and its most common formulation is recognizing textual entailment (rte): it is the problem of predicting whether the first sentence entails the second one.
there are lots of models trained for this task in the huggingface repo, with roberta-large-mnli being a good one. you can use it to evaluate equivalence of two texts. if each text entails another, they are equivalent, so you can estimate the degree of equivalence as the product of the entailment scores in both directions.
import torch
from transformers import autotokenizer, automodelforsequenceclassification

tokenizer = autotokenizer.from_pretrained(""roberta-large-mnli"")
model = automodelforsequenceclassification.from_pretrained(""roberta-large-mnli"")

def test_entailment(text1, text2):
    batch = tokenizer(text1, text2, return_tensors='pt').to(model.device)
    with torch.no_grad():
        proba = torch.softmax(model(**batch).logits, -1)
    return proba.cpu().numpy()[0, model.config.label2id['entailment']]

def test_equivalence(text1, text2):
    return test_entailment(text1, text2) * test_entailment(text2, text1)

print(test_equivalence(""i'm a good person"", ""i'm not a good person""))  # 2.0751484e-07
print(test_equivalence(""i'm a good person"", ""you are a good person""))  # 0.49342492
print(test_equivalence(""i'm a good person"", ""i'm not a bad person""))   # 0.94236994",https://stackoverflow.com/questions/69091576,nlp,07-09-2021 16:18,1421.0,4.0,3.0,True,10-04-2023 17:25,18-09-2021 18:10,Implementation Issues
77592601,why am i getting a rate limit error when i use the openai api for my first time?,"import openai

openai.api_key = ""myapikey""

completion = openai.chatcompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""give me 3 ideas for apps i could build with openai apis ""}])
print(completion.choices[0].message.content)

this is my code that i am trying to run. but whenever i run it, i keep getting this error message:
traceback (most recent call last):
  file ""/users/easwar/downloads/chatgpt api/01 chatgpt simple"", line 5, in <module>
    completion = openai.chatcompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""give me 3 ideas for apps i could build with openai apis ""}])
  file ""/users/easwar/library/python/3.9/lib/python/site-packages/openai/api_resources/chat_completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  file ""/users/easwar/library/python/3.9/lib/python/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  file ""/users/easwar/library/python/3.9/lib/python/site-packages/openai/api_requestor.py"", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  file ""/users/easwar/library/python/3.9/lib/python/site-packages/openai/api_requestor.py"", line 700, in _interpret_response
    self._interpret_response_line(
  file ""/users/easwar/library/python/3.9/lib/python/site-packages/openai/api_requestor.py"", line 765, in _interpret_response_line
    raise self.handle_error_response(
openai.error.ratelimiterror: you exceeded your current quota, please check your plan and billing details.

the main error is at the bottom.
usage limit
this is what my usage limit looks like on the openai website. i don't understand why most of it is expired because this is my first time using the api.
i would be really thankful to anyone who can help me fix this.","['openai-api', 'chatgpt-api']",77592657,"i had this same issue during my first time.
the problem is not from you code.
you have just used up you api key quota.
and the only way that you can fix it is by creating a very new openai account, and then requesting for the key on that one.
it will definitely work.
be careful not to use same details as this other accounts.",https://stackoverflow.com/questions/77592601,openai-api,02-12-2023 23:51,2749.0,1.0,2.0,True,25-04-2024 21:10,02-12-2023 23:51,Implementation Issues
70404372,attributeerror: &#39;field&#39; object has no attribute &#39;vocab&#39; preventing me to run the code,"i have found this code and i wanna see what is the object that im printing in the last line. im new in field of nlp so please help me fix this code, because it gives attributeerror: 'field' object has no attribute 'vocab'error. by the way i have found out that torchtext has been changed and the error is probably related to these changes, and the code probably was working before.
import spacy
from torchtext.legacy.data import field
spacy_eng = spacy.load(""en"")
def tokenize_eng(text):
    return [tok.text for tok in spacy_eng.tokenizer(text)]

english = field(
    tokenize=tokenize_eng, lower=true, init_token=""<sos>"", eos_token=""<eos>""
)
print([english.vocab.stoi[""<sos>""]])","['nlp', 'torchtext']",70412909,"you have to build the vocabulary for the english field before you try to access it. you will need a dataset to build the vocabulary, which will be the dataset you are looking to build a model for. you can use english.build_vocab(...). here are the docs for build_vocab.
also, if you would like to learn how to migrate what you are doing to the new version of torchtext, here is a good resource.",https://stackoverflow.com/questions/70404372,nlp,18-12-2021 14:47,1410.0,0.0,1.0,True,19-12-2021 15:43,18-12-2021 14:57,Implementation Issues
74107063,openai fine tuning each of the classes must start with a different token error,"i am trying to run a fine tune similar to the one in openai cookbook example for a multiclass classification problem. after preparing the train and valid jsonl files with fine_tunes.prepare_data, when i try to run the recommended fine_tunes.create command, i'm getting the following error:

if compute_classification_metrics is true, each of the classes must start with a different token. you can view your class tokenizations at  fine-tune failed. for help, please contact openai and include your fine-tune id.",['openai-api'],74107064,"looks like this error comes when the completion value is of more than a single token. after changing the completion values to numerical ids to ensure that they are of single tokens, the fine tune ran fine.
i'm not sure why the prepare_data step itself didn't say any error regarding this given that i used openai command line tool to prepare that.",https://stackoverflow.com/questions/74107063,openai-api,18-10-2022 07:14,1680.0,1.0,1.0,True,09-12-2022 23:54,09-12-2022 23:54,Implementation Issues
76448287,how can i solve importerror: using the `trainer` with `pytorch` requires `accelerate&gt;=0.20.1` when using huggingface&#39;s trainarguments?,"i'm using the transformers library in google colab, and
when i am using trainingarguments from transformers library i'm getting import error with this  code:
from transformers import trainingarguments

training_args = trainingarguments(
    output_dir = ""/content/our-model"",
    learning_rate=2e-5,
    per_device_train_batch_size= 64,
    per_device_eval_batch_size = 16,
    num_train_epochs = 2,
    weight_decay = 0.01,
    evaluation_strategy = ""epoch"",
    save_strategy = ""epoch"",
    load_best_model_at_end = true,
    push_to_hub = false
)

this is the error i'm getting:
<ipython-input-28-0518ea5ff407> in <cell line: 2>()
      1 from transformers import trainingarguments
----> 2 training_args = trainingarguments(
      3     output_dir = ""/content/our-model"",
      4     learning_rate=2e-5,
      5     per_device_train_batch_size= 64,

4 frames
/usr/local/lib/python3.10/dist-packages/transformers/training_args.py in _setup_devices(self)
   1670         if not is_sagemaker_mp_enabled():
   1671             if not is_accelerate_available(min_version=""0.20.1""):
-> 1672                 raise importerror(
   1673                     ""using the `trainer` with `pytorch` requires `accelerate>=0.20.1`: please run `pip install transformers[torch]` or `pip install accelerate -u`""
   1674                 )

importerror: using the `trainer` with `pytorch` requires `accelerate>=0.20.1`: please run `pip install transformers[torch]` or `pip install accelerate -u 

i already tried pip install for 0.20.1 version of accelerate and pip install transformers[torch]
and both didn't worked.","['python', 'nlp', 'importerror', 'huggingface-transformers', 'huggingface']",76452964,"if you're not particular about which transformers and accelerate version to tie to, then do this to use the most up-to-date version in google colab:
! pip install -u accelerate
! pip install -u transformers

then the issue you are having with accelerate should auto-resolve itself.
note:

underspecifying pip install -u transformers instead of pip install transformers[pytorch] might be easier since that's what most of the users do and the developers of the library will make sure that the basic pip works with the common functions and class like trainingarguments

instead of specifying accelerate to the pip install accelerate>=0.20.1, if you have no particular need to fixed the version, automatically upgrading to the latest version might get you more stability when using the library, esp. with ""hot""/""trending"" libraries that are constantly changing (almost) daily.



if further debugging is necessary, i.e. if the above didn't work. to check your transformers and accelerate version, do this:
import accelerate

accelerate.__version__

most probably you might have an importerror at the first line if accelerate is not already installed when you installed transformers.
and then if the first line works and the 2nd line is not outputting a version >=0.20.1, then that is the cause of your issue.
the current versions to-date (july 2023) are:
import accelerate
import transformers

transformers.__version__, accelerate.__version__

[out]:
('4.30.1', '0.21.0')

here's an example notebook with the model that you wish to use as per the comments in your question, 

if the error persist after the pip install ..., try restarting the runtime.
if you can't find the buttons to press to restart, try this in the cell restart kernel in google colab then re-run the cells for import ...
import os
os._exit(00)",https://stackoverflow.com/questions/76448287,python,10-06-2023 21:51,42296.0,27.0,4.0,True,12-06-2024 05:57,12-06-2023 00:43,Implementation Issues
77679383,validationerror: 1 validation error for structuredtool,"i was getting an error when trying to use a pydantic schema as an args_schema parameter value on a @tool decorator, following the deeplearning.ai course.
my code was:
from pydantic import basemodel, field

class searchinput(basemodel):
    query: str = field(description=""thing to search for"")

@tool(args_schema=searchinput)
def search(query: str) -> str:
    """"""searches for weather online""""""
    return ""21c""

and was getting this error:
validationerror: 1 validation error for structuredtool
args_schema subclass of basemodel expected (type=type_error.subclass; expected_class=basemodel)","['python', 'openai-api', 'pydantic', 'langchain']",77679407,"downgrading to pydantic 1.10.10 worked for me.
add pydantic==1.10.10 to your requirements.txt and install it with pip install -r requirements.txt
or with the command pip install pydantic==1.10.10",https://stackoverflow.com/questions/77679383,python,18-12-2023 13:23,3616.0,3.0,2.0,True,21-12-2023 06:03,18-12-2023 13:28,Tool Setup/Errors
71113891,spacy tokenization add extra white space for dates with hyphen separator when i manually build the doc,"i've been trying to solve a problem with the spacy tokenizer for a while, without any success. also, i'm not sure if it's a problem with the tokenizer or some other part of the pipeline.
description
i have an application that for reasons besides the point, creates a spacy doc from the spacy vocab and the list of tokens from a string (see code below). note that while this is not the simplest and most common way to do this, according to spacy doc this can be done.
however, when i create a doc for a text that contains compound words or dates with hyphen as a separator, the behavior i am getting is not what i expected.
import spacy
from spacy.language import doc

# my current way
doc = doc(nlp.vocab, words=tokens)  # tokens is a well defined list of tokens for a certein string

# standard way
doc = nlp(""my text..."")

for example, with the following text, if i create the doc using the standard procedure, the spacy tokenizer recognizes the ""-"" as tokens but the doc text is the same as the input text, in addition the spacy ner model correctly recognizes the date entity.
import spacy

doc = nlp(""what time will sunset be on 2022-12-24?"")
print(doc.text)

tokens = [str(token) for token in doc]
print(tokens)

# show entities
print(doc.ents[0].label_)
print(doc.ents[0].text)

output:
what time will sunset be on 2022-12-24?
['what', 'time', 'will', 'sunset', 'be', 'on', '2022', '-', '12', '-', '24', '?']

date
2022-12-24

on the other hand, if i create the doc from the model's vocab and the previously calculated tokens, the result obtained is different. note that for the sake of simplicity i am using the tokens from doc, so i'm sure there are no differences in tokens. also note that i am manually running each pipeline model in the correct order with the doc, so at the end of this process i would theoretically get the same results.
however, as you can see in the output below, while the doc's tokens are the same, the doc's text is different, there were blank spaces between the digits and the date separators.
doc2 = doc(nlp.vocab, words=tokens)

# run each model in pipeline
for model_name in nlp.pipe_names:
    pipe = nlp.get_pipe(model_name)
    doc2 = pipe(doc2)

# print text and tokens
print(doc2.text)
tokens = [str(token) for token in doc2]
print(tokens)

# show entities
print(doc.ents[0].label_)
print(doc.ents[0].text)

output:
what time will sunset be on 2022 - 12 - 24 ? 
['what', 'time', 'will', 'sunset', 'be', 'on', '2022', '-', '12', '-', '24', '?']

date
2022 - 12 - 24

i know it must be something silly that i'm missing but i don't realize it.
could someone please explain to me what i'm doing wrong and point me in the right direction?
thanks a lot in advance!
edit
following the talha tayyab suggestion, i have to create an array of booleans with the same length that my list of tokens to indicate for each one, if the token is followed by an empty space. then pass this array in doc construction as follows: doc = doc(nlp.vocab, words=words, spaces=spaces).
to compute this list of boolean values ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½based on my original text string and list of tokens, i imp following vanilla function:
def get_spaces(self, text: str, tokens: list[str]) -> list[bool]:
     
    # spaces
    spaces = []
    # copy text to easy operate
    t = text.lower()

    # iterate over tokens
    for token in tokens:

        if t.startswith(token.lower()):

            t = t[len(token):]  # remove token

            # if after removing token we have an empty space
            if len(t) > 0 and t[0] == "" "":
                spaces.append(true)
                t = t[1:]  # remove space
            else:
                spaces.append(false)

    return spaces


with these two improvements in my code, the result obtained is as expected. however, now i have the following question:
is there a more spacy-like way to compute whitespace, instead of using my vanilla implementation?","['python', 'python-3.x', 'nlp', 'tokenize', 'spacy-3']",71118272,"please try this:
from spacy.language import doc
doc2 = doc(nlp.vocab, words=tokens,spaces=[1,1,1,1,1,1,0,0,0,0,0,0])
# run each model in pipeline
for model_name in nlp.pipe_names:
    pipe = nlp.get_pipe(model_name)
    doc2 = pipe(doc2)

# print text and tokens
print(doc2.text)
tokens = [str(token) for token in doc2]
print(tokens)

# show entities
print(doc.ents[0].label_)
print(doc.ents[0].text)

# you can also replace 0 with false and 1 with true

this is the complete syntax:
doc = doc(nlp.vocab, words=words, spaces=spaces)

spaces are a list of boolean values indicating whether each word has a subsequent space. must have the same length as words, if specified. defaults to a sequence of true.
so you can choose which ones you gonna have space and which ones you do not need.
reference:",https://stackoverflow.com/questions/71113891,python,14-02-2022 15:00,1558.0,3.0,2.0,True,18-01-2023 11:25,18-01-2023 11:25,Implementation Issues
34721984,stopword removing when using the word2vec,"i have been trying word2vec for a while now using the gensim's word2vec library. my question is do i have to remove stopwords from my input text?  because, based on my initial experimental results, i could see words like 'of', 'when'.. (stopwords) popping up when i do a model.most_similar('someword')..?
but i didn't see anywhere referring that stop word removal is necessary with word2vec? does the word2vec is supposed to handle stop words even if you don't remove them?
what are the must do pre processing things (like for topic modeling, it's almost a must that you should do stopword removal)?","['nlp', 'gensim', 'word2vec']",34737150,"personaly i think, removal of stop word will give better results, check link
also for topic modeling, you shlould perform preprocessing on the text, following things you must do,

remove of stop words.
tokenization.
stemming and lemmatization.",https://stackoverflow.com/questions/34721984,nlp,11-01-2016 12:49,21661.0,27.0,3.0,True,03-09-2022 11:51,12-01-2016 06:25,Task-specific Help
73698110,keras model fit throws shape mismatch error,"i am building a siamese network using keras(tensorflow) where the target is a binary column, i.e., match or mismatch(1 or 0). but the model fit method throws an error saying that the y_pred is not compatible with the y_true shape. i am using the binary_crossentropy loss function.
here is the error i see:

here is the code i am using:
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[tf.keras.metrics.recall()])
history = model.fit([x_train_entity_1.todense(),x_train_entity_2.todense()],np.array(y_train),
                    epochs=2, 
                    batch_size=32,
                    verbose=2,
                    shuffle=true)

my input data shapes are as follows:
inputs:
x_train_entity_1.shape is (700,2822)
x_train_entity_2.shape is (700,2822)

target:
y_train.shape is (700,1)

in the error it throws, y_pred is the variable which was created internally. what is y_pred dimension is 2822 when i am having a binary target. and 2822 dimension actually matches the input size, but how do i understand this?
here is the model i created:
in_layers = []
out_layers = []
for i in range(2):
  input_layer = input(shape=(1,))
  embedding_layer = embedding(embed_input_size+1, embed_output_size)(input_layer)
  lstm_layer_1 = bidirectional(lstm(1024, return_sequences=true,recurrent_dropout=0.2, dropout=0.2))(embedding_layer)
  lstm_layer_2 = bidirectional(lstm(512, return_sequences=true,recurrent_dropout=0.2, dropout=0.2))(lstm_layer_1)

  in_layers.append(input_layer)
  out_layers.append(lstm_layer_2)

merge = concatenate(out_layers)
dense1 = dense(256, activation='relu', kernel_initializer='he_normal', name='data_embed')(merge)
drp1 = dropout(0.4)(dense1)
btch_norm1 = batchnormalization()(drp1)
dense2 = dense(32, activation='relu', kernel_initializer='he_normal')(btch_norm1)
drp2 = dropout(0.4)(dense2)
btch_norm2 = batchnormalization()(drp2)
output = dense(1, activation='sigmoid')(btch_norm2)
model = model(inputs=in_layers, outputs=output)
model.summary()

since my data is very sparse, i used todense. and there the type is as follows:
type(x_train_entity_1) is scipy.sparse.csr.csr_matrix
type(x_train_entity_1.todense()) is numpy.matrix
type(x_train_entity_2) is scipy.sparse.csr.csr_matrix
type(x_train_entity_2.todense()) is numpy.matrix

summary of last few layers as follows:","['tensorflow', 'keras', 'deep-learning', 'neural-network', 'nlp']",73698693,"mismatched shape in the input layer. the input shape needs to match the shape of a single element passed as x, or dataset.shape[1:]. so since your dataset size is (700,2822), that is 700 samples of size 2822. so your input shape should be 2822.
change:
input_layer = input(shape=(1,))

to:
input_layer = input(shape=(2822,))",https://stackoverflow.com/questions/73698110,tensorflow,13-09-2022 05:59,908.0,0.0,2.0,True,13-09-2022 07:30,13-09-2022 07:24,Tool Setup/Errors
67811619,"this method is deprecated, __call__ should be used instead, how to solve this problem in bert?","i am trying to use the batch_encode_plus() function from bert. the problem comes when it says that this function does not exist. i go to the documentation in this page and it says the following: ""this method is deprecated, __call__ should be used instead."". i tried to use __call__ as a function but it does not work.
then i get into this page and it seems that the function batch_encode_plus() was replaced by __call__. but when i use the function is not working. i tried to use encoded_plus() but it does not give the the expected results.
i do not know how to use this __call__ function, any suggestions?","['python', 'huggingface-transformers', 'huggingface-tokenizers']",76509315,"i've been working with this recently. here is a good look at some documentation for the call function
call( inputs*args**kwargs ) ï¿½ï¿½ï¿½ a list or a list of list of dict
parameters:

args (str or list[str]) ï¿½ï¿½ï¿½ one or several texts (or one list of prompts) with masked tokens.
targets (str or list[str], optional) ï¿½ï¿½ï¿½ when passed, the model will limit the scores to the passed targets instead of looking up in the whole vocab. if the provided targets are not in the model vocab, they will be tokenized and the first resulting token will be used (with a warning, and that might be slower).
top_k (int, optional) ï¿½ï¿½ï¿½ when passed, overrides the number of predictions to return.

returns: a list or a list of list of dict
here's an example of how i used it. (keep in mind that i'm using the fill-mask mode of bert which is a one word prediction model and the python language)
    from transformers imter = pipeline('fill-mask', model='bert-base-uncased')
    print(predicter.__call__(""example string. hello [mask]"",top_k=1)

the model will predict the word that is tokenized by [mask]
the example output for this is:
    {'score': 0.9415972232818604, 'token': 1037, 'token_str': 'kitty', 'sequence': 'example string. hello kitty'}",https://stackoverflow.com/questions/67811619,python,02-06-2021 19:47,688.0,1.0,1.0,True,22-06-2023 16:29,03-06-2021 21:42,Implementation Issues
78270035,pytorch cuda allocated memory is going into 100&#39;s of gb,"i am trying to get inference from huggingface transformer model running using pytorch framework. i have a gpu instance running and when i am checking the cuda memory summary, i find that allocated memory (total allocation) is increasing by 100's of gb's with each inference e.g. after 2nd inference allocated memory (total allocation)  was 19gb, with 3rd inference allocated memory (total allocation)  was 205gb. this total allocation is freed up. the memory maps don't show any anomalous pattern. current usage and peak usage from nearly constant. my inference sagemaker instance has 128gb of cpu memory only and 24 gb of gpu memory.

so, i have three queries/concerns:

how is it possible that total allocation memory is more than sagemaker instance on which inference is running.
how do i control this anomalous behaviour?
is this a concern that i need to rectify, as the inference seems to be running fine.","['memory-management', 'pytorch', 'huggingface-transformers', 'large-language-model']",78270162,"tot alloc and tot freed show the total amount of memory allocated or freed over the memory snapshot. they are accumulated stats.
cur usage shows how much memory is currently being used by your process. peak usage shows the highest amount of memory used at a single time.
peak usage is the main value you should be concerned about - you will get a cuda memory error if this value tries to exceed your gpu's memory.
additionally, the cuda memory profiler profiles cuda memory, not system memory. the values shown have nothing to do with system/cpu memory.",https://stackoverflow.com/questions/78270035,memory-management,03-04-2024 19:22,121.0,0.0,1.0,True,03-04-2024 21:37,03-04-2024 21:37,Implementation Issues
74160976,an error in implementing regex function on a list,"i was trying to implement a regex on a list of grammar tags in python, for finding the tense form of the list of grammar. and i wrote the following code to implement it.
data preprocessing:
from nltk import word_tokenize, pos_tag
import nltk

text = ""he will have been doing his homework."" 

tokenized = word_tokenize(text)
tagged = pos_tag(tokenized)
tags = []
for i in range(len(tagged)):
    t = tagged[i]
    tags.append(t[1])
print(tags)

regex formula i.e. to be implemented
grammar = r""""""
future_perfect_continuous: {<md><vb><vbn><vbg>}
future_continuous:         {<md><vb><vbg>}
future_perfect:            {<md><vb><vbn>}
past_perfect_continuous:   {<vbd><vbn><vbg>}
present_perfect_continuous:{<vbp|vbz><vbn><vbg>}
future_indefinite:         {<md><vb>}
past_continuous:           {<vbd><vbg>}
past_perfect:              {<vbd><vbn>}
present_continuous:        {<vbz|vbp><vbg>}
present_perfect:           {<vbz|vbp><vbn>}
past_indefinite:           {<vbd>}
present_indefinite:        {<vbz>|<vbp>}

function to implement the regex on the list tags
def check_grammar(grammar, tags):
    cp = nltk.regexpparser(grammar)
    result = cp.parse(tags)
    print(result)
    result.draw()
 
check_grammar(grammar, tags)

but it returned an error as:
traceback (most recent call last):
  file ""/home/samar/desktop/twitter_tense/main.py"", line 35, in <module>
    check_grammar(grammar, tags)
  file ""/home/samar/desktop/twitter_tense/main.py"", line 31, in check_grammar
    result = cp.parse(tags)
  file ""/home/samar/.local/lib/python3.8/site-packages/nltk/chunk/regexp.py"", line 1276, in parse
    chunk_struct = parser.parse(chunk_struct, trace=trace)
  file ""/home/samar/.local/lib/python3.8/site-packages/nltk/chunk/regexp.py"", line 1083, in parse
    chunkstr = chunkstring(chunk_struct)
  file ""/home/samar/.local/lib/python3.8/site-packages/nltk/chunk/regexp.py"", line 95, in __init__
    tags = [self._tag(tok) for tok in self._pieces]
  file ""/home/samar/.local/lib/python3.8/site-packages/nltk/chunk/regexp.py"", line 95, in <listcomp>
    tags = [self._tag(tok) for tok in self._pieces]
  file ""/home/samar/.local/lib/python3.8/site-packages/nltk/chunk/regexp.py"", line 105, in _tag
    raise valueerror(""chunk structures must contain tagged "" ""tokens or trees"")
valueerror: chunk structures must contain tagged tokens or trees","['python', 'nlp', 'nltk', 'nlp-question-answering']",74161157,"your call to the cp.parse() function expects each of the tokens in your sentence to be tagged, however, the tags list you created only contains the tags but not the tokens as well, hence your valueerror. the solution is to instead pass the output from the pos_tag() call (i.e. tagged) to your check_grammar call (see below).
solution
from nltk import word_tokenize, pos_tag
import nltk

text = ""he will have been doing his homework."" 
tokenized = word_tokenize(text)
tagged = pos_tag(tokenized)
print(tagged)
# output
>>> [('he', 'prp'), ('will', 'md'), ('have', 'vb'), ('been', 'vbn'), ('doing', 'vbg'), ('his', 'prp$'), ('homework', 'nn'), ('.', '.')]

my_grammar = r""""""
future_perfect_continuous: {<md><vb><vbn><vbg>}
future_continuous:         {<md><vb><vbg>}
future_perfect:            {<md><vb><vbn>}
past_perfect_continuous:   {<vbd><vbn><vbg>}
present_perfect_continuous:{<vbp|vbz><vbn><vbg>}
future_indefinite:         {<md><vb>}
past_continuous:           {<vbd><vbg>}
past_perfect:              {<vbd><vbn>}
present_continuous:        {<vbz|vbp><vbg>}
present_perfect:           {<vbz|vbp><vbn>}
past_indefinite:           {<vbd>}
present_indefinite:        {<vbz>|<vbp>}""""""


def check_grammar(grammar, tags):
    cp = nltk.regexpparser(grammar)
    result = cp.parse(tags)
    print(result)
    result.draw()


check_grammar(my_grammar, tagged)

output
>>> (s
>>>   he/prp
>>>   (future_perfect_continuous will/md have/vb been/vbn doing/vbg)
>>>   his/prp$
>>>   homework/nn
>>>   ./.)",https://stackoverflow.com/questions/74160976,python,22-10-2022 04:00,58.0,1.0,1.0,True,22-10-2022 05:01,22-10-2022 05:01,Implementation Issues
76873337,text mining in r: delete first sentence of each document,"i have several documents and do not need the first sentence of each document.
i could not find a solution so far.
here is an example. the structure of the data looks like this




case_number
text




1
today is a good day. it is sunny.


2
today is a bad day. it is rainy.




so the results should look like this




case_number
text




1
it is sunny.


2
it is rainy.




here is the example dataset:
case_number <- c(1, 2)

text <- c(""today is a good day. it is sunny."",
          ""today is a bad day. it is rainy."")

data <- data.frame(case_number, text)","['r', 'text-mining']",76874280,"if there's a chance that sentences might include some punctuation (e.g. abbreviations or numerics), and you are using some text mining library anyway, it makes perfect sense to let it handle tokenization.
with {tidytext} :
library(dplyr)
library(tidytext)

# exmple with punctuation in 1st sentence
data <- data.frame(case_number = c(1, 2),
                   text = c(""today is a good day, above avg. for sure, by 5.1 points. it is sunny."",
                            ""today is a bad day. it is rainy.""))
# tokenize to sentences, converting tokens to lowercase is optional
data %>% 
  unnest_sentences(s, text)
#>   case_number                                                        s
#> 1           1 today is a good day, above avg. for sure, by 5.1 points.
#> 2           1                                             it is sunny.
#> 3           2                                      today is a bad day.
#> 4           2                                             it is rainy.

# drop 1st record of every case_number group
data %>% 
  unnest_sentences(s, text) %>% 
  filter(row_number() > 1, .by = case_number)
#>   case_number            s
#> 1           1 it is sunny.
#> 2           2 it is rainy.

created on 2023-08-10 with reprex v2.0.2",https://stackoverflow.com/questions/76873337,r,10-08-2023 07:05,54.0,0.0,1.0,True,10-08-2023 09:14,10-08-2023 08:14,Preprocessing Tasks
76711533,how to use the python openai client with both azure and openai at the same time?,"openai offers a python client, currently in version 0.27.8, which supports both azure and openai.
here are examples of how to use it to call the chatcompletion for each provider:
# openai_chatcompletion.py

""""""test openai's chatcompletion endpoint""""""
import os
import openai
import dotenv
dotenv.load_dotenv()
openai.api_key = os.environ.get('openai_api_key')

# hello, world.
api_response = openai.chatcompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
    {""role"": ""user"", ""content"": ""hello!""}
  ],
  max_tokens=16,
  temperature=0,
  top_p=1,
  frequency_penalty=0,
  presence_penalty=0,
)

print('api_response:', type(api_response), api_response)
print('api_response.choices[0].message:', type(api_response.choices[0].message), api_response.choices[0].message)

and:
# azure_openai_35turbo.py

""""""test microsoft azure's chatcompletion endpoint""""""
import os
import openai
import dotenv
dotenv.load_dotenv()

openai.api_type = ""azure""
openai.api_base = os.getenv(""azure_openai_endpoint"") 
openai.api_version = ""2023-05-15""
openai.api_key = os.getenv(""azure_openai_key"")


# hello, world.
# in addition to the `api_*` properties above, mind the difference in arguments
# as well between openai and azure:
# - openai from openai uses `model=""gpt-3.5-turbo""`!
# - openai from azure uses `engine=""ï¿½ï¿½ï¿½deployment nameï¿½ï¿½ï¿½""`! ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
#   > you need to set the engine variable to the deployment name you chose when
#   > you deployed the gpt-35-turbo or gpt-4 models.
#  this is the name of the deployment i created in the azure portal on the resource.
api_response = openai.chatcompletion.create(
  engine=""gpt-35-turbo"", # engine = ""deployment_name"".
  me"": ""user"", ""content"": ""hello!""}
  ],
  max_tokens=16,
  temperature=0,
  top_p=1,
  frequency_penalty=0,
  presence_penalty=0,
)

print('api_response:', type(api_response), api_response)
print('api_response.choices[0].message:', type(api_response.choices[0].message), api_response.choices[0].message)

i.e. api_type and other settings are globals of the python library.
here is a third example to transcribe audio (it uses whisper, which is available on openai but not on azure):
# openai_transcribe.py

""""""
test the transcription endpoint

""""""
import os
import openai
import dotenv
dotenv.load_dotenv()


openai.api_key = os.getenv(""openai_api_key"")
audio_file = open(""minitests/minitests_data/bilingual-english-bosnian.wav"", ""rb"")
transcript = openai.audio.transcribe(
    model=""whisper-1"",
    file=audio_file,
    prompt=""part of a bosnian language class."",
    response_format=""verbose_json"",
)
print(transcript)

these are minimal examples but i use similar code as part of my webapp (a flask app).
now my challenge is that i'd like to:

use the chatcompletion endpoint from azure; but:
use the transcribe endpoint from openai (since it's not available on azure)

is there any way to do so?
i have a few options in mind:

changing the globals before every call. but i'm worried that this might cause side-effects i did not expect.
duplicating/forking the library to have two versions run concurrently, one for each provider, but this also feels very messy.
use an alternative client for openai's whisper, if any.

i'm not too comfortable with these and feel i may have missed a more obvious solution.
or of courseï¿½ï¿½ï¿½ alternatively, i could just use whisper with a different provider (e.g. replicate) or an alternative to whisper altogether.
see also

someone reported the issue (but without a solution) on github (openai/openai-python): using azure and openai at the same time #411","['python', 'azure', 'python-module', 'openai-api', 'openai-whisper']",76740496,"each api in the library accepts per-method overrides for the configuration options. if you want to access the azure api for chat completions, you can explicitly pass in your azure config. for the transcribe endpoint, you can explicitly pass the openai config. for example:
import os
import openai

api_response = openai.chatcompletion.create(
    api_base=os.getenv(""azure_openai_endpoint""),
    api_key=os.getenv(""azure_openai_key""),
    api_type=""azure"",
    api_version=""2023-05-15"",
    engine=""gpt-35-turbo"",
    messages=[
    {""role"": ""user"", ""content"": ""hello!""}
    ],
    max_tokens=16,
    temperature=0,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0,
)
print(api_response)



audio_file = open(""minitests/minitests_data/bilingual-english-bosnian.wav"", ""rb"")
transcript = openai.audio.transcribe(
    api_key=os.getenv(""openai_api_key""),
    model=""whisper-1"",
    file=audio_file,
    prompt=""part of a bosnian language class."",
    response_format=""verbose_json"",
)
print(transcript)",https://stackoverflow.com/questions/76711533,python,18-07-2023 09:52,10485.0,3.0,2.0,True,30-11-2023 13:55,18-07-2023 15:50,Implementation Issues
73586255,r text analysis: counting occurences of any combinations of words from two different keyword lists with a given distance of each other,"thanks for reading. for a reserach project, i'm doing some text analysis. we are analyzing large texts (company reports) and i'm looking to count keyword frequencies within that text.
however, i have two lists of keywords, and i dont want to count the number of occurances of these words, but the number of times any two words from these lists appear within a certain distance from each other in the main text.
text <- c(""the house is blue. the car is very big and red."")
words1 <- c(""car"", ""house"") 
words2 <- c(""blue"", ""red"") 

the desired functionality should, for example, return 1 for distance 3. (number of any combinations in given distance.)
i know about the text_count function from the stringb package and kwic from quantea. however, thats not really what im looking for.
thanks, any help is appreciated.","['r', 'nlp', 'corpus', 'quanteda']",73597973,"the quanteda package has the function fcm() that counts frequency of their co-occurrences.
require(quanteda)
txt <- c(""the house is blue. the car is very big and red."")
toks <- tokens(txt) %>% tokens_tolower()
fcm(toks, window = 3, tri = false)
#> feature co-occurrence matrix of: 10 by 10 features.
#>         features
#> features the house is blue . car very big and red
#>    the     1     2  4    2 4   2    2   2   2   2
#>    house   2     0  2    1 2   1    1   1   1   1
#>    is      4     2  1    2 4   2    2   2   2   2
#>    blue    2     1  2    0 2   1    1   1   1   1
#>    .       4     2  4    2 1   2    2   2   2   2
#>    car     2     1  2    1 2   0    1   1   1   1
#>    very    2     1  2    1 2   1    0   1   1   1
#>    big     2     1  2    1 2   1    1   0   1   1
#>    and     2     1  2    1 2   1    1   1   0   1
#>    red     2     1  2    1 2   1    1   1   1   0",https://stackoverflow.com/questions/73586255,r,02-09-2022 17:40,289.0,1.0,2.0,True,17-09-2022 09:38,17-09-2022 09:38,Implementation Issues
41162876,get weight matrices from gensim word2vec,"i am using gensim word2vec package in python.
i would like to retrieve the w and w' weight matrices that have been learn during the skip-gram learning.
it seems to me that model.syn0 gives me the first one but i am not sure how i can get the other one. any idea?
i would actually love to find any exhaustive documentation on models accessible attributes because the official one does not seem to be precise (for instance syn0 is not described as an attribute)","['python', 'machine-learning', 'nlp', 'word2vec', 'gensim']",47925136,"the model.wv.syn0 contains the input embedding matrix. output embedding is stored in model.syn1 when it's trained with hierarchical softmax (hs=1) or in model.syn1neg when it uses negative sampling (negative>0). that's it! when both hierarchical softmax and negative sampling are not enabled, word2vec uses a single weight matrix model.wv.syn0 for training.
see also a related discussion here.",https://stackoverflow.com/questions/41162876,python,15-12-2016 11:19,9105.0,16.0,2.0,True,27-10-2023 03:15,02-02-2021 07:18,Implementation Issues
75744031,why do we need to write a function to &quot;compute metrics&quot; with huggingface question answering trainer when evaluating squad?,"currently, i'm trying to build a extractive qa pipeline, following the huggingface course on the matter. there, they show how to create a compute_metrics() function to evaluate the model after training. however, i was wondering if there's a way to obtain those metrics on training, and pass the compute_metrics() function directly to the trainer. they are training using only the training loss, and i would like to have the evaluation f1 score on training.
but, as i see it, it might be a little bit tricky, because they need the original spans to calculate the squad metrics, but you don't get those original spans passed on your tokenized training dataset.
predicted_answer = {'id': '56be4db0acb8001400a502ec', 'prediction_text': 'denver broncos'}
theoretical_answer = {'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['denver broncos', 'denver broncos', 'denver broncos'], 'answer_start': [177, 177, 177]}}

metric.compute(predictions=predicted_answers, references=theoretical_answers)

that's why they make the whole compute_metrics() function, taking a few extra parameters than the prediction outputted in the evaluation loop, as they need to rebuild those spans.
q: how do i make the squad metric outputs f1 and accuracy scores from evaluate? how do i use the squad metric with the trainer object?","['python', 'machine-learning', 'nlp', 'huggingface-transformers', 'squad']",75751929,"the compute_metrics function can be passed into the trainer so that it validating on the metrics you need, e.g.
from transformers import trainer

trainer = trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
trainer.train()

i'm not sure if it works out of the box with the code to process the train_dataset and validation_dataset in the course code 
but this ones shows how the trainer + compute_metrics work 

before proceeding to read the rest of the answer, here's some disclaimers:

try to get through the full course chapter 1-9 and the compute_metrics and usage of evaluate.metric would make a sense why you can't plug in evaluate.metric directly to the trainer object. 

alternatively, walking through this book would help too: 



and now, here goes...
firstly, lets take a look at what the evaluate library is/does
from 
from evaluate import load

squad_metric = load(""squad"")

predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]
references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]

results = squad_metric.compute(predictions=predictions, references=references)

print(results)

[out]:
{'exact_match': 100.0, 'f1': 100.0}

next, we take a look at what the compute_metrics argument in the trainer expects
from line 600 
    metric = evaluate.load(""squad_v2"" if data_args.version_2_with_negative else ""squad"")

    def compute_metrics(p: evalprediction):
        return metric.compute(predictions=p.predictions, references=p.label_ids)

    # initialize our trainer
    trainer = questionansweringtrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset if training_args.do_train else none,
        eval_dataset=eval_dataset if training_args.do_eval else none,
        eval_examples=eval_examples if training_args.do_eval else none,
        tokenizer=tokenizer,
        data_collator=data_collator,
        post_process_function=post_processing_function,
        compute_metrics=compute_metrics,
    )


the compute_metrics argument in the questionansweringtrainer is expecting a function that:

[in]: takes in an evalprediction object as input
[out]: returns a dict of keys-value pairs where the key is the name of the output metric in string type and the value is expected to a floating point

un momento! (wait a minute!) what are these questionansweringtrainer and evalprediction objects?
q: why are you not using the normal trainer object?
a: the questionansweringtrainer is a specific sub-class of the trainer object that is used for the qa task. if you're going to train a model to evaluate on the squad dataset, then the questionansweringtrainer is the most appropriate trainer object to use.
[suggestion]: most probably huggingface devs and dev-advocate should add some notes on the object in questionansweringtrainer 
q: what is this evalprediction object then?
a: officially, i guess it's this: 
if we look at the doc:  and the code, it looks like the object is a custom container class that holds the (i) predictions, (ii) label_ids and (iii) inputs np.ndarray. these are what the model's  inference function need to return in order for the compute_metrics to work as expected.
class evalprediction:
    """"""
    evaluation output (always contains labels), to be used to compute metrics.
    parameters:
        predictions (`np.ndarray`): predictions of the model.
        label_ids (`np.ndarray`): targets to be matched.
        inputs (`np.ndarray`, *optional*)
    """"""

    def __init__(
        self,
        predictions: union[np.ndarray, tuple[np.ndarray]],
        label_ids: union[np.ndarray, tuple[np.ndarray]],
        inputs: optional[union[np.ndarray, tuple[np.ndarray]]] = none,
    ):
        self.predictions = predictions
        self.label_ids = label_ids
        self.inputs = inputs

    def __iter__(self):
        if self.inputs is not none:
            return iter((self.predictions, self.label_ids, self.inputs))
        else:
            return iter((self.predictions, self.label_ids))

    def __getitem__(self, idx):
        if idx == 0:
            return self.predictions
        elif idx == 1:
            return self.label_ids
        elif idx == 2:
            return self.inputs


hey, you still haven't answer the question of how i can use the evaluate.metrics('squad') directly to the the compute_metrics args!
yes, for now, you can't directly use it but it's a simple wrapper.
step 1. make sure the model you want to use outputs the required evalprediction object that contains, predictions and label_ids
if you're using most the models supported for qa in huggingface's transformers library, they should already output the expected evalprediction.
otherwise, take a look at models supported by 
step 2: since the model inference outputs evalprediction but the compute_metrics expects a dictionary outputs, _you have to wrap the evaluate.metrics function
e.g.
    metric = evaluate.load(""squad_v2"" if data_args.version_2_with_negative else ""squad"")

    def compute_metrics(p: evalprediction):
        return metric.compute(predictions=p.predictions, references=p.label_ids)

q: do we really always need to write that wrapper function?
a: for now, yes, it is by design not directly integrated with the outputs of the evaluate.metrics to give the different metrics' developers freedom to define how they want their inputs/outputs to look like.
but there might be hope to make compute_metrics more integrated with evaluate.metric if someone picks this feature request up!",https://stackoverflow.com/questions/75744031,python,15-03-2023 11:22,15598.0,2.0,1.0,True,18-03-2023 23:11,18-03-2023 22:21,Conceptual Questions
78631323,is there a way to filter and exclude documents when doing similarity search in a vector db using langchain?,"so far my research only shows me how to filter to a specific a specific document or page but it doesn't show how to exclude some documents from the search.
results_with_scores = db.similarity_search_with_score(""foo"", filter=dict(page=1))","['langchain', 'vector-database', 'faiss', 'retrieval-augmented-generation', 'similarity-search']",78656891,"this depends on the underlying vector database being used. the arguments to filter will typically be passed to the vector database, and behavior will be implementation specific.
a common choice of vector database is chromadb, the filter arguments are passed to where, you can consult the filter documentation in the chromadb guide. see also inequality in the unofficial chromadb cookbook. you will want to use a $ne expression.
if you are using a different vector database you may need to consult to documentation for that database and even check the langchain code to see how things get passed through.",https://stackoverflow.com/questions/78631323,langchain,17-06-2024 07:24,1093.0,1.0,1.0,True,22-06-2024 18:36,17-06-2024 15:10,Implementation Issues
74500309,python regex to match a colon either side (left and right) of a word,"at a complete loss here - trying to match a a colon either side of any given word in a passage of text.
for example:
:wave: hello guys! :partyface: another huge win for us all to celebrate!

an appropriate regex that would match:
:wave:
:partyface:

really appreciate your help!
\w*:\b","['python', 'regex', 'nlp']",74500391,"to catch all the content
:[^:]*:

to catch the content between
(?<=:)[^:]*(?=:)",https://stackoverflow.com/questions/74500309,python,19-11-2022 13:11,393.0,1.0,1.0,True,19-11-2022 13:21,19-11-2022 13:13,Uncategorized
78827482,can&#39;t suppress warning from transformers/src/transformers/modeling_utils.py,"my implementation for the automodel autotokenizer classes are fairly simple:
from transformers import automodel, autotokenizer
import numpy as np
from rank_bm25 import bm25okapi
from sklearn.neighbors import nearestneighbors

class embeddingmodels:

    def bert(self, model_name, text):
        tokenizer = autotokenizer.from_pretrained(model_name)
        model = automodel.from_pretrained(model_name)
        inputs = tokenizer(text, return_tensors=""pt"", truncation=true, padding=true)
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()
        return embeddings
    
    def create_chunks(self, text, chunk_size):
        return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

but i can't get this warning to go away:
a parameter name that contains 'beta' will be renamed internally to 'bias'. 
please use a different name to suppress this warning.
a parameter name that contains 'gamma' will be renamed internally to 'weight'. 
please use a different name to suppress this warning.

there is no reference to the word beta or gamma anywhere in my repo.
updating the package, suppressing the warnings with import warnings","['python', 'machine-learning', 'pytorch', 'huggingface-transformers', 'tokenize']",78844884,"before loading from pretrained model set transformers logger level to error as shown below. it sure is really frustrating not being able to leverage the warnings library filter
    loggers = [logging.getlogger(name) for name in logging.root.manager.loggerdict]
    for logger in loggers:
        if ""transformers"" in logger.name.lower():
            logger.setlevel(logging.error)

    # now you can load state dict from pretrained
    model = transformers.bertmodel.from_pretrained(
        ""bert-base-uncased"",
        use_safetensors=true,
        return_dict=false,
        attn_implementation=""sdpa"",
    )",https://stackoverflow.com/questions/78827482,python,02-08-2024 23:04,2052.0,1.0,1.0,True,07-08-2024 17:01,02-08-2024 23:25,Implementation Issues
77461542,entity extraction in rasa,"i want to create a chatbot that extract words from the user message as entity and send it to dictionary and in return get the meaning of that word.
but the problem is entity values are not getting extracted, and i am getting empty brackets [ ]. i am trying to solve this issue for weeks now. now, i am exhausted and desperate. please help me to figure this out.
here is all the files:

installation versions on my system are:
rasa version      :         3.6.13
minimum compatible version: 3.5.0
rasa sdk version  :         3.6.2
python version    :         3.10.0
operating system  :         windows-10-10.0.19045-sp0","['machine-learning', 'nlp', 'entity', 'rasa-nlu', 'rasa-sdk']",77463236,"you need to add the regexentityextractor to the pipeline in your config.yml.
config.yml:
pipeline:
- name: regexentityextractor
  case_sensitive: false
  use_lookup_tables: true
  use_regexes: true
  use_word_boundaries: true

additionally you will need to modify your nlu training data to match the correct format for extracting entities.
before:
clarify the term [sympathy] for me. (term)

after:
clarify the term [sympathy](term) for me. 

the rasa documentation goes into more detail about how to format nlu training data.",https://stackoverflow.com/questions/77461542,machine-learning,10-11-2023 16:35,423.0,0.0,1.0,True,11-11-2023 08:51,11-11-2023 08:51,Implementation Issues
79257787,how to get intermediary chain step outputs in final output?,"for simplicity sake i have the following chain:

extract names from a list
validate these names against a second list of names

what i want is to receive a json with all intermediary steps at the end as well as the input:
{""first_value"": ""dave, john, carrot"", ""first_prompt_output"" ""dave, john"", ""possible_values"": ""john""...}

but i am confused by the lc docs and i seem to be able to get all of the inputs using the runnable passthrough, but in a hard to read format. i fiddled with it for half an hour (tried runnableparallel, runnablepassthrough.assign(), ...) but i don't seem to be able to get it, there must be some key feature i'm missing.
{'result': {'third_prompt_output': 'johny'},
 'first_prompt_output': {'first_prompt_output': {'first_prompt_output': ['dave',
    'john']},
  'possible_values': {'first_value': 'dave, john',
   'possible_values': ['john']}}}

first_prompt = prompttemplate.from_template(""""""find all names in the following text and extract it as json with a field `first_prompt_output`: {first_value}""
                                                    first_prompt_output:"""""")

second_prompt = prompttemplate.from_template(""""""here is a list of possible values: {possible_values} and a list of found value {first_prompt_output}. find values that are in both lists. return a json with the fields `first_prompt_output` and `second_prompt_output` and `possible_values`."""""")

first_value = ""dave, john""
possible_values = [""john""]

first_chain = (
    first_prompt
    | llm
    | simplejsonoutputparser()
)

second_chain = (
    second_prompt
    | llm
    | simplejsonoutputparser()
)

chain = (
    {""first_prompt_output"": first_chain, ""possible_values"": runnablepassthrough(), ""first_value"": runnablepassthrough()} 
    | runnableparallel(result={""second_prompt_output"": second_chain, ""first_value"": itemgetter(""first_value"")})
)

chain.invoke({""first_value"": first_value, ""possible_values"": possible_values})

i tried using the runnableparallel or runnablepassthrough.assign, but neither does what i expect it to do. what i basically need is a dict.update() but in the pipeline.","['langchain', 'large-language-model', 'chain']",79264564,"i have not found a way to do this with langchain, but i found a function that allows me to flatten the output and results in what i want, although it seems a bit clunky and i believe there must be a better solution.
the key is to add the following function to the chain:
def flatten_dict(*vars) -> dict:
    '''
    flatten a dictionary by removing unnecessary mid-level keys.
    returns a runnable (chainable) function.
    '''
    flat = {}
    for var in vars:
        keys = [k for k in var]
        for key in keys:
            if isinstance(var[key], dict):
                flat.update(var[key])
            else:
                flat[key] = var[key]
    return flat

chain = (
    {""first_prompt_output"": first_chain, ""possible_values"": runnablepassthrough(), ""first_value"": runnablepassthrough()} 
    | runnableparallel(result={""second_prompt_output"": second_chain, ""first_value"": itemgetter(""first_value"")})
)
| flatten_dict",https://stackoverflow.com/questions/79257787,langchain,06-12-2024 11:15,79.0,1.0,1.0,True,09-12-2024 10:07,06-12-2024 11:18,Implementation Issues
75951190,sentence transformer use of evaluator,"i came across this script which is second link on this page  and this explanation
i am using all-mpnet-base-v2 (link) and i am using my custom data
i am having hard time understanding use of
evaluator = embeddingsimilarityevaluator.from_input_examples(
    dev_samples, name='sts-dev')

the documentation says:

evaluator ï¿½ï¿½ï¿½ an evaluator (sentence_transformers.evaluation) evaluates the model performance during training on held-out dev data. it is used to determine the best model that is saved to disc.

but in this case, as we are fine tuning on our own examples, train_dataloaderhas train_samples which has our model sentences and scores.
q1. how is train_samples different than dev_samples?
q2a: if the model is going to print performance against dev_samples then how is it going to help ""to determine the best model that is saved to disc""?
q2b: are we required to run dev_samples a the model saved on the disc and then compare scores?
q3. if my goal is to take a single model and then fine tune it, is it okay to skip parameters evaluator and evaluation_steps?
q4. how to determine total steps in the model? do i need to set evaluation_steps?

updated
i followed the answer provided by kyle and have below follow up questions
in the fit method i used the evaluator and below data was written to a file

q5. which metric is used to select the best epoch? is it cosine_pearson?
q6: why steps are -1 in the above output?
q7a: how to find steps based upon size of my data, batch size etc.
currently i have kept them to 1000. but not sure if that it is too much. i am running for 10 epochs, i have 2509 examples in the training data and batch size is 64.
q7b: are my steps going to be 2509/64? if yes then 1000 seems to be too high number","['python', 'nlp', 'sentence-transformers']",75981408,"question 1

how is train_samples different from dev_samples in the context of the embeddingsimilarityevaluator?

one needs to have a ""held-out"" split of data to be used for evaluation during training to avoid over-fitting. this ""held-out"" set is commonly referred to as the ""development set"" as it is the set of data that is used during development of the model/system. a pedagogical analogy can be drawn between a traditional education curriculum and that of training deep learning models: if one were to give students all the questions for a given topic, and then use the same subset of questions for evaluation, then eventually (most) students will learn to memorise the set of answers they repeatedly see while practicing, instead of learning the procedures to solve the questions in general. so if you are using your own custom data, make sure that a subset of that data is allocated to dev_samples in addition to train_samples and test_samples. alternatively, if your own data is scarce, you can use the original training data to supplement your own training, development and test sets. the ""test set"" is the one that is only used after training has completed to determine the final performance of the model (i.e. all samples in the test set (ideally) haven't been seen before).
question 2

how is the model going to determine the best model that is saved to disc? are we required to run dev_samples against the model saved on the disc and then compare scores?

the previous answer alludes to how this will work, but in brief, once the evaluator has been instantiated, it will measure the correlation against the gold labels and then return the similarity score (depending on what main_similarity was initially set). if the produced embeddings (based on the development set) offer a higher correlation with their gold labels, and therefore, a higher score overall, then this ""better"" model is saved to disk. hence, there is no need for you to ""run dev_samples against the model saved on the disc and then compare scores"", this process happens automatically provided everything has been set up appropriately.
question 3

if my goal is to take a single model and then fine tune it, is it okay to skip parameters evaluator and evaluation_steps?

based on the above answers, you can understand why you cannot ""skip the evaluator and evaluation_steps"". the evaluator is an integral part of ""fine-tuning"" (i.e. training) the model.
question 4

how to determine the total number of steps for the model? i need to set evaluation_steps.

the evaluation_steps parameter sets the number of training steps that must occur before the model is evaluated using the evaluator. if the authors have set this to 1000, then leave it as is unless you notice problems with training. alternatively, experiment with either increasing of decreasing it and select a value that works best for training.
follow-up questions
question 5

which metric is used to select the best epoch? is it cosine_pearson?

by default, the maximum of the cosine spearman, manhattan spearman, euclidean spearman and dot product spearman is used.
question 6

why are steps -1 in the output?

the -1 lets the user know that the evaluator was called after all training steps occurred for a particular epoch.
if the steps_per_epoch was not set when calling the model.fit(), it defaults to none which sets the number of steps_per_epoch to the size of the train_dataloader which is passed to train_objectives when model.fit() is initially called, i.e.:
model.fit(train_objectives=[(train_dataloader, train_loss)],
          ...)

in your case, train_samples is 2,509 and train_batch_size is 64, so the size of train_dataloader, and therefore steps_per_epoch, will be 39.
if the steps_per_epoch, is less than the evaluation_steps, then the number of training steps won't reach or exceed evaluation_steps and so additional calls to _eval_during_training on line 737 won't occur. this isn't a problem as the evaluation is forced to call at the end of each epoch anyway based on line 747.
question 7

how do i find the number of evaluation_steps based on the size of my training data (2,509 samples) and batch size (64)? is 1000 too high?

the evaluation_steps is available to tell the model during the training process whether it should prematurely run an evaluation using the evaluator part-way through an epoch. otherwise, the evaluation is forced to run at the end of the epoch after steps_per_epoch have completed.
based on the numbers you provided, you could, for example, set evaluation_steps to 20 to get an evaluation to run approx. half-way through an epoch (assuming an epoch is 39 training_steps). see this answer and its question for more info. on batch size vs. epochs vs. steps per epoch.",https://stackoverflow.com/questions/75951190,python,06-04-2023 15:22,2768.0,4.0,1.0,True,14-04-2023 14:56,14-04-2023 14:56,Implementation Issues
73315383,in spacy: add a span (doc[a:b]) as entity in a spacy doc (python),"i am using regex over a whole document to catch the spans in which such regex occurs:
import spacy
import re

nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""the united states of america (usa) are commonly known as the united states (u.s. or us) or america."")

expression = r""[uu](nited|\.?) ?[ss](tates|\.?)""
for match in re.finditer(expression, doc.text):
    start, end = match.span()
    span = doc.char_span(start, end)
    # this is a span object or none 
    # if match doesn't map to valid token sequence
    if span is not none:
        print(""found match:"", span.text)

there is a way to get the span (list of tokens) corresponding to the regex match on the doc even if the boundaries of the regex match do not correspond to token boundaries.
see:
how can i expand the match to a valid token sequence? in 
so far so good.
now that i have a collectuon of spans how do i convert them into entities?
i am aware of the entity ruler:
the entityruler is a pipeline component (see also the link above) but that entityruler takes patterns as inputs to search in the doc and not spans.
if i want to use regex over the whole document to get the collection os spans i want to convert into ents what is the next step here? entityruler? how? or something else?
put simpler:
nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""the aplicable law is article 102 section b sentence 6 that deals with robery"")

i would like to generate an spacy ent (entity) out of doc[5,10] with label ""law"" in order to be able to:
a) loop over all the law entities in the texts
b) use the visualizer to display the different entities contained in the doc","['python', 'nlp', 'spacy', 'named-entity-recognition']",73331138,"the most flexible way to add spans as entities to a doc is to use doc.set_ents:
from spacy.tokens import span

span = doc.char_span(start, end, label=""ent"")
doc.set_ents(entities=[span], default=""unmodified"")

use the default option to specify how to set all the other tokens in the doc. by default the other tokens are set to o, but you can use default=""unmodified"" to leave them untouched, e.g. if you're adding entities incrementally.",https://stackoverflow.com/questions/73315383,python,11-08-2022 04:45,1963.0,4.0,1.0,True,15-08-2022 06:22,15-08-2022 06:22,Implementation Issues
68738363,building own classifier based pos tagger using nltk&#39;s sklearnclassifier and classifierbasedpostagger,"i'm trying to build my own classifier based pos tagger using sklearnclassifier and classifierbasedpostagger. the code that i've tried is given below.
from nltk.corpus import treebank
nltk.download('treebank')

data = treebank.tagged_sents()
train_data = data[:3500]
test_data = data[3500:]

from nltk.classify import sklearnclassifier
from sklearn.naive_bayes import bernoullinb
from nltk.tag.sequential import classifierbasedpostagger

bnb = sklearnclassifier(bernoullinb())
bnb_tagger = classifierbasedpostagger(train=train_data,
                                      classifier_builder=bnb.train)

# evaluate tagger on test data and sample sentence
print(bnb_tagger.evaluate(test_data))

# see results on our previously defined sentence
print(bnb_tagger.tag(nltk.word_tokenize(sentence)))

this code is yielding the following error:
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
c:\users\abdull~1.imr\appdata\local\temp/ipykernel_6580/266992580.py in <module>
      4 
      5 bnb = sklearnclassifier(bernoullinb())
----> 6 bnb_tagger = classifierbasedpostagger(train=train_data,
      7                                       classifier_builder=bnb.train)
      8 

~\miniconda3\envs\nlp_course\lib\site-packages\nltk\tag\sequential.py in __init__(self, feature_detector, train, classifier_builder, classifier, backoff, cutoff_prob, verbose)
    637 
    638         if train:
--> 639             self._train(train, classifier_builder, verbose)
    640 
    641     def choose_tag(self, tokens, index, history):

~\miniconda3\envs\nlp_course\lib\site-packages\nltk\tag\sequential.py in _train(self, tagged_corpus, classifier_builder, verbose)
    673         if verbose:
    674             print(""training classifier ({} instances)"".format(len(classifier_corpus)))
--> 675         self._classifier = classifier_builder(classifier_corpus)
    676 
    677     def __repr__(self):

~\miniconda3\envs\nlp_course\lib\site-packages\nltk\classify\scikitlearn.py in train(self, labeled_featuresets)
    110 
    111         x, y = list(zip(*labeled_featuresets))
--> 112         x = self._vectorizer.fit_transform(x)
    113         y = self._encoder.fit_transform(y)
    114         self._clf.fit(x, y)

~\miniconda3\envs\nlp_course\lib\site-packages\sklearn\feature_extraction\_dict_vectorizer.py in fit_transform(self, x, y)
    288             feature vectors; always 2-d.
    289         
--> 290         return self._transform(x, fitting=true)
    291 
    292     def inverse_transform(self, x, dict_type=dict):

~\miniconda3\envs\nlp_course\lib\site-packages\sklearn\feature_extraction\_dict_vectorizer.py in _transform(self, x, fitting)
    233                     if feature_name in vocab:
    234                         indices.append(vocab[feature_name])
--> 235                         values.append(self.dtype(v))
    236 
    237             indptr.append(len(indices))

typeerror: float() argument must be a string or a number, not 'nonetype'

how to do it right?","['python', 'scikit-learn', 'nlp', 'nltk', 'pos-tagger']",68894824,"according to the comment from this issue, this is a consequence of a bug in scikit-learn. scikit-learn's _transform method of dictvectorizer in sklearn/feature_extraction/_dict_vectorizer.py fails when the input argument x contains mappings to none. according to tom aarsen, we can now use the following example to make the work done:
import nltk
from nltk.corpus import treebank

from nltk.classify import sklearnclassifier
from sklearn.naive_bayes import bernoullinb
from nltk.tag.sequential import classifierbasedpostagger

nltk.download('treebank')

data = treebank.tagged_sents()
train_data = data[:3]
test_data = data[3:]

class customclassifierbasedpostagger(classifierbasedpostagger):

    def feature_detector(self, tokens, index, history):
        return {
            key: str(value) # ensure that the feature value is a string. converts none to 'none'
            for key, value in super().feature_detector(tokens, index, history).items()
        }

bnb = sklearnclassifier(bernoullinb())
bnb_tagger = customclassifierbasedpostagger(train=train_data,
                                            classifier_builder=bnb.train,
                                            verbose=true)

sentence = ""this is a sample sentence which i just made for fun.""
# evaluate tagger on test data and sample sentence
print(bnb_tagger.evaluate(test_data))

# see results on our previously defined sentence
print(bnb_tagger.tag(nltk.word_tokenize(sentence)))

the output will be like:
[nltk_data] downloading package treebank to c:\users\tom/nltk_data...
[nltk_data]   package treebank is already up-to-date!
constructing training corpus for classifier.
training classifier (58 instances)
0.09338289371682999
[('this', 'nnp'), ('is', 'nnp'), ('a', 'nnp'), ('sample', 'nnp'), ('sentence', 'nnp'), ('which', 'nnp'), ('i', 'nnp'), ('just', 'nnp'), ('made', 'nnp'), ('for', 'nnp'), ('fun', 'nnp'), ('.', 'nnp')]",https://stackoverflow.com/questions/68738363,python,11-08-2021 08:17,263.0,0.0,1.0,True,23-08-2021 14:55,11-08-2021 08:25,Conceptual Questions
66637485,spacy 3.0.1 accuracy prediction,"how to test accuracy of a spacy pretrained model in version 3.0.1. i want to see my output how accurate my tested model is predicted.this the code below for spacy version 2 but it doesn't work in spacy version 3.can somone tell me the code on spacy version 3.
 from spacy.gold import goldparse
 from spacy.scorer import scorer

def evaluate(nlp, examples, ent='person'):
scorer = scorer()
for input_, annot in examples:
    text_entities = []
    for entity in annot.get('entities'):
        if ent in entity:
            text_entities.append(entity)
    doc_gold_text = nlp.make_doc(input_)
    gold = goldparse(doc_gold_text, entities=text_entities)
    pred_value = nlp(input_)
    scorer.score(pred_value, gold)
return scorer.scores

examples = [
(""trump says he's answered mueller's russia inquiry questions \u2013 live"",{""entities"":[[0,5,""person""],[25,32,""person""],[35,41,""gpe""]]}),
(""alexander zverev reaches atp finals semis then reminds lendl who is boss"",{""entities"":[[0,16,""person""],[55,60,""person""]]}),
(""britain's worst landlord to take nine years to pay off string of fines"",{""entities"":[[0,7,""gpe""]]}),
(""tom watson: people's vote more likely given weakness of may's position"",{""entities"":[[0,10,""person""],[56,59,""person""]]}),
]

nlp = spacy.load('en_core_web_sm')
results = evaluate(nlp, examples)
print(results)","['python', 'nlp', 'spacy']",68151189,"personnally i had used this method, and i wish it will help you in your work:
in your case, i think:
from spacy.training import example

#get test data

test_data = [
    (""trump says he's answered mueller's russia inquiry questions \u2013 
    live"",{""entities"":[[0,5,""person""],[25,32,""person""],[35,41,""gpe""]]}),
    (""alexander zverev reaches atp finals semis then reminds lendl who is 
    boss"",{""entities"":[[0,16,""person""],[55,60,""person""]]}),
    (""britain's worst landlord to take nine years to pay off string of fines"", 
    {""entities"":[[0,7,""gpe""]]}),
    (""tom watson: people's vote more likely given weakness of may's position"", 
    {""entities"":[[0,10,""person""],[56,59,""person""]]}),
]

#formatted test data in order to adapt with the new version 3 of spacy

#get nlp object
nlp = spacy.load('en_core_web_sm')

new_test_data = []
for text, annots in test_data:
    new_test_data.append(example.from_dict(nlp.make_doc(text), annots))

#end formatted test data

#begin evaluation
#using , the evaluate() methos

scores_model = nlp.evaluate(new_test_data)

#print scores that you want
#precision_model = scores_model[""ents_p""]
#recall_model = scores_model[""ents_r""]
#f_score_model = scores_model[""ents_f""]
#scores_entities = scores_model[""ents_per_type""]",https://stackoverflow.com/questions/66637485,python,15-03-2021 11:53,1250.0,1.0,1.0,True,27-06-2021 22:46,17-03-2021 04:14,Implementation Issues
71865451,open_model_zoo demos stuck at reading model,"i execute some open_model_zoo demos, it works successfully when i choose the cpu device.
but when i change the device to myriad or gpu, it will stuck and do nothing.
(i've used hello_query_device.py to checked, my pc can detected the neural compute stick 2 )
version : openvino_2022.1.0.643 at windows10
picture of error message","['python', 'bert-language-model', 'openvino']",71868694,"the bert-large-uncased-whole-word-masking-squad-0001 model is supported on cpu and gpu, while wav2vec2-base model is supported on cpu only.
refer to intelï¿½ï¿½ï¿½s pre-trained models device support and public pre-trained models device support for open model zoo models' compatibility with cpu, gpu and myriad devices.",https://stackoverflow.com/questions/71865451,python,14-04-2022 01:25,153.0,-1.0,1.0,True,14-04-2022 08:31,14-04-2022 05:09,Data Wrangling
75845842,is the default `trainer` class in huggingface transformers using pytorch or tensorflow under the hood?,"question
according to the official documentation, the trainer class ""provides an api for feature-complete training in pytorch for most standard use cases"".
however, when i try to actually use trainer in practice, i get the following error message that seems to suggest that tensorflow is currently being used under the hood.
tensorflow/core/platform/cpu_feature_guard.cc:193] this tensorflow binary is optimized with oneapi deep neural network library (onednn) to use the following cpu instructions in performance-critical operations:  avx2 fma
to enable them in other operations, rebuild tensorflow with the appropriate compiler flags.

so which one is it? does the huggingface transformers library use pytorch or tensorflow for their internal implementation of trainer? and is it possible to switch to only using pytorch? i can't seem to find a relevant parameter in trainingarguments.
why does my script keep printing out tensorflow related errors? shouldn't trainer be using pytorch only?
source code
from transformers import gpt2tokenizer
from transformers import gpt2lmheadmodel
from transformers import textdataset
from transformers import datacollatorforlanguagemodeling
from transformers import trainer
from transformers import trainingarguments

import torch

# load the gpt-2 tokenizer and lm head model
tokenizer    = gpt2tokenizer.from_pretrained('gpt2')
lmhead_model = gpt2lmheadmodel.from_pretrained('gpt2')

# load the training dataset and divide blocksize
train_dataset = textdataset(
    tokenizer=tokenizer,
    file_path='./datasets/tinyshakespeare.txt',
    block_size=64
)

# create a data collator for preprocessing batches
data_collator = datacollatorforlanguagemodeling(
    tokenizer=tokenizer,
    mlm=false
)

# defining the training arguments
training_args = trainingarguments(
    output_dir='./models/tinyshakespeare', # output directory for checkpoints
    overwrite_output_dir=true,             # overwrite any existing content

    per_device_train_batch_size=4,         # sample batch size for training
    dataloader_num_workers=1,              # number of workers for dataloader
    max_steps=100,                         # maximum number of training steps
    save_steps=50,                         # after # steps checkpoints are saved
    save_total_limit=5,                    # maximum number of checkpoints to save

    prediction_loss_only=true,             # only compute loss during prediction
    learning_rate=3e-4,                    # learning rate
    fp16=false,                            # use 16-bit (mixed) precision

    optim='adamw_torch',                   # define the optimizer for training
    lr_scheduler_type='linear',            # define the learning rate scheduler

    logging_steps=5,                       # after # steps logs are printed
    report_to='none',                      # report to wandb, tensorboard, etc.
)

if __name__ == '__main__':
    torch.multiprocessing.freeze_support()

    trainer = trainer(
        model=lmhead_model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
    )

    trainer.train()","['python', 'tensorflow', 'pytorch', 'huggingface-transformers']",75873210,"it depends on how the model is trained and how you load the model. most popular models on transformers supports both pytorch and tensorflow (and sometimes also jax).
from transformers import autotokenizer, automodelforseq2seqlm
from transformers import tfautomodelforseq2seqlm

model_name = ""google/flan-t5-large""

model = automodelforseq2seqlm.from_pretrained(model_name)

# this would work if the model's backend is pytorch.
print(type(next(model.parameters())))


tf_model = tfautomodelforseq2seqlm.from_pretrained(model_name)

# the `model.parameters()` would not work for tensorflow,
# instead you can try `.summary()`
tf_model.summary()

[out]:
<class 'torch.nn.parameter.parameter'>

model: ""tft5_for_conditional_generation""
_________________________________________________________________
 layer (type)                output shape              param #   
=================================================================
 shared (embedding)          multiple                  32899072  
                                                                 
 encoder (tft5mainlayer)     multiple                  341231104 
                                                                 
 decoder (tft5mainlayer)     multiple                  441918976 
                                                                 
 lm_head (dense)             multiple                  32899072  
                                                                 
=================================================================
total params: 783,150,080
trainable params: 783,150,080
non-trainable params: 0
_________________________________________________________________



maybe something like:

def which_backend(model):
  try:
    model.parameters()
    return 'torch'
  except:
    try:
      model.summary()
      return 'tensorflow'
    except:
      return 'i have no idea... maybe jax?'



q: so if i use trainer, it's pytorch?
a: yes, most probably the model has pytorch backend, and the training loop (optimizer, loss, etc.) uses pytorch. but the trainer() isn't the model, it's the wrapper object.
q: and if i want to use trainer for tensorflow backend models, i should use tftrainer?
not really. in the latest version of transformers, the tftrainer object is deprecated, see 
it is recommended that you use keras' sklearn-style .fit() training if you are using a model with tensorflow backend.




q: why does my script keep printing out tensorflow related errors? shouldn't trainer be using pytorch only?
try checking your transformers version, most probably you are using an outdated version that uses some deprecated objects, e.g. textdataset (see how to resolve ""only integer tensors of a single element can be converted to an index"" error when creating a dataset to fine-tune gpt2 model?)
in the later versions, most probably pip install transformers>=4.26.1, the trainer shouldn't be activating tf warnings and using tftrainer would have raised warnings to suggest users to use keras instead.",https://stackoverflow.com/questions/75845842,python,26-03-2023 04:19,2916.0,4.0,2.0,True,29-03-2023 05:09,26-03-2023 04:45,Implementation Issues
73543698,&#39;lazycorpusloader&#39; is not iterable,"i was doing some cleaning in the text with a func
def spacy_tokenizer(sentence):
    # create token object from spacy
    docs = nlp(sentence)

    # correct spelling
    tokens = docs._.outcome_spellcheck
    tokens = nlp(tokens)

    # lemmatize each token and convert each token into lowercase
    tokens = [word.lemma_.lower().strip() if word.lemma_ != ""propn"" else word.lower_ for word in tokens]
    
    # remove stopwords
    tokens = [word for word in tokens if word not in stopwords and word not in punctuations]
    
    # remove links
    tokens = [remove_urls(word) for word in tokens]
    
    # return preprocessed list of tokens
    return tokens


but it raise an error in this part tokens = [word for word in tokens if word not in stopwords and word not in punctuations]
in specific:
<ipython-input-13-b662eacf73f8> in <listcomp>(.0)
     12 
     13     # remove stopwords
---> 14     tokens = [word for word in tokens if word not in stopwords and word not in punctuations]
     15 
     16     # remove links

typeerror: argument of type 'lazycorpusloader' is not iterable","['python', 'python-3.x', 'nlp', 'spacy']",73544303,"it seems that lazycorpusloader from nltk (not spacy) is being iterated by you and it probably should not. stopwords is of nltk.corpus.util.lazycorpusloader type. you might want to use: stopwords.words('english') instead.
please also check this answer from data science stack exchange.",https://stackoverflow.com/questions/73543698,python,30-08-2022 13:58,386.0,0.0,1.0,True,31-08-2022 06:49,31-08-2022 06:49,Implementation Issues
71011333,"runtimeerror: stack expects each tensor to be equal size, but got [7, 768] at entry 0 and [8, 768] at entry 1","when running this code:
embedding_matrix = torch.stack(embeddings)

i got this error:

runtimeerror: stack expects each tensor to be equal size, but got [7, 768] at entry 0 and [8, 768] at entry 1

i'm trying to get embedding using bert via:
split_sent = sent.split()
tokens_embedding = []
j = 0
for full_token in split_sent:
    curr_token = ''
    x = 0
    for i,_ in enumerate(tokenized_sent[1:]): 
        token = tokenized_sent[i+j]
        piece_embedding = bert_embedding[i+j]
        if token == full_token and curr_token == '' :
            tokens_embedding.append(piece_embedding)
            j += 1
            break                                     
sent_embedding = torch.stack(tokens_embedding)
embeddings.append(sent_embedding)
embedding_matrix = torch.stack(embeddings)

does anyone know how i can fix this?","['python', 'pytorch', 'runtime-error', 'tensor', 'bert-language-model']",71014842,"as per pytorch docs about torch.stack() function, it needs the input tensors in the same shape to stack. i don't know how will you be using the embedding_matrix but either you can add padding to your tensors (which will be a list of zeros at the end till a certain user-defined length and is recommended if you will train with this stacked tensor, refer this tutorial) to make them equidimensional or you can simply use something like torch.cat(data,dim=0).",https://stackoverflow.com/questions/71011333,python,06-02-2022 20:23,40516.0,6.0,1.0,True,21-08-2024 18:06,21-08-2024 18:06,Conceptual Questions
73862111,trouble retrieving value from a list inside a python dictionary,"i am developing a discord bot in python with py-cord and i have implemented the openai api to allow users to query the ai and modify the different weights and biases.
i am trying to make the weights specific to each guild, so people in one server can have different settings to someone in a different server. to achieve this, i am trying to use a dictionary where the guild id is the key and the weights are in a list as the values, but i keep getting keyerror exceptions.
import os
import discord
import openai
import re
import asyncio
from discord.ext import commands
from gtts import gtts
from discord import ffmpegpcmaudio
from mutagen.mp3 import mp3

bot = discord.bot(intents=discord.intents.default())

guilds = {}

@bot.event
async def on_ready():
    bot.temp = 1
    bot.topp = 0.5
    bot.freqpen = 0.3
    bot.prespen = 0.3
    x = datetime.datetime.now()
    print('logged in as {0.user} at'.format(bot), x, ""\n"")

class mymodal(discord.ui.modal):
    def __init__(self, *args, **kwargs) -> none:
        super().__init__(*args, **kwargs)

        self.add_item(discord.ui.inputtext(label=f""temperature. current: {bot.temp}""))
        self.add_item(discord.ui.inputtext(label=f""frequency penalty. current: {bot.freqpen}""))
        self.add_item(discord.ui.inputtext(label=f""presence penalty. current: {bot.prespen}""))
        self.add_item(discord.ui.inputtext(label=f""top p. current: {bot.topp}""))

    async def callback(self, interaction: discord.interaction):
        guilds[f'{bot.id}'] = [self.children[0].value, self.children[1].value, self.children[2].value, self.children[3].value]
        embed = discord.embed(title=""new gpt-3 weights and biases"", color=0xff5733)
        embed.add_field(name=f""temperature: {bot.temp}"", value=""controls randomness: lowering results in less random completions. i recommend not going higher than 1."", inline=false)
        embed.add_field(name=f""frequency penalty: {bot.freqpen}"", value=""how much to penalize new tokens based on their existing frequency in the text so far. "", inline=false)
        embed.add_field(name=f""presence penalty: {bot.prespen}"", value=""how much to penalize new tokens based on whether they appear in the text so far. increases the model's likelihood to talk about new topics. will not function above 2."", inline=false)
        embed.add_field(name=f""top p: {bot.topp}"", value=""controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. will not function above 1."", inline=false)
        await interaction.response.send_message(embeds=[embed])

@bot.slash_command(description=""change the gpt-3 weights and biases"")
async def setvalues(ctx: discord.applicationcontext):
    bot.id = ctx.guild.id
    modal = mymodal(title=""modify gpt-3 weights"")
    await ctx.send_modal(modal)

bot.run('token')

this creates a modal for users to input the values they want and then sends those values in a list to the dictionary called guilds with the key being bot.id. however, when i run a command i created to test pulling a value from the list, i get a keyerror exception. the command i run to check is:
@bot.slash_command()
async def dictionarytest(ctx):
    await ctx.respond(f'{guilds[bot.id][1]}')

the error i get is

ignoring exception in command dictionarytest: traceback (most recent
call last):   file
""/home/liam/.local/lib/python3.10/site-packages/discord/commands/core.py"",
line 127, in wrapped
ret = await coro(arg)   file ""/home/liam/.local/lib/python3.10/site-packages/discord/commands/core.py"",
line 911, in _invoke
await self.callback(ctx, **kwargs)   file ""/home/liam/pycharmprojects/discordbot/maintest.py"", line 76, in
dictionarytest
await ctx.respond(f'{str(guilds[bot.id][1])}') keyerror: 545151014702022656
the above exception was the direct cause of the following exception:
traceback (most recent call last):   file
""/home/liam/.local/lib/python3.10/site-packages/discord/bot.py"", line
1009, in invoke_application_command
await ctx.command.invoke(ctx)   file ""/home/liam/.local/lib/python3.10/site-packages/discord/commands/core.py"",
line 359, in invoke
await injected(ctx)   file ""/home/liam/.local/lib/python3.10/site-packages/discord/commands/core.py"",
line 135, in wrapped
raise applicationcommandinvokeerror(exc) from exc discord.errors.applicationcommandinvokeerror: application command
raised an exception: keyerror: 545151014702022656","['python', 'python-3.x', 'discord', 'pycord', 'openai-api']",73862209,"since we can't see the data associated with your app, it's hard to know for sure, but i see something suspicious that i think may be your problem. the error you are getting is pretty self-explanatory.  when executing this line:
await ctx.respond(f'{guilds[bot.id][1]}')

the error message is telling you that there isn't any key bot.id in the guilds dict, and bot.id has the value 545151014702022656. so in this case, the key is an integer.
the only place you add values to the guild dict is this line:
guilds[f'{bot.id}'] = [self.children[0].value, self.children[1].value, self.children[2].value, self.children[3].value]

here, you are adding a value to the guilds dict with a key that is a string. i bet that's your problem. the integer 545151014702022656 and the string ""545151014702022656"" aren't the same thing. you fail to match the keys you have added to guilds, because you're adding string keys, but looking for integer keys.  so i bet all you have to do to fix your code is change the above line to:
guilds[bot.id] = [self.children[0].value, self.children[1].value, self.children[2].value, self.children[3].value]",https://stackoverflow.com/questions/73862111,python,27-09-2022 03:43,99.0,0.0,1.0,True,16-04-2023 14:52,16-04-2023 14:50,Implementation Issues
79548202,"gpt-2 and other models from huggingface -100 label index for training, instead of pad token","i understand the -100 label id is used so that the predictions for these are not included when calculating the loss.
however on huggingface, they state
""complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not"", when replacing pad tokens. in their implementation, they use nn.crossentropyloss(), which has an argument ""ignore_index"".
is there any benefit to changing the id to -100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id? or are the results the same?
the way it is written makes me think there is some benefit, but the description of ""ignore_index"" appears to achieve what is wanted.","['nlp', 'huggingface-transformers', 'pre-trained-model']",79551169,the author of the tutorial you mentioned sets it to -100 and uses ignore_index to save a few lines of code. you don't see the line where the author pass something to ignore_index because it has a default value. the default value of ignore_index for nn.crossentropyloss is -100. using this value instead of the respective pad token id allows you to write some model indepent training code and you don't have to pass the pad token id from tokenizer down to the loss function.,https://stackoverflow.com/questions/79548202,nlp,01-04-2025 09:21,57.0,0.0,1.0,True,04-04-2025 22:41,04-04-2025 22:41,Implementation Issues
78356841,getting 404 on openai azure endpoint,"using the azureopenai client, and getting a 404
asyncazureopenai(
                api_key=os.getenv(""azure_openai_api_key""),  
                api_version=""2024-01-25-preview"",
                azure_deployment=""xxx-staging"",
                azure_endpoint=os.getenv(""azure_openai_endpoint"", """"),
            )

as you can see in the image below, i'm using the 0125 model version. interestingly, i don't even see my model version listed directly here:

i've tried this on openai-1.23.2 and openai-1.14.3","['python', 'azure', 'openai-api']",78360252,"found the answer.
found the answer. the problem was actually in the request params.normally, openai expects a model name to be passed in for model like
gpt-4-1106-preview
but in the case of azure, it expects the given model name in your deployment, so whatever name you wrote down for the deployment should be passed in here.
params = {
    ""model"": model.name, // name in azure
    ""messages"": chat_messages,
    ""max_tokens"": model_max_tokens,
    ""temperature"": temperature,
    ""stream"": stream,
}",https://stackoverflow.com/questions/78356841,python,20-04-2024 02:38,503.0,-1.0,1.0,True,21-04-2024 02:49,20-04-2024 22:24,Uncategorized
71048521,how to freeze parts of t5 transformer model,"i know that t5 has k, q and v vectors in each layer. it also has a feedforward network. i would like to freeze k, q and v vectors and only train the feedforward layers on each layer of t5. i use pytorch library. the model could be a wrapper for huggingface t5 model or a modified version of it. i know how to freeze all parameters using the following code:
tokenizer = autotokenizer.from_pretrained(underlying_model_name)
model = t5forconditionalgeneration.from_pretrained(underlying_model_name)

for p in model.parameters():
    p.requires_grad = false # freezing

could you please guide me how can i do this?
this github project probably could be helpful but it's for roberta and gpt, could i adapt it for t5?","['huggingface-transformers', 't5-transformer']",71068151,"i've adapted a solution based on this discussion from the huggingface forums.
basically, you have to specify the names of the modules/pytorch layers that you want to freeze.
in your particular case of t5, i started by looking at the model summary:
from transformers import t5modelforconditionalgeneration

model = t5modelforconditionalgeneration.from_pretrained(""t5-small"")
print(model)

this gives the following (abbreviated output):
t5forconditionalgeneration(
  (shared): embedding(32128, 512)
  (encoder): t5stack(
    (embed_tokens): embedding(32128, 512)
    (block): modulelist(
      (0): t5block(
        (layer): modulelist(
          (0): t5layerselfattention(
            (selfattention): t5attention(
              (q): linear(in_features=512, out_features=512, bias=false)
              (k): linear(in_features=512, out_features=512, bias=false)
              (v): linear(in_features=512, out_features=512, bias=false)
              (o): linear(in_features=512, out_features=512, bias=false)
              (relative_attention_bias): embedding(32, 8)
            )
            (layer_norm): t5layernorm()
            (dropout): dropout(p=0.1, inplace=false)
          )
          (1): t5layerff(
            (densereludense): t5densereludense(
              (wi): linear(in_features=512, out_features=2048, bias=false)
              (wo): linear(in_features=2048, out_features=512, bias=false)
              (dropout): dropout(p=0.1, inplace=false)
            )
            (layer_norm): t5layernorm()
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
      )
[...]  # abbreviated output

with this, we can then generate a list of modules that we want to freeze. in particular, i decided to freeze the entire t5layerselfattention block for the encoder (and, additionally, the t5layercrossattention for the decoder):
# all modules in the 
modules_to_freeze = [model.encoder.block[i].layer[0] for i in range(len(model.encoder.block))]
# and the decoder modules, which has both a selfattention (layer[0]) 
modules_to_freeze.extend([model.decoder.block[i].layer[0] for i in range(len(model.decoder.block))])
# and crossattention (layer[1]) block
modules_to_freeze.extend([model.decoder.block[i].layer[1] for i in range(len(model.decoder.block))])

and then simply freeze all the parameters in the respective modules:
for module in modules_to_freeze:
    for param in module.parameters():
        param.requires_grad = false  # actual freezing operation

you can verify that these are actually frozen in your model by running the following:
for param in model.parameters():
    print(param.requires_grad)

which should print quite a few false as well. if you really only want to freeze k, q and v, you can adapt the above process to just sub-select the modules you want.",https://stackoverflow.com/questions/71048521,huggingface-transformers,09-02-2022 11:10,3363.0,2.0,2.0,True,30-05-2023 10:36,10-02-2022 15:25,Implementation Issues
77010524,"will using arguments - max_length, truncate, and padding in tranformers pipeline affect the output?","hello so i was checking sentiment of a text using transformers pretrained model ,but doing so gave me error

runtimeerror: the size of tensor a (1954) must match the size of tensor b (512) at non-singleton dimension 1

i went through few post which suggested that setting max_length as 512 will sort the error.
it did resolve the error, but i want to know how it affects the quality of output. does it truncate my text? for example, if the length of my text is 1195 will it process till 512, something like text[:512]?","['python', 'nlp', 'huggingface-transformers', 'sentiment-analysis']",77017166,"yes. it means the sentiment will be based on the first 512 tokens, and any tokens after that will not influence the result.
note that this is tokens, not characters. if text was your raw string, and if we assume that on average each token is 2.5 characters, then truncating at 512 tokens would be the same as text[:1280].
(the characters per token can vary a lot based on the model, the tokenizer, the language, the domain, but mainly how unusual the string is compared to the text used to train the tokenizer.)
by the way, according to  if you don't specify truncation then no truncation is applied; and if you do, but don't specify max_length then it will default to the maximum supported by the model. so setting max_length and not changing anything else shouldn't have fixed it. (i've not tested anything, or read the code, that is just based on my understanding of the documentation.)",https://stackoverflow.com/questions/77010524,python,30-08-2023 18:08,873.0,0.0,1.0,True,01-09-2023 19:07,01-09-2023 19:07,Implementation Issues
72480729,error when taking fft2d in tensorflow on gpu,"the code below runs on cpu without any problems, but when i change to gpu in colab it fails to calculate the fft2d.
import tensorflow as tf

sample_fft_input = tf.random.uniform((2, 10, 20))
sfi = tf.cast(sample_fft_input , tf.complex64)
sfi = tf.math.real(tf.signal.fft2d(sfi))
print(sfi.shape) -> tensorshape([2, 10, 20])

but on gpu:
---------------------------------------------------------------------------
internalerror                             traceback (most recent call last)
<ipython-input-41-094e0f7d5037> in <module>()
      3 sample_fft_input = tf.random.uniform((2, 10, 20))
      4 sfi = tf.cast(sample_fft_input, tf.complex64)
----> 5 sfi = tf.math.real(tf.signal.fft2d(sfi))
      6 print(sfi.shape)

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   7162 def raise_from_not_ok_status(e, name):
   7163   e.message += ("" name: "" + name if name is not none else """")
-> 7164   raise core._status_to_exception(e) from none  # pylint: disable=protected-access
   7165 
   7166 

internalerror: fft failed : type=1 in.shape=[2,10,20] [op:fft2d]","['tensorflow', 'machine-learning', 'nlp', 'tensorflow2.0', 'fft']",72482363,i downgraded to tensorflow 2.8.2 and the problem is gone.,https://stackoverflow.com/questions/72480729,tensorflow,02-06-2022 18:20,157.0,0.0,1.0,True,02-06-2022 21:08,02-06-2022 18:27,Tool Setup/Errors
62669261,how to encode multiple sentences using transformers.berttokenizer?,"i would like to create a minibatch by encoding multiple sentences using transform.berttokenizer. it seems working for a single sentence. how to make it work for several sentences?
from transformers import berttokenizer

tokenizer = berttokenizer.from_pretrained('bert-base-uncased')

# tokenize a single sentence seems working
tokenizer.encode('this is the first sentence')
>>> [2023, 2003, 1996, 2034, 6251]

# tokenize two sentences
tokenizer.encode(['this is the first sentence', 'another sentence'])
>>> [100, 100] # expecting 7 tokens","['word-embedding', 'huggingface-transformers', 'huggingface-tokenizers']",62688252,"transformers >= 4.0.0:
use __call__ method of the tokenizer. it will generate a dictionary which contains the input_ids, token_type_ids and the attention_mask as list for each input sentence:
tokenizer(['this is the first sentence', 'another setence'])

output:
{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 102], [101, 2178, 2275, 10127, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}

transformers < 4.0.0:
use tokenizer.batch_encode_plus (documentation). it will generate a dictionary which contains the input_ids, token_type_ids and the attention_mask as list for each input sentence:
tokenizer.batch_encode_plus(['this is the first sentence', 'another setence'])

output:
{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 102], [101, 2178, 2275, 10127, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}

applies to call and batch_encode_plus:
in case you only want to generate the input_ids, you have to set return_token_type_ids and return_attention_mask to false:
tokenizer.batch_encode_plus(['this is the first sentence', 'another setence'], return_token_type_ids=false, return_attention_mask=false)

output:
{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 102], [101, 2178, 2275, 10127, 102]]}",https://stackoverflow.com/questions/62669261,word-embedding,01-07-2020 03:32,24158.0,15.0,2.0,True,08-03-2023 22:10,07-10-2021 05:55,Implementation Issues
72845812,"bert-base-uncased: typeerror: tuple indices must be integers or slices, not tuple","i want to see embeddings for the input text i give to the model, and then feed it to the rest of the bert. to do so, i partitioned the model into two sequential models, but i must have done it wrong because rest_of_bert model raises typeerror. original model does not raise any error with the input_ids as input processed with text_to_input function.
input[0]:
import torch
from transformers import berttokenizer, bertmodel

tokenizer = berttokenizer.from_pretrained('bert-base-uncased')
cls_token_id = tokenizer.cls_token_id
sep_token_id = tokenizer.sep_token_id
pad_token_id = tokenizer.pad_token_id

model = bertmodel.from_pretrained('bert-base-uncased', output_hidden_states=true)
model.eval()

output[0]:
some weights of the model checkpoint at bert-base-uncased were not used when initializing bertmodel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.layernorm.bias', 'cls.predictions.transform.layernorm.weight', 'cls.seq_relationship.bias']
- this is expected if you are initializing bertmodel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a bertforsequenceclassification model from a bertforpretraining model).
- this is not expected if you are initializing bertmodel from the checkpoint of a model that you expect to be exactly identical (initializing a bertforsequenceclassification model from a bertforsequenceclassification model).
bertmodel(
  (embeddings): bertembeddings(
    (word_embeddings): embedding(30522, 768, padding_idx=0)
    (position_embeddings): embedding(512, 768)
    (token_type_embeddings): embedding(2, 768)
    (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
    (dropout): dropout(p=0.1, inplace=false)
  )
  (encoder): bertencoder(
    (layer): modulelist(
      (0): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (1): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (2): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (3): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (4): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (5): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (6): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (7): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (8): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (9): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (10): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (11): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
    )
  )
  (pooler): bertpooler(
    (dense): linear(in_features=768, out_features=768, bias=true)
    (activation): tanh()
  )
)

input[1]:
def text_to_input(text):
  x = tokenizer.encode(text, add_special_tokens=false) # returns python list
  x = [cls_token_id] + x + [sep_token_id]
  token_count = len(x)
  pad_count = 512 - token_count
  x = x + [pad_token_id for i in range(pad_count)]
  return torch.tensor([x])

extract_embeddings = torch.nn.sequential(list(model.children())[0])
rest_of_bert = torch.nn.sequential(*list(model.children())[1:])

input_ids = text_to_input('a sentence.')
x_embedding = extract_embeddings(input_ids)
output = rest_of_bert(x_embedding)

output[1]:
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
<ipython-input-5-d371d8a2fb3c> in <module>()
     12 input_ids = text_to_input('a sentence.')
     13 x_embedding = extract_embeddings(input_ids)
---> 14 output = rest_of_bert(x_embedding)

4 frames
/usr/local/lib/python3.7/dist-packages/transformers/utils/generic.py in __getitem__(self, k)
    220             return inner_dict[k]
    221         else:
--> 222             return self.to_tuple()[k]
    223 
    224     def __setattr__(self, name, value):

typeerror: tuple indices must be integers or slices, not tuple","['python', 'machine-learning', 'pytorch', 'huggingface-transformers', 'bert-language-model']",72847541,"each bertlayer returns a tuple that contains at least one tensor (depending on what output you requested). the first element of the tuple is the tensor you want to feed to the next bertlayer.
a more huggingface-like approach would be calling the model with output_hidden_states:
o = model(input_ids, output_hidden_states=true)
print(len(o.hidden_states))

output:
13

the first tensor of the hidden_states tuple is the output of your extract_embeddings object (token embeddings). the other 12 tensors are the contextualized embeddings that are the output of each bertlayer.
you should, by the way, provide an attention mask, because otherwise, your padding tokens will affect your output. the tokenizer is able to do that for you and you can replace your whole text_to_input method with:
tokenizer('a sentence.', return_tensors='pt', padding='max_length', max_length=512)",https://stackoverflow.com/questions/72845812,python,03-07-2022 10:43,1448.0,0.0,1.0,True,03-07-2022 15:02,03-07-2022 14:33,Implementation Issues
77868284,combining falcon 40b instruct with langchain,"i want to create a local llm using falcon 40b instruct model and combine it with lanchain so i can give it a pdf or some resource to learn from so i can query it ask it questions, learn from it and ultimately be able to derive insights from the pdf report from an excel sheet.
for now, i just want to load a pdf using langchain and have the falcon-40b-instruct model as the agent.
i want to build an llm where i can make it interact with my own data using langchain.
here is my attempt so far:
from langchain_community.llms import huggingfacehub

llm = huggingfacehub(
repo_id=model_name,
task=""text-generation"",
model_kwargs={
""max_new_tokens"": 512,
""top_k"": 30,
""temperature"": 0.1,
""repetition_penalty"": 1.03
},
huggingfacehub_api_token=""hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""
)

i reached the following stage:
from langchain_community.chat_models.huggingface import chathuggingface
llm = chathuggingface(llm=llm)

yet i get this error:

hfhub 401 client error: unauthorized for url

i am doing do this to be able to run the following:
qa_chain = retrievalqa.from_chain_type(
llm=llm,
retriever=vector_db.as_retriever()
)

what am i missing and is there a way to be able to do this fully local like doing the falcon model and pass it to chathuggingface?","['nlp', 'chatbot', 'langchain', 'large-language-model', 'falcon']",77870500,"the message of hfhub 401 client error: unauthorized for url indicates that you don't have access to endpoint service from huggingface > 
as you want to run everything locally, here's an example, using huggingfacepipeline function
from transformers import autotokenizer, automodelforcausallm, pipeline

# model_id = ""tiiuae/falcon-7b-instruct""  # this model is too large to run on nvidia 4090 with 16g ram
model_id = ""gpt2""
tokenizer = autotokenizer.from_pretrained(model_id)
model = automodelforcausallm.from_pretrained(model_id)
pipeline = pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=200,
)


from langchain_community.llms.huggingface_pipeline import huggingfacepipeline
llm = huggingfacepipeline(pipeline=pipeline)


from langchain.prompts import prompttemplate
template = """"""question: {question}

answer: let's think step by step.""""""
prompt = prompttemplate.from_template(template)

chain = prompt | llm
question = ""tell me about italy""

print(chain.invoke({""question"": question}))",https://stackoverflow.com/questions/77868284,nlp,23-01-2024 17:43,328.0,0.0,1.0,True,24-01-2024 03:54,24-01-2024 02:17,Implementation Issues
21492480,python isristemmer for arabic text,"i am running the following code on idle(python) and i want to enter arabic string and get the stemming for it but actually it doesn't work
>>> from nltk.stem.isri import isristemmer
>>> st = isristemmer()
>>> w= 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½'
>>> join = w.decode('windows-1256')
>>> print st.stem(join).encode('windows-1256').decode('utf-8')

the result of running it is the same text in w which is 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½' which is not the stem
but when do the following:
>>> print st.stem(u'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½')

the result succeeded and returns the stem which is 'ï¿½ï¿½ï¿½ï¿½ï¿","['python', 'utf-8', 'arabic', 'stemming']",21506361,"ok, i solved the problem by myself using the following:
w = 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½' 
st.stem(w.decode('utf-8'))

and it gives the root correctly which is ""ï¿½ï",https://stackoverflow.com/questions/21492480,python,01-02-2014 00:17,10727.0,6.0,7.0,True,18-12-2023 03:08,19-07-2020 07:30,Implementation Issues
66110901,fine-tuned albert question and answering with huggingface,"i'm trying to create a question and answering ai, i would like it to be as accurate as possible without having to train the model myself.
i can create a simple ai using the existing base models like so via their documentation:
from transformers import alberttokenizer, albertforquestionanswering
import torch
tokenizer = alberttokenizer.from_pretrained('albert-base-v2')
model = albertforquestionanswering.from_pretrained('albert-base-v2')
question, text = ""what does he like?"", ""he likes bears""
inputs = tokenizer(question, text, return_tensors='pt')
start_positions = torch.tensor([1])
end_positions = torch.tensor([3])
outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
loss = outputs.loss
start_scores = outputs.start_logits
end_scores = outputs.end_logits

answer_start = torch.argmax(start_scores)  # get the most likely beginning of answer with the argmax of the score
answer_end = torch.argmax(end_scores) + 1
tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[""input_ids""][0][answer_start:answer_end]))

however this model doesn't answer questions as accurate as others.  on the huggingface site i've found an example that i'd like to use of a fine-tuned model
however the instructions show how to train a model like so.  the example works on the page so clearly a pretrained model of the exists.
does anyone know how i can reuse the existing models so i don't have to train one from scratch?","['python', 'nlp', 'artificial-intelligence', 'torch', 'bert-language-model']",66111128,"turns out i just needed to grab an additional identifier when trying to request the model:
from transformers import alberttokenizer, albertforquestionanswering
import torch

model_path = 'ktrapeznikov/albert-xlarge-v2-squad-v2';

tokenizer = alberttokenizer.from_pretrained(model_path)
model = albertforquestionanswering.from_pretrained(model_path)

for future reference this information can be grabbed from the transformers use button.  seem in the image below.",https://stackoverflow.com/questions/66110901,python,08-02-2021 23:32,996.0,0.0,1.0,True,09-02-2021 00:02,08-02-2021 23:41,Implementation Issues
74519464,attributeerror: &#39;tuple&#39; object has no attribute &#39;rank&#39; when calling model.fit() in nlp task,"i'm following this tutorial

however, while implementing the ann based on the tf-idf features, i'm getting this error
attributeerror: 'tuple' object has no attribute 'rank'
this is the snippet-
from sklearn.feature_extraction.text import tfidfvectorizer
tvec1 = tfidfvectorizer(max_features=100000,ngram_range=(1, 3))
tvec1.fit(x_train)


x_train_tfidf = tvec1.transform(x_train)
x_validation_tfidf = tvec1.transform(x_validation).toarray()

seed = 7
np.random.seed(seed)
import tensorflow as tf
from tensorflow.keras.models import sequential
from tensorflow.keras.layers import dense, dropout
from tensorflow.keras.layers import flatten
#from tensorflow.keras.layers.embeddings import embedding
from tensorflow.keras.layers import embedding
from tensorflow.keras.preprocessing import sequence

def batch_generator(x_data, y_data, batch_size):
    samples_per_epoch = x_data.shape[0]
    number_of_batches = samples_per_epoch/batch_size
    counter=0
    index = np.arange(np.shape(y_data)[0])
    while 1:
        index_batch = index[batch_size*counter:batch_size*(counter+1)]
        x_batch = x_data[index_batch,:].toarray()
        y_batch = y_data[y_data.index[index_batch]]
        counter += 1
        yield x_batch,y_batch
        if (counter > number_of_batches):
            counter=0

model = sequential()
model.add(dense(64, activation='relu', input_dim=100000))
model.add(dense(1, activation='sigmoid'))
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(batch_generator(x_train_tfidf, y_train, 32), epochs=5, validation_data=(x_validation_tfidf, y_validation),steps_per_epoch=x_train_tfidf.shape[0]/32)

this is the error-
---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
~\appdata\local\temp\ipykernel_13000\1276649087.py in <module>
      1 model.fit(batch_generator(x_train_tfidf, y_train, 32),
      2                     epochs=5, validation_data=(x_validation_tfidf, y_validation),
----> 3                     steps_per_epoch=x_train_tfidf.shape[0]/32)

~\appdata\roaming\python\python37\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1145           use_multiprocessing=use_multiprocessing,
   1146           model=self,
-> 1147           steps_per_execution=self._steps_per_execution)
   1148 
   1149       # container that configures and calls `tf.keras.callback`s.

~\appdata\roaming\python\python37\site-packages\tensorflow\python\keras\engine\data_adapter.py in get_data_handler(*args, **kwargs)
   1362   if getattr(kwargs[""model""], ""_cluster_coordinator"", none):
   1363     return _clustercoordinatordatahandler(*args, **kwargs)
-> 1364   return datahandler(*args, **kwargs)
   1365 
   1366 

~\appdata\roaming\python\python37\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)
   1164         use_multiprocessing=use_multiprocessing,
   1165         distribution_strategy=ds_context.get_strategy(),
-> 1166         model=model)
   1167 
   1168     strategy = ds_context.get_strategy()

~\appdata\roaming\python\python37\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)
    826       return tensor_shape.tensorshape([none for _ in shape.as_list()])
    827 
--> 828     output_shapes = nest.map_structure(_get_dynamic_shape, peek)
    829     output_types = nest.map_structure(lambda t: t.dtype, peek)
    830 

~\appdata\roaming\python\python37\site-packages\tensorflow\python\util\nest.py in map_structure(func, *structure, **kwargs)
    865 
    866   return pack_sequence_as(
--> 867       structure[0], [func(*x) for x in entries],
    868       expand_composites=expand_composites)
    869 

~\appdata\roaming\python\python37\site-packages\tensorflow\python\util\nest.py in <listcomp>(.0)
    865 
    866   return pack_sequence_as(
--> 867       structure[0], [func(*x) for x in entries],
    868       expand_composites=expand_composites)
    869 

~\appdata\roaming\python\python37\site-packages\tensorflow\python\keras\engine\data_adapter.py in _get_dynamic_shape(t)
    822       shape = t.shape
    823       # unknown number of dimensions, `as_list` cannot be called.
--> 824       if shape.rank is none:
    825         return shape
    826       return tensor_shape.tensorshape([none for _ in shape.as_list()])

attributeerror: 'tuple' object has no attribute 'rank'","['python', 'keras', 'neural-network', 'nlp', 'tf-idf']",78104970,"this happens when function expects a tensor, but other type of data is passed instead.
for instance, numpy array, like the one below:
y.shape


out: (2000,)
y.shape.rank

out: attributeerror: 'tuple' object has no attribute 'rank'
to correct this, data should be converted to tensor.
tf.constant(y).shape.rank

out: 1",https://stackoverflow.com/questions/74519464,python,21-11-2022 13:08,926.0,1.0,1.0,True,11-09-2024 15:42,21-11-2022 13:12,Implementation Issues
66087475,chatterbot error- oserror: [e941] can&#39;t find model &#39;en&#39;,"i tried running my first chatterbot program (its from the pypi page of chatterbot), and when i run it, i get an error. the error is related to spacy, but i am unable to find a solution.
here is the code:
from chatterbot import chatbot
from chatterbot.trainers import chatterbotcorpustrainer

chatbot = chatbot('ron obvious')

trainer = chatterbotcorpustrainer(chatbot)

trainer.train(""chatterbot.corpus.english"")

chatbot.get_response(""hello, how are you today?"")

and here is the error:
traceback (most recent call last):
  file ""c:/users/user/desktop/bot.py"", line 77, in <module>
    chatbot = chatbot('ron obvious')
  file ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\chatterbot\chatterbot.py"", line 28, in __init__
    self.storage = utils.initialize_class(storage_adapter, **kwargs)
  file ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\chatterbot\utils.py"", line 33, in initialize_class
    return class(*args, **kwargs)
  file ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\chatterbot\storage\sql_storage.py"", line 20, in __init__
    super().__init__(**kwargs)
  file ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\chatterbot\storage\storage_adapter.py"", line 21, in __init__
    'tagger_language', languages.eng
  file ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\chatterbot\tagging.py"", line 13, in __init__
    self.nlp = spacy.load(self.language.iso_639_1.lower())
  file ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\spacy\__init__.py"", line 47, in load
    return util.load_model(name, disable=disable, exclude=exclude, config=config)
  file ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\spacy\util.py"", line 328, in load_model
    raise ioerror(errors.e941.format(name=name, full=old_model_shortcuts[name]))
oserror: [e941] can't find model 'en'. it looks like you're trying to load a model from a shortcut, which is deprecated as of spacy v3.0. to load the model, use its full name instead:

nlp = spacy.load(""en_core_web_sm"")

for more details on the available models, see the models directory:  if you want to create a blank model, use spacy.blank: nlp = spacy.blank(""en"")

it would be helpful if someone finds a solution for this.","['python', 'windows', 'spacy', 'chatterbot']",66087946,"make sure you actually have the right spacy model installed. for example, install en_core_web_sm with the python -m spacy download en_core_web_sm command in the terminal.
next, fix this error:
file ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\chatterbot\tagging.py"", line 13, in __init__
    self.nlp = spacy.load(self.language.iso_639_1.lower())

that is,

open the c:\users\user\appdata\local\programs\python\python37\lib\site-packages\chatterbot\tagging.py file
go to line 13
replace self.nlp = spacy.load(self.language.iso_639_1.lower()) with

if self.language.iso_639_1.lower() == 'en':
    self.nlp = spacy.load('en_core_web_sm')
else:
    self.nlp = spacy.load(self.language.iso_639_1.lower())

you will need to add more conditions for other languages you need to support.",https://stackoverflow.com/questions/66087475,python,07-02-2021 11:41,34041.0,10.0,9.0,True,23-04-2024 23:35,23-04-2024 23:35,Tool Setup/Errors
71205484,using torchtext vocab with torchscript,"i'm trying to use the torchtext vocab layer along with torchscript but i'm getting some errors and i was wondering if someone here has made it work.
my current model is
class vocabtext(torch.nn.module):

    def __init__(self):

        super(vocabtext, self).__init__()

        self.embedding = torch.nn.embedding(10,128)

        vocab = ['this', 'is', 'a', 'test']

        counter = counter(vocab)

        self.lookup = text.vocab.vocab(counter)

        self.tensor = torch.tensor

    def forward(self, x: str):

        x_mapped = self.lookup(x)

        x_mapped = self.tensor(x_mapped).int()

        x_mapped = self.embedding(x_mapped)

        

        return x

that works when i do a pass of the model like this:
example_str = [""is""]

model(example_str)

but when i try to compile it with torchscript it fails:
model_scripted = torch.jit.script(model)   

model_scripted.save('model_scripted.pt')

with the following error:
runtimeerror: 
unknown builtin op: aten::tensor.
here are some suggestions: 

for when i map the result of the lookup layer during the forward function
i think is due to typing as the vocab layer expects strings as input but the embedding layer will use tensors. im doing a cast in the middle of the forward.
i have a working notebook in colab to reproduce this issue if anybody wants:","['nlp', 'pytorch', 'torchtext', 'torchscript']",71205542,"turns out i had to change the function to built the tensor, found at",https://stackoverflow.com/questions/71205484,nlp,21-02-2022 11:22,295.0,0.0,1.0,True,23-02-2022 08:37,23-02-2022 08:37,Implementation Issues
70489853,count words in texts that are not in a given dictionary,"how can i find and count words that are not in a given dictionary?
the example below counts every time specific dictionary words (clouds and storms) appear in the text.
library(""quanteda"")
txt <- ""forty-four americans have now taken the presidential oath. the words have been spoken during rising tides of prosperity and the still waters of peace. yet, every so often the oath is taken amidst gathering clouds and raging storms. at these moments, america has carried on not simply because of the skill or vision of those in high office, but because we the people have remained faithful to the ideals of our forbearers, and true to our founding documents.""   
mydict <- dictionary(list(all_terms = c(""clouds"", ""storms"")))
dfmat <- tokens(txt) %>%
  tokens_select(mydict) %>%
  dfm()
dfmat

the output:
docs    clouds storms
  text1      1      1

how can i instead generate a count of all words that are not in the dictionary (clouds/storms)? ideally with stopwords excluded.
e.g., desired output:
docs    forty-four americans ...
  text1      1      1","['r', 'nlp', 'word-count', 'quanteda']",70501318,"when you check the help file for tokens_select (run ?tokens_select) you can see that the third argument is selection. the default value is ""keep"", yet what you want is ""remove"". since this is a common thing to do, there is also a dedicated tokens_remove command, which i use below to to remove stopwords.
dfmat <- tokens(txt) %>%
  tokens_select(mydict, selection = ""remove"") %>%
  tokens_remove(stopwords::stopwords(language = ""en"")) %>% 
  dfm()
dfmat
#> document-feature matrix of: 1 document, 38 features (0.00% sparse) and 0 docvars.
#>        features
#> docs    forty-four americans now taken presidential oath . words spoken rising
#>   text1          1         1   1     2            1    2 4     1      1      1
#> [ reached max_nfeat ... 28 more features ]

i think this is what you are trying to do.
created on 2021-12-28 by the reprex package (v2.0.1)",https://stackoverflow.com/questions/70489853,r,26-12-2021 21:02,84.0,0.0,2.0,True,28-12-2021 17:14,27-12-2021 15:26,Preprocessing Tasks
46983843,is it possible to filter on a non-indexed field with doc_values=true in elasticsearch,"in elasticsearch 5.6 using the following mapping:
""category"" => [
    ""type""=>""keyword"",
    ""doc_values""=>true,
    ""index""=>false
    ""store""=>true
]

i was given advice that it was possible to write a query that filters on this field because of its doc_values setting, even though the index attribute was set to false, but it seems like doc_values fields are only useful for aggregations and sorting. 
is it possible to create a query which filters on this field?","['elasticsearch', 'search', 'lucene', 'information-retrieval', 'elasticsearch-5']",47041799,"an field that is not indexed is, by definition, not searchable. elasticsearch won't put it in the inverted index (which is used for searching). if you try to run a search query, you will get an error like cannot search on field [category] since it is not indexed.",https://stackoverflow.com/questions/46983843,elasticsearch,27-10-2017 21:47,11515.0,4.0,3.0,True,20-07-2023 09:15,30-10-2017 20:25,Implementation Issues
65699672,how to force a certain tag in spacy?,"i'm using spacy '3.0.0rc2' with a custom model. unfortunately my training data is low in hyphens (-), therefore the hyphen often gets tagged as noun.
is there some way to force a certain tag or pos, to make sure that all the  - tokens get tagged with punct?
basically i am looking for a solution like proposed in the answer to this question here:
how to force a pos tag in spacy before/after tagger?
unfortunately this does not seem to work anymore (at least for spacy 3) and raises an error:
valueerror: [e1005] unable to set attribute 'pos' in tokenizer exception for '{g}'. tokenizer exceptions are only allowed to specify orth and norm.

(same when trying to assign the tag attribute)
i know that it would be possible to create a custom component with a matcher that looks just for the hyphen and assigns the right tag. however this seems to be overkill when considering that i currently just want to handle one token.
is there some way to force tags in spacy 3, without re-tagging during processing using a custom component?
ideally i would want to modify the tag attribute and let the pos attribute get assigned automatically by spacy based on that tag attribute.
as in the spacy-annotations tag=hyph should be mapped to pos=punct.","['python', 'nlp', 'spacy']",65701412,"in spacy v3, exceptions like this can be implemented in the attribute_ruler component:
ruler = nlp.add_pipe(""attribute_ruler"")
patterns = [[{""orth"": ""-""}]]
attrs = {""tag"": ""hyph"", ""pos"": ""punct""}
ruler.add(patterns=patterns, attrs=attrs)

be aware that the attribute ruler runs the pattern matching once based on the initial doc state, so you can't use the output attrs of one rule as the input pattern for another. this comes up in pipelines like en_core_web_sm, where the included attribute ruler does the tag->pos mapping. so if you have another rule that should match on a pos pattern, you'd have to add a second attribute ruler component to handle those cases.
see:",https://stackoverflow.com/questions/65699672,python,13-01-2021 10:07,1533.0,2.0,2.0,True,26-04-2021 09:51,13-01-2021 10:52,Implementation Issues
62031910,textplot::plot.btm(): avoid thick edges and weird colors in cluster visualization,"goal and tools
i currently try to familiarize with the r packages btm and textplot, that is how to create readable and meaningful visualizations of biterm topic models (btm models) created with btm via textplot. textplot::plot.btm() is a method that creates a cluster visualization of models created with btm::btm(). both the documentation of btm::btm() and the documentation of textplot::plot.btm() contain code examples dedicated to plotting. according to the documentation, textplot::plot.btm() returns an object of class ggplot.
previous attempts and observations
after installing the packages mentioned in these examples, namely concaveman, ggraph and igraph, i could successfully replicate these example plots. also, my own plots looked like the examples.
though, when i start with a fresh r session and run my script (see minimal code below), my plots suddenly look different. the colors of the cluster shapes and especially the edges are no longer light pastel shades like in the demo plots, but rather dark and bright shades, e.g. a dark brown that yields a low contrast regarding the black tokens (see this screenshot). besides that, the edges have become extremely thick, so they cover and exceed the cluster shapes underneath. that way, the plot is unreadable and definitely looks broken. very weird.
i have noticed that r outputs something like*
load required namespace: ggraph
load required namespace: concaveman

when i run textplot::plot.btm(), although sometimes only concaveman is mentioned for an unknown reason. calling class() indicates the return value is of class ggraph which inherits from ggplot. it seems to me that these packages are properly installed and used by the function if needed. all involved packages are updated, that is btm 0.3.1 and textplot 0.1.2 are installed to explicitly mention the core package versions.
*i get these in german and translated literally.
my questions

how can i ensure that my plots always look as intended, that is have light pastel shades and adequately sized edges?
why do my plots look differently, that is have bright dark shades and extremly thick edges?
bonus question in terms of readability ;) : how can i ensure that all tokens have a readable font size? i noticed in both my and the example plots, that tokens with a low frequency are very tiny and, thus, hard to read.

many thanks for your help!
this is my first post on stackoverflow ever, so please let me know if i missed best practices of asking questions.
minimal code
library(btm)
library(textplot)
library(udpipe)
    
data(""brussels_reviews_anno"", package = ""udpipe"")
brussel_reviews <- subset(brussels_reviews_anno, language == ""nl"")
brussel_reviews <- subset(brussel_reviews, xpos %in% c(""nn"", ""nnp"", ""nns""))
brussel_reviews <- brussel_reviews[, c(""doc_id"", ""lemma"")]
    
btm_model <- btm(brussel_reviews, k = 5)
    
plot(btm_model, top_n = 15,
     title = ""topic clusters of top 15 biterms"",
     labels = c(""1 - too dark color masking terms"",
                ""2 - looks okish"",
                ""3 - too thick edges"",
                ""4 - too thick edges"",
                ""5 - too thick edges""))","['r', 'plot', 'cluster-computing', 'visualization', 'topic-modeling']",62220854,"i have actually found the answer to question 2 shortly after posting my question, but also got in touch with the developer of the btm package to clarify question 1 and would like to share with you in my own words. though, i might not explain every detail completely accurately, so corrections are welcome.
summary

how correct? attach the ggraph package in every session (see corrected minimal code below).
why wrong? with ggraph attached, the plot is drawn with another method added by ggraph.
how ensure readable tokens? i see no option, because we cannot change that some tokens very rarely cooccur with their neighbor terms. still, a readable minimal display size could be introduced. i will consider a smaller sliding window to avoid rare biterms with e.g. btm::btm(window = 5).

corrected minimal code
library(btm)
library(ggraph)
library(textplot)
library(udpipe)

[...]

details

how? for btm plots to look as expected, you not only have to install the packages
concaveman, ggraph and igraph, but to attach the ggraph package in every session. (of course, also btm is needed to produce btm models to visualize.)
looking at the documentation of btmand textplot, it is striking that ggraph is always attached in any example code. at first, though, i understood the lists as which packages need to be installed, not necessarily to be attached, especially after seeing the package lists vary from example to example:
btm::btm() mentions:

library(igraph)
library(btm)
library(ggraph)

textplot::plot.btm() mentions:
library(textplot)
library(ggraph)
library(concaveman)

textplot::textplot_bitermclusters(), the underlying function, mentions:
library(igraph)
library(ggraph)
library(concaveman)
library(btm)


why? plots look differently with my minimal code, because the package ggraph is not attached. attaching ggraph is not necessary for plot objects to be produced properly by plot.btm(), but for the plot objects to be printed correctly. more precisely, ggraph plots via ggplot2::print.ggplot(), but extends the internal generic function ggplot2::ggplot_build() by a method for ggraph objects, namely ggplot_build.ggraph(). there, the algorithm of how to plot is temporarily changed before calling ggplot_build.ggplot(), yielding the different, more pleasant look.
ensure readable tokens? in textplot::plot.btm(), it seems that the token size depends on how often the token cooccurs with its neighbor tokens within a topic cluster. i would like the option to set a meaningful minimal display size, because i see no point in including tokens i cannot read, especially given that i cannot infer the frequency from the token size that precisely anyway. one thing to try is to narrow the sliding window which biterms are build in via e.g. btm::btm(window = 5). that way, i might avoid too many rare combinations of tokens and, thus, tiny tokens in the plot. i am not sure, though.",https://stackoverflow.com/questions/62031910,r,26-05-2020 21:43,291.0,0.0,1.0,True,03-11-2022 00:47,03-11-2022 00:47,Implementation Issues
79471648,"when using &#39;interrupt&#39; followed by &#39;new command({ resume: ...})&#39;, get an undefined message error from langchain + langgraph","when i invoke a graph that includes interrupts in one of its nodes, it seems to get into an invalid/ unrecoverable state, with the following error:
        return chatgeneration.message;
                              ^

typeerror: cannot read properties of undefined (reading 'message')
    at chatopenai.invoke (file:///users/user/code/lgdemo//node_modules/@langchain/core/dist/language_models/chat_models.js:64:31)

the first encounter of the interrupt appears to go well, but after the second encounter this occurs.
(i include the minimal code to reproduce this in full below the question text.)
the main logic is in approvenode, which contains the interrupt.

if the user responds with y, it proceeds to the toolsnode which the agentnode requested.
if the user responds with anything else (e.g. n), it proceeds to end

the issue is that once it proceeds to end,
the subsequent call to graph.invoke results in this error.
another thing that i have tried is to change the logic in approvenode such that:

if the user responds with y, it proceeds to the toolsnode which the agentnode requested. (same as before)
if the user responds with anything else (e.g. n), it proceeds to back to agentnode. (this has changed)

(and i change the main graph accordingly to reflect this updated flow)
however, this results in the same error as above, just that it happens after the first interrupt instead of the second interrupt.
questions:

is the workflow that i have defined valid? is there a better way to structure it?
what's the underlying cause of this particular error message? (unfortunately being able to reproduce it hasn't given me clues as to why it occurs)
otherwise, how can i implement this such that i get a simple approve/ deny for tool calls going on?


references used:





main graph:
const workflow = new stategraph(messagesannotation)
  .addnode('agent', agentnode)
  .addnode('tools', toolsnode)
  .addnode('approve', approvenode, {
    ends: ['tools', end],
  })
  .addedge(start, 'agent')
  .addedge('tools', 'agent')
  .addconditionaledges('agent', agentrouter, ['approve', end]);
const checkpointer = new memorysaver();
const graph = workflow.compile({
  checkpointer,
});
const graphconfig = {
  configurable: { thread_id: '0x0004' },
};

tools, nodes, and routers:
const cmdfootool = tool(async function(inputs) {
  console.log('===tool cmd_foo===');
  return inputs.name;
}, {
  name: 'cmd_foo',
  description: 'invoke when you want to do a foo.',
  schema: z.object({
    name: z.string('any string'),
  }),
});
const cmdbartool = tool(async function(inputs) {
  console.log('===tool qry_bar===');
  return inputs.name;
}, {
  name: 'qry_bar',
  description: 'invoke when you want to query a bar.',
  schema: z.object({
    name: z.string('any string'),
  }),
});
const tools = [cmdfootool, cmdbartool];
const llmwithtools = llm.bindtools(tools);

const toolsnode = new toolnode(tools);

async function agentnode(state) {
  console.log('===agent node===');
  const response = await llmwithtools.invoke(state.messages);
  console.log('=response=',
    '\ncontent:', response.content,
    '\ntool_calls:', response.tool_calls.map((toolcall) => (toolcall.name)));
  return { messages: [response] };
}

async function approvenode (state) {
  console.log('===approve node===');
  const lastmsg = state.messages.at(-1);
  const toolcall = lastmsg.tool_calls.at(-1);

  const interruptmessage = `please review the following tool invocation:
${toolcall.name} with inputs ${json.stringify(toolcall.args, undefined, 2)}
do you approve (y/n)`;

  console.log('=interrupt pre=');
  const interruptresponse = interrupt(interruptmessage);
  console.log('=interrupt post=');

  const isapproved = (interruptresponse.trim().charat(0).tolowercase() === 'y');
  const goto = (isapproved) ? 'tools' : end;
  console.log('=result=\n', { isapproved, goto });
  return new command({ goto });
}

function hastoolcalls(message) {
  return message?.tool_calls?.length > 0;
}

async function agentrouter (state) {
  const lastmsg = state.messages.at(-1);
  if (hastoolcalls(lastmsg)) {
    return 'approve';
  }
  return end;
}

simulate a run:
let state;
let agentresult;
let inputtext;
let invokewith;

// step 1: prompt
inputtext = 'pls perform a foo with name ""asdf"".';
console.log('===human prompt===\n', inputtext);
invokewith = { messages: [new humanmessage(inputtext)] };
agentresult = await graph.invoke(invokewith, graphconfig);

state = await graph.getstate(graphconfig);
console.log('===state next===\n', state.next);
console.log('=last msg=\n', agentresult.messages.at(-1).content);
console.log('=last tool calls=\n', agentresult.messages.at(-1).tool_calls);

// step 2: interrupted in the 'approve' node, human in the loop authorises
inputtext = 'yes'
console.log('===human interrupt response===\n', inputtext);
invokewith = new command({ resume: inputtext });
agentresult = await graph.invoke(invokewith, graphconfig);

state = await graph.getstate(graphconfig);
console.log('===state next===\n', state.next);
console.log('=last msg=\n', agentresult.messages.at(-1).content);
console.log('=last tool calls=\n', agentresult.messages.at(-1).tool_calls);

// step 3: prompt
inputtext = 'pls perform a foo with name ""zxcv"".';
console.log('===human prompt===\n', inputtext);
invokewith = { messages: [new humanmessage(inputtext)] };
agentresult = await graph.invoke(invokewith, graphconfig);

state = await graph.getstate(graphconfig);
console.log('===state next===\n', state.next);
console.log('=last msg=\n', agentresult.messages.at(-1).content);
console.log('=last tool calls=\n', agentresult.messages.at(-1).tool_calls);

// step 4: interrupted in the 'approve' node, human in the loop does not authorise
inputtext = 'no';
console.log('===human interrupt response===\n', inputtext);
invokewith = new command({ resume: inputtext });
agentresult = await graph.invoke(invokewith, graphconfig);

state = await graph.getstate(graphconfig);
console.log('===state next===\n', state.next);
console.log('=last msg=\n', agentresult.messages.at(-1).content);
console.log('=last tool calls=\n', agentresult.messages.at(-1).tool_calls);

// step 5: prompt
inputtext = 'pls perform a foo with name ""ghjk"".';
console.log('===human prompt===\n', inputtext);
invokewith = { messages: [new humanmessage(inputtext)] };
agentresult = await graph.invoke(invokewith, graphconfig);

state = await graph.getstate(graphconfig);
console.log('===state next===\n', state.next);
console.log('=last msg=\n', agentresult.messages.at(-1).content);
console.log('=last tool calls=\n', agentresult.messages.at(-1).tool_calls);


full output:
===human prompt===
 pls perform a foo with name ""asdf"".
===agent node===
(node:58990) [dep0040] deprecationwarning: the `punycode` module is deprecated. please use a userland alternative instead.
(use `node --trace-deprecation ...` to show where the warning was created)
=response= 
content:  
tool_calls: [ 'cmd_foo' ]
===approve node===
=interrupt pre=
===state next===
 [ 'approve' ]
=last msg=
 
=last tool calls=
 [
  {
    name: 'cmd_foo',
    args: { name: 'asdf' },
    type: 'tool_call',
    id: 'call_u7ciywdtesfatz5bgg2uavuz'
  }
]
===human interrupt response===
 yes
===approve node===
=interrupt pre=
=interrupt post=
=result=
 { isapproved: true, goto: 'tools' }
===tool cmd_foo===
===agent node===
=response= 
content: the foo operation has been performed with the name ""asdf"". 
tool_calls: []
===state next===
 []
=last msg=
 the foo operation has been performed with the name ""asdf"".
=last tool calls=
 []
===human prompt===
 pls perform a foo with name ""zxcv"".
===agent node===
=response= 
content:  
tool_calls: [ 'cmd_foo' ]
===approve node===
=interrupt pre=
===state next===
 [ 'approve' ]
=last msg=
 
=last tool calls=
 [
  {
    name: 'cmd_foo',
    args: { name: 'zxcv' },
    type: 'tool_call',
    id: 'call_kkf91c8g6enwwlrlfon8tylj'
  }
]
===human interrupt response===
 no
===approve node===
=interrupt pre=
=interrupt post=
=result=
 { isapproved: false, goto: '__end__' }
===state next===
 []
=last msg=
 
=last tool calls=
 [
  {
    name: 'cmd_foo',
    args: { name: 'zxcv' },
    type: 'tool_call',
    id: 'call_kkf91c8g6enwwlrlfon8tylj'
  }
]
===human prompt===
 pls perform a foo with name ""ghjk"".
===agent node===
file:///users/user/code/lgdemo/node_modules/@langchain/core/dist/language_models/chat_models.js:64
        return chatgeneration.message;
                              ^

typeerror: cannot read properties of undefined (reading 'message')
    at chatopenai.invoke (file:///users/user/code/lgdemo//node_modules/@langchain/core/dist/language_models/chat_models.js:64:31)
    at process.processticksandrejections (node:internal/process/task_queues:105:5)
    at async runnablecallable.agentnode [as func] (file:///users/user/code/lgdemo//test.js:51:20)
    at async runnablecallable.invoke (file:///users/user/code/lgdemo//node_modules/@langchain/langgraph/dist/utils.js:79:27)
    at async runnablesequence.invoke (file:///users/user/code/lgdemo//node_modules/@langchain/core/dist/runnables/base.js:1274:33)
    at async _runwithretry (file:///users/user/code/lgdemo//node_modules/@langchain/langgraph/dist/pregel/retry.js:67:22)
    at async pregelrunner._executetaskswithretry (file:///users/user/code/lgdemo//node_modules/@langchain/langgraph/dist/pregel/runner.js:217:33)
    at async pregelrunner.tick (file:///users/user/code/lgdemo//node_modules/@langchain/langgraph/dist/pregel/runner.js:45:40)
    at async compiledstategraph._runloop (file:///users/user/code/lgdemo//node_modules/@langchain/langgraph/dist/pregel/index.js:1296:17)
    at async createandrunloop (file:///users/user/code/lgdemo//node_modules/@langchain/langgraph/dist/pregel/index.js:1195:17) {
  pregeltaskid: '7bd60c12-4beb-54b7-85a7-9bc1461600f5'
}

node.js v23.3.0","['javascript', 'langchain', 'langgraph']",79507858,"i'm an engineer on the langchain team, and what follows is a copy & paste of my response to the same question posed as a github issue on the langgraphjs repository.

i haven't executed your code, but i think that the issue could be that on refusal you're not inserting a toolmessage into the messages state. there are some docs on this here
you can handle this on refusal by returning a command with an update:  field that has a tool message. for example:
async function approvenode (state) {
  console.log('===approve node===');
  const lastmsg = state.messages.at(-1);
  const toolcall = lastmsg.tool_calls.at(-1);

  const interruptmessage = `please review the following tool invocation:
${toolcall.name} with inputs ${json.stringify(toolcall.args, undefined, 2)}
do you approve (y/n)`;

  console.log('=interrupt pre=');
  const interruptresponse = interrupt(interruptmessage);
  console.log('=interrupt post=');

  const isapproved = (interruptresponse.trim().charat(0).tolowercase() === 'y');
  if (isapproved) {
      return new command({ goto: 'tools' });
  }

  // rejection case
  return new command({
    goto: end,
    update: {
      messages: [
        new toolmessage({
          status: ""error""
          content: `the user declined your request to execute the ${toolcall.name} tool, with arguments ${json.stringify(toolcall.args)}`,
          tool_call_id: toolcall.id
        }]
    });
}

also bear in mind that this implementation is not handling parallel tool calls. to handle parallel tool calls you have a few options.

decline to process all tool calls if the user disallows any tool call

you'll need to add one  rejection toolmessage per tool call, as shown above
in this model you might as well also only call interrupt once for the whole batch of calls


allow approved calls to proceed without running denied calls:

two ways to do this:
option 1: process all of the interrupts/approvals in a loop and return a command that routes to tools if any calls are approved (or end if no calls are approved)

to prevent the declined calls from processing, you'll want to use a send object in the goto field and send a copy of the aimessage with the tool calls filtered down to just the approved list.
you'll still need the array of toolmessage in the update field of the command as above - one for each declined call.


option 2: use an array of send in your conditional edge to fan out the tool calls to the tools node (by sending a filtered copy of the aimessage, as mentioned above) and do the interrupt in the tool handler.

without send here you would wind up processing all tool messages in the same node, which would cause the approved tool calls to be reprocessed every time the graph is interrupted after that particular tool call is approved.





here's a hastily-written example of how you could write a wrapper that requires approval for individual tool handlers for use with the ""option 2"" approach mentioned in the last bullet above:
function requiresapproval<toolhandlert extends (...args: unknown[]) => unknown>(toolhandler: toolhandlert) {
  return (...args: unknown[]) => {
    const interruptmessage = `please review the following tool invocation: ${toolhandler.name}(${args.map(json.stringify).join"", ""})`;
    const interruptresponse = interrupt(interruptmessage);
    const isapproved = (interruptresponse.trim().charat(0).tolowercase() === 'y');
    if (isapproved) {
      return toolhandler(..args);
    }
    throw new error(`the user declined your request to execute the ${toolhandler.name} tool, with arguments ${json.stringify(args)}`);
  }
}",https://stackoverflow.com/questions/79471648,javascript,27-02-2025 05:09,383.0,2.0,1.0,True,17-03-2025 08:46,27-02-2025 08:09,Conceptual Questions
78917743,how to process data on gpu instead of ram for this python code?,"i'm currently using the following code to process audio data, but it runs on the ram. i want to offload the processing to the gpu to improve performance.
my code :
def prepare_dataset(batch):
    audio = batch[""audio""]
    batch[""input_features""] = feature_extractor(
        audio[""array""], 
        sampling_rate=audio[""sampling_rate""]
    ).input_features[0]
    batch[""labels""] = tokenizer(batch[""sentence""]).input_ids
    return batch

common_voice = common_voice.map(
    prepare_dataset, 
    remove_columns=common_voice.column_names[""train""], 
    num_proc=1
)

how can i modify this code to utilize the gpu for processing instead of the ram? any guidance or specific changes are much appreciated!","['nlp', 'gpu', 'torch', 'openai-whisper']",78918851,"you can using the following code to process audio data on gpu
import torch
device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
print(device)

def prepare_dataset(batch):
    audio = batch[""audio""]

    input_features = feature_extractor(audio[""array""], sampling_rate=audio[""sampling_rate""]).input_features[0]
    batch[""input_features""] = torch.tensor(input_features).to(device)

    labels = tokenizer(batch[""sentence""]).input_ids
    batch[""labels""] = torch.tensor(labels).to(device)
    return batch

common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[""train""])",https://stackoverflow.com/questions/78917743,nlp,27-08-2024 08:03,64.0,1.0,1.0,True,28-08-2024 04:42,27-08-2024 10:54,Implementation Issues
77839828,openai assistants api: why does a single question i ask my assistant spend so many tokens?,"i have a nodejs program that connects to openai's assistant api to create messages. i have followed this documentation from openai to create the steps below:

i have created an assistant (gpt-4-1106-preview) and a thread in that assistant that i'm accessing to interact with.
add a message to the thread. the message contains around 1000 tokens, checked via 

    openai.beta.threads.messages.create(threadid, {
        role: ""user"",
        content: createmessage(),
    });


run the assistant

    await openai.beta.threads.runs.create(threadid, {
        assistant_id: assistantid,
        instructions:
             ""please address the user as mahesh. the user is an administrator."",
    });


check the status. i'm running this every 5 seconds until the status is ""completed""

    await openai.beta.threads.runs.retrieve(threadid, runid);


get the last response from the assistant

   const messages = await openai.beta.threads.messages.list(threadid, {
      limit: 1,
   });

this code takes around 250,000 tokens to complete. the image shows today's token usage for three requests.","['openai-api', 'gpt-4', 'openai-assistants-api']",77841022,"there could be multiple reasons why your cost of running an assistant is very high.
what openai model do you use?
if you take a look at the official openai documentation, you'll see that they use the gpt-4-1106-preview model. they state:

we recommend using openaiï¿½ï¿½ï¿½s latest models with the assistants api for
best results and maximum compatibility with tools.

but older models might be good enough. it depends on what your assistant is used for. you can lower the cost of running the assistant just by changing the model. of course, if you see that the performance of the assistant is considerably worse, then you need to use the latest models. just take a look at the table below to see what a difference a model decision can make:



model
input
output




gpt-4-1106-preview
$0.01 / 1k tokens
$0.03 / 1k tokens


gpt-3.5-turbo-1106
$0.001 / 1k tokens
$0.002 / 1k tokens



how long have you been using the same thread?
as stated in the official openai documentation:

assistants can access persistent threads. threads simplify ai
application development by storing message history and truncating it
when the conversation gets too long for the modelï¿½ï¿½ï¿½s context length.
you create a thread once, and simply append messages to it as your
users reply.
/ ... /
threads and messages represent a conversation session between an
assistant and a user. there is no limit to the number of messages you
can store in a thread. once the size of the messages exceeds the
context window of the model, the thread will attempt to include as
many messages as possible that fit in the context window and drop the
oldest messages.

the tread is storing the message history! the gpt-4-1106-preview has a context window of 128,000 tokens. so, if you chat with your assistant using the same thread long enough, you will fill up the thread up to the context window of your chosen model.
if you chhe gpt-4-1106-preview this means that after some time chatting with your assistant using the same thread, a single question you ask your assistant means that you used 128,000 tokens. your recent question might contain 1,000 tokens, but you also need to keep in mind that hundreds of messages that were either asked by you or answered by the assistant in the past were also sent to the assistants api.
in your case, you can see that today you spent 760,564 context tokens. you have probably been using the same thread for quite some time.
how often do you check the run status?
you said that you check the run status to see if it has been moved to completed every 5 seconds. try to increase this number, let's say 10 seconds, to make fewer api calls. you pay for every api call you make.",https://stackoverflow.com/questions/77839828,openai-api,18-01-2024 13:39,2072.0,0.0,1.0,True,10-04-2024 10:18,22-01-2024 16:00,Implementation Issues
78919818,embedding using the langchain_aws is giving none value,"i am trying to embed a text using the langchain_aws bedrockembeddings, but when i invoke the function, i get a list with the none values.
here's the code:
from langchain_community.llms.bedrock import bedrock 
from langchain_aws import bedrockembeddings
import boto3

# initialize the bedrock client
bedrock_client = boto3.client(service_name='bedrock-runtime')

# initialize bedrock embeddings
bedrock_embeddings = bedrockembeddings(
    model_id=""amazon.titan-text-express-v1"",
    credentials_profile_name=""default"",
    client=bedrock_client,
    region_name=""ap-south-1""
)


embed_data=bedrock_embeddings.embed_documents([""this is a content of the document"", ""this is another document""])

print(embed_data)


output:
[none, none]","['python', 'langchain', 'large-language-model', 'embedding', 'amazon-bedrock']",78924664,"amazon.titan-text-express-v1 is not embedding model. its a text generation model.
use amazon.titan-embed-text-v2:0",https://stackoverflow.com/questions/78919818,python,27-08-2024 16:05,266.0,0.0,1.0,True,29-08-2024 09:58,29-08-2024 09:58,Implementation Issues
29989464,how do i remove 1 instance of x characters in a string and find the word it makes in python3?,"this is what i have so far, but i'm stuck.  i'm using nltk for the word list and trying to find all the words with the letters in ""sand"".  from this list i want to find all the words i can make from the remaining letters.
import nltk.corpus.words.words()
pwordlist = []

for w in wordlist:
    if 's' in w:
        if 'a' in w:
            if 'n' in w:
                if 'd' in w:
                    pwordlist.append(w)

in this case i have to use all the letters to find the words possible.
i think this will work for finding the possible words with the remaining letters, but i can't figure out how to remove only 1 instance of the letters in 'sand'.
puzzle_letters = nltk.freqdist(x)

[w for w in pwordlist if len(w) = len(pwordlist) and nltk.freqdist(w) = puzzle_letters]","['python', 'python-3.x', 'nltk']",29990395,"i would separate the logic into four sections:

a function contains(word, letters), which we'll use to detect whether a word contains ""sand""
a function subtract(word, letters), which we'll use to remove ""sand"" from the word.
a function get_anagrams(word), which finds all of the anagrams of a word.
the main algorithm that combines all of the above to find words that are anagrams of other words once you remove ""sand"".

ï¿½ï¿½
from collections import counter

words = ??? #todo: somehow get a list of every english word.

def contains(word, letters):
    return not counter(letters) - counter(word)

def subtract(word, letters):
    remaining = counter(word) - counter(letters)
    return """".join(remaining.elements())

anagrams = {}
for word in words:
    base = """".join(sorted(word))
    anagrams.setdefault(base, []).append(word)
def get_anagrams(word):
    retanagrams.get("""".join(sorted(word)), [])

for word in words:
    if contains(word, ""sand""):
        reduced_word = subtract(word, ""sand"")
        matches = get_anagrams(reduced_word)
        if matches:
            print word, matches

running the above code on the words with friends dictionary, i get a lot of results, including:
...
cowhands ['chow']
credentials ['reticle', 'tiercel']
cyanids ['icy']
daftness ['efts', 'fest', 'fets']
dahoons ['oho', 'ooh']
daikons ['koi']
daintiness ['seniti']
daintinesses ['sienites']
dalapons ['opal']
dalesman ['alme', 'lame', 'male', 'meal']
...",https://stackoverflow.com/questions/29989464,python,01-05-2015 15:07,156.0,1.0,3.0,True,26-03-2022 17:48,01-05-2015 16:16,Implementation Issues
79300067,reproducing huggingface clip&#39;s output for the text encoder,"i am trying to de-compose clip's text_model from huggingface but i'm running into some issues i don't understand.
in particular, as far as i understand calling clip.text_model should be the same as:

calculating the text embeddings with clip.text_model.embeddings
feeding the embeddings to clip.text_model.encoder
using clip.text_model.final_layer_norm

but when i try to compare the outputs i get different values for the two approaches.
here is my code so far:
device = ""cuda"" if torch.cuda.is_available() else ""cpu""

model = clipmodel.from_pretrained(""openai/clip-vit-base-patch32"")
model = model.to(device)
processor = clipprocessor.from_pretrained(""openai/clip-vit-base-patch32"")

def decomposed_text_model(text, processor, model, device):
    inputs = processor(text=text, return_tensors=""pt"", padding=true)
    attn_mask = inputs[""attention_mask""].clone().detach().to(torch.bool).to(device)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    embeddings = model.text_model.embeddings(inputs[""input_ids""])
    position_embeddings=model.text_model.embeddings.position_embedding.weight[:inputs['input_ids'].shape[1]]
    embeddings = embeddings + position_embeddings.unsqueeze(0)
    encoder_output = model.text_model.encoder(
        inputs_embeds=embeddings, 
        attention_mask=attn_mask).last_hidden_state

    embeddings = model.text_model.final_layer_norm(encoder_output)

    return embeddings

def text_model(text, processor, model):
  inputs = processor(text=""a photo of a cat"", return_tensors=""pt"")
  inputs = {k: v.to(device) for k, v in inputs.items()}
  return model.text_model(**inputs)

# two step text approach
out1 = decomposed_text_model(""a photo of a cat"", processor, model)

out1 = out1.last_hidden_state[0, -1, :] # get eos token
out1 = out1.squeeze()

# one step text approach
out2 = text_model(""a photo of a cat"", processor, model)
out2 = out2.last_hidden_state[0, -1, :] # get eos token
out2 = out2.squeeze()

# compare
out1 = out1 / out1.norm(p = 2, dim=-1, keepdim=true)
out2 = out2 / out2.norm(p = 2, dim=-1, keepdim=true)

diff = torch.max(torch.abs(out1 - out2))
print(diff)

with diff being a somewhat high number (more fine-grained logging also revealed significant differences between the two eos tensors).
what am i missing? please understand that this approach is necessary to implement something, so i cannot just call text_model.","['python-3.x', 'huggingface-transformers', 'huggingface', 'clip']",79422353,"there are a few errors in your code, but the most important step you forgot to reproduce the logic of the clip text model is the use of the 4d-causualmask. you can find the relevant code here and your code should look as follows:
import torch
from transformers import clipmodel, cliptokenizerfast
from transformers.modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask 

device = ""cuda"" if torch.cuda.is_available() else ""cpu""

m = clipmodel.from_pretrained(""openai/clip-vit-base-patch32"")
m = m.to(device)
t = cliptokenizerfast.from_pretrained(""openai/clip-vit-base-patch32"")

text = ""a photo of a cat""
inputs = t(text, return_tensors=""pt"")
inputs.to(device)

@torch.no_grad()
def decomposed_text_model(inputs, model):    
    input_ids = inputs['input_ids']
    attention_mask = inputs['attention_mask']
    input_shape = input_ids.size()
    hidden_states = model.text_model.embeddings(input_ids=input_ids)
    
    causal_attention_mask = _create_4d_causal_attention_mask(
        input_shape, hidden_states.dtype, device=hidden_states.device
    )
    attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)
    
    encoder_output = model.text_model.encoder(
        inputs_embeds=hidden_states, 
        attention_mask=attention_mask,
        causal_attention_mask=causal_attention_mask
        ).last_hidden_state

    embeddings = model.text_model.final_layer_norm(encoder_output)

    return embeddings

@torch.no_grad()
def text_model(inputs, model):
  return model.text_model(**inputs)

# two step text approach
out1 = decomposed_text_model(inputs, m)

out1 = out1[0, -1, :] # get eos token
out1 = out1.squeeze()

# one step text approach
out2 = text_model(inputs, m)
out2 = out2.last_hidden_state[0, -1, :] # get eos token
out2 = out2.squeeze()

# compare
print(torch.allclose(out1, out2))

output:
true",https://stackoverflow.com/questions/79300067,python-3.x,21-12-2024 20:36,89.0,2.0,1.0,True,07-02-2025 23:15,07-02-2025 23:10,Implementation Issues
77811059,openai api error: &quot;nameerror: name &#39;client&#39; is not defined&quot;,"this is my code:
import telebot
import openai
 
bot = telebot.telebot(""0"")
openai.api_key = ""0""
 
@bot.message_handler(content_types=['text'])
def lalala(message):
    print(message.chat.title, message.chat.username)
    if message.chat.id == -1002097745017:
    #print(message.text)
        if ""@0"" in message.text:
            message.text = (message.text).replace(""@0 "", """")
            #print(message.text)
            response = client.completions.create(model=""gpt-3.5-turbo-0613"", prompt=message.text, max_tokens=1000)
            full_response = response['choices'][0]['text']  # use the text property of the first element of the choices list to access the full response
            lines = full_response.splitlines()  # split the response into individual lines
            for line in lines:  # iterate over the lines
                try:
                    #print(line)
                    bot.send_message(message.chat.id, line)  # send each line back to the user as a separate message
                except exception as e:
                    print(e)
    else:
        bot.send_message(message.chat.id, ""work only - tg.com/123123"")
 
bot.polling(none_stop=true, interval=0)

i'm getting the following error:

nameerror: name 'client' is not defined

if i change this...
response = client.completions.create(model=""gpt-3.5-turbo-0613"", prompt=message.text, max_tokens=1000)

...to this.
response = openai.completion.create(model=""text-davinci-003"", prompt=message.text, max_tokens=1000)

then i'm getting the following error:

this is a chat model and not supported in the v1/completions endpoint. did you mean to use v1/chat/completions?","['python', 'telegram', 'openai-api', 'gpt-3']",77811071,"you tried multiple combinations, but not the right one.

use openai not client, because you didn't initialize openai with client, but with openai.
use completions.create method name, not completion.create if you're using openai python sdk version >=1.0.0.
use the completions model, not the chat completions model. while the text-davinci-003 is a completions model, it will not work because it was deprecated not long ago. use gpt-3.5-turbo-instruct instead, which is the recommended replacement for the text-davinci-003.

the following should work:
response = openai.completions.create(model=""gpt-3.5-turbo-instruct"", prompt=message.text, max_tokens=1000)",https://stackoverflow.com/questions/77811059,python,13-01-2024 10:24,3655.0,0.0,1.0,True,14-01-2024 11:32,14-01-2024 11:32,Conceptual Questions
65779837,importerror caused by file with the same name in working dir and file from imported package,"i've bumped into an issue when trying to run a python script and for simplicity let's call it my_tokenizer.py and its content is just importing hugging face's transformers. unfortunately, trying to run it from the working directory leads to importerror and it seems it is caused due to the name of the file that is in the working directory and has the same name as the file that transformer package uses somewhere in its internals.
having 2 files in the working directory:

/project/my_tokenizer.py (contains only line with import ""import transformers"")
/project/tokenizers.py (empty file)

and running python my_tokenizer.py leads to following importerror:
traceback (most recent call last):
  file ""project/my_tokenizer.py"", line 1, in <module>
    import transformers
  file ""/users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/__init__.py"", line 54, in <module>
    from .data import (
  file ""/users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/data/__init__.py"", line 6, in <module>
    from .processors import (
  file ""/users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/data/processors/__init__.py"", line 5, in <module>
    from .glue import glue_convert_examples_to_features, glue_output_modes, glue_processors, glue_tasks_num_labels
  file ""/users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/data/processors/glue.py"", line 24, in <module>
    from ...tokenization_utils import pretrainedtokenizer
  file ""/users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/tokenization_utils.py"", line 26, in <module>
    from .tokenization_utils_base import (
  file ""/users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/tokenization_utils_base.py"", line 31, in <module>
    from tokenizers import addedtoken
importerror: cannot import name 'addedtoken' from 'tokenizers' (/users/radoslawslowinski/project/tokenizers.py)


although i could just rename my file from project/tokenizers.py to something else, i'd like to know why it occurs.","['python', 'python-3.x', 'huggingface-transformers', 'huggingface-tokenizers']",65787421,"i think i've understood what causes the issue - it is shadowing file with the same name in package transformer (that internally import another package called tokenizers) with my local file called tokenizers.py.
it is so because my working directory is first on list of paths that will be searched to find imports. it can be checked with:
import sys
print(sys.path)
from transformers import basictokenizer

and to proof that search for imports starts from directory in which you call script, you can move first sys.path to the end of the list and the following code will work:
import sys
sys.path = sys.path[1:] + sys.path[:1]
import transformers",https://stackoverflow.com/questions/65779837,python,18-01-2021 18:05,593.0,1.0,2.0,True,04-01-2025 12:02,19-01-2021 08:21,Implementation Issues
76357844,how to extract subtitles from youtube videos in varied languages,"i have used the code below to extract subtitles from youtube videos, but it only works for videos in english. i have some videos in spanish, so i would like to know how i can modify the code to extract spanish subtitles too?
from pytube import youtube
from youtube_transcript_api import youtubetranscriptapi

# define the video url or id of the youtube video you want to extract text from
video_url = '

# download the video using pytube
youtube = youtube(video_url)
video = youtube.streams.get_highest_resolution()
video.download()

# get the downloaded video file path
video_path = video.default_filename

# get the video id from the url
video_id = video_url.split('v=')[-1]

# get the transcript for the specified video id
transcript = youtubetranscriptapi.get_transcript(video_id)

# extract the text from the transcript
captions_text = ''
for segment in transcript:
    caption = segment['text']
    captions_text += caption + ' '

# print the extracted text
print(captions_text)","['python', 'web-scraping', 'nlp', 'youtube', 'video-streaming']",76358528,"use - list_transcripts - for get the list of available languages:
example:
video_id = 'xygoniso-ky'
transcript_list = youtubetranscriptapi.list_transcripts(video_id)

then, loop the transcript_list variable to see the available languages obtained:
example:
for x, tr in enumerate(transcript_list):
  print(tr.language_code)

in this case, the result is:

es

modify your code for loop the languages available on the video and download the generated captions:
example:
# variables for store the downloaded captions:
all_captions = []
caption = none
captions_text = ''

# loop all languages available for this video and download the generated captions:
for x, tr in enumerate(transcript_list):
  print(""downloading captions in "" + tr.language + ""..."")
  transcript_obtained_in_language = transcript_list.find_transcript([tr.language_code]).fetch()
  for segment in transcript_obtained_in_language:
    caption = segment['text']
    captions_text += caption + ' '
  all_captions.append({""language "" : tr.language_code + "" - "" + tr.language, ""captions"" : captions_text})
  caption = none
  captions_text = ''
  print(""=""*20)
print(""done"")

in the all_captions variable, will be stored the captions and the language obtained from the given video_id.",https://stackoverflow.com/questions/76357844,python,29-05-2023 13:35,1188.0,2.0,2.0,True,01-09-2024 14:48,30-05-2023 05:47,Implementation Issues
71923159,how to get output_attentions of a pretrained distilbert model?,"i am using a pretrained distilbert model:
from transformers import tfdistilbertmodel,distilbertconfig

dbert = 'distilbert-base-uncased'

config = distilbertconfig(max_position_embeddings=256 , dropout=0.2, 
                          attention_dropout=0.2, 
                          output_hidden_states=true,
                          output_attentions=true) #or true

dbert_model = tfdistilbertmodel.from_pretrained(dbert, config)

input_ids_in = tf.keras.layers.input(shape=(256,), name='input_id', dtype='int32')
input_masks_in = tf.keras.layers.input(shape=(256,), name='attn_mask', dtype='int32') 

outputs = dbert_model([input_ids_in, input_masks_in], output_attentions = 1)

i am trying to get the output_attentions. but the output is of length 1 and is given as:

tfbasemodeloutput([('last_hidden_state',
<kerastensor: shape=(none, 256, 768) dtype=float32 (created by layer 'tf_distil_bert_model_6')>)])

i have given ""output_attentions = true"" in config and in forward pass ""output_attentions = 1"" is specified. can anyone let me know what i am doing wrong?
edit:
i have changed the default configuration value of max_positional_embeddings  of 512 to 256. if i change my model instantiation to
dbert_model = tfdistilbertmodel.from_pretrained('distilbert-base-uncased',config=config)

it gives me the following error.
valueerror: cannot reshape array of size 393216 into shape (256,768)

768*512 being 393216. so it might be related to config code.
any ideas?","['python', 'tensorflow', 'tf.keras', 'huggingface-transformers', 'distilbert']",71949300,"i am posting the answer as @cronoik suggested: i modified the code as  dbert_model = tfdistilbertmodel.from_pretrained('distilbert-base-uncased',config, output_attentions=true) this gave both hidden states and attention in output.",https://stackoverflow.com/questions/71923159,python,19-04-2022 10:06,1127.0,2.0,1.0,True,21-04-2022 05:20,20-04-2022 07:05,Implementation Issues
62386631,cannot import bertmodel from transformers,"i am trying to import bertmodel from transformers, but it fails. this is code i am using
from transformers import bertmodel, bertformaskedlm

this is the error i get
importerror: cannot import name 'bertmodel' from 'transformers'

can anyone help me fix this?","['python', 'nlp', 'pytorch', 'huggingface-transformers', 'bert-language-model']",62386694,"fixed the error. this is the code
from transformers.modeling_bert import bertmodel, bertformaskedlm",https://stackoverflow.com/questions/62386631,python,15-06-2020 10:47,22809.0,4.0,4.0,True,26-04-2023 13:48,15-06-2020 12:44,Implementation Issues
77540512,how to extract calculations using tf-idf,"i used tfidfvectorizer to extract tf-idf but don't know how it calculates the results. when i calculate it manually, i get a different answer, so i want to extract the values ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½that the function calculates in order to learn how it works.
data = ['souvenir shop|architecture and art|culture and history', 'souvenir shop|resort|diverse cuisine|fishing|folk games|beautiful scenery', 'diverse cuisine|resort|beautiful scenery']

vectorizer = tfidfvectorizer()
tfidf_matrix = vectorizer.fit_transform(data)
</","['python', 'tf-idf', 'tfidfvectorizer']",77541207,"have a look in the scikit documentation at the attributes section.
try this:
print(vectorizer.vocabulary_)

output
{'souvenir': 14,
 'shop': 13,
 'architecture': 1,
 'and': 0,
 'art': 2,
 'culture': 5,
 'history': 10,
 'resort': 11,
 'diverse': 6,
 'cuisine': 4,
 'fishing': 7,
 'folk': 8,
 'games': 9,
 'beautiful': 3,
 'scenery': 12}

you get the idf calculations with print(vectorizer.idf_)
output
array([1.69314718, 1.69314718, 1.69314718, 1.28768207, 1.28768207,
       1.69314718, 1.28768207, 1.69314718, 1.69314718, 1.69314718,
       1.69314718, 1.28768207, 1.28768207, 1.28768207, 1.28768207])

for your case you can do this (with pandas):
df_idf = pd.dataframe(
    vectorizer.idf_, index=vectorizer.get_feature_names_out(), columns=[""idf_weights""]
)

display(df_idf)

output
             idf_weights
and          1.693147
architecture 1.693147
art          1.693147
beautiful    1.287682
cuisine      1.287682
culture      1.693147
diverse      1.287682
fishing      1.693147
folk         1.693147
games        1.693147
history      1.693147
resort       1.287682
scenery      1.287682
shop         1.287682
souvenir     1.287682",https://stackoverflow.com/questions/77540512,python,24-11-2023 03:02,97.0,0.0,1.0,True,24-11-2023 19:37,24-11-2023 19:37,Implementation Issues
75888473,equivalent of apache lucene &quot;proximity searches&quot; in r,"i'm working on a corpus of documents (clinical narratives from hospital stays), mainly using the quanteda package.
the objective is to be able to classify documents based on the presence/absence of a feature, let's say ""spastic cough"".
i would like to be able to reproduce the behaviour of an apache lucene ""proximity search"" ( using r.
let's take an example:
""spastic and productive cough in a 91-year-old patient following femoral neck surgery""
i would begin tokenizing the phrase as follows:
toks = 
tokens(
c(text1 = ""spastic and productive cough in a 91-year-old patient following femoral neck surgery""), 
remove_punct = t, remove_symbols = t, remove_numbers = t, padding = t
) %>% 
tokens_remove(pattern = stopwords(""en"",source = ""nltk""))

which yields the following output:
tokens consisting of 1 document.
text1 :
[1] ""spastic""     ""productive""  ""cough""       ""91-year-old"" ""patient""     ""following""   ""femoral""    
[8] ""neck""        ""surgery"" 

i can then proceed to generate n-grams and skip-grams:
toks = tokens_ngrams(toks,n=4,skip = 0:3)

toks
[1] ""spastic_productive_cough_91-year-old""     ""spastic_productive_cough_patient""        
  [3] ""spastic_productive_cough_following""       ""spastic_productive_cough_femoral""        
  [5] ""spastic_productive_91-year-old_patient""   ""spastic_productive_91-year-old_following""
  [7] ""spastic_productive_91-year-old_femoral""   ""spastic_productive_91-year-old_neck""     
  [9] ""spastic_productive_patient_following""     ""spastic_productive_patient_femoral""      
 [11] ""spastic_productive_patient_neck""          ""spastic_productive_patient_surgery""      
 [13] ""spastic_productive_following_femoral""     ""spastic_productive_following_neck""       
 [15] ""spastic_productive_following_surgery""     ""spastic_cough_91-year-old_patient""       
 [17] ""spastic_cough_91-year-old_following""      ""spastic_cough_91-year-old_femoral""       
 [19] ""spastic_cough_91-year-old_neck""           ""spastic_cough_patient_following""         
 [21] ""spastic_cough_patient_femoral""            ""spastic_cough_patient_neck""              
 [23] ""spastic_cough_patient_surgery""            ""spastic_cough_following_femoral""         
 [25] ""spastic_cough_following_neck""             ""spastic_cough_following_surgery""         
 [27] ""spastic_cough_femoral_neck""               ""spastic_cough_femoral_surgery""           
 [29] ""spastic_91-year-old_patient_following""    ""spastic_91-year-old_patient_femoral""     
 [31] ""spastic_91-year-old_patient_neck""         ""spastic_91-year-old_patient_surgery""     
.........

at this point i guess i could simply:
any(str_detect(as.character(toks),""spastic_cough""))
[1] true

but i'm not sure i'm using the correct approach as it feels clunky compared to how a lucene query would work. if i were trying to identify patients with ""spastic cough"" using apache lucene to query the corpus i may use something like ""spastic cough""~3 where ""~3"" means that any skip-gram 0:3 would match.
any input about how and where i could improve my method?
edit:
this may do the trick: 
but, at the moment, i can't figure out how to include it in the workflow.
edit 2:
it seems like i can query the corpus using subset_query using a lucene like syntax. the big problem i'm facing now is that ""corpustools"" isn't accepting as input tokens object and the function tokens_to_corpus() isn't working for me. this prevents me from being able to control the tokenization process","['r', 'nlp', 'lucene', 'bioinformatics', 'quanteda']",75919543,"actually, after delving deeper into the documentation, the ""corpustools"" package offers all i need for an apache lucene like experience in r =)",https://stackoverflow.com/questions/75888473,r,30-03-2023 12:46,80.0,0.0,1.0,True,03-04-2023 12:34,30-03-2023 14:31,Uncategorized
60492839,how to compare sentence similarities using embeddings from bert,"i am using the huggingface transformers package to access pretrained models. as my use case needs functionality for both english and arabic, i am using the bert-base-multilingual-cased pretrained model. i need to be able to compare the similarity of sentences using something such as cosine similarity. to use  this, i first need to get an embedding vector for each sentence, and can then compute the cosine similarity.
firstly, what is the best way to extratc the semantic embedding from the bert model? would taking the last hidden state of the model after being fed the sentence suffice?
import torch
from transformers import bertmodel, berttokenizer

model_class = bertmodel
tokenizer_class = berttokenizer
pretrained_weights = 'bert-base-multilingual-cased'

tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)

sentence = 'this is a test sentence'

input_ids = torch.tensor([tokenizer.encode(sentence, add_special_tokens=true)])
with torch.no_grad():
    output_tuple = model(input_ids)
    last_hidden_states = output_tuple[0]

print(last_hidden_states.size(), last_hidden_states)

secondly, if this is a sufficient way to get embeddings from my sentence, i now have another problem where the embedding vectors have different lengths depending on the length of the original sentence. the shapes output are [1, n, vocab_size], where n can have any value. 
in order to compute two vectors' cosine similarity, they need to be the same  length. how can i do this here? could something as naive as first summing across axis=1 still work? what other options do i have?","['python', 'vector', 'nlp', 'cosine-similarity', 'huggingface-transformers']",60493083,"you can use the [cls] token as a representation for the entire sequence. this token is typically prepended to your sentence during the preprocessing step. this token that is typically used for classification tasks (see figure 2 and paragraph 3.2 in the bert paper).
it is the very first token of the embedding.
alternatively you can take the average vector of the sequence (like you say over the first(?) axis), which can yield better results according to the huggingface documentation (3rd tip).
note that bert was not designed for sentence similarity using the cosine distance, though in my experience it does yield decent results.",https://stackoverflow.com/questions/60492839,python,02-03-2020 16:20,30439.0,30.0,5.0,True,25-01-2023 07:38,02-03-2020 16:25,Implementation Issues
15388831,what are all possible pos tags of nltk?,how do i find a list with all possible pos tags used by the natural language toolkit (nltk)?,"['python', 'nltk']",15389153,"the book has a note how to find help on tag sets, e.g.:
nltk.help.upenn_tagset()

others are probably similar. (note: maybe you first have to download tagsets from the download helper's models section for this)",https://stackoverflow.com/questions/15388831,python,13-03-2013 14:59,163448.0,202.0,9.0,True,24-05-2023 12:32,24-05-2023 12:30,Implementation Issues
41879957,stanford corenlp and emoji?,"so far when i tried to use emoji and using the pos tagger, it appeared as unknown symbols, small boxes. is there a way to get the pos tagger to work with emoji? emoji (eg ï¿½ï¿½ï¿½ï¿½) the unicode versio","['java', 'nlp', 'stanford-nlp']",41885643,"provided the character encoding is correct throughout your code, system and the stanford corenlp code, emoji should be represented correctly. however, you'll have two more fundamental problems:
first, emoji are one character long and they are unlikely to be tagged as anything other than an indefinite article. 'a' in english. a smart tokenizer might make better sense of emoji, but i doubt it.
secondly, and more importantly, pos taggers annotate parts of speech. emoji are not a part of speech. in the very least, they are an independent, new class of tokens, but certainly not grammatical.
all that said ... you know their character codes ... they're already tagged.",https://stackoverflow.com/questions/41879957,java,26-01-2017 17:59,693.0,1.0,2.0,True,18-09-2023 21:16,18-09-2023 21:16,Implementation Issues
68417246,"affinity propagation did not converge, this model will not have any cluster centers","when i try to cluster using affinity propagation, the below error occurs and the number of clusters is one.
""...\anaconda\lib\site-packages\sklearn\cluster\_affinity_propagation.py:246: convergencewarning: affinity propagation did not converge, this model will not have any cluster centers.
  warnings.warn(""affinity propagation did not converge, this model """"

below is the code i tried.
def build_feature_matrix(documents, feature_type='frequency',
                         ngram_range=(1, 1), min_df=0.0, max_df=1.0):

    feature_type = feature_type.lower().strip()  
    
    if feature_type == 'binary':
        vectorizer = countvectorizer(binary=true, min_df=min_df,
                                     max_df=max_df, ngram_range=ngram_range)
    elif feature_type == 'frequency':
        vectorizer = countvectorizer(binary=false, min_df=min_df,
                                     max_df=max_df, ngram_range=ngram_range)
    elif feature_type == 'tfidf':
        vectorizer = tfidfvectorizer(min_df=min_df, max_df=max_df, 
                                     ngram_range=ngram_range)
    else:
        raise exception(""wrong feature type entered. possible values: 'binary', 'frequency', 'tfidf'"")

    feature_matrix = vectorizer.fit_transform(documents).astype(float)
    
    return vectorizer, feature_matrix

vectorizer, feature_matrix = build_feature_matrix(filtered_list_6,
                                                  feature_type='tfidf',
                                                  min_df=0.15, max_df=0.85,
                                                  ngram_range=(1, 2))

def affinity_propagation(feature_matrix):
    
    sim = feature_matrix * feature_matrix.t
    sim = sim.todense()
    ap = affinitypropagation()
    ap.fit(sim)
    clusters = ap.labels_          
    return ap, clusters

ap_obj, clusters = affinity_propagation(feature_matrix=feature_matrix)
df[len(df.columns)] = clusters

c = counter(clusters)   
print(c.items())

total_clusters = len(c)
print('total clusters:', total_clusters)

could someone point what i am doing wrong here?
thanks in advance!","['python', 'text', 'nlp', 'data-science']",70085670,"i could change the damping value, max_iter and preference values to eliminate the issue. initially you can start with damping = 0.9, max_iter = 1000.
you can change the preference value as needed and this will change the number of clusters generated by the model",https://stackoverflow.com/questions/68417246,python,17-07-2021 03:50,3471.0,0.0,3.0,True,15-02-2023 16:29,18-07-2021 07:39,Uncategorized
66441952,tokenize list of strings without comma separation,"i'm still new to python and want to know how i can tokenize a list of strings without every word being separated by a comma.
for example, starting from a list like ['i have to get groceries.','i need some bananas.','anything else?'], i want to obtain a list like this: ['i have to get groceries .', 'i need some bananas .', 'anything else ?']. the point is thus not to create a list with separate tokens necessarily, but to create a list with sentences in which all words and punctuation marks are separated from each other.
any ideas? i only managed to create a list of comma-separated tokens, using this code:
import nltk
nltk.download('punkt')
from nltk import word_tokenize 
tokenized = []
for line in unique:
      tokenized.append(word_tokenize(line))","['python', 'nlp', 'tokenize']",66442075,"you can join the tokenized lines with a space, just use
from nltk import word_tokenize
unique = ['i have to get groceries.','i need some bananas.','anything else?']
tokenized = ["" "".join(word_tokenize(line)) for line in unique]
print(tokenized)
# => ['i have to get groceries .', 'i need some bananas .', 'anything else ?']",https://stackoverflow.com/questions/66441952,python,02-03-2021 15:09,363.0,1.0,1.0,True,02-05-2023 16:50,02-05-2023 16:50,Implementation Issues
76213873,how to finetune a zero-shot model for text classification,"i need a model that is able to classify text for an unknown number of classes (i.e. the number might grow over time). the entailment approach for zero-shot text classification seems to be the solution to my problem, the model i tried facebook/bart-large-mnli doesn't perform well on my annotated data. is there a way to fine-tune it without losing the robustness of the model?
my dataset looks like this:
# 
world, ""afghan army dispatched to calm violence kabul, afghanistan - government troops intervened in afghanistan's latest outbreak of deadly fighting between warlords, flying from the capital to the far west on u.s. and nato airplanes to retake an air base contested in the violence, officials said sunday...""
sports, ""johnson helps d-backs end nine-game slide (ap) ap - randy johnson took a four-hitter into the ninth inning to help the arizona diamondbacks end a nine-game losing streak sunday, beating steve trachsel and the new york mets 2-0."" 
business, ""retailers vie for back-to-school buyers (reuters) reuters - apparel retailers are hoping their\back-to-school fashions will make the grade among\style-conscious teens and young adults this fall, but it could\be a tough sell, with students and parents keeping a tighter\hold on their wallets.""

p.s.: this is an artificial question that was created because this topic came up in the comment section of this post which is related to this post.","['python', 'nlp', 'huggingface-transformers']",76213874,"concept explanation
before i answer your question, it is crucial to understand how the entailment approach for zero-shot text classification works. this approach requires a model that was trained for nli, which means, that it is able to determine if the hypothesis is:

supported,
not supported,
undetermined

by a given premise [1]. you can verify that for the model you mentioned with the following code:
from transformers import automodelforsequenceclassification, autotokenizer
nli_model = automodelforsequenceclassification.from_pretrained('facebook/bart-large-mnli')
# it will output three logits
print(nli_model.classification_head.out_proj)
# each vector corresponds to the following labels
print(nli_model.config.id2label)

output:
linear(in_features=1024, out_features=3, bias=true)
{0: 'contradiction', 1: 'neutral', 2: 'entailment'}

the entailment approach, proposed by yin et. al, utilizes these nli capabilities by using the text as premise and formulating a hypothesis for each possible class with the template:
""the text is about {}ï¿½ï¿½ï¿½

that means when you have a text and three potential classes, you will pass three sequences to the nli model and compare the entailment logits to classify the text.
finetuning
to fine-tune an nli model on your annotated data, you, therefore, need to formulate your text classification task as an nli task! that means, you need to generate premises and the labels need to be either contradiction or entailment. the contradiction label is included to avoid the model only seeing hypotheses that are entailed by their respective premise (i.e. the model needs to learn contraction to predict a low score for entailment for the zero-shot text classification task).
the following code shows you an example of how to prepare your dataset:
import random
from datasets import load_dataset
from transformers import  autotokenizer

your_dataset = load_dataset(""ag_news"", split=""test"")
id2labels = [""world"", ""sports"", ""business"", ""sci/tech""]
your_dataset = your_dataset.map(lambda x: {""class"": id2labels[x[""label""]]}, remove_columns=[""label""])

print(your_dataset[0])

# the relevant code
t = autotokenizer.from_pretrained('facebook/bart-large-mnli')
template = ""this example is {}.""

def create_input_sequence(sample):
  text = sample[""text""]
  label = sample[""class""][0]
  contradiction_label = random.choice([x for x in id2labels if x!=label])

  encoded_sequence = t(text*2, [template.format(label), template.format(contradiction_label)])
  encoded_sequence[""labels""] = [2,0]
  encoded_sequence[""input_sentence""] = t.batch_decode(encoded_sequence.input_ids)

  return encoded_sequence

train_dataset = your_dataset.map(create_input_sequence, batched=true, batch_size=1, remove_columns=[""class"", ""text""])
print(train_dataset[0])

output:
{'text': ""fears for t n pension after talks unions representing workers at turner   newall say they are 'disappointed' after talks with stricken parent firm federal mogul."", 
'class': 'business'}

{'input_ids': [0, 597, 12541, 13, 255, 234, 4931, 71, 1431, 1890, 2485, 4561, 1138, 23, 6980, 1437, 1437, 188, 1250, 224, 51, 32, 128, 7779, 19051, 108, 71, 1431, 19, 35876, 4095, 933, 1853, 18059, 922, 4, 2, 2, 713, 1246, 16, 2090, 4, 2], 
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
'labels': 2, 
'input_sentence': ""<s>fears for t n pension after talks unions representing workers at turner   newall say they are 'disappointed' after talks with stricken parent firm federal mogul.</s></s>this example is business.</s>""}

robustness
finetuning will obviously reduce the robustness (i.e. the ability to provide decent results for classes that weren't part of your fine-tuning dataset) of your model. to avoid that you could try:

to stop training before conversion and check if the performance is still sufficient for your needs.
wise-ft proposed by wortsmann et. al. pseudocode is shown in appendix a.",https://stackoverflow.com/questions/76213873,python,09-05-2023 23:01,6294.0,2.0,1.0,True,09-05-2023 23:06,09-05-2023 23:06,Implementation Issues
45170093,latent dirichlet allocation with prior topic words,"context
i'm trying to extract topics from a set of texts using latent dirichlet allocation from scikit-learn's decomposition module.
this works really well, except for the quality of topic words found/selected.
in a article by li et al (2017), the authors describe using prior topic words as input for the lda. they manually choose 4 topics and the main words associated/belonging to these topics. for these words they set the default value to a high number for the associated topic and 0 for the other topics. all other words (not manually selected for a topic) are given equal values for all topics (1). this matrix of values is used as input for the lda.
my question
how can i create a similar analysis with the latentdirichletallocation module from scikit-learn using a customized default values matrix (prior topics words) as input?
(i know there's a topic_word_prior parameter, but it only takes one float instead of a matrix with different 'default values'.)","['python', 'scikit-learn', 'nlp', 'topic-modeling']",74941735,"using anis' help, i created a subclass of the original module, and edited the function that sets the starting values matrix. for all prior topic words you wish to give as input, it transforms the components_ matrix by multiplying the values with the topic values of that (prior) word.
this is the code:
# list with prior topic words as tuples
# (word index, [topic values])
prior_topic_words = []

# example (word at index 3000 belongs to topic with index 0)
prior_topic_words.append(
    (3000, [(np.finfo(np.float64).max/4),0.,0.,0.,0.])
)

# custom subclass for ptw-guided lda
from sklearn.utils import check_random_state
from sklearn.decomposition._online_lda import _dirichlet_expectation_2d
class ptwguidedlatentdirichletallocation(latentdirichletallocation):

    def __init__(self, n_components=10, doc_topic_prior=none, topic_word_prior=none, learning_method=ï¿½ï¿½ï¿½batchï¿½ï¿½ï¿½, learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1, total_s00.0, perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=none, verbose=0, random_state=none, n_topics=none, ptws=none):
        super(ptwguidedlatentdirichletallocation, self).__init__(n_components, doc_topic_prior, topic_word_prior, learning_method, learning_decay, learning_offset, max_iter, batch_size, evaluate_every, total_samples, perp_tol, mean_change_tol, max_doc_update_iter, n_jobs, verbose, random_state, n_topics)
        self.ptws = ptws

    def _init_latent_vars(self, n_features):
        """"""initialize latent variables.""""""

        self.random_state_ = check_random_state(self.random_state)
        self.n_batch_iter_ = 1
        self.n_iter_ = 0

        if self.doc_topic_prior is none:
            self.doc_topic_prior_ = 1. / self.n_topics
        else:
            self.doc_topic_prior_ = self.doc_topic_prior

        if self.topic_word_prior is none:
            self.topic_word_prior_ = 1. / self.n_topics
        else:
            self.topic_word_prior_ = self.topic_word_prior

        init_gamma = 100.
        init_var = 1. / init_gamma
        # in the literature, this is called `lambda`
        self.components_ = self.random_state_.gamma(
            init_gamma, init_var, (self.n_topics, n_features))

        # transform topic values in matrix for prior topic words
        if self.ptws is not none:
            for ptw in self.ptws:
                word_index = ptw[0]
                word_topic_values = ptw[1]
                self.components_[:, word_index] *= word_topic_values

        # in the literature, this is `exp(e[log(beta)])`
        self.exp_dirichlet_component_ = np.exp(
            _dirichlet_expectation_2d(self.components_))

initiation is the same as the original latentdirichletallocation class, but now you can provide prior topic words using the ptws parameter.",https://stackoverflow.com/questions/45170093,python,18-07-2017 14:46,2897.0,4.0,2.0,True,28-12-2022 14:55,27-12-2022 12:45,Implementation Issues
79032225,aggregating output from langchain lcel elements,"i have two chains, one that generates a document and one that creates a short document resume. i want to chain them, using the output from the first on inside the other one. but i want to get both outputs in the result.
before lcel, i could do it using llmchain's output_key parameter. with lcel, there seems to be a runnablepassthrough class, but i don't seem to get how to use it to aggregate the output.
code example:
generate_document_chain = generate_document_prompt | llm | stroutputparser()
resume_document_chain = resume_document_prompt | llm | stroutputparser()

aggregated_chain = generate_document_chain | resume_document_chain 
content = aggregated_chain.invoke({""topic"": topic})","['python', 'langchain', 'py-langchain']",79033375,"perhaps the following is what you want. it feeds the output of the first chain into second chain as input.
from langchain_core.runnables import runnablepassthrough

aggregated_chain = generate_document_chain | {
    ""first_chain_output"": runnablepassthrough(),
    ""second_chain_output"": resume_document_chain
}
content = aggregated_chain.invoke({""topic"": topic})

then the output will be a dictionary with keys: ""first_chain_output"" and ""second_chain_output"".
you can also use runnablepassthrough.assign. unlike the case above, the key of generate_document_chain output should match the input variable name of the second chain. below, the input variable of the second chain is  assumed to be ""input"" (btw, the input variable of the first chain is ""topic"").
aggregated_chain = (
    {""input"": generate_document_chain} 
    | runnablepassthrough.assign(second_chain_output=resume_document_chain)
)

the output of this chain will be a dict with keys: ""input"" and ""second_chain_output"".",https://stackoverflow.com/questions/79032225,python,27-09-2024 16:30,124.0,2.0,1.0,True,28-09-2024 15:49,28-09-2024 15:49,Implementation Issues
78216871,integrating llama index vectorstoreindex with langchain agents for rag applications,"i have been reading the documentation all day and can't seem to wrap my head around how i can create a vectorstoreindex with llama_index and use the created embeddings as supplemental information for a rag application/chatbot that can communicate with a user. i want to use llama_index because they have some cool ways to perform more advanced retrieval techniques like sentence window retrieval and auto-merging retrieval (to be fair i have not investigated if langchain also supports these types of vector retrieval methods). i want to use langchain because of its functionality for developing more complex prompt templates (similarly i have not really investigated if llama_index supports this).
my goal is to ultimately evaluate how these different retrieval methods perform within the context of the application/chatbot. i know how to evaluate them with a separate evaluation questions file, but i would like to do things like compare the speed and humanness of responses, token usage, etc.
the code for a minimal reproducible example would be as follows

langchain chatbot initiation

from langchain_core.prompts import chatprompttemplate, messagesplaceholder

from langchain.memory import chatmessagehistory


prompt = chatprompttemplate.from_messages(
    [
        (
            ""system"",
            """"""you are the world's greatest... \
            use this document base to help you provide the best support possible to everyone you engage with. 
            """""",
        ),
        messagesplaceholder(variable_name=""messages""),
    ]
)

chat = chatopenai(model=llm_model, temperature=0.7)



chain = prompt | chat


chat_history = chatmessagehistory()

while true:
    user_input = input(""you: "")
    chat_history.add_user_message(user_input)
    
    response = chain.invoke({""messages"": chat_history.messages})
    
    if user_input.lower() == 'exit':
        break
    
    print(""ai:"", response)
    chat_history.add_ai_message(response)



llama index sentence window retrieval

from llama_index.core.node_parser import sentencewindownodeparser
        from llama_index.core.indices.postprocessor import metadatareplacementpostprocessor
        from llama_index.core.postprocessor import llmrerank
    
class sentencewindowutils:
    def __init__(self, documents, llm, embed_model, sentence_window_size):
        self.documents = documents
        self.llm = llm
        self.embed_model = embed_model
        self.sentence_window_size = sentence_window_size
        # self.save_dir = save_dir

        self.node_parser = sentencewindownodeparser.from_defaults(
            window_size=self.sentence_window_size,
            window_metadata_key=""window"",
            original_text_metadata_key=""original_text"",
        )

        self.sentence_context = servicecontext.from_defaults(
            llm=self.llm,
            embed_model=self.embed_model,
            node_parser=self.node_parser,
        )

    def build_sentence_window_index(self, save_dir):
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
            sentence_index = vectorstoreindex.from_documents(
                self.documents, service_context=self.sentence_context
            )
            sentence_index.storage_context.persist(persist_dir=save_dir)
        else:
            sentence_index = load_index_from_storage(
                storagecontext.from_defaults(persist_dir=save_dir),
                service_context=self.sentence_context,
            )

        return sentence_index

    def get_sentence_window_query_engine(self, sentence_index, similarity_top_k=6, rerank_top_n=3):
        postproc = metadatareplacementpostprocessor(target_metadata_key=""window"")
        rerank = llmrerank(top_n=rerank_top_n, service_context=self.sentence_context)

        sentence_window_engine = sentence_index.as_query_engine(
            similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]
        )

        return sentence_window_engine


sentence_window = sentencewindowutils(documents=documents, llm = llm, embed_model=embed_model, sentence_window_size=1)
sentence_window_1 = sentence_window.build_sentence_window_index(save_dir='./indexes/sentence_window_index_1')
sentence_window_engine_1 = sentence_window.get_sentence_window_query_engine(sentence_window_1)

both blocks of code independently will run. but the goal is that when a query is performed that warrants a retrieval to the existing document base, i can use the sentence_window_engine that was built. i suppose i could retrieve relevant information based on the query and then pass that information into a subsequent prompt for the chatbot, but i would like to try and avoid including the document data in a prompt.
any suggestions?","['python', 'langchain', 'embedding', 'large-language-model', 'llama-index']",78249192,"i never found an exact way to retrieve the information via llama_index like i had hoped but i basically found a workaround by doing what i initially wanted to avoid by querying my document base and adding that as context information to my chatbot as such
#### conversation prompt chain #####
prompt = chatprompttemplate.from_messages(
    [
        (
            ""system"",
            """"""you are the world's greatest...
            you have access to an extensive document base of information.
            relevant information to the user query is provided below. use the information at your own discretion if it improves the quality of the response.
            a summary of the previous conversation is also provided to contextualize you on previous conversation.

            <<relevant information>>
            {relevant_information}


            << previous conversation summary>>
            {previous_conversation}


            << current prompt >>
            {user_input}
            """""",
        ),
        messagesplaceholder(variable_name=""messages""),
    ]
)

chat = chatopenai(model=llm_model, temperature=0.0)



chain = prompt | chat


### application start ###


while true:
    # some code....
    if route['destination'] == ""data querying"":
                formatted_response = query_and_format_sql(username, password, host, port, mydatabase, query_prompt, model = 'gpt-4', client_name = client_name, user_input=user_input)
                print(formatted_response)
                chat_history.add_ai_message(aimessage(f'the previous query triggered a sql agent response that was {formatted_response}'))
        else:
            # search document base
            rag_context = sentence_window_engine_1.query(user_input)
    
            # inject the retrieved information into the chatbot's context
            context_with_relevant_info = {
                ""user_input"": user_input,
                ""messages"": chat_history.messages,
                ""previous_conversation"": memory.load_memory_variables({}),
                ""relevant_information"": rag_context # ==> inject relevant information from llama_index here
            }
            
            response = chain.invoke(context_with_relevant_info)


i haven't ran into a token issue yet but i can imagine if my application grows and scales it may run into problem trying to inject relevant information, the message history, and the prompt. i limit my memory with a conversationbuffermemoryhistory and that seems to work ok for now.",https://stackoverflow.com/questions/78216871,python,25-03-2024 02:50,1175.0,0.0,2.0,True,13-03-2025 22:11,13-03-2025 22:11,Implementation Issues
76067091,gpu out of memory fine tune flan-ul2,"outofmemoryerror: cuda out of memory. tried to allocate 256.00 mib
(gpu 0; 15.78 gib total capacity; 14.99 gib already allocated; 3.50
mib free; 14.99 gib reserved in total by pytorch) if reserved memory
is >> allocated memory try setting max_split_size_mb to avoid
fragmentation.  see documentation for memory management and
pytorch_cuda_alloc_conf

i have standard_nc24s_v3 single node gpu with 448gb memory and 4 gpus. however the error message says the total capacity is 15.78gib. is the fine tune not using 4 gpus? how to get all the 4 gpus used in the fine tune of flan-ul2 using huggingface transformers?","['gpu', 'huggingface-transformers', 'huggingface-tokenizers', 'gpt-3', 'fine-tuning']",76320287,"i solve the issue by using the following package versions.
!pip install transformers==4.28.1
!pip install sentencepiece==0.1.97
!pip install accelerate==0.18.0
!pip install bitsandbytes==0.37.2
!pip install torch==1.13.1",https://stackoverflow.com/questions/76067091,gpu,20-04-2023 18:13,521.0,1.0,1.0,True,24-05-2023 05:23,20-04-2023 18:33,Tool Setup/Errors
71492980,huggingface sagemaker,"i am trying to use the text2text (translation) model facebook/m2m100_418m to run on sagemaker.
so if you click on deploy and then sagemaker there is some boilerplate code that works well but i can't seem to find how to pass it the arguments src_lang=""en"", tgt_lang=""fr"" just like when using the pipeline or transformers.
so right now it translates into random languages.
i'm guessing i should add it in here somehow but it's not documented.
predictor.predict({
    'inputs': ""the answer to the universe is""
})

does anybody have an idea of how to pass arguments to the predict method?
edit
this is the code that was wrong where you will need to change the hf_task:
import sagemaker

role = sagemaker.get_execution_role()
# hub model configuration. 
hub = {
    'hf_model_id':'facebook/m2m100_418m',
    'hf_task':'text2text-generation'
}

# create hugging face model class
huggingface_model = huggingfacemodel(
    transformers_version='4.6.1',
    pytorch_version='1.7.1',
    py_version='py36',
    env=hub,
    role=role, 
)

# deploy model to sagemaker inference
predictor = huggingface_model.deploy(
    initial_instance_count=1, # number of instances
    instance_type='ml.m5.xlarge' # ec2 instance type
)```","['python', 'artificial-intelligence', 'amazon-sagemaker', 'huggingface-transformers', 'huggingface-tokenizers']",71509545,"ok i figured it out.
the task was wrong, the source and target language need to be in the task (hf_task)
so for example:
'hf_task': 'translation_en_to_fr'",https://stackoverflow.com/questions/71492980,python,16-03-2022 07:09,478.0,1.0,3.0,True,17-03-2022 09:04,17-03-2022 09:04,Task-specific Help
77159136,efficiently using hugging face transformers pipelines on gpu with large datasets,"i'm relatively new to python and facing some performance issues while using hugging face transformers for sentiment analysis on a relatively large dataset. i've created a dataframe with 6000 rows of text data in spanish, and i'm applying a sentiment analysis pipeline to each row of text. here's a simplified version of my code:
import pandas as pd
import torch
from tqdm import tqdm
from transformers import pipeline


data = {
    'td': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'text': [
        # ... (your text data here)
    ]
}

df_model = pd.dataframe(data)

device = 0 if torch.cuda.is_available() else -1
py_sentimiento = pipeline(""sentiment-analysis"", model=""finiteautomata/beto-sentiment-analysis"", tokenizer=""finiteautomata/beto-sentiment-analysis"", device=device, truncation=true)

tqdm.pandas()
df_model['py_sentimiento'] = df_model['text'].progress_apply(py_sentimiento)
df_model['py_sentimiento'] = df_model['py_sentimiento'].apply(lambda x: x[0]['label'])

however, i've encountered a warning message that suggests i should use a dataset for more efficient processing. the warning message is as follows:
""you seem to be using the pipelines sequentially on gpu. in order to maximize efficiency please use a dataset.""

i have a two questions:
what does this warning mean, and why should i use a dataset for efficiency?
how can i modify my code to batch my data and use parallel computing to make better use of my gpu resources, what code or function or library should be used with hugging face transformers?
i'm eager to learn and optimize my code.","['python', 'gpu', 'huggingface-transformers', 'huggingface-datasets']",77452808,"i think you can ignore this message. i found it being reported on different websites this year, but if i get it correctly, this github issue on the huggingface transformers ( shows that the warning can be safely ignored. in addition, batching or using datasets might not remove the warning or automatically use the resources in the best way. you can do call_count = 0 in here ( to ignore the warning, as explained by martin weyssow above.
how can i modify my code to batch my data and use parallel computing to make better use of my gpu resources:
you can add batching like this:
py_sentimiento = pipeline(""sentiment-analysis"", model=""finiteautomata/beto-sentiment-analysis"", tokenizer=""finiteautomata/beto-sentiment-analysis"", batch_size=8, device=device, truncation=true)

and most importantly, you can experiment with the batch size that will result to the highest gpu usage possible on your device and particular task.
huggingface provides here some rules to help users figure out how to batch:  making the best resource/gpu usage possible might take some experimentation and it depends on the use case you work on every time.
what does this warning mean, and why should i use a dataset for efficiency?
this means the gpu utilization is not optimal, because the data is not grouped together and it is thus not processed efficiently. using a dataset from the huggingface library datasets will utilize your resources more efficiently.
however, it is not so easy to tell what exactly is going on, especially considering that we donï¿½ï¿½ï¿½t know exactly how the data looks like, what the device is and how the model deals with the data internally. the warning might go away by using the datasets library, but that does not necessarily mean that the resources are optimally used.
what code or function or library should be used with hugging face transformers?
here is a code example with pipelines and the datasets library:  it mentions that using iterables will fill your gpu as fast as possible and batching might also help with computational time improvements.
in your case it seems you are doing a relatively small poc (doing inference for under 10,000 documents with a medium size model), so i donï¿½ï¿½ï¿½t think you need to use pipelines. i assume the sentiment analysis model is a classifier and you want to keep using pandas as shown in the post, so here is how you can combine both. this is usually fast enough for my experiments and prints no warnings about the resources.
from transformers import autotokenizer, automodelforsequenceclassification
import torch as t
import pandas as pd
        
model = automodelforsequenceclassification.from_pretrained(""finiteautomata/beto-sentiment-analysis"")
tokenizer = autotokenizer.from_pretrained(""finiteautomata/beto-sentiment-analysis"")
      
def classify_dataframe_row(
    example: pd.series,
):
    output = model(**tokenizer(example[""text""], return_tensors=""pt""))
    prediction = t.argmax(output[0]).detach().numpy()
    return prediction

dataset = pd.read_csv(""file"")
dataset = dataset.assign(
    prediction=dataset.progress_apply(classify_dataframe_row, axis=1)
)

as soon as your inference starts, either with this snippet or with the datasets library code, you can run nvidia-smi in a terminal and check what the gpu usage is and play around with the parameters to optimize it. beware that running the code on your local machine with a gpu vs running it on a larger machine, e.g., a linux server with perhaps a more powerful gpu might lead to different performance and might need different tuning. if you wish to run the code for larger document collections, you can split the data in order to avoid gpu memory errors locally, or in order to speed up the inference with concurrent runs in a server.",https://stackoverflow.com/questions/77159136,python,22-09-2023 15:57,37222.0,18.0,2.0,True,23-01-2024 15:01,08-10-2023 20:35,Implementation Issues
75779489,reading files in a directory and saving each dynamically,"text_open = open(""inputfiles/(22).txt"", ""r"")
text = text_open.read()

doc = nlp(text)
db.add(doc)
db.to_disk(""./outputfiles/22.spacy"")

i am trying to loop over each of the 500+ documents in the inputfiles folder and output them through db.to_disk. instead of changing the hard coded numbers every-time, how would i dynamically rename each new output file to match the input file?
if i were to open the directory with glob/os, how do i then add this to the output directory without overwriting every single file with the hardcoded '22' or creating new 22(1).spacy, 22(2).spacy etc files?
thanks!","['python', 'spacy']",75779504,"if you're using python 3.6+, then use f-strings.
for i in range(1, 501):
    text_open = open(f""inputfiles/({i}).txt"", ""r"")
    text = text_open.read()

    doc = nlp(text)
    db.add(doc)
    db.to_disk(f""./outputfiles/{i}.spacy"")

would it help?",https://stackoverflow.com/questions/75779489,python,19-03-2023 00:39,37.0,0.0,1.0,True,19-03-2023 00:44,19-03-2023 00:39,Implementation Issues
66171956,number of learnable parameters of multiheadattention,"while testing (using pytorch's multiheadattention), i noticed that increasing or decreasing the number of heads of the multi-head attention does not change the total number of learnable parameters of my model.
is this behavior correct? and if so, why?
shouldn't the number of heads affect the number of parameters the model can learn?","['python', 'python-3.x', 'nlp', 'pytorch', 'attention-model']",66176914,"the standard implementation of multi-headed attention divides the model's dimensionality by the number of attention heads.
a model of dimensionality d with a single attention head would project embeddings to a single triplet of d-dimensional query, key and value tensors (each projection counting d2 parameters, excluding biases, for a total of 3d2).
a model of the same dimensionality with k attention heads would project embeddings to k triplets of d/k-dimensional query, key and value tensors (each projection counting dï¿½ï¿½d/k=d2/k parameters, excluding biases, for a total of 3kd2/k=3d2).

references:
from the original paper:

the pytorch implementation you cited:",https://stackoverflow.com/questions/66171956,python,12-02-2021 12:31,2741.0,5.0,1.0,True,04-01-2023 10:48,16-03-2021 09:59,Implementation Issues
71661321,"valueerror: dimensions must be equal, but are 2 and 64 for &#39;{{node binary_crossentropy/mul}} with input shapes[?,2], [?,64]","i'm trying binary classification of text with bi-lstm model but getting this error: valueerror: dimensions must be equal, but are 2 and 64 for '{{node binary_crossentropy/mul}} = mul[t=dt_float](binary_crossentropy/cast, binary_crossentropy/log)' with input shapes: [?,2], [?,64].
i am a beginner please provide some valuable solutions.
text=df['text']
label=df['label']

x = pad_sequences(x, maxlen=max_len,padding=pad_type,truncating=trunc_type)
y = pd.get_dummies(label).values    
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.20)
print(x_train.shape,y_train.shape)
print(x_test.shape,y_test.shape)

#model creation
model=tf.keras.sequential([
 # add an embedding layer
 tf.keras.layers.embedding(word_count, 16, input_length=max_len),
 tf.keras.layers.dropout(0.2),
 # add another bi-lstm layer
 tf.keras.layers.bidirectional(tf.keras.layers.lstm(2,return_sequences=true)),
 # add a dense layer
 tf.keras.layers.dense(32, activation=tf.keras.activations.relu),
 tf.keras.layers.dense(32, activation=tf.keras.activations.relu),
 tf.keras.layers.dense(32, activation=tf.keras.activations.relu),
 tf.keras.layers.dense(32, activation=tf.keras.activations.softmax),
 # add the prediction layer
 tf.keras.layers.dense(1, activation=tf.keras.activations.sigmoid),
])
model.compile(loss=tf.keras.losses.binarycrossentropy(), optimizer=tf.keras.optimizers.adam(), metrics=['accuracy'])
model.summary()
history = model.fit(x_train,  y_train, validation_data=(x_test,  y_test), epochs = 10, batch_size=batch_size, callbacks = [callback_func], verbose=1)","['tensorflow', 'keras', 'nlp', 'tensorflow2.0']",71662595,"the output dimension of the prediction layer of the binary classification should be 2:
# add the prediction layer
tf.keras.layers.dense(2, activation=tf.keras.activations.sigmoid)

flatten:
#model creation
model=tf.keras.sequential([
 # add an embedding layer
 tf.keras.layers.embedding(word_count, 16, input_length=max_len),
 tf.keras.layers.dropout(0.2),
 # add another bi-lstm layer
 tf.keras.layers.bidirectional(tf.keras.layers.lstm(2,return_sequences=true)),
 # add flatten
 tf.keras.layers.flatten(),  #<========================
 # add a dense layer
 tf.keras.layers.dense(32, activation=tf.keras.activations.relu),
 tf.keras.layers.dense(32, activation=tf.keras.activations.relu),
 tf.keras.layers.dense(32, activation=tf.keras.activations.relu),
 tf.keras.layers.dense(32, activation=tf.keras.activations.softmax),
 # add the prediction layer
 tf.keras.layers.dense(2, activation=tf.keras.activations.sigmoid),
])",https://stackoverflow.com/questions/71661321,tensorflow,29-03-2022 11:37,2440.0,1.0,1.0,True,29-03-2022 13:42,29-03-2022 12:02,Task-specific Help
72875686,nlp timer *how do i get the order of certain numbers in a string to use setting a timer?,"import time
import datetime
from nltk_utils import bag_of_words, tokenize
from nltk.tokenize.treebank import treebankworddetokenizer
import multiprocessing
from playsound import playsound


def is_number(x):
    if type(x) == str:
        x = x.replace(',', '')
    try:
        float(x)
    except:
        return false
    return true

def text2int (textnum, numwords={}):
    units = [
        'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight',
        'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen',
        'sixteen', 'seventeen', 'eighteen', 'nineteen',
    ]
    tens = ['', '', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety']
    scales = ['hundred', 'thousand', 'million', 'billion', 'trillion']
    ordinal_words = {'first':1, 'second':2, 'third':3, 'fifth':5, 'eighth':8, 'ninth':9, 'twelfth':12}
    ordinal_endings = [('ieth', 'y'), ('th', '')]

    if not numwords:
        numwords['and'] = (1, 0)
        for idx, word in enumerate(units): numwords[word] = (1, idx)
        for idx, word in enumerate(tens): numwords[word] = (1, idx * 10)
        for idx, word in enumerate(scales): numwords[word] = (10 ** (idx * 3 or 2), 0)

    textnum = textnum.replace('-', ' ')

    current = result = 0
    curstring = ''
    onnumber = false
    lastunit = false
    lastscale = false

    def is_numword(x):
        if is_number(x):
            return true
        if word in numwords:
            return true
        return false

    def from_numword(x):
        if is_number(x):
            scale = 0
            increment = int(x.replace(',', ''))
            return scale, increment
        return numwords[x]

    for word in textnum.split():
        if word in ordinal_words:
            scale, increment = (1, ordinal_words[word])
            current = current * scale + increment
            if scale > 100:
                result += current
                current = 0
            onnumber = true
            lastunit = false
            lastscale = false
        else:
            for ending, replacement in ordinal_endings:
                if word.endswith(ending):
                    word = ""%s%s"" % (word[:-len(ending)], replacement)

            if (not is_numword(word)) or (word == 'and' and not lastscale):
                if onnumber:
                    # flush the current number we are building
                    curstring += repr(result + current) + "" ""
                curstring += word + "" ""
                result = current = 0
                onnumber = false
                lastunit = false
                lastscale = false
            else:
                scale, increment = from_numword(word)
                onnumber = true

                if lastunit and (word not in scales):
                    # assume this is part of a string of individual numbers to
                    # be flushed, such as a zipcode ""one two three four five""
                    curstring += repr(result + current)
                    result = current = 0

                if scale > 1:
                    current = max(1, current)

                current = current * scale + increment
                if scale > 100:
                    result += current
                    current = 0

                lastscale = false
                lastunit = false
                if word in scales:
                    lastscale = true
                elif word in units:
                    lastunit = true

    if onnumber:
        curstring += repr(result + current)

    return curstring


input = ""please will you set a ten seconds timer""
sentence1 = text2int(input)

sentence = tokenize(sentence1)
print(sentence)


if ""hour"" in sentence or 'hours' in sentence:
    remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours', 'please',
              'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"", 'a', 'for',
              'and', 'if', 'privacy', 'time', 'but', 'end', 'put', 'me', 'my', 'will',
              'you', 'now', 'right', 'privacy', 'rite', 'wright', 'write', 'your', 'go', 'ahead', 't']
    remove = set(remove)
    search = set(sentence) - set(remove)
    hours = treebankworddetokenizer().detokenize(search)
    hours1 = (f""{hours}"")
    print(hours1 + ' hours starting now')
else:
    hours1 = '0'

if ""minutes"" in sentence or 'minute' in sentence:
    remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours', 'please',
              'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"", 'a', 'for',
              'and', 'if', 'privacy', 'time', 'but', 'end', 'put', 'me', 'my', 'will',
              'you', 'now', 'right', 'privacy', 'rite', 'wright', 'write', 'your', 'go', 'ahead', 't']
    remove = set(remove)
    search = set(sentence) - set(remove)
    minutes = treebankworddetokenizer().detokenize(search)
    minutes1 = (f""{minutes}"")
    print(minutes + ' minutes starting now')
else:
    minutes1 = '0'

if ""seconds"" in sentence or 'second' in sentence:
    remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours', 'please',
              'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"", 'a', 'for',
              'and', 'if', 'privacy', 'time', 'but', 'end', 'put', 'me', 'my', 'will',
              'you', 'now', 'right', 'privacy', 'rite', 'wright', 'write', 'your', 'go', 'ahead', 't']
    remove = set(remove)
    search = set(sentence) - set(remove)
    seconds = treebankworddetokenizer().detokenize(search)
    seconds1 = (f""{seconds}"")
    print(seconds + ' seconds starting now')
else:
    seconds1 = '0'


def countdown(h, m, s):
    total_seconds = h * 3600 + m * 60 + s
    while total_seconds > 0:
        timer = datetime.timedelta(seconds = total_seconds)

        print(timer, end=""\r"")

        time.sleep(1)
        print(total_seconds)
        total_seconds -= 1

    print('timer ended')



h = hours1
m = minutes1
s = seconds1
countdown(int(h), int(m), int(s))

in this code, you can only ask for a timer of x hours, x minutes, or x seconds but not a combination of them. i want to be able to say set a timer for 10 minutes and 45 seconds and have the system know that the number 10 is associated with minutes and 45 is associated with seconds.
also i am brand new to coding so i know this code probably isn't pretty.","['python', 'nlp', 'stringtokenizer', 'nlu']",73029415,"i figured it out. if you are looking for a way to set a timer with natural language then look no further. i really hope that people improve and critique this but as it sits right now i have a working piece of code. again i am sure there are flaws and things to improve on so if you know any i would be all ears. anyway here is the code.
from nltk.tokenize.treebank import treebankworddetokenizer
from nltk_utils import bag_of_words, tokenize
import time
from datetime import datetime
import datetime
import pyttsx3

try:
    engine = pyttsx3.init()
except importerror:
    print('requested driver not found')
except runtimeerror:
    print('driver fails to initialize')

voices = engine.getproperty('voices')
for voice in voices:
    print(voice.id)
engine.setproperty('voice',
                   'hkey_local_machine\software\microsoft\speech\voices\tokens\tts_ms_en-us_zira_11.0')  # diffrent voices = us_david, gb_hazel, us_zira
rate = engine.getproperty('rate')
engine.setproperty('rate', rate)


def speak_text_cmd(cmd):
    engine.say(cmd)
    engine.runandwait()


def is_number(x):
    if type(x) == str:
        x = x.replace(',', '')
    try:
        float(x)
    except:
        return false
    return true

def text2int (textnum, numwords={}):
    units = [
        'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight',
        'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen',
        'sixteen', 'seventeen', 'eighteen', 'nineteen',
    ]
    tens = ['', '', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety']
    scales = ['hundred', 'thousand', 'million', 'billion', 'trillion']
    ordinal_words = {'first':1, 'third':3, 'fifth':5, 'eighth':8, 'ninth':9, 'twelfth':12}
    ordinal_endings = [('ieth', 'y'), ('th', '')]

    if not numwords:
        numwords['and'] = (1, 0)
        for idx, word in enumerate(units): numwords[word] = (1, idx)
        for idx, word in enumerate(tens): numwords[word] = (1, idx * 10)
        for idx, word in enumerate(scales): numwords[word] = (10 ** (idx * 3 or 2), 0)

    textnum = textnum.replace('-', ' ')

    current = result = 0
    curstring = ''
    onnumber = false
    lastunit = false
    lastscale = false

    def is_numword(x):
        if is_number(x):
            return true
        if word in numwords:
            return true
        return false

    def from_numword(x):
        if is_number(x):
            scale = 0
            increment = int(x.replace(',', ''))
            return scale, increment
        return numwords[x]

    for word in textnum.split():
        if word in ordinal_words:
            scale, increment = (1, ordinal_words[word])
            current = current * scale + increment
            if scale > 100:
                result += current
                current = 0
            onnumber = true
            lastunit = false
            lastscale = false
        else:
            for ending, replacement in ordinal_endings:
                if word.endswith(ending):
                    word = ""%s%s"" % (word[:-len(ending)], replacement)

            if (not is_numword(word)) or (word == 'and' and not lastscale):
                if onnumber:
                    # flush the current number we are building
                    curstring += repr(result + current) + "" ""
                curstring += word + "" ""
                result = current = 0
                onnumber = false
                lastunit = false
                lastscale = false
            else:
                scale, increment = from_numword(word)
                onnumber = true

                if lastunit and (word not in scales):
                    # assume this is part of a string of individual numbers to
                    # be flushed, such as a zipcode ""one two three four five""
                    curstring += repr(result + current)
                    result = current = 0

                if scale > 1:
                    current = max(1, current)

                current = current * scale + increment
                if scale > 100:
                    result += current
                    current = 0

                lastscale = false
                lastunit = false
                if word in scales:
                    lastscale = true
                elif word in units:
                    lastunit = true

    if onnumber:
        curstring += repr(result + current)

    return curstring

numbers = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight',
        'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen',
        'sixteen', 'seventeen', 'eighteen', 'nineteen']




input = search8
sentence = tokenize(input)


if ""timer"" in sentence:
    input = treebankworddetokenizer().detokenize(sentence)
    timerinput1 = text2int(input)

    timerinput = tokenize(timerinput1)
    try:

        if ""hour"" in timerinput or 'hours' in timerinput:
            if ""hour"" in timerinput:
                stringinput1 = input.split('hour', 1)[0]
                stringinput = tokenize(stringinput1)
                remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                          'please',
                          'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                          'a', 'for',
                          'and', 'if', 'time', 'but', 'end', 'put', 'me',
                          'my', 'will',
                          'you', 'now', 'right', 'rite', 'wright',
                          'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time', 'start']
                remove = set(remove)
                search = set(stringinput) - set(remove)
                minutes = treebankworddetokenizer().detokenize(search)
                hours1 = (f""{minutes}"")
            if 'hours' in timerinput:
                stringinput1 = input.split('hours', 1)[0]
                stringinput = tokenize(stringinput1)
                remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                          'please',
                          'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                          'a', 'for',
                          'and', 'if', 'time', 'but', 'end', 'put', 'me',
                          'my', 'will',
                          'you', 'now', 'right', 'rite', 'wright',
                          'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time', 'start']
                remove = set(remove)
                search = set(stringinput) - set(remove)
                minutes = treebankworddetokenizer().detokenize(search)
                hours1 = (f""{minutes}"")
        else:
            hours1 = '0'

        if ""minutes"" in timerinput or 'minute' in timerinput:
            if ""hour"" in timerinput:
                stringinput1 = input.split('hour', 1)[1]
                stringinput = tokenize(stringinput1)
                remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                          'please',
                          'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                          'a', 'for',
                          'and', 'if', 'time', 'but', 'end', 'put', 'me',
                          'my', 'will',
                          'you', 'now', 'right', 'rite', 'wright',
                          'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time', 'start']
                remove = set(remove)
                search = set(stringinput) - set(remove)
                minutes = treebankworddetokenizer().detokenize(search)
                minutes1 = (f""{minutes}"")
            if 'hours' in timerinput:
                stringinput1 = input.split('hours', 1)[1]
                stringinput = tokenize(stringinput1)
                remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                          'please',
                          'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                          'a', 'for',
                          'and', 'if', 'time', 'but', 'end', 'put', 'me',
                          'my', 'will',
                          'you', 'now', 'right', 'rite', 'wright',
                          'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time', 'start']
                remove = set(remove)
                search = set(stringinput) - set(remove)
                minutes = treebankworddetokenizer().detokenize(search)
                minutes1 = (f""{minutes}"")
            else:
                if ""minute"" in timerinput:
                    stringinput1 = input.split('minute', 1)[0]
                    stringinput = tokenize(stringinput1)
                    remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                              'please',
                              'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                              'a', 'for',
                              'and', 'if', 'time', 'but', 'end', 'put', 'me',
                              'my', 'will',
                              'you', 'now', 'right', 'rite', 'wright',
                              'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time',
                              'start']
                    remove = set(remove)
                    search = set(stringinput) - set(remove)
                    minutes = treebankworddetokenizer().detokenize(search)
                    minutes1 = (f""{minutes}"")
                if 'minutes' in timerinput:
                    stringinput1 = input.split('minutes', 1)[0]
                    stringinput = tokenize(stringinput1)
                    remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                              'please',
                              'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                              'a', 'for',
                              'and', 'if', 'time', 'but', 'end', 'put', 'me',
                              'my', 'will',
                              'you', 'now', 'right', 'rite', 'wright',
                              'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time',
                              'start']
                    remove = set(remove)
                    search = set(stringinput) - set(remove)
                    minutes = treebankworddetokenizer().detokenize(search)
                    minutes1 = (f""{minutes}"")

        else:
            minutes1 = '0'

        if ""seconds"" in timerinput or 'second' in timerinput:
            if ""minute"" in timerinput:
                stringinput1 = input.split('minute', 1)[1]
                stringinput = tokenize(stringinput1)
                remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                          'please',
                          'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                          'a', 'for',
                          'and', 'if', 'time', 'but', 'end', 'put', 'me',
                          'my', 'will',
                          'you', 'now', 'right', 'rite', 'wright',
                          'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time',
                          'start']
                remove = set(remove)
                search = set(stringinput) - set(remove)
                seconds = treebankworddetokenizer().detokenize(search)
                seconds1 = (f""{seconds}"")
            if 'minutes' in timerinput:
                stringinput1 = input.split('minutes', 1)[1]
                stringinput = tokenize(stringinput1)
                remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                          'please',
                          'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                          'a', 'for',
                          'and', 'if', 'time', 'but', 'end', 'put', 'me',
                          'my', 'will',
                          'you', 'now', 'right', 'rite', 'wright',
                          'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time',
                          'start']
                remove = set(remove)


                search = set(stringinput) - set(remove)
                seconds = treebankworddetokenizer().detokenize(search)
                seconds1 = (f""{seconds}"")
            else:
                if ""second"" in timerinput or 'seconds' in timerinput:
                    remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                              'please',
                              'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                              'a', 'for',
                              'and', 'if', 'time', 'but', 'end', 'put', 'me',
                              'my', 'will',
                              'you', 'now', 'right', 'rite', 'wright',
                              'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time',
                              'start']
                    remove = set(remove)
                    search = set(timerinput) - set(remove)
                    seconds = treebankworddetokenizer().detokenize(search)
                    seconds1 = (f""{seconds}"")

        else:
            seconds1 = '0'

        if ""minutes"" in timerinput or 'minute' in timerinput:

            if ""seconds"" in timerinput or 'second' in timerinput:
                 speak_text_cmd('starting timer for ' + minutes1 + ' minutes and ' + seconds1 + ' seconds')

            if ""hours"" in timerinput or 'hour' in timerinput:
                speak_text_cmd(hours1 + ' hour and ' + minutes1 + ' minute timer starting now')

            else:
                speak_text_cmd('starting timer for ' + minutes1 + ' minutes')
        else:
            if ""seconds"" in timerinput or 'second' in timerinput:
                speak_text_cmd(seconds1 + "" timer starting now"")

            if ""hours"" in timerinput or 'hour' in timerinput:
                speak_text_cmd(hours1 + "" timer starting now"")

    except valueerror:
            speak_text_cmd('i didnt quite get that, can you say that again')


def countdown(h, m, s):
    total_seconds = h * 3600 + m * 60 + s
    while total_seconds > 0:
        timer = datetime.timedelta(seconds=total_seconds)

        speak_text_cmd(timer, end=""\r"")

        time.sleep(1)
        speak_text_cmd(total_seconds)
        total_seconds -= 1

    speak_text_cmd('timer has ended')
#data = str.split('from',1)[0]

h = hours1
m = minutes1
s = seconds1
countdown(int(h), int(m), int(s))

you can have inputs with numbers as words like ""set a timer for thirty minutes and fifteen seconds"" or not like ""set a timer for 30 minutes and 15 seconds"". just replace the ""search8"" with what you want to say.
enjoy!",https://stackoverflow.com/questions/72875686,python,05-07-2022 21:23,71.0,1.0,2.0,True,18-07-2022 22:53,05-07-2022 22:11,Implementation Issues
77454475,typeerror when using openai-api,"using the code below and openai version 0.28.0 i get an error which i can't resolve:
file """", line 11, in 
typeerror: string indices must be integers, not 'str'
which indice is it complaining about. seems i'm a little blind today...
import requests
from bs4 import beautifulsoup
from docx import document
import openai

# set your openai api key
openai.api_key = ""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""


# url of the website you want to scrape
website_url = ""


# send a get request to the website
response = requests.get(website_url)

# parse the html content of the website using beautifulsoup
soup = beautifulsoup(response.content, ""html.parser"")

# extract text blocks larger than 100 characters
text_blocks = []
for paragraph in soup.find_all(""p""):
    text = paragraph.get_text().strip()
    if len(text) >= 100:
        text_blocks.append(text)

# translate text blocks from english to german using openai's chat api
translated_text_blocks = []
for text_block in text_blocks:
    chat_input = f""translate the following english text to german: '{text_block}'""
    response = openai.chatcompletion.create(
        model=""gpt-3.5-turbo"",  # use the language model
        messages=[
            {""role"": ""system"", ""content"": ""you are a helpful assistant.""},
            {""role"": ""user"", ""content"": chat_input},
        ],
    )

    # extract translated text from the api response
    translated_text = response.choices[0].message[""content""][""body""]
    translated_text_blocks.append(translated_text)

# create a new word document
document = document()

# add translated text blocks to the word document
for translated_text in translated_text_blocks:
    document.add_paragraph(translated_text)

# save the word document
document.save(""translated_content.docx"")

the full console output is:
>>> # send a get request to the website
>>> 
>>> response = requests.get(website_url)
>>> # parse the html content of the website using beautifulsoup
>>> 
>>> soup = beautifulsoup(response.content, ""html.parser"")      
>>> # extract text blocks larger than 100 characters
>>> 
>>> text_blocks = []
>>> for paragraph in soup.find_all(""p""):
...     text = paragraph.get_text().strip()
...     if len(text) >= 100:
...         text_blocks.append(text)
... # translate text blocks from english to german using openai's chat api
...
>>> translated_text_blocks = []
>>> for text_block in text_blocks:
...     chat_input = f""translate the following english text to german: '{text_block}'""
...     response = openai.chatcompletion.create(
...         model=""gpt-3.5-turbo"",  # use the language model
...         messages=[
...             {""role"": ""system"", ""content"": ""you are a helpful assistant.""},
...             {""role"": ""user"", ""content"": chat_input},
...         ],
...     )
...     # extract translated text from the api response
...     translated_text = response.choices[0].message[""content""][""body""]
...     translated_text_blocks.append(translated_text)
... # create a new word document
...
traceback (most recent call last):
  file ""<stdin>"", line 11, in <module>
typeerror: string indices must be integers, not 'str'
>>> document = document()
>>> # add translated text blocks to the word document
>>>
>>> for translated_text in translated_text_blocks:
...     document.add_paragraph(translated_text)
... # save the word document
...
>>> document.save(""translated_content.docx"")
>>> print(""translated text blocks have been saved to 'translated_content.docx'."")
translated text blocks have been saved to 'translated_content.docx'.","['python', 'openai-api']",77455938,"your problem is caused by this line of code:
translated_text = response.choices[0].message[""content""][""body""]
response.choices[0].message[""content""] is already your response from openai api in str type
and so you are getting this error because you are trying to get item from str by key what is wrong.
so just replace this line on this line:
translated_text = response.choices[0].message[""content""]",https://stackoverflow.com/questions/77454475,python,09-11-2023 15:38,253.0,0.0,1.0,True,09-11-2023 19:41,09-11-2023 15:41,Conceptual Questions
75396481,openai gpt-3 api error: &quot;this model&#39;s maximum context length is 4097 tokens&quot;,"i am making a request to the completions endpoint. my prompt is 1360 tokens, as verified by the playground and the tokenizer. i won't show the prompt as it's a little too long for this question.
here is my request to openai in nodejs using the openai npm package.
const response = await openai.createcompletion({
  model: 'text-davinci-003',
  prompt,
  max_tokens: 4000,
  temperature: 0.2
})

when testing in the playground my total tokens after response are 1374.
when submitting my prompt via the completions api i am getting the following error:
error: {
  message: ""this model's maximum context length is 4097 tokens, however you requested 5360 tokens (1360 in your prompt; 4000 for the completion). please reduce your prompt; or completion length."",
  type: 'invalid_request_error',
  param: null,
  code: null
}

if you have been able to solve this one, i'd love to hear how you did it.","['openai-api', 'gpt-3']",75397187,"the max_tokens parameter is shared between the prompt and the completion. tokens from the prompt and the completion all together should not exceed the token limit of a particular openai model.
as stated in the official openai article:

depending on the model used, requests can use up to 4097 tokens shared
between prompt and completion. if your prompt is 4000 tokens, your
completion can be 97 tokens at most.
the limit is currently a technical limitation, but there are often
creative ways to solve problems within the limit, e.g. condensing your
prompt, breaking the text into smaller pieces, etc.

note: for counting tokens before(!) sending an api request, see this answer.
gpt-4 and gpt-4 turbo models:




latest model
description
max tokens
training data




gpt-4-1106-preview
gpt-4 turbo  the latest gpt-4 model with improved instruction following, json mode, reproducible outputs, parallel function calling, and more. returns a maximum of 4,096 output tokens. this preview model is not yet suited for production traffic. learn more.
128,000 tokens
up to apr 2023


gpt-4-vision-preview
gpt-4 turbo with vision  ability to understand images, in addition to all other gpt-4 turbo capabilties. returns a maximum of 4,096 output tokens. this is a preview model version and not suited yet for production traffic. learn more.
128,000 tokens
up to apr 2023


gpt-4
currently points to gpt-4-0613. see continuous model upgrades.
8,192 tokens
up to sep 2021


gpt-4-0613
snapshot of gpt-4 from june 13th 2023 with improved function calling support.
8,192 tokens
up to sep 2021


gpt-4-32k
currently points to gpt-4-32k-0613. see continuous model upgrades.
32,768 tokens
up to sep 2021


gpt-4-32k-0613
snapshot of gpt-4-32k from june 13th 2023 with improved function calling support.
32,768 tokens
up to sep 2021


gpt-4-0314 (legacy)
snapshot of gpt-4 from march 14th 2023 with function calling support. this model version will be deprecated on june 13th 2024.
8,192 tokens
up to sep 2021


gpt-4-32k-0314 (legacy)
snapshot of gpt-4-32k from march 14th 2023 with function calling support. this model version will be deprecated on june 13th 2024.
32,768 tokens
up to sep 2021




gpt-3.5 models:




latest model
description
max tokens
training data




gpt-3.5-turbo-1106
updated gpt 3.5 turbo  the latest gpt-3.5 turbo model with improved instruction following, json mode, reproducible outputs, parallel function calling, and more. returns a maximum of 4,096 output tokens. learn more.
16,385 tokens
up to sep 2021


gpt-3.5-turbo
currently points to gpt-3.5-turbo-0613. will point to gpt-3.5-turbo-1106 starting dec 11, 2023. see continuous model upgrades.
4,096 tokens
up to sep 2021


gpt-3.5-turbo-16k
currently points to gpt-3.5-turbo-0613. will point to gpt-3.5-turbo-1106 starting dec 11, 2023. see continuous model upgrades.
16,385 tokens
up to sep 2021


gpt-3.5-turbo-instruct
similar capabilities as text-davinci-003 but compatible with legacy completions endpoint and not chat completions.
4,096 tokens
up to sep 2021


gpt-3.5-turbo-0613 (legacy)
snapshot of gpt-3.5-turbo from june 13th 2023. will be deprecated on june 13, 2024.
4,096 tokens
up to sep 2021


gpt-3.5-turbo-16k-0613 (legacy)
snapshot of gpt-3.5-16k-turbo from june 13th 2023. will be deprecated on june 13, 2024.
16,385 tokens
up to sep 2021


gpt-3.5-turbo-0301 (legacy)
snapshot of gpt-3.5-turbo from march 1st 2023. will be deprecated on june 13th 2024.
4,096 tokens
up to sep 2021




gpt-3 models (legacy):




latest model
description
max tokens
training data




text-curie-001
very capable, faster and lower cost than davinci.
2,049 tokens
up to oct 2019


text-babbage-001
capable of straightforward tasks, very fast, and lower cost.
2,049 tokens
up to oct 2019


text-ada-001
capable of very simple tasks, usually the fastest model in the gpt-3 series, and lowest cost.
2,049 tokens
up to oct 2019


davinci
most capable gpt-3 model. can do any task the other models can do, often with higher quality.
2,049 tokens
up to oct 2019


curie
very capable, but faster and lower cost than davinci.
2,049 tokens
up to oct 2019


babbage
capable of straightforward tasks, very fast, and lower cost.
2,049 tokens
up to oct 2019


ada
capable of very simple tasks, usually the fastest model in the gpt-3 series, and lowest cost.
2,049 tokens
up to oct 2019




gpt base models:




latest model
description
max tokens
training data




babbage-002
replacement for the gpt-3 ada and babbage base models.
16,384 tokens
up to sep 2021


davinci-002
replacement for the gpt-3 curie and davinci base models.
16,384 tokens
up to sep 2021",https://stackoverflow.com/questions/75396481,openai-api,09-02-2023 09:18,110339.0,45.0,3.0,True,28-11-2023 16:46,13-03-2023 14:20,Implementation Issues
78438846,read a stream of a word document (.doc) in python,"i'm trying to read a word document (.doc) to create a customwordloader for langchain.  i'm currently able to read .docx files using the python-docx package.
the stream is created by reading a word document from a sharepoint site.
here is code for docs:
class customwordloader(baseloader):
    """"""
    this class is a custom loader for word documents. it extends the baseloader class and overrides its methods.
    it uses the python-docx library to parse word documents and optionally splits the text into manageable documents.
    
    attributes:
    stream (io.bytesio): a binary stream of the word document.
    filename (str): the name of the word document.
    """"""
    def __init__(self, stream, filename: str):
        # initialize with a binary stream and filename
        self.stream = stream
        self.filename = filename

    def load_and_split(self, text_splitter=none):
        # use python-docx to parse the word document from the binary stream
        doc = docxdocument(self.stream)
        # extract and concatenate all paragraph texts into a single string
        text = ""\n"".join([p.text for p in doc.paragraphs])

        # check if a text splitter utility is provided
        if text_splitter is not none:
            # use the provided splitter to divide the text into manageable documents
            split_text = text_splitter.create_documents([text])
        else:
            # without a splitter, treat the entire text as one document
            split_text = [{'text': text, 'metadata': {'source': self.filename}}]

        # add source metadata to each resulting document
        for doc in split_text:
            if isinstance(doc, dict):
                doc['metadata'] = {**doc.get('metadata', {}), 'source': self.filename}
            else:
                doc.metadata = {**doc.metadata, 'source': self.filename}

        return split_text

my solution will be deployed on a docker using ""3.11.8-alpine3.18"" (a slim version of unix).
for security reasons, i can't download the file locally, so i really need to able to read the stream like my example: doc = docxdocument(self.stream)
i tried to find the equivalent package to python-docx that is able to read a .docx but not a .doc.","['python', 'langchain', 'doc']",78449665,"i was able to do it using textract. i have to save the stream in a file locally, but that's the only way i found.
here is my code:
class customwordloader(baseloader):
""""""
a custom loader for word documents, extending baseloader. it reads word documents from a binary stream,
writes them temporarily to disk, and uses textract to extract text. if textract fails, an exception is raised.
""""""
def __init__(self, stream, filename: str):
    self.stream = stream
    self.filename = filename

def load_and_split(self, text_splitter=none):
    # generate a unique filename
    temp_filename = str(uuid.uuid4()) + '.doc'
    
    # create a temporary directory
    temp_dir = os.path.join(os.getcwd(), 'temp')
    os.makedirs(temp_dir, exist_ok=true)
    
    # full path to the temporary file
    temp_file_path = os.path.join(temp_dir, temp_filename)
    
    # write the content of the stream into the temporary file
    with open(temp_file_path, 'wb') as f:
        f.write(self.stream.read())
    
    # use textract to extract the text from the file
    text = textract.process(temp_file_path).decode('utf-8')
    
    if text_splitter is not none:
        split_text = text_splitter.create_documents([text])
    else:
        split_text = [{'text': text, 'metadata': {'source': self.filename}}]

    for doc in split_text:
        if isinstance(doc, dict):
            doc['metadata'] = {**doc.get('metadata', {}), 'source': self.filename}
        else:
            doc.metadata = {**doc.metadata, 'source': self.filename}

    # remove the temporary file
    os.remove(temp_file_path)

    return split_text

i hope this can help someone!",https://stackoverflow.com/questions/78438846,python,06-05-2024 19:53,384.0,0.0,1.0,True,08-05-2024 15:36,06-05-2024 20:03,Implementation Issues
74522493,creating and visualizing spacy spans,"i have a problem visualizing manually created spans in spacy:
given the simple code:
from spacy.tokens import span
text = ""welcome to the bank of china. ""
nlp = spacy.blank(""en"")
doc = nlp(text)

doc.spans[""xx""] = [span(doc, 0, 1, ""org"")]
doc.spans[""sc""] = [
    span(doc, 3, 6, ""org""), 
    span(doc, 5, 6, ""gpe""),
    span(doc, 2, 4, ""welcome"")
]

the following visualizer works:
displacy.render(doc, style=""span"")


but if the spans do not contain the key ""sc"" it does not work

the error is key error ""sc""
what is the problem there? why is the rendering not showing me all the spans?
the code giving the error is:
doc.spans[""xx""] = [
    span(doc, 3, 6, ""org""), 
    span(doc, 5, 6, ""gpe""),
    span(doc, 2, 4, ""welcome"")
]
displacy.render(doc, style=""span"", options ={""spans_key"":""xx""})","['python', 'spacy']",74527341,"as explained in the displacy documentation, by default the spans in the key ""sc"" are used. you can change it with the spans_key parameter.
render doesn't take spans_key correctly, you have to include it in options.
from the docs, modified to use render instead of serve:
doc.spans[""custom""] = [span(doc, 3, 6, ""bank"")]
options = {""spans_key"": ""custom""}
displacy.render(doc, style=""span"", options=options)",https://stackoverflow.com/questions/74522493,python,21-11-2022 17:05,424.0,0.0,1.0,True,22-11-2022 13:44,22-11-2022 13:44,Conceptual Questions
77563897,bert topic clasiffying over a quarter of documents in outlier topic -1,"i am running bert topic with default options
import pandas as pd
from sentence_transformers import sentencetransformer
import time
import pickle
from bertopic import bertopic

llm_mod =  ""all-minilm-l6-v2""
model = sentencetransformer(llm_mod)

embeddings = model.encode(skills_augmented, show_progress_bar=true)
bertopic_model = bertopic(verbose=true)

i have a dataset of 40,000 documents that are only one short sentence. 13,573 of the documents get placed in the -1 topic (below distribution across top 5 topics).
-1      13573
 0       1593
 1       1043
 2        628
 3        627

from the documentation: the -1 refers to all outliers and should typically be ignored. is there a parameter i can use to get less documents in -1? perhaps get a more even distribution across topics? would running kmeans be better?","['python', 'nlp', 'bert-language-model', 'topic-modeling']",77565599,"from the documentation :

the main way to reduce your outliers in bertopic is by using the .reduce_outliers function. to make it work without too much tweaking, you will only need to pass the docs and their corresponding topics. you can pass outlier and non-outlier documents together since it will only try to reduce outlier documents and label them to a non-outlier topic.

the following is a minimal example:
from bertopic import bertopic

# train your bertopic model
topic_model = bertopic()
topics, probs = topic_model.fit_transform(docs)

# reduce outliers
new_topics = topic_model.reduce_outliers(docs, topics)

you can find all the strategies for reducing outliers in this page outlier reduction",https://stackoverflow.com/questions/77563897,python,28-11-2023 12:32,1108.0,-1.0,1.0,True,28-11-2023 16:32,28-11-2023 16:13,Implementation Issues
69585176,using sentence-bert with other features in scikit-learn,"i have a dataset, one feature is text and 4 more features. sentence-bert vectorizer transforms text data into tensors. i can use these sparse matrices directly with a machine learning classifier. can i replace the text column with tensors? and, how can i train the model. the code below is how i transform the text into vectors.
model = sentencetransformer('sentence-transformers/labse')
sentence_embeddings = model.encode(x_train['tweet'], convert_to_tensor=true, show_progress_bar=true)
sentence_embeddings1 = model.encode(x_test['tweet'], convert_to_tensor=true, show_progress_bar=true)","['python', 'machine-learning', 'nlp', 'embedding', 'bert-language-model']",69595712,"let's assume this is your data
x_train = pd.dataframe({
    'tweet':['foo', 'foo', 'bar'],
    'feature1':[1, 1, 0],
    'feature2':[1, 0, 1],
})
y_train = [1, 1, 0]

and you are willing to use it with sklearn api (cross-validation, pipeline, grid-search, and so on). there is a utility named columntransformer which can map pandas data frames to the desired data using user-defined arbitrary functions! what you have to do is define a function and create an official sklearn.transformer from it.
model = sentencetransformer('mrm8488/bert-tiny-finetuned-squadv2') # model named is changed for time and computation gians :)
embedder = functiontransformer(lambda item:model.encode(item, convert_to_tensor=true, show_progress_bar=false).detach().cpu().numpy())

after that, you would be able to use the transformer like any other transformer and map your text column into semantic space, like:
preprocessor = columntransformer(
    transformers=[('embedder', embedder, 'tweet')],
    remainder='passthrough'
    )
x_train = preprocessor.fit_transform(x_train) # x_train.shape => (len(df), your_transformer_model_hidden_dim + your_features_count)

x_train would be the data you wanted. it's proper to use with sklearn ecosystem.
gnb = gaussiannb()
gnb.fit(x_train, y_train) 

output:
gaussiannb(priors=none, var_smoothing=1e-09)
caveat: numerical features and the tweets embeddings should belong to the same scale otherwise some would dominate others and degrade the performance",https://stackoverflow.com/questions/69585176,python,15-10-2021 13:03,2079.0,3.0,1.0,True,16-10-2021 12:47,15-10-2021 15:26,Conceptual Questions
64896922,how do i truncate long document for bert?,"i am trying some bert tutorial in my language, document(korean, non-latin)
however, document is very long. so i have to truncate it. and i dont know how.
if there is a text(ex: 5, ""i have a brown cat"") longer than max_length(for ex:3), then which one is the right truncation do i have to make? (dont think about start/end word/mask)
a: [(""i have a""), (""brown cat [pad]"")] or  b:[(""i have a""), (""have a brown""), (""a brown cat"")]
which one should be better?
or is there any better solution?",['nlp'],64897138,"well, the notion is, you have to do the truncation before prepending and appending the [cls] and [sep] tokens. and you can indeed choose the number max tokens of 512, for bert-base model alone. so, the idea is, first you choose the max tokens less than 512 (if you are using bert-base). then, split the sentence to its list of word-pieces, then truncate the sentence to max_tokens - 2. with this, when you add [cls] and [sep] tokens, it would have a number of tokens equal to max_tokens. for any sentence having the number of tokens (including [cls] and [sep]) less than max_tokens, you can always append with zeros.",https://stackoverflow.com/questions/64896922,nlp,18-11-2020 16:13,2059.0,0.0,2.0,True,30-01-2021 15:34,18-11-2020 16:34,Implementation Issues
74379471,spacy matcher pattern in + regex tag,"my goal is to match with spacy the sentences that contain one of the following words:
['studium','abschluss','ausbildung']
i can solve the problem with this line:
pattern = [{""lower"": {'in':['studium','abschluss', 'ausbildung']}}]

my problem is that in german there is a vast use of composed words like hochschulstudium, masterstudium, studiengang etc.
how can use the regex inside the in sentence to match all words containing the word studium?","['python', 'regex', 'nlp', 'nltk', 'spacy']",74379663,"you can use the regex operator:
import re
l = ['abschluss', 'ausbildung']
pattern = [{'lower': {'regex':fr'^(?:{""|"".join(map(re.escape, l))}|[^\w\d_]*studium)$'}}]

note:

map(re.escape, l) - escapes the items in the l list
""|"".join(...) - joins the words as alternatives (word1|word2|wordn)
^(?:...|[^\w\d_]*studium)$ - a regex that matches

^ - start of string (here, token)
(?:...|[^\w\d_]*studium) - a non-capturing group matching any of the l items or any zero or more letters ([^\w\d_]*) followed with studium
$ - end of string (token here).",https://stackoverflow.com/questions/74379471,python,09-11-2022 18:05,394.0,1.0,1.0,True,09-11-2022 18:19,09-11-2022 18:09,Implementation Issues
75682331,does chatgpt-3.5-turbo api recounts the tokens in billing when sending the conversation history in api,"when creating a chat app using chatgpt-3.5-turbo model. does the api consider the whole tokens (including the assistant messages and old set of messages) in billing or just the last message from the user is counted in billing whenever i resend the api request with a new message appended to the conversation?
for eg:
messages = [
    {""role"": ""system"", ""content"": ""you are a kind helpful assistant.""},
]
     

while true:
    message = input(""user : "")
    if message:
        messages.append(
            {""role"": ""user"", ""content"": message},
        )
        chat = openai.chatcompletion.create(
            model=""gpt-3.5-turbo"", messages=messages
        )
    
    reply = chat.choices[0].message.content
    print(f""chatgpt: {reply}"")
    messages.append({""role"": ""assistant"", ""content"": reply})","['openai-api', 'chatgpt-api']",75729602,"as mentioned in openai document:

the total number of tokens in an api call affects how much your api call costs, as you pay per token 
both input and output tokens count toward these quantities. for example, if your api call used 10 tokens in the message input and you received 20 tokens in the message output, you would be billed for 30 tokens.

to see how many tokens are used by an api call, check the usage field in the api response
response['usage']['total_tokens']

each time you append previous chats to messages, the number of total_token will increases. so all tokens of previous messages will be considered in the bill.",https://stackoverflow.com/questions/75682331,openai-api,09-03-2023 08:57,2939.0,4.0,1.0,True,27-03-2023 06:41,10-03-2023 07:25,Implementation Issues
72235951,issue installing spacyr on r 4.2,"i am having trouble installing spacyr on r version 4.2. what is strange is that i've used this package in the past just fine. the installation is pretty basic. per their site you do the following:
install.packages(""spacyr"")
library(""spacyr"")
spacy_install()

however, when i run spacy_install() i get the error: error: one or more python packages failed to install [error code 1] with the traceback:
5.
stop(sprintf(fmt, ...), call. = call.)
4.
stopf(fmt, result)
3.
reticulate::conda_install(envname, packages, pip = pip, conda = conda)
2.
process_spacy_installation_conda(conda, version, lang_models, 
python_version, prompt, envname = envname, pip = pip)
1.
spacy_install()


i've tried uninstalling and reinstalling spacyr.","['r', 'spacy', 'failed-installation']",72905993,"i solved my problem:
i tried more things like install_miniconda() (see the following code block)

install.packages('spacyr')
spacyr:::install_miniconda()
library(spacyr);spacy_install(prompt = false)


but i was still unable to use spacy.
i deleted my miniconda folder in my home directory and then ran the above code and i was able to use spacy.",https://stackoverflow.com/questions/72235951,r,13-05-2022 22:30,714.0,0.0,1.0,True,08-07-2022 02:11,14-05-2022 15:11,Implementation Issues
66888668,name entity recognition (ner) for multiple languages,"i am writing some code to perform named entity recognition (ner), which is coming along quite nicely for english texts. however, i would like to be able to apply ner to any language. to do this, i would like to 1) identify the language of a text, and then 2) apply the ner for the identified language. for step 2, i'm doubting to a) translate the text to english, and then apply the ner (in english), or b) apply the ner in the language identified.
below is the code i have so far. what i would like is for the ner to work for text2, or in any other language, after this language is first recognized:
import spacy
from spacy_langdetect import languagedetector
from langdetect import detectorfactory

text = 'in 1793, alexander hamilton recruited webster to move to new york city and become an editor for a federalist party newspaper.'
text2 = 'em 1793, alexander hamilton recrutou webster para se mudar para a cidade de nova york e se tornar editor de um jornal do partido federalista.'

# step 1: identify the language of a text
detectorfactory.seed = 0
nlp = spacy.load('en_core_web_sm')
nlp.add_pipe(languagedetector(), name='language_detector', last=true)
doc = nlp(text)
print(doc._.language)

# step 2: ner
entities = [(str(x), x.label_) for x in nlp(str(text)).ents]
print(entities)

does anyone have any experience with this? much appreciated!","['python', 'nlp', 'spacy', 'named-entity-recognition']",66890744,"spacy needs to load the correct model for the right language.
see  for available models.
import spacy
from langdetect import detect
nlp={}    
for lang in [""en"", ""es"", ""pt"", ""ru""]: # fill in the languages you want, hopefully they are supported by spacy.
    if lang == ""en"":
        nlp[lang]=spacy.load(lang + '_core_web_lg')
    else: 
        nlp[lang]=spacy.load(lang + '_core_news_lg')

def entites(text):
     lang = detect(text)
     try:
         nlp2 =nlp[lang]
     except keyerror:
         return exception(lang + "" model is not loaded"")
     return [(str(x), x.label_) for x in nlp2(str(text)).ents]

then, you could run the two steps together
ents = entites(text)
print(ents)",https://stackoverflow.com/questions/66888668,python,31-03-2021 13:16,2163.0,3.0,1.0,True,01-04-2021 18:38,31-03-2021 14:07,Implementation Issues
67295568,no attribute &quot;str&quot; on dataframe when creating a plot,"i filtered largest 5 tweets with max polarity after sentimental analysis.
maxx = df.nlargest(5,['polarity']).astype(str)
maxx

output:
unnamed: 0  clean_tweet tweet_tokenized polarity    subjectivity    sentiment_type  scores  compound    sentiment_type  pca
315 315 best ofï¿½ï¿½ï¿½ luck bidenï¿½ï¿½ï¿½ï¿½   best / luck biden   1.0 0.3 positive    {'neg': 0.0, 'neu': 0.122, 'pos': 0.878, 'comp...   0.802   positive    [-0.06151099614792966, -0.030998756958434074]

and now i'd like to create some wordcloud but i'm getting error:
hero.wordcloud(maxx, max_words=100)

attributeerror: 'dataframe' object has no attribute 'str'","['python', 'pandas', 'string', 'dataframe', 'nlp']",67295663,"sorting based on polarity
df['polarity'] = df['polarity'].astype('float')   
maxx = df.nlargest(5, 'polarity')

if you are using wordcloud package try this
from wordcloud import wordcloud

text_data = ' '.join(maxx['clen_tweet']) 

wordcloud = wordcloud().generate(text_data)
plt.imshow(wordcloud2)
plt.axis(""off"")
plt.show()

using texthero
import texthero as hero

hero.wordcloud(maxx['clean_tweet'], max_words=100)",https://stackoverflow.com/questions/67295568,python,28-04-2021 07:20,87.0,0.0,1.0,True,28-04-2021 07:35,28-04-2021 07:31,Preprocessing Tasks
78698002,how can i get the lastmodifieddate (or any date) in azure open ai service maps?,"i created an azure resource for testing that i am populating with a custom blob i created in azure only for testing, i created the skillset, the indexer, the index, everything for the retrieval of the date and up until the index all the data is being sent properly:
example 1
example 2
my main interest is to do a hybrid search (so i can use a scoring profile i have created to use freshness... and then when i go to open azure ai the date is not part of the mapping and the files are not getting the date, and i really do not know why:
example 3
example 4
i have gone back and forth between index (up until where all the data is being sent properly) and open azure ai tool, but i have not found how to make it take the date into consideration, does anybody know how ?","['azure', 'openai-api', 'langchain', 'azure-openai', 'azure-ai-search']",78720869,"the field of type datetimeoffset is not supported while doing custom field mapping in azure open ai.
all of the fields content data, file name, title,url and vector fields asked is of type string.
whatever the fields you do map for content data will be used by model for search query and the references are given by the field file name.
if you want your date as a result in the reference you need to convert the type to string.

then you will get the date field for mapping.
if you map it to file name then instead of file name you will get the date as reference but no filename.

next, if you want to search in the query like
""give me content before 20th apr 2024""
kind of query then make the date field as string type and searchable as true then map it to content data.",https://stackoverflow.com/questions/78698002,azure,02-07-2024 15:27,211.0,0.0,1.0,True,08-07-2024 12:30,03-07-2024 07:57,Implementation Issues
