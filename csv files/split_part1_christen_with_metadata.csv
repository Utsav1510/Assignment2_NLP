question_id,title,body,tags,accepted_answer_id,accepted_answer_body,link,tag,creation_date,view_count,score,answer_count,is_answered,last_activity_date,last_edit_date
71694329,need help to remove punctuation and replace numbers for an nlp task,"for example, i have a string:
sentence = ['cracked $300 million','she\'s resolutely, smitten ', 'that\'s creative [r]', 'the market ( knowledge check : prices up!']

i want to remove the punctuation and replace numbers with the 'ï¿½ï¿½' symbol.
i have tried this but can only replace one or the other when i try to run them both.
my code is below
import re
s =([re.sub(r'[!"":$()[]\',]',' ', word) for word in sentence]) 

s= [([re.sub(r'\d+','ï¿½ï¿½', word) for word in s])]
s)

i think the problem could be in the square brackets??
thank y","['python-3.x', 'regex', 'list', 'nlp']",71703522,"if you want to replace some specific punctuation symbols with a space and any digit chunks with a ï¿½ï¿½ sign, you can use
import re
rx = re.compile(r'''[][!"":$()',]|(\d+)''')
sentence = ['cracked $300 million','she\'s resolutely, smitten ', 'that\'s creative [r]', 'the market ( knowledge check : prices up!']
s = [rx.sub(lambda x: 'ï¿½ï¿½' if x.group(1) else ' ', word) for word in sentence] 
print(s) # => ['cracked  ï¿½ï¿½ million', 'she s resolutely  smitten ', 'that s creative  r ', 'the market   knowledge check   prices up ']

see the "" rel=""nofollow noreferrer"">python demo.
note where [] are inside a character class: when ] is at the start, it does not need to be escaped and [ does not have to be escaped at all inside character classes. i also used a triple-quoted string literal, so you can use "" and  ' as is without extra escaping.
so, here, [][!"":$()',]|(\d+) matches ], [, !, "", :, $, (, ), ' or , or matches and captures into group 1 one or more digits. if group 1 matched, the replacement is the euro sign, else, it is a space.",https://stackoverflow.com/questions/71694329,python-3.x,31-03-2022 14:26,397.0,1.0,3.0,True,01-04-2022 07:51,31-03-2022 15:40
71467995,value error when trying to train a spacy model,"i tried training a spacy model but recently i started to get some errors , i got the below error and i would like some one to help me resolve error
def train_model(model, train_data, optimizer, batch_size, epochs=10):
        losses = {}
        random.seed(1)
    
        for epoch in range(epochs):
            random.shuffle(train_data)
    
            batches = minibatch(train_data, size=batch_size)
            for batch in batches:
                # split batch into texts and labels
                texts, labels = zip(*batch)
    
                # update model with texts and labels
                nlp.update(texts, labels, sgd=optimizer, losses=losses)
            print(""loss: {}"".format(losses['textcat']))
    
        return losses['textcat']




optimizer = nlp.begin_training()
batch_size = 5
epochs = 20
    
# training the model
train_model(nlp, train_data, optimizer, batch_size, epochs)

below is the error which shows that there is a value error
valueerror                                                       
                 traceback (most recent call last)
~\appdata\local\temp/ipykernel_16120/3494358196.py in <module>
      4 
      5 # training the model
----> 6 train_model(nlp, train_data, optimizer, batch_size, epochs)

~\appdata\local\temp/ipykernel_16120/3158014372.py in train_model(model, train_data, optimizer, batch_size, epochs)
     12 
     13             # update model with texts and labels
---> 14             nlp.update(texts, labels, sgd=optimizer, losses=losses)
     15         print(""loss: {}"".format(losses['textcat']))
     16 

~\anaconda3\lib\site-packages\spacy\language.py in update(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)
   1132         """"""
   1133         if _ is not none:
-> 1134             raise valueerror(errors.e989)
   1135         if losses is none:
   1136             losses = {}

valueerror: [e989] `nlp.update()` was called with two positional arguments. this may be due to a backwards-incompatible change to the format of the training data in spacy 3.0 onwards. the 'update' function should now be called with a batch of example objects, instead of `(text, annotation)` tuples.","['python', 'python-3.x', 'nlp', 'spacy', 'spacy-3']",71476698,"base on documentation they made some changes in version 3.x and now it uses directly batch without spliting texts, labels = zip(*batch).
            for batch in batches:
                nlp.update(batch, sgd=optimizer, losses=losses)

that's all.",https://stackoverflow.com/questions/71467995,python,14-03-2022 12:42,601.0,2.0,1.0,True,15-03-2022 03:29,15-03-2022 02:37
61121982,asking gpt-2 to finish sentence with huggingface transformers,"i am currently generating text from left context using the example script run_generation.py of the huggingface transformers library with gpt-2:
$ python transformers/examples/run_generation.py \
  --model_type gpt2 \
  --model_name_or_path gpt2 \
  --prompt ""hi, "" --length 5

=== generated sequence 1 ===
hi,  could anyone please inform me

i would like to generate short complete sentences. is there any way to tell the model to finish a sentence before length words?

note: i don't mind changing model, but would prefer an auto-regressive one.","['nlp', 'pytorch', 'huggingface-transformers', 'gpt-2']",64542919,"unfortunately there is no way to do so. you can set the length parameter to a greater value and then just discard the incomplete part at the end.
even gpt3 doesn't support completing a sentence before a specific length. gpt3 support ""sequences"" though. sequences force the model to stop when certain condition is fulfilled. you can find more information about in thi article",https://stackoverflow.com/questions/61121982,nlp,09-04-2020 13:12,2641.0,4.0,2.0,True,30-04-2022 02:20,29-11-2020 11:58
66654470,spacy importerror: cannot import name deque in jupyter notebook,"i want to use spacy in a python project on juptyer notebook but when i try to import the module i get the error ""importerror: cannot import name deque"". i installed spacy in pycharm in my virtual env via pip install -u spacy
i've seen another question on here that was similar (importerror cannot import name deque with spacy) but i can import spacy in pycharm just fine so i assume it's a jupyter notebook specific problem.
i also checked the spacy version on pycharm and jn and they're both the same, however it seems like my virtual env and jn don't use the same python version (3.8.1 in my venv on pycharm and 3.6.0 on jn)
[python versions in pycharm and jn]
may that be the problem or is something else causing it?
i also tried to uninstall and install again in pycharm but it didn't change anything and also tried to pip install spacy in jn again but that also didn't solve it, jn told me i might have to restart the kernel but after restarting it still couldn't import spacy without that error.
i've tried so much that after some time i just get the error importerror: cannot import name errors but i assume it's still the same problme.
i'm very grateful for any suggestions and tips!","['python', 'python-3.x', 'jupyter-notebook', 'pycharm', 'spacy']",66659074,"so for future references:
i just 'solved' this problem by making a new environment with python 3.6 since afaik jupyter notebook doesn't support python versions highter than 3.6.x as of now yet, set up a new venv and now both pycharm and jn use the same python version (3.6.12) and i could successfully import spacy.
so i'm assuming the different python versions really were the problem.
maybe it's not even necessary to create a new venv and everything but i wanted to start clean again to not have further problems.",https://stackoverflow.com/questions/66654470,python,16-03-2021 11:36,1416.0,2.0,1.0,True,23-03-2024 06:29,16-03-2021 13:20
65854722,huggingface albert tokenizer nonetype error with colab,"i simply tried the sample code from hugging face website: 
from transformers import alberttokenizer, albertmodel
tokenizer = alberttokenizer.from_pretrained('albert-base-v2')
text = ""replace me by any text you'd like.""
encoded_input = tokenizer(text, return_tensors='pt')

then i got the following error at the tokenizer step:
encoded_input = tokenizer(text, return_tensors='pt')
typeerror: 'nonetype' object is not callable
i tried the same code on my local machine, it worked no problem. the problem seems within colab. however, i do need help to run this model on colab gpu.
my python version on colab is python 3.6.9.","['google-colaboratory', 'huggingface-transformers', 'huggingface-tokenizers']",66089859,"i found the answer. after install, import the alberttokenizer and tokenizer=..., i received an error asking me to install sentencepiece package. however, after i install this package and run tokenizer again, i started receiving the error above.
so i open a brand new colab session, and install everything including the sentencepiece before creating tokenizer, and this time it worked. the nonetype error simply means it doesn't know what is albert-base-v2. however if you install the packages in right order colab will recognize better the relationship between alberttokenizer and sentencepiece.
in short for this to work in colab

open a new colab session
install transformers and sentencepiece
import alberttokenizer
create tokenizer.",https://stackoverflow.com/questions/65854722,google-colaboratory,23-01-2021 01:00,13036.0,21.0,6.0,True,14-05-2023 11:05,11-05-2023 17:46
25889173,nhunspell - how to generate all recognized words?,"is it possible with nhunspell to generate the list of all recognized/valid words? as far as i remember it was possible with hunspell (unmunch) from the command line.
from current hunspell documentation:
munch: dictionary generation from vocabularies (it needs an affix file, too).
unmunch: list all recognized words of a myspell dictionary 
wordforms: word generation (hunspell version of unmunch)","['nlp', 'hunspell', 'nhunspell']",25895341,"no, that is not possible with nhunspell at the moment because it isn't part of the hunspell library but only the hunspell command line tool. i've implemented nearly all functions of the hunspell library in nhunspell, but not all command line tools. if you want it implemented please suggest this feature in the nhunspell forum:",https://stackoverflow.com/questions/25889173,nlp,17-09-2014 11:21,621.0,2.0,1.0,True,19-12-2023 15:18,19-12-2023 15:18
72428445,error node: &#39;binary_crossentropy/cast&#39; cast string to float is not supported while train model,"i want to train my data i already make my data to string with word2vec pretrain model from here  and success to make a model, but when i want to train the dataset i got error like this
unimplementederror                        traceback (most recent call last)
<ipython-input-28-85ce60cd1ded> in <module>()
      1 history = model.fit(x_train, y_train, epochs=6,
      2                     validation_data=(x_test, y_test),
----> 3                     validation_steps=30)

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     53     ctx.ensure_initialized()
     54     tensors = pywrap_tfe.tfe_py_execute(ctx._handle, device_name, op_name,
---> 55                                         inputs, attrs, num_outputs)
     56   except core._notokstatusexception as e:
     57     if name is not none:

unimplementederror: graph execution error:

#skiping error

node: 'binary_crossentropy/cast'
cast string to float is not supported
     [[{{node binary_crossentropy/cast}}]] [op:__inference_train_function_21541]

the code :
file = gzip.open(urlopen('
vocab_and_vectors = {}
# put words as dict indexes and vectors as words values
for line in file:
    values = line.split()
    word = values [0].decode('utf-8')
    vector = np.asarray(values[1:], dtype='float32')
    vocab_and_vectors[word] = vector

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

# how many features should the tokenizer extract
features = 500
tokenizer = tokenizer(num_words = features)
# fit the tokenizer on our text
tokenizer.fit_on_texts(df[""comment""].tolist())

# get all words that the tokenizer knows
word_index = tokenizer.word_index
print(len(word_index))
# put the tokens in a matrix
x = tokenizer.texts_to_sequences(df[""comment""].tolist())
x = pad_sequences(x)

# prepare the labels
y = df[""sentiments""].values

# split in train and test
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, stratify=y, random_state=30)
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

embedding_matrix = np.zeros((len(word_index) + 1, 300))
for word, i in word_index.items():
    embedding_vector = vocab_and_vectors.get(word)
    # words that cannot be found will be set to 0
    if embedding_vector is not none:
        embedding_matrix[i] = embedding_vector

from tensorflow.keras.models import sequential
from tensorflow.keras.layers import lstm, dense, embedding

model = tf.keras.sequential([
    tf.keras.layers.embedding(len(word_index)+1, 300, input_length=x.shape[1], weights=[embedding_matrix], trainable=false),
    tf.keras.layers.bidirectional(tf.keras.layers.lstm(64,  return_sequences=true)),
    tf.keras.layers.bidirectional(tf.keras.layers.lstm(32)),
    tf.keras.layers.dense(64, activation='relu'),
    tf.keras.layers.dropout(0.5),
    tf.keras.layers.dense(1, activation='sigmoid')
])
model.summary()
model.compile(loss='binary_crossentropy',
              optimizer=tf.keras.optim

izers.adam(1e-4),
              metrics=['accuracy'])

history = model.fit(x_train, y_train, epochs=6,
                    validation_data=(x_test, y_test), 
                    validation_steps=30)

my data :","['python', 'machine-learning', 'nlp', 'lstm', 'tf.keras']",72429641,"i think the problem is, that you are having strings as classes (""positif"", ""negativ"") as shown in your data excerpt. just transform them to

negativ=0 and positiv=1

and it should work.
if not we would have to look a little bit deeper into your code.
edit:
so as suspected the problem where the strings as classes. you can change them to float with:
df[""sentiments""].loc[df[""sentiments""]==""positif""]=1.0
df[""sentiments""].loc[df[""sentiments""]==""negatif""]=0.0

after this you also should change the dtype of the y-nparry to float:
y = np.asarray(y).astype(""float64"")

this should do the trick.
for reference see also my colab code",https://stackoverflow.com/questions/72428445,python,30-05-2022 01:54,5734.0,2.0,2.0,True,07-08-2023 07:57,30-05-2022 08:22
66029870,how to resolve no module named &#39;nltk.translate.meteor_score&#39; on google colab,no module named 'nltk.translate.meteor_score',"['jupyter-notebook', 'nlp', 'nltk', 'google-colaboratory']",66465205,"try !pip install -u nltk
because the preinstalled nltk version on colab is 3.2.5 and the last available is 3.5
it works for me",https://stackoverflow.com/questions/66029870,jupyter-notebook,03-02-2021 14:48,3573.0,2.0,2.0,True,11-04-2023 22:47,03-02-2021 15:31
64646867,downloading huggingface pre-trained models,"once i have downloaded a pre-trained model on a colab notebook, it disappears after i reset the notebook variables.
is there a way i can download the model to use it for a second occasion?
tokenizer = berttokenizer.from_pretrained('bert-base-uncased')","['python', 'nlp', 'google-colaboratory', 'huggingface-transformers']",64647354,"mount your google drive:
from google.colab import drive
drive.mount('/content/drive')

do your stuff and save your models:
from transformers import berttokenizer

tokenizer = berttokenizer.from_pretrained('bert-base-uncased')
tokenizer.save_pretrained('/content/drive/my drive/tokenizer/')

reload it in a new session:
tokenizer2 = berttokenizer.from_pretrained('/content/drive/my drive/tokenizer/')",https://stackoverflow.com/questions/64646867,python,02-11-2020 13:52,9316.0,5.0,1.0,True,05-08-2022 07:52,02-11-2020 14:00
77337720,how to change the fully connected network in a gpt model on huggingface?,"i'm following this tutorial on training a causal language model from scratch.
in the tutorial they load the standard gpt2 as follows:
from transformers import autotokenizer, gpt2lmheadmodel, autoconfig

config = autoconfig.from_pretrained(
    ""gpt2"",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
model = gpt2lmheadmodel(config)

how can i load the same model, but use my custom fully connected network instead of the standard one? mainly want to experiment with variations such as more/less layers, different activation functions, etc.
i found the source code here, but it's very convoluted and i can't figure out how to replace the fully connected parts with a custom ones or what structure the custom one should have in the first place (e.g., input/output size).
update
for example, using a fc network as such:
class fc_model(nn.module):
    def __init__(self):
        super(fc_model, self).__init__()

        self.fc1 = nn.linear(768,256)
        self.fc2 = nn.linear(256,256)
        self.fc3 = nn.linear(256,50000)

    def forward(self, x):
        x = torch.sin(self.fc1(x)) + torch.rand(1)
        x = torch.sin(self.fc2(x))
        x = self.fc3(x)
        return x","['machine-learning', 'pytorch', 'nlp', 'huggingface-transformers', 'gpt-2']",77339487,"i'm assuming by the fully connected network you're referring to the fully connected (fc) / linear layer.
from transformers import autotokenizer, gpt2lmheadmodel, autoconfig, gpt2config
configuration = gpt2config()
model = gpt2lmheadmodel(configuration)
print(model) 

the above would show you the modules inside the model:
gpt2lmheadmodel(
  (transformer): gpt2model(
    (wte): embedding(50257, 768)
    (wpe): embedding(1024, 768)
    (drop): dropout(p=0.1, inplace=false)
    (h): modulelist(
      (0-11): 12 x gpt2block(
        (ln_1): layernorm((768,), eps=1e-05, elementwise_affine=true)
        (attn): gpt2attention(
          (c_attn): conv1d()
          (c_proj): conv1d()
          (attn_dropout): dropout(p=0.1, inplace=false)
          (resid_dropout): dropout(p=0.1, inplace=false)
        )
        (ln_2): layernorm((768,), eps=1e-05, elementwise_affine=true)
        (mlp): gpt2mlp(
          (c_fc): conv1d()
          (c_proj): conv1d()
          (act): newgeluactivation()
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
    )
    (ln_f): layernorm((768,), eps=1e-05, elementwise_affine=true)
  )
  (lm_head): linear(in_features=768, out_features=50257, bias=false)
)

you can now access and update the fc layer by:
model.lm_head = nn.sequential(
    nn.linear(in_features = 768, out_features = 256),
    nn.relu(inplace = true),
    nn.dropout1d(0.25),
    nn.linear(in_features = 256, out_features = 128)
)

the above is just a sample, you can experiment with different combinations.",https://stackoverflow.com/questions/77337720,machine-learning,21-10-2023 20:40,365.0,-1.0,2.0,True,22-10-2023 15:52,22-10-2023 15:52
66528347,add a custom component to pipeline in spacy 3,"i trained a ner model with spacy3. i would like to add a custom component (add_regex_match) to the pipeline for ner task. the aim is to improve the existing ner results.
this is the code i want to implement:
import spacy
from spacy.language import language
from spacy.tokens import span
import re

nlp = spacy.load(r""\src\spacy3\ner_spacy3_hortisem\training\ml_rule_model"")

@language.component(""add_regex_match"")
def add_regex_entities(doc):   
    new_ents = []

    label_z = ""zeit""
    regex_expression_z = r""^(?:(?:31(\/|-|\.)(?:0?[13578]|1[02]|(?:januar|mï¿½ï¿½rz|mai|juli|august|oktober|dezember)))\1|(?:(?:29|30)(\/|-|\.)(?:0?[1,3-9]|1[0-2]|(?:januar|mï¿½ï¿½rz|april|mai|juni|juli|august|september|oktober|november|dezember))\2))(?:(?:1[6-9]|[2-9]\d)?\d{2})$|^(?:29(\/|-|\.)(?:0?2|(?:februar))\3(?:(?:(?:1[6-9]|[2-9]\d)?(?:0[48]|[2468][048]|[13579][26])|(?:(?:16|[2468][048]|[3579][26])00))))$|^(?:0?[1-9]|1\d|2[0-8])(\/|-|\.)(?:(?:0?[1-9]|(?:januar|februaapril|mai|juni|juli|august|september))|(?:1[0-2]|(?:oktober|november|dezember)))\4(?:(?:1[6-9]|[2-9]\d)?\d{2})$""
    for match in re.finditer(regex_expression_z, doc.text):  # find match in text
        start, end = match.span()  # get the matched token indices
        entity = span(doc, start, end, label=label_z)
        new_ents.append(entity)
        
    label_b = ""bbch_stadium""
    regex_expression_b = r""bbch(\s?\d+)\s?(\/|\-|(bis)?)\s?(\d+)?""
    for match in re.finditer(regex_expression_b, doc.text):  # find match in text
        start, end = match.span()  # get the matched token indices
        entity = span(doc, start, end, label=label_b)
        new_ents.append(entity)

    doc.ents = new_ents
    return doc
nlp.add_pipe(""add_regex_match"", after=""ner"")

nlp.to_disk(""./training/ml_rule_regex_model"")

doc = nlp(""20/03/2021 8 mï¿½ï¿½rz 2021 bbch 15, fliegen, flugbrand . brandenburg, in berlin, schnecken, bbch 13-48, bbch 3 bis 34"")

printnt.text, ent.label_) for ent in doc.ents])

when i want to evaluate the saved model ml_rule_regex_model using the command line python -m spacy project run evaluate, i got the error:
'valueerror: [e002] can't find factory for 'add_regex_match' for language german (de). this usually happens when spacy calls nlp.create_pipe with a custom component name that's not registered on the current language class. if you're using a transformer, make sure to install 'spacy-transformers'. if you're using a custom component, make sure you've added the decorator @language.component (for function components) or @language.factory (for class components).'
how should i do it? has anyone had experience? thank you very much for your tips.","['python', 'named-entity-recognition', 'spacy-3']",67398255,"when i want to evaluate the saved model ml_rule_regex_model using the command line python -m spacy project run evaluate, i got the error...

you haven't included the project.yml of your spacy project, where the evaluate command is defined. i will assume it calls spacy evaluate? if so, that command has a --code or -c flag to provide a path to a python file with additional code, such as registered functions. by providing this file and pointing it to the definition of your new add_regex_match component, spacy will be able to parse the configuration file and use the model.",https://stackoverflow.com/questions/66528347,python,08-03-2021 10:44,1845.0,1.0,1.0,True,21-11-2023 16:09,19-06-2021 07:46
5941580,is there a search engine that will give a direct answer?,"i've been wondering about this for a while and i can't see why google haven't tried it yet - or maybe they have and i just don't know about it.
is there a search engine that you can type a question into which will give you a single answer rather than a list of results which you then have to trawl through yourself to find what you want to know?
for example, this is how i would design the system:
userï¿½ï¿½ï¿½s input: ï¿½ï¿½ï¿½where do you go to get your eyes tested?ï¿½ï¿½ï¿½
system output: ï¿½ï¿½ï¿½opticians. certainty: 95%ï¿½ï¿½ï¿½
this would be calculated as follows:

the input is parsed from natural language into a simple search string, probably something like ï¿½ï¿½ï¿½eye testingï¿½ï¿½ï¿½ in this case.  the term ï¿½ï¿½ï¿½where do you goï¿½ï¿½ï¿½ would also be interpreted by the system and used when comparing results.
the search string would be fed into a search engine.
the system would then compare the contents of the results to findstion is asking (i.e. what, where, who, how etc.)
once a suitable answer is determined, the system displays it to the user along with a measure of how sure it is that the answer is correct.

due to the dispersed nature of the internet, a correct answer is likely to appear multiple times, especially for simple questions.  for this particular example, it wouldnï¿½ï¿½ï¿½t be too hard for the system to recognise that this word keeps cropping up in the results and that it is almost certainly the answer being searched for.
for more complicated questions, a lower certainty would be shown, and possibly multiple results with different levels of certainty.  the user would also be offered the chance to see the sources which the system calculated the results from.
the point of this system is that it simplifies searching.  many times when we use a search engine, weï¿½ï¿½ï¿½re just looking for something really simple or trivial.  returning a long list of results doesnï¿½ï¿½ï¿½t seem lient way of answering the question, even though the answer is almost certainly hidden away in those results.  
just take a look at the google results for the above question to see my point:

the results given don't immediately answer the question - they need to be searched through by the user before the answer they really want is found.  search engines are great directories.  they're really good for giving you more information about a subject, or telling you where to find a service, but they're not so good at answering direct questions.
there are many aspects that would have to be considered when creating the system ï¿½ï¿½ï¿½ for example a websiteï¿½ï¿½ï¿½s accuracy would have to be taken into account when calculating results.
although the system should work well for simple questions, it may be quite a task to make it work for more complicated xample, common misconceptions would need to be handled as a special case.  if the system finds evidence that the userï¿½ï¿½ï¿½s question has a common misconception as an answer, it should either point this out when providing the answer, or even simply disregard the most common answer in favour of the one provided by the website that points out that it is a common misconception.  this would all have to be weighed up by comparing the accuracy and quality of conflicting sources.
it's an interesting question and would involve a lot of research, but surely it would be worth the time and effort?  it wouldn't always be right, but it would make simple queries a lot quicker for the user","['search', 'nlp', 'search-engine', 'information-retrieval', 'nlp-question-answering']",5952260,"such a system is called an automatic question answering (qa) system, or a natural language search engine.  it is not to be confused with a social question answering service, where answers are produced by humans. qa is a well studied area, as evidenced by almost a decade of trec qa track publications, but it is one of the more difficult tasks in the field of natural language processing (nlp) because it requires a wide range of intelligence (parsing, search, information extraction, coreference, inference).  this may explain why there are relatively few freely available online systems today, most of which are more like demos.  several include:

answerbus
start - mit
qualim - microsoft
textmap - isi
asked!
wolfram alpha

major search engines have shown interest in question answering technology. in an interview on jun 1, 2011, eric scmidt said, googleï¿½ï¿½ï¿½s new strategy for search is to provide answers, not just links. ""'we can literally compute the right answer,' said schmidt, referencing advances in artificial intelligence technology"" (source).
matthew goltzbach, head of products for google enterprise has stated that ""question answering is the future of enterprise search.""  yahoo has also forecasted that the future of search involves users getting real-time answers instead of links.  these big players are incrementally introducing qa technology as a supplement to other kinds of search results, as seen in google's ""short answers"".
while ibm's jeopardy-playing watson has done much to popularize machines answering question (or answers), many real-world challenges remain in the general form of question answering.
see also the related question on open source qa frameworks.
update:

2013/03/14: google and bing search execs discuss how search is evolving to conversational question answering (allthingsd)",https://stackoverflow.com/questions/5941580,search,09-05-2011 19:34,11050.0,10.0,5.0,True,16-08-2023 14:38,23-12-2013 21:06
73847703,why do i need to specify vectors (en_core_web_lg) in spacy config file when i run model training using blank model?,"here is the start of my config file:
# this is an auto-generated partial config. to use it with 'spacy train'
# you can run spacy init fill-config to auto-fill all default settings:
# python -m spacy init fill-config ./base_config.cfg ./config.cfg
[paths]
train = null
dev = null
vectors = ""en_core_web_lg""

but i am training my model on my own labels and spans.
what are the vectors = ""en_core_web_lg"" used for?
after all i am using the following logic to train my model:
# load a new spacy model:
nlp = spacy.blank(""en"")
# create a docbin object:
db = docbin()
for text, annotations in input: # data in previous format
    doc = nlp(text)
    ents = []
    spans = []
    for start, end, label in annotations: # add character indexes
        spans.append(span(doc, 0, len(doc), label=label))
        span = doc.char_span(start, end, label=label)
        ents.append(span)
        doc.ents = ents # label the text with the ents
        group = spangroup(doc, name=""sc"", spans=spans)
        doc.spans[""sc""] = group
        db.add(doc)
db.to_disk(output_path)

please explain where these vectors are used in such configuration?
consider i have a list of annotated data in the format of [(text_1, [(start_1, end_1, label_1)]), (text_2, [(start_2, end_2, label_1)]).....]","['python-3.x', 'spacy', 'named-entity-recognition', 'spancat']",73851381,the static word vectors are included as a tok2vec feature if you have include_static_vectors = true in the tok2vec config.,https://stackoverflow.com/questions/73847703,python-3.x,25-09-2022 20:37,596.0,1.0,1.0,True,26-09-2022 08:25,25-09-2022 20:43
75664012,"i want to make an ai text classifier using openai api, based on gpt2 but i cannot find the api documentation for the gpt2","i wanted to create an ai text classifier project for my college, i wanted to use gpt2 api for the same as it is more reliable to catch the content generated by gpt 3.5, so how can i use gpt2 documentation? also any useful resources for the same are welcome
i tried going through model section of the documentation but couldn't find for gpt2, there's only for gpt 3.5","['machine-learning', 'artificial-intelligence', 'openai-api', 'language-model', 'gpt-2']",75768183,"gpt-2 is not available through the openai api, only gpt-3 and above so far. i would recommend accessing the model through the huggingface transformers library, and they have some documentation out there but it is sparse. there are some tutorials you can google and find, but they are a bit old, which is to be expected since the model came out years ago now. also, what do you mean by, ""i wanted to use gpt2 api for the same as it is more reliable to catch the content generated by gpt 3.5""?  by all accounts, gpt-3 should be much better than gpt-2 at text classification, and by signing up for a free trial with openai you can use their api for free (with provided credits for only three months). if you want to train the gpt-2 xl model, you will probably get better results than gpt-3's ada, but then you have compute resources you have to worry about.",https://stackoverflow.com/questions/75664012,machine-learning,07-03-2023 15:33,827.0,-1.0,1.0,True,24-03-2023 18:53,07-03-2023 15:36
78136056,torchtext functions in newest version analogue,"good day all, i'm trying to solve task, where it was used previously torchtext.dataset.translationdataset, torch.data.field and torch.data.bucketiterator. but, after updating they were removed and i don't know how it can be used now. has anybody faced with this problem, how have you solved it? i'd be the greatful, if you share a link with advice if it's possible.
i've tried to read docs and find info on the internet, but unsuccessfully. it always cite torchtext.legacy, but it doesn't have it in up-to-date versions. i understand, that i can download to it's version, but don't want this yet.","['nlp', 'version', 'torchtext']",78151058,"i think these are the imports in the newer version of torch and their latest documentation

torchtext.dataset.translationdataset -> torchtext.datasets.translationdataset
torch.data.field -> torchtext.data.field
torch.data.bucketiterator -> torchtext.data.bucketiterator

let me know if you have trouble importing them",https://stackoverflow.com/questions/78136056,nlp,10-03-2024 13:36,157.0,1.0,1.0,True,13-03-2024 03:11,13-03-2024 03:07
59719477,execute nltk.stem.snowballstemmer in pandas,"i have a four column dataframe with two columns of tokenized words that have had stop words removed and converted to lower case and am now attempting to stem.  

i'm not sure if the apply() method accesses the series plus its individual cells or if i need another way of stepping into each record so tried both (i think!)
from nltk.stem import snowballstemmer
stemmer = nltk.stem.snowballstemmer('english')

i've tried:

df_2['headline'] = df_2['headline'].apply(lambda x: stemmer.stem(item) for item in x)

--------------------------------------------------------------------------- typeerror                                 traceback (most recent call
  last)  in ()
  ----> 1 df_2['headline__'] = df_2['headline'].apply(lambda x: stemmer.stem(item) for item in x)
~\appdata\local\continuum\anaconda3\envs\learn-env\lib\site-packages\pandas\core\series.py
  in apply(self, func, convert_dtype, args, **kwds)    3192
  else:    3193                 values = self.astype(object).values
  -> 3194                 mapped = lib.map_infer(values, f, convert=convert_dtype)    3195     3196         if len(mapped) and
  isinstance(mapped[0], series):
pandas/_libs/src\inference.pyx in pandas._libs.lib.map_infer()
typeerror: 'generator' object is not callable

i believe this typeerror is similar to the one that says 'list' object is not callable and fixed that one with the apply() method and out of ideas here.  
df_2['headline'] = df_2['headline'].apply(lambda x: stemmer.stem(x))

--------------------------------------------------------------------------- attributeerror                            traceback (most recent call
  last)  in ()
  ----> 1 df_2['headline'] = df_2['headline'].apply(lambda x: stemmer.stem(x))
        2 
        3 df_2.head()
~\appdata\local\continuum\anaconda3\envs\learn-env\lib\site-packages\pandas\core\series.py
  in apply(self, func, convert_dtype, args, **kwds)    3192
  else:    3193                 values = self.astype(object).values
  -> 3194                 mapped = lib.map_infer(values, f, convert=convert_dtype)    3195     3196         if len(mapped) and
  isinstance(mapped[0], series):
pandas/_libs/src\inference.pyx in pandas._libs.lib.map_infer()
 in (x)
  ----> 1 df_2['headline'] = df_2['headline'].apply(lambda x: stemmer.stem(x))
        2 
        3 df_2.head()
~\appdata\local\continuum\anaconda3\envs\learn-env\lib\site-packages\nltk\stem\snowball.py
  in stem(self, word)    1415     1416         """"""
  -> 1417         word = word.lower()    1418     1419         if word in self.stopwords or len(word) <= 2:
attributeerror: 'list' object has no attribute 'lower'","['python', 'pandas', 'nlp', 'nltk']",59719865,"you need to specify the axis for the apply.
here is a full working example:
import pandas as pd

df = pd.dataframe({
    'col_1' : [['ducks'], ['dogs']],
    'col_2' : [['he', 'eats', 'apples'], ['she', 'has', 'cats', 'dogs']],
    'col_3' : ['some data 1', 'some data 2'],
    'col_4' : ['another data 1', 'another data 2']
})
df.head()

output
    col_1   col_2                   col_3       col_4
0   [ducks] [he, eats, apples]      some data 1 another data 1
1   [dogs]  [she, has, cats, dogs]  some data 2 another data 2

now let us apply stemming for the tokenized columns:
import nltk
from nltk.stem import snowballstemmer
stemmer = nltk.stem.snowballstemmer('english')

df.col_1 = df.apply(lambda row: [stemmer.stem(item) for item in row.col_1], axis=1)
df.col_2 = df.apply(lambda row: [stemmer.stem(item) for item in row.col_2], axis=1)

check the new content of the dataframe. 
df.head()

output
    col_1   col_2                   col_3       col_4
0   [duck]  [he, eat, appl]         some data 1 another data 1
1   [dog]   [she, has, cat, dog]    some data 2 another data 2",https://stackoverflow.com/questions/59719477,python,13-01-2020 15:24,1316.0,2.0,2.0,True,15-06-2023 18:50,13-01-2020 18:16
54959340,nltk language modeling confusion,"i want to train a language model using nltk in python but i got into several problems.
first of all, i don't know why my words turn into just characters as i write something like this :
s = ""natural-language processing (nlp) is an area of computer science "" \
""and artificial intelligence concerned with the interactions "" \
""between computers and human (natural) languages.""
s = s.lower();


paddedline = pad_both_ends(word_tokenize(s),n=2);

train, vocab = padded_everygram_pipeline(2, paddedline)
print(list(vocab))
lm = mle(2);
lm.fit(train,vocab)

and the printed vocab is something like this that is clearly not correct(i don't want to work with characters!),this is part of output.:
<s>', '<', 's', '>', '</s>', '<s>', 'n', 'a', 't', 'u', 'r', 'a', 'l', '-', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', '</s>', '<s>', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', '</s>', '<s>', '(', '</s>', '<s>', 'n', 'l', 'p', '</s>', '<s>', ')', '</s>'

why my input turns into characters?
i did this work in another way but with no luck :
paddedline = pad_both_ends(word_tokenize(s),n=2);
#train, vocab = padded_everygram_pipeline(2, tokens)
#train = everygrams(paddedline,max_len = 2);

train = ngrams(paddedline,2);
vocab = vocabulary(paddedline,unk_cutoff = 1);
print(list(train))

lm = mle(2);
lm.fit(train,vocab)

when i run this code my train is absolute nothing,empty! it shows me ""[]"" !!
wired thing is when i comment at this line from above code:
vocab = vocabulary(paddedline,unk_cutoff = 1);

now my train data is ok and something like this that is correct :
[('<s>', 'natural-language'), ('natural-language', 'processing'), ('processing', '('), ('(', 'nlp'), ('nlp', ')'), (')', 'is'), ('is', 'an'), ('an', 'area'), ('area', 'of'), ('of', 'computer'), ('computer', 'science'), ('science', 'and'), ('and', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'concerned'), ('concerned', 'with'), ('with', 'the'), ('the', 'interactions'), ('interactions', 'between'), ('between', 'computers'), ('computers', 'and'), ('and', 'human'), ('human', '('), ('(', 'natural'), ('natural', ')'), (')', 'languages'), ('languages', '.'), ('.', '</s>')]

whats wrong with it? 
by the way, i have to say that i'm not an expert in python or nltk and it's my first experience.
the next question is how can i use kneser-ney smoothing or add-one smoothing on the training language model? 
and am i doing language model training the right way?
my training data is simple :
""natural-language processing (nlp) is an area of computer science "" \
    ""and artificial intelligence concerned with the interactions "" \
    ""between computers and human (natural) languages.""

thanks.","['python', 'machine-learning', 'nlp', 'nltk']",54961142,"the padded_everygram_pipeline function expects a list of list of n-grams. you should fix your first code snippet as follows. also python generators are lazy sequences, you can't iterate them more than once.
from nltk import word_tokenize
from nltk.lm import mle
from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline

s = ""natural-language processing (nlp) is an area of computer science "" \
    ""and artificial intelligence concerned with the interactions "" \
    ""between computers and human (natural) languages.""
s = s.lower()

paddedline = [list(pad_both_ends(word_tokenize(s), n=2))]

train, vocab = padded_everygram_pipeline(2, paddedline)

lm = mle(2)

lm.fit(train, vocab)

print(lm.counts)",https://stackoverflow.com/questions/54959340,python,02-03-2019 14:03,4988.0,1.0,2.0,True,03-06-2022 00:27,05-01-2020 13:58
71828322,ir: how do you match documents based on index values and return the document?,"i have a pandas data frame df which has the top 10 documents from a corpus ranked based on their bm25 score, and indexed by their doc_id.




doc_id
rank
bm25 score




1234
1
3.3472


5678
2
3.3238




i also have a list documents containing all of the documents paired up with their doc_id, such that the list is in the following form: [['first doc_id', 'first doc text], ['second doc_id', 'second doc text], ...].
i need to take the doc_id for the top 3 ranked documents in df, and match each one with the corresponding doc_id in documents and print out the document text. i know how to get the doc_id for a particular rank from df by doing df.index[df['rank'] == 1][0], but i'm unsure how to go from there to get the corresponding document text.","['python', 'pandas', 'dataframe', 'information-retrieval']",71828542,"you can convert your list to dataframe and merge:
documents = [[1234, 'first doc text'],
             [5678, 'second doc text'],
             [5679, 'third doc text'],
             [5680, 'fourth doc text']]

(df[df['rank'].le(3)]
 .merge(pd.dataframe(documents,
                     columns=['doc_id', 'text']),
        on='doc_id')
)

output:
   doc_id  rank  bm25 score             text
0    1234     1      3.3472   first doc text
1    5678     2      3.3238  second doc text
2    5679     3      3.2000   third doc text

used input:
   doc_id  rank  bm25 score
0    1234     1      3.3472
1    5678     2      3.3238
2    5679     3      3.2000
3    5680     4      3.1000

alternatively, if you want a list using python:
top3 = set(df.loc[df['rank'].le(3), 'doc_id'])
out = [text for id, text in documents if id in top3]

output: ['first doc text', 'second doc text', 'third doc text']",https://stackoverflow.com/questions/71828322,python,11-04-2022 13:00,224.0,-1.0,1.0,True,11-04-2022 13:39,11-04-2022 13:39
68048737,training spacy - nameerror,"i need to train a spacy model to improve the accuracy to identify products. i'm struggling with training my spacy model. i have the following code:
train_data = [('..., {'entities': [(36,55,'product')]})]
nlp = spacy.load(""en_core_web_lg"")
ner = nlp.get_pipe(""ner"")

optimizer = nlp.create_optimizer()
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""]

with nlp.disable_pipes(*other_pipes): # only train ner
    for itn in range(50):
        random.shuffle(train_data)
        losses = {}
        for text, annotations in train_data:
            doc = nlp.make_doc(text)
            example = example.from_dict(doc, annotations)
            nlp.update([example], drop=0.25, sgd=optimizer, losses=losses)

but it's failing due to:
nameerror                                 traceback (most recent call last)
<ipython-input-4-903f2be7114f> in <module>
     15         for text, annotations in train_data:
     16             doc = nlp.make_doc(text)
---> 17             example = example.from_dict(doc, annotations)
     18             nlp.update([example], drop=0.25, sgd=optimizer, losses=losses)
     19 print(losses)

nameerror: name 'example' is not defined

how do i need to define example?","['python', 'nlp', 'spacy']",68049966,"it's hot here...
thanks for the hint i missed importing:
from spacy.training import example
when moving the code from jupyter to visual studio code for the deployment",https://stackoverflow.com/questions/68048737,python,19-06-2021 16:22,1083.0,-1.0,2.0,True,01-11-2022 00:30,19-06-2021 18:48
67664837,problems understanding ndcg format in pytrec_eval?,"i am using pytrec_eval to calculate ndcg scores. for example, for qrel:
qrel = {
    'q1': {
        'd1': 0,
        'd2': 1,
        'd3': 0,
    }
}

and run:
run = {
    'q1': {
        'd1': 1.0,
        'd2': 0.0,
        'd3': 1.5,
    }
}

the ndcg score can be calculated like this:
import pytrec_eval
import json    
evaluator = pytrec_eval.relevanceevaluator(
    qrel, {'ndcg'})

print(json.dumps(evaluator.evaluate(run), indent=1))


 ""q1"": {
  ""ndcg"": 0.5
 }

my understanding is that ndcg takes into account the order of the retrieved documents indices, however, if you change the document ordering in the run you still get the same ndcg score, for example:
run2 = {
    'q1': {
        'd1': 1.0,
        'd3': 1.5,
        'd2': 0.0,

    }
}

evaluator = pytrec_eval.relevanceevaluator(qrel, {'ndcg'})
print(json.dumps(evaluator.evaluate(run2), indent=1))

is this the expected behavior of calculating ndcg? what is the usage of the qrel? my undertanding is that the qrel tells you how relevant is the retrieved document, while run, is the resulting ranking of your query and ir system. then, why if i change the order of run the ndcg score is the same?","['python-3.x', 'machine-learning', 'nlp', 'information-retrieval']",75764096,"in order to compute the ndcg, you need to know what is the relevance of each document in a ranked list of results for this query. this information is contained in qrels. ranking means that you first need to sort retrieved documents in descending order of their score. so you basically sort documents, and then go rank by rank, starting from the lowest rank (i.e. the top-scored document), which is 1. for each rank i you get the document's ground-truth relevance rel_i from qrels, and then you divide this relevance by log_2(i+1) to get a term for this rank i. you sum all these terms across all ranks, and you get the discounted cumulative gain (dcg) for this query.
therefore, pytrec_eval internally needs to create a sorted list from the dictionary mapping from doc id to score, in order to get the ranks. this is why the order of the document-score pairs in the dictionary you pass as an input doesn't matter. now, as an additional detail: to get the ndcg (i.e. normalized dcg), you divide the dcg by the ideal dcg, which is the maximum dcg achievable by any model; to get the idcg, you sort the ground-truth (as opposed to retrieved) documents in descending order of their relevance score and compute the dcg. again, to get the ground-truth relevance scores you need qrels.",https://stackoverflow.com/questions/67664837,python-3.x,23-05-2021 22:10,713.0,3.0,1.0,True,17-03-2023 05:40,23-05-2021 22:44
71607906,understanding gpu usage huggingface classification - total optimization steps,"i am training huggingface longformer for a classification problem and got below output.

i am confused about total optimization steps. as i have 7000 training data points and 5 epochs and total train batch size (w. parallel, distributed & accumulation) = 64, shouldn't i get
7000*5/64 steps? that comes to 546.875? why is it showing  total optimization steps = 545

why in the below output, there are 16 steps of input ids are automatically padded from 1500 to 1536 to be a multiple of config.attention_window: 512 then  [ 23/545 14:24 < 5:58:16, 0.02 it/s, epoch 0.20/5]? what are these steps?


==========================================================
***** running training *****
  num examples = 7000
  num epochs = 5
  instantaneous batch size per device = 4
  total train batch size (w. parallel, distributed & accumulation) = 64
  gradient accumulation steps = 16
  total optimization steps = 545
initializing global attention on cls token...
input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
initializing global attention on cls token...
input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
initializing global attention on cls token...
input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
initializing global attention on cls token...
input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
initializing global attention on cls token...
input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
initializing global attention on cls token...
input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
initializing global attention on cls token...
input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
initializing global attention on cls token...
input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
initializing global attention on cls token...
input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
initializing global attention on cls token...
input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
initializing global attention on cls token...
input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
initializing global attention on cls token...
input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
initializing global attention on cls token...
input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
initializing global attention on cls token...
input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
initializing global attention on cls token...
input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
initializing global attention on cls token...
input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
 [ 23/545 14:24 < 5:58:16, 0.02 it/s, epoch 0.20/5]
epoch   training loss   validation loss


#update
adding trainer and trainingarguments
#class weights
class customtrainer(trainer):
    def compute_loss(self, model, inputs, return_outputs=false):
        labels = inputs.get(""labels"")
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get(""logits"")
        # compute custom loss (suppose one has 3 labels with different weights)
        loss_fct = nn.crossentropyloss(weight=torch.tensor([1.0, 0.5243])).to(device)
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1)).to(device)
        return (loss, outputs) if return_outputs else loss

 trainer = customtrainer(
        model=model,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=train_df_tuning_dataset_tokenized,
        eval_dataset=val_dataset_tokenized
    )



# define the training arguments
training_args = trainingarguments(
    
    
num_train_epochs = 5,# changed this from 5
per_device_train_batch_size = 4,#4,#8,
gradient_accumulation_steps = 16,
per_device_eval_batch_size= 16,#16
evaluation_strategy = ""epoch"",

save_strategy = ""epoch"",
learning_rate=2e-5,
load_best_model_at_end=true,
greater_is_better=false,

disable_tqdm = false, 

weight_decay=0.01,
optim=""adamw_torch"",#removing on 18 march from huggingface example notebook
run_name = 'longformer-classification-16march2022'
)","['python', 'nlp', 'gpu', 'huggingface-transformers']",71655154,"1. why 545 optimization steps?
looking at the implementation of the transformers package, we see that the trainer uses a variable called max_steps when printing the total optimization steps message in the train method:
logger.info(""***** running training *****"")
logger.info(f""  num examples = {num_examples}"")
logger.info(f""  num epochs = {num_train_epochs}"")
logger.info(f""  instantaneous batch size per device = {args.per_device_train_batch_size}"")
logger.info(f""  total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size}"")
logger.info(f""  gradient accumulation steps = {args.gradient_accumulation_steps}"")
logger.info(f""  total optimization steps = {max_steps}"")

permalink to the above snippet in the transformers repo
the trainer has the following bit of code earlier in the train method:
class trainer:
    [...]
    def train(self) -> none:
        [some irrelevant code ommited here...]

        total_train_batch_size = args.train_batch_size * args.gradient_accumulation_steps * args.world_size
        if train_dataset_is_sized:
            num_update_steps_per_epoch = len(train_dataloader) // args.gradient_accumulation_steps
            num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)
            if args.max_steps > 0:
                max_steps = args.max_steps
                num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(
                    args.max_steps % num_update_steps_per_epoch > 0
                )
                # may be slightly incorrect if the last batch in the training datalaoder has a smaller size but it's
                # the best we can do.
                num_train_samples = args.max_steps * total_train_batch_size
            else:
                max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)
                num_train_epochs = math.ceil(args.num_train_epochs)
                num_train_samples = len(self.train_dataset) * args.num_train_epochs

permalink to the above snippet in the transformers repo
total_train_batch_size = args.train_batch_size * args.gradient_accumulation_steps * args.world_size in your example will be equal to total_train_batch_size = 4 * 16 * 1 = 64, as expected.
then we have num_update_steps_per_epoch = len(train_dataloader) // args.gradient_accumulation_steps which will give us num_update_steps_per_epoch = len(train_dataloader) // 16.
now the length of a dataloader is equal to the number of batches in that dataloader. since you have 7000 samples and we have a per_device_train_batch_size of 4, this will give us 7000 / 4 = 1750 batches. going back to num_update_steps_per_epoch we now have num_update_steps_per_epoch = 1750 // 16 = 109 (python integer division takes the floor)
you don't have a number of max steps specified so then we get to max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch) which gives us max_steps = math.ceil(5 * 109) = 545.
2. why does the padding operation get logged 16 times?
in a transformers architecture, you technically don't have to pad all your samples to be the same length. what actually matters is that samples within a batch are the same length, that length can differ from batch to batch.
this means that this message will appear for every batch that goes through a forward pass. as to why the message appeared 16 times even though 23 batches have actually gone through a forward pass i can think of two possible reasons:

the logging of the padding operation and the logging of the progress bar are happening on two different threads and the former is lagging behind a bit
(extremely unlikely) you had batches that did not need to be padded because all samples had the same length and that length was a multiple of 512 already.",https://stackoverflow.com/questions/71607906,python,24-03-2022 18:46,1619.0,3.0,1.0,True,29-03-2022 00:30,28-03-2022 23:03
76305454,pyspark use documentassembler on array&lt;string&gt;,"i am trying to use documentassembler for array of strings. the documentation says: ""the documentassembler can read either a string column or an array[string])"".
but when i do a simple example:
data = spark.createdataframe([[[""spark nlp is an open-source text processing library.""]]]).todf(""text"")
documentassembler = documentassembler().setinputcol(""text"").setoutputcol(""document"")
result = documentassembler.transform(data)

result.select(""document"").show(truncate=false)

i am getting an error
analysisexception: [cannot_up_cast_datatype] cannot up cast input from ""array<string>"" to ""string"".
the type path of the target object is:
- root class: ""java.lang.string""
you can either add an explicit cast to the input data or choose a higher precision type of the field in the target object

maybe i don't understand something?","['apache-spark', 'pyspark', 'apache-spark-sql', 'nlp', 'johnsnowlabs-spark-nlp']",76306399,"i think you just added an extra [] around the input
this is working:
data = spark.createdataframe([[""spark nlp is an open-source text processing library.""]]).todf(""text"")
documentassembler = documentassembler().setinputcol(""text"").setoutputcol(""document"")
result = documentassembler.transform(data)

result.select(""document"").show(truncate=false)

+----------------------------------------------------------------------------------------------+
|document                                                                                      |
+----------------------------------------------------------------------------------------------+
|[{document, 0, 51, spark nlp is an open-source text processing library., {sentence -> 0}, []}]|
+----------------------------------------------------------------------------------------------+",https://stackoverflow.com/questions/76305454,apache-spark,22-05-2023 10:55,273.0,0.0,1.0,True,22-05-2023 12:52,22-05-2023 11:13
59327637,how do i train gpt 2 from scratch?,"i want to train gpt 2 from scratch but there is only fine-tuning approach based on pretrained models in articles i found.
i've used this  for train with existing model. should i edit these python scripts to train from scratch?","['python', 'machine-learning', 'nlp', 'nlg']",59356838,"i found the answer in 

if you want to not use the released model at all, for instance because
you want to train a model with incompatible hyperparameters, it should
be sufficient to just skip the restore from the released model
checkpoint (around train.py:164-177) on your first run so the
parameters will all be randomly initialized.",https://stackoverflow.com/questions/59327637,python,13-12-2019 17:57,4509.0,8.0,1.0,True,27-05-2024 17:24,22-12-2019 13:00
75222608,how to force openai to save the context of the whole chat?,"how do you maintain historical context in repeat api calls?
i was searching that question but has found 0 answers
so i found this demo chat: 
and its source code: 
it can save context of conversation for real, but can anyboy tell me how?",['openai-api'],75230098,"the only one solution is the send to api all conversation history that must be under 3500 symbols, cuz u need around 500 points to make answer
as example in json format:
{""messages"":[""user:hello"",""bot:hello there!"",""user:whats your name"",""bot:my name is bot. what's yours?"",""user:now spell your name backwards pls"",""bot:tob"",""user:and now capitalise letters"",""bot:tob""]}

but in the end you must add ""bot:"" to suggest ai that he must answer by his role now.
example works very well",https://stackoverflow.com/questions/75222608,openai-api,24-01-2023 14:02,989.0,0.0,1.0,True,31-01-2023 12:35,27-01-2023 10:58
71108243,valueerror: requesting 5-fold cross-validation but provided less than 5 examples for at least one class,"i have been training a text classifier to then later use to predict characters of a tv show. so far, my code looks like:
vectorizer = tfidfvectorizer(ngram_range=(1,2),min_df=0.001, max_df=0.75,stop_words='english')
x = vectorizer.fit_transform(data['text'])
y = data['character']
print(x.shape, y.shape) #prints (5999, 1429) (5999,)

# get baseline performance
kf = kfold(n_splits=5)
most_frequent = dummyclassifier(strategy='most_frequent')
print(cross_val_score(most_frequent , x, y=y, cv=kf, n_jobs= -1, scoring=""accuracy"").mean())

# fine-tune classifier
base_clf = calibratedclassifiercv(cv=kf, base_estimator=logisticregression(n_jobs= -1, solver='lbfgs' ))

param_grid = {'base_estimator__c': [0.01, 0.05, 0.1, 0.5, 1.0, 10, 20, 50],
'base_estimator__class_weight': ['balanced', 'auto']}

search = gridsearchcv(base_clf, param_grid, cv=kf, scoring='f1_micro')
search.fit(x, y)

# use best classifier to get performance estimate
clf = search.best_estimator_.base_estimator
print(cross_val_score(clf, x, y=y, cv=kf, n_jobs= -1, scoring='f1_micro').mean())

however, i keep getting the following error:
valueerror                                traceback (most recent call last)
/var/folders/fv/h7n33cb5227g4t5lxym8g_800000gn/t/ipykernel_2208/2611717736.py in <module>
      6 
      7 search = gridsearchcv(base_clf, param_grid, cv=kf, scoring='f1_micro')
----> 8 search.fit(x, y)
      9 
     10 # use best classifier to get performance estimate

~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py in inner_f(*args, **kwargs)
     61             extra_args = len(args) - len(all_args)
     62             if extra_args <= 0:
---> 63                 return f(*args, **kwargs)
     64 
     65             # extra_args > 0

~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py in fit(self, x, y, groups, **fit_params)
    878             refit_start_time = time.time()
    879             if y is not none:
--> 880                 self.best_estimator_.fit(x, y, **fit_params)
    881             else:
    882                 self.best_estimator_.fit(x, **fit_params)

~/opt/anaconda3/lib/python3.9/site-packages/sklearn/calibration.py in fit(self, x, y, sample_weight)
    301             if n_folds and np.any([np.sum(y == class_) < n_folds
    302                                    for class_ in self.classes_]):
--> 303                 raise valueerror(f""requesting {n_folds}-fold ""
    304                                  ""cross-validation but provided less than ""
    305                                  f""{n_folds} examples for at least one class."")

valueerror: requesting 5-fold cross-validation but provided less than 5 examples for at least one class.

i am not quite sure how to resolve this error and would truly appreciate any advice.
thank you in advance!","['python', 'scikit-learn', 'svm', 'cross-validation', 'text-classification']",71113296,you need to check the distribution of your target value data['character'] : it seems that the number of values in one of the classes in the target column is too small. to do it you can use : data['character'].value_counts(),https://stackoverflow.com/questions/71108243,python,14-02-2022 07:09,584.0,0.0,1.0,True,14-02-2022 14:14,14-02-2022 07:43
65851441,error in installing &quot;topicmodels&quot; package in google colab,"since i need more computational resources, i started running my r code on google colab. i have no problem with installing most of the packages i need, but for the ""topicmodels"" package when i run the code below:
install.packages('topicmodels', repos='

i get the following error message on google colab:
installing package into ï¿½ï¿½ï¿½/usr/local/lib/r/site-libraryï¿½ï¿½ï¿½
(as ï¿½ï¿½ï¿½libï¿½ï¿½ï¿½ is unspecified)

warning message in install.packages(""topicmodels"", repos = ""
ï¿½ï¿½ï¿½installation of package ï¿½ï¿½ï¿½topicmodelsï¿½ï¿½ï¿½ had non-zero exit statusï¿½ï¿½ï¿½

can any","['r', 'google-colaboratory', 'topic-modeling']",65852081,"try running this in a code cell before the installation of the topicmodels package.
system2('sudo', 'apt-get install libgsl0-dev')

this installs a required library in the unix environment of colab that you would normally install from a command prompt like this.
sudo apt-get install libgsl0-dev

in python notebooks, you would do this.
!sudo apt-get install libgsl0-dev

but this doesn't seem to work in r notebooks so the system2 call does the work.",https://stackoverflow.com/questions/65851441,r,22-01-2021 19:17,887.0,2.0,1.0,True,02-03-2024 10:31,02-03-2024 10:31
70583938,how replace a dot (.) in sentence except when it appears in an abbreviation using regular expression,"i want to replace every dot with a space in a sentence except when it is used with an abbreviation. when it is used with an abbreviation, i want to replace it with '' null.
abbreviation means a dot surrounded at least two capital letters.
my regex are working except they catch u.s.
r1 = r'\b((?:[a-z]\.){2,})\s*'
r2 = r'(?:[a-z]\.){2,}'

'u.s.a is abbr  x.y  is not. but i.i.t. is also valid abbvr and so is m.tech'

should become

'usa is abbr  x y  is not but iit is also valid abbvr and so is mtech'

update: it should not be considering any numbers or special characters.
x.2 -> x 2
x. -> x 
x.* -> x -","['python', 'regex', 'nlp', 'data-cleaning', 'python-re']",70584011,"you can use
import re
s='u.s.a is abbr  x.y  is not. but i.i.t. is also valid abbvr and so is m.tech, x.2, x., x.*'
print(re.sub(r'(?<=[a-z])(\.)(?=[a-z])|\.', lambda x: '' if x.group(1) else ' ', s))
# =>  usa is abbr  x y  is not  but iit  is also valid abbvr and so is mtech, x 2, x , x *

see the python demo. here is a regex demo. it matches

(?<=[a-z])(\.)(?=[a-z]) - group 1: a . char that is immediately preceded and followed with an uppercase ascii letter
| - or
\. - a dot (in any other context)

if group 1 matches, the replacement is an empty string, else, the replacement is a space.
to make it unicode-aware, install pypi regex library (pip install regex) and use
import regex
s='u.s.a is abbr  x.y  is not. but i.i.t. is also valid abbvr and so is m.tech, x.2, x., x.*'
print(regex.sub(r'(?<=\p{lu})(\.)(?=\p{lu})|\.', lambda x: '' if x.group(1) else ' ', s))

the \p{lu} matches any unicode uppercase letter.",https://stackoverflow.com/questions/70583938,python,04-01-2022 19:15,344.0,1.0,1.0,True,06-01-2022 17:26,06-01-2022 05:58
75060885,gpu out of memory when fastapi is used with sentencetransformers inference,"i'm currently using fastapi with gunicorn/uvicorn as my server engine. inside fastapi get method i'm using sentencetransformer model with gpu:
# ...

from sentence_transformers import sentencetransformer

encoding_model = sentencetransformer(model_name, device='cuda')

# ...
app = fastapi()

@app.get(""/search/"")
def encode(query):
    return encoding_model.encode(query).tolist()

# ...

def main():
    uvicorn.run(app, host=""127.0.0.1"", port=8000)


if __name__ == ""__main__"":
    main()

i'm using the following config for gunicorn:
timeout 0
graceful_timeout 120
keep_alive 5
workers 10

uvicorn has all default settings, and is started in docker container casually:
cmd [""uvicorn"", ""app.main:app"", ""--host"", ""0.0.0.0"", ""--port"", ""8000""]

so, inside docker container i have 10 gunicorn workers, each using gpu.
the problem is the following:
after some load my api fails with the following message:
torch.cuda.outofmemoryerror: cuda out of memory. 
tried to allocate 734.00 mib 
(gpu 0; 15.74 gib total capacity; 
11.44 gib already allocated; 
189.56 mib free; 
11.47 gib reserved in total by pytorch) 
if reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  see documentation for memory management and pytorch_cuda_alloc_conf","['python', 'pytorch', 'fastapi', 'sentence-transformers']",75469807,"the problem was that there were 10 replicas of my transformer model on gpu, as @chris mentioned above.
my solution was to use celery as rpc manager (rabbitmq+redis backend setup) and a separate container for gpu-bound computations, so now there is only one instance of my model on gpu, and no race between different processes' models.",https://stackoverflow.com/questions/75060885,python,09-01-2023 17:23,2457.0,2.0,1.0,True,16-02-2023 08:53,09-01-2023 22:03
77341456,why does my transformer model have more parameters than the huggingface implementation?,"i'm loading a gpt model from huggingface as follows:
from transformers import autotokenizer, gpt2lmheadmodel, autoconfig

config = autoconfig.from_pretrained(
    ""gpt2"",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)

standard_gpt2 = gpt2lmheadmodel(config).to(device)
standard_gpt2_model_size = sum(t.numel() for t in standard_gpt2.parameters())
print(f""gpt-2 size: {standard_gpt2_model_size/1000**2:.1f}m parameters"")
# >>> gpt-2 size: 124.4m parameters

if i print the model architecture i get:
gpt2lmheadmodel(
  (transformer): gpt2model(
    (wte): embedding(50257, 768)
    (wpe): embedding(1024, 768)
    (drop): dropout(p=0.1, inplace=false)
    (h): modulelist(
      (0-11): 12 x gpt2block(
        (ln_1): layernorm((768,), eps=1e-05, elementwise_affine=true)
        (attn): gpt2attention(
          (c_attn): conv1d()
          (c_proj): conv1d()
          (attn_dropout): dropout(p=0.1, inplace=false)
          (resid_dropout): dropout(p=0.1, inplace=false)
        )
        (ln_2): layernorm((768,), eps=1e-05, elementwise_affine=true)
        (mlp): gpt2mlp(
          (c_fc): conv1d()
          (c_proj): conv1d()
          (act): newgeluactivation()
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
    )
    (ln_f): layernorm((768,), eps=1e-05, elementwise_affine=true)
  )
  (lm_head): linear(in_features=768, out_features=50257, bias=false)
)

focusing on the last layer -- lm_head, it has in_features=768, out_features=50257
so why when i replace just that one layer with the exact same number of parameters i get different results?
standard_gpt2.lm_head = nn.sequential(
    nn.linear(in_features = 768, out_features = 50257, bias=false)
)
standard_gpt2_model_size = sum(t.numel() for t in standard_gpt2.parameters())
print(f""gpt-2 size: {standard_gpt2_model_size/1000**2:.1f}m parameters"")        
# >>> gpt-2 size: 163.0m parameters","['machine-learning', 'pytorch', 'nlp', 'huggingface-transformers', 'huggingface']",77341769,"that is because the linear layer of lm_head doesn't have separate weights. it shares its weight tensor with the token embedding layer. you can confirm this with data-ptr, which returns the address of the first element of the tensor:
from torch import nn
from transformers import autotokenizer, gpt2lmheadmodel, autoconfig

model_id = ""gpt2""

tokenizer = autotokenizer.from_pretrained(model_id)

standard_gpt2 = gpt2lmheadmodel.from_pretrained(model_id)
standard_gpt2_model_size = sum(t.numel() for t in standard_gpt2.parameters())
print(f""gpt-2 size: {standard_gpt2_model_size} parameters"")

print(f""token embedding layer address {standard_gpt2.transformer.wte.weight.untyped_storage().data_ptr()}"")

print(f""lm_head address {standard_gpt2.lm_head.weight.untyped_storage().data_ptr()}"")

# replacing the default head
standard_gpt2.lm_head = nn.linear(in_features = 768, out_features = 50257, bias=false)
standard_gpt2_model_size = sum(t.numel() for t in standard_gpt2.parameters())
print(f""gpt-2 size after replacing lm_head: {standard_gpt2_model_size} parameters"")

print(f""token embedding layer address after replacing lm_head {standard_gpt2.transformer.wte.weight.untyped_storage().data_ptr()}"")

print(f""lm_head address after replacing lm_head {standard_gpt2.lm_head.weight.untyped_storage().data_ptr()}"")

output:
gpt-2 size: 124439808 parameters
token embedding layer address: 96251233152832
lm_head address: 96251233152832
gpt-2 size after replacing lm_head: 163037184 parameters
token embedding layer address after replacing lm_head: 96251233152832
lm_head address after replacing lm_head: 134800505946176

i assume you want to keep sharing the weights, in this case, you should call something like this after assigning your new head:
standard_gpt2.lm_head = nn.sequential(
    nn.linear(in_features = 768, out_features = 50257, bias=false)
)

standard_gpt2.lm_head[0].weight = standard_gpt2.transformer.wte.weight


standard_gpt2_model_size = sum(t.numel() for t in standard_gpt2.parameters())
print(f""gpt-2 size with tied weights+custom head: {standard_gpt2_model_size} parameters"")

print(f""token embedding layer address with tied weights+custom head: {standard_gpt2.transformer.wte.weight.untyped_storage().data_ptr()}"")

print(f""lm_head address with tied weights+custom head: {standard_gpt2.lm_head[0].weight.untyped_storage().data_ptr()}"")

output:
gpt-2 size: 124439808 parameters
token embedding layer address 134800505946176
lm_head address 134800505946176
gpt-2 size with tied weights+custom head: 124439808 parameters
token embedding layer address with tied weights+custom head: 134800505946176
lm_head address with tied weights+custom head: 134800505946176",https://stackoverflow.com/questions/77341456,machine-learning,22-10-2023 19:43,754.0,1.0,1.0,True,22-10-2023 22:20,22-10-2023 22:20
72713906,how to create a custom spapcy pipeline component using the thinc model,"i'd like to create a custom pipeline component in spacy which uses a pre-trained thinc model. i'd like to modify the output prediction from thinc and then pass the modified value back into the pipeline i.e. effectively modifying the ner pipeline component.

i was thinking of doing this via a custom pipeline component, something like:
from spacy.language import language

@language.component(""my_ner"")
def my_ner(doc):

    class_probabilities = thinc_do_something(data, model, num_samples)
    class_value = np.argmax(class_probabilities, axis=1)
    
    return doc

nlp = spacy.load(""en_core_web_sm"", exclude=[""ner""])
nlp.add_pipe(""my_ner"", after=""parser"")  # insert after the parser
print(nlp.pipe_names)  # ['tagger', 'parser', 'my_ner']
doc = nlp(""this is a sentence."")

my aim is for the pipe to run as per the original ner component, but with my custom ner component modifying the class probabilities. unfortunately i don't understand from the spacy documentation:

how to access the pre trained model from inside the pipeline?
how to access the data used for the model prediction within the pipeline?
where i need to write the model predicted value back to as part of my modified ner pipline?
is there a better way of doing this?","['nlp', 'spacy', 'spacy-3']",72738855,"i have not heard of anyone doing something like that before, and while it is possible, it is not as simple as you suggest. the example component you have is for simple stateless components that are just a function. in order to modify how a trainable pipe works you'd have to make your own pipe, by subclassing an existing one or otherwise.
you should look at existing pipes for reference, the textcat is probably one of the simpler ones. for trainable pipes, when used in a pipeline they basically use predict and set_annotations, as shown in the trainablepipe implementation.
rather than subclassing, it might also be easier to just copy the component you want to use, modify a few bits, and give it a new name.",https://stackoverflow.com/questions/72713906,nlp,22-06-2022 10:32,281.0,1.0,1.0,True,24-06-2022 04:00,22-06-2022 20:28
77930113,openai trouble with api key definition,"i've been trying to make use of langchain openai but i just cant seem to get it to work.
i tried the openai quickstart test:

from openai import openai
client = openai()

completion = client.chat.completions.create(
  model=""gpt-3.5-turbo"",
  messages=[
    {""role"": ""system"", ""content"": ""you are a poetic assistant, skilled in explaining complex programming concepts with creative flair.""},
    {""role"": ""user"", ""content"": ""compose a poem that explains the concept of recursion in programming.""}
  ]
)

print(completion.choices[0].message)


but it would give the error:

 line 98, in __init__
    raise openaierror(
openai.openaierror: the api_key client option must be set either by passing api_key to the client or by setting the openai_api_key environment variable


but after setting the api key like so:
import openai
import os

os.environ[""openai_api_key""] = ""myapikey""

client = openai.openai()

completion = client.chat.completions.create(
  model=""gpt-3.5-turbo"",
  messages=[
    {""role"": ""system"", ""content"": ""you are a poetic assistant, skilled in explaining complex programming concepts with creative flair.""},
    {""role"": ""user"", ""content"": ""compose a poem that explains the concept of recursion in programming.""}
  ]
)

print(completion.choices[0].messages)

it would present me with a massive wall of errors:
    file ""c:\users\jasha\desktop\code\python\gpt\gpt0\gpt0.py"", line 9, in <module>enter code here
        completion = client.chat.completions.create(
      file ""c:\users\jasha\appdata\local\programs\python\python311\lib\site-packages\openai\_utils\_utils.py"", line 271, in wrapper
        return func(*args, **kwargs)
      file ""c:\users\jasha\appdata\local\programs\python\python311\lib\site-packages\openai\resources\chat\completions.py"", line 659, in create
        return self._post(
      file ""c:\users\jasha\appdata\local\programs\python\python311\lib\site-packages\openai\_base_client.py"", line 1180, in post
        return cast(responset, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
      file ""c:\users\jasha\appdata\local\programs\python\python311\lib\site-packages\openai\_base_client.py"", line 869, in request
        return self._request(
      file ""c:\users\jasha\appdata\local\programs\python\python311\lib\site-packages\openai\_base_client.py"", line 945, in _request
        return self._retry_request(
      file ""c:\users\jasha\appdata\local\programs\python\python311\lib\site-packages\openai\_base_client.py"", line 993, in _retry_request
        return self._request(
      file ""c:\users\jasha\appdata\local\programs\python\python311\lib\site-packages\openai\_base_client.py"", line 945, in _request
        return self._retry_request(
      file ""c:\users\jasha\appdata\local\programs\python\python311\lib\site-packages\openai\_base_client.py"", line 993, in _retry_request
        return self._request(
      file ""c:\users\jasha\appdata\local\programs\python\python311\lib\site-packages\openai\_base_client.py"", line 960, in _request
        raise self._make_status_error_from_response(err.response) from none
    openai.ratelimiterror: error code: 429 - {'error': {'message': 'you exceeded your current quota, please check your plan and billing details. for more information on this error, read the docs:  'type': 'insufficient_quota', 'param': none, 'code': 'insufficient_quota'}}","['python', 'openai-api']",77930177,"first you need to create a "".env"" file at your project folder like this
# once you add your api key below, make sure to not share it with anyone! the api key should remain private.
openai_api_key=abc123

then on your code initialization, it reads by default that environment variable:
from openai import openai
client = openai()
# defaults to getting the key using os.environ.get(""openai_api_key"")
# if you saved the key under a different environment variable name, you can do something like:
# client = openai(
#   api_key=os.environ.get(""custom_env_name""),
# )

also make sure you have the library properly installed.
alternatively, you can pass the api key directly but that is not recommended as it exposes it in the code.
!pip install --upgrade pip
from openai import openai
client = openai(api_key='yourapikey')

completion = client.chat.completions.create(
  model=""gpt-3.5-turbo"",
  messages=[
    {""role"": ""system"", ""content"": ""you are a poetic assistant, skilled in explaining complex programming concepts with creative flair.""},
    {""role"": ""user"", ""content"": ""compose a poem that explains the concept of recursion in programming.""}
  ]
)

print(completion.choices[0].message)

after you posted your error log, i finally understood.
it says you have no credits on your account. you need a valid key, or buy credits on your account. check your account usage here.
  raise self._make_status_error_from_response(err.response) from none
    openai.ratelimiterror: error code: 429 - {'error': {'message': 'you exceeded your current quota, please check your plan and billing details. for more information on this error, read the docs:  'type': 'insufficient_quota', 'param': none, 'code': 'insufficient_quota'}}

source",https://stackoverflow.com/questions/77930113,python,02-02-2024 22:15,1338.0,-1.0,1.0,True,02-02-2024 23:03,02-02-2024 22:58
65408563,repeating entity in replacing entity with their entity label using spacy,"code:
import spacy
nlp = spacy.load(""en_core_web_md"")

#read txt file, each string on its own line
with open(""./try.txt"",""r"") as f:
    texts = f.read().splitlines()

#substitute entities with their tags
docs = nlp.pipe(texts)
out = []
for doc in docs:
    out_ = """"
    for tok in doc:
        text = tok.text
        if tok.ent_type_:
            text = tok.ent_type_
        out_ += text + tok.whitespace_
    out.append(out_)

# write to file
with open(""./out_try.txt"",""w"") as f:
    f.write(""\n"".join(out))

contents of input file:
georgia recently became the first u.s. state to ""ban muslim culture.
his friend nicolas j. smith is here with bart simpon and fred.
apple is looking at buying u.k. startup for $1 billion

contents of output file:
gpe recently became the ordinal gpe state to ""ban norp culture.
his friend person person person is here with person person and person.
org is looking at buying gpe startup for moneymoney money

i need to avoid this problem in above sentences. for example in (in sentence 2 'person person person' to become one entity person.","['python', 'nlp', 'spacy', 'named-entity-recognition']",65416050,"lets try:
import spacy
from spacy.gold import biluo_tags_from_offsets, spans_from_biluo_tags
nlp = spacy.load(""en_core_web_md"")

#read txt file, each string on its own line
with open(""./try.txt"",""r"") as f:
    texts = f.read().splitlines()

docs = nlp.pipe(texts)
out_text = """"
for doc in docs:
    offsets = []
    for ent in doc.ents:
        offsets.append((ent.start_char, ent.end_char, ent.label_))
    tags = biluo_tags_from_offsets(doc, offsets)
    text = *zip([tok for tok in doc],tags),
    out = []
    for item in text:
        tag = item[1].split(""-"")
        if tag[0] == ""o"":
            out.append(item[0].text+item[0].whitespace_)
        if tag[0] == ""u"":
            out.append(item[0].ent_type_+item[0].whitespace_)
        elif tag[0] == ""l"":
            out.append(item[0].ent_type_+item[0].whitespace_)
    out_text += """".join(out)+""\n""

with open(""out_try.txt"",""w"") as f:
    f.write(out_text)

contents of the output file:
gpe recently became the ordinal gpe state to ""ban norp culture.
his friend person is here with person and person.
org is looking at buying gpe startup for money",https://stackoverflow.com/questions/65408563,python,22-12-2020 12:14,326.0,0.0,1.0,True,27-06-2022 15:42,27-06-2022 15:42
78552532,what does the error &#39;module &#39;langchain&#39; has no attribute &#39;verbose&#39; refer to?,"kind of new to langchain/qdrant but i'm building a recommendation engine to recommend users based on the contents of their associated pdf files, and i need to process pdfs and store their chunks in a vector database (i'm using qdrant) for establishing context for the rag agent. i don't exactly understand if this error is pertaining to some sort of version requirement, since the only prior error i found had to do with langchain versions before 0.1.x:
found this prior issue
however that issue was closed, and downgrading to versions below 0.1.x given the current releases of langchain doesn't seem feasible given what most of my current environment has recent dependencies.
i tried different versions of langchain and different versions all of the corresponding langchain third-party libraries. currently, these are the important parts of my requirements file (i think):
langchain==0.2.1
langchain-community==0.2.1
langchain-core==0.2.1
langchain-experimental==0.0.59
langchain-openai==0.1.7
langchain-text-splitters==0.2.0
langcodes==3.4.0
langsmith==0.1.57

openai==1.28.1 
python==3.12.3

looking for some sort of workaround, or a diagnosis as to what may package may be causing the problem. my current program output:
traceback (most recent call last):
  file ""/users/danielperlov/dperlov/jobsmatch/backend/ml_model/resume_preprocessor/main.py"", line 28, in <module>
    main()
  file ""/users/danielperlov/dperlov/jobsmatch/backend/ml_model/resume_preprocessor/main.py"", line 17, in main
    processor = pdfresumeprocessor(openai_api_key)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""/users/danielperlov/dperlov/jobsmatch/backend/ml_model/resume_preprocessor/gpt_class.py"", line 16, in __init__
    self.model = chatopenai(api_key=openai_api_key, temperature=0, model_name='gpt-3.5-turbo-16k-0613')
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""/users/danielperlov/dperlov/jobsmatch/backend/ml_model/resume_preprocessor/.venv/lib/python3.12/site-packages/pydantic/v1/main.py"", line 339, in __init__
    values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""/users/danielperlov/dperlov/jobsmatch/backend/ml_model/resume_preprocessor/.venv/lib/python3.12/site-packages/pydantic/v1/main.py"", line 1064, in validate_model
    value = field.get_default()
            ^^^^^^^^^^^^^^^^^^^
  file ""/users/danielperlov/dperlov/jobsmatch/backend/ml_model/resume_preprocessor/.venv/lib/python3.12/site-packages/pydantic/v1/fields.py"", line 437, in get_default
    return smart_deepcopy(self.default) if self.default_factory is none else self.default_factory()
                                                                             ^^^^^^^^^^^^^^^^^^^^^^
  file ""/users/danielperlov/dperlov/jobsmatch/backend/ml_model/resume_preprocessor/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py"", line 72, in _get_verbosity
    return get_verbose()
           ^^^^^^^^^^^^^
  file ""/users/danielperlov/dperlov/jobsmatch/backend/ml_model/resume_preprocessor/.venv/lib/python3.12/site-packages/langchain_core/globals.py"", line 72, in get_verbose
    old_verbose = langchain.verbose
                  ^^^^^^^^^^^^^^^^^
attributeerror: module 'langchain' has no attribute 'verbose'","['python', 'version-control', 'openai-api', 'langchain', 'py-langchain']",78573154,"in my case, the code can fix the problem:
import langchain
langchain.verbose = false
langchain.debug = false
langchain.llm_cache = false",https://stackoverflow.com/questions/78552532,python,30-05-2024 01:37,1772.0,1.0,1.0,True,04-06-2024 02:41,30-05-2024 21:15
71962152,how to solve &#39;str&#39; object has no attribute &#39;lemma_&#39; using spacy?,"i tried to do a lemmatization for my dataframe using spacy in python. the code that i used is like this below:
# import spacy's language model
nlp = spacy.load(""en_core_web_sm"")

# function to lemmatize text
def lemmatization(texts):
    output = []
    for i in texts:
        lem = [str(token).lemma_ for token in nlp(i) or str(token) in [""-pron-""]]
        output.append(' '.join(lem))
    return output

train['clean_tweet'] = lemmatization(train['clean_tweet'])
test['clean_tweet'] = lemmatization(test['clean_tweet'])

turns out i get an error which said:

'str' object has no attribute 'lemma_'

how can i resolve this?","['python', 'spacy', 'lemmatization']",71976633,"string_ = ""i am will be playing football tommorrow"" # dummy string 
obj = nlp(string_)
lemmatize_token = [x.lemma_ for x in obj]

print(lemmatize_token)
['i', 'be', 'will', 'be', 'play', 'football', 'tommorrow']",https://stackoverflow.com/questions/71962152,python,22-04-2022 00:20,2231.0,0.0,1.0,True,23-04-2022 06:00,23-04-2022 06:00
73952853,getting an error install a package on the terminal to use hugging face in vs cod,"i am using the steps from the hugging face website ( in order to start using hugging face in visual studio code and install all the transformers.
i was on the last process, where i had to type ""pip install transformers[flax]"", then i got an error, so i installed rust-land, however, i still ended up getting an error;
requirement already satisfied: transformers[flax] in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (4.22.2)
requirement already satisfied: filelock in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (3.8.0)
requirement already satisfied: requests in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (2.28.1)
requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (0.12.1)
requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (0.10.0)
requirement already satisfied: packaging>=20.0 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (21.3)
requirement already satisfied: tqdm>=4.27 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (4.64.1)
requirement already satisfied: regex!=2019.12.17 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from 
transformers[flax]) (2022.9.13)
requirement already satisfied: numpy>=1.17 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (1.23.3)
requirement already satisfied: pyyaml>=5.1 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (6.0)
collecting transformers[flax]
  using cached transformers-4.22.1-py3-none-any.whl (4.9 mb)
  using cached transformers-4.22.0-py3-none-any.whl (4.9 mb)
  using cached transformers-4.21.3-py3-none-any.whl (4.7 mb)
  using cached transformers-4.21.2-py3-none-any.whl (4.7 mb)
  using cached transformers-4.21.1-py3-none-any.whl (4.7 mb)
  using cached transformers-4.21.0-py3-none-any.whl (4.7 mb)
  using cached transformers-4.20.1-py3-none-any.whl (4.4 mb)
  using cached transformers-4.20.0-py3-none-any.whl (4.4 mb)
  using cached transformers-4.19.4-py3-none-any.whl (4.2 mb)
  using cached transformers-4.19.3-py3-none-any.whl (4.2 mb)
  using cached transformers-4.19.2-py3-none-any.whl (4.2 mb)
  using cached transformers-4.19.1-py3-none-any.whl (4.2 mb)
  using cached transformers-4.19.0-py3-none-any.whl (4.2 mb)
  using cached transformers-4.18.0-py3-none-any.whl (4.0 mb)
collecting sacremoses
  using cached sacremoses-0.0.53-py3-none-any.whl
collecting jax!=0.3.2,>=0.2.8
  using cached jax-0.3.21.tar.gz (1.1 mb)
  preparing metadata (setup.py) ... done
collecting flax>=0.3.5
  using cached flax-0.6.1-py3-none-any.whl (185 kb)
collecting optax>=0.0.8
  using cached optax-0.1.3-py3-none-any.whl (145 kb)
collecting transformers[flax]
  using cached transformers-4.17.0-py3-none-any.whl (3.8 mb)
  using cached transformers-4.16.2-py3-none-any.whl (3.5 mb)
  using cached transformers-4.16.1-py3-none-any.whl (3.5 mb)
  using cached transformers-4.16.0-py3-none-any.whl (3.5 mb)
  using cached transformers-4.15.0-py3-none-any.whl (3.4 mb)
collecting tokenizers<0.11,>=0.10.1
  using cached tokenizers-0.10.3.tar.gz (212 kb)
  installing build dependencies ... done
  getting requirements to build wheel ... done
  preparing metadata (pyproject.toml) ... done
collecting transformers[flax]
  using cached transformers-4.14.1-py3-none-any.whl (3.4 mb)
  using cached transformers-4.13.0-py3-none-any.whl (3.3 mb)
  using cached transformers-4.12.5-py3-none-any.whl (3.1 mb)
  using cached transformers-4.12.4-py3-none-any.whl (3.1 mb)
  using cached transformers-4.12.3-py3-none-any.whl (3.1 mb)
  using cached transformers-4.12.2-py3-none-any.whl (3.1 mb)
  using cached transformers-4.12.1-py3-none-any.whl (3.1 mb)
  using cached transformers-4.12.0-py3-none-any.whl (3.1 mb)
  using cached transformers-4.11.3-py3-none-any.whl (2.9 mb)
  using cached transformers-4.11.2-py3-none-any.whl (2.9 mb)
  using cached transformers-4.11.1-py3-none-any.whl (2.9 mb)
  using cached transformers-4.11.0-py3-none-any.whl (2.9 mb)
  using cached transformers-4.10.3-py3-none-any.whl (2.8 mb)
  using cached transformers-4.10.2-py3-none-any.whl (2.8 mb)
  using cached transformers-4.10.1-py3-none-any.whl (2.8 mb)
  using cached transformers-4.10.0-py3-none-any.whl (2.8 mb)
  using cached transformers-4.9.2-py3-none-any.whl (2.6 mb)
collecting huggingface-hub==0.0.12
  using cached huggingface_hub-0.0.12-py3-none-any.whl (37 kb)
collecting transformers[flax]
  using cached transformers-4.9.1-py3-none-any.whl (2.6 mb)
  using cached transformers-4.9.0-py3-none-any.whl (2.6 mb)
  using cached transformers-4.8.2-py3-none-any.whl (2.5 mb)
  using cached transformers-4.8.1-py3-none-any.whl (2.5 mb)
  using cached transformers-4.8.0-py3-none-any.whl (2.5 mb)
  using cached transformers-4.7.0-py3-none-any.whl (2.5 mb)
collecting huggingface-hub==0.0.8
  using cached huggingface_hub-0.0.8-py3-none-any.whl (34 kb)
collecting transformers[flax]
  using cached transformers-4.6.1-py3-none-any.whl (2.2 mb)
  using cached transformers-4.6.0-py3-none-any.whl (2.3 mb)
  using cached transformers-4.5.1-py3-none-any.whl (2.1 mb)
  using cached transformers-4.5.0-py3-none-any.whl (2.1 mb)
  using cached transformers-4.4.2-py3-none-any.whl (2.0 mb)
  using cached transformers-4.4.1-py3-none-any.whl (2.1 mb)
  using cached transformers-4.4.0-py3-none-any.whl (2.1 mb)
  using cached transformers-4.3.3-py3-none-any.whl (1.9 mb)
  using cached transformers-4.3.2-py3-none-any.whl (1.8 mb)
  using cached transformers-4.3.1-py3-none-any.whl (1.8 mb)
  using cached transformers-4.3.0-py3-none-any.whl (1.8 mb)
  using cached transformers-4.2.2-py3-none-any.whl (1.8 mb)
collecting tokenizers==0.9.4
  using cached tokenizers-0.9.4.tar.gz (184 kb)
  installing build dependencies ... done
  getting requirements to build wheel ... done
  preparing metadata (pyproject.toml) ... done
collecting transformers[flax]
  using cached transformers-4.2.1-py3-none-any.whl (1.8 mb)
  using cached transformers-4.2.0-py3-none-any.whl (1.8 mb)
  using cached transformers-4.1.1-py3-none-any.whl (1.5 mb)
  using cached transformers-4.1.0-py3-none-any.whl (1.5 mb)
  using cached transformers-4.0.1-py3-none-any.whl (1.4 mb)
collecting flax==0.2.2
  using cached flax-0.2.2-py3-none-any.whl (148 kb)
collecting transformers[flax]
  using cached transformers-4.0.0-py3-none-any.whl (1.4 mb)
  using cached transformers-3.5.1-py3-none-any.whl (1.3 mb)
requirement already satisfied: protobuf in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (3.19.6)
collecting sentencepiece==0.1.91
  using cached sentencepiece-0.1.91.tar.gz (500 kb)
  preparing metadata (setup.py) ... done
collecting tokenizers==0.9.3
  using cached tokenizers-0.9.3.tar.gz (172 kb)
  installing build dependencies ... done
  getting requirements to build wheel ... done
  preparing metadata (pyproject.toml) ... done
collecting transformers[flax]
  using cached transformers-3.5.0-py3-none-any.whl (1.3 mb)
  using cached transformers-3.4.0-py3-none-any.whl (1.3 mb)
collecting tokenizers==0.9.2
  using cached tokenizers-0.9.2.tar.gz (170 kb)
  installing build dependencies ... done
  getting requirements to build wheel ... done
  preparing metadata (pyproject.toml) ... done
collecting sentencepiece!=0.1.92
  using cached sentencepiece-0.1.97-cp310-cp310-win_amd64.whl (1.1 mb)
collecting transformers[flax]
  using cached transformers-3.3.1-py3-none-any.whl (1.1 mb)
warning: transformers 3.3.1 does not provide the extra 'flax'
collecting tokenizers==0.8.1.rc2
  using cached tokenizers-0.8.1rc2.tar.gz (97 kb)
  installing build dependencies ... done
  getting requirements to build wheel ... done
  preparing metadata (pyproject.toml) ... done
requirement already satisfied: colorama in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from tqdm>=4.27->transformers[flax]) (0.4.5)
requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from packaging>=20.0->transformers[flax]) (3.0.9)
requirement already satisfied: idna<4,>=2.5 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from requests->transformers[flax]) (3.4)
requirement already satisfied: charset-normalizer<3,>=2 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from requests->transformers[flax]) (2.1.1)
requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from requests->transformers[flax]) (1.26.12)
requirement already satisfied: certifi>=2017.4.17 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from requests->transformers[flax]) (2022.9.24)
collecting joblib
  using cached joblib-1.2.0-py3-none-any.whl (297 kb)
requirement already satisfied: six in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from sacremoses->transformers[flax]) (1.16.0)
collecting click
  using cached click-8.1.3-py3-none-any.whl (96 kb)
building wheels for collected packages: tokenizers
  building wheel for tokenizers (pyproject.toml) ... error
  error: subprocess-exited-with-error

  ï¿½ï¿½ building wheel for tokenizers (pyproject.toml) did not run successfully.
  ï¿½ï¿½ï¿½ exit code: 1
  ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½> [48 lines of output]
      c:\users\user\appdata\local\temp\pip-build-env-hhrbpvks\overlay\lib\site-packages\setuptools\dist.py:530: userwarning: normalizing '0.8.1.rc2' to '0.8.1rc2'
        warnings.warn(tmpl.format(**locals()))
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build\lib.win-amd64-cpython-310
      creating build\lib.win-amd64-cpython-310\tokenizersers\__init__.py -> build\lib.win-amd64-cpython-310\tokenizers
      creating build\lib.win-amd64-cpython-310\tokenizers\models
      copying tokenizers\models\__init__.py -> build\lib.win-amd64-cpython-310\tokenizers\models
      creating build\lib.win-amd64-cpython-310\tokenizers\decoders
      copying tokenizers\decoders\__init__.py -> build\lib.win-amd64-cpython-310\tokenizers\decoders
      creating build\lib.win-amd64-cpython-310\tokenizers\normalizers
      copying tokenizers\normalizers\__init__.py -> build\lib.win-amd64-cpython-310\tokenizers\normalizers
      creating build\lib.win-amd64-cpython-310\tokenizers\pre_tokenizers
      copying tokenizers\pre_tokenizers\__init__.py -> build\lib.win-amd64-cpython-310\tokenizers\pre_tokenizers
      creating build\lib.win-amd64-cpython-310\tokenizers\processors
      copying tokenizers\processors\__init__.py -> build\lib.win-amd64-cpython-310\tokenizers\processors
      creating build\lib.win-amd64-cpython-310\tokenizers\trainers
      copying tokenizers\trainers\__init__.py -> build\lib.win-amd64-cpython-310\tokenizers\trainers
      creating build\lib.win-amd64-cpython-310\tokenizers\implementations
      copying tokenizers\implementations\base_tokenizer.py -> build\lib.win-amd64-cpython-310\tokenizers\implementations       
      copying tokenizers\implementations\bert_wordpiece.py -> build\lib.win-amd64-cpython-310\tokenizers\implementations       
      copying tokenizers\implementations\byte_level_bpe.py -> build\lib.win-amd64-cpython-310\tokenizers\implementations       
      copying tokenizers\implementations\char_level_bpe.py -> build\lib.win-amd64-cpython-310\tokenizers\implementations       
      copying tokenizers\implementations\sentencepiece_bpe.py -> build\lib.win-amd64-cpython-310\tokenizers\implementations    
      copying tokenizers\implementations\__init__.py -> build\lib.win-amd64-cpython-310\tokenizers\implementations
      copying tokenizers\__init__.pyi -> build\lib.win-amd64-cpython-310\tokenizers
      copying tokenizers\models\__init__.pyi -> build\lib.win-amd64-cpython-310\tokenizers\models
      copying tokenizers\decoders\__init__.pyi -> build\lib.win-amd64-cpython-310\tokenizers\decoders
      copying tokenizers\normalizers\__init__.pyi -> build\lib.win-amd64-cpython-310\tokenizers\normalizers
      copying tokenizers\pre_tokenizers\__init__.pyi -> build\lib.win-amd64-cpython-310\tokenizers\pre_tokenizers
      copying tokenizers\processors\__init__.pyi -> build\lib.win-amd64-cpython-310\tokenizers\processors
      copying tokenizers\trainers\__init__.pyi -> build\lib.win-amd64-cpython-310\tokenizers\trainers
      running build_ext
      running build_rust
      error: can't find rust compiler
     
      if you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. installing from the wheel would avoid the need for a rust compiler.
     
      to update pip, run:
     
          pip install --upgrade pip
     
      and then retry package installation.
     
      if you did intend to build this package from source, try installing a rust compiler from your system package manager and 
ensure it is on the path during installation. alternatively, rustup (available at  is the recommended way to 
download and update the rust compiler toolchain.
      [end of output]

  note: this error originates from a subprocess, and is likely not a problem with pip.
  error: failed building wheel for tokenizers
failed to build tokenizers
error: could not build wheels for tokenizers, which is required to install pyproject.toml-based projects

do you know how i can successfully install this into vs code and use hugging face properly?","['tensorflow2.0', 'torch', 'huggingface-transformers', 'huggingface-tokenizers', 'huggingface']",73952880,"if you did intend to build this package from source, try installing a rust compiler from your system package manager and 
ensure it is on the path during installation. alternatively, rustup (available at  is the recommended way to 
download and update the rust compiler toolchain.
      [end of output]

that's the primary error that you're having. you're going to need to install the rust-lang compiler in order to finish the install.",https://stackoverflow.com/questions/73952853,tensorflow2.0,04-10-2022 19:37,949.0,1.0,1.0,True,04-10-2022 21:09,04-10-2022 21:09
77680320,getting different score values between manual cross validation and cross_val_score,"i created a python for loop to split the training dataset into stratified kfolds and used a classifier inside the loop to train it. then used the trained model to predict with the validation data. the metrics achieved using this process where quite different to that achieved with the cross_val_score function. i expected the same results using both methods.
this code is for text classification and i use tf-idf to vectorize the text
code for manual implementation of cross validation:
#importing metrics functions to measure performance of a  model
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
from sklearn.model_selection import stratifiedkfold
data_validation = []  # list used to store the results of model validation using cross validation
skf = stratifiedkfold(n_splits=5, shuffle=true, random_state=42)
accuracy_val = []
f1_val = []

# use ravel function to flatten the multi-dimensional array to a single dimension
for train_index, val_index in (skf.split(x_train, y_train)):
    x_tr, x_val = x_train.ravel()[train_index], x_train.ravel()[val_index] 
    y_tr, y_val  = y_train.ravel()[train_index] , y_train.ravel()[val_index]
    tfidf=tfidfvectorizer()
    x_tr_vec_tfidf = tfidf.fit_transform(x_tr) # vectorize the training folds
    x_val_vec_tfidf = tfidf.transform(x_val) # vectorize the validation fold    
    #instantiate model 
    model= multinomialnb(alpha=0.5, fit_prior=false) 
    #training the empty model with our training dataset
    model.fit(x_tr_vec_tfidf, y_tr)  
    predictions_val = model.predict(x_val_vec_tfidf) # make predictions with the validation dataset
    acc_val = accuracy_score(y_val, predictions_val)
    accuracy_val.append(acc_val)
    f_val=f1_score(y_val, predictions_val)
    f1_val.append(f_val)

avg_accuracy_val = np.mean(accuracy_val)
avg_f1_val = np.mean(f1_val)

# temp list to store the metrics 
temp = ['naivebayes']
temp.append(avg_accuracy_val)   #validation accuracy score 
temp.append(avg_f1_val)         #validation f1 score
data_validation.append(temp)    
#create a table ,using dataframe, which contains the metrics for all the trained and tested ml models
result = pd.dataframe(data_validation, columns = ['algorithm','accuracy score : validation','f1-score  : validation'])
result.reset_index(drop=true, inplace=true)
result      

output:
    algorithm   accuracy score : validation     f1-score : validation
0   naivebayes  0.77012                      0.733994

now code to use cross_val_score function:
from sklearn.model_selection import cross_val_score
from sklearn.feature_extraction.text import countvectorizer, tfidfvectorizer
scores = ['accuracy', 'f1']
#text vectorization of training and testing datasets using nlp technique tf-idf
tfidf=tfidfvectorizer()
x_tr_vec_tfidf = tfidf.fit_transform(x_train)
skf = stratifiedkfold(n_splits=5, shuffle=true, random_state=42)
nb=multinomialnb(alpha=0.5, fit_prior=false) 
for score in [""accuracy"", ""f1""]:
    print (f'{score}: {cross_val_score(nb,x_tr_vec_tfidf,y_train,cv=skf,scoring=score).mean()} ')

output:
accuracy: 0.7341283583255231 
f1: 0.7062017090972422 

as can be seen the accuracy and f1 metrics are quite different using the two methods. the difference in metrics is much worse when i use the kneighborsclassfier.","['python', 'machine-learning', 'scikit-learn', 'cross-validation', 'text-classification']",77680741,"tl;dr: the two ways of calculation are not equivalent due to the different way you handle the tf-idf transformation; the first calculation is the correct one.

in the first calculation you correctly apply fit_transform only to the training data of each fold, and transform to the validation data fold:
x_tr_vec_tfidf = tfidf.fit_transform(x_tr) # vectorize the training folds
x_val_vec_tfidf = tfidf.transform(x_val) # vectorize the validation fold    

but in the second calculation you do not do that; instead, you apply fit_transform to the whole of the training data, before it is split to training and validation folds:
x_tr_vec_tfidf = tfidf.fit_transform(x_train)

hence the difference. the fact that you seem to get a better accuracy with the second, wrong way of calculation, is due to information leakage (your validation data is not actually unseen, they have participated in the tf-idf transformation).

the correct way to use cross_val_score when we have transformations is via a pipeline (api, user's guide):
from sklearn.pipeline import pipeline

tfidf = tfidfvectorizer()
nb = multinomialnb(alpha=0.5, fit_prior=false) 

pipeline = pipeline([('transformer', tfidf), ('estimator', nb)])

skf = stratifiedkfold(n_splits=5, shuffle=true, random_state=42)
scores = cross_val_score(pipeline, x_train, y_train, cv = skf)",https://stackoverflow.com/questions/77680320,python,18-12-2023 16:05,119.0,2.0,1.0,True,28-02-2024 03:44,28-02-2024 03:44
66877729,calculate coherence for non-gensim topic model,"i've built a topic model, with:

input: list of tokenized lists
output: a m x t matrix (with each cell indicating the probability of word i appearing in topic k).
output: a k x n matrix (with each cell indicating the probability of topic k in document j).

to find the optimal number of topics, i want to calculate the coherence for a model. however, i am only aware of gensim's coherencemodel, which seems to require a gensim model as input.
are there any other packages/implementations that i could use to calculate the coherence of a computed topic model? or, if it is indeed possible to use the coherencemodel without inputting a ldamodel, could someone show me how to do that?","['python-3.x', 'nlp', 'package', 'topic-modeling']",66880243,"actually, you can do this with the gensim package.
input_data = list of list with tokenized texts
topics = list with top n words per topic
import gensim.corpora as corpora
from gensim.models.coherencemodel import coherencemodel

id2word = corpora.dictionary(input_data)
corpus = [id2word.doc2bow(text) for text in input_data]

cm = coherencemodel(
    topics=topics,
    texts=input_data,
    corpus=corpus,
    dictionary=id2word,
    coherence='c_v')
coherence = cm.get_coherence()",https://stackoverflow.com/questions/66877729,python-3.x,30-03-2021 20:02,1066.0,0.0,1.0,True,20-05-2024 00:52,31-03-2021 00:04
59409984,is there a way to extract information from contracts using ml with including contract files and targeted strings as inputs and outputs?,"i'm studying whether we could extract certain fields of information (e.g. contract parties, start and end dates) from contracts automatically.
i am wondering if those pieces of information could be extracted with ml by having the whole contract as input and the information as output without tagging or annotating the whole text.
i understand that the extraction should be ran separately for each targeted field.","['machine-learning', 'nlp', 'information-extraction']",59414169,"first question - how are the contracts stored?  are they pdfs or text-based?
if they're pdfs, there are a handful of packages that can extract text from a pdf (e.g. pdftotext).  
second question - is the data you're looking for in the same place in every document?  
if so, you can extract the information you're looking for (like start and end dates) from a known location in the contract.  if not, you'll have to do something more sophisticated.  for example you may need to do a text search for ""start date"", if the same terminology is used in every contract.  if different terminology is used from contract to contract, you may need to work to extract meaning from the text, which can be done using some sophisticated natural language processing (nlp).  
without more knowledge of your problem or a concrete example, it's hard to say what your best option may be.",https://stackoverflow.com/questions/59409984,machine-learning,19-12-2019 12:38,1486.0,0.0,1.0,True,23-11-2024 13:34,23-11-2024 13:34
70787674,is it possible to get r to identify countries in a dataframe?,"this is what my dataset currently looks like. i'm hoping to add a column with the country names that correspond with the 'paragraph' column, but i don't even know how to start going about with that. should i upload a list of all country names and then use the match function?
any suggestions for a more optimal way would be appreciated! thank you.

the output of dput(head(dataset, 20)) is as follows:
structure(list(category = c(""state ownership and privatization;...row.names = c(na, 20l), class = ""data.frame"")","['r', 'dataframe', 'nlp', 'columnsorting']",70790613,"use the package ""countrycode"":
toy data:
df <- data.frame(entry_number = 1:5,
                 text = c(""a few paragraphs that might contain the country name congo or democratic republic of congo"",
                          ""more text that might contain myanmar or burma, as well as thailand"",
                          ""sentences that do not contain a country name can be returned as na"",
                          ""some variant of u.s or the united states"",
                          ""something with an accent samï¿½ï¿½oa""))

this is how you can match the country names in a separate column:
library(tidyr)
library(dplyr)
#install.packages(""countrycode"")
library(countrycode)
all_country <- countryname_dict %>% 
  # filter out non-ascii country names:
  filter(grepl('[a-za-z]', country.name.alt)) %>%
  # define column `country.name.alt` as an atomic vector:
  pull(country.name.altgt;% 
  # change to lower-case:
  tolower()

# define alternation pattern of all country names:
library(stringr)
pattern <- str_c(all_country, collapse = '|')  # a huge alternation pattern!

df %>%
  # extract country name matches
  mutate(country = str_extract_all(tolower(text), pattern))
  entry_number                                                                                       text
1            1 a few paragraphs that might contain the country name congo or democratic republic of congo
2            2                         more text that might contain myanmar or burma, as well as thailand
3            3                         sentences that do not contain a country name can be returned as na
4            4                                                   some variant of u.s or the united states
5            5                                                            something with an accent samï¿½ï¿½oa
                              country
1 congo, democratic republic of congo
2         myanma, burma, thailand
3                                    
4                       united states
5                              samï¿½ï¿½oa
</p",https://stackoverflow.com/questions/70787674,r,20-01-2022 14:08,1732.0,-2.0,1.0,True,20-01-2022 17:18,20-01-2022 17:12
79200020,"getting access denied error when hitting api (although, api is working fine when running on localhost via django)","i used gpt4all to create a rag pipeline with the help of langchain. when i use api for other purposes, it works fine but when i hit the rag pipeline, it gives access denied error.
error getting: permission error
root cause: unable to create model download directory in system profile. access denied to 'c:\windows\system32\config\systemprofile.cache'

environment:
python version: 3.12
operating system: windows
key packages:

langchain
gpt4all
pydantic

full error:
{
    ""error"":""error in database chain: failed to create model download directory"",
    ""traceback"":""traceback (most recent call last):

file ""c:\python312\lib\site-packages\gpt4all\gpt4all.py"", line 323, in retrieve_model
    os.makedirs(default_model_directory, exist_ok=true)

file """", line 215, in makedirs

file """", line 225, in makedirs
    permissionerror: [winerror 5] access is denied: 
    'c:\\windows\\system32\\config\\systemprofile\\.cache'

the above exception was the direct cause of the following exception:

traceback (most recent call last):

file ""c:\inetpub\ line 143, in post
    hmgpt_response = query_response(input_query, query_intent)

file ""c:\inetpub\ line 281, in query_response
    return response_functions.get(intent_response, irrelevant)(query)

file ""c:\inetpub\ line 251, in logistics
    chain = log_chain(load_llm(), vector_db(), memory(), log_prompt())

file ""c:\inetpub\ line 90, in load_llm
    loaded_llm = gpt4all(

file ""c:\python312\lib\site-packages\langchain_core\load\serializable.py"", line 125, in init
    super().init(*args, **kwargs)

file ""c:\python312\lib\site-packages\pydantic\main.py"", line 212, in init
    validated_self = self.pydantic_validator.validate_python(data, self_instance=self)

file ""c:\python312\lib\site-packages\pydantic\_internal\_decorators_v1.py"", line 148, in _wrapper1
    return validator(values)

file ""c:\python312\lib\site-packages\langchain_core\utils\pydantic.py"", line 208, in wrapper
    return func(cls, values)

file ""c:\python312\lib\site-packages\langchain_community\llms\gpt4all.py"", line 145, in validate_environment
    values[""client""] = gpt4allmodel(

file ""c:\python312\lib\site-packages\gpt4all\gpt4all.py"", line 235, in init
    self.config: configtype = self.retrieve_model(model_name, model_path=model_path, allow_download=allow_download, verbose=verbose) 

file ""c:\python312\lib\site-packages\gpt4all\gpt4all.py"", line 325, in retrieve_model
    raise runtimeerror(""failed to create model download directory"") from e

runtimeerror: failed to create model download directory
""}

tried giving manuall path to the cache:
os.environ[""tiktoken_cache_dir""] = ""embeddings-cache""

but didn't worked.
note: i've deployed this on a server using iis server.","['django', 'iis', 'langchain', 'rag', 'gpt4all']",79201966,"i've deployed this on a server using iis server

the error message is clear, it is an identity permission problem. what identity does your application access as? anonymous identity or something else?
but based on the error message, it seems that your code related to some system driver folder permission. you can try to set admin permission in application pool identity.

open the application pools node underneath the machine node. select the application pool you want to change.
right click the application pool and select advanced settings.
select the identity list item and click the ellipsis (the button with the three dots).
check custom account and set....
enter enter the local administrator name and password.",https://stackoverflow.com/questions/79200020,django,18-11-2024 13:01,53.0,0.0,1.0,True,19-11-2024 02:58,18-11-2024 20:14
75841853,get content output from openai library,"i prepare integration openai api with codeigniter 4, for this i use library:


i write controller ci4:
public function openaipost()
  {
    $response = array();
    if ($this->request->getmethod() === 'post'){
    question = $this->request->getpost('question');
    $open_ai_key = getenv('openai_api_key');
    $open_ai = new openai($open_ai_key);
    $complete = $open_ai->chat([
      'model' => 'gpt-3.5-turbo',
      'messages' => [
         [
           ""role"" => ""user"",
           ""content"" => $question
         ]
       ],
       'temperature' => 1.0,
       'max_tokens' => 10,
       'frequency_penalty' => 0,
       'presence_penalty' => 0,
    ]);
    $response['complete'] = $complete;
   }
  echo json_encode($response);
}

but i get in output:
    {
        ""id"": ""chatcmpl-6xxhq2flm08vhe8kkhstpilkvpmts"",
        ""object"": ""chat.completion"",
        ""created"": 1679748856,
        ""model"": ""gpt-3.5-turbo-0301"",
        ""usage"": {
            ""prompt_tokens"": 9,
            ""completion_tokens"": 10,
            ""total_tokens"": 19
        },
        ""choices"": [{
            ""message"": {
                ""role"": ""assistant"",
                ""content"": ""this is a test of the ai language model.""
            },
            ""finish_reason"": ""length"",
            ""index"": 0
        }]
    }

how to get only content from response?",['openai-api'],75862403,"you should all you need in the json response returned by the openai api you have.
you should navigate the json object and extract the message content using the following line.
$content = $complete.choices[0].message.content
it should give you ""this is a test of the ai language model.""",https://stackoverflow.com/questions/75841853,openai-api,25-03-2023 12:58,1547.0,-1.0,1.0,True,28-03-2023 04:43,28-03-2023 04:37
78390452,how to adjust spacy tokenizer so that it splits number followed by dot at line end in german model,"i have a use case in spacy where i want to find phone numbers in german sentences. unfortunately the tokenizer is not doing the tokenization as expected. when the number is at the end of a sentence the number and the dot is not split into two tokens. english and german version differ here, see the following code:
import spacy

nlp_en = spacy.blank(""en"")
nlp_de = spacy.blank(""de"")

text = ""die nummer lautet 1234 123444.""

doc_en = nlp_en(text)
doc_de = nlp_de(text)

print(doc_en[-1]) #output is: .
print(doc_de[-1]) #output is: 123444.

expected output is: 123444. is split into two tokens.
but i also want to use the ""de"" version as it has other meaningful defaults for german sentences...
my spacy version: 3.7.4
in a similar case i was able to solve the problem with nlp_de.tokenizer.add_special_case but here i need to match a number that i don't know. and i couldn't find a way to use regex with add_special_case
i also had a look at:
is it possible to change the token split rules for a spacy tokenizer?
which seems promising. but i wasn't able to figure out how to adjust the tokenizer. i guess i should use a custom tokenizer and the information from

!?","['python', 'spacy', 'tokenize']",78390619,"you may use 'suffixes' to fix issues with punctuation. here is an example:
import spacy


nlp_en = spacy.blank(""en"")
nlp_de = spacy.blank(""de"")

text = ""die nummer lautet 1234 123448.""


suffixes = nlp_de.defaults.suffixes + [r'\.',]
suffix_regex = spacy.util.compile_suffix_regex(suffixes)
nlp_de.tokenizer.suffix_search = suffix_regex.search

doc_en = nlp_en(text)
doc_de = nlp_de(text)

print(doc_en[-1]) #output is: .
print(doc_de[-1]) #output is: .",https://stackoverflow.com/questions/78390452,python,26-04-2024 12:28,137.0,2.0,1.0,True,26-04-2024 13:16,26-04-2024 12:32
78391442,chatgpt api keeps returning error:404 failed to get response from api,"i am using chatgpt's api for text classification task, which i upload a dataset and ask the chatgpt to decide whether my text has something to do with real estate. my code is:
basic setup
import json
import requests
from os import getenv

# load json data from file
with open('/policy_cleaned.json', 'r', encoding='utf-8') as file:
    data = json.load(file)

# api settings
api_url = ""
headers = {
    ""content-type"": ""application/json"",
    ""authorization"": f""bearer sk-proj-...1u""
}


api function
def classify_text(text):
    prompt = f""classify the following text whether it is related to the chinese real estate industry, and if it is specifically about chinese real estate policy: {text}""
    payload = {
        ""model"": ""gpt-4-turbo"",  
        ""prompt"": prompt,
        ""max_tokens"": 64,
        ""temperature"": 0.1
    }
    response = requests.post(api_url, headers=headers, json=payload)
    if response.status_code == 200:
        result = response.json()
        return result
    else:
        return {""error"": ""failed to get response from api"", ""status_code"": response.status_code}


run on my dataset and output response
results = []
for item in data:
    classification = classify_text(item['cleanedcontent'])
    results.append({
        ""policyid"": item['policyid'],
        ""title"": item['title'],
        ""classification"": classification
    })

with open('classified_data.json', 'w', encoding='utf-8') as file:
    json.dump(results, file, ensure_ascii=false)

the program keeps returning in the returning json file:

{""error"": ""failed to get response from api"", ""status_code"": 404}.

i am a rookie using this, so i am desperate for any help!","['python', 'openai-api', 'chatgpt-api']",78392299,"here's a modified version of your classify_text method that should work. the changes are based on the examples used in openai's api reference.
import json
import requests

def classify_text(text):
    system = ""classify the following text whether it is related to the chinese real estate industry, and if it is specifically about chinese real estate policy""
    payload = json.dumps({
        ""model"": ""gpt-4-turbo"",  
        ""messages"": [
            {""role"": ""system"", ""content"": system},
            {""role"": ""user"", ""content"": text}
        ],
        ""max_tokens"": 64,
        ""temperature"": 0.1
    })
    response = requests.post(api_url, headers=headers, data=payload)

    # consider printing the response output to debug any issues
    print(response.json())
    
    if response.status_code == 200:
        result = response.json()
        return result
    else:
        return {""error"": ""failed to get response from api"", ""status_code"": response.status_code}",https://stackoverflow.com/questions/78391442,python,26-04-2024 15:18,1204.0,1.0,1.0,True,26-04-2024 18:05,26-04-2024 16:03
70679187,tensorflow keras text_to_sequence return a list of lists,"i have a problem in text_to_sequence in tf.keras
test_data = 'the invention relates to the fields of biotechnology, virology, epidemiology and public health, and is method for obtaining of new inactivated vaccine against coronavirus covid-19. the essence matter of invention is covid-19 virus sars-cov-2/kz_almaty/04.2020 strain isolated on the territory of the republic of kazakhstan. the strain of covid-19 virus according to the optimal cultivation conditions is produced in the vero cell culture system, inactivated by formaldehyde, clarified by low-speed centrifugation, purified and concentrated by diafiltration on diafiltration unit of millipore pellicon cassette system. sterilizing filtration is carried out through cascades of filters with a pore diameter of 0.45/0.22 ï¿½ï¿½m. 2 % aluminum hydroxide gel algidrogel, 85 is added in the obtained virus pool (viral concentrate) to final concentration of 0.5 mg/0.5 ml and bottled in glass vials. the vaccine obtained in this way is safe at intrapeneal introduction to white mice and intravenously - to rabbits. the vaccine provides 80 % protection against covid-19 infection for at least 6 months after two vaccinations. the vaccine keeps its properties for 12 months at 4-6ï¿½ï¿½c.'

i have this string test data and i am trying to predict it's classification from a model that i have trained.
the problem is that when i call text_to_sequence:
test = tf.keras.preprocessing.text.text_to_word_sequence(test_data)
test = token.texts_to_sequences(test)
print(test)

somehow it returns a list of lists and not a list of the word tokens.
[[1], [7726], [1], [13], [7726], [1], [2997], [1], [1], [7509], [1], [1], [1], [1], [4842], [1], [7167], [1], [1], [1], [1], [1], [4842], [1], [1], [1], [8383], [1], [1], [1], [1], [1], [1], [7167], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [5979], [1], [6054], [1], [13], [1], [1], [7509], [1], [13], [1], [1], [1], [1], [1], [14214], [1], [1], [1], [1], [689],, [1], [1], [4842], [1], [7167], [1], [1], [1], [1], [1], [1], [1], [7167], [1], [1], [7509], [1], [9204], [1], [1], [1], [1], [1], [7167], [1], [1], [1], [4842], [1], [7167], [1], [1], [5979], [1], [1], [7167], [1], [1], [1], [6054], [1], [1], [1], [7509], [1], [1], [4842], [1], [7167], [6054], [1], [1], [1], [1]]

test = pad_sequences(test, maxlen=max_length, padding='post')
test

so the output of the padding for max_length 200 is this:
array([[   1,    0,    0, ...,    0,    0,    0],
       [7726,    0,    0, ...,    0,    0,    0],
       [   1,    0,    0, ...,    0,    0,    0],
       ...,
       [   1,    0,    0, ...,    0,    0,    0],
       [   1,    0,    0, ...,    0,    0,    0],
       [   1,    0,    0, ...,    0,    0,    0]], dtype=int32)

where it should be a single array with 200 lenght.
i have done some tests and it seems that the problem is text_to_sequence which returns this faulty list.
any ideas what seems to be the cause? should i change the input of text_to_sequence or is there any other solution?","['python', 'tensorflow', 'keras', 'nlp', 'tokenize']",70679317,"you should not use text_to_word_sequence if you are already using the class tokenizer. since the tokenizer repeats what text_to_word_sequence actually does, namely tokenize. try something like this:
import tensorflow as tf

tokenizer = tf.keras.preprocessing.text.tokenizer(num_words=300, filters = ' ', oov_token='unk')
test_data = 'the invention relates to the fields of biotechnology, virology, epidemiology and public health, and is method for obtaining of new inactivated vaccine against coronavirus covid-19. the essence matter of invention is covid-19 virus sars-cov-2/kz_almaty/04.2020 strain isolated on the territory of the republic of kazakhstan. the strain of covid-19 virus according to the optimal cultivation conditions is produced in the vero cell culture system, inactivated by formaldehyde, clarified by low-speed centrifugation, purified and concentrated by diafiltration on diafiltration unit of millipore pellicon cassette system. sterilizing filtration is carried out through cascades of filters with a pore diameter of 0.45/0.22 ï¿½ï¿½m. 2 % aluminum hydroxide gel algidrogel, 85 is added in the obtained virus pool (viral concentrate) to final concentration of 0.5 mg/0.5 ml and bottled in glvials. the vaccine obtained in this way is safe at intraperitoneal introduction to white mice and intravenously - to rabbits. the vaccine provides 80 % protection against covid-19 infection for at least 6 months after two vaccinations. the vaccine keeps its properties for 12 months at 4-6ï¿½ï¿½c.'
test = [test_data]
tokenizer.fit_on_texts(test)
test = tokenizer.texts_to_sequences(test)
test = tf.keras.preprocessing.sequence.pad_sequences(test, maxlen=200, padding='post')

print(test.shape)
# (1, 200)
</p",https://stackoverflow.com/questions/70679187,python,12-01-2022 09:32,432.0,1.0,1.0,True,12-01-2022 09:44,12-01-2022 09:44
71203411,"openai: `prompt` column/key is missing. please make sure you name your columns/keys appropriately, then retry","i want to run gpt-3 for text classification. as the first step, i prepare data using openai cli. i got a csv file which looks like as follow:

i wrote following command for preparing the data:
openai tools fine_tunes.prepare_data -f ""path\\training_dataset.csv""

however, i got following error:

i am not sure about ""name columns/keys appropriately"". is there any convention that i should follow? any help would be really appreciated to fix the error","['python', 'openai-api', 'gpt-3']",71368459,"you may convert your csv/tsv file to json, rename the header as prompt and completion.
like this:
| prompt | completion |
| -------- | -------------- |
| text1    | result1        |
| text2    | result2        |",https://stackoverflow.com/questions/71203411,python,21-02-2022 08:46,3186.0,1.0,2.0,True,18-01-2023 12:07,21-02-2022 09:17
76393740,how to get back the predicted text from model output in huggingface?,"i have following toy example which deidentify a given text. i coded the following but the output does not makes sense. i am guessing that the way i am trying to get back the predicted text is incorrect.
import torch 
from transformers import autotokenizer, automodelfortokenclassification
tokenizer = autotokenizer.from_pretrained(""obi/deid_bert_i2b2"", do_lower_case=true)
model = automodelfortokenclassification.from_pretrained(""obi/deid_bert_i2b2"")

text = ""patient john doe visited the hospital on 01/05/2023 with complaints of chest pain.""

encoded_input = tokenizer(text, padding=true, return_tensors='pt')
outputs = model(**encoded_input)

# get the predicted labels
predicted_labels = torch.argmax(outputs.logits, dim=2).squeeze()

# convert the predicted labels back to text
predicted_tokens = tokenizer.batch_decode(predicted_labels, skip_special_tokens=true)

# print the predicted tokens
print(predicted_tokens)

output:
['[unused33]', '[unused33]', '[unused33]', '[unused7]', '[unused29]', '[unused33]', '[unused33]', '[unused33]', '[unused33]', '[unused33]', '[unused35]', '[unused12]', '[unused35]', '[unused12]', '[unused23]', '[unused33]', '[unused33]', '[unused33]', '[unused33]', '[unused33]', '[unused33]', '[unused33]', '[unused33]']","['nlp', 'huggingface-transformers', 'huggingface']",76394716,"you are right, your decoding step isn't correct. the class labels are not part of the tokenizer vocabulary but of the model config (id2label):
from transformers import autotokenizer, automodelfortokenclassification

t = autotokenizer.from_pretrained(model_id)
m = automodelfortokenclassification.from_pretrained(model_id)

encoded_text = t(text, return_tensors=""pt"")

with torch.no_grad():
    logits = m(**encoded_text).logits

token_class_ids = logits.argmax(-1)

predictions = [(t.decode(t_id),m.config.id2label[c.item()]) for t_id, c in zip(encoded_text[""input_ids""][0], token_class_ids[0])]
print(*predictions, sep=""\n"")

output:
('[cls]', 'o')
('pat', 'o')
('##ient', 'o')
('john', 'b-patient')
('do', 'l-patient')
('##e', 'o')
('visited', 'o')
('the', 'o')
('hospital', 'o')
('on', 'o')
('01', 'u-date')
('/', 'i-date')
('05', 'u-date')
('/', 'i-date')
('202', 'l-date')
('##3', 'o')
('with', 'o')
('complaints', 'o')
('of', 'o')
('chest', 'o')
('pain', 'o')
('.', 'o')
('[sep]', 'o')

in case you are only interested in inference, you might want to check out the token classification pipeline:
from transformers import pipeline

model_id = ""obi/deid_bert_i2b2""

text = ""patient john doe visited the hospital on 01/05/2023 with complaints of chest pain.""

p = pipeline(""token-classification"", model_id)
p(text)

output:
[{'entity': 'b-patient',
  'score': 0.9976101,
  'index': 3,
  'word': 'john',
  'start': 8,
  'end': 12},
 {'entity': 'l-patient',
  'score': 0.98856366,
  'index': 4,
  'word': 'do',
  'start': 13,
  'end': 15},
 {'entity': 'u-date',
  'score': 0.99967885,
  'index': 10,
  'word': '01',
  'start': 41,
  'end': 43},
 {'entity': 'i-date',
  'score': 0.83500373,
  'index': 11,
  'word': '/',
  'start': 43,
  'end': 44},
 {'entity': 'u-date',
  'score': 0.9905285,
  'index': 12,
  'word': '05',
  'start': 44,
  'end': 46},
 {'entity': 'i-date',
  'score': 0.9776883,
  'index': 13,
  'word': '/',
  'start': 46,
  'end': 47},
 {'entity': 'l-date',
  'score': 0.9986461,
  'index': 14,
  'word': '202',
  'start': 47,
  'end': 50}]",https://stackoverflow.com/questions/76393740,nlp,02-06-2023 22:15,760.0,1.0,1.0,True,03-06-2023 05:48,02-06-2023 23:33
69313218,spacy - identify token in pattern matching,"i would like to make a program that extract from a sentences two words, and i need to know if one of the word is the departure, and if the other is the arrival.
i tried to make a pattern and set the token type inside the custom extension attribute, for example for this sentence: ""i would like a london paris train"".
i created this pattern:
pattern = [
    {""pos"":  ""propn"", ""_"": {""type"": ""arrival""}},
    {""pos"":  ""propn"", ""_"": {""type"": ""departure""}}
]

and i also created a custom_extension attribute inside my python code:
token.set_extension('type', default="""", force=true)

my problem is that, if type in my pattern doesn't match the default value, nothing is matched.
i would like to know if there is a better solution with pattern matching to identify matched tokens.
thanks a lot by advance","['python', 'spacy']",69332106,"what you're trying to do is ""semantic role labelling"" and it's hard. you absolutely can't do this with just pattern matching.
the very simplest thing you can do that might work, which will work on your example, is to use spacy's ner model to get all loc or gpo entities, and assume the first one is the departure and the second one is the arrival. that'll be really brittle though.",https://stackoverflow.com/questions/69313218,python,24-09-2021 10:06,142.0,0.0,1.0,True,26-09-2021 05:25,24-09-2021 10:23
79081924,"with spacy, how can i get all lemmas from a string?","i have a pandas data frame with a column of text values (documents).  i want to apply lemmatization on these values with the spacy library using the pandas apply function.  i've defined my to_lemma function to iterate through the words in the document and concatenate the corresponding lemmas in the output string, however this is very slow.  is there a way to extract the lemmatized form of a document in spacy?
def to_lemma(text):
    tp = nlp(text)
    line = """"
    for word in tp:
        line = line + word.lemma_ + "" ""
    return line","['python', 'pandas', 'nlp', 'spacy', 'lemmatization']",79086290,"there are many ways to speed up spacy processing. the question which of them make sense for you depends mostly on the size of your input.

the most obvious one is not individually apply the model to every single row, but rather use batch processing. use nlp.pipe() with an iterable of strings. this means it is easier to not use apply.
disable components that you do not use. for token level processing where you need the lemmas this would be 'parser' (the dependency parser) and 'ner' (the named entity recognition component).
increase the batch_size (objects to buffer) in pipe(). the default is 1000. obviously this only makes sense to touch if you have the memory to increase it a lot.
increase the number of processors used using n_process. this will increase the time it takes to initially load the model but decrease the processing time. in my experience this starts making sense at about 500k+ texts. note that this also requires the code to be run in an if __name__ == '__main__': wrapper.

basic example with 1. and 2.:
texts = df[""column_name""]
nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])
lemmas = []
for processed_doc in nlp.pipe(texts):
    lemmas.append("" "".join([token.lemma_ for token in processed_doc]))
df[""column_name_lemmas""] = lemmas

advanced example for all four:
if __name__ == '__main__':
    texts = df[""column_name""]
    nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])
    lemmas = []
    for processed_doc in nlp.pipe(texts, batch_size=10000, n_process=4):
        lemmas.append("" "".join([token.lemma_ for token in processed_doc]))
    df[""column_name_lemmas""] = lemmas",https://stackoverflow.com/questions/79081924,python,12-10-2024 21:03,105.0,-1.0,2.0,True,14-10-2024 23:07,14-10-2024 23:07
77625508,how to activate verbosity in langchain,"i'm using langchain 0.0.345. i cannot get a verbose output of what's going on under the hood using the lcel approach to chain building.
i have this code:
from langchain.chat_models import chatopenai
from langchain.prompts import chatprompttemplate
from langchain.schema.output_parser import stroutputparser
from langchain.globals import set_verbose

set_verbose(true)

prompt = chatprompttemplate.from_template(""tell me a joke about {topic}"")
model = chatopenai()
output_parser = stroutputparser()

chain = prompt | model | output_parser

chain.invoke({""topic"": ""ice cream""})

according to the documentation using set_verbose is the way to have a verbose output showing intermediate steps, prompt builds etc. but the output of this script is just a string without any intermediate steps.
actually, the module langchain.globals does not appear even mentioned in the api documentation.
i have also tried setting the verbose=true parameter in the model creation, but it also does not work. this used to work with the former approach building with classes and so.
how is the recommended and current approach to have the output logged so you can understand what's going on?
thanks!","['python', 'langchain', 'large-language-model']",77629872,"you can add a callback handler to the invoke method's configuration.
like this:
from langchain.callbacks.tracers import consolecallbackhandler

# ...your code

chain.invoke({""topic"": ""ice cream""}, config={'callbacks': [consolecallbackhandler()]})

code with change incorporated:
from langchain.chat_models import chatopenai
from langchain.prompts import chatprompttemplate
from langchain.schema.output_parser import stroutputparser
from langchain.callbacks.tracers import consolecallbackhandler

prompt = chatprompttemplate.from_template(""tell me a joke about {topic}"")
model = chatopenai()
output_parser = stroutputparser()

chain = prompt | model | output_parser

chain.invoke({""topic"": ""ice cream""}, config={'callbacks': [consolecallbackhandler()]})

the output isn't the same as the original ""verbose mode"", but this is the closest alternative.
alternatives
for more targeted output or less ""verbosity""
try attaching a callback handler to specific objects. for example:
chatopenai().with_config({'callbacks': [consolecallbackhandler()]})

you can learn more about customizing callbacks here
for high verbosity
global debug still works with lcel:
from langchain.globals import set_debug

set_debug(true)

# your code

for a gui
you can use weights and biases or langsmith",https://stackoverflow.com/questions/77625508,python,08-12-2023 09:14,14728.0,10.0,2.0,True,11-12-2023 19:52,11-12-2023 19:52
77867650,"you tried to access openai.file, but this is no longer supported in openai&gt;=1.0.0","i am doing fine tuning in chatgpt where i am looking to adjust the model so that it defines a set of subtopics (or related concepts) based on a specific topic (concept).
this is my training dataset:
{""prompt"":""wellbeing of child->"",""completion"":"" immunization, equal nurturing of boys and girls, parents mental health, shaking baby, children with disabilities, dental care (only gum care), importance of height and weight measurement, feeding during sickness, playing with rattle, make mealtimes fun, avoid physical maltreatment, reducing screen time, childrens common diseases, nurturing care for children\n""}
{""prompt"":""learning and development->"",""completion"":"" reducing screen time, ways of learnings, family values, show name talk, importance of height and weight measurement, toilet training, sorting and matching, drowning, learning through play\n""}
{""prompt"":""hygiene and safety->"",""completion"":"" hygiene, safety, drowning, handwashing, avoid physical maltreatment\n""}
{""prompt"":""breastfeeding->"",""completion"":"" importance of breastfeeding, method of breastfeeding, demerits of infant formula milk, maternal nutrition, sore nipples, feeding during sickness, complementary feeding, healthy feeding, interactions with baby during breastfeeding\n""}

this is the script i am running:
import openai
import json
import os


api_key ="" ""
openai.api_key = api_key


##cheking training data
!openai tools fine_tunes.prepare_data -f training_data_prepared.jsonl -q

response = openai.file.create(
    file=open(""training_data_prepared.jsonl"",""rb""),
    purpose = ""fine-tune"")

print(response)

in the response function i am getting this error:
apiremovedinv1: 

you tried to access openai.file, but this is no longer supported in openai>=1.0.0 - see the readme at  for the api.

you can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

a detailed migration guide is available here: 

i know this is an openai library version error, but i was looking for a solution to solve it but nothing works.","['python', 'python-3.x', 'openai-api', 'chatgpt-api']",77868315,"change from:
response = openai.file.create(
    file=open(""training_data_prepared.jsonl"",""rb""),
    purpose = ""fine-tune"")

to
response = client.files.create(
        file=open(""training_data_prepared.jsonl"",""rb""),
        purpose = ""fine-tune"")

detailed migration guide here:

a glimse of changes:

 alternatively, you can use old version(not recommended though) 
pip install openai==0.28",https://stackoverflow.com/questions/77867650,python,23-01-2024 15:58,1396.0,1.0,1.0,True,24-01-2024 19:45,24-01-2024 19:45
78815849,azure openai whipser rest endpoints,"i wanted to use whisper deployed through azure openai but i am having trouble finding the right resources for it.
i am trying to integrate a translator using whisper in a flutter app that will take multilingual input and give out the output in english.
right now the transcription is working using this endpoint:
 
and structuring the request like:
var uri = uri.parse(whisperendpoint);
          var request =  uri)
            ..headers['api-key'] = whisperapikey
            ..files.add(await  filepath));

what is the endpoint for translation, tts and other services?
i have tried sending the text transcribed by whisper back to be translated but i'd prefer direct translation by whisper","['azure', 'rest', 'openai-api', 'openai-whisper']",78816231,"you can use below endpoint for trnaslation.


and it will convert it into english text.
here, is the sample output with bengali as the input file language.

if you want to convert this text to speech then use  tts-1 or tts-1-hd models in azure openai.
make sure you have your open ai resource in supported region of above models",https://stackoverflow.com/questions/78815849,azure,31-07-2024 10:41,365.0,0.0,1.0,True,31-07-2024 12:09,31-07-2024 12:09
74837617,is there a javascript implementation of cl100k_base tokenizer?,"openai's new embeddings api uses the cl100k_base tokenizer. i'm calling it from the node.js client, but i don't see any easy way of slicing my strings so they don't exceed the openai limit of 8192 tokens.
this would be trivial if i could first encode the string, slice it to the limit, then decode it and send it to the api.","['node.js', 'machine-learning', 'nlp', 'tokenize', 'openai-api']",76137089,@dqbd/tiktoken supports the cl100k_base encoding.,https://stackoverflow.com/questions/74837617,node.js,17-12-2022 21:35,2805.0,3.0,2.0,True,19-05-2023 03:57,18-04-2023 23:07
66284360,how to get biobert embeddings,"i have field within a pandas dataframe with a text field for which i want to generate biobert embeddings. is there a simple way with which i can generate the vector embeddings? i want to use them within another model.
here is a hypothetical sample of the data frame




visit code
problem assessment




1234
ge reflux working diagnosis well


4567
medication refill order working diagnosis note called in brand benicar 5mg qd 30 prn refill




i have tried this package, but receive an error upon installation

error:
collecting biobert-embedding
  using cached biobert-embedding-0.1.2.tar.gz (4.8 kb)
error: could not find a version that satisfies the requirement torch==1.2.0 (from biobert-embedding) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2, 1.7.1)
error: no matching distribution found for torch==1.2.0 (from biobert-embedding)

any help is greatly appreciated!","['python', 'nlp', 'data-science', 'biopython', 'bert-language-model']",66298132,"try to install it as follows:
pip install biobert-embedding==0.1.2 torch==1.2.0 -f 

i extended your sample dataframe to illustrate how you can now calculate the sentence vectors for your problem assessments and use these to calculate for example the cosine similarity between similar visit codes.
>>> from biobert_embedding.embedding import biobertembedding
>>> from scipy.spatial import distance
>>> import pandas as pd

>>> data = {'visit code': [1234, 1235, 4567, 4568], 
        'problem assessment': ['ge reflux working diagnosis well', 
                               'other reflux diagnosis poor', 
                               'medication refill order working diagnosis note called in brand benicar 5mg qd 30 prn refill',
                               'medication must be refilled diagnosis note called in brand olmesartan 10mg qd 40 prn refill']}

>>> df = pd.dataframe(data)
>>> df






visit code
problem assessment




0
1234
ge reflux working diagnosis well


1
1234
other reflux diagnosis poor


2
4567
medication refill order working diagnosis note called in brand benicar 5mg qd 30 prn refill


3
4567
medication must be refilled diagnosis note called in brand olmesartan 10mg qd 40 prn refill




>>> biobert = biobertembedding()
>>> df['sentence embedding'] = df['problem assessment'].apply(lambda sentence: biobert.sentence_vector(sentence))
>>> df






visit code
problem assessment
sentence embedding




0
1234
ge reflux working diagnosis well
tensor([ 2.7189e-01, -1.6195e-01,  5.8270e-02, -3.2730e-01,  7.5583e-02, ...


1
1234
other reflux diagnosis poor
tensor([ 1.6971e-01, -2.1405e-01,  3.4427e-02, -2.3090e-01,  1.6007e-02, ...


2
4567
medication refill order working diagnosis note called in brand benicar 5mg qd 30 prn refill
tensor([ 1.5370e-01, -3.9875e-01,  2.0089e-01,  4.1506e-02, 6.9854e-02,  ...


3
4567
medication must be refilled diagnosis note called in brand olmesartan 10mg qd 40 prn refill
tensor([ 2.2128e-01, -2.0283e-01,  2.2194e-01,  9.1156e-02,  1.1620e-01, ...




>>> df.groupby('visit code')['sentence embedding'].apply(lambda sentences: 1 - distance.cosine(sentences.values) )


visit code
1234    0.950492
4567    0.969715
name: sentence embedding, dtype: float64

we can see that, as expected, the similar sentences lie very close together",https://stackoverflow.com/questions/66284360,python,19-02-2021 20:09,4080.0,2.0,1.0,True,06-09-2022 01:57,19-02-2021 20:19
67391381,if a condition is true how to extract specific rows addressing that condition from a csv file as print result,"if i have 2 conditions: 1. a document is health relate 2. a document is not health-related
and my document satisfies one of the above-mentioned conditions. as an output, it must pull out data from a csv file from rows associated with this condition say row 1, 5, and 8, and print it.","['python', 'pandas', 'conditional-statements', 'text-mining']",67391840,"import pandas as pd

df = pd.read_csv('path/to/csv')

print(df.iloc[1, 5, 8])",https://stackoverflow.com/questions/67391381,python,04-05-2021 19:56,156.0,-2.0,1.0,True,24-07-2021 12:07,24-07-2021 12:07
75394143,openai api error: &quot;no module named &#39;openai.embeddings_utils&#39;; &#39;openai&#39; is not a package&quot;,"i want to use openai.embeddings_utils import get_embeddings
so already install openai
name: openai
version: 0.26.5
summary: python client library for the openai api
home-page: 
author: openai
author-email: support@openai.com
license: 
location: /users/lima/desktop/paprika/openai/.venv/lib/python3.9/site-packages
requires: aio requests, tqdm
required-by: 

this is my openai
but why not use openai.embeddings_utils??","['python', 'python-3.x', 'pip', 'openai-api', 'azure-openai']",77454610,"for my case, check the version of openai.
openai.embeddings_utils does not exist in latest openai 1.2.0, but exists in 0.27.7",https://stackoverflow.com/questions/75394143,python,09-02-2023 04:16,32002.0,11.0,9.0,True,22-01-2025 16:41,09-06-2023 08:36
77656467,attention mask error when fine-tuning mistral 7b using transformers trainer,"i'm trying to fine-tune mistralai/mistral-7b-v0.1 using the following sample notebook
i follow the steps in the notebook, but the training fails with:
***** running training *****
  num examples = 344
  num epochs = 3
  instantaneous batch size per device = 2
  total train batch size (w. parallel, distributed & accumulation) = 2
  gradient accumulation steps = 1
  total optimization steps = 500
  number of trainable parameters = 21,260,288
  0%|          | 0/500 [00:00<?, ?it/s]you're using a llamatokenizerfast tokenizer. please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
`use_cache=true` is incompatible with gradient checkpointing. setting `use_cache=false`...
  file ""/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py"", line 293, in forward
    raise valueerror(
valueerror: attention mask should be of size (2, 1, 512, 1024), but is torch.size([2, 1, 512, 512])

any ideas where this issue regarding the extension mask could result from? my tokenized data is exactly of size 512. why is it expecting size 1024 and these particular 4 dimensions?","['python', 'huggingface-transformers', 'mistral-7b']",77657100,"experiencing the same issue, downgrading transformers to 4.35.2 instead of latest version 4.36.0 seems to work fine.",https://stackoverflow.com/questions/77656467,python,13-12-2023 20:50,1745.0,0.0,1.0,True,06-02-2024 16:55,06-02-2024 16:55
75747955,transcription via openai&#39;s whisper: assertionerror: incorrect audio shape,"i'm trying to use openai's open source whisper library to transcribe audio files.
here is my script's source code:
import whisper

model = whisper.load_model(""large-v2"")

# load the entire audio file
audio = whisper.load_audio(""/content/file.mp3"")
#when i write that code snippet here ==> audio = whisper.pad_or_trim(audio) the first 30 secs are converted and without any problem they are converted.

# make log-mel spectrogram and move to the same device as the model
mel = whisper.log_mel_spectrogram(audio).to(model.device)

# detect the spoken language
_, probs = model.detect_language(mel)
print(f""detected language: {max(probs, key=probs.get)}"")

# decode the audio
options = whisper.decodingoptions(fp16=false)
result = whisper.decode(model, mel, options)

# print the recognized text if available
try:
    if hasattr(result, ""text""):
        print(result.text)
except exception as e:
    print(f""error while printing transcription: {e}"")

# write the recognized text to a file
try:
    with open(""output_of_file.txt"", ""w"") as f:
        f.write(result.text)
        print(""transcription saved to file."")
except exception as e:
    print(f""error while saving transcription: {e}"")

in here:
# load the entire audio file
audio = whisper.load_audio(""/content/file.mp3"")

when i write below: "" audio = whisper.pad_or_trim(audio) "", the first 30 secs of the sound file is transcribed without any problem and language detection works as well,
but when i delete it and want the whole file to be transcribed, i get the following error:

assertionerror: incorrect audio shape

what should i do? should i change the structure of the sound file? if yes, which library should i use and what type of script should i write?","['python', 'python-3.x', 'ffmpeg', 'openai-api', 'openai-whisper']",75803686,"i had the same problem and after some digging i found that whisper.decode is meant to extract metadata about the input, such as the language, and hence the limit to 30 seconds. (see source code for decode function here)
in order to transcribe (even audio longer than 30 seconds) you can use whisper.transcribe as shown in the following snippet
import whisper

model = whisper.load_model(""large-v2"")

# load the entire audio file
audio = whisper.load_audio(""/content/file.mp3"")

options = {
    ""language"": ""en"", # input language, if omitted is auto detected
    ""task"": ""translate"" # or ""transcribe"" if you just want transcription
}
result = whisper.transcribe(model, audio, **options)
print(result[""text""])

you can find some documentation of the transcribe method in the source code along with  some documentation about the decodingoptions structure",https://stackoverflow.com/questions/75747955,python,15-03-2023 17:07,3584.0,4.0,1.0,True,01-04-2024 18:54,01-04-2024 18:54
76194617,how do you convert json to python function arguments?,"i'm trying to make a program that allows anyone to use the chatgpt api using my api key using the official openai api. i am using a flask server to listen to requests. here is my code:
from flask import flask, request
import openai, json

openai.api_key = api_key

app = flask(__name__)

@app.route('/v1/chat/completions', methods=[""post""])
def index():
    return openai.chatcompletion.create(json.loads(request.get_json(force=true)))

app.run(debug=true, port=8080)

and here's my request body:
{'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'what is 2 + 2?'}], 'temperature': 0}

there seems to be a parsing error because it says: openai.error.invalidrequesterror: must provide an 'engine' or 'model' parameter to create a <class 'openai.api_resources.chat_completion.chatcompletion'> even though in my requests i give it a model.","['python', 'artificial-intelligence', 'openai-api']",76195025,"you can't convert one string into multiple parameters. plus, as mentioned in the comments, you don't need json.loads as request.get_json() should return what you want - a dictionary.
if you were providing a dictionary with matching keys you could do
payload = request.get_json()
return openai.chatcompletion.create(**payload)

otherwise, parse each key manually
payload = request.get_json()
return openai.chatcompletion.create(
    model=payload[""model""], 
    messages=payload[""messages""]
)

also, flask prefers you return text / rendered html, rather than a direct python object",https://stackoverflow.com/questions/76194617,python,07-05-2023 14:46,286.0,-1.0,1.0,True,07-05-2023 16:29,07-05-2023 16:13
68767293,how to get all labels in a column out of one hot encoded columns?,"reversing one hot encoding of multi-labeled data.
i want to convert 20+ one hot encoded columns into a column with label names.
the data is also multi-labeled, and in the label column, i expect to mention all the labels.
from this:
labela  labelb labelc 
  0        0     1
  1        1     0  

to:
labela  labelb labelc   labels
      0        0     1   ['labelc']
      1        1     0   ['labela','labelb']","['python', 'dataframe', 'nlp', 'dataset']",68948594,"this code will convert all the one hot encoded column labels into a single column with a list of labels in it.
from this:
labela  labelb labelc 
  0        0     1
  1        1     0  

to:
labela  labelb labelc   labels
      0        0     1   ['labelc']
      1        1     0   ['labela','labelb']

code:
def get_col_name(row):    
    b = (df.loc[row.name] == 1)
    c = list(b.index[b])
    return c

df['label'] = df.apply(get_col_name, axis=1)",https://stackoverflow.com/questions/68767293,python,13-08-2021 05:38,638.0,0.0,2.0,True,27-08-2021 05:58,27-08-2021 05:58
73643948,python get multiple docx file names and extract specific words from the files to generate a dataframe or table,"i hope to read multiple word documents (docx files) in a folder and then search a specific word e.g. ""laptop"" from each of docx file to generate a table or a dataframe.
for instance: in my folder i have file_1.docx, file_2.docx ... file_n.docx, each file may or may not contain work ""laptop"". in the end i hope to generate a table like:
filename          keyword
file_1.docx       ""laptop""
file_2.docx       ""laptop""
...","['python', 'text-mining', 'python-docx', 'docx2txt']",73644325,"if you are using python3.x you will need to do

pip install python-docx

not to be confuse with docx as i had some issues using this.
import os
from docx import document
import pandas as pd

match_word = ""laptop""
match_items = []
folder = 'c:\\dev\\docs'
file_names = os.listdir(folder)
file_names = [file for file in file_names if file.endswith('.docx')]
file_names = [os.path.join(folder, file) for file in file_names]

for file in file_names:
    document = document(file)
    for paragraph in document.paragraphs:
        if match_word in paragraph.text:
            match_items.append([file, match_word])

the_df = pd.dataframe(
    match_items,
    columns=['file_name', 'word_match'],
    index=[i[0] for i in match_items]
)

print(the_df)

output:
file_name              word_match
c:\dev\docs\c.docx     laptop",https://stackoverflow.com/questions/73643948,python,08-09-2022 05:02,1332.0,0.0,1.0,True,08-09-2022 06:01,08-09-2022 05:09
68429393,how to convert pandas multiple columns of text into tensors?,"hi i am working on key point analysis task, which is shared by ibm, here is the link. in the given dataset there are more than one rows of text and anyone can please tell me how can i convert the text columns into tensors and again assign them in the same dataframe because there are other columns of data there. 
problem
here i am facing a problem that i have never seen this kind of data before like have multiple text columns, how can i convert all those columns into tensors and then apply a model. most of the time data is like : one text column
and other columns are label, example: movie reviews , toxic comment classification.
def clean_text(text):
""""""
    text: a string

    return: modified initial string
""""""
text = text.lower()  # lowercase text
text = replace_by_space_re.sub(' ',
                               text)  
text = bad_symbols_re.sub('',
                          text)  
text = text.replace('x', '')
#    text = re.sub(r'\w+', '', text)
text = ' '.join(word for word in text.split() if word not in stopwords) 
return text","['machine-learning', 'deep-learning', 'nlp', 'data-preprocessing']",68432623,"if i got your question right you will do sth like the following:

from transformers import robertatokenizer
tokenizer = robertatokenizer.from_pretrained(""roberta-base"")
df[""args""]=df[""args""].apply(lambda x:tokenizer(x)['input_ids'])

this will convert sentences into token arrays.",https://stackoverflow.com/questions/68429393,machine-learning,18-07-2021 13:09,606.0,1.0,1.0,True,19-07-2021 19:53,19-07-2021 19:53
79173053,how to convert character indices to bert token indices,"i am working with a question-answer dataset uclnlp/adversarial_qa.
from datasets import load_dataset
ds = load_dataset(""uclnlp/adversarial_qa"", ""adversarialqa"")

how do i map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like bert. here's an example row from my dataset:
d0 = ds['train'][0]
d0

{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',
 'title': 'brain',
 'context': 'another approach to brain function is to examine the consequences of damage to specific brain areas. even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the bloodï¿½ï¿½ï¿½brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. in humans, the effects of strokes and other types of brain damage have been a key source of information about brain functioause there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. in animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',
 'question': 'what sare the benifts of the blood brain barrir?',
 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},
 'metadata': {'split': 'train', 'model_in_the_loop': 'combined'}}

after tokenization, the answer indices are 56  and 16:
from transformers import berttokenizerfast
bert_tokenizer = berttokenizerfast.from_pretrained('bert-large-uncased', return_token_type_ids=true)

bert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])
'isolated from the bloodstream'

i want to create a new dataset with the answer's token indices, e.g., 56 ad 60.
this is from a linkedin learning class. the instructor did the conversion and created the csv file but he did not share it or the code to do that. this is the expected result:","['python', 'nlp', 'dataset', 'large-language-model', 'bert-language-model']",79175157,"you should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices.
the following function does the above for you:
def get_token_indices(example):
    # tokenize with `return_offsets_mapping=true` to get character offsets for each token
    encoded = tokenizer(
        example['question'], 
        example['context'], 
        return_offsets_mapping=true
    )

    # find character start and end from the original answer
    char_start = example['answers']['answer_start'][0]
    char_end = char_start + len(example['answers']['text'][0])

    # identify token indices for the answer
    start_token_idx = none
    end_token_idx = none
    
    for i, (start, end) in enumerate(encoded['offset_mapping']):
        if start <= char_start < end: 
            start_token_idx = i
        if start < char_end <= end:
            end_token_idx = i
            break

    example['answer_start_token_idx'] = start_token_idx
    example['answer_end_token_idx'] = end_token_idx
    return example

here's how you can use and test this function:
ds = load_dataset(""uclnlp/adversarial_qa"", ""adversarialqa"")
tokenizer = berttokenizerfast.from_pretrained('bert-large-uncased', return_token_type_ids=true)

tokenized_ds = ds['train'].map(get_token_indices)


# example
d0_tokenized = tokenized_ds[0]
print(""tokenized start index:"", d0_tokenized['answer_start_token_idx'])
print(""tokenized end index:"", d0_tokenized['answer_end_token_idx'])

answer_tokens = tokenizer.decode(
    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]
)
print(""tokenized answer:"", answer_tokens)

output:
tokenized start index: 56
tokenized end index: 60
tokenized answer: isolated from the bloodstream",https://stackoverflow.com/questions/79173053,python,09-11-2024 15:15,39.0,2.0,1.0,True,10-11-2024 15:18,09-11-2024 15:27
74909057,if dataframe column has specific words alter value,"i have a dataframe, example:
df = [{'id': 1, 'text': 'text contains ok words'}, , {'id':2, 'text':'text contains word apple'}, {'id':3, 'text':'text contains words ok'}]

example:
keywords = ['apple', 'orange', 'lime']

and i want to check all columns 'text' to check if contains any word from my keywords, if so i want to alter that text column to: 'disconsider this case'
i've tried to tokenize the column but then i'm not able to use the function i created to check, here is the example:
df = pd.dataframe(df)

def remove_keywords(inpt):
    keywords = ['apple', 'orange', 'lime']    
    if any(x in word for x in keyword):
        return 'disconsider this case'
    else:
        return inpt

        
df['text'] = df['text'].apply(remove_keywords)
df

df['text'] = df.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)
for word in df['text']:
    if 'apple' in df['text']:
        return 'disconsider this case'

any help appreciated. thanks!!","['python', 'dataframe', 'nlp']",74909417,"this worked for me using pandas and a loop
import pandas as pd
keywords=['apple', 'orange', 'lime']
df = pd.dataframe([{'id': 1, 'text': 'text contains ok words'}, {'id':2, 'text':'text contains word apple'}, {'id':3, 'text':'text contains words ok'}])
print(df)
for i in range(len(df)):
        if any(word in df.iat[i,1] for word in keywords):
            df.iat[i,1]='discondider in this case'
print(df)",https://stackoverflow.com/questions/74909057,python,24-12-2022 16:52,46.0,3.0,1.0,True,24-12-2022 22:51,24-12-2022 22:51
73912673,converting word to vector using glove,"i loaded my glove package as follows:
import gensim.downloader as api
model = api.load(""glove-wiki-gigaword-100"")

and would want to create a function where i pass in a word and the glove model, and it will return the corresponding vector, for instance,
def convert_word_to_vec(word, model):

and when i pass in convert_word_to_vec(lol, model) it will return the vectors for the word lol
is there a way around this? thank you!","['python', 'deep-learning', 'stanford-nlp', 'gensim']",73913859,"usage:
import gensim.downloader as api
model = api.load(""glove-wiki-gigaword-100"")

vector = model['lol']
print(vector)  # array with shape (100,)

please check the documentations for more options:",https://stackoverflow.com/questions/73912673,python,30-09-2022 18:28,616.0,0.0,1.0,True,01-10-2022 08:13,01-10-2022 08:13
73089823,how to find the actual sentence from sentence transformer?,"i am trying to do semantic search with sentence transformer and faiss.
i am able to generate emebdding  from corpus and perform query with the query xq.
but what are t
from sentence_transformers import sentencetransformer, util
model = sentencetransformer(""flax-sentence-embeddings/st-codesearch-distilroberta-base"")

def get_embeddings(code_snippets: str):
    return model.encode(code_snippets)

def build_vector_database(atlas_datapoints):
    dimension = 768  # dimensions of each vector

    corpus = [""tom loves candy"",
                    ""this is a test""
                    ""hello world""
                    ""jerry loves programming""]

    code_snippet_emddings = get_embeddings(corpus)
    print(code_snippet_emddings.shape)

    d = code_snippet_emddings.shape[1]
    index = faiss.indexflatl2(d)
    print(index.is_trained)

    index.add(code_snippet_emddings)
    print(index.ntotal)

    k = 2
    xq = model.encode([""jerry loves candy""])

    d, i = index.search(xq, k)  # search
    print(i)
    print(d)

this code returns
[[0 1]]
[[1.3480902 1.6274161]]

but i cant find which sentence xq is matching with and not the matching scores only.
how can i find the top-n matching string from the corpus.","['python', 'numpy', 'huggingface-transformers', 'sentence-transformers', 'faiss']",73113673,"to retrieve the query results, try something like this using the variables from your code.
[corpus[i] for i in i]

but if you have corpus as a np.array object, you can do some cool slicing like this:
import numpy as np

# if you corpus are in array form.
corpus = np.array(['abc def', 'foo bar', 'bar bar sheep'])

# and indices can be list of integers.
indices = [1,0]

# results.
corpus[indices]

and it can get a little cooler if your indices are already np.array, like output of faiss, and if you have 2 queries with 1x2xk results:
import numpy as np

corpus = np.array(['abc def', 'foo bar', 'bar bar sheep'])

indices = np.array([[1,0], [0,2]])

corpus[indices]


additional notes
the faiss.indexflatl2 object returns these through the search() function:

labels ï¿½ï¿½ï¿½ output labels of the nns, size n*k

i.e. i in your code snippet refers to indices of the top-k results


distances ï¿½ï¿½ï¿½ output pairwise distances, size n*k

i.e. d in your code snippet referring to the distance of the top-k results from your query string.



since you have only 1 query, the n=1, therefore your i and d matrice are of size 1x1xk.
see also:

"" rel=""nofollow noreferrer"">",https://stackoverflow.com/questions/73089823,python,23-07-2022 10:10,581.0,2.0,2.0,True,25-07-2022 18:23,25-07-2022 18:21
78986801,importerror: cannot import name &#39;iterator&#39; from &#39;typing_extensions&#39; (/databricks/python/lib/python3.10/site-packages/typing_extensions.py),"i'm facing the following issue on azure databricks when trying to import from openai import openai after installing openai.
here's the error:
importerror: cannot import name 'iterator' from 'typing_extensions' (/databricks/python/lib/python3.10/site-packages/typing_extensions.py)

i looked up similar issues and found that using --force-reinstall like this:
pip install --force-reinstall typing-extensions==4.5
pip install --force-reinstall openai==1.8

worked for some users. however, it did not work in my case.
how do i resolve this?","['python', 'databricks', 'azure-databricks', 'openai-api']",78986810,"to resolve this erratic issue simply run dbutils.library.restartpython() after installing openai and before importing from openai import openai like this
!pip install openai==1.42.0

# restart the python process on databricks
dbutils.library.restartpython()

from openai import openai

as the databricks documentation says:
you can programmatically restart the python process on databricks (using dbutils.library.restartpython()) to ensure that locally installed or upgraded libraries function correctly in the python kernel for your current sparksession.",https://stackoverflow.com/questions/78986801,python,15-09-2024 07:06,138.0,0.0,1.0,True,15-09-2024 07:54,15-09-2024 07:54
30290338,keep non-stemmed tokens on elasticsearch,"i'm using a stemmer (for the brazilian portuguese language) when i index documents on elasticsearch. this is what my default analyzer looks like(nvm minor mistakes here because i've copied this by hand from my code in the server):
{
    ""analysis"":{
        ""filter"":{
            ""my_asciifolding"": {
                ""type"": ""asciifolding"",
                ""preserve_original"": true,  
            },
            ""stop_pt"":{
                ""type"": ""stop"",
                ""ignore_case"": true,
                ""stopwords"": ""_brazilian_""
            },
            ""stemmer_pt"": {
                ""type"": ""stemmer"",
                ""language"": ""brazilian""
            }
        },
        ""analyzer"": {
            ""default"": {
                ""type"": ""custom"",
                ""tokenizer"": ""standard"",
                ""filter"": [
                    ""lowercase"",
                    ""my_asciifolding"",
                    ""stop_pt"",
                    ""stemmer_pt""
                ]
            }
        }
    }
}

i haven't really touched my type mappings (apart from a few numeric fields i've declared ""type"":""long"") so i expect most fields to be using this default analyzer i've specified above.
this works as expected, but the thing is that some users are frustrated because (since tokens are being stemmed), the query ""vulnerabilities"" and the query ""vulnerable"" return the same results, which is misleading because they expect the results having an exact match to be ranked first.
whats is the default way (if any) to do this in elasticsearch? (maybe keep  the unstemmed tokens in the index as well as the stemmed tokens?) i'm using version 1.5.1.","['elasticsearch', 'information-retrieval']",30292385,"i ended up using ""fields"" field to index my attributes in different ways. not sure whether this is optimal but this is the way i'm handling it right now:

add another analyzer (i called it ""no_stem_analyzer"") with all filters that the ""default"" analyzer has, minus ""stemmer"".
for each attribute i want to keep both non stemmed and stemmed variants, i did this (example for field ""description""):
""mappings"":{
  ""_default_"":{
    ""properties"":{
      ""description"":{
        ""type""=>""string"",
        ""fields"":{
          ""no_stem"":{
            ""type"":""string"",
            ""index"":""analyzed"",
            ""analyzer"":""no_stem_analyzer""
          },
          ""stemmed"":{
            ""type"":""string"",
            ""index"":""analyzed"",
            ""analyzer"":""default""                
          }
        }
      } 
    },//.. other attributes here
  }
}  


at search time (using query_string_query) i must also indicate (using field ""fields"") that i want to search all sub-fields (e.g. ""description.*"")
i also based my approach upon [this answer].(elasticsearch customize score for synonyms/stemming)",https://stackoverflow.com/questions/30290338,elasticsearch,17-05-2015 18:10,301.0,0.0,1.0,True,11-08-2022 00:07,11-08-2022 00:07
79375157,correct topics from lda sequence model in gensim,"python's gensim package offers a dynamic topic model called ldaseqmodel(). i have run into the same problem as in this issue from the gensim mailing list (which has not been solved). the problem is that the model infers a topic that is logically impossible in the sense that it assigns a non-zero probability to a word in a time slice where the word was not used. this is a reproduction of the problem:
from gensim.corpora import dictionary
from gensim.models import ldaseqmodel

common_texts = [
    ['human', 'interface', 'computer'],
    ['survey', 'user', 'computer', 'system', 'response', 'time'],
    ['eps', 'user', 'interface', 'system'],
    ['system', 'human', 'system', 'eps'],
    ['user', 'response', 'time'],
    ['trees'],
    ['graph', 'trees'],
    ['graph', 'minors', 'trees'],
    ['graph', 'minors', 'survey']
]
common_dictionary = dictionary(common_texts)
common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]

model = ldaseqmodel(corpus=common_corpus, id2word=common_dictionary, num_topics=1, time_slice=[5, 4])

model.print_topic_times(topic=0)

time_slice=[5, 4] means that the first time slice contains the documents in the first 5 items of the common_texts list. the term graph is not in the first time slice, but print_topic_times() says it is. the output is:
[[('system', 0.13896054593348167),
  ('user', 0.10696589214152682),
  ('trees', 0.10664464447111177),
  ('graph', 0.10643809153102356),
  ('computer', 0.07494460648968987),
  ('human', 0.07494460648968987),
  ('interface', 0.07494460648968987),
  ('response', 0.07494460648968987),
  ('time', 0.07494460648968987),
  ('eps', 0.07494460648968987),
  ('minors', 0.07474199433434457),
  ('survey', 0.01658119265037265)],
 [('system', 0.13882862152464212),
  ('graph', 0.10742799576320598),
  ('trees', 0.10713473662111127),
  ('user', 0.1064043188010877),
  ('minors', 0.07517325760789559),
  ('computer', 0.07474729274679391),
  ('human', 0.07474729274679391),
  ('interface', 0.07474729274679391),
  ('response', 0.07474729274679391),
  ('time', 0.07474729274679391),
  ('eps', 0.07474729274679391),
  ('survey', 0.01654731320129382)]]

do i have to set additional parameters to obtain correct results?
i have run this with python 3.10.12 and gensim 4.3.3.
update january 23, 2025
i've experimented with the alphas, passes, and em_min_iter parameters, none of which have an effect on the problem.","['python', 'dynamic', 'gensim', 'topic-modeling']",79410881,"since the dynamic topic model (dtm) is a probabilistic model, word probabilities are never zero even for words that do not occurr in a time slice. but the dtm has a temporal smoothing parameter that influences the temporal continuity of topics. in the ldaseqmodel(), it's the chain_variance parameter. by increasing it, words that do not occurr in a time slice get lower probabilities, also in the toy model given above.",https://stackoverflow.com/questions/79375157,python,21-01-2025 16:24,83.0,0.0,1.0,True,04-02-2025 07:31,23-01-2025 11:29
71816814,split text into tokens on different rows in a dataframe,"i am new to this but i am trying to split text in a pandas dataframe into individual rows consisting of each tokens of the text and also its respective pos and tag. for example:
            text
   1        police officers arrest teen.
   2        man agrees to help.

what i am trying to achieve here is:
sentence#  token     pos   tag
   1       police    nns   b-np
           officers  nns   i-np
           arrest    vbp   b-vp
           teen      nn    b-np
   2       man       nnp   b-np
           agrees    vbz   b-vp
           to        to    b-vp
           help      vb    b-vp","['python', 'csv', 'split', 'token', 'pos-tagger']",71818071,"the nltk module can help you do what you want. this code makes use of nltk to create a new dataframe with similar output to your desired output. in order to get matching tags to your desired output, you will likely need to supply your own chunk parser. i am no expert in pos and iob tagging.
import pandas as pd
from nltk import word_tokenize, pos_tag, tree2conlltags, regexpparser

# orig data
d = {'text': [""police officers arrest teen."", ""man agrees to help.""]}
# orig dataframe
df = pd.dataframe(data = d)

# new data
new_d = {'sentence': [], 'token': [], 'pos': [], 'tag': []}

# grammar taken from nltk.org
grammar = r""np: {<[cdjnp].*>+}""
parser = regexpparser(grammar)

for idx, row in df.iterrows():
    temp = tree2conlltags(parser.parse(pos_tag(word_tokenize(row[""text""]))))
    new_d['token'].extend(i[0] for i in temp)
    new_d['pos'].extend(i[1] for i in temp)
    new_d['tag'].extend(i[2] for i in temp)
    new_d['sentence'].extend([idx + 1] * len(temp))

# new dataframe
new_df = pd.dataframe(data = new_d)

print(f""***original dataframe***\n\n {df}\n"")
print(f""***new dataframe***\n\n {new_df}"")

output:
***original dataframe***

                            text
0  police officers arrest teen.
1           man agrees to help.

***new dataframe***

    sentence     token  pos   tag
0         1    police  nnp  b-np
1         1  officers  nns  i-np
2         1    arrest  vbp     o
3         1      teen   nn  b-np
4         1         .    .     o
5         2       man   nn  b-np
6         2    agrees  vbz     o
7         2        to   to     o
8         2      help   vb     o
9         2         .    .     o

note after doing a pip install of nltk, before the above code can run, you will likely have to call nltk.download a few times. the error message you get should tell you what to execute. for example, you will likely need to execute this
>>> import nltk
>>> nltk.download('punkt')
>>> nltk.download('averaged_perceptron_tagger')",https://stackoverflow.com/questions/71816814,python,10-04-2022 12:57,853.0,0.0,1.0,True,10-04-2022 19:38,10-04-2022 15:01
63126386,where can i get the pretrained word embeddinngs for bert?,"i know that bert has total vocabulary size of 30522 which contains some words and subwords. i want to get the initial input embeddings of bert. so, my requirement is to get the table of size [30522, 768] to which i can index by token id to get its embeddings. where can i get this table?","['embedding', 'huggingface-transformers', 'bert-language-model']",63128700,"the bertmodels have get_input_embeddings():
import torch
from transformers import bertmodel, berttokenizer

tokenizer = berttokenizer.from_pretrained('bert-base-uncased')
bert = bertmodel.from_pretrained('bert-base-uncased')

token_embedding = {token: bert.get_input_embeddings()(torch.tensor(id))  for token, id in tokenizer.get_vocab().items()}

print(len(token_embedding))
print(token_embedding['[cls]'])

output:
30522
tensor([ 1.3630e-02, -2.6490e-02, -2.3503e-02, -7.7876e-03,  8.5892e-03,
        -7.6645e-03, -9.8808e-03,  6.0184e-03,  4.6921e-03, -3.0984e-02,
         1.8883e-02, -6.0093e-03, -1.6652e-02,  1.1684e-02, -3.6245e-02,
         8.3482e-03, -1.2112e-03,  1.0322e-02,  1.6692e-02, -3.0354e-02,
        -1.2372e-02, -2.5173e-02, -8.9602e-03,  8.1994e-03, -2.0011e-02,
        -1.5901e-02, -3.8394e-03,  1.4241e-03,  7.0500e-03,  1.6092e-03,
        -2.7764e-03,  9.4931e-03, -2.2768e-02,  1.9317e-02, -1.3442e-02,
        -2.3763e-02, -1.4617e-02,  9.7735e-03, -2.2428e-03,  3.0642e-02,
         6.7829e-03, -2.6471e-03, -1.8553e-02, -1.2363e-02,  7.6489e-03,
        -2.5461e-03, -3.1498e-01,  6.3761e-03,  4.8914e-02, -7.7636e-03,
         6.0919e-02,  2.1346e-02, -3.9741e-02,  2.2853e-01,  2.6502e-02,
        -1.0144e-03, -7.8480e-03, -1.9995e-03,  1.7057e-02, -3.3270e-02,
         4.5421e-03,  6.1751e-03, -1.0077e-01, -2.0973e-02, -1.4512e-04,
        -9.6657e-03,  1.0871e-02, -1.4786e-02,  2.6437e-04,  2.1166e-02,
         1.6492e-02, -5.1928e-03, -1.1857e-02, -9.9159e-03, -1.4363e-02,
        -1.2405e-02, -1.2973e-02,  2.6778e-02, -1.0986e-02,  1.0572e-02,
        -2.5566e-02,  5.2494e-03,  1.5890e-02, -5.1504e-03, -7.5859e-03,
         2.0259e-02, -7.0155e-03,  1.6359e-02,  1.7487e-02,  5.4297e-03,
        -8.6403e-03,  2.8821e-02, -7.8964e-03,  1.9259e-02,  2.3868e-02,
        -4.3472e-03,  5.5662e-02, -2.1940e-02,  4.1779e-03, -5.7216e-03,
         2.6712e-02, -5.0371e-03,  2.4923e-02, -1.3429e-02, -8.4337e-03,
         9.8188e-02, -1.2940e-03,  1.2865e-02, -1.5930e-03,  3.6437e-03,
         1.5569e-02,  1.8620e-02, -9.0643e-03, -1.9740e-02,  1.0530e-02,
        -2.7359e-03, -7.5283e-03,  1.1492e-03,  2.6162e-03, -6.2757e-03,
        -8.6096e-03,  6.6221e-01, -3.2235e-03, -4.1309e-02,  3.3047e-03,
        -2.5040e-03,  1.2838e-04, -6.8073e-03,  6.0291e-03, -9.8468e-03,
         8.0641e-03, -1.9815e-03,  2.5801e-02,  5.7429e-03, -1.0712e-02,
         2.9176e-02,  5.9414e-03,  2.4795e-02, -1.7887e-02,  7.3183e-01,
         1.0964e-02,  5.9942e-03, -4.6157e-02,  4.0131e-02, -9.7481e-03,
        -8.9496e-01,  1.6385e-02, -1.9816e-03,  1.4691e-02, -1.9837e-02,
        -1.7611e-02, -4.5263e-04, -1.8605e-02, -1.5660e-02, -1.0709e-02,
         1.8016e-02, -3.4149e-03, -1.2632e-02,  4.2877e-03, -3.9169e-01,
         1.0016e-02, -1.0955e-02,  4.5133e-03, -5.1150e-03,  4.9968e-03,
         1.7852e-02,  1.1313e-02,  2.6519e-03,  3.3658e-01, -1.8168e-02,
         1.3170e-02,  7.3927e-03,  5.2521e-03, -9.6230e-03,  1.2844e-02,
         4.1554e-01, -9.7247e-03, -4.2439e-03,  5.5287e-04,  1.8271e-02,
        -1.3889e-03, -2.0502e-03, -8.1946e-03, -6.5979e-06, -7.2764e-04,
        -1.4625e-03, -6.9872e-03, -6.9633e-03, -8.0701e-03,  1.9936e-02,
         4.8370e-03,  8.6883e-03, -4.9246e-02, -2.0028e-02,  1.4124e-03,
         1.0444e-02, -1.1236e-02, -4.4654e-03, -2.0491e-02, -2.7654e-02,
        -3.7079e-02,  1.3215e-02,  6.9498e-02, -3.1109e-02,  7.0562e-03,
         1.0887e-02, -7.8090e-03, -1.0501e-02, -4.8735e-03, -6.8399e-04,
         1.4717e-02,  4.4342e-03,  1.6012e-02, -1.0427e-02, -2.5767e-02,
        -2.2699e-01,  8.6569e-02,  2.3453e-02,  4.6362e-02,  3.5609e-03,
         2.1353e-02,  2.3703e-02, -2.0252e-02,  2.1580e-02,  7.2652e-03,
         2.0933e-01,  1.2108e-02,  1.0869e-02,  7.0568e-03, -3.1132e-02,
         2.0505e-02,  3.2248e-03, -2.2724e-03,  5.5342e-03,  3.0563e-03,
         1.9542e-02,  1.2827e-03,  1.5952e-02, -1.5458e-02, -3.8455e-03,
        -4.9417e-03, -1.0446e-02,  7.0516e-03,  2.2467e-03, -9.3643e-03,
         1.9163e-02,  1.4239e-02, -1.5816e-02,  8.7413e-03,  2.4737e-02,
        -7.3777e-03, -4.0975e-02,  9.4948e-03,  1.4700e-02,  2.6819e-02,
         1.0706e-02,  1.0621e-02, -7.1816e-03, -8.5402e-03,  1.2261e-02,
        -4.8679e-03, -9.6136e-03,  7.8765e-04,  3.8504e-02, -7.7485e-03,
        -6.5018e-03,  3.4352e-03,  2.2931e-04,  5.7456e-03, -4.8441e-03,
        -9.0898e-03,  8.6298e-03,  5.4740e-03,  2.2274e-02, -2.1218e-02,
        -2.6795e-02, -3.5337e-03,  1.0785e-02,  1.2475e-02, -6.1160e-03,
         1.0729e-02, -9.7955e-03,  1.8543e-02, -6.0488e-03, -4.5744e-03,
         2.7089e-03,  1.5632e-02, -1.2928e-02, -3.0778e-03, -1.0325e-02,
        -7.9550e-03, -6.3065e-02,  2.1062e-02, -6.6717e-03,  8.4616e-03,
         1.4475e-02,  1.1477e-01, -2.2838e-02, -3.7491e-02, -3.6218e-02,
        -3.1994e-02, -8.9252e-03,  3.1720e-02, -1.1260e-02, -1.2980e-01,
        -1.0315e-03, -4.7242e-03, -2.0092e-02, -9.4521e-01, -2.2178e-02,
        -4.4297e-04,  1.9711e-02,  3.3402e-02, -1.0513e-02,  1.4492e-02,
        -1.9697e-02, -9.8452e-03, -1.7347e-02,  2.3472e-02,  7.6570e-02,
         1.9504e-02,  9.3617e-03,  8.2672e-03, -1.0471e-02, -1.9932e-03,
         2.0000e-02,  2.0485e-02,  1.0977e-02,  1.7720e-02,  1.3532e-02,
         7.3682e-03,  3.4906e-04,  1.8772e-03,  1.9976e-02, -3.2041e-02,
        -8.9169e-03,  1.2900e-02, -1.3331e-02,  6.6207e-03, -5.7063e-03,
        -1.1482e-02,  8.3907e-03, -6.4162e-03,  1.5816e-02,  7.8921e-03,
         4.4177e-03,  2.2568e-02,  1.0239e-02, -3.0194e-04,  1.3294e-02,
        -2.1606e-02,  3.8832e-03,  2.4475e-02,  4.3808e-02, -2.1031e-03,
        -1.2163e-02, -4.0786e-02,  1.5565e-02,  1.4750e-02,  1.6645e-02,
         2.8083e-02,  1.8920e-03, -1.4733e-04, -2.6208e-02,  2.3780e-02,
         1.8657e-04, -2.2931e-03,  3.0334e-03, -1.7294e-02, -2.3001e-02,
         8.6004e-03, -3.3497e-02,  2.5660e-02, -1.9225e-02, -2.7186e-02,
        -2.1020e-02, -3.5213e-02, -1.8228e-03, -8.2840e-03,  1.1212e-02,
         1.0387e-02, -3.4194e-01, -1.9705e-03,  1.1558e-02,  5.1976e-03,
         7.4498e-03,  5.7142e-03,  2.8401e-02, -7.7551e-03,  1.0682e-02,
        -1.2657e-02, -1.8065e-02,  2.6681e-03,  3.3947e-03, -4.5565e-02,
        -2.1170e-02, -1.7830e-02,  3.4679e-03, -2.2051e-02, -5.4176e-03,
        -1.1517e-02, -3.4155e-02, -3.0335e-03, -1.3915e-02,  6.2173e-03,
        -1.1101e-02, -1.5308e-02,  9.2188e-03, -7.5665e-03,  6.5685e-03,
         8.0935e-03,  3.1139e-03, -5.5047e-03, -3.1347e-02,  2.2140e-02,
         1.0865e-02, -2.7849e-02, -4.9580e-03,  1.8804e-03,  1.0007e-01,
        -1.8013e-03, -4.8792e-03,  1.5534e-02, -2.0179e-02, -1.2351e-02,
        -1.3871e-02,  1.1439e-02, -9.0208e-03,  1.2580e-02, -2.5973e-02,
        -2.0398e-02, -1.9464e-03,  4.3189e-03,  2.0707e-02,  5.0029e-03,
        -1.0679e-02,  1.2298e-02,  1.0269e-02,  2.2228e-02,  2.9754e-02,
        -2.6392e-03,  1.9286e-02, -1.5137e-02,  2.1914e-01,  1.3030e-02,
        -7.4460e-03, -9.6818e-04,  2.9736e-02,  9.8722e-03, -5.6688e-03,
         4.2518e-03,  1.8941e-02, -6.3909e-03,  8.0590e-03, -6.7893e-03,
         6.0878e-03, -5.3970e-03,  7.5776e-04,  1.1374e-03, -5.0035e-03,
        -1.6159e-03,  1.6764e-02,  9.1251e-03,  1.3020e-02, -1.0368e-02,
         2.2141e-02, -2.5411e-03, -1.5227e-02,  2.3444e-02,  8.4076e-04,
        -1.1465e-01,  2.7017e-03, -4.4961e-03,  2.9762e-04, -3.9612e-03,
         8.9038e-05,  2.8683e-02,  5.0068e-03,  1.6509e-02,  7.8983e-04,
         5.7728e-03,  3.2685e-02, -1.0457e-01,  1.2989e-02,  1.1278e-02,
         1.1943e-02,  1.5258e-02, -6.2411e-04,  1.0682e-04,  1.2087e-02,
         7.2984e-03,  2.7758e-02,  1.7572e-02, -6.0345e-03,  1.7211e-02,
         1.4121e-02,  6.4663e-02,  9.1813e-03,  3.2555e-03, -3.2667e-02,
         2.9132e-02, -1.7770e-02,  1.5302e-03, -2.9944e-02, -2.0706e-02,
        -3.6528e-03, -1.5497e-02,  1.5223e-02, -1.4751e-02, -2.2381e-02,
         6.9636e-03, -8.0838e-03, -2.4583e-03, -2.0677e-02,  8.8132e-03,
        -6.9554e-04,  1.6965e-02,  1.8535e-01,  3.5843e-04,  1.0812e-02,
        -4.2391e-03,  8.1779e-03,  3.4144e-02, -1.8996e-03,  2.9939e-03,
         3.6898e-04, -1.0144e-02, -5.7416e-03, -5.7676e-03,  1.7565e-01,
        -1.5793e-03, -2.6617e-02, -1.2572e-02,  3.0421e-04, -1.2132e-02,
        -1.4168e-02,  1.2154e-02,  8.4700e-03, -1.6284e-02,  2.6983e-03,
        -6.8554e-03,  2.7829e-01,  2.4060e-02,  1.1130e-02,  7.6095e-04,
         3.1341e-01,  2.1668e-02,  1.0277e-02, -3.0065e-02, -8.3565e-03,
         5.2488e-03, -1.1287e-02, -1.8266e-02,  1.1814e-02,  1.2662e-02,
         2.9036e-04,  7.0254e-04, -1.4084e-02,  1.2925e-02,  3.9504e-03,
        -7.9568e-03,  3.2794e-02,  7.3839e-03,  2.4609e-02,  9.6109e-03,
        -8.7206e-03,  9.2571e-03, -3.5850e-03, -8.9996e-03,  2.3120e-03,
        -1.8475e-02, -1.9610e-02,  1.1994e-02,  6.7156e-03,  1.9903e-02,
         3.0703e-02, -4.9538e-03, -6.1673e-02, -6.4986e-03, -2.1317e-02,
        -3.3650e-03,  2.3200e-03, -6.2224e-03,  3.7458e-03,  1.1542e-02,
        -1.0181e-02, -8.4711e-03,  1.1603e-02, -5.6247e-03, -1.0220e-02,
        -8.6501e-04, -1.2285e-02, -8.7487e-03, -1.1265e-02,  1.6322e-02,
         1.5160e-02,  1.8882e-02,  5.1557e-03, -8.8616e-03,  4.2153e-03,
        -1.9450e-02, -8.7365e-03, -9.7867e-03,  1.1667e-02,  5.0613e-03,
         2.8221e-03, -7.1795e-03,  9.3306e-03, -4.9663e-02,  1.7708e-02,
        -2.0959e-02, -3.3989e-02,  2.2581e-03,  5.1748e-03, -1.0133e-01,
         2.1052e-03,  5.5644e-03,  1.3607e-03,  8.8388e-03,  1.0244e-02,
        -3.8072e-03,  5.9209e-03,  6.7993e-03,  1.1594e-02, -1.1802e-02,
        -2.4233e-03, -5.1504e-03, -1.1903e-02,  1.4075e-02, -4.0701e-03,
        -2.9465e-02, -1.7579e-03,  4.3654e-03,  1.0429e-02,  3.7096e-02,
         8.6493e-03,  1.5871e-02,  1.8034e-02, -3.2165e-03, -2.1941e-02,
         2.6274e-02, -7.6941e-03, -5.9618e-03, -1.4179e-02,  8.0281e-03,
         1.1293e-02, -6.6936e-05,  1.2899e-02,  1.0056e-02, -6.3919e-04,
         2.0299e-02,  3.1528e-03, -4.8988e-03,  3.2754e-03, -1.1003e-01,
         1.8414e-02,  2.2272e-03, -2.2185e-02, -4.8672e-03,  1.9643e-03,
         3.0928e-02, -8.9599e-03, -1.1446e-02, -1.3794e-02,  7.1943e-03,
        -5.8965e-03,  2.2605e-03, -2.6114e-02, -5.6616e-03,  6.5073e-03,
         9.2219e-02, -6.7243e-03,  4.4427e-04,  7.2846e-03, -1.1021e-02,
         7.8802e-04, -3.8878e-03,  1.0489e-02,  9.2883e-03,  1.8895e-02,
         2.1808e-02,  6.2590e-04, -2.6519e-02,  7.0343e-04, -2.9067e-02,
        -9.1515e-03,  1.0418e-03,  8.3222e-03, -8.7548e-03, -2.0637e-03,
        -1.1450e-02, -8.8985e-04, -4.4062e-03,  2.3629e-02, -2.7221e-02,
         3.2008e-02,  6.6325e-03, -1.1302e-02, -1.0138e-03, -1.6902e-01,
        -8.4473e-03,  2.8536e-02,  1.4117e-03, -1.2136e-02, -1.4781e-02,
         4.9960e-03,  3.3916e-02,  5.2710e-03,  1.7382e-02, -4.6315e-03,
         1.1680e-02, -9.1395e-03,  1.8310e-02,  1.2321e-02, -2.4871e-02,
         1.1535e-02,  5.0308e-03,  5.5028e-03, -7.2184e-03, -5.5210e-03,
         1.7085e-02,  5.7236e-03,  1.7463e-03,  1.9969e-03,  6.1670e-03,
         2.9347e-03,  1.3946e-02, -1.9984e-03,  1.0091e-02,  1.0388e-03,
        -6.1902e-03,  3.0905e-02,  6.6038e-03, -9.1223e-02, -1.8411e-02,
         5.4185e-03,  2.4396e-02,  1.5696e-02, -1.2742e-02,  1.8126e-02,
        -2.6138e-02,  1.1170e-02, -1.3058e-02, -1.9386e-02, -5.9828e-03,
         1.9176e-02,  1.9962e-03, -2.1538e-03,  3.3003e-02,  1.8407e-02,
        -5.9498e-03, -3.2533e-03, -1.8917e-02, -1.5897e-02, -4.7057e-03,
         5.4162e-03, -3.0037e-02,  8.6773e-03, -1.7942e-03,  6.6826e-03,
        -1.1929e-02, -1.4076e-02,  1.6709e-02,  1.6860e-03, -3.3842e-03,
         8.6805e-03,  7.1340e-03,  1.5147e-02], grad_fn=<embeddingbackward>)",https://stackoverflow.com/questions/63126386,embedding,28-07-2020 02:57,2703.0,3.0,2.0,True,26-03-2022 15:59,03-08-2020 01:03
76961114,how can i train a model for specific information extraction,"i have a dataset of verdict sentences texts that looks like this
""the defendant has been found guilty of armed robbery and is hereby sentenced to 10 years in prison, and sentencing him to a fine of five dollars and fees. after considering the defendant's cooperation with the investigation and lack of prior convictions, the sentence is reduced to 5 years in prison starting from the day this ruling becomes final.""
how can i train/tune a model/(which models are best) and preprocess the data to understand the text and give me only the reduced sentence ""5 years in prison starting from the day this ruling becomes final."" as output","['deep-learning', 'nlp']",76961188,"this looks like a problem to be framed as summary generation task.
to keep things simple and provide you with a starting point, here are some essential aspects to consider

you can have 2 types of summarization: abstractive and extractive.

your case is the former, as you do want to have just a
summary/inherent logic of the input text captured, not to extract only some
specific words. if you wanted only the latter you could also frame your problem as a ner (named-entity recognition one), not only an extractive summarization one.

you would need to generate training pairs such as [full_text,summary] in order to train your model.

for such a dataset (abstractive summarization) you can try searching
online, here is an example of such dataset :


once you see how the dataset looks like, you can try to create a
similar dataset on your own and then fine-tune an llm to cater for
your use-case. here is a tutorial on huggingface on how to perform fine-tuning on abstractive summarization on the exact dataset specified above:",https://stackoverflow.com/questions/76961114,deep-learning,23-08-2023 11:55,486.0,0.0,1.0,True,23-08-2023 13:21,23-08-2023 11:58
76117261,why in this regex pattern left \b performs as left \b?,"why in this case left \b performs as left \b?
\b\$[0-9]+(\.[0-9][0-9])?\b

this pattern should omit phrases such a$99.99 because of performance of left \b.
\b detects phrases which  have  not been bounded with letters, digits or underscore. but not! i examined it in regex101.
as you see it detects phrases such tyh666.8 but doesn't detect $99
however, right \b performs completely correctly!
surprisingly, i changed it to left \b it worked!
\b detects phrases which have been bounded with letters, digits or underscore. but here it works as a left \b!
and i have no idea why?!
as you see it detects phrases which have not been bounded with letters, digits or underscore","['regex', 'nlp', 'boundary']",76117391,"your understanding of \b is incorrect. it matches positions where there are ""word"" characters on one side, and ""not word"" characters on the other. $ is not a word character, so it will only match where $ is immediately preceded by a word character (alphanumerics, plus in some implementations e.g. @ and _)",https://stackoverflow.com/questions/76117261,regex,27-04-2023 06:16,88.0,-1.0,1.0,True,27-04-2023 06:34,27-04-2023 06:24
74563930,how can i fix this &quot;indentationerror: expected an indented block&quot;?,"def remove_stopwords(text,nlp,custom_stop_words=none,remove_small_tokens=true,min_len=2):
    if custom_stop_words:
       nlp.defaults.stop_words |= custom_stop_words
    filtered_sentence =[] 
    doc = nlp (text)
    for token in doc:
    
        if token.is_stop == false: 
        
           if remove_small_tokens:
              if len(token.text)>min_len:
                 filtered_sentence.append(token.text)
          else:
              filtered_sentence.append(token.text) 
              return "" "".join(filtered_sentence) 
          if len(filtered_sentence)>0
          else none

i am getting the error for the last else:
the goal of this last part is, if after the stopword removal, words are still left in the sentence, then the sentence should be returned as a string else return null. i'd be so thankful for any advice.
else none
^
indentationerror: expected an indented block","['python', 'python-3.x', 'if-statement', 'nlp', 'topic-modeling']",74564195,"your entire code is not properly indented
def remove_stopwords(text,nlp,custom_stop_words=none,remove_small_tokens=true,min_len=2):
    if custom_stop_words:
        nlp.defaults.stop_words |= custom_stop_words

    filtered_sentence =[] 
    doc = nlp (text)
    for token in doc:
        
        if token.is_stop == false: 
            
            if remove_small_tokens:
                if len(token.text)>min_len:
                    filtered_sentence.append(token.text)
            else:
                filtered_sentence.append(token.text)
                
    if len(filtered_sentence) > 0:           
        return "" "".join(filtered_sentence) 
    else:
        return none",https://stackoverflow.com/questions/74563930,python,24-11-2022 16:42,82.0,0.0,2.0,True,10-12-2023 01:45,24-11-2022 16:55
62262359,annotating entities from multiple token-spanning entities,"i am building a unit annotater based on the idea of medacy. first all basic unit types are set, which are then used to build more complex units. for example: 

m/sï¿½ï¿½ → m distance / s duration ï¿½ï¿½ → m/s speed ï¿½ï¿½ → m/sï¿½ï¿½
  acceleration

for this purpose i changed part of the tokenization so that numbers are always seperated from alphabetic characters and such.

'< 2.0 m/sï¿½ï¿½' → ['<', '2.0', 'm', '/', 's', 'ï¿½ï¿½']

however, my current issue is that i can only achieve the last step (acceleration) by merging the tokens whenever an entity is recognized. this results in the loss of feature and entity information of underlying tokens which i definetly want to avoid. therefore i disabled the merging of tokens, but now i cannot achieve the last step of annotating entities like acceleration. this is because the matcher works token-based. as seen below the matcher is not able to detect the entity since it spans over multiple tokens. (notd correctly over multiple tokens.)
[{'ent_type': 'speed'}, {'text': {'regex': r'(^)?2|ï¿½ï¿½'}}]
['<', '2.0', 'm', '/', 's', 'ï¿½ï¿½']

adding all the possible token combinations to the acceleration matcher would be a none solution for me since it would interfere with the concept of building all units from bottom up.
another solution which has come to my mind is using multiple entity rulers, since first the basic units would have to be tagged and then the subsequently the more complex one. however, it seems to run into the same tokenization issue and furthermore i get the error message that there can only be one entity ruler. 'entity_ruler' already exists in pipeline. existing names: ['entity_ruler']
in summary i want to annotate entities using entities spanning over multiple tokens. hence token based matching does not work.

this is called right after creating a blank spacy model.
def remove_units(nlp):
    suffixesnlp.defaults.suffixes)
    units = '(?<=[0-9])(?:km|kmï¿½ï¿½|kmï¿½ï¿½|m|mï¿½ï¿½|mï¿½ï¿½|dm|dmï¿½ï¿½|dmï¿½ï¿½|cm|cmï¿½ï¿½|cmï¿½ï¿½|mm|mmï¿½ï¿½|mmï¿½ï¿½|ha|ï¿½ï¿½m|nm|yd|in|ft|kg|g|mg|ï¿½ï¿½g|t|lb|oz|m/s|km/h|kmh|mph|hpa|pa|mbar|mb|mb|kb|kb|gb|gb|tb|tb|t|g|m|k|%|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½/ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½/ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½s# for splitting backslashes from other characters '/s' -> '/','s'
    prefix_regex = spacy.util.compile_prefix_regex(prefixes)
    nlp.tokenizer.prefix_search = prefix_regex.search


from spacy.matcher import matcher
from spacy.tokens import span
from spacy.tokens import token
import re


class unitannotator(object):
    name=""unit_annotator""
    dependencies = []

    def __init__(self, nlp):
        self.nlp = nlp

        token.set_extension('is_duration_unit', default=false, force=true)
        token.set_extension('is_memory_unit', default=false, force=true)
        token.set_extension('is_fraction_unit', default=false, force=true)
        token.set_extension('is_angle_unit', default=false, force=true)
        token.set_extension('is_distance_unit', default=false, force=true)
        token.set_extension('is_pressure_unit', default=false, force=true)
        token.set_extension('is_voltage_unit', default=false, force=true)
        token.set_extension('is_speed_unit', default=false, force=true)
        token.set_extension('is_acceleration_unit', default=false, force=true)
        token.set_extension('is_frequency_unit', default=false, force=true)
        token.set_extension('is_volume_unit', default=false, force=true)
        token.set_extension('is_torque_unit', default=false, force=true)
        token.set_extension('is_operator', default=false, force=true)
        token.set_extension('is_measurement', default=false, force=true)

        # for splitting equations first
        self.split_matcher1 = matcher(nlp.vocab)
        self.split_matcher1.add('split1', none,
            [{'text': {'regex': r'[/\>\<]'}}], # 'km/h' -> 'km','/','h'

        )
        self.split_matcher2 = matcher(nlp.vocab)
        self.split_matcher2.add('split2', none,
            [{'text': {'regex': r'[ï¿½ï¿½ï¿½ï¿½]'}}], # 'mï¿½ï¿½' -> 'm','ï¿½ï¿½'
            [{'text': {'regex': r'\)'}}], # '8)' -> '8',')'
            # todo: fix splitting of '(mis)interventions'
        )       


        self.duration_matcher = matcher(nlp.vocab)
        self.duration_matcher.add('unit_of_duration', none,
                            [{'orth': 'ms'}],
                            [{'lower': 'msec'}],
                            [{'lower': 'milisecond'}],
                            [{'lower': 'miliseconds'}],                            
                            [{'orth': '                  [{'lower': 'sec'}],
                            [{'lower': 'second'}],
                            [{'lower': 'seconds'}],  
                            [{'lower': 'min'}],
                            [{'lower': 'mins'}],
                            [{'lower': 'minute'}],
                            [{'lower': 'minutes'}],                            
                            [{'orth': 'h'}],
                            [{'lower': 'hour'}],
                            [{'lower': 'hours'}]
        )

        self.memory_matcher = matcher(nlp.vocab)
        self.memory_matcher.add('unit_of_memory', none,
                            [{'lower': 'kb'}],
                            [{'lower': 'kbs'}],
                            [{'lower': 'kbit'}],
                            [{'lower': 'kbits'}],
                            [{'lower': 'mb'}],
                            [{'lower': 'mbs'}],
                            [{'lower': 'mbit'}],
                            [{'lower': 'mbits'}],
                            [{'lower': 'gb'}],
                            [{'lower': 'gbs'}],
                            [{'lower': 'gbit'}],
                            [{'lower': 'gbits'}],
                            [{'lower': 'tb'}],
                            [{'lower': 'tbs'}],
                            [{'lower': 'bit'}],
                            [{'lower': 'bits'}],
                            [{'lower': 'byte'}],
                            [{'lower': 'bytes'}],
                            [{'lower': 'kilobyte'}],
                            [{'lower': 'kilobytes'}],
                            [{'lower': 'megabyte'}],
                            [{'lower': 'megabytes'}],
                            [{'lower': 'gigabyte'}],
                            [{'lower': 'gigabytes'}],
                            [{'lower': 'terrabyte'}],
                            [{'lower': 'terrabytes'}],                        
        )

        self.fraction_matcher = matcher(nlp.vocab)
        self.fraction_matcher.add('unit_of_fraction', none,
                            [{'orth': '%'}],
                            [{'lower': 'percent'}],
                            [{'lower': 'per'}, {'lower': 'cent'}]
        )

        self.angle_matcher = matcher(nlp.vocab)
        self.angle_matcher.add('unit_of_angle', none,
                            [{'lower': 'ï¿½ï¿½'}],
                            [{'lower': 'ï¿½ï¿½c'}],
                            [{'lower': 'deg'}],
                            [{'lower': 'degs'}],
                            [{'lower': 'degree'}],
                            [{'lower': 'degrees'}],
        )

        self.distance_matcher = matcher(nlp.vocab)
        self.distance_matcher.add('unit_of_distance', none,
                            [{'orth': 'nm'}],    
                            [{'lower': 'nanometer'}],
                            [{'lower': 'nanometers'}],
                            [{'orth': 'ï¿½ï¿½m'}],    
                            [{'lower': 'micrometer'}],
                       r': 'mircometers'}],
                            [{'orth': 'mm'}],    
                            [{'lower': 'milimeter'}],
                            [{'lower': 'milimeters'}],
                            [{'orth': 'cm'}], 
                            [{'lower': 'cendurationter'}],
                            [{'lower': 'cendurationters'}],
                            [{'orth': 'm'}],
                            [{'lower': 'meter'}],
                            [{'lower': 'meters'}],
                            [{'orth': 'km'}],
                            [{'lower': 'kilometer'}],
                            [{'lower': 'kilometers'}],
                            [{'lower': 'zoll'}],                        
        )

        self.pressure_matcher = matcher(nlp.vocab)
        self.pressure_matcher.add('unit_of_pressure', none,
                            [{'lower': 'bar'}] # maybe add f/a
        )

        self.voltage_matcher = matcher(nlp.vocab)
        self.voltage_matcher.add('unit_of_voltage', none,
                            [{'orth': 'v'}],
                            [{'lower': 'volt'}],
        )        

        self.speed_matcher = matcher(nlp.vocab)
        self.speed_matcher.add('unit_of_speed', none,
                            [{'ent_type': 'distance'}, {'lower': {'regex': r'/|p'}}, {'ent_type': 'duration'}]
        )

        self.acceleration_matcher = matcher(nlp.vocab)
        self.acceleration_matcher.add('unit_of_acceleration', none,
                            [{'ent_type': 'speed'}, {'text': {'regex': r'(^)?2|ï¿½ï¿½'}}]
        )

        self.frequency_matcher = matcher(nlp.vocab)
        self.frequency_matcher.add('unit_of_frequency', none,
                            [{'lower': 'hz'}],
                            [{'lower': 'herz'}], # common misspelling
                            [{'lower': 'hertz'}],
                            [{'lower': '1'}, {'orth': '/'}, {'ent_type': 'duration'}]
        )

        self.volume_matcher = matcher(nlp.vocab)
        self.volume_matcher.add('uni_volume', none,
                            [{'lower': 'l'}],
                            [{'lower': 'liter'}],
                            [{'ent_type': 'distance'}, {'text': {'regex': r'(^)?3|ï¿½ï¿½'}}]
        )

        self.torque_matcher = matcher(nlp.vocab)
        self.torque_matcher.add('unit_of_torque', none,
                            [{'orth': 'nm'}],
                            [{'lower': 'newtonmeter'}]
        )

        # todo: rpm matcher

        self.operator_matcher = matcher(nlp.vocab)
        self.operator_matcher.add('operator', none, # for now only < and >
                            [{'orth': '<'}, {'like_num': true}],
                            [{'orth': '>'}, {'like_num': true}],
                            [{'orth': '<'}, {'orth': '='}, {'like_num': true}],
                            [{'orth': '>'}, {'orth': '='}, {'like_num': true}],
                            [{'orth': '+'}, {'orth': '/'}, {'like_num': true}], # like_num already includes + and -
        )

    self.measurement_matcher = matcher(nlp.vocab)
        self.measurement_matcher.add('measurement', none,
                            [{'like_num': true}, {'ent_type': 'duration'}],
                            [{'like_num': true}, {'ent_type': 'memory'}],
                            [{'like_num': true}, {'ent_type': 'fraction'}],
                            [{'like_num': true}, {'ent_type': 'angle'}],
                            [{'like_num': true}, {'ent_type': 'distance'}],
                            [{'like_num': true}, {'ent_type': 'pressure'}],
                            [{'like_num': true}, {'ent_type': 'voltage'}],
                            [{'like_num': true}, {'ent_type': 'speed'}],
                            [{'like_num': true}, {'ent_type': 'acceleration'}],
                            [{'like_num': true}, {'ent_type': 'frequency'}],
                            [{'like_num': true}, {'ent_type': 'volume'}],
                            [{'like_num': true}, {'ent_type': 'torque'}],
                            [{'ent_type': 'operator'},{'ent_type': 'duration'}],
                            [{'ent_type': 'operator'},{'ent_type': 'memory'}],
                            [{'ent_type': 'operator'},{'ent_type': 'fraction'}],
                            [{'ent_type': 'operator'},{'ent_type': 'angle'}],
                            [{'ent_type': 'operator'},{'ent_type': 'distance'}],
                            [{'ent_type': 'operator'},{'ent_type': 'pressure'}],
                            [{'ent_type': 'operator'},{'ent_type': 'voltage'}],
                            [{'ent_type': 'operator'},{'ent_type': 'speed'}],
                            [{'ent_type': 'operator'},{'ent_type': 'acceleration'}],
                            [{'ent_type': 'operator'},{'ent_type': 'frequency'}],
                            [{'ent_type': 'operator'},{'ent_type': 'volume'}],
                            [{'ent_type': 'operator'},{'ent_type': 'torque'}],
                            # todo: 20 ... 30 unit, 20 to 30 unit, 20 of 60 unit
        )

    def __call__(self, doc):
        nlp = self.nlp

        # split tokens containg a backslash 'km/h' -> 'km','/','h'
        with doc.retokenize() as retokenizer:    
            matches1 = self.split_matcher1(doc)
            for match_id, start, end in matches1:
                span = span(doc, start, end)       
                if len(span.text) > 1:
                    if '/' in span.text:
                        split = re.split('(/)', span.text)
                    if '>' in span.text:
                        split = re.split('(\>)', span.text)
                    if '<' in span.text:
                        split = re.split('(\<)', span.text)
                    heads = [(doc[start], i) for i,_ in enumerate(split)]
                    retokenizer.split(doc[start], split, heads=heads)
        # split tokens containg ')ï¿½ï¿½ï¿½ï¿½'
        with doc.retokenize() as retokenizer:
            matches2 = self.split_matcher2(doc)
            for match_id, start, end in :
                span = span(doc, start, end)       
                if len(span.text) > 1:
                    split = [x for x in span.text]
                    heads = [(doc[start], i) for i,_ in enumerate(split)]
                    retokenizer.split(doc[start], split, heads=heads)

        def annotate(matcher, unit_type: str, attribute):
            with doc.retokenize() as retokenizer:
                #match and tag units
                matches = matcher(doc)
                entities = list(doc.ents)
                add_flag = true
                for match_id, start, end in matches:                    
                    span = span(doc, start, end, label=unit_type)
                    for token in span:
                        setattr(token._, attribute, true)                   
                    try:
                        if len(span) > 1:
                            #retokenizer.merge(span)
                            pass
                    except valueerror:
                        pass

                    for e in entities[:]:
                        r_e = range(e.start+1,e.end+1)
                        r_n = range(start+1,end+1)
                        # remove smaller entities which would overlap with the new one
                        if (end-start > e.end-e.start and (start+1 in r_e or end in r_e)) or (start < e.start and end > e.end):
                            entities.remove(e)
                            continue
                        # check if entity to be added would overlap with an existing bigger one 
                        if (e.end-e.start > end-start and (e.start+1 in r_n or e.end in r_n)) or (e.start < start and e.end > end): 
                            add_flag = false

                    if(add_flag):
                        entities.append(span)

                    add_flag = true

                doc.ents = entities

        annotate(self.duration_matcher, 'duration', 'is_duration_unit')
        annotate(self.memory_matcher, 'memory', 'is_memory_unit')
        annotate(self.fraction_matcher, 'fraction', 'is_fraction_unit')        
        annotate(self.angle_matcher, 'angle', 'is_angle_unit')
        annotate(self.distance_matcher, 'distance', 'is_distance_unit')
        annotate(self.pressure_matcher, 'pressure', 'is_pressure_unit')
        annotate(self.voltage_matcher, 'voltage', 'is_voltage_unit')
        annotate(self.speed_matcher, 'speed', 'is_speed_unit')
        annotate(self.acceleration_matcher, 'acceleration', 'is_acceleration_unit')
        annotate(self.frequency_matcher, 'frequency', 'is_frequency_unit')
        annotate(self.volume_matcher, 'volume', 'is_volume_unit')
        annotate(self.torque_matcher, 'torque', 'is_torque_unit')
        annotate(self.operator_matcher, 'operator', 'is_operator')
        annotate(self.measurement_matcher, 'measurement', 'is_measurement')

        return doc","['nlp', 'spacy']",62279376,"i have since found a rather obvious solution.

['<', '2.0', 'm', '/', 's', 'ï¿½ï¿½']
m speed / speed s speed

those are three tokens of the entity type speed. therefore it is enough to use the 'one or more' quantifier.
[{'ent_type': 'speed', 'op': '+'}, {'text': {'regex': r'(^)?2|ï¿½ï¿½'}}]

in this solution, the entity types are still overwritten, but the underlying units are still stored as features on each tok",https://stackoverflow.com/questions/62262359,nlp,08-06-2020 12:42,604.0,0.0,1.0,True,20-07-2022 12:54,08-06-2020 13:27
78288681,using chatgpt4 vision in asp.net core,"i am working on a web application with openai integration. i have the standard chat prompt and response implemented, but i am having issues accessing the vision api. all of the examples i can find are in python. i whipped up quick jupyter notebook and called the vision model with my api key and it worked great. looking for a way to translate this code into c# or a viable solution to use the code in my application.
from openai import openai

client = openai(api_key=""___"")

response = client.chat.completions.create(
    model=""gpt-4-vision-preview"",
    messages=[
    {
      ""role"": ""user"",
      ""content"": [
        {""type"": ""text"", ""text"": ""does this image show a fender stratocaster electric guitar?        respond with yes or no.""},
        {
          ""type"": ""image_url"",
          ""image_url"": {
            ""url"": ""
          },
        },
      ],
    }
  ],
  max_tokens=300,
)

print(response.choices[0])

i have tried to access ""chat.completions"" from my openai instance in c#, but they do not seem to exist.","['c#', 'asp.net-core', 'openai-api', 'chat-gpt-4']",78288778,"the documentation includes a curl sample which can easily be translated to a c# 
curl  \
  -h ""content-type: application/json"" \
  -h ""authorization: bearer $openai_api_key"" \
  -d '{
    ""model"": ""gpt-4-vision-preview"",
    ""messages"": [
      {
        ""role"": ""user"",
        ""content"": [
          {
            ""type"": ""text"",
            ""text"": ""whatï¿½ï¿½ï¿½s in this image?""
          },
          {
            ""type"": ""image_url"",
            ""image_url"": {
              ""url"": ""
            }
          }
        ]
      }
    ],
    ""max_tokens"": 300
  }'
<",https://stackoverflow.com/questions/78288681,c#,07-04-2024 17:25,589.0,-1.0,1.0,True,27-07-2024 21:55,27-07-2024 21:55
76416680,how to structure data for question-answering task to fine-tune a model with huggingface run_qa.py example?,"import sagemaker
import boto3
from sagemaker.huggingface import huggingface

try:
    role = sagemaker.get_execution_role()
except valueerror:
    iam = boto3.client('iam')
    role = iam.get_role(rolename='sagemaker_execution_role')['role']['arn']
        
hyperparameters = {
    'model_name_or_path':'t5-base',
    'output_dir':'/opt/ml/model'
    # add your remaining hyperparameters
    # more info here 
}

# git configuration to download our fine-tuning script
git_config = {'repo': ' 'v4.26.0'}

# creates hugging face estimator
huggingface_estimator = huggingface(
    entry_point='run_qa.py',
    source_dir='./examples/pytorch/question-answering',
    instance_type='ml.p3.2xlarge',
    instance_count=1,
    role=role,
    git_config=git_config,
    transformers_version='4.26.0',
    pytorch_version='1.13.1',
    py_version='py39',
    hyperparameters = hyperparameters
)

# starting the train job
huggingface_estimator.fit()

given the above script (launch_training.py) which can be found here:  how should my data be structured for a generative question-answering task?
for context: i am training t5 on some synthetic company text data so that i can then prompt it with questions such as ""how can companyx improve sales?"" or ""how can companyx reduce the turnover rate?""
i have tried formatting my data as question-answer pairs, e.g. {""question"": ""how can companyx improve the performance of their marketing campaigns?"", ""answer"": ""the recent marketing campaign of companyx attracted a 20% increase in new customers. it suggests that if companyx focuses on customer-centric strategies and amplifies their digital marketing efforts, they might achieve even better results.""} but this gives valueerror: need either a dataset name or a training/validation file
i am passing an s3 uri to huggingface_estimator.fit(), namely huggingface_estimator.fit({""train_data_uri"": ""s3://fine-tuning/q-a_pairs.json""})","['python', 'nlp', 'huggingface-transformers', 'language-model', 'nlp-question-answering']",76425973,"the code snippet you're using with sagemaker and the huggingface example comes from 
the example uses the dataset formatted as how it is in the squad dataset, 
each example should look like this:
{
    ""answers"": {
        ""answer_start"": [1],
        ""text"": [""this is a test text""]
    },
    ""context"": ""this is a test context."",
    ""id"": ""1"",
    ""question"": ""is this a test?"",
    ""title"": ""train test""
}

the actual data file from squad would come from  and looks something like:
{
    ""context"": ""following the disbandment of destiny's child in june 2005, she released her second solo album, b'day (2006), which contained hits \""d\u00e9j\u00e0 vu\"", \""irreplaceable\"", and \""beautiful liar\"". beyonc\u00e9 also ventured into acting, with a golden globe-nominated performance in dreamgirls (2006), and starring roles in the pink panther (2006) and obsessed (2009). her marriage to rapper jay z and portrayal of etta james in cadillac records (2008) influenced her third album, i am... sasha fierce (2008), which saw the birth of her alter-ego sasha fierce and earned a record-setting six grammy awards in 2010, including song of the year for \""single ladies (put a ring on it)\"". beyonc\u00e9 took a hiatus from music in 2010 and took over management of her career; her fourth album 4 (2011) was subsequently mellower in tone, exploring 1970s funk, 1980s pop, and 1990s soul. her critically acclaimed fifth studio album, beyonc\u00e9 (2013), was distinguished from previous releases by its experimental production and exploration of darker themes."",
    ""qas"": [{
        ""answers"": [{
            ""answer_start"": 207,
            ""text"": ""acting""
        }],
        ""question"": ""after her second solo album, what other entertainment venture did beyonce explore?"",
        ""id"": ""56be86cf3aeaaa14008c9076""
    }, {
        ""answers"": [{
            ""answer_start"": 369,
            ""text"": ""jay z""
        }],
        ""question"": ""which artist did beyonce marry?"",
        ""id"": ""56be86cf3aeaaa14008c9078""
    }, {
        ""answers"": [{
            ""answer_start"": 565,
            ""text"": ""six""
        }],
        ""question"": ""to set the record for grammys, how many did beyonce win?"",
        ""id"": ""56be86cf3aeaaa14008c9079""
    }, {
        ""answers"": [{
            ""answer_start"": 260,
            ""text"": ""dreamgirls""
        }],
        ""question"": ""for what movie did beyonce receive  her first golden globe nomination?"",
        ""id"": ""56bf6e823aeaaa14008c9627""
    }, {
        ""answers"": [{
            ""answer_start"": 586,
            ""text"": ""2010""
        }],
        ""question"": ""when did beyonce take a hiatus in her career and take control of her management?"",
        ""id"": ""56bf6e823aeaaa14008c9629""
    }, {
        ""answers"": [{
            ""answer_start"": 180,
            ""text"": ""beyonc\u00e9""
        }],
        ""question"": ""which album was darker in tone from her previous work?"",
        ""id"": ""56bf6e823aeaaa14008c962a""
    }, {
        ""answers"": [{
            ""answer_start"": 406,
            ""text"": ""cadillac records""
        }],
        ""question"": ""after what movie portraying etta james, did beyonce create sasha fierce?"",
        ""id"": ""56bf6e823aeaaa14008c962b""
    }, {
        ""answers"": [{
            ""answer_start"": 48,
            ""text"": ""june 2005""
        }],
        ""question"": ""when did destiny's child end their group act?"",
        ""id"": ""56d43da72ccc5a1400d830bd""
    }, {
        ""answers"": [{
            ""answer_start"": 95,
            ""text"": ""b'day""
        }],
        ""question"": ""what was the name of beyonc\u00e9's second solo album?"",
        ""id"": ""56d43da72ccc5a1400d830be""
    }, {
        ""answers"": [{
            ""answer_start"": 260,
            ""text"": ""dreamgirls""
        }],
        ""question"": ""what was beyonc\u00e9's first acting job, in 2006?"",
        ""id"": ""56d43da72ccc5a1400d830bf""
    }, {
        ""answers"": [{
            ""answer_start"": 369,
            ""text"": ""jay z""
        }],
        ""question"": ""who is beyonc\u00e9 married to?"",
        ""id"": ""56d43da72ccc5a1400d830c0""
    }, {
        ""answers"": [{
            ""answer_start"": 466,
            ""text"": ""sasha fierce""
        }],
        ""question"": ""what is the name of beyonc\u00e9's alter-ego?"",
        ""id"": ""56d43da72ccc5a1400d830c1""
    }]
}

breaking it down a little, if you have a data in json format that looks like this:

import json

from datasets import load_dataset


two_qas = {
    ""data"": [{
        ""title"": ""destinys_child"",
        ""paragraphs"": [{
                ""context"": ""following the disbandment of destiny's child in june 2005, she released her second solo album, b'day (2006), which contained hits \""d\u00e9j\u00e0 vu\"", \""irreplaceable\"", and \""beautiful liar\"". beyonc\u00e9 also ventured into acting, with a golden globe-nominated performance in dreamgirls (2006), and starring roles in the pink panther (2006) and obsessed (2009). her marriage to rapper jay z and portrayal of etta james in cadillac records (2008) influenced her third album, i am... sasha fierce (2008), which saw the birth of her alter-ego sasha fierce and earned a record-setting six grammy awards in 2010, including song of the year for \""single ladies (put a ring on it)\"". beyonc\u00e9 took a hiatus from music in 2010 and took over management of her career; her fourth album 4 (2011) was subsequently mellower in tone, exploring 1970s funk, 1980s pop, and 1990s soul. her critically acclaimed fifth studio album, beyonc\u00e9 (2013), was distinguished from previous releases by its experimental production and exploration of darker themes."",
                ""qas"": [{
                    ""answers"": [{
                        ""answer_start"": 207,
                        ""text"": ""acting""
                    }],
                    ""question"": ""after her second solo album, what other entertainment venture did beyonce explore?"",
                    ""id"": ""56be86cf3aeaaa14008c9076""
                }, {
                    ""answers"": [{
                        ""answer_start"": 369,
                        ""text"": ""jay z""
                    }],
                    ""question"": ""which artist did beyonce marry?"",
                    ""id"": ""56be86cf3aeaaa14008c9078""
                }, {
                    ""answers"": [{
                        ""answer_start"": 466,
                        ""text"": ""sasha fierce""
                    }],
                    ""question"": ""what is the name of beyonc\u00e9's alter-ego?"",
                    ""id"": ""56d43da72ccc5a1400d830c1""
                }]
            },

            {
                ""context"": ""a self-described \""modern-day feminist\"", beyonc\u00e9 creates songs that are often characterized by themes of love, relationships, and monogamy, as well as female sexuality and empowerment. on stage, her dynamic, highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music. throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with destiny's child, making her one of the best-selling music artists of all time. she has won 20 grammy awards and is the most nominated woman in the award's history. the recording industry association of america recognized her as the top certified artist in america during the 2000s decade. in 2009, billboard named her the top radio songs artist of the decade, the top female artist of the 2000s and their artist of the millennium in 2011. time listed her among the 100 most influential people in the world in 2013 and 2014. forbes magazine also listed her as the most powerful female musician of 2015."",
                ""qas"": [{
                    ""answers"": [{
                        ""answer_start"": 104,
                        ""text"": ""love, relationships, and monogamy""
                    }],
                    ""question"": ""in her music, what are some recurring elements in them?"",
                    ""id"": ""56be88473aeaaa14008c9080""
                }, {
                    ""answers"": [{
                        ""answer_start"": 935,
                        ""text"": ""influential""
                    }],
                    ""question"": ""time magazine named her one of the most 100 what people of the century?"",
                    ""id"": ""56be88473aeaaa14008c9083""
                }, {
                    ""answers"": [{
                        ""answer_start"": 985,
                        ""text"": ""forbes""
                    }],
                    ""question"": ""which magazine declared her the most dominant woman musician?"",
                    ""id"": ""56be88473aeaaa14008c9084""
                }, {
                    ""answers"": [{
                        ""answer_start"": 736,
                        ""text"": ""2000s""
                    }],
                    ""question"": ""in which decade did the recording industry association of america recognize beyonce as the the top certified artist?"",
                    ""id"": ""56bf725c3aeaaa14008c9643""
                }]
            }
        ]

    }]
}


with open('my_qas_dataset.json', 'w') as fout:
    json.dump(two_qas, fout)
    
    
ds = load_dataset(""json"", 
        data_files={
            'train': 
            'my_qas_dataset.json'
        }, 
        field=""data""
    )


then to train a model, the easiest way out is to push to huggingface hub, 
after that you can use load_dataset when you change the script on  and save a local script on your machine, e.g. on ./scripts/run_qa.py
finally, instead of using the the git_config, you can do this:
hyperparameters = {
    'model_name_or_path':'t5-base',
    'output_dir':'/opt/ml/model'
    # add your remaining hyperparameters
    # more info here 
}

# creates hugging face estimator
huggingface_estimator = huggingface(
    entry_point='run_qa.py',
    source_dir='./scripts',
    instance_type='ml.p3.2xlarge',
    instance_count=1,
    role=role,
    transformers_version='4.26.0',
    pytorch_version='1.13.1',
    py_version='py39',
    hyperparameters = hyperparameters
)",https://stackoverflow.com/questions/76416680,python,06-06-2023 16:30,1063.0,2.0,1.0,True,07-06-2023 19:13,07-06-2023 19:13
78846004,how can i use structured_output with azure openai with the openai python library?,"i want to use structured output with azure openai.
i tried the following code, based on the code given in 
from pydantic import basemodel
from openai import azureopenai

class step(basemodel):
    explanation: str
    output: str


class mathresponse(basemodel):
    steps: list[step]
    final_answer: str


client = azureopenai(api_key='[redacted]',
                     api_version='2024-05-01-preview',
                     azure_endpoint='[redacted]')

completion = client.beta.chat.completions.parse(
    model=""gpt-4omini-2024-07-18-name"",
    messages=[
        {""role"": ""system"", ""content"": ""you are a helpful math tutor.""},
        {""role"": ""user"", ""content"": ""solve 8x + 31 = 2""},
    ],
    response_format=mathresponse,
)

message = completion.choices[0].message
if message.parsed:
    print(message.parsed.steps)
    print(message.parsed.final_answer)
else:
    print(message.refusal)

i get the error:
openai.badrequesterror: error code: 400:
{
    ""error"": {
        ""message"": ""invalid parameter: response_format must be one of json_object, text."",
        ""type"": ""invalid_request_error"",
        ""param"": ""response_format"",
        ""code"": ""none""
    }
}

how to fix it?
i ran pip install -u openai: i use openai==1.40.1 and python 3.11.

i also tried  using  using azure+ gpt-4o mini (2024-07-18), it didn't work either, same error message:
from openai import azureopenai

# replace these variables with your azure openai endpoint and api key
endpoint = ""
api_key = ""<your-api-key>""
deployment_name = ""<your-deployment-name>"" # replace with your deployment name
model = deployment_name

# api endpoint for the completion request
api_url = f""{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-06-01""


client = azureopenai(api_key='[redacted]',
                     api_version='2024-07-01-preview',
                     azure_endpoint='

math_tutor_prompt = '''
    you are a helpful math tutor. you will be provided with a math problem,
    and your goal will be to output a step by step solution, along with a final answer.
    for each step, just provide the output as an equation use the explanation field to detail the reasoning.
'''

def get_math_solution(question):
    response = client.chat.completions.create(
    model=model,
    messages=[
        {
            ""role"": ""system"",
            ""content"": math_tutor_prompt
        },
        {
            ""role"": ""user"",
            ""content"": question
        }
    ],
    response_format={
        ""type"": ""json_schema"",
        ""json_schema"": {
            ""name"": ""math_reasoning"",
            ""schema"": {
                ""type"": ""object"",
                ""properties"": {
                    ""steps"": {
                        ""type"": ""array"",
                        ""items"": {
                            ""type"": ""object"",
                            ""properties"": {
                                ""explanation"": {""type"": ""string""},
                                ""output"": {""type"": ""string""}
                            },
                            ""required"": [""explanation"", ""output""],
                            ""additionalproperties"": false
                        }
                    },
                    ""final_answer"": {""type"": ""string""}
                },
                ""required"": [""steps"", ""final_answer""],
                ""additionalproperties"": false
            },
            ""strict"": true
        }
    }
    )

    return response.choices[0].message


# testing with an example question
question = ""how can i solve 8x + 7 = -23""

result = get_math_solution(question)

print(result.content)","['python', 'nlp', 'azure-openai', 'gpt-4']",78946352,"usingï¿½ï¿½gpt-4o-2024-08-06, which finally got deployed today (2024-09-03) on azure, made it work. code example from learn.microsoft.com:
from pydantic import basemodel
from openai import azureopenai

endpoint = ""
api_key = ""your-azure-openai-key""
deployment_name = 'deployment name' # replace with your gpt-4o 2024-08-06 deployment name

client = azureopenai(api_key=api_key,
                     api_version='2024-08-01-preview',
                     azure_endpoint=endpoint)

class calendarevent(basemodel):
    name: str
    date: str
    participants: list[str]

completion = client.beta.chat.completions.parse(
    model=deployment_name, # replace with the model deployment name of your gpt-4o 2024-08-06 deployment
    messages=[
        {""role"": ""system"", ""content"": ""extract the event information.""},
        {""role"": ""user"", ""content"": ""alice and bob are going to a science fair on friday.""},
    ],
    response_format=calendarevent,
)

event = completion.choices[0].message.parsed

print(event)
print(completion.model_dump_json(indent=2))

output:
name='science fair' date='friday' participants=['alice', 'bob']
{
  ""id"": ""chatcmpl-a3xdrvolxpjeaaqigddswi990weid"",
  ""choices"": [
    {
      ""finish_reason"": ""stop"",
      ""index"": 0,
      ""logprobs"": null,
      ""message"": {
        ""content"": ""{\""name\"":\""science fair\"",\""date\"":\""friday\"",\""participants\"":[\""alice\"",\""bob\""]}"",
        ""refusal"": null,
        ""role"": ""assistant"",
        ""function_call"": null,
        ""tool_calls"": [],
        ""parsed"": {
          ""name"": ""science fair"",
          ""date"": ""friday"",
          ""participants"": [
            ""alice"",
            ""bob""
          ]
        }
      },
      ""content_filter_results"": {
        ""hate"": {
          ""filtered"": false,
          ""severity"": ""safe""
        },
        ""self_harm"": {
          ""filtered"": false,
          ""severity"": ""safe""
        },
        ""sexual"": {
          ""filtered"": false,
          ""severity"": ""safe""
        },
        ""violence"": {
          ""filtered"": false,
          ""severity"": ""safe""
        }
      }
    }
  ],
  ""created"": 1725406029,
  ""model"": ""gpt-4o-2024-08-06"",
  ""object"": ""chat.completion"",
  ""service_tier"": null,
  ""system_fingerprint"": ""fp_b2ffeb31ff"",
  ""usage"": {
    ""completion_tokens"": 17,
    ""prompt_tokens"": 32,
    ""total_tokens"": 49
  },
  ""prompt_filter_results"": [
    {
      ""prompt_index"": 0,
      ""content_filter_results"": {
        ""hate"": {
          ""filtered"": false,
          ""severity"": ""safe""
        },
        ""self_harm"": {
          ""filtered"": false,
          ""severity"": ""safe""
        },
        ""sexual"": {
          ""filtered"": false,
          ""severity"": ""safe""
        },
        ""violence"": {
          ""filtered"": false,
          ""severity"": ""safe""
        }
      }
    }
  ]
}

tested with python 3.11.7 and openai==1.43.0.",https://stackoverflow.com/questions/78846004,python,07-08-2024 23:14,1229.0,0.0,2.0,True,04-09-2024 00:18,09-08-2024 20:01
74397563,how to convert speech to text in python - opus file format,"i have some .opus audio files that need to be converted to text in order to run some analytics. i am aware that there is the python speechrecognition package that can do this with .wav files as demonstrated in this tutorial.
does anyone know how to convert .opus files to text, or convert .opus to .wav?
i have tried the python speechrecognition package with no success.","['python', 'nlp', 'speech-to-text', 'speech', 'opus']",74398200,"here is a solution which employs ffmpeg and the os library to first convert all .opus files in the specified directory to .wav, and then perform speech recognition on the resulting .wav files using the speech_recognition module:
solution
import os
import speech_recognition as sr

path = './audio-files/'
file_type_to_convert = "".opus""
file_type_to_recognize = "".wav""

for filename in os.listdir(path):
    if filename.endswith(file_type_to_convert):
        os.system(""ffmpeg -i \""{}\"" -vn \""{}\"""".format(path + filename,
                                                       path + filename[:-len(file_type_to_convert)] +
                                                       file_type_to_recognize))
recognizer = sr.recognizer()  # instantiate recognizer
rec_output = {}  # create list to store output of speech recognized files

# iterate over each file of specified type to be recognized
for file_to_recognize in os.listdir(path):
    if file_to_recognize.endswith(file_type_to_recognize):
        audio = sr.audiofile(path + file_to_recognize)
        with audio as source:
            audio_data = recognizer.record(audio)
        # recognize & append output
        # note: google recognizer is online only, sphinx is the only offline option which uses cmu sphinx engine
        rec_output[file_to_recognize[:-len(file_type_to_recognize)]] = recognizer.recognize_google(audio_data,
                                                                                                   language='en-us')

# display each file's output
for key, val in rec_output.items():
    print(key)
    print(val)
    # output: 
    # file name
    # recognized words in each file",https://stackoverflow.com/questions/74397563,python,11-11-2022 02:56,825.0,1.0,1.0,True,11-11-2022 13:06,11-11-2022 13:06
42471570,how can i split documents into training set and test set?,"i am trying to build a classification model. i have 1000 text documents in local folder. i want to divide them into training set and test set with a split ratio of 70:30(70 ï¿½ï¿½ï¿½ training and 30 ï¿½ï¿½ï¿½ test). what is the better approach to do so? i am using python.

i wanted a approach programmatically to split the training set and test set. first to read the files in local directory. second, to build a list of those files and shuffle them. thirdly to split them into a training set and test set.
i tried a few ways by using built-in python keywords and functions only to fail. lastly, i got the idea of approaching it. also cross-validation is a good option to be considered for the building general classification","['machine-learning', 'scikit-learn', 'text-classification']",42471788,"there will be a few steps:

get a list of the files
randomize the files
split files into training and testing sets
do the thing

1. get a list of the files
let's assume that your files all have the extension .data and they're all in the folder /ml/data/. we want to get a list of all of these files. this is done simply with the os module. i'm assuming you don't have any subdirectories; this would change if there were.
import os

def get_file_list_from_dir(datadir):
    all_files = os.listdir(os.path.abspath(datadir))
    data_files = list(filter(lambda file: file.endswith('.data'), all_files))
    return data_files

so if we were to call get_file_list_from_dir('/ml/data'), we would get back a list of all the .data files in that directory (equivalent in the shell to the glob /ml/data/*.data).
2. randomize the files
we don't want the sampling to be predictable, as that is considered a poor way to train an ml classifier.
from random import shuffle

def randomize_files(file_list):
    shuffle(file_list)

note that random.shuffle performs an in-place shuffling, so it modifies the existing list. (of course this function is rather silly since you could just call shuffle instead of randomize_files; you can write this into another function to make it make more sense.)
3. split files into training and testing sets
i'll assume a 70:30 ratio instead of any specific number of documents. so:
from math import floor

def get_training_and_testing_sets(file_list):
    split = 0.7
    split_index = floor(len(file_list) * split)
    training = file_list[:split_index]
    testing = file_list[split_index:]
    return training, testing

4. do the thing
this is the step where you open each file and do your training and testing. i'll leave this to you!

cross-validation
out of curiosity, have you considered using cross-validation? this is a method of splitting your data so that you use every document for training and testing. you can customize how many documents are used for training in each ""fold"". i could go more into depth on this if you like, but i won't if you don't want to do it.
all right, since you requested it, i will explain this a little bit more.
so we have a 1000-document set of data. the idea of cross-validation is that you can use all of it for both training and testing ï¿½ï¿½ï¿½ï¿½ï¿½just not at once. we split the dataset into what we call ""folds"". the number of folds determines the size of the training and testing sets at any given point in time.
let's say we want a 10-fold cross-validation system. this means that the training and testing algorithms will run ten times. the first time will train on documents 1-100 and test on 101-1000. the second fold will train on 101-200 and test on 1-100 and 201-1000.
if we did, say, a 40-fold cv system, fold would train on document 1-25 and test on 26-1000, the second fold would train on 26-40 and test on 1-25 and 51-1000, and on.
to implement such a system, we would still need to do steps (1) and (2) from above, but step (3) would be different. instead of splitting into just two sets (one for training, one for testing), we could turn the function into a generator ï¿½ï¿½ï¿½ a function which we can iterate through like a list.
def cross_validate(data_files, folds):
    if len(data_files) % folds != 0:
        raise valueerror(
            ""invalid number of folds ({}) for the number of ""
            ""documents ({})"".format(folds, len(data_files))
        )
    fold_size = len(data_files) // folds
    for split_index in range(0, len(data_files), fold_size):
        training = data_files[split_index:split_index + fold_size]
        testing = data_files[:split_index] + data_files[split_index + fold_size:]
        yield training, testing

that yield keyword at the end is what makes this a generator. to use it, you would use it like so:
def ml_function(datadir, num_folds):
    data_files = get_file_list_from_dir(datadir)
    randomize_files(data_files)
    for train_set, test_set in cross_validate(data_files, num_folds):
        do_ml_training(train_set)
        do_ml_testing(test_set)

again, it's up to you to implement the actual functionality of your ml system.",https://stackoverflow.com/questions/42471570,machine-learning,26-02-2017 17:08,18007.0,4.0,4.0,True,31-05-2024 03:28,31-05-2024 03:28
78716179,check if a fine-tuned openai model was successfully deleted,"i am doing some work with the openai api with python. i'm working with fine-tuning and i am working on deleting an existing model and starting over again. i want to be able to check if the deletion actually succeeded or failed.
the openai.models.delete() function returns a modeldeleted object. what should i do with it?","['python', 'openai-api']",78716237,"a modeldeleted object has a boolean deleted attribute which tells you whether the model was deleted.
api reference
python source code",https://stackoverflow.com/questions/78716179,python,07-07-2024 00:08,146.0,0.0,1.0,True,07-07-2024 00:54,07-07-2024 00:54
62691279,how to disable tokenizers_parallelism=(true | false) warning?,"i use pytorch to train huggingface-transformers model, but every epoch, always output the warning:
the current process just got forked. disabling parallelism to avoid deadlocks... to disable this warning, please explicitly set tokenizers_parallelism=(true | false)

how to disable this warning?","['python', 'pytorch', 'huggingface-transformers', 'huggingface-tokenizers']",62703850,"set the environment variable to the string ""false""
either by
tokenizers_parallelism=false

in your shell
or by:
import os
os.environ[""tokenizers_parallelism""] = ""false""

in the python script",https://stackoverflow.com/questions/62691279,python,02-07-2020 07:35,102867.0,74.0,5.0,True,26-07-2023 14:52,07-04-2022 16:07
77174018,getting `typeerror: issubclass() arg 1 must be a class` when trying to load `nlp = spacy.load(&quot;en_ner_bc5cdr_md&quot;)`,"i'm using spacy to analyze a large set of medical text for commentary about diagnoses, which was working fine when i left it last week.
now when i try to load the scispacy library en_ner_bc5cdr_md i am getting the type error typeerror: issubclass() arg 1 must be a class
i used this exact library to analyze the same text last week and the only thing that has changed is that i've restarted my computer.
i'm running an anaconda distribution with python 3.8.8 on osx version 13.5.2 (22g91)
any ideas what's going on?
this is the code, it never gets past importing the model.
import spacy
import scispacy
import pandas as pd

text = """"""the patient was diagnosed with pneumonia last year. he has a history of asthma and hypertension.  
his copd symptoms have worsened over the last 2 months. the patient also suffers from migraines.""""""

nlp = spacy.load(""en_ner_bc5cdr_md"") 

doc = nlp(text)

labels = []
counts = []

for ent in doc.ents:
    if ent.label_ == 'disease':
        if ent.text not in labels:
            labels.append(ent.text)
            counts.append(1)
        else:
            idx = labels.index(ent.text)
            counts[idx] += 1
            
df = pd.dataframe({'diagnosis': labels, 'count': counts})
print(df)

if you are able to run on you machine please let me know the parameters. you will also need to run - !pip install","['python', 'tensorflow', 'machine-learning', 'nlp', 'spacy']",77174910,"i was able to load en_ner_bc5cdr_md with spacy == 3.0.9 and python 3.10.12. you can try the following steps:

!pip install spacy==3.0.9 ('!' is for notebook cell)
!pip install 

and then try:
import spacy
nlp = spacy.load(""en_ner_bc5cdr_md"") 

sorry, currently i don't have any local machine but i tested it on google colab and it's working fine on my end.",https://stackoverflow.com/questions/77174018,python,25-09-2023 15:37,864.0,2.0,1.0,True,25-09-2023 18:15,25-09-2023 15:52
79016929,machine learning model predicts training labels themselves as result,"i am trying to build a model to predict ""species"" based on data with features ""message"", ""tail"", and ""finger"", and label ""species""(see the first few rows of data.csv below):



message
fingers
tail
species




pluvia arbor aquos
4
no
aquari


cosmix xeno nebuz odbitaz
5
yes
zorblax


solarix glixx novum galaxum quasar
5
yes
zorblax


arbor insectus pesros ekos dootix nimbus
2
yes
florian



my code is:
import warnings
warnings.simplefilter(""ignore"")
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import labelencoder
from sklearn.feature_extraction.text import countvectorizer
from sklearn.naive_bayes import multinomialnb

df = pd.read_csv(""data.csv"")
x = np.asarray(df[[""message"", ""fingers"", ""tail""]])
x = [str (item) for item in x]
y = df[""species""]

le = labelencoder()
y = le.fit_transform(y)

cv = countvectorizer()
x = cv.fit_transform(x).toarray()

model = multinomialnb()
model.fit(x, y)

test_data = pd.read_csv('test.csv')
test_data_array = np.asarray(df[[""message"", ""fingers"", ""tail""]])
test_data_array = [str (item) for item in test_data_array]
test_data_array = cv.fit_transform(test_data_array).toarray()

y_prediction = model.predict(test_data_array)
y_prediction = le.inverse_transform(y_prediction)

print(y_prediction)

i followed this tutorial for the same.
the problem is, when i tried running it, it just outputs the species column of the original training data word-for-word apart from a few differences (there are 493 results while the test data consisted of 299 entries, and the training data consisted of 500 entries). it doesn't actually predict anything for the test data. i don't understand why the code won't work. could someone help out?","['python', 'machine-learning', 'text-classification', 'naivebayes', 'machine-learning-model']",79017406,"the problem is that you read the test data into test_data, but then use the original dataframe, df, containing the training data, to make the test set.
change this line:
test_data_array = np.asarray(df[[""message"", ""fingers"", ""tail""]])

to:
test_data_array = np.asarray(test_data[[""message"", ""fingers"", ""tail""]])

and you should have the correct number of predictions.
remember to also compare y_prediction to test_data['species'].",https://stackoverflow.com/questions/79016929,python,24-09-2024 04:08,49.0,-1.0,1.0,True,24-09-2024 07:18,24-09-2024 04:46
75715416,extract multiple values from a free text column in a cvs file,"i have a cvs file with a column that consists of a series of medical tests, in free text, with the date, the test names and results, like it follows.
i need to extract the values of each of those medical tests and turn them into columns.
i would like to know if there is a way of doing it without having to train a model of nlp to extract those information.




patient
results




1
""01/03/2022 - hmg - plaques: 65000 01/03/2022 - hmg - haemoglobin: 7.8 01/05/2022 - urea: 50.0 01/05/2022 - hmg - plaques: 80000""


2
""06/01/2022 - alt/tgp: 25.0 06/01/2022 - ast/tgo: 40.0 06/03/2022 - bilirrubin: 0.8""


3
""01/03/2022 - hmg - haematocrit: 40 01/03/2022 - hmg - haemoglobin: 10.2""




csv:
patient;results 
1;""01/03/2022 - hmg - plaques: 65000 01/03/2022 - hmg - haemoglobin: 7.8 01/05/2022 - urea: 50.0 01/05/2022 - hmg - plaques: 80000"" 
2;""06/01/2022 - alt/tgp: 25.0 06/01/2022 - ast/tgo: 40.0 06/03/2022 - bilirrubin: 0.8"" 
3;""01/03/2022 - hmg - haematocrit: 40 01/03/2022 - hmg - haemoglobin: 10.2""","['r', 'nlp', 'extract']",75715841,"assuming df is defined reproducibly as in the note at the end insert a newline before each date, separate into distinct rows, separate the date and test and then the test and value.
library(dplyr)
library(tidyr)

df %>%
  mutate(results = gsub("" (../../....)"", ""\n\\1"", results)) %>%
  separate_rows(results, sep = ""\n"") %>%
  separate(results, c(""date"", ""test""), sep = "" - "", extra = ""merge"") %>%
  separate(test, c(""test"", ""value""), sep = "": "", convert = true)

giving:
# a tibble: 9 ï¿½ï¿½ 4
  patient date       test                value
    <int> <chr>      <chr>               <dbl>
1       1 01/03/2022 hmg - plaques     65000  
2       1 01/03/2022 hmg - haemoglobin     7.8
3       1 01/05/2022 urea                 50  
4       1 01/05/2022 hmg - plaques     80000  
5       2 06/01/2022 alt/tgp              25  
6   2 06/01/2022 ast/tgo              40  
7       2 06/03/2022 bilirrubin            0.8
8       3 01/03/2022 hmg - haematocrit    40  
9       3 01/03/2022 hmg - haemoglobin    10.2

note
lines <- 'patient;results 
1;""01/03/2022 - hmg - plaques: 65000 01/03/2022 - hmg - haemoglobin: 7.8 01/05/2022 - urea: 50.0 01/05/2022 - hmg - plaques: 80000"" 
2;""06/01/2022 - alt/tgp: 25.0 06/01/2022 - ast/tgo: 40.0 06/03/2022 - bilirrubin: 0.8"" 
3;""01/03/2022 - hmg - haematocrit: 40 01/03/2022 - hmg - haemoglobin: 10.2""'
df <- read.csv2(text = lines)",https://stackoverflow.com/questions/75715416,r,12-03-2023 18:25,71.0,0.0,2.0,True,12-03-2023 20:07,12-03-2023 19:15
72885929,rebel: relation extraction by end-to-end language generation,"i am trying to run the following pieces of code:
from transformers import pipeline

triplet_extractor = pipeline('text2text-generation', model='babelscape/rebel-large', tokenizer='babelscape/rebel-large')
# we need to use the tokenizer manually since we need special tokens.
extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(""punta cana is a resort town in the municipality of higuey, in la altagracia province, the eastern most province of the dominican republic"", return_tensors=true, return_text=false)[0][""generated_token_ids""]])
print(extracted_text[0])
# function to parse the generated text and extract the triplets
def extract_triplets(text):
    triplets = []
    relation, subject, relation, object_ = '', '', '', ''
    text = text.strip()
    current = 'x'
    for token in text.replace(""<s>"", """").replace(""<pad>"", """").replace(""</s>"", """").split():
        if token == ""<triplet>"":
            current = 't'
            if relation != '':
                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
                relation = ''
            subject = ''
        elif token == ""<subj>"":
            current = 's'
            if relation != '':
                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
            object_ = ''
        elif token == ""<obj>"":
            current = 'o'
            relation = ''
        else:
            if current == 't':
                subject += ' ' + token
            elif current == 's':
                object_ += ' ' + token
            elif current == 'o':
                relation += ' ' + token
    if subject != '' and relation != '' and object_ != '':
        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
    return triplets
extracted_triplets = extract_triplets(extracted_text[0])
print(extracted_triplets)

unfortunately i get the following error.

typeerror: can't convert {'output_ids': [[0, 50267, 221, 20339, 2615, 102, 1437, 50266, 1587, 7330, 1073, 13249, 493, 16517, 1437, 50265, 2034, 11, 5, 6833, 15752, 10014, 1437, 50266, 18978, 3497, 1437, 50265, 247, 1437, 50267, 19664, 1780, 219, 1437, 50266, 1587, 7330, 1073, 13249, 493, 16517, 1437, 50265, 2034, 11, 5, 6833, 15752, 10014, 1437, 50266, 18978, 3497, 1437, 50265, 247, 1437, 50267, 1587, 7330, 1073, 13249, 493, 16517, 1437, 50266, 18978, 3497, 1437, 50265, 247, 1437, 50267, 18978, 3497, 1437, 50266, 1587, 7330, 1073, 13249, 493, 16517, 1437, 50265, 6308, 6833, 15752, 10014, 2]]} to sequence

can anyone provide a solution for this error?","['python', 'huggingface-transformers']",72892743,"i can run the code successfully without an error pf screenshot. maybe can you check with your transformers version?
i have a colab notebook using transformers model/tokenizers functions directly maybe you can use them directly.

i hope this helps, thanks!",https://stackoverflow.com/questions/72885929,python,06-07-2022 15:08,633.0,1.0,1.0,True,07-07-2022 05:36,06-07-2022 15:28
70693229,how to get bigram/trigram of word from prelisted unigram from a document corpus / dataframe column,"i have a dataframe with text in one of its columns.
i have listed some predefined keywords which i need for analysis and words associated with it (and later make a wordcloud and counter of occurrences) to understand topics /context associated with such keywords.
use case:
df.text_column()

keywordlist = [coca , food, soft, aerated, soda]

lets say one of the rows of the text column has text : ' coca cola is expanding its business in soft drinks and aerated water'.
another entry like : 'lime soda is the best selling item in fast food stores'
my objective is to get bigram/trigram like:
'coca_cola','coca_cola_expanding', 'soft_drinks', 'aerated_water', 'business_soft_drinks', 'lime_soda', 'food_stores'

kindly help me to do that [python only]","['python', 'nlp', 'nltk']",70712556,"first, you can optioanlly load the nltk's stop word list and remove any stop words from the text (such as ""is"", ""its"", ""in"", and ""and""). alternatively, you can define your own stop words list, as well as even extend the nltk's list with additional words. following, you can use nltk.bigrams() and nltk.trigrams() methods to get bigrams and trigrams joined with an underscore _, as you asked. also, have a look at collocations.
edit:
if you haven't already, you need to include the following once in your code, in order to download the stop words list.
nltk.download('stopwords')

code:
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

word_data = ""coca cola is expanding its business in soft drinks and aerated water""
#word_data = ""lime soda is the best selling item in fast food stores""

# load nltk's stop word list
stop_words = list(stopwords.words('english'))
# extend the stop words list
#stop_words.extend([""best"", ""selling"", ""item"", ""fast""])

# tokenize the string and remove stop words
word_tokens = word_tokenize(word_data)
clean_word_data = [w for w in word_tokens if not w.lower() in stop_words]
    
# get bigrams
bigrams_list = [""_"".join(item) for item in nltk.bigrams(clean_word_data)]
print(bigrams_list)

# get trigrams 
trigrams_list = [""_"".join(item) for item in nltk.trigrams(clean_word_data)]
print(trigrams_list)

update
once you get the bigram and trigram lists, you can check for matches against your keyword list to keep only the relevant ones.
keywordlist = ['coca' , 'food', 'soft', 'aerated', 'soda']

def find_matches(n_grams_list):
    matches = []
    for k in keywordlist:
        matching_list = [s for s in n_grams_list if k in s]
        [matches.append(m) for m in matching_list if m not in matches]
    return matches

all_matching_bigrams = find_matches(bigrams_list) # find all mathcing bigrams  
all_matching_trigrams = find_matches(trigrams_list) # find all mathcing trigrams

# join the two lists
all_matches = all_matching_bigrams + all_matching_trigrams
print(all_matches)

output:
['coca_cola', 'business_soft', 'soft_drinks', 'drinks_aerated', 'aerated_water', 'coca_cola_expanding', 'expanding_business_soft', 'business_soft_drinks', 'soft_drinks_aerated', 'drinks_aerated_water']",https://stackoverflow.com/questions/70693229,python,13-01-2022 08:06,1671.0,1.0,1.0,True,05-12-2024 19:07,13-01-2022 08:11
78451428,python accelerate package thrown error when using trainer from transformers,"i'm trying out this hugging face tutorial
i'm trying to use a trainer to train my mode. the code errors out at this point:
from datasets import load_dataset
from transformers import autotokenizer, automodelforsequenceclassification, datacollatorwithpadding, trainingarguments, trainer

checkpoint = ""bert-base-uncased""
tokenizer = autotokenizer.from_pretrained(checkpoint)
raw_datasets = load_dataset(""glue"", ""mrpc"")

def tokenize_function(example):
    return tokenizer(example[""sentence1""], example[""sentence2""], truncation=true)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=true)

data_collator = datacollatorwithpadding(tokenizer=tokenizer)

training_args = trainingarguments(""test-trainer"")

model = automodelforsequenceclassification.from_pretrained(checkpoint, num_labels=2)

#the above code works upto here
#the following line fails
trainer = trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets[""train""],
    eval_dataset=tokenized_datasets[""validation""], 
    tokenizer=tokenizer,
)

the error is displayed as:
file ""tutorial.py"", line 21, in <module>
    trainer = trainer(
              ^^^^^^^^
  file ""/opt/miniconda3/envs/py3env/lib/python3.12/site-packages/transformers/trainer.py"", line 388, in __init__
    self.create_accelerator_and_postprocess()
  file ""/opt/miniconda3/envs/py3env/lib/python3.12/site-packages/transformers/trainer.py"", line 4364, in create_accelerator_and_postprocess
    self.accelerator = accelerator(**args)
                       ^^^^^^^^^^^^^^^^^^^
typeerror: accelerator.__init__() got an unexpected keyword argument 'use_seedable_sampler'

versions:
python: 3.12.3
transformer: 4.40.2
datasets: 2.19.1
accelerate: 0.21.0","['python', 'huggingface-transformers']",78451429,"seems like you have to force update accelerate with the specific version 0.30.0.
simply installing 'accelerate' won't work,
$ conda install accelerate

....as it will pick the latest to be 0.21.0 as listed below,
$ conda search accelerate
# name                       version           build  channel             
huggingface_accelerate          0.20.3 py310hca03da5_0  pkgs/main           
huggingface_accelerate          0.20.3 py311hca03da5_0  pkgs/main           
huggingface_accelerate          0.20.3  py38hca03da5_0  pkgs/main           
huggingface_accelerate          0.20.3  py39hca03da5_0  pkgs/main           
huggingface_accelerate          0.21.0 py310hca03da5_0  pkgs/main           
huggingface_accelerate          0.21.0 py311hca03da5_0  pkgs/main           
huggingface_accelerate          0.21.0 py312hca03da5_0  pkgs/main           
huggingface_accelerate          0.21.0  py38hca03da5_0  pkgs/main           
huggingface_accelerate          0.21.0  py39hca03da5_0  pkgs/main           
pyopengl-accelerate            3.1.5  py38heec5a64_0  pkgs/main           
pyopengl-accelerate            3.1.5  py39heec5a64_0  pkgs/main  

you will have to force install 0.30.0 by specifying it as the version,
$ conda install accelerate=0.30.0",https://stackoverflow.com/questions/78451428,python,08-05-2024 22:31,4648.0,1.0,1.0,True,11-05-2024 02:29,10-05-2024 01:38
77006745,oserror: meta-llama/llama-2-7b-chat-hf is not a local folder,"i'm trying to replied the code from this hugging face blog. at first i installed the transformers and created a token to login to hugging face hub:
pip install transformers
huggingface-cli login

after that it is said to use use_auth_token=true when you have set a token. unfortunately after running the code i get an error:
from transformers import autotokenizer
import transformers
import torch

model = ""meta-llama/llama-2-7b-chat-hf""

tokenizer = autotokenizer.from_pretrained(model, use_auth_token=true)
pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    torch_dtype=torch.float16,
    device_map=""auto"",
)

sequences = pipeline(
    'i liked ""breaking bad"" and ""band of brothers"". do you have any recommendations of other shows i might like?\n',
    do_sample=true,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    max_length=200,
)
for seq in sequences:
    print(f""result: {seq['generated_text']}"")

error:
oserror: meta-llama/llama-2-7b-chat-hf is not a local folder and is not a valid model identifier listed on '
if this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=true`.

it says that the model cannot be found, but you can find it in the list of models on hugging face here.
this is the version of the transformers package i'm using:
> pip show transformers

name: transformers
version: 4.33.0.dev0
summary: state-of-the-art machine learning for jax, pytorch and tensorflow
home-page: 
author: the hugging face team (past and future) with the help of all our contributors (
author-email: transformers@huggingface.co
license: apache 2.0 license
location: /users/quinten/opt/miniconda3/lib/python3.9/site-packages
requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm
required-by: spacy-transformers

does anyone know how to fix this error?","['python', 'huggingface-transformers', 'huggingface', 'llama']",77006862,"def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):

the pretrained_model_name_or_path may the model repo or the model path
in your case the model repo is ""meta-llama/llama-2-7b-chat-hf"" which is right.
according to 
you must agree to the terms and conditions in the above link in order to access the model.",https://stackoverflow.com/questions/77006745,python,30-08-2023 09:34,20646.0,4.0,2.0,True,04-10-2024 09:20,30-08-2023 09:42
75763642,tf-idf value is not matching the output of tfidfvectorizer,"i am learning nlp and was interested in understanding the tf-idf model using the sklearn library and the class tfidfvectorizer
i have pasted the sample code below.

corpus = [
    'this is the first document.',
    'this is the second second document.',
    'and the third one.',
    'is this the first document?',
]

vectorizer = tfidfvectorizer()
x = vectorizer.fit_transform(corpus)
pd.dataframe(x.toarray(), columns = vectorizer.get_feature_names())

the feature names:
vectorizer.get_feature_names()
['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
and the tf-idf values are:
array([[0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,
        0.        , 0.35872874, 0.        , 0.43877674],
       [0.        , 0.27230147, 0.        , 0.27230147, 0.        ,
        0.85322574, 0.22262429, 0.        , 0.27230147],
       [0.55280532, 0.        , 0.        , 0.        , 0.55280532,
        0.        , 0.28847675, 0.55280532, 0.        ],
       [0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,
        0.        , 0.35872874, 0.        , 0.43877674]])

i was interested in calculating the tf-idf value of the term ""document"" for the above mentioned corpus, which comes out to be 0.43877674 for the first document.
i tried using the below formula both for base 10 and base e (natural logarithm), since smooth_idf=true by default and as per the documentation written in 
using the tfidftransformerï¿½ï¿½ï¿½s default settings, tfidftransformer(norm='l2', use_idf=true, smooth_idf=true, sublinear_tf=false) the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which is computed as

where n is the total number of documents in the document set, and df(t) is the number of documents in the document set that contain term t

according to the output from the program written, it should be 0.43877674","['machine-learning', 'scikit-learn', 'nlp', 'tf-idf', 'tfidfvectorizer']",75769023,"your calculation is correct, you are just missing the normalization. with default parameters each document is normalized, so that the euclidian length of each document vector equals 1. you can disable the normalization with  the parameter norm=none
corpus = [
    'this is the first document.',
    'this is the second second document.',
    'and the third one.',
    'is this the first document?',
]

vectorizer = tfidfvectorizer(norm=none)
x = vectorizer.fit_transform(corpus)

results in:
array([[0.        , 1.22314355, 1.51082562, 1.22314355, 0.        ,
        0.        , 1.        , 0.        , 1.22314355],
       [0.        , 1.22314355, 0.        , 1.22314355, 0.        ,
        3.83258146, 1.        , 0.        , 1.22314355],
       [1.91629073, 0.        , 0.        , 0.        , 1.91629073,
        0.        , 1.        , 1.91629073, 0.        ],
       [0.        , 1.22314355, 1.51082562, 1.22314355, 0.        ,
        0.        , 1.        , 0.        , 1.22314355]])

exactly the tfidf value you calculated for the token 'document' in the first document.",https://stackoverflow.com/questions/75763642,machine-learning,17-03-2023 03:58,554.0,1.0,1.0,True,17-03-2023 14:52,17-03-2023 05:24
71692836,convert constituent string to tree object (stanza),"i am doing some experiments in python with stanza and i have converted a lot of sentences into parsetree objects and saved them into a file like this:
# sent_id = not relevant to the question
# text = might be ok for some , but not for me
(s (none might be) (opinion_1_negative_standard (polar_expression ok)) (none for some , but) (opinion_0_negative_standard (polar_expression not)) (none for) (all_01 (source me)))

and now, i want to read that file, get the strings of the tree and convert them again into a parsetree object, but i am not able to find how it should be done properly.
i checked data conversion, but i can not find what i am trying to do, and the constituency parser examples show how to get the string representation but not how to convert that string into a tree.
regards.","['python', 'stanford-nlp', 'parse-tree']",71694237,"i have found the solution reading the source code in their github.
inside the constituency module, in the tree_reader file there is a method called read_trees(text) that it does exactly what i wanted.
regards.",https://stackoverflow.com/questions/71692836,python,31-03-2022 12:53,752.0,1.0,1.0,True,29-07-2023 23:05,29-07-2023 23:05
78734833,how do i increase max_new_tokens,"i'm facing this error while running my code:

valueerror: input length of input_ids is 1495, but max_length is set to 20. this can lead to unexpected behavior. you should consider increasing max_length or, better yet, setting max_new_tokens.

i wanted the code to generate the query instead it says about the max length issue as basically i am using 8 bit quantized llama using vector embedding to develop a rag chat bot
import os
from langchain.document_loaders import pypdfloader
from langchain.text_splitter import charactertextsplitter
from transformers import bitsandbytesconfig, autotokenizer
from langchain_huggingface import chathuggingface, huggingfacepipeline
from time import time
from langchain.embeddings import huggingfaceembeddings
from langchain.vectorstores import chroma
from langchain.chains import retrievalqa

# load pdf and split into chunks
def split_doc(file, chunk_size, chunk_overlap):
    text_splitter = charactertextsplitter(
        separator=""\n\n"",
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len
    )
    return text_splitter.split_documents(file)

loader = pypdfloader(""/kaggle/input/report/report.pdf"")
pages = loader.load()
docs = split_doc(pages, 700, 450)

# configure and load the model
model_name = ""nousresearch/llama-2-7b-chat-hf""
bnb_config = bitsandbytesconfig(load_in_8bit=true)
llm = huggingfacepipeline.from_model_id(
    model_id=model_name,
    task=""text-generation"",
    model_kwargs={""trust_remote_code"": true, ""quantization_config"": bnb_config}
)
chat_model = chathuggingface(llm=llm)

# set up embeddings and vector store
embedding_model_name = ""sentence-transformers/all-mpnet-base-v2""
embedding_model_kwargs = {""device"": ""cuda""}
embeddings = huggingfaceembeddings(model_name=embedding_model_name, model_kwargs=embedding_model_kwargs)
vectordb = chroma.from_documents(documents=docs, embedding=embeddings, persist_directory=""chroma_db"")
retriever = vectordb.as_retriever()

# set up the qa system
qa = retrievalqa.from_chain_type(
    llm=llm,
    chain_type=""stuff"",
    retriever=retriever,
    verbose=true
)

# define the testing function
def test_rag(qa, query):
    print(f""query: {query}\n"")
    result = qa.run(query)
    print(""\nresult: "", result)

query = ""what were the main topics in this ""
test_rag(qa, query)# get stuck here","['huggingface-transformers', 'langchain', 'huggingface-tokenizers', 'llama', 'huggingface-hub']",78781795,"thanks it helped i added the following details:
using the pipeline_kwargs in huggingface.py file i was able to find the variable i could use although using this method will render the quantization method a bit useless as you will consume more memory upon increasing the tokens
llm = huggingfacepipeline.from_model_id(
    model_id=model_name,
    task=""text-generation"",
    model_kwargs={
        ""trust_remote_code"": true,
        ""quantization_config"": bnb_config,
        ""use_auth_token"": auth_token
    },
    pipeline_kwargs={""max_new_tokens"": 8096}# this part is how i reconfigured the tokens

)",https://stackoverflow.com/questions/78734833,huggingface-transformers,11-07-2024 09:58,3388.0,0.0,2.0,True,24-07-2024 06:20,12-07-2024 15:42
52455774,googletrans stopped working with error &#39;nonetype&#39; object has no attribute &#39;group&#39;,"i was trying googletrans and it was working quite well. since this morning i started getting below error. i went through multiple posts from stackoverflow and other sites and found probably my ip is banned to use the service for sometime. i tried using multiple service provider internet that has different ip and stil facing the same issue ? i also tried to use googletrans on different laptops , still same issue ..is googletrans package broken or something google did at their end ?
>>> from googletrans import translator
>>> translator = translator()
>>> translator.translate('ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½.')

traceback (m""<pyshell#2>"", line 1, in <module>
    translator.translate('ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿""/library/frameworks/python.framework/versions/3.6/lib/python3.6/site-packages/googletrans/client.py"", line 172, in translate
    data = self._translate(text, dest, src)
  file ""/library/frameworks/python.framework/versions/3.6/lib/python3.6/site-packages/googletrans/client.py"", line 75, in _translate
    token = self.token_acquirer.do(text)
  file ""/library/frameworks/python.framework/versions/3.6/lib/python3.6/site-packages/googletrans/gtoken.py"", line 180, in do
    self._update()
  file ""/library/frameworks/python.framework/versions/3.6/lib/python3.6/site-packages/googletrans/gtoken.py"", line 59, in _update
    code = unicode(self.re_tkk.search(r.text).group(1)).replace('var ', '')
attributeerror: 'nonetype' object has no attribute 'group'","['python', 'nlp', 'google-translate', 'googletrans']",52456197,"update 01/12/2020: this issue re-emerged lately, (apparently) caused once again by some changes on the google translation api.
a solution is being discussed (again) in this github issue. although there is not a definitive solution yet a pull request seem to be solving the problem: 
while we wait for it to be approved it can be installed like this:
$ pip uninstall googletrans
$ git clone 
$ cd ./py-googletrans
$ git checkout origin/feature/enhance-use-of-direct-api
$ python setup.py install

original answer:
apparently it's a recent and widespread problem on google's side.
quoting various github discussions, it happens when google sends you directly the raw token.
it's being discussed right now and there is already a pull request to fix it, so it should be resolved in the next few days.
for reference, see:
 <-- exact same problem reported on the github repo
 <-- seemingly same problem on a text-to-speech library
 <-- pull request to fix the issue
to apply this patch (without waiting for the pull request to be accepted) simply install the library from the forked repo  (uninstall the official library first):
$ pip uninstall googletrans
$ git clone 
$ cd ./py-googletrans
$ python setup.py install

you can clone it anywhere on your system and install it globally or while inside a virtualenv.",https://stackoverflow.com/questions/52455774,python,22-09-2018 10:29,233992.0,224.0,18.0,True,23-03-2023 07:49,10-12-2022 18:36
45126071,how to extract numbers (along with comparison adjectives or ranges),"i am working on two nlp projects in python, and both have a similar task to extract numerical values and comparison operators from sentences, like the following:
""... greater than $10 ... "",
""... weight not more than 200lbs ..."",
""... height in 5-7 feets ..."",
""... faster than 30 seconds ... ""

i found two different approaches to solve this problem:

using very complex regular expressions.
using named entity recognition (and some regexes, too).

how can i parse numerical values out of such sentences? i assume this is a common task in nlp.

the desired output would be something like:
input:

""greater than $10""

output:
{'value': 10, 'unit': 'dollar', 'relation': 'gt', 'position': 3}","['python', 'regex', 'nlp', 'nltk', 'spacy']",45130193,"i would probably approach this as a chunking task and use nltk's part of speech tagger combined with its regular expression chunker.  this will allow you to define a regular expression based on the part of speech of the words in your sentences instead of on the words themselves.  for a given sentence, you can do the following:
import nltk

# example sentence
sent = 'send me a table with a price greater than $100'

the first thing i would do is to modify your sentences slightly so that you don't confuse the part of speech tagger too much.  here are some examples of changes that you can make (with very simple regular expressions) but you can experiment and see if there are others:
$10 -> 10 dollars
200lbs -> 200 lbs
5-7 -> 5 - 7 or 5 to 7

so we get:
sent = 'send me a table with a price greater than 100 dollars'

now you can get the parts of speech from your sentence:
sent_pos = nltk.pos_tag(sent.split())
print(sent_pos)

[('send', 'vb'), ('me', 'prp'), ('a', 'dt'), ('table', 'nn'), ('with', 'in'), ('a', 'dt'), ('price', 'nn'), ('greater', 'jjr'), ('than', 'in'), ('100', 'cd'), ('dollars', 'nns')]

we can now create a chunker which will chunk your pos tagged text according to a (relatively) simple regular expression:
grammar = 'numericalphrase: {<nn|nns>?<rb>?<jjr><in><cd><nn|nns>?}'
parser = nltk.regexpparser(grammar)

this defines a parser with a grammar that chunks numerical phrases (what we'll call your phrase type). it defines your numerical phrase as: an optional noun, followed by an optional adverb, followed by a comparative adjective, a preposition, a number, and an optional noun. 
this is just a suggestion for how you may want to define your phrases, but i think that this will be much simpler than using a regular expression on the words themselves.  
to get your phrases you can do:
print(parser.parse(sent_pos))
(s
  send/vb
  me/prp
  a/dt
  table/nn
  with/in
  a/dt
  (numericalphrase price/nn greater/jjr than/in 100/cd dollars/nns))  

or to get only your phrases you can do:
print([tree.leaves() for tree in parser.parse(sent_pos).subtrees() if tree.label() == 'numericalphrase'])

[[('price', 'nn'),
  ('greater', 'jjr'),
  ('than', 'in'),
  ('100', 'cd'),
  ('dollars', 'nns')]]",https://stackoverflow.com/questions/45126071,python,16-07-2017 07:19,6701.0,37.0,2.0,True,28-10-2022 21:25,12-03-2020 08:20
75672080,map (1:1) n input sentences to n given sentences by similarity,"i want to map n_a input sentences to n_b given sentences so that the mapping is one-to-one. that is,

every n_a is assigned
no n_a appears more than once

unfortunately the inputs vary slightly over time. here is a representative mapping:
{ ""typea"":            ""step1 typea (before typeb)""
, ""typed"":            ""type d""
, ""actiona"":          ""actiona-suffix: typeb or type d (type e available)""
, ""typee"":            ""typee - (not-actiona)""
, ""actionb"":          ""actionb some descriptive words""
, ""typea subtypea"":   ""subtypea typea or typex - (not for typeb)""
, ""actiona subtypea"": ""actiona-suffix: subtypea (type e available)""
, ""typeb subtypea"":   ""subtypea typeb""
, ""typec subtypea"":   ""subtypea typec""
, ""typeb"":            ""typeb (not subtypea)""
, ""typef"":            ""get typef or typef-subtypea""
, ""typef actionb"":    ""actionb typef or typef subtypea""
}

following [1], i've created this workflow using the sentence_transformers package[2]:

bert -> mean-pool -> cosine similarity

given the inputs it is clear that string-based alignment plus edit-distance (of the type featured in rapidfuzz[3]) won't work. but i'm not sure if bert is the best approach, or if it is overkill for my needs. perhaps i should use a word embedding model (word2vec, glove, etc) rather than a sentence embedding model. for this particular task i wonder if bert could be tweaked to perform better.
the categories aren't well differentiated so that bert sometimes maps an input to more than one given. an input n_a can be the best match for multiple givens n_b0, ..., n_bi. in such cases i keep the map with the best score and for the others fall upon the 2nd-best, 3rd-best, ... map. how can i improve bert's performance to avoid these duplicates?

current implementation below:
import pandas as pd
import sentence_transformers as st

model = st.sentencetransformer('all-mpnet-base-v2')

sentences_input = [ ... ]
sentences_given = [ ... ]

# create the encodings and apply mean pooling.
enc_input = model.encode(sentences_input)
enc_given = model.encode(sentences_given)

# calculate a cosine similarity matrix
cos_similarity = st.util.cos_sim(enc_input, enc_given)

# as a pandas dataframe, label the matrix with the sentences.
df = pd.dataframe\
  (columns=sentences_input, index=sentences_given, dtype=float)
for i, sgiven in enumerate(sentences_given):
  for j, sinput in enumerate(sentences_input):
    df.loc[sgiven, sinput] = cos_similarity[j,i].item()

# for each given sentence, extract the best-scoring input sentence. which
# unfortunately is not a one-to-one mapping.
mapping_bad_duplicates = df.idxmax(axis=1)

# create a one-to-one mapping by iterating over the matches in order of best
# score. for each map, blocklist the row and column sentences, preventing
# duplicates.
mapping_good = {}
by_scores = sorted(df.unstack().items(), key=lambda k: k[1], reverse=true)
sentences = set(sentences_input) | set(sentences_given)
for (sinput,sgiven), score in by_scores:
  if not sentences:
    break
  if sgiven not in sentences or sinput not in sentences:
    continue
  mapping_good[sgiven] = sinput
  sentences.remove(sgiven)
  sentences.remove(sinput)

# convert the result to a dataframe
mapping_good_df = pd.series(mapping_good)


bert for measuring text similarity
sbert: semantic textual similarity
rapidfuzz","['python', 'deep-learning', 'nlp', 'similarity']",75672558,"there is no need for any fancy nlp at all. this looks like a relatively straightforward assignment problem. for a string a of a_list and a string b of b_list, define score(a, b) to be the number of words in common of a and b, and then solve the assignment problem to maximise the scores.
below i didn't even properly solve the assignment problem, and instead returned a greedy solution, and yet it still found the best matches:
def score(a, b):
    b_words = b.replace('-', ' ').split()
    return sum((w in b_words) for w in a.split())

def greedy_matching(a_list, b_list):
    b_available = set(b_list)
    for a in sorted(a_list, key=len, reverse=true):
        b = max(b_available, key=lambda b: score(a.lower(), b))
        yield (a, b)
        b_available.remove(b)

a_list = ['typec subtypea', 'actiona', 'typed', 'typee', 'typef', 'actiona subtypea', 'typeb', 'typef actionb', 'typeb subtypea', 'actionb', 'typea subtypea', 'typea']

b_list = ['typeb (not subtypea)', 'typee - (not-actiona)', 'actionb some descriptive words', 'get typef or typef-subtypea', 'type d', 'subtypea typea or typex - (not for typeb)', 'step1 typea (before typeb)', 'subtypea typec', 'actiona-suffix: subtypea (type e available)', 'subtypea typeb', 'actionb typef or typef subtypea', 'actiona-suffix: typeb or type d (type e available)']

b_list = [s.lower().replace('type ', 'type') for s in b_list]
pairs = list(greedy_matching(a_list, b_list))

print(*pairs, sep='\n')

output:
('actiona subtypea', 'actiona-suffix: subtypea (typee available)')
('typec subtypea', 'subtypea typec')
('typeb subtypea', 'subtypea typeb')
('typea subtypea', 'subtypea typea or typex - (not for typeb)')
('typef actionb', 'actionb typef or typef subtypea')
('actiona', 'actiona-suffix: typeb or typed (typee available)')
('actionb', 'actionb some descriptive words')
('typed', 'typed')
('typee', 'typee - (not-actiona)')
('typef', 'get typef or typef-subtypea')
('typeb', 'typeb (not subtypea)')
('typea', 'step1 typea (before typeb)')",https://stackoverflow.com/questions/75672080,python,08-03-2023 10:51,88.0,1.0,1.0,True,08-03-2023 11:41,08-03-2023 11:41
76264711,ent.sent.text in spacy returns labels instead of the sentence for ner problem,"i'm trying to solve a name entity recognision(ner) problem using spacy of the pdf files. i want to get the modal verbs(will, shall, should, must, etc..) from the pdf files.
i trained the data in spacy. when predicting using the trained modal, the ent.sent.text attribute of the modal usualy returns the text or can say the sentence from which the label extracted. but in my case it returns the label itself not the sentence. anyone help me please.
the codes are giving below:
code for data preparation
def load_training_data_from_csv(file_path):
    nlp = spacy.load('en_core_web_md')
    train_data = []
    with open(file_path, 'r', encoding='cp1252') as f:
        reader = csv.dictreader(f)
        for row in reader:
            sentence = row['text']
            start, end = int(row['start']), int(row['end'])
            label = row['label']
            train_data.append((sentence, {""entities"": [(start, end, label)]}))
            # check the alignment
            from spacy.training import offsets_to_biluo_tags
            doc = nlp.make_doc(sentence)
            tags = offsets_to_biluo_tags(doc, [(start, end, label)])
            if '-' in tags:
                print(f""warning: misaligned entities in '{sentence}' with entities {[(start, end, label)]}"")
    return train_data

training the model
def train_spacy_ner(train_data, n_iter=10):
    # load the existing model
    nlp = spacy.load('en_core_web_md')

    # add the ner pipeline if it doesn't exist
    if ""ner"" not in nlp.pipe_names:
        ner = nlp.create_pipe(""ner"")
        nlp.add_pipe(ner, last=true)
    else:
        ner = nlp.get_pipe(""ner"")


    # add the new label ""currency"" to the ner model
    ner.add_label(""will"")
    ner.add_label(""shall"")
    ner.add_label(""must"")


    # train the ner model
    optimizer = nlp.begin_training()
    for i in range(n_iter):
        print(""epoch - "", i) if i % 2 == 0 or i == n_iter else none
        random.shuffle(train_data)
        losses = {}
        for text, annotations in train_data:
            doc = nlp.make_doc(text)
            example = spacy.training.example.from_dict(doc, annotations)
            nlp.update([example], sgd=optimizer, losses=losses)
        print(""loss : "", losses) if i % 2 == 0 or i == n_iter else none

    return nlp

calling the functions
# nlp = spacy.load(""en_core_web_md"")
file_path = ""/content/traindata.csv""
train_data = load_training_data_from_csv(file_path)

# train the model
nlp = train_spacy_ner(train_data)
nlp.to_disk('custom_ner')

predicting using model (here is the problem starting)
import spacy

nlp = spacy.load('custom_ner')
text = ""the language will be in english""

doc = nlp(text)
# print(doc.ents)
for ent in doc.ents:
  print(ent.sent.text, ent.start_char, ent.end_char, ent.label_)

ent.sent.text should return the sentence used above. but here the label itself is returing.
output getting
will 13 17 will

expecting output
the language will be in english 13 17 will

train data




text
start
end
label




i will do the procedures
2
6
will


you should send the letters
4
10
should","['python', 'machine-learning', 'nlp', 'spacy', 'named-entity-recognition']",76271463,"the reason is that calling the below code. so remove it from the train function
#optimizer = nlp.begin_training()

which will also reinitialize all models. as a result, the parser (which performs the sentence splitting), will predict the sentence boundaries using a zeroed-out softmax layer and will start detecting a boundary after every token.
so, should remove the line that calls begin_training. then later when you update the pipe, you can remove the sgd parameter and the pipe will create an optimizer internally:
nlp.update([example], losses=losses)",https://stackoverflow.com/questions/76264711,python,16-05-2023 15:20,264.0,0.0,1.0,True,17-05-2023 11:12,17-05-2023 06:44
46290313,how to break up document by sentences with spacy,"how can i break a document (e.g., paragraph, book, etc) into sentences. 
for example, ""the dog ran. the cat jumped"" into [""the dog ran"", ""the cat jumped""]  with spacy?","['python', 'spacy', 'sentence', 'text-segmentation']",55819791,"the up-to-date answer is this:
from __future__ import unicode_literals, print_function
from spacy.lang.en import english # updated

raw_text = 'hello, world. here are two sentences.'
nlp = english()
nlp.add_pipe('sentencizer')
doc = nlp(raw_text)
sentences = [sent.text.strip() for sent in doc.sents]",https://stackoverflow.com/questions/46290313,python,19-09-2017 01:14,39332.0,39.0,6.0,True,20-10-2023 16:07,26-05-2021 12:05
76217781,how to continue training with huggingface trainer?,"when training a model with huggingface trainer object, e.g. from 
from transformers import seq2seqtrainer, seq2seqtrainingarguments

import os
os.environ[""wandb_disabled""] = ""true""

batch_size = 2

# set training arguments - these params are not really tuned, feel free to change
training_args = seq2seqtrainingarguments(
    output_dir=""./"",
    evaluation_strategy=""steps"",
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    predict_with_generate=true,
    logging_steps=2,  # set to 1000 for full training
    save_steps=16,    # set to 500 for full training
    eval_steps=4,     # set to 8000 for full training
    warmup_steps=1,   # set to 2000 for full training
    max_steps=16,     # delete for full training
    # overwrite_output_dir=true,
    save_total_limit=1,
    #fp16=true, 
)


# instantiate trainer
trainer = seq2seqtrainer(
    model=multibert,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
)

trainer.train()

when it finished training, it outputs:
trainoutput(global_step=16, training_loss=10.065429925918579, metrics={'train_runtime': 541.4209, 'train_samples_per_second': 0.059, 'train_steps_per_second': 0.03, 'total_flos': 19637939109888.0, 'train_loss': 10.065429925918579, 'epoch': 0.03})


if we want to continue training with more steps, e.g. max_steps=16 (from previous trainer.train() run) and another max_steps=160, do we do something like this?
from transformers import seq2seqtrainer, seq2seqtrainingarguments

import os
os.environ[""wandb_disabled""] = ""true""

batch_size = 2

# set training arguments - these params are not really tuned, feel free to change
training_args = seq2seqtrainingarguments(
    output_dir=""./"",
    evaluation_strategy=""steps"",
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    predict_with_generate=true,
    logging_steps=2,  # set to 1000 for full training
    save_steps=16,    # set to 500 for full training
    eval_steps=4,     # set to 8000 for full training
    warmup_steps=1,   # set to 2000 for full training
    max_steps=16,     # delete for full training
    # overwrite_output_dir=true,
    save_total_limit=1,
    #fp16=true, 
)


# instantiate trainer
trainer = seq2seqtrainer(
    model=multibert,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
)

# first 16 steps.
trainer.train()


# set training arguments - these params are not really tuned, feel free to change
training_args_2 = seq2seqtrainingarguments(
    output_dir=""./"",
    evaluation_strategy=""steps"",
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    predict_with_generate=true,
    logging_steps=2,  # set to 1000 for full training
    save_steps=16,    # set to 500 for full training
    eval_steps=4,     # set to 8000 for full training
    warmup_steps=1,   # set to 2000 for full training
    max_steps=160,     # delete for full training
    # overwrite_output_dir=true,
    save_total_limit=1,
    #fp16=true, 
)


# instantiate trainer
trainer = seq2seqtrainer(
    model=multibert,
    tokenizer=tokenizer,
    args=training_args_2,
    train_dataset=train_data,
    eval_dataset=val_data,
)

# continue training for 160 steps
trainer.train()

if the above is not the canonical way to continue training a model, how to continue training with huggingface trainer?

edited
with transformers version, 4.29.1, trying @maciej-skorski answer with seq2seqtrainer,
trainer = seq2seqtrainer(
    model=multibert,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
    resume_from_checkpoint=true
)

its throwing an error:
typeerror: seq2seqtrainer.__init__() got an unexpected keyword argument 'resume_from_checkpoint'","['python', 'machine-learning', 'huggingface-transformers', 'huggingface-trainer']",76246674,"if your use-case is about adjusting a somewhat-trained model then it can be solved just the same way as fine-tuning. to this end, you pass the current model state along with a new parameter config to the trainer object in pytorch api. i would say, this is canonical :-)
the code you proposed matches the general fine-tuning pattern from huggingface docs
trainer = trainer(
    model,
    tokenizer=tokenizer,
    training_args,
    train_dataset=...,
    eval_dataset=...,
)

you may also resume training from existing checkpoints
trainer.train(resume_from_checkpoint=true)",https://stackoverflow.com/questions/76217781,python,10-05-2023 11:20,8640.0,5.0,1.0,True,19-05-2023 12:35,15-05-2023 17:47
72804704,reduce fasttext memory usage for big models,"i trained a machine learning sentence classification model that uses, among other features, also the vectors obtained from a pretrained fasttext model (like these) which is 7gb.  i use the pretrained fasttext italian model: i am using this word embedding only to get some semantic features to feed into the effective ml model.
i built a simple api based on fasttext that, at prediction time, computes the vectors needed by the effective ml model. under the hood, this api receives a string as input and calls get_sentence_vector. when the api starts, it loads the fasttext model into memory.
how can i reduce the memory footprint of fasttext, which is loaded into ram?
constraints:

my model works fine, training was time-consuming and expensive, so i wouldn't want to retrain it using smaller vectors
i need the fasttext ability to handle out-of-vocabulary words, so i can't use just vectors but i need the full model
i should reduce the ram usage, even at the expense of a reduction in speed.

at the moment, i'm starting to experiment with compress-fasttext...
please share your suggestions and thoughts even if they do not represent full-fledged solutions.","['python', 'machine-learning', 'optimization', 'nlp', 'fasttext']",73458984,"there is no easy solution for my specific problem: if you are using a fasttext embedding as a feature extractor, and then you want to use a compressed version of this embedding, you have to retrain the final classifier, since produced vectors are somewhat different.
anyway, i want to give a general answer for
fasttext models reduction

unsupervised models (=embeddings)
you are using pretrained embeddings provided by facebook or you trained your embeddings in an unsupervised fashion. format .bin. now you want to reduce model size/memory consumption.
straight-forward solutions:

compress-fasttext library: compress fasttext word embedding models by orders of magnitude, without significantly affecting their quality; there are also available several pretrained compressed models (other interesting compressed models here).

fasttext native reduce_model: in this case, you are reducing vector dimension (eg from 300 to 100), so you are explictly losing expressiveness; under the hood, this method employs pca.


if you have training data and can perform retraining, you can use floret, a fasttext fork by explosion (the company of spacy), that uses a more compact representation for vectors.
if you are not interested in fasttext ability to represent out-of-vocabulary words (words not seen during training), you can use .vec file (containing only vectors and not model weights) and select only a portion of the most common vectors (eg the first 200k words/vectors). if you need a way to convert .bin to .vec, read this answer.
note: gensim package fully supports fasttext embedding (unsupervised mode), so these operations can be done through this library (more details in this answer)
supervised models
you used fasttext to train a classifier, producing a .bin model. now you want to reduce classifier size/memory consumption.

the best solution is fasttext native quantize: the model is retrained applying weights quantization and feature selection. with the retrain parameter, you can decide whether to fine-tune the embeddings or not.
you can still use fasttext reduce_model, but it leads to less expressive models and the size of the model is not heavily reduced.",https://stackoverflow.com/questions/72804704,python,29-06-2022 16:14,2053.0,1.0,1.0,True,29-08-2022 08:08,30-06-2022 08:42
75940556,fuzzy string matching in python for structured strings?,"i have a python implementation of fuzzy matching using the levenshtein similarity. i'm pretty happy with it but i feel i'm leaving a lot on the table by not considering the structure of the strings.
here are some examples of matches that are clearly good, but not captured well by levenshtein :

the hobbit  / hobbit/the
charlies angles / charlie's angels
apples & pairs / apples and pairs

i think some normalization ahead of using levenshtein would be good - eg. replace all & with and, remove punctuation, etc...  not sure i want to jump straight to stop-word removal and lematization, but something along those line
to avoid re-inventing the wheel, is there any easy way to do this? or an alternative to levenshtine that addresses these issues (short of some bert embeddings)","['nlp', 'string-matching', 'fuzzy-comparison']",75983735,"rapidfuzz.utils.default_process might be an option to consider for preprocessing.

rapidfuzz.utils.default_process(sentence: str) ï¿½ï¿½ï¿½ str
this function preprocesses a string by:

removing all non alphanumeric characters
trimming whitespaces
converting all characters to lower case

parameters:
sentence (str) ï¿½ï¿½ï¿½ string to preprocess
returns:
processed_string ï¿½ï¿½ï¿½ processed string
return type:
str",https://stackoverflow.com/questions/75940556,nlp,05-04-2023 14:17,1273.0,0.0,2.0,True,11-04-2023 08:10,05-04-2023 14:19
75561777,stemming texts separates words into letters,"i am trying to process my text using tokenization, stemming, normalization and stop-word/punctuation removal, etc.
when i use snowball stemming technique, my text gets separated into letters with commas in between.
def processed_tweets(text):

  punctuate_text= str.maketrans('', '', string.punctuation+string.digits)
  text = text.translate(punctuate_text)

  tokens = word_tokenize(text)

  stop_words = set(stopwords.words('english'))
  filtered_words = [w for w in tokens if not w in stop_words]

  #applying stemming 
  snow_stemmer = snowballstemmer(language='english')
  text = [snow_stemmer.stem(word) for word in text]

  return text


tweet_df['processed_tweets'] = tweet_df['tweet body'].apply(processed_tweets)
tweet_df.head()

this is the output i am getting:

following is the output for print(tokens)

this is not the case when using lemmatization though. is there an issue on how i am writing my code or the technique i am using (stemming vs lemmatization)?","['python', 'sentiment-analysis', 'stemming', 'lemmatization']",75564815,"pretty much a very small misunderstanding on the use of tokenize on my part. editing how i apply stemming to tokenized words instead of the 'text' string works:
text = [snow_stemmer.stem(word) for word in filtered_words]",https://stackoverflow.com/questions/75561777,python,24-02-2023 22:05,368.0,0.0,2.0,True,25-02-2023 10:38,25-02-2023 10:27
69765540,can&#39;t get dimensions right - cnn for text classification,"this is my cnn class:
class cnn(nn.module):
    def __init__(
        self,
        vocab_size,
        emb_dim,
        out_channels,
        kernel_sizes,
        dropout,
    ):
        super().__init__()
        
        self.embedding = nn.embedding(vocab_size, emb_dim)
        self.conv_0 = nn.conv2d(in_channels=1, out_channels=out_channels, kernel_size=(kernel_sizes[0], emb_dim), 2)
        
        self.conv_1 = nn.conv2d(in_channels=1, out_channels=out_channels, kernel_size=(kernel_sizes[1], emb_dim), 2)
        
        self.conv_2 = nn.conv2d(in_channels=1, out_channels=out_channels, kernel_size=(kernel_sizes[2], emb_dim), 2)
        
        self.fc = nn.linear(len(kernel_sizes) * out_channels, 1)
        
        self.dropout = nn.dropout(dropout)

        
        
    def forward(self, text):
        
        embedded = self.embedding(text)
        print('embedded', embedded.shape)
        embedded = embedded.unsqueeze(1)  # may be reshape here
        print('embedded', embedded.shape)

        conved_0 = f.relu(self.conv_0(embedded)).squeeze(3)  # may be reshape here
        print('conved_0', conved_0.shape)
        conved_1 = f.relu(self.conv_1(embedded)).squeeze(3)  # may be reshape here
        print('conved_1', conved_1.shape)
        conved_2 = f.relu(self.conv_2(embedded)).squeeze(3)  # may be reshape here
        print('conved_2', conved_2.shape)
        
        pooled_0 = f.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)
        print('pooled_0', pooled_0.shape)
        pooled_1 = f.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)
        print('pooled_1', pooled_1.shape)
        pooled_2 = f.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)
        print('pooled_2', pooled_2.shape)
        
        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))
        print('cat', cat.shape)
            
        return self.fc(cat)

variables:
kernel_sizes = [3, 4, 5]
vocab_size = len(text.vocab)
out_channels = 64
dropout = 0.2
dim = 300

model = cnn(vocab_size=vocab_size, emb_dim=dim, out_channels=out_channels,
            kernel_sizes=kernel_sizes, dropout=dropout)

and training:
import numpy as np

min_loss = np.inf

cur_patience = 0

for epoch in range(1, max_epochs + 1):
    train_loss = 0.0
    model.train()
    pbar = tqdm(enumerate(train_iter), total=len(train_iter), leave=false)
    pbar.set_description(f""epoch {epoch}"")
    for it, batch in pbar: 
        #your code goes here
        opt.zero_grad()
        input = batch.text[0].to(device)
        output = model(input)
        train_loss = loss_func(output, batch.label)
        train_loss.backward()
        opt.step()
        

    train_loss /= len(train_iter)
    val_loss = 0.0
    model.eval()
    pbar = tqdm(enumerate(valid_iter), total=len(valid_iter), leave=false)
    pbar.set_description(f""epoch {epoch}"")
    for it, batch in pbar:
        # your code goes here
        input = batch.text[0].to(device)
        output = model(input)
        val_loss = loss_fn(output, batch.label)

    val_loss /= len(valid_iter)
    if val_loss < min_loss:
        min_loss = val_loss
        best_model = model.state_dict()
    else:
        cur_patience += 1
        if cur_patience == patience:
            cur_patience = 0
            break
    
    print('epoch: {}, training loss: {}, validation loss: {}'.format(epoch, train_loss, val_loss))
model.load_state_dict(best_model)

i get this error:

runtimeerror: expected 4-dimensional input for 4-dimensional weight [64, 1, 3, 300], but got 3-dimensional input of size [894, 1, 300] instead

in this line:

---> 32         conved_0 = f.relu(self.conv_0(embedded)).squeeze(3)

i've tried using conv1d, but still had problems with dimensions. could somebody please explain what should i fix here for the network to train?

edit:
this is my class but with conv1d:

class cnn(nn.module):

    def __init__(
        self,
        vocab_size,
        emb_dim,
        out_channels,
        kernel_sizes,
        dropout,
    ):
        super().__init__()
        
        self.embedding = nn.embedding(vocab_size, emb_dim)
        self.conv_0 = nn.conv1d(in_channels=1, out_channels=out_channels, kernel_size=kernel_sizes[0])
        
        self.conv_1 = nn.conv1d(in_channels=1, out_channels=out_channels, kernel_size=kernel_sizes[1])
        
        self.conv_2 = nn.conv1d(in_channels=1, out_channels=out_channels, kernel_size=kernel_sizes[2])
        
        self.fc = nn.linear(len(kernel_sizes) * out_channels, 1)
        
        self.dropout = nn.dropout(dropout)

        
        
    def forward(self, text):
        
        embedded = self.embedding(text)
        print('embedded', embedded.shape)
        embedded = embedded.unsqueeze(1)  # may be reshape here
        print('embedded', embedded.shape)

        conved_0 = f.relu(self.conv_0(embedded))  # may be reshape here
        print('conved_0', conved_0.shape)
        conved_1 = f.relu(self.conv_1(embedded))  # may be reshape here
        print('conved_1', conved_1.shape)
        conved_2 = f.relu(self.conv_2(embedded))  # may be reshape here
        print('conved_2', conved_2.shape)
        
        pooled_0 = f.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)
        print('pooled_0', pooled_0.shape)
        pooled_1 = f.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)
        print('pooled_1', pooled_1.shape)
        pooled_2 = f.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)
        print('pooled_2', pooled_2.shape)
        
        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))
        print('cat', cat.shape)
            
        return self.fc(cat)

dimensions output:
embedded torch.size([1115, 300])
embedded torch.size([1115, 1, 300])
conved_0 torch.size([1115, 64, 298])
conved_1 torch.size([1115, 64, 297])
conved_2 torch.size([1115, 64, 296])
pooled_0 torch.size([1115, 64])
pooled_1 torch.size([1115, 64])
pooled_2 torch.size([1115, 64])
cat torch.size([1115, 192])

error:

valueerror: target size (torch.size([128])) must be the same as input size (torch.size([1115, 1]))","['python', 'conv-neural-network', 'text-classification', 'dimensions', 'word-embedding']",69773346,"what i was missing is that i had batch_first parameter set to true, which swaped batch_size and seq_len. once i've set it to false, everything worked perfectly.",https://stackoverflow.com/questions/69765540,python,29-10-2021 08:06,185.0,0.0,1.0,True,29-10-2021 18:29,29-10-2021 08:52
76139203,using openai to call openai.completion.create the response is shorter than what chatgpt returns,"i have a paid account for chatgpt and using the latest 4 version, when i ask it to answer a question it send back a response that is like 4k characters.  when i use the api for same question, i get back a much shorter response that is about 1k characters.  i decided to ask  chatgpt what parameters to use in the api call to get back the same response and i pasted it below.   it used max tokens as 30 so i upped it to 4k, but the response is still too short. i then modified the engine to use davinci003 from 002 with no luck.
any ideas on what i can change, i spent about 2 hours researching and still can't get it to work.
def generate_completion(prompt):
    response = openai.completion.create(
      engine=""text-davinci-003"",
      prompt=prompt,
      max_tokens=4000,
      n=1,
      stop=none,
      temperature=0.8,
      top_p=1,
      frequency_penalty=0,
      presence_penalty=0,
    )
    generated_text = response.choices[0].text.strip()
    return generated_text","['response', 'openai-api', 'short']",76144563,"you need to use chat instead of completion.
for python, the example code is as follows:
completion = openai.chatcompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
    {""role"": ""user"", ""content"": ""hello!""}
  ]
)

if you have access to gpt-4 api, then you can change the model to gpt-4. note that having a paid account doesn't guarantee access to gpt-4 api. otherwise, you can use model gpt-3.5-turbo or gpt-3.5-turbo-0301, which is accessible to all but less powerful than the gpt-4 model.",https://stackoverflow.com/questions/76139203,response,30-04-2023 01:37,3415.0,0.0,1.0,True,15-09-2024 10:00,15-09-2024 10:00
75524716,how to store stanza span in mongodb collection?,"i am trying to add a list of dictionaries (whose name is stanzanerlist) like the following:
stanzanerlist = [{
  ""text"": ""harry potter"",
  ""type"": ""per"",
  ""start_char"": 141,
  ""end_char"": 153
}, {
  ""text"": ""hogwarts"",
  ""type"": ""loc"",
  ""start_char"": 405,
  ""end_char"": 413
}, {
  ""text"": ""jk rowling"",
  ""type"": ""per"",
  ""start_char"": 505,
  ""end_char"": 515
}]

as a field in a mongodb document in a collection.
i am inserting the whole document as follows with stanzanerlist as the last item in mongodocument:
mongodocument = {
        ""_id"": urlid,
        ""source"": sourcename,
        ""stanzadoc"": stanzadoc.to_serialized(),
        ""stanzaver"": stanzaver,
        # ""timestamp"": datetime.now(tzinfo),
        ""timestamp"": datetime.now(
            tz=pytz.timezone(cfgdata[""timezone""][""name""])
        ),
        ""stanzanerlist"": stanzanerlist,
    }
try:
        mdbrc = mdbcoll.insert_one(
            mongodocument
        )  # insert fails if url/_id already exists
        return mdbrc
except pymongo.errors.duplicatekeyerror:
        # manage the record update
        print(f""article {urlid} already exists!"")

but while all other fields work well, the addition of stanzanerlist gives the following error:
cannot encode object: {
  ""text"": ""harry potter"",
  ""type"": ""per"",
  ""start_char"": 141,
  ""end_char"": 153
}, of type: <class 'stanza.models.common.doc.span'>

and i'm not able to understand if and how  i could achieve that addition.","['python', 'mongodb', 'pymongo', 'stanford-nlp']",75532725,"pymongo doesn't natively know how to convert <class 'stanza.models.common.doc.span'> types to an acceptable bson data type.
you could ""teach"" pymongo how to do the proper conversion/encoding using a custom bson.codec_options.typeencoder and then pymongo would automatically perform type conversions as it does for other types.  or, you could do the conversion/encoding each time yourself before storing the span in your mongodb collection.
fortunately, stanford nlp stanza has convenience methods for type conversions.  <class 'stanza.models.common.doc.span'> has a to_dict method that will convert the type to type dict, which pymongo does know how to encode.
so, in your code snippet, you could change the mongodocument assignment of ""stanzanerlist"" to:
""stanzanerlist"": [stan.to_dict() for stan in stanzanerlist]

... and then each <class 'stanza.models.common.doc.span'> will be converted to a dict and pymongo should be able to store it.",https://stackoverflow.com/questions/75524716,python,21-02-2023 18:49,97.0,0.0,1.0,True,22-02-2023 12:32,22-02-2023 12:12
79247672,error in getting captum text explanations for text classification,"i have the following code that i am using to identify the most influential words used to correctly predict the text in the test dataset
import pandas as pd
import torch
from torch.utils.data import dataloader
from transformers import berttokenizer, bertforsequenceclassification, adamw
from sklearn.metrics import accuracy_score
from captum.attr import integratedgradients

# loading data
train_df = pd.read_csv('train_dataset.csv')
test_df = pd.read_csv('test_dataset.csv')

# tokenizer
tokenizer = berttokenizer.from_pretrained('bert-base-uncased')

def preprocess_data(df, tokenizer, max_len=128):
    inputs = tokenizer(list(df['text']), padding=true, truncation=true, max_length=max_len, return_tensors=""pt"")
    labels = torch.tensor(df['label'].values)
    return inputs, labels

train_inputs, train_labels = preprocess_data(train_df, tokenizer)
test_inputs, test_labels = preprocess_data(test_df, tokenizer)

# dataloader
train_dataset = torch.utils.data.tensordataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)
train_loader = dataloader(train_dataset, batch_size=16, shuffle=true)

test_dataset = torch.utils.data.tensordataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)
test_loader = dataloader(test_dataset, batch_size=16)

# model setup
device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
model = bertforsequenceclassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)

# optimizer
optimizer = adamw(model.parameters(), lr=5e-5)

# training loop
model.train()
for epoch in range(3):  # train for 3 epochs
    for batch in train_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
    print(f""epoch {epoch+1} loss: {loss.item()}"")

# evaluation
model.eval()
correct_predictions = []
with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        outputs = model(input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)
        correct_predictions.extend(
            (preds == labels).cpu().numpy().tolist()
        )
accuracy = accuracy_score(test_labels.numpy(), correct_predictions)
print(f""test accuracy: {accuracy:.2f}"")

# integrated gradients
ig = integratedgradients(model)

def get_influential_words(input_text, model, tokenizer, ig, device):
    model.eval()
    # tokenizing the input text
    inputs = tokenizer(input_text, return_tensors=""pt"", truncation=true, padding=true, max_length=128)
    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # explicitly convert to longtensor
    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # explicitly convert to longtensor

    print(""input ids shape:"", input_ids.shape, ""dtype:"", input_ids.dtype)
    print(""attention mask shape:"", attention_mask.shape, ""dtype:"", attention_mask.dtype)
    # forward function for ig
    def forward_func(input_ids):
        outputs = model(input_ids, attention_mask=attention_mask)
        return outputs.logits

    # applying integrated gradients
    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=true)
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())
    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()

    return list(zip(tokens, token_importances))

# analysing influential words for correctly predicted texts
for idx, correct in enumerate(correct_predictions):
    if correct:
        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
        print(f""influential words for text: {test_df['text'].iloc[idx]}"")
        print(influential_words)

but i am getting the following error in running the above.
some weights of bertforsequenceclassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
you should probably train this model on a down-stream task to be able to use it for predictions and inference.
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: futurewarning: this implementation of adamw is deprecated and will be removed in a future version. use the pytorch implementation torch.optim.adamw instead, or set `no_deprecation_warning=true` to disable this warning
  warnings.warn(
epoch 1 loss: 0.4719192385673523
epoch 2 loss: 0.39585667848587036
epoch 3 loss: 0.14659778773784637
test accuracy: 0.70
input ids shape: torch.size([1, 8]) dtype: torch.int64
attention mask shape: torch.size([1, 8]) dtype: torch.int64
---------------------------------------------------------------------------
runtimeerror                              traceback (most recent call last)
<ipython-input-9-f047b509c98d> in <cell line: 90>()
     90 for idx, correct in enumerate(correct_predictions):
     91     if correct:
---> 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
     93         print(f""influential words for text: {test_df['text'].iloc[idx]}"")
     94         print(influential_words)

18 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2549         # remove once script supports set_grad_enabled
   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-> 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   2552 
   2553 

runtimeerror: expected tensor for argument #1 'indices' to have one of the following scalar types: long, int; but got torch.cuda.floattensor instead (while checking arguments for embedding)","['machine-learning', 'pytorch', 'nlp', 'huggingface-transformers', 'text-classification']",79248379,"you need to slightly change the gradients calculation class. also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly.
i think that using layerintegratedgradients is better for debugging bert - in line with this tutorial 
below please find snippet that works:
from captum.attr import layerintegratedgradients


def custom_forward(inputs):
    preds = predict(inputs)
    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)
lig = layerintegratedgradients(custom_forward, model.bert.embeddings)
def get_influential_words(input_text, model, tokenizer, ig, device):
    model.eval()
    # tokenizing the input text
    inputs = tokenizer(input_text, return_tensors=""pt"", truncation=true, padding=true, max_length=128)
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    # print(""input ids shape:"", input_ids.shape, ""dtype:"", input_ids.dtype)
    # print(""attention mask shape:"", attention_mask.shape, ""dtype:"", attention_mask.dtype)

    attributions, delta = lig.attribute(input_ids, return_convergence_delta=true)
    
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())
    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()

    return list(zip(tokens, token_importances))

results = []

for idx, correct in enumerate(correct_predictions):
    if correct:
        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
        print(f""influential words for text: {test_df['text'].iloc[idx]}"")
        print(influential_words)",https://stackoverflow.com/questions/79247672,machine-learning,03-12-2024 12:47,92.0,2.0,1.0,True,03-12-2024 16:12,03-12-2024 14:45
74468766,fast way to search for a series of keywords among articles,"to illustrate with an example:
i have a few keywords (case sensitive).
 kw <- c(""american express"", ""inc said"")

i have quite a few articles.
 data(""acq"")
 dv <- sapply(1:length(acq),function(x) acq[[x]]$content) #doing data transformation so that dv is just a vector of strings

i want the following table as an output
temp <- sapply(1:length(kw),function(x) stringr::str_detect(dv,kw[x]))

the problem is, i have millions of records and the method that i am using is not efficient enough.","['r', 'algorithm', 'nlp']",74469280,"what about parallelizing? this is an example based on your code:
library(parallel)

n_cores <- 2 # number of cores for parallel processing
cl <- makecluster(n_cores)
emp <- parsapply(cl, 1:length(acq), fun=function(x,i) str_detect(acq[[x]]$content,kw[i]))
stopcluster(cl)",https://stackoverflow.com/questions/74468766,r,17-11-2022 00:19,45.0,0.0,1.0,True,17-11-2022 17:21,17-11-2022 17:21
74190959,how to write all the sentences with the word &quot;apple&quot; from a txt file,"i have tried this using regex, loops and simple functions as well but cannot figure out anything. here are the few codes i have tried.
import re
fp = open(""apple.txt"")
re.findall(r""([^.]*?apple[^.]*\.)"",fp)","['python', 'nlp', 'nltk']",74191100,"with open(""./waba-crash.txt"", ""r"") as input:
    content = input.read().replace('\n', '')
sentences = list(map(str.strip, content.split(""."")))
with open(""./resultfile.txt"", ""w"") as output:
    for result in sentences:
        if ('police' in result or 'police' in result):
            output.write(f'{result}. \n')

there is no need for regex. it might not be the cleanest answer but it works. this is the beauty of python. when working with files i recommend you to use with open instead of open(). since this will automatically close the for you. otherwise you'd need to use the close() method at the end of your file.
hope this helps you! enjoy",https://stackoverflow.com/questions/74190959,python,25-10-2022 08:13,84.0,0.0,2.0,True,25-10-2022 09:13,25-10-2022 09:13
17531684,"n-grams in python, four, five, six grams?","i'm looking for a way to split a text into n-grams.
normally i would do something like:
import nltk
from nltk import bigrams
string = ""i really like python, it's pretty awesome.""
string_bigrams = bigrams(string)
print string_bigrams

i am aware that nltk only offers bigrams and trigrams, but is there a way to split my text in four-grams, five-grams or even hundred-grams?
thanks!","['python', 'string', 'nltk', 'n-gram']",17547860,"great native python based answers given by other users. but here's the nltk approach (just in case, the op gets penalized for reinventing what's already existing in the nltk library).
there is an ngram module that people seldom use in nltk. it's not because it's hard to read ngrams, but training a model base on ngrams where n > 3 will result in much data sparsity.
from nltk import ngrams

sentence = 'this is a foo bar sentences and i want to ngramize it'

n = 6
sixgrams = ngrams(sentence.split(), n)

for grams in sixgrams:
  print(grams)",https://stackoverflow.com/questions/17531684,python,08-07-2013 16:35,265843.0,181.0,17.0,True,01-05-2023 11:51,09-11-2015 03:42
75904558,attributeerror: &#39;list&#39; object has no attribute &#39;similarity&#39; when using dense passage retriever (pinecone) in haystack - python,"i'm getting the following error:
""attributeerror: 'list' object has no attribute 'similarity'""

when trying to compile my code.
i'm running a nlp q&a pipeline with haystack.
i've recently tried to implement the pinecone vector database, which is causing the error.
the pipeline until the error is essentially as follows :
initialise pinecone datastore -> pass data to convert to haystack compatible docs -> pre-process docs -> pass to haystack dense passage retriever.
to simplify the situation i've collected the various modeules and put all the code in a single executable python file and shared below:
import logging
import os
from haystack.document_stores import inmemorydocumentstore
from haystack.pipelines.standard_pipelines import textindexingpipeline
import asyncio
import time
> #from kai.pinecone_system import initiate_pinecone
from haystack import pipeline

from haystack.document_stores import pineconedocumentstore
> ####remove
def initiate_pinecone():
print(""testing pinecone"")
env=""eu-west1-gcp""
api=""fake-api-key""
document_store = pineconedocumentstore(
api_key=api,
index='esmo', 
environment=env,

>     )
return document_store
> ####remove

logging
logging.basicconfig(
format=""%(levelname)s - %(name)s -  %(message)s"", level=logging.warning
> )
logging.getlogger(""haystack"").setlevel(logging.info)

doc store
document_store = initiate_pinecone()

from haystack.nodes import textconverter, pdftotextconverter, docxtotextconverter, preprocessor
from haystack.utils import convert_files_to_docs

data to docs
doc_dir = ""data/esmo""
> #converter = textconverter(remove_numeric_tables=true, valid_languages=[""en""])
> #doc_txt = converter.convert(file_path=""data/esmo"", meta=none)[0]
all_docs = convert_files_to_docs(dir_path=doc_dir)

pre-processor
from haystack.nodes import preprocessor
preprocessor = preprocessor(
clean_empty_lines=true,
clean_whitespace=true,
clean_header_footer=false,
split_by=""word"",
split_length=150,
split_respect_sentence_boundary=true,
split_overlap=0
> )
processed_esmo_docs = preprocessor.process(all_docs)
print(f""n_files_input: {len(all_docs)}\nn_docs_output: {len(processed_esmo_docs)}"")
print(processed_esmo_docs[0])

write document objects into document store
import os
from haystack.pipelines.standard_pipelines import textindexingpipeline

files_to_index = [doc_dir + ""/"" + f for f in os.listdir(doc_dir)]
indexing_pipeline = textindexingpipeline(document_store)
indexing_pipeline.run_batch(file_paths=files_to_index)

from haystack.nodes import densepassageretriever
retriever = densepassageretriever(
document_store=processed_esmo_docs,
>     #document_store=all_docs,
query_embedding_model=""facebook/dpr-question_encoder-single-nq-base"",
passage_embedding_model=""facebook/dpr-ctx_encoder-single-nq-base"",
max_seq_len_query=64,
max_seq_len_passage=256,
batch_size=2,
use_gpu=true,
embed_title=true,
use_fast_tokenizers=true)

> ##initialize reader
from haystack.nodes import farmreader
reader = farmreader(model_name_or_path=""michiyasunaga/biolinkbert-large"", use_gpu=true)

> ##get pipeline up (retriever / reader)
from haystack.pipelines import extractiveqapipeline
pipe = extractiveqapipeline(reader, retriever)
prediction = """"


many thanks in advance for any advice.
i've tried changing the pinecone vector db to cosine and dotproduct.
altered the pre-processing and also removed the pre-processing which had no effect.
i understand thayt the document store is expecting an attribute called similarity, but i'm not sure what that is exactly.","['python', 'database', 'nlp', 'artificial-intelligence', 'haystack']",75904786,"i have modified your question for security reasons.
in any case, i think you are instantiating the retriever incorrectly.
as you can see in the documentation, densepassageretriever.__init__ expects the document_store parameter, which consists of the document store to be queried; instead, you are incorrectly using the preprocessed documents.
you should try the following retriever initialization:
retriever = densepassageretriever(
document_store=document_store,
...)",https://stackoverflow.com/questions/75904558,python,01-04-2023 06:13,790.0,2.0,1.0,True,01-04-2023 07:25,01-04-2023 07:04
69694277,could not find function &#39;spacy-transformers.transformermodel.v3&#39; in function registry &#39;architectures&#39;,"i was trying to create a custom ner model. i used spacy library to create the model. and this line of code is to create the config file from the base.config file.
my code is :


!python -m spacy init fill-config /content/drive/mydrive/ner_re_new/ner/base_config.cfg /content/drive/mydrive/ner_re_new/ner/config.cfg



error :


catalogue.registryerror: [e893] could not find function 'spacy-transformers.transformermodel.v3' in function registry 'architectures'. if you're using a custom function, make sure the code is available. if the function is provided by a third-party package, e.g. spacy-transformers, make sure the package is installed in your environment.



available names:


spacy-legacy.characterembed.v1, 
spacy-legacy.hashembedcnn.v1, 
spacy-legacy.maxoutwindowencoder.v1, 
spacy-legacy.mishwindowencoder.v1, 
spacy-legacy.multihashembed.v1, 
spacy-legacy.textcatbow.v1, 
spacy-legacy.textcatcnn.v1, 
spacy-legacy.textcatensemble.v1, 
spacy-legacy.tok2vec.v1, 
spacy-legacy.transitionbasedparser.v1, 
spacy-transformers.tok2vectransformer.v1,
spacy-transformers.transformerlistener.v1, 
spacy-transformers.transformermodel.v1, 
spacy.characterembed.v1, 
spacy.entitylinker.v1, 
spacy.hashembedcnn.v1, 
spacy.maxoutwindowencoder.v2, 
spacy.mishwindowencoder.v2, 
spacy.multihashembed.v1, 
spacy.pretraincharacters.v1, 
spacy.pretrainvectors.v1, 
spacy.tagger.v1, 
spacy.textcatbow.v1, 
spacy.textcatcnn.v1, 
spacy.textcatensemble.v2, 
spacy.textcatlowdata.v1, 
spacy.tok2vec.v2, 
spacy.tok2veclistener.v1, 
spacy.torchbilstmencoder.v1, 
spacy.transitionbasedparser.v1, 
spacy.transitionbasedparser.v2","['named-entity-recognition', 'bert-language-model', 'spacy-3', 'spacy-transformers']",69695803,"this happened since spacy had a new update 3.1 recently. and the base_config file have the architecture mentioned as ""spacy-transformers.transformermodel.v3"". change it into ""spacy-transformers.transformermodel.v1""
[components.transformer.model]
@architectures = ""spacy-transformers.transformermodel.v1""
name = ""roberta-base""
tokenizer_config = {""use_fast"": true}",https://stackoverflow.com/questions/69694277,named-entity-recognition,24-10-2021 06:10,5524.0,2.0,4.0,True,23-02-2024 11:20,25-10-2021 08:08
67564014,bert to xlnet train model,"i'm trying to do something like this in xlnet but i can't find this part in the documentation, any help would be valuable, thanks!
# we access the transformer model within our bert object using the bert attribute 
# (eg bert.bert instead of bert)

embeddings = bert.bert(input_ids, attention_mask=mask)[1]  # access pooled activations with [1]

(instead of bert.bert i'm trying to do it with xlnet)","['python', 'tensorflow', 'nlp', 'tf.keras', 'transformer-model']",67577114,"to solve this, let's first see what exactly hides behind the bert.bert property. for this, we can inspect the source code of the library for the tfbertmodel class. there, we can see that it is defined as
self.bert = tfbertmainlayer(config, name=""bert"")

where the tfbertmainlayer is exactly what the name suggests - the main bert transformer component. to be precise, it is defined as follows:
# ...
self.embeddings = tfbertembeddings(config, name=""embeddings"")
self.encoder = tfbertencoder(config, name=""encoder"")
self.pooler = tfbertpooler(config, name=""pooler"") if add_pooling_layer else none
# ...

if we check the source code for tfxlnetmodel, we can see that there is only one property as well, which is defined as
self.transformer = tfxlnetmainlayer(config, name=""transformer"")

since this is similar enough in its name, you should get the same result by simply calling xlnet.transformer(...), although i can't guarantee that all the input parameters work the same.",https://stackoverflow.com/questions/67564014,python,17-05-2021 04:29,181.0,0.0,1.0,True,17-05-2021 21:16,17-05-2021 07:13
71345719,using spacy to remove names from a data frame in python 3.9,"i am working with spacy package v3.2.1 in python 3.9 and wanted to understand how i can use it to remove names from a data frame. i tried following the spacy documentation and i am able to identity names correctly, but not understanding how i can remove them. my goal is to remove all names from a specific column of the data frame.
actual




id
comment




a123
i am five years old, and my name is john


x907
today i met with dr. jacob




what i am trying to accomplish




id
comment




a123
i am five years old, and my name is


x907
today i met with dr.




code:
#loading packages
import spacy
import pandas as pd
from spacy import displacy


#loading csv
df = pd.read_csv('names.csv)

#loading spacy large model
nlp = spacy.load(""en_core_web_lg"")

#checking/testing is spacy large is identifying named entities
df['test_col'] = df['comment'].apply(lambda x: list(nlp(x).ents)) 


what my code does




id
comment
test_col




a123
i am five years old, and my name is john
[(john)]


x907
today i met with dr. jacob
[(jacob)]




but how do i go from removing those names from the comment column? i think i some sort of function that iterates over each row of the data frame and removes the identified entities. would appreciate your help
thank you","['python', 'python-3.x', 'spacy']",71348446,"you can use
import spacy
import pandas as pd

# test dataframe
df = pd.dataframe({'id':['a123','x907'], 'comment':['i am five years old, and my name is john', 'today i met with dr. jacob']})

# initialize the model
nlp = spacy.load('en_core_web_trf')

def remove_names(text):
    doc = nlp(text)
    newstring = text
    for e in reversed(doc.ents):
        if e.label_ == ""person"": # only if the entity is a person
            newstring = newstring[:e.start_char] + newstring[e.start_char + len(e.text):]
    return newstring

df['comment'] = df['comment'].apply(remove_names)
print(df.to_string())

output:
     id                               comment
0  a123  i am five years old, and my name is
1  x907                 today i met with dr.",https://stackoverflow.com/questions/71345719,python,04-03-2022 01:41,2212.0,1.0,2.0,True,04-03-2022 08:22,04-03-2022 01:46
76456284,tokenizing very large text datasets (cannot fit in ram/gpu memory) with tensorflow,"how do we tokenize very large text datasets that don't fit into memory in tensorflow? for image datasets, there is the imagedatagenerator that loads the data per batch to the model, and preprocesses the data. however for text datasets, tokenization is performed before training the model. can the dataset be split into batches for the tokenizer or is there a tensorflow batch tokenizer function that already exist? can this be done without having to import external libraries?
i know that there are external libraries that does this. for example from,","['python', 'tensorflow', 'nlp', 'tokenize', 'data-preprocessing']",76456354,"in your case you need to define your own data processing pipeline using the tf.data module. based on this module you can define an own/customized tf.data.dataset. those datasets support a lot of features like parsing of records into a specific format (using the map function) or batching.
here is a complete example of how you could use the tf.data module for building your own pipeline:",https://stackoverflow.com/questions/76456284,python,12-06-2023 11:37,655.0,0.0,1.0,True,12-06-2023 12:57,12-06-2023 12:57
45332410,roc for multiclass classification,"i'm doing different text classification experiments. now i need to calculate the auc-roc for each task. for the binary classifications, i already made it work with this code:
scaler = standardscaler(with_mean=false)

enc = labelencoder()
y = enc.fit_transform(labels)

feat_sel = selectkbest(mutual_info_classif, k=200)

clf = linear_model.logisticregression()

pipe = pipeline([('vectorizer', dictvectorizer()),
                 ('scaler', standardscaler(with_mean=false)),
                 ('mutual_info', feat_sel),
                 ('logistregress', clf)])
y_pred = model_selection.cross_val_predict(pipe, instances, y, cv=10)
# instances is a list of dictionaries

#visualisation roc-auc

fpr, tpr, thresholds = roc_curve(y, y_pred)
auc = auc(fpr, tpr)
print('auc =', auc)

plt.figure()
plt.title('receiver operating characteristic')
plt.plot(fpr, tpr, 'b',
label='auc = %0.2f'% auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([-0.1,1.2])
plt.ylim([-0.1,1.2])
plt.ylabel('true positive rate')
plt.xlabel('false positive rate')
plt.show()

but now i need to do it for the multiclass classification task. i read somewhere that i need to binarize the labels, but i really don't get how to calculate roc for multiclass classification. tips?","['python', 'scikit-learn', 'text-classification', 'roc', 'multiclass-classification']",45335434,"as people mentioned in comments you have to convert your problem into binary by using onevsall approach, so you'll have n_class number of roc curves.
a simple example:
from sklearn.metrics import roc_curve, auc
from sklearn import datasets
from sklearn.multiclass import onevsrestclassifier
from sklearn.svm import linearsvc
from sklearn.preprocessing import label_binarize
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

iris = datasets.load_iris()
x, y = iris.data, iris.target

y = label_binarize(y, classes=[0,1,2])
n_classes = 3

# shuffle and split training and test sets
x_train, x_test, y_train, y_test =\
    train_test_split(x, y, test_size=0.33, random_state=0)

# classifier
clf = onevsrestclassifier(linearsvc(random_state=0))
y_score = clf.fit(x_train, y_train).decision_function(x_test)

# compute roc curve and roc area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# plot of a roc curve for a specific class
for i in range(n_classes):
    plt.figure()
    plt.plot(fpr[i], tpr[i], label='roc curve (area = %0.2f)' % roc_auc[i])
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('false positive rate')
    plt.ylabel('true positive rate')
    plt.title('receiver operating characteristic example')
    plt.legend(loc=""lower right"")
    plt.show()",https://stackoverflow.com/questions/45332410,python,26-07-2017 16:16,105160.0,42.0,4.0,True,19-02-2022 06:07,18-02-2021 15:12
75038237,not loading all checkpoints when training again,"i want to be able to start training the relevant model from the continuation of the previous day's training, but each time the training starts from a certain checkpoint, not from the last checkpoint, and this makes the training time of the model longer each time. by changing the value of ""continue_from_global_step"" parameter to 1, there was no change in the result.
code snippet related to loading checkpoints:
training
if args.do_train:
    # if output files already exists, assume to continue training from latest checkpoint (unless overwrite_output_dir is set)
    continue_from_global_step = 0 # if set to 0, start training from the beginning
    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir:
        checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/*/' + weights_name, recursive=true)))
        if len(checkpoints) > 0:
            checkpoint = checkpoints[-1]
            logger.info(""resuming training from the latest checkpoint: %s"", checkpoint)
            continue_from_global_step = int(checkpoint.split('-')[-1])
            model = model_class.from_pretrained(checkpoint)
            model.to(args.device)
    
    train_dataset, features = load_and_cache_examples(args, model, tokenizer, processor, evaluate=false)
    global_step, tr_loss = train(args, train_dataset, features, model, tokenizer, processor, continue_from_global_step)
    logger.info("" global_step = %s, average loss = %s"", global_step, tr_loss)","['python-3.x', 'machine-learning', 'nlp', 'training-data', 'checkpoint']",75045505,the problem was solved by manually giving the absolute address of the last checkpoint. the last checkpoint was recognized as wrong.,https://stackoverflow.com/questions/75038237,python-3.x,07-01-2023 05:07,58.0,0.0,1.0,True,16-01-2023 10:09,16-01-2023 10:09
74904141,how do i use server sent events to stream data from openai&#39;s api using ajax and php?,"how can i use server-sent events (sse) to stream data from the above api to a browser client using javascript and php? i've been poring over this for hours but i can't seem to figure out what's wrong. for reference, i am trying to adapt the solution here: stream data from openai gpt-3 api using php
the rest of my code remains more or less the same as the one in the question above. the only part i've modified that isn't working is this:
    curl_setopt($ch, curlopt_writefunction, function ($curl, $data) {
        # str_repeat(' ',1024*8) is needed to fill the buffer and will make streaming the data possible
        $data = json_decode($data, true);

        $text = $data['choices'][0]['text'];

        echo $text . str_repeat(' ', 1024 * 8);
        return strlen($data);
    });

first, i'm trying to return only the ""text"" property in ""choices"" array (see sample api response below).
here's the response i'm getting:

notice: trying to access array offset on value of type null in c:\file_path\sse.php.

secondly, how do i stream the ""text"" to an element on the client in real time? here's my implementation so far.
javascript
        $.ajax({
          type: ""post"",
          url: ""sse.php"",
          data: json.stringify({
            prompt: ""what is the best way to"",
            num_completions: 1,
            temperature: 0.5,
          }),
          contenttype: ""application/json"",
          success: function (response) {
            const source = new eventsource(""sse.php"");

            source.onmessage = function (event) {
              const div = document.getelementbyid(""response"");
              div.innerhtml += event.data + ""<br>"";
              console.log(event);
            };
          },
        });

sample chunks of data streamed by the api look like so. i am trying to stream only the ""text"" part back to the browser.
data: {""id"": ""cmpl-xxxxxxxxxxxxxxxxxxxxxxx"", ""object"": ""text_completion"", ""created"": 1671700494, ""choices"": [{""text"": "" best"", ""index"": 0, ""logprobs"": null, ""finish_reason"": null}], ""model"": ""text-davinci-003""}

data: {""id"": ""cmpl-xxxxxxxxxxxxxxxxxxxxxxx"", ""object"": ""text_completion"", ""created"": 1671700494, ""choices"": [{""text"": "" way"", ""index"": 0, ""logprobs"": null, ""finish_reason"": null}], ""model"": ""text-davinci-003""}

data: {""id"": ""cmpl-xxxxxxxxxxxxxxxxxxxxxxx"", ""object"": ""text_completion"", ""created"": 1671700494, ""choices"": [{""text"": "" to"", ""index"": 0, ""logprobs"": null, ""finish_reason"": null}], ""model"": ""text-davinci-003""}

data: [done]

how should i be implementing this? i'm at my wits' end. thanks in advance.","['php', 'ajax', 'server-sent-events', 'openai-api']",75436651,"i found my way around it using the following code:
//placed at the beginning of the script
@ini_set('zlib.output_compression', 0);
ob_implicit_flush(true);
ob_end_flush();

header(""content-type: text/event-stream"");
header(""cache-control: no-cache"");

//initialize curl and set the necessary headers and request parameters
...
...
curl_setopt($curl, curlopt_writefunction, function ($curl, $data) {
    echo $data;
    return strlen($data);
});

$curl_response = curl_exec($curl);

echo $curl_response;

i then used javascript to extract the text as such:
source.onmessage = function (event) {
  const div = document.getelementbyid(""response"");
  text = json.parse(event.data).choices[0].text;
  div.innerhtml += text;
};",https://stackoverflow.com/questions/74904141,php,23-12-2022 21:32,3349.0,2.0,1.0,True,13-02-2023 13:40,15-01-2023 17:59
68889124,spacy divides sentences inconsistently,"i'm using spacy and i'm having problems with how it splits sentences. for some reason, some sentences are split into two or tree parts instead of one so when i try to train a model with tensorflow i get an error that the dimensions of the vectors do not match.
i have tried changing the tokeniser to the whitespace tokeniser as indicated in this link. so, my class looks like (with the benepar component commented out):
class beneparannotator(syntaxannotator):
    class whitespacetokenizer:
        def __init__(self, vocab):
            self.vocab = vocab

        def __call__(self, text):
            words = text.split("" "")
            spaces = [true] * len(words)
            # avoid zero-length tokens
            for i, word in enumerate(words):
                if word == """":
                    words[i] = "" ""
                    spaces[i] = false
            # remove the final trailing space
            if words[-1] == "" "":
                words = words[0:-1]
                spaces = spaces[0:-1]
            else:
                spaces[-1] = false

            return doc(self.vocab, words=words, spaces=spaces)

    def __init__(self):
        self.nlp = spacy.load('en')
        # self.nlp.add_pipe(beneparcomponent(""benepar_en2""))
        self.nlp.tokenizer = self.whitespacetokenizer(self.nlp.vocab)
        self.extract_arc_representation = false
   
    ... other stuff ...


def ... other stuff ...

now, when parsing the sentences with:
def parse_sentences(nlp, captions: list[str]) -> list[span]:

    parsed_sentences = []
    for caption in tqdm(captions, desc=""parsing sentences""):
        parsed_caption = nlp(caption)
        if len(list(parsed_caption.sents)) > 1:  # this if is for debug
            length = len(list(parsed_caption.sents))
            as_list = list(parsed_caption.sents)
            pass
        parsed_sentence = list(parsed_caption.sents)[0]
        parsed_sentences.append(parsed_sentence)

    return parsed_sentences

this way of getting the sentence is taken from here, as i want to retrieve the sentences parsed with benepar but as some of them are divided into several sentences, instead of getting the whole parsed sentence i get several parsed pieces of a sentence.
and for some reason, for some sentences they are divided in 2 or 3 parts:

example 1: a black sheep having just had it s haired shaved off
it's divided in: [a black sheep, having just had it s haired shaved off]

example 2: street signs say s. 3rd av. and no left turn
it's divided in: [street signs say s., 3rd av., and no left turn]

example 3: one turn way only sign in the middle of the road
it's divided in: [one turn way, only sign in the middle of the road]

example 4: two cows standing in the grass in front of tree 's
it's divided in: [two cows standing in the grass in front of tree, 's]

example 5: a derby car # 30 is seen cutting a corner with it s right door ajar
it's divided in: [a derby car, # 30 is seen cutting a corner with it s right door ajar]


i have other sentences similar to example 1 and they are split correctly in only one sentence. for example:
a back sheep that is standing in the grass
it's divided in: [a back sheep that is standing in the grass]
so when i get the sents for one caption with list(parsed_caption.sents)[0] i'm getting the whole caption instead only a part of the caption.
where is the problem?
----- update to try to explain better my problem. -----
i have a list of sentences that i want to parse with benepar and then use them as input in a tool to see if i get better results. due to limitations of the tool's anaconda environment, i need to use the 2019 version which is the one that works on python 3.6.
the problem i have is that some sentences are split and benepar parses them as independent chunks (instead of the whole sentence together) and as i try to get them with list(parsed_caption.sents)[0] (as there should only be one element, the whole sentence) and i get only one chunk, when i run the experiments i get an error that the size of the original sentence and the parsed sentence don't match.
i have over 616k sentences for the training set. most of them are parsed fine and not divided in chunks and if i do:
caption = list(parsed_caption.sents)[0]

and then:
caption._.parse_string

i have the entire parsed sentences from benepar.
let's say i have this sentence: the time for action is now.
my expected content in list(parsed_caption.sents)[0] should be:
[the time for action is now.]

so when i want to get the parsed sentence with caption._.parse_string i get:
(s (np (np (dt the) (nn time)) (pp (in for) (np (nn action)))) (vp (vbz is) (advp (rb now))) (. .))

but the problem is that for some sentences they are divided like (for example):
[the time, for action is now.]

so if i try to get the sentence with list(parsed_caption.sents)[0] i will only get a chunk of the sentence parsed. i only expect 1 sentence, no more.
as a result of only getting the time, when i'm training the model and the sentence the time for action is now. is selected i get an error because the length of the time for action is now. and the time is not the same.","['python', 'nlp', 'spacy']",68904898,"i finally managed to find the solution to my needs.
since tool berkeley neural parser gives two options (nltk and spacy) i tried the nltk option and managed to achieve my goal. due to the limitations of the environment i have to use the 2019 version which supports python 3.6.
with nltk i only have to do:
import benepar

class beneparannotator(syntaxannotator):
    def __init__(self):
        self.parser = benepar.parser(""benepar_en2"")
        self.extract_arc_representation = false
....

and then:
for caption in tqdm(captions, desc=""parsing sentences""):
    parsed_caption = parser.parse(caption)
    tree_as_string = parsed_caption.pformat(margin=500, indent=0, nodesep='', parens='()', quotes=false)
    parsed_sentences.append(tree_as_string)

in tree_as_string you will get the parsed sentence:
(s (np (dt a) (nn restaurant)) (vp (vbz has) (np (jj modern) (jj wooden) (nns tables) (cc and) (nns chairs)))) 

you will have to adjust the margin parameter to set the length of the phrase, because if the phrase is longer than the margin, pformat will split it into several lines with the \n character in this way (not exact, just to show an example):
(s\n (np\n (dt a)\n (nn restaurant)) (vp\n (vbz has) (np\n (jj modern) (jj wooden) (nns tables) (cc and) (nns chairs)))) 

and in this case, all sentences are processed correctly and i can train models without differences in the length of the original label vector and the parsed vector, because now sentences that with spacy were split into several parts are no longer split into several parts.
i hope this helps someone using the same version of the tool.
best regards.",https://stackoverflow.com/questions/68889124,python,23-08-2021 08:00,1693.0,2.0,3.0,True,24-08-2021 09:18,23-08-2021 11:41
77292726,feeding my classifier one document at a time,"i want my modelbuilder class to feed a self._classifier = multinomialnb() with the content of some webpages i scraped. the documents are many and pretty big, so i can't load the whole set to memory. i'm reading them file by file. here's the relevant portion of code:
x = []  
y = []

# loop over all files in my docs folder and for each file:
x.append(self._vectorize_text(file.read()))
y.append(category['label'])
# end of the loop

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
self._classifier.fit(x_train, y_train)
# ...

text pre-processing and vectorization functions:
def _vectorize_text(self, text):
    preprocessed_content = preprocess_text(text)
    tfidf_vector = self._vectorizer.fit_transform([preprocessed_content])
    return tfidf_vector.toarray()

def preprocess_text(text):
    words = word_tokenize(text)
    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stopwords.words('english')]
    cleaned_text = ' '.join(words)
    return cleaned_text

i get an error at classifier.fit when i start training my model:

valueerror: setting an array element with a sequence. the requested array has an inhomogeneous shape after 2 dimensions. the detected shape was (121, 1) + inhomogeneous part.

how can i resolve this?","['python', 'machine-learning', 'scikit-learn', 'nlp', 'tfidfvectorizer']",77294107,"you're going to have two problems doing this:

vocabulary. suppose that in document #2, you have the word ""eggs."" in document #1, the word ""eggs"" does not appear. in order for the vector for #1 and the vector for #2 to have the same meaning, the vectorizer for step #1 needs to know to leave an empty column for the word ""eggs.""
the immediate symptom you see is that the vectors end up having different lengths, and numpy cannot represent jagged arrays. if you tried to solve this by padding all arrays to the same length, you would have the problem that counts representing the same word are assigned to different columns.
one approach to solving this is to run a countvectorizer over your dataset, keeping the vocabulary from each document, but throwing away the vector. then, you use tf-idf with a fixed vocabulary representing all words that appear in your dataset.
this answer describes how to do this.

inverse document frequency. tf-idf is term frequency, within one document, multiplied by inverse document frequency for that term, within all documents.
if you fit this one document at a time, you're essentially setting the idf term to 1.
in order to compute idf, it must have either all documents, or at least a count of how many documents have each term in the vocabulary.
this answer has a library which can deal with this problem; i haven't personally tried it.",https://stackoverflow.com/questions/77292726,python,14-10-2023 11:59,59.0,0.0,1.0,True,14-10-2023 18:53,14-10-2023 17:47
57607392,error while importing matcher from spacy.matcher,"i am trying to use spacy in order to get proper names from a text, but when i run the code i get
traceback (most recent call last):
  file ""c:/users/l/desktop/spacy.py"", line 2, in <module>
    import spacy
  file ""c:\users\l\desktop\spacy.py"", line 3, in <module>
    from spacy.matcher import matcher
modulenotfounderror: no module named 'spacy.matcher'; 'spacy' is not a package

i installed spacy and its trained model with pip using
pip install spacy
python -m spacy download en_core_web_sm 

here is the code i am working on:

import spacy
from spacy.matcher import matcher


# load pre-trained model
nlp = spacy.load('en_core_web_sm')

# initialize matcher with a vocab
matcher = matcher(nlp.vocab)


def extract_name(resume_text):
    nlp_text = nlp(resume_text)

    # first name and last name are always proper nouns
    pattern = [{'pos': 'propn'}, {'pos': 'propn'}]

    matcher.add('name', none, *pattern)

    matches = matcher(nlp_text)

    for match_id, start, end in matches:
        span = nlp_text[start:end]
        return span.text


text =""some text""

fullname = extract_name(text)

can anyone help with this error?","['python', 'python-3.x', 'spacy']",57608045,"the problem is that you called your file spacy.py, while you're also trying to use the package spacy. never name your script the same as an existing module.
solution: rename your file to something different (e.g. ""main.py"" or ""app.py"").",https://stackoverflow.com/questions/57607392,python,22-08-2019 10:34,1106.0,1.0,1.0,True,12-04-2024 12:42,12-04-2024 12:42
79354638,remove leading space from shell gpt code output?,"i'm using shell gpt to send queries to gpt from linux terminal. one problem - it prints a space before each line of code it gives, for example:
 #!/bin/bash

 # get the current date and time
 current_date_time=$(date '+%y-%m-%d %h:%m:%s')
       

this leading space is very annoying when copypasting the code.
what needs to be modified in the shell gpt source code to fix this?","['python', 'openai-api', 'chatgpt-api']",79355295,"loacte site-packages/rich/markdown.py
in the method __rich_console__ of the class codeblock :
code, self.lexer_name, theme=self.theme, word_wrap=true, padding=1

change padding=1 to padding=0",https://stackoverflow.com/questions/79354638,python,14-01-2025 10:14,59.0,0.0,1.0,True,14-01-2025 14:51,14-01-2025 14:51
72118367,tensorflow: invalidargumenterror: graph execution error:,"i have the following preprocessing for a tensorflow neural-network:
import csv
from tensorflow.keras.preprocessing.text import tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from tensorflow.keras.layers import input,dense,lstm,flatten,globalaveragepooling1d,embedding,dropout

!wget --no-check-certificate \
     \
    -o /tmp/bbc-text.csv



# stopwords list from 
# convert it to a python list and paste it here
stopwords = [""a"", ""about"", ""above"", ""after"", ""again"", ""against"", ""all"", ""am"", ""an"", ""and"", ""any"", ""are"", ""as"", ""at"",
             ""be"", ""because"", ""been"", ""before"", ""being"", ""below"", ""between"", ""both"", ""but"", ""by"", ""could"", ""did"", ""do"",
             ""does"", ""doing"", ""down"", ""during"", ""each"", ""few"", ""for"", ""from"", ""further"", ""had"", ""has"", ""have"", ""having"",
             ""he"", ""he'd"", ""he'll"", ""he's"", ""her"", ""here"", ""here's"", ""hers"", ""herself"", ""him"", ""himself"", ""his"", ""how"",
             ""how's"", ""i"", ""i'd"", ""i'll"", ""i'm"", ""i've"", ""if"", ""in"", ""into"", ""is"", ""it"", ""it's"", ""its"", ""itself"",
             ""let's"", ""me"", ""more"", ""most"", ""my"", ""myself"", ""nor"", ""of"", ""on"", ""once"", ""only"", ""or"", ""other"", ""ought"",
             ""our"", ""ours"", ""ourselves"", ""out"", ""over"", ""own"", ""same"", ""she"", ""she'd"", ""she'll"", ""she's"", ""should"",
             ""so"", ""some"", ""such"", ""than"", ""that"", ""that's"", ""the"", ""their"", ""theirs"", ""them"", ""themselves"", ""then"",
             ""there"", ""there's"", ""these"", ""they"", ""they'd"", ""they'll"", ""they're"", ""they've"", ""this"", ""those"", ""through"",
             ""to"", ""too"", ""under"", ""until"", ""up"", ""very"", ""was"", ""we"", ""we'd"", ""we'll"", ""we're"", ""we've"", ""were"",
             ""what"", ""what's"", ""when"", ""when's"", ""where"", ""where's"", ""which"", ""while"", ""who"", ""who's"", ""whom"", ""why"",
             ""why's"", ""with"", ""would"", ""you"", ""you'd"", ""you'll"", ""you're"", ""you've"", ""your"", ""yours"", ""yourself"",
             ""yourselves""]

#----------------------------------- ream from csv and remove the stopwords
sentences = []
labels = []
with open(""/tmp/bbc-text.csv"", 'r') as csvfile:
    reader = csv.reader(csvfile, delimiter=',')
    next(reader)
    for row in reader:
        labels.append(row[0])
        sentence = row[1]
        for word in stopwords:
            token = "" "" + word + "" ""
            sentence = sentence.replace(token, "" "")
            sentence = sentence.replace("" "", "" "")
        sentences.append(sentence)


#----------------------------------  tokenize sentences
tokenizer = tokenizer(oov_token=""<oov>"")
tokenizer.fit_on_texts(sentences)
sequences = tokenizer.texts_to_sequences(sentences)
padded = pad_sequences(sequences, padding = 'post')

#--------------------------------- tokenize labels
label_tokenizer = tokenizer()
label_tokenizer.fit_on_texts(labels)
# label_word_index = label_tokenizer.word_index
label_seq = label_tokenizer.texts_to_sequences(labels)`

and finally here is the neural network which works by the prepared data:
train_sentence = tf.convert_to_tensor(padded,tf.int32)
train_label = tf.convert_to_tensor(label_seq,tf.int32)

input = input(shape=(2441,))
x = embedding(input_dim=10000,output_dim=128)(input)
x = lstm(64,return_sequences=true)(x)
x = lstm(64,return_sequences=true)(x)
x = lstm(64,return_sequences=true)(x)
x = dropout(0.2)(x)
x = lstm(64)(x)
x = flatten()(x)
output = dense(5, activation='softmax')(x)
model = tf.keras.models.model(input,output)

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(x=train_sentence,y=train_label,epochs=10)

but, it fails with the following error:
invalidargumenterror: graph execution error:","['python', 'tensorflow', 'keras', 'nlp']",72119180,"the input_dim of the embedding layer has to correspond to the size of your data's vocabulary + 1. also, your labels should begin from zero and not from one when using the sparse_categorical_crossentropy loss function. here is a working example based on your code and data:
# ...
# ...
train_sentence = tf.convert_to_tensor(padded,tf.int32)
train_label = tf.convert_to_tensor(label_seq,tf.int32)
train_label = train_label - 1

input = input(shape=(2441,))
x = embedding(input_dim=len(tokenizer.word_index) + 1,output_dim=128)(input)
x = lstm(64,return_sequences=true)(x)
x = lstm(64,return_sequences=true)(x)
x = lstm(64,return_sequences=true)(x)
x = dropout(0.2)(x)
x = lstm(64)(x)
x = flatten()(x)
output = dense(5, activation='softmax')(x)
model = tf.keras.models.model(input,output)

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(x=train_sentence,y=train_label,epochs=10)",https://stackoverflow.com/questions/72118367,python,04-05-2022 19:17,5652.0,2.0,1.0,True,20-07-2022 05:37,04-05-2022 20:34
77946231,langchain loader with power point not working,"the below def load_documents function is able to load various documents such as .docx, .txt, and .pdf into langchain.  i would also like to be able to load power point documents and found a script here:  that i added to below function.
however, the function is unable to read .pptx files because i am not able to pip install unstructuredpowerpointloader.  can somebody please suggest a way to do this or to augment below function so i can load .pptx files?
python function follows below:
def load_document(file):
    import os
    name, extension = os.path.splitext(file)

    if extension == '.pdf':
        from langchain.document_loaders import pypdfloader
        print(f'loading {file}')
        loader = pypdfloader(file)
    elif extension == '.docx':
        from langchain.document_loaders import docx2txtloader
        print(f'loading {file}')
        loader = docx2txtloader(file)
    elif extension == '.txt':
        from langchain.document_loaders import textloader
        print(f'loading {file}')
        loader = textloader(file)
    elif extension == '.pptx':
        from langchain_community.document_loaders import unstructuredpowerpointloader
        print(f'loading {file}')
        loader = unstructuredpowerpointloader(file)
    else:
        print('document format is not supported!')
        return none

    data = loader.load()
    return data

the error i am getting is because !pip install unstructured is failing. i tried also tried !pip install -q unstructured[""all-docs""]==0.12.0 but was unsuccessful again. appreciate any help!","['python', 'powerpoint', 'loader', 'langchain']",78085922,"try with this:  unstructured[docx,pptx]",https://stackoverflow.com/questions/77946231,python,06-02-2024 08:29,1288.0,0.0,1.0,True,01-03-2024 06:36,06-02-2024 12:03
78521181,llmgraphtransformer.convert_to_graph_documents(documents) attributeerror: &#39;str&#39; object has no attribute &#39;content&#39;,"from langchain_core.documents import document

text = """"""
marie curie, born in 1867, was a polish and naturalised-french physicist and chemist who conducted pioneering research on radioactivity.
she was the first woman to win a nobel prize, the first person to win a nobel prize twice, and the only person to win a nobel prize in two scientific fields.
her husband, pierre curie, was a co-winner of her first nobel prize, making them the first-ever married couple to win the nobel prize and launching the curie family legacy of five nobel prizes.
she was, in 1906, the first woman to become a professor at the university of paris.
""""""
documents = [document(page_content=text)]
graph_documents = llm_transformer.convert_to_graph_documents(documents)
print(f""nodes:{graph_documents[0].nodes}"")
print(f""relationships:{graph_documents[0].relationships}"")---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-38-0a6a66b2d25f> in <cell line: 10>()
      8 """"""
      9 documents = [document(page_content=text)]
---> 10 graph_documents = llm_transformer.convert_to_graph_documents(documents)
     11 print(f""nodes:{graph_documents[0].nodes}"")
     12 print(f""relationships:{graph_documents[0].relationships}"")

2 frames
/usr/local/lib/python3.10/dist-packages/langchain_experimental/graph_transformers/llm.py in process_response(self, document)
    593             nodes_set = set()
    594             relationships = []
--> 595             parsed_json = self.json_repair.loads(raw_schema.content)
    596             for rel in parsed_json:
    597                 # nodes need to be deduplicated using a set

attributeerror: 'str' object has no attribute 'content'

i am using the colab notebooks environment, llm uses the huggingfaceh4/starchat2-15b-v0.1 api
i found a similar issue on github--","['python', 'python-3.x', 'langchain']",78552213,"this is probably because you are using a model other than gpt4.
for this to work with some other models, you need to pass your own prompt to the llmgraphtransformer (reference)
something like this:
from langchain_experimental.graph_transformers.llm import unstructuredrelation, examples

system_prompt = """"""
you are a data scientist working for a company that is building a knowledge graph database. 
your task is to extract information from data and convert it into a knowledge graph database.
provide a set of nodes in the form [head, head_type, relation, tail, tail_type].
it is important that the head and tail exists as nodes that are related by the relation. if you can't pair a relationship with a pair of nodes don't add it.
when you find a node or relationship you want to add try to create a generic type for it that describes the entity you can also think of it as a label.
you must generate the output in a json format containing a list with json objects. each object should have the keys: ""head"", ""head_type"", ""relation"", ""tail"", and ""tail_type"".
""""""

system_message = systemmessage(content=system_prompt)
parser = jsonoutputparser(pydantic_object=unstructuredrelation)

human_prompt = prompttemplate(
    template=""""""
examples:
{examples}

for the following text, extract entities and relations as in the provided example.
{format_instructions}\ntext: {input}"""""",
    input_variables=[""input""],
    partial_variables={
        ""format_instructions"": parser.get_format_instructions(),
        ""node_labels"": none,
        ""rel_types"": none,
        ""examples"": examples,
    },
)

human_message_prompt = humanmessageprompttemplate(prompt=human_prompt)

chat_prompt = chatprompttemplate.from_messages(
    [system_message, human_message_prompt]
)

llm_transformer = llmgraphtransformer(llm=llm, prompt=chat_prompt)",https://stackoverflow.com/questions/78521181,python,23-05-2024 05:55,884.0,-1.0,1.0,True,29-05-2024 22:43,24-05-2024 15:38
77037891,typeerror: issubclass() arg 1 must be a class,"i am trying to use the spacy library again for my npl task. somedays back it was working totally fine with spacy.load(""en_core_web_sm""). i thought of using medium instead of small, but now nothing is working. i play around with spacy versions from the latest (3.6.1) to older (3.2.0). i am able to install spacy 3.2.0 and can download (!python -m spacy download en_core_web_md) medium pipeline too, however getting the ""typeerror"" error while loading. ( i tried for both small & medium, and got the same error). i will appreciate your help. thanks in advance!!
!pip install spacy==3.2.0
!python -m spacy download en_core_web_md
import spacy
from pydantic import basemodel

import spacy

nlp = spacy.load(""en_core_web_md"")
error:-
typeerror                                 traceback (most recent call last)
<ipython-input-2-8bdf9ac04f54> in <module>
      1 # initialize spacy model
----> 2 nlp = spacy.load(""en_core_web_md"")

~\anaconda3\lib\site-packages\spacy\__init__.py in load(name, vocab, disable, exclude, config)
     49     returns (language): the loaded nlp object.
     50     """"""
---> 51     return util.load_model(
     52         name, vocab=vocab, disable=disable, exclude=exclude, config=config
     53     )

~\anaconda3\lib\site-packages\spacy\util.py in load_model(name, vocab, disable, exclude, config)
    418             return get_lang_class(name.replace(""blank:"", """"))()
    419         if is_package(name):  # installed as package
--> 420             return load_model_from_package(name, **kwargs)  # type: ignore[arg-type]
    421         if path(name).exists():  # path to model data directory
    422             return load_model_from_path(path(name), **kwargs)  # type: ignore[arg-type]

~\anaconda3\lib\site-packages\spacy\util.py in load_model_from_package(name, vocab, disable, exclude, config)
    451     """"""
    452     cls = importlib.import_module(name)
--> 453     return cls.load(vocab=vocab, disable=disable, exclude=exclude, config=config)  # type: ignore[attr-defined]
    454 
    455 

~\anaconda3\lib\site-packages\en_core_web_md\__init__.py in load(**overrides)
      8 
      9 def load(**overrides):
---> 10     return load_model_from_init_py(__file__, **overrides)

~\anaconda3\lib\site-packages\spacy\util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config)
    613     if not model_path.exists():
    614         raise ioerror(errors.e052.format(path=data_path))
--> 615     return load_model_from_path(
    616         data_path,
    617         vocab=vocab,

~\anaconda3\lib\site-packages\spacy\util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config)
    486     overrides = dict_to_dot(config)
    487     config = load_config(config_path, overrides=overrides)
--> 488     nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude)
    489     return nlp.from_disk(model_path, exclude=exclude, overrides=overrides)
    490 

~\anaconda3\lib\site-packages\spacy\util.py in load_model_from_config(config, vocab, disable, exclude, auto_fill, validate)
    523     # registry, including custom subclasses provided via entry points
    524     lang_cls = get_lang_class(nlp_config[""lang""])
--> 525     nlp = lang_cls.from_config(
    526         config,
    527         vocab=vocab,

~\anaconda3\lib\site-packages\spacy\language.py in from_config(cls, config, vocab, disable, exclude, meta, auto_fill, validate)
   1783                     # the pipe name (key in the config) here is the unique name
   1784                     # of the component, not necessarily the factory
-> 1785                     nlp.add_pipe(
   1786                         factory,
   1787                         name=pipe_name,

~\anaconda3\lib\site-packages\spacy\language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)
    786                     lang_code=self.lang,
    787                 )
--> 788             pipe_component = self.create_pipe(
    789                 factory_name,
    790                 name=name,

~\anaconda3\lib\site-packages\spacy\language.py in create_pipe(self, factory_name, name, config, raw_config, validate)
    669         # we're calling the internal _fill here to avoid constructing the
    670         # registered functions twice
--> 671         resolved = registry.resolve(cfg, validate=validate)
    672         filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]
    673         filled = config(filled)

~\anaconda3\lib\site-packages\thinc\config.py in resolve(cls, config, schema, overrides, validate)
    744         validate: bool = true,
    745     ) -> dict[str, any]:
--> 746         resolved, _ = cls._make(
    747             config, schema=schema, overrides=overrides, validate=validate, resolve=true
    748         )

~\anaconda3\lib\site-packages\thinc\config.py in _make(cls, config, schema, overrides, resolve, validate)
    793         if not is_interpolated:
    794             config = config(orig_config).interpolate()
--> 795         filled, _, resolved = cls._fill(
    796             config, schema, validate=validate, overrides=overrides, resolve=resolve
    797         )

~\anaconda3\lib\site-packages\thinc\config.py in _fill(cls, config, schema, validate, resolve, parent, overrides)
    848                     schema.__fields__[key] = copy_model_field(field, any)
    849                 promise_schema = cls.make_promise_schema(value, resolve=resolve)
--> 850                 filled[key], validation[v_key], final[key] = cls._fill(
    851                     value,
    852                     promise_schema,

~\anaconda3\lib\site-packages\thinc\config.py in _fill(cls, config, schema, validate, resolve, parent, overrides)
    847                     field = schema.__fields__[key]
    848                     schema.__fields__[key] = copy_model_field(field, any)
--> 849                 promise_schema = cls.make_promise_schema(value, resolve=resolve)
    850                 filled[key], validation[v_key], final[key] = cls._fill(
    851                     value,

~\anaconda3\lib\site-packages\thinc\config.py in make_promise_schema(cls, obj, resolve)
   1055                 sig_args[name] = (annotation, default)
   1056         sig_args[""__config__""] = _promiseschemaconfig
-> 1057         return create_model(""argmodel"", **sig_args)
   1058 
   1059 

~\anaconda3\lib\site-packages\pydantic\main.cp38-win_amd64.pyd in pydantic.main.create_model()

~\anaconda3\lib\site-packages\pydantic\main.cp38-win_amd64.pyd in pydantic.main.modelmetaclass.__new__()

~\anaconda3\lib\site-packages\pydantic\fields.cp38-win_amd64.pyd in pydantic.fields.modelfield.infer()

~\anaconda3\lib\site-packages\pydantic\fields.cp38-win_amd64.pyd in pydantic.fields.modelfield.__init__()

~\anaconda3\lib\site-packages\pydantic\fields.cp38-win_amd64.pyd in pydantic.fields.modelfield.prepare()

~\anaconda3\lib\site-packages\pydantic\fields.cp38-win_amd64.pyd in pydantic.fields.modelfield._type_analysis()

~\anaconda3\lib\typing.py in __subclasscheck__(self, cls)
    772         if self._special:
    773             if not isinstance(cls, _genericalias):
--> 774                 return issubclass(cls, self.__origin__)
    775             if cls._special:
    776                 return issubclass(cls.__origin__, self.__origin__)

**typeerror: issubclass() arg 1 must be a class**","['python', 'python-3.x', 'pip', 'spacy', 'spacy-3']",77038347,"you can add below to your requirements:
typing-inspect==0.8.0
typing_extensions==4.5.0

there appears to be a bug in pydantic v1.10.7 and earlier related to the recent release of typing_extensions v4.6.0 that causes errors for import spacy and any other spacy commands for python 3.8 and python 3.9

for spacy v3.2 and v3.3, we have published patch releases with fixes for the typing_extension requirement. upgrade to spacy v3.2.6+ or v3.3.3
python -m pip install 'spacy~=3.2.6'


pydantic should be v1.10.8 or higher
here is the complete solution in this link:",https://stackoverflow.com/questions/77037891,python,04-09-2023 12:54,11632.0,8.0,1.0,True,22-05-2024 06:06,22-05-2024 06:06
76994077,openai api: how to use chatgpt code interpreter through api?,"how to use chatgpt code interpreter through api?
when this works ""
this returns an error ""there are no such model"":
response = openai.completion.create(
        engine=""code-interpreter"", # gpt-4-code-interpreter 
        prompt=final_prompt,
        temperature=0,
        max_tokens=150,
        top_p=1.0,
        frequency_penalty=0.0,
        presence_penalty=0.0,
        stop=[""#"", "";""]
    )",['openai-api'],76994163,"update: april 2024
the openai assistants api v2 has been released. see the migration guide.

update: november 2023
the code interpreter tool is now available through the openai assistants api v1, as stated in the official openai documentation:

code interpreter allows the assistants api to write and run python
code in a sandboxed execution environment. this tool can process files
with diverse data and formatting, and generate files with data and
images of graphs. code interpreter allows your assistant to run code
iteratively to solve challenging code and math problems. when your
assistant writes code that fails to run, it can iterate on this code
by attempting to run different code until the code execution succeeds.

i created a personal math tutor and made a youtube tutorial about it. it's a tutorial on how to use the assistants api with the code interpreter tool.

as the error says, the code interpreter api doesn't exist as of today.
see the list of all openai models in the official openai documentation.
also, there's a discussion on the official openai forum.",https://stackoverflow.com/questions/76994077,openai-api,28-08-2023 15:15,3907.0,4.0,1.0,True,26-04-2024 10:57,29-08-2023 16:37
70827241,how to replace dataframe text column with only the 1st occuring word / words before a comma,"the dataframe for the problem statement looks like




name
uid
search_text




b
14
kj


s
2
hsa,isd


d
10
sa,ad,ad


e
99
pid, pd,dd,ef


g
8
dd




i want the dataframe search_text to be stripped and replaced on the 1st word before comma.(i dont want to manually map it and replace). so it would look like.




name
uid
search_text




b
14
kj


s
2
hsa


d
10
sa


e
99
pid


g
8
dd




is there any convenient way to do that?","['python', 'pandas', 'nlp']",70827411,"extract the first alphanumerics in the string
df['search_text'] = df['search_text'].str.extract('(^\w+)')



   name  uid search_text
0    b   14          kj
1    s    2         hsa
2    d   10          sa
3    e   99         pid
4    g    8          dd",https://stackoverflow.com/questions/70827241,python,23-01-2022 22:43,128.0,2.0,2.0,True,23-01-2022 23:15,23-01-2022 22:51
59464601,date and time parsing with(ex: 21/12/201115:57),"how to parse date time format:

case 1: 21/12/201115:57

known:
%d%d%d%d%d%d%d%d%d%d%d%d

if i had a separator i would use .split operator. but since i do not
have a separator how do i deal with this?


desired output:
21/12/2011 15:57






case 2: 2112201115:57

known: ddmmyyyyhh:mm(i am sure of the format i would be receiving)

if i had a separator i would use .split operator.
but since i do not have a separator how do i deal with this?

desired output:
21/12/2011 15:57","['python', 'regex', 'nlp']",59464722,"this regex should work to parse your values:
(\d{2})(/?)(\d{2})\2(\d{4})(\d{2}:\d{2})

it looks for two digits (day), followed by an optional /, two more digits (month), then a / if there was one earlier (using a back-reference to the second capture group), then 4 digits (year) and finally 2 digits, a : and 2 digits (the time). we can substitute with
\1/\3/\4 \5

to get the desired output result:
21/12/2011 15:57
21/12/2011 15:57

demo on regex101
in python:
import re

times = ['21/12/201115:57', '2112201115:57']
for time in times:
    print(re.sub(r'(\d{2})(/?)(\d{2})\2(\d{4})(\d{2}:\d{2})', r'\1/\3/\4 \5', time))

output:
21/12/2011 15:57
21/12/2011 15:57",https://stackoverflow.com/questions/59464601,python,24-12-2019 06:16,46.0,-4.0,2.0,True,08-11-2021 19:22,08-11-2021 19:22
79026693,numpy error : implicit conversion to a numpy array is not allowed. please use `.get()` to construct a numpy array explicitly,"i 'am trying to find similar vector with spacy and numpy. i found the code following url :
mapping word vector to the most similar/closest word using spacy
but i'm getting type error
import numpy as np

your_word = ""country""

ms = nlp.vocab.vectors.most_similar(
    np.asarray([nlp.vocab.vectors[nlp.vocab.strings[your_word]]]), 
    n=10, )

words = [nlp.vocab.strings[w] for w in ms[0][0]] distances = ms[2] print(words)

error :
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
cell in[139], line 6
      1 import numpy as np
      3 your_word = ""country""
      5 ms = nlp.vocab.vectors.most_similar(
----> 6     np.asarray([nlp.vocab.vectors[nlp.vocab.strings[your_word]]]), 
      7     n=10,
      8 )
     10 words = [nlp.vocab.strings[w] for w in ms[0][0]]
     11 distances = ms[2]

file cupy/_core/core.pyx:1475, in cupy._core.core._ndarray_base.__array__()

typeerror: implicit conversion to a numpy array is not allowed. please use `.get()` to construct a numpy array explicitly.

i'm using gpu, how can i fix this?","['python', 'numpy', 'gpu', 'spacy']",79026730,"from the
reference - mapping word vector to the most similar/closest word using spacy
reference - 
you need to convert cupy arrays  to be explicitly converted to numpy arrays before operations on the cpu
edit lets make reshape the word vector into a 2d array with one row and multiple columns
import numpy as np

your_word = ""country""


word_vector = nlp.vocab.vectors[nlp.vocab.strings[your_word]]


word_vector_2d = word_vector.reshape(1, -1)  # '-1' lets numpy infer the correct size for the second dimension


ms = nlp.vocab.vectors.most_similar(
    word_vector_2d, 
    n=10,
)


words = [nlp.vocab.strings[w] for w in ms[0][0]]
distances = ms[2]
print(words)",https://stackoverflow.com/questions/79026693,python,26-09-2024 10:03,287.0,2.0,1.0,True,26-09-2024 10:45,26-09-2024 10:30
69787306,training custom word2vec model,"i have my own dataset in which i want to use gensim word2vec to train but i'm not sure how to do it.
from google.colab import files
import io
uploaded = files.upload()
data_path = 'chatbot_dataset.txt'
with open(data_path, 'r') as f:
    lines = f.read().split('\n')

for line in lines:
    input_text = line.split('\t')[0]
    if len(input_text.split()) > max_sentence_length:
      break
    target_text = '<start> ' + line.split('\t')[1] + "" <end>""
    input_texts.append(input_text)
    target_texts.append(target_text)

model = word2vec(lines, min_count=1,workers=3,size=100,window=3,sg=1)
model.wv.get_vector('hello')

but i got this error while doing it, even though the word 'hello' is already in my dataset:
keyerror                                  traceback (most recent call last)
<ipython-input-15-b41c8cb17d3b> in <module>()
    140 model.wv.vector_size
    141 #check out how 'pem' is represented in an array of 100 numbers
--> 142 model.wv.get_vector('hello')
    143 #find words with similar meaning to 'pen'
    144 model.wv.most_similar('to')

1 frames
/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py in word_vec(self, word, use_norm)
    450             return result
    451         else:
--> 452             raise keyerror(""word '%s' not in vocabulary"" % word)
    453 
    454     def get_vector(self, word):

keyerror: ""word 'hello' not in vocabulary""","['python', 'machine-learning', 'nlp', 'word2vec']",69790920,"you're feeding lines, which appears to be a list of plain strings, to word2vec.
word2vec is instead expecting a re-iterable sequence of items, where each item is a pre-tokenized list-of-strings. by passing it a sequence of plain strings instead, when word2vec interprets one string as a list, it will see it as a list-of-single-characters ï¿½ï¿½ï¿½ so the entire set of 'words' it learns will just be single-characters. (there may have been a warning in your logs about that, or if you were running with at least info logging, progress-reporting that shows a suspiciously-tiny number of discovered unique words.)
you can look at what your model's volcabulary wound up being by examining model.wv.index_to_key - for example, peeking at the 10 most-common words found by print(model.wv.index_to_key[:10]. if that doesn't look right, make sure you're properly preprocessing/tokenizing the corpus you'll be handing to word2vec<.
separately: min_count=1 is never a good idea with word2vec. only words with multiple varied usage examples can achieve useful word-vectors, and usually discarding the rarest words, as with the default min_count=5, ensures the best-quality vectors for all surviving words. (if there are words with fewer than 5 usage examples for which you need vectors, the best approach is obtain more varied-usage training data.)",https://stackoverflow.com/questions/69787306,python,31-10-2021 13:22,717.0,0.0,1.0,True,31-10-2021 22:00,31-10-2021 21:01
19727261,how to count the number of spoken syllables in an audio file?,"i have many audio files with clean audio and only spoken voice in mandarin chinese. i need to estimate of how many syllables are spoken in each file. is there a tool for os x, windows, or linux that can estimate these?
sample01.wav 15
sample02.wav 8
sample03.wav 5
sample04.wav 1
sample05.wav 18

as there are many files, command-line or batch-capable software is preferred, e.g.:
$ application sample01.wav
15


a solution that uses speech-to-text, then counts the number of characters present would be suitable to.","['nlp', 'speech-recognition']",19814655,"the automatic segmentation of speech is an active scientific domain, meaning that there is no method that works perfectly.
in 2009, de jong and wempe proposed a method to automatically detect syllables in a human speech signal using praat. this methods compares well with man-made segmentation, and has been employed in many third-party scientific studies. you can find a detailed description of the method in their scientific article (pdf), along with an historical perspective on previously proposed methods. the praat script per se and a couple of tutorials can be found on a dedicated website (www - speechrate).
you may also be interested in another segmentation algorithm developed by harma that has been implemented in matlab (harma syllable segmentation)",https://stackoverflow.com/questions/19727261,nlp,01-11-2013 13:00,5945.0,5.0,4.0,True,03-05-2022 11:02,20-02-2015 03:02
70383843,extracting text in between certain text/tags in python,"so let's say i have a piece of text:
note: this is a file i'm reading in so it isn't actually commented out in the code
"" ## battery life: pretty medicore at 12 hours, the creative, iaudio, rio, and iriver players last longer. 
customer service[-3]## customer service: awful. listen to this one, remember the battery statement i made earlier. 
##if your battery dies and it's still under warranty, guess what, you're out of luck, because apple doesn't cover the battery in the ipod warranty. 
##they have the nerve to charge you $30 for a warrantied ipod.
technical service[-3]## ""

and i want to extract the elements just before the square brackets, for example, in this piece of text i would like: customer service[-3] and technical service[-3]
is there any cool way to got about doing this (using some application of regex) because i'm stumped at the moment.","['python', 'string', 'nlp']",70383905,"addressed in regex way.
demo: 
pattern: \w+ \w+\[-?\d+]",https://stackoverflow.com/questions/70383843,python,16-12-2021 18:19,32.0,0.0,1.0,True,16-12-2021 18:25,16-12-2021 18:25
65396968,how do i interpret my bert output from huggingface transformers for sequence classification and tensorflow?,"i am using bert for a sequence classification task with 3 labels. to do this, i am using huggingface transformers with tensorflow, more specifically the tfbertforsequenceclassification class with the bert-base-german-cased model (yes, using german sentences).
i am by no means an expert in nlp, which is why i pretty much followed this approch here:  (with some tweaks of course)
everything seems to be working fine, but the output i receive from my model is what throws me off.
here's just some of the output along the way for context.
the main difference i have to the example from the article is the number of labels. i have 3 while the article only featured 2.
i use a labelencoder from sklearn.preprocessing to process my labels
label_encoder = labelencoder()
y_integer_encoded = label_encoder.fit_transform(y)

*y here is a list of labels as strings, so something like this
['e_3', 'e_1', 'e_2',]

then turns into this:
array([0, 1, 2], dtype=int64)

i then use the berttokenizer to process my text and create the input datasets (training and testing).
these are the shapes of those:
 <tensorslicedataset shapes: ({input_ids: (99,), token_type_ids: (99,), attention_mask: (99,)}, ()), types: ({input_ids: tf.int32, token_type_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>

i then train the model as per huggingface docs.
the last epoch while training the model looks like this:
epoch 3/3
108/108 [==============================] - 24s 223ms/step - loss: 25.8196 - accuracy: 0.7963 - val_loss: 24.5137 - val_accuracy: 0.7243

then i run model.predict on an example sentence and get this output (yes i tokenized the sentence accordingly just like the other article does). the output looks like this:
array([ 3.1293588, -5.280143 ,  2.4700692], dtype=float32)

and lastly that's the softmax function i apply in the end and it's output:
tf_prediction = tf.nn.softmax(tf_output, axis=0).numpy()[0]

output: 0.6590041

so here's my question:
i don't quite understand that output. with an accuracy of ~70% (validation accuracy), my model should be okay in predicting the labels. yet only the logits from the direct output don't mean much to me tbh and the output after the softmax function seems to be on a linear scale, as if it came from a sigmoid function. how do i interpret this and translate it to the label i am trying to predict?
and also: shouldn't i feed one hot encoded labels into my bert model for it to work? i always thought bert needs that but it seems like it doesn't.","['python', 'tensorflow', 'bert-language-model', 'huggingface-transformers']",65397572,"your output means that probability of the first class is 65.9%.
you can feed your labels either as integers or as one-hot vectors. you have to use an appropriate loss function (categorical_crossentropy with one-hot or sparse_categorical_crossentropy with integers).",https://stackoverflow.com/questions/65396968,python,21-12-2020 17:04,6415.0,5.0,1.0,True,15-02-2024 14:33,15-02-2024 14:33
77662162,how to enable cuda for huggingface trainer on windows?,"i am trying to use the trainer from the transformers library from huggingface in python:
from transformers import seq2seqtrainingarguments
from transformers import seq2seqtrainer

#  ...

training_args = seq2seqtrainingarguments(fp16=true,

# ...

trainer = seq2seqtrainer( args=training_args,

# ...

i get this error message:

valueerror: fp16 mixed precision training with amp or apex (--fp16)
and fp16 half precision evaluation (--fp16_full_eval) can only be
used on cuda or npu devices or certain xpu devices (with ipex).

it seems like i am missing some cuda installation, but i can't figure out, what exactly i need. i tried (without success):
py -m pip install --upgrade setuptools pip wheel
py -m pip install nvidia-pyindex
py -m pip install nvidia-cuda-runtime-cu12

py -m pip install nvidia-nvml-dev-cu12
py -m pip install nvidia-cuda-nvcc-cu12

system info:

windows 11 build 22621
python 3.11.7, running inside a venv
geforce rtx 4070

thanks for any ideas!","['python', 'huggingface-transformers']",77678135,"found it. adding this line to the code
model.to('cuda')

gave a more meaningful error message:

torch not compiled with cuda enabled

created the correct pip install command here: 
(something like pip3 install torch torchvision torchaudio --index-url  )
and it worked.",https://stackoverflow.com/questions/77662162,python,14-12-2023 17:49,1404.0,0.0,1.0,True,18-12-2023 09:33,14-12-2023 21:23
76394423,"do i need any environment variables set to execute some code, call openai&#39;s api, and return a response?","i was going through a course in openai's api using an in-browser jupyter notebook page but wanted to copy some example code from there into a local ide. i installed python and the jupyter extention in vs code and the openai library. my code is below:
import openai
import os

# from dotenv import load_dotenv, find_dotenv
# _ = load_dotenv(find_dotenv()) # read local .env file

openai.api_key  = ""my api key is here""

def get_completion(prompt, model=""gpt-3.5-turbo""):
    messages = [{""role"": ""user"", ""content"": prompt}]
    response = openai.chatcompletion.create(
        model=model,
        messages=messages,
        temperature=0, # this is the degree of randomness of the model's output
    )
    return response.choices[0].message[""content""]

prompt = f""""""
determine whether each item in the following list of \
topics is a topic in the text below, which
is delimited with triple backticks.

give your answer as list with 0 or 1 for each topic.\

list of topics: {"", "".join(topic_list)}

text sample: '''{story}'''
""""""
response = get_completion(prompt)
print(response)

i installed python and imported the openai library. when i run i am getting the error:
apiconnectionerror: error communicating with openai: ('connection aborted.', remotedisconnected('remote end closed connection without response'))

i'm assuming that's because i commented out lines 3 and 4 in the code because i am unsure what they do and do not know how to use the dotenv library. is it simple to set this up just to make a basic call to the openai api? that's all i'm trying to do with this code right now.","['python', 'openai-api', 'dotenv']",76394444,"usually, you load your api key from your .env file but, as you are hardcoding it, you don't need anything else.
the error you are getting might be related to the absence of the topic_list and story definitions.",https://stackoverflow.com/questions/76394423,python,03-06-2023 03:17,954.0,1.0,1.0,True,07-08-2024 09:57,07-08-2024 09:57
79458703,how can i load a pretrained transformers model that was manually downloaded?,"i am unable to download huggingface models through the python functions due to ssl certificate errors.  perhaps it's due to my company firewall.
i am able to download the contents of a huggingface model repo through a browser to a folder.  i'm trying to load this model from disk using tfpretrainedmodel.from_pretrained() and autotokenizer.from_pretrained().  from the docs, it seems this is a valid option.
i'm receiving an error message that isn't useful attributeerror: 'nonetype' object has no attribute 'from_pretrained'.  appreciate any help!
example repo:

code
from transformers import pipeline, tfpretrainedmodel, autotokenizer
import os

dir = ""./models/twitter-roberta-base-sentiment-latest/""
print(os.listdir(dir)) # confirm the folder contents

model = tfpretrainedmodel.from_pretrained(dir)
tokenizer = autotokenizer.from_pretrained(dir)

analyze = pipeline(task=""sentiment-analyis"", model=model, tokenizer=tokenizer)
print(analyze(""this is good""))
print(analyze(""this is bad""))

output
2025-02-21 16:40:05.896448: i tensorflow/core/util/port.cc:113] onednn custom operations are on. you may see slightly different numerical results due to floating-point round-off errors from different computation orders. to turn them off, set the environment variable `tf_enable_onednn_opts=0`.
2025-02-21 16:40:06.653841: i tensorflow/core/util/port.cc:113] onednn custom operations are on. you may see slightly different numerical results due to floating-point round-off errors from different computation orders. to turn them off, set the environment variable `tf_enable_onednn_opts=0`.
warning:tensorflow:from c:\users\xxxxx\.pyenv\pyenv-win\versions\3.12.8\lib\site-packages\tf_keras\src\losses.py:2976: the name tf.losses.sparse_softmax_cross_entropy is deprecated. please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

['config.json', 'gitattributes', 'merges.txt', 'pytorch_model.bin', 'readme.md', 'special_tokens_map.json', 'tf_model.h5', 'vocab.json']
traceback (most recent call last):
  file ""c:\users\xxxxx\onedrive - dupont\python projects\huggingface\sentiment.py"", line 8, in <module>
    model = tfpretrainedmodel.from_pretrained(dir)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""c:\users\xxxxx\.pyenv\pyenv-win\versions\3.12.8\lib\site-packages\transformers\modeling_tf_utils.py"", line 2726, in from_pretrained
    config, model_kwargs = cls.config_class.from_pretrained(
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
attributeerror: 'nonetype' object has no attribute 'from_pretrained'

docs

pretrained_model_name_or_path (str, optional) ï¿½ï¿½ï¿½ can be either:
a string, the model id of a pretrained model hosted inside a model repo on huggingface.co.
*a path to a directory containing model weights saved using save_pretrained(), e.g., ./my_model_directory/.*
a path or url to a pytorch state_dict save file (e.g, ./pt_model/pytorch_model.bin). in this case, from_pt should be set to true and a configuration object should be provided as config argument. this loading path is slower than converting the pytorch model in a tensorflow model using the provided conversion scripts and loading the tensorflow model afterwards.
none if you are both providing the configuration and state dictionary (resp. with keyword arguments config and state_dict).
<","['python', 'huggingface-transformers']",79475157,"it looks like the provided tensorflow state-dict is missing some weights. when you load the pytorch variant, the results look pretty decent:
from transformers import automodelforsequenceclassification, autotokenizer, pipeline

local_path = ""/content/twitter-roberta-base-sentiment-latest""

tokenizer = autotokenizer.from_pretrained(local_path)
model = automodelforsequenceclassification.from_pretrained(local_path)

analyze = pipeline(task=""sentiment-analysis"", model=model, tokenizer=tokenizer)

print(analyze(""this is good""))
print(analyze(""this is bad""))

output:
[{'label': 'positive', 'score': 0.9303026795387268}]
[{'label': 'negative', 'score': 0.8278112411499023}]

but when you execute the same code with the tensorflow equivalent (i.e. tfautomodelforsequenceclassification instead of automodelforsequenceclassification), you receive a warning that the classifier (important component) is randomly initialized and the result you receive are not useful at all:
some layers of tfrobertaforsequenceclassification were not initialized from the model checkpoint at /content/twitter-roberta-base-sentiment-latest and are newly initialized: ['classifier']
you should probably train this model on a down-stream task to be able to use it for predictions and inference.

[{'label': 'negative', 'score': 0.3959597945213318}]
[{'label': 'neutral', 'score': 0.35170331597328186}]

there was an issue reported addressing that in 2023 (link), but no action so far from the model creators. you can fix the tensorflow state_dict if you have to use this framework. please open a separate question if you need assistance.
to answer your original question on how to load locally stored models from huggingface, you usually don't need to figure out which classes you need because the pipeline will already take care of that for you. you can simplify your code in the following way:
from transformers import pipeline

local_path = ""/content/twitter-roberta-base-sentiment-latest""

analyze = pipeline(task=""sentiment-analysis"", model=local_path)

you can also control with the framework parameter (values tf or pt), which framework you want to use. in general, something you experienced with cardiffnlp/twitter-roberta-base-sentiment-latest can happen again, therefore always verify the results and read the warning messages.",https://stackoverflow.com/questions/79458703,python,21-02-2025 21:54,176.0,1.0,2.0,True,28-02-2025 10:04,22-02-2025 02:36
76064928,is there such thing as dataset improvement?,"i know that we can use explained machine learning to find why a model chose a certain classification.
i wonder if there is a way i can find which features are going to improve my current model.
i will explain what i mean by this.
case: nlp classification of sports, there is a paragraph talking about ronaldo scores against uruguay...
is there a method that can ask which ronaldo you mean (ronaldo de lime the brazilian player or cristiano ronaldo the portuguese)?
so the model can get a higher accuracy result to classify the paragraph about brazilian team or about portugal team?","['nlp', 'artificial-intelligence']",76078374,"i suppose you could use some named entity recognition (ner) mechanism to detect ""ronaldo"" as a person's name. then you could ask users about him and finally narrow down final answers based on the user's input.",https://stackoverflow.com/questions/76064928,nlp,20-04-2023 13:57,42.0,0.0,1.0,True,01-10-2023 04:24,01-10-2023 04:24
70673763,how to use embedding layer along with textvectorization in functional api,"just starting on tensorflow
working on imdb dataset. process: text encoding using textvectorization layer and passing it to embedded layer:
# create a custom standardization function to strip html break tags '<br />'.
def custom_standardization(input_data):
  lowercase = tf.strings.lower(input_data)
  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')
  return tf.strings.regex_replace(stripped_html,
                              '[%s]' % re.escape(string.punctuation), '')


# vocabulary size and number of words in a sequence.
vocab_size = 10000
sequence_length = 100

# use the text vectorization layer to normalize, split, and map strings to
# integers. note that the layer uses the custom standardization defined above.
# set maximum_sequence length as all samples are not of the same length.
vectorize_layer = textvectorization(
standardize=custom_standardization,
max_tokens=vocab_size,
output_mode='int',
output_sequence_length=sequence_length)

# make a text-only dataset (no labels) and call adapt to build the vocabulary.
text_ds = train_ds.map(lambda x, y: x)
vectorize_layer.adapt(text_ds)

i then try to build a functional api:
embedding_dim=16
text_model_catprocess2 = vectorize_layer
text_model_embedd = tf.keras.layers.embedding(vocab_size, embedding_dim, name = 'embedding')(text_model_catprocess2)
text_model_embed_proc = tf.keras.layers.lambda(embedding_mean_standard)(text_model_embedd)
text_model_dense1 = tf.keras.layers.dense(2, activation = 'relu')(text_model_embed_proc)
text_model_dense2 = tf.keras.layers.dense(2, activation = 'relu')(text_model_dense1)
text_model_output = tf.keras.layers.dense(1, activation = 'sigmoid')(text_model_dense2)

however, this is giving the following error:
~\anaconda3\lib\site-packages\keras\backend.py in dtype(x)
1496 
1497   """"""
-> 1498   return x.dtype.base_dtype.name
1499 
1500 

attributeerror: exception encountered when calling layer ""embedding"" (type embedding).

'str' object has no attribute 'base_dtype'

call arguments received:
  ï¿½ï¿½ï¿½ inputs=<keras.layers.preprocessing.text_vectorization.textvectorization object at 0x0000029b483aadc0>

upon making a sequential api like this, it is working fine:
embedding_dim=16
modelcheck = tf.keras.sequential([
vectorize_layer,
tf.keras.layers.embedding(vocab_size, embedding_dim, name=""embedding""),
tf.keras.layers.globalaveragepooling1d(),
tf.keras.layers.dense(16, activation='relu'),
tf.keras.layers.dense(1)
])

i am not sure why this is happening. is it necessary for the functional api to have an input? please help","['python', 'tensorflow', 'keras', 'word-embedding', 'functional-api']",70678084,"you have two options. either you use a sequential model and it will work as you have confirmed because you do not have to define an input layer, or you use the functional api where you have to define an input layer:
embedding_dim = 16
text_model_input = tf.keras.layers.input(dtype=tf.string, shape=(1,))
text_model_catprocess2 = vectorize_layer(text_model_input)
text_model_embedd = tf.keras.layers.embedding(vocab_size, embedding_dim, name = 'embedding')(text_model_catprocess2)
text_model_embed_proc = tf.keras.layers.lambda(embedding_mean_standard)(text_model_embedd)
text_model_dense1 = tf.keras.layers.dense(2, activation = 'relu')(text_model_embed_proc)
text_model_dense2 = tf.keras.layers.dense(2, activation = 'relu')(text_model_dense1)
text_model_output = tf.keras.layers.dense(1, activation = 'sigmoid')(text_model_dense2)
model = tf.keras.model(text_model_input, text_model_output)",https://stackoverflow.com/questions/70673763,python,11-01-2022 21:38,2998.0,2.0,1.0,True,16-06-2022 20:25,16-06-2022 20:25
78060804,how to get the shap value per class?,"i want to get shap value per class. i have checked tutorial and i found below example how to do this. however, the code do not work because of shap_value.shape is (10,none,6). 10 is your the number of samples, 4 is class.
import datasets
import pandas as pd
import transformers
import shap

dataset = datasets.load_dataset(""emotion"", split=""train"")
data = pd.dataframe({""text"": dataset[""text""], ""emotion"": dataset[""label""]})

# load the model and tokenizer
tokenizer = transformers.autotokenizer.from_pretrained(
    ""nateraw/bert-base-uncased-emotion"", use_fast=true
)
model = transformers.automodelforsequenceclassification.from_pretrained(
    ""nateraw/bert-base-uncased-emotion""
).cuda()

# build a pipeline object to do predictions
pred = transformers.pipeline(
    ""text-classification"",
    model=model,
    tokenizer=tokenizer,
    device=0,
    return_all_scores=true,
)
explainer = shap.explainer(pred)
shap_values = explainer(data[""text""][:3])
shap.plots.bar(shap_values[:, :, ""joy""].mean(0))

are there any way to get bar plot for per class?","['machine-learning', 'deep-learning', 'nlp', 'shap', 'xai']",78068645,"after installing shap 41.0, you have to do : !pip3 install mxnet-mkl==1.6.0 numpy==1.23.1 . after that if you encounter : dtype: np.bool you can change: np.bool_ in source code. of course you can see some warning however graph will be produced !
edit: if you want to use current version only do this : !pip3 install mxnet-mkl==1.6.0 numpy==1.23.1",https://stackoverflow.com/questions/78060804,machine-learning,26-02-2024 12:08,288.0,0.0,1.0,True,27-02-2024 16:22,26-02-2024 13:52
74554805,notfounderror using bert preprocessing from tfhub,"i'm trying to use the pre-trained bert models on tensorflow hub to do some simple nlp. i'm on a 2021 macbook pro (apple silicon) with python 3.9.13 and tensorflow v2.9.2. however, preprocessing any amount of text returns a ""notfounderror"" that i can't seem to resolve. the link to the preprocessing model is here: ( and i have pasted my code/error messages below. does anyone know why this is happening and how i can fix it? thanks in advance.
code
bert_preprocess = hub.keraslayer(""
bert_encoder = hub.keraslayer(""
print(bert_preprocess([""test""]))

output
output exceeds the size limit. open the full output data in a text editor
---------------------------------------------------------------------------
notfounderror                             traceback (most recent call last)
cell in [42], line 3
      1 bert_preprocess = hub.keraslayer(""
      2 bert_encoder = hub.keraslayer(""
----> 3 print(bert_preprocess([""test""]))

file ~/miniforge3/envs/tfenv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     65 except exception as e:  # pylint: disable=broad-except
     66   filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67   raise e.with_traceback(filtered_tb) from none
     68 finally:
     69   del filtered_tb

file ~/miniforge3/envs/tfenv/lib/python3.9/site-packages/tensorflow_hub/keras_layer.py:237, in keraslayer.call(self, inputs, training)
    234   else:
    235     # behave like batchnormalization. (dropout is different, b/181839368.)
    236     training = false
--> 237   result = smart_cond.smart_cond(training,
    238                                  lambda: f(training=true),
    239                                  lambda: f(training=false))
    241 # unwrap dicts returned by signatures.
    242 if self._output_key:

file ~/miniforge3/envs/tfenv/lib/python3.9/site-packages/tensorflow_hub/keras_layer.py:239, in keraslayer.call.<locals>.<lambda>()
...
     [[statefulpartitionedcall/statefulpartitionedcall/bert_pack_inputs/partitionedcall/raggedconcat/arithmeticoptimizer/addopsrewrite_leaf_0_add_2]] [op:__inference_restored_function_body_209194]

call arguments received by layer ""keras_layer_6"" (type keraslayer):
  ï¿½ï¿½ï¿½ inputs=[""'test'""]
  ï¿½ï¿½ï¿½ training=none
</","['python', 'tensorflow', 'bert-language-model', 'data-preprocessing', 'tensorflow-hub']",74591835,"update: while using bert preprocessing from tfhub, tensorflow and tensorflow_text versions should be same so please make sure that installed both versions are same. it happens because you're using latest version for tensorflow_text but you're using other versions for python and tensorflow but there is internal dependancy with versions for tensorflow and tensorflow_text which should be same.
!pip install -u tensorflow
!pip install -u tensorflow-text
import tensorflow as tf
import tensorflow_text as text

# or install with a specific version
!pip install -u tensorflow==2.11.*
!pip install -u tensorflow-text==2.11.*
import tensorflow as tf
import tensorflow_text as text

i have executed below lines of code in google colab and it's working fine,
bert_preprocess = hub.keraslayer(""
bert_encoder = hub.keraslayer(""
print(bert_preprocess([""test""])) 

here is output:
{'input_type_ids': <tf.tensor: shape=(1, 128), dtype=int32, numpy=
array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
      dtype=int32)>, 'input_mask': <tf.tensor: shape=(1, 128), dtype=int32, numpy=
array([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
      dtype=int32)>, 'input_word_ids': <tf.tensor: shape=(1, 128), dtype=int32, numpy=
array([[ 101, 3231,  102,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0]], dtype=int32)>}

i hope it will help you to resolve your issue, thank you!",https://stackoverflow.com/questions/74554805,python,24-11-2022 01:51,407.0,0.0,1.0,True,27-11-2022 16:37,24-11-2022 02:07
77883320,text has \n and response has both \n and &quot;real new lines&quot;. print() output both as line breaks,"i have some python function like this:
def translate(text):
    while true:
        response = openai.chatcompletion.create(
            model=""gpt-4"",
            messages=[
                {""role"": ""system"", ""content"": ""you are a translator, you will receive english language sentences from a videogame needing localization. you must respect the punctuation and capitalization. you must maintain \n as is""},
                {""role"": ""system"", ""content"": ""the target language is spanish""},
                {""role"": ""user"", ""content"": text}
            ],
            temperature=0
        )
        return response.choices[0].message['content']

and for the text input parameter i have a string like this (but really longer):
text_to_translate = """"""
blocks melee and\n ranged units' line of sight.
sink the unit. sunken\n units are more vulnerable to enemy attacks.
teleport the unit through\n this portal to an exit portal.
""""""

for the response i'm doing:
translated_text = translate(text_to_translate)
print(translated_text, flush=true)

the problem is, print() is taking \n and (correctly) inserting line breaks on the output like this:
bloquea la lï¿½ï¿½nea de visiï¿½ï¿½n de
unidades de combate cuerpo a cuerpo y a distancia.
hunde la unidad. las unidades
hundidas son mï¿½ï¿½s vulnerables a los ataques enemigos.
teletransporta la unidad a travï¿½ï¿½s
de este portal a un portal de salida.

how can i make it so \n stay the same but the actual line breaks from the triple-quoted strings are","['python', 'string', 'artificial-intelligence', 'openai-api']",77883499,"a string literal (regardless of the quotes used) is not the same thing as a str value. the literal contains a \n digraph; the str value it creates contains u+000a.
>>> x = """"""foo\nbar
...baz""""""
>>> len(x)
11  # not 12
>>> x
'foo\nbar\nbaz'  # string representation of a string uses a digraph
>>> print(x)  # u+000a is rendered as a line break and a carriage return
foo
bar
baz",https://stackoverflow.com/questions/77883320,python,25-01-2024 22:36,283.0,0.0,1.0,True,26-01-2024 11:57,26-01-2024 11:57
23429117,saving nltk drawn parse tree to image file,"is there any way to save the draw image from tree.draw() to an image file programmatically? i tried looking through the documentation, but i couldn't find anything.","['python', 'tree', 'nlp', 'nltk', 'text-parsing']",24748479,"i had exactly the same need, and looking into the source code of nltk.draw.tree i found a solution:
from nltk import tree
from nltk.draw.util import canvasframe
from nltk.draw import treewidget

cf = canvasframe()
t = tree.fromstring('(s (np this tree) (vp (v is) (adjp pretty)))')
tc = treewidget(cf.canvas(),t)
cf.add_widget(tc,10,10) # (10,10) offsets
cf.print_to_file('tree.ps')
cf.destroy()

the output file is a postscript, and you can convert it to an image file using imagemagick on terminal:
$ convert tree.ps tree.png

i think this is a quick and dirty solution; it could be inefficient in that it displays the canvas and destroys it later (perhaps there is an option to disable display, which i couldn't find). please let me know if there is any better way.",https://stackoverflow.com/questions/23429117,python,02-05-2014 13:16,14782.0,19.0,4.0,True,24-01-2023 20:40,31-01-2016 20:22
77996614,cannot use gpt api on google collab,"i'm working on a project in google colab where i need to automatically generate text using pre-created prompts with the gpt-4 model from openai. i wrote the following lines:
!pip install openai
!pip install cohere tiktoken
import openai
import os
os.environ[""openai_api_key""] = ""my secret api""

response = openai.completion.create(
  model=""gpt-4"", 
  prompt=""hi"",
  temperature=0.7,
  max_tokens=150
)
print(response.choices[0].text.strip())

however, executing this code results in the following error:

apiremovedinv1:
you tried to access openai.completion, but this is no longer supported in openai>=1.0.0 - see the readme at  for the api.
you can run openai migrate to automatically upgrade your codebase to use the 1.0.0 interface.
alternatively, you can pin your installation to the old version, e.g. pip install openai==0.28

i've looked through the openai python library documentation, the migration guide, and various resources for a solution that is compatible with google colab as of february 2024, but i haven't found a clear answer on how to proceed.

i checked the official github repository: 
i reviewed the discussion and migration guide here: 
i looked for information on using gpt-4 specifically here: 
i also reviewed recent commits for any clues: 

could someone provide guidance or an updated code snippet that works with the latest version of the openai library in google colab?
i don't know much about it, i need a solution that works on google colab and is up to date as of february 2024.","['google-colaboratory', 'openai-api', 'chatgpt-api', 'gpt-4']",77996653,"i assume you want to use the gpt-4 model.
if this is the case, then first of all, you're trying to use the method name (i.e., completion.create), which is not compatible with the chat completions api.
change this...
openai.completion.create

...to this.
openai.chatcompletion.create

but if you do this, you'll still run into errors. why?
there are still some mistakes left to fix:

you're trying to use the method name (i.e., chatcompletion.create) that works with the older openai sdk (i.e., 0.28), but not with the newer openai sdk (i.e., >=1.0.0).
the prompt parameter is not valid if you want to use the chat completions api. you should use the messages parameter instead.
message retrieval is different if you want to use the chat completions api.

see the full code with comments below.
import os
from openai import openai
client = openai()
openai.api_key = os.getenv('openai_api_key')

response = client.chat.completions.create( # changed
  model=""gpt-4"", 
  messages=[ # changed
    {""role"": ""system"", ""content"": ""you are a helpful assistant.""},
    {""role"": ""user"", ""content"": ""hello!""}
  ],
  temperature=0.7,
  max_tokens=150
)

print(response.choices[0].message) # changed",https://stackoverflow.com/questions/77996614,google-colaboratory,14-02-2024 18:43,1374.0,-4.0,1.0,True,17-02-2024 10:29,17-02-2024 10:29
40879520,nltk - convert a chunked tree into a list (iob tagging),"i need to perform named entity recognition / classification, and generate output in iob tagged format.
i'm using a nltk chunker, as delivered by nltk-train library, but that produces a tree, not a list of iob tags.
def chunk_iob(list_of_words):
    nltk_tagger = nltk.data.load(""taggers/conll2002_aubt.pickle"")
    nltk_chunker = nltk.data.load(""chunkers/conll2002_naivebayes.pickle"")

    t = nltk_tagger.tag(list_of_words)
    print(t)
    c = nltk_chunker.parse(t)
    print(c)

and we get c as a tree, like:
(s
  (loc barcelona/nc)
  (per juan/nc :/fd)

...
but i am looking for something like:
barcelona - loc
juan - per
...

which is the iob tagged list of the list_of_words parameter, in the same order as list_of_words.
how can i get that tagged list from the tree?","['nltk', 'nltk-trainer']",40888970,"what you are looking for is tree2conlltags and its reverse conlltags2tree. here's how it works:
from nltk import word_tokenize, pos_tag, ne_chunk
from nltk import conlltags2tree, tree2conlltags


tree = ne_chunk(pos_tag(word_tokenize(""new york is my favorite city"")))
print tree
# (s (gpe new/nnp york/nnp) is/vbz my/prp$ favorite/jj city/nn)

iob_tags = tree2conlltags(tree)
print iob_tags
# [('new', 'nnp', u'b-gpe'), ('york', 'nnp', u'i-gpe'), ('is', 'vbz', u'o'), ('my', 'prp$', u'o'), ('favorite', 'jj', u'o'), ('city', 'nn', u'o')]

tree = conlltags2tree(iob_tags)
print tree
# (s (gpe new/nnp york/nnp) is/vbz my/prp$ favorite/jj city/nn)

note that the iob tags are in this format b-{tag} for beginning, i-{tag} for inside and o for outside.",https://stackoverflow.com/questions/40879520,nltk,30-11-2016 03:02,9887.0,1.0,2.0,True,10-06-2022 12:40,27-06-2017 21:56
56822991,an nlp model that suggest a list of words in an incomplete sentence,"i have somewhat read a bunch of papers which talks about predicting missing words in a sentence. what i really want is to create a model that suggest a word from an incomplete sentence. 
  example:

  incomplete sentence :
  i bought an ___________  because its rainy.

  suggested words:
      umbrella
      soup
      jacket

from the journal i have read, they have utilized microsoft sentence completion dataset for predicting missing words from a sentence. 
  example :

  incomplete sentence :

  im sad because you are __________

  missing word options:
  a) crying
  b) happy
  c) pretty
  d) sad
  e) bad

i don't want to predict a missing word from a list of options. i want to suggest a list of words from an incomplete sentence. is it feasible? please enlighten me cause im really confused. what is state of the art model i can use for suggesting a list of words (semantically coherent) from an incomplete sentence?
is it necessary that the list of suggested words as an output is included in the training dataset?",['nlp'],56828558,"this is exactly how the bert model was trained: mask some random words in the sentence, and make your network predict these words. so yes, it is feasible. and not, it is not necessary to have the list of suggested words as a training input. however, these suggested words should be the part of the overall vocabulary with which this bert has been trained.
i adapted this answer to show how the completion function may work.
# install this package to obtain the pretrained model
# ! pip install -u pytorch-pretrained-bert

import torch
from pytorch_pretrained_bert import berttokenizer, bertformaskedlm

tokenizer = berttokenizer.from_pretrained('bert-base-uncased')
model = bertformaskedlm.from_pretrained('bert-base-uncased')
model.eval(); # turning off the dropout

def fill_the_gaps(text):
    text = '[cls] ' + text + ' [sep]'
    tokenized_text = tokenizer.tokenize(text)
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
    segments_ids = [0] * len(tokenized_text)
    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensors = torch.tensor([segments_ids])
    with torch.no_grad():
        predictions = model(tokens_tensor, segments_tensors)
    results = []
    for i, t in enumerate(tokenized_text):
        if t == '[mask]':
            predicted_index = torch.argmax(predictions[0, i]).item()
            predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]
            results.append(predicted_token)
    return results

print(fill_the_gaps(text = 'i bought an [mask] because its rainy .'))
print(fill_the_gaps(text = 'im sad because you are [mask] .'))
print(fill_the_gaps(text = 'im worried because you are [mask] .'))
print(fill_the_gaps(text = 'im [mask] because you are [mask] .'))

the [mask] symbol indicates the missing words (there can be any number of them). [cls] and [sep] are bert-specific special tokens. the outputs for these particular prints are
['umbrella']
['here']
['worried']
['here', 'here']

the duplication is not surprising - transformer nns are generally good at copying words. and from semantic point of view, these symmetric continuations look indeed very likely.
moreover, if it is not a random word which is missing, but exactly the last word (or last several words), you can utilize any language model (e.g. another famous sota language model, gpt-2) to complete the sentence.",https://stackoverflow.com/questions/56822991,nlp,30-06-2019 06:53,1418.0,2.0,1.0,True,08-11-2022 10:05,30-06-2019 09:50
78904049,load_state_dict getting random results,"import os
os.environ[""cuda_visible_devices""]=""0""
import torch
from transformers import autotokenizer, autoconfig, automodelforcausallm, gpt2lmheadmodel
model = automodelforcausallm.from_pretrained(
    ""gpt2"",
    device_map='auto',
)
model2 = gpt2lmheadmodel(model.config).to(""cuda"")
model2.load_state_dict(model.state_dict())
tokenizer = autotokenizer.from_pretrained(""gpt2"")

t = tokenizer(""hello_world"", return_tensors=""pt"")[""input_ids""].to(""cuda"")
a = model(t).logits
b = model2(t).logits
print(a - b)
print(a)
print(b)

model2 behaves very differently from the model (loss being much higher), but the model structures and parameters are exactly the same. from the output, it looks like something is randomized for model2. could anyone tell what was going on? i have the ""accelerate"" package installed.
the config and the parameters are the same. i also checked the forward functions, and there is no difference at all. however, setting model2.transformer.forward = model.transformer.forward, and then the two models would behave the same.","['python', 'pytorch', 'huggingface-transformers', 'pre-trained-model', 'gpt-2']",78906920,"you need to set the models to eval mode to disable dropout if you want them to produce the same results
import os
os.environ[""cuda_visible_devices""]=""0""
import torch
from transformers import autotokenizer, autoconfig, automodelforcausallm, gpt2lmheadmodel

model = automodelforcausallm.from_pretrained(
    ""gpt2"",
    device_map='auto',
)

model2 = gpt2lmheadmodel(model.config).to(""cuda"")
model2.load_state_dict(model.state_dict())

# set to eval
model.eval()
model2.eval()

tokenizer = autotokenizer.from_pretrained(""gpt2"")

t = tokenizer(""hello_world"", return_tensors=""pt"")[""input_ids""].to(""cuda"")
a = model(t).logits
b = model2(t).logits

assert (a == b).all()",https://stackoverflow.com/questions/78904049,python,23-08-2024 01:34,64.0,1.0,1.0,True,23-08-2024 17:07,23-08-2024 02:02
62740690,how do i check for specific words in a list of tokenized sentences and then mark them as one or zero?,"i am trying to map specific words in a list to another list of tokenized sentences and if the word is found in the sentence then i append a 1 to a list of its category and 0 to the rest of categories.
for example:
category_a=[""stain"",""sweat"",""wet"",""burn""]
category_b=[""love"",""bad"",""favorite""]
category_c=[""packaging"",""delivery""]
tokenized_sentences=['this deodorant does not stain my clothes','i love this product','i sweat all day']
for i in category_a:
    for j in tokenized_sentences:
          if(i in nltk.word_tokenize(j)):
                 list_a.append(j)
                 tag_a,tag_b,tag_c=([],)*3
                 tag_a.append(1)
                 tag_b.append(0)
                 tag_c.append(0)
                 final=tag_a+tag_b+tag_c

similarly for category_b and category_c
expected output:this deodorant does not stain my clothes-->[1,0,0]
                i love this product-->[0,1,0]
                i sweat all day-->[1,0,0]
                great fragrance-->[0,0,0]

i am getting duplicate outputs for each sentence like: i love this product-->[1,0,0]
i love this product-->[1,0,0] and
also like this:[i love this product,i sweat all day]-->[0,1,0]
also, if a sentence has words from two different categories ex: 'this product does not stain and i love it'
the expected output would be [1,1,0] 

how do i get the output in the required format?","['python', 'list', 'nltk', 'tokenize']",62741134,"this should do the job:
category_b = [""love"", ""bad"", ""favorite""]
category_c = [""packaging"", ""delivery""]
sentences = ['this deodorant does not stain my clothes', 'i love this product', 'i sweat all day']

results = []

for sentence in sentances:
    cat_a = 0
    cat_b = 0
    cat_c = 0
    for word in sentance.split():
        if cat_a == 0:
            cat_a = 1 if word in category_a else 0
        if cat_b == 0:
            cat_b = 1 if word in category_b else 0
        if cat_c == 0:
            cat_c = 1 if word in category_c else 0

    results.append((sentance, [cat_a, cat_b, cat_c]))


print(results)

this code will check if each sentence contains word of each of the given categories and save both of them (the sentence and result) in form of a tuple. all tuples will be appended to a list called results.
output:
[('this deodorant does not stain my clothes', [1, 0, 0]), ('i love this product', [0, 1, 0]), ('i sweat all day', [1, 0, 0])]",https://stackoverflow.com/questions/62740690,python,05-07-2020 12:26,92.0,-1.0,2.0,True,04-11-2023 03:49,04-11-2023 03:49
76735605,cuda/pytorch transformer model import error,"i am trying to load a vision transformer model from huggingface transformers using the following code:
model = pix2structforconditionalgeneration.from_pretrained('google/deplot')
processor = pix2structprocessor.from_pretrained('google/deplot')

however i get this error. i am currently using torch==2.0.1, torchvision==0.15.2 and cuda = 12.0.
---------------------------------------------------------------------------
importerror                               traceback (most recent call last)
cell in[17], line 1
----> 1 model = pix2structforconditionalgeneration.from_pretrained('google/deplot')
      2 processor = pix2structprocessor.from_pretrained('google/deplot')
      3 url = ""


importerror: /usr/local/lib/python3.8/dist-packages/fused_layer_norm_cuda.cpython-38-x86_64-linux-gnu.so: undefined symbol: _zn3c106detail14torchcheckfailepkcs2_jrknst7__cxx1112basic_stringicst11char_traitsicesaiceee

this is my environment list:
package                      version
---------------------------- --------------------------
cuda-python                  12.1.0rc1+1.g9e30ea2.dirty
cudf                         22.12.0
cugraph                      22.12.0
cugraph-dgl                  22.12.0
cugraph-service-client       22.12.0
cugraph-service-server       22.12.0
cuml                         22.12.0
cupy-cuda12x                 12.0.0a2
numpy                        1.22.2
nvidia-cublas-cu11           11.10.3.66
nvidia-cuda-cupti-cu11       11.7.101
nvidia-cuda-nvrtc-cu11       11.7.99
nvidia-cuda-runtime-cu11     11.7.99
nvidia-cudnn-cu11            8.5.0.96
nvidia-cufft-cu11            10.9.0.58
nvidia-curand-cu11           10.2.10.91
nvidia-cusolver-cu11         11.4.0.1
nvidia-cusparse-cu11         11.7.4.91
nvidia-dali-cuda110          1.22.0
nvidia-nccl-cu11             2.14.3
nvidia-nvtx-cu11             11.7.91
nvidia-pyindex               1.0.9
openai                       0.27.8
opencv                       4.6.0
python-hostlist              1.23.0
pytorch-lightning            1.2.1
pytorch-quantization         2.1.2
sentence-transformers        2.2.2
torch                        2.0.1
torch-tensorrt               1.4.0.dev0
torchaudio                   2.0.2
torchinfo                    1.8.0
torchtext                    0.13.0a0+fae8e8c
torchvision                  0.15.2

any help or guidance would be appreciated. thanks","['pytorch', 'huggingface-transformers']",76736970,"you should install the function from the transformers package. you could use the following code:
from transformers import pix2structconfig, pix2structforconditionalgeneration, pix2structprocessor

model = pix2structforconditionalgeneration.from_pretrained('google/deplot')
processor = pix2structprocessor.from_pretrained('google/deplot')

this should load the model. i used this version of the package:
pip list
package                       version
----------------------------- --------
transformers                  4.31.0",https://stackoverflow.com/questions/76735605,pytorch,21-07-2023 06:31,642.0,1.0,1.0,True,21-07-2023 10:00,21-07-2023 06:46
50019632,importerror: cannot import name &#39;summarywriter&#39;,"can anyone help me on this error, i am trying to use allennlp models and getting the below error.
 from allennlp.training.metrics import average, booleanaccuracy, 
 categoricalaccuracy
 file ""/home/administrator/aman/venv-kbs/lib/python3.6/site- 
 packages/allennlp/training/__init__.py"", line 3, in <module>
 from allennlp.training.trainer import trainer
 file ""/home/administrator/aman/venv-kbs/lib/python3.6/site- 
 packages/allennlp/training/trainer.py"", line 21, in <module>
 from tensorboard import summarywriter
 importerror: cannot import name 'summarywriter'

please find below the packages installed with the versions. 
tensorboard             1.0.0a6
tensorboard-pytorch     0.7.1
tensorboardx            0.8
tensorflow              1.4.0
tensorflow-tensorboard  0.4.0","['python', 'python-3.x', 'nlp', 'tensorboard']",50039872,change from tensorboard import summarywriter to from tensorboardx import summarywriter,https://stackoverflow.com/questions/50019632,python,25-04-2018 10:13,4673.0,1.0,2.0,True,08-12-2021 07:48,22-08-2019 19:07
62710872,how to store word vector embeddings?,"i am using bert word embeddings for sentence classification task with 3 labels. i am using google colab for coding. my problem is, since i will have to execute the embedding part every time i restart the kernel, is there any way to save these word embeddings once it is generated? because, it takes a lot of time to generate those embeddings.
the code i am using to generate bert word embeddings is -
[get_features(text_list[i]) for text_list[i] in text_list]

here, gen_features is a function which returns word embedding for each i in my list text_list.
i read that converting embeddings into bumpy tensors and then using np.save can do it. but i actually don't know how to code it.","['python-3.x', 'keras', 'nlp', 'word-embedding', 'bert-language-model']",62711312,"you can save your embeddings data to a numpy file by following these steps:
all_embeddings = here_is_your_function_return_all_data()
all_embeddings = np.array(all_embeddings)
np.save('embeddings.npy', all_embeddings)

if you're saving into google colab, then you can download it to your local computer. whenever you need it, just upload it and load it.
all_embeddings = np.load('embeddings.npy')

that's it.
btw, you can also directly save your file to google drive.",https://stackoverflow.com/questions/62710872,python-3.x,03-07-2020 07:51,18176.0,9.0,1.0,True,29-12-2022 21:00,29-12-2022 21:00
69139831,how to see the class name of one hot encoded?,"i have a csv file that includes two columns: a 'text' of a tweet and its ""label'. each tweet could belong to one of these 4 categories: hate, neutral, counterhate and non-asian aggression.
i did one hot encode y values for train and test vectors by the following code in python:
encoder = labelencoder()
y_train = encoder.fit_transform(train['label'].values)
y_train = to_categorical(y_train) 
y_test = encoder.fit_transform(test['label'].values)
y_test = to_categorical(y_test)

which if you print the first index:
print(y_train[0])

the answer is:
[0. 1. 0. 0.]

we know that each label is converted to a vector of length 4, where each position corresponds to a label class. how can i find the position of each class?
for example: hate=0, counterhate=1,...","['python', 'machine-learning', 'nlp', 'one-hot-encoding']",69140197,"first, consider that the encoder class fits on the training set then transforms it, but only transforms the test set! i recommend using the method inverse_transform to retrieve your original labels.
from sklearn import preprocessing
le = preprocessing.labelencoder()
le.fit(['hate', 'neutral', 'counterhate and non-asian', 'aggression'])
print(list(le.classes_))
print(le.transform(['counterhate and non-asian', 'hate', 'neutral']))
print(le.inverse_transform([2, 2, 1]))

output:
['aggression', 'counterhate and non-asian', 'hate', 'neutral']
[1 2 3]
['hate' 'hate' 'counterhate and non-asian']",https://stackoverflow.com/questions/69139831,python,11-09-2021 03:43,882.0,0.0,1.0,True,11-09-2021 09:22,11-09-2021 09:22
74497166,"huggingface: expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu","i am confusing about my fine-tune model implemented by huggingface model. i am able to train my model, but while i want to predict it, i always get this error. the most similar problem is this. my transformers version is 4.24.0, but it didn't seem to help me. i also try this. below is my code snippet.
from transformers import autotokenizer
from transformers import datacollatorforseq2seq
from transformers import automodelforseq2seqlm, seq2seqtrainingarguments, seq2seqtrainer
from transformers import pipeline
from tqdm import tqdm
from datasets import dataset

import pandas as pd
import numpy as np
import pyarrow as pa
import gc
import torch as t
import pickle

path = './datas/batch_answers - train_data (no-blank).csv'
epoch = 1
learning_rate = 2e-5
train_batch_size = 16
eval_batch_size = 16
device = 'cuda' if t.cuda.is_available() else 'cpu'

df = pd.read_csv(path)
df = df.drop(labels='s', axis=1)
df = df.iloc[:, 1:5]
df = df.to_numpy()
qdata = []

for i in tqdm(range(len(df))):
    argument = df[i][0][1:-1]
    response = df[i][1][1:-1]
    qprime = df[i][2][1:-1]
    
    qdata.append({'statement':argument+'\n'+response, 'argument_sentence_summary':qprime})
    
qtable = pa.table.from_pylist(qdata)
qdataset = dataset(qtable)
qdataset = qdataset.train_test_split(train_size=0.8)

qmodel = automodelforseq2seqlm.from_pretrained(""t5-small"")
qtokenizer = autotokenizer.from_pretrained(""t5-small"")
qdata_collator = datacollatorforseq2seq(tokenizer=qtokenizer, model=qmodel)

def qpreprocessing(data):
    model_input = qtokenizer(data['statement'], max_length=250, truncation=true)
    labels = qtokenizer(text_target=data['argument_sentence_summary'], max_length=75, truncation=true)

    model_input['labels'] = labels['input_ids']
    
    return model_input

qtoken = qdataset.map(qpreprocessing, batched=true)

qtraining_args = seq2seqtrainingarguments(
    output_dir=""./result"",
    evaluation_strategy=""epoch"",
    learning_rate=learning_rate,
    per_device_train_batch_size=train_batch_size,
    per_device_eval_batch_size=eval_batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=epoch,
    fp16=true,
)

qtrainer = seq2seqtrainer(
    model=qmodel,
    args=qtraining_args,
    train_dataset=qtoken['train'],
    eval_dataset=qtoken['test'],
    tokenizer=qtokenizer,
    data_collator=qdata_collator
)

old_collator = qtrainer.data_collator
qtrainer.data_collator = lambda data: dict(old_collator(data))
qtrainer.train()

qp = pipeline('summarization', model=qmodel, tokenizer=qtokenizer)
qp(qdataset['test'][0]['statement']) #break in this line

the full traceback:
runtimeerror                              traceback (most recent call last)
cell in [20], line 3
      1 qp = pipeline('summarization', model=qmodel, tokenizer=qtokenizer)
      2 # temp = t.tensor(qdataset['test'][0]['statement']).to(device)
----> 3 qp(qdataset['train'][0]['statement'])

file ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\text2text_generation.py:250, in summarizationpipeline.__call__(self, *args, **kwargs)
    226 def __call__(self, *args, **kwargs):
    227     r""""""
    228     summarize the text(s) given as inputs.
    229 
   (...)
    248           ids of the summary.
    249     """"""
--> 250     return super().__call__(*args, **kwargs)

file ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\text2text_generation.py:150, in text2textgenerationpipeline.__call__(self, *args, **kwargs)
    121 def __call__(self, *args, **kwargs):
    122     r""""""
    123     generate the output text(s) using text(s) given as inputs.
    124 
   (...)
    147           ids of the generated text.
    148     """"""
--> 150     result = super().__call__(*args, **kwargs)
    151     if (
    152         isinstance(args[0], list)
    153         and all(isinstance(el, str) for el in args[0])
    154         and all(len(res) == 1 for res in result)
    155     ):
    156         return [res[0] for res in result]

file ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\base.py:1074, in pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)
   1072     return self.iterate(inputs, preprocess_params, forward_params, postprocess_params)
   1073 else:
-> 1074     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)

file ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\base.py:1081, in pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)
   1079 def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):
   1080     model_inputs = self.preprocess(inputs, **preprocess_params)
-> 1081     model_outputs = self.forward(model_inputs, **forward_params)
   1082     outputs = self.postprocess(model_outputs, **postprocess_params)
   1083     return outputs

file ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\base.py:990, in pipeline.forward(self, model_inputs, **forward_params)
    988     with inference_context():
    989         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)
--> 990         model_outputs = self._forward(model_inputs, **forward_params)
    991         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(""cpu""))
    992 else:

file ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\text2text_generation.py:172, in text2textgenerationpipeline._forward(self, model_inputs, **generate_kwargs)
    170 generate_kwargs[""max_length""] = generate_kwargs.get(""max_length"", self.model.config.max_length)
    171 self.check_inputs(input_length, generate_kwargs[""min_length""], generate_kwargs[""max_length""])
--> 172 output_ids = self.model.generate(**model_inputs, **generate_kwargs)
    173 out_b = output_ids.shape[0]
    174 if self.framework == ""pt"":

file ~\anaconda3\envs\ame\lib\site-packages\torch\autograd\grad_mode.py:27, in _decoratorcontextmanager.__call__.<locals>.decorate_context(*args, **kwargs)
     24 @functools.wraps(func)
     25 def decorate_context(*args, **kwargs):
     26     with self.clone():
---> 27         return func(*args, **kwargs)

file ~\anaconda3\envs\ame\lib\site-packages\transformers\generation_utils.py:1339, in generationmixin.generate(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)
   1331         logger.warning(
   1332             ""a decoder-only architecture is being used, but right-padding was detected! for correct ""
   1333             ""generation results, please set `padding_side='left'` when initializing the tokenizer.""
   1334         )
   1336 if self.config.is_encoder_decoder and ""encoder_outputs"" not in model_kwargs:
   1337     # if model is encoder decoder encoder_outputs are created
   1338     # and added to `model_kwargs`
-> 1339     model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
   1340         inputs_tensor, model_kwargs, model_input_name
   1341     )
   1343 # 4. prepare `input_ids` which will be used for auto-regressive generation
   1344 if self.config.is_encoder_decoder:

file ~\anaconda3\envs\ame\lib\site-packages\transformers\generation_utils.py:583, in generationmixin._prepare_encoder_decoder_kwargs_for_generation(self, inputs_tensor, model_kwargs, model_input_name)
    581 encoder_kwargs[""return_dict""] = true
    582 encoder_kwargs[model_input_name] = inputs_tensor
--> 583 model_kwargs[""encoder_outputs""]: modeloutput = encoder(**encoder_kwargs)
    585 return model_kwargs

file ~\anaconda3\envs\ame\lib\site-packages\torch\nn\modules\module.py:1130, in module._call_impl(self, *input, **kwargs)
   1126 # if we don't have any hooks, we want to skip the rest of the logic in
   1127 # this function, and just call forward.
   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1129         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1130     return forward_call(*input, **kwargs)
   1131 # do not call functions when jit is used
   1132 full_backward_hooks, non_full_backward_hooks = [], []

file ~\anaconda3\envs\ame\lib\site-packages\transformers\models\t5\modeling_t5.py:941, in t5stack.forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)
    939 if inputs_embeds is none:
    940     assert self.embed_tokens is not none, ""you have to initialize the model with valid token embeddings""
--> 941     inputs_embeds = self.embed_tokens(input_ids)
    943 batch_size, seq_length = input_shape
    945 # required mask seq length can be calculated via length of past

file ~\anaconda3\envs\ame\lib\site-packages\torch\nn\modules\module.py:1130, in module._call_impl(self, *input, **kwargs)
   1126 # if we don't have any hooks, we want to skip the rest of the logic in
   1127 # this function, and just call forward.
   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1129         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1130     return forward_call(*input, **kwargs)
   1131 # do not call functions when jit is used
   1132 full_backward_hooks, non_full_backward_hooks = [], []

file ~\anaconda3\envs\ame\lib\site-packages\torch\nn\modules\sparse.py:158, in embedding.forward(self, input)
    157 def forward(self, input: tensor) -> tensor:
--> 158     return f.embedding(
    159         input, self.weight, self.padding_idx, self.max_norm,
    160         self.norm_type, self.scale_grad_by_freq, self.sparse)

file ~\anaconda3\envs\ame\lib\site-packages\torch\nn\functional.py:2199, in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2193     # note [embedding_renorm set_grad_enabled]
   2194     # xxx: equivalent to
   2195     # with torch.no_grad():
   2196     #   torch.embedding_renorm_
   2197     # remove once script supports set_grad_enabled
   2198     _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-> 2199 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)

runtimeerror: expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)

should that mean i need another way to predict my test dataset instead of using pipeline? big thanks for help.","['python', 'pytorch', 'data-science', 'huggingface-transformers']",74506560,"i do get the idea from the comment. the way i solve this is i can still train my qmodel on 'cuda', but if i want to do the prediction, i'll need to put my qmodel to 'cpu'. so i modify my last few lines code to below:
qtrainer.train()

qmodel = qmodel.to('cpu') #put my model to cpu

qp = pipeline('summarization', model=qmodel, tokenizer=qtokenizer)
print(qp(qdataset['test'][0]['statement']))

and it works.",https://stackoverflow.com/questions/74497166,python,19-11-2022 02:36,8834.0,1.0,1.0,True,20-11-2022 08:12,19-11-2022 06:03
77505283,cross-encoder transformer converges every input to the same cls embedding,"i'm trying to create a cross-encoder model starting from ""distilbert-base-uncased"" using huggingface transformers with pytorch. the architecture is simple: get the cls embedding from the concatenated input strings (this is handled by the huggingface tokenizer), then pass it through a final fc linear layer to 1 output logit. the loss function is the built-in torch.nn.bcewithlogitsloss function.
this model failed to learn correctly on my dataset, instead quickly converging the cls embedding (prior the the final fc linear layer) to the same embedding for every input (in fact, the other tokens also converge to the same embedding). the last layer then maps this embedding to the ratio of positives in the training sample, which is the expected behavior assuming a constant embedding function.
for debugging purposes i simply fed it a dummy dataset consisting of the same 3 sentence pairs over and over (1 labelled positive, the other 2 negative). the same behavior persisted, but when i froze the transformer parameters (so that only the final fc was being trained), the model correctly overfit on the data point as expected.
my model architecture:
class crossencodermodel(nn.module):
    """"""
    architecture:
    - transformer
    - final fc linear layer to one output for binary classification
    """"""

    def __init__(
        self, transformer_model: str, tokenizer: pretrainedtokenizerfast
    ) -> none:
        super(paragraphcrossencodermodel, self).__init__()

        self.transformer = automodel.from_pretrained(transformer_model)
        print(type(self.transformer))
        self.transformer.resize_token_embeddings(len(tokenizer))
        self.fc = nn.linear(self.transformer.config.hidden_size, 1)

    def forward(self, input_ids: tensor, attention_mask: tensor) -> tensor:
        outputs = self.transformer(
            input_ids=input_ids,
            attention_mask=attention_mask,
        )

        cls_embedding = outputs.last_hidden_state[:, 0, :]
        logits = self.fc(cls_embedding).squeeze(-1)
        return logits

my loss/update (using adam optimizer):
loss = torch.nn.bcewithlogitsloss(logits, labels)
loss.backward()

optimizer.step()
optimizer.zero_grad()

i tried varying the learning rate and batch size, neither of which changed the convergence to the same cls embedding. i suspect something is wrong with my model architecture, but i'm having trouble finding what exactly. the behavior also persists when i replace the loss function with a manual target:
class testloss(nn.module):
    def __init__(self):
        super(testloss, self).__init__()

    def forward(self, logits: tensor, labels: tensor) -> tensor:
        return torch.sum(torch.abs(logits - torch.tensor([10.0, -10.0, -10.0]).to(device)))
    # still converges to all embeddings being the same","['pytorch', 'huggingface-transformers']",77510118,"okay, after a lot of debugging i tried changing my optimizer. i was using adam which worked well when i was using a dual-encoder architecture. changing to sgd fixed the issue and the model learns correctly now.
not super sure why adam wasn't working, will update if i figure it out.",https://stackoverflow.com/questions/77505283,pytorch,18-11-2023 00:53,678.0,3.0,1.0,True,19-11-2023 08:44,19-11-2023 08:44
76206458,header issue in generated tdm via python,"i'm having some trouble with my python tdm. right now it accepts a generated csv from my other application and then creates a term document matrix out of it. the current issue is that some words in the dictionary that have 0 frequency are still appearing in the header.
this is the current output
so in this case words like one, simply, focus, money, etc all the way to the right should not be added/displayed at all in the created tdm file.
def termdocumentmatrix():
    # get filenames of csv files
    filenames = filedialog.askopenfilename(
        title=""datafluent | open csv files for tdm"", filetypes=[(""comma separated value"", ""*.csv"")]
    )

    # check file paths
    absolute_path = os.path.dirname(__file__)
    relative_path = ""temp/upload/to_tdm""
    folderdir = os.path.join(absolute_path, relative_path)

    # set new filename for generated csv
    new_filename = path(filenames).stem
    new_filename = new_filename.replace(' ', '_')

    # upload file to temp folder
    try:
        copyfile_tdm(filenames, folderdir)
    except:
        mb.showerror(title=""error!"", message=""file can't be opened! might be wrong format or damaged!"")

    # read raw data from file
    data = pd.read_csv(filenames, header=none)
    tdmfile = data[0].str.cat(sep=' ')

    # clean data by removing commas and new lines
    tdmfile = tdmfile.replace("","", """")
    tdmfile = tdmfile.replace(""\\n"", """")

    # create lemmatization object
    lemmatizer = wordnetlemmatizer()

    # tokenize text into sentences
    tokenizer = sent_tokenize(tdmfile)

    # lemmatize words to get their proper meaning and remove stop words
    lemmawords = []
    for sentence in tokenizer:
        # convert non-alphabetic characters to spaces
        sentence = re.sub('[^a-za-z]', ' ', sentence)
        tokens = word_tokenize(sentence.lower())
        # remove stop words and lemmatize remaining words
        lemmawords += [lemmatizer.lemmatize(token) for token in tokens if token not in set(stopwords.words('english'))]

    # create bag of words dictionary and filter out words with low frequency
    min_frequency = 2
    word_counts = counter(lemmawords)
    dictionary = {word: i for i, word in enumerate(word_counts.keys()) if word_counts[word] >= min_frequency}

    # build bag of words model
    sentence_vectors = []
    for sentence in tokenizer:
        sentence_words = set(word_counts.keys()).intersection(set(word_tokenize(sentence)))
        vector = [word_counts[word] for word in sentence_words if word in dictionary]
        sentence_vectors.append(vector)

    sentence_vectors = np.asarray(sentence_vectors)

    # write output to csv file
    output_path = f""{new_filename}_tdm.csv""
    with open(output_path, mode='w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        header = [word for word in dictionary.keys() if word in set(lemmawords)]
        writer.writerow(header)
        for row in sentence_vectors:
            if not all(x == 0 for x in row):
                writer.writerow(row)

    # open output file
    os.system(f'start {output_path}')

i've tried a couple of fixes but it resulted in things getting a lot worse. i tried building the sentence_vectors and the header_words together but no luck. i tried adjusting the header but no luck as well.","['python', 'pandas', 'numpy', 'nltk']",76206834,"you can use:
# tokenize text into sentences
tokenizer = sent_tokenize(tdmfile)

# new code from here
min_frequency = 2
stop_words = stopwords.words('english')

bags = []
for sentence in tokenizer:
    sentence = re.sub('[^a-za-z]', ' ', sentence)
    tokens = [lemmatizer.lemmatize(token)
                  for token in word_tokenize(sentence.lower())
                  if token not in stop_words]
    bags.append(counter(tokens))

bags = pd.dataframe(bags).fillna(0).astype(int)
bags = bags.loc[: , bags.sum() > min_frequency]

# export to file
bags.replace(0, '').to_csv(f'{new_filename}_tdm.csv', index=false)

output:
>>> bags
     document  entrepreneur  businessman  choose  ten  make  given  answer  ...  attack  company  trick  cybercriminals  personal  vulnerable  sensitive  apps
0           1             2            1       1    1     1      1       1  ...       0        0      0               0         0           0          0     0
1           0             0            0       0    0     0      0       0  ...       0        0      0               0         0           0          0     0
2           0             1            1       0    0     0      0       0  ...       0        0      0               0         0           0          0     0
3           0             0            0       0    0     0      0       0  ...       0        0      0               0         0           0          0     0
4           0             0            0       0    0     1      0       0  ...       0        0      0               0         0           0          0     0
..        ...           ...          ...     ...  ...   ...    ...     ...  ...     ...      ...    ...             ...       ...         ...        ...   ...
267         0             0            0       0    0     0      0       0  ...       0        0      0               0         0           0          0     0
268         0             0            0       0    0     0      0       0  ...       0        0      0               0         0           0          0     0
269         0             0            0       0    0     0      0       0  ...       0        0      0               0         0           0          1     0
270         0             0            0       0    0     0      0       0  ...       0        0      0               0         0           0          0     1
271         0             0            0       0    0     0      0       0  ...       0        0      0               0         0           0          0     0

[272 rows x 337 columns]",https://stackoverflow.com/questions/76206458,python,09-05-2023 06:33,48.0,0.0,1.0,True,09-05-2023 08:17,09-05-2023 06:36
27416164,what is conll data format?,"i am using a open source jar (mate parser) which outputs in the conll 2009 format after dependency parsing. i want to use the dependency parsing results for information extraction, however, i only understand part of the output in the conll data format.
can someone explain the conll data format?","['nlp', 'text-parsing', 'text-mining', 'information-extraction']",27425613,"there are many different conll formats since conll is a different shared task each year. the format for conll 2009 is described here. each line represents a single word with a series of tab-separated fields. _s indicate empty values. mate-parser's manual says that it uses the first 12 columns of conll 2009:
id form lemma plemma pos ppos feat pfeat head phead deprel pdeprel

the definition of some of these columns come from earlier shared tasks (the conll-x format used in 2006 and 2007):

id (index in sentence, starting at 1)
form (word form itself)
lemma (word's lemma or stem)
pos (part of speech)
feat (list of morphological features separated by |)
head (index of syntactic parent, 0 for root)
deprel (syntactic relationship between head and this word)

there are variants of those columns (e.g., ppos but not pos) that start with p indicate that the value was automatically predicted rather a gold standard value.
update: there is now a conll-u data format as well which extends the conll-x format.",https://stackoverflow.com/questions/27416164,nlp,11-12-2014 05:45,55493.0,67.0,2.0,True,08-06-2023 03:25,08-06-2023 03:25
64974507,gensim most similar word to vector,"i am using the pretrained word vectors from wikipedia, ""glove-wiki-gigaword-100"", in gensim. as this example documentation shows, you can query the most similar words for a given word or set of words using
model_gigaword.wv.most_similar(positive=['dirty','grimy'],topn=10)

however, i would like to query the most similar words to a given vector, specified as an array (of the same format as a word-vector from the pretrained model). for example, the result from adding or subtracting two word-vectors in the pretrained model, like
vec = model_gigaword['king']-model_gigaword['man']

output: (for vec)
array([-0.696     , -1.26119   , -0.49109   ,  0.91179   ,  0.23077281,
       -0.18835002, -0.65568995, -0.29686698, -0.60074997, -1.35762   ,
       -0.11816999,  0.01779997, -0.74096   ,  0.21192   , -0.407071  ,
       -1.04871   , -0.480674  , -0.95541   , -0.06046999,  0.20678002,
       -1.1516    , -0.98955095,  0.44508   ,  0.32682198, -0.03306001,
       -0.31138003,  0.87721   ,  0.34279   ,  0.78621   , -0.297459  ,
        0.529243  , -0.07398   ,  0.551844  ,  0.54218   , -0.39394   ,
        0.96368   ,  0.22518003,  0.05197001, -0.912573  , -0.718755  ,
        0.08056   ,  0.421177  , -0.34256   , -0.71294   , -0.25391   ,
       -0.65362   , -0.31369498,  0.216278  ,  0.41873002, -0.21784998,
        0.21340999,  0.480393  ,  0.47077006, -1.00272   ,  0.16624999,
       -0.07340002,  0.09219003, -0.02021003, -0.58403   , -0.47306   ,
        0.05066001, -0.64416003,  0.80061007,  0.224344  , -0.20483994,
       -0.33785298, -1.24589   ,  0.08900005, -0.08385998, -0.195515  ,
        0.08500999, -0.55749   ,  0.19473001, -0.0751    , -0.61184   ,
       -0.08018   , -0.34303   ,  1.03759   , -0.36085004,  0.93508005,
       -0.00997001, -0.57282   ,  0.33101702,  0.271261  ,  0.47389007,
        1.1219599 , -0.00199997, -1.609     ,  0.57377803, -0.17023998,
       -0.22913098, -0.33818996, -0.367797  ,  0.367965  , -1.08955   ,
       -0.664806  ,  0.05213001,  0.40829998,  0.125692  , -0.44967002],
      dtype=float32)

how do i get the most similar words to vec?","['python', 'nlp', 'gensim', 'word2vec']",64974659,"you can directly use this with model_gigaword.wv.most_similar
your_word_vector = np.array([-0.696, -1.26119, -0.49109, 0.91179, 0.23077281,
       -0.18835002, -0.65568995, -0.29686698, -0.60074997, -1.35762   ,
       -0.11816999,  0.01779997, -0.74096   ,  0.21192   , -0.407071  ,
       -1.04871   , -0.480674  , -0.95541   , -0.06046999,  0.20678002,
       -1.1516    , -0.98955095,  0.44508   ,  0.32682198, -0.03306001,
       -0.31138003,  0.87721   ,  0.34279   ,  0.78621   , -0.297459  ,
        0.529243  , -0.07398   ,  0.551844  ,  0.54218   , -0.39394   ,
        0.96368   ,  0.22518003,  0.05197001, -0.912573  , -0.718755  ,
        0.08056   ,  0.421177  , -0.34256   , -0.71294   , -0.25391   ,
       -0.65362   , -0.31369498,  0.216278  ,  0.41873002, -0.21784998,
        0.21340999,  0.480393  ,  0.47077006, -1.00272   ,  0.16624999,
       -0.07340002,  0.09219003, -0.02021003, -0.58403   , -0.47306   ,
        0.05066001, -0.64416003,  0.80061007,  0.224344  , -0.20483994,
       -0.33785298, -1.24589   ,  0.08900005, -0.08385998, -0.195515  ,
        0.08500999, -0.55749   ,  0.19473001, -0.0751    , -0.61184   ,
       -0.08018   , -0.34303   ,  1.03759   , -0.36085004,  0.93508005,
       -0.00997001, -0.57282   ,  0.33101702,  0.271261  ,  0.47389007,
        1.1219599 , -0.00199997, -1.609     ,  0.57377803, -0.17023998,
       -0.22913098, -0.33818996, -0.367797  ,  0.367965  , -1.08955   ,
       -0.664806  ,  0.05213001,  0.40829998,  0.125692  , -0.44967002])

model_gigaword.wv.most_similar(positive=[your_word_vector], topn=10)

[('vajiravudh', 0.7130449414253235),
 ('prajadhipok', 0.6764554381370544),
 ('andrianampoinimerina', 0.6474215984344482),
 ('jeongjo', 0.6449092626571655),
 ('taejong', 0.6352322697639465),
 ('rehoboam', 0.6319528818130493),
 ('injo', 0.6317901611328125),
 ('gojong', 0.6302404999732971),
 ('seonjo', 0.6272163391113281),
 ('elessar', 0.6250109672546387)]


these results will be almost garbage, as expected. read the reason below.

one important point though. i see you are trying to find the words that are similar to the difference vector in the euclidean space of the word vectors. the difference between king and man results in a vector that is similar to the difference between queen and woman means that the length and direction of the difference vector encode the contextual difference between the 2 respective pairs of words.
the literal position of that vector maybe garbage because by checking it in the euclidean space, you will anchor it on the origin. both the difference vectors (king->man and queen->woman) above are anchored on 'king' and 'queen' respectively.
the intuition you should have is that a->b and c->d may have similar vectors connecting them even though a, b and c, d may line in completely separate parts of the euclidean space, if they have a similar contextual difference between them. this is what the vector space in a properly trained word2vec is encoding.",https://stackoverflow.com/questions/64974507,python,23-11-2020 18:48,3526.0,2.0,1.0,True,06-08-2021 13:00,06-08-2021 13:00
77708212,can&#39;t import adaptertrainer,"i'm trying text classification with adapters in google colab and faced a lot of problems with libs.
!pip install -u adapter-transformers 

from transformers import trainingarguments, adaptertrainer, evalprediction

gives an error:
runtimeerror: failed to import transformers.adapters.trainer because of the following error (look up to see its traceback):
failed to import transformers.trainer_seq2seq because of the following error (look up to see its traceback):
cannot import name 'default_hp_search_backend' from 'transformers.integrations' (/usr/local/lib/python3.10/dist-packages/transformers/integrations/__init__.py)

most confusing part is that i already ran this notebook earlier.
i've tried a lot of manipulations with libraries:
installed using git, deleted and installed different parts.","['python', 'pip', 'artificial-intelligence', 'huggingface-transformers']",77733975,"you probably already have transformers installed. adapter-transformersï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ is a fork of the transformers library and can't be installed in the same environment:

adapter-transformers is a direct fork of transformers. this means our package includes all the awesome features of huggingfaceï¿½ï¿½ï¿½s original package, plus the adapter implementation. as both packages share the same namespace, they ideally should not be installed in the same environment.

note: the adapter-transformers package is deprecated and replaced by the adapters package. install it using :
pip install adapters

you need to import the adaptertrainer from adapters instead of transformer"" rel=""nofollow noreferrer"">transitioning) :
from transformers import trainingarguments, evalprediction
from  adapters import adaptertrainer

see see here for more informations.",https://stackoverflow.com/questions/77708212,python,23-12-2023 15:55,1241.0,1.0,1.0,True,29-12-2023 22:48,29-12-2023 19:38
77762811,getting response none when using chroma db along with llamaindex for querying custom pdf,"i making a project which uses chromadb (0.3.29), llama-index (0.6.34.post1) and langchain (0.0.245), and openai (0.27.8).but i am getting response none when i tried to query in custom pdfs.even they are getting embedded successfully , below are my codes:
import os, re
import shutil
import time
from grpc import servicercontext
import vectordb
from langchain import openai
from llama_index import gpttreeindex, simpledirectoryreader, llmpredictor,gptvectorstoreindex,prompthelper, vectorstoreindex
from llama_index import langchainembedding, servicecontext,  prompt
from llama_index import storagecontext, load_index_from_storage
from langchain.embeddings import openaiembeddings
from langchain.llms import azureopenai
# import azure openai
#from langchain_community.llms import azureopenai
import chromadb
from llama_index.vector_stores import chromavectorstore
 
from dotenv import load_dotenv
load_dotenv()
#openai.api_key = os.getenv[""openai_api_key""]



def regenrate_tokens(collection_name,persist_directory): 
         
        if os.path.isdir((persist_directory)):
            print(""directory existed ,replacing previous directory"")
            shutil.rmtree(persist_directory)
            print(""recreating embeddings..."")
            vector=vectordb.creatingchromadb(collection_name,persist_directory)
            vector.storage_context.persist(persist_dir= persist_directory)

        else:
            print(""directory does not exit, creating new embeddings."")
            vector=vectordb.creatingchromadb(collection_name,persist_directory)
            vector.storage_context.persist(persist_dir= persist_directory)
        
        time.sleep(10) # sleep for 10 seconds

        return('token regenrated, you can ask the questions. ï¿½ï¿½ï¿½ï¿½')

def query__from_knowledge_base(question):
    persist_directory = './chromadb'
    collection_name = ""chromavectorstore""

  
    if(question == 'regenerate tokens'):
        return(regenrate_tokens(collection_name,perectory))
    
    index = vectordb.loadfromdisk(collection_name,persist_directory)
    print(index)
    # define custom prompt
    # template_str = (
    #     ""we have provided context information below. \n""
    #     ""---------------------\n""
    #     ""{context_str}""
    #     ""\n---------------------\n""
    #     ""given this information, please answer the question: {query_str}\n""
    # )
    template_str = """"""create a final answer to the given questions using the provided document excerpts(in no particular order) as references. always include a ""sources"" section in your answer including only the minimal set of sources needed to answer the question. always include the source preview of source. if answer has step in document please response in step. if you are unable to answer the question, simply state that you do not know. do not attempt to fabricate an answer and leave the sources section empty.

        ""---------------------\n""
        ""{context_str}""
        ""\n---------------------\n""
        ""given this information, please answer the question: {query_str}\n""
    """"""

    qa_template = prompt(template_str)
    
    query_engine = index.as_query_engine(text_qa_template=qa_template)
    print(query_engine)
    response = query_engine.query(question)
    print(question)
    # print(response)
    response = str(response)   
    response = re.sub(r'answer:', '', response)
    response = response.strip()
    return(response)
    

#print(regenrate_tokens())
#print(query__from_knowledge_base('enabling online archive for the userï¿½ï¿½ï¿½s mailbox.'))

file vectordb.py,
containing creation and querying methods are below:
def creatingchromadb(collection_name,persist_directory):

    documents = simpledirectoryreader('./static/upload/').load_data()
    # deployment_name = ""text-davinci-003""
    deployment_name = ""gpt-3.5-turbo""
    open_version=""30/08/2023""

    # create llm via azure openai service
    llm = azureopenai(deployment_name=deployment_name,openai_api_version=openai_api_version)
    llm_predictor = llmpredictor(llm=llm)
    llm_predictor = llmpredictor(llm = llm_predictor)
    embedding_llm = langchainembedding(openaiembeddings())

    # define prompt helper
    max_input_size = 3000
    num_output = 256
    chunk_size_limit = 1000 # token window size per document
    max_chunk_overlap = 20 # overlap for each token fragment
    prompt_helper = prompthelper(max_input_size=max_input_size, num_output=num_output,
                              max_chunk_overlap=max_chunk_overlap, chunk_size_limit=chunk_size_limit)

    service_context = servicecontext.from_defaults(llm_predictor=llm_predictor, embed_model=embedding_llm, prompt_helper=prompt_helper)
    chroma_client = chromadb.client(settings(
    chroma_db_impl=""duckdb+parquet"",
    persist_directory= persist_directory))

    print(collection_name)

    # create a collection
    chroma_collection = chroma_client.get_or_create_collection(collection_name,embedding_function=embedding_llm)
    # 
    print(chroma_collection.count())

    vector_store = chromavectorstore(chroma_collection)
    storage_context = storagecontext.from_defaults(vector_store=vector_store)
    index = gptvectorstoreindex.from_documents(documents, storage_context=storage_context, service_context=service_context)
    print(chroma_collection.count())
    print(chroma_collection.get()['documents'])
    print(chroma_collection.get()['metadatas'])

    # index.storage_context.persist()
    return index

def loadfromdisk(collection_name,persist_directory):
    chroma_client = chromadb.client(settings(
    chroma_db_impl=""duckdb+parquet"",
    persist_directory= persist_directory))

    print(collection_name)

    chroma_collection = chroma_client.get_or_create_collection(collection_name)
    vector_store = chromavectorstore(chroma_collection=chroma_collection)
    index = gptvectorstoreindex.from_vector_store(vector_store=vector_store)
    return index

if we tried to regenerate tokens and try to query from pdfs then its shows ""none"" response, even if those files are embedded properly.","['python', 'langchain', 'py-langchain', 'llama-index', 'chromadb']",77766651,"there were multiple issues with this code so i've taken the liberty of editing extensively.

you don't need to use langchain, llamaindex is enough
you don't need to persist your storage context to disk, the chroma client does that automatically when you create a persistent client
your prompt needed to be a prompttemplate

from dotenv import load_dotenv
load_dotenv()

import re
from llama_index import (
    simpledirectoryreader, 
    vectorstoreindex, 
    servicecontext, 
    storagecontext
)
from llama_index.vector_stores import chromavectorstore
from llama_index.llms import openai
from llama_index.embeddings import openaiembedding
from llama_index.prompts import prompttemplate
import chromadb

import logging
import sys
logging.basicconfig(stream=sys.stdout, level=logging.debug)
logging.getlogger().addhandler(logging.streamhandler(stream=sys.stdout))

def refresh_data(collection_name,persist_directory): 

    chroma_client = chromadb.persistentclient(path=persist_directory)
    embed_model = openaiembedding()
    documents = simpledirectoryreader('./static/upload/').load_data()
    chroma_collection = chroma_client.get_or_create_collection(collection_name)
    vector_store = chromavectorstore(chroma_collection=chroma_collection)
    storage_context = storagecontext.from_defaults(vector_store=vector_store)
    service_context = servicecontext.from_defaults(embed_model=embed_model)
    index = vectorstoreindex.from_documents(
        documents, storage_context=storage_context, service_context=service_context
    )
    return('data refreshed, you can ask the questions. ï¿½ï¿½ï¿½ï¿½')

def query__from_knowledge_base(question):

    persist_directory = './chromadb'
    collection_name = ""chromavectorstore""
  
    if(question == 'refresh_data'):
        return(refresh_data(collection_name,persist_directory))
    
    chroma_client = chromadb.persistentclient(persist_directory)
    chroma_collection = chroma_client.get_or_create_collection(collection_name)
    vector_store = chromavectorstore(chroma_collection=chroma_collection)
    llm = openai(model=""gpt-3.5-turbo"") # default, could try gpt-4
    service_context = servicecontext.from_defaults(llm=llm)
    index = vectorstoreindex.from_vector_store(
        vector_store,
        service_context=service_context,
    )

    template_str = """"""crenal answer to the given questions using the provided document excerpts(in no particular order) as references. always include a ""sources"" section in your answer including only the minimal set of sources needed to answer the question. always include the source preview of source. if answer has step in document please response in step. if you are unable to answer the question, simply state that you do not know. do not attempt to fabricate an answer and leave the sources section empty.

        ""---------------------\n""
        ""{context_str}""
        ""\n---------------------\n""
        ""given this information, please answer the question: {query_str}\n""
    """"""

    qa_template = prompttemplate(template_str)
    
    query_engine = index.as_query_engine(text_qa_template=qa_template)
    response = query_engine.query(question)
    response = str(response)   
    response = re.sub(r'answer:', '', response)
    response = response.strip()
    return(response)
    

print(refresh_data(""chromavectorstore"",""./chromadb""))
print(query__from_knowledge_base('what are some of the latest advances in computer vision?'))

i've tested this locally with some pdfs and it seems to work and do what you wanted, good luck with your project!",https://stackoverflow.com/questions/77762811,python,05-01-2024 06:21,1061.0,0.0,1.0,True,11-03-2024 01:41,11-03-2024 01:41
57359982,remove stopwords in french and english in tfidfvectorizer,"i am trying to remove stopwords in french and english in tfidfvectorizer. so far, i've only managed to remove stopwords from the english language. when i try to enter the french language for the stop_words, i get an error message that says it's not built-in.
in fact, i get the following error message:
valueerror: not a built-in stop list: french

i have a text document containing 700 lines of text mixed in french and english. 
i am doing a clustering project of these 700 lines using python. however, a problem arises with my clusters: i am getting clusters full of french stopwords, and this is messing up the efficiency of my clusters.
my question is the following:
is there any way to add french stopwords or manually update the built-in english stopword list so that i can get rid of these unnecessary words? 
here's the tfidfvectorizer code that contains my stopwords code:
tfidf_vectorizer = tfidfvectorizer(max_df=0.8, max_features=200000,
                             min_df=0.2, stop_words='english',
                             use_idf=true, tokenizer=tokenize_and_stem, 
ngram_range=(1,3))

the removal of these french stopwords will allow me to have clusters that are representative of the words that are recurring in my document. 
for any doubt regarding the relevance of this question, i had asked a similar question last week. however, it is not similar as it does not use tfidfvectorizer.
any help would be greatly appreciated. thank you.","['python', 'nltk', 'stop-words', 'tfidfvectorizer']",57360463,"you can use good stop words packages from nltk or spacy, two super popular nlp libraries for python. since achultz has already added the snippet for using stop-words library, i will show how to go about with nltk or spacy.
nltk:
from nltk.corpus import stopwords

final_stopwords_list = stopwords.words('english') + stopwords.words('french')
tfidf_vectorizer = tfidfvectorizer(max_df=0.8,
  max_features=200000,
  min_df=0.2,
  stop_words=final_stopwords_list,
  use_idf=true,
  tokenizer=tokenize_and_stem,
  ngram_range=(1,3))

nltk will give you 334 stopwords in total.
spacy:
from spacy.lang.fr.stop_words import stop_words as fr_stop
from spacy.lang.en.stop_words import stop_words as en_stop

final_stopwords_list = list(fr_stop) + list(en_stop)
tfidf_vectorizer = tfidfvectorizer(max_df=0.8,
  max_features=200000,
  min_df=0.2,
  stop_words=final_stopwords_list,
  use_idf=true,
  tokenizer=tokenize_and_stem,
  ngram_range=(1,3))

spacy gives you 890 stopwords in total.",https://stackoverflow.com/questions/57359982,python,05-08-2019 13:48,27033.0,6.0,3.0,True,01-06-2022 06:05,05-08-2019 15:35
5391840,stemming english words with lucene,"i'm processing some english texts in a java application, and i need to stem them.
for example, from the text ""amenities/amenity"" i need to get ""amenit"".
the function looks like:
string stemterm(string term){
   ...
}

i've found the lucene analyzer, but it looks way too complicated for what i need.

is there a way to use it to stem words without building an analyzer? i don't understand all the analyzer business...
edit: i actually need a stemming + lemmatization. can lucene do this?","['java', 'lucene', 'stemming', 'porter-stemmer']",5394769,"import org.apache.lucene.analysis.porterstemmer;
...
string stemterm (string term) {
    porterstemmer stemmer = new porterstemmer();
    return stemmer.stem(term);
}

see here for more details.  if stemming is all you want to do, then you should use this instead of lucene.
edit: you should lowercase term before passing it to stem().",https://stackoverflow.com/questions/5391840,java,22-03-2011 13:14,65128.0,29.0,7.0,True,26-05-2021 05:03,21-05-2013 13:19
77900626,firebase openai api error: &quot;openai.createcompletion is not a function&quot;,"i am having an issue with running my firebase function. it needs to generate a challenge once a day (i have it set up to run every minute now for debug) and to save it in my realtime database. this code is like this:
const functions = require(""firebase-functions"");
const admin = require(""firebase-admin"");
const openai = require(""openai"");

admin.initializeapp();

// create a new instance of the openai api client
const openai = new openai({
  apikey: functions.config().openai.key,
});

exports.generatedailychallenge = functions.pubsub.schedule(""* * * * *"")
    .onrun(async (context) => {
      try {
        const response = await openai.createcompletion({
          model: ""text-davinci-003"",
          prompt: ""you are an ai model powering a social media app based "" +
                ""around challenges. the app works in a similar way to "" +
                ""bereal, meaning that once a day it generates a challenge "" +
                ""for the users. the challenge needs to be simple to "" +
                ""complete and require just a single photo taken from both "" +
                ""cameras, just keep this in mind, but don't mention it as "" +
                ""the users are used to it. the challenge is supposed to be "" +
                ""fun, but easy to complete so users don't have to spend "" +
                ""too much time on it. the goal is to connect with your "" +
                ""online friends in a fun and engaging way. mainly to just "" +
                ""see what they were up to at that specific moment. give me "" +
                ""just the challenge name and a brief description of the "" +
                ""challenge as if you were challenging a user to do it"",
          max_tokens: 150,
        });

        const challengetext = response.data.choices[0].text.trim();
        console.log(""generated challenge:"", challengetext);

        // save to firestore
        const db = admin.firestore();
        await db.collection(""challenges"").add({
          text: challengetext,
          createdat: admin.firestore.fieldvalue.servertimestamp(),
        });
      } catch (error) {
        console.error(""error generating or saving challenge:"", error.message);
      }
    });

however when i run it i get this error in a log:
    {
  ""textpayload"": ""error generating or saving challenge: openai.createcompletion is not a function"",
  ""insertid"": ""65b7b5f100015b11309c47bd"",
  ""resource"": {
    ""type"": ""cloud_function"",
    ""labels"": {
      ""project_id"": ""nocena-dea56"",
      ""function_name"": ""generatedailychallenge"",
      ""region"": ""us-central1""
    }
  },
  ""timestamp"": ""2024-01-29t14:28:01.088849z"",
  ""labels"": {
    ""runtime_version"": ""nodejs16_20240121_16_20_2_rc00"",
    ""instance_id"": ""0087599d42bd678f896459629280089ca75acae30403ae873cd7bb1e45f4fe7e158e416746bfcd7cebb55af5ab4abe7bf80c2b4277d2d0d089ec3f3e043ca6018f63"",
    ""execution_id"": ""lzchc47te50n""
  },
  ""logname"": ""projects/nocena-dea56/logs/cloudfunctions.googleapis.com%2fcloud-functions"",
  ""trace"": ""projects/nocena-dea56/traces/8bff527e9d2bcf302a301c3630917ee8"",
  ""receivetimestamp"": ""2024-01-29t14:28:01.426853583z""
}

when i tried debugging it i got told i donï¿½ï¿½ï¿½t initialise the function properly. however that is not the case. and i also tried changing the way i initialise it so many time. i am using this to call it:  (it is the version 4.26.0), so i should initialise it this way.
if nobody can help me maybe you can recommend me what other ai to use, because at this point i am suspecting it might be just an issue with openai api.","['javascript', 'firebase', 'openai-api', 'gpt-3']",77900685,"you have openai node.js sdk v4 or newer.
there are multiple changes in >=v4 compared to <v4, mainly:

initialization
method names
message retrieval

you did the initialization correctly. there are two problems left, plus one additional problem that is not related to the sdk version.

problem 1: incorrect method name
you need to change the method name from this...
openai.createcompletion

...to this.
openai.completions.create


problem 2: incorrect message retrieval
change this...
response.data.choices[0].text

...to this.
response.choices[0].text


additional problem: using a deprecated model
also, the text-davinci-003 has been deprecated. the gpt-3.5-turbo-instruct model is the recommended replacement.",https://stackoverflow.com/questions/77900626,javascript,29-01-2024 14:59,1388.0,0.0,1.0,True,01-02-2024 12:27,01-02-2024 12:27
66127689,dynamically count occurences of multiple words within lists,"i'm trying to count the occurences of multiple keywords within each phrases of a dataframe. this seems similar to other questions but not quite the same.
here we have a df and a list of lists containing keywords/topics:
df=pd.dataframe({'phrases':['very expensive meal near city center','very good meal and waiters','nice restaurant near center and public transport']})

topics=[['expensive','city'],['good','waiters'],['center','transport']]

for each phrase, we want to count how many words match in each separate topic. so the first phrase should score 2 for 1st topic, 0 for 2nd topic and 1 for 3rd topic, etc
i've tried this but it does not work:
from collections import counter
topnum=0
for t in topics:
counts=[]
topnum+=1
results = counter()
for line in df['phrases']:
  for c in line.split(' '):
    results[c] = t.count(c)
  counts.append(sum(results.values()))
df['topic_'+str(topnum)] = counts

i'm not sure what i'm doing wrong, ideally i would end up with a count of matching words for each topic/phrases combinations but instead the counts seem to repeat themselves:
phrases                                            topic_1  topic_2     topic_3
very expensive meal near city centre              2             0           0
very good meal and waiters                        2             2           0
nice restaurant near center and public transport  2             2           2

many thanks to whoever can help me.
best wishes","['python', 'pandas', 'nlp', 'token']",66127993,"here is a solution that defines a helper function called find_count and applies it as a lambda to the dataframe.
import pandas as pd
df=pd.dataframe({'phrases':['very expensive meal near city center','very good meal and waiters','nice restaurant near center and public transport']})
topics=[['expensive','city'],['good','waiters'],['center','transport']]

def find_count(row, topics_index):
    count = 0
    word_list = row['phrases'].split()
    for word in word_list:
        if word in topics[topics_index]:
            count+=1
    return count

df['topic 1'] = df.apply(lambda row:find_count(row,0), axis=1)
df['topic 2'] = df.apply(lambda row:find_count(row,1), axis=1)
df['topic 3'] = df.apply(lambda row:find_count(row,2), axis=1)

print(df)

#output
                                            phrases  topic 1  topic 2  topic 3
0              very expensive meal near city center        2        0        1
1                        very good meal and waiters        0        2        0
2  nice restaurant near center and public transport        0        0        2",https://stackoverflow.com/questions/66127689,python,09-02-2021 21:48,121.0,0.0,1.0,True,09-02-2021 22:11,09-02-2021 22:04
68546867,meaning of output/training status of 256 in stanford nlp ner?,"i have a python program where i am using os.sys to train the stanford ner from the command line. this returns an output/training status which i save in the variable ""status"", and it is usually 0. however, i just ran it and got an output of 256, as well as not creating a file for the trained model. this error is only occurring for larger sets of training data. i searched through the documentation on the stanford nlp website and there doesn't seem to be info on the meanings of the outputs or why increasing training data might affect the training. thanks in advance for any help and problem code is below.
cmdtosys = ""java -mx20g -cp stanford-corenlp-4.2.2.jar edu.stanford.nlp.ie.crf.crfclassifier -prop "" + self.trainpropfilename + "" -ner.usesutime false test -ner.applynumericclassifiers false test ""

status = os.system(cmdtosys)

note: self.trainpropfilename is just the property file","['python', 'nlp', 'stanford-nlp', 'named-entity-recognition']",68554916,"status is an exit code, and non-zero exit codes mean your program failed. this is not a stanford nlp convention, it's how all programs work on unix/linux.
there should be an error somewhere, maybe you ran out of memory? you'll have to track that down to find out what's wrong.",https://stackoverflow.com/questions/68546867,python,27-07-2021 14:34,77.0,2.0,1.0,True,28-07-2021 05:35,27-07-2021 18:12
75614444,openai chat completions api: why do i get null response?,"i am trying to carry out api calls to the newly release gpt-3.5-turbo model and have the following code, which should send a query (via the $query variable) to the api and then extract the content of a responding message from the api.
but i am getting null responses on each call.
any ideas what i have done incorrectly?
$ch = curl_init();

$query = ""what is the capital city of england?"";

$url = '

$api_key = 'sk-**************************************';

$post_fields = [
    ""model"" => ""gpt-3.5-turbo"",
    ""messages"" => [""role"" => ""user"",""content"" => $query],
    ""max_tokens"" => 500,
    ""temperature"" => 0.8
];

$header  = [
    'content-type: application/json',
    'authorization: bearer ' . $api_key
];

curl_setopt($ch, curlopt_url, $url);
curl_setopt($ch, curlopt_returntransfer, 1);
curl_setopt($ch, curlopt_post, 1);
curl_setopt($ch, curlopt_postfields, json_encode($post_fields));
curl_setopt($ch, curlopt_ $header);

$result = curl_exec($ch);
if (curl_errno($ch)) {
    echo 'error: ' . curl_error($ch);
}
curl_close($ch);

$response = json_decode($result);

$response = $response->choices[0]->message[0]->content;","['php', 'curl', 'openai-api', 'chatgpt-api']",75615117,"the reason why you're getting null response is because the json body could not be parsed.
you get the following error: ""we could not parse the json body of your request. (hint: this likely means you aren't using your http library correctly. the openai api expects a json payload, but what was sent was not valid json. if you have trouble figuring out how to fix this, please send an email to support@openai.com and include any relevant code you'd like help with.)"".
change this...
$post_fields = [
    ""model"" => ""gpt-3.5-turbo"",
    ""messages"" => [""role"" => ""user"",""content"" => $query],
    ""max_tokens"" => 12,
    ""temperature"" => 0
];

...to this.
$post_fields = array(
    ""model"" => ""gpt-3.5-turbo"",
    ""messages"" => array(
        array(
            ""role"" => ""user"",
            ""content"" => $query
        )
    ),
    ""max_tokens"" => 12,
    ""temperature"" => 0
);

working example
if you run php test.php in cmd, the openai api will return the following completion:

string(40) ""
the capital city of england is london.""

test.php
<?php
    $ch = curl_init();

    $url = '

    $api_key = 'sk-xxxxxxxxxxxxxxxxxxxx';

    $query = 'what is the capital city of england?';

    $post_fields = array(
        ""model"" => ""gpt-3.5-turbo"",
        ""messages"" => array(
            array(
                ""role"" => ""user"",
                ""content"" => $query
            )
        ),
        ""max_tokens"" => 12,
        ""temperature"" => 0
    );

    $header  = [
        'content-type: application/json',
        'authorization: bearer ' . $api_key
    ];

    curl_setopt($ch, curlopt_url, $url);
    curl_setopt($ch, curlopt_returntransfer, 1);
    curl_setopt($ch, curlopt_post, 1);
    curl_setopt($ch, curlopt_postfields, json_encode($post_fields));
    curl_setopt($ch, curlopt_ $header);

    $result = curl_exec($ch);
    if (curl_errno($ch)) {
        echo 'error: ' . curl_error($ch);
    }
    curl_close($ch);

    $response = json_decode($result);
    var_dump($response->choices[0]->message->content);
?>",https://stackoverflow.com/questions/75614444,php,02-03-2023 10:51,8606.0,1.0,1.0,True,12-06-2024 17:29,12-06-2024 17:29
77728888,return arguments from function calling with openai api when streaming?,"i've made a simple openai api example with function calling. i'm only using function calling to format the response, i'm not calling multiple functions or any external apis.
when i don't stream the response i can return the function arguments, which is the data that i need.
in my nextjs route handler:
export async function post(request: request) {
  try {
    const openai = new openai({
      apikey: process.env[""openai_api_key""],
    });
    const response = await openai.chat.completions.create({
      model: ""gpt-4"",
      // stream: true,
      messages: [
        {
          role: ""user"",
          content: ""give me 5 questions and answers for a pub quiz"",
        },
      ],
      tools: [
        {
          type: ""function"",
          function: {
            name: ""get_questions_and_answers"",
            description: ""get questions and answers"",
            parameters: simplejsonschema,
          },
        },
      ],
      tool_choice: {
        type: ""function"",
        function: { name: ""get_questions_and_answers"" },
      },
    });
    return response.json(
       json.parse(
         response.choices[0].message.tool_calls?.[0].function.arguments || """",
       ),
    );
  } catch (servererror) {
    console.error({ servererror });
    throw new error();
  }
}

simplejsonschema.json:
{
  ""type"": ""object"",
  ""properties"": {
    ""getquestions"": {
      ""type"": ""array"",
      ""items"": {
        ""type"": ""object"",
        ""properties"": {
          ""question"": {""type"": ""string""},
          ""answer"": {""type"": ""string""}
        },
        ""required"": [""question"", ""answer""]
      }
    }
  },
  ""required"": [""getquestions""]
}

response from api:
{""getquestions"":[{""question"":""what is the capital of australia?"",""answer"":""canberra""},{""question"":""who wrote 'to kill a mockingbird'?"",""answer"":""harper lee""},{""question"":""what is the highest peak in the world?"",""answer"":""mount everest""},{""question"":""who is known as the 'father of computers'?"",""answer"":""charles babbage""},{""question"":""what is the largest ocean in the world?"",""answer"":""pacific ocean""}]}

this is fine when developing locally, however when deployed to vercel the request sometimes times out. i've tried to add streaming as this is the recommended solution:
const response = await openai.chat.completions.create({
  model: ""gpt-4"",
  stream: true,
  messages: [
    {
      role: ""user"",
      content: ""give me 5 questions and answers for a pub quiz"",
    },
  ],
  tools: [
    {
      type: ""function"",
      function: {
        name: ""get_questions_and_answers"",
        description: ""get questions and answers"",
        parameters: simplejsonschema,
      },
    },
  ],
  tool_choice: {
    type: ""function"",
    function: { name: ""get_questions_and_answers"" },
  },
});

const stream = openaistream(response);
return new streamingtextresponse(stream);

however now the response has a lot of unnecessary data. and when i try to json.parse on the client i get errors.
response from api:
{""tool_calls"":[ {""id"": ""call_ihxvzkz5esmzphc6tozntmzb"", ""type"": ""function"", ""function"": {""name"": ""get_questions_and_answers"", ""arguments"": ""{\n  \""getquestions\"": [\n    {\n      \""question\"": \""question 1\"",\n      \""answer\"": \""answer 1\""\n    },\n    {\n      \""question\"": \""question 2\"",\n      \""answer\"": \""answer 2\""\n    },\n    {\n      \""question\"": \""question 3\"",\n      \""answer\"": \""answer 3\""\n    },\n    {\n      \""question\"": \""question 4\"",\n      \""answer\"": \""answer 4\""\n    },\n    {\n      \""question\"": \""question 5\"",\n      \""answer\"": \""answer 5\""\n    }\n  ]\n}""}}

as far as i can see the docs only cover using usechat but i have some particular requirements so i need to handle the fetching and form state myself: 
why am i getting invalid json?
here is a repository which reproduces the issue:","['json', 'next.js', 'openai-api']",77741254,"this is the response you are getting:
{""tool_calls"":[ {""id"": ""call_hrxqlp3yzehson43tmyzjmlr"", ""type"": ""function"", ""function"": {""name"": ""get_questions_and_answers"", ""arguments"": ""{\n  \""getquestions\"": [\n    {\n      \""question\"": \""what is the capital city of france?\"",\n      \""answer\"": \""paris\""\n    },\n    {\n      \""question\"": \""who painted the mona lisa?\"",\n      \""answer\"": \""leonardo da vinci\""\n    },\n    {\n      \""question\"": \""what is the largest planet in our solar system?\"",\n      \""answer\"": \""jupiter\""\n    },\n    {\n      \""question\"": \""what is the national flower of england?\"",\n      \""answer\"": \""rose\""\n    },\n    {\n      \""question\"": \""which country is famous for its tulips?\"",\n      \""answer\"": \""netherlands\""\n    }\n  ]\n}""}}

i used  to auto correct the json and it just adds ""]}"". for some reason openai is not sending correct json response. you have to add it
accumulatedtext += ""]}"";

then response works:

this is too specific error. if openai updates its response api, it might send the json data correctly. so a better approach would be parsing in try/catch
try {
      const parsed = json.parse(accumulatedtext);
      console.log({ parsed });
    } catch (error) {
      // you should error for each specific case
      accumulatedtext += ""]}"";
      console.log(""correct accumulatedtext in catch block"", accumulatedtext);
    }",https://stackoverflow.com/questions/77728888,json,28-12-2023 19:51,1733.0,0.0,4.0,True,29-08-2024 21:18,31-12-2023 09:30
72369203,why are these dots not equal in python?,"i am working on texts and have the left dot from an input text and right dot typed from a keyboard. however, in python, they are not being treated as equal.
'ï¿½ï¿½ï¿½' == '.'
out[870]: false

what could be a possible reason, and how can i recreate the left dot using the keyboard","['python', 'python-3.x', 'string', 'text', 'nlp']",72369251,"the dot on the left is not a period: it is a one dot leader unicode character.
in python, you can print it by using ""\u2024"":
print('\u2024')

this outputs:
ï¿½ï¿½ï¿½

you can use this for comparison purposes as well. if you do:
print('ï¿½ï¿½ï¿½' == '\u2024')

it will output
true",https://stackoverflow.com/questions/72369203,python,24-05-2022 20:39,111.0,2.0,3.0,True,24-05-2022 22:29,24-05-2022 22:29
69249187,how to test a model before fine-tuning in pytorch lightning?,"doing things on google colab.

transformers: 4.10.2
pytorch-lightning: 1.2.7

import torch
from torch.utils.data import dataloader
from transformers import bertjapanesetokenizer, bertforsequenceclassification
import pytorch_lightning as pl

dataset_for_loader = [
    {'data':torch.tensor([0,1]), 'labels':torch.tensor(0)},
    {'data':torch.tensor([2,3]), 'labels':torch.tensor(1)},
    {'data':torch.tensor([4,5]), 'labels':torch.tensor(2)},
    {'data':torch.tensor([6,7]), 'labels':torch.tensor(3)},
]
loader = dataloader(dataset_for_loader, batch_size=2)

for idx, batch in enumerate(loader):
    print(f'# batch {idx}')
    print(batch)

category_list = [
    'dokujo-tsushin',
    'it-life-hack',
    'kaden-channel',
    'livedoor-homme',
    'movie-enter',
    'peachy',
    'smax',
    'sports-watch',
    'topic-news'
]

tokenizer = bertjapanesetokenizer.from_pretrained(model_name)

max_length = 128
dataset_for_loader = []
for label, category in enumerate(tqdm(category_list)):
    # file ./text has lots of articles, categorized by category
    # and they are just plain texts, whose content begins from forth line
    for file in glob.glob(f'./text/{category}/{category}*'):
        lines = open(file).read().splitlines()
        text = '\n'.join(lines[3:])
        encoding = tokenizer(
            text,
            max_length=max_length, 
            padding='max_length',
            truncation=true
        )
        encoding['labels'] = label
        encoding = { k: torch.tensor(v) for k, v in encoding.items() }
        dataset_for_loader.append(encoding)

seed=lambda:0.0

# random.shuffle(dataset_for_loader) # ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
random.shuffle(dataset_for_loader,seed)
n = len(dataset_for_loader)
n_train = int(0.6*n)
n_val = int(0.2*n)
dataset_train = dataset_for_loader[:n_train]
dataset_val = dataset_for_loader[n_train:n_train+n_val]
dataset_test = dataset_for_loader[n_train+n_val:]

dataloader_train = dataloader(
    dataset_train, batch_size=32, shuffle=true    
) 
dataloader_val = dataloader(dataset_val, batch_ize=256)

class bertforsequenceclassification_pl(pl.lightningmodule):
    def __init__(self, model_name, num_labels, lr):
        super().__init__()
        self.save_hyperparameters()
        self.bert_sc = bertforsequenceclassification.from_pretrained(
            model_name,
            num_labels=num_labels
        )

    def training_step(self, batch, batch_idx):
        output = self.bert_sc(**batch)
        loss = output.loss
        self.log('train_loss', loss)
        return loss

    def validation_step(self, batch, batch_idx):
        output = self.bert_sc(**batch)
        val_loss = output.loss
        self.log('val_loss', val_loss)

    def test_step(self, batch, batch_idx):
        labels = batch.pop('labels')
        output = self.bert_sc(**batch)
        labels_predicted = output.logits.argmax(-1)
        num_correct = ( labels_predicted == labels ).sum().item()
        accuracy = num_correct/labels.size(0)
        self.log('accuracy', accuracy)

    def configure_optimizers(self):
        return torch.optim.adam(self.parameters(), lr=self.hparams.lr)

checkpoint = pl.callbacks.modelcheckpoint(
    monitor='val_loss',
    mode='min',
    save_top_k=1,
    save_weights_only=true,
    dirpath='model/',
)
trainer = pl.trainer(
    gpus=1,
    max_epochs=10,
    callbacks = [checkpoint]
)

model = bertforsequenceclassification_pl(
    model_name, num_labels=9, lr=1e-5
)

### (a) ###

# i think this is where i am doing fine-tuning
trainer.fit(model, dataloader_train, dataloader_val)

# this is to score after fine-tuning
test = trainer.test(test_dataloaders=dataloader_test)
print(f'accuracy: {test[0][""accuracy""]:.2f}')

but i am not really sure how to do a test before fine-tuning, in order to compare two models before and after fine-tuning, in order to show how effective fine-tuning is.
inserting the following two lines to ### (a) ###:
test = trainer.test(test_dataloaders=dataloader_test)
print(f'accuracy: {test[0][""accuracy""]:.2f}')

i got this result:
---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-13-c8b2c67f2d5c> in <module>()
      9 
     10 # 6-19
---> 11 test = trainer.test(test_dataloaders=dataloader_test)
     12 print(f'accuracy: {test[0][""accuracy""]:.2f}')
     13 

/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in test(self, model, test_dataloaders, ckpt_path, verbose, datamodule)
    896         self.verbose_test = verbose
    897 
--> 898         self._set_running_stage(runningstage.testing, model or self.lightning_module)
    899 
    900         # if you supply a datamodule you can't supply train_dataloader or val_dataloaders

/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in _set_running_stage(self, stage, model_ref)
    563         the trainer and the model
    564         """"""
--> 565         model_ref.running_stage = stage
    566         self._running_stage = stage
    567 

attributeerror: 'nonetype' object has no attribute 'running_stage'

i noticed that trainer.fit() can take none as arguments other than model, so i tried this:
trainer.fit(model)
test=trainer.test(test_dataloaders=dataloader_test)
print(f'accuracy: {test[0][""accuracy""]:.2f}')

the result:
misconfigurationexception: no `train_dataloader()` method defined. lightning `trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.

thanks.","['huggingface-transformers', 'pytorch-lightning']",69254313,"the trainer needs to call its .fit() in order to set up a lot of things and then only you can do .test() or other methods.
you are right about putting a .fit() just before .test() but the fit call needs to a valid one. you have to feed a dataloader/datamodule to it. but since you don't want to do a training/validation in this fit call, just pass limit_[train/val]_batches=0 while trainer construction.
trainer = trainer(gpus=..., ..., limit_train_batches=0, limit_val_batches=0)
trainer.fit(model, dataloader_train, dataloader_val)
trainer.test(model, dataloader_test) # without fine-tuning

the fit call here will just set things up for you and skip training/validation. and then the testing follows. next time run the same code but without the limit_[train/val]_batches, this will do the pretraining for you
trainer = trainer(gpus=..., ...)
trainer.fit(model, dataloader_train, dataloader_val)
trainer.test(model, dataloader_test) # with fine-tuning


clarifying a bit about .fit() taking none for all but model: its not quite true - you must provide either a dataloader or a datamodule.",https://stackoverflow.com/questions/69249187,huggingface-transformers,20-09-2021 04:44,6752.0,1.0,1.0,True,20-09-2021 13:25,20-09-2021 05:21
66896467,removing stopwords from a string with ordered set and join retains a single stopword,"i don't understand why i don't remove the stopword ""a"" in this loop. it seems so obvious that this should work...
given a list of stop words, write a function that takes a string and returns a string stripped of the stop words. output: stripped_paragraph = 'want figure out how can better data scientist'

below i define 'stopwords'

i split all the words by a space, make a set of words while retaining the order

loop through the ordered and split substring set ('osss' var) and conditionally remove each word if it's a word in the list 'stopwords'
paragraph = 'i want to figure out how i can be a better data scientist'
def rm_stopwards(par):
    stopwords = ['i', 'as', 'to', 'you', 'your','but','be', 'a']
    osss = list(list(dict.fromkeys(par.split(' ')))) # ordered_split_shortened_set
    for word in osss:
        if word.strip() in stopwords:
            osss.remove(word)
        else:
            next
    return ' '.join(osss)
print(""stripped_paragraph = ""+""'""+(rm_stopwards(paragraph))+""'"")



my incorrect output is: 'want figure out how can a better data scientist'
correct output: 'want figure out how can better data scientist'
edit: note that .strip() in the condition check with word.strip() is unnecessary and i still get the same output, that was me checking to make sure there wasn't an extra space somehow
edit2: this is an interview question, so i can't use any imports","['python', 'nlp']",66896663,"paragraph = 'i want to figure out how i can be a better data scientist'

def rm_stopwards(par):
 stopwords = ['i', 'as', 'to', 'you', 'your','but','be', 'a']
 osss = list(list(dict.fromkeys(par.split(' ')))) # ordered_split_shortened_set
 x = list(osss)
 for word in osss:
    if word.strip() in stopwords:
        x.remove(word)
    #else:
    #    next
 ret = ' '.join(x)
 return ret

print(""stripped_paragraph = ""+""'""+(rm_stopwards(paragraph))+""'"")",https://stackoverflow.com/questions/66896467,python,31-03-2021 23:15,910.0,0.0,3.0,True,31-03-2021 23:42,31-03-2021 23:40
77879635,how to reset parameters from automodelforsequenceclassification?,"currently to reinitialize a model for automodelforsequenceclassification, we can do this:
from transformers import automodel, autoconfig, automodelforsequenceclassification

m = ""moussakam/frugalscore_tiny_bert-base_bert-score""
config = autoconfig.from_pretrained(m)
model_from_scratch = automodel(config)

model_from_scratch.save_pretrained(""frugalscore_tiny_bert-from_scratch"")

model = automodelforsequenceclassification(
  ""frugalscore_tiny_bert-from_scratch"", local_files_only=true
)

is there some way to reinitialize the model weights without saving a new pretrained model initialized with autoconfig?
model = automodelforsequenceclassification(
  ""moussakam/frugalscore_tiny_bert-base_bert-score"", 
  local_files_only=true
  reinitialize_weights=true
)

or something like:
model = automodelforsequenceclassification(
  ""moussakam/frugalscore_tiny_bert-base_bert-score"", 
  local_files_only=true
)

model.reinitialize_parameters()","['python', 'machine-learning', 'huggingface-transformers', 'text-classification', 'safe-tensors']",77879847,"that is the purpose of from_config (i.e. creating a model but not loading the respective weights):
from transformers import automodel, autoconfig, automodelforsequenceclassification

m = ""moussakam/frugalscore_tiny_bert-base_bert-score""
config = autoconfig.from_pretrained(m)

no_weights_model = automodelforsequenceclassification.from_config(config)
weights_model = automodelforsequenceclassification.from_pretrained(m)

import torch

print(torch.allclose(no_weights_model.bert.embeddings.word_embeddings.weight,weights_model.bert.embeddings.word_embeddings.weight))

output:
false",https://stackoverflow.com/questions/77879635,python,25-01-2024 11:34,357.0,1.0,1.0,True,25-01-2024 12:26,25-01-2024 12:26
67458203,how do i retain numbers while preprocessing data using gensim in python?,"i have used gensim.utils.simple_preprocess(str(sentence) to create a dictionary of words that i want to use for topic modelling. however, this is also filtering important numbers (house resolutions, bill no, etc) that i really need. how did i overcome this? possibly by replacing digits with their word form. how do i go about it, though?","['nlp', 'gensim', 'preprocessor', 'lda', 'latent-semantic-analysis']",67467103,"you don't have to use simple_preprocess() - it's not doing much, it's not that configurable or sophisticated, and typically the other gensim algorithms just need lists-of-tokens.
so, choose your own tokenization - which in some cases, depnding on your source data, could be as simple as a .split() on whitespace.
if you want to look at what simple_preprocess() does, as a model, you can view its python source at:",https://stackoverflow.com/questions/67458203,nlp,09-05-2021 13:21,733.0,1.0,1.0,True,10-05-2021 08:21,09-05-2021 13:46
64634027,how to verify if two text datasets are from different distribution?,"i have two text datasets. each dataset consists of multiple sequences and each sequence can have more than one sentence.
how do i measure if both datasets are from same distribution?
the purpose is to verify transfer learning from one distribution to another only if the difference between the distributions is statistically significant.
i am panning to use chi-square test but not sure if it will help for text data considering the high degrees of freedom.
update:
example:
supppose i want to train a sentiment classification model. i train a model on imdb dataset and evaluate on imdb and yelp datasets. i found that my model trained on imdb still does well on yelp. but the question is how different these datasets are?
train dataset : 
eval 1: 
eval 2: 
now,

how different are train and eval 1?
how different are train and eval 2?
is the dissimilarity between train and eval 2 by chance ? what is the statistical significance and p value?","['machine-learning', 'nlp', 'statistics', 'data-analysis', 'chi-squared']",64698436,"the question ""are text a and text b coming from the same distribution?"" is somehow poorly defined.  for example, these two questions (1,2) can be viewed as generated from the same distribution (distribution of all questions on stackexchange) or from different distributions (distribution of two different subdomains of stackexchange). so it's not clear what is the property that you want to test.
anyway, you can come up with any test statistic of your choice, approximate its distribution in case of ""single source"" by simulation, and calculate the p-value of your test.
as a toy example, let's take two small corpora: two random articles from english wikipedia. i'll do it in python
import requests
from bs4 import beautifulsoup
urls = [
    ' 
    '
]
texts = [beautifulsoup(requests.get(u).text).find('div', {'class': 'mw-parser-output'}).text for u in urls]

now i use a primitive tokenizer to count individual words in texts, and use root mean squared difference in word relative frequencies as my test statistic. you can use any other statistic, as long as you calculate it consistently.
import re
from collections import counter
from copy import deepcopy
token = re.compile(r'([^\w\d]+|\d+|[^\w\s])')
counters = [counter(re.findall(token, t)) for t in texts]
print([sum(c.values()) for c in counters])  
# [5068, 4053]: texts are of approximately the same size

def word_freq_rmse(c1, c2):
    result = 0
    vocab = set(c1.keys()).union(set(c2.keys()))
    n1, n2 = sum(c1.values()), sum(c2.values())
    n = len(vocab)
    for word in vocab:
        result += (c1[word]/n1 - c2[word]/n2)**2 / n
    return result**0.5

print(word_freq_rmse(*counters))
# rmse is 0.001178, but is this a small or large difference?

i get a value of 0.001178, but i don't know whether it's a large difference. so i need to simulate the distribution of this test statistic under the null hypothesis: when both texts are from the same distribution. to simulate it, i merge two texts into one, and then split them randomly, and calculate my statistic when comparing these two random parts.
import random
tokens = [tok for t in texts for tok in re.findall(token, t)]
split = sum(counters[0].values())
distribution = []
for i in range(1000):
    random.shuffle(tokens)
    c1 = counter(tokens[:split])
    c2 = counter(tokens[split:])
    distribution.append(word_freq_rmse(c1, c2))

now i can see how unusual is the value of my observed test statistic under the null hypothesis:
observed = word_freq_rmse(*counters)
p_value = sum(x >= observed for x in distribution) / len(distribution)
print(p_value)  # it is 0.0
print(observed, max(distribution), sum(distribution) / len(distribution)) # 0.0011  0.0006 0.0004

we see that when texts are from the same distribution, my test statistic is on average 0.0004 and almost never exceeds 0.0006, so the value of 0.0011 is very unusual, and the null hypothesis that two my texts originate from the same distribution should be rejected.",https://stackoverflow.com/questions/64634027,machine-learning,01-11-2020 16:14,2646.0,5.0,2.0,True,30-03-2022 09:54,03-11-2020 03:40
69191305,how to add new special token to the tokenizer?,"i want to build a multi-class classification model for which i have conversational data as input for the bert model (using bert-base-uncased).

query: i want to ask a question.
answer: sure, ask away.
query: how is the weather today?
answer: it is nice and sunny.
query: okay, nice to know.
answer: would you like to know anything else?

apart from this i have two more inputs.
i was wondering if i should put special token in the conversation to make it more meaning to the bert model, like:

[cls]query: i want to ask a question.  [eot]
answer: sure, ask away.  [eot]
query: how is the weather today?  [eot]
answer: it is nice and sunny.  [eot]
query: okay, nice to know.  [eot]
answer: would you like to know anything else?  [sep]

but i am not able to add a new [eot] special token.
or should i use [sep] token for this?
edit: steps to reproduce
from transformers import autotokenizer, automodelforsequenceclassification
tokenizer = autotokenizer.from_pretrained(""bert-base-uncased"")
model = automodelforsequenceclassification.from_pretrained(""bert-base-uncased"")

print(tokenizer.all_special_tokens) # --> ['[unk]', '[sep]', '[pad]', '[cls]', '[mask]']
print(tokenizer.all_special_ids)    # --> [100, 102, 0, 101, 103]

num_added_toks = tokenizer.add_tokens(['[eot]'])
model.resize_token_embeddings(len(tokenizer))  # --> embedding(30523, 768)

tokenizer.convert_tokens_to_ids('[eot]')  # --> 30522

text_to_encode = '''query: i want to ask a question. [eot]
answer: sure, ask away. [eot]
query: how is the weather today? [eot]
answer: it is nice and sunny. [eot]
query: okay, nice to know. [eot]
answer: would you like to know anything else?'''

enc = tokenizer.encode_plus(
  text_to_encode,
  max_length=128,
  add_special_tokens=true,
  return_token_type_ids=false,
  return_attention_mask=false,
)['input_ids']

print(tokenizer.convert_ids_to_tokens(enc))

result:

['[cls]', 'query', ':', 'i', 'want', 'to', 'ask', 'a', 'question',
'.', '[', 'e', '##ot', ']', 'answer', ':', 'sure', ',', 'ask', 'away',
'.', '[', 'e', '##ot', ']', 'query', ':', 'how', 'is', 'the',
'weather', 'today', '?', '[', 'e', '##ot', ']', 'answer', ':', 'it',
'is', 'nice', 'and', 'sunny', '.', '[', 'e', '##ot', ']', 'query',
':', 'okay', ',', 'nice', 'to', 'know', '.', '[', 'e', '##ot', ']',
'answer', ':', 'would', 'you', 'like', 'to', 'know', 'anything',
'else', '?', '[sep]']","['bert-language-model', 'huggingface-tokenizers', 'sentencepiece']",69194717,"as the intention of the [sep] token was to act as a separator between two sentence, it fits your objective of using [sep] token to separate sequences of query and answer.
you also try to add different tokens to mark the beginning and end of query or answer as <boq> and <eoq> to mark the beginning and end of query. likewise, <boa> and <eoa> to mark the beginning and end of answer.
sometimes, using the existing token works much better than adding new tokens to the vocabulary, as it requires huge number of training iterations as well as the data to learn the new token embedding.
however, if you want to add a new token if your application demands so, then it can be added as follows:
num_added_toks = tokenizer.add_tokens(['[eot]'], special_tokens=true) ##this line is updated
model.resize_token_embeddings(len(tokenizer))

###the tokenizer has to be saved if it has to be reused
tokenizer.save_pretrained(<output_dir>)",https://stackoverflow.com/questions/69191305,bert-language-model,15-09-2021 10:24,30834.0,12.0,2.0,True,14-06-2023 08:57,15-09-2021 19:37
77082292,how to view vespa embedding?,"i tried the following block of code to implement nearest neighbor search algorithm in vespa.

i was able to run it successfully but was unable to identify where this vector db/embedding is getting saved.
i want to query the embedding column, which i was unable to see, however i am able to see the title, track and other columns.
![here i was able to view title and artist (","['nearest-neighbor', 'word-embedding', 'vespa', 'vector-database', 'semantic-search']",77082705,"just add ""summary"" to the indexing statement in your schema:
field embedding type tensor<float>(x[384]) {
        indexing: input title | embed e5 | attribute | index | summary

you can configure different sets of fields to be returned for different queries by configuring multiple document summaries.",https://stackoverflow.com/questions/77082292,nearest-neighbor,11-09-2023 13:59,218.0,0.0,1.0,True,11-09-2023 14:54,11-09-2023 14:08
71486242,attributeerror: &#39;t5config&#39; object has no attribute &#39;adapters&#39;,"how to solve this error? i've created the .pkl object of the t5-base model and tried to execute it but suddenly i got this error message. i wondered a bit, tried to google it but didn't get any reason why i got this error!!!!","['nlp', 'pytorch', 'huggingface-transformers', 'summarization']",71492751,"well, i was thinking right!!!
i did 2 experiments :

install only transformers library :
when i load the model, each layer of the model was without an adapter attribute!!

install only adapter-transformers library :
when i load the model, each layer of the model was with an adapter attribute!!


conclusion : install adapter-transformers",https://stackoverflow.com/questions/71486242,nlp,15-03-2022 17:05,1346.0,0.0,1.0,True,16-03-2022 07:11,16-03-2022 07:11
62626459,spacy: automatically find lemma patterns in text,"i'm learning how to use spacy.
based on the example below, my goal is to get more patterns of lemmas that are often associated with the word iphone (i have a text database where such patterns can be found).
for example ""iphone is the best smartphone"", ""iphone is too expensive"", etc.
do i need to find those patterns by hand. or is it possible to make this automatic (at least to get suggestions or something like that).
my final goal is to build a tool that would take as input some text and based on those patterns identify iphone, samsung abc, etc...
import spacy
from spacy.matcher import matcher

nlp = spacy.load(""en_core_web_sm"")
matcher = matcher(nlp.vocab)

pattern1 = [{""lemma"": ""buy""}, {""noun"": ""iphone""}]
matcher.add(""buy_iphone"", none, pattern1)

doc = nlp(""i'm gonna buy an iphone"")

print(doc)","['nlp', 'pattern-matching', 'spacy']",62640591,"i am not sure matcher is the right choise here, because it matches exact phrases. but in your example, there is an additional article. instead you could do something like this to get verb-phone pairs:
import spacy
nlp = spacy.load(""en_core_web_sm"")

#extracts verb-phone pairs, given verbs and phones
def extract_verbs(doc, verb_lemmas, phones):
    results = []
    verbs = [verb for verb in doc if verb.lemma_ in verb_lemmas]
    for verb in verbs:
        for child in verb.children:
            if child.lower_ in phones:
                results.append((verb.lemma_,child.text))
    return results

#extracts verb-phone pairs given phones
def extract_phones(doc, phones):
    results = []
    phones = [phone for phone in doc if phone.lower_ in phones]
    for phone in phones:
        results.append((phone.head.lemma_,phone.text))
    return results

doc = nlp(""i'm gonna buy an iphone. samsung sucks. i always wanted an iphone, but i just got a samsung."")
verb_lemmas = [""buy""]
phones = [""samsung"", ""iphone""]
print(extract_verbs(doc, verb_lemmas, phones)) #returns [('buy', 'iphone')]
print(extract_phones(doc,  phones)) #returns [('buy', 'iphone'), ('suck', 'samsung'), ('want', 'iphone'), ('get', 'samsung')]",https://stackoverflow.com/questions/62626459,nlp,28-06-2020 18:11,607.0,0.0,2.0,True,17-03-2022 21:29,29-06-2020 07:26
72885859,pandas find multiple words from a list and assign boolean value if found,"so, i have dataframe like this,
data = {
  ""properties"": [""financialoffice"",""gas station"", ""office"", ""k-12 school"", ""commercial, office""],
}
df = pd.dataframe(data)

this is my list,
proplist = [""office"",""other - mall"",""gym""]

what i am trying to do is using the list i am trying to find out which words exactly matches with the dataframe column and for each word from the dataframe i need to assign a boolean true/false value or 0/1. it has to be a exact match.
output like this,
properties         flag
financialoffice    false
gas station        false
office             true
k-12 school        false
commercial, office true

so, it returns true for only ""office"" because it is the exact match from the list. financialoffice is not because it is not in the list. also, for the last one commercial, office it is true because office is found in the list even though commercial not. so, even one of them is present it will be true.
df[""flag""] = df[""properties""].isin(proplist)

above code works fine to assign a boolean true/false but it returns false for the last one(commercial,office) as it tries to find the exact match.
any help is appreciated.","['python', 'pandas', 'regex', 'dataframe', 'text-mining']",72885953,"use a crafted regex with word delimiter:
import re

regex = r'\b(?:%s)\b' % '|'.join(map(re.escape, proplist))
# '\\b(?:office|other\\ \\-\\ mall|gym)\\b'

df['flag'] = df['properties'].str.contains(regex, regex=true)
# for a case insensitive match add the case=false parameter

output:
           properties   flag
0     financialoffice  false
1         gas station  false
2              office   true
3         k-12 school  false
4  commercial, office   true",https://stackoverflow.com/questions/72885859,python,06-07-2022 15:03,426.0,-2.0,3.0,True,28-08-2022 12:04,28-08-2022 12:04
77248199,how to convert doccano exported jsonl format to spacy formatï¿½,"i want to use my own data set to train a named entity recognition model. the data set is exported by the annotation tool doccano. the format is jsonl (not json), but spacy does not support such a data format input model. how do i convert it? ?
here's what my dataset looks like:
{""id"":17,""text"":""in this work, the effect of cfs with zeolite on the mechanical, tribological prop-erties, and structure of ptfe was investigated. \nthe developed materials with a cf content of 1ï¿½ï¿½ï¿½5 wt.% retained their deformation and strength properties at the level of the initial polymer. \nthe compressive stress of pcm increased by 7ï¿½ï¿½ï¿½53%, and the yield point by 30% relative to the initial polymer. \nit was found that with an increase in the content of fillers, the degree of crystallinity increased, and the density decreased in comparison with unfilled ptfe. \ncombining fillers (cf\/zt) into ptfe reduced the wear rate by 810 times relative to the initial polymer. tribochemicalere shown by ir spectroscopy.\nsem established the formation of secondary structures in the form of tribofilms on the friction surface, which, together with cfs, protect the surface layer of the material from destruction during friction. \nthe wear resistance of the composite material ptfe\/cf\/zt was effectively improved, and the coefficient of friction was low compared to ptfe\/cf\/kl and ptfe\/cf\/vl."",""entities"":[{""id"":298,""label"":""composite"",""start_offset"":1049,""end_offset"":1059},{""id"":545,""label"":""composite"",""start_offset"":960,""end_offset"":971},{""id"":299,""label"":""composite"",""start_offset"":1064,""end_offset"":1074},{""id"":607,""label"":""value"",""start_offset"":176,""end_offset"":184}],""relations"":[],""comments"":[]}

i have also tried many online methods, but none of them seem to work.","['nlp', 'spacy', 'named-entity-recognition', 'doccano']",77275727,"you can modify the script given at explosion/projects/pipelines/ner_demo/scripts
/convert.py:
import json
import warnings

import spacy
from spacy.tokens import docbin

def read_jsonl(fpath):
    with open(fpath, ""r"") as f:
        for line in f:
            yield json.loads(line)


nlp = spacy.blank(""en"")
doc_bin = docbin()
docs = []
for data in read_jsonl(""data.jsonl""):
    doc = nlp.make_doc(data[""text""])
    ents = []
    for entity in data[""entities""]:
        start = entity[""start_offset""]
        end = entity[""end_offset""]
        label = entity[""label""]
        span = doc.char_span(
            start_idx=start,
            end_idx=end,
            label=label,
            alignment_mode=""strict"",
        )
        if span is none:
            msg = (
                f""skipping entity [{start}, {end}, {label}] in the ""
                ""following text because the character span ""
                ""'{doc.text[start:end]}' does not align with token ""
                ""boundaries:\n\n{repr(text)}\n""
            )
            warnings.warn(msg)
        else:
            ents.append(span)
    doc.set_ents(entities=ents)
    doc_bin.add(doc)

doc_bin.to_disk(""train.spacy"")

the data format you have given looks a bit different from the doccano format that i'm used to, but the above should work.",https://stackoverflow.com/questions/77248199,nlp,07-10-2023 02:55,841.0,0.0,2.0,True,09-04-2024 08:13,06-01-2024 22:46
63481527,tensorflow valueerror: failed to convert a numpy array to a tensor (unsupported object type list),"i'm trying to write this code into colab. interestingly, i was running the same code in colab a few days ago but now it won't work. the code also works in kaggle kernel. i tried changing the tensorflow version but all of them give different errors. why do you think i can't run this code? this is the colab notebook if you needed more info.
thanks in advance!
class disasterdetector:
def __init__(self, tokenizer, bert_layer, max_len =30, lr = 0.0001,
             epochs = 15, batch_size = 32, dtype = tf.int32 ,
             activation = 'sigmoid', optimizer = 'sgd',
             beta_1=0.9, beta_2=0.999, epsilon=1e-07,
             metrics = 'accuracy', loss = 'binary_crossentropy'):
    
    self.lr = lr
    self.epochs = epochs
    self.max_len = max_len
    self.batch_size = batch_size
    self.tokenizer = tokenizer
    self.bert_layer = bert_layer
    self.models = []

    self.activation = activation
    self.optimizer = optimizer
    self.dtype = dtype
    
    self.beta_1 = beta_1
    self.beta_2 = beta_2
    self.epsilon =epsilon
    
    self.metrics = metrics
    self.loss = loss
    
def encode(self, texts):
    all_tokens = []
    masks = []
    segments = []
    
    for text in texts:
        
        tokenized = self.tokenizer.convert_tokens_to_ids(['[cls]'] + self.tokenizer.tokenize(text) + ['[sep]'])
        
        len_zeros = self.max_len - len(tokenized)
        
        
        padded = tokenized + [0] * len_zeros
        mask = [1] * len(tokenized) + [0] * len_zeros
        segment = [0] * self.max_len
        
        all_tokens.append(padded)
        masks.append(mask)
        segments.append(segment)
        
    print(len(all_tokens[0]))
    return np.array(all_tokens), np.array(masks), np.array(segments)
    
def make_model(self):
    

    input_word_ids = input(shape = (self.max_len, ), dtype=tf.int32,
                        name = 'input_word_ids')
    
    input_mask = input(shape = (self.max_len, ), dtype=tf.int32,
                       name = 'input_mask')
    
    segment_ids = input(shape = (self.max_len, ), dtype=tf.int32,
                        name = 'segment_ids')


    #pooled output is the output of dimention and

    pooled_output, sequence_output = self.bert_layer([input_word_ids,
                                                 input_mask,
                                                 segment_ids])

    clf_output = sequence_output[:, 0, :]
    out = tf.keras.layers.dense(1, activation = self.activation)(clf_output)
    #out = tf.keras.layers.dense(1, activation = 'sigmoid', input_shape =  (clf_output,) )(clf_output)
    

    model = model(inputs = [input_word_ids, input_mask, segment_ids],
                  outputs = out)
    if self.optimizer is 'sgd':
        optimizer = sgd(learning_rate = self.lr)

    elif self.optimizer is 'adam': 
        optimizer = adam(learning_rate = self.lr, beta_1=self.beta_1,
                         beta_2=self.beta_2, epsilon=self.epsilon)

    model.compile(loss = self.loss, optimizer = self.optimizer,
                  metrics = [self.metrics])
    
    return model




def train(self, x, k = 3):    
    kfold = stratifiedkfold(n_splits = k, shuffle = true)


    for fold, (train_idx, val_idx) in enumerate(kfold.split(x['cleaned_text'], x['target'])):
        print('fold: ', fold)

        x_trn = self.encode(x.loc[train_idx, 'cleaned_text'])
        x_val = self.encode(x.loc[val_idx, 'cleaned_text'])
        y_trn = np.array(x.loc[train_idx, 'target'], dtype = np.uint8)
        y_val = np.array(x.loc[val_idx, 'target'], dtype = np.uint8)
        print('the data type of y train: ', type(y_trn))
        print('x_val shape', x_val[0].shape)
        print('x_trn shape', x_trn[0].shape)
        
        model = self.make_model()
        print('model made.')
        model.fit(x_trn, tf.convert_to_tensor(y_trn),
                validation_data = (x_val, tf.convert_to_tensor(y_val)),
                batch_size=self.batch_size, epochs = self.epochs)

        self.models.append(model)

and after calling the train function of the class i get that error.
classifier = disasterdetector(tokenizer = tokenizer, bert_layer = bert_layer, max_len = max_len, lr = 0.0001,
                  epochs = 10,  activation = 'sigmoid',
                batch_size = 32,optimizer = 'sgd',
                beta_1=0.9, beta_2=0.999, epsilon=1e-07)
classifier.train(train_cleaned)

and here is the error:
valueerror                                traceback (most 

recent call last)
<ipython-input-10-106c756f2e47> in <module>()
----> 1 classifier.train(train_cleaned)

8 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum
     97   ctx.ensure_initialized()
---> 98   return ops.eagertensor(value, ctx.device_name, dtype)
     99 
    100 

valueerror: failed to convert a numpy array to a tensor (unsupported object type list).","['python', 'tensorflow', 'valueerror', 'bert-language-model']",63596468,"well, it turns out that by not giving the appropriate maximum sequence length, tensorflow throws this error. by changing the max_len variable to 54 i could run my program with no difficulty.  so the problem was not about the type of the input or the numpy arrays.",https://stackoverflow.com/questions/63481527,python,19-08-2020 06:55,6564.0,2.0,1.0,True,26-08-2020 11:16,19-08-2020 11:17
75026436,constructing tensorflow dataset and applying textvectorization layer using map method,"i'm attempting to construct input to an embedding layer for an nlp model. however, i am having problems with converting raw text data to the numerical input required by the embedding layer.
here is some example data to illustrate what i wish to feed to the nlp model:
# 0 = negative
# 1 = positive
documents = [['topology freaking sucks man, what a waste of time!', 0], ['wow bro you a nlp fan? tell me more i want to know', 1], 
['you know, i will eventually die',0], ['the secret to happiness is to only be depresssed',0], 
['what is the floor without feet', 1], ['regicide is permissable only in historical situations',1],
['i do not like delivering wehat based products for i am allergic to wheat', 0], 
['why does he ring the large bell every hour?',0],
['wisdom comes not from experience but from knowing',1], 
['little is known of the inner workings of the feline mind', 1]]

each document contains one sentence and one label. this data format was inspired by the tutorial prompt i am working on:

your task
your task in this lesson is to design a small document classification problem with 10 documents of one sentence each and associated labels of positive and negative outcomes and to train a network with word embedding on these data.

i utilize the textvectorization function from the keras library:
# create preprocessing layer
vocab_size = 500 # max amount of vocabulary amongst all documents
max_sequence_length = 50 # maximum amount of words/tokens that will be considered in each document
# output mode 'int' will assign unique integer per token, so in our example below, 'topology' is assigned the value
# 19. notice that these integers are randomly assigned and essentially acts as a hashmap
int_vectorize_layer = textvectorization(
    max_tokens=vocab_size,
    output_mode='int',
    output_sequence_length = max_sequence_length
)

the issue now becomes applying this vectorized layer to the raw data documents. here is the following code i have to convert the raw data into a tensorflow dataset object:
# applies adapted layer to tensorflow dataset
def int_vectorize_text(sentence, label):
  sentence = tf.expand_dims(sentence, -1)
  sentence = tf.squeeze(sentence, axis=-1)
  return int_vectorize_layer(sentence), label


# passes raw data as a generator to the dataset from_generator constructor
def generate_data(sentences, labels):
  for s, l in zip(sentences,labels):
    yield s, l

# split raw data between training and validation set
train_docs = documents[:8]
val_docs = documents[8:]

# separate sentences and labels
train_sentences = [d[0] for d in train_docs]
train_labels = [d[1] for d in train_docs]

val_sentences = [d[0] for d in val_docs]
val_labels = [d[1] for d in val_docs]

# convert to tensors
train_sentences_tensor = tf.convert_to_tensor(train_sentences)
train_labels_tensor = tf.convert_to_tensor(train_labels)

val_sentences_tensor = tf.convert_to_tensor(val_sentences)
val_labels_tensor = tf.convert_to_tensor(val_labels)

# build tensorflow dataset using the above generator function on the newly constructed tensor objects
train_dataset = tf.data.dataset.from_generator(
    generate_data, (tf.string, tf.int32), args=(train_sentences_tensor, train_labels_tensor))
val_dataset = tf.data.dataset.from_generator(
    generate_data, (tf.string, tf.int32), args=(val_sentences_tensor, val_labels_tensor))

# adapt layer using training sentences
int_vectorize_layer.adapt(train_sentences)

# now here is where the error occurs
int_train_df = train_dataset.map(int_vectorize_text) # error
int_val_df = val_dataset.map(int_vectorize_text)

as you can see, an error occurs when we attempt to map the int_vectorize_text to the tensorflow dataset. specifically, i get the following error:
typeerror                                 traceback (most recent call last)
/home/akagi/documents/projects/mlmastery nlp tutorial/lesson 5 - learned embedding.ipynb cell 7 in <cell line: 21>()
     19 # use the map method to apply the int_vectorize_text function to each element of the dataset
     20 int_vectorize_layer.adapt(train_sentences)
---> 21 int_train_df = train_dataset.map(int_vectorize_text)
     22 int_val_df = val_dataset.map(int_vectorize_text)

file ~/documents/projects/.venv/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:2294, in datasetv2.map(self, map_func, num_parallel_calls, deterministic, name)
   2291   if deterministic is not none and not debug_mode:
   2292     warnings.warn(""the `deterministic` argument has no effect unless the ""
   2293                   ""`num_parallel_calls` argument is specified."")
-> 2294   return mapdataset(self, map_func, preserve_cardinality=true, name=name)
   2295 else:
   2296   return parallelmapdataset(
   2297       self,
   2298       map_func,
   (...)
   2301       preserve_cardinality=true,
   2302       name=name)

file ~/documents/projects/.venv/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:5499, in mapdataset.__init__(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)
   5497 self._use_inter_op_parallelism = use_inter_op_parallelism
   5498 self._preserve_cardinality = preserve_cardinality
-> 5499 self._map_func = structured_function.structuredfunctionwrapper(
...
    '>' not supported between instances of 'nonetype' and 'int'
    
    call arguments received by layer 'text_vectorization' (type textvectorization):
      ï¿½ï¿½ï¿½ inputs=tf.tensor(shape=<unknown>, dtype=string)

which seems to imply that a nonetype is being passed. however, i checked the construction of train_dataset and it appears to be correct. here is what it looks like:
(<tf.tensor: shape=(), dtype=string, numpy=b'topology freaking sucks man, what a waste of time!'>, <tf.tensor: shape=(), dtype=int32, numpy=0>)
(<tf.tensor: shape=(), string, numpy=b'wow bro you a nlp fan? tell me more i want to know'>, <tf.tensor: shape=(), dtype=int32, numpy=1>)
(<tf.tensor: shape=(), dtype=string, numpy=b'you know, i will eventually die'>, <tf.tensor: shape=(), dtype=int32, numpy=0>)
(<tf.tensor: shape=(), dtype=string, numpy=b'the secret to happiness is to only be depresssed'>, <tf.tensor: shape=(), dtype=int32, numpy=0>)
(<tf.tensor: shape=(), dtype=string, numpy=b'what is the floor without feet'>, <tf.tensor: shape=(), dtype=int32, numpy=1>)
(<tf.tensor: shape=(), dtype=string, numpy=b'regicide is permissable only in historical situations'>, <tf.tensor: shape=(), dtype=int32, numpy=1>)
(<tf.tensor: shape=(), dtype=string, numpy=b'i do not like delivering wehat based products for i am allergic to wheat'>, <tf.tensor: shape=(), dtype=int32, numpy=0>)
(<tf.tensor: shape=(), dtype=string, numpy=b'why does he ring the large bell every hour?'>, <tf.tensor: shape=(), dtype=int32, numpy=0>)

furthermore, if i apply int_vectorize_text manually in a loop like so:
for x in train_dataset:
    print(int_vectorize_text(x[0], x[1]))

no error occurs and i get the desired output. what is going on here?","['python', 'tensorflow', 'machine-learning', 'keras', 'nlp']",75030447,"after reviewing @alonetogether's clean and more appropriate solution, it appears your issue is stemming from train_dataset and val_dataset definitions. the documentation for the tf.data.dataset.from_generator function recommends that one

... use the output_signature argument. in this case the output will be assumed to consist of objects with the classes, shapes and types defined by tf.typespec objects from output_signature argument

as you didn't use the output_signature argument, it defaulted to using the deprecated way which uses either the output_types argument alone, or together with output_shapes. in your case, output_types was set to (tf.string, tf.int32) but because you left the output_shapes argument empty, it defaulted to ""unknown"".
later when you go to map the int_vectorize_text function, it attempts to check if the input shape rank is greater than 1, however, it receives ""shape=<unknown>"" which is of type nonetype and so the typeerror manifests when comparing with type int.
knowing all this, you can simply add ((), ()) as the output shape in your from_generator function call after the output type (tf.string, tf.int32). hence, replace these lines:
train_dataset = tf.data.dataset.from_generator(
    generate_data, (tf.string, tf.int32), args=(train_sentences_tensor, train_labels_tensor))

val_dataset = tf.data.dataset.from_generator(
    generate_data, (tf.string, tf.int32), args=(val_sentences_tensor, val_labels_tensor))

with:
train_dataset = tf.data.dataset.from_generator(
    generate_data, output_types=(tf.string, tf.int32), output_shapes=((), ()), args=(train_sentences_tensor, train_labels_tensor))

val_dataset = tf.data.dataset.from_generator(
    generate_data, output_types=(tf.string, tf.int32), output_shapes=((), ()), args=(val_sentences_tensor, val_labels_tensor))

or, the tensorflow recommended way as @alonetogether demonstrated:
train_dataset = tf.data.dataset.from_generator(
    generate_data, output_signature=(
         tf.tensorspec(shape=(), dtype=tf.string),
         tf.tensorspec(shape=(), dtype=tf.int32)), args=(train_sentences_tensor, train_labels_tensor))

val_dataset = tf.data.dataset.from_generator(
    generate_data, output_signature=(
         tf.tensorspec(shape=(), dtype=tf.string),
         tf.tensorspec(shape=(), dtype=tf.int32)), args=(val_sentences_tensor, val_labels_tensor))

i've removed my original solution as i don't believe in propagating code that is suboptimal. full credit to @alonetogether for showing how it's supposed to be done. my intent with this edit is to hopefully explain the error and why it occurred so that you and future readers have a better understanding.",https://stackoverflow.com/questions/75026436,python,06-01-2023 02:43,723.0,2.0,2.0,True,06-01-2023 14:00,06-01-2023 03:18
75050771,php curl open ai remove the quotes from post data,"below is my php curl script that works to post to open ai, this is all working as it should.
i want to be able to set the values from posted data like this.
 $getopenaitemperature = $_post[openaitemperature]; 
 $getmaxtokens = $_post[openaimaximumlength]; 
 $gettopp = $_post[openaitopp];`

but when i do it added quotes to the posted values and it stops working.
like this.
 $postdata = [
  'model' => $getopenaimodel,
  'prompt' => $getrequest,
  'temperature' => ""0.24"",
  'max_tokens => ""250"",
  'top_p' => ""1"",

but it needs to look like this to work.
 $postdata = [
   'model' => $getopenaimodel,
   'prompt' => $getrequest,
   'temperature' => 0.24,
   'max_tokens => 250,
   'top_p' => 1,

how can i remove the quotes around the numbers ? the quotes around the model and prompt are fine its just the numbers.
*** the script below here works fine ***
  $getopenaitemperature = 0.5;
  $getmax_tokens = 250;
  $gettop_p = 1;


  $openai_api_key = ""sk-123"";
  $getopenaimodel = ""text-davinci-003"";
  $getrequest ""my question"";
  $ch = curl_init();
  $headers  = [
        'accept: application/json',
        'content-type: application/json',
        'authorization: bearer '.$openai_api_key.''
    ];
 $postdata = [
   'model' => $getopenaimodel,
   'prompt' => $getrequest,
   'temperature' => $getopenaitemperature,
   'max_tokens' => $gettopp,
   'top_p' => $getmaxtokens,
   'best_of' => 2,
   'frequency_penalty' => 0.0,
   'presence_penalty' => 0.0,
    'stop' => '[""\n""]',
  ];

  curl_setopt($ch, curlopt_url, '
  curl_setopt($ch, curlopt_returntransfer, 1);
  curl_setopt($ch, curlopt_ $headers);
  curl_setopt($ch, curlopt_post, 1);
  curl_setopt($ch, curlopt_postfields, json_encode($postdata)); 

  $result = curl_exec($ch);`

i have tried a number of things like php trim() and str_replace but nothing worked.","['php', 'curl', 'openai-api']",75050912,"you can cast the strings to an int or a float like so:
$postdata['temperature'] = (float) $postdata['temperature'];
$postdata['max_tokens'] = (int) $postdata['max_tokens'];

check the php docs  for type casting",https://stackoverflow.com/questions/75050771,php,08-01-2023 19:45,1429.0,0.0,1.0,True,09-01-2023 11:50,09-01-2023 11:50
78552651,how to fix error `oserror: &lt;model&gt; does not appear to have a file named config.json.` when loading custom fine-tuned model?,"preface
i am new to implementing the nlp model. i have successfully fine-tuned llama 3-8b variants with qlora and uploaded them to huggingface.
the directories are filled with these files:
-  .gitattributes
- adapter_config.json
- adapter_model.safetensors
- special_tokens_map.json
- tokenizer.json
- tokenizer_config.json
- training_args.bin

implementation

i am trying to load this model through this:

model_id_1 = ""ferguso/llama-8b-pcl-v3""

tokenizer_1 = autotokenizer.from_pretrained(model_id_1)

quantization_config = bitsandbytesconfig(
    load_in_8bit=true,
)

model_1 = automodelforcausallm.from_pretrained(
    model_id_1,
    quantization_config=quantization_config,
)

but it shows the error oserror: ferguso/llama-8b-pcl-v3 does not appear to have a file named config.json. checkout ' for available files.

so then i am trying to load the config.json from the original model which is meta-llama/meta-llama-3-8b:

original_model = ""meta-llama/meta-llama-3-8b""
model_id_1 = ""ferguso/llama-8b-pcl-v3""

tokenizer_1 = autotokenizer.from_pretrained(model_id_1)

quantization_config = bitsandbytesconfig(
    load_in_8bit=true,
)

original_config = autoconfig.from_pretrained(original_model)
original_config.save_pretrained(model_id_1)

model_1 = automodelforcausallm.from_pretrained(
    model_id_1,
    quantization_config=quantization_config,
    config = original_config
)

but still, it shows another error oserror: error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ferguso/llama-8b-pcl-v3.
questions
how to load the fine-tuned model properly?","['pytorch', 'nlp', 'huggingface-transformers', 'large-language-model', 'peft']",78564203,"your directory contains only the files of the peft-adapter and the files required to load the tokenizer, but the base model weights are missing. i assume you have used the save_pretrained method from peft. this method only saves the adapter weights and config (i use a smaller model for my answer and a different task type!):
from peft import loraconfig, tasktype, get_peft_model, peftmodel
from transformers import automodelfortokenclassification
from pathlib import path

# ferguso/llama-8b-pcl-v3 in your case 
adapter_path = 'bla'
# meta-llama/meta-llama-3-8b in your case
base_model_id = ""distilbert/distilbert-base-uncased""

peft_config = loraconfig(task_type=tasktype.token_cls, target_modules=""all-linear"")

# automodelforcausallm in your case
model = automodelfortokenclassification.from_pretrained(base_model_id)
model = get_peft_model(model, peft_config)

model.save_pretrained(adapter_path)

print(*list(path(adapter_path).iterdir()), sep='\n')

output:
bla/adapter_config.json
bla/readme.md
bla/adapter_model.safetensors

to load your pretrained model successfully, you need to load this base_model weights as well and use the peft model class to load the adapter:
model = automodelfortokenclassification.from_pretrained(base_model_id)
model = peftmodel.from_pretrained(model, adapter_path)

you can also merge the adapter weights back with merge_and_unload and save it:
model.merge_and_unload().save_pretrained('bla2')
print(*list(path('bla2').iterdir()), sep='\n')

output:
bla2/config.json
bla2/model.safetensors

this way you will be able to load the model without peft and only transformers as you tried in the example code of your question.",https://stackoverflow.com/questions/78552651,pytorch,30-05-2024 02:36,3082.0,1.0,1.0,True,05-06-2024 07:22,01-06-2024 15:32
75454265,openai gpt-3 api: does fine-tuning have a token limit?,"in the documentation for gpt-3 api, it says:

one limitation to keep in mind is that, for most models, a single api
request can only process up to 2,048 tokens (roughly 1,500 words)
between your prompt and completion.

in the documentation for fine tuning model, it says:

the more training samples you have, the better. we recommend having at
least a couple hundred examples. in general, we've found that each
doubling of the dataset size leads to a linear increase in model
quality.

my question is, does the 1,500 words limit also apply to fine tune model? does ""doubling of the dataset size"" mean number of training datasets instead of size of each training dataset?","['openai-api', 'gpt-3']",75458209,"gpt-3 models have token limits because you can only provide 1 prompt and get 1 completion. therefore, as stated in the official openai article:

depending on the model used, requests can use up to 4097 tokens shared
between prompt and completion. if your prompt is 4000 tokens, your
completion can be 97 tokens at most.

whereas, fine-tuning as such doesn't have a token limit (i.e., you can have a million training examples, a million prompt-completion pairs), as stated in the official openai documentation:

the more training examples you have, the better. we recommend having
at least a couple hundred examples. in general, we've found that each
doubling of the dataset size leads to a linear increase in model
quality.

but, each fine-tuning prompt-completion pair does have a token limit. each fine-tuning prompt-completion pair should not exceed the token limit.",https://stackoverflow.com/questions/75454265,openai-api,14-02-2023 23:38,4375.0,3.0,1.0,True,12-08-2023 15:06,10-04-2023 10:41
76773026,module &#39;chainlit&#39; has no attribute &#39;langchain_factory&#39;,"i downloaded the repo: 
i created a virtualenv for this one to install the requirments.txt and run the application.
after i run the application using the following command.
chainlit run app.py -w

but i am getting the error:
module 'chainlit' has no attribute 'langchain_factory'","['attributes', 'langchain', 'large-language-model']",76796815,"in the earlier versions of chainlit, the factory concept was included as part of the api. however, after receiving feedback and evaluating its usefulness, the developers decided to remove it in order to simplify the api and avoid confusion among users.
in the latest version of chainlit, you will no longer find the following apis: langchain_factory, langchain_run, langchain_postprocess, llama_index_factory, and langflow_factory.
if your code looks like below,
@cl.langchain_factory
def factory():
    prompt = prompttemplate(template=template, input_variables=[""question""])
    llm_chain = llmchain(prompt=prompt, llm=llm, verbose=true)

    return llm_chain

use the following code to use chainlit if you have installed a latest version of chainlit in your machine,
@cl.on_chat_start
def main():
    # instantiate the chain for that user session
    prompt = prompttemplate(template=template, input_variables=[""question""])
    llm_chain = llmchain(prompt=prompt, llm=llm, verbose=true)

    # store the chain in the user session
    cl.user_session.set(""llm_chain"", llm_chain)


@cl.on_message
async def main(message: str):
    # retrieve the chain from the user session
    llm_chain = cl.user_session.get(""llm_chain"")  # type: llmchain

    # call the chain asynchronously
    res = await llm_chain.acall(message, callbacks=[cl.asynclangchaincallbackhandler()])

    # do any post processing here

    # send the response
    await cl.message(content=res[""text""]).send()

for more: migration guide to chainlit v0.6.0",https://stackoverflow.com/questions/76773026,attributes,26-07-2023 15:49,3097.0,4.0,1.0,True,30-07-2023 07:50,30-07-2023 07:50
77199870,openai api: what is the proper format to fine-tune the openai model?,"i am creating an openai model for one of the catalog book through which users can ask anything from the book, and the model is able to answer it.
i created a q&a list from the book (70 qa). chatgpt gives me the following format to submit it to the fine-tune api endpoint:
""{
    ""context"": ""introducing bauter - an innovative thermal insulation product which has been developed by engineers for many years for the production of paints and other thin-coating materials that results in a unique group of thermo coating solutions. as part of the market demand for a product that will work in conditions where insulation has not yet been possible or difficult, our product resolves many problems in construction and industry. bauter is a family of various products with many applications."",
    ""question"": ""what is bauter?"",
    ""answer"": ""bauter is an innovative thermal insulation product developed by engineers for the production of paints and other thin-coating materials. it offers unique thermo coating solutions and addresses insulation challenges in construction and industry.""
},""

what is the proper format and the proper way to fine-tune the openai model?","['openai-api', 'pre-trained-model', 'chatgpt-api', 'fine-tuning']",77201018,"the proper format to fine-tune the openai model depends on the model you want to fine-tune.
as of today, you can fine-tune the following openai models, as stated in the official openai documentation:

gpt-3.5-turbo-0613 (recommended)
babbage-002
davinci-002

let's say you want to create a chatbot that occasionally gives sarcastic responses, as shown in the official openai documentation.
if you want to fine-tune the gpt-3.5-turbo-0613 model, use the following format:
{""messages"": [{""role"": ""system"", ""content"": ""marv is a factual chatbot that is also sarcastic.""}, {""role"": ""user"", ""content"": ""what's the capital of france?""}, {""role"": ""assistant"", ""content"": ""paris, as if everyone doesn't know that already.""}]}
{""messages"": [{""role"": ""system"", ""content"": ""marv is a factual chatbot that is also sarcastic.""}, {""role"": ""user"", ""content"": ""who wrote 'romeo and juliet'?""}, {""role"": ""assistant"", ""content"": ""oh, just some guy named william shakespeare. ever heard of him?""}]}
{""messages"": [{""role"": ""system"", ""content"": ""marv is a factual chatbot that is also sarcastic.""}, {""role"": ""user"", ""content"": ""how far is the moon from earth?""}, {""role"": ""assistant"", ""content"": ""around 384,400 kilometers. give or take a few, like that really matters.""}]}

if you want to fine-tune the babbage-002 or davinci-002 model, use the following format:
{""prompt"": ""what's the capital of france?"", ""completion"": ""paris, as if everyone doesn't know that already.""}
{""prompt"": ""who wrote 'romeo and juliet'?"", ""completion"": ""oh, just some guy named william shakespeare. ever heard of him?""}
{""prompt"": ""how far is the moon from earth?"", ""completion"": ""around 384,400 kilometers. give or take a few, like that really matters.""}",https://stackoverflow.com/questions/77199870,openai-api,29-09-2023 06:31,692.0,0.0,1.0,True,02-10-2023 08:42,02-10-2023 08:42
73561069,wordnet not returning pertainym for &quot;south korean&quot; even though pertainym exists - python,"i'm trying to do a pertainym search for ""south korean"":
input = ""south korean.a.01.south korean""
lemma = wn.lemma(input)

according to the princeton wordnet page, this should return ""south korea""... yet in my code i'm getting the error message that there is no no lemma for ""south korean"" with part of speech 'a'.
nltk.corpus.reader.wordnet.wordneterror: no lemma 'south korean' with part of speech 'a'

the code works with other words like chinese, russian, and others using the exact same setup, with the online princeton search showing the same part of speech (adjective). any idea why? maybe there is a special way to input words with spaces in them?
i originally thought maybe there was a discrepancy between wordnet 3.0 and 3.1 and so upgraded to 3.1, but no luck.","['nlp', 'nltk', 'wordnet']",73561913,"maybe there is a special way to input words with spaces in them?

yes, they need to be replaced with underline.
so i think your example becomes:
input = ""south_korean.a.01.south_korean""
lemma = wn.lemma(input)

the documentation never explicitly mentions it, as far as i can see, but does show it in a few places.
(i'm not set up to test it at the moment, but if that still doesn't work, try lowercasing the first part, i.e. ""south_korean.a.01.south_korean"" but according to e.g.  it should match case-insensitively.)",https://stackoverflow.com/questions/73561069,nlp,31-08-2022 19:37,119.0,2.0,1.0,True,31-08-2022 21:08,31-08-2022 19:45
70529646,spacy: detect if entity is quoted,"i need to detect if a given entity is surrounded by quotes, either single or double quotes. how would i go about this.
my first thought was to add a custom extension to the span:
def is_quoted(span):
   prev_token = span.doc[span.start - 1]
   next_token = span.doc[span.end + 1]

   return prev_token in [""\"""", ""'""] and next_token in [""\"""", ""'""]

span.set_extension(""is_quoted"", getter=is_quoted)

but would this really be the most efficient way of doing this? i only want to do this on entities.
or am i better of just writing a custom matcher, with a specific regex? but this will then run on my entire document.","['spacy', 'spacy-3']",70530567,"your custom extension looks fine if you only care about quotes immediately before and after. you just need to handle the case where the span is at the start or end of the doc correctly, which you aren't doing now - if the span is at the start you'll check doc[-1], the last token, for example.
do you care about things like john said, ""i never though i'd meet peter smith!"", where ""peter smith"" is an entity? if so i would figure out a policy for nested quotes (maybe just ignore them if rare) and create an extension that walks through each sentence and marks each token as in quotes, or not in quotes (with quotes themselves defined however you want).
if you care about complex cases i wouldn't use a matcher for this - it can't handle nesting well, and i think any solution with it would be more complicated than basic state tracking. (if you only care about immediately before and after it should be fine.)",https://stackoverflow.com/questions/70529646,spacy,30-12-2021 09:21,237.0,2.0,1.0,True,30-12-2021 11:13,30-12-2021 11:13
71900161,huggingface pipeline: userwarning: `grouped_entities` is deprecated and will be removed in version v5.0.0. how to improve about this warning?,"i am using the following code
!pip install datasets transformers[sentencepiece]
from transformers import pipeline
ner = pipeline(""ner"", grouped_entities=true, model='dbmdz/bert-large-cased-finetuned-conll03-english') #named entity recognition (ner)
ner(""my name is <name> and i work at <office> in <location>."")

and i got the following warning. how to improve upon this warning?

userwarning: grouped_entities is deprecated and will be removed in
version v5.0.0, defaulted to
aggregation_strategy=""aggregationstrategy.simple"" instead.","['python', 'warnings', 'pipeline', 'huggingface-transformers']",71900333,"according to [huggingface]: pipelines - class transformers.tokenclassificationpipeline (emphasis is mine):

grouped_entities (bool, optional, defaults to false) - deprecated, use aggregation_strategy instead. whether or not to group the tokens corresponding to the same entity together in the predictions or not.

so, your line of code could be:
ner = pipeline(""ner"", aggregation_strategy=""simple"", model=""dbmdz/bert-large-cased-finetuned-conll03-english"")  # named entity recognition (ner)",https://stackoverflow.com/questions/71900161,python,17-04-2022 07:47,1458.0,0.0,1.0,True,17-04-2022 08:20,17-04-2022 08:20
68459166,python - using tf-idf to summarise dataframe text column,"i have a dataframe with a column containing text.
i want to create a new column that contains a tuple/list of the top 'n' tf-idf scoring words in each row as a way of summarizing what is in the text.
an example dataframe (with a large amount of brevity) is:
df = pd.dataframe({'ref': [1,2,3,4,5], 'text': [""the cow jumped off the other cow"", 
                                                ""the fox had a fox"", 
                                                ""the spanner was a tool to tool"", 
                                                ""the football player played football"",
                                                ""the house had a house""]})

i have spent the last few days trying to find a solution, but i can only find examples which finds the top tf-idf words for the whole corpus, rather than for each row in a dataframe based on the whole corpus.
can anyone steer me in the right direction?","['python', 'tf-idf']",68500946,"here is a possible solution:
from sklearn.feature_extraction.text import tfidfvectorizer
import numpy as np
import pandas as pd

n = 3 # top n tf-idf words

tfidf = tfidfvectorizer(token_pattern=r""\w+"") # no words are left out
x = tfidf.fit_transform(df['text'])
ind = (-x.todense()).argpartition(n)[:, :n]
top_words = pd.series(
    map(
        lambda words_values: dict(zip(*words_values)),
        zip(
            np.array(tfidf.get_feature_names())[ind],
            np.asarray(np.take_along_axis(x, ind, axis=1).todense()),
        ),
    ),
)

here is the result:
>>> top_words
0    {'cow': 0.7111977362687212, 'other': 0.3555988681343606, 'off': 0.3555988681343606}
1    {'fox': 0.8665817814049075, 'had': 0.34957636239744133, 'a': 0.2901799593148741}
2    {'tool': 0.7218960199361867, 'was': 0.36094800996809334, 'spanner': 0.36094800996809334}
3    {'football': 0.8014723840888909, 'player': 0.40073619204444544, 'played': 0.40073619204444544}
4    {'house': 0.8665817814049075, 'had': 0.34957636239744133, 'a': 0.2901799593148741}",https://stackoverflow.com/questions/68459166,python,20-07-2021 17:37,851.0,2.0,1.0,True,23-07-2021 14:37,23-07-2021 09:02
69243459,python dataframe delete sentences number from list,"i have a column of (quite) long texts in a dataframe, and for each text, a list of sentences indexes that i would like to delete. the sentences indexes were generated by spacy when i split texts into sentences. please consider the following example:
import pandas as pd
import spacy
nlp = spacy.load('en_core_web_sm')

data = {'text': ['i am a. i am 30 years old. i live in ny.','i am b. i am 25 years old. i live in sd.','i am c. i am 30 years old. i live in tx.'], 'todel': [[1, 2], [1], [1, 2]]}

df = pd.dataframe(data)

def get_sentences(text):
    text_clean = nlp(text)
    sentences = text_clean.sents
    sents_list = []
    for sentence in sentences:
        sents_list.append(str(sentence))
    return sents_list

df['text'] = df['text'].apply(get_sentences)

print(df)

which gives the following:
                                           text   todel
0  [i am a., i am 30 years old., i live in ny.]  [1, 2]
1   [i am b. i am 25 years old., i live in sd.]     [1]
2   [i am c. i am 30 years old., i live in tx.]  [1, 2]

how would you delete the sentences stored in todel efficiently, knowing that i have a very large dataset with more than 50 sentences to drop for each row ?
my expected output would be:
                                  text   todel
0                      [i live in ny.]  [1, 2]
1  [i am 25 years old., i live in sd.]     [1]
2                      [i live in tx.]  [1, 2]","['python', 'list', 'dataframe', 'apply', 'spacy']",69243913,"based on the answer of @user1740577:
def fun(sen, lst):
    return [i for j, i in enumerate(sen) if j not in lst]

df['text'] = df.apply(lambda row : fun(row['text'],row['todel']), axis=1)

yields the wanted result, based on the indexing of spacy:
                           text    todel
0                     [i am a.]   [1, 2]
1  [i am b. i am 25 years old.]      [1]
2  [i am c. i am 30 years old.]   [1, 2]",https://stackoverflow.com/questions/69243459,python,19-09-2021 12:51,84.0,-1.0,1.0,True,05-06-2022 15:00,19-09-2021 13:37
73146232,what is a regex expression that can prune down repeating identical characters down to a maximum of two repeats?,"i feel i am having the most difficulty explaining this well enough for a search engine to pick up on what i'm looking for.  the behavior is essentially this:
string = ""aaaaaaaaare yooooooooou okkkkkk""

would become ""aare yoou okk"", with the maximum number of repeats for any given character is two.
matching the excess duplicates, and then re.sub -ing it seems to me the approach to take, but i can't figure out the regex statement i need.
the only attempt i feel is even worth posting is this - (\w)\1{3,0}
which matched only the first instance of a character repeating more than three times - so only one match, and the whole block of repeated characters, not just the ones exceeding the max of 2. any help is appreciated!","['python', 'regex', 'nlp', 'data-preprocessing']",73146268,"the regexp should be (\w)\1{2,} to match a character followed by at least 2 repetitions. that's 3 or more when you include the initial character.
the replacement is then \1\1 to replace with just two repetitions.
string = ""aaaaaaaaare yooooooooou okkkkkk""
new_string = re.sub(r'(\w)\1{2,}', r'\1\1', string)",https://stackoverflow.com/questions/73146232,python,28-07-2022 01:23,382.0,0.0,2.0,True,28-07-2022 06:47,28-07-2022 03:24
70297475,fill in word that letter is located in,"i am processing keystroke data, and need to find the word that a keystroke is located within. because there can be invisible keystrokes (like shift) or deleted keystrokes, this is not a trivial problem where i can just iterate the index of keystrokes, and locate the word. rather, i need to find the space-delimited word that the keystroke is produced within. i do have the full text and existing text available, which i should be able to leverage. i've tried solutions using fill(), lag(), and cumsum(), but none are working.
i have a dataframe like the below, where i group by experiment_id:
x <- tibble(
  experiment_id = rep(c('1a','1b'),each=12),
  keystroke = rep(c('a','space','shift','b','e','delete','a','d','space','m','a','n'),2),
  existing_text = rep(c('a','a ','a ','a b','a be','a b','a ba','a bad','a bad ',
                    'a bad m','a bad ma','a bad man'),2),
  final_text = 'a bad man'
)

the additional column should look like this, where space belongs to the word it follows, and deletes and the deleted keystrokes are part of the final word:
within_word = c('a','a','bedeletead','bedeletead','bedeletead','bedeletead','bedeletead','bedeletead','bedeletead','man','man','man')

is there a way to derive this?
edit for additional help: in the comments below the answer, @onyambu mentioned that there is a simpler solution using the keystroke column. i've found that in my larger, more complex data that existing_text is not always reliable. i would strongly prefer a solution that relies on keystroke primarily. i've also added in complications due to deletions.","['r', 'nlp', 'tidyverse']",70461018,"below are two approaches:
the first uses the information in existing_text only for the grouping and constructs the within_words columns based on this grouping and keystroke.
the second approach uses only the information in keystroke.

first approach: grouping based on existing_text and content based on keystroke:
we take three steps:
first, we caclulate the grouping based on strsplit where we look for spaces \\s that are preceeded by words \\w. we need to correct the values for ""shift"" since they should be counted to the word after ""space"".
step two is the replace ""shift"" (and all other similar functions which the example data doesn't contain) with """".
third, we collapse the strings with paste0(..., collapse = """").
library(tidyverse)

x %>%

  # step1: construct grouping:
  mutate(word_grp = lengths(strsplit(existing_text, ""(?<=\\w)\\s"", perl = true)) %>% 
           if_else(keystroke == ""shift"", lead(., default = last(.)), .)) %>%
  group_by(experiment_id, word_grp) %>% 

  # step 2 & 3: first replace keys like ""shift"" with """", the collapse with `paste0`
  mutate(within_word = str_replace_all(keystroke, c(""shift"" = """", ""space"" = """")) %>% 
           paste0(., collapse = """"))

#> # a tibble: 24 x 6
#> # groups:   experiment_id, word_grp [6]
#>    experiment_id keystroke existing_text final_text word_grp within_word
#>    <chr>         <chr>     <chr>         <chr>         <int> <chr>      
#>  1 1a            a         ""a""           a bad man         1 a          
#>  2 1a            space     ""a ""          a bad man         1 a          
#>  3 1a            shift     ""a ""          a bad man         2 bedeletead 
#>  4 1a            b         ""a b""         a bad man         2 bedeletead 
#>  5 1a            e         ""a be""        a bad man         2 bedeletead 
#>  6 1a            delete    ""a b""         a bad man         2 bedeletead 
#>  7 1a            a         ""a ba""        a bad man         2 bedeletead 
#>  8 1a            d         ""a bad""       a bad man         2 bedeletead 
#>  9 1a            space     ""a bad ""      a bad man         2 bedeletead 
#> 10 1a            m         ""a bad m""     a bad man         3 man        
#> # ï¿½ï¿½ï¿½ with 14 more rows



second approach: based on information in keystrokes only.
here is one approach which only uses the information in keystroke. however, if we only want to use the data in keystroke things get mue laborious.
here is a short explanation of the steps taken below:
step 1a: data cleaning
we need to clean the data in keystrokes so that they can be used for the new column within_word. this means two things: (a) we need to replace every keystroke that should not be printed in within_word with """". and before this we need to (b) change the leading keystroke based on the function of that key. in the case of shift this means we need to set the leading keystroke toupper. for your example data this is really simple, because there is only shift we need to take care of. however, in your real data there might be many similar other keys such as alt or ^. so we need to repeat step 1a for each key. ideally we would come up with a function taking the name of the key and the function that it uses on the leading keystroke. note that we do not yet include ""space"" in this step, since we need it in step 2.
to see how many keys you need to take care of in your actual data we can filter for those keystrokes that don't change the existing_text. in your example data this is only shift:
# get all keystrokes that don't change the existing_text directly
x %>% 
  select(keystroke, existing_text) %>% 
  filter(existing_text == lag(existing_text, default = """"))

#> # a tibble: 2 x 2
#>   keystroke existing_text
#>   <chr>     <chr>        
#> 1 shift     ""a ""         
#> 2 shift     ""a ""

step 2: create grouping
we need to create the grouping of the words in within_text. this is the most complicated step. below we first look for rows where within_word == ""space"" and which succeeding row is != ""space"". we use data.table::rleid on the result to get a run-length id for this variable. finally we need to subtract 1 for those rows which within_word == ""space"".
step 3: data prep before final step
this is basically similar to step 1a, we need to replace ""space"" with """" because we don't want it in our result. however, since we needed this column for step 2 we have to finalize the data cleaning in this step.
step 4: collapse the strings in within_word
finally, we group by experiment_id and by word_grp and collapse the strings in within_word with paste0(..., collapse = """").
library(tidyverse)

  # step 1a: data cleaning
  mutate(within_word = if_else(lag(keystroke, default = first(keystroke)) == ""shift"",
                               toupper(keystroke),
                               keystroke) %>%
                          str_replace_all(., c(""shift"" = """"))) %>%  
 
  # step 1b to 1n: repeat step 1a for other keys like alt, ^ etc. 

  # step 2: create groups
  group_by(experiment_id) %>% 
  mutate(word_grp = data.table::rleid(
      within_word == ""space"" & lead(within_word, default = first(keystroke)) != ""space""
    ) %>% if_else(within_word == ""space"", . - 1l, .)) %>% 

  # step 3: data prep before final step
  ungroup %>% 
  mutate(within_word = str_replace(within_word, ""space"", """")) %>%
 
  # step 4: collapse
  group_by(experiment_id, word_grp) %>% 
  mutate(within_word = paste0(within_word, collapse = """"))

#> # a tibble: 24 x 6
#> # groups:   experiment_id, word_grp [6]
#>    experiment_id keystroke existing_text final_text within_word word_grp
#>    <chr>         <chr>     <chr>         <chr>      <chr>          <int>
#>  1 1a            a         ""a""           a bad man  a                  1
#>  2 1a            space     ""a ""          a bad man  a                  1
#>  3 1a            shift     ""a ""          a bad man  bedeletead         3
#>  4 1a            b         ""a b""         a bad man  bedeletead         3
#>  5 1a            e         ""a be""        a bad man  bedeletead         3
#>  6 1a            delete    ""a b""         a bad man  bedeletead         3
#>  7 1a            a         ""a ba""        a bad man  bedeletead         3
#>  8 1a            d         ""a bad""       a bad man  bedeletead         3
#>  9 1a            space     ""a bad ""      a bad man  bedeletead         3
#> 10 1a            m         ""a bad m""     a bad man  man                5
#> # ï¿½ï¿½ï¿½ with 14 more rows

created on 2021-12-23 by the reprex package (v0.3.0)",https://stackoverflow.com/questions/70297475,r,09-12-2021 22:25,207.0,1.0,2.0,True,23-12-2021 14:19,22-12-2021 22:59
76462873,adding multiple special cases for spacy tokenizer,"i am trying to segment text in a txt file (utf-8) into sentences using spacy. it segments sentences with abbreviations (e.g., mr., dr., etc.) as separate sentences when it is meant to read as a single sentence. for example: 'mr. john doe says' becomes
sentence 0: dr.
sentence 1: jane doe says
i tried to use nlp.tokenizer.add_special_case to recognize dr. as a special case, and it works for one case (code below). but because i have many abbreviations in the rest of the dataset, i would like to have a list of abbreviations (preferably from a text file but really just a list is fine!) where it adds everything on the list as special cases.
this is my code:
import spacy
import pathlib
from spacy.attrs import orth, norm

nlp = spacy.load('en_core_web_sm')
nlp.tokenizer.add_special_case('dr.', [{orth: 'dr .', norm: 'doctor'}])

file_name = r""text_test_sentence.txt"" #filename of textfile to split
doc = nlp(pathlib.path(file_name).read_text(encoding=""utf-8""))
sentences = list (doc.sents) 

thank you in advance!!!","['spacy', 'sentence']",76523323,"if you would like to like add multiple rules to your tokenizer, then i would suggest writing a for loop over a list that stores all the various abbreviations that you would like to add to the special cases.",https://stackoverflow.com/questions/76462873,spacy,13-06-2023 08:07,245.0,0.0,1.0,True,21-06-2023 12:31,13-06-2023 08:45
73949640,repetitive word predictions in rnn,"hello dear community,
i am training a seq2seq model to generate a question based on a graph. both train and val loss are converging, but the generated questions (on either train or test set) are nonsense and contain mostly repetition of tokens. i tried various hyper parameters and double checked input and outputs tensors.
something that i do find odd is that the output out (see below) starts containing some values, which i consider as unusually high. this starts happening around half way through the first epoch:
out:  tensor([[  0.2016, 103.7198,  90.4739,  ...,   0.9419,   0.4810,  -0.2869]]

my guess for that is vanishing/exploding gradients, which i thought i had handeled by gradient clipping, but now i am not sure about this:
for p in model_params:
        p.register_hook(lambda grad: torch.clamp(
            grad, -clip_value, clip_value))

below are the training curves (10k samples, batch size=128, lr=0.065, lr_decay=0.99, dropout=0.25)

encoder (a gnn, learning node embeddings of the input graph, that consists of around 3-4 nodes and edges. a single graph embedding is obtained by pooling the node embeddings and feeding them as the initial hidden state to the decoder):
class questiongraphgnn(torch.nn.module):
    def __init__(self,
                 in_channels,
                 hidden_channels,
                 out_channels,
                 dropout,
                 aggr='mean'):
        super(questiongraphgnn, self).__init__()
        nn1 = torch.nn.sequential(
            torch.nn.linear(in_channels, hidden_channels),
            torch.nn.relu(),
            torch.nn.linear(hidden_channels, in_channels * hidden_channels))
        self.conv = nnconv(in_channels, hidden_channels, nn1, aggr=aggr)
        self.lin = nn.linear(hidden_channels, out_channels)
        self.dropout = dropout

    def forward(self, x, edge_index, edge_attr):
        x = self.conv(x, edge_index, edge_attr)
        x = f.leaky_relu(x)
        x = f.dropout(x, p=self.dropout)
        x = self.lin(x)
        return x

decoder (the out vector from above is printed in the forward() function):
class decoderrnn(nn.module):
    def __init__(self,
                 embedding_size,
                 output_size,
                 dropout):
        super(decoderrnn, self).__init__()
        self.output_size = output_size
        self.dropout = dropout

        self.embedding = nn.embedding(output_size, embedding_size)
        self.gru1 = nn.gru(embedding_size, embedding_size)
        self.gru2 = nn.gru(embedding_size, embedding_size)
        self.gru3 = nn.gru(embedding_size, embedding_size)
        self.out = nn.linear(embedding_size, output_size)
        self.logsoftmax = nn.logsoftmax(dim=1)

    def forward(self, inp, hidden):
        output = self.embedding(inp).view(1, 1, -1)
        output = f.leaky_relu(output)

        output = f.dropout(output, p=self.dropout)
        output, hidden = self.gru1(output, hidden)

        output = f.dropout(output, p=self.dropout)
        output, hidden = self.gru2(output, hidden)
        output, hidden = self.gru3(output, hidden)

        out = self.out(output[0])
        print(""out: "", out)
        output = self.logsoftmax(out)
        return output, hidden

i am using pytorchs nllloss().
optimizer is sgd.
i call optimizer.zero_grad() right before the backward and optimizer step and i switch the training/evaluation mode for training, evaluation and testing.
what are your thoughts on this?
thank you very much!
edit
dimensions of the encoder:
in_channels=301 (this is the size of the initial node embeddings)
hidden_channels=256
out_channels=301 (this will also be the size of the final graph embedding, after mean pooling the node embeddings)
dimensions of the decoder:
embedding_size=301 (the size of the previously pooled graph embedding)
output_size=number of words in my vocabulary. in the training above around 1.2k
i am using top-k sampling and my train loop follows the nmt tutorial  similarily, my translation function, that takes the data of a single graph, decodes a question as such:
def translate(self, data):
    # get node embeddings of the input graph
    h = self.encoder(data.node_embeddings,
                     data.edge_index, data.edge_embeddings)

    # pool node embeddings into single graph embedding
    graph_embedding = self.get_graph_embeddings(h, data.graph_dict)

    # pass graph embedding through decoder
    self.encoder.eval()
    self.decoder.eval()
    with torch.no_grad():
        # initialize first input and hidden state
        decoder_input = decoder_input = torch.tensor(
            [[self.vocab.sos['idx']]], device=self.device)
        decoder_hidden = graph_embedding.view(1, 1, -1)

        decoder_tokens = []
        for di in range(self.dec_max_length):
            decoder_output, decoder_hidden = self.decoder(
                decoder_input, decoder_hidden)
            topv, topi = decoder_output.data.topk(1)
            if topi.item() == self.vocab.eos['idx']:
                break
            else:
                word = self.vocab.index2word[topi.item()]
                word = word.upper(
                ) if word == self.vocab.unk['token'].lower() else word
                decoder_tokens.append(word)
            decoder_input = topi.squeeze().detach()

        return decoder_tokens

also: at times, the output-vector of the final gru layer (self.gru3(...)) inside the forward() function (5th line from the bottom) outputs a lot of values being (close to) 1 and -1. i suppose these might otherwise be a lot higher/lower without clipping. this might be alright, but seems unusual to me. an example:
tensor([[[-0.9984, -0.9950,  1.0000, -0.9889, -1.0000, -0.9770, -0.0299,
          -0.9996,  0.9996,  1.0000, -0.0176, -0.5815, -0.9998, -0.0265,
          -0.1471,  0.9998, -1.0000, -0.2356,  0.9964,  0.9936, -0.9998,
           0.0652, -0.9999,  0.9999, -1.0000, -0.9998, -0.9999,  0.9998,
          -1.0000, -0.9997,  0.9850,  0.9994, -0.9998, -1.0000, -1.0000,
           0.9977,  0.9015, -0.9982,  1.0000,  0.9980, -1.0000,  0.9859,
           0.6670,  0.9998,  0.3827,  0.9999,  0.9953, -0.9989,  0.1287,
           1.0000,  1.0000, -1.0000,  0.9778,  1.0000,  1.0000, -0.9907, ...","['python', 'pytorch', 'nlp', 'recurrent-neural-network', 'gnn']",73950685,"your code looks good, and given the training/validation curves you posted, it looks like it's doing alright.
how are you generating text samples? are you just taking the word the model predicts with the highest probability, appending to the end of your input sequence, and calling forward again? this sampling technique, called greedy sampling, can lead to behavior you described. maybe another sampling technique could help (see beam search",https://stackoverflow.com/questions/73949640,python,04-10-2022 14:42,226.0,0.0,1.0,True,04-10-2022 16:22,04-10-2022 16:22
72395380,how to drop sentences that are too long in huggingface?,"i'm going through the huggingface tutorial and it appears as the library has automatic truncation, to cut sentences that are too long, based on a max value, or other things.
how can i remove sentences for the same reasoning (sentences are too long, based on a max value, etc), instead of truncating them? e.g., if the sentence is too long, drop it.
example for truncation:
from transformers import autotokenizer

checkpoint = ""distilbert-base-uncased-finetuned-sst-2-english""
tokenizer = autotokenizer.from_pretrained(checkpoint)
sentence_input = 'this is an input'

result = tokenizer(sentence_input, padding=true, truncation=true, return_tensors=""pt"")

example to prepare samples in a batch
from datasets import load_dataset
from transformers import autotokenizer, datacollatorwithpadding

raw_datasets = load_dataset(""glue"", ""mrpc"")
checkpoint = ""bert-base-uncased""
tokenizer = autotokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example[""sentence1""], example[""sentence2""], truncation=true)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=true)
data_collator = datacollatorwithpadding(tokenizer=tokenizer)","['python', 'huggingface-transformers', 'huggingface-tokenizers', 'huggingface-datasets']",72397605,"a filter is all you need:
import pandas
from datasets import dataset
from transformers import autotokenizer

df = pandas.dataframe([{""sentence1"": ""bla"", ""sentence2"": ""bla""}, {""sentence1"": ""bla ""*600, ""sentence2"": ""bla""}])
dataset = dataset.from_pandas(df)


checkpoint = ""bert-base-uncased""
tokenizer = autotokenizer.from_pretrained(checkpoint)

#not truncating the samples allows us to filter them
def tokenize_function(example):
    return tokenizer(example[""sentence1""], example[""sentence2""])


tokenized_datasets = dataset.map(tokenize_function, batched=true)
print(len(tokenized_datasets))
tokenized_datasets = tokenized_datasets.filter(lambda example: len(example['input_ids']) <= tokenizer.max_model_input_sizes[checkpoint])
print(len(tokenized_datasets))


output:
token indices sequence length is longer than the specified maximum sequence length for this model (1205 > 512). running this sequence through the model will result in indexing errors
2
1",https://stackoverflow.com/questions/72395380,python,26-05-2022 16:54,1747.0,3.0,1.0,True,26-05-2022 20:19,26-05-2022 20:19
11798389,what nlp tools to use to match phrases having similar meaning or semantics,"i am working on a project which requires me to match a phrase or keyword with a set of similar keywords. i need to perform semantic analysis for the same. 
an example:
relevant qt
cheap health insurance
affordable health insurance
low cost medical insurance
health plan for less
inexpensive health coverage
common meaning
low cost health insurance
here the the word under common meaning column should match the under relevant qt column. i looked at a bunch of tools and techniques to do the same. s-match seemed very promising, but i have to work in python, not in java. also latent semantic analysis looks good but i think its more for document classification based upon a keyword rather than keyword matching. i am somewhat familiar with nltk. could someone provide some insight on what direction i should proceed and what tools i should use for the same?","['python', 'nlp', 'nltk', 'latent-semantic-indexing']",27642477,"if you have a big corpus, where these words occur, available, you can train a model to represent each word as vector. for instance, you can use deep learning via word2vecï¿½ï¿""skip-gram and cbow models"", they are implemented in the gensim software package 
in the word2vec model, each word is represented by a vector, you can then measure the semantic similarity between two words by measuring the cosine of the vectors representing th words. semantic similar words should have a high cosine similarity, for instance:
model.similarity('cheap','inexpensive') = 0.8

(the value is made up, just for illustration.)
also, from my experiments, summing a relatively small number of words  (i.e., up to 3 or 4 words) preserves the semantics, for instance:
vector1 = model['cheap']+model['health']+model['insurance']
vector2 = model['low']+model['cost']+model['medical']+model['insurance']

similarity(vector1,vector2) = 0.7

(again, just for illustration.)
you can use this semantic similarity measure between words as a measure to generate your clusters.",https://stackoverflow.com/questions/11798389,python,03-08-2012 15:09,18831.0,17.0,3.0,True,13-11-2023 17:47,29-04-2017 16:09
76502113,error with few-shot prompting using gpt 3.5,"i am trying to train gpt 3.5 model with few-shot prompting using messages argument instead of prompt argument. it throws an error even though it's clearly mentioned in openai documentation that we can train a model this way.
import openai

conversation=[
        {""role"": ""system"", ""content"": ""you are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""who won the world series in 2020?""},
        {""role"": ""assistant"", ""content"": ""the los angeles dodgers won the world series in 2020.""},
    ]

def askgpt(question):
    conversation.append({""role"": ""user"", ""content"": question})
    openai.api_key = ""openai key""
    response = openai.completion.create(
        model = ""gpt-3.5-turbo"",
        messages = conversation,
        temperature = 0.6
        #max_tokens = 150,
    )
    #conversation.append({""role"": ""assistant"",""content"":response})
    #print(response)
    #print(response[""choices""][0][""message""][""content""])

    
    conversation.append({""role"": ""assistant"", ""content"": response.choices[0].message.content})
    print(response.choices[0].message.content)

def main():
    while true:
        print('gpt: ask me a question\n')
        myqn = input()
        askgpt(myqn)
        print('\n')


main()

error:

openai.error.invalidrequesterror: unrecognized request argument supplied: messages

i tried to give ""conversations"" to the model inside ""responses"" but it soesn't seem to work.","['chatbot', 'openai-api', 'gpt-3', 'chatgpt-api']",76504545,"you made a mistake between chat completion and completion. see documentation.
completion: 
chat completion:",https://stackoverflow.com/questions/76502113,chatbot,18-06-2023 18:58,608.0,0.0,1.0,True,19-06-2023 07:38,18-06-2023 19:01
66444961,genome representation for a nltk sentence,"given a nltk grammar, how can i represent a sentence using an arrays of integers?
i am using nltk to generate some sentences from a specific grammar. i would like to generate an array of integers to represent a genome for a generated sentence (phenotype).
with that representation of integers i would evolve the genome in a genetic algorithm, performing some mutations to get better sentences.
for example,
from nltk import cfg
from nltk.parse.generate import generate, demo_grammar

g = cfg.fromstring(demo_grammar)
sentence = next(generate(g, n=1))

print(sentence) # ex: ['the', 'man', 'saw', 'the', 'park']

convert_to_genotype(sentence) # returns [253, 69, 221, 97, 190, 254, 67, 137, 95, 72, 54, 232, 11, 136] for example.

how can i create the convert_to_genotype function?
thanks","['python', 'nltk', 'genetic-algorithm', 'context-free-grammar']",66547591,"after some research, i created an implementation that creates a phenotype for a given genome. that was what i was looking for to evolve my individuals created using the rules from a grammar.
import nltk
from nltk import cfg

grammar = cfg.fromstring(""""""
string -> letter | letter string
letter -> vowel | consonant | char
char   -> ' '|'!'|'?'|','|'.'

vowel       -> lower_vowel | upper_vowel
lower_vowel -> 'a'|'e'|'o'|'i'|'u'
upper_vowel -> 'a'|'e'|'i'|'o'|'u'

consonant       -> lower_consonant | upper_consonant
lower_consonant -> 'b'|'c'|'d'|'f'|'g'|'h'|'j'|'k'|'l'|'m'|'n'|'p'|'q'|'r'|'s'|'t'|'v'|'w'|'x'|'y'|'z'
upper_consonant -> 'b'|'c'|'d'|'f'|'g'|'h'|'j'|'k'|'l'|'m'|'n'|'p'|'q'|'r'|'s'|'t'|'v'|'w'|'x'|'y'|'z'
"""""")

def genome_to_grammar(array):
  sb = []
  stack = [grammar.start()]
  index = 0
  wraps = 0

  while stack:
    symbol = stack.pop()
    if isinstance(symbol, str):
      sb.append(symbol)
    else:
      rules = [i for i in grammar.productions() if i.lhs().symbol() == symbol.symbol()]
      rule_index = 0
      if len(rules) > 1:
        rule_index = array[index] % len(rules)
        index += 1
        if index >= len(array):
          index = 0
          wraps += 1
          if wraps > 10:
            return none
      rule = rules[rule_index]
      for production in reversed(rule.rhs()):
        stack.append(production)

  return ''.join(sb)

genome = [253, 69, 221, 97, 190, 254, 67, 137, 95, 72, 54, 232, 11, 136]
print(genome_to_grammar(genome))",https://stackoverflow.com/questions/66444961,python,02-03-2021 18:19,86.0,0.0,1.0,True,09-03-2021 13:13,03-03-2021 02:50
69338657,keeping numbers in doc2vec tokenization,"iï¿½ï¿½ï¿½m in the process of trying to get document similarity values for a corpus of approximately 5,000 legal briefs with doc2vec (i recognize that the corpus may be a little bit small, but this is a proof-of-concept project for a larger corpus of approximately 15,000 briefs iï¿½ï¿½ï¿½ll have to compile later).
basically, every other component in the creation of the model is going relatively well so far ï¿½ï¿½ï¿½ each brief i have is in a text file within a larger folder, so i compiled them in my script using glob.glob ï¿½ï¿½ï¿½ but iï¿½ï¿½ï¿½m running into a tokenization problem.  the difficulty is, as these documents are legal briefs, they contain numbers that iï¿½ï¿½ï¿½d like to keep, and many of the guides iï¿½ï¿½ï¿½ve been using to help me write the code use gensimï¿½ï¿½ï¿½s simple preprocessing, which i believe eliminates digits from the corpus, in tandem with the taggeddocument feature. however, i want to do as little preprocessing on the texts as possible.
below is theple_preprocess for genism.utils.tokenize, but when i do that, i get generator objects that donï¿½ï¿½ï¿½t appear workable in my final doc2vec model, and i canï¿½ï¿½ï¿½t actually see how the corpus looks. when iï¿½ï¿½ï¿½ve tried to use other tokenizers, like nltk, i donï¿½ï¿½ï¿½t know how to fit that into the taggeddocument component.
brief_corpus = []
for brief_filename in brief_filenames:
    with codecs.open(brief_filename, ""r"", ""utf-8"") as brief_file:
        brief_corpus.append(
            gensim.models.doc2vec.taggeddocument(
                gensim.utils.simple_preprocess( 
                    brief_file.read()),
                    [""{}"".format(brief_filename)])) #tagging each brief with its filename

iï¿½ï¿½ï¿½d appreciate any advice that anyone can give that would help me combine a tokenizer that just separated on whitespace and didnï¿½ï¿½ï¿½t eliminate any numbers with the taggeddocument feature. thank you!
updatudimentary code for some basic tokenization (i do plan on refining it further) without having to resort to gensim's simple_preprocessing function. however, i'm having difficulty (again!) when using the taggeddocument feature - but this time, the tags (which i want to be the file names of each brief) don't match the tokenized document. basically, each document has a tag, but it's not the right one.
can anyone possibly advise where i might have gone wrong with the new code below? thanks!
briefs = []
brieflist = [p for p in os.listdir(filepath) if p.endswith('.txt')]
for brief in brieflist:
     str = open(filepath + brief,'r').read()
     tokens = re.findall(r""[\w']+|[.,!?;]"", str)
     tagged_data = [taggeddocument(tokens, [brief]) for brief in brieflist]
     briefs.append(tagged_data)","['python', 'tokenize', 'word-embedding', 'doc2vec']",69340154,"you're likely going to want to write your own preprocessing/tokenization functions. but don't worry, it's not hard to outdo gensim's simple_preprocess, even with very crude code.
the only thing doc2vec needs as the words of a taggeddocument is a list of string tokens (typically words).
so first, you might be surprised how well it works to just do a default python string .split() on your raw strings - which just breaks text on whitespace.
sure, a bunch of the resulting tokens will then be mixes of words & adjoining punctuation, which may be nearly nonsense.
for example, the word 'lawsuit' at the end of the sentence might appear as 'lawsuit.', which then won't be recognized as the same token as 'lawsuit', and might not appear enough min_count times to even be considered, or otherwise barely rise above serving as noise.
but especially for both longer documents, and larger datasets, no one token, or even 1% of all tokens, has that much influence. this isn't exact-keyword-search, where failing to return a document with 'lawsuit.' for a query on 'lawsuit' would be a fatal failure. a bunch of words 'lost' to such cruft may have hadly any effect on the overall document, or model, performance.
as your datasets seem manageable enough to run lots of experiments, i'd suggest trying this dumbest-possible tokenization ï¿½ï¿½ï¿½ only .split() ï¿½ï¿½ï¿½ just as a baseline to become confident that the algorithm still mostly works as well as some more intrusive operation (like simple_preprocess()).
then, as you notice, or suspect, or ideally measure with some repeatable evaluation, that some things you'd want to be meaningful tokens aren't treated right, gradually add extra steps of stripping/splitting/canonicalizing characters or tokens. but as much as possible: checking that the extra complexity of ntime, is actually delivering benefits.
for example, further refinements could be some mix of:

for each token created by the simple split(), strip off any non-alphanumeric leading/trailing chars. (advantages: eliminates that punctuation-fouling-words cruft. disadvantages: might lose useful symbols, like the leading $ of monetary amounts.)
before splitting, replace certain single-character punctuation-marks (like say ['.', '""', ',', '(', ')', '!', '?', ';', ':']) with the same character with spaces on both sides - so that they're never connected with nearby words, and instead survive a simple .split() as standalone tokens. (advantages: also prevents words-plus-punctuation cruft. disadvantages: breaks up numbers like 2,345.77 or some useful abbreviations.)
at some appropriate stage in tokenization, canonicalize many varied tokens into a smaller set of tokens that may be more meaningful than each of them as rare standalone tokens. for example, $0.01 through $0.99 might all be turned into $0_xx - which then has a better chance of influencting the model, & being associated with 'tiny amount' concepts, than the original standalone tokens. or replacing all digits with #, so that numbers of similar magnitudes share influence, without diluting the model with a token for every single number.

the exact mix of heuristics, and order of operations, will depend on your goals. but with a corpus only in the thousands of docs (rather than hundreds-of-thousands or millions), even if you do these replacements in a fairly inefficient way (lots of individual string- or regex- replacements in serial), it'll likely be a manageable preprocessing cost.
but you can start simple & only add complexity that your domain-specific knowledge, and evaluations, justifies.",https://stackoverflow.com/questions/69338657,python,26-09-2021 20:21,640.0,1.0,1.0,True,27-09-2021 04:14,27-09-2021 04:14
75622285,openai chat completions api error: &quot;openai.createchatcompletion is not a function&quot;,"i have this in my mern stack code file, and it works well.
exports.chatbot = async (req, res) => {
  console.log(""openai chatbot post"");

  const { textinput } = req.body;

  try {

    const response = await openai.createcompletion({
      model: ""text-davinci-003"",
      prompt: `
            what is your name?
            my name is chatbot.
            how old are you?
            i am 900 years old.
            ${textinput}`,
      max_tokens: 100,
      temperature: 0,
    });
    if (response.data) {
      if (response.data.choices[0].text) {
        return res.status(200).json(response.data.choices[0].text);
      }
    }
  } catch (err) {
    return res.status(404).json({ message: err.message });
  }
};

while i change the api request, use the new chat completions api. this one doesn't work.

exports.chatbot = async (req, res) => {
  console.log(""openai chatbot post"");

  const { textinput } = req.body;

  try {
    const completion = await openai.createchatcompletion({
      model: ""gpt-3.5-turbo"",
      messages: [{ role: ""user"", content: textinput }],
    });
    console.log(completion.data.choices[0].message);

    if (completion.data) {
      if (completion.data.choices[0].message) {
        return res.status(200).json(completion.data.choices[0].message);
      }
    }

  } catch (err) {
    return res.status(404).json({ message: err.message });
  }
};

error i'm getting:

post  404 (not found)","['axios', 'openai-api', 'chatgpt-api']",75626662,"you need to reinstall the openai npm package. it has only just been updated with the createchatcompletion in the past 2 days.
when i reinstalled the package and ran your code it worked successfully.",https://stackoverflow.com/questions/75622285,axios,03-03-2023 00:59,8422.0,6.0,3.0,True,12-06-2024 17:17,12-06-2024 17:17
10252448,how to check whether a sentence is correct (simple grammar check in python)?,"how to check whether a sentence is valid in python?
examples:
i love stackoverflow - correct
i stackoverflow love - incorrect","['python', 'nlp', 'grammar']",10252472,"check out nltk.  they have support for grammars that you can use to parse your sentence.  you can define a grammar, or use one that is provided, along with a context-free parser.  if the sentence parses, then it has valid grammar; if not, then it doesn't.  these grammars may not have the widest coverage (eg, it might not know how to handle a word like stackoverflow), but this approach will allow you to say specifically what is valid or invalid in the grammar.  chapter 8 of the nltk book covers parsing and should explain what you need to know.
an alternative would be to write a python interface to a wide-coverage parser (like the stanford parser or c&c).  these are statistical parsers that will be able to understand sentences even if they haven't seen all the words or all the grammatical constructions before.  the downside is that sometimes the parser will still return a parse for a sentence with bad grammar because it will use the statistics to make the best guess possible.
so, it really depends on exactly what your goal is.  if you want very precise control over what is considered grammatical, use a context-free parser with nltk.  if you want robustness and wide-coverage, use a statistical parser.",https://stackoverflow.com/questions/10252448,python,20-04-2012 19:33,107124.0,69.0,6.0,True,25-05-2023 08:49,12-03-2013 04:41
61482810,fine tuning a pretrained language model with simple transformers,"in his article 'language model fine-tuning for pre-trained transformers' thilina rajapakse (
provides the following code snippet for fine-tuning a pre-trained model using the library simpletransformers:
from simpletransformers.language_modeling import languagemodelingmodel
import logging


logging.basicconfig(level=logging.info)
transformers_logger = logging.getlogger(""transformers"")
transformers_logger.setlevel(logging.warning)

train_args = {
    ""reprocess_input_data"": true,
    ""overwrite_output_dir"": true,
}

model = languagemodelingmodel('bert', 'bert-base-cased', args=train_args)

model.train_model(""data/train.txt"", eval_file=""data/text.txt"")

model.eval_model(""data/test.txt"")

he then adds:

we assume that you have combined all the text in your dataset into two
  text files train.txt and test.txt which can be found in the data/
  directory.

i have 2 questions:
question 1
does the highlighted sentence above implies that the entire corpus will be merged into one text file?  so assuming that the training corpus is comprised of 1,000,000 text files, are we supposed to merge them all in one text file with code like this?
import fileinput
with open(outfilename, 'w') as fout, fileinput.input(filenames) as fin:
    for line in fin:
        fout.write(line)

question 2
i presume that i can use the pretrained model: bert-base-multilingual-cased.  correct?","['python-3.x', 'huggingface-transformers', 'language-model', 'simpletransformers']",61487842,"question 1
yes, the input to the train_model() and eval_model() methods need to be a single file.
dynamically loading from multiple files will likely be supported in the future
question 2
yes, you can use bert-base-multilingual-cased model.
you will find a much more detailed, updated guide on language model training here.
disclaimer: i am the creator of the above library.",https://stackoverflow.com/questions/61482810,python-3.x,28-04-2020 14:35,2151.0,1.0,1.0,True,10-12-2024 01:23,03-08-2020 12:12
72249052,how to use the pretrained model in an application?,"i dont understand how to use the trained model.
for example i trained the model using the code from 
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.preprocessing.text import tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

imdb, info = tfds.load(""imdb_reviews"",with_info=true, as_supervised=true)
train_data, test_data = imdb['train'], imdb['test']
training_sentences = []
training_labels = []
testing_sentences = []
testing_labels = []
for s,l in train_data:
    training_sentences.append(str(s.numpy()))
    training_labels.append(l.numpy())
for s,l in test_data:
    testing_sentences.append(str(s.numpy()))
    testing_labels.append(l.numpy())

training_labels_final = np.array(training_labels)
testing_labels_final = np.array(testing_labels)

vocab_size = 10000
embedding_dim=16
max_length = 120
trunc_type= 'post'
oov_tok=""<oov>""

tokenizer = tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(training_sentences)
word_index = tokenizer.word_index
word_index

sequences = tokenizer.texts_to_sequences(training_sentences)
padded = pad_sequences(sequences, maxlen=max_length, 
                       truncating = trunc_type)
testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sequences, maxlen=max_length)

model = tf.keras.sequential([
    tf.keras.layers.embedding(vocab_size, embedding_dim,
                             input_length=max_length),
    tf.keras.layers.simplernn(32),
    tf.keras.layers.dense(10, activation='relu'),
    tf.keras.layers.dense(1, activation='sigmoid')
])
model.summary()

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

num_epochs=30
history=model.fit(padded, training_labels_final, epochs=num_epochs, validation_data = (testing_padded, testing_labels_final))

it is all from this link.
then i added the last line of code
model.save('name.model')

then i open the new script where i want to use this model. i suggests that i will put a sentence (""he is a bad cat."") and i will get 0 or 1 depending wether it represents good sentiment or bad sentiment (i think i must get 0).
import tensorflow as tf
from tensorflow import keras 
import numpy as np
from tensorflow.keras.preprocessing.text import tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

model=tf.keras.models.load_model('name.model')

print(model.output_shape)

prediction=model.predict([""he is a bad cat.""])
print(prediction) 

and i get error.
question 1: in which format i must put this sentance? if i give this model to my friend how does he know in which format he must put the sentence to this model?
question 2: i noticed that the output will have (none,1) format. but i hoped to see one number (0 or 1) but not strange vector. what is going and how to get 0 or 1?","['tensorflow', 'nlp', 'lstm']",72574356,"before you make predictions you have to convert your text to input format.
for example, to make a prediction of ""he is a bad cat."" you have to tokenize the sentence.
#tokenizing the prediction data
sentence=['he is a bad cat.']
sequences=tokenizer.texts_to_sequences(sentence)

then you have to add padding to it
#padding the tokenized sentence
padded=pad_sequences(sequences,maxlen=max_length,truncating=trunc_type)

then you can make predictions on the padded sequence
model1.predict(padded) #output: array([[0.00079787]], dtype=float32)

the output will be a float not exactly 0 or 1 because the model will give the value based on the given sentence close to the negative or positive side on the embedding.
for more details please refer to this working gist. thank you.",https://stackoverflow.com/questions/72249052,tensorflow,15-05-2022 14:14,145.0,0.0,1.0,True,10-06-2022 12:31,15-05-2022 15:59
78953075,completions.create() got an unexpected keyword argument &#39;request_timeout&#39;,"i am using autogen from microsoft with the below code:
import autogen
from autogen import assistantagent, userproxyagent

config_list = [
    {
        'model': 'gpt-4',
        'api_key': 'api_key'
    }
]

llm_config={
    ""request_timeout"": 600,
    ""seed"": 42,
    ""config_list"": config_list,
    ""temperature"": 0
}

assistant = autogen.assistantagent(
    name=""assistant"",
    llm_config=llm_config,
    system_message=""chief technical officer of a tech company""
)

user_proxy = autogen.userproxyagent(
    name=""user_proxy"",
    human_input_mode=""always"",
    max_consecutive_auto_reply=10,
    is_termination_msg=lambda x: x.get(""content"", """").rstrip().endswith(""terminate""),
    code_execution_config={""work_dir"": ""web""},
    llm_config=llm_config,
    system_message=""""""reply terminate if the task has been solved at full satisfaction.
otherwise, reply continue, or the reason why the task is not solved yet.""""""
)

task = """"""
write python code to output numbers 1 to 100
""""""

user_proxy.initiate_chat(
    assistant,
    message=task
)

when i try to run the python, it gives me this error:
completions.create() got an unexpected keyword argument 'request_timeout'
[autogen.oai.client: 09-05 14:32:12] {164} warning - the api key specified is not a valid openai format; it won't work with the openai-hosted model.
[autogen.oai.client: 09-05 14:32:12] {164} warning - the api key specified is not a valid openai format; it won't work with the openai-hosted model.
user_proxy (to assistant):


write python code to output numbers 1 to 100


--------------------------------------------------------------------------------
traceback (most recent call last):
  file ""c:\users\hp\desktop\prj\autogen-ve\scripts\runningbots.py"", line 42, in <module>

how to resolve this?","['python', 'artificial-intelligence', 'openai-api', 'ms-autogen']",78953465,"please change your code to the following in order to make it work:
# config_list = [
#     {
#         'model': 'gpt-4o',
#         'api_key': 'api_key_here'
#     }
# ]
llm_config={""config_list"": config_list}

after this it should throw the error: you do not have access to the model, after which you should make a minimum payment of 5$ to open ai to access it",https://stackoverflow.com/questions/78953075,python,05-09-2024 12:37,1694.0,-1.0,2.0,True,19-01-2025 13:07,05-12-2024 13:35
78157432,openai api error: &quot;typeerror: openaiapi is not a constructor&quot;,"i am having an issue trying to get this code to call my openai_api_key in the .env file. i am very new to node.js, so please speak to me like i'm an idiot.
see the error at the bottom after the code.
code:
require('dotenv').config();
const { openaiapi } = require('openai');

// initialize openai api client with the api key from your .env file
const openaiclient = new openaiapi(process.env.openai_api_key);


/**
 * generates a response using chatgpt-4 turbo based on the provided user input.
 * @param {string} userinput - the user's message input.
 * @returns {promise<string>} - the generated response from chatgpt-4 turbo.
 */
async function generateresponse(userinput) {
  try {
    console.log('sending input to openai api:', userinput);
    const response = await openaiclient.createchatcompletion({
      model: ""gpt-4-turbo"",
      messages: [{
        role: ""user"",
        content: userinput
      }],
    });

    if (response.data.choices && response.data.choices.length > 0) {
      console.log('received response from openai api');
      return response.data.choices[0].message.content;
    } else {
      console.log('no response from openai api.');
      throw new error('no response from openai api');
    }
  } catch (error) {
    console.error('failed to generate response from openai api:', error.message, error.stack);
    throw error; // rethrow to handle it in the calling function
  }
}

module.exports = { generateresponse };

error:
l:\ai projects\gpt-pilot\workspace\js-rock_bot\chatservice.js:5
const openaiclient = new openaiapi(process.env.openai_api_key);
                     ^

typeerror: openaiapi is not a constructor
    at object.<anonymous> (l:\ai projects\gpt-pilot\workspace\js-rock_bot\chatservice.js:5:22)
    at module._compile (node:internal/modules/cjs/loader:1376:14)
    at module._extensions..js (node:internal/modules/cjs/loader:1435:10)
    at module.load (node:internal/modules/cjs/loader:1207:32)
    at module._load (node:internal/modules/cjs/loader:1023:12)
    at module.require (node:internal/modules/cjs/loader:1235:19)
    at require (node:internal/modules/helpers:176:18)
    at client.<anonymous> (l:\ai projects\gpt-pilot\workspace\js-rock_bot\bot.js:34:30)
    at client.emit (node:events:518:28)
    at messagecreateaction.handle (l:\ai projects\gpt-pilot\workspace\js-rock_bot\node_modules\discord.js\src\client\actions\messagecreate.js:28:14)

node.js v20.11.1

i also tried this but it's not working:
const { openaiapi } = require('openai');","['node.js', 'discord', 'openai-api', 'gpt-4']",78158916,"step 1: install the most recent openai node.js sdk
you said in the comment above that you would like to use the most recent syntax. so, first of all, make sure that you have the most recent openai node.js sdk installed. run the following command:
npm install openai@latest

as of today, v4.29.0 is the most recent one. before you proceed, check the version by running the following command:
npm view openai version

you should see 4.29.0 in the terminal.
step 2: fix the code
you have more than one mistake in your code, but you don't know about them yet because the initialization was the first error that was thrown. after you fix this error, only then will the next error be thrown.
the openai node.js sdk >=v4 (i.e., including the newest one) has a lot of breaking changes compared to <v4. among them, it's also the initialization that caused your error. but as i said, you have more mistakes in your code.
the following is the correct initialization using the openai node.js sdk >=v4:
const openai = require(""openai"");

const openai = new openai({
  apikey: process.env.openai_api_key,
});

the following is the correct method name using the openai node.js sdk >=v4:
openai.chat.completions.create

the following is the correct message retrieval using the openai node.js sdk >=v4:
response.choices[0].message.content

full code
require('dotenv').config();
const openai = require(""openai"");

// initialization
const openai = new openai({
  apikey: process.env.openai_api_key,
});

async function generateresponse(userinput) {
  try {
    console.log('sending input to openai api:', userinput);
    const response = await openai.chat.completions.create({ // method name
      model: ""gpt-4-turbo"",
      messages: [{
        role: ""user"",
        content: userinput
      }],
    });

    if (response.choices && response.choices.length > 0) {
      console.log('received response from openai api');
      return response.choices[0].message.content; // message retrieval
    } else {
      console.log('no response from openai api.');
      throw new error('no response from openai api');
    }
  } catch (error) {
    console.error('failed to generate response from openai api:', error.message, error.stack);
    throw error;
  }
}

module.exports = { generateresponse };",https://stackoverflow.com/questions/78157432,node.js,14-03-2024 00:00,433.0,-1.0,1.0,True,22-03-2024 12:35,22-03-2024 12:35
11351290,nltk tokenization and contractions,"i'm tokenizing text with nltk, just sentences fed to wordpunct_tokenizer. this splits contractions (e.g. 'don't' to 'don' +"" ' ""+'t') but i want to keep them as one word. i'm refining my methods for a more measured  and precise tokenization of text, so i need to delve deeper into the nltk tokenization module beyond simple tokenization. 
i'm guessing this is common and i'd like feedback from others who've maybe had to deal with the particular issue before.
edit: 
yeah this a general, splattershot question i know
also, as a novice to nlp, do i need to worry about contractions at all?
edit: 
the sexprtokenizer or treebankwordtokenizer seems to do what i'm looking for for now.","['python', 'nlp', 'nltk']",11354795,"which tokenizer you use really depends on what you want to do next. as inspectorg4dget said, some part-of-speech taggers handle split contractions, and in that case the splitting is a good thing. but maybe that's not what you want. to decide which tokenizer is best, consider what you need for the next step, and then submit your text to  to see how each nltk tokenizer behaves.",https://stackoverflow.com/questions/11351290,python,05-07-2012 19:32,15944.0,21.0,4.0,True,26-10-2022 19:56,31-08-2012 11:15
75258945,oserror: [e053] could not read meta.json from model-best.zip,"i tried to load the trained spacy model but this error appear:
oserror: [e053] could not read meta.json from model-best.zip

this is my code:
nlp_ner = spacy.load(""model-best.zip"")","['machine-learning', 'model', 'spacy', 'named-entity-recognition']",75262615,"for spacy 3, you can load models from 3 sources:

pretrained models (downloaded via a command like python -m spacy download your_model
custom models you have trained via spacy train.
loaded models via nlp.from_disk

usually, any of these models is stored as a folder or directory, with an structure similar to this one (for a ner model, which it seems it is what you are attempting to load):
/path/to/your/model/
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ model-best  <== this directory is what you might have
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ config.cfg
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ meta.json
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ner
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ cfg
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ model
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ moves
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ tok2vec
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ cfg
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ model
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
this discards ""loading directly from a .zip file"" as a valid option.
i think you may want to try the following:

try to unzip model-best.zip and see if you find a similar directory structure than the one shown above. if you are in a linux-based system, here is how.
if the previous structure is confirmed, then proceed with step 3, otherwise your file may be corrupted, or not a spacy model as such, and you won't be able to load the model.
try nlp_ner = spacy.load(""/path/to/your/model-best"") (nlp_ner = spacy.load(""./model-best"") in your case) and see if it works.

hope it helps.",https://stackoverflow.com/questions/75258945,machine-learning,27-01-2023 13:44,2179.0,0.0,1.0,True,28-01-2023 11:48,28-01-2023 11:48
73020900,unable to implement nltk.stopwords,"i am trying to remove stopwords in my data with nltk, but after several attempts i am unable to remove the stopwords. the tokenization part of my code works, but i am unable to understand why stopwords does not work.
def pre_process(text):
    
    # remove special characters and digits
    text=re.sub(""(\\d|\\w|_)+"","" "",text)
    text=re.split(""\w+"",text)
    
    return text
text = dat['text'].apply(lambda x:pre_process(x))
nltk.download('stopwords')

def remove_stopwords(text):
    for word in text:
        if word in stopwords.words('english'):
            text.remove(word)
        return text

text_stopword = text.apply(lambda x:remove_stopwords(x))

the code should remove words such as 'the', but after running my csv through the code, that words such as 'the' is still present.
current results:
text returns:
[tv, future, in, the, hands, of, viewers, with...
text_stopword returns:
[tv, future, in, the, hands, of, viewers, with...","['python', 'nltk']",73021239,"your return statement in remove_stopwords function is wrongly indented. due to that function returns text right after the first iteration.
please go with:
def remove_stopwords(text):
    for word in text:
        if word in stopwords.words('english'):
            text.remove(word)
    return text",https://stackoverflow.com/questions/73020900,python,18-07-2022 10:32,33.0,0.0,1.0,True,18-07-2022 10:56,18-07-2022 10:35
76482024,how to get more detailed results sources with langchain,"i am trying to put together a simple ""q&a with sources"" using langchain and a specific url as the source data. the url consists of a single page with quite a lot of information on it.
the problem is that retrievalqawithsourceschain is only giving me the entire url back as the source of the results, which is not very useful in this case.
is there a way to get more detailed source info?
perhaps the heading of the specific section on the page?
a clickable url to the correct section of the page would be even more helpful!
i am slightly unsure whether the generating of the result source is a function of the language model, url loader or simply retrievalqawithsourceschain alone.
i have tried using unstructuredurlloader and seleniumurlloader with the hope that perhaps more detailed reading and input of the data would help - sadly not.
relevant code excerpt:
llm = chatopenai(temperature=0, model_name='gpt-3.5-turbo')
chain = retrievalqawithsourceschain.from_llm(llm=llm, retriever=vectorstore.as_retriever())

result = chain({""question"": question})

print(result['answer'])
print(""\n sources : "",result['sources'] )","['python', 'openai-api', 'gpt-3', 'langchain', 'chatgpt-api']",76483595,"chatgpt is very flexible, and the more explicit you are better results you can get. this link show the docs for the function you are using. there is a parameter for langchain.prompts.baseprompttemplate that allows you to give chatgpt more explicit instructions.
it looks like the base prompt template is this

use the following knowledge triplets to answer the question at the end. if you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nquestion: {question}\nhelpful answer:

you can add in another sentence giving chatgpt more clear instructions

please format the answer with json of the form { ""answer"": ""{your_answer}"", ""relevant_quotes"": [""list of quotes""] }. substitutde your_answer as the answer to the question, but also include relevant quotes from the source material in the list.

you may need to tweak it a little bit to get chatgpt responding well. then you should be able to parse it.
chatgpt has 3 message types in the api

user - a message from an end user to the model
model - a message from the model to the end user
system - a message from the prompt engineer to model to add instructions. lang chain doesn't use this since it's a one-shot prompt

i strongly recommend these courses on chatgpt since they are from andrew ng and very high quality.",https://stackoverflow.com/questions/76482024,python,15-06-2023 11:31,8845.0,6.0,2.0,True,17-04-2024 04:15,15-06-2023 13:52
76990736,differences between langchain &amp; llamaindex,"i'm currently working on developing a chatbot powered by a large language model (llm), and i want it to provide responses based on my own documents. i understand that using a fine-tuned model on my documents might not yield direct responses, so i'm exploring the concept of retrieval-augmented generation (rag) to enhance its performance.
in my research, i've come across two tools, langchain and llamaindex, that seem to facilitate rag. however, i'm struggling to understand the main differences between them. i've noticed that some tutorials and resources use both tools simultaneously, and i'm curious about why one might choose to use one over the other or when it makes sense to use them together.
could someone please provide insights into the key distinctions between langchain and llamaindex for rag, and when it is beneficial to use one tool over the other or combine them in chatbot development?","['chatbot', 'langchain', 'large-language-model', 'llama-index']",77318216,"tl;dr
you'll be fine with just langchain, however, llamaindex is optimized for indexing, and retrieving data.

here are the details
to answer your question, it's important we go over the following terms:
retrieval-augmented generation
retrieval-augmented generation (or rag) is an architecture used to help large language models like gpt-4 provide better responses by using relevant information from additional sources and reducing the chances that an llm will leak sensitive data, or ï¿½ï¿½ï¿½hallucinateï¿½ï¿½ï¿½ incorrect or misleading information.
vector embeddings
vector embeddings are numerical vector representations of data. they are not only limited to text but can also represent images, videos, and other types of data. they are usually created using an embedding model such as openai's text-embedding-ada-002see here for more information)
langchain vs. llamaindex
let me start off by saying that it's not either langchain or llamaindex. as you mentioned in your question, both tools can be used together to enhance your rag application.
langchain
you can think of langchain as a framework rather than a tool. it provides a lot of tools right out of the box that enable you to interact with llms. key langchain components include chains. chains allow the chaining of components together, meaning you could use a prompttemplate and a llmchain to:

create a prompt
query a llm

here's a quick example:
...

prompt = prompttemplate(template=template, input_variables=[""questions""])

chain = llmchain(
    llm=llm,
    prompt=prompt
)

chain.run(query)

you can read more about langchain components here.
llamaindex
llamaindex, (previously known as gpt index), is a data framework specifically designed for llm apps. its primary focus is on ingesting, structuring, and accessing private or domain-specific data. it offers a set of tools that facilitate the integration of custom data into llms.
based on my experience with llamaindex, it is an ideal solution if you're looking to work with vector embeddings. using its many available plugins you could load (or ingest) data from many sources easily, and generate vector embeddings using an embedding model.
one key feature of llamaindex is that it is optimized for index querying. after the data is ingested, an index is created. this index represents your vectorized data and can be easily queried like so:
...

query_engine = index.as_query_engine()
response = query_engine.query(""stackoverflow is awesome."")

llamaindex abstracts this but it is essentially taking your query ""stackoverflow is awesome."" and comparing it with the most relevant information from your vectorized data (or index) which is then provided as context to the llm.
wrapping up
it should be clear to you now why you might choose one or both technologies for your specific use case. if your app requires indexing and retrieval capabilities, and while you'll be just fine using langchain (as it can handle that as well) i recommend integrating with llamaindex since it is optimized for that task and it is generally easier to ingest data using all the plugins and data connectors. otherwise, if you just need to work with llms stick with only langchain.
if you'd like to read more, i cover both langchain and llamaindex on my blog. here's a post looking at langchain and llamaindex.
note: i am the author of this post.",https://stackoverflow.com/questions/76990736,chatbot,28-08-2023 07:22,85314.0,137.0,3.0,True,03-07-2024 06:34,09-03-2024 00:23
62945590,how can i transform verbs from present tense to past tense with using nlp library?,"what i would like to do
i would like to transform verbs from  present tense to past tense with using nlp library like below.
as she leaves the kitchen, his voice follows her.

#output
as she left the kitchen, his voice followed her.

problem
there is no way to transform from present tense to past tense.
i've checked the following similar question, but they only introduced the way to transform from
past tense to present tense.

using nltk and wordnet; how do i convert simple tense verb into its present, past or past participle form?

what i tried to do
i was able to transform verbs from past tense to present tense using spacy.
however, there is no way to do the same thing from present tense to past tense.
text = ""as she left the kitchen, his voice followed her.""
doc_dep = nlp(text)
for i in range(len(doc_dep)):
    token = doc_dep[i]
    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_) 
    if token.pos_== 'verb':
        print(token.text)
        print(token.lemma_)
        text = text.replace(token.text, token.lemma_)
print(text)

#output
'as she leave the kitchen, his voice follow her.'

development environment
python 3.7.0
spacy version 2.3.1","['python', 'python-3.x', 'nlp', 'stanford-nlp', 'spacy']",63107066,"as far as i know spacy does not have any built-in function for this type of transformation, but you can use an extension where you map present/past tense pairs, and where you don't have the appropriate pairs 'ed' suffix for the past participle of weak verbs as below:
verb_map = {'leave': 'left'}

def make_past(token):
    return verb_map.get(token.text, token.lemma_ + 'ed')

spacy.tokens.token.set_extension('make_past', getter=make_past, force=true)

text = ""as she leave the kitchen, his voice follows her.""
doc_dep = nlp(text)
for i in range(len(doc_dep)):
    token = doc_dep[i]
    if token.tag_ in ['vbp', 'vbz']:
        print(token.text, token.lemma_, token.pos_, token.tag_) 
        text = text.replace(token.text, token._.make_past)
print(text)

output:
leave leave verb vbp
follows follow verb vbz
as she left the kitchen, his voice followed her.",https://stackoverflow.com/questions/62945590,python,17-07-2020 01:02,5977.0,8.0,3.0,True,13-03-2025 01:45,13-03-2025 01:43
78566807,how do you format the messages parameters in openai using model gpt-4 using python,"why do they have 20 different codes for 20 different models in openai?  in any case, i've tried to mimic my code as close as possible to existing codes found on the internet but i keep getting errors.  i realize that there are different endpoints (not that i know what an endpoint is) and you have to write code that fits to a specific endpoint but i think i've found the correct code for model gpt-4 and i still cannot get the code to work:
model = 'gpt-4'
prompt=f""translate the following from ancient greek into english: {txt}\n"",
messages = [{'role':'user','content': prompt}]
obj = client.chat.completions.create(model=model, messages=messages)

i've also tried:
messages = [{'role':'system','content': prompt}]

the error message i'm getting is:
openai.badrequesterror: error code: 400 - {'error': {'message': ""invalid type for 'messages[0].content[0]': expected an object, but got a string instead."", 'type': 'invalid_request_error', 'param': 'messages[0].content[0]', 'code': 'invalid_type'}}
i see very little python code on the chatgpt website regarding what you have do with different endpoints.  i was able to get some other openai code to work using the babbage model but it provided bad translations.",['openai-api'],78567004,"try this:
client = openai(
    api_key=api_key
)

response = client.chat.completions.create(
    model=""gpt-4"",
    messages=[
        {
            ""role"": ""system"",
            ""content"": ""you are a helpful assistant""
        },
        {
            ""role"": ""user"",
            ""content"": ""test""
        }
    ],
    temperature=0.5,
    max_tokens=64,
)

print(response.choices[0].message.content)

this gives me:
hello! how can i assist you today?

and it should work with all chat completion models.
on further testing of your code, i have found the culprit: the comma at the end of the line:
prompt=f""translate the following from ancient greek into english: {txt}\n"",

if you remove this, your error disappears.",https://stackoverflow.com/questions/78566807,openai-api,02-06-2024 14:59,1592.0,1.0,1.0,True,02-06-2024 16:20,02-06-2024 15:16
73182692,how to choose grid search (when working with trainer.hyperparameter_search)?,"i want to run trainer.hyperparameter_search (with grid search) and i haven't seen any hp algorithm type parameter.
how can i configure trainer.hyperparameter_search to run with grid-search ?","['python', 'deep-learning', 'huggingface-transformers', 'huggingface']",73216034,"you can use optuna for this:
def hp_search(trial):
    return {
        ""learning_rate"": trial.suggest_float(""learning_rate"", 5e-5, 5e-6, log=true),
        ""num_train_epochs"": trial.suggest_int(""num_train_epochs"", 3,10),
        ""per_device_train_batch_size"": trial.suggest_categorical(""per_device_train_batch_size"", [1,2,4,6,8,16,32]),
    }

trainer.hyperparameter_search(direction=""maximize"", hp_space=hp_space)

this thread should also bring more light to the task at hand.",https://stackoverflow.com/questions/73182692,python,31-07-2022 10:59,772.0,1.0,1.0,True,09-08-2022 14:01,09-08-2022 14:01
71100013,keras textvectorization adapt throws attributeerror,"i'm trying to apply text categorization using keras. i have imported my data as a pandas dataframe and have converted it to a tf.dataset. the problem is that i cannot use the textvectorization layer of keras as the below code throws this error:
attributeerror: 'nonetype' object has no attribute 'ndims'
my csv's headers:

class index : int32
title: string
description: string

what have i missed ? below is my code:
import re
import string
import tensorflow as tf
import pandas as pd
from tensorflow import keras

def create_dataset(csv_file, batch_size):
    df = pd.read_csv(csv_file)
    labels = df.pop('class index').transform(lambda x: x - 1)

    n_labels = len(pd.unique(labels))

    ds = tf.data.dataset.from_tensor_slices((dict(df), labels))
    ds = ds.batch(batch_size)
    ds = ds.shuffle(10000)

    return ds, n_labels


def load_data(data_dir, batch_size):
    train_ds, n_labels = create_dataset(data_dir + '/train.csv', batch_size)
    train_total_batches = len(train_ds)

    raw_test_ds, _ = create_dataset(data_dir + '/test.csv', batch_size)

    raw_train_ds = train_ds.take(int(round(0.8 * train_total_batches)));
    raw_val_ds = train_ds.skip(int(round(0.8 * train_total_batches))).take(int(round(0.2 * train_total_batches)));

    return raw_train_ds, raw_val_ds, raw_test_ds, n_labels


raw_train_ds, raw_val_ds, raw_test_ds, n_labels = load_data('.', 64)


def custom_standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    stripped_backslash = tf.strings.regex_replace(lowercase, ""\'"", '' '')
    return tf.strings.regex_replace(stripped_backslash, '[%s]' % re.escape(string.punctuation), '')


max_features = 5000
sequence_length = 250

vectorize_layer = keras.layers.experimental.preprocessing.textvectorization(
    standardize=custom_standardization,
    max_tokens=max_features,
    output_mode='int',
    output_sequence_length=sequence_length)

train_text = raw_train_ds.map(lambda x, y: x)
vectorize_layer.adapt(train_text)


edit 1: sample from train_test:
{'title': <tf.tensor: shape=(64,), dtype=string, numpy=
array([b'malaysia testing 3 people for bird flu; says outbreak isolated',
       b""kroger's profit climbs, misses forecast (reuters)"",
       b'blasts shake najaf as u.s. planes attack rebels',
       ...(omitted)
       b'kerry camp makes video to defuse attacks (ap)'], dtype=object)>, 'description': <tf.tensor: shape=(64,), dtype=string, numpy=
array([b'malaysian officials on saturday were testing three people who fell ill in a village hit by the deadly h5n1 bird flu strain, after international health officials warned that the virus appeared to be entrenched in parts of ',
       b'reuters - kroger co. , the top u.s.\\grocer, on tuesday said quarterly profit rose 29 percent as it\\kept a tight rein on expenses and sales rebounded.',
       b' najaf, iraq (reuters) - strong blasts were heard in the  besieged city of najaf early sunday as u.s. military planes  unleashed cannon and howitzer fire and a heavy firefight  erupted.',
       b'manny ramirez homered and drove in five runs as the red sox earned their fifth straight victory.',
      ...(omitted)
      dtype=object)>}","['python', 'pandas', 'tensorflow', 'keras', 'nlp']",71100212,"since you are using a internal dictionary, you can try something like this:
import tensorflow as tf


d ={""title"": [""malaysia testing 3 people for bird flu; says outbreak isolate"",
           ""kroger's profit climbs, misses forecast (reuters)"",
           ""blasts shake najaf as u.s. planes attack rebels""], 
 ""description"": [
                 ""kerry camp makes video to defuse attacks (ap)"", 
                 ""malaysian officials on saturday were testing three people who fell ill in a village hit by the deadly h5n1 bird flu strain, after international health officials warned that the virus appeared to be entrenched in parts of "", 
                 "" najaf, iraq (reuters) - strong blasts were heard in the  besieged city of najaf early sunday as u.s. military planes  unleashed cannon and howitzer fire and a heavy firefight  erupted.""
 ]}

train_text = tf.data.dataset.from_tensor_slices(d).batch(2)

max_features = 5000
sequence_length = 250

vectorize_layer = tf.keras.layers.textvectorization(
    max_tokens=max_features,
    output_mode='int',
    output_sequence_length=sequence_length)

#this example assumes that you have already excluded the labels.
#train_text = raw_train_ds.map(lambda x, y: x)

train_text = train_text.map(lambda x: tf.concat([x['title'], x['description']], axis=0))
vectorize_layer.adapt(train_text)

this example assumes that you have already excluded the labels.",https://stackoverflow.com/questions/71100013,python,13-02-2022 11:01,989.0,1.0,1.0,True,13-02-2022 12:15,13-02-2022 11:14
78001331,huggingface tokenizer not adding the padding tokens,"i am trying to follow this to translate english sentences to japanese.
using this line:
import torch
from transformers import autotokenizer
from auto_gptq import autogptqforcausallm

quantized_model_dir = ""webbigdata/alma-7b-ja-gptq-ja-en""
model_basename = ""gptq_model-4bit-128g""

tokenizer = autotokenizer.from_pretrained(quantized_model_dir)

model = autogptqforcausallm.from_quantized(
        quantized_model_dir,
        model_basename=model_basename,
        use_safetensors=true,
        device=""cuda:0"")

using this:
prompt1=""translate this from japanese to english:\njapanese: ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nenglish:""
input_ids = tokenizer(prompt1, return_tensors=""pt"", >
which is a sequence of 52 tokens.
if i input a different this may vary.
from the mode config i see max_length=512 (which i guess is the input size).
shouldn't the result from the tokenizer be of size 512 always do match the model's input size? or does this happen when the input is given to the model?
the arguments of the tokenizer imply that the padding is done there.","['python', 'python-3.x', 'huggingface-transformers', 'huggingface-tokenizers', 'machine-translation']",78002514,"depends on what you want to do with the padded tokens, most probably if you're going to just run inference or feed it to the trainer object, then you wont need special arguments to get the batch size shape to be a fixed length. the trainer object or model forward() function usually takes care of that.
p/s: it looks like you're using the alma machine translation model, i'm guessing you're trying to tune/use the model, so the tokenizer's output doesn't need to emit the pad tokens.

but if you would like to get the tokenizer to output the shape that's padded with the pad tokens, try this:
prompt1=""translate this from japanese to english:\njapanese: ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nenglish:&quout]:
torch.size([1, 200])

see",https://stackoverflow.com/questions/78001331,python,15-02-2024 13:48,1264.0,1.0,1.0,True,15-02-2024 16:44,15-02-2024 16:44
76743561,does hugging face model.generate for flan-t5 default is summarization?,"given the following code. why does the function:
model.generate()
returns a summary, where does it order to do summary and not some other task? where can i see the documentation for that as well.
model_name = ï¿½ï¿½ï¿½google/flan-t5-baseï¿½ï¿½ï¿½
model = automodelforseq2seqlm.from_pretrained(model_name)
dataset_name = ï¿½ï¿½ï¿½knkarthick/dialogsumï¿½ï¿½ï¿½
dataset = load_dataset(dataset_name)

for i in example_indices:
  dialog = dataset[ï¿½ï¿½ï¿½testï¿½ï¿½ï¿½][i][ï¿½ï¿½ï¿½dialogueï¿½ï¿½ï¿½]
  input = tokenizer(dialog,sentence,return_tensors=ï¿½ï¿½ï¿½ptï¿½ï¿½ï¿½)

  ground_truth = dataset[ï¿½ï¿½ï¿½testï¿½ï¿½ï¿½][i][ï¿½ï¿½ï¿½summaryï¿½ï¿½ï¿½]

  model_summary = model.generate(input[ï¿½ï¿½ï¿½input_idsï¿½ï¿½ï¿½],max_new_tokens=50)
  summa","['python', 'python-3.x', 'huggingface-transformers', 'huggingface']",76752292,"well, it's all in the dataset:
dataset_name = ï¿½ï¿½ï¿½knkarthick/dialogsumï¿½ï¿½ï¿½


dialogsum: a real-life scenario dialogue summarization dataset 
dialogsum is a large-scale dialogue summarization dataset, consisting of 13,460 dialogues with corresponding manually labeled summaries and topics.

transformer based models like t5, which you are using, are not explicitly told what to do at the time of inference. they learn to map from an input sequence to an output sequence. during training, the model was frequently exposed to a certain pattern (input: dialog, output: summary). now when you provide it with a similar input during inference, it is likely to produce a similar output.
so to summarize (no pun intended), this isn't any default behaviour for model.generate. it's just how your training dataset i",https://stackoverflow.com/questions/76743561,python,22-07-2023 11:26,961.0,1.0,2.0,True,13-08-2023 11:04,22-07-2023 17:56
76137110,openai whisper hangs/freezes on some audio files,"i am experiencing issues with openai's whisper and faster-whisper when processing audio files. specifically, some of the files fail to fully process and the progress bar freezes, occurring randomly across durations. i suspect this issue may be related to hardware performance, as i am using a mid-range gpu and cpu. i have attempted to fix the issue by breaking the speech recognition into smaller chunks, but the problem persists. can you suggest any debugging steps or solutions to this issue?","['speech-recognition', 'torch', 'openai-api', 'openai-whisper']",76162076,it does not seem to happen when i use cpu mode. perhaps it's a resource constraint issue.,https://stackoverflow.com/questions/76137110,speech-recognition,29-04-2023 15:49,2609.0,1.0,2.0,True,26-06-2023 15:12,07-06-2023 16:18
36572221,how to find ngram frequency of a column in a pandas dataframe?,"below is the input pandas dataframe i have.

i want to find the frequency of unigrams & bigrams. a sample of what i am expecting is shown below
how to do this using nltk or scikit learn?
i wrote the below code which takes a string as input. how to extend it to series/dataframe?
from nltk.collocations import *
desc='john is a guy person you him guy person you him'
tokens = nltk.word_tokenize(desc)
bigram_measures = nltk.collocations.bigramassocmeasures()
finder = bigramcollocationfinder.from_words(tokens)
finder.ngram_fd.viewitems()","['pandas', 'nlp', 'scikit-learn', 'nltk', 'text-mining']",36573116,"if your data is like
import pandas as pd
df = pd.dataframe([
    'must watch. good acting',
    'average movie. bad acting',
    'good movie. good acting',
    'pathetic. avoid',
    'avoid'], columns=['description'])

you could use the countvectorizer of the package sklearn:
from sklearn.feature_extraction.text import countvectorizer
word_vectorizer = countvectorizer(ngram_range=(1,2), analyzer='word')
sparse_matrix = word_vectorizer.fit_transform(df['description'])
frequencies = sum(sparse_matrix).toarray()[0]
pd.dataframe(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])

which gives you :
                frequency
good            3
pathetic        1
average movie   1
movie bad       2
watch           1
good movie      1
watch good      3
good acting     2
must            1
movie good      2
pathetic avoid  1
bad acting      1
average         1
must watch      1
acting          1
bad             1
movie           1
avoid           1

edit
fit will just ""train"" your vectorizer : it will split the words of your corpus and create a vocabulary with it. then transform can take a new document and create vector of frequency based on the vectorizer vocabulary.
here your training set is your output set, so you can do both at the same time (fit_transform). because you have 5 documents, it will create 5 vectors as a matrix. you want a global vector, so you have to make a sum.
edit 2
for big dataframes, you can speed up the frequencies computation by using:
frequencies = sum(sparse_matrix).data

or
frequencies = sparse_matrix.sum(axis=0).t",https://stackoverflow.com/questions/36572221,pandas,12-04-2016 11:39,16963.0,14.0,1.0,True,12-07-2022 08:37,19-07-2016 14:52
73791396,python: cosine similarity between sentences with synonyms,"how to calculate cosine similarity, if two sentences have any common word in the form of synonyms. for example,
sent1 = ""you are a good coder.""
sent2 = ""i am new programmer""
consider coder is synonym of programmer here. without considering these two specific words as synonym i get a cosine score as zero(0). but considering as synonyms, it should give some cosine value. please suggest how to approach or try to modify my below sample code. please consider a custom synonym-dictionary or list instead of any api-based dictionary.
import math
import re
from collections import counter

word = re.compile(r""\w+"")    
def get_cosine(vec1, vec2):
    intersection = set(vec1.keys()) & set(vec2.keys())
    numerator = sum([vec1[x] * vec2[x] for x in intersection])

    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])
    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])
    denominator = math.sqrt(sum1) * math.sqrt(sum2)

    if not denominator:
        return 0.0
    else:
        return float(numerator) / denominator    
def text_to_vector(text):
    words = word.findall(text)
    return counter(words)

synonyms = {""india"": ""hindustan"",
            ""usa"": ""america"",}    
text2 = ""i live in india""    
sentences = [""india"",
            ""he belongs to usa"", 
            ""hindustan is synonym of my country name"",
            ""usa and america is same"",
            ""you live in a great country."",
            ""all countries are great to live"",]    
cosinetolist = []
for i in sentences:
    vector1 = text_to_vector(i)
    vector2 = text_to_vector(text2) 
    cosine = get_cosine(vector1, vector2)
    cosinetolist.append((cosine,i,))
l = cosinetolist
print(l)","['python', 'nlp', 'python-re', 'cosine-similarity']",73791479,"try changing each word with its synonymin the sentence beforecomputing thesimilarity as so:
text = ' '.join([w if not w in synonyms  else synonyms[w] for w in text.split(' ')])

this splits the sentence, and for each word in the sentence it takes the synonym value if the word is in the keys.
thus giving:
import math
import re
from collections import counter

word = re.compile(r""\w+"")    
def get_cosine(vec1, vec2):
    intersection = set(vec1.keys()) & set(vec2.keys())
    numerator = sum([vec1[x] * vec2[x] for x in intersection])

    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])
    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])
    denominator = math.sqrt(sum1) * math.sqrt(sum2)

    if not denominator:
        return 0.0
    else:
        return float(numerator) / denominator    
def text_to_vector(text):
    words = word.findall(text)
    return counter(words)

synonyms = {""india"": ""hindustan"",
            ""usa"": ""america"",}    

def map_synon(text):
    return ' '.join([w if not w in synonyms  else synonyms[w] for w in text.split(' ')])
text2 = ""i live in india""
text2 = map_synon(text2)

sentences = [""he belongs to usa"", 
            ""hindustan is synonym of my country name"",
            ""usa and america is same"",
            ""you live in a great country."",
            ""all countries are great to live"",]    
cosinetolist = []


for i in sentences:
    vector1 = text_to_vector(map_synon(i))
    vector2 = text_to_vector(text2) 
    cosine = get_cosine(vector1, vector2)
    cosinetolist.append((cosine,i,))
l = cosinetolist
print(l)

output:
[(0.0, 'he belongs to usa'),
(0.1889822365046136, 'hindustan is synonym of my country name'),
(0.0, 'usa and america is same'),
(0.4082482904638631, 'you live in a great country.'), 
(0.20412414523193154, 'all countries are great to live')]",https://stackoverflow.com/questions/73791396,python,20-09-2022 18:51,384.0,0.0,1.0,True,20-09-2022 20:02,20-09-2022 20:02
75987139,openai chat completions api: why does it take so long to get a completion?,"i'm trying to use gpt-3.5 in my flutter app. i got answers but it takes 30-60 seconds to get a response. the code is the following:
 future<string> getresponse(string message) async {
    openai.apikey = openapikey;
    try {
      final chatcompletion = await openai.instance.chat.create(
        model: 'gpt-3.5-turbo',
        messages: [
          openaichatcompletionchoicemessagemodel(
            content: message,
            role: openaichatmessagerole.user,
          ),
        ],
      );
      print(chatcompletion);
      return chatcompletion.choices.first.message.content;
    } catch (e) {
      return ""something went wrong. please try again later."";
    }
  }

right now i have a personal account, and i donï¿½ï¿½ï¿½t have a paid subscription at the openai site. is something wrong with my code, or should i select a paid plan and will this solve the issue and will the response faster","['flutter', 'dart', 'openai-api', 'chatgpt-api']",75987183,"this is probably due to the openai server being overloaded.
as explained on the official openai forum by @rob.wheatley:

the last few days have been really quite bad. even with streaming, a
response could take a long time to start. but last night, as i was
testing my new streaming interface, i noticed some odd, but promising,
behavior. randomly, i would get very quick responses. they were rare
at first. /.../ this morning, all responses have been quick so far.
so, the whole thing looks like a capacity issue to me. not great if
you are building a commercial app.

sources:

discusion 1
discusion 2",https://stackoverflow.com/questions/75987139,flutter,11-04-2023 14:38,3621.0,-3.0,1.0,True,12-06-2024 17:05,12-06-2024 17:05
48395570,how to initialize word-embeddings for out of vocabulary word?,"i am trying to use conll-2003 ner (english) dataset and i am trying to utilize pretrained embeddings for it. i am using senna pretrained embeddings. now i have around 20k words in my vocabulary and out of this i have embedding available for only 9.5k words.
my current approach is to initialize an array of 20k x embedding_size with zeros and initialize the 9.5k words whose embeddings is known to me and make all the embeddings learn-able.
my question is what is the best way to do this? any reference to such research will be very helpful?","['machine-learning', 'nlp', 'deep-learning', 'word-embedding']",48397825,"i would suggest three ways to tackle this problem, each with different strengths: 

instead of using the senna embeddings, try using fasttext embeddings. the advantage here is that they can infer embeddings for oov words using character n-grams. for the exact methodology used, check the associated paper. gensim has implemented all the functionality needed. this will greatly reduce the problem, and you can further fine-tune the induced embeddings as you describe. the inconvenience is that you have to change from senna to fasttext.
try using morphological or sementic similarity to initialize the oov words. for morphological, i mean using a distance like levenshtein to select an embedding. for an oov word like apple, choose the closest (according to levenshtein distance) word that you have an embeddings for, e.g., apples. in my experience, this can work remarkably well. on the other hand, semantic similarity would suggest using for instance synonyms, obtained from resources like wordnet or even averaging the embeddings of words that the oov frequently co-occurs with. 
after having reduced the sparsity with the ways described above, then proceed with  or random initialization that is discussed in other responses.",https://stackoverflow.com/questions/48395570,machine-learning,23-01-2018 06:35,4997.0,2.0,3.0,True,21-08-2022 14:52,23-01-2018 10:48
73046919,why is model.fit working without clear attribute and label separation and the same method is not working for model.evaluate?,"i am working on building distillbert model for imdb dataset where the text is classified either as positive or negative. in my code i have first tokenised the 'text' data -
from datasets import load_dataset
imdb = load_dataset(""imdb"")

from transformers import autotokenizer
tokenizer = autotokenizer.from_pretrained(""distilbert-base-uncased"")

def preprocess_function(examples):
    return tokenizer(examples[""text""], truncation=true)

tokenized_imdb = imdb.map(preprocess_function, batched=true)

after this i have added padding and converted the dataset to tf dataset.
from transformers import datacollatorwithpadding
data_collator = datacollatorwithpadding(tokenizer=tokenizer, return_tensors=""tf"")

import tensorflow as tf
tf_train_set = tokenized_imdb[""train""].to_tf_dataset(
    columns=[""attention_mask"", ""input_ids"", ""label""],
    shuffle=true,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_set = tokenized_imdb[""test""].to_tf_dataset(
    columns=[""attention_mask"", ""input_ids"", ""label""],
    shuffle=false,
    batch_size=16,
    collate_fn=data_collator,
)

from transformers import create_optimizer

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb[""train""]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, 
num_train_steps=total_train_steps)

from transformers import tfautomodelforsequenceclassification

model = tfautomodelforsequenceclassification.from_pretrained(""distilbert-base- 
uncased"", num_labels=2)

model.compile(optimizer=optimizer,metrics=. 
['accuracy','sparse_categorical_accuracy','auc','precision','recall'])

then, i am trying to find out the base case accuracy of the pre-trained model for the same dataset as with which fine-tuning will be done, i.e., the model's accuracy before fine-tuning it for the downstream task.
base_model_result= model.evaluate(x=tf_validation_set) 

print(base_model_result)

this is where i am getting the error -
attributeerror: 'nonetype' object has no attribute 'shape'

this clearly means that i need to provide attribute and label values separately.
however, this same strategy is working fine for model.fit
results = model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)

now if i need to split tf_validation_set, how can i do that and make model.validation work. i have tried converting tf_validation_set to a list but it's still not working.
for resolving this issue i tried converting the tf dataset to list and then separate the attributes and the label column like so -
x = list(map(lambda x: [x['input_ids'],x['attention_mask']], tf_validation_set))
y = list(map(lambda x: x['labels'], tf_validation_set))

base_loss, base_acc = model.evaluate(x,y,verbose=1) 

print('base accuracy:', base_acc)

but here i am getting following error -
valueerror: data cardinality is ambiguous:make sure all arrays contain the same number of samples.


how can i fix this?","['python', 'tensorflow', 'machine-learning', 'keras', 'nlp']",73565819,"the input data could be:

a numpy array (or array-like), or a list of arrays (in case the model has multiple inputs).
a tensorflow tensor, or a list of tensors (in case the model has multiple inputs).
a dict mapping input names to the corresponding array/tensors, if the model has named inputs.
a tf.data dataset. should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights).
a generator or keras.utils.sequence returning (inputs, targets) or (inputs, targets, sample_weights).

the ""input data"" or the parameter ""x"" in the fit()/evaluate() method passed to the model is of type tf.data dataset that returns a  tuple of either (inputs, targets) or (inputs, targets, sample_weights).
if ""x"" is a tf.data dataset instance, ""y"" should not be specified (since targets will be obtained from x). kindly, refer this for more information.
your code is working fine in colab, please find the gist here. thank you!",https://stackoverflow.com/questions/73046919,python,20-07-2022 06:31,153.0,1.0,1.0,True,01-09-2022 07:44,20-07-2022 08:19
78443980,fasttext language_identification in r returns too many arguments - how to match to texts?,"fasttext language_identification returns multiple predictions per original text, and also fails to indicate which belong to which original document.
there are differing numbers of predictions per original document too -- their github forums are closed now, but does anyone know how to match the output to the original texts?
code:
df = data.frame(doc_id = seq(1, 5),
speechtext = c(""hello. fake text entry 1."", ""fake text entry 2"", ""more text"", ""text in a
different language"", ""hola""))

library(fasttext)
# download .ftz pretrained model from 
file_ftz = system.file(""language_identification/lid.176.ftz"", package = ""fasttext"")
lang1 = language_identification(df$speechtext,
                                pre_trained_language_model_path = file_ftz,
                                verbose = t)

i was expecting one prediction per original text, or at least a consistent number, or some way of marking which document the predictions align with.
really i could guess based on the largest number per series of a few elements outputted, but this doesn't seem optimal -- it does seem like a bug.
(i tried adding intern = t as an argument per r - fasttext how to load output into a dataframe from command line -- this is not recognized as an argument).","['r', 'nlp', 'fasttext', 'language-detection']",78444037,"the first argument to fasttext::language_identification() is defined as:

either a valid character string to a valid path where each line represents a different text extract or a vector of text extracts (emphasis mine)

you have line breaks in your input data:
df$speechtext[4]
[1] ""text in a\ndifferent language""

as one prediction is generated per line, you'll get two predictions from this element. you have two options:

remove new lines in your input data. this makes sense in this case.
keep new lines and map document ids to each line. this makes sense if new lines might actually be in different languages.

remove new lines
if you replace new lines with spaces you will get the same number of predictions returned as input rows.
in the regex below, i have used the pcre \v which matches newlines and any character considered vertical whitespace. this now produces five rows, one relating to each input row.
language_identification(gsub(""\\v"", "" "", df$speechtext, perl = true), file_ftz)
#    iso_lang_1   prob_1
#        <char>    <num>
# 1:         en 0.220767
# 2:         en 0.388695
# 3:         en 0.613707
# 4:         en 0.757671
# 5:         es 0.721487

\v includes several vertical space characters (such as form feed and line separator), so should cover all possible types of new line. for full details see the table here.
keep new lines and map document id to each line
alternatively, if different lines of each input document might be in different languages, you may not want to remove new lines. in this case, you can predict each line separately and then map the document ids to each line:
# as before
lang1 <- language_identification(df$speechtext, file_ftz)

# add document ids
lang1$doc_id <- rep(
    df$doc_id,
    lengths(strsplit(df$speechtext, ""\\v"", perl = true))
)

lang1
#    iso_lang_1   prob_1 doc_id
#        <char>    <num>  <int>
# 1:         en 0.220767      1
# 2:         en 0.388695      2
# 3:         en 0.613707      3
# 4:         en 0.932691      4
# 5:         en 0.571937      4
# 6:         es 0.721487      5",https://stackoverflow.com/questions/78443980,r,07-05-2024 16:46,132.0,1.0,1.0,True,09-05-2024 07:13,07-05-2024 16:55
8967544,using stanford corenlp,"i am trying to get around using the stanford corenlp. i used some code from the web to understand what is going on with the coreference tool. i tried running the project in eclipse but keep encountering an out of memory exception. i tried increasing the heap size but there isnt any difference.
why this keeps happening? is this a code specific problem?
here is my code:
import edu.stanford.nlp.dcoref.corefchain;
import edu.stanford.nlp.dcoref.corefcoreannotations;
import edu.stanford.nlp.pipeline.annotation;
import edu.stanford.nlp.pipeline.stanfordcorenlp;


import java.util.iterator;
import java.util.map;
import java.util.properties;


public class testmain {

    public static void main(string[] args) {

        string text = ""viki is a smart boy. he knows a lot of things."";
        annotation document = new annotation(text);
        properties props = new properties();
        props.put(""annotators"", ""tokenize, ssplit, pos, parse, dcoref"");
        stanfordcorenlp pipeline = new stanfordcorenlp(props);
        pipeline.annotate(document);


        map<integer, corefchain> graph = document.get(corefcoreannotations.corefchainannotation.class);



        iterator<integer> itr = graph.keyset().iterator();
    
        while (itr.hasnext()) {
        
             string key = itr.next().tostring();
        
             string value = graph.get(key).tostring();
        
             system.out.println(key + "" "" + value);      
        }

   }
}","['java', 'eclipse', 'nlp', 'stanford-nlp']",8971424,"i found similar problem when building small application using stanford corenlp in eclipse.
increasing eclipse's heap size will not solve your problem.
after doing search, it is ant build tool heap size that should be increased, but i have no idea how to do that.
so i give up eclipse and use netbeans instead.
ps: you will eventually get out of memory exception with default setting in netbeans. but it can easily solved by adjust setting -xms per application basis.",https://stackoverflow.com/questions/8967544,java,23-01-2012 05:29,9441.0,7.0,3.0,True,24-03-2025 16:34,24-03-2025 16:34
78863932,runtimeerror: numpy is not available (transformers),"i basically just want to use the transformers pipeline() to classify data, but independent of which model i try to use, it returns the same error, stating numpy is not available
code i'm running:
pipe = pipeline(""text-classification"", model=""adamlucek/roberta-llama3.1405b-twitter-sentiment"")   
sentiment_pipeline('today is a great day!')
# other model i've tried: 
sentiment_pipeline = pipeline(model=""cardiffnlp/twitter-roberta-base-sentiment-latest"", tokenizer=""cardiffnlp/twitter-roberta-base-sentiment-latest"")
sentiment_pipeline('today is a great day!')

error i receive:
runtimeerror                              traceback (most recent call last)
cell in[49], line 1
----> 1 sentiment_pipeline('today is a great day!')

file ~\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\transformers\pipelines\text_classification.py:156, in textclassificationpipeline.__call__(self, inputs, **kwargs)
    122 """"""
    123 classify the text(s) given as inputs.
    124 
   (...)
    153     if `top_k` is used, one such dictionary is returned per label.
    154 """"""
    155 inputs = (inputs,)
--> 156 result = super().__call__(*inputs, **kwargs)
    157 # todo try and retrieve it in a nicer way from _sanitize_parameters.
    158 _legacy = ""top_k"" not in kwargs

file ~\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\transformers\pipelines\base.py:1257, in pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)
   1249     return next(
   1250         iter(
   1251             self.get_iterator(
   (...)
   1254         )
   1255     )
   1256 else:
-> 1257     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)

file ~\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\transformers\pipelines\base.py:1265, in pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)
   1263 model_inputs = self.preprocess(inputs, **preprocess_params)
   1264 model_outputs = self.forward(model_inputs, **forward_params)
-> 1265 outputs = self.postprocess(model_outputs, **postprocess_params)
   1266 return outputs

file ~\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\transformers\pipelines\text_classification.py:208, in textclassificationpipeline.postprocess(self, model_outputs, function_to_apply, top_k, _legacy)
    204 outputs = model_outputs[""logits""][0]
    206 if self.framework == ""pt"":
    207     # to enable using fp16 and bf16
--> 208     outputs = outputs.float().numpy()
    209 else:
    210     outputs = outputs.numpy()

runtimeerror: numpy is not available

i already tried simply un- and reinstalling transformers and numpy and for both the most recent versions are installed (and should be compatible).
anyone has an idea on how to solve this?","['python', 'python-3.x', 'numpy', 'huggingface-transformers']",78864891,"try:
pip install ""numpy<2""

then restart the kernel.",https://stackoverflow.com/questions/78863932,python,12-08-2024 23:43,976.0,3.0,1.0,True,13-08-2024 07:23,13-08-2024 07:23
64337550,"neither pytorch nor tensorflow &gt;= 2.0 have been found.models won&#39;t be available and only tokenizers, configuration and file/data utilities can be used","i am trying to install transformers using pip
pip install transformers

after import transformers
this error show
neither pytorch nor tensorflow >= 2.0 have been found.models won't be available and only tokenizers, configuration, and file/data utilities can be used.

although i install tensorflow-gpu= 2.3.1 and using conda
system info
windows 10 
python 3.6
cuda 10.1
tensorflow-gpu= 2.3.1","['python', 'tensorflow', 'nlp']",64351300,"i found the problem after investigate for 10 hours
i installed tensorflow  by using conda install tensorflow-gpu 
and transformers by using pip after remove tensorflow-gpu and install it by using pip
it works fine",https://stackoverflow.com/questions/64337550,python,13-10-2020 14:49,59224.0,28.0,7.0,True,02-04-2025 08:20,13-10-2020 14:55
45336491,sklearn - feature extraction from text - normalize text features by merging plural and singular forms,"i am doing some text classification right now using sklearn.
as first step i obviously need to use vectorizer - either countvectorizer or tfidfvectorizer. the issue which i want to tackle is that in my documents often times i have singular and plural forms of same word. when performing vectorization i want to 'merge' singular and plural forms and treat them as a same text feature.
obviously i can manually pre-process texts and just replace all plural word forms with singular word forms when i know which words have this issue. but maybe there is some way to do it in a more automated way, when words which are extremely similar to each other are merged into same feature?
update.
based on the answer provided earlier, i needed to perform a stemming. below is a sample code which stems all words in 'review' column of a dataframe df, which i then use in vectorization and classification. just in case anyone finds it useful.
from nltk.stem.snowball import snowballstemmer

stemmer = snowballstemmer(""english"")


df['review_token']=df['review'].apply(lambda x : filter(none,x.split("" "")))

df['review_stemmed']=df['review_token'].apply(lambda x : [stemmer.stem(y) for y in x])

df['review_stemmed_sentence']=df['review_stemmed'].apply(lambda x : "" "".join(x))","['python', 'scikit-learn', 'text-mining', 'feature-extraction', 'text-classification']",45337495,"i think what you need is stemming, namely removing the endings of words that have a common root, and it's one of the basic operations in preprocessing text data.
here's some rules for stemming and lemmatization explained:",https://stackoverflow.com/questions/45336491,python,26-07-2017 20:10,1181.0,1.0,1.0,True,21-03-2022 00:27,21-03-2022 00:27
68155580,sparknlp java error while trying to display model results,"i'm trying to output the results from a practice nlp model created using spark-nlp. however, i keep getting the error below. can anyone help me out here. the .show() method works earlier in the code, when i attempt to output the dataframe. it just fails whenever i attempt to output any parts of the model results.
i'm running the code from jupyter notebook on a windows machine. i have pyspark spark-3.0.3 with hadoop 2.7 on my machine.
code used
import findspark
findspark.init()
findspark.find()
import pyspark

import sparknlp
from sparknlp.base import *
from sparknlp.annotator import *
from pyspark.ml import pipeline

spark = sparknlp.start()

data = spark.createdataframe([['peter is a godo person living in germany. paula is also a good person. she lives in london']]).todf('text')

data.show(truncate=false)

document = documentassembler().setinputcol('text').setoutputcol('document').setcleanupmode('shrink')

sentence = sentencedetector().setinputcols('document').setoutputcol('sentence')

sentence.setexplodesentences(true)

tokenizer = tokenizer().setinputcols('sentence').setoutputcol('token')

checker = norvigsweetingmodel.pretrained().setinputcols(['token']).setoutputcol('checked')

embeddings = wordembeddingsmodel.pretrained().setinputcols(['sentence','checked']).setoutputcol('embeddings')

ner = nerdlmodel.pretrained().setinputcols(['sentence','checked','embeddings']).setoutputcol('ner')

converter = nerconverter().setinputcols(['sentence','checked','ner']).setoutputcol('chunk')

pipeline = pipeline().setstages([document,sentence,tokenizer,checker,embeddings,ner,converter])

model = pipeline.fit(data)

result = model.transform(data)

#line that triggers error
result.select('chunk.result').show(truncate=false)

error
---------------------------------------------------------------------------
py4jjavaerror                             traceback (most recent call last)
<ipython-input-75-4f3ba5a75c4a> in <module>
----> 1 result.select('chunk.result').show(truncate=false)

c:\spark\python\pyspark\sql\dataframe.py in show(self, n, truncate, vertical)
    440             print(self._jdf.showstring(n, 20, vertical))
    441         else:
--> 442             print(self._jdf.showstring(n, int(truncate), vertical))
    443 
    444     def __repr__(self):

c:\spark\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py in __call__(self, *args)
   1302 
   1303         answer = self.gateway_client.send_command(command)
-> 1304         return_value = get_return_value(
   1305             answer, self.gateway_client, self.target_id, self.name)
   1306 

c:\spark\python\pyspark\sql\utils.py in deco(*a, **kw)
    126     def deco(*a, **kw):
    127         try:
--> 128             return f(*a, **kw)
    129         except py4j.protocol.py4jjavaerror as e:
    130             converted = convert_exception(e.java_exception)

c:\spark\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py in get_return_value(answer, gateway_client, target_id, name)
    324             value = output_converter[type](answer[2:], gateway_client)
    325             if answer[1] == reference_type:
--> 326                 raise py4jjavaerror(
    327                     ""an error occurred while calling {0}{1}{2}.\n"".
    328                     format(target_id, ""."", name), value)

py4jjavaerror: an error occurred while calling o1393.showstring.
: org.apache.spark.sparkexception: job aborted due to stage failure: task 6 in stage 39.0 failed 1 times, most recent failure: lost task 6.0 in stage 39.0 (tid 174, desktop-g6lq7l8, executor driver): org.apache.spark.sparkexception: failed to execute user defined function(hassimpleannotate$$lambda$2720/1692472191: (array<array<struct<annotatortype:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>>) => array<struct<annotatortype:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)
    at org.apache.spark.sql.catalyst.expressions.generatedclass$generatediteratorforcodegenstage2.processnext(unknown source)
    at org.apache.spark.sql.execution.bufferedrowiterator.hasnext(bufferedrowiterator.java:43)
    at org.apache.spark.sql.execution.wholestagecodegenexec$$anon$1.hasnext(wholestagecodegenexec.scala:729)
    at scala.collection.iterator$$anon$10.hasnext(iterator.scala:458)
    at scala.collection.iterator$$anon$10.hasnext(iterator.scala:458)
    at scala.collection.iterator$groupediterator.fill(iterator.scala:1209)
    at scala.collection.iterator$groupediterator.hasnext(iterator.scala:1215)
    at scala.collection.iterator$$anon$11.hasnext(iterator.scala:489)
    at scala.collection.iterator$$anon$10.hasnext(iterator.scala:458)
    at scala.collection.iterator$$anon$10.hasnext(iterator.scala:458)
    at org.apache.spark.sql.catalyst.expressions.generatedclass$generatediteratorforcodegenstage3.processnext(unknown source)
    at org.apache.spark.sql.execution.bufferedrowiterator.hasnext(bufferedrowiterator.java:43)
    at org.apache.spark.sql.execution.wholestagecodegenexec$$anon$1.hasnext(wholestagecodegenexec.scala:729)
    at org.apache.spark.sql.execution.sparkplan.$anonfun$getbytearrayrdd$1(sparkplan.scala:345)
    at org.apache.spark.rdd.rdd.$anonfun$mappartitionsinternal$2(rdd.scala:872)
    at org.apache.spark.rdd.rdd.$anonfun$mappartitionsinternal$2$adapted(rdd.scala:872)
    at org.apache.spark.rdd.mappartitionsrdd.compute(mappartitionsrdd.scala:52)
    at org.apache.spark.rdd.rdd.computeorreadcheckpoint(rdd.scala:349)
    at org.apache.spark.rdd.rdd.iterator(rdd.scala:313)
    at org.apache.spark.scheduler.resulttask.runtask(resulttask.scala:90)
    at org.apache.spark.scheduler.task.run(task.scala:127)
    at org.apache.spark.executor.executor$taskrunner.$anonfun$run$3(executor.scala:463)
    at org.apache.spark.util.utils$.trywithsafefinally(utils.scala:1377)
    at org.apache.spark.executor.executor$taskrunner.run(executor.scala:466)
    at java.util.concurrent.threadpoolexecutor.runworker(unknown source)
    at java.util.concurrent.threadpoolexecutor$worker.run(unknown source)
    at java.lang.thread.run(unknown source)
caused by: java.lang.exception: feature number of words in the dictionary is not set
    at com.johnsnowlabs.nlp.serialization.feature.$anonfun$getordefault$1(feature.scala:81)
    at scala.option.getorelse(option.scala:189)
    at com.johnsnowlabs.nlp.serialization.feature.getordefault(feature.scala:81)
    at com.johnsnowlabs.nlp.hasfeatures.$$(hasfeatures.scala:39)
    at com.johnsnowlabs.nlp.hasfeatures.$$$(hasfeatures.scala:39)
    at com.johnsnowlabs.nlp.annotatormodel.$$(annotatormodel.scala:14)
    at com.johnsnowlabs.nlp.annotators.spell.norvig.norvigsweetingmodel.allwords$lzycompute(norvigsweetingmodel.scala:125)
    at com.johnsnowlabs.nlp.annotators.spell.norvig.norvigsweetingmodel.allwords(norvigsweetingmodel.scala:124)
    at com.johnsnowlabs.nlp.annotators.spell.norvig.norvigsweetingmodel.getsuggestion(norvigsweetingmodel.scala:189)
    at com.johnsnowlabs.nlp.annotators.spell.norvig.norvigsweetingmodel.getbestspellingsuggestion(norvigsweetingmodel.scala:170)
    at com.johnsnowlabs.nlp.annotators.spell.norvig.norvigsweetingmodel.checkspellword(norvigsweetingmodel.scala:154)
    at com.johnsnowlabs.nlp.annotators.spell.norvig.norvigsweetingmodel.$anonfun$annotate$1(norvigsweetingmodel.scala:137)
    at scala.collection.traversablelike.$anonfun$map$1(traversablelike.scala:238)
    at scala.collection.mutable.resizablearray.foreach(resizablearray.scala:62)
    at scala.collection.mutable.resizablearray.foreach$(resizablearray.scala:55)
    at scala.collection.mutable.arraybuffer.foreach(arraybuffer.scala:49)
    at scala.collection.traversablelike.map(traversablelike.scala:238)
    at scala.collection.traversablelike.map$(traversablelike.scala:231)
    at scala.collection.abstracttraversable.map(traversable.scala:108)
    at com.johnsnowlabs.nlp.annotators.spell.norvig.norvigsweetingmodel.annotate(norvigsweetingmodel.scala:136)
    at com.johnsnowlabs.nlp.hassimpleannotate.$anonfun$dfannotate$1(hassimpleannotate.scala:24)
    ... 27 more

driver stacktrace:
    at org.apache.spark.scheduler.dagscheduler.failjobandindependentstages(dagscheduler.scala:2059)
    at org.apache.spark.scheduler.dagscheduler.$anonfun$abortstage$2(dagscheduler.scala:2008)
    at org.apache.spark.scheduler.dagscheduler.$anonfun$abortstage$2$adapted(dagscheduler.scala:2007)
    at scala.collection.mutable.resizablearray.foreach(resizablearray.scala:62)
    at scala.collection.mutable.resizablearray.foreach$(resizablearray.scala:55)
    at scala.collection.mutable.arraybuffer.foreach(arraybuffer.scala:49)
    at org.apache.spark.scheduler.dagscheduler.abortstage(dagscheduler.scala:2007)
    at org.apache.spark.scheduler.dagscheduler.$anonfun$handletasksetfailed$1(dagscheduler.scala:973)
    at org.apache.spark.scheduler.dagscheduler.$anonfun$handletasksetfailed$1$adapted(dagscheduler.scala:973)
    at scala.option.foreach(option.scala:407)
    at org.apache.spark.scheduler.dagscheduler.handletasksetfailed(dagscheduler.scala:973)
    at org.apache.spark.scheduler.dagschedulereventprocessloop.doonreceive(dagscheduler.scala:2239)
    at org.apache.spark.scheduler.dagschedulereventprocessloop.onreceive(dagscheduler.scala:2188)
    at org.apache.spark.scheduler.dagschedulereventprocessloop.onreceive(dagscheduler.scala:2177)
    at org.apache.spark.util.eventloop$$anon$1.run(eventloop.scala:49)
    at org.apache.spark.scheduler.dagscheduler.runjob(dagscheduler.scala:775)
    at org.apache.spark.sparkcontext.runjob(sparkcontext.scala:2114)
    at org.apache.spark.sparkcontext.runjob(sparkcontext.scala:2135)
    at org.apache.spark.sparkcontext.runjob(sparkcontext.scala:2154)
    at org.apache.spark.sql.execution.sparkplan.executetake(sparkplan.scala:472)
    at org.apache.spark.sql.execution.sparkplan.executetake(sparkplan.scala:425)
    at org.apache.spark.sql.execution.collectlimitexec.executecollect(limit.scala:47)
    at org.apache.spark.sql.dataset.collectfromplan(dataset.scala:3627)
    at org.apache.spark.sql.dataset.$anonfun$head$1(dataset.scala:2697)
    at org.apache.spark.sql.dataset.$anonfun$withaction$1(dataset.scala:3618)
    at org.apache.spark.sql.execution.sqlexecution$.$anonfun$withnewexecutionid$5(sqlexecution.scala:100)
    at org.apache.spark.sql.execution.sqlexecution$.withsqlconfpropagated(sqlexecution.scala:160)
    at org.apache.spark.sql.execution.sqlexecution$.$anonfun$withnewexecutionid$1(sqlexecution.scala:87)
    at org.apache.spark.sql.sparksession.withactive(sparksession.scala:767)
    at org.apache.spark.sql.execution.sqlexecution$.withnewexecutionid(sqlexecution.scala:64)
    at org.apache.spark.sql.dataset.withaction(dataset.scala:3616)
    at org.apache.spark.sql.dataset.head(dataset.scala:2697)
    at org.apache.spark.sql.dataset.take(dataset.scala:2904)
    at org.apache.spark.sql.dataset.getrows(dataset.scala:300)
    at org.apache.spark.sql.dataset.showstring(dataset.scala:337)
    at sun.reflect.nativemethodaccessorimpl.invoke0(native method)
    at sun.reflect.nativemethodaccessorimpl.invoke(unknown source)
    at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)
    at java.lang.reflect.method.invoke(unknown source)
    at py4j.reflection.methodinvoker.invoke(methodinvoker.java:244)
    at py4j.reflection.reflectionengine.invoke(reflectionengine.java:357)
    at py4j.gateway.invoke(gateway.java:282)
    at py4j.commands.abstractcommand.invokemethod(abstractcommand.java:132)
    at py4j.commands.callcommand.execute(callcommand.java:79)
    at py4j.gatewayconnection.run(gatewayconnection.java:238)
    at java.lang.thread.run(unknown source)
caused by: org.apache.spark.sparkexception: failed to execute user defined function(hassimpleannotate$$lambda$2720/1692472191: (array<array<struct<annotatortype:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>>) => array<struct<annotatortype:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)
    at org.apache.spark.sql.catalyst.expressions.generatedclass$generatediteratorforcodegenstage2.processnext(unknown source)
    at org.apache.spark.sql.execution.bufferedrowiterator.hasnext(bufferedrowiterator.java:43)
    at org.apache.spark.sql.execution.wholestagecodegenexec$$anon$1.hasnext(wholestagecodegenexec.scala:729)
    at scala.collection.iterator$$anon$10.hasnext(iterator.scala:458)
    at scala.collection.iterator$$anon$10.hasnext(iterator.scala:458)
    at scala.collection.iterator$groupediterator.fill(iterator.scala:1209)
    at scala.collection.iterator$groupediterator.hasnext(iterator.scala:1215)
    at scala.collection.iterator$$anon$11.hasnext(iterator.scala:489)
    at scala.collection.iterator$$anon$10.hasnext(iterator.scala:458)
    at scala.collection.iterator$$anon$10.hasnext(iterator.scala:458)
    at org.apache.spark.sql.catalyst.expressions.generatedclass$generatediteratorforcodegenstage3.processnext(unknown source)
    at org.apache.spark.sql.execution.bufferedrowiterator.hasnext(bufferedrowiterator.java:43)
    at org.apache.spark.sql.execution.wholestagecodegenexec$$anon$1.hasnext(wholestagecodegenexec.scala:729)
    at org.apache.spark.sql.execution.sparkplan.$anonfun$getbytearrayrdd$1(sparkplan.scala:345)
    at org.apache.spark.rdd.rdd.$anonfun$mappartitionsinternal$2(rdd.scala:872)
    at org.apache.spark.rdd.rdd.$anonfun$mappartitionsinternal$2$adapted(rdd.scala:872)
    at org.apache.spark.rdd.mappartitionsrdd.compute(mappartitionsrdd.scala:52)
    at org.apache.spark.rdd.rdd.computeorreadcheckpoint(rdd.scala:349)
    at org.apache.spark.rdd.rdd.iterator(rdd.scala:313)
    at org.apache.spark.scheduler.resulttask.runtask(resulttask.scala:90)
    at org.apache.spark.scheduler.task.run(task.scala:127)
    at org.apache.spark.executor.executor$taskrunner.$anonfun$run$3(executor.scala:463)
    at org.apache.spark.util.utils$.trywithsafefinally(utils.scala:1377)
    at org.apache.spark.executor.executor$taskrunner.run(executor.scala:466)
    at java.util.concurrent.threadpoolexecutor.runworker(unknown source)
    at java.util.concurrent.threadpoolexecutor$worker.run(unknown source)
    ... 1 more
caused by: java.lang.exception: feature number of words in the dictionary is not set
    at com.johnsnowlabs.nlp.serialization.feature.$anonfun$getordefault$1(feature.scala:81)
    at scala.option.getorelse(option.scala:189)
    at com.johnsnowlabs.nlp.serialization.feature.getordefault(feature.scala:81)
    at com.johnsnowlabs.nlp.hasfeatures.$$(hasfeatures.scala:39)
    at com.johnsnowlabs.nlp.hasfeatures.$$$(hasfeatures.scala:39)
    at com.johnsnowlabs.nlp.annotatormodel.$$(annotatormodel.scala:14)
    at com.johnsnowlabs.nlp.annotators.spell.norvig.norvigsweetingmodel.allwords$lzycompute(norvigsweetingmodel.scala:125)
    at com.johnsnowlabs.nlp.annotators.spell.norvig.norvigsweetingmodel.allwords(norvigsweetingmodel.scala:124)
    at com.johnsnowlabs.nlp.annotators.spell.norvig.norvigsweetingmodel.getsuggestion(norvigsweetingmodel.scala:189)
    at com.johnsnowlabs.nlp.annotators.spell.norvig.norvigsweetingmodel.getbestspellingsuggestion(norvigsweetingmodel.scala:170)
    at com.johnsnowlabs.nlp.annotators.spell.norvig.norvigsweetingmodel.checkspellword(norvigsweetingmodel.scala:154)
    at com.johnsnowlabs.nlp.annotators.spell.norvig.norvigsweetingmodel.$anonfun$annotate$1(norvigsweetingmodel.scala:137)
    at scala.collection.traversablelike.$anonfun$map$1(traversablelike.scala:238)
    at scala.collection.mutable.resizablearray.foreach(resizablearray.scala:62)
    at scala.collection.mutable.resizablearray.foreach$(resizablearray.scala:55)
    at scala.collection.mutable.arraybuffer.foreach(arraybuffer.scala:49)
    at scala.collection.traversablelike.map(traversablelike.scala:238)
    at scala.collection.traversablelike.map$(traversablelike.scala:231)
    at scala.collection.abstracttraversable.map(traversable.scala:108)
    at com.johnsnowlabs.nlp.annotators.spell.norvig.norvigsweetingmodel.annotate(norvigsweetingmodel.scala:136)
    at com.johnsnowlabs.nlp.hassimpleannotate.$anonfun$dfannotate$1(hassimpleannotate.scala:24)
    ... 27 more","['python', 'apache-spark', 'pyspark', 'nlp', 'johnsnowlabs-spark-nlp']",68795913,"i figured out the problem. i installed sparknlp using conda, but i also installed it using pip. for some reason, juypter will not recognize sparknlp when i install it using conda. however, when both versions were installed it creates this error. i uninstalled the conda version and left only the pip method installed. this solved the problem.",https://stackoverflow.com/questions/68155580,python,27-06-2021 21:53,668.0,1.0,1.0,True,15-08-2021 23:01,27-06-2021 22:08
75964691,bert_vocab.bert_vocab_from_dataset returning wrong vocabulary,"i'm trying to build a tokenizer following the tf's tutorial  i'm basically doing the same thing only with a different dataset. the dataset in question is a txt file in which the first two columns are an english sentence or word and the translation in italian, here a snippet:
hi. ciao!   cc-by 2.0 (france) attribution: tatoeba.org #538123 (cm) & #607364 (cero)
hi. ciao.   cc-by 2.0 (france) attribution: tatoeba.org #538123 (cm) & #4522287 (guybrush88)
run!    corri!  cc-by 2.0 (france) attribution: tatoeba.org #906328 (papabear) & #906347 (guybrush88)
run!    corra!  cc-by 2.0 (france) attribution: tatoeba.org #906328 (papabear) & #906348 (guybrush88)
run!    correte!    cc-by 2.0 (france) attribution: tatoeba.org #906328 (papabear) & #906350 (guybrush88)
who?    chi?    cc-by 2.0 (france) attribution: tatoeba.org #2083030 (ck) & #2126402 (guybrush88)

it can be downloaded at 
i've preprocessed it and turned the english and italian sentences to tensorflow datasets to be fed to the tokenizer as illustrated in this code:
import tensorflow as tf
from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab
import tensorflow_text as tf_text
import os
import numpy as np

eng_dataset, ita_dataset = np.genfromtxt('ita_eng_dataset.txt',
                                         usecols=(0, 1),
                                         encoding='utf-8',
                                         unpack=true,
                                         dtype='str')

eng_dataset_tensor = tf.convert_to_tensor(eng_dataset)
ita_dataset_tensor = tf.convert_to_tensor(ita_dataset)

eng_tf_dataset = tf.data.dataset.from_tensor_slices(eng_dataset_tensor)
ita_tf_dataset = tf.data.dataset.from_tensor_slices(ita_dataset_tensor)

the problems arise when i try to fed it to bert_vocab_from_dataset:
bert_tokenizer_params = dict(lower_case=true)
reserved_tokens = [""[pad]"", ""[unk]"", ""[start]"", ""[end]""]

bert_vocab_args = dict(
    # the target vocabulary size
    vocab_size=8000,
    # reserved tokens that must be included in the vocabulary
    reserved_tokens=reserved_tokens,
    # arguments for `text.berttokenizer`
    bert_tokenizer_params=bert_tokenizer_params,
    # arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`
    learn_params={},
)

eng_vocab = bert_vocab.bert_vocab_from_dataset(eng_tf_dataset, **bert_vocab_args)
ita_vocab = bert_vocab.bert_vocab_from_dataset(ita_tf_dataset, **bert_vocab_args)

but the results are wrong:
print(eng_vocab[:20])
print(ita_vocab[1980:2000])
print(len(eng_vocab), len(ita_vocab))

which outputs
['about', 'breakfast', 'coffee', 'correct', 'finally', 'heat', 'japanese', 'large', 'lie', 'old', 'peel', 'science', 'step', 'swimming', 'work', '##ans', '##b', '##der', '##ins', '##ish']
['##omfortable', '##ong', '##ony', '##op', '##ouse', '##ply', '##rch', '##rous', '##rove', '##roved', '##sists', '##tained', '##ten', '##unted', '##val', '##ze', 'advice', 'agitated', 'amazed', 'argued']
665 2413

as you can see the italian vocabulary contains english text and are both very little (it maybe due to the dataset but seems odd to be so small, less than 1000 vocabs even).
i also tried batching the input dataset as in the tensorflow tutorial but it gave the same results.
i'm using python 3.8 on pycharm with windows 11 and tensorflow 2.10","['python', 'tensorflow', 'deep-learning', 'tokenize', 'bert-language-model']",77761403,"solved, it was just the np.genfromtxt non using '\t' as delimiter by default.",https://stackoverflow.com/questions/75964691,python,08-04-2023 10:30,195.0,2.0,1.0,True,04-01-2024 21:54,04-01-2024 21:54
76963311,llama-cpp-python not using nvidia gpu cuda,"i have been playing around with oobabooga text-generation-webui  on my ubuntu 20.04 with my nvidia gtx 1060 6gb for some weeks without problems. i have been using llama2-chat models sharing memory between my ram and nvidia vram. i installed without much problems following the intructions on its repository.
so what i want now is to use the model loader llama-cpp with its package llama-cpp-python bindings to play around with it by myself. so using the same miniconda3 environment that oobabooga text-generation-webui uses i started a jupyter notebook and i could make inferences and everything is working well but only for cpu.
a working example bellow,
from llama_cpp import llama

llm = llama(model_path=""/mnt/lxdata/llama.cpp/models/meta-llama2/llama-2-7b-chat/ggml-model-q4_0.bin"", 
            n_gpu_layers=32, n_threads=6, n_ctx=3584, n_batch=521, verbose=true), 

prompt = """"""[inst] <<sys>>
name the planets in the solar system? 
<</sys>>
[/inst] 
""""""
output = llm(prompt, max_tokens=350, echo=true)
print(output['choices'][0]['text'].split('[/inst]')[-1])


of course! here are the eight planets in our solar system, listed in order from closest to farthest from the sun:

mercury
venus
earth
mars
jupiter
saturn
uranus
neptune



note that pluto was previously considered a planet but is now classified as a dwarf planet due to its small size and unique orbit.

i want to make inference using gpu as well. what is wrong?
why can't i offload to gpu like the parameter n_gpu_layers=32 specifies and also like oobabooga text-generation-webui already does on the same miniconda environment whithout any problems?","['python', 'python-3.x', 'nlp', 'llama', 'llama-cpp-python']",76963364,"after searching around and suffering quite for 3 weeks i found out this issue on its repository.
the llama-cpp-python needs to known where is the libllama.so shared library. so exporting it before running my python interpreter, jupyter notebook etc. did the trick.
for using the miniconda3 installation used by oobabooga text-generation-webui i exported it like bellow:
export llama_cpp_lib=/yourminicondapath/miniconda3/lib/python3.10/site-packages/llama_cpp_cuda/libllama.so

voilï¿½ï¿½!!!!
on importing  from llama_cpp import llama i get

ggml_init_cublas: found 1 cuda devices:
device 0: nvidia geforce gtx 1060, compute capability 6.1

and on
llm = llama(model_path=""/mnt/lxdata/llama.cpp/models/meta-llama2/llama-2-7b-chat/ggml-model-q4_0.bin"", 
            n_gpu_layers=28, n_threads=6, n_ctx=3584, n_batch=521, verbose=true), 

...

llama_model_load_internal: using cuda for gpueleration
llama_model_load_internal: mem required  = 2381.32 mb (+ 1026.00 mb per state)
llama_model_load_internal: allocating batch_size x (512 kb + n_ctx x 128 b) = 480 mb vram for the scratch buffer
llama_model_load_internal: offloading 28 repeating layers to gpu
llama_model_load_internal: offloaded 28/35 layers to gpu
llama_model_load_internal: total vram used: 3521 mb
...",https://stackoverflow.com/questions/76963311,python,23-08-2023 16:35,64188.0,12.0,9.0,True,02-02-2025 09:37,15-09-2023 16:01
78596502,no registered action found for name &#39;action_redirect_to_scholarship&#39; in rasa action.py,"i am making a chatbot for my college as a project using rasa. i am planning to train this bot accordingly but at the latest i am facing a problem related to actions.py file. well you see that, i want to bot to redirect to particular website, when the user asks about ""scholarships"" but instead it gives error. like below (in action.py's terminal):
> 2024-06-08 22:51:22 info     rasa_sdk.endpoint  - starting action endpoint server...
> 2024-06-08 22:51:22 info     rasa_sdk.endpoint  - starting plugins...
> 2024-06-08 22:51:22 info     rasa_sdk.endpoint  - action endpoint is up and running on 
> 2024-06-08 22:53:02 error    rasa_sdk.endpoint  - no registered action found for name 'action_redirect_to_scholarship'.

i tried whatever things youtube and google recommended me. things like:

ensuring that the action name action_redirect_to_scholarship matches exactly in actions.py, domain.yml, and any other relevant files (e.g., stories, rules).
the utter_redirect_to_scholarship response is aligned properly under responses.
ensure you are using the correct version of rasa that supports the domain version ""3.1"".
they are also in same directory

errors in rasa server:
your input ->  scholarship
2024-06-08 22:53:02 error    rasa.core.processor  - encountered an exception while running action 'action_redirect_to_scholarship'.bot will continue, but the actions events are lost. please check the logs of your action server for more information.
traceback (most recent call last):
  file ""c:\users\prath\desktop\colbot\rasa_venv\lib\site-packages\rasa\core\actions\action.py"", line 780, in run
    response: any = await self.action_endpoint.request(
  file ""c:\users\prath\desktop\colbot\rasa_venv\lib\site-packages\rasa\utils\endpoints.py"", line 184, in request
    raise clientresponseerror(
rasa.utils.endpoints.clientresponseerror: 404, not found, body='b'{""error"":""no registered action found for name \'action_redirect_to_scholarship\'."",""action_name"":""action_redirect_to_scholarship""}''

the above exception was the direct cause of the following exception:

traceback (most recent call last):
  file ""c:\users\prath\desktop\colbot\rasa_venv\lib\site-packages\rasa\core\processor.py"", line 982, in _run_action
    events = await action.run(
  file ""c:\users\prath\desktop\colbot\rasa_venv\lib\site-packages\rasa\core\actions\action.py"", line 810, in run
    raise rasaexception(
rasa.shared.exceptions.rasaexception: failed to execute custom action 'action_redirect_to_scholarship'

actions.py
above is my actions.py
stories.yml:
version: ""3.1""

stories:

- story: happy path
  steps:
  - intent: greet
  - action: utter_greet
  - intent: mood_great
  - action: utter_happy

- story: sad path 1
  steps:
  - intent: greet
  - action: utter_greet
  - intent: mood_unhappy
  - action: utter_cheer_up
  - action: utter_did_that_help
  - intent: affirm
  - action: utter_happy

- story: sad path 2
  steps:
  - intent: greet
  - action: utter_greet
  - intent: mood_unhappy
  - action: utter_cheer_up
  - action: utter_did_that_help
  - intent: deny
  - action: utter_goodbye

- story: ask branch
  steps:
  - intent: greet
  - action: utter_greet
  - intent: branches
  - action: utter_branches

- story: redirect to scholarship site
  steps:
  - intent: check_scholarships
  - action: action_redirect_to_scholarship


**my nlu.yml **:
version: ""3.1""

nlu:
- intent: greet
  examples: |
    - hey
    - hello
    - hi
    - hello there
    - good morning
    - good evening
    - moin
    - hey there
    - let's go
    - hey dude
    - goodmorning
    - goodevening
    - good afternoon

- intent: goodbye
  examples: |
    - cu
    - good by
    - see you later
    - good night
    - bye
    - goodbye
    - have a nice day
    - see you around
    - bye bye
    - see you later

- intent: affirm
  examples: |
    - yes
    - y
    - indeed
    - of course
    - that sounds good
    - correct

- intent: deny
  examples: |
    - no
    - n
    - never
    - i don't think so
    - don't like that
    - no way
    - not really

- intent: mood_great
  examples: |
    - perfect
    - great
    - amazing
    - feeling like a king
    - wonderful
    - i am feeling very good
    - i am great
    - i am amazing
    - i am going to save the world
    - super stoked
    - extremely good
    - so so perfect
    - so good
    - so perfect

- intent: mood_unhappy
  examples: |
    - my day was horrible
    - i am sad
    - i don't feel very well
    - i am disappointed
    - super sad
    - i'm so sad
    - sad
    - very sad
    - unhappy
    - not good
    - not very good
    - extremely sad
    - so sad

- intent: bot_challenge
  examples: |
    - are you a bot?
    - are you a human?
    - am i talking to a bot?
    - am i talking to a human?
    - tell me about you?
    - what is your name?
    - about you

- intent: branches
  examples: |
    - what are the branches available in the college?
    - branches of the college
    - branches

- intent: check_scholarships
  examples: |
    - tell me about scholarships
    - what scholarships are available?
    - scholarship details
    - how can i get a scholarship?
    - tell me about scholarships available in st john's college of engineering and management
    - tell me about scholarships available in this college


domain.yml:
version: ""3.1""

intents:
  - greet
  - goodbye
  - affirm
  - deny
  - mood_great
  - mood_unhappy
  - bot_challenge
  - branches
  - check_scholarships

responses:
  utter_greet:
    - text: ""hey! how are you?""

  utter_cheer_up:
    - text: ""here is something to cheer you up:""
      image: ""

  utter_did_that_help:
    - text: ""did that help you?""

  utter_happy:
    - text: ""great, carry on!""

  utter_goodbye:
    - text: ""bye""

  utter_branches:
    - text: |
        branches available in st. john college of engineering & management are:<br>
        1. civil engineering<br>
        2. computer engineering<br>
        3. information technology<br>
        4. electronics & computer science<br>
        5. mechanical engineering<br>
        6. artificial intelligence/machine learning<br>
        7. data science

  utter_iamabot:
    - text: |
        college enquiry bot, also known as college-e-bot, is an ai-powered virtual assistant designed to assist you with queries related to colleges, courses, achievements, and other relevant information. it is programmed to provide helpful responses based on the queries you ask. it is made by prathamesh patil from s.e. a.i.m.l. college-e-bot can fetch information about various college branches, details about specific colleges, their achievements, placements, and more. it aims to provide you with accurate and relevant information to aid in your college decision-making process. if you have any questions or need assistance, feel free to ask college-e-bot, and it will do its best to help you!

  utter_redirect_to_scholarship:
    - text: ""redirecting to site for more information: [click here](

actions:
  - action_redirect_to_scholarship

session_config:
  session_expiration_time: 60
  carry_over_slots_to_new_session: true

my endpoints.yml:
action_endpoint:
  url: ""

i have absolutely no idea of how to solve this. i would appreciate some help here.","['rasa', 'rasa-nlu', 'rasa-core', 'rasa-x', 'rasa-sdk']",78971883,"even i also faced the same issue, so after a bunch of trials, i finally found the solution.
my case is a little different because iam using docker container to run rasa (from rasa/rasa base image) and rasa_sdk  (from rasa/rasa-sdk). here i have 2 containers. ok, let's come to the solution. the command is

python -m rasa_sdk --actions actions

this should work for you. if you are also using docker the run
cmd [""run"", ""python"", ""-m"", ""rasa_sdk"", ""--actions""  "" actions""]

reason for the problem:the reason is pretty simple rasa_sdk is unable to locate the actions",https://stackoverflow.com/questions/78596502,rasa,08-06-2024 17:44,130.0,0.0,1.0,True,11-09-2024 02:44,08-06-2024 17:52
78660117,how can i export a tokenizer from huggingface transformers to coreml?,"i load a tokenizer and a bert model from huggingface transformers, and export the bert model to coreml:
from transformers import autotokenizer, automodelfortokenclassification
import torch

# load the tokenizer
tokenizer = autotokenizer.from_pretrained(""huawei-noah/tinybert_general_4l_312d"")

# load the model
model = automodelfortokenclassification.from_pretrained(""huawei-noah/tinybert_general_4l_312d"")

# example usage
text = ""hugging face is creating a tool that democratizes ai.""
inputs = tokenizer(text, return_tensors=""pt"")

requirements:
pip install transformers torch

how can i export a tokenizer from huggingface transformers to coreml?","['python', 'huggingface-transformers', 'coreml']",78819126,"this is the bert tokenizer i used and it works well. a lot of this is from zach nagengast and julien chaumond. hope it helps! all you need is a vocab.txt file of the tokenizer's vocab which can be found here - 
# credit to julien from huggingface

import foundation

enum tokenizererror: error {
    case toolong(string)
}

class berttokenizer {
    private let basictokenizer = basictokenizer()
    private let wordpiecetokenizer: wordpiecetokenizer
    private let maxlen = 512
    
    private let vocab: [string: int]
    private let ids_to_tokens: [int: string]
    
    init() {
        let url = bundle.main.url(forresource: ""vocab"", withextension: ""txt"")!
        let vocabtxt = try! string(contentsof: url)
        let tokens = vocabtxt.split(separator: ""\n"").map { string($0) }
        var vocab: [string: int] = [:]
        var ids_to_tokens: [int: string] = [:]
        for (i, token) in tokens.enumerated() {
            vocab[token] = i
            ids_to_tokens[i] = token
        }
        self.vocab = vocab
        self.ids_to_tokens = ids_to_tokens
        self.wordpiecetokenizer = wordpiecetokenizer(vocab: self.vocab)
    }
    
    
    func tokenize(text: string) -> [string] {
        var tokens: [string] = []
        for token in basictokenizer.tokenize(text: text) {
            for subtoken in wordpiecetokenizer.tokenize(word: token) {
                tokens.append(subtoken)
            }
        }
        return tokens
    }
    
    private func createattentionmask(tokenids: [int]) -> [int] {
        return tokenids.map { $0 != 0 ? 1 : 0 }
    }
    
    private func converttokenstoids(tokens: [string]) -> [int] {
        if tokens.count > maxlen {
            let truncatedtokens = array(tokens.prefix(maxlen))
            return truncatedtokens.map { vocab[$0]! }
        } else {
            return tokens.map { vocab[$0]! }
        }
    }
    
    private func padsequence(_ sequence: [int], tolength length: int, paddingvalue: int = 0) -> [int] {
        if sequence.count >= length {
            return array(sequence.prefix(length))
        } else {
            return sequence + array(repeating: paddingvalue, count: length - sequence.count)
        }
    }
    
    /// main entry point
    func tokenizetoids(text: string, maxlength: int = 512) -> (tokenids: [int], attentionmask: [int]) {
        let tokens = [""[cls]""] + tokenize(text: text) + [""[sep]""]
        var tokenids = converttokenstoids(tokens: tokens)
        tokenids = padsequence(tokenids, tolength: maxlength)
        let attentionmask = createattentionmask(tokenids: tokenids)
        return (tokenids, attentionmask)
    }
    
    func tokentoid(token: string) -> int {
        return vocab[token]!
    }
    
    /// un-tokenization: get tokens from tokenids
    func untokenize(tokens: [int]) -> [string] {
        return tokens.map { ids_to_tokens[$0]! }
    }
    
    /// un-tokenization:
    func convertwordpiecetobasictokenlist(_ wordpiecetokenlist: [string]) -> string {
        var tokenlist: [string] = []
        var individualtoken: string = """"
        
        for token in wordpiecetokenlist {
            if token.starts(with: ""##"") {
                individualtoken += string(token.suffix(token.count - 2))
            } else {
                if individualtoken.count > 0 {
                    tokenlist.append(individualtoken)
                }
                
                individualtoken = token
            }
        }
        
        tokenlist.append(individualtoken)
        
        return tokenlist.joined(separator: "" "")
    }
}



class basictokenizer {
    let neversplit = [
        ""[unk]"", ""[sep]"", ""[pad]"", ""[cls]"", ""[mask]""
    ]
    
    func tokenize(text: string) -> [string] {
        let splittokens = text.folding(options: .diacriticinsensitive, locale: nil)
            .components(separatedby: nscharacterset.whitespaces)
        let tokens = splittokens.flatmap({ (token: string) -> [string] in
            if neversplit.contains(token) {
                return [token]
            }
            var toks: [string] = []
            var currenttok = """"
            for c in token.lowercased() {
                if c.isletter || c.isnumber || c == ""ï¿½ï¿½"" {
                    currenttok += string(c)
                } else if currenttok.count > 0 {
                    toks.append(currenttok)
                    toks.append(string(c))
                    currenttok = """"
                } else {
                    toks.append(string(c))
                }
            }
            if currenttok.count > 0 {
                toks.append(currenttok)
            }
            return toks
        })
        return tokens
    }
}


class wordpiecetokenizer {
    private let unktoken = ""[unk]""
    private let maxinputcharsperword = 100
    private let vocab: [string: int]
    
    init(vocab: [string: int]) {
        self.vocab = vocab
    }
    
    func substr(_ s: string, _ r: range<int>) -> string? {
        let stringcount = s.count
       stringcount < r.upperbound || stringcount < r.lowerbound {
            return nil
        }
        let startindex = s.index(s.startindex, offsetby: r.lowerbound)
        let endindex = s.index(startindex, offsetby: r.upperbound - r.lowerbound)
        return string(s[startindex..<endindex])
    }
    
    func tokenize(word: string) -> [string] {
        if word.count > maxinputcharsperword {
            return [unktoken]
        }
        var outputtokens: [string] = []
        var isbad = false
        var start = 0
        var subtokens: [string] = []
        while start < word.count {
            var end = word.count
            var cur_substr: string? = nil
            while start < end {
                var substr = substr(word, start..<end)!
                if start > 0 {
                    substr = ""##\(substr)""
                }
                if vocab[substr] != nil {
                    cur_substr = substr
                    break
                }
                end -= 1
            }
            if cur_substr == nil {
                isbad = true
                break
            }
            subtokens.append(cur_substr!)
            start = end
        }
        if isbad {
            outputtokens.append(unktoken)
        } else {
            outputtokens.append(contentsof: subtokens)
        }
        return outputtokens
    }
}",https://stackoverflow.com/questions/78660117,python,23-06-2024 23:59,296.0,0.0,1.0,True,01-08-2024 04:30,24-06-2024 16:53
77210041,troubleshooting pytorch and hugging face&#39;s pre-trained deberta model on windows 11 with an rtx 3070 gpu,"i'm running windows 11 on my desktop, which has an nvidia rtx 3070 gpu. i'm working on an nlp task using hugging face's automodelforsequenceclassification and i want to utilize my gpu for training. i've successfully installed pytorch 1.9.0 with cuda 11.1 and confirmed that cuda is available on my system.
however, when i try to run my script, i encounter an importerror suggesting that i need to install the accelerate library. when i attempt to do so, it not only fails but also replaces my existing pytorch 1.9.0 installation with version 2.1.0. i've tried various commands like pip install transformers[torch] and pip install accelerate -u, but they all result in the same issue.
the error message also indicates that accelerate requires at least pytorch 1.10, but i can't find a compatible cuda version for my rtx 3070.
does anyone have a solution for running a proper installation of transformers + torch + accelerate?","['pytorch', 'nlp', 'gpu', 'huggingface-transformers', 'huggingface-tokenizers']",77222088,"i have installed pytorch on multiple combinations (os+hardware).
i have installed pytorch successfully using those commands (in a virtual environment):

%pip install --upgrade transformers

%pip install --upgrade torch torchvision torchaudio --index-url 

%pip install accelerate (will take latest at the time of writing @0.23.0)

%pip install evaluate datasets


these helped me kick-start any of the projects which require huggingface. i hope it helps you.",https://stackoverflow.com/questions/77210041,pytorch,01-10-2023 10:15,520.0,0.0,1.0,True,04-10-2023 07:25,04-10-2023 07:25
78474215,how to load pretrained model to transformers pipeline and specify multi-gpu?,"i have a local server with multiple gpus and i am trying to load a local model and specify which gpu to use since we want to split gpu between team members.
i can successfully specify 1 gpu using device_map='cuda:3' for smaller model, how to do this on multiple gpu like cuda:[4,5,6] for larger model?
(i tried using device_map = 'auto', 'balanced', 'sequential', which will spread model automatically. but this is not what we want...)
import torch
from transformers import llamaforcausallm

model_dir = '/models/llama-2-13b-chat-hf'

# 'auto' 'balanced' 'sequential' 'balanced_low_0'
# 'cuda:3',

model = llamaforcausallm.from_pretrained(model_dir,
                                         device_map='cuda:[3,4,5]',#how to make things work here?
                                         torch_dtype=torch.float32 
                                        )","['pytorch', 'huggingface-transformers']",78475865,"i guess the easiest way to achieve what you want is exporting cuda_visible_devices:
import os
os.environ[""cuda_visible_devices""] = ""1""
#or
os.environ[""cuda_visible_devices""] = ""0,1""

import torch
from transformers import llamaforcausallm

model_dir = '/models/llama-2-13b-chat-hf'
model = llamaforcausallm.from_pretrained(model_dir,
                                         device_map='auto')

if you want to use the device_map you have to map each layer by yourself:
# distillroberta because it is smaller

from transformers import automodelformaskedlm

model = automodelformaskedlm.from_pretrained(""distilbert/distilroberta-base"")
# parameter names
print([x[0] for x in model.named_parameters()])

output:
['roberta.embeddings.word_embeddings.weight',
 'roberta.embeddings.position_embeddings.weight',
 'roberta.embeddings.token_type_embeddings.weight',
 'roberta.embeddings.layernorm.weight',
 'roberta.embeddings.layernorm.bias',
 'roberta.encoder.layer.0.attention.self.query.weight',
 'roberta.encoder.layer.0.attention.self.query.bias',
...
 'roberta.encoder.layer.5.output.layernorm.weight',
 'roberta.encoder.layer.5.output.layernorm.bias',
 'lm_head.bias',
 'lm_head.dense.weight',
 'lm_head.dense.bias',
 'lm_head.layer_norm.weight',
 'lm_head.layer_norm.bias']

you don't need to map each weight. it is enough when you map the layers:
# device map example for distillroberta:
from transformers import autotokenizer, automodelformaskedlm

device_map= {'roberta.embeddings':'cpu', 'roberta.encoder':0, 'lm_head':'cpu'}

model = automodelformaskedlm.from_pretrained(""distilbert/distilroberta-base"", device_map = device_map)",https://stackoverflow.com/questions/78474215,pytorch,13-05-2024 18:42,750.0,1.0,1.0,True,14-05-2024 05:21,14-05-2024 05:21
73916269,how to define ner_model in spacy,"def all_ents(v):
        return [(ent.text, ent.label_) for ent in ner_model(v).ents]

df1['entities'] = df1['text'].apply(lambda v: all_ents(v))

df1.head()

when executing this shows ner_model not defined can i please know how to define ner model in spacy","['python', 'nlp', 'spacy', 'named-entity-recognition']",73955069,"something tells me you are not loading your spacy model properly. not knowing how df1 looks like, i decided to go with one of my own, as follows:
import spacy
import pandas as pd


# building my own `df1`, it should look similar to yours
texts = [
    ""net income was $9.4 million compared to the prior year of $2.7 million."",
    ""revenue exceeded twelve billion dollars, with a loss of $1b."",
    ""i don't have any entity in me""
]
df1 = pd.dataframe(texts, columns =['text'])

# loading spacy model
model_to_use = ""en_core_web_lg""  # or use the path to your own model
ner_model = spacy.load(model_to_use)

# your code works now
def all_ents(v):
        return [(ent.text, ent.label_) for ent in ner_model(v).ents]

df1['entities'] = df1['text'].apply(lambda v: all_ents(v))

note:
in my own experience, if df1 is considerably large (i.e., it contains thousands of sentences), you may want to convert df1[""text""] into a list or a generator, and then apply these hints. if that's not your case or if your are not interested in an speed-optimal code, then do not pay attention to this note and go ahead with your current implementation.",https://stackoverflow.com/questions/73916269,python,01-10-2022 06:27,189.0,0.0,1.0,True,05-10-2022 01:14,01-10-2022 10:59
69957729,applying function to pandas dataframe: is there a more efficient way of doing this?,"i have a dataframe that has a small number of columns but many rows (about 900k right now, and it's going to get bigger as i collect more data). it looks like this:





author
title
date
category
text
url




0
amira charfeddine
wild fadhila 01
2019-01-01
novel
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½""text-align: left;"">nan


1
amira charfeddine
wild fadhila 02
2019-01-01
novel
ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½""text-align: left;"">nan


2
253826
1515368_7636953
2010-12-28
/forums/forums/91/
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿""text-align: left;"">


3
250442
1504416_7580403
2010-12-21
/forums/sports/
\n\n\n\n\n\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿""text-align: left;"">


4
312628
1504416_7580433
2010-12-21
/forums/sports/
quel est le rï¿½ï¿½sultat final\n,,,,????





the ""text"" column has a string of text that may be just a few words (in the case of a forum post) or it may a portion of a novel and have tens of thousands of words (as in the two first rows above).
i have code that constructs the dataframe from various corpus files (.txt and .json), then cleans the text and saves the cleaned dataframe as a pickle file.
i'm trying to run the following code to analyze how variable the spelling of different words are in the corpus. the functions seem simple enough: one counts the occurrence of a particular spelling variable in each text row; the other takes a list of such frequencies and computes a gini coefficient for each lemma (which is just a numerical measure of how heterogenous the spelling is). it references a spelling_var dictionary that has a lemma as its key and the various ways of spelling that lemma as values. (like {'color': ['color', 'colour']} except not in english.)
this code works, but it uses a lot of cpu time. i'm not sure how much, but i use pythonanywhere for my coding and this code sends me into the tarpit (in other words, it makes me exceed my daily allowance of cpu seconds).
is there a way to do this so that it's less cpu intensive? preferably without me having to learn another package (i've spent the past several weeks learning pandas and am liking it, and need to just get on with my analysis). once i have the code and have finished collecting the corpus, i'll only run it a few times; i won't be running it everyday or anything (in case that matters).
here's the code:
import pickle
import pandas as pd
import re

with open('1_raw_df.pkl', 'rb') as pickle_file:
    df = pickle.load(pickle_file)

spelling_var = {
    'illi': [""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"", ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½""],
    'besh': [""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"", ""ï¿½ï¿½ï¿½ï¿½""],
    ...
    }

spelling_df = df.copy()

def ot; + re.escape(word) + r""\b""
    return df['text'].str.count(pattern)

def compute_gini(freq_list):
    proportions = [f/sum(freq_list) for f in freq_list]
    squared = [p**2 for p in proportions]
    return 1-sum(squared)

for w, var in spelling_var.items():
    count_list = []
    for v in var:
        count_list.append(count_word(spelling_df, v))
        gini = compute_gini(count_list)
    spelling_df[w] = gini","['python', 'pandas', 'dataframe', 'nlp']",69958879,"i rewrote two lines in the last double loop, see the comments in the code below. does this solve your issue?
gini_lst = []
for w, var in spelling_var.items():
    count_list = []
    for v in var:
        count_list.append(count_word(spelling_df, v))
        #gini = compute_gini(count_list)  # don't think you need to compute this at every iteration of the inner loop, right?
    #spelling_df[w] = gini  # having this inside of the loop creates a new column at each iteration, which could crash your cpu
    gini_lst.append(compute_gini(count_list))

# this creates a df with a row for each lemma with its associated gini value
df_lemma_gini = pd.dataframe(data={""lemma_column"": list(spelling_var.keys()), ""gini_column"": gini_lst})",https://stackoverflow.com/questions/69957729,python,13-11-2021 19:52,261.0,3.0,1.0,True,14-11-2021 03:08,14-11-2021 03:08
15330725,how to get all the hyponyms of a word/synset in python nltk and wordnet?,"i have a list of all the nouns in wordnet. i want to remove all the nouns that are not vehicles. how do i do it? below is the pseudo-code i want to make, but i do not know how to make it work:
for word in wordlist:
  if not ""vehicle"" in wn.synsets(word):
    wordlist.remove(word)","['python', 'nltk', 'wordnet']",15330863,"from nltk.corpus import wordnet as wn
vehicle = wn.synset('vehicle.n.01')
typesofvehicles = list(set([w for s in vehicle.closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))

this will give you all the unique words from every synset that is a hyponym of the noun ""vehicle"" (1st sense).",https://stackoverflow.com/questions/15330725,python,11-03-2013 03:29,16697.0,10.0,3.0,True,26-05-2024 02:51,26-05-2024 02:40
70242454,how to check if a given english sentence contains all non-meaning words using python?,"i want to check in a python program if a given english sentence contains all non-meaning words.
return true if sentence has all words that have no meaning
e.g. sdfsdf sdf ssdf fsdf dsd  sd
return false if sentence contains at least one word that has meaning
e.g. hello asdf
here is the code i wrote.
updated the code for is_meaningless variable
import nltk

nltk.download('words')

from nltk.corpus import words

def is_sentence_meaningless(sentence):
  is_meaningless = true
  for word in sentence.split():
    if(word in words.words()):
      is_meaningless = false
      break
  return is_meaningless    


print(is_sentence_meaningless(""sss sss asdfasdf asdfasdfa asdfasfsd""))

print(is_sentence_meaningless("" sss sss asdfasdf asdfasdfa asdfasfsd test""))

is there a better alternative to this code? also, how can i add my own corpus to it? for example i have few domain specific words that i want it to return as true, is that possible?","['python', 'python-3.x', 'dictionary', 'nltk']",70242523,"you can use set.difference method (note that since words in nltk.corpus.words are mostly in lower case, have to use str.lower method as well, e.g. ""hello"" is in but ""hello"" isn't):
def is_sentence_meaningless(sentence, domain_specific_words):
    s_set = set(sentence.lower().split())
    if s_set.difference(words.words()+domain_specific_words) == s_set:
        return true
    return false

just fyi but your function does not do what your explanation says.",https://stackoverflow.com/questions/70242454,python,06-12-2021 08:07,1219.0,2.0,2.0,True,06-12-2021 10:45,06-12-2021 10:45
77376190,"chatbot that will generate a document draft with python, langchain, and openai","i'm attempted to pass draft documents and have my chatbot generate a template using a prompt create a non disclosure agreement draft for california between mike llc and fantasty world. with my code below the response i'm getting is:
""i'm sorry, but i cannot generate a non-disclosure agreement draft for you. however, you can use the provided context information as a template to create a non-disclosure agreement between mike llc and fantasty world. just replace the placeholders in the template with the appropriate names and information for your specific agreement.
here is my setup:
import sys
import os
import openai
import constants
import gradio as gr
from langchain.chat_models import chatopenai

from llama_index import simpledirectoryreader, gptlistindex, gptvectorstoreindex, llmpredictor, prompthelper, load_index_from_storage

# disable ssl certificate verification (for debugging purposes)
os.environ['requests_ca_bundle'] = ''  # set it to an empty string

os.environ[""openai_api_key""] = constants.apikey
openai.api_key = os.getenv(""openai_api_key"")
print(os.getenv(""openai_api_key""))

def createvecorindex(path):
    max_input = 4096
    tokens = 512
    chunk_size = 600
    max_chunk_overlap = 0.1

    prompt_helper = prompthelper(max_input, tokens, max_chunk_overlap, chunk_size_limit=chunk_size)

    #define llm
    llmpredictor = llmpredictor(llm=chatopenai(temperature=.7, model_name='gpt-3.5-turbo', max_tokens=tokens))

    #load data
    docs = simpledirectoryreader(path).load_data()

    #create vector index
    vectorindex = gptvectorstoreindex(docs, llmpredictor=llmpredictor, prompt_helper=prompt_helper)
    vectorindex.storage_context.persist(persist_dir='vectorindex.json')

    return vectorindex

vectorindex = createvecorindex('docs')

in my docs directory, i have a few examples of non-disclosure agreements to create the vector index.
this was my first attempt at the query:
def chatbot(input_index):
    query_engine = vectorindex.as_query_engine()
    response = query_engine.query(input_index)
    return response.response

gr.interface(fn=chatbot, inputs=""text"", outputs=""text"", title=""super awesome chatbot"").launch()

i can't seem to get it to generate the draft, it keeps giving me the ""i cannot generate a draft"" response
i also tried to create a clause for the word draft, but the setup below is essential useing the trained model instead my vector.
def chatbot(input_index):
    query_engine = vectorindex.as_query_engine()

    # if the ""draft"" clause is active:
    if ""draft"" in input_index.lower():
        # query the vectorindex for relevant information/context
        vector_response = query_engine.query(input_index).response
        print(vector_response)
        # use vector_response as context to query the openai api for a draft
        prompt = f""based on the information: '{vector_response}', generate a draft for the input: {input_index}""
        
        response = openai.completion.create(
            engine=""text-davinci-002"",
            prompt=prompt,
            max_tokens=512,
            temperature=0.2
        )
        
        openai_response = response.choices[0].text.strip()
        
        return openai_response

    # if ""draft"" clause isn't active, use just the vectorindex response
    else:
        print('else clause')
        return query_engine.query(input_index).response","['python', 'openai-api', 'langchain', 'llama-index']",77377392,"here is how i solved it:
import os
import openai
import constants
import gradio as gr
from langchain.chat_models import chatopenai

from llama_index import simpledirectoryreader, gptlistindex, gptvectorstoreindex, llmpredictor, prompthelper, load_index_from_storage

# disable ssl certificate verification (for debugging purposes)
os.environ['requests_ca_bundle'] = ''  # set it to an empty string

os.environ[""openai_api_key""] = constants.apikey
openai.api_key = os.getenv(""openai_api_key"")

def createvecorindex(path):
    max_input = 4096
    tokens = 512
    chunk_size = 600
    max_chunk_overlap = 0.1

    prompt_helper = prompthelper(max_input, tokens, max_chunk_overlap, chunk_size_limit=chunk_size)

    #define llm
    llmpredictor = llmpredictor(llm=chatopenai(temperature=0.3, model_name='gpt-3.5-turbo', max_tokens=tokens))

    #load data
    docs = simpledirectoryreader(path).load_data()

    #create vector index
    vectorindex = gptvectorstoreindex(docs, llmpredictor=llmpredictor, prompt_helper=prompt_helper)
    vectorindex.storage_context.persist(persist_dir='vectorindex.json')

    return vectorindex

vectorindex = createvecorindex('docs')

def chatbot(input_index):
    # query_engine = vectorindex.as_query_engine() simple query engine only
    query_engine = vectorindex.as_chat_engine()
    response = query_engine.query(input_index)
    return response.response

gr.interface(fn=chatbot, inputs=""text"", outputs=""text"", title=""super awesome chatbot"").launch()

i updated the engine to as_chat_engine() instead of as_query_engine() and now i'm getting more complex responses",https://stackoverflow.com/questions/77376190,python,27-10-2023 17:53,846.0,0.0,1.0,True,22-03-2024 10:30,22-03-2024 10:30
75631315,"using natural language processing, how can we add our own stop words to a list?","i am testing the library below, based on this code sample:
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import english_stop_words
from collections import counter

df_new = pd.dataframe(['okay', 'yeah', 'thank', 'im'])
stop_words = text.english_stop_words.union(df_new)
#stop_words

w_counts = counter(w for w in ' '.join(df['text_without_stopwords']).split() if w.lower() not in stop_words)


df_words = pd.dataframe.from_dict(w_counts, orient='index').reset_index()
df_words.columns = ['word','count']


import seaborn as sns
# selecting top 20 most frequent words
d = df_words.nlargest(columns=""count"", n = 25) 
plt.figure(figsize=(20,5))
ax = sns.barplot(data=d, x= ""word"", y = ""count"")
ax.set(ylabel = 'count')
plt.show()

i'm seeing this chart.

i'm trying to add these words to stop words: 'okay', 'yeah', 'thank', 'im'
but...they are all coming through!! what's wrong here??","['python', 'python-3.x', 'nlp']",75632562,"instead of join all the filtered words into io.stringio buffer and loading it to a dataframe, a much more straightforward/quick way is using collections.counter with its most_common function to get word counts right away:
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import english_stop_words
from collections import counter

# sample dataframe
df = pd.dataframe({'text_without_stopwords': ['my stop text hex words',
                                              'with some stop boards words', 'stop text']})
w_counts = counter(w for w in ' '.join(df['text_without_stopwords']).split()
                   if w.lower() not in english_stop_words)
plt.bar(*zip(*w_counts.most_common(25)))
plt.xticks(rotation=60)
plt.show()


sample plot:",https://stackoverflow.com/questions/75631315,python,03-03-2023 19:48,86.0,1.0,2.0,True,20-05-2023 11:19,20-05-2023 11:19
65266689,web scraping python for arabic text,"i am trying to web scrape the website: "" to get only the heading news and put them in a csv file using beautifulsoup and python, this is the code i am using after i look into the html source code ""view-source:
import urllib.request,sys,time
from bs4 import beautifulsoup
import requests
import pandas as pd

pagestoget= 1

upperframe=[]  
for page in range(1,pagestoget+1):
    print('processing page :', page)
    url = '
    print(url)
    
    #an exception might be thrown, so the code should be in a try-except block
    try:
        #use the browser to get the url. this is suspicious command that might blow up.
        page=requests.get(url)                             # this might throw an exception if something goes wrong.
    
    except exception as e:                                   # this describes what to do if an exception is thrown
        error_type, error_obj, error_info = sys.exc_info()      # get the exception information
        print ('error for link:',url)                          #print the link that cause the problem
        print (error_type, 'line:', error_info.tb_lineno)     #print error info and line that threw the exception
        continue                                              #ignore this page. abandon this and go back.
    time.sleep(2)   
    soup=beautifulsoup(page.text,'html.parser')
    frame=[]
    links=soup.find_all('li',attrs={'class':'o-listicle__item'})
    print(len(links))
    filename=""news.csv""
    f=open(filename,""w"", encoding = 'utf-8')
    headers=""statement,link\n""
    f.write(headers)
    
    for j in links:
        statement = j.find(""div"",attrs={'class':'row d-flex'}).text.strip()
       # link = ""
        link += j.find(""div"",attrs={'class':'col-lg-4 col-md-4 col-sm-6 col-xs-6'}).find('a')['href'].strip()
        frame.append((statement,link))
        f.write(statement.replace("","",""^"")+"",""+link+"",""+date.replace("","",""^"")+"",""+source.replace("","",""^"")+"",""+label.replace("","",""^"")+""\n"")
    upperframe.extend(frame)
f.close()
data=pd.dataframe(upperframe, columns=['statement','link'])
data.head()

but after i run the code i am getting the pandas data frame and csv file empty, any suggestion why is that? knowing that i want to get the text between  tags.","['python', 'dataframe', 'web-scraping', 'nlp', 'arabic']",65271640,"if i understand correctly, you want to get the text part of the news headlines and the href link to these news. you further want to write them into a csv file. the problem with your code is  for j in links: is not executed because soup.find_all('li',attrs={'class':'o-listicle__item'}) returns an empty list. you should be careful with the names and classes of the tags that you are searching. below code gets news texts and their links, it also writes them to the csv file using pd.dataframe.
import urllib.request,sys,time
from bs4 import beautifulsoup 
import requests
import pandas as pd

pagestoget = 1

for page in range(1,pagestoget+1):
    print('processing page :', page)
    url = ' + str(page)
    print(url)

    #an exception might be thrown, so the code should be in a try-except block
    try:
        #use the browser to get the url. this is suspicious command that might blow up.
        page = requests.get(url)                             # this might throw an exception if something goes wrong.

    except exception as e:                                   # this describes what to do if an exception is thrown
        error_type, error_obj, error_info = sys.exc_info()      # get the exception information
        print('error for link:',url)                          #print the link that cause the problem
        print(error_type, 'line:', error_info.tb_lineno)     #print error info and line that threw the exception
        continue                                              #ignore this page. abandon this and go back.

    soup = beautifulsoup(page.text,'html.parser')
    texts = []
    links = []
    filename = ""news.csv""
    f = open(filename,""w"", encoding = 'utf-8')

    statement = soup.find(""div"",attrs={'class':'row d-flex'})
    divs = statement.find_all(""div"",attrs={'class':'col-lg-4 col-md-4 col-sm-6 col-xs-6'})

    for div in divs:
        txt = div.find(""img"",attrs={'class':'rumor__thumb'})
        texts.append(txt['alt'])
        lnk = div.find(""a"",attrs={'class':'rumor--archive'})
        links.append(lnk['href'])

data = pd.dataframe(list(zip(texts, links)), columns=['statement', 'link'])
data.to_csv(f, encoding='utf-8', index=false)
f.close()",https://stackoverflow.com/questions/65266689,python,12-12-2020 15:32,1323.0,0.0,1.0,True,13-02-2021 11:33,13-02-2021 11:33
67511285,using transformers class bertforquestionanswering for extractive question answering,"i'm using a bert model for extractive qa task with the transformers class library bertforquestionanswering.  extractive question answering is the task of answering a question for a given context text and outputting the start and end indexes of where the answer matches in the context. my code is the following:
model = bertforquestionanswering.from_pretrained('bert-base-uncased',
    cache_dir=os.getenv(""cache_dir"", ""../../models""))
question = ""what is the capital of italy?""
text = ""the capital of italy is rome.""
inputs = tokenizer.encode_plus(question, text, return_tensors='pt')
start, end = model(**inputs)
start_max = torch.argmax(f.softmax(start, dim = -1))
end_max = torch.argmax(f.softmax(end, dim = -1)) + 1 ## add one ##because of python list indexing
answer = tokenizer.decode(inputs[""input_ids""][0][start_max : end_max])
print(answer)

i get this error
start_max = torch.argmax(f.softmax(start, dim = -1))
  file ""/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py"", line 1583, in softmax
    ret = input.softmax(dim)
attributeerror: 'str' object has no attribute 'softmax'

i have also tried this approach, slightly different
encoding = tokenizer.encode_plus(text=question,text_pair=text, add_special=true)
inputs = encoding['input_ids']  #token embeddings
sentence_embedding = encoding['token_type_ids']  #segment embeddings
tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens
start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))
start_index = torch.argmax(start_scores)
end_index = torch.argmax(end_scores)
answer = ' '.join(tokens[start_index:end_index+1])

but the error is likely the same:
    start_index = torch.argmax(start_scores)
typeerror: argmax(): argument 'input' (position 1) must be tensor, not str

i assume due to the unpack of the output as
start, end = model(**inputs)

if so, how to correct unpack this model's outputs?","['python', 'bert-language-model', 'huggingface-transformers']",71189395,"due to version update, the model returns a dictionary and not a tuple of start, end.
you can add the following parameter:
return_dict=false",https://stackoverflow.com/questions/67511285,python,12-05-2021 21:38,1148.0,1.0,1.0,True,19-02-2022 21:34,12-05-2021 22:08
40585370,mallet basic usage. first steps,"i'm trying to use mallet with literally no expirience in topic modeling and etc. my purpose is to get n topics of m documents that i have right now, classify every document with one or more topic (doc 1 = topic 1; doc 2 = topic 2 and possibly topic 3) and classify with this results new document in future. i tried to use bigartm for this first, but found nothing for classification in this program, only topic modeling. so mallet, i created a corpus.txt file with following format:
doc.num. \t(tab) label(actualy 1 everywhere) \t text
1        1        some text of document to classify
2        1        another doc text
... 
for now i could get topics from this file after turning it to feature sequence format for mallet with
bin/mallet import-file --input corpus.txt --output foo.mallet--keep-sequence 
and then get topics from it
bin/mallet train-topics --input foo.mallet --output-state state.gz --output-topic-keys topic-keys.txt --output-doc-topics doc-topics.txt

so general question now is what to use in mallet (train classifier?) to assign every existing document to a topic i found and to save this result to apply to future document that i want to classify with this topics.
thanks","['topic-modeling', 'mallet']",40592259,"what you're looking for is described as ""inference"" in mallet topic models. training a classifier is a separate package, aimed at directly learning relationships between words and a pre-existing set of classes.
here are directions for using inference on new documents:
when you train a model with the train-topics command, add the --inferencer-filename [filename] option. this option will create a topic inference tool based on the current, trained model and save it in a file.
if you already have a trained model, for example from --output-state or --output-model you can initialize from that state or model, run 0 iterations of sampling, and output an inferencer.
once you've created the inferencer file, use the mallet command bin/mallet infer-topics --help to get information on using topic inference.
note that you must make sure that the new data is compatible with your training data. otherwise word id 425 might mean a completely different word. this will make all topics look equally probable. use the option --use-pipe-from [mallet training file] in the mallet command bin/mallet import-file or import-dir to specify a training file.",https://stackoverflow.com/questions/40585370,topic-modeling,14-11-2016 09:25,888.0,2.0,2.0,True,17-10-2022 03:44,14-11-2016 20:00
77043285,nltk sentence_bleu() returns 0 while evaluating chinese sentences,"i'm trying to evaluate chinese sentence bleu scores with nltk's sentence_bleu() function. the code is as follows:
import nltk
import jieba

from transformers import autotokenizer, berttokenizer, bartforconditionalgeneration

src = 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½'
ref = 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½'

checkpoint = 'fnlp/bart-base-chinese'
tokenizer = berttokenizer.from_pretrained(checkpoint)
model = bartforconditionalgeneration.from_pretrained(checkpoint)

hypothesis_translations = []

for sentence in [src]:
    inputs = tokenizer(sentence, return_tensors=""pt"", truncation=true, max_length=100, return_token_type_ids=false)
    outputs = model.generate(**inputs)
    translated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=true)
    hypothesi=false)
outputs_ref = model.generate(**inputs_ref)
tokenized_ref = tokenizer.decode(outputs_ref[0], skip_special_tokens=true)

nltk_bleu = nltk.translate.bleu_score.sentence_bleu(tokenized_ref, hypothesis_translations)
print(nltk_bleu)

the output of printing nltk_bleu is 0.
but when i use the corpus_score() of sacrebleu library, it returns normal and expected results:
import evaluate
from sacrebleu.metrics import bleu

bleu = bleu()
bleu_score = bleu.corpus_score(references=tokenized_ref, hypotheses=hypothesis_translations)
print(bleu_score)

which returns:

bleu = 4.79 73.3/3.6/1.9/1.0 (bp = 1.000 ratio = 15.000 hyp_len = 15 ref_len = 1)

how can i make the nltk sentence_score return correct results?

update after adding nltk's method 3 into consideration:
from nltk.translate.bleu_score import smoothingfunction
smooth_fn = smoothingfunction()
nltk_bleu = nltk.translate.bleu_score.sentence_bleu(tokenized_ref, hypothesis_translations, smoothing_function=smooth_fn.method3)

the value of nltk_bleu is still 0.","['python', 'nltk', 'cjk', 'bleu']",77131799,"the function sentence_bleu expects a list of list of tokens as reference, and a list of tokens as hypothesis. your supplied input just does not correlate with the expectations.
once you fix it, you will get:
smooth_fn = smoothingfunction()
nltk_bleu = nltk.translate.bleu_score.sentence_bleu([tokenized_ref.split(' ')], hypothesis_trans
lations[0].split(' '), smoothing_function=smooth_fn.method3)
print(nltk_bleu)

>>>
0.43560338053780967

also, you should take into account that by default it calculates bleu-4 (for 4-grams) and also consider difference from the smoothing functions.",https://stackoverflow.com/questions/77043285,python,05-09-2023 09:25,478.0,0.0,2.0,True,19-09-2023 05:17,18-09-2023 02:41
73024546,why don&#39;t spacy transformer models do ner for non-english models?,"why is it that spacy transformer models for languages like spanish (es_dep_news_trf) don't do named entity recognition.
however, for english (en_core_web_trf) it does.
in code:
import spacy    
nlp=spacy.load(""en_core_web_trf"")
doc=nlp(""my name is john smith and i work at apple and i like visiting the eiffel tower"")
print(doc.ents)
(john smith, apple, the eiffel tower)
    
nlp=spacy.load(""es_dep_news_trf"")
doc=nlp(""mi nombre es john smith y trabajo en apple y me gusta visitar la torre eiffel"")
print(doc.ents)
()

why doesn't spanish extract entities but english does?","['spacy', 'named-entity-recognition', 'spacy-transformers']",73032340,"it has to do with the available training data. ner is only included for the trf models if the training data has ner annotation on the exact same data as for tagging and parsing.
training trf models on partial annotation does not work well in practice and an independent ner component (as in the cnn pipelines) would mean including an additional transformer component in the pipeline, which would make the pipeline a lot larger and slower.",https://stackoverflow.com/questions/73024546,spacy,18-07-2022 15:06,918.0,0.0,2.0,True,19-07-2022 07:20,18-07-2022 15:08
71102925,training with spacy on full dataset,"when i train my spacy model as follows
spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy

the model gets trained on train.spacy data file, and scored on dev.spacy. then output_updated/model-best is the model with the highest score.
is this best model finally trained on a combination of both train and dev data? i understand, it makes sense to split those datasets to avoid overfitting, but given little training data, i would like the final model to be trained on all data i have at hand.",['spacy'],71107409,"no, spacy does not automatically merge your datasets before training model-best. if you want to do that you would need to manually create a new training data set.
if you have so little data that seems like a good idea, you should probably prioritize getting more data.",https://stackoverflow.com/questions/71102925,spacy,13-02-2022 16:56,506.0,1.0,1.0,True,14-02-2022 07:34,14-02-2022 07:34
77906649,why token embedding different from the embedding by the bartforconditionalgeneration model,"why both the embeddings are different even when i generate them using same bartforconditionalgenration model?
first embedding is generated by combining token embedding and positional embedding from
embed_pos = modelbart.model.encoder.embed_positions(input_ids.input_ids)
inputs_embeds = modelbart.model.encoder.embed_tokens(input_ids.input_ids)

the second embedding by the model via
output = modelbart(input_ids.input_ids)
print(""\n\n output: \n\n"",output.encoder_last_hidden_state)

shouldn't the embedding by first and second be same? what to do so that difference of the embedding from first and second be zero?","['machine-learning', 'pytorch', 'nlp', 'huggingface-transformers', 'bart']",77909164,"the first embeddings (input + position) are the first layer of the model. these embeddings are used to map tokens to vectors.
the second set of embeddings (encoder_last_hidden_state) are the outputs of the final layer in the model's encoder.
these embeddings are supposed to be different.",https://stackoverflow.com/questions/77906649,machine-learning,30-01-2024 13:18,127.0,0.0,1.0,True,30-01-2024 20:13,30-01-2024 14:38
67518675,text mining between a data frame column and 2 lists in r,"so i created two lists composed of words :
fruits <- c(""banana"",""apple"",""strawberry"")
homemade <- c(""kitchen"",""homemade"",""mom"",""dad"",""sister"")

and here is my dataset




description
iscake




apple cake cooked by mom
yes


pie from the bakery
no


strawberry dessert by dad
no




i want to create a text mining code so that when df$description contains one or multiple words from ""fruits"" and one or multiple words from homemade, df$iscake become ""ok""
expected output




description
iscake




apple cake cooked by mom
yes


pie from the bakery
no


strawberry dessert by dad
ok




df <- df %>% mutate(iscake=ifelse(description %in% fruits & description %in% homemade, ""ok"", iscake))

i have no error messages but apparently is doesn't work because when i subset if iscake==""ok"" i always have 0 obs.","['r', 'list', 'dataframe', 'text-mining', 'rdata']",67518883,"you can create a pattern from fruits and homemade vector and use it in grepl :
df$iscake[grepl(paste0(fruits, collapse = '|'), df$description) & 
          grepl(paste0(homemade, collapse = '|'), df$description)] <- 'ok'

df
#                description iscake
#1  apple cake cooked by mom     ok
#2       pie from the bakery     no
#3 strawberry dessert by dad     ok",https://stackoverflow.com/questions/67518675,r,13-05-2021 11:44,112.0,0.0,1.0,True,13-05-2021 17:00,13-05-2021 11:55
68611032,is it possible to use evaluation metrics (like ndcg) as a loss function?,"i am working on a information retrieval model called dpr which is a basically a neural network (2 berts) that ranks document, given a query. currently, this model is trained in binary manners (documents are whether related or not related) and uses negative log likelihood (nll) loss. i want to change this binary behavior and create a model that can handle graded relevance (like 3 grades: relevant, somehow relevant, not relevant). i have to change the loss function because currently, i can only assign 1 positive target for each query (dpr uses pytorch nllloss) and this is not what i need.
i was wondering if i could use a evaluation metric like ndcg (normalized discounted cumulative gain) to calculate the loss. i mean, the whole point of a loss function is to tell how off our prediction is and ndcg is doing the same.
so, can i use such metrics in place of loss function with some modifications? in case of ndcg, i think something like subtracting the result from 1 (1 - ndcg_score) might be a good loss function. is that true?
with best regards, ali.","['neural-network', 'ranking', 'loss-function', 'evaluation', 'information-retrieval']",68621705,"yes, this is possible. you would want to apply a listwise learning to rank approach instead of the more standard pairwise loss function.
in pairwise loss, the network is provided with example pairs (rel, non-rel) and the ground-truth label is a binary one (say 1 if the first among the pair is relevant, and 0 otherwise).
in the listwise learning approach, however, during training you would provide a list instead of a pair and the ground-truth value (still a binary) would indicate if this permutation is indeed the optimal one, e.g. the one which maximizes ndcg. in a listwise approach, the ranking objective is thus transformed into a classification of the permutations.
for more details, refer to this paper.
obviously, the network instead of taking features as input may take bert vectors of queries and the documents within a list, similar to colbert. unlike colbert, where you feed in vectors from 2 docs (pairwise training), for listwise training u need to feed in vectors from say 5 documents.",https://stackoverflow.com/questions/68611032,neural-network,01-08-2021 13:48,4165.0,3.0,2.0,True,02-08-2023 13:20,02-08-2021 09:01
71787993,keras pad_sequence and tokenizer,"i learn on kaggle dataset here to practice on nlp i have an error when i tokenize the tweets and go to padding them i got an error i search for an solution but i don't get answer

# get tha max number of word in tweets
texts = df['text']
length = texts.apply(lambda p:len(p.split()))

x = df ['text']
y = df['target']
x_train,x_test , y_train,y_test =train_test_split(x,y,test_size=.30,random_state=41)


tokenize = tokenizer()
tokenize.fit_on_texts(x)
x = tokenize.texts_to_sequences(x)

print('start padding ...')

# padding tweets to be the same length
x = pad_sequences(x ,maxlen=length)

i got this error
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
/tmp/ipykernel_34/2607522322.py in <module>
      8 
      9 # padding tweets to be the same length
---> 10 x = pad_sequences(x ,maxlen=length)

/opt/conda/lib/python3.7/site-packages/keras/preprocessing/sequence.py in pad_sequences(sequences, maxlen, dtype, padding, truncating, value)
    152   return sequence.pad_sequences(
    153       sequences, maxlen=maxlen, dtype=dtype,
--> 154       padding=padding, truncating=truncating, value=value)
    155 
    156 keras_export(

/opt/conda/lib/python3.7/site-packages/keras_preprocessing/sequence.py in pad_sequences(sequences, maxlen, dtype, padding, truncating, value)
     83                          .format(dtype, type(value)))
     84 
---> 85     x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)
     86     for idx, s in enumerate(sequences):
     87         if not len(s):

/opt/conda/lib/python3.7/site-packages/numpy/core/numeric.py in full(shape, fill_value, dtype, order, like)
    340         fill_value = asarray(fill_value)
    341         dtype = fill_value.dtype
--> 342     a = empty(shape, dtype, order)
    343     multiarray.copyto(a, fill_value, casting='unsafe')
    344     return a

typeerror: 'series' object cannot be interpreted as an integer","['python', 'tensorflow', 'keras', 'nlp', 'tokenize']",71792693,"the problem is that length is not an integer but a pandas series. try something like this:
from sklearn.model_selection import train_test_split
import pandas as pd
import tensorflow as tf 

df = pd.dataframe({'text': ['is upset that he cant update his facebook by texting it... and might cry as a result  school today also. blah!',
                                    '@kenichan i dived many times for the ball. managed to save 50%  the rest go out of bounds',
                                    'my whole body feels itchy and like its on fire', 
                                    '@nationwideclass no, its not behaving at all. im mad. why am i here? because i cant see you all over there.',
                                    '@kwesidei not the whole crew'],
                          'target': [0, 1, 0, 0, 1]})
x = df['text'].values
y = df['target'].values

max_length = max([len(d.split()) for d in x])
x_train, x_test ,y_train, y_test =train_test_split(x,y,test_size=.30,random_state=41)

tokenize = tf.keras.preprocessing.text.tokenizer()
tokenize.fit_on_texts(x)
x = tokenize.texts_to_sequences(x)

print('start padding ...')

x = tf.keras.preprocessing.sequence.pad_sequences(x, maxlen=max_length)
print(x)

start padding ...
[[ 9 10 11 12  3 13 14 15 16 17 18  4 19 20 21 22 23 24 25 26 27]
 [ 0  0  0 28  1 29 30 31 32  2 33 34 35 36 37  2 38 39 40 41 42]
 [ 0  0  0  0  0  0  0  0  0  0  0 43  5 44 45 46  4 47  6 48 49]
 [50 51  6  7 52 53  8 54 55 56 57  1 58 59  1  3 60 61  8 62 63]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 64  7  2  5 65]]

if you want to use post-padding, run:
x = tf.keras.preprocessing.sequence.pad_sequences(x, maxlen=max_length, padding='post')",https://stackoverflow.com/questions/71787993,python,07-04-2022 19:36,2124.0,1.0,1.0,True,08-04-2022 06:41,08-04-2022 06:41
77149413,openai api vba function returns #value! but msgbox displays response,"i am trying to integrate the openai api into excel. the http request to openai chat completion works correctly and the response is ok. when i display it with a msgbox, it looks fine.
but when the function is called in a sheet, the returned value is #value!
public function gpt(inputprompt) as string
  'define variables
  dim request as object
  dim text, response, api, api_key, displaytext, gptmodel as string
  dim gpttemp as double
  dim startpos as long
  dim rng as range
  dim  as object
  dim countarray as variant

 
  'api info
  api = ""
  api_key = trim(range(""api_key"").value)
  'note: for future upgrades, please replace with gpt-4 etc
  gptmodel = range(""model_number"").value
  
  if api_key = """" then 'api key is missing!
    msgbox ""error: api cannot be blank! please go to 'configuration' tab and enter a valid openai api key"", vbexclamation, ""gpt for excel""
    frmstatus.hide
    exit function
  end if
    
  'clean input text and make json safe
  text = cleaninput(inputprompt)
  
  'create request object
  set  = createobject(""msxml2.xml
  'set  = new msxml2.xml
  
  'get temp from config panel
  gpttemp = range(""gpt_temperature"").value
  
  'assemble request body
  dim requestbody as string
  requestbody = ""{""""model"""": """"gpt-4"""", """"messages"""": [{""""role"""": """"user"""", """"content"""": """""" & text & """"""}], """"temperature"""": 0.7}""

  with 
     .open ""post"", api, false
     .setrequestheader ""content-type"", ""application/json""
     .setrequestheader ""authorization"", ""bearer "" & api_key
     .send (requestbody)
  end with

  if  = 200 then 'successfully called openai api
   response = 
   'get usage info from response object
   countarr = extractusageinfo(response)
    
   startpos = instr(response, """"""content"""":"") + len(""""""content"""":"") + 2
   endpos = instr(startpos, response, ""},"")
   displaytext = trim(mid(response, startpos, endpos - startpos))
   displaytext = mid(displaytext, 1, len(displaytext) - 2)
   displaytext = cleanoutput(displaytext)
    
   set request = nothing
   if fillactive = false then frmstatus.hide
   msgbox (gpt)
   gpt = displaytext
   msgbox (gpt)
   if range(""log_data"").value = ""yes"" then
    call updatelogfile(countarr)
   end if
  exit function

end function","['excel', 'vba', 'openai-api', 'worksheet-function']",77149631,"you are limited as to what you can do in a custom worksheet function. while i can't find an explicit reference to making web or api calls in the ms docs my experience of worksheet functions leads me to not be surprised that this doesn't work.
an alternative would be to add a button or shape to your worksheet then associated that button with a sub such as
sub buttonclicked()
    dim response as string, inputprompt as string
    inputprompt = cstr(thisworkbook.sheets(""sheet1"").range(""a1"").value)
    response = gpt(inputprompt)
    thisworkbook.sheets(""sheet1"").range(""a2"").value = response
end sub

which uses your existing gpt function to get a response for the text in cell a1 and loads it into cell a2 (both in 'sheet1' in this case ... obviously you can change the cells and worksheet as needed).",https://stackoverflow.com/questions/77149413,excel,21-09-2023 10:28,208.0,0.0,1.0,True,21-09-2023 10:59,21-09-2023 10:45
69366075,how to convert pandas text column to nltk text object,"i have following dataframe in pandas
publish_date    headline_text
20030219        aba decides against community broadcasting 
20030219        act fire witnesses must be aware of defamation
20030219        a g calls for infrastructure protection summit
20030219        air nz staff in aust strike for pay rise
20030219        air nz strike to affect australian travellers

i want to convert headline_text column to nltk text object in order to apply all nltk methods on it.
i am doing following, but it does not seem to work
headline_text = nlp_df['headline_text'].apply(lambda x: ''.join(x))","['python', 'pandas', 'nltk']",69366131,"you can do:
nltk_col = df.headline_text.apply(lambda row: nltk.text(row.split(' ')))

to assign this column to the dataframe, you can then do:
df=df.assign(nltk_texts=nltk_col)

then we can check the type of the first row in the new nltk_texts column:
print(type(df.nltk_texts.loc[0])) # outputs: nltk.text.text

to unify all rows into a single nltk text object, you can do:
single = nltk.text([word for row in df.headline_text for word in row.split(' ')])

then print(type(single)) will output nltk.text.text.",https://stackoverflow.com/questions/69366075,python,28-09-2021 17:25,172.0,1.0,1.0,True,29-12-2021 16:19,29-12-2021 16:19
53301916,python/gensim - what is the meaning of syn0 and syn0norm?,"i know that in gensims keyedvectors-model, one can access the embedding matrix by the attribute model.syn0. there is also a syn0norm, which doesn't seem to work for the glove model i recently loaded. i think i also have seen syn1 somewhere previously. 
i haven't found a doc-string for this and i'm just wondering what's the logic behind this?
so if syn0 is the embedding matrix, what is syn0norm? what would then syn1 be and generally, what does syn stand for?","['python', 'deep-learning', 'nlp', 'gensim', 'word-embedding']",53333072,"these names were inherited from the original google word2vec.c implementation, upon which the gensim word2vec class was based. (i believe syn0 only exists in recent versions for backward-compatbility.)
the syn0 array essentially holds raw word-vectors. from the perspective of the neural-network used to train word-vectors, these vectors are a 'projection layer' that can convert a one-hot encoding of a word into a dense embedding-vector of the right dimensionality. 
similarity operations tend to be done on the unit-normalized versions of the word-vectors. that is, vectors that have all been scaled to have a magnitude of 1.0. (this makes the cosine-similarity calculation easier.) the syn0norm array is filled with these unit-normalized vectors, the first time they're needed. 
this syn0norm will be empty until either you do an operation (like most_similar()) that requires it, or you explicitly do an init_sims() call. if you explicitly do an init_sims(replace=true) call, you'll actually clobber the raw vectors, in-place, with the unit-normed vectors. this saves the memory that storing both vectors for every word would otherwise require. (however, some word-vector uses may still be interested in the original raw vectors of varying magnitudes, so only do this when you're sure most_similar() cosine-similarity operations are all you'll need.)
the syn1 (or syn1neg in the more common case of negative-sampling training) properties, when they exist on a full model (and not for a plain keyedvectors object of only word-vectors), are the model neural network's internal 'hidden' weights leading to the output nodes. they're needed during model training, but not a part of the typical word-vectors collected after training. 
i believe the syn prefix is just a convention from neural-network variable-naming, likely derived from 'synapse'.",https://stackoverflow.com/questions/53301916,python,14-11-2018 13:56,8698.0,9.0,1.0,True,05-06-2021 20:52,14-11-2018 19:21
68886509,confused about implementation of skip layers in cnn,"i'm reading about alphago zero's network structure and came across this cheatsheet:

i'm having a hard time understanding how skip connections work dimensionally.
specifically, it seems like each residual layer ends up with 2 stacked copies of the input it receives. would this not cause the input size to grow exponentially with the depth of the network?
and could this be avoided by changing the output channel size of the conv2d filter? i see that in_c and out_c don't have to be the same in pytorch, but i don't know enough to understand the implications of these values being different.","['machine-learning', 'deep-learning', 'pytorch', 'openai-api']",68889043,"with skip connection, you can indeed end up with twice the number of channels per connection. this is the case when you are concatenating the channels together. however, it doesn't necessarily have to grow exponentially, if you keep the number of output channels (what you refer to as out_c) under control.
for instance, if you have a skip connection providing a total of n channels and the convolutional layer gets in_c channels as input. then you can define out_c as n as well, such that the resulting number of channels after concatenation is equal to 2*n. ultimately, you decide on the number of output channels for each convolution, it is all about network capacity and how much it will be capable of learning.",https://stackoverflow.com/questions/68886509,machine-learning,23-08-2021 02:00,395.0,0.0,1.0,True,23-08-2021 07:58,23-08-2021 07:58
68024199,how to modify spacy tokenizer to split urls into individual words,"i want to modify the default tokenizer to split url's into individual words. here's what i currently have

import spacy
nlp = spacy.blank('en')
infixes = nlp.defaults.infixes + [r'\.']
infix_regex = spacy.util.compile_infix_regex(infixes)
nlp.tokenizer.infix_finditer = infix_regex.finditer
print(list(nlp(' 
# ['
# want it to be [' '.', 'internet', '.', 'com']

i'm looking at the usage examples and source code for the tokenizer, but i can't work out this particular case.","['python', 'nlp', 'spacy']",68030892,"you're not seeing results you want because url get caught by url_match first (it has higher precedence):
import spacy
nlp = spacy.blank('en')
txt = 'check this out 
doc = nlp(txt)
nlp.tokenizer.explain(txt)


[('token', 'check'),
 ('token', 'this'),
 ('token', 'out'),
 ('url_match', '

one of the possible solutions:
nlp.tokenizer.url_match = none
infixes = nlp.defaults.infixes + [r'\.']
infix_regex = spacy.util.compile_infix_regex(infixes)
nlp.tokenizer.infix_finditer = infix_regex.finditer
doc = nlp(txt)
list(doc)


[check, this, out,  ., internet, ., com]",https://stackoverflow.com/questions/68024199,python,17-06-2021 17:43,558.0,2.0,1.0,True,18-06-2021 10:37,17-06-2021 19:28
45658520,how to extract sentences containing citation mark from text file,"for example, i have 3 sentences like at below where 1 sentence in the middle contains citation mark (warren and pereira, 1982). the citation is always in bracket with this format: (~string~comma(,)~space~number~)

he lives in nidarvoll and tonight i must reach a train to oslo at 6 oclock. the system, called bustuc is built upon the classical system chat-80 (warren and pereira, 1982). chat-80 was a state of the art natural language system that was 
  impressive on its own merits.

i'm using regex to extract only the middle sentence but it keeps print all the 3 sentences.
the result should be like this:

the system, called bustuc is built upon the classical system chat-80 (warren and pereira, 1982).","['python', 'regex', 'nlp', 'text-extraction', 'citations']",45659366,"the setup... 2 sentences representing the cases of interest:
text = ""he lives in nidarvoll and tonight i must reach a train to oslo at 6 oclock. the system, called bustuc is built upon the classical system chat-80 (warren and pereira, 1982). chat-80 was a state of the art natural language system that was impressive on its own merits.""

t2 = ""he lives in nidarvoll and tonight i must reach a train to oslo at 6 oclock. the system, called bustuc is built upon the classical system chat-80 (warren and pereira, 1982) fgbhdr was a state of the art natural. chat-80 was a state of the art natural language system that was impressive on its own merits.""

first, to match  in the case where the citation is at the end of a sentence:
p1 = ""\. (.*\([a-za-z]+ .* [0-9]+\)\.+?)""

to match when the citation is not at the end of a sentence:
p2 = ""\. (.*\([a-za-z]+ .* [0-9]+\)[^\.]+\.+?)""

combining both cases with the `|' regex operator:
p_main = re.compile(""\. (.*\([a-za-z]+ .* [0-9]+\)\.+?)""
                ""|\. (.*\([a-za-z]+ .* [0-9]+\)[^\.]+\.+?)"")

running:
>>> print(re.findall(p_main, text))
[('the system, called bustuc is built upon the classical system chat-80 (warren and pereira, 1982).', '')]

>>>print(re.findall(p_main, t2))
[('', 'the system, called bustuc is built upon the classical system chat-80 (warren and pereira, 1982) fgbhdr was a state of the art natural.')]

in both cases you get the sentence with the citation.
a good resource is the python regular expressions documentation and the accompanying regex howto page.
cheers",https://stackoverflow.com/questions/45658520,python,13-08-2017 08:30,1215.0,0.0,2.0,True,24-06-2023 14:52,24-06-2023 14:52
61301118,load pickle notfittederror: countvectorizer - vocabulary wasn&#39;t fitted,"i am trying to classify spam messages using scikit machine learning.once i dump both vectorizer and classifier in respective.pkl files and import tem in temp.py for predictipn i am getting this error:
raise notfittederror(msg % {'name': type(estimator).__name__})
notfittederror: countvectorizer - vocabulary wasn't fitted

once i build a model saved the model with the name(my_model.pkl) ,(vectorizer.pkl)and restarting my kernel, but when i load the saved model(sample.pkl) during prediction on sample text it is giving same  volcubary not found error.
app.py:
import pandas as pd
df = pd.read_csv('spam.csv', encoding=""latin-1"")
#drop the columns not needed
df.drop(['unnamed: 2', 'unnamed: 3', 'unnamed: 4'], axis=1, inplace=true)

#create a new column label which has the same values as v1 then set the ham and spam values to 0 and 1 which is the standard format for our prediction
df['label'] = df['v1'].map({'ham': 0, 'spam': 1})

#create a new column having the same values as v2 column
df['message'] = df['v2']

#now drop the v1 and v2
df.drop(['v1', 'v2'], axis=1, inplace=true)

#print(df.head(10))

from sklearn.feature_extraction.text import countvectorizer

bow_transformer = countvectorizer().fit_transform(df['message'])

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import multinomialnb
from sklearn.metrics import classification_report

#split the data
x_train, x_test, y_train, y_test = train_test_split(bow_transformer, df['label'], test_size=0.33, random_state=42)

#naive bayes classifier
clf = multinomialnb()
clf.fit(x_train,y_train)
clf.score(x_test,y_test)

y_pred = clf.predict(x_test)
print(classification_report(y_test, y_pred))

pickle.dump(bow_transformer, open(""vector.pkl"", ""wb""))
pickle.dump(clf, open(""my_model.pkl"", ""wb""))

temp.py:::i am doing prediction in this file
from sklearn.feature_extraction.text import countvectorizer


cv=countvectorizer()


vectorizer = pickle.load(open(""my_model.pkl"", ""rb""))
selector = pickle.load(open(""vector.pkl"", ""rb""))

test_set=[""heloo how are u""]

new_test=cv.transform(test_set)","['python', 'machine-learning', 'scikit-learn', 'nlp', 'classification']",61302206,"in your app.py you are pickling the document-term matrix instead of the vectorizer,
pickle.dump(bow_transformer, open(""vector.pkl"", ""wb""))

where bow_transformer is
bow_transformer = countvectorizer().fit_transform(df['message'])

and in your temp.py when you unpickle it, you just have the document-term matrix.the right way to pickle it would be:
bow_transformer = countvectorizer().fit(df['message'])
bow_transformer_dtm = bow_transformer.transform(df['message'])

now you can pickle your bow_transformer using
pickle.dump(bow_transformer, open(""vector.pkl"", ""wb""))

which will be a transformer instead of the document term matrix.
and in your temp.py you could unpickle it and use it as illustrated below:
selector = pickle.load(open(""vector.pkl"", ""rb""))
test_set=[""heloo how are u""]
new_test=selector.transform(test_set)",https://stackoverflow.com/questions/61301118,python,19-04-2020 07:47,510.0,0.0,1.0,True,23-11-2022 12:55,19-04-2020 08:21
76383308,ruby-openai api gem in ruby on rails: how to implement a streaming conversation?,"openai provides an api which allows you to implement ai services such as chagpt or dal-e.
for ruby on rails application, and there are couple of gems available, obe of them being ruby-openai.
it works very well, but the only problem is that it doesn't come with the stream conversation feature, meaning that you can only send one question request at a time without any history tracking of the conversation. in other words, the api forgets every question you asked after having sent the reply.
so how can we fix this?","['ruby-on-rails', 'ruby', 'openai-api', 'chatgpt-api']",76383309,"basically you need to implement the whole behaviour yourself. here are all the implementation step, including the implementation of the dal-e ai with a response with several pictures rather then just one.
you can also find my whole repository here and clone the app!!!
implementing a stream conversation feature
basic implementation
check out doug berkley's notion page for basic implementation of the api
implement a streaming conversation
by default the openai gem does not come with that feature, hence having to implement it yourself

create your database with 3 tables (conversations, questions, answers) with thw following sctructure:

# schema.rb
activerecord::schema[7.0].define(version: 2023_05_29_194913) do
  create_table ""answers"", force: :cascade do |t|
    t.text ""content""
    t.integer ""question_id"", null: false
    t.datetime ""created_at"", null: false
    t.datetime ""updated_at"", null: false
    t.index [""question_id""], name: ""index_answers_on_question_id""
  end

  create_table ""conversations"", force: :cascade do |t|
    t.text ""initial_question""
    t.datetime ""created_at"", null: false
    t.datetime ""updated_at"", null: false
    t.text ""historic""
  end

  create_table ""questions"", force: :cascade do |t|
    t.text ""content""
    t.integer ""conversation_id"", null: false
    t.datetime ""created_at"", null: false
    t.datetime ""updated_at"", null: false
    t.index [""conversation_id""], name: ""index_questions_on_conversation_id""
  end

  add_foreign_key ""answers"", ""questions""
  add_foreign_key ""questions"", ""conversations""
end


routes

rails.application.routes.draw do
  root ""pages#home"" # supposes that you have a pages controller with a home action
  resources :conversations, only: [:create, :show]
  post ""question"", to: ""conversations#ask_question""
end


home page view (with just a button that redirects to the create conversation action -- see bellow)

<h1>let's talk</h1>
<%= button_to ""create new conversation"", conversations_path, method: :post, class: ""btn btn-primary my-3"" %>


controller app/controllers/conversations_controller.rb

class conversationscontroller < applicationcontroller
  def create
    @convo = conversation.create
    redirect_to conversation_path(@convo)
  end

  def show
    @convo = conversation.find(params[:id])
  end

  def ask_question
    @question = question.new(content: params[:entry])
    conversation = conversation.find(params[:conversation])
    @question.conversation = conversation
    @question.save
    if conversation.historic.nil?
      response = openaiservice.new(params[:entry]).call 
      conversation.historic = ""#{@question.content}\n#{response}""
    else
      response = openaiservice.new(""#{conversation.historic}\n#{params[:entry]}"").call
      conversation.historic += ""\n#{@question.content}\n#{response}""
    end
    conversation.save
    @answer = answer.create(content: response, question: @question)
    redirect_to conversation_path(conversation)
  end
end


show page app/views/conversations/show.html.erb

<h1>this is your conversation</h1>
<p>ask your question</p>
<form action=""<%= question_path %>"", method=""post"">
  <input type=""hidden"" name=""conversation"" value=""<%= @convo.id %>"">
  <textarea rows=""5"" cols=""33"" name=""entry""></textarea>
  <input type=""submit"" class=""btn btn-primary"">
</form>

<br>

<ul>
  <% @convo.questions.each do |question| %>
    <li>
      q: <%= question.content.capitalize %> <%= ""?"" if question.content.strip.last != ""?"" %>
    </li>
    <li>
      a: <%= question.answers.first.content %>
    </li>
  <% end %>
</ul>

<%= link_to ""back"", root_path %>



rails s and test :)

resources:





going further:",https://stackoverflow.com/questions/76383308,ruby-on-rails,01-06-2023 15:05,980.0,0.0,1.0,True,02-06-2023 13:38,02-06-2023 13:38
77989391,how do i extract data from a document using the openai api?,"i want to extract key terms from rental agreements.
to do this, i want to send the pdf of the contract to an ai service that must return some key terms in json format.
what are some of the different libraries and companies that can do this? so far, i've explored the openai api, but it isn't as straightforward as i would have imagined.
when using the chatgpt interface, it works very well, so i thought using the api should be equally simple.
it seems like i need to read the pdf text first and then send the text to openai api.
any other ideas to achieve this will be appreciated.","['python', 'artificial-intelligence', 'openai-api', 'openai-assistants-api']",77990527,"note: the code below works with the openai assistants api v1. in april 2024, the openai assistants api v2 was released. see the migration guide.

what you want to use is the assistants api.
as of today, there are 3 tools available:

code interpreter
knowledge retrieval
function calling

you need to use the knowledge retrieval tool. as stated in the official openai documentation:

retrieval augments the assistant with knowledge from outside its
model, such as proprietary product information or documents provided
by your users. once a file is uploaded and passed to the assistant,
openai will automatically chunk your documents, index and store the
embeddings, and implement vector search to retrieve relevant content
to answer user queries.

i've built a customer support chatbot in the past. take this as an example. in your case, you want the assistant to use your pdf file (i used the knowledge.txt file). take a look at my github and youtube.
customer_support_chatbot.py
import os
from openai import openai
client = openai()
openai.api_key = os.getenv('openai_api_key')

# step 1: upload a file with an ""assistants"" purpose
my_file = client.files.create(
  file=open(""knowledge.txt"", ""rb""),
  purpose='assistants'
)
print(f""this is the file object: {my_file} \n"")

# step 2: create an assistant
my_assistant = client.beta.assistants.create(
    model=""gpt-3.5-turbo-1106"",
    instructions=""you are a customer support chatbot. use your knowledge base to best respond to customer queries."",
    name=""customer support chatbot"",
    tools=[{""type"": ""retrieval""}]
)
print(f""this is the assistant object: {my_assistant} \n"")

# step 3: create a thread
my_thread = client.beta.threads.create()
print(f""this is the thread object: {my_thread} \n"")

# step 4: add a message to a thread
my_thread_message = client.beta.threads.messages.create(
  thread_id=my_thread.id,
  role=""user"",
  content=""what can i buy in your online store?"",
  file_ids=[my_file.id]
)
print(f""this is the message object: {my_thread_message} \n"")

# step 5: run the assistant
my_run = client.beta.threads.runs.create(
  thread_id=my_thread.id,
  assistant_id=my_assistant.id,
  instructions=""please address the user as rok benko.""
)
print(f""this is the run object: {my_run} \n"")

# step 6: periodically retrieve the run to check on its status to see if it has moved to completed
while my_run.status in [""queued"", ""in_progress""]:
    keep_retrieving_run = client.beta.threads.runs.retrieve(
        thread_id=my_thread.id,
        run_id=my_run.id
    )
    print(f""run status: {keep_retrieving_run.status}"")

    if keep_retrieving_run.status == ""completed"":
        print(""\n"")

        # step 7: retrieve the messages added by the assistant to the thread
        all_messages = client.beta.threads.messages.list(
            thread_id=my_thread.id
        )

        print(""------------------------------------------------------------ \n"")

        print(f""user: {my_thread_message.content[0].text.value}"")
        print(f""assistant: {all_messages.data[0].content[0].text.value}"")

        break
    elif keep_retrieving_run.status == ""queued"" or keep_retrieving_run.status == ""in_progress"":
        pass
    else:
        print(f""run status: {keep_retrieving_run.status}"")
        break",https://stackoverflow.com/questions/77989391,python,13-02-2024 16:13,7994.0,3.0,1.0,True,22-04-2024 18:18,15-02-2024 19:15
68394241,"unstructured data, nlp lemmatize book review","here i have m trying to read the content let's say 'book1.txt' and here i have to remove all the special characters and punctuation marks and word tokenise the content using nltk's word tokeniser.
lemmatize those token using  wordnetlemmatizer
and write those token into csv file one by one.
here is the code i m using which obviously is not working but just need some suggestion on this please.
    import nltk
from nltk.stem import wordnetlemmatizer
import csv
from nltk.tokenize import word_tokenize

file_out=open('data.csv','w')
with open('book1.txt','r') as myfile:
  for s in myfile:
    words = nltk.word_tokenize(s)
    words=[word.lower() for word in words if word.isalpha()]
    for word in words:
      token=wordnetlemmatizer().lemmatize(words,'v')
      filtered_sentence=[""""]
      for n in words:
        if n not in token:
          filtered_sentence.append(""""+n)
        file_out.writelines(filtered_sentence+[""\n""])","['python', 'nltk', 'file-writing', 'file-read', 'lemmatization']",68396056,"there's some issues here, most notably with the last 2 for loops.
the way you are doing it made it write it as follows:
word1
word1word2
word1word2word3
word1word2word3word4
........etc

i'm guessing that is not the expected output. i'm assuming the expected output is:
word1
word2
word3
word4
........etc (without creating duplicates)

i applied the code below to a 3 paragraph cat ipsum file. note that i changed some variable names due to my own naming conventions.
import nltk
nltk.download('punkt')
nltk.download('wordnet')
from nltk.stem import wordnetlemmatizer
from nltk.tokenize import word_tokenize
from pprint import pprint


# read the text into a single string.
with open(""book1.txt"") as infile:
    text = ' '.join(infile.readlines())
words = word_tokenize(text)
words = [word.lower() for word in words if word.isalpha()]


# create the lemmatized word list
results = []
for word in words:
    # you were using words instead of word below
    token = wordnetlemmatizer().lemmatize(word, ""v"")
    # check if token not already in results. 
    if token not in results:
        results.append(token)


# sort results, just because :)
results.sort()

# print and save the results
pprint(results)
print(len(results))
with open(""nltk_data.csv"", ""w"") as outfile:
    outfile.writelines(results)",https://stackoverflow.com/questions/68394241,python,15-07-2021 12:54,107.0,0.0,1.0,True,15-07-2021 14:49,15-07-2021 13:14
71904056,nltk plaintextcorpusreader reading files in and splitting them on delimiters,"i would like to split input text based on delimiters and only extract a specific part to process using nltk, here is the input information example:
[t] troubleshooting ad-2500 and ad-2600 no picture scrolling b/w . 
##repost from january 13 , 2004 with a better fit title . 
support[-3][u]##apex does n't answer the phone . 
player[-2][p]##unfortunately it turns out to be the "" disposable "" type . 
battery[+2]##i treat the battery well and it has lasted . 
sound quality[+2], fm[+1], earpiece[+1]##while i had the phone , the positive features were : good sound quality and an excellent fm phone and earpiece . 
speakerphone[+3][u]##you can be up to about 3 feet away from it and it will still work perfectly . 
size[+2],weight[+2]##i like the size and weight of this little critter . 
[t]excellent picture quality / color 
canon g3[+3]##i bought my canon g3 about a month ago and i have to say i am very satisfied . 
zoom[+2],lense[+2]##the extended zoom range and faster lense put it at the top of it 's class . 

i am trying to split the file using nltk to split the lines then only use the part after the ##. here is my attempt, however i could not find a solution to split the file best on delimiter:
# import  natural language toolkit library
import nltk
# importing operator module
import operator

from nltk.corpus import plaintextcorpusreader

# root folder where the text files are located
corpus_root = ""data""

# read the list of files
filelists = plaintextcorpusreader(corpus_root, '.*', encoding='utf-8')

# list down the ids of the files read from the local storage
filelists.fileids()

# read the text from specific file,
# like plaintext corpora support methods to read the corpus as 
# raw text, a list of words, a list of sentences, or a list of paragraphs.
rawlist = filelists.raw('text.txt')
wordslist = filelists.words('text.txt')
sentslist = filelists.sents('text.txt')
paraslist = filelists.paras('text.txt')

print(""a list of filenames:"")
print(filelists.fileids(),'\n')
print(""a list of words:"")
print(wordslist,'\n')
print(""a list of sentences:"")
print(sentslist,'\n')
print(""a list of paragraphs:"")
print(paraslist,'\n')
print(""a list of raw text:"")
print(rawlist,'\n')

desired output:
troubleshooting ad-2500 and ad-2600 no picture scrolling b/w . 
repost from january 13 , 2004 with a better fit title .
apex does n't answer the phone . 
unfortunately it turns out to be the "" disposable "" type . 
i treat the battery well and it has lasted . 
while i had the phone , the positive features were : good sound quality and an excellent fm phone and earpiece . 
you can be up to about 3 feet away from it and it will still work perfectly . 
i like the size and weight of this little critter . 
excellent picture quality / color 
i bought my canon g3 about a month ago and i have to say i am very satisfied . 
the extended zoom range and faster lense put it at the top of it 's class .","['python', 'nltk', 'delimiter']",71928953,i used the existing corpora import function in nltk for the utilization of the files for this project. first i found the actual directory of the folders from nltk.corpus import product_reviews_1 as the product review 1 is a known module in the current nltk data package. then running nltk.corpus.product_reviews_1.abspaths() to get the exact path of the folders. after this i copied the folders into the corpora directory,https://stackoverflow.com/questions/71904056,python,17-04-2022 17:39,504.0,0.0,1.0,True,19-04-2022 17:26,17-04-2022 18:14
78848363,how to extract terms and probabilities from tmresult$terms in topic modeling?,"i like to create separate word clouds for each of my 8 topics in an lda model. i extracted top 40 words across 8 topics - an object of length 320 containing top words and occurrence probabilities.
i am struggling with accessing the terms and probabilities from my top_words_vector object. it is hard to reproduce bc of the tmresult object, but any hint would be much appreciated:
    textdata <- base::readrds(url("" ""rb""))

english_stopwords <- readlines("" encoding = ""utf-8"")

corpus <- corpus(dataframesource(textdata))

processedcorpus <- tm_map(corpus, content_transformer(tolower))
processedcorpus <- tm_map(processedcorpus, removewords, english_stopwords)
processedcorpus <- tm_map(processedcorpus, removepunctuation, preserve_intra_word_dashes = true)
processedcorpus <- tm_map(processedcorpus, removenumbers)
processedcorpus <- tm_map(processedcorpus, stemdocument, language = ""en"")
processedcorpus <- tm_map(processedcorpus, stripwhitespace)

minimumfrequency <- 5
dtm <- documenttermmatrix(processedcorpus, control = list(bounds = list(global = c(minimumfrequency, inf))))
sel_idx <- slam::row_sums(dtm) > 0
dtm <- dtm[sel_idx, ]
textdata <- textdata[sel_idx, ]

k <- 8
set.seed(9161)
# compute the lda model, inference via 100 iterations of gibbs sampling
topicmodel <- lda(dtm, k, method=""gibbs"", control=list(iter = 100, verbose = 25))
tmresult <- topicmodels::posterior(topicmodel)
tmresult$terms 

top_words_vector = c() # an empty container for 320 length, top#40 words across 8 topics
for(i in 1:8){
  top_words_vector = c(top_words_vector,sort(tmresult$terms[i,], decreasing=true)[1:40])
}

top_words_vector

wordcloud() takes terms and probs separately, that's what i am trying to extract from top_words_vector:
mycolors <- brewer.pal(8, ""dark2"")
wordcloud(c(""apple"", ""banana""), c(0.8,0.2), random.order = true, color = mycolors)","['r', 'tm', 'topic-modeling']",78848876,"names(top_words_vector) accesses the names of the stored values.
library(tm)
library(topicmodels)
library(rcolorbrewer)
library(wordcloud)


mycolors <- brewer.pal(8, ""dark2"")
wordcloud(names(top_words_vector), top_words_vector, random.order = true, color = mycolors)",https://stackoverflow.com/questions/78848363,r,08-08-2024 12:16,38.0,0.0,1.0,True,08-08-2024 14:02,08-08-2024 13:24
78802377,calculation of pricing of tokens in openai calls,"i'm trying to price the tokens used in a call to openai. i have a txt file with plain text that was uploaded to qdrant. when i ask the following question:
who is michael jordan?
and use the get_openai_callback function to track the number of tokens and the price of the operation, one of the keys of information in the output doesn't make sense to me.
tokens used: 85
    prompt tokens: 68
    completion tokens: 17
successful requests: 1
total cost (usd): $0.00013600000000000003

why does the prompt tokens value differ from the input value? the amount of tokens in the input text (which is what i understand as prompt token) is:
query = 'who is michael jordan'

encoding = tiktoken.encoding_for_model('gpt-3.5-turbo-instruct')
print(f""tokens: {len(encoding.encode(query))}"")

4

, but the output in the response is like 68. i considered the idea that prompt tokens were the sum of the base tokens (txt file) added to the question tokens, but the math doesn't fit.
number of tokens in the txt file: 17
arquivo txt: 'michael jeffrey jordan is an american businessman and former basketball player who played as a shooting guard'
query + file_token: 21 (4+17)
could anyone help me understand the pricing calculation?
i tried to search openai's own documentation, github and other forums, but i don't think it's easy to find information or that it's open to the public. i want to understand if i'm missing something or if it's a calculation that users don't have access to.
update
for any future questions from other users:
import langchain 
langchain.debug = true

run the get_openai_callback() function and see the entire log appear on the screen. the value of the ""prompts"" key is a list containing a string that is the instruction on how the response should be given. the number of tokens for this prompt is the value that appears in the prompt tokens.","['python', 'token', 'openai-api', 'langchain', 'qdrant']",78802404,"prompt tokens includes your question and any context provided, plus additional system messages and formatting added by the api. while completion tokens generated in the response.
in your example:
visible query: who is michael jordan? (4 tokens)
text from file: michael jeffrey jordan is an american businessman and former basketball player who played as a shooting guard (17 tokens)
expected: 4+17=21
4+17=21 tokens.
however, you see 68 prompt tokens because the api adds tokens for roles, instructions, and other metadata.to understand the exact token count, you can log the full request payload or use openai's token counting tools.
this extra context explains why the prompt token count is higher than expected.",https://stackoverflow.com/questions/78802377,python,27-07-2024 20:18,476.0,0.0,1.0,True,27-07-2024 20:56,27-07-2024 20:56
71935529,"model was constructed with shape (none, 50) for input kerastensor(type_spec=tensorspec(shape=(none, 50),","i have followed some kind of training and the final jupyter notebook was this:

i understand the entire code, and how the model was trained.
however, at the end i am predicting emotions for tweets in the test dataset like this:
i = random.randint(0, len(test_labels)-1)
print('sentence:', test_tweets[i])
print('emotion:', index_to_class[test_labels[i]])
p = model.predict(np.expand_dims(test_seq[i], axis=0))[0]
pred_class = index_to_class[np.argmax(p).astype('uint8')]
print('predicted emotion:', pred_class)

this works perfectly fine.
however i want to test the model prediction with random sentences, like:
sentence = 'i love you more than ever'
print('sentence:',  sentence)
#print('emotion:', index_to_class[test_labels[i]])
p = model.predict(np.expand_dims(sentence, axis=0))[0]
pred_class = index_to_class[np.argmax(p).astype('uint8')]
print('predicted emotion:', pred_class)

but i got this error:
sentence: i love you more than ever
warning:tensorflow:model was constructed with shape (none, 50) for input kerastensor(type_spec=tensorspec(shape=(none, 50), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=""created by layer 'embedding_input'""), but it was called on an input with incompatible shape (none,).

what am i missing here?","['python', 'tensorflow', 'keras', 'nlp']",71935636,"your model needs an integer sequence, not a raw string. try converting the sentence to its corresponding integer sequence first:
sentence = 'i love you more than ever'
print('sentence:',  sentence)
#print('emotion:', index_to_class[test_labels[i]])
sentence = get_sequences(tokenizer, np.expand_dims(sentence, axis=0))
p = model.predict(sentence)[0]
pred_class = index_to_class[np.argmax(p).astype('uint8')]
print('predicted emotion:', pred_class)

sentence: i love you more than ever
predicted emotion: joy",https://stackoverflow.com/questions/71935529,python,20-04-2022 07:10,540.0,1.0,2.0,True,20-04-2022 08:05,20-04-2022 08:00
74461417,i am doing nlp lstm next word prediction. but i get error of to_categorical &quot;indexerror: index 2718 is out of bounds for axis 1 with size 2718&quot;,"below is the full code:
import spacy
from tensorflow.keras.utils import to_categorical
from keras.preprocessing.text import tokenizer
import numpy as np
import keras
from keras.models import sequential
from keras.layers import dense,lstm,embedding

def read_file(filepath):    
    with open(filepath) as f:
        str_text = f.read()
    
    return str_text

moby_text = read_file('moby_dick.txt')

nlp = spacy.load('en_core_web_sm')
doc = nlp(moby_text)

#getting tokens using list comprehension
tokens = [token.text.lower() for token in doc]

#cleaning text
tokens = [token for token in tokens if token not in '\n\n \n\n\n!""-#$%&()--.*+,-/:;<=>?@[\\]^_`{|}~\t\n ']

train_len = 10+1 # 10 i/p and 1 o/p
text_sequences = []
for i in range(train_len,len(tokens)):
    seq = tokens[i-train_len:i]
    text_sequences.append(seq)
tokenizer = tokenizer()
tokenizer.fit_on_texts(text_sequences)
sequences = tokenizer.texts_to_sequences(text_sequences) 

for i in sequences[0]:
    print(f'{i} : {tokenizer.index_word[i]}')

sequences = np.array(sequences)
vocabulary_size = len(tokenizer.word_counts)



def create_model(vocabulary_size, seq_len):
    
    model = sequential()
    model.add(embedding(vocabulary_size, 25, input_length=seq_len))
    model.add(lstm(100,return_sequences=true))
    model.add(lstm(100))
    model.add(dense(100,activation='relu'))
    model.add(dense(vocabulary_size, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
    return model

x = sequences[:,:-1]
y = sequences[:,-1]
y = to_categorical(y, num_classes=vocabulary_size)

here in the to_categorical i'm getting the error. i don't understand why? and after reading so many articles i still don't get how  to solve it.
indexerror: index 2718 is out of bounds for axis 1 with size 2718


error
seq_len = x.shape[1]
model = create_model(vocabulary_size, seq_len)
model.fit(x, y,  epochs=100,verbose=1)

i don't understand the error. i have searched the error and tried different ways to solve it but i can't find anything to solve it. also, i guess this is because the indices for lists start at 0. and i have done
y = y - 1
y = to_categorical(y, num_classes=vocabulary_size)

but this doesn't work because it gives error in the model. so i am back to square one.
node: 'sequential/embedding/embedding_lookup'
indices[13,9] = 2718 is not in [0, 2718)
     [[{{node sequential/embedding/embedding_lookup}}]] [op:__inference_train_function_5647]

so how can i solve it?
can someone please help me out? thank you!!!","['tensorflow', 'keras', 'nlp', 'lstm', 'tokenize']",74462902,"the tokenizer doesn't use 0, it starts counting with 1:

0 is a reserved index that won't be assigned to any word.

try this:
vocabulary_size = len(tokenizer.word_counts) + 1",https://stackoverflow.com/questions/74461417,tensorflow,16-11-2022 13:35,75.0,0.0,1.0,True,16-11-2022 15:12,16-11-2022 14:13
9143442,algorithm for keyword/phrase trend search similar to twitter trends,"wanted some ideas about building a tool which can scan text sentences (written in english language) and build a keyword rank, based on the most occurrences of words or phrases within the texts. 
this would be very similar to the twitter trends wherin twitter detects and reports the top 10 words within the tweets.
i have identified the high level steps in the algorithm as follows  

scan the text and remove all the common , frequent words ( such as, ""the"" , ""is"" , ""are"", ""what"" , ""at"" etc..)
add the remaining words to a hashmap. if the word is already in the map then increment its count.
to get the top 10 words , iterate through the hashmap and find out the top 10 counts.

step 2 and 3 are straightforward but i do not know in step 1 how do i detect the important words within a text and segregate them from the common words (prepositions, conjunctions etc )
also if i want to track phrases what could be the approach ?
for example if i have a text saying ""this honey is very good"" 
i might want to track ""honey"" and ""good"" but i may also want to track the phrases ""very good"" or ""honey is very good"" 
any suggestions would be greatly appreciated.
thanks in advance","['twitter', 'nlp']",9143625,"actually, your step 1 would be quite similar to step 3 in the sense that you may want to constitute an absolute database of the most common words in the english language in the first place. such a list is available easily on the internet (wikipedia even has an article referencing the 100 most common words in the english language.) you can store those words in a hashmap and while scanning your text contents just ignore the common tokens.
if you don't trust wikipedia and the already existing listing for common words, you can build your own database. for that purpose, just scan thousands of tweets (the more the better) and make your own frequency chart.
you're facing an n-gram-like problem.
do not reinvent the wheel. what you seem to be wanting to do has been done thousands of times, just use existing libs or pieces of code (check the external links section of the n-gram wikipedia page.)",https://stackoverflow.com/questions/9143442,twitter,04-02-2012 18:46,2606.0,2.0,3.0,True,19-09-2022 08:03,05-02-2012 02:34
55046327,generate a word cloud to show frequenices of numbers in python,"i have a pandas dataframe which consists of grade points of students. i want to generate the word cloud or number cloud for the grades. is there any way to achieve it. i tried all possible ways but all my efforts in vain. basically what i want is word cloud that contains numbers in it. from the column cgpa.
here is what i tried : 
import pandas as pd
from wordcloud import wordcloud
import matplotlib.pyplot as plt
df = pd.read_csv(""vtu_marks.csv"")
# rounding off
df = df[df['cgpa'].isnull() == false]
df['cgpa'] = df['cgpa'].round(decimals=2)

wordcloud = wordcloud(max_font_size=50,max_words=100,background_color=""white"").generate(string)
plt.figure()
plt.imshow(wordcloud, interpolation=""bilinear"")
plt.axis(""off"")
plt.show()

but i am getting an error
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
<ipython-input-47-29ec36ebbb1e> in <module>()
----> 1 wordcloud = wordcloud(max_font_size=50, max_words=100, background_color=""white"").generate(string)
      2 plt.figure()
      3 plt.imshow(wordcloud, interpolation=""bilinear"")
      4 plt.axis(""off"")
      5 plt.show()

/usr/local/lib/python3.6/dist-packages/wordcloud/wordcloud.py in generate(self, text)
    603         self
    604         """"""
--> 605         return self.generate_from_text(text)
    606 
    607     def _check_generated(self):

/usr/local/lib/python3.6/dist-packages/wordcloud/wordcloud.py in generate_from_text(self, text)
    585         """"""
    586         words = self.process_text(text)
--> 587         self.generate_from_frequencies(words)
    588         return self
    589 

/usr/local/lib/python3.6/dist-packages/wordcloud/wordcloud.py in generate_from_frequencies(self, frequencies, max_font_size)
    381         if len(frequencies) <= 0:
    382             raise valueerror(""we need at least 1 word to plot a word cloud, ""
--> 383                              ""got %d."" % len(frequencies))
    384         frequencies = frequencies[:self.max_words]
    385 

valueerror: we need at least 1 word to plot a word cloud, got 0.

you can find the data here.","['pandas', 'matplotlib', 'nlp', 'nltk', 'word-cloud']",55049190,"after setting up your data and rounding as desired we can count up the frequency of each score:
counts = df['cgpa'].value_counts()

we need to make sure that the indices here are strings, floats will raise an error (this is what was wrong in your example attempt). so, we can convert them to strings as:
counts.index = counts.index.map(str)

#below alternative works for pandas versions >= 0.19.0
#counts.index = counts.index.astype(str)

we can then use the .generate_from_frequencies method to get what you desire:
wordcloud = wordcloud().generate_from_frequencies(counts)
plt.figure()
plt.imshow(wordcloud, interpolation=""bilinear"")
plt.axis(""off"")
plt.show()

this gave me the following:

full mwe:
import pandas as pd
from wordcloud import wordcloud
import matplotlib.pyplot as plt

df = pd.read_csv(""vtu_marks.csv"")
# rounding off
df = df[df['cgpa'].isnull() == false]
df['cgpa'] = df['cgpa'].round(decimals=2)

counts = df['cgpa'].value_counts()

counts.index = counts.index.map(str)
#counts.index = counts.index.astype(str)

wordcloud = wordcloud().generate_from_frequencies(counts)
plt.figure()
plt.imshow(wordcloud, interpolation=""bilinear"")
plt.axis(""off"")
plt.show()",https://stackoverflow.com/questions/55046327,pandas,07-03-2019 14:36,13767.0,2.0,1.0,True,18-05-2023 03:10,09-02-2020 10:03
78853409,nllb fine-tuning error: missing data_prefix configuration (english-german translation),"i'm attempting to fine-tune the nllb model ""facebook/nllb-200-distilled-600m"" for a scientific translation task from english (eng_latn) to german (deu_latn). i followed the official guidelines for fine-tuning by authors of nllb.
documentation: link
this is the code block which is giving error:
data_config = ""/content/sample_data/data_config.json""
output_dir = ""/content/outputs""
model_folder = ""/content/drive/mydrive/thesis/nllb-checkpoints""
drop = 0.1
src = ""eng_latn""
tgt = ""deu_latn""
!python /content/fairseq/examples/nllb/modeling/train/train_script.py \
    cfg=nllb200_dense3.3b_finetune_on_fbseed \
    cfg/dataset=default \
    cfg.dataset.lang_pairs=""$src-$tgt"" \
    cfg.fairseq_root=$(pwd) \
    cfg.output_dir=$output_dir \
    cfg.dropout=$drop \
    cfg.warmup=10 \
    cfg.finetune_from_model=$model_folder/checkpoint.pt

this is the error:
/content/fairseq/examples/nllb/modeling/train/train_script.py:287: userwarning: 
the version_base parameter is not specified.
please specify a compatability version level, or none.
will assume defaults for version 1.1
  @hydra.main(config_path=""conf"", config_name=""base_config"")
/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: userwarning: future hydra versions will no longer change working directory at job runtime by default.
see  for more information.
  ret = run_job(
training dir:  /content/outputs
error executing job with overrides: ['cfg=nllb200_dense3.3b_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_latn-deu_latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/mydrive/lass_kg_data/thesis/nllb-checkpoints/checkpoint.pt']
traceback (most recent call last):
  file ""/content/fairseq/examples/nllb/modeling/train/train_script.py"", line 289, in main
    train_module = trainmodule(config)
  file ""/content/fairseq/examples/nllb/modeling/train/train_script.py"", line 122, in __init__
    assert cluster_name in cfg.dataset.data_prefix
omegaconf.errors.configattributeerror: key 'data_prefix' is not in struct
    full_key: cfg.dataset.data_prefix
    object_type=dict

set the environment variable hydra_full_error=1 for a complete stack trace.

so far, i understand there is a missing data_prefix configuration. i created a demo custom data_config.json. which looks like this:
{
    ""data_prefix"": ""/content/sample_data"",
    ""train_data"": ""train_demo.json"",
    ""test_data"": ""test_demo.json"",
    ""lang_pairs"": ""eng_latn-deu_latn""
}

while the official documentation provides some information, i'm encountering difficulties in applying it to my specific use case. can someone share a detailed guide or point me to helpful resources on fine-tuning nllb?","['python', 'nlp', 'machine-translation', 'fine-tuning', 'fairseq']",78854613,"while i can't help you with the concrete error message you are getting (my guess would be issues with structure of the provided json files), my personal recommendation would be to fine-tune nllb in the transformers library, specifically using the seq2seqtrainer.
i did this before for multiple models, including nllb, check out this repository: 
this way the fine-tuning and inference process for the nllb model is the same as any bilingual model (you can find guides for those more easiely), with the only exception that you load the tokenizer like so:
tokenizer = nllbtokenizer.from_pretrained(model_path, src_lang=""eng_latn"", tgt_lang=""deu_latn"")

and generate translations like this:
model.generate(tokenized_chunk.input_ids, forced_bos_token_id=tokenizer.encode(""deu_latn"")[1], max_length=512)",https://stackoverflow.com/questions/78853409,python,09-08-2024 14:46,161.0,1.0,1.0,True,10-08-2024 20:19,10-08-2024 20:19
74610298,translation with multi-lingual bert model,"i want to translate my dataframe using multi-lingual bert.
i have copied this code but in place of text, i want to use my own dataframe.
from transformers import berttokenizer, tfbertmodel
tokenizer = berttokenizer.from_pretrained('bert-base-multilingual-cased')
model = tfbertmodel.from_pretrained(""bert-base-multilingual-cased"")
text = ""replace me by any text you'd like.""
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)

however, i get some errors when using it like below.
df  =pd.read_csv(""/content/drive/text.csv"")
encoded_input = tokenizer(df, return_tensors='tf')

error
valueerror: text input must of type `str` (single example), `list[str]` (batch or single pretokenized example) or `list[list[str]]` (batch of pretokenized examples).

my dataframe looks like this
0    there is xxxx increased opacity within the rig...
1    there is xxxx increased opacity within the rig...
2    there is xxxx increased opacity within the rig...
3    interstitial markings are diffusely prominent ...
4    interstitial markings are diffusely prominent ...
name: findings, dtype: object","['pandas', 'nlp', 'bert-language-model', 'generative-pretrained-transformer']",74611016,"the first one is using a string to tokenizer.
the second one you are trying to tokenizer an entire dataframe, not a string.",https://stackoverflow.com/questions/74610298,pandas,29-11-2022 07:13,406.0,0.0,1.0,True,13-12-2022 16:01,13-12-2022 16:01
65160277,spacy tokenizer with only &quot;whitespace&quot; rule,"i would like to know if the spacy tokenizer could tokenize words only using the ""space"" rule.
for example:
sentence= ""(c/o oxford university )""

normally, using the following configuration of spacy:
nlp = spacy.load(""en_core_news_sm"")
doc = nlp(sentence)
for token in doc:
   print(token)

the result would be:
 (
 c
 /
 o
 oxford
 university
 )

instead, i would like an output like the following (using spacy):
(c/o 
oxford 
university
)

is it possible to obtain a result like this using spacy?","['python', 'python-3.x', 'nlp', 'spacy']",65166022,"let's change nlp.tokenizer with a custom tokenizer with token_match regex:
import re
import spacy
from spacy.tokenizer import tokenizer

nlp = spacy.load('en_core_web_sm')
text = ""this is it's""
print(""before:"", [tok for tok in nlp(text)])

nlp.tokenizer = tokenizer(nlp.vocab, token_match=re.compile(r'\s+').match)
print(""after :"", [tok for tok in nlp(text)])


before: [this, is, it, 's]
after : [this, is, it's]

you can further adjust tokenizer by adding custom suffix, prefix, and infix rules.
an alternative, more fine grained way would be to find out why it's token is split like it is with nlp.tokenizer.explain():
import spacy
from spacy.tokenizer import tokenizer
nlp = spacy.load('en_core_web_sm')
text = ""this is it's. i'm fine""
nlp.tokenizer.explain(text)


you'll find out that split is due to special rules:
[('token', 'this'),
 ('token', 'is'),
 ('special-1', 'it'),
 ('special-2', ""'s""),
 ('suffix', '.'),
 ('special-1', 'i'),
 ('special-2', ""'m""),
 ('token', 'fine')]

that could be updated to remove ""it's"" from exceptions like:
exceptions = nlp.defaults.tokenizer_exceptions
filtered_exceptions = {k:v for k,v in exceptions.items() if k!=""it's""}
nlp.tokenizer = tokenizer(nlp.vocab, rules = filtered_exceptions)
[tok for tok in nlp(text)]


[this, is, it's., i, 'm, fine]

or remove split on apostrophe altogether:
filtered_exceptions = {k:v for k,v in exceptions.items() if ""'"" not in k}
nlp.tokenizer = tokenizer(nlp.vocab, rules = filtered_exceptions)
[tok for tok in nlp(text)]


[this, is, it's., i'm, fine]

note the dot attached to the token, which is due to the suffix rules not specified.",https://stackoverflow.com/questions/65160277,python,05-12-2020 18:04,7108.0,5.0,3.0,True,15-05-2023 21:44,07-12-2020 09:23
77653666,import error in training arguments in colaboratory,"i am using google colaboratory for my nlp project. i installed transformers and other libraries, but i got an error.
from transformers import trainer, trainingarguments

batch_size = 64
logging_steps = len(stationary_dataset_encoded[""train""]) // batch_size
model_name = f""{model_ckpt}-finetuned-stationary-update""
training_args = trainingarguments(output_dir=model_name,
                                  num_train_epochs=10,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy=""epoch"",
                                  disable_tqdm=false,
                                  logging_steps=logging_steps,
                                  push_to_hub=false,
                                  log_level=""error"")

in the training_args line, i got an import error. itï¿½ï¿½ï¿½s showing as:
importerror                               traceback (most recent call last)
<ipython-input-98-839907b16fa0> in <cell line: 6>()
      4 logging_steps = len(stationary_dataset_encoded[""train""]) // batch_size
      5 model_name = f""{model_ckpt}-finetuned-stationary-update""
----> 6 training_args = trainingarguments(output_dir=model_name,
      7                                   num_train_epochs=10,
      8                                   learning_rate=2e-5,

4 frames
/usr/local/lib/python3.10/dist-packages/transformers/training_args.py in _setup_devices(self)
   1785     def __str__(self):
   1786         self_as_dict = asdict(self)
-> 1787
   1788         # remove deprecated arguments. that code should be removed once
   1789         # those deprecated arguments are removed from trainingarguments. (todo: v5)

importerror: using the `trainer` with `pytorch` requires `accelerate>=0.20.1`: please run `pip install transformers[torch]` or `pip install accelerate -u

i tried to reinstall transformers, use pip install accelerate -u and also used pytorch accelerate >= 0.20.1.
how can i resolve this issue?","['python', 'nlp', 'huggingface']",77653714,"there might be an issue with your transformers library installation.

uninstall the current transformers library.
reinstall the transformers library.
install the accelerate library.
ensure that you have the correct version of pytorch.

!pip uninstall -y transformers
!pip install transformers
!pip install accelerate -u

check your pytorch version with the following command:
import torch
print(torch.__version__)

the message suggests that you need to install the accelerate library with version 0.20.1 or higher.
you can install it using the following command:
!pip install accelerate -u
you might want to try installing the transformers library with the torch extra. this can be done with the following command:
!pip install transformers[torch]",https://stackoverflow.com/questions/77653666,python,13-12-2023 12:41,774.0,0.0,1.0,True,02-01-2024 15:58,02-01-2024 15:58
75272415,nltk.download(&#39;punkt&#39;) giving output as false,"here is my code:
df['num_words'] = df['text'].apply(lambda x:len(nltk.word_tokenize(x)))

which gives me the following error:
resource punkt not found.
please use the nltk downloader to obtain the resource:
   
>>> import nltk
>>> nltk.download('punkt')
  
for more information see: 

attempted to load tokenizers/punkt/english.pickle

then i tried to install nltk and download the file 'punkt' using nltk.download('punkt').
but i am getting this error.
i tried some alternative codes like:
import nltk
import ssl

try:
    _create_unverified_ = ssl._create_unverified_context
except attributeerror:
    pass
else:
    ssl._create_default_ = _create_unverified_

nltk.download()

also tried changing the networks as at some places i found it is saying server issue.","['python', 'machine-learning', 'nlp', 'data-science', 'nltk']",75272570,"try to launch the jupyter notebooks session as administrator (open the command or anaconda prompt as administrator).
the last option would be to download the corpus manually. you may find this, helpful in your case.",https://stackoverflow.com/questions/75272415,python,29-01-2023 03:18,534.0,0.0,1.0,True,21-09-2023 03:03,21-09-2023 03:03
74853108,spacy: generalize a language factory that gets a regular expression to create spans in a text,"working with spacy it is possible to define spans in a document that correspond to a regular expression matching on the text.
i would like to generalize this into a language factory.
the code to create a span could be like this:
nlp = spacy.load(""en_core_web_sm"")
text = ""this is pepa pig text comprising a brake and fig. 45. the house is white.""
doc=nlp(text)
def _component(doc, name, regular_expression):
    if name not in doc.spans:
        doc.spans[name] = []
    for i, match in enumerate(re.finditer(regular_expression, doc.text)):
        label = name + ""_"" + str(i)
        start, end = match.span()
        span = doc.char_span(start, end, alignment_mode = ""expand"")
        span_to_add = span(doc, span.start, span.end, label=label)

        doc.spans[name].append(span_to_add)
    return doc
doc = _component(doc, 'pepapig', r""pepa\spig"")  

i would like to generalize this into a factory.
the factory would take a particular list of regular expressions with names like:
[{'name':'pepapig','rex':r""pepa\spig""},{'name':'pepapig2','rex':r""george\spig""}]]

the way i try to do this is as follows (code does not work)
@language.factory(""myregexes6"", default_config={})
def add_regex_match_as_span(nlp, name, regular_expressions):   
    for i,rex_d in enumerate(regular_expressions):
        print(rex_d)
        name = rex_d['name']
        rex = rex_d['rex']
        _component(doc, name=name, regular_expression=rex, debug=false)

    return doc

nlp.add_pipe(add_regex_match_as_span(nlp, ""mc"", regular_expressions=[{'name':'pepapig','rex':r""pepa\spig""},{'name':'pepapig2','rex':r""george\spig""}]))

i am looking to for the solution to the above code
the error i get is:
[e966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. expected string, but got this is pepa pig text comprising a brake and fig. 45. the house is white. (name: 'none').

- if you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.

- if you passed in a component like `textcategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.

- if you're using a custom component: add the decorator `@language.component` (for function components) or `@language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@language.component('your_name')`. you can then run `nlp.add_pipe('your_name')` to add it to the pipeline.

last edit
how can the factory be saved into a .py file and reread from other file?","['python', 'spacy']",74854337,"i think that you need to follow what's in [the documentation for custom components][1]. 
here's how i tried to solve the problem you're facing. 
i would start first by creating the component which should be a class in this case because you have parameters ""a state"". in this case, the parameters are a list of {'name': name, 'rex': rex} called regex_list.
class regexcomponent:
  def __init__(self, regex_list):
    self.regex_list = regex_list
  
  def __call__(self, doc):
    for re_item in self.regex_list:
      if re_item['name'] not in doc.spans:
        
        doc.spans[re_item['name']] = []
      for i, match in enumerate(re.finditer(re_item['rex'], doc.text)):
          label = re_item['name'] + ""_"" + str(i)
          start, end = match.span()
          span = doc.char_span(start, end, alignment_mode = ""expand"")
          span_to_add = span(doc, span.start, span.end, label=label)

          doc.spans[re_item['name']].append(span_to_add)
    return doc

now that you have your component, you need a ""factory"" to create it with the specified parameters. here's how you can do it:
@language.factory(""myregex"", default_config={})
def create_regex(nlp, name, regex_list):   
    return regexcomponent(regex_list)

nlp and name should always be there while regex is the input of your regex component. 
here's a sample of how you can call your newly created component:
regex_list = [{'name':'pepapig','rex':r""pepa\spig""},{'name':'pepapig2','rex':r""george\spig""}]
nlp = spacy.load(""en_core_web_sm"")
nlp.add_pipe(""myregex"", ""mc"", config={'regex_list': regex_list})
text = ""this is pepa pig text comprising a brake and fig. 45. the house is white. hello george pig""
doc=nlp(text)
print(doc.spans)  # {'pepapig': [pepa pig], 'pepapig2': [george pig]}

if you created this component in a seperate file 'custom_pipe.py', you can call it this way:
import spacy

from custom_pipe import regexcomponent

regex_list = [
    {""name"": ""pepapig"", ""rex"": r""pepa\spig""},
    {""name"": ""pepapig2"", ""rex"": r""george\spig""},
]
nlp = spacy.load(""en_core_web_sm"")
nlp.add_pipe(""myregex"", ""mc"", config={""regex_list"": regex_list})
text = ""this is pepa pig text comprising a brake and fig. 45. the house is white. hello george pig""
doc = nlp(text)
print(doc.spans)

i hope you found my answer helpful ! 
[1]:",https://stackoverflow.com/questions/74853108,python,19-12-2022 16:11,656.0,2.0,1.0,True,21-12-2022 14:03,21-12-2022 12:46
65431837,transformers v4.x: convert slow tokenizer to fast tokenizer,"i'm following the transformer's pretrained model xlm-roberta-large-xnli example
from transformers import pipeline
classifier = pipeline(""zero-shot-classification"",
                      model=""joeddav/xlm-roberta-large-xnli"")

and i get the following error
valueerror: couldn't instantiate the backend tokenizer from one of: (1) a `tokenizers` library serialization file, (2) a slow tokenizer instance to convert or (3) an equivalent slow tokenizer class to instantiate and convert. you need to have sentencepiece installed to convert a slow tokenizer to a fast one.

i'm using transformers version '4.1.1'","['python', 'nlp', 'huggingface-transformers', 'huggingface-tokenizers']",65431838,"according to transformers v4.0.0 release, sentencepiece was removed as a required dependency. this means that

""the tokenizers that depend on the sentencepiece library will not be available with a standard transformers installation""

including the xlmrobertatokenizer. however, sentencepiece can be installed as an extra dependency
pip install transformers[sentencepiece]

or
pip install sentencepiece

if you have transformers already installed.",https://stackoverflow.com/questions/65431837,python,23-12-2020 22:44,44735.0,37.0,5.0,True,05-03-2024 16:41,23-12-2020 22:58
63012476,labelencoder instance is not fitted yet,"i have a code for prediction of unseen data in a sentence classification task.
the code is
from sklearn.preprocessing import labelencoder

maxlen = 1152

### predict new unseen data ###
tokenizer = tokenizer()
label_enc = labelencoder()
x_test = ['this is boring', 'wow i like this you did a great job']

x_test = tokenizer.texts_to_sequences(x_test)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)

a = (model.predict(x_test)>0.5).astype(int).ravel()
print(a)

reverse_pred = label_enc.inverse_transform(a.ravel())
print(reverse_pred)

but i am getting this error
[1 1]
---------------------------------------------------------------------------
notfittederror                            traceback (most recent call last)
<ipython-input-33-7e12dbe8aec1> in <module>()
     39 print(a)
     40 
---> 41 reverse_pred = label_enc.inverse_transform(a.ravel())
     42 print(reverse_pred)

1 frames
/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py in check_is_fitted(estimator, attributes, msg, all_or_any)
    965 
    966     if not attrs:
--> 967         raise notfittederror(msg % {'name': type(estimator).__name__})
    968 
    969 

notfittederror: this labelencoder instance is not fitted yet. call 'fit' with appropriate arguments before using this estimator.

i have used sequential model and the model.fit is written as history=model.fit() in the training part. why am i getting this error?","['python-3.x', 'tensorflow', 'keras', 'scikit-learn', 'nlp']",63012894,"following the sklearn documentation and what reported here, you have simply to fit your encoder before making an inverse transform
y = ['positive','negative','positive','negative','positive','negative']
label_enc = labelencoder()
label_enc.fit(y)

model_predictions = np.random.uniform(0,1, 3)
model_predictions = (model_predictions>0.5).astype(int).ravel()
model_predictions = label_enc.inverse_transform(model_predictions)",https://stackoverflow.com/questions/63012476,python-3.x,21-07-2020 10:25,4761.0,1.0,1.0,True,19-07-2022 07:12,19-07-2022 07:12
59956670,parsing city of origin / destination city from a string,"i have a pandas dataframe where one column is a bunch of strings with certain travel details. my goal is to parse each string to extract the city of origin and destination city (i would like to ultimately have two new columns titled 'origin' and 'destination').
the data:
df_col = [
    'new york to venice, italy for usd271',
    'return flights from brussels to bangkok with etihad from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½407',
    'from los angeles to guadalajara, mexico for usd191',
    'fly to australia new zealand from paris from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½422 return including 2 checked bags'
]

this should result in:
origin: new york, usa; destination: venice, italy
origin: brussels, bel; destination: bangkok, thailand
origin: los angeles, usa; destination: guadalajara, mexico
origin: paris, france; destination: australia / new zealand (this is a complicated case given two countries)

thus far i have tried:
a variety of nltk methods, but what has gotnltk.pos_tag method to tag each word in the string. the result is a list of tuples with each word and associated tag. here's an example...
[('fly', 'nnp'), ('to', 'to'), ('australia', 'nnp'), ('&', 'cc'), ('new', 'nnp'), ('zealand', 'nnp'), ('from', 'in'), ('paris', 'nnp'), ('from', 'in'), ('ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½422', 'nnp'), ('return', 'nn'), ('including', 'vbg'), ('2', 'cd'), ('checked', 'vbd'), ('bags', 'nns'), ('!', '.')]

i am stuck at this stage and am unsure how to best implement this. can anyone point me in the right direction, please","['python', 'regex', 'pandas', 'nlp', 'nltk']",59959188,"tl;dr
pretty much impossible at first glance, unless you have access to some api that contains pretty sophisticated components. 
in long
from first look, it seems like you're asking to solve a natural language problem magically. but lets break it down and scope it to a point where something is buildable. 
first, to identify countries and cities, you need data that enumerates them, so lets try:  
and top of the search results, we find  that leads to the world-cities.json file. now we load them into sets of countries and cities. 
import requests
import json

cities_url = ""
cities_json = json.loads(requests.get(cities_url).content.decode('utf8'))

countries = set([city['country'] for city in cities_json])
cities = set([city['name'] for city in cities_json])

now given data, lets try to build component one:

task: detect if any substring in the texts matches a city/country.
tool:  (a fast string search/match)
metric: no. of correctly identified cities/countries in string

lets put them together.
import requests
import json
from flashtext import keywordprocessor

cities_url = ""
cities_json = json.loads(requests.get(cities_url).content.decode('utf8'))

countries = set([city['country'] for city in cities_json])
cities = set([city['name'] for city in cities_json])


keyword_processor = keywordprocessor(case_sensitive=false)
keyword_processor.add_keywords_from_list(sorted(countries))
keyword_processor.add_keywords_from_list(sorted(cities))


texts = ['new york to venice, italy for usd271',
'return flights from brussels to bangkok with etihad from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½407',
'from los angeles to guadalajara, mexico for usd191',
'fly to australia new zealand from paris from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½422 return including 2 checked bags']
keyword_processor.extract_keywords(texts[0])

[out]:
['york', 'venice', 'italy']

hey, what went wrong?!
doing due dil""new york"" is not in the data, 
>>> ""new york"" in cities
false

what the?! #$%^&* for sanity sake, we check these:
>>> len(countries)
244
>>> len(cities)
21940

yes, you cannot just trust a single data source, so lets try to fetch all data sources.
from  you find another link  lets munge this...
import requests
import json

cities_url = ""
cities1_json = json.loads(requests.get(cities_url).content.decode('utf8'))

countries1 = set([city['country'] for city in cities1_json])
cities1 = set([city['name'] for city in cities1_json])

dr5hn_cities_url = ""
dr5hn_countries_url = ""

cities2_json = json.loads(requests.get(dr5hn_cities_url).content.decode('utf8'))
countries2_json = json.loads(requests.get(dr5hn_countries_url).content.decode('utf8'))

countries2 = set([c['name'] for c in countries2_json])
cities2 = set([c['name'] for c in cities2_json])

countries = countries2.union(countries1)
cities = cities2.union(cities1)

and now that we are neurotic, we do sanity checks.
>>> len(countries)
282
>>> len(cities)
127793

wow, that's a lot more cities than previously. 
lets try the flashtext code again.
from flashtext import keywordprocessor

keyword_processor = keywordprocessor(case_sensitive=false)
keyword_processor.add_keywords_from_list(sorted(countries))
keyword_processor.add_keywords_from_list(sorted(cities))

texts = ['new york to venice, italy for usd271',
'return flights from brussels to bangkok with etihad from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½407',
'from los angeles to guadalajara, m91',
'fly to australia new zealand from paris from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½422 return including 2 checked bags']

keyword_processor.extract_keywords(texts[0])

[out]:
['york', 'venice', 'italy']

seriously?! there is no new york?! $%^&*
okay, for more sanity checks, lets ""york"" in the list of cities.
>>> [c for c in cities if 'york' in c.lower()]
['yorklyn',
 'west york',
 'west new york',
 'yorktown heights',
 'east riding of yorkshire',
 'yorke peninsula',
 'yorke hill',
 'yorktown',
 'jefferson valley-yorktown',
 'new york mills',
 'city of york',
 'yorkville',
 'yorkton',
 'new york county',
 'east york',
 'east new york',
 'york castle',
 'york county',
 'yorketown',
 'new york city',
 'york beach',
 'yorkshire',
 'north yorkshire',
 'yorkeys knob',
 'york',
 'york town',
 'york harbor',
 'north york']

eureka! it's because it's call ""new york city"" and not ""new york""!
you: what kind of prank is this?! 
linguist: welcome to the world of natural language processing, where natural language is a social construct subjective to communal and idiolectal variant. 
you: cut the crap, tell me how to solve this. 
nlp practitioner (a real one that works on noisy user-generate texts): you just have to add to the list. but before that, check your metric given the list you already have.
for every texts in your sample ""test set"", you should provide some truth labels to make sure you can ""measure your metric"".
from itertools import zip_longest
from flashtext import keywordprocessor

keyword_processor = keywordprocessor(case_sensitive=false)
keyword_processor.add_keywords_from_list(sorted(countries))
keyword_processor.add_keywords_from_list(sorted(cities))

texts_labels = [('new york to venice, italy for usd271', ('new york', 'venice', 'italy')),
('return flights from brussels to bangkok with etihad from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½407', ('brussels', 'bangkok')),
('from los angeles to guadalajara, mexico for usd191', ('los angeles', 'guadalajara')),
('fly to australia new zealand from paris from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½422 return including 2 checked bags', ('australia', 'new zealand', 'paris'))]

# no. of correctly extracted terms.
true_positives = 0
false_positives = 0
total_truth = 0

for text, label in texts_labels:
    extracted = keyword_processor.extract_keywords(text)

    # we're making some assumptions here that the order of 
    # extracted and the truth must be the same.
    true_positives += sumxtracted, label) if e == l)
    false_positives += sum(1 for e, l in zip_longest(extracted, label) if e != l)
    total_truth += len(label)

    # just visualization candies.
    print(text)
    print(extracted)
    print(label)
    print()

actually, it doesn't look that bad. we get an accuracy of 90%:
>>> true_positives / total_truth
0.9

but i %^&*(-ing want 100% extraction!!
alright, alright, so look at the ""only"" error that the above approach is making, it's simply that ""new york"" isn't in the list of cities. 
you: why don't we just add ""new york"" to the list of cities, i.e. 
keyword_processor.add_keyword('new york')

print(texts[0])
print(keyword_processor.extract_keywords(texts[0]))

[out]:
['new york', 'venice', 'italy']

you: see, i did it!!! now i deserve a beer.
linguist: how about 'i live in marawi'?
>>> keyword_processor.extract_keywords('i live in marawi')
[]

nlp practitioner (chiming in): how about 'i live in jeju'? 
>>> keyword_processor.extract_keywords('i live in jeju')
[]

a raymond hettinger fan (from farway): ""there must be a better way!""
yes, there is what if we just try something silly like adding keywords of cities that ends with ""city"" into our keyword_processor?
for c in cities:
    if 'city' in c.lower() and c.endswith('city') and c[:-5] not in cities:
        if c[:-5].strip():
            keyword_processor.add_keyword(c[:-5])
            print(c[:-5])

it works!
now lets retry our regression test examples:
from itertools import zip_longest
from flashtext import keywordprocessor

keyword_processor = keywordprocessor(case_sensitive=false)
keyword_processor.add_keywords_from_list(sorted(countries))
keyword_processor.add_keywords_from_list(sorted(cities))

for c in cities:
    if 'city' in c.lower() and c.endswith('city') and c[:-5] not in cities:
        if c[:-5].strip():
            keyword_processor.add_keyword(c[:-5])

texts_labels = [('new york to venice, italy for usd271', ('new york', 'venice', 'italy')),
('return flights from brussels to bangkok with etihad from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½407', ('brussels', 'bangkok')),
('from los angeles to guadalajara, mexico for  angeles', 'guadalajara')),
('fly to australia new zealand from paris from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½422 return including 2 checked bags', ('australia', 'new zealand', 'paris')),
('i live in florida', ('florida')), 
('i live in marawi', ('marawi')), 
('i live in jeju', ('jeju'))]

# no. of correctly extracted terms.
true_positives = 0
false_positives = 0
total_truth = 0

for text, label in texts_labels:
    extracted = keyword_processor.extract_keywords(text)

    # we're making some assumptions here that the order of 
    # extracted and the truth must be the same.
    true_positives += sum(1 for e, l in zip_longest(extracted, label) if e == l)
    false_positives += sum(1 for e, l in zip_longest(extracted, label) if e != l)
    total_truth += len(label)

    # just visualization candies.
    print(text)
    print(extracted)
    print(label)
    print()

[out]:
new york to venice, italy for usd271
['new york', 'venice', 'italy']
('new york', 'venice', 'italy')

return flights fromangkok with etihad from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½407
['brussels', 'bangkok']
('brussels', 'bangkok')

from los angeles to guadalajara, mexico for usd191
['los angeles', 'guadalajara', 'mexico']
('los angeles', 'guadalajara')

fly to australia new zealand from paris from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½422 return including 2 checked bags
['australia', 'new zealand', 'paris']
('australia', 'new zealand', 'paris')

i live in florida
['florida']
florida

i live in marawi
['marawi']
marawi

i live in jeju
['jeju']
jeju

100% yeah, nlp-bunga !!!
but seriously, this is only the tip of the problem. what happens if you have a sentence like this:
>>> keyword_processor.extract_keywords('adam flew to bangkok from singapore and then to china')
['adam', 'bangkok', 'singapore', 'china']

why is adam extracted as a city?!
then you do some more neurotic checks:
>>> 'adam' in cities
adam

congrao another nlp rabbit hole of polysemy where the same word has different meaning, in this case, adam most probably refer to a person in the sentence but it is also coincidentally the name of a city (according to the data you've pulled from).
i see what you did there... even if we ignore this polysemy nonsense, you are still not giving me the desired output:
[in]:
['new york to venice, italy for usd271',
'return flights from brussels to bangkok with etihad from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½407',
'from los angeles to guadalajara, mexico for usd191',
'fly to australia new zealand from paris from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½422 return including 2 checked bags'
]

[out]:
origin: new york, usa; destination: venice, italy
origin: brussels, bel; destination: bangkok, thailand
origin: los angeles, usa; destination: guadalajara, mexico
origin: paris, france; destination: australia / new zealand (this is a complicated case given two countries)
g>: even with the assumption that the preposition (e.g. from, to) preceding the city gives you the ""origin"" / ""destination"" tag, how are you going to handle the case of ""multi-leg"" flights, e.g. 
>>> keyword_processor.extract_keywords('adam flew to bangkok from singapore and then to china')

what's the desired output of this sentence:
> adam flew to bangkok from singapore and then to china

perhaps like this? what is the specification? how (un-)structured is your input text?
> origin: singapore
> departure: bangkok
> departure: china

lets try to build component two to detect prepositions.
lets take that assumption you have and try some hacks to the same flashtext methods. 
what if we add to and from to the list?
from itertools import zip_longest
from flashtext import keywordprocessor

keyword_processor = keywordprocessor(case_sensitive=false)
keyword_processor.add_keywords_from_list(sorted(countries))
keyword_processor.add_keywords_from_list(sorted(cities))

for c in cities:
    if 'city' in c.lower() and c.endswith('city') and c[:-5] not in cities:
        if c[:-5].strip():
            keyword_processor.add_keyword(c[:-5])

keyword_processor.add_keyword('to')
keyword_processor.add_keyword('from')

texts = ['new york to venice, italy for usd271',
'return flights from brussels to bangkok with etihad from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½407',
'from los angeles to guadalajara, mexico for usd191',
'fly to australia new zealand from paris from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½422 return including 2 checked bags']


for text in texts:
    extracted = keyword_processor.extract_keywords(text)
    print(text)
    print(extracted)
    print()

[out]:
new york to venice, italy for usd271
['new york', 'to', 'venice', 'italy']

return flights from brussels to bangkok with etihad from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½407
['from', 'brussels', 'to', 'bangkok', 'from']

from los angeles to guadalajara, mexico for usd191
['from', 'los angeles', 'to', 'guadalajara', 'mexico']

fly to australia new zealuding 2 checked bags
['to', 'australia', 'new zealand', 'from', 'paris', 'from']

heh, that's pretty crappy rule to use to/from,

what if the ""from"" is referring the price of the ticket?
what if there's no ""to/from"" preceding the country/city? 

okay, lets work with the above output and see what we do about the problem 1. maybe check if the term after the from is city, if not, remove the to/from?
from itertools import zip_longest
from flashtext import keywordprocessor

keyword_processor = keywordprocessor(case_sensitive=false)
keyword_processor.add_keywords_from_list(sorted(countries))
keyword_processor.add_keywords_from_list(sorted(cities))

for c in cities:
    if 'city' in c.lower() and c.endswith('city') and c[:-5] not in cities:
        if c[:-5].strip():
            keyword_processor.add_keyword(c[:-5])

keyword_processor.add_keyword('to')
keyword_processor.add_keyword('from')

texts = ['new york to venice, italy for usd271',
'return flights from brussels to bangkok with etihad from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½407',
'from los angeles to guadalajara, mexico for usd191',
'fly to australia new zealand from paris from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½422 return including 2 checked bags']


cted = keyword_processor.extract_keywords(text)
    print(text)

    new_extracted = []
    extracted_next = extracted[1:]
    for e_i, e_iplus1 in zip_longest(extracted, extracted_next):
        if e_i == 'from' and e_iplus1 not in cities and e_iplus1 not in countries:
            print(e_i, e_iplus1)
            continue
        elif e_i == 'from' and e_iplus1 == none: # last word in the list.
            continue
        else:
            new_extracted.append(e_i)

    print(new_extracted)
    print()

that seems to do the trick and remove the from that doesn't precede a city/country. 
[out]:
new york to venice, italy for usd271
['new york', 'to', 'venice', 'italy']

return flights from brussels to bangkok with etihad from ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½407
from none
['from', 'brussels', 'to', 'bangkok']

from los angeles to guadalajara, mexico for usd191
['from', 'los angeles', 'to', 'guadalajara', 'mexico']

fly to australia new zealand from paris from ï¿½ï¿½

uding 2 checked bags
from none
['to', 'australia', 'new zealand', 'from', 'paris']

but the ""from new york"" still isn't solve!!
linguist: think carefully, should ambiguity be resolved by making an informed decision to make ambiguous phrase obvious? if so, what is the ""information"" in the informed decision? should it follow a certain template first to detect the information before filling in the ambiguity?
you: i'm losing my patience with you... you're bringing me in circles and circles, where's that ai that can understand human language that i keep hearing from the news and google and facebook and all?!
you: what you gave me are rule based and where's the ai in all these?
nlp practitioner: didn't you wanted 100%? writing ""business logics"" or rule-based systems would be the only way to really achieve that ""100%"" given a specific data set without any preset data set that one can use for ""training an ai"".
you: what do you mean by training an ai? why can't i just use google or facebook or amazon or microsoft or even ibm's ai? 
nlp practitioner: let me introduce you to 





welcome to the world of computational linguistics and nlp!
in short
yes, there's no real ready-made magical solution and if you want to use an ""ai"" or machine learning algorithm, most probably you would need a lot more training data like the texts_labels pairs shown in the above example.",https://stackoverflow.com/questions/59956670,python,28-01-2020 20:39,10018.0,33.0,2.0,True,03-04-2024 10:26,28-01-2020 20:46
46786211,counting the frequency of words in a pandas data frame,"i have a table like below:
      urn                   firm_name
0  104472               r.x. yah & co
1  104873        big building society
2  109986          st james's society
3  114058  the kensington society ltd
4  113438      mmv oil associates ltd

and i want to count the frequency of all the words within the firm_name column, to get an output like below:

i have tried the following code:
import pandas as pd
import nltk
data = pd.read_csv(""x:\firm_data.csv"")
top_n = 20
word_dist = nltk.freqdist(data['firm_name'])
print('all frequencies')
print('='*60)
rslt=pd.dataframe(word_dist.most_common(top_n),columns=['word','frequency'])

print(rslt)
print ('='*60)

however the following code does not produce a unique word count.","['python', 'pandas', 'nltk']",46786277,"iiuic, use value_counts()
in [3361]: df.firm_name.str.split(expand=true).stack().value_counts()
out[3361]:
society       3
ltd           2
james's       1
r.x.          1
yah           1
associates    1
st            1
kensington    1
mmv           1
big           1
&             1
the           1
co            1
oil           1
building      1
dtype: int64


or,
pd.series(np.concatenate([x.split() for x in df.firm_name])).value_counts()


or,
pd.series(' '.join(df.firm_name).split()).value_counts()


for top n, for example 3
in [3379]: pd.series(' '.join(df.firm_name).split()).value_counts()[:3]
out[3379]:
society    3
ltd        2
james's    1
dtype: int64


details
in [3380]: df
out[3380]:
      urn                   firm_name
0  104472               r.x. yah & co
1  104873        big building society
2  109986          st james's society
3  114058  the kensington society ltd
4  113438      mmv oil associates ltd",https://stackoverflow.com/questions/46786211,python,17-10-2017 08:54,75575.0,54.0,5.0,True,03-11-2024 20:12,17-10-2017 09:01
76879007,install the correct onnxruntime for chromadb with pip install,"i am trying to install chromadb on my jupyter notebook (anaconda) using:
pip install chromadb

i get error:
error: could not find a version that satisfies the requirement onnxruntime>=1.14.1 (from chromadb) (from versions: 1.2.0, 1.3.0, 1.4.0, 1.5.1, 1.5.2, 1.6.0, 1.7.0, 1.8.0, 1.8.1, 1.9.0, 1.10.0, 1.11.0, 1.11.1)
error: no matching distribution found for onnxruntime>=1.14.1 (from chromadb)

ok, so i run:
pip install onnxruntime

and it installed onnxruntime 1.11.1
but chromadb requires >1=1.14.1
i am assuming that the highest onnxruntime compatible with my os (mac) is 1.11.1.
is there a way around it?","['python', 'pip', 'openai-api', 'vector-database', 'chromadb']",76879118,"onnxruntime 1.11.1 supports python 3.6 (see the middle of the left column). later versions don't support 3.6. i guess you use python 3.6. to install a later version of onxruntime upgrade python. all versions up to the current 1.15.1 requires at least 3.7; 1.15.0 and 1.15.1 don't provide wheels for python 3.7, only for 3.8+.",https://stackoverflow.com/questions/76879007,python,10-08-2023 20:08,5762.0,1.0,3.0,True,31-08-2024 06:46,31-08-2024 06:46
72289714,how to validate hugging face organization token?,"/whoami-2 endpoint returns unauthorized for organization tokens, the ones that start with api_....
$ curl  -h ""authorization: bearer api_<token>""
> { ""error"": ""unauthorized"" }

at the same time i can use the same token to get private models. should i use some other endpoint to validate the tokens?","['curl', 'huggingface-transformers', 'huggingface']",72297034,"you are requesting to the wrong endpoint. it seems the endpoint is updated and i got a similar error with sending requests to the older endpoint (whoami).
just send the request to whoami-v2 like:
$ curl  -h ""authorization: bearer ${token}""
> {""type"": """",""name"":""sadra"",""fullname"":""sadra"",""email"":"""",""emailverified"":true,""plan"":"""",""periodend"":,""avatarurl"":"""",""orgs"":[]}

nb: according to docs, it seems old tokens were api_xxx or api_org_xxx, while all new ones start with hf_xxx. so, maybe creating a new token might be helpful if you still face issue with new endpoint.
so, same thing happens for organization tokens:
$ curl  -h ""authorization: bearer api_org_xxx""
> {""type"":""org"",""name"":""testmy"",""fullname"":""testorg"",""email"":null,""plan"":""no_plan"",""periodend"":null,""avatarurl"":""",https://stackoverflow.com/questions/72289714,curl,18-05-2022 13:00,1253.0,1.0,1.0,True,19-05-2022 15:23,19-05-2022 08:27
71339209,nltk: adding negative words for sentiment analysis,"i am working on sentiment analysis using nltk and spacy. while working, i need to add new words into the negative variables so that it will show negative polarity value when those words appears in any sentence. i don't know how to do that, could someone help me please?","['machine-learning', 'nlp', 'artificial-intelligence', 'nltk']",71344239,"how are you doing the sentiment analysis so far? it would help to see samples to know what exactly you are trying to do. if you are using some kind of trained model that gives you a sentiment value or sentiment class then it definitely isn't as simple as just telling the model to see those words as negative, you would have to re-train/fine-tune the model.
of course you could mix the results of the model with your own post-editing of the results by checking if there are certain words in the text and if so rate it even lower than the model rating. in general i am pretty sure that a trained model yields a better performance than anything rule-based you could build yourself. depending if you have available data the best performance would probably be to fine-tune a pretrained model, but for this nltk and spacy aren't the best/most user friendly.
edit: some ways to run toxicity analysis
models trained to detect toxicity
the most powerful and state-of-the-art way to do this analysis would probably be to used pretrained transformer models which were fine-tuned on the probably best annotated available dataset for this topic which is the one released for the jigsaw toxicity detection challenges.
in python you can find some models for this on huggingface, e.g.:


there you also have an api to see how it works and what the model can detect.
purely rule-based
since you have a list of slurs you are probably expected to use more of a rule-based approach. a basic approach for assigning a toxicity value to a sentence would be: split the tweet into sentences using nltk's sent_tokenize(). then split each sentence into words using word_tokenize(). set all words to lowercase. count how many toxic words are in the sentence. the number of toxic word occurences is the profanity score of that sentence.
mix rule-based and sentiment analysis
since your approach so far seems to be to use a sentiment analysis module, you could try to mix the sentiment score you get from nltks sentiment analysis module/vader module with a rule based approach that counts the number of words from the list.
you should realize that sentiment analysis is not the same as profanity or toxicity detection though. if you give something like ""i am extremely sad"" to nltks sentiment analysis it will return a very negative score even though the sentence has no profanity or toxicity. on the other hand, if you give something like ""i am so fucking happy"" to the sentiment analysis it will at least detect that this is not too negative, which is a benefit compared to a purely rule based approach which would mark this as profanity/toxicity. so it makes sense to combine the approaches, but doesnt make much sense to just insert the list you have into the sentiment analysis.
what you could do for example is weight each score as 50% of the overall score. first you calculate the sentiment score and then you apply your own rule-based score as described before onto that score to make it lower if any of the slurs occur.",https://stackoverflow.com/questions/71339209,machine-learning,03-03-2022 14:55,980.0,-1.0,2.0,True,16-10-2022 14:33,16-10-2022 14:33
72077504,predict numeric variable from a text variable using word embeddings in r,"i have a text variable with reviews of movies and another variables with ratings ï¿½ï¿½ï¿½ i want to try to use the text reviews to predict the ratings.
here are some example data:
movie_reviews <- c(""i really loved the movie plot"", ""this movie really sucked"", ""i really found this movie thought provoking"", ""ahh what a boring movie"", ""a wonderful movie, with a wonderful end"", ""great action movie: very thrilling"", ""worst movie ever, it never stopped being cheesy"", ""enjoying, feelgood movie for the entire family"", ""i will definitely watch this movie again"")

movie_ratings <- c(8, 2, 6, 3, 9, 8.5, 3.5, 9.5, 7.5)  
  
movie_df <- tibble(movie_reviews, movie_ratings) 


thank you","['r', 'nlp', 'word-embedding', 'r-text']",72077530,"for this you can use the text-package
# create word embedding representations of your text
help(textembed)
reviews_embeddings <- textembed(movie_df, 
                                model = ""bert-base-uncased"", # select model you want from huggingface
                                layers = 11:12) # select which layers you want to use

# train the word embeddings to the numeric variable using ridge regression 
reviews_rating_model <- texttrain(reviews_embeddings$movie_reviews, 
                                  movie_df$movie_ratings) 
# see the results
reviews_rating_model

result
$results

    pearson's product-moment correlation

data:  predy_y$predictions and predy_y$y
t = 5.621, df = 7, p-value = 0.0003991
alternative hypothesis: true correlation is greater than 0
95 percent confidence interval:
 0.6785761 1.0000000
sample estimates:
      cor 
0.9047823",https://stackoverflow.com/questions/72077504,r,01-05-2022 14:18,198.0,1.0,1.0,True,02-05-2022 15:15,02-05-2022 15:15
77946203,trouble querying redis vector store when using huggingfaceembeddings in langchain,"i'm trying to create an rag (retrieval-augmented generation) system using langchain and redis vector store. the langchain documentation provides an example of how to store and query data from redis, which is shown below:
from langchain.embeddings.openai import openaiembeddings
from langchain.embeddings import huggingfaceembeddings
from langchain_community.vectorstores.redis import redis
import os

embeddings = openaiembeddings()

metadata = [
    {
        ""user"": ""john"",
        ""age"": 18,
        ""job"": ""engineer"",
        ""credit_score"": ""high"",
    },
    {
        ""user"": ""derrick"",
        ""age"": 45,
        ""job"": ""doctor"",
        ""credit_score"": ""low"",
    },
    {
        ""user"": ""nancy"",
        ""age"": 94,
        ""job"": ""doctor"",
        ""credit_score"": ""high"",
    },
    {
        ""user"": ""tyler"",
        ""age"": 100,
        ""job"": ""engineer"",
        ""credit_score"": ""high"",
    },
    {
        ""user"": ""joe"",
        ""age"": 35,
        ""job"": ""dentist"",
        ""credit_score"": ""medium"",
    },
]
texts = [""foo"", ""foo"", ""foo"", ""bar"", ""bar""]

rds = redis.from_texts(
    texts,
    embeddings,
    metadatas=metadata,
    redis_url=""redis://localhost:6379"",
    index_name=""users"",
)

results = rds.similarity_search(""foo"")

this example uses openaiembeddings, and it works perfectly. however, when i opt for a different embedding method, such as
embeddings = huggingfaceembeddings(
    model_name=""sentence-transformers/paraphrase-multilingual-minilm-l12-v2"",
            model_kwargs={'device': 'cpu'}
        ) 

i encounter an issue when trying to query the redis vector store. i receive the following error for rds.similarity_search(""foo""):
responseerror: error parsing vector similarity query: query vector blob size (1536) does not match index's expected size (6144).

i'm not sure why this error is occurring and how to fix it. has anyone else faced a similar problem or can provide guidance on resolving this issue? your help would be greatly appreciated!","['redis', 'huggingface-transformers', 'langchain', 'py-langchain']",77951377,"essentially the issue is that you are trying to reuse the same index despite using two different embedding models.
i ran your code and if i change the embedding model and then provide a different name for the index, thus presumably creating a new one instead of reusing an existing one, everything works as expected.
if for some reason you want to use the same index with different embedding models, i think you would need to be more specific about the vector schemas and/or modify the embedding vectors prior to saving them in the database. i did not try this myself though, so it is just speculation.
note: when i was running the code i received a warning to use the embeddings implementation of langchain_community instead of the langchain one, as the latter seems to be deprecated. perhaps doing this you would also receive other, potentially more meaningful, errors.",https://stackoverflow.com/questions/77946203,redis,06-02-2024 08:24,1006.0,0.0,1.0,True,06-02-2024 23:15,06-02-2024 09:33
61787119,fasttext 0.9.2 - why is recall &#39;nan&#39;?,"i trained a supervised model in fasttext using the python interface and i'm getting weird results for precision and recall.
first, i trained a model:
model = fasttext.train_supervised(""train.txt"", wordngrams=3, epoch=100, pretrainedvectors=pretrained_model)

then i get results for the test data:
def print_results(n, p, r):
    print(""n\t"" + str(n))
    print(""p@{}\t{:.3f}"".format(1, p))
    print(""r@{}\t{:.3f}"".format(1, r))

print_results(*model.test('test.txt'))

but the results are always odd, because they show precision and recall @1 as identical, even for different datasets, e.g. one output is:
n   46425
p@1 0.917
r@1 0.917

then when i look for the precision and recall for each label, i always get recall as 'nan':
print(model.test_label('test.txt'))

and the output is:
{'__label__1': {'precision': 0.9202150724134941, 'recall': nan, 'f1score': 1.8404301448269882}, '__label__5': {'precision': 0.9134956983264135, 'recall': nan, 'f1score': 1.826991396652827}}

does anyone know why this might be happening?
p.s.: to try a reproducible example of this behavior, please refer to  and run it with fasttext 0.9.2","['python-3.x', 'nlp', 'text-classification', 'precision-recall', 'fasttext']",61923324,"it looks like fasttext 0.9.2 has a bug in the computation of recall, and that should be fixed with this commit.
installing a ""bleeding edge"" version of fasttext e.g. with
pip install git+ --quiet

and rerunning your code should allow to get rid of the nan values in the recall computation.",https://stackoverflow.com/questions/61787119,python-3.x,14-05-2020 00:21,2131.0,5.0,1.0,True,16-04-2022 07:22,16-04-2022 07:22
55871850,mosestokenizer issue: [winerror 2] the system cannot find the file specified,"can't figure out why is this problem appearing.
from mosestokenizer import mosesdetokenizer

with mosesdetokenizer('en') as detokenize:
    print(detokenize([""hi"", 'my', 'name', 'is', 'artem']))

this is what i get:
stdbuf was not found; communication with perl may hang due to stdio buffering.
traceback (most recent call last):
  file ""c:\users\artemlaptiev\documents\github\temp\foo.py"", line 3, in <module>
    with mosesdetokenizer('en') as detokenize:
  file ""c:\programfiles\anaconda\lib\site-packages\mosestokenizer\detokenizer.py"", line 47, in __init__
    super().__init__(argv)
  file ""c:\programfiles\anaconda\lib\site-packages\toolwrapper.py"", line 52, in __init__
    self.start()
  file ""c:\programfiles\anaconda\lib\site-packages\toolwrapper.py"", line 92, in start
    cwd=self.cwd
  file ""c:\programfiles\anaconda\lib\subprocess.py"", line 709, in __init__
    restore_signals, start_new_session)
  file ""c:\programfiles\anaconda\lib\subprocess.py"", line 997, in _execute_child
    startupinfo)
filenotfounderror: [winerror 2] the system cannot find the file specified

thank you for help!","['python', 'nlp', 'anaconda', 'nltk', 'tokenize']",57626295,"use sacremoses instead of moses.
pip install -u sacremoses

and
from sacremoses import mosestokenizer, mosesdetokenizer
with mosesdetokenizer() as detokenize:
    print(detokenize([""hi"", 'my', 'name', 'is', 'artem']))

for complete details sacremoses",https://stackoverflow.com/questions/55871850,python,26-04-2019 16:43,1449.0,3.0,1.0,True,11-04-2024 10:15,26-04-2019 22:10
76603417,typescript langchain add field to document metadata,"how should i add a field to the metadata of langchain's documents?
for example, using the charactertextsplitter gives a list of documents:
const splitter = new charactertextsplitter({
  separator: "" "",
  chunksize: 7,
  chunkoverlap: 3,
});
splitter.createdocuments([text]);

a document will have the following structure:
{
  ""pagecontent"": ""blablabla"",
  ""metadata"": {
    ""name"": ""my-file.pdf"",
    ""type"": ""application/pdf"",
    ""size"": 12012,
    ""lastmodified"": 1688375715518,
    ""loc"": { ""lines"": { ""from"": 1, ""to"": 3 } }
  }
}

and i want to add a field to the metadata","['typescript', 'langchain']",76646169,"it isn't currently shown how to do this in the recommended text splitter documentation, but the 2nd argument of createdocuments can take an array of objects whose properties will be assigned into the metadata of every element of the returned documents array.
mymetadata = { url: "" }
const documents = await splitter.createdocuments([text], [mymetadata],
  { chunkheader, appendchunkoverlapheader: true });

after this, documents will contain an array, with each element being an object with pagecontent and metadata properties. under metadata, the properties from mymetadata above will also appear. pagecontent will also have the text of chunkheader prepended.
{
  pagecontent: <chunkheader plus the chunk>,
  metadata: <all properties of mymetadata plus loc (text line numbers of chunk)>
}",https://stackoverflow.com/questions/76603417,typescript,03-07-2023 09:19,4630.0,1.0,3.0,True,31-08-2023 08:20,03-07-2023 09:26
76313499,getting entities from pre-saved docbin,"i have around 700k documents that i want to process in spacy and save into a docbin for later use.
i wrote a code to do a keywords search using phrasematcher and it worked great. i'm now trying to build a knowledge graph out of the docbin i have and i can't seem to be able to access the entities to use them in the graph logic. i read somewhere that docbins don't keep that information (?) but when i print docbin.tokens i get some values and not just an empty output.
this might be a very stupid question but i'm quite lost and the documentation does not seem to be detailed enough for this.
import spacy
from spacy.tokens import docbin
from spacy.vocab import vocab

nlp = spacy.load('fr_dep_news_trf')
docbinpath = r'c:\[redacted]\frdocbin.nlp'

loadeddocbin = docbin(vocab()).from_disk(docbinpath)  

doclist=list(loadeddocbin.get_docs(nlp.vocab))
for doc in doclist
    people = list(set([ent.text for ent in doc.ents if ent.label_=='person']))    
  



this doesn't produce any errors but doc.ents is empty.
this is the code for saving the docbin:
frdoc_bin = docbin (store_user_data=true,attrs=['ent_type','lemma','like_email','like_url','like_num','orth','pos','head','dep'])
doc = frnlp(text)
frdoc_bin.add(doc)
frdoc_bin.to_disk(createdmodelpath+r'\frdocbin'+'.nlp')","['python', 'nlp', 'spacy']",76332921,"edit: i figured out the issue from the spacy discussion, it's quite simply that the fr model i was using doesn't support ner. switched fr_core_news_lg and it worked :)",https://stackoverflow.com/questions/76313499,python,23-05-2023 09:58,304.0,0.0,2.0,True,25-05-2023 13:37,23-05-2023 10:13
40240929,nlp with racket,"i am studying nlp with racket and dr. racket.
i am working with this code:
#lang racket

(define english-1
  '((initial (1))
    (final (9))
    (from 1 to 3 by np)
    (from 1 to 2 by det)
    (from 2 to 3 by n)
    (from 3 to 4 by bv)
    (from 4 to 5 by adv)
    (from 4 to 5 by |#|)
    (from 5 to 6 by det)
    (from 5 to 7 by det)
    (from 5 to 8 by |#|)
    (from 6 to 7 by adj)    
    (from 6 to 6 by mod)
    (from 7 to 9 by n)
    (from 8 to 8 by mod)
    (from 8 to 9 by adj)
    (from 9 to 4 by cnj)
    (from 9 to 1 by cnj)))

(define (getf x y)
  (if (eq? (car x) y)
      (cadr x)
      (getf (cdr x) y)))

(define (initial-nodes network)
  (list-ref (assoc 'initial network) 1))

(define (final-nodes network)
  (list-ref  (assoc 'final network) 1))

(define (transitions network)
  (filter (lambda (x) (eq? (car x) 'from)) network))

(define (trans-node transition)
  (getf transition 'from))

(define(trans-newnode transition)
  (getf transition 'to))

(define (trans-label transition)
  (getf transition 'by))

(define abbreviations
  '((np kim sandy lee)
    (det a the her)
    (n consumer man woman)
    (bv is was)
    (cnj and or)
    (adj happy stupid)
    (mod very)
  (adv often always sometimes)))

(define (recognize network tape)
  ;; returns t if sucessfully recognizes tape - nil otherwise
  (call/cc (lambda (return)
             (define (recognize-next node tape network)
               (if (and (null? tape) (member node (final-nodes network)))
                   (return #t) ; success
                   (for ([transition (transitions network)])
                           ;; try each transition of the network
                           (when (equal? node (trans-node transition)) ; if it starts at the right node
                               (for ([newtape (recognize-move (trans-label transition) tape)])
                                       ;; try each possible new value of tape
                                 (recognize-next (trans-newnode transition) newtape network))))))
             (for ([initialnode (initial-nodes network)])
               (recognize-next initialnode tape network))
             null))) ; failed to recognize

(define (recognize-move label tape)
  (if (or (eq? label (car tape))
          (member (car tape) (assoc label abbreviations)))
      (list (cdr tape))
      (if (eq? label '|#|)
          (list tape)
          null)))

(require racket/trace)
(trace recognize-move)
(recognize-move english-1 '(hahaha))

the code seems to be mostly fine. however, i keep getting a error messaging related to the recognize-move function:
member: not a proper list: #f

and i thought i was dealing with lists... how can i solve this?","['nlp', 'lisp', 'racket']",40241434,"the problem is with this form:
(member (car tape) (assoc label abbreviations))

if assoc does not find anything the result is #f. (member 'anything #f) will not work. in common lisp false is the same as an empty list so member on false will work, but not in scheme. you can perhaps make sure it's a list like this:
(member (car tape) (or (assoc label abbreviations) '()))",https://stackoverflow.com/questions/40240929,nlp,25-10-2016 13:10,560.0,1.0,2.0,True,24-03-2025 10:47,25-10-2016 14:57
75617865,openai chat completions api error: &quot;invalidrequesterror: unrecognized request argument supplied: messages&quot;,"i am currently trying to use openai's most recent model: gpt-3.5-turbo. i am following a very basic tutorial.
i am working from a google collab notebook. i have to make a request for each prompt in a list of prompts, which for sake of simplicity looks like this:
prompts = ['what are your functionalities?', 'what is the best name for an ice-cream shop?', 'who won the premier league last year?']

i defined a function to do so:
import openai

# load your api key from an environment variable or secret management service
openai.api_key = 'my_api'

def get_response(prompts: list, model = ""gpt-3.5-turbo""):
  responses = []

  
  restart_sequence = ""\n""

  for item in prompts:

      response = openai.completion.create(
      model=model,
      messages=[{""role"": ""user"", ""content"": prompt}],
      temperature=0,
      max_tokens=20,
      top_p=1,
      frequency_penalty=0,
      presence_penalty=0
    )

      responses.append(response['choices'][0]['message']['content'])

  return responses

however, when i call responses = get_response(prompts=prompts[0:3]) i get the following error:
invalidrequesterror: unrecognized request argument supplied: messages

any suggestions?
replacing the messages argument with prompt leads to the following error:
invalidrequesterror: [{'role': 'user', 'content': 'what are your functionalities?'}] is valid under each of {'type': 'array', 'minitems': 1, 'items': {'oneof': [{'type': 'integer'}, {'type': 'object', 'properties': {'buffer': {'type': 'string', 'description': 'a serialized numpy buffer'}, 'shape': {'type': 'array', 'items': {'type': 'integer'}, 'description': 'array shape'}, 'dtype': {'type': 'string', 'description': 'stringified dtype'}, 'token': {'type': 'string'}}}]}, 'example': '[1, 1313, 451, {""buffer"": ""abcdefgh"", ""shape"": [1024], ""dtype"": ""float16""}]'}, {'type': 'array', 'minitems': 1, 'maxitems': 2048, 'items': {'oneof': [{'type': 'string'}, {'type': 'object', 'properties': {'buffer': {'type': 'string', 'description': 'a serialized numpy buffer'}, 'shape': {'type': 'array', 'items': {'type': 'integer'}, 'description': 'array shape'}, 'dtype': {'type': 'string', 'description': 'stringified dtype'}, 'token': {'type': 'string'}}}], 'default': '', 'example': 'this is a test.', 'nullable': false}} - 'prompt'","['python', 'openai-api', 'chatgpt-api']",75619702,"problem
you used the wrong method to get a completion. when using the openai sdk, whether you use python or node.js, you need to use the right method.
which method is the right one? it depends on the openai model you want to use.
solution
the tables below will help you figure out which method is the right one for a given openai model.
step 1: find in the table below which api endpoint is compatible with the openai model you want to use.



api endpoint
model group
model name




/v1/chat/completions
ï¿½ï¿½ï¿½ gpt-4  ï¿½ï¿½ï¿½ gpt-3.5
ï¿½ï¿½ï¿½ gpt-4 and dated model releases  ï¿½ï¿½ï¿½ gpt-4-32k and dated model releases  ï¿½ï¿½ï¿½ gpt-4-1106-preview  ï¿½ï¿½ï¿½ gpt-4-vision-preview ï¿½ï¿½ï¿½ gpt-3.5-turbo and dated model releases  ï¿½ï¿½ï¿½ gpt-3.5-turbo-16k and dated model releases  ï¿½ï¿½ï¿½ fine-tuned versions of gpt-3.5-turbo


/v1/completions (legacy)
ï¿½ï¿½ï¿½ gpt-3.5  ï¿½ï¿½ï¿½ gpt base
ï¿½ï¿½ï¿½ gpt-3.5-turbo-instruct  ï¿½ï¿½ï¿½ babbage-002  ï¿½ï¿½ï¿½ davinci-002


/v1/assistants

all models except gpt-3.5-turbo-0301 supported.  retrieval tool requires gpt-4-1106-preview or gpt-3/td>
ï¿½ï¿½ï¿½ whisper-1


/v1/audio/translations
whisper
ï¿½ï¿½ï¿½ whisper-1


/v1/audio/speech
tts
ï¿½ï¿½ï¿½ tts-1  ï¿½ï¿½ï¿½ tts-1-hd


/v1/fine_tuning/jobs
ï¿½ï¿½ï¿½ gpt-3.5  ï¿½ï¿½ï¿½ gpt base
ï¿½ï¿½ï¿½ gpt-3.5-turbo  ï¿½ï¿½ï¿½ babbage-002  ï¿½ï¿½ï¿½ davinci-002


/v1/embeddings
embeddings
ï¿½ï¿½ï¿½ text-embedding-ada-002


/v1/moderations
moderations
ï¿½ï¿½ï¿½ text-moderation-stable  ï¿½ï¿½ï¿½ text-moderation-latest




step 2: find in the table below which method you need to use for the api endpoint you selected in the table above.
note: pay attention, because you have to use the meth""s-table-container"">


api endpoint
method for the python sdk v0.28.1
method for the python sdk >=v1.0.0
method for the node.js sdk v3.3.0
method for the node.js sdk >=v4.0.0




/v1/chat/completions
openai.chatcompletion.create
openai.chat.completions.create
openai.createchatcompletion
openai.chat.completions.create


/v1/completions (legacy)
openai.completion.create
openai.completions.create
openai.createcompletion
openai.completions.create


/v1/assistants
/
openai.beta.assistants.create
/
openai.beta.assistants.create


/v1/audio/transcriptions
openai.audio.transcribe
openai.audio.transcriptions.create
openai.createtranscription
openai.audio.transcriptions.create


/v1/audio/translations
openai.audio.translate
openai.audio.translations.create
openai.createtranslation
openai.audio.translations.create


/v1/audio/speech
/
openai.audio.speech.create
/
openai.audio.speech.create


/v1/fine_tuning/jobs
/
openai.fine_tuning.jobs.create
/
openai.finetuning.jobs.create


/v1/embeddings
openai.embedding.create
openai.embeddings.create
openai.createembedding
openai.embeddings.create


/v1/moderations
openai.moderation.create
openai.moderations.create
openai.createmoderation
openai.moderations.create



python sdk v1.0.0 working example for the gpt-3.5-turbo model
if you run test.py, the openai api will return the following completion:

hello! how can i assist you today?

test.py
import os
from openai import openai

client = openai(
    api_key = os.getenv(""openai_api_key""),
)

completion = client.chat.completions.create(
  model = ""gpt-3.5-turbo"",
  messages = [
    {""role"": ""system"", ""content"": ""you are a helpful assistant.""},
    {""role"": ""user"", ""content"": ""hello!""},
  ]
)

print(completion.choices[0].message.content.strip())


node.js sdk v4.0.0 working example for the gpt-3.5-turbo model
if you run test.js, the openai api will return the following completion:

hello! how can i assist you today?

test.js
const openai = require(""openai"");

const openai = new openai({
  apikey: process.env.openai_api_key,
});

async function main() {
  const completion = await openai.chat.completions.create({
    model: ""gpt-3.5-turbo"",
    messages: [
      { role: ""system"", content: ""you are a helpful assistant."" },
      { role: ""user"", content: ""hello!"" },
    ],
  });

  console.log(completion.choices[0].message.content.trim());
}

main();",https://stackoverflow.com/questions/75617865,python,02-03-2023 15:55,84852.0,38.0,4.0,True,12-06-2024 17:23,12-06-2024 17:23
70325022,list() constructor with list?,"i'm new in python and i don't understand the purpose of list() function in this piece of code:
documents = [(list(movie_reviews.words(fileid)), category)
              for category in movie_reviews.categories()
              for fileid in movie_reviews.fileids(category)]

the method words() is already returning a list of tokenized words from a string, and i don't see any difference between that and
documents = [(movie_reviews.words(fileid), category)
              for category in movie_reviews.categories()
              for fileid in movie_reviews.fileids(category)]","['python', 'nltk']",70325451,"there are three possibilities:

it is a mistake, no call to list() is required.

the interface only guarantees that the method returns an iterable type, which may be any one of: list, set, iterator, generator, etc. the specific movie_reviews.words() may return a list today, but either that may change in future versions, or in other classes with similar interfaces (child/parent/or simply similar interface).
whether this is the case, should be stated explicitly in the documentation, or could be gleamed out of the inheritance hierarchy.

the method performs some sort of memoization, while keeping a copy of the returned list. a good practice would be to copy the cached-list inside the method, but maybe they returned a shared list-object.
if the method returns a reference to a shared list object, then it is a good idea to call list(), in order to create a new list object. without the copy operation, any change to the list by one side (inside the method vs. through documents variable) will confuse the other side. if you change the list through documents varaible, then calling movie_reviews.words(fileid) with the same fileid may return the wrong value.
in general, although this is bad design, this happens in real code. i once had to debug such an issue in live code. usually, in case of memoization, it is better to return an immutable type such as a tuple, instead of a list, which will guarantee both speed and safety.",https://stackoverflow.com/questions/70325022,python,12-12-2021 15:38,109.0,0.0,1.0,True,12-12-2021 16:43,12-12-2021 16:43
79221167,blip2 type mismatch exception,"i'm trying to create an image captioning model using hugging face blip2 model on colab. my code was working fine till last week (nov 8) but it gives me an exception now.
to install packages i use the following command:
!pip install -q git+ transformers bitsandbytes datasets

to load blip2 processor and model i use the following code:
model_name = ""salesforce/blip2-opt-2.7b""
processor = autoprocessor.from_pretrained(model_name)
model = blip2forconditionalgeneration.from_pretrained(model_name,device_map=""auto"",load_in_8bit=false)

i use the following code to generate captions:
def generate_caption(processor, model, image_path):
  image = pilimage.open(image_path).convert(""rgb"")
  print(""image shape:"" + image.size)

  device = ""cuda"" if torch.cuda.is_available() else ""cpu""

  # preprocess the image
  inputs = processor(images=image, return_tensors=""pt"").to(device)

  print(""input shape:"", inputs['pixel_values'].shape)

  print(""device:"", device) # additional debugging

  for key, value in inputs.items():
    print(f""key: {key}, shape: {value.shape}"")

  # generate caption
  with torch.no_grad():
      generated_ids = model.generate(**inputs)
      caption = processor.decode(generated_ids[0], skip_special_tokens=true)

  return caption

here is the code that uses this method to generate captions:
  image_path = ""my_image_path.jpg""
  caption = generate_caption(processor, model, image_path)
  print(f""{image_path}: {caption}""

finally, this is the outputs and errors of running the code above:
image shape:  (320, 240)
input shape: torch.size([1, 3, 224, 224])
device: cuda
key: pixel_values, shape: torch.size([1, 3, 224, 224])
---------------------------------------------------------------------------   
.
.
.
/usr/local/lib/python3.10/dist-packages/transformers/models/blip_2/modeling_blip_2.py in generate(self, pixel_values, input_ids, attention_mask, interpolate_pos_encoding, **generate_kwargs)
   2314         if getattr(self.config, ""image_token_index"", none) is not none:
   2315             special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)
-> 2316             inputs_embeds[special_image_mask] = language_model_inputs.flatten()
   2317         else:
   2318             logger.warning_once(

runtimeerror: shape mismatch: value tensor of shape [81920] cannot be broadcast to indexing result of shape [0]

i have searched the internet and used various ai models for help but to no avail. my guess is that this is a package update problem since my code had no problem last week. (i tried to restore my code to nov 8 version but it throws an exception.) moreover, i don't understand how 81920 is calculated in the error message.","['python', 'artificial-intelligence', 'huggingface-transformers', 'large-language-model']",79228250,"i had the same issue. you need to add a prompt in the processor:
prompt = "" ""

inputs = processor(images=image, text=prompt, return_tensors=""pt"").to(device=""cuda"", dtype=torch.float16)

hope it helps.",https://stackoverflow.com/questions/79221167,python,24-11-2024 22:05,333.0,2.0,2.0,True,26-11-2024 21:19,26-11-2024 10:36
3638488,automatic text translation,"what tools or web services are available for machine text translation.
for example
english text > server or lib > german text

libraries are also acceptable. 
is google language api the only one ?","web-services, machine-learning, nlp, machine-translation",3640391,"actually, just going with google's translation api is probably the best and easiest thing to do. 
google's api is easy to use and, depending on the language pairs being translated, their translation system is either as good or much better than everything else. 
open source translation packages
however, there are also some really good open source tools for machine translation. state-of-the-art packages include:

cdec (c++)
joshua (java)
moses (c++)
phrasal (java) - soon to be released

unlike translation apis, you can use these tools without needing access to the internet. more importantly, you can use these tools without running into any throttling or limits that the free apis impose if you are trying to translate larger amounts of data.
training data
to use the open source machine translation packages, you'll need training data. if you're translating between english to german, or between some other european languages, you can use phillip koehn's europarl parallel corpus. 
if you're interested in a european union (eu) language that's not in the europarl parallel corpus, you can gather the data by crawling the proceedings of the european parliament. all the eu proceedings are translated into each of the eu languages and made available for free online, which makes them a very good source of machine translation training data.",https://stackoverflow.com/q/3638488,"web-services, machine-learning, nlp, machine-translation",03-09-2010 18:19,2543.0,2.0,1.0,True,22-03-2022 12:46,04-09-2010 00:23
21974917,retrieving row and column names in r,"so i have a matrix tmatrix that i'm cycling through, and i want to put the row and column names for every cell that contains a value that is not finite into a table.  i've tried to doing the following, but i keep getting na for the row and column names.  what's going on?
    aa <- 1:rowlength
    bb <- 1:ncol(nmatrix)
    for(i in aa){
        for(j in bb){
            if (is.finite(tmatrix[i,j])==false){
                tns <- matrix(data=na,nrow=1,ncol=4)
                tns[1,1] <- tmatrix[i,j]
                tns[1,2] <- nmatrix[i,j]
                tns[1,3] <- paste(rownames(tmatrix)[tmatrix[i,j]])
                tns[1,4] <- paste(colnames(tmatrix)[tmatrix[i,j]])
                tminf <- rbind(tminf,tns)
            }
            pmatrix[i,j] <- pt(tmatrix[i,j],n1+n2-2)
        }
    }","['r', 'information-retrieval']",21975066,"never mind, i figured it out.  i had the index wrong.  it should be like this:
aa <- 1:rowlength
bb <- 1:ncol(nmatrix)
for(i in aa){
    for(j in bb){
        if (is.finite(tmatrix[i,j])==false){
            tns <- matrix(data=na,nrow=1,ncol=4)
            tns[1,1] <- tmatrix[i,j]
            tns[1,2] <- nmatrix[i,j]
            tns[1,3] <- rownames(tmatrix)[i]
            tns[1,4] <- colnames(tmatrix)[j]
            tminf <- rbind(tminf,tns)
        }
        pmatrix[i,j] <- pt(tmatrix[i,j],n1+n2-2)
    }
}",https://stackoverflow.com/questions/21974917,r,23-02-2014 21:37,183.0,0.0,2.0,True,23-03-2023 15:09,23-03-2023 15:09
2287962,c++ - how to read unicode characters( hindi script for e.g. ) using c++ or is there a better way through some other programming language?,"i have a hindi script file like this:
3.  ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½

i have to write a program which adds a position to each and every word in each sentence.
thus the numbering for every line for a particular word position should start off with 1 in parentheses. the output should be something like this.
3.  ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(1) ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(2) ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(3) ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(4) ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(5) ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(6) ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(7) ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(8) ï¿½ï¿½ï¿½(9)

tp to 'ï¿½ï¿½ï¿½'  in hindi. serial nos remain as it is untouched.
i thought reading character by character could be a solution. how can i do this?
the thing is i am able to get word positions for my english text using c++ as i was able to read character by character using ascii values in c++ but i don't have a clue to how to go about the same for the hindi text.
the final aim of all this is to see which word position of the english text maps to which postion in hindi. this way i can achieve bidirectional alignment.
thank you for your time...:","c++, utf-8, nlp",2288897,"i would seriously suggest that you'd use python for an applicatin like this.
it will lift the burden of decoding the strigns (not to mention allocating memory for them and the like). you will be free to concentrate on your problem, instead of problems of the language.
for example, if the sentence above is contained in an utf-8 file, and you are uisng python2.x.
if you use python 3.x it is even more readible, as you don't have to prefix the unicode strings with 'u"" ', as in this example (but you will be missing a lot of 3rd party libraries:
separators = [u""ýýý"", u"","", u"".""]
text = open(""indiantext.txt"").read()
#this converts the encoded text to an internal unicode object, where
# all characters are properly recognized as an entity:
text = text.decode(""utf-8"")

#this breaks the text on the white spaces, yielding a list of words:
words = text.split()

counter = 1

output = """"
for word in words:
    #if the last char is a separator, and is joined to the word:
    if word[-1] in separators and len(word) > 1:
        #word up to the second to last char:
        output += word[:-1] + u""(%d) "" % counter
        counter += 1
        #last char
        output += word[-1] +  u""(%d) "" % counter
    else:
        output += word + u""(%d) "" % counter
    counter += 1

print output

this is an ""unfolded"" example, as you get more used to python there are shorer ways to express this. you can learn the basics of teh language in just a couple of hours, following a tutorial. (for example, the one at  itself)",https://stackoverflow.com/q/2287962,"c++, utf-8, nlp",18-02-2010 10:51,10936.0,7.0,7.0,True,31-05-2022 19:05,31-05-2022 19:05
45735070,keras text preprocessing - saving tokenizer object to file for scoring,"i've trained a sentiment classifier model using keras library by following the below steps(broadly).

convert text corpus into sequences using tokenizer object/class
build a model using the model.fit() method 
evaluate this model

now for scoring using this model, i was able to save the model to a file and load from a file. however i've not found a way to save the tokenizer object to file. without this i'll have to process the corpus every time i need to score even a single sentence. is there a way around this?","['machine-learning', 'neural-network', 'nlp', 'deep-learning', 'keras']",45737582,"the most common way is to use either pickle or joblib. here you have an example on how to use pickle in order to save tokenizer:
import pickle

# saving
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.highest_protocol)

# loading
with open('tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)",https://stackoverflow.com/questions/45735070,machine-learning,17-08-2017 12:25,53982.0,65.0,6.0,True,16-09-2021 02:40,30-12-2017 13:35
76272624,what is the use case of system role,"this is from the official documentation from chatgpt chat completion:
openai.chatcompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
        {""role"": ""system"", ""content"": ""you are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""who won the world series in 2020?""},
        {""role"": ""assistant"", ""content"": ""the los angeles dodgers won the world series in 2020.""},
        {""role"": ""user"", ""content"": ""where was it played?""}
    ]
)

my first understanding for the system role is a message that just to greeting the user. but it doesn't make sense to greet user by 'you are a helpful assistant.'.and it also explains:

the system message helps set the behavior of the assistant. in the example above, the assistant was instructed with ""you are a helpful assistant.""

so do i write the behavior of the ai in the system role like: you're professional assistant if i want the ai to be a pro or i can write in the role like: you're a funny assistant if i would like it to be a interesting ai.
or it simply just a greeting message?","['openai-api', 'chatgpt-api']",76273345,it's not for greeting the user but to say how chatgpt should act. there are a lot of sample prompts in the web. here you'll find some samples for those system prompts:,https://stackoverflow.com/questions/76272624,openai-api,17-05-2023 13:25,9091.0,9.0,1.0,True,17-05-2023 16:22,17-05-2023 16:22
72955752,finding the scores for each tweet with a bert-based sentiment analysis model,"i am doing a sentiment analysis of twitter posts and i have a question regarding ï¿½ï¿½ï¿½german sentiment classification with bertï¿½ï¿½ï¿½:
i would like to display the sentiment score (positive, negative, neutral) for each tweet like it is shown on the models card"" rel=""nofollow noreferrer"">huggingface(screenshot)
i tried to go through the implementation of the mode by stepping into every line of code but could not figure out how to find the scores.

my code is based on the following code:

model = sentimentmodel()

texts = [
    ""mit keinem guten ergebniss"",""das ist gar nicht mal so gut"",
    ""total awesome!"",""nicht so schlecht wie erwartet"",
    ""der test verlief positiv."",""sie fï¿½ï¿½hrt ein grï¿½ï¿½nes auto.""]
       
result = model.predict_sentiment(texts)
print(result)
</code","['python', 'twitter', 'sentiment-analysis', 'huggingface-transformers', 'bert-language-model']",72956272,"you can inherit from the model's class and define a function to output the scores:
from typing import list
import torch
from germansentiment import sentimentmodel


class sentimentmodel(sentimentmodel):
    def __init__(self):
        super().__init__()
        
    def predict_sentiment_proba(self, texts: list[str])-> list[str]:
        texts = [self.clean_text(text) for text in texts]
        # add special tokens takes care of adding [cls], [sep], <s>... tokens in the right way for each model.
        # truncation=true limits number of tokens to model's limitations (512)
        encoded = self.tokenizer.batch_encode_plus(texts, padding=true, add_special_tokens=true,truncation=true, return_tensors=""pt"")
        
        encoded = encoded.to(self.device)
        with torch.no_grad():
                logits = self.model(**encoded)
        
        #label_ids = torch.argmax(logits[0], axis=1)
        return torch.nn.softmax(dim=1)(logits[0]), self.model.config.id2label
   
texts = [""mit keinem guten ergebniss"",""das ist gar nicht mal so gut"",
    ""total awesome!"",""nicht so schlecht wie erwartet"",
    ""der test verlief positiv."",""sie fï¿½ï¿½hrt ein grï¿½ï¿½nes auto.""]

model = sentimentmodel()
scores, ids = model.predict_sentiment_proba(texts)

scores
>tensor([[1.1602e-03, 9.9877e-01, 6.8676e-05],
        [8.8440e-04, 9.9909e-01, 2.3437e-05],
        [9.8738e-01, 1.2542e-02, 7.6997e-05],
        [9.7940e-01, 2.0516e-02, 8.2444e-05],
        [4.1755e-04, 4.6088e-04, 9.9912e-01],
        [2.1236e-05, 5.3932e-05, 9.9992e-01]])
ids
>{0: 'positive', 1: 'negative', 2: 'neutral'}

scores.argmax(dim=-1)
>tensor([1, 1, 0, 0, 2, 2]) #negative, negative, positive, positive, neutral, neutral
</code",https://stackoverflow.com/questions/72955752,python,12-07-2022 16:46,1238.0,2.0,3.0,True,13-10-2022 10:47,13-07-2022 20:27
59760328,how does torch.distributed.barrier() work,"i've read all the documentations i could find about torch.distributed.barrier(), but still having trouble understanding how it's being used in this script and would really appreciate some help.
so the official doc of torch.distributed.barrier says it ""synchronizes all processes.this collective blocks processes until the whole group enters this function, if async_op is false, or if async work handle is called on wait().""
it's used in two places in the script: 
first place
    if args.local_rank not in [-1, 0] and not evaluate:
        torch.distributed.barrier()  # make sure only the first process in distributed training process the dataset, and the others will use the cache

        ... (preprocesses the data and save the preprocessed data)

    if args.local_rank == 0 and not evaluate:
        torch.distributed.barrier() 

second place
    if args.local_rank not in [-1, 0]:
        torch.distributed.barrier()  # make sure only the first process in distributed training will download model & vocab

        ... (loads the model and the vocabulary)

    if args.local_rank == 0:
        torch.distributed.barrier()  # make sure only the first process in distributed training will download model & vocab

i'm having trouble relating the comment in the code to the functionality of this function stated in the official doc. how does it make sure only the first process executes the code between the two calls of torch.distributed.barrier() and why it only checks whether the local rank is 0 before the second call? 
thanks in advance!","['python', 'pytorch', 'huggingface-transformers']",59766443,"first you need to understand the ranks. to be brief: in a multiprocessing context we typically assume that rank 0 is the first process or base process. the other processes are then ranked differently, e.g. 1, 2, 3, totalling four processes in total.
some operations are not necessary to be done in parallel or you just need one process to do some preprocessing or caching so that the other processes can use that data.
in your example, if the first if statement is entered by the non-base processes (rank 1, 2, 3), they will block (or ""wait"") because they run into the barrier. they wait there, because barrier() blocks until all processes have reached a barrier, but the base process has not reached a barrier yet.
so at this point the non-base processes (1, 2, 3) are blocked, but the base process (0) continues. the base process will do some operations (preprocess and cache data, in this case) until it reaches the second if-statement. there, the base process will run into a barrier. at this point, all processes have stopped at a barrier, meaning that all current barriers can be lifted and all processes can continue. because the base process prepared the data, the other processes can now use that data.
perhaps the most important thing to understand is:

when a process encounters a barrier it will block
the position of the barrier is not important (not all processes have to enter the same if-statement, for instance)
a process is blocked by a barrier until all processes have encountered a barrier, upon which those barriers are lifted for all processes",https://stackoverflow.com/questions/59760328,python,15-01-2020 22:14,31203.0,22.0,1.0,True,14-06-2022 09:52,14-06-2022 09:52
77544203,how to integrate custom openaimodel into a automodelforsequenceclassification model?,"i've developed a custom openaimodel module that acts like bert models but makes an openai embeddings request and returns the results when called. i want to use this module, utilizing hugging face's transformers library inside an automodelforsequenceclassification model. however, when i try to replace the base_model with my openaimodel using the following code, the base_model property doesn't change as expected:
from transformers import automodelforsequenceclassification
import torch

num_labels = ... # define the appropriate number of labels
train_arch = ... # define parameters specific to your training architecture

model = automodelforsequenceclassification.from_pretrained('dbmdz/distilbert-base-turkish-cased', num_labels=num_labels)
model.base_model = openaimodel(train_arch)
model.classifier = torch.nn.linear(model.base_model.config.dim, num_labels)
model.config = openaimodelconfig()
model.config_class = type(openaimodelconfig)
tokenizer = openaitokenizer()

after executing model.base_model = openaimodel(train_arch), when i inspect model.base_model, it still appears to be distilbert and not the expected openaimodel.
i can successfully change all other values of the sequenceclassification model, but the base_model remains unaltered. i'm unsure why this is happening and how to resolve it.
what is the correct way to integrate my model into automodelforsequenceclassification? or should i be using a different approach to change the base_model? (i tried to load my model with from_pretrained but automodel class doesn't support custom model structure as i see)
any help or suggestions would be greatly appreciated!","['python', 'pytorch', 'nlp', 'huggingface-transformers', 'huggingface']",77547197,"if we are using a distilbert model, changing the model.distilbert layer, not model.base_model, was enough to do what i intended.
when i did model.distilbert = openaimodel(**kwargs) i achieved my goal.",https://stackoverflow.com/questions/77544203,python,24-11-2023 15:26,92.0,0.0,1.0,True,25-11-2023 08:46,24-11-2023 15:33
65153422,create a vocabulary with pos,"i would like to create a list of semantic entities (nouns, verbs, punct, etc.) using pos tagging.
i am currently running the following code
import spacy
import pandas as pd
    
nlp = spacy.load('en_core_web_sm',disable=['ner','textcat'])

def fun(text):
    doc = nlp(text)
    pos = """"
    for token in doc:
        pos += token.pos_ + "" ""
    return pos

df['s']= df.text.apply(fun)

to create the structure of sentences.
so, for example, if i have the column text (see below), this code generate the column s which contains all the information about semantic structure:
text                                                s
0   ï¿½ï¿½ï¿½i will meet quite a few people, itï¿½ï¿½ï¿½s well...   punct noun verb verb det det adj noun punct pr...
1   says ï¿½ï¿½ï¿½cristiano ronaldoï¿½ï¿½ï¿½s family still ownsï¿½ï¿½ï¿½... verb punct propn propn part noun adv verb punc...
2   joe biden plagiarized donald trump in his... propn propn verb propn propn adp dam wondering if i can create a vocabulary of nouns, verbs, det, adj, ... by editing the code above or if i need to consider a different approach.
to take all the entities (nouns, verbs,...) in the dataframe, i would look at selecting only unique values, in order to creat a list for each of them.
example of output (it can be also in lists rather than in a dataframe)
punct      noun        verb         ....
ï¿½ï¿½ï¿½           i          will 
,          people      meet
ï¿½ï¿½ï¿½          family      says
                       owns
                      plagiarized
</","['python', 'pandas', 'nlp', 'spacy', 'part-of-speech']",65160717,"you can try:
import spacy
import pandas as pd
nlp = spacy.load('en_core_web_sm',disable=['ner','textcat'])

texts = ['""i will meet quite a few people, it\'s well', 
         'says ""cristiano ronaldo\'s family still owns""',
         'joe biden plagiarized donald trump in his...']

df = pd.dataframe({""text"":texts})

d = dict()
def func(text):
    doc = nlp(text)
    for tok in doc:
        if tok.pos_ not in d:
            d[tok.pos_] = [tok.text]
        else:
            d[tok.pos_].append(tok.text)
            
df.text.apply(func)

pprint(d)


{'adj': ['few'],
 'adp': ['in'],
 'adv': ['well', 'still'],
 'aux': [""'s""],
 'det': ['quite', 'a', 'his'],
 'noun': ['people', 'family'],
 'part': [""'s""],
 'pron': ['i', 'it'],
 'propn': ['cristiano', 'ronaldo', 'joe', 'biden', 'donald', 'trump'],
 'punct': ['""', ',', '""', '""', '...'],
 'verb': ['will', 'meet', 'says', 'owns', 'plagiarized']}

note, you don't need pandas dependence at all:
docs = nlp.pipe(texts)
d = dict()
for doc in docs:
    for tok in doc:
        if tok.pos_ not in d:
            d[tok.pos_] = [tok.text]
        else:
            d[tok.pos_].append(tok.text)
pprint(d)


{'adj': ['few'],
 'adp': ['in'],
 'adv': ['well', 'still'],
 'aux': [""'s""],
 'det': ['quite', 'a', 'his'],
 'noun': ['people', 'family'],
 'part': [""'s""],
 'pron': ['i', 'it'],
 'propn': ['cristiano', 'ronaldo', 'joe', 'biden', 'donald', 'trump'],
 'punct': ['""', ',', '""', '""', '...'],
 'verb': ['will', 'meet', 'says', 'owns', 'plagiarized']}

these will collect all the tokens under their pos.
if you only need list of unique tokens:
texts = ['""i will will meet quite a few people, it\'s well', 
         'says ""cristiano ronaldo\'s family still owns""',
         'joe biden plagiarized donald trump in his...']

docs = nlp.pipe(texts)
d = dict()
for doc in docs:
    for tok in doc:
        if tok.pos_ not in d:
            d[tok.pos_] = [tok.text]
        elif tok.text not in d[tok.pos_]:
            d[tok.pos_].append(tok.text)
pprint(d)


{'adj': ['few'],
 'adp': ['in'],
 'adv': ['well', 'still'],
 'aux': [""'s""],
 'det': ['quite', 'a', 'his'],
 'noun': ['people', 'family'],
 'part': [""'s""],
 'pron': ['i', 'it'],
 'propn': ['cristiano', 'ronaldo', 'joe', 'biden', 'donald', 'trump'],
 'punct': ['""', ',', '...'],
 'verb': ['will', 'meet', 'says', 'owns', 'plagiarized']}",https://stackoverflow.com/questions/65153422,python,05-12-2020 03:19,249.0,0.0,1.0,True,26-09-2021 03:59,26-09-2021 03:59
56554380,why can&#39;t i import functions in bert after pip install bert,"i am a beginner for bert, and i am trying to use files of bert given on the github:
however i cannot import files(such as run_classifier, optimisation and so on) from bert after using pip install bert to install bert in terminal. i tried to run following codes in jupiter notebook:
import bert
from bert import run_classifier

and the error is:
importerror: cannot import name 'run_classifier'

then i found the file named 'bert' in \anaconda3\lib\python3.6\site-packages, and there were no python files named 'run_classifier', 'optimization' etc inside it. so i downloaded those files from github and put them into file 'bert' by myself. after doing this i could import run_classifier.
however, another problem occurred. i couldn't use the functions inside the files although i could import them.
for example, there's a function convert_to_unicode in tokenization.py:
help on module bert.tokenization in bert:

name

    bert.tokenization - tokenization classes.    
functions

    convert_to_unicode(text)
    converts `text` to unicode (if it's not already), assuming utf-8 input.

then i tried this:
import tokenization from bert
convert_to_unicode('input.txt')

and the error is:
nameerror: name 'convert_to_unicode' is not defined

then i tried:
from tokenization import convert_to_unicode

and the error is:
modulenotfounderror: no module named 'tokenization'

i am really confused about this.","['python', 'nlp', 'bert-language-model']",56557888,"the package you're looking for is bert-tensorflow, not bert.
bert-tensorflow is the python package for google's bert implementation.
bert is a serialization library.",https://stackoverflow.com/questions/56554380,python,12-06-2019 03:43,20032.0,8.0,3.0,True,09-02-2023 13:39,25-06-2019 09:00
76464175,setfit training with a pandas dataframe,"i would like to train a zero shot classifier on an annotated sample dataset.
i am following some tutorials but as all use their own data and the same pretarined model, i am trying to confirm: is this the best approach?
data example: 

import pandas as pd
from datasets import dataset
    
# sample feedback data, it will have 8 samples per label
feedback_dict = [
    {'text': 'the product is great and works well.', 'label': 'product performance'},
    {'text': 'i love the design of the product.', 'label': 'product design'},
    {'text': 'the product is difficult to use.', 'label': 'usability'},
    {'text': 'the customer service was very helpful.', 'label': 'customer service'},
    {'text': 'the product was delivered on time.', 'label': 'delivery time'}
]

# create a dataframe with the feedback data
df = pd.dataframe(feedback_dict)

# convert to dataset format
df = dataset.from_pandas(df)

by having the previous data format, this is the approach for model finetunning:
from setfit import setfitmodel, setfittrainer

# select a model
model = setfitmodel.from_pretrained(""sentence-transformers/paraphrase-mpnet-base-v2"")

# training with setfit
trainer = setfittrainer(
    model=model,
    train_dataset=df, # to keep the code simple i do not create the df_train
    eval_dataset=df, # to keep the code simple i do not create the df_eval
    column_mapping={""text"": ""text"", ""label"": ""label""} 
)

trainer.train()

the issue here is that the process never ends after more than 500 hours in a laptop, and the dataset it is only about 88 records with 11 labels.","['python', 'huggingface-transformers', 'few-shot-learning']",76538663,"i tried to run the example you posted on google colab, it took 37 seconds to run the training.
here's you code with some tweak to make it work on colab:
### install libraries
%%capture
!pip install datasets setfit

after installing the libraries, run the following code:
### import dataset
import pandas as pd
from datasets import dataset
# sample feedback data, it will have 8 samples per label
feedback_dict = [
    {'text': 'the product is great and works well.', 'label': 'product performance'},
    {'text': 'i love the design of the product.', 'label': 'product design'},
    {'text': 'the product is difficult to use.', 'label': 'usability'},
    {'text': 'the customer service was very helpful.', 'label': 'customer service'},
    {'text': 'the product was delivered on time.', 'label': 'delivery time'}
]
# create a dataframe with the feedback data
df = pd.dataframe(feedback_dict)
# convert to dataset format
df = dataset.from_pandas(df)

### run training
from setfit import setfitmodel, setfittrainer
# select a model
model = setfitmodel.from_pretrained(""sentence-transformers/paraphrase-mpnet-base-v2"")
# training with setfit
trainer = setfittrainer(
    model=model,
    train_dataset=df, # to keep the code simple i do not create the df_train
    eval_dataset=df, # to keep the code simple i do not create the df_eval
    column_mapping={""text"": ""text"", ""label"": ""label""} 
)
trainer.train()

and finally, you can download the trained model on drive and then download it on you pc manually.
### download model to drive
from google.colab import drive
drive.mount('/content/drive')
trainer.model._save_pretrained('/content/drive/path/to/target/folder')

if your main issue is the training time, this should fix it.",https://stackoverflow.com/questions/76464175,python,13-06-2023 10:54,1190.0,4.0,2.0,True,24-06-2023 10:34,14-06-2023 08:47
3114734,decision trees for document classification,"i wanted to know that is it possible to use decision trees for document classification, and if yes then how should the data representation be?
i know the use of the r package party for decision trees.","['r', 'nlp', 'classification', 'text-mining', 'document-classification']",3115987,"one way is to have a huge matrix where each row is a document, and each column is a word.  and the values in the cells are the number of times that word showed in that document.
then, if you are dealing with ""supervised learning"" case, you should have another column for the classifier, and from there on you can use a command like ""rpart"" (from the rpart package), to create your classification tree.  the command would be entering a formula to rpart, in a similar fashion as you would to a linear model (lm).
if you want, you could also try to first group your words to ""groups of words"", and then have each column belonging to a different group of words, with a number indication how many words in the document belonged to that group.  for that i would have a look at the ""tm"" package.  (if you end up doing something with that, please consider maybe posting about it here, so we could learn from it)",https://stackoverflow.com/questions/3114734,r,24-06-2010 23:57,5015.0,1.0,3.0,True,16-01-2023 16:58,16-01-2023 16:57
79247594,euclidian distance from word to sentence after doing vectorizer,"i have dataframe with 1000 text rows.
i did tfidfvectorizer.
now  i want to create a new field which give me the distance from  each sentence to the word that i want, lets say the word ""king"". df['king']
i thought about taking in each sentence the 5 closet words to the word king and make average of them.
i will glad to know how to do that or to hear about another method.","['pandas', 'dataframe', 'nlp', 'text-classification', 'tf-idf']",79248087,"i am not convinced that the euclidean distance would be the optimal measure. i would actually look at similarity scores:
import pandas as pd
from sklearn.feature_extraction.text import tfidfvectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

data = {
    'text': [
        ""the king sat on the throne with wisdom."",
        ""a queen ruled the kingdom alongside the king."",
        ""knights were loyal to their king."",
        ""the empire prospered under the rule of a wise monarch.""
    ]
}
df = pd.dataframe(data)

tfidf = tfidfvectorizer()
tfidf_matrix = tfidf.fit_transform(df['text'])

try:
    king_vector = tfidf.transform([""king""]).toarray()
except keyerror:
    print(""the word 'king' is not in the vocabulary."")
    king_vector = np.zeros((1, tfidf_matrix.shape[1]))

similarities = cosine_similarity(tfidf_matrix, king_vector).flatten()

feature_names = np.array(tfidf.get_feature_names_out())

def get_top_n_words(row_vector, top_n=5):
    indices = row_vector.argsort()[::-1][:top_n]
    return feature_names[indices]

averages = []
for i in range(tfidf_matrix.shape[0]):
    sentence_vector = tfidf_matrix[i].toarray().flatten()
    top_words = get_top_n_words(sentence_vector)
    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]
    averages.append(np.mean(top_similarities))

df['king_similarity'] = similarities
df['avg_closest_similarity'] = averages

print(df)

which would give you
                                                text  king_similarity  \
0            the king sat on the throne with wisdom.         0.240614   
1      a queen ruled the kingdom alongside the king.         0.259779   
2                  knights were loyal to their king.         0.274487   
3  the empire prospered under the rule of a wise ...         0.000000   

   avg_closest_similarity  
0                     0.0  
1                     0.0  
2                     0.0  
3                     0.0  

that being said, if you absolutely want to focus on euclidean distance, here is a method:
import pandas as pd
from sklearn.feature_extraction.text import tfidfvectorizer
import numpy as np
from scipy.spatial.distance import euclidean

data = {
    'text': [
        ""the king sat on the throne with wisdom."",
        ""a queen ruled the kingdom alongside the king."",
        ""knights were loyal to their king."",
        ""the empire prospered under the rule of a wise monarch.""
    ]
}
df = pd.dataframe(data)

tfidf = tfidfvectorizer()
tfidf_matrix = tfidf.fit_transform(df['text']).toarray()

feature_names = tfidf.get_feature_names_out()
if ""king"" in feature_names:
    king_index = np.where(feature_names == ""king"")[0][0]
    king_vector = np.zeros_like(tfidf_matrix[0])
    king_vector[king_index] = 1
else:
    print(""the word 'king' is not in the vocabulary."")
    king_vector = np.zeros_like(tfidf_matrix[0])

df['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]

print(df)


which gives
                                                text  king_distance
0            the king sat on the throne with wisdom.       1.232385
1      a queen ruled the kingdom alongside the king.       1.216734
2                  knights were loyal to their king.       1.204586
3  the empire prospered under the rule of a wise ...       1.414214",https://stackoverflow.com/questions/79247594,pandas,03-12-2024 12:25,46.0,1.0,1.0,True,03-12-2024 14:51,03-12-2024 14:28
78549270,how do i fix this error related to langchain invoke? (ai chatbot in python with the chatgpt api and langchain for memory storage),"here's my code:
import pickle, os
from langchain_openai.chat_models import chatopenai
from langchain.schema import (
    aimessage,
    humanmessage,
    systemmessage
)

def execute_prompt(text, history, jarvis_setup):
    print(f""you said: {text}"")
    history.append(humanmessage(content = text))
    response = jarvis_setup(history)
    history.append(aimessage(content = response.content))
    with open('jarvismemory.txt', 'wb') as file:
        pickle.dump(history, file)
        
    print(response.content)

def main():
    jarvis_setup = chatopenai(openai_api_key=""api_key"", model = ""gpt-3.5-turbo"", temperature = 0.7, max_tokens = 400)
    #history = [systemmessage(content=""you are a human-like virtual assistant named jarvis."", additional_kwargs={})]
    if os.path.exists(""jarvismemory.txt""):
        with open(""jarvismemory.txt"", ""rb"") as file:
            history = pickle.load(file)
    else:
        with open(""jarvismemory.txt"", ""wb"") as file:
            history = [systemmessage(content=""you are a human-like virtual assistant named jarvis. answer all questions as shortly as possible, unless a longer, more detailed response is requested."", additional_kwargs={})]
            pickle.dump(history, file)
    
    while true:
        print(""\n"")
        print(""enter prompt."")
        text = input().lower()
        print(""prompt sent."")
    
        if text:
            execute_prompt(text, history, jarvis_setup)
                        
        else:
            print(""no prompt given."")
            continue
                    
if __name__ == ""__main__"":
    main()

and i get this error:
langchaindeprecationwarning: the method basechatmodel.__call__ was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. use invoke instead.
warn_deprecated(
traceback (most recent call last):
file ""c:\users\maste\documents\coding\python\jarvis\jarvistextinpuhjhjghyjvjt.py"", line 44, in 
main()
file ""c:\users\maste\documents\coding\python\jarvis\jarvistextinpuhjhjghyjvjt.py"", line 37, in main
execute_prompt(text, history, jarvis_setup)
file ""c:\users\maste\documents\coding\python\jarvis\jarvistextinpuhjhjghyjvjt.py"", line 12, in execute_prompt
response = jarvis_setup(history)
file ""c:\users\maste\appdata\roaming\python\python310\site-packages\langchain_core_api\deprecation.py"", line 148, in warning_emitting_wrapper
return wrapped(*args, **kwargs)
file ""c:\users\maste\appdata\roaming\python\python310\site-packages\langchain_core\language_models\chat_models.py"", line 847, in call
generation = self.generate(
file ""c:\users\maste\appdata\roaming\python\python310\site-packages\langchain_core\language_models\chat_models.py"", line 456, in generate
raise e
file ""c:\users\maste\appdata\roaming\python\python310\site-packages\langchain_core\language_models\chat_models.py"", line 446, in generate
self._generate_with_cache(
file ""c:\users\maste\appdata\roaming\python\python310\site-packages\langchain_core\language_models\chat_models.py"", line 671, in _generate_with_cache
result = self._generate(
file ""c:\users\maste\appdata\roaming\python\python310\site-packages\langchain_openai\chat_models\base.py"", line 520, in _generate
message_dicts, params = self._create_message_dicts(messages, stop)
file ""c:\users\maste\appdata\roaming\python\python310\site-packages\langchain_openai\chat_models\base.py"", line 533, in _create_message_dicts
message_dicts = [_convert_message_to_dict(m) for m in messages]
file ""c:\users\maste\appdata\roaming\python\python310\site-packages\langchain_openai\chat_models\base.py"", line 533, in 
message_dicts = [_convert_message_to_dict(m) for m in messages]
file ""c:\users\maste\appdata\roaming\python\python310\site-packages\langchain_openai\chat_models\base.py"", line 182, in _convert_message_to_dict
if (name := message.name or message.additional_kwargs.get(""name"")) is not none:
attributeerror: 'systemmessage' object has no attribute 'name'
i'm guessing i need to add "".invoke"" somewhere in the code based on some research i did on the issue, but i'm a beginner.
i found this website showcasing a very similar error and how to fix it: 
you can translate the page to english with google translate and the translations are sufficient to understand. it says to add "".invoke"" in the place you can see shown on the website. not sure how to implement this into my code though. also, this might not be the right solution.
i also looked at the langchain website and it also says to use ""invoke"" but i can't find examples of it being used in a full line of code.","['langchain', 'chatgpt-api']",78560168,"here's  the solution! i just figured it out. very simple mistake! when changing the langchain_community to langchain_openai, remove the "".chat_models""! that's all it was!
so this line:
from langchain_community.chat_models import chatopenai
should be this:
from langchain_openai import chatopenai
this is how i figured it out:

also, at least in my code, i had to add "".invoke"" after jarvis_setup here:
response = jarvis_setup(history)
with those two changes, i get no warnings and no errors!",https://stackoverflow.com/questions/78549270,langchain,29-05-2024 11:37,4997.0,1.0,2.0,True,03-10-2024 13:53,03-10-2024 13:53
78949607,trainer huggingface - runtimeerror: cannot pin &#39;torch.cuda.floattensor&#39; only dense cpu tensors can be pinned,"i recently got the following error:
runtimeerror: cannot pin 'torch.cuda.floattensor' only dense cpu tensors can be pinned
when doing lora on a small llm.
i saw on a discord someone saying:

the issue likely stems from the fact that you are manually placing
your inputs on the gpu (with to(model.device)), but the trainer
expects data to be on the cpu and will handle the transfer to the gpu
internally.

i can't find anything of the sort written in the trainer documentation of huggingface 
is it true? if not, how can i get rid of that error?
mre:
import torch
from torch.utils.data import dataset
from transformers import automodelforcausallm, autotokenizer
from transformers import trainingarguments
from transformers import trainer
from peft import loraconfig, get_peft_model

model_name = ""croissantllm/croissantllmbase""
tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforcausallm.from_pretrained(model_name, torch_dtype=torch.float16, device_map=""auto"")

texts = [
    ""the first sentence for fine-tuning. </s>"",
    ""the second sentence for fine-tuning. </s>""
]

inputs = [tokenizer(text, return_tensors=""pt"").to(model.device) for text in texts]

lora_config = loraconfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=[""q_proj"", ""v_proj""],
)

model = get_peft_model(model, lora_config)

class customdataset(dataset):
    def __init__(self, input_list):
        self.input_list = input_list

    def __len__(self):
        return len(self.input_list)

    def __getitem__(self, idx):
        input_ids = self.input_list[idx]['input_ids'].squeeze()
        labels = input_ids.clone()
        return {""input_ids"": input_ids, ""labels"": labels}

train_dataset = customdataset(inputs)

training_args = trainingarguments(
    output_dir=""./lora_croissantllm"",
    per_device_train_batch_size=1,
    num_train_epochs=1,
    save_steps=10,
    save_total_limit=2,
    logging_dir=""./logs"",
    logging_steps=10,
)

trainer = trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

trainer.train()

the issue is fairly easy to reproduce directly on colab (run %pip install --upgrade torch transformers peft in the first cell).","['nlp', 'huggingface-transformers']",79112186,"since pinning memory is only available on cpu and not gpu, when running on gpu on colab, you can just disable it by setting dataloader_pin_memory to false for trainingarguments
training_args = trainingarguments(
    output_dir=""./lora_croissantllm"",
    dataloader_pin_memory=false,
    per_device_train_batch_size=1,
    num_train_epochs=1,
    save_steps=10,
    save_total_limit=2,
    logging_dir=""./logs"",
    logging_steps=10,
)",https://stackoverflow.com/questions/78949607,nlp,04-09-2024 16:09,1523.0,2.0,1.0,True,24-10-2024 09:27,15-09-2024 11:44
78158335,rasa postback from button click not working - facebook messenger,"issue :
enter image description here
the search flight button is stopped working.
but the prompt ""search flight"" is working
also , i can't see the log in webhook requests when i click the postback button
note : to turn my local server endpoints to  endpoints, i used ngrok
rasa version = 3.2.6
i tried to cross check these setup
messanger setup
webhook setup
messaging and messaging_postback subscriptions (is active.)
code for postback (which is not directing to payload webhook url)
""start_over_crsl"": {
    ""attachment"": {
      ""type"": ""template"",
      ""payload"": {
        ""template_type"": ""generic"",
        ""elements"": [
          {
            ""title"": ""flights ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"",
            ""image_url"": ""
            ""subtitle"": ""planning a trind you the best flight options!"",
            ""buttons"": [
              {
                ""type"": ""postback"",
                ""title"": ""search flights"",
                ""payload"": ""/flight""
              }
            ]
          },

...
but suggestion chip is working perfectly (redirecting perfectly to webhook url)
""quick_replies"": [
  {
    ""content_type"": ""text"",
    ""title"": ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ flights"",
    ""payload"": ""/flight""
  }, ...
</","['python', 'rasa', 'rasa-nlu', 'facebook-webhooks']",78204602,"there is a new policy came from facebook. to access the webhook, the chatbot application needs to be live.
the issue is resolved by clicking live toggle on the developer.facebook application dashboard page.",https://stackoverflow.com/questions/78158335,python,14-03-2024 05:37,57.0,-1.0,1.0,True,22-03-2024 09:29,22-03-2024 09:29
77461195,open ai typeerror: &#39;completion&#39; object is not subscriptable,"after open ai aprade the code , i cannot use the old one and keep getting the error like 'completion' object is not subscriptable.
could any one help me, thanks in advance!!

from openai import openai

client = openai(
  api_key= 'sk-mghptvikcoo3lvbgpnuqt3blbkfj4etsnjgm7dpkp3jpfcmw',  # this is also the default, it can be omitted
)


f_prompt = ""as senior telcom agent at at&t mobility, craft 100-word conversations starting with the customer's message. address various topics (e.g., at&t store info, plans, device support, internet security, voicemail, promotions, early termination, directv, billing, porting). avoid initiating service cancellation, but reveal the answer to whether the customer decided to stop using the service is {churn}.""
f_sub_prompt = ""{churn}""

df = data_10

for index, row in df.iterrows():
    churn = row[""churn""]

    for i in range(1):  # 1 times each
        prompt = f_prompt.format(churn=churn)
        sub_prompt = f_sub_prompt.format(churn=churn)
        print(sub_prompt)


        response = client.completions.create(
            model=""text-davinci-003"",
            prompt=prompt,
            temperature=1,
            max_tokens=300,
            top_p=1,
            frequency_penalty=1,
            presence_penalty=0
        )

        finish_reason = response['choices'][0]['finish_reason']
        response_txt = response['choices'][0]['text']

        new_row = {
            'churn': churn,
            'prompt': prompt,
            'sub_prompt': sub_prompt,
            'response_txt': response_txt,
            'finish_reason': finish_reason}
        new_row = pd.dataframe([new_row])
        df = pd.concat([df, new_row], axis=0, ignore_index=true)


df.to_csv(""out_openai_completion.csv"", index=false)


no
typeerror                                 traceback (most recent call last)
 in <cell line: 16>()
33         )
34
---> 35         finish_reason = response['choices'][0]['finish_reason']
36         response_txt = response['choices'][0]['text']
37
typeerror: 'completion' object is not subscriptable","['python', 'openai-api']",77464160,"it should be
response.choices[0].finish_reason
response.choices[0].text",https://stackoverflow.com/questions/77461195,python,10-11-2023 15:39,6650.0,0.0,1.0,True,11-11-2023 09:36,10-11-2023 17:10
77599445,chat gpt 4 api connect with php using file upload feature,"i am trying to connect to gpt-4 api using php. i have a paid account and a valid api key. i use the api key all the time, with gpt-3.5.-turbo model.
i have created an assistant in my account. i want to connect to that assistant and create a new thread for each user that uploads a file. for each file uploaded, i need to return 3 email subjects regarding the file content.
here is my code:
$api_key = 'sk-xx'; // replace with your actual openai api key
$assistant_id = 'asst_xx'; // your assistant id

// handle file upload from the form
$file_path = ""test.txt"";

// step 1: upload the file
$ch = curl_init('
curl_setopt($ch, curlopt_returntransfer, true);
curl_setopt($ch, curlopt_post, true);
curl_setopt($ch, curlopt_ array('authorization: bearer ' . $api_key));
curl_setopt($ch, curlopt_postfields, array(
    'purpose' => 'assistants',
    'file' => new curlfile($file_path),
));
$response_upload = curl_exec($ch);
$info_upload = curl_getinfo($ch);
curl_close($ch);

$file_response = json_decode($response_upload, true);

if (isset($file_response['id'])) {
    $file_id = $file_response['id'];

    // step 2: create a new thread with the user's message and the attached file
    $ch = curl_init("" // include assistant_id as a url parameter
    curl_setopt($ch, curlopt_returntransfer, true);
    curl_setopt($ch, curlopt_ [
        'content-type: application/json',
        'authorization: bearer ' . $api_key,
        'openai-beta: assistants=v1'
    ]);
    curl_setopt($ch, curlopt_post, 1);
    curl_setopt($ch, curlopt_postfields, json_encode([
        'messages' => [
            [
                'role' => 'user',
                'content' => 'i want 3 email subjects based on the content of the uploaded file',
                'file_ids' => [$file_id]
            ]
            
        ]
    ]));

    $thread_creation_response = curl_exec($ch);
    curl_close($ch);

    // output the response for debugging
    print_r($file_response);
    print_r($thread_creation_response);
    $thread_creation_data = json_decode($thread_creation_response, true);
    echo 'thread-id: ' . $thread_creation_data['id'] . '<br>';

    // step 3: get the assistant's response
    $ch = curl_init(""
    curl_setopt($ch, curlopt_returntransfer, true);
    curl_setopt($ch, curlopt_ [
        'authorization: bearer ' . $api_key,
        'openai-beta: assistants=v1',
        'content-type: application/json', // add this line to specify the content type
    ]);
    curl_setopt($ch, curlopt_post, 1);
    curl_setopt($ch, curlopt_postfields, json_encode([
        'role' => 'user',
        'content' => 'i want 3 email subjects based on the content of the uploaded file',
        'file_ids' => [$file_id]
    ]));

    $thread_response = curl_exec($ch);
    curl_close($ch);

    // decode the json response from the assistant
    $thread_data = json_decode($thread_response, true);

    // extract and display the entire assistant's response
    echo ""assistant's response:<br>"";
    print_r($thread_data);


} else {
    // handle the case where the file upload response does not contain the expected data
    echo ""error uploading file."";
}

the output is:
array ( 
    [object] => file 
    [id] => file-yyy 
    [purpose] => assistants 
    [filename] => test.txt 
    [bytes] => 1174 
    [created_at] => 1701688272 
    [status] => processed 
    [status_details] => 
    ) 
    { 
        ""id"": ""thread_mmm"", 
        ""object"": ""thread"", 
        ""created_at"": 1701688274,
        ""metadata"": {} 
    }
    thread-id: thread_mmm 

assistant's response:
array ( 
    [id] => msg_uuu 
    [object] => thread.message 
    [created_at] => 1701688275 
    [thread_id] => thread_mmm 
    [role] => user 
    [content] => array ( 
        [0] => array ( 
            [type] => text 
            [text] => array ( 
                [value] => i want 3 email subjects based on the content of the uploaded file 
                [annotations] => array ( ) 
            ) 
        ) 
    ) 
    [file_ids] => array ( 
        [0] => file-yyy 
    ) 
    [assistant_id] => 
    [run_id] => 
    [metadata] => array ( ) 
)

it seems to upload the file, returns the file id, creates a new thread, it returns the thread id but then it does not return the 3 email subjects that i asked for.
please help!
thank you!","['php', 'openai-api', 'chatgpt-api', 'gpt-4', 'chat-gpt-4']",77600354,"got it! the 'run' link should be re-runned (maybe with a sleep of 2-3 seconds) until status is 'completed', or 'failed' or 'cancelled' or 'expired'.
then, if 'completed' (means everything ok), retrieve messages from "" and it should contain the required output.
thank you!",https://stackoverflow.com/questions/77599445,php,04-12-2023 12:28,2071.0,0.0,1.0,True,04-12-2023 14:48,04-12-2023 13:31
45605946,how to do text pre-processing using spacy?,"how to do preprocessing steps like stopword removal , punctuation removal , stemming and lemmatization in spacy using python.
i have text data in csv file like paragraphs and sentences. i want to do text cleaning. 
kindly give example by loading csv in pandas dataframe","['python', 'nlp', 'spacy']",49248776,"this may help:
import spacy #load spacy
nlp = spacy.load(""en"", disable=['parser', 'tagger', 'ner'])
stops = stopwords.words(""english"")

def normalize(comment, lowercase, remove_stopwords):
    if lowercase:
        comment = comment.lower()
    comment = nlp(comment)
    lemmatized = list()
    for word in comment:
        lemma = word.lemma_.strip()
        if lemma:
            if not remove_stopwords or (remove_stopwords and lemma not in stops):
                lemmatized.append(lemma)
    return "" "".join(lemmatized)


data['text_after_clean'] = data['text'].apply(normalize, lowercase=true, remove_stopwords=true)",https://stackoverflow.com/questions/45605946,python,10-08-2017 06:24,38301.0,18.0,5.0,True,24-02-2023 17:15,10-08-2017 10:02
76779241,"openai chat completions api error: &quot;statuscode: 429, reasonphrase: &#39;too many requests&#39;&quot;","i am passing requests to the openai api with .net core web api. i am getting an error. i have a balance in my openai account.
code:
public async task<string> sendpromptandgetresponse()
    {
        const string requesturi = ""
        var requestbody = new
        {
            model = ""gpt-3.5-turbo"",
            messages = ""how are you?"",
            temperature = 0,
            max_tokens = 100
        };

        _ =
            new system.net. apikey);

        var response = await _
            requesturi,
            new stringcontent(jsonconvert.serializeobject(requestbody), encoding.utf8, ""application/json""));

        response.ensuresuccessstatuscode();

        var responsebody = jsonconvert.deserializeobject<responsebody>(await response.content.readasstringasync());
        return responsebody.choices[0].message.content.trim();
    }

error:
statuscode: 429, reasonphrase: 'too many requests'","['.net-core', 'openai-api', 'chatgpt-api']",76780561,"problem
you didn't set the messages parameter correctly. the role and content properties of the messages parameter are required.
wrong:
messages = ""how are you?"",

see the official openai documentation.


solution
correct:
messages = new[] {
  new { role = ""system"", content =  ""you are a helpful assistant."" },
  new { role = ""user"", content =  ""hello!"" },
},


working example
also, this blog post might help you.
disclaimer: all credit for the code and screenshot below goes to the author of the blog, ricardo mauro.
step 1: install standard.ai.openai c# library
dotnet add standard.ai.openai

step 2: create an openai account
step 3: create the class openaiproxy
using standard.ai.openai.models.services.foundations.chatcompletions;

namespace consoleappopenai;

public interface iopenaiproxy
{
  task<chatcompletionmessage[]> sendchatmessage(string message);
}

step 4: create the implementation class openaiproxy.cs
using standard.ai.openai.clients.openais;
using standard.ai.openai.models.configurations;
using standard.ai.openai.models.services.foundations.chatcompletions;

namespace consoleappopenai;

public class openaiproxy : iopenaiproxy
{
  readonly openaiclient openaiclient;

    //all messages in the conversation
  readonly list<chatcompletionmessage> _messages;

  public openaiproxy(string apikey, string organizationid)
  {
    //initialize the configuration with api key and sub
    var openaiconfigurations = new openaiconfigurations
    {
      apikey = apikey,
      organizationid = organizationid
    };

    openaiclient = new openaiclient(openaiconfigurations);

    _messages = new();
  }

  void stackmessages(params chatcompletionmessage[] message)
  {
    _messages.addrange(message);
  }

  static chatcompletionmessage[] tocompletionmessage(
    chatcompletionchoice[] choices)
    => choices.select(x => x.message).toarray();

  //public method to send messages to openai
  public task<chatcompletionmessage[]> sendchatmessage(string message)
  {
    var chatmsg = new chatcompletionmessage() 
    { 
      content = message, 
      role = ""user"" 
    };
    return sendchatmessage(chatmsg);
  }
    
  //where business happens
  async task<chatcompletionmessage[]> sendchatmessage(
    chatcompletionmessage message)
  {
    //we should send all the messages
    //so we can give open ai context of conversation
    stackmessages(message);

    var chatcompletion = new chatcompletion
    {
      request = new chatcompletionrequest
      {
        model = ""gpt-3.5-turbo"",
        messages = _messages.toarray(),
        temperature = 0.2,
        maxtokens = 800
      }
    };

    var result = await openaiclient
      .chatcompletions
      .sendchatcompletionasync(chatcompletion);

    var choices = result.response.choices;

    var messages = tocompletionmessage(choices);

    //stack the response as well - everything is context to open ai
    stackmessages(messages);

    return messages;
  }
}

step 5: set up the api key
iopenaiproxy chatopenai = new openaiproxy(
    apikey: ""your-api-key"",
    organizationid: ""your-organization-id"");

step 6: use the chat gpt model in our application
var msg = console.readline();

do
{
  var results = await chatopenai.sendchatmessage(msg);

  foreach (var item in results)
  {
    console.writeline($""{item.role}: {item.content}"");
  }
  
  console.writeline(""next prompt:"");
  msg = console.readline();
  
} while (msg != ""bye"");

screenshot:",https://stackoverflow.com/questions/76779241,.net-core,27-07-2023 11:21,1813.0,0.0,1.0,True,12-06-2024 16:49,12-06-2024 16:49
44786888,"will word2vec be more efficient in text based plagiarism detection than wordnet or any other word embeddings like glove, fasttext etc?","i am a beginner in learning word2vec and just started to do some study on word2vec from the internet. i have gone through almost all the questions in quora and stackoverflow but didn't get my answer anywhere from the previous questions. so my question is-

is it possible to apply word2vec in plagiarism detection?
if yes, then will word2vec be more efficient in text-based plagiarism detection than wordnet or any other word embeddings like glove, fasttext, etc?

thanks in advance.","['nlp', 'wordnet', 'word2vec', 'word-embedding', 'plagiarism-detection']",44800929,"yes, these ""dense embedding"" models of word meaning like word2vec may be useful in plagiarism detection. (they're also likely useful in obfuscating plagiarism from simple detectors, as they can assist automated transforms on existing text that change the words while keeping the meaning similar.)
only by testing within a particular system and with respect to quantitative evaluations will you know for sure how well it can work, or whether a particular embedding is better or worse than something like wordnet. 
among word2vec, fastttext, and glove, results will probably be very similar ï¿½ï¿½ï¿½ they all use roughly the same info (word co-occurrences within a sliding context window) to make maximally-predictive word-vectors ï¿½ï¿½ï¿½ so they behave very similarly with similar training data. 
any differences are subtle ï¿½ï¿½ï¿½ the non-glove options might work better for very larger vocabularies; fasttext is essentially the word2vec in some modes, but adds new options for either modeling subword ngrams (which can theetter-than-random vectors for future out-of-vocabulary words) or optimizing the vectors for classification problems. 
but the vectors for known words, which can be trained with plentiful training data, are going to be very similar in capabilities if the training processes are similarly meta-optimized for your task.",https://stackoverflow.com/questions/44786888,nlp,27-06-2017 17:56,1254.0,1.0,1.0,True,05-04-2023 14:18,05-04-2023 14:18
72555117,read file lines and merge them based on their length,"edit here is a a text file:

i  have a file test_file.txt  that consists of lines of various length sizes (number of words).
i want to load each line; check its length, if the length is more than or equal >= a minimum threshold (say 20 words), then i append that line to the list named container: container =  [].
else, i will have to load another line, and merge it with the current line till i reach that desired length size, then append the resulted line merge into the list container. i will have to do that for all the lines in the file.
here is my code, it works until the last two lines, it ignores them.
# creating a generator to load file lines, one by one:

def gen_file_reader(file_path):
    with open(file_path, encoding='utf-8') as file:
        for line in file.readlines():
            yield line

container = [] # list that will contain the results
lines = gen_file_reader('test_file.txt') # calling the generator function


x = """"
for line in lines:
    while len(x.split()) < 20:
        x = x + line
        break
    else:
        container.append(x)
        x = """"
        container.append(line)

i noticed my code doesn't work for the last two lines in the file, maybe because of the break keyword in the while statement ... there could be other bugs i am not aware of!
edit: the end result for the example file (assuming we go rid of blank and empty lines), would look like this for the first 4 items in the list container:
[""project gutenberg's the beautiful and damned, by f. scott fitzgerald this ebook is for the use of anyone anywhere at no cost and with"",
 'almost no restrictions whatsoever.  you may copy it, give it away or re-use it under the terms of the project gutenberg license included',
 'with this ebook or online at  title: the beautiful and damned author: f. scott fitzgerald release date: october 22, 2003 [ebook #9830]',
 'last updated: january 29, 2020 language: english character set encoding: utf-8 *** start of this project gutenberg ebook the beautiful and damned ***']","['python', 'while-loop', 'nlp', 'generator']",72555259,"in your logic, if your trailing line cannot be concatenated to contain 20 or more words, it will not be added to the container
and i think it's better to do the merge logic directly in the generator
def gen_file_reader(file_path):
    with open(file_path, encoding='utf-8') as file:
        for line in file:
            try:
                while len(line.split()) < 20:
                    line += next(file)
                yield line
            except stopiteration:
                yield line


lines = gen_file_reader('test_file.txt')  # calling the generator function
print(list(lines))


attached my test_file.txt
my name is cn-lanbao my name is cn-lanbao my name is cn-lanbao
how are you my name is cn-lanbao my name is cn-lanbao my name is cn-lanbao my name is cn-lanbao
my name is cn-lanbao my name is cn-lanbao
my name is cn-lanbao my name is cn-lanbao
my name is how are you
my name is how are you
my name is cn-lanbao
my name is how are you

and output
['my name is cn-lanbao my name is cn-lanbao my name is cn-lanbao\nhow are you my name is cn-lanbao my name is cn-lanbao my name is cn-lanbao my name is cn-lanbao\n', 'my name is cn-lanbao my name is cn-lanbao\nmy name is cn-lanbao my name is cn-lanbao\nmy name is how are you\n', 'my name is how are you\nmy name is cn-lanbao\nmy name is how are you\n']",https://stackoverflow.com/questions/72555117,python,09-06-2022 05:28,114.0,0.0,1.0,True,09-06-2022 06:17,09-06-2022 06:17
71062589,python make clusters of synonyms,"i have a long list of words :
verbs = ['be','have', 'find', 'use', 'show', 'increase', 'detect', 'do', 'determine', 'demonstrate', 'observe','suggest', ...]

and i want to make clusters from these words based on which ones are synonyms (semantically close). i want to compare each element of the list with all the rest and for those that have a similarity score > 0.7 , group them together. i am using wordnet but i keep getting this error:
for i, verb in enumerate(verbs):
    for j in range(i + 1, len(verbs)):
        verbs[i].wup_similarity(verbs[j])


    error message : 
    ---->        verbs[i].wup_similarity(verbs[j])
    ---->        attributeerror: 'str' object has no attribute 'wup_similarity'

maybe that's not even the right approach, but can anyone help?","['python', 'nlp', 'cluster-analysis', 'wordnet']",71062804,"regarding the updated question, this solution works on my machine.
verbs = ['be','have', 'find', 'use', 'show', 'increase', 'detect', 'do', 'determine', 'demonstrate', 'observe','suggest']

for i, verb in enumerate(verbs):
    for j in range(i + 1, len(verbs)):
        v1 = wordnet.synset(verbs[i]+ '.v.01')
        v2 = wordnet.synset(verbs[j]+ '.v.01')
        wup_score = v1.wup_similarity(v2)
        if wup_score > 0.7:
            print(f""{verbs[i]} and {verbs[j]} are similar"")
            #or do whatever you want to do with similar words.

regarding the original question:
i'am no expert in this, so maybe this does not help at all. currently you do str.wup_similarity(str). however according to this documentation (search for 'wup_similarity' on that website) i think it should be synset1.wup_similarity(synset2).
so my proposal would be to do:
for i, verb in enumerate(verbs):
    for j in range(i + 1, len(verbs)):
        for syni in wordnet.synsets(verb[i]):
            for synj in wordnet.synsets(verb[j]):
                for li in syni.lemmas():
                    for lj in synj.lemmas():
                        v1 = wordnet.synset(verbs[i]+ '.v.01')
                        v2 = wordnet.synset(verbs[j]+ '.v.01')
                        v1.wup_similarity(v2)",https://stackoverflow.com/questions/71062589,python,10-02-2022 09:24,460.0,2.0,1.0,True,10-02-2022 10:23,10-02-2022 10:08
63747454,is there a way to optimize spacy training?,"i'm currently training a spacy model for multi-label text classification. there are 6 labels: anger, anticipation, disgust, fear, joy, sadness, surprise and trust. the dataset is over 200k. however, per epoch is taking 4 hours. i was wondering if there's a way to optimize the training and do it faster, maybe i'm skipping something here that can improve the model.

training_data
train_data = list(zip(train_texts, [{""cats"": cats} for cats in final_train_cats]))

[...
  {'cats': {'anger': 1,
    'anticipation': 0,
    'disgust': 0,
    'fear': 0,
    'joy': 0,
    'sadness': 0,
    'surprise': 0,
    'trust': 0}}),
 ('mausoleum',
  {'cats': {'anger': 1,
    'anticipation': 0,
    'disgust': 0,
    'fear': 0,
    'joy': 0,
    'sadness': 0,
    'surprise': 0,
    'trust': 0}}),
 ...]

training
nlp = spacy.load(""en_core_web_sm"")
category = nlp.create_pipe(""textcat"", config={""exclusive_classes"": true})
nlp.add_pipe(category)

# add label to text classifier
category.add_label(""trust"")
category.add_label(""fear"")
category.add_label(""disgust"")
category.add_label(""surprise"")
category.add_label(""anticipation"")
category.add_label(""anger"")
category.add_label(""joy"")

optimizer = nlp.begin_training()
losses = {}

for i in range(100):
    random.shuffle(train_data)

    print('...')
    for batch in minibatch(train_data, size=8):
        texts = [nlp(text) for text, entities in batch]
        annotations = [{""cats"": entities} for text, entities in batch]
        nlp.update(texts, annotations, sgd=optimizer, losses=losses)
    print(i, losses)

...
0 {'parser': 0.0, 'tagger': 27.018985521040854, 'textcat': 0.0, 'ner': 0.0}
...
1 {'parser': 0.0, 'tagger': 27.01898552104131, 'textcat': 0.0, 'ner': 0.0}
...","['python', 'performance', 'machine-learning', 'spacy']",63760442,"""200k-record dataset is taking 4 hours per-epoch"" doesn't tell us much:

make sure you're not blowing out memory (are you?) how much ram is it taking?
you are presumably running single-thread, due to the gil. see e.g. this on how to turn off the gil to run training multicore. how many cores do you have?


putting texts = [nlp(text) ...] inside your inner-loop for batch in minibatch(train_data, size=8): looks like trouble, because your code will always hold the gil, even though you only need it for c-library string calls on processing the input text i.e. the parser stage, not for training.
refactor your code so you first run nlp() pipeline on all your input, then save some intermediate representation (array or whatever). keep that code separate to your training loop, so training can be multithreaded.


i can't comment on your choice of minibatch() parameters, but 8 seems very small, and those parameters seem to matter for performance, so try tweaking them (/grid-search a few values).
finally, once you first check all of the above, find the fastest unicore/multicore box you can, and with enough ram.",https://stackoverflow.com/questions/63747454,python,04-09-2020 20:08,1324.0,1.0,1.0,True,13-12-2021 23:02,06-09-2020 03:30
10851959,how can i match words regardless of tense or form?,"i am currently working on a script that runs through a document, pulls out all keywords, and then attempts to match these keywords with those found in other documents. there are some specifics that complicate this, but they are not very pertinent to me question. basically i would like to be able to match words regardless of the tense in which they appear. 
for example: if given the strings ""swim"", ""swam"", and ""swimming"", i would like a program that can recognize that these are all the same word, though whether it would store the word as swim, swam or swimming doesn't matter all that much to me.
i'm aware that this problem could be mostly solved with a dictionary containing all of these word forms, but i am unaware of any dictionary that is mapped in such a way to be useful for this. i would prefer a solution or library that is compatible with python, since that is what i am currently using for this scripting, but i would be fine with a solution in just about any language (save haskell or eiffel or something similarly obscure/difficult to work with)","['python', 'nlp', 'nltk', 'string-matching']",10852024,"check out pywordnet.
>>> n['dog']
dog(n.)
>>> n['dog'].getsenses()
('dog' in {noun: dog, domestic dog, canis familiaris},
 'dog' in {noun: frump, dog}, 'dog' in {noun: dog},
 'dog' in {noun: cad, bounder, blackguard, dog, hound, heel},
 'dog' in {noun: pawl, detent, click, dog},
 'dog' in {noun: andiron, firedog, dog, dogiron})


edit: this library was discontinued in 2006 when it was merged into nltk",https://stackoverflow.com/questions/10851959,python,01-06-2012 14:17,1595.0,5.0,3.0,True,21-11-2023 21:00,02-06-2012 02:00
40826165,splitting words using nltk module in python,"i am trying to find a way for splitting words in python using the nltk module. i am unsure how to reach my goal given the raw data i have which is a list of tokenized words e.g.
['usingvariousmolecularbiology', 'techniques', 'toproduce', 'genotypes', 'following', 'standardoperatingprocedures', '.', 'operateandmaintainautomatedequipment', '.', 'updatesampletrackingsystemsandprocess', 'documentation', 'toallowaccurate', 'monitoring', 'andrapid', 'progression', 'ofcasework']

as you can see many words are stuck together (i.e. 'to' and 'produce' are stuck in one string 'toproduce'). this is an artifact of scraping data from a pdf file and i would like to find a way using the nltk module in python to split the stuck-together words (i.e. split 'toproduce' into two words: 'to' and 'produce'; split 'standardoperatingprocedures' into three words: 'standard', 'operating', 'procedures').
i appreciate any help!","['python', 'nltk', 'text-analysis', 'text-processing']",40835052,"i believe you will want to use word segmentation in this case, and i am not aware of any word segmentation features in the nltk that will deal with english sentences without spaces. you could use pyenchant instead. i offer the following code only by way of example. (it would work for a modest number of relatively short strings--such as the strings in your example list--but would be highly inefficient for longer strings or more numerous strings.) it would need modification, and it will not successfully segment every string in any case.
import enchant  # pip install pyenchant
eng_dict = enchant.dict(""en_us"")

def segment_str(chars, exclude=none):
    """"""
    segment a string of chars using the pyenchant vocabulary.
    keeps longest possible words that account for all characters,
    and returns list of segmented words.

    :param chars: (str) the character string to segment.
    :param exclude: (set) a set of string to exclude from consideration.
                    (these have been found previously to lead to dead ends.)
                    if an excluded word occurs later in the string, this
                    function will fail.
    """"""
    words = []

    if not chars.isalpha():  # don't check punctuation etc.; needs more work
        return [chars]

    if not exclude:
        exclude = set()

    working_chars = chars
    while working_chars:
        # iterate through segments of the chars starting with the longest segment possible
        for i in range(len(working_chars), 1, -1):
            segment = working_chars[:i]
            if eng_dict.check(segment) and segment not in exclude:
                words.append(segment)
                working_chars = working_chars[i:]
                break
        else:  # no matching segments were found
            if words:
                exclude.add(words[-1])
                return segment_str(chars, exclude=exclude)
            # let the user know a word was missing from the dictionary,
            # but keep the word
            print('""{chars}"" not in dictionary (so just keeping as one segment)!'
                  .format(chars=chars))
            return [chars]
    # return a list of words based on the segmentation
    return words

as you can see, this approach (presumably) mis-segments only one of your strings:
>>> t = ['usingvariousmolecularbiology', 'techniques', 'toproduce', 'genotypes', 'following', 'standardoperatingprocedures', '.', 'operateandmaintainautomatedequipment', '.', 'updatesampletrackingsystemsandprocess', 'documentation', 'toallowaccurate', 'monitoring', 'andrapid', 'progression', 'ofcasework']
>>> [segment(chars) for chars in t]
""genotypes"" not in dictionary (so just keeping as one segment)
[['using', 'various', 'molecular', 'biology'], ['techniques'], ['to', 'produce'], ['genotypes'], ['following'], ['standard', 'operating', 'procedures'], ['.'], ['operate', 'and', 'maintain', 'automated', 'equipment'], ['.'], ['updates', 'ample', 'tracking', 'systems', 'and', 'process'], ['documentation'], ['to', 'allow', 'accurate'], ['monitoring'], ['and', 'rapid'], ['progression'], ['of', 'casework']]

you can then use chain to flatten this list of lists:
>>> from itertools import chain
>>> list(chain.from_iterable(segment_str(chars) for chars in t))
""genotypes"" not in dictionary (so just keeping as one segment)!
['using', 'various', 'molecular', 'biology', 'techniques', 'to', 'produce', 'genotypes', 'following', 'standard', 'operating', 'procedures', '.', 'operate', 'and', 'maintain', 'automated', 'equipment', '.', 'updates', 'ample', 'tracking', 'systems', 'and', 'process', 'documentation', 'to', 'allow', 'accurate', 'monitoring', 'and', 'rapid', 'progression', 'of', 'casework']",https://stackoverflow.com/questions/40826165,python,27-11-2016 05:52,4766.0,1.0,2.0,True,09-11-2021 18:45,27-11-2016 06:44
49710537,pytorch / gensim - how do i load pre-trained word embeddings?,"i want to load a pre-trained word2vec embedding with gensim into a pytorch embedding layer.
how do i get the embedding weights loaded by gensim into the pytorch embedding layer?","['python', 'pytorch', 'neural-network', 'gensim', 'word-embedding']",49802495,"i just wanted to report my findings about loading a gensim embedding with pytorch.


solution for pytorch 0.4.0 and newer:

from v0.4.0 there is a new function from_pretrained() which makes loading an embedding very comfortable.
here is an example from the documentation.
import torch
import torch.nn as nn

# floattensor containing pretrained weights
weight = torch.floattensor([[1, 2.3, 3], [4, 5.1, 6.3]])
embedding = nn.embedding.from_pretrained(weight)
# get embeddings for index 1
input = torch.longtensor([1])
embedding(input)

the weights from gensim can easily be obtained by:
import gensim
model = gensim.models.keyedvectors.load_word2vec_format('path/to/file')
weights = torch.floattensor(model.vectors) # formerly syn0, which is soon deprecated

as noted by @guglie: in newer gensim versions the weights can be obtained by model.wv:
weights = model.wv



solution for pytorch version 0.3.1 and older:

i'm using version 0.3.1 and from_pretrained() isn't available in this version.
therefore i created my own from_pretrained so i can also use it with 0.3.1.
code for from_pretrained for pytorch versions 0.3.1 or lower:
def from_pretrained(embeddings, freeze=true):
    assert embeddings.dim() == 2, \
         'embeddings parameter is expected to be 2-dimensional'
    rows, cols = embeddings.shape
    embedding = torch.nn.embedding(num_embeddings=rows, embedding_dim=cols)
    embedding.weight = torch.nn.parameter(embeddings)
    embedding.weight.requires_grad = not freeze
    return embedding

the embedding can be loaded then just like this:
embedding = from_pretrained(weights)

i hope this is helpful for someone.",https://stackoverflow.com/questions/49710537,python,07-04-2018 18:21,61772.0,52.0,6.0,True,23-01-2023 05:16,23-01-2023 05:16
75679788,how is an object of type basemodeloutput in huggingface transformer library subscriptable?,"i have a code where self.encoder is an object of type vitencoder. vitencoder's forward method returns an object of type basemodeloutput.
when i call the forward method of vitencoder, i get back an object that is subscriptable, how is this possible?
# self.encoder defined in the init method of class vitmodel
self.encoder = vitencoder(config)

# code snippet in the forward method of class vitmodel
encoder_outputs = self.encoder(
            embedding_output,
            head_mask=head_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
sequence_output = encoder_outputs[0] # how is this subscriptable?
print(f""the encoder output is {encoder_outputs}"") # prints the type of object as basemodeloutput

this is the definition of basemodeloutput class.",['huggingface-transformers'],75688750,"the basemodeloutput class inherits from modeloutput which implements _getitem_:
from typing import list
from dataclasses import dataclass

@dataclass
class x():
  bla:list[str] =none
  def __getitem__(self, i:int):
    return self.bla[i]

x = x([""tldr"", ""no"", ""yes""])
print(x[1])

output:
no",https://stackoverflow.com/questions/75679788,huggingface-transformers,09-03-2023 01:58,398.0,1.0,1.0,True,09-03-2023 18:36,09-03-2023 02:04
75804599,openai api: how do i count tokens before(!) i send an api request?,"openai's text models have a context length, e.g.: curie has a context length of 2049 tokens.
they provide max_tokens and stop parameters to control the length of the generated sequence. therefore the generation stops either when stop token is obtained, or max_tokens is reached.
the issue is: when generating a text, i don't know how many tokens my prompt contains. since i do not know that, i cannot set max_tokens = 2049 - number_tokens_in_prompt.
this prevents me from generating text dynamically for a wide range of text in terms of their length. what i need is to continue generating until the stop token.
my questions are:

how can i count the number of tokens in python api so that i will set max_tokens parameter accordingly?
is there a way to set max_tokens to the max cap so that i won't need to count the number of prompt tokens?","['python', 'openai-api', 'chatgpt-api', 'gpt-3', 'gpt-4']",75804651,"how do i count tokens before(!) i send an api request?
as stated in the official openai article:

to further explore tokenization, you can use our interactive tokenizer
tool, which allows you to calculate the number of tokens and see how
text is broken into tokens. alternatively, if you'd like to tokenize
text programmatically, use tiktoken as a fast bpe tokenizer
specifically used for openai models.

how does a tokenizer work?
a tokenizer can split the text string into a list of tokens, as stated in the official openai example on counting tokens with tiktoken:

tiktoken is a fast open-source tokenizer by openai.
given a text string (e.g., ""tiktoken is great!"") and an encoding
(e.g., ""cl100k_base""), a tokenizer can split the text string into a
list of tokens (e.g., [""t"", ""ik"", ""token"", "" is"", "" great"", ""!""]).
splitting text strings into tokens is useful because gpt models see
text in the form of tokens. knowing how many tokens are in a text
string can tell you:

whether the string is too long for a text model to process and
how much an openai api call costs (as usage is priced by token).


which encodings does openai use for its models?
as of april 2024, tiktoken supports 2 encodings used by openai models (source 1, source 2):



encoding name
openai models




o200k_base
ï¿½ï¿½ï¿½ gpt-4o models (gpt-4o)


cl100k_base
ï¿½ï¿½ï¿½ gpt-4 models (gpt-4)ï¿½ï¿½ï¿½ gpt-3.5 turbo models (gpt-3.5-turbo)ï¿½ï¿½ï¿½ gpt base models (davinci-002, babbage-002)ï¿½ï¿½ï¿½ embeddings models (text-embedding-ada-002, text-embedding-3-large, text-embedding-3-small)ï¿½ï¿½ï¿½ fine-tuned models (ft:gpt-4, ft:gpt-3.5-turbo, ft:davinci-002, ft:babbage-002)



note: the p50k_base and r50k_base encodings were used for models that are deprecated as of april 2024.
what tokenizer libraries are out there?
official openai libr"" rel=""noreferrer"">tiktoken

3rd-party libraries:

python: gpt2tokenizerfast
node.js: tiktoken, gpt4-tokenizer, gpt3-tokenizer, gpt-3-encoder
.net / c#: tryagi.tiktoken, sharptoken, tiktokensharp, gpt tokenizer
java: jtokkit, gpt2-tokenizer-java
php: gpt-3-encoder-php

how do i use tiktoken?

install or upgrade tiktoken: pip install --upgrade tiktoken
write the code to count tokens, where you have two options.

option 1: search in the table above for the correct encoding for a given openai model
if you run get_tokens_1.py, you'll get the following output:

9

get_tokens_1.py
import tiktoken

def num_tokens_from_string(string: str, encoding_name: str) -> int:
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

print(num_tokens_from_string(""hello world, let's test tiktoken."", ""cl100k_base""))


option 2: use tiktoken.encoding_for_model() to automatically load the correct encoding for a given openai model
if you run get_tokens_2.py, you'll get the following output:

9

get_tokens_2.py
import tiktoken

def num_tokens_from_string(string: str, encoding_name: str) -> int:
    encoding = tiktoken.encoding_for_model(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

print(num_tokens_from_string(""hello world, let's test tiktoken."", ""gpt-3.5-turbo""))

note: if you take a careful look at the usage field in the openai api response, you'll see that it reports 10 tokens used for an identical message. that's 1 token more than tiktoken. i still haven't figured out why. i tested this in the past. as @jota mentioned in the comment below, there still seems to be a mismatch between the token usage reported by the openai api response and tiktoken.",https://stackoverflow.com/questions/75804599,python,21-03-2023 17:35,100762.0,87.0,4.0,True,20-05-2024 09:47,10-04-2024 00:26
72947973,why ml models installed with pip need to download something else after installation?,"why does this statement download the model? why isn't it downloaded when i install the package with pip3 install keybert? how can i pre-load it to the docker image so it wouldn't be downloaded every time?
from keybert import keybert
kw_model = keybert()

right now my dockerfile does the following:
run pip install --user -r requirements.txt

requirements.txt:
google-cloud-pubsub==2.8.0
google-cloud-logging==2.6.0
requests==2.28.0
keybert==0.5.1","['python', 'docker', 'pip', 'bert-language-model']",73033721,"one potential solution is

run this code on your local computer to save a copy of the model to a local directory. e.g. save to a directory ""keybert""

from keybert import keybert
kw_model = keybert()
kw_model.model.embedding_model.save(""keybert"")


add the local copy of the model to the docker image using the copy command in the dockerfile

# copy local code to the container image.
copy ./keybert/ ./keybert/


in your script running in the docker container, load the model from the directory

from keybert import keybert
new_kw_model = keybert(""./keybert"")

the reason for this behavior is that keybert uses other sbert models, and you can use keybert with more than one model: 
so you'd add a copy of whichever model best suits your purposes to the docker image",https://stackoverflow.com/questions/72947973,python,12-07-2022 06:36,320.0,1.0,1.0,True,19-07-2022 10:31,19-07-2022 10:31
75624727,why i am getting less bleu score?,"from nltk.translate.bleu_score import sentence_bleu
reference = [['this', 'is', 'ae', 'test']]
candidate = ['this', 'is', 'ad', 'test']
score = sentence_bleu(reference, candidate)
print(score)

i am using this code to calculate the bleu score and the score i am getting is 1.0547686614863434e-154. i wander why i am getting so small value even only one letter is different in candidate list.
score = sentence_bleu(reference, candidate,weights = [1])

i tried adding weight = [1] as a parameter and it gave me 0.75 as output.  i cant understand why i have to add weight to get a reasonable result. any help would be appreciated.
i thought its maybe because the sentence is not long enough so i added more words:
from nltk.translate.bleu_score import sentence_bleu
reference = [['this', 'is', 'ae', 'test','rest','pep','did']]
candidate = ['this', 'is', 'ad', 'test','rest','pep','did']
score = sentence_bleu(reference, candidate)
print(score)

now i am getting 0.488923022434901 but still i think is too low value.","['python', 'nlp', 'nltk', 'bleu']",75625303,"by default, sentence_bleu is configured with 4 weights: 0.25 for unigram, 0.25 for bigram, 0.25 for trigram, 0.25 for quadrigram. the length of weights give the order of ngram, so the bleu score is computed for 4 levels of ngrams.
when you use weights=[1], you only analyze unigram:
reference = [['this', 'is', 'ae', 'test','rest','pep','did']]
candidate = ['this', 'is', 'ad', 'test','rest','pep','did']

>>> sentence_bleu(reference, candidate)  # default weights, order of ngrams=4
0.488923022434901

but you can also consider unigrams are more important than bigrams which are more important than tri and quadigrams:
>>> sentence_bleu(reference, candidate, weights=[0.5, 0.3, 0.1, 0.1])
0.6511772622175621

you can also use smoothingfunction methods and read the docstring from source code to better understanding.",https://stackoverflow.com/questions/75624727,python,03-03-2023 08:26,2206.0,2.0,2.0,True,03-03-2023 15:04,03-03-2023 08:31
75737683,the argument type &#39;int&#39; can&#39;t be assigned to the parameter type &#39;duration&#39;,"i upgraded my package chat_gpt_sdk to 2.0.4 and the flutter sdk to the latest version but now i get this error the argument type 'int' can't be assigned to the parameter type 'duration'. i need to be on this version to have access to the chatgpt turbo model. can anyone help me with this
      return 'count: $count';
    }).take(10);

    openai = openai.instance.build(
      token: ""$apikey"",
      baseoption: 
        receivetimeout: 30000,
        connecttimeout: 5000,
        sendtimeout: 5000,

      ),
      islogger: true,
    );
  }","['android', 'ios', 'flutter', 'dart', 'openai-api']",75737715,"you need to provide receivetimeout , connecttimeout and  sendtimeout in duration
baseoption: 
  receivetimeout: duration(milliseconds: 30000),
  connecttimeout: duration(seconds: 5),
  sendtimeout: duration(seconds: 5),
),",https://stackoverflow.com/questions/75737683,android,14-03-2023 19:41,2419.0,0.0,2.0,True,14-03-2023 19:46,14-03-2023 19:45
34363250,understanding word2vec&#39;s skip-gram structure and output,"i have a two-fold question about the skip-gram model in word2vec:

the first part is about structure: as far as i understand it, the skip-gram model is based on one neural network with one input weight matrix w, one hidden layer of size n, and c output weight matrices w' each used to produce one of the c output vectors. is this correct?

the second part is about the output vectors: as far as i understand it, each output vector is of size v and is a result of a softmax function. each output vector node corresponds to the index of a word in the vocabulary, and the value of each node is the probability that the corresponding word occurs at that context location (for a given input word). the target output vectors are not, however, one-hot encoded, even if the training instances are. is this correct?


the way i imagine it is something along the following lines (made-up example):
assuming the vocabulary ['quick', 'fox', 'jumped', 'lazy', 'dog'] and a context of c=1, and assuming that for the input word 'jumped' i see the two output vectors looking like this:
[0.2 0.6 0.01 0.1 0.09]
[0.2 0.2 0.01 0.16 0.43]
i would interpret this as 'fox' being the most likely word to show up before 'jumped' (p=0.6), and 'dog' being the most likely to show up after it (p=0.43).
do i have this right? or am i completely off?","['vector', 'machine-learning', 'nlp', 'word2vec']",38483158,"your understanding in both parts seem to be correct, according to this paper :

the paper explains word2vec in detail and at the same time, keeps it very simple - it's worth a read for a thorough understanding of the neural net architecture used in word2vec.

the structure of skip gram does use a single neural net, with input as one-hot encoded target-word and expected-output as one-hot encoded context words. after the neural-net is trained on the text-corpus, the input weight matrix w   is used as the input-vector representations of words in the corpus and the output weight matrix w' which is shared across all the c outputs (output-vectors in the terminology of the question, but avoiding that to prevent confusion with output-vector representations used next..), becomes the output-vector representations of words. usually the output-vector representations are ignored, and the input-vector representations, w are used as the word embeddings. to get into the dimensionality of the matrices, if we assume a vocabulary size of v, size of hidden layer as n, we will have w as (v,n) matrix, with each row representing the input vector of the indexed word in the vocabulary. w' will be a (n,v) matrix, with each column representing the output vector of the indexed word. in this way we get n-dimensional vectors for words.
as you mentioned, each of the outputs(avoiding using the term output vector) is of size v and are the result of a softmax function, with each node in the output giving the probability of the word occurring as a context word for the given target word, resulting in the outputs not being one-hot encoded.but the expected outputs are indeed one-hot encoded, i.e in training phase, the error is computed by subtracting the one-hot encoded vector of the actual word occurring at that context position, from the neural-net output and then the weights are updated using gradient descent.

referring to the example you mentioned, with c=1 and with a vocabulary of ['quick', 'fox', 'jumped', 'lazy', 'dog'].
if the output from the skip-gram is [0.2 0.6 0.01 0.1 0.09], where the correct target word is 'fox' then error is calculated as:
[0 1 0 0 0] - [0.2 0.6 0.01 0.1 0.09] = [-0.2 0.4 -0.01 -0.1 -0.09]

and the weight matrices are updated to minimize this error.",https://stackoverflow.com/questions/34363250,vector,18-12-2015 20:12,4516.0,6.0,2.0,True,05-03-2025 13:45,05-03-2025 13:45
72193062,sci-kit tf-idf - unsure of interpretation of td-idf array?,"i have a subset of a dataframe like:
<out>
pagenumber    top_words_only
56            people sun flower festival 
75            sunflower sun architecture red buses festival

i want to calculate tf-idf on the english_tags df column with each row acting as a document. i have tried:
vectorizer = tfidfvectorizer(lowercase = true, max_df = 0.8, min_df = 5, stop_words = 'english')
vectors = vectorizer.fit_transform(df['top_words_only'])

if i print the array it comes out as:
array([[0.        , 0.        , 0.        , ..., 0.        , 0.35588179,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       ...,
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ]])

but i am a little confused by what this means - why are there so many o values? does implementing tfidfvectorizer() automatically calculate the tf-idf values for each tag taking into account all documents (i.e. corpus)?","['python', 'scikit-learn', 'nlp', 'tf-idf']",72193798,"calling fit_transform calculates a vector for each supplied document. each vector will be the same size. the size of the vector is the number of unique words across the supplied documents. the number of zero values in the vector will be the vector size - number of unique values in the document.
using your top_words as a simple example. you show 2 documents:
'people sun flower festival'
'sunflower sun architecture red buses festival'

these have a total of 8 unique words (vectorizer.get_feature_names_out() will give you these):
'architecture', 'buses', 'festival', 'flower', 'people', 'red', 'sun', 'sunflower'

calling fit_transform with those 2 documents will give 2 vectors (1 for each doc), each with length 8 (number of unique words across the documents).
the first document, 'people sun flower festival' has 4 words, so, you get 4 values in the vector, and 4 zeros. similarly 'sunflower sun architecture red buses festival' gives 6 values and 2 zeros.
the more documents you pass in with different words, the longer the vector gets, and the more likely the zeros are.
from sklearn.feature_extraction.text import tfidfvectorizer

top_words = ['people sun flower festival', 'sunflower sun architecture red buses festival']

vectorizer = tfidfvectorizer()
vectors = vectorizer.fit_transform(top_words)

print(f'feature names: {vectorizer.get_feature_names_out().tolist()}')
tfidf = vectors.toarray()
print('')
print(f'top_words[0] = {top_words[0]}')
print(f'tfidf[0] = {tfidf[0].tolist()}')
print('')
print(f'top_words[1] = {top_words[1]}')
print(f'tfidf[1] = {tfidf[1].tolist()}')

the above code will print:
feature names: ['architecture', 'buses', 'festival', 'flower', 'people', 'red', 'sun', 'sunflower']

top_words[0] = people sun flower festival
tfidf[0] = [0.0, 0.0, 0.40993714596036396, 0.5761523551647353, 0.5761523551647353, 0.0, 0.40993714596036396, 0.0]

top_words[1] = sunflower sun architecture red buses festival
tfidf[1] = [0.4466561618018052, 0.4466561618018052, 0.31779953783628945, 0.0, 0.0, 0.4466561618018052, 0.31779953783628945, 0.4466561618018052]",https://stackoverflow.com/questions/72193062,python,10-05-2022 21:18,77.0,0.0,1.0,True,10-05-2022 23:06,10-05-2022 21:23
70965996,custom query with deepstackai haystack,"i am exploring deepset haystack and found it very interesting for multiple use cases like a chatbot, search engine, document search, etc
but have not found any reference where i can create multiple indexes for different documents and search based on indexes. i thought of using meta tags for conditional search(on a particular area) by tagging the documents first and then using the params parameter of query api but the same doesn't seem to work and throws an error(i used its vanilla docker-compose based setup)","['elasticsearch', 'bert-language-model', 'nlp-question-answering', 'haystack']",70967941,"you can use multiple indices in the same document store if you want to support multiple use cases, indeed. the write_documents method of the document store has a parameter index so that you can store documents for your different use cases in different indices. in the same way, you can pass an index parameter to the query method.
as you expected, there is an alternative solution that uses the meta field of documents. however, the format needs to be slightly different. your query needs to have the following format:
{""query"": ""what's the capital town?"", ""params"": {""filters"": {""name"": ""75_algeria75.txt""}}}

and your documents need to have the following format:
{'text': 'algeria is...', 'meta':{'name': ""75_algeria75.txt""}}",https://stackoverflow.com/questions/70965996,elasticsearch,03-02-2022 04:37,709.0,2.0,1.0,True,08-07-2022 16:17,08-07-2022 16:17
77084206,how can i enhance morphological information for english models in spacy?,"i am trying to detect verbs that are in the imperative mood using english models in spacy but i am seeing morphological features that are inconsistent with the examples found in the morphology documentation. this issue is similar to this unanswered extracting english imperative mood from verb tags with spacy question. specifically, there seems to very few mood features identified.
i am not sure if i am missing some configuration or if i need to somehow train the model to better identify morphological features. before i go down the path of training, i'd like to understand why what i am doing is not matching the documentation.
i have written a small example that demonstrates the discrepancy.
'''
prerequisites

pip install spacy
python -m spacy download en_core_web_lg
'''
import spacy

nlp = spacy.load(""en_core_web_lg"")

def show_morph_as_markdown_table(doc):
    print(""|context|token|lemma|pos|tag|morph|"")
    print(""|----|----|----|----|----|----|"")
    for token in doc:
        print(f'|{doc}|{token.text}|{token.lemma_}|{token.pos_}|{token.tag_}|{token.morph.to_dict()}|')

def show_morph_for_sentences_as_markdown_table(sentences):
    sentence_docs = list(nlp.pipe(sentences))
    for sentence_doc in sentence_docs:
        show_morph_as_markdown_table(sentence_doc)

example_sentences = [
    ""i was reading the paper"",
    ""i donï¿½ï¿½ï¿½t watch the news, i read the paper"",
    ""i read the paper yesterday""
]

show_morph_for_sentences_as_markdown_table(example_sentences)

i have trimmed the output to include only rows shown in the morphology documentation.




context
token
lemma
pos
tag
morph




i was reading the paper
reading
read
verb
vbg
{'aspect': 'prog', 'tense': 'pres', 'verbform': 'part'}


i donï¿½ï¿½ï¿½t watch the news, i read the paper
read
read
verb
vbd
{'tense': 'past', 'verbform': 'fin'}


i read the paper yesterday
read
read
verb
vbp
{'tense': 'pres', 'verbform': 'fin'}




this is very different from the expected output of:




context
token
lemma
pos
tag
morph




i was reading the paper
reading
read
verb
vbg
{'verbform': 'ger'}


i donï¿½ï¿½ï¿½t watch the news, i read the paper
read
read
verb
vbd
{'verbform': 'fin', 'mood': 'ind', 'tense': 'pres'}


i read the paper yesterday
read
read
verb
vbp
{'verbform': 'fin', 'mood': 'ind', 'tense': 'past'}




i've tried adding a morphologizer to the pipeline using the default_morph_model but was met with an initialization error. i don't know enough about the pipeline yet to understand why.
from spacy.pipeline.morphologizer import default_morph_model

config = {""model"": default_morph_model}
nlp.add_pipe(""morphologizer"", config=config)

# valueerror: [e109] com 'morphologizer' could not be run. did you forget to call `initialize()`?

# trying to fix above error with the following
nlp.initialize()

# [e955] can't find table(s) lexeme_norm for language 'en' in spacy-lookups-data. make sure you have the package installed or provide your own lookup tables if no default lookups are available for your language.


in researching further, it appears that spacy version 3 manages tag_map and morph_rules with attributeruler. could it be possible that the downloadable models aren't including the same information that the documentation is using?
i'm hoping for an easy configuration fix that i am missing or a pointer to the right rabbit hole (i've been down many).","['nlp', 'pos-tagger', 'spacy-3']",77102380,"this table is the docs is just meant to be a generic example of the kinds of annotation you might see, and the exact annotation from each individual model may be different, also for each individual release/version of a model.
you're not going to have much luck detecting imperatives using the en_core_web_* models because the training data doesn't distinguish imperatives from other forms. the rules that handle the tagset conversion are largely based on this table (note that there's no mood=imp for any ptb tag):

however, it does look like some of the ud english corpora do include mood=imp or use fine-grained tags that distinguish imperatives. to start out, you could test out a pretrained ud english ewt model from a tool like stanza or trankit to see if that works well enough for your task. it can be a difficult distinction to make, so i don't know how good the overall performance might be, though.
if you'd like to keep working with spacy, you could use spacy-stanza with the default ""en"" models, which are trained on ud english ewt.",https://stackoverflow.com/questions/77084206,nlp,11-09-2023 19:05,332.0,1.0,1.0,True,14-09-2023 06:40,11-09-2023 22:11
34427678,&#39;utf-8&#39; decode error when loading a word2vec module,"i have to use a word2vec module containing tons of chinese characters. the module was trained by my coworkers using java and is saved as a bin file. 
i installed gensim and tries to load the module, but following error occurred: 
in [1]: import gensim  

in [2]: model = gensim.models.word2vec.load_word2vec_format('/data5/momo-projects/user_interest_classification/code/word2vec/vectors_groups_1105.bin', binary=true)

unicodedecodeerror: 'utf-8' codec can't decode bytes in position 96-97: unexpected end of data

i tried to load the module both in python 2.7 and 3.5, failed in the same way. so how can i load the module in gensim? thanks.","['python', 'nlp', 'gensim', 'word2vec']",34447464,"the module was tons of chinese characters trained by java. i cannot figure out the encoding format of the original corpus. the error can be solved as the description in gensim faq, 
using load_word2vec_format with a flag for ignoring the character decoding errors:
in [1]: import gensim

in [2]: model = gensim.models.word2vec.load_word2vec_format('/data5/momo-projects/user_interest_classification/code/word2vec/vectors_groups_1105.bin', binary=true, unicode_errors='ignore')

but i've no idea whether it matters when ignoring the encoding errors.",https://stackoverflow.com/questions/34427678,python,23-12-2015 02:24,8572.0,5.0,3.0,True,25-08-2023 06:27,30-12-2015 10:59
77332489,problem installing &quot;openai php sdk&quot; in php project,"i want to use openai requests in my php project. so on the net i found i should install ""openai php sdk"" with the composer require openai/api command.
however, composer gives me this error message: could not find a matching version of package openai/api. check the package spelling, your version constraint and that the package is available in a stability which matches your minimum-stability (dev).
from this i cannot understand what should i do. the package spelling should be ok. the version constraint (>8.1) also.","['php', 'openai-api']",77332548,"you are requiring the wrong package. there isn't any package named openai/api.
you can use these packages to make requests to open ai.
here's the list in case you don't want to login into openai:

orhanerday/open-ai
tectalic/openai
openai-php/client

for example, you can run this command too add the third package: composer require openai-php/client",https://stackoverflow.com/questions/77332489,php,20-10-2023 16:29,831.0,2.0,1.0,True,22-10-2023 22:44,22-10-2023 22:44
76689997,"i have trained a custom transformer model on language modeling, now how do i make predictions with it?","i have trained a transformer model on language modeling (i.e predicting the next character given the context) on a dataset. context_length = 200, i want the model to predict when the input is not of length context_length, so how do i have to modify my code so i can predict on varied input shape and also help me in writing code for a function to generate next characters.
class embed(keras.layers.layer):
    """"""word_embedding + positional_embedding """"""
    def __init__(self):
        super().__init__()
        self.word_embed = keras.layers.embedding(vocab_size, d_model) # (b, t) =(vocab_size, d_model)=> (b, t, d_model)
        self.position_embed = keras.layers.embedding(context_length, d_model) # (b, t) =(context_length, d_model)=> (b, t, d_model)

    def call(self, inputs):
        b, t = inputs.shape # when training context_length = t
        tok_embed = self.word_embed(inputs) # (b, t, d_model)
        pos_embed = self.position_embed(tf.range(t)) # (t, d_model)
        return  tok_embed + pos_embed # (b, t, d_model)
    
    def get_config(self):
        base_config = super().get_config()
        return {**base_config}

class multiheadattention(keras.layers.layer):
    def __init__(self, mask: bool):
        super().__init__()
        self.mask = mask
        self.linear = keras.layers.dense(d_model, use_bias=false)
        self.linearqkv = keras.layers.dense(d_k, use_bias=false), keras.layers.dense(d_k, use_bias=false), keras.layers.dense(d_v, use_bias=false)
        self.dropout = keras.layers.dropout(0.1)
    
    def attention(self, q, k, v):
        def mask_tensor(x):
            tril = tf.experimental.numpy.tril(tf.ones_like(x))
            return tf.where(tril==0, float('-inf'), x)
        scores = q @ tf.transpose(k, perm=[0, 2, 1])/k.shape[-1]**0.5 # (b, t, t)
        scores = mask_tensor(scores) if self.mask else scores
        return tf.nn.softmax(scores, axis=-1) @ v # (b, t, d_v)

    def head(self, x):
        q, k, v = self.linearqkv[0](x), self.linearqkv[1](x), self.linearqkv[2](x)
        return self.attention(q, k, v)

    def call(self, x):
        heads = tf.concat([self.head(x) for _ in range(h)], axis=-1)
        output = self.linear(heads)
        output = self.dropout(output)
        return output
    
    def get_config(self):
        base_config = super().get_config()
        return {**base_config, ""mask"": self.mask}

def feedforward():
    return keras.sequential([
        keras.layers.dense(d_in),
        keras.layers.relu(),
        keras.layers.dense(d_model),
        keras.layers.dropout(0.2)
    ])

inputs = keras.input(shape=(200,))
x = embed()(inputs)
for _ in range(n):
    z = multiheadattention(mask=true)(x)
    x = keras.layers.layernormalization()(z + x)

    z = feedforward()(x)
    x = keras.layers.layernormalization()(z + x)
outputs = keras.layers.dense(vocab_size, activation=""softmax"")(x) # (b, t, vocab_size)

model = keras.model(inputs=inputs, outputs=outputs, name=""transformer"")

i think maybe there's a problem in the embed layer, when adding tok_embed and
pos_embed. i think it can be modified so it can take inputs of varied length.
padding can affect the model performance, so is there any other way?
please help, thank you.
edit:
there was no problem in training, got a good accuracy.","['python', 'tensorflow', 'keras', 'nlp', 'tf.keras']",76718519,"i have changed the code so the transformer model can take inputs of varied length.
def transformer():
    class embed(keras.layers.layer):
        """"""word_embedding + positional_embedding """"""
        def __init__(self, **kwargs):
            super().__init__(**kwargs)
            self.word_embed = keras.layers.embedding(vocab_size, d_model) # (b, t) =(vocab_size, d_model)=> (b, t, d_model)
            self.position_embed = keras.layers.embedding(max_length, d_model) # (b, t) =(max_length, d_model)=> (b, t, d_model)

        def call(self, inputs):
            b, t = inputs.shape # if training, t = max_length
            tok_embed = self.word_embed(inputs) # (b, t, d_model)
            pos_embed = self.position_embed(tf.range(max_length)) # (max_length, d_model) =[:t, :]=> (t, d_model)
            return tok_embed + pos_embed[:t, :] # (b, t, d_model) + (t, d_model) ==> (b, t, d_model)

        def get_config(self):
            base_config = super().get_config()
            return {**base_config}
       
    class multiheadattention(keras.layers.layer):
        def __init__(self, causal: bool, **kwargs):
            super().__init__(**kwargs)
            self.causal = causal
            self.linear = keras.layers.dense(d_model, use_bias=false)
            self.linearqkv = [keras.layers.dense(d_k, use_bias=false),
                              keras.layers.dense(d_k, use_bias=false),
                              keras.layers.dense(d_v, use_bias=false)]
            self.dropout = keras.layers.dropout(0.1)

        def attention(self, q, k, v):
            def mask_tensor(x):
                tril = tf.experimental.numpy.tril(tf.ones_like(x))
                return tf.where(tril==0, float('-inf'), x)
            scores = q @ tf.transpose(k, perm=[0, 2, 1])/k.shape[-1]**0.5 # (b, t, t)
            scores = mask_tensor(scores) if self.causal else scores
            return tf.nn.softmax(scores, axis=-1) @ v # (b, t, d_v)

        def head(self, x):
            q, k, v = self.linearqkv[0](x), self.linearqkv[1](x), self.linearqkv[2](x)
            return self.attention(q, k, v)

        def call(self, x):
            heads = tf.concat([self.head(x) for _ in range(h)], axis=-1)
            output = self.linear(heads)
            return self.dropout(output)

        def get_config(self):
            base_config = super().get_config()
            return {**base_config, ""causal"": self.causal}
        
    def feedforward():
        return keras.sequential([
            keras.layers.dense(d_in),
            keras.layers.relu(),
            keras.layers.dense(d_model),
            keras.layers.dropout(0.1)
        ])
    
    inputs = keras.input(shape=(none,)) # so can take inputs of varied length
    x = embed()(inputs)
    for _ in range(n): # transformer's decoder
        z = multiheadattention(causal=true)(x)
        x = keras.layers.layernormalization()(keras.layers.add()([z, x]))

        z = feedforward()(x)
        x = keras.layers.layernormalization()(keras.layers.add()([z, x]))
    outputs = keras.layers.dense(vocab_size, activation=""softmax"")(x) # (b, t, vocab_size)

    model = keras.model(inputs=inputs, outputs=outputs, name=""transformer"")
    print(""number of parameters in the model"", model.count_params())
    return model

don't use model.predict(...) for generation, use model(..., training=false)
def generate_file(prompt: str, num_char: int, temperature=1):
    def next_char(seq):
        return sentence(tf.argmax(model(np.array(encode(seq))[np.newaxis], training=false)/temperature, axis=-1)[0].numpy().tolist())[-1]
    seq = prompt 
    for i in range(num_char):
        if len(seq) >= max_length: 
            seq += next_char(seq[-(max_length-1):]) # last max_length-1 characters so can predict char at max_length
        elif len(seq) < max_length:
            seq += next_char(seq)
    print(seq)
    with open(ï¿½ï¿½ï¿½machine_generated_text.txtï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½wï¿½ï¿½ï¿½) as f:
        f.w",https://stackoverflow.com/questions/76689997,python,14-07-2023 17:28,211.0,2.0,1.0,True,19-07-2023 13:47,15-07-2023 05:23
77544604,importing deepspeech error: module not found,"i am trying to install deepsearch library so that i can use the pretrained model to build the speech to text project.
modulenotfounderror                       traceback (most recent call last)
<ipython-input-8-a1bcd52700aa> in <cell line: 1>()
----> 1 import deepspeech
      2 
      3 # path to the pre-trained model files
      4 model_path = ""/content/deepspeech-0.9.3-models.pbmm""
      5 scorer_path = ""/content/deepspeech-0.9.3-models.scorer""  # optional, for improved performance

modulenotfounderror: no module named 'deepspeech'


i have got the module not found error, and i tried using version also but it gives me the error of couldn't found the suitable version.
error: could not find a version that satisfies the requirement deepspeech==0.9.3 (from versions: none)
error: no matching distribution found for deepspeech==0.9.3


i tried using deepspeech github directory when i tried to install the setupfile and requirements.txt. i have got the error:
 collecting absl-py==0.9.0
  downloading absl-py-0.9.0.tar.gz (104 kb)
     ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 104.0/104.0 kb 3.3 mb/s eta 0:00:00
  error: subprocess-exited-with-error
  
  ï¿½ï¿½ python setup.py egg_info did not run successfully.
  ï¿½ï¿½ï¿½ exit code: 1
  ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½> see above for output.
  
  note: this error originates from a subprocess, and is likel","['python', 'deep-learning', 'nlp', 'mozilla-deepspeech']",77547213,"latest release of deepspeech was 3 years back in december 2020:

you are not able to install deepsearch because it only supports python <= 3.6  which is outdated. if you want to use, you can download older version of python separately and install deepsearch.",https://stackoverflow.com/questions/77544604,python,24-11-2023 16:48,256.0,1.0,1.0,True,30-10-2024 20:32,25-11-2023 16:45
74357973,how to increase count if function called by another function in python,"i am trying to count all tokens given by func1 and print out the result from func2. the problem is that each time func2 gets called the counter does not update for natural reasons because we call the function over and over again. are there any ways of solving this? note: only func2 is supposed to be changed
import nltk

def func1(tokens):
    listoftokens = tokens.split()
    for token in listoftokens:
        func2(token)

def func2(token):
    count = 0
    nltk_tokens = nltk.word_tokenize(token)
    count += len(nltk_tokens)
    print(count) # expected to be 4 but is printing : 1 1 1 1

func1(""a b c d"")","['python', 'function', 'token', 'nltk', 'counter']",74358027,"you can do like this
import nltk

count = 0
def func1(tokens):
    listoftokens = tokens.split()
    for token in listoftokens:
        func2(token)

def func2(token):
    global count
    nltk_tokens = nltk.word_tokenize(token)
    count += len(nltk_tokens)
    print(count) # expected to be 4 but is printing : 1 1 1 1

func1(""a b c d"")",https://stackoverflow.com/questions/74357973,python,08-11-2022 08:53,53.0,0.0,2.0,True,08-11-2022 09:10,08-11-2022 09:00
78015622,should json doc be split?,"i'm working with movies_data.json that has documents like this:
[
  {
    ""metadata"": {
      ""id"": 580489,
      ""original_title"": ""venom: let there be carnage"",
      ""popularity"": 5401.308,
      ""release_date"": ""2021-09-30"",
      ""vote_average"": 6.8,
      ""vote_count"": 1736,
      ""revenue"": 424000000,
      ""tagline"": """",
      ""poster_url"": ""
      ""adult"": 0
    },
    ""embedded_data"": {
      ""overview"": ""after finding a host body in investigative reporter eddie brock, the alien symbiote must face a new enemy, carnage, the alter ego of serial killer cletus kasady."",
      ""genre"": ""['science fiction', 'action', 'adventure']""
    }
  },
....
]

is it fine to split .json  documents? like in my case i have meta data and embedded data fields. now if i split it then one's meta data might get wrongly associated with other's embedded data.
i've parsed json into string and on splitting, but my data gets dispersed e.g. one's meta data is getting associated with other's embedded data.
it made me questioning how should i split my data in such cases?
so far, my code looks like this:
const loader = new jsonloader(
    ""/input.json""
);

let docs = await loader.load();
// console.log(docs);
docs = json.stringify(docs)

const splitter = new recursivecharactertextsplitter({
    chunksize: 400,
    chunkoverlap: 1
});

const docoutput = await splitter.createdocuments([docs]);

console.log(docoutput);","['langchain', 'langchain-js']",78190718,"you most likely do not want to split the metadata and embedded data of a single movie object. unfortunately, keeping the data together in a single document is not possible to achieve with jsonloader and the format of your json file. the loader will load all strings it finds in the file into a separate document.
here's an approach that will probably achieve what you want:

load the json file into memory and return an array of objects.
iterate through the array and create a document for each object.
at this point, you have an array of documents. you can do whatever you need with them. for example, pass them into a vectorstore for retrieval later.

example:
import { readfilesync } from ""fs"";
import { document } from ""langchain/document"";
import { memoryvectorstore } from ""langchain/vectorstores/memory"";
import { openaiembeddings } from ""@langchain/openai"";

const filename = ""movies_data.json""
const jsondata = readfilesync(filename).tostring();
const movies = json.parse(jsondata);
const docs = [];

for (const movie of movies) {
    const doc = new document({
        pagecontent: movie.embedded_data.overview,
        metadata: {
            id: movie.metadata.id,
            original_title: movie.metadata.original_title,
            popularity: movie.metadata.popularity,
            release_date: movie.metadata.release_date,
            vote_average: movie.metadata.vote_average,
            vote_count: movie.metadata.vote_count,
            revenue: movie.metadata.revenue,
            tagline: movie.metadata.tagline,
            poster_url: movie.metadata.poster_url,
            adult: movie.metadata.adult,
            genre: movie.embedded_data.genre,  // is this metadata?
            source: filename,
        }
    });
    docs.push(doc);
}

// load docs into vectorstore
const vectorstore = await memoryvectorstore.fromdocuments(
  docs,
  new openaiembeddings()
);

references:

document loader > json (langchain)",https://stackoverflow.com/questions/78015622,langchain,18-02-2024 11:56,1331.0,1.0,1.0,True,20-03-2024 03:23,18-02-2024 16:29
67789544,given a word can we get all possible lemmas for it using spacy?,"the input word is standalone and not part of a sentence but i would like to get all of its possible lemmas as if the input word were in different sentences with all possible pos tags. i would also like to get the lookup version of the word's lemma.
why am i doing this?
i have extracted lemmas from all the documents and i have also calculated the number of dependency links between lemmas. both of which i have done using en_core_web_sm. now, given an input word, i would like to return the lemmas that are linked most frequently to all the possible lemmas of the input word.
so in short, i would like to replicate the behaviour of token._lemma for the input word with all possible pos tags to maintain consistency with the lemma links i have counted.","['python', 'nlp', 'spacy', 'lemmatization', 'spacy-3']",67796633,"i found it difficult to get lemmas and inflections directly out of spacy without first constructing an example sentence to give it context.  this wasn't ideal, so i looked further and found lemmainflect did this very well.
> from lemminflect import getalllemmas, getinflection, getallinflections, getallinflectionsoov

> getalllemmas('watches')
{'noun': ('watch',), 'verb': ('watch',)}

> getallinflections('watch')
{'nn': ('watch',), 'nns': ('watches', 'watch'), 'vb': ('watch',), 'vbd': ('watched',), 'vbg': ('watching',), 'vbz': ('watches',),  'vbp': ('watch',)}",https://stackoverflow.com/questions/67789544,python,01-06-2021 13:13,2423.0,4.0,3.0,True,28-01-2023 11:00,02-06-2021 00:41
75733749,openai embeddings api: how to change the embedding output dimension?,"in the official openai node library create embeddings if for example using the model text-embedding-ada-002 the embeddings returned is an array of around 1536.
import {configuration, openaiapi} from 'openai'

openai = new openaiapi(this.configuration)
const parameters= {
      model: 'text-embedding-ada-002',
      input: text,
    }
// make the embedding request and return the result
const resp = await openai.createembedding(parameters)
const embeddings = embedding?.data.data[0].embedding


i would like to be able to limit the length of the list of embeddings returned.","['node.js', 'openai-api', 'azure-openai', 'text-embedding-ada-002']",75733818,"you need to use the dimensions parameter with the openai embeddings api.
as stated in the official openai documentation:

by default, the length of the embedding vector will be 1536 for
text-embedding-3-small or 3072 for text-embedding-3-large. you
can reduce the dimensions of the embedding by passing in the
dimensions parameter without the embedding losing its
concept-representing properties. we go into more detail on embedding
dimensions in the embedding use case section.",https://stackoverflow.com/questions/75733749,node.js,14-03-2023 13:25,5720.0,4.0,1.0,True,12-06-2024 16:40,11-04-2023 09:34
26394748,nltk python error: &quot;typeerror: &#39;dict_keys&#39; object is not subscriptable&quot;,"i'm following instructions for a class homework assignment and i'm supposed to look up the top 200 most frequently used words in a text file.
here's the last part of the code:
fdist1 = freqdist(nsmytext)
vocab=fdist1.keys()
vocab[:200]

but when i press enter after the vocab 200 line, it returns:
traceback (most recent call last):
file ""<stdin>"", line 1, in <module>
typeerror: 'dict_keys' object is not subscriptable

any suggestions on how to fix this so it can correctly return an answer?","['python', 'python-3.x', 'dictionary', 'key', 'nltk']",26394855,"looks like you are using python 3. in python 3 dict.keys() returns an iterable but not indexable object. the most simple (but not so efficient) solution would be:
vocab = list(fdist1.keys())[:200]

in some situations it is desirable to continue working with an  iterator object instead of a list. this can be done with itertools.islice():
import itertools
vocab_iterator = itertools.islice(fdist1.keys(), 200)",https://stackoverflow.com/questions/26394748,python,16-10-2014 01:20,205607.0,112.0,7.0,True,15-05-2023 01:24,15-05-2023 01:24
71417857,position of that noun and verb,"i have a rule-based code that prints out the noun which is followed by a verb in a sentence
for text_id, text in enumerate(news_df['news_title'].values):
    
    # remove the comma and full stops
    text = text.replace(',', '').replace('.', '').replace('-','')
    sentence_tags = postag(text.lower())
    
    print(text)
    
    # sentences parts
    for index, part in enumerate(sentence_tags):
        try:
            
            if 'nn' in part[1] and 'vb' in sentence_tags[index + 1][1]:
            print("">"", part[0])
            break
            
        elif 'nn' in part[1] and 'nn' in sentence_tags[index + 1][1] and 'vb' in sentence_tags[index + 2][1]:
            print("">"", part[0],  sentence_tags[index + 1][0])
            break
            
        elif 'nn' in part[1] and 'nn' in sentence_tags[index + 1][1] and 'nn' in sentence_tags[index + 2][1] and 'vb' in sentence_tags[index + 3][1]:
            print("">"", part[0],  sentence_tags[index + 1][0], sentence_tags[index + 2][0])
            break

        except:
            pass
    print()

the output of a sentence following this rule:
high school football players charged after video surfaces showing hazing
> school football players

trump accuser pushes new york to pass the adult survivors act plans to sue
>trump accuser

is there a way to also print out the position of that noun that was printed due to the rule?
for example :
>trump accuser , [0,5,""nn""] , [6,13,""vb""]","['python', 'nlp', 'nltk', 'pos-tagger']",71477291,"i changed the script and separated the state machine segment. the most serious problem with this program imo is it's just returning the first pattern (you can fix it quickly).
import pandas as pd
import nltk
postag = nltk.pos_tag
df = pd.dataframe({'text':['high school football players charged after video surfaces showing hazing', 'trump accuser pushes new york to pass the adult survivors act plans to sue']})
for text_id, text in enumerate(df['text'].values):
    
    # remove the comma and full stops
    text = text.replace(',', '').replace('.', '').replace('-','')
    tokens = nltk.word_tokenize(text.lower())
    sentence_tags = postag(tokens)
    words = [item[0] for item in sentence_tags]
    start_end = []
    temp = 0
    for word in words:
      start_end.append([temp, temp+len(word)])
      temp+= (len(word)+1) 
    tags = [item[1] for item in sentence_tags]
    words_to_print = []
    tags_to_print = []
    start_end_to_print = []
    # the state machine 
    verb = false
    first_noun = false
    second_noun = false
    third_noun = false
    for w, t, se in zip(words, tags, start_end):
      if t.startswith('nn'):
        words_to_print.append(w)
        tags_to_print.append(t)
        start_end_to_print.append(se)
        first_noun = true

      elif t.startswith('nn') and first_noun:
        words_to_print.append(w)
        tags_to_print.append(t)
        start_end_to_print.append(se)
        second_noun = true

      elif t.startswith('nn') and second_noun:
        words_to_print.append(w)
        tags_to_print.append(t)
        start_end_to_print.append(se)
        third_noun = true

      elif t.startswith('vb') and (first_noun or second_noun or third_noun):
        break 
      
      elif (first_noun or second_noun or third_noun):
        words_to_print = []
        tags_to_print = []
        start_end_to_print = []
        verb = false
        first_noun, second_noun, third_noun = false, false, false
    
    print('> ', ' '.join(words_to_print), ' '.join([str(item[0])+' '+str(item[1]) for item in zip(start_end_to_print, tags_to_print)]))   
      

output:
>  school football players [5, 11] nn [12, 20] nn [21, 28] nns
>  trump accuser [0, 5] nn [6, 13] nn",https://stackoverflow.com/questions/71417857,python,10-03-2022 01:32,126.0,1.0,1.0,True,15-03-2022 05:03,10-03-2022 07:04
78210297,how to convert pretrained hugging face model to .pt and run it fully locally?,"i'm attempting to convert this model in .pt format. it's working fine for me so i dont want to fine-tune it. how can i export it to .pt and run interface?
i tried using this to convert to .pt:
from transformers import autoconfig, autoprocessor, automodelforctc, autotokenizer, wav2vec2processor
import librosa
import torch



# define the model name
model_name = ""urukhan/wav2vec2-russian""

# load the model and tokenizer
config = autoconfig.from_pretrained(model_name)
model = automodelforctc.from_pretrained(model_name, config=config)
processor = wav2vec2processor.from_pretrained(model_name)
tokenizer = autotokenizer.from_pretrained(model_name)

# save the model as a .pt file
torch.save(model.state_dict(), ""model.pt"")

# save the tokenizer as well if needed
tokenizer.save_pretrained(""model-tokenizer"")

but unfortunately its not running the interface :
model = automodelforctc.from_pretrained(""model.pt"")
processor = autoprocessor.from_pretrained(""model.pt"")


# perform inference with the model
file = 'here is wav.wav'
audio, _ = librosa.load(file, sr = 16000)
audio = list(audio)
def map_to_result(batch):
  with torch.no_grad():
    input_values = torch.tensor(batch, device=""cpu"").unsqueeze(0) #, device=""cuda""
    logits = model(input_values).logits
  pred_ids = torch.argmax(logits, dim=-1)
  batch = processor.batch_decode(pred_ids)[0]
  return batch
map_to_result(audio)
print(map_to_result(audio))


model.eval()

and encountered an error:
`model.pt is not a local folder and is not a valid model identifier listed on '
`","['machine-learning', 'pytorch', 'huggingface-transformers', 'transformer-model']",78256222,"so, the solution was to save model and it's weights by using save_pretrained not by torch.save()",https://stackoverflow.com/questions/78210297,machine-learning,23-03-2024 09:18,870.0,-1.0,2.0,True,01-04-2024 15:06,23-03-2024 10:02
72971576,can we get columns names sorted in the order of their tf-idf values (if exists) for each document?,"i'm using sklearn tfidfvectorizer. i'm trying to get the column names in a list in the order of thier tf-idf values in decreasing order for each document? so basically, if a document has all the stop words then we don't need any column names.
import pandas as pd
from sklearn.feature_extraction.text import tfidfvectorizer

msg = [""my name is venkatesh"",
       ""trying to get the significant words for each vector"",
       ""i want to get the list of words name in the decresasing order of their tf-idf values for each vector"",
       ""is to my""]

stopwords=['is','to','my','the','for','in','of','i','their']

tfidf_vect = tfidfvectorizer(stop_words=stopwords)

tfidf_matrix=tfidf_vect.fit_transform(msg)

pd.dataframe(tfidf_matrix.toarray(), 
                 columns=tfidf_vect.get_feature_names_out())


i want to generate a column with the list word names in the decreasing order of their tf-idf values
so the column would be like this
    ['venkatesh','name']
    ['significant','trying','vector','words','each','get']
    ['decreasing','idf','list','order','tf','values','want','each','get','name','vector','words']
    [] # empty list since the document consists only stopwords

above is the primary result i'm looking for, it would be great if we get the sorted dict with tdfidf values as keys and the list of words as values asociated with that tfidf value for each document
so,the result would be like the below
{'0.785288':['venkatesh'],'0.619130':['name']}
{'0.47212':['significant','trying'],'0.372225':['vector','words','each','get']}
{'0.314534':['decreasing','idf','list','order','tf','values','want'],'0.247983':['each','get','name','vector','words']}
{} # empty dict since the document consists only stopwords","['python-3.x', 'pandas', 'tf-idf', 'columnsorting', 'tfidfvectorizer']",72991608,"i think this code does what you want and avoids using pandas:
from itertools import groupby

sort_func = lambda v: v[0] # sort by first value in tuple
all_dicts = []
for row in tfidf_matrix.toarray():
    sorted_vals = sorted(zip(row, tfidf_vect.get_feature_names()), key=sort_func, reverse=true)
    all_dicts.append({val:[g[1] for g in group] for val, group in groupby(sorted_vals, key=sort_func) if val != 0})
    

you could make it even less readable and put it all in a single comprehension! :-)",https://stackoverflow.com/questions/72971576,python-3.x,13-07-2022 19:22,427.0,0.0,2.0,True,15-07-2022 09:04,15-07-2022 07:00
49207097,counting specific words in a sentence,"i am currently trying to solve this homework question.
my task is to implement a function that returns a vector of word counts in a given text. i am required to split the text into words then use nltk's tokeniser to tokenise each sentence.
this is the code i have so far:
import nltk
import collections
nltk.download('punkt')
nltk.download('gutenberg')
nltk.download('brown')

def word_counts(text, words):
""""""return a vector that represents the counts of specific words in the text
>>> word_counts(""here is sentence one. here is sentence two."", ['here', 'two', 'three'])
[2, 1, 0]
>>> emma = nltk.corpus.gutenberg.raw('austen-emma.txt')
>>> word_counts(emma, ['the', 'a'])
[4842, 3001]
""""""

from nltk.tokenize import tweettokenizer
text = nltk.sent_tokenize(text)
words = nltk.sent_tokenize(words)

wordlist = []

for sen in text, words:
    for word in nltk.word_tokenize(sen):

        wordlist.append(text, words).split(word)

counter = tweettokenizer(wordlist)
return counter

there are two doctests that should give the result of:
[2, 1, 0] and [4842, 3001]
this is the error message i am getting from my code

i've spent all day trying to tackle this and i feel i'm getting close but i don't know what i'm doing wrong, the script is giving me an error every time.
any help will be very appreciated.
thank you.","['python', 'vector', 'nltk']",49207932,"this is how i would use nltk to get to the result your homework wants:
import nltk
import collections
from nltk.tokenize import tweettokenizer
# nltk.download('punkt')
# nltk.download('gutenberg')
# nltk.download('brown')

def word_counts(text, words):
    """"""return a vector that represents the counts of specific words in the text
    word_counts(""here is one. here is two."", ['here', 'two', 'three'])
    [2, 1, 0]
    emma = nltk.corpus.gutenberg.raw('austen-emma.txt')
    word_counts(emma, ['the', 'a'])
    [4842, 3001]
    """"""  

    texttok = nltk.word_tokenize(text) 
    counts =  nltk.freqdist(texttok)   # this counts all word occurences

    return [counts[x] for x in words] # this returns what was counted for *words

r1 = word_counts(""here is one. here is two."", ['here', 'two', 'three'])
print(r1) #    [2, 1, 0]

emma = nltk.corpus.gutenberg.raw('austen-emma.txt')
r2 = word_counts(emma, ['the', 'a'])
print(r2) # [4842, 3001]

your code does multiple things that look just wrong:

for sen in text, words:
    for word in nltk.word_tokenize(sen):

        wordlist.append(text, words).split(word)



sent_tokenize() takes a string and returns a list of sentences from it - you store the results in 2 variables text, words and then you try to iterate over tuple of them? words is not a text with sentences to begin, this makes not much sense to me
wordlist is a list, if you use the .append() on it, append() returns none. nonehas no .split() function.",https://stackoverflow.com/questions/49207097,python,10-03-2018 08:40,1488.0,1.0,1.0,True,21-01-2022 07:10,10-03-2018 11:39
75280636,how to pick up adjectives or nouns out of a text?,"for example like:
sentence = 'an old lady lives in a small red house. she has three cute cats, and their names match with their colors: white, cinnamon, and chocolate. they are poor but happy.'

so i hope to get 2 lists like these:
adj = ['old','small','red','cute','white','poor','happy']
noun = ['lady','house','cats','names','colors','cinnamon','chocolate']

i saw someone mentioned nltk, but i haven't used the package so i would hope for some instructions.","['python', 'nlp', 'nltk']",75282394,"what you need is called part-of-speech(pos) tagging, you could check:

nltk:  
spacy: 

if not enough, there are a lot of additional instructions for newbies out there if you google 'pos tagging + python'.
btw, i would recommend spacy as it is more modern.",https://stackoverflow.com/questions/75280636,python,30-01-2023 05:46,133.0,-1.0,1.0,True,30-01-2023 09:45,30-01-2023 09:45
21902411,how to get domain of words using wordnet in python?,"how can i find domain of words using nltk python module and wordnet?
suppose i have words like (transaction, demand draft, cheque, passbook) and the domain for all these words is ""bank"". how can we get this using nltk and wordnet in python?
i am trying through hypernym and hyponym relationship:
for example:
from nltk.corpus import wordnet as wn
sports = wn.synset('sport.n.01')
sports.hyponyms()
[synset('judo.n.01'), synset('athletic_game.n.01'), synset('spectator_sport.n.01'),    synset('contact_sport.n.01'), synset('cycling.n.01'), synset('funambulism.n.01'), synset('water_sport.n.01'), synset('riding.n.01'), synset('gymnastics.n.01'), synset('sledding.n.01'), synset('skating.n.01'), synset('skiing.n.01'), synset('outdoor_sport.n.01'), synset('rowing.n.01'), synset('track_and_field.n.01'), synset('archery.n.01'), synset('team_sport.n.01'), synset('rock_climbing.n.01'), synset('racing.n.01'), synset('blood_sport.n.01')]

and
bark = wn.synset('bark.n.02')
bark.hypernyms()
[synset('noise.n.01')]","['python', 'nlp', 'nltk', 'wordnet']",21904027,"there is no explicit domain information in the princeton wordnet nor the nltk's wn api. 
i would recommend you get a copy of the wordnet domain resource and then link your synsets using the domains, see 
after you've registered and completed the download you will see a wn-domains-3.2-20070223 textfile, which is a tab-delimited file with first column the offset-partofspeech identifier and the 2nd column contains the domain tags separated by spaces, e.g.
00584282-v  military pedagogy
00584395-v  military school university
00584526-v  animals pedagogy
00584634-v  pedagogy
00584743-v  school university
00585097-v  school university
00585271-v  pedagogy
00585495-v  pedagogy
00585683-v  psychological_features

then you use the following script to access synsets' domain(s):
from collections import defaultdict
from nltk.corpus import wordnet as wn

# loading the wordnet domains.
domain2synsets = defaultdict(list)
synset2domains = defaultdict(list)
for i in open('wn-domains-3.2-20070223', 'r'):
    ssid, doms = i.strip().split('\t')
    doms = doms.split()
    synset2domains[ssid] = doms
    for d in doms:
        domain2synsets[d].append(ssid)

# gets domains given synset.
for ss in wn.all_synsets():
    ssid = str(ss.offset).zfill(8) + ""-"" + ss.pos()
    if synset2domains[ssid]: # not all synsets are in wordnet domain.
        print ss, ssid, synset2domains[ssid]

# gets synsets given domain.        
for dom in sorted(domain2synsets):
    print dom, domain2synsets[dom][:3]

also look for the wn-affect that is very useful to disambiguate words for sentiment within the wordnet domain resource.

with updated nltk v3.0, it comes with the open multilingual wordnet ( and since the french synsets share the same offset ids, you can simply use the wnd as a crosslingual resource. the french lemma names can be accessed as such:
# gets domains given synset.
for ss in wn.all_synsets():
    ssid = str(ss.offset()).zfill(8) + ""-"" + ss.pos()
    if synset2domains[ssid]: # not all synsets are in wordnet domain.
        print ss, ss.lemma_names('fre'), ssid, synset2domains[ssid]

note that the most recent version of nltk changes synset properties to ""get"" functions: synset.offset -> synset.offset()",https://stackoverflow.com/questions/21902411,python,20-02-2014 08:42,8449.0,11.0,4.0,True,31-08-2022 16:21,19-12-2019 13:17
76162470,openai chat completions api error: &quot;request failed with status code 400&quot;,"i want to upgrade my gpt-3 to gpt-3.5 turbo in node.js. but i have a problem with that.
my code:
const askai = async (message) => {
  try {
    const openaiinstance = await _createopenaiinstance()

    const response = await openaiinstance.createcompletion({
      model: 'gpt-3.5-turbo',
      messages: [{ role: 'user', content: message }]
    })

    const repliedmessage = response.data.choices[0].message.content

    return repliedmessage
  } catch (err) {
    logger.error('', '', 'ask ai error: ' + err.message)
    return sendinternalerror(err)
  }
}

but when i try to input the message:
askai('suggest me a job position for auto cad user')

it's returning an error:
request failed with status code 400","['node.js', 'openai-api', 'chatgpt-api']",76163985,"problem
you used the wrong function to get a completion for the given model.
the function createcompletion works with the completions api. in other words, the function createcompletion works with gpt-3 models (e.g., text-davinci-003) or gpt base models (e.g., davinci-002).
solution
note: openai nodejs sdk v4 was released on august 16, 2023, and is a complete rewrite of the sdk. among other things, there are changes in method names. see the v3 to v4 migration guide.




model
nodejs function (sdk v3)
nodejs function (sdk v4)




gpt-3.5 and gpt-4
openai.createchatcompletion
openai.chat.completions.create


gpt base and gpt-3
openai.createcompletion
openai.completions.create




ï¿½ï¿½ï¿½ if you have the openai nodejs sdk v3: change createcompletion to createchatcompletion
const askai = async (message) => {
  try {
    const openaiinstance = await _createopenaiinstance()

    const response = await openaiinstance.createchatcompletion({
      model: 'gpt-3.5-turbo',
      messages: [{ role: 'user', content: message }]
    })

    const repliedmessage = response.data.choices[0].message.content

    return repliedmessage
  } catch (err) {
    logger.error('', '', 'ask ai error: ' + err.message)
    return sendinternalerror(err)
  }
}

ï¿½ï¿½ï¿½ if you have the openai nodejs sdk v4: change createcompletion to chat.completions.create
note: there are also changes in extracting the message content in v4.
const askai = async (message) => {
  try {
    const openaiinstance = await _createopenaiinstance()

    const response = await openaiinstance.chat.completions.create({
      model: 'gpt-3.5-turbo',
      messages: [{ role: 'user', content: message }]
    })

    const repliedmessage = response.choices[0].message.content

    return repliedmessage
  } catch (err) {
    logger.error('', '', 'ask ai error: ' + err.message)
    return sendinternalerror(err)
  }
}
<",https://stackoverflow.com/questions/76162470,node.js,03-05-2023 09:48,2500.0,1.0,1.0,True,12-06-2024 16:57,12-06-2024 16:57
76359515,hugging face transformers trainer: per_device_train_batch_size vs auto_find_batch_size,"a hugging face transformers trainer can receive a per_device_train_batch_size argument, or an auto_find_batch_size argument.
however, they seem to have different effects. one thing to consider is that per_device_train_batch_size defaults to 8: it is always set, and you can't disable it.
i have also observed that if i run into oom errors, lowering per_device_train_batch_size can solve the issue, but auto_find_batch_size doesn't solve the problem. this is quite counter-intuitive, since it should find a batch size that is small enough (i can do it manually).
so: what does auto_find_batch_size do, exactly?","['nlp', 'artificial-intelligence', 'huggingface-transformers']",76362222,"the auto_find_batch_size argument is an optional argument which can be used in addition to the per_device_train_batch_size argument.
as you point out, lowering the batch size is one way to resolve out-of-memory errors. the auto_find_batch_size argument automates the lowering process. enabling this, will use find_executable_batch_size from accelerate, which:

operates with exponential decay, decreasing the batch size in half after each failed run

the per_device_train_batch_size is used as the initial batch size to start off with. so if you use the default of 8, it starts training with a batch size of 8 (on a single device), & if it fails, it will restart the training procedure with a batch size of 4.",https://stackoverflow.com/questions/76359515,nlp,29-05-2023 18:11,11799.0,3.0,1.0,True,30-05-2023 06:50,29-05-2023 20:10
62027482,"remove all words that are not nouns, verbs, adjectives, adverbs, or proper names. spacy python","i wrote the code below and i want print out the words in the first 10 sentences, and i want to remove all words that are not nouns, verbs, adjectives, adverbs, or proper names.but i dont know how?  can anyone help me? 
! pip install wget
import wget
url = '
wget.download(url, 'moby_dick.txt')
documents = [line.strip() for line in open('moby_dick.txt', encoding='utf8').readlines()]

import spacy

nlp = spacy.load('en')

tokens = [[token.text for token in nlp(sentence)] for sentence in documents[:200]]
pos = [[token.pos_ for token in nlp(sentence)] for sentence in documents[:100]]
pos","['python', 'nlp', 'nltk', 'spacy']",62027959,"all you need is to know which pos symbols are used to represent these entities. here is the list from spacy documentation. this code will help you with this requirement:
import spacy

nlp = spacy.load('en_core_web_sm') #you can use other methods
# excluded tags
excluded_tags = {""noun"", ""verb"", ""adj"", ""adv"", ""adp"", ""propn""}
document = [line.strip() for line in open('moby_dick.txt', encoding='utf8').readlines()]

sentences = document[:10] #first 10 sentences
new_sentences = []
for sentence in sentences:
    new_sentence = []
    for token in nlp(sentence):
        if token.pos_ not in excluded_tags:
            new_sentence.append(token.text)
    new_sentences.append("" "".join(new_sentence))


now, new_sentences have the same sentences like before but without any nouns, verbs, ... etc. you can make sure of that by iterating over sentences and new_sentences to see the different:
for old_sen, new_sen in zip(sentences, new_sentences):
    print(""before:"", old_sen)
    print(""after:"", new_sen)
    print()
before: loomings .
after: .

before: call me ishmael .
after: me .

before: some years ago -- never mind how long precisely -- having little or no money in my purse , and nothing particular to interest me on shore , i thought i would sail about a little and see the watery part of the world .
after: some -- -- or no my , and nothing to me , i i a and the the .

before: it is a way i have of driving off the spleen and regulating the circulation .
after: it is a i have the and the .

...
...",https://stackoverflow.com/questions/62027482,python,26-05-2020 16:59,4753.0,-1.0,1.0,True,30-11-2022 06:19,26-05-2020 17:03
60937617,how to reconstruct text entities with hugging face&#39;s transformers pipelines without iob tags?,"i've been looking to use hugging face's pipelines for ner (named entity recognition). however, it is returning the entity labels in inside-outside-beginning (iob) format but without the iob labels. so i'm not able to map the output of the pipeline back to my original text. moreover, the outputs are masked in bert tokenization format (the default model is bert-large).
for example: 
from transformers import pipeline
nlp_bert_lg = pipeline('ner')
print(nlp_bert_lg('hugging face is a french company based in new york.'))

the output is:
[{'word': 'hu', 'score': 0.9968873858451843, 'entity': 'i-org'},
{'word': '##gging', 'score': 0.9329522848129272, 'entity': 'i-org'},
{'word': 'face', 'score': 0.9781811237335205, 'entity': 'i-org'},
{'word': 'french', 'score': 0.9981815814971924, 'entity': 'i-misc'},
{'word': 'new', 'score': 0.9987512826919556, 'entity': 'i-loc'},
{'word': 'york', 'score': 0.9976728558540344, 'entity': 'i-loc'}]

as you can see, new york is broken up into two tags.
how can i map hugging face's ner pipeline back to my original text?
transformers version: 2.7","['nlp', 'tokenize', 'transformer-model', 'named-entity-recognition', 'huggingface-transformers']",61909224,"edit 12/2023:
as pointed out, the grouped_entities parameter has been deprecated. the correct way is to use the aggregation_strategy parameters as pointed in the source code .
for instance:
text = 'hugging face is a french company based in new york.'
tagger = pipeline(task='ner', aggregation_strategy='simple')
named_ents = tagger(text)
pd.dataframe(named_ents)

gives the following output
[
   {
      ""entity_group"":""org"",
      ""score"":0.96934015,
      ""word"":""hugging face"",
      ""start"":0,
      ""end"":12
   },
   {
      ""entity_group"":""misc"",
      ""score"":0.9981816,
      ""word"":""french"",
      ""start"":18,
      ""end"":24
   },
   {
      ""entity_group"":""loc"",
      ""score"":0.9982121,
      ""word"":""new york"",
      ""start"":42,
      ""end"":50
   }
]

original answer:
the 17th of may, a new pull request  with what you are asking for has been merged, therefore now our life is way easier, you can you it in the pipeline like
ner = pipeline('ner', grouped_entities=true)

and your output will be as expected. at the moment you have to install from the master branch since there is no new release yet. you can do it via
pip install git+git://github.com/huggingface/transformers.git@48c3a70b4eaedab1dd9ad49990cfaa4d6cb8f6a0",https://stackoverflow.com/questions/60937617,nlp,30-03-2020 18:58,13215.0,12.0,4.0,True,02-11-2024 14:24,14-04-2020 14:32
73383418,how can we pass a list of strings to a fine tuned bert model?,"i want  to pass a list of strings instead of a single string input to my fine tuned bert question classification model.
this is my code which accept a single string input.
questionclassification_model = tf.keras.models.load_model('/content/drive/mydrive/questionclassification_model')
tokenizer = berttokenizer.from_pretrained('bert-base-cased')

def prepare_data(input_text):
    token = tokenizer.encode_plus(
        input_text,
        max_length=256, 
        truncation=true, 
        padding='max_length', 
        add_special_tokens=true,
        return_tensors='tf'
    )
    return {
        'input_ids': tf.cast(token['input_ids'], tf.float64),
        'attention_mask': tf.cast(token['attention_mask'], tf.float64)
    }

def make_prediction(model, processed_data, classes=['easy', 'medium', 'hard']):
    probs = model.predict(processed_data)[0]
    return classes[np.argmax(probs)],probs;

i don't want to use a for loop over the list as it takes more execution time.
when i tried to pass a list as input to the tokenizer it was returning same output for every input.
input_text = [""what is gandhi commonly considered to be?,father of the nation in india"",""what is the long-term warming of the planets overall temperature called?, global warming""]
processed_data = prepare_data(input_text)


{'input_ids': <tf.tensor: shape=(1, 256), dtype=float64, numpy=
array([[101., 100., 100., 102.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.]])>, 'attention_mask': <tf.tensor: shape=(1, 256), dtype=float64, numpy=
array([[1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])>}

and that is not the right tokens for the input text.
thanks in advance...","['python', 'nlp', 'huggingface-transformers', 'bert-language-model', 'huggingface-tokenizers']",73399659,"different methods for one sentence vs batches
there are different methods for encoding one sentence versus encoding a batch of sentences
according to the documentation ( the encode_plus method expects the first parameter to be ""this can be a string, a list of strings (tokenized string using the tokenize method) or a list of integers (tokenized string ids using the convert_tokens_to_ids method).""
(emphasis mine) - so that if you're passing a list of strings to this particular method, they are interpreted as a list of tokens, not sentences, and obviously all those very long ""tokens"" like ""what is gandhi commonly considered to be?,father of the nation in india"" do not match anything in the vocabulary so they get mapped to the out-of-vocabulary id.
if you want to encode a batch of sentences, then you need to pass your list of strings to the batch_encode_plus method (",https://stackoverflow.com/questions/73383418,python,17-08-2022 05:46,2882.0,1.0,2.0,True,18-08-2022 08:19,18-08-2022 04:37
78613782,azure openai: where are assistants api persistent resources stored?,"when interacting with the assistants api, some persistent resources are created, namely: assistants, threads and messages. all have an unique id and the api exposes methods/endpoints to retrieve these resources.
i can't find anywhere in the documentation an explanation of this persistency:

where is the data stored?
for how long?
is there a cost associated with this?
is gdpr compliant?","['azure', 'openai-api', 'azure-openai', 'openai-assistants-api']",78616220,"i've received an answer by @ashokpeddakotla-msft in the microsoft learn forum. the link to the thread is here.
tldr:

where is the data stored?

there are two types of data stored in the assistants api:
stateful entities: threads, messages, and runs created during assistants use.
files: uploaded during assistants setup or as part of a message.
data is stored in a secure, microsoft-managed storage account that is logically separated.

for how long?

all used data persists in this system unless you explicitly delete this data. use the delete function with the thread id of the thread you want to delete. clearing the run in the assistants playground does not delete threads, however deleting them using delete function will not list them in the thread page.

is there a cost associated with this?

storage has no direct cost related to it. for more information, see the pricing page.

is gdpr compliant?

the assistant api adheres to relevant data protection regulations, such as gdpr and ccpa, to ensure that user data is handled responsibly and in compliance with applicable laws.",https://stackoverflow.com/questions/78613782,azure,12-06-2024 15:55,590.0,1.0,1.0,True,13-06-2024 06:46,12-06-2024 15:59
78039417,questions about training llms on large text datasets for text generation from scratch,"i made a fully custom made gpt in jax (with keras 3), using tensorflow for the data pipeline.
i've trained the model on the shakespeare dataset and got good results (so no problem with the model).
now i want to train it on the tiny-stories dataset which is pretty big with gpt of 15m parameters.
here is the code for loading the data:
def get_dataset_lists(ds_path:str):
    dataset = open(ds_path, ""r"", encoding=""utf-8"").read() # [...]
    dataset = dataset.split(""<|endoftext|>"")
    r.shuffle(dataset)
    dataset:list = spm.encode( # llama's sentence piece encoder
            tf.strings.strip(dataset).numpy().tolist(), 
            add_bos=true,
            add_eos=false
        ) # [[sos story], ..., [sos story]]
    print(""\tnumber of stories:"", len(dataset))
    return dataset

def tf_dataload(
    dataset:list,
    batch_size:int,
    maxlen:int,
    shift:int,
):
    import functools; import operator
    dataset = functools.reduce(operator.iconcat, dataset, [])
    num_tokens = len(dataset); print(""\tnumber of tokens in the dataset is"", num_tokens)
    unique_tok = set(dataset); print(""\tnumber of unique tokens in the dataset is"", len(unique_tok))
    # [sos story ... sos story]
    dataset = tf.data.dataset.from_tensor_slices(dataset)
    dataset = dataset.window(maxlen+1, shift=shift, drop_remainder=true)
    # [[...], [...], [...], ...] shape(m, maxlen+1)
    dataset = dataset.flat_map(lambda window: window.batch(maxlen+1))
    dataset = dataset.shuffle(10_000*batch_size, reshuffle_each_iteration=reshuffle_each_iteration)
    # [ [ [...], [...], [...], ...], ...] shape(m//b, b, maxlen+1)
    dataset = dataset.batch(batch_size, drop_remainder=true, num_parallel_calls=tf.data.autotune)
    dataset = dataset.shuffle(batch_size*100)
    dataset = dataset.map(lambda window: (window[:, :-1], window[:, 1:]), num_parallel_calls=tf.data.autotune).prefetch(tf.data.autotune)
    return dataset # (shape(m//b, b, maxlen) shape(m//b, b, maxlen))

def load_data(
    train_ds_path:str,
    val_ds_path:str,
    batch_size:int,
    maxlen:int,
    shift:int,
):  
    print(""training dataset:"")
    train_ds = tf_dataload(get_dataset_lists(train_ds_path), batch_size, maxlen, shift, reshuffle_each_iteration=true)
    print(""validation dataset:"")
    val_ds = tf_dataload(get_dataset_lists(val_ds_path), batch_size, maxlen, shift, reshuffle_each_iteration=true)
    print(f""\n{train_ds}\n{val_ds}"")
    datasets = {""train"": train_ds.repeat(), ""val"":val_ds}
    return datasets


i've certain questions regarding
the value of the shift?
first i set it equal to 1, but the training was very slow, even after 100000 steps it didn't converge even though it was decreasing slowly (i think there's no problem with the learning rate as i plotted loss vs lr and selected the max learning rate possible and used cosine decay with warmup)


so i looked into karpathy's llama-2 repo and the shift was equal to maxlen.
so i set it equal to maxlen and trained it for 100000 steps but the model is learning very slowly, and didn't get a loss even close to what karpathy got
(i don't know what's the problem, as i've closely followed karpathy's llama2 repo)
what is shift generally equal to when pre-training an llm on language modelling?
shouldn't it be 1, because the transformer model is not positionally invariant, and it would affect model performance if shift is not equal to 1? but then the number of samples will be very large...?

and for what number of steps to train a llm given the number of tokens

you may find the below helpful...
@dataclass
class gptargs:
    """"""gpt configuration""""""
    d_model:int = 288
    num_layers:int = 6
    num_heads:int = 6
    max_context_length:int = 256
    vocab_size:int = vocab_size # 32k
    output_units:int = none # equal to vocab_size if none in model init  
    assert d_model % 2 == 0
    assert d_model % num_heads == 0
    dropout_rate:float = 0.1

@dataclass
class targs:
    # lr scheduler
    init_lr:float = 1e-7
    max_lr:float = 6.5e-4
    min_lr:float = 0.1*max_lr # the factor is usually 0.1 or 0.0
    num_steps:int = 100_000
    warmup_steps:int = 1000 # 1000, to make training more stable instead of 2000
    decay_steps:int = num_steps

    # optimizer
    beta1:float = 0.9
    beta2:float = 0.95
    weight_decay:float = 1e-1
    clipvalue:float = 1e0
    num_grad_accumalation_steps:int = 4
    # num_tok_per_update = batch_size * maxlen * gradient_accumalation = 128 * 256 * 4 = 131_072

    # training
    checkpoint:str = 'weights/gptstories/epoch{epoch}.weights.h5'
    train_ds_path:str = ""tinystoriesdataset/tinystories-train.txt""
    val_ds_path:str = ""tinystoriesdataset/tinystories-valid.txt""
    steps_per_epoch = eval_freq = 2000
    eval_steps:int = 200
    batch_size:int = 128 
    patience:int = 10 # early stopping with restore best weights

update 1:
i thought that the model wasn't getting the training samples uniformly so i modified the data pipeline and also increased the number of steps to 200,000.
but there were no significant improvements. the training was still very slow by the end and loss was decreasing by 0.01 every epoch (of 2000 steps)... got a loss of 1.67 on validation set
def pretokenize_and_save_dataset(dataset_path:str, num_shards:int, shard_dir:str):
    dataset = open(dataset_path, ""r"", encoding=""utf-8"").read() # [...]
    dataset = dataset.split(""<|endoftext|>"")
    r.shuffle(dataset)
    dataset:list = spm.encode(
            tf.strings.strip(dataset).numpy().tolist(), 
            add_bos=true,
            add_eos=false
        ) # [[sos story], ..., [sos story]]
    print(""dataset:"")
    print(""\tnumber of stories:"", len(dataset))

    # flatten
    dataset = functools.reduce(operator.iconcat, dataset, [])
    num_tokens = len(dataset); print(""\tnumber of tokens in the dataset:"", num_tokens)
    print(""\tnumber of unique tokens in the dataset:"", len(set(dataset)))
    
    dataset = np.asarray(dataset, dtype=np.uint16) # [sos story ... sos story]
    print(""\tavg length of story:"", num_tokens/((dataset==1).sum()))

    # shard and save dataset
    sharded_datasets_list = np.array_split(dataset, num_shards) # [[sos story...], [...], [...], ...]
    filenames = [os.path.join(shard_dir, f""shard{i+1}.npy"") for i in range(num_shards)]
    
    for filename, sharded_ds in zip(filenames, sharded_datasets_list):
        with open(filename, ""wb"") as f:
            np.save(f, sharded_ds)
    return filenames

def load_data_as_tfds(
    dataset:np.ndarray,
    maxlen:int,
    shift:int,
):
    # [sos story ... sos story]
    dataset = tf.data.dataset.from_tensor_slices(dataset.tolist())
    dataset = dataset.window(maxlen+1, shift=shift, drop_remainder=true)
    # [[...], [...], [...], ...] shape(m, maxlen+1)
    dataset = dataset.flat_map(lambda window: window.batch(maxlen+1))
    dataset = dataset.shuffle(10_000*128)
    return dataset

def batch_tfds(
        dataset:tf.data.dataset,
        batch_size:int,
):
    dataset = dataset.batch(batch_size, drop_remainder=true, num_parallel_calls=tf.data.autotune)
    dataset = dataset.shuffle(batch_size*1000)
    dataset = dataset.map(lambda window: (window[:, :-1], window[:, 1:]), num_parallel_calls=tf.data.autotune)
    dataset = dataset.repeat().prefetch(tf.data.autotune)
    return dataset

def load_data(
    dataset_path:str,
    batch_size:int,
    maxlen:int,
    shift:int,
    num_shards:int,
    shard_dir:str
):  
    if os.path.exists(shard_dir) and os.listdir(shard_dir):
        filenames = glob.glob(os.path.join(shard_dir, ""*.npy""))
    else:
        os.makedirs(shard_dir)
        filenames = pretokenize_and_save_dataset(dataset_path, num_shards=num_shards, shard_dir=shard_dir)
    r.shuffle(filenames)
    to_tfds = lambda dataset: load_data_as_tfds(dataset, maxlen=maxlen, shift=shift)
    num_train_shards = round(0.9651*num_shards)
    num_val_shards = num_shards-num_train_shards

    print(""training dataset:"")
    print(f""\tnumber of files taken for training: {num_train_shards}/{num_shards}"")
    train_datasets_lists = [to_tfds(np.load(filename)) for filename in filenames[:num_train_shards]]
    train_ds = tf.data.dataset.sample_from_datasets(train_datasets_lists, weights=[1/num_train_shards]*num_train_shards)
    # [ [ [...], [...], [...], ...], ...] shape(m//b, b, maxlen+1)
    train_ds = batch_tfds(train_ds, batch_size=batch_size)

    print(""validation dataset:"")
    print(f""\tnumber of files taken for validation: {num_val_shards}/{num_shards}"")
    val_datasets_lists = [to_tfds(np.load(filename)) for filename in filenames[num_train_shards:]]
    val_ds = tf.data.dataset.sample_from_datasets(val_datasets_lists, weights=[1/num_val_shards]*num_val_shards)
    # [ [ [...], [...], [...], ...], ...] shape(m//b, b, maxlen+1)
    val_ds = batch_tfds(val_ds, batch_size=batch_size)

    print(f""\n{train_ds}\n{val_ds}"")
    datasets = {""train"": train_ds, ""val"":val_ds}
    return datasets","['python', 'tensorflow', 'deep-learning', 'nlp', 'tf.data.dataset']",78132351,"replaced keras's gradient accumulation argument in adamw with a custom implementation like in karpathy's and now the loss is decreasing faster.
if you are using keras's num_grad_accum, increase num_steps to
num_steps *= num_grad_accum",https://stackoverflow.com/questions/78039417,python,22-02-2024 08:33,470.0,-1.0,1.0,True,12-12-2024 12:23,12-12-2024 12:23
74542420,how to extract required information using gpt-3 api,"i tried the steps mentioned in this article.

there is a screenshot that says: here's an example from the openai playground.
i typed all the text in ""playground"" but do not get similar response as shown in that image. i expected similar text like  {""name"":""william"", ""company"":""billcheese""} i am not sure how to configure the parameters in openai web interface.

update:
i used this code:
import json
import re, textwrap 
 
import openai
openai.api_key = 'xxx'

prompt = f""""""
hi matt! this is steve jobs with inforation edge limited ! i'm interested in having you join our team here. 
""""""

completion = openai.completion.create(
    model=""text-davinci-002"",
    prompt=textwrap.dedent(prompt),
    max_tokens=20,
    temperature=0,
)

try:
    json_str_response = completion.choices[0].text
    json_str_response_clean = re.search(r"".*(\{.*\})"", json_str_response).groups()[0]
    print (json.loads(json_str_response_clean))

except (attributeerror, json.decoder.jsondecodeerror) as exception:
    print(""could not decode completion response from openai:"")
    print(completion)
    raise exception


and got this error:
could not decode completion response from openai:
attributeerror: 'nonetype' object has no attribute 'groups'","['python', 'openai-api']",74692969,"you're running into this problem: regex: attributeerror: 'nonetype' object has no attribute 'groups'
take a look at this line:
json_str_response_clean = re.search(r"".*(\{.*\})"", json_str_response).groups()[0]

the regex can't find anything matching the pattern, so it returns none. none does not have .groups() so you get an error. i don't have enough details to go much further, but the link above might get you there.",https://stackoverflow.com/questions/74542420,python,23-11-2022 06:06,2572.0,0.0,3.0,True,18-01-2023 21:27,18-01-2023 21:27
58057021,oserror: [e050] can&#39;t find model &#39;en&#39;,"i am trying to use this pytextrank library of python- 
 but i am unable to resolve this error , earlier i was getting an error that ip.json can't be found, but then was resolved 
    import pytextrank
    import sys
    path_stage0=""data/ip.json"" 
    path_stage1=""o1.json""

    with open(path_stage1,'w') as f:
        for graf in pytextrank.parse_doc(pytextrank.json_iter(path_stage0)):
            f.write(""%s\n"" % pytextrank.pretty_print(graf._asdict()))
            print(pytextrank.pretty_print(graf))


    oserror                                   traceback (most recent call last)
    <ipython-input-12-a20b437ea0f1> in <module>
          6 
          7 with open(path_stage1,'w') as f:
    ----> 8     for graf in pytextrank.parse_doc(pytextrank.json_iter(path_stage0)):
          9         f.write(""%s\n"" % pytextrank.pretty_print(graf._asdict()))
         10         print(pytextrank.pretty_print(graf))

~\anaconda3\lib\site-packages\pytextrank\pytextrank.py in parse_doc(json_iter)
    259                 print(""graf_text:"", graf_text)
    260 
--> 261             grafs, new_base_idx = parse_graf(meta[""id""], graf_text, base_idx)
    262             base_idx = new_base_idx
    263 

~\anaconda3\lib\site-packages\pytextrank\pytextrank.py in parse_graf(doc_id, graf_text, base_idx, spacy_nlp)
    185     if not spacy_nlp:
    186         if not spacy_nlp:
--> 187             spacy_nlp = spacy.load(""en"")
    188 
    189         spacy_nlp = spacy_nlp

~\anaconda3\lib\site-packages\spacy\__init__.py in load(name, **overrides)
     25     if depr_path not in (true, false, none):
     26         deprecation_warning(warnings.w001.format(path=depr_path))
---> 27     return util.load_model(name, **overrides)
     28 
     29 

~\anaconda3\lib\site-packages\spacy\util.py in load_model(name, **overrides)
    137     elif hasattr(name, ""exists""):  # path or path-like to model data
    138         return load_model_from_path(name, **overrides)
--> 139     raise ioerror(errors.e050.format(name=name))
    140 
    141 

oserror: [e050] can't find model 'en'. it doesn't seem to be a shortcut link, a python package or a valid path to a data directory.","['python', 'nlp', 'spacy', 'pytextrank']",63986422,"solved error on notebook using:
!python -m spacy download en_core_web_md

you can download packages as per your requirements like:
!python -m spacy download en_core_web_sm'

or
!python -m spacy download en_core_web_lg",https://stackoverflow.com/questions/58057021,python,23-09-2019 06:51,18364.0,5.0,7.0,True,11-07-2023 08:13,18-03-2021 09:07
67348511,`highway.forward: input must be present` in elmo embedding?,"i use elmo embeddings for my nlp task. the pretrain was in the indonesian language from this git. importing the library by using the syntax
from elmoformanylangs import embedder
causing the following error:
typeerror: highway.forward: input must be present
please help me to understand what the error message means.","['python', 'nlp', 'embedding', 'word-embedding', 'elmo']",67539473,"not sure if this helps, but this refers to the unimplemented superclass method (forward) in torch.nn.module. this class has the following definiton.
forward: callable[..., any] = _forward_unimplemented
if you scroll down a bit you will see the definiton of _forward_unimplemented:
def _forward_unimplemented(self, *input: any) -> none:

the highway forward definiton has to match this signature too, therefore you will need a *input argument too.
i got my hungarian version working with the following signature and first line, probably this could help you too.
    def forward(self, *input: torch.tensor) -> type(none): #pylint: disable=arguments-differ
    current_input = input[0]

i just edited my \elmoformanylangs\modules\highway.py file under the site-packages of my python environment, and got it working.",https://stackoverflow.com/questions/67348511,python,01-05-2021 16:31,499.0,0.0,3.0,True,25-05-2022 12:45,02-05-2021 07:42
74344614,"is there a way in python to extract only the core text (without boxes, footer etc.) from a pdf?","i am trying to extract only the core text from a ""rich"" pdf document, meaning that it has a lot of tables, graphs, boxes, footers etc.  in which i am not interested in.
i tried with some common python packages like pypdf2, pdfplumber or pdfreader.the problem is that apparently they extract all the text present in the pdf, including those parts listed above in which i am not interested.
as an example:
from pypdf2 import pdfreader
file = pdfreader(file)
page = file.pages[10] 
text = page.extract_text()

this code will get me the whole text from page 11, including footers, box, text from a table and the number of the page, while what i would like is only the core text.
unluckily the only solution i found up to now is to copy paste in another file the core text.
is there any method/package which can automatically recognize the main text from the other parts of the pdf and return me only that?
thank you for your help!!!","['python', 'text', 'text-mining', 'text-extraction', 'pdfplumber']",74524833,"per  d.l's comment, please add some reproducible code and, preferably, a pdf to work with.
however, i think i can answer at least part of your question. jsvine's pdfplumber is an incredibly robust python pdf processing package. pdfplumber contains a bounding box functionality that lets you extract text from within (.within_bbox(...)) or from outside (.outside_bbox) the 'bounding box' -- or geographical area -- delineated on the page object. every character object extracted from the page contains location information such as y1 - distance of top of character from bottom of page and distance of left side of character from left side of page. if the majority of pages within the .pdf you are trying to extract text from contain footnotes, i would recommend only extracting text above the y1 value. given that footnotes are typically well below the end of a page, except for academic papers using chicago style citations, you should still be able to set a standard .bbox for where you want to extract text (within a set .bbox that does not include footnotes or out of a set .bbox that does not include footnotes).
to your question about tables, that poses a trickier question. tables are by far the trickiest thing to detect and/or extract from. pdfplumber offers, to my knowledge, the most robust open source table detection/extraction capabilities out there. to extract the area outside a table, i would call the .find_tables(...) function on each page object to return a .bbox of the table and extract around that. however -- this is not perfect. it is not always able to detect tables.
regarding your 3rd question, how to exclude boxes, are you referring to text boxes? please provide further clarification!
finally -- to reiterate my first point -- pdfplumber is an incredibly robust package. that being said, extracting text from .pdf files is really tough. good luck -- please provide more information and i will be happy to help as best i can.",https://stackoverflow.com/questions/74344614,python,07-11-2022 09:48,6412.0,3.0,2.0,True,07-02-2023 18:57,07-11-2022 10:14
72747399,finetuning layoutlm on funsd-like dataset - index out of range in self,"i'm experimenting with huggingface transformers to finetune microsoft/layoutlmv2-base-uncased through automodelfortokenclassification on my custom dataset that is similar to funsd (pre-processed and normalized). after a few iterations of training i get this error :
 traceback (most recent call last):
  file ""layoutlmv2/train.py"", line 137, in <module>
    trainer.train()
  file ""..../lib/python3.8/site-packages/transformers/trainer.py"", line 1409, in train
    return inner_training_loop(
  file ""..../lib/python3.8/site-packages/transformers/trainer.py"", line 1651, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  file ""..../lib/python3.8/site-packages/transformers/trainer.py"", line 2345, in training_step
    loss = self.compute_loss(model, inputs)
  file ""..../lib/python3.8/site-packages/transformers/trainer.py"", line 2377, in compute_loss
    outputs = model(**inputs)
  file ""..../lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  file ""..../lib/python3.8/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py"", line 1228, in forward
    outputs = self.layoutlmv2(
  file ""..../lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  file ""..../lib/python3.8/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py"", line 902, in forward
    text_layout_emb = self._calc_text_embeddings(
  file ""..../lib/python3.8/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py"", line 753, in _calc_text_embeddings
    spatial_position_embeddings = self.embeddings._calc_spatial_position_embeddings(bbox)
  file ""..../lib/python3.8/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py"", line 93, in _calc_spatial_position_embeddings
    h_position_embeddings = self.h_position_embeddings(bbox[:, :, 3] - bbox[:, :, 1])
  file ""..../lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  file ""..../lib/python3.8/site-packages/torch/nn/modules/sparse.py"", line 158, in forward
    return f.embedding(
  file ""..../lib/python3.8/site-packages/torch/nn/functional.py"", line 2203, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
indexerror: index out of range in self

after further inspection (vocab size, bboxes, dimensions, classes...) i noticed that there's negative values inside the input tensor causing the error. while input tensors of successful previous iterations have unsigned integers only. these negative numbers are returned by _calc_spatial_position_embeddings(self, bbox) in modeling_layoutlmv2.py
line 92 :
h_position_embeddings = self.h_position_embeddings(bbox[:, :, 3] - bbox[:, :, 1])


what may cause the returned input values to be negative?
what could i do to prevent this error from happening?


example of the input tensor that triggers the error in torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) :
tensor([[ 0, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 11, 11,  9,  9,  9,  9,  9,  9,  9,  9,  9,
          9,  9,  9,  9,  9,  9,  9, 10, 10, 12, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12,
         12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
         11, 11, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,
         12, 12, 12, 12, 12, 12, 12, 12,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,
          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,
          8,  5,  5,  5,  5,  5,  5, -6, -6, -6, -6, -6, -6,  1,  1,  1,  1,  1,
          5,  5,  5,  5,  5,  5,  7,  5,  7,  7,  0,  0,  0,  0,  0,  0,  0,  0,
          0,  0,  0,  0,  0,  0,  0,  0]])","['neural-network', 'nlp', 'pytorch', 'huggingface-transformers']",72755675,"after double checking the dataset and specifically the coordinates of the labels, i've found that some rows bbox coordinates lead to zero width or height. here's a simplified example:
x1, y1, x2, y2 = dataset_row[""bbox""]
print((x2-x1 < 1) or (y2-y1 < 1)) #output is sometimes true

after removing these labels from the dataset, the issue was resolved.",https://stackoverflow.com/questions/72747399,neural-network,24-06-2022 17:13,1255.0,1.0,2.0,True,04-03-2023 13:22,26-06-2022 10:01
72635517,how can i separate words in a corpus according to their pos?,"iï¿½ï¿½ï¿½m exploring a textual corpus and i would like to be able to separate words following their grammatical type, for example consider only verbs and nouns.
i use spacyr to do lemmatization with the spacy_parse() function and have seen in quanteda reference ( that there is a as.tokens() function that let me build a token object with the result of spacy_parse().
as.tokens(
  x,
  concatenator = ""/"",
  include_pos = c(""none"", ""pos"", ""tag""),
  use_lemma = false,
  ...
)

this way, i can get back something that looks like this (text is in french):
etu1_repres_1 :
 [1] ""ok/propn""        "",/punct""         ""dï¿½ï¿½jï¿½ï¿½/adv""        "",/punct""         ""je/pron""         ""pense/verb""      ""que/sconj""      
 [8] ""je/pron""         ""ï¿½ï¿½tre/aux""        ""influencer/verb"" ""de/adp""          ""par/adp""

letï¿½ï¿½ï¿½s say i would like to separate the tokens and keep only tokens of type pron and verb.
q1: how can i separate them from the other tokens to keep only:
e ""je/pron""         ""pense/verb""  ""je/pron""        ""influencer/verb""

q2: how can i do to remove the ""/pron"" or ""/verb"" part of each token to be able to build a data-feature matrix with only the lemmas.
thanks a lot for helping,
gabriel","['r', 'spacy', 'quanteda']",73412973,"library(""quanteda"")
#> package version: 3.2.1
#> unicode version: 14.0
#> icu version: 70.1
#> parallel computing: 10 of 10 threads used.
#> see  for tutorials and examples.

toks <- 
  as.tokens(list(etu1_repres_1 = c(""ok/propn"", "",/punct"", ""dï¿½ï¿½jï¿½ï¿½/adv"", "",/punct"", 
                                   ""je/pron"", ""pense/verb"", ""que/sconj"", ""je/pron"", 
                                   ""ï¿½ï¿½tre/aux"", ""influencer/verb"", ""de/adp"", ""par/adp"")))

# part 1
toks2 <- tokens_keep(toks, c(""*/pron"", ""*/verb""))
toks2
#> tokens consisting of 1 document.
#> etu1_repres_1 :
#> [1] ""je/pron""         ""pense/verb""      ""je/pron""         ""influencer/verb""

# part 2
toks3 <- tokens_split(toks2, ""/"") |>
  tokens_remove(c(""pron"", ""verb""))
toks3s consisting of 1 document.
#> etu1_repres_1 :
#> [1] ""je""         ""pense""      ""je""         ""influencer""
dfm(toks3)
#> document-feature matrix of: 1 document, 3 features (0.00% sparse) and 0 docvars.
#>                features
#> docs            je pense influencer
#>   etu1_repres_1  2     1          1

created on 2022-08-19 by the reprex package (v2.0.1)",https://stackoverflow.com/questions/72635517,r,15-06-2022 17:27,121.0,0.0,1.0,True,19-08-2022 07:02,21-06-2022 15:38
1269146,how to strip headers/footers from project gutenberg texts?,"i've tried various methods to strip the license from project gutenberg texts, for use as a corpus for a language learning project, but i can't seem to come up with an unsupervised, reliable approach.  the best heuristic i've come up with so far is stripping the first twenty eight lines and the last 398, which worked for a large number of the texts.  any suggestions as to ways i can automatically strip the text (which is very similar for lots of the texts, but with slight differences in each case, and a few different templates, as well), as well as suggestions for how to verify that the text has been stripped accurately, would be very useful.","nlp, text-processing, heuristics, corpus, stripping",1273560,"you weren't kidding. it's almost as if they were trying to make the job ai-complete. i can think of only two approaches, neither of them perfect.
1) set up a script in, say, perl, to tackle the most common patterns (e.g., look for the phrase ""produced by"", keep going down to the next blank line and cut there) but put in lots of assertions about what's expected (e.g. the next text should be the title or author). that way when the pattern fails, you'll know it. the first time a pattern fails, do it by hand. the second time, modify the script.
2) try amazon's mechanical turk.",https://stackoverflow.com/q/1269146,"nlp, text-processing, heuristics, corpus, stripping",12-08-2009 22:48,4184.0,22.0,4.0,True,05-11-2023 23:28,14-04-2011 14:54
72001249,how to get up and running with spacy for vietnamese?,"i success with english
python -m spacy download en_core_web_lg
python -m spacy download en_core_web_sm

python -m spacy download en

i read  . how to use with vietnamese use xx?","['python', 'nlp', 'spacy']",72001339,"call python terminal from anaconda navigator.
python -m spacy download xx_sent_ud_sm

or
python -m spacy download xx_ent_wiki_sm

test
import spacy
from spacy import displacy

nlp = spacy.load(""xx_sent_ud_sm"")
doc = nlp(""hï¿½ï¿½m nay trï¿½ï¿½ï¿½i nï¿½ï¿½ï¿½ng to"")
displacy.serve(doc, style=""dep"")

or""lang-py prettyprint-override"">import spacy
from spacy import displacy

nlp = spacy.load(""xx_ent_wiki_sm"")
doc = nlp(""hï¿½ï¿½m nay trï¿½ï¿½ï¿½i nï¿½ï¿½ï¿½ng to"")
displacy.serve(doc, style=""ent"")

resu"" rel=""nofollow noreferrer"">",https://stackoverflow.com/questions/72001249,python,25-04-2022 14:33,2348.0,0.0,2.0,True,04-04-2024 03:53,07-05-2022 07:03
76170604,huggingface - pipeline with a fine-tuned pre-trained model errors,"i have a pre-trained model from facebook/bart-large-mnli i used the trainer in order to train it on my own dataset.
model = bartforsequenceclassification.from_pretrained(""facebook/bart-large-mnli"", num_labels=14, ignore_mismatched_sizes=true)

and then after i train it, i try to use the following (creating a pipeline with the fine-tuned model):
# import the transformers pipeline library
from transformers import pipeline

# initializing zero-shot classifier
classifier = pipeline(""zero-shot-classification"", model=model, tokenizer=tokenizer, id2label=id2label)

i get the following error from it:

failed to determine 'entailment' label id from the label2id mapping in the model config. setting to -1. define a descriptive label2id mapping in the model config to ensure correct outputs.

i tried searching the web for a solution but i can't find anything, you can refer to my previous question when i had trouble training it here

how to solve the first error:
applying this solves the first error.
second error:
i'm getting the following error:
runtimeerror: expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)

i tried deleting my custom metrics and it fixed it for a while but it didn't last, this error keeps coming.
the error is coming from here:
sequences = ""some text sequence""
classifier = pipeline(""zero-shot-classification"", model=model, tokenizer=tokenizer)
classifier(sequences, list(id2label.values()), multi_label=false)
# id2label is a dictionary mapping each label to its integer id

i also tried trainer.save_model(actual_model) but it saved only some of the stuff and when i loaded it it was like i didn't train it at all.

if i change the line to:
classifier = pipeline(""zero-shot-classification"", model=model, tokenizer=tokenizer) # old

classifier = pipeline(""zero-shot-classification"", model=model.to('cpu'), tokenizer=tokenizer) # new

it works fine, but if i change it to:
classifier = pipeline(""zero-shot-classification"", model=model.to('cuda'), tokenizer=tokenizer)

i get the same error too, my model was trained on a gpu cluster and iw ant to test it as such, is it possible of am i missing something?

from what i checked the option the to function can get are: cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, privateuseone","['python', 'pipeline', 'huggingface-transformers', 'text-classification', 'huggingface']",76273808,"after the model training, your model seems to be still placed on your gpu. the error message you receive:

runtimeerror: expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)

is thrown, because the input tensors that are generated from the pipeline are still on cpu. that is also the reason why the pipeline works as expected when you move the model to cpu with model.to('cpu').
per default, the pipeline will perform its actions on cpu, you change that behavior by specifying the device parameter.
# cuda
classifier = pipeline(""zero-shot-classification"", model=model, tokenizer=tokenizer, device=0)

#cpu
classifier = pipeline(""zero-shot-classification"", model=model, tokenizer=tokenizer, device=""cpu"")",https://stackoverflow.com/questions/76170604,python,04-05-2023 07:32,3103.0,2.0,1.0,True,18-05-2023 04:53,18-05-2023 04:53
72729360,how to train a ml model converting text to code,"looking for a working example colab/notebook showing training or fine-tuning of a text generation model capable of converting ""short text"" -> ""programming code text"".
i'm learning the topic and would like to fine-tune it with a custom metric on some public github repos.
all i found so far are models that ""continue a sentence"" or simply generate the text out of the blue. many thanks!","['machine-learning', 'deep-learning', 'nlp', 'code-generation', 'huggingface-transformers']",72729614,"first, you can see codexglue and their repository, we have four categories:

code-code (clone detection, defect detection, cloze test, code completion, code repair, and code-to-code translation)
text-code (natural language code search, text-to-code generation)
code-text (code summarization)
text-text (documentation translation)

you want text-to-code generation task. base benchmark on codexglue, one of the best models for this task is cotext. cotext support these programming languages : ""go"" ,""java"", ""javascript"", ""php"", ""python"", ""ruby"". you can find the pre-trained of this model on huggingface from here and explaining about how to fine-tune this here.",https://stackoverflow.com/questions/72729360,machine-learning,23-06-2022 11:26,1669.0,0.0,1.0,True,23-06-2022 13:07,23-06-2022 11:46
1598940,"in natural language processing, what is the purpose of chunking?","in natural language processing, what is the purpose of chunking?","computer-science, nlp",1600249,"chunking is also called shallow parsing and it's basically the identification of parts of speech and short phrases (like noun phrases).  part of speech tagging tells you whether words are nouns, verbs, adjectives, etc, but it doesn't give you any clue about the structure of the sentence or phrases in the sentence.  sometimes it's useful to have more information than just the parts of speech of words, but you don't need the full parse tree that you would get from parsing.  
an example of when chunking might be preferable is named entity recognition.  in ner, your goal is to find named entities, which tend to be noun phrases (though aren't always), so you would want to know that president barack obama is in the following sentence:

president barack obama criticized insurance companies and banks as he urged supporters to pressure congress to back his moves to revamp the health-care system and overhaul financial regulations. (source)

but you wouldn't necessarily care that he is the subject of the sentence.
chunking has also been fairly commonly used as a preprocessing step for other tasks like example-based machine translation, natural language understanding, speech generation, and others.",https://stackoverflow.com/q/1598940,"computer-science, nlp",21-10-2009 05:51,19231.0,19.0,3.0,True,21-03-2023 03:28,18-01-2018 16:15
76233163,"valueerror: `run` not supported when there is not exactly one output key. got [&#39;answer&#39;, &#39;sources&#39;, &#39;source_documents&#39;]. (langchain/streamlit)","i got an error says
valueerror: `run` not supported when there is not exactly one output key. got ['answer', 'sources', 'source_documents'].

here's the traceback error
file ""c:\users\science-01\anaconda3\envs\gpt-dev\lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py"", line 565, in _run_script
    exec(code, module.__dict__)
file ""c:\users\science-01\documents\working folder\chat bot\streamlit\alpha-test.py"", line 67, in <module>
    response = chain.run(prompt, return_only_outputs=true)
file ""c:\users\science-01\anaconda3\envs\gpt-dev\lib\site-packages\langchain\chains\base.py"", line 228, in run
    raise valueerror(

i tried to run langchain on streamlit. i use retrievalqawithsourceschain and chatprompttemplate
here is my code
import os

import streamlit as st

from apikey import apikey

from langchain.document_loaders import pypdfloader
from langchain.document_loaders import directoryloader
from langchain.text_splitter import recursivecharactertextsplitter
from langchain.embeddings.openai import openaiembeddings
from langchain.vectorstores import chroma
from langchain.chains import retrievalqawithsourceschain
from langchain.llms import openai

from langchain.prompts.chat import (
    chatprompttemplate,
    systemmessageprompttemplate,
    humanmessageprompttemplate,
)

from langchain.chat_models import chatopenai

os.environ['openai_api_key'] = apikey

st.title('ï¿½ï¿½ï¿½ï¿½ openai testing')
prompt = st.text_input('put your prompt here')

loader = directoryloader('./',glob='./*.pdf', loader_cls=pypdfloader)
pages = loader.load_and_split()

text_splitter = recursivecharactertextsplitter(
    chunk_size = 1000,
    chunk_overlap  = 200,
    length_function = len,
)

docs = text_splitter.split_documents(pages)
embeddings = openaiembeddings()

docsearch = chroma.from_documents(docs, embeddings)

system_template = """"""
use the following p context to answer the users question.
if you don't know the answer, just say that ""i don't know"", don't try to make up an answer.
----------------
{summaries}""""""

messages = [
    systemmessageprompttemplate.from_template(system_template),
    humanmessageprompttemplate.from_template(""{question}"")
]
prompt = chatprompttemplate.from_messages(messages)

chain_type_kwargs = {""prompt"": prompt}
llm = chatopenai(model_name=""gpt-3.5-turbo"", temperature=0, max_tokens=256)  # modify model_name if you have access to gpt-4
chain = retrievalqawithsourceschain.from_chain_type(
    llm=llm,
    chain_type=""stuff"",
    retriever=docsearch.as_retriever(search_kwargs={'k':2}),
    return_source_documents=true,
    chain_type_kwargs=chain_type_kwargs
)

if prompt:
    response = chain.run(prompt, return_only_outputs=true)
    st.write(response)

it seems like the error is in chain.run(), anyone know how to solve this error?","['python', 'openai-api', 'langchain', 'py-langchain']",76431181,"i found the solution, change this code
if prompt:
    response = chain.run(prompt, return_only_outputs=true)
    st.write(response)

to this
if st.button('generate'):
    if prompt:
        with st.spinner('generating response...'):
            response = chain({""question"": prompt}, return_only_outputs=true)
            answer = response['answer']
            st.write(answer)
    else:
        st.warning('please enter your prompt')

i also added st.button, st.spinner, and st.warning (optional)",https://stackoverflow.com/questions/76233163,python,12-05-2023 04:38,13694.0,5.0,2.0,True,27-09-2024 12:28,27-09-2024 12:28
77304286,value from redux is different inside component body then in the return function,"i am trying to get the latest value of history from redux store to pass in a payload. the issue is whenever i submit a query, the history is getting updated and looping with map() inside return() it is showing the latest value but at // console here inside submitquery() it is keep showing old values.
export default function chat() {
  const dispatch = usedispatch();
  const { custominstructionid, chatid } = useparams()
  const { history } = useselector((state) => state.customdata)
  const [query, setquery] = usestate('');
  const [loading, setloading] = usestate(false)

  const handlesubmitquery = async () => {
    if(!query || query === """" || query === null) {
      return notification(""query can't be empty."",  ""error"")
    }
    setloading(true)
    const data = { id: math.random(), role: 'user', content: query }
    setquery("""")
    dispatch(adddata(data))
    const config = { query: [...history, data], key: custominstructionid }
    try {
      const response = await fetch("" {
        method: ""post"",
        headers: {
          accept: ""application/json, text/plain, */*"",
          ""content-type"": ""application/json"",
          authorization: `bearer ${token}`
        },
        body: json.stringify(config),
      });
      if (!response.ok || !response.body) {
        throw response.statustext;
      }
      const reader = response.body.getreader();
      const decoder = new textdecoder();
      const looprunner = true;
      let str = """"
      let count = 0
      const resid = math.random()
      setloading(false);
      /* eslint-disable no-await-in-loop */
      while (looprunner) {
        const { value, done } = await reader.read();
        if (done) {
            // console here
          handlesavehistory()
          break;
        }
        const decodedchunk = decoder.decode(value, { stream: true });
        if(value[0] === 10){
          str += ""\n""
        } else {
          str += decodedchunk
        }
        if(count === 0){
          dispatch(adddata({ id: resid, role: 'assistant', content: str }))
        } else {
          dispatch(updatedata({ id: resid, content: str }))
        }
        count += 1
      }
      // console here
    } catch (err) {
      console.error(err, ""err"");
    } finally {
      setloading(false);
    }
  }

  const handlesavehistory = async () => {
    // console here
    const config = savehistory({ history, custominstructionid, chatid }, token)
    axios(config)
    .then((response) => {
      console.log(response.data)
    })
    .catch((error) => {
      console.log(error)
      notification(`unable to save the history.`, ""error"")
    })
  }

  const handlekeydown = (event) => {
    if (event.key === 'enter') {
        handlesubmitquery();
    }
  };

  return (
    <page title=""query"" sx={{ height: '100%' }}>
      <container sx={{ height: '100%' }}>
        <box sx={{ display: 'flex', flexdirection: 'column', justifycontent: 'space-between', height: '100%' }}>
          <box sx={{ overflowy: 'scroll', height: '70vh'}}>
            {(history && history.length > 0) ? history.map((item) =>
              <card key={item.id} sx={{ marginbottom: '1rem' }}>
                <typography sx={{ padding: '1rem', backgroundcolor: item.role === 'user' ? '#f8eff0' : '#edeff1' }}>
                  <div dangerouslysetinnerhtml={{ __html: item.content.replace(/\n/g, '<br />') }} />
                </typography>
              </card>
            ) : 
            <card sx={{ marginbottom: '1rem' }}>
              <typography sx={{ padding: '1rem', backgroundcolor: '#edeff1' }}>hi! how can i assist you today?</typography>
            </card>
            }
            {loading && loading === true &&
            <card sx={{ marginbottom: '1rem' }}>
              <typography sx={{ padding: '1rem', backgroundcolor: '#edeff1' }}>
                <skeleton variant=""text"" sx={{ fontsize: '1rem' }} />
              </typography>
            </card>
            }
          </box>
        </box>
      </container>
    </page>
  );
}

redux store:
export const counterslice = createslice({
  name: ""customdata"",
  initialstate: {
    history: [],
    boolean: true,
    string: """"
  },
  reducers: {
    adddata: (state, action) => {
      state.history.push(action.payload)
    },
    updatedata: (state, action) => {
      state.history = state.history.map(data => {
        if (data.id === action.payload.id) {
          return {
            ...data,
            content: action.payload.content
          };
        }
        return data;
      });
    },
    removedata: (state, action) => {
      state.history = state.history.filter(data => data.id !== action.payload)
    },
    recentdata: (state, action) => {
      state.history = action.payload
    },
    cleardata: (state) => {
      state.history = []
    }
  }
});

export const { adddata, updatedata, removedata, cleardata, recentdata } = counterslice.actions;

export default counterslice.reducer;

note: it is not the whole code, just showing the context of the issue. if anyone know why is this occuring.
in the below image, history is indeed getting updating with dispatch(), and i am able to see it in the ui (it is coming word by word just like in chatgpt). when the whole response has finished then i am executing the savehistory function, but in that function it is not taking the most recent question/answer.

history(in ui):

abc
def
ghi
jkl

history(in savehistory func):

abc
def

update:
the issue is when i call save func inside handlesubmit func. below solved the issue but i know it is not a good practice.
  useeffect(() => {
    if(ready === true) {
      handlesavehistory()
      setready(false)
    }
  }, [ready]);","['javascript', 'reactjs', 'redux', 'openai-api']",77309679,"async work needs to be done outside the component using createasyncthunk. the following should go in your redux store:
const handlesavehistory = createasyncthunk(
  'customdata/handlesavehistory',
  async (custominstructionid, chatid, thunkapi) => {
    // console here
    const config = thunkapi.dispatch(savehistory({
      history: thunkapi.getstate().history,
      custominstructionid,
      chatid,
    }, token))
    axios(config)
      .then((response) => {
        console.log(response.data)
      })
      .catch((error) => {
        console.log(error)
        notification(`unable to save the history.`, ""error"")
      })
  } 
);

const fetchuserbyid = createasyncthunk(
  'customdata/handlesubmitquery',
  async (query, custominstructionid, chatid, thunkapi) => {
    if(!query || query === """" || query === null) {
      return notification(""query can't be empty."",  ""error"")
    }
    const data = { id: math.random(), role: 'user', content: query }
    thunkapi.dispatch(adddata(data))
    const config = { query: [...history, data], key: custominstructionid }
    try {
      const response = await fetch("" {
        method: ""post"",
        headers: {
          accept: ""application/json, text/plain, */*"",
          ""content-type"": ""application/json"",
          authorization: `bearer ${token}`
        },
        body: json.stringify(config),
      });
      if (!response.ok || !response.body) {
        throw response.statustext;
      }
      const reader = response.body.getreader();
      const decoder = new textdecoder();
      const looprunner = true;
      let str = """"
      let count = 0
      const resid = math.random()
      /* eslint-disable no-await-in-loop */
      while (looprunner) {
        const { value, done } = await reader.read();
        if (done) {
            // console here
          thunkapi.dispatch(handlesavehistory(custominstructionid, chatid))
          break;
        }
        const decodedchunk = decoder.decode(value, { stream: true });
        if(value[0] === 10){
          str += ""\n""
        } else {
          str += decodedchunk
        }
        if(count === 0){
          thunkapi.dispatch(adddata({ id: resid, role: 'assistant', content: str }))
        } else {
          thunkapi.dispatch(updatedata({ id: resid, content: str }))
        }
        count += 1
      }
      // console here
    } catch (err) {
      console.error(err, ""err"");
    }
  }
)

the issue is your hook handlesavehistory function closed over an old version of the data. you have async functions that are executing in a previously-rendered version of your component.
initial state:
chat1 (visible to user)
 - history1
 - handlesubmitquery1
 - handlesavehistory1

next state:
(re-rendered after executing `await fetch`)

chat1 (dead)                      chat2 (visible to user)
 - history1                        - history2
 - handlesubmitquery1 (awaiting)   - handlesubmitquery2
 - handlesavehistory1              - handlesavehistory2

now the fetch returns in the old, dead chat1 component
next state:
chat1 (dead)                      chat2 (visible to user)
 - history1                        - history2
 - handlesubmitquery1              - handlesubmitquery2
 - handlesavehistory1 (console!)   - handlesavehistory2",https://stackoverflow.com/questions/77304286,javascript,16-10-2023 18:45,49.0,0.0,1.0,True,17-10-2023 19:12,17-10-2023 18:03
78821038,colab: not enough ram to load llama 3,"i was following a tutorial in youtube, when wanted to load llama3 8b:
model_name = ""meta-llama/meta-llama-3-8b-instruct""

tokenizer = autotokenizer.from_pretrained(model_name, use_auth_token=hugging_face_key)
model = automodelforcausallm.from_pretrained(model_name, use_auth_token=hugging_face_key)

got:
your session has failed because all available ram has been used

tried: model = automodelforcausallm.from_pretrained(model_name, use_auth_token=hugging_face_key, low_cpu_mem_usage=true)
but again the same error","['python', 'machine-learning', 'google-colaboratory', 'huggingface-transformers']",78821115,"given that itï¿½ï¿½ï¿½s an 8-billion-parameter model, requiring approximately 16gb of space, free colab notebooks lack the capacity to load it",https://stackoverflow.com/questions/78821038,python,01-08-2024 12:45,319.0,0.0,1.0,True,01-08-2024 21:36,01-08-2024 21:35
23704510,how do i test whether an nltk resource is already installed on the machine running my code?,"i just started my first nltk project and am confused about the proper setup. i need several resources like the punkt tokenizer and the maxent pos tagger. i myself downloaded them using the gui nltk.download(). for my collaborators i of course want that this things get downloaded automatically. i haven't found any idiomatic code for that in the docu. 
am i supposed to just put nltk.data.load('tokenizers/punkt/english.pickle') and their like into the code? is this going to download the resources every time the script is run? am i to provide feedback to the  user (i.e. my co-developers) of what is being downloaded and why this is taking so long? there must be gear out there that does the job, right? :)
//edit to explify my question: 
how do i test whether an nltk resource (like the punkt tokenizer) is already installed on the machine running my code, and install it if it is not?","['python', 'nlp', 'nltk']",23715469,"you can use the nltk.data.find() function, see 
>>> import nltk
>>> nltk.data.find('tokenizers/punkt.zip')
zipfilepathpointer(u'/home/alvas/nltk_data/tokenizers/punkt.zip', u'')

when the resource is not available you'll find the error:
traceback (most recent call last):
  file ""<stdin>"", line 1, in <module>
  file ""/usr/local/lib/python2.7/dist-packages/nltk-3.0a3-py2.7.egg/nltk/data.py"", line 615, in find
    raise lookuperror(resource_not_found)
lookuperror: 
**********************************************************************
  resource u'punkt.zip' not found.  please use the nltk downloader
  to obtain the resource:  >>> nltk.download()
  searched in:
    - '/home/alvas/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

most probably, you would like to do something like this to ensure that your collaborators have the package:
>>> try:
...     nltk.data.find('tokenizers/punkt')
... except lookuperror:
...     nltk.download('punkt')
... 
[nltk_data] downloading package punkt to /home/alvas/nltk_data...
[nltk_data]   package punkt is already up-to-date!
true",https://stackoverflow.com/questions/23704510,python,16-05-2014 21:03,21339.0,37.0,3.0,True,06-05-2024 13:59,17-05-2014 19:53
78628145,langchain pandas agent not following instructions,"i'm working with a langchain pandas agent using gpt-4 from azure openai as the llm. i'm working with a dataframe that contains enterprise data from our employees, and the main objective is to retrieve information from our employees using the agent. currently, we are having two main issues:

when asked to find information from a partial substring, the agent never retrieves the information. it either looks for an exact match or looks for a substring that contains the input.

when asked to retrieve the employee with the highest or lowest weekly hours, it does not check for ties even when instructed to look for ties. it just doesn't work.


this is the code that i'm using, along with the prefix and suffix that i'm using.
prefix:
prefix='you are a pandas agent. you must work with the dataframe df containing information about the company's employees.

your answer must only include information retrieved from df, and you must not create mockup or sample data. you will be penalized if you do.

the user may ask you questions using a substring of the names of our employees.

follow these useful instructions when retrieving information regarding an employee name:

if an exact match is found, retrieve the information in natural language.

if not, then include a str.contains search ignoring nans and case insensitive in this fashion. for example, if they ask for alice west, look for:

df['names'].str.contains('alice', case=false, na=false) & df['names'].str.contains('west', case=false, na=false)

and retrieve the information found. if we have more than 20 rows, just retrieve the information on the first 20 rows.

when sorting information like retrieving the highest or lowest values of a column, always check for ties. if there are ties, retrieve the first 3 rows of information.

for instance, if the maximum hours of weekly work happens to be 10 but more than one employee has that, then print up to 20 rows.'

suffix:
suffix ='you must answer in natural language and must never make up information. you will be penalized if you do.'

code:
data = {
    ""names"": [""john w. doe"", ""alice smith"", ""john adams jr."", ""alice johnson"", ""john jr. doe""],
    ""city"": [""new york"", ""los angeles"", ""chicago"", ""houston"", ""chicago""],
    ""station"": [""station a"", ""station b"", ""station c"", ""station d"", ""station e""],
    ""starting_year"": [2015, 2017, 2015, 2018, 2019],
    ""duration_hours_week"": [40, 35, 40, 30, 45]
}

df = pd.dataframe(data)


create_pandas_dataframe_agent(
            llm=_model,
            df=df,
            suffix=suffix,
            include_df_in_prompt=true,
            agent_type=agenttype.openai_functions,
            prefix=prefix,
            max_iterations=5,
            verbose=true)

when asked to execute this query: ""what are the weekly hours of john doe?"" it retrieves nothing.
the employee is not in the dataframe provided. please check the name of the employee. i noticed that the code the agent is using in repl  is:
df['name'].str.contains('john doe', na=ignore, case=false)

and this gives an empty dataframe. it should be:
df['name'].str.contains('john', na=false, case=false) & df['name'].str.contains('doe', na=false, case=false)

to retrieve data for: ""john w. doe"" & ""john jr. doe"".
the final error is that when asked who has the longest working hours, it goes in the repl for:
df['duration_hours_week'].nlargest(1)

so it retrieves the information of one employee, but there are ties with the highest value. the prefix instructs the model to always check for ties, and it is not working.
i'm wondering what is the best way to instruct this agent to follow the instructions given. is it better to use tools or functions for this matter?","['azure', 'openai-api', 'langchain']",78637102,"actually, to get best result you need to give proper prefix, suffix, invoke query and at last a best llm model.
with below prefix and same invoke query, i got the output as expected
prefix = """"""
you are a pandas agent. you must work with the dataframe df containing information about the company's employees.
your answer must only include information retrieved from df, and you must not create mockup or sample data. you will be penalized if you do.
the user may ask you questions using a substring of the names of our employees.
follow these useful instructions when retrieving information regarding an employee name:
if an exact match is found, retrieve the information in natural language.
if not, then include a str.contains search ignoring nans and case insensitive in this fashion. for example, if they ask for alice west, split it in two names like alice and west then look for it in dataframe:
df['names'].str.contains('alice', case=false, na=false) & df['names'].str.contains('west', case=false, na=false)
and retrieve the information found. if we have more than 20 rows, just retrieve the information on the first 20 rows.
when sorting information like retrieving the highest or lowest values of a column, always check for ties. if there are ties, retrieve the information for all employees with the tied value, up to 20 rows.
below are the columns in my dataframe to use for the query.
names,city,station,starting_year,duration_hours_week.
""""""

output:

and

so, in prefix and suffix try to give more specific with examples and dataframe detail.",https://stackoverflow.com/questions/78628145,azure,16-06-2024 02:32,399.0,0.0,1.0,True,22-06-2024 20:45,22-06-2024 20:45
62830783,"scripts missing for gpt-2 fine tune, and inference in hugging-face github?","i am following the documentation on the hugging face website, in there they say that to fine-tune gpt-2 i should use the script
run_lm_finetuning.py for fine-tuning, and the script run_generation.py
for inference.
however, both scripts don't actually exist on github anymore.
does anybody know whether the documentation is outdated? or where to find those two scripts?
thanks","['python', 'huggingface-transformers', 'language-model', 'gpt-2']",62830856,"it looks like they've been moved around a couple times and the docs are indeed out of date, the current version can be found in run_language_modeling.py here",https://stackoverflow.com/questions/62830783,python,10-07-2020 08:59,443.0,1.0,2.0,True,25-05-2022 07:40,29-11-2020 12:03
77009626,chroma document retrieval in langchain not working in flask frontend,"i am using langchain to create a chroma database to store pdf files through a flask frontend. i am able to query the database and successfully retrieve data when the python file is ran from the command line.
i am however trying to use the the feature through flask, and so far i can't get it to work. this is the class i am using to query the database:
from langchain.embeddings import openaiembeddings
from langchain.chains import retrievalqa
from langchain.llms import openai
from langchain.vectorstores import chroma

class chat_db:
    def __init__(self):
        persist_directory = 'chromadb'
        embedding = openaiembeddings()
        vectordb = chroma(persist_directory=persist_directory, embedding_function=embedding)
        retriever = vectordb.as_retriever(search_kwargs={""k"": 2})
        self.qa = retrievalqa.from_chain_type(llm=openai(), chain_type=""stuff"",
                                         retriever= retriever)
        
    def chat_over_documents(self, query):
        result = self.qa({""query"": query})
        return result['result'] 

#if __name__ == ""__main__"":
#    query = ""where was the us declaration of independence signed?""
#    vector_db = chat_db()
#    result= vector_db.chat_over_documents(query)
#    print(result)

if i uncomment the ""main"" section, i get the result i expect. if i run the query through 'app.py', i get the canned langchain result 'i'm sorry, don't know'.
this is how i am calling it from 'app.py'
## app.py
...
chat_db = chat_db()
@app.route('/message', methods=['post'])
def message():
    user_message = request.json['message']
    if ""document"" in user_message.lower():
        response = chat_db.chat_over_documents(user_message)
        return jsonify({""message"": response})
    else:
        response = assistant.chat_with_gpt(user_message)
        return jsonify({""message"": response})



i am not sure what i am doing incorrectly; any help is highly appreciated.
i have tried tracing data using the browser's developer tool and debugging in terminal. from the logs, my suspicion is that langchain is making two openai api calls even before chroma db completes initializing.","['flask', 'openai-api', 'langchain', 'chromadb']",77143558,"ultimately i went with a pure python and chroma db solution, removing langchain (only from this part as i still use langchain in other parts of the project.)
the solution that worked:
import chromadb
from chromadb.config import settings
class chat_db:
    def __init__(self, persist_directory=""../data/chromadb""):
        self.persist_directory = persist_directory             
        self.client_settings = settings(is_persistent= true, persist_directory= persist_directory, anonymized_telemetry=false)
        self.persistent_client = chromadb.client(settings= self.client_settings)
        self.doc_collection = self.persistent_client.get_or_create_collection(name = ""books"")
       
        #i thought i could just use self.persistent_client for querying but it did not work. 
        # had to create a persistentclient to use for querying.
        self.queryclient = chromadb.persistentclient(path= persist_directory, settings= self.client_settings)
        

    def chat_over_documents(self, collection_name, query, k = 5):
        collection = self.queryclient.get_collection(name=collection_name)
        results = collection.query(query_texts= query, n_results= k)
        flat_results = [item for sublist in results['documents'] for item in sublist] 
        return flat_results

   ...
   #other methods for preprocessing and adding docs to the chroma collection omitted


in app.py
...
document_handler = chat_db()
...
@app.route('/message', methods=['post'])
def message():
    user_message = request.json['message']
    if ""/document"" in user_message.lower():
        keywords = assistant.preprocess_keywords(user_message)
        vector_mgs = document_handler.chat_over_documents(""books"", keywords)

        response = assistant.query_stored_documents(user_message, vector_mgs)
        return jsonify({""message"": response})
    else:
        response = assistant.chat_with_gpt(user_message)
        return jsonify({""message"": response})

note: the assistant class uses langchain for chatting and other features. however, its query_stored_documents() method builds a prompt using system and user messages and query openai chat completion endpoint to get the desired result.",https://stackoverflow.com/questions/77009626,flask,30-08-2023 15:50,4171.0,0.0,2.0,True,14-10-2023 18:51,14-10-2023 18:51
56275467,tokenisation with spacy - how to get left and right tokens,"i am using spacy for text tokenization and getting stuck with it:
import spacy
nlp = spacy.load(""en_core_web_sm"")
mytext = ""this is some sentence that spacy will not appreciate""
doc = nlp(mytext)

for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)

returns something that seems to me to say that tokenisation was succesful: 
this this det dt nsubj xxxx true false 
is be verb vbz root xx true true 
some some det dt det xxxx true true 
sentence sentence noun nn attr xxxx true false 
that that adp in mark xxxx true true 
spacy spacy noun nn nsubj xxxx true false 
will will verb md aux xxxx true true 
not not adv rb neg xxx true true 
appreciate appreciate verb vb ccomp xxxx true false

but on the other hand
[token.text for token in doc[2].lefts]

returns an empty list. is there a bug in lefts/rights?
beginner at natural language processing, hope i am not falling into a conceptual trap. using spacy v'2.0.4'.","['python', 'nlp', 'token', 'spacy']",56298740,"this is what the dependencies of that sentence look like:
import spacy

nlp = spacy.load(""en_core_web_sm"")
doc = nlp(u""this is some sentence that spacy will not appreciate"")
for token in doc:
    print(token.text, token.dep_, token.head.text, token.head.pos_,
            [child for child in token.children])

this nsubj is verb []
is root is verb [this, sentence]
some det sentence noun []
sentence attr is verb [some, appreciate]
that mark appreciate verb []
spacy nsubj appreciate verb []
will aux appreciate verb []
not neg appreciate verb []
appreciate relcl sentence noun [that, spacy, will, not]

so we see that doc[2] (""some"") has an empty child vector. however ""is"" (doc[1]) does not. if we instead run...
print([token.text for token in doc[1].lefts])  
print([token.text for token in doc[1].rights])  

we get...
['this']
['sentence']

the functions you are using navigate the dependency tree, not the document, hence why you are getting empty results for some words.
if you just want previous and following tokens, you can just do something like...
for ix, token in enumerate(doc):
    if ix == 0:
        print('previous: %s, current: %s, next: %s' % ('', doc[ix], doc[ix + 1]))
    elif ix == (len(doc) - 1):
        print('previous: %s, current: %s, next: %s' % (doc[ix - 1], doc[ix], ''))
    else:
        print('previous: %s, current: %s, next: %s' % (doc[ix - 1], doc[ix], doc[ix + 1]))

or...
for ix, token in enumerate(doc):
    if ix > 0:
        print('previous: %s' % doc[:ix])
    print('current: %s' % doc[ix])
    print('following: %s' % doc[ix:])",https://stackoverflow.com/questions/56275467,python,23-05-2019 12:40,3150.0,0.0,1.0,True,26-11-2021 16:34,23-05-2019 13:59
2264806,how to automatically determine text quality?,"a lot of natural language processing (nlp) algorithms and libraries have a hard time working with random texts from the web, usually because they are presupposing clean, articulate writing. i can understand why that would be easier than parsing youtube comments.
my question is: given a random piece of text, is there a process to determine whether that text is well written, and is a good candidate for use in nlp? what is the general name for these algorithm?
i would appreciate links to articles, algorithms or code libraries, but i would settle for good search terms.",nlp,2269655,"'well written' and 'good for nlp' may go together but don't have to. for a text to be 'good for nlp', it maybe should contain whole sentences with a verb and a dot at the end, and it should perhaps convey some meaning. for a text to be well written it should also be well-structured, cohesive, coherent, correctly substitute nouns for pronouns, etc. what you need depends on your application. 
the chances of a sentence to be properly processed by an nlp tool can often be estimated by some simple heuristics: is it too long (>20 or 30 words, depending on the language)? too short? does it contain many weird characters? does it contain urls or email adresses? does it have a main verb? is it just a list of something? to my knowledge, there is no general name for this, nor any particular algorithm for this kind of filtering - it's called 'preprocessing'.
as to a sentence being well-written: some work has been done on automatically evaluating readability, cohesion, and coherence, e.g. the articles by miltsakaki (evaluation of text coherence for electronic essay scoring systems and real-time web text classification and analysis of reading difficulty) or higgins (evaluating multiple aspects of coherence in student essays). these approaches are all based on one or the other theory of discourse structure, such as centering theory. the articles are rather theory-heavy and assume knowledge of both centering theory as well as machine learning. nonetheless, some of these techniques have successfully been applied by ets to automatically scoring student's essays and i think this is quite similar to what you are trying to do, or at least, you may be able to adapt a few ideas. 
all this being said, i believe that within the next years, nlp will have to develop techniques  to process language which is not well-formed with respect to current standards. there is a massive amount of extremely valuable data out there on the web, consisting of exactly the kinds of text you mentioned: youtube comments, chat messages, twitter and facebook status messages, etc. all of them potentially contain very interesting information. so, who should adapt - the people wrting that way or nlp?",https://stackoverflow.com/q/2264806,nlp,15-02-2010 08:57,12912.0,24.0,6.0,True,19-09-2024 17:19,15-02-2010 13:59
796412,how to turn plural words singular?,"i'm preparing some table names for an orm, and i want to turn plural table names into single entity names. my only problem is finding an algorithm that does it reliably. here's what i'm doing right now:

if a word ends with -ies, i replace the ending with -y
if a word ends with -es, i remove this ending. this doesn't always work however - for example, it replaces types with typ
otherwise, i just remove the trailing -s

does anyone know of a better algorithm?","['algorithm', 'nlp', 'lemmatization']",796445,"those are all general rules (and good ones) but english is not a language for the faint of heart :-).
my own preference would be to have a transformation engine along with a set of transformations (surprisingly enough) for doing the actual work. you would run through the transformations (from specific to general) and, when a match was found, apply the transformation to the word and stop.
regular expressions would be an ideal approach to this due to their expressiveness. an example rule set:
 1. if the word is fish, return fish.
 2. if the word is sheep, return sheep.
 3. if the word is ""radii"", return ""radius"".
 4. if the word ends in ""ii"", replace that ""ii"" with ""us"" (octopii,virii).
 5. if a word ends with -ies, replace the ending with -y
 6. if a word ends with -es, remove it.
 7. otherwise, just remove any trailing -s.

note the requirement to keep this transformation set up to date. for example, let's say someone adds the table name types. this would currently be captured by rule #6 and you would get the singular value typ, which is obviously wrong.
the solution is to insert a new rule somewhere before #6, something like:
 3.5: if the word is ""types"", return ""type"".

for a very specific transformation, or perhaps somewhere later if it can be made more general.
in other words, you'll basically need to keep this transformation table updated as you find all those wondrous exceptions that english has spawned over the centuries.

the other possibility is to not waste your time with general rules at all.
since the use case of this requirement is currently only to singularise the table names, and that set of table names will be relatively tiny (at least compared to the set of plural english words), just create another table (or some sort of data structure) called singulars which maps all the current plural table names (employees, customers) to singular object names (employee, customer).
then every time a table is added to your schema, ensure you add an entry to the singulars ""table"" so you can singularize it.",https://stackoverflow.com/questions/796412,algorithm,28-04-2009 06:05,24550.0,24.0,13.0,True,31-05-2022 19:20,31-05-2022 19:20
76428082,how to create a entity ruler pattern that includes dot and hyphen?,"i am trying to include brazilian cpf as entity on my ner app using spacy. the current code is the follow:
import spacy
from spacy.pipeline import entityruler

nlp = spacy.load(""pt_core_news_sm"")

text = ""joï¿½ï¿½o mora na bahia, 22/11/1985, seu cpf ï¿½ï¿½ 111.222.333-11""
ruler = nlp.add_pipe(""entity_ruler"")
patterns = [
    {""label"": ""cpf"", ""pattern"": [{""shape"": ""ddd.ddd.ddd-dd""}]},
]

ruler.add_patterns(patterns)
doc = nlp(text)

#extract entities
for ent in doc.ents:
    print (ent.text, ent.label_)

the result was only:
joï¿½ï¿½o per
bahia loc

i tried using regex too:
{""label"": ""cpf"", ""pattern"": [{""text"": {""regex"": r""^\d{3}\.\d{3}\.\d{3}\-\d{2}$""}}]},

but not worked too
how can i fix that to retrie","['python', 'named-entity-recognition', 'spacy-3']",76446923,"after looking for token spacings, the brazilian tokenizer split cpf in two parts:
token_spacings = [token.text_with_ws for tokenï¿½ï¿½inï¿½ï¿½doc]

result:
['joï¿½ï¿½o ', 'mora ', 'na ', 'bahia', ', ', '22/11/1985', ', ', 'seu ', 'cpf ', 'ï¿½ï¿½ ', '111.222.',ï¿½ï¿½'333-11']

so i think you may try this:
import spacy
from spacy.pipeline import entityruler

nlp = spacy.load(""pt_core_news_sm"")

text = ""joï¿½ï¿½o mora na bahia, 22/11/1985, seu cpf ï¿½ï¿½ 111.222.333-11""
ruler = nlp.add_pipe(""entity_ruler"")
patterns = [
    {""label"": ""cpf"", ""pattern"": [
            {""shape"": ""ddd.ddd.""},
            {""shape"": ""ddd-dd""},
    ]},
]

ruler.add_patterns(patterns)
doc = nlp(text)

#extract entities
for ent in doc.ents:
    print (ent.text,",https://stackoverflow.com/questions/76428082,python,08-06-2023 01:41,177.0,1.0,1.0,True,10-06-2023 15:38,09-06-2023 17:53
71371437,why is the length of the word_index greater than num_words?,"i have a code, about text preprocessing for deep learning:
from keras.preprocessing.text import tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer = tokenizer(num_words = 10000)
tokenizer.fit_on_texts(x)
tokenizer.word_index

but when i check the length of tokenizer.word_index, safe in the knowledge to get 10000, i get 13233.the length of x is equal to 11541(a dataframe column containing 11541, if it matters to know, however). so my question arises: which is vocabulary size? num_words or the length of word_index? it seems i have confused! any helps appreciated.","['python-3.x', 'tensorflow', 'keras', 'deep-learning', 'nlp']",71375456,"according to the official docs, the argument num_words is,

the maximum number of words to keep, based on word frequency. only the most common num_words-1 words will be kept.

word_index will hold all the words which are present in texts. but the difference is observed when you use tokenizer.texts_to_sequences. for instance, let us consider some sentences,
texts = [
    'hello world' , 
    'hello python' , 
    'python' , 
    'hello java' ,
    'hello java' , 
    'hello python'
]
# frequency of words, hello -> 5, python -> 3 , java -> 2 , world -> 1
tokenizer = tf.keras.preprocessing.text.tokenizer( num_words=3 )
tokenizer.fit_on_texts( texts )
print( tokenizer.word_index )

the output of the above snippet will be,
{'hello': 1, 'python': 2, 'java': 3, 'world': 4}

according to the docs, the top num_words-1 words ( based on their frequency ) must only be used while transforming the words to indices. in our case num_words=3 and hence we'd expect the tokenizer to only use 2 words for the transformation. the two most common words in texts are hello and python. consider this example to inspect the output of texts_to_sequences
input_seq = [
    'hello' , 
    'hello java' , 
    'hello python' , 
    'hello python java'
]
print( tokenizer.texts_to_sequences( input_seq ) )

the output,
[[1], [1], [1, 2], [1, 2]]

observe that in the first sentence, hello is encoded as expected. in the second sentence, the word java isn't encoded as it was not included in the vocabulary. in the third sentence, both the words hello and python are included, which is the expected behavior as per our assumption. in the fourth sentence, the word java isn't encoded in the output.

so my question arises: which is vocabulary size? num_words or the length of word_index?

as you might have understood, num_words is the vocab size as only these many words are being encoded in the output. rest of the words, in our case java and world are simply omitted from the transformation.",https://stackoverflow.com/questions/71371437,python-3.x,06-03-2022 15:04,899.0,2.0,1.0,True,07-03-2022 01:04,06-03-2022 15:51
73195383,how spacy matcher works?,"i am trying to create a dataset for trainin using spacy matcher, so i am using the matcher explorer but i dont understand exactly how it works.
url: url-matcher
my idea is from the text in the url (malware news), label correctly the word ""conti"", however when i try it using spacy matcher, it recognize ""costa rica"", ""one"", ""attack"" and other words as ""conti""!
why is this? can somebody clarify it? how should i do it to just label ""conti"" word?
thank you","['nlp', 'spacy']",73200710,"solved!
i dont know how the official matcher explorer works but i did some tests with pycharm and it is matching correctly.",https://stackoverflow.com/questions/73195383,nlp,01-08-2022 14:38,60.0,0.0,1.0,True,22-10-2022 15:21,22-10-2022 15:21
62848208,classification: tweet sentiment analysis - order of steps,"i am currently working on a tweet sentiment analysis and have a few questions regarding the right order of the steps. please assume that the data was already preprocessed and prepared accordingly. so this is how i would proceed:

use train_test_split (80:20 ratio) to withhold a test
data set.
vectorize x_train since the tweets are not numerical.

in the next steps, i would like to identify the best classifier. please assume those were already imported. so i would go on by:

hyperparameterization (grid-search) including a cross-validation approach.
in this step, i would like to identify the best parameters of each
classifier. for knn the code is as follows:

model = kneighborsclassifier()
n_neighbors = range(1, 10, 2)
weights = ['uniform', 'distance']
metric = ['euclidean', 'manhattan', 'minkowski']

# define grid search
grid = dict(n_neighbors=n_neighbors, weights=weights ,metric=metric)
cv = repeatedstratifiedkfold(n_splits=10, n_repeats=3, random_state=1)
grid_search = gridsearchcv(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
grid_result = grid_search.fit(train_tf, y_train)

# summarize results
print(""best: %f using %s"" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print(""%f (%f) with: %r"" % (mean, stdev, param))


compare the accuracy (depending on the best hyperparameters) of the classifiers
choose the best classifier
take the withheld test data set (from train_test_split()) and use the best classifier on the test data

is this the right approach or would you recommend changing something (e. g. doing the cross-validation alone and not within the hyperparametrization)? does it make sense to test the test data as the final step or should i do it earlier to assess the accuracy for an unknown data set?","['python', 'machine-learning', 'classification', 'sentiment-analysis', 'text-classification']",62848546,"there are lots of ways to do this and people have strong opinions about it and i'm not always convinced they fully understand what they advocate.
tl;dr: your methodology looks great and you're asking sensible questions.
having said that, here are some things to consider:

why are you doing train-test split validation?
why are you doing hyperparameter tuning?
why are you doing cross-validation?

yes, each of these techniques are good at doing something specific; but that doesn't necessarily mean they should all be part of the same pipeline.
first off, let's answer these questions:

train-test split is useful for testing your classifier's inference abilities. in other words, we want to know how well a classifier performs in general (not on the data we used for training). the test portion allows us to evaluate our classifier without using our training portion.

hyperparameter-tuning is useful for evaluating the effect of hyperparameters on the performance of a classifier. for it to be meaningful, we must compare two (or more) models (using different hyperparameters) but trained preferably using the same training portion (to eliminate selection bias). what do we do once we know the best performing hyperparameters? will this set of hyperparameters always perform optimally? no. you will see that, due to the stochastic nature of classification, one hyperparameter set may work best in experiment a then another set of hyperparameters may work best on experiment b. rather, hyperparameter tuning is good for generalizing about which hyperparameters to use when building a classifier.

cross-validation is used to smooth out some of the stochastic randomness associated with building classifiers. so, a machine learning pipeline may produce a classifier that is 94% accurate using 1 test-fold and 83% accuracy using another test-fold. what does it mean? it might mean that 1 fold contains samples that are easy. or it might mean that the classifier, for whatever reason, is actually better. you don't know because it's a black box.


practically, how is this helpful?
i see little value in using test-train split and cross-validation. i use cross-validation and report accuracy as an average over the n-folds. it is already testing my classifier's performance. i don't see why dividing your training data further to do another round of train-test validation is going to help. use the average. having said that, i use the best performing model of the n-fold models created during cross-validation as my final model. as i said, it's black-box, so we can't know which model is best but, all else being equal, you may as well use the best performing one. it might actually be better.
hyperparameter-tuning is useful but it can take forever to do extensive tuning. i suggest adding hyperparameter tuning to your pipeline but only test 2 sets of hyperparameters. so, keep all your hyperparameters constant except 1. e.g. batch size = {64, 128}. run that, and you'll be able to say with confidence, ""oh, that made a big difference: 64 works better than 128!"" or ""well, that was a waste of time. it didn't make much difference either way."" if the difference is small, ignore that hyperparameter and try another pair. this way, you'll slowly tack towards optimal without all the wasted time.
in practice, i'd say leave the extensive hyperparameter-tuning to academics and take a more pragmatic approach.
but yeah, you're methodology looks good as it is. i think you thinking about what you're doing and that already puts you a step ahead of the pack.",https://stackoverflow.com/questions/62848208,python,11-07-2020 10:51,260.0,3.0,1.0,True,14-09-2022 18:24,14-09-2022 18:24
70925238,sharing spacy model between processes,"my code is using python's multiprocessing for parallel computation. as part of the computation spacy is used. is it safe to create a single spacy object with nlp = spacy.load(""de_core_news_lg"") and access it by multiple processes for named entity recognition?","['python-multiprocessing', 'spacy']",70987845,"you can take advantange of multiprocessing with spacy by passing the n_process argument to nlp.pipe. for example:
docs = [""this is the first doc"", ""this is the second doc""]

nlp = spacy.load(""en_core_web_sm"")  # use your model here

docs_tokens = []
for doc in nlp.pipe(docs, n_process=2):
    tokens = [t.text for t in doc]
    docs_tokens.append(tokens)

there's more about this in the spacy documentation, as well as this speed faq.",https://stackoverflow.com/questions/70925238,python-multiprocessing,31-01-2022 11:31,1670.0,2.0,1.0,True,29-05-2022 10:46,29-05-2022 10:46
76942351,determine category of sentence,"i have a list of books with title and organization.
i would like to get the category of each book.
category list is predefined and choose one of them.
books
title 1: agriculturalpolicy and the decisions ofagriculturalproducers as to income and investment
organization 1: western agricultural economics association
categories: agriculture, engineering, healthcare
can i find the category of the book using python?
i tried using transformers from huggingface but i don't have predefined data to train or evaluate.
i am not sure whether there is an existing solution or not.","['python', 'nlp', 'huggingface']",76951958,"if the categories are well distinct, zero-shot classification can be used without training or specifying keywords.
check out this introductory page.",https://stackoverflow.com/questions/76942351,python,21-08-2023 04:13,58.0,-1.0,1.0,True,17-09-2023 08:38,17-09-2023 08:38
71944781,error in trying to push model to huggingface,"so, i am making a chatbot for discord using google collaboratory. however whenever, i try to push it on huggingface.co. it does deploy and instead gives me an error. also, while trying to clone the repo, it's giving me an error fatal: destination path 'dialogpt-small-technoblade' already exists and is not an empty directory.
the error that is being shown
the code i'm using
!pip install huggingface_hub
!sudo apt-get install git-lfs
!git config --global user.email ""email""
!git config --global user.name ""username""

!transformers-cli login
!git clone 
!ls -al

!git lfs install
!git add .
!git commit -m ""initial commit""

!git log
!git push","['git', 'google-colaboratory', 'huggingface-transformers']",71944840,"to my knowledge, all you have to do is delete a folder on your drive to the name of dialogpt-small-technoblade.
otherwise, i don't know. see what the results are if you try it on a vm (for linux, i recommend virtualbox)",https://stackoverflow.com/questions/71944781,git,20-04-2022 18:49,2002.0,0.0,1.0,True,20-04-2022 21:36,20-04-2022 21:36
69701424,removing duplicate values from list (python),"i'm try to make an inverted index for some nlp to see how many times a word appears in a document. i'm doing this via a dictionary but my output is like this (here the word man appears in documents 1 and 11)
{'man': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
 1, 1, 1, 1, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11],
 'upon': [1, 1, 1, 3, 3, 3, 1539, 1539, 1539]}

how do i get rid of these duplicate values so i just have
{'man': [1,11], 'upon': [1,3,1539]}","['python', 'dictionary', 'indexing', 'nlp']",69701429,"just convert values to sets and then back to lists:
my_dict = {k: list(set(v)) for k, v in my_dict.items()}",https://stackoverflow.com/questions/69701424,python,24-10-2021 23:11,75.0,-2.0,1.0,True,27-10-2021 13:13,27-10-2021 13:13
61945586,how to check term similarity within a pandas column with similarity.jarowinkler,"i would need to check if two or more words in a list are similar. 
to do this, i am using the jaro wrinkler distance as follows:
from similarity.jarowinkler import jarowinkler

word1='sweet chili'
word2='sriracha chilli'

jarowinkler = jarowinkler()
print(jarowinkler.similarity(word1, word2))

it seems to be able to detect the similarity between words, but i would need to set a threshold to select only words that are similar at 80%. 
my difficulties, however, are in checking all the words within a data frame's column: 
words

sweet chili
sriracha chilli
tomato
mayonnaise 
water
milk
still water
sparkling water
wine
chicken 
beef
...

what i would like to do is: 
- starting with the first element, check the similarity between this one and the others; if the similarity is greater than a threshold (80%), save it in a new array;
- check the second element (sriracha chilli) as above; 
- and so on. 
could you please tell me how to run such a similar loop?","['python', 'pandas', 'nlp', 'cosine-similarity']",61946055,"with the given data
using the strsim package
if the real dataframe has many columns, consider making a dataframe with just the words column


new_df = pd.dataframe({'words': df.words})


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from similarity.jarowinkler import jarowinkler
import numpy as np

df = pd.dataframe({'words': ['sweet chili', 'sriracha chilli', 'tomato', 'mayonnaise ', 'water', 'milk', 'still water', 'sparkling water', 'wine', 'chicken ', 'beef']})

# call similarity method
jarowinkler = jarowinkler()

# remove whitespace
df.words = df.words.str.strip()

# create column of matching values for each word
words = df.words.tolist()

for word in words:
    df[word] = df.words.apply(lambda x: jarowinkler.similarity(x, word))

|    | words           |   sweet chili |   sriracha chilli |   tomato |   mayonnaise |    water |     milk |   still water |   sparkling water |     wine |   chicken |     beef |
|---:|:----------------|--------------:|------------------:|---------:|-------------:|---------:|---------:|--------------:|------------------:|---------:|----------:|---------:|
|  0 | sweet chili     |      1        |          0.605772 | 0.419192 |     0.39697  | 0.513131 | 0        |      0.515152 |          0.460101 | 0.560606 |  0.322511 | 0.560606 |
|  1 | sriracha chilli |      0.605772 |          1        | 0.411111 |     0.388889 | 0.344444 | 0.438889 |      0.460101 |          0.488889 | 0.438889 |  0.529365 | 0        |
|  2 | tomato          |      0.419192 |          0.411111 | 1        |     0.488889 | 0.411111 | 0.472222 |      0.590909 |          0.411111 | 0        |  0        | 0        |
|  3 | mayonnaise      |      0.39697  |          0.388889 | 0.488889 |     1        | 0.433333 | 0.45     |      0.460606 |          0.544444 | 0.45     |  0.328571 | 0        |
|  4 | water           |      0.513131 |          0.344444 | 0.411111 |     0.433333 | 1        | 0        |      0.430303 |          0.511111 | 0.633333 |  0.447619 | 0.483333 |
|  5 | milk            |      0        |          0.438889 | 0.472222 |     0.45     | 0        | 1        |      0.560606 |          0.538889 | 0.5      |  0.595238 | 0        |
|  6 | still water     |      0.515152 |          0.460101 | 0.590909 |     0.460606 | 0.430303 | 0.560606 |      1        |          0.749854 | 0.44697  |  0.489177 | 0        |
|  7 | sparkling water |      0.460101 |          0.488889 | 0.411111 |     0.544444 | 0.511111 | 0.538889 |      0.749854 |          1        | 0.544444 |  0.431746 | 0        |
|  8 | wine            |      0.560606 |          0.438889 | 0        |     0.45     | 0.633333 | 0.5      |      0.44697  |          0.544444 | 1        |  0.595238 | 0.5      |
|  9 | chicken         |      0.322511 |          0.529365 | 0        |     0.328571 | 0.447619 | 0.595238 |      0.489177 |          0.431746 | 0.595238 |  1        | 0        |
| 10 | beef            |      0.560606 |          0        | 0        |     0        | 0.483333 | 0        |      0        |          0        | 0.5      |  0        | 1        |

see values greater than 80%

none except the exact matching values

df.set_index('words', inplace=true)

np.where(df[words] > 0.8, df[words], np.nan)

array([[ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan,  1., nan, nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan,  1., nan, nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan,  1., nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan,  1., nan, nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan,  1., nan, nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan,  1., nan, nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan,  1., nan],
       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.]])

add a heatmap
mask = np.zeros_like(df[words])
mask[np.triu_indices_from(mask)] = true
with sns.axes_style(""white""):
    f, ax = plt.subplots(figsize=(7, 5))
    ax = sns.heatmap(df[words], mask=mask, square=true, cmap=""ylgnbu"")",https://stackoverflow.com/questions/61945586,python,21-05-2020 23:44,556.0,2.0,1.0,True,06-11-2021 16:40,06-11-2021 16:40
68404498,no module named &#39;en_core_web_sm&#39;,"i am trying to use the en_core_web_sm in jupyter notebook.
i have tried to use: conda install en_core_web_sm but it did not work how can i install this ?","['python', 'jupyter-notebook', 'nlp', 'spacy']",68405046,"en_core_web_sm is a language model provided by spacy. first, you have to install spacy, then try this snippet:
import spacy
from spacy.lang.en.examples import sentences 

nlp = spacy.load(""en_core_web_sm"")
doc = nlp(sentences[0])
print(doc.text)
for token in doc:
    print(token.text, token.pos_, token.dep_)",https://stackoverflow.com/questions/68404498,python,16-07-2021 06:38,3150.0,1.0,2.0,True,09-09-2022 20:09,17-07-2021 07:55
69757539,"deploying huggingface zero-shot classification in sagemaker using template returns error, missing positional argument &#39;candidate_labels&#39;","i'm using the generated code from huggingface, task: zero-shot classification, configuration: aws and running it in sagemaker's jupyterlab
from sagemaker.huggingface import huggingfacemodel
import sagemaker

role = sagemaker.get_execution_role()
# hub model configuration. 
hub = {
    'hf_model_id':'facebook/bart-large-mnli',
    'hf_task':'zero-shot-classification'
}

# create hugging face model class
huggingface_model = huggingfacemodel(
    transformers_version='4.6.1',
    pytorch_version='1.7.1',
    py_version='py36',
    env=hub,
    role=role, 
)

# deploy model to sagemaker inference
predictor = huggingface_model.deploy(
    initial_instance_count=1, # number of instances
    instance_type='ml.m5.xlarge' # ec2 instance type
)

predictor.predict({
    'inputs': ""hi, i recently bought a device from your company but it is not working as advertised and i would like to get reimbursed!""
})

the following error returned:

modelerror: an error occurred (modelerror) when calling the
invokeendpoint operation: received client error (400) from primary
with message ""{   ""code"": 400,   ""type"": ""internalserverexception"",
""message"": ""call() missing 1 required positional argument:
\u0027candidate_labels\u0027"" } "". see ...
in account **** for more information.

i tried running them differently such as this,
predictor.predict({
    'inputs': ""hi, i recently bought a device from your company but it is not working as advertised and i would like to get reimbursed!"",
    'candidate_labels': ['science', 'life']
})

but still don't work. how should i run it?","['python', 'model', 'amazon-sagemaker', 'huggingface-transformers']",69763741,"the schema of request body for a zero-shot classification model is defined in this link.
{
    ""inputs"": ""hi, i recently bought a device from your company but it is not working as advertised and i would like to get reimbursed!"",
    ""parameters"": {
        ""candidate_labels"": [
            ""refund"",
            ""legal"",
            ""faq""
        ]
    }
}",https://stackoverflow.com/questions/69757539,python,28-10-2021 16:11,596.0,0.0,1.0,True,29-10-2021 06:06,29-10-2021 06:06
76005509,openai whisper api error: &quot;you must provide a model parameter&quot;,"i am testing a script to convert audio to text. however, while calling the api i am getting an error.
here is the path i am using
  '

and the error i am getting is
""error"": {
        ""message"": ""you must provide a model parameter"",
        ""type"": ""invalid_request_error"",
        ""param"": null,
        ""code"": null
    }

i have tried model=whisper-1 and model=davinci-002, both of them are raising the same error. is there anything that i am missing?","['javascript', 'openai-api', 'openai-whisper']",76005575,"take a look at the official openai documentation.

if you want to use the whisper api, note the following:

correct api endpoint: 
there are two required parameters:

file
model



try this:
fetch(' {
  method: 'post',
  headers: {
    'authorization': 'bearer sk-xxxxxxxxxxxxxxxxxxxx',
    'content-type': 'multipart/form-data',
  },
  body: {
    file: 'your_audio_file.mp3',
    model: 'whisper-1',
  }
});",https://stackoverflow.com/questions/76005509,javascript,13-04-2023 12:36,2381.0,0.0,2.0,True,14-04-2023 08:37,14-04-2023 08:37
74767053,converting word2vec output into dataframe for sklearn,"i am attempting to use gensim's word2vec to transform a column of a pandas dataframe into a vector that i can pass to a sklearn classifier to make a prediction.
i understand that i need to average the vectors for each row. i have tried following this guide but i am stuck, as i am getting models back but i don't think i can access the underlying embeddings to find the averages.
please see a minimal, reproducible example below:
import pandas as pd, numpy as np
from gensim.models import word2vec
from gensim.models.doc2vec import doc2vec, taggeddocument
from sklearn.feature_extraction.text import countvectorizer

temp_df = pd.dataframe.from_dict({'id': [1,2,3,4,5], 'contdata': [np.random.randint(1, 10 + 1)]*5, 
                                'text': ['lorem ipsum dolor sit amet', 'consectetur adipiscing elit.', 'sed elementum ultricies varius.',
                                         'nunc vel risus sed ligula ultrices maximus id qui', 'pellentesque pellentesque sodales purus,'],
                                'class': [1,0,1,0,1]})
temp_df['text_lists'] = [x.split(' ') for x in temp_df['text']]

w2v_model = word2vec(temp_df['text_lists'].values, min_count=1)

cv = countvectorizer()
count_model = pd.dataframe(data=cv.fit_transform(temp_df['text']).todense(), columns=list(cv.get_feature_names_out()))

using sklearn's countvectorizer, i am able to get a simple frequency representation that i can pass to a classifier. how can i get that same format using word2vec?
this toy example produces:
adipiscing  amet    consectetur dolor   elementum   elit    id  ipsum   ligula  lorem   ... purus   qui risus   sed sit sodales ultrices    ultricies   varius  vel
0   0   1   0   1   0   0   0   1   0   1   ... 0   0   0   0   1   0   0   0   0   0
1   1   0   1   0   0   1   0   0   0   0   ... 0   0   0   0   0   0   0   0   0   0
2   0   0   0   0   1   0   0   0   0   0   ... 0   0   0   1   0   0   0   1   1   0
3   0   0   0   0   0   0   1   0   1   0   ... 0   1   1   1   0   0   1   0   0   1
4   0   0   0   0   0   0   0   0   0   0   ... 1   0   0   0   0   1   0   0   0   0

while this runs without error, i cannot access the embedding that i can pass with this current format. i would like to produce the same format, with the exception of instead of there being counts, its the word2vec value embeddings","['python', 'scikit-learn', 'nlp', 'gensim', 'word2vec']",74774922,"while yo might not be able to help it if your original data comes from a pandas dataframe, neither gensim nor scikit-learn work with dataframe-style data natively. rather, they tend to use raw numpy arrays, or base python datastructures like lists or iterable sequences.
trying to shoehorn interim raw vectors into the pandas style of data structure tends to add code complication & wasteful overhead.
that's especially true if the vectors are dense vectors, where essentially all of a smaller-number of dimensions are nonzero, as in word2vec-like algorthms. but that's also true if the vectors are the kinds of sparse vectors, with a giant number of dimensions, but most dimensions 0, that come from countvectorizer and various ""bag-of-words""-style text models.
so first, i'd recommend against putting the raw outputs of word2vec or countvectorizer, which are usually interim representations on the way to completing some other task, into a dataframe.
if you want to have the final assigned-labels in the dataframe, for analysis or reporting in the pandas style, only add those final outputs in the end. but to understand the interim vector representations, and then to pass them to things like scikit-learn classifiers in the formats those classes expect, keep those vectors (and inspect them yourself for clarity) in the their raw numpy vector formats.
in particular, after word2vec runs (with the parameters you've shown), there'll be a 100-dimensional vector per word. not per multi-word text. and the 100-dimensions have no names other than their indexes 0 to 99.
and unlike the dimensions of the countvectorizer representation, which are counts of individual words, each dimension of the ""dense embedding"" will be some floating-point decimal value that has no clear or specific interpretation alone: it's only directions/neighborhoods in the whole space, shearing across many dimensions, that vaguely correspond with useful or human-nameable concepts.
if you want to turn the per-word 100-dimensional vectors into vectors for a multi-word text, there are many potential ways to do so ï¿½ï¿½ï¿½ but one simple choice is to simply average together the n word-vectors into 1 summary vector. gensim's class holding the word-vectors inside the word2vec model, keyedvectors, has a .get_mean_vector() method that can help. for example:
texts_as_wordlists = [x.split(' ') for x in temp_df['text']]
text_vectors = [w2v_model.wv.get_mean_vector(wordlist) for wordlist in texts_as_wordlists]

there are many other potential ways to use word-vectors to model a longer text. for example, you might reweight the words before averagine. but a simple average is a reasonable first baseline approach. (other algorithms related to word2vec, like the 'paragraph vector' algorihtm implemented by the doc2vec class, can also create a vector for a multi-word text, and such a vector is not just the average of its word-vectors.)
two other notes on using word2vec:

word2vec vectors only get good when trained on lots of word-usage data. toy-sized examples trained on only hundreds, or even tens-of-thousands, of words rarely show anything useful, or anything resembling the power of this algorithm on larger data set.
min_count=1 is essentially always a bad idea with this algorithm. related to the point above, the algorithm needs multiple subtly-contrasting usage examples of any word to have any chance of placing it meaningfully in the shared-coordinate space. words with just one, or even a few, usages tend to get awful vectors not generalizable to the word's real meaning as would be evident from a larger sample of its use. and, in natural-language corpora, such few-example words are very numerous - so they wind up taking a lot of the training time, and achieving their bad representations actually worsens the vectors for surrounding words, that could be better because there are enough training examples. so, the best practice with word2vec is usually to ignore the rarest words ï¿½ï¿½ï¿½ train as if they weren't even there. (the class's default is min_count=5 for good reasons, and if that results in your model missing vectors for words you think you need, get more data showing uses of those words in real contexts, rather than lowering>min_count.)",https://stackoverflow.com/questions/74767053,python,12-12-2022 05:48,1137.0,1.0,1.0,True,12-12-2022 17:22,12-12-2022 16:44
71055114,how to tokenize/parse data in an excel sheet using spacy,"i'm trying to convert an excel sheet into a doc object using spacy, i spent the last couple of days trying to go around it but it seems a bit challenging.  i have opened the sheet in both openpyxl and pandas, i can read the excel sheet and output the content but i couldn't integrate spacy to create doc/token objects.
is it possible to process excel sheets in spacy's pipeline?
thank you!","['excel', 'nlp', 'spacy', 'doc']",71107475,"spacy has no support for excel.
you could use pandas to read either the csv(if csv format)
or excel file
like
     import pandas as pd
     df = pd.read_csv(file)

or
     df  = pd.read_excel(file)

respectively.
select required text column and iterate over  df 'column' values and pass them over to nlp() of spacy",https://stackoverflow.com/questions/71055114,excel,09-02-2022 18:49,1568.0,1.0,1.0,True,14-02-2022 05:24,09-02-2022 22:12
70724874,nlp text classification countvectorizer shape error,"i have a text dataset which has one column for reviews and another column for labels. i want to build a decision tree model by using that dataset, i used vectorizer but it gives valueerror: number of labels=37500 does not match number of samples=1  error. vect.vocabulary_ returns {'review': 0} review is the column name. so i think it does not fit to all data. here is the code below, any help is appreciated.
from sklearn.model_selection import train_test_split
x_train, x_test,y_train, y_test = train_test_split(data.iloc[:,:-1],data.iloc[:,-1:],
test_size = 0.25, random_state = 42)

from sklearn.feature_extraction.text import countvectorizer
vect = countvectorizer()
vect.fit(x_train)
x_train_dtm = vect.transform(x_train)
x_train_dtm = vect.fit_transform(x_train)
x_test_dtm = vect.transform(x_test)

from sklearn.tree import decisiontreeclassifier 
dtc = decisiontreeclassifier()
dtc.fit(x_train_dtm, y_train)
y1_pred_class = dtc.predict(x_test_dtm)

also x_train_dtm.shape is <bound method spmatrix.get_shape of <1x1 sparse matrix of type '<class 'numpy.int64'>' with 1 stored elements in compressed sparse row format>>","['python', 'scikit-learn', 'nlp', 'decision-tree', 'text-classification']",70728452,"it worked when i changed this part:
x_train, x_test,y_train, y_test = train_test_split(data['text'],
data['tag'],test_size = 0.25, random_state = 42)",https://stackoverflow.com/questions/70724874,python,15-01-2022 19:48,274.0,0.0,2.0,True,16-01-2022 08:28,15-01-2022 20:34
76800148,sqldatabasechain with gpt-4 returns sql syntax error,"i wrote a program trying to query local sqlite db, and it worked fine for text-davinci-003:
llm = openai(model_name=""text-davinci-003"", verbose=true)

however, after i changed it to gpt-4:
llm = chatopenai(model_name=""gpt-4-0613"", verbose=true)
...
db_chain = sqldatabasechain.from_llm(
    llm,
    db,
    verbose=true,
    use_query_checker=true,
    return_intermediate_steps=true,
)

with get_openai_callback() as cb:
    # no intermediate steps
    # result = db_chain.run(query)

    # if intermediate steps are needed...
    result = db_chain(query)
    intermediate_steps = result[""intermediate_steps""]

    print("""")

    try:
        sql_result = intermediate_steps[3]
        print(""sql query result:"")
        print(json.dumps(ast.literal_eval(sql_result), indent=4))
    except exception as e:
        print(f""error while parsing the sql result:\n{e}"")
        print("""")
        print(intermediate_steps)
    
    print("""")

    print(cb)

... everything still works, except the final sql query contained more text in addition to sql query, i.e.:
> entering new sqldatabasechain chain...
have the user visited some news website? if yes, list all the urls.
do not specify timestamp unless query said so.
do not specify limit unless query said so.
sqlquery:the original query appears to be correct as it doesn't seem to have any of the common mistakes listed. here is the same query:

select ""url"" from browsinghistory where ""title"" like '%news%'traceback (most recent call last):
  file ""c:\path\python311\lib\site-packages\sqlalchemy\engine\base.py"", line 1968, in _exec_single_context
    self.dialect.do_execute(
  file ""c:\path\python311\lib\site-packages\sqlalchemy\engine\default.py"", line 920, in do_execute
    cursor.execute(statement, parameters)
sqlite3.operationalerror: near ""the"": syntax error

the above exception was the direct cause of the following exception:

traceback (most recent call last):
  file ""d:\path\run.py"", line 292, in <module>
    database_mode(llm, filepath, delimiter)
  file ""d:\path\run.py"", line 156, in database_mode
    llm.query_database(db_path=db_path, query=query)
  file ""d:\path\modules\chatbot.py"", line 220, in query_database
    result = db_chain(query)
             ^^^^^^^^^^^^^^^
  file ""c:\path\appdata\local\programs\python\python311\lib\site-packages\langchain\chains\base.py"", line 140, in __call__
    raise e
  file ""c:\path\appdata\local\programs\python\python311\lib\site-packages\langchain\chains\base.py"", line 134, in __call__
    self._call(inputs, run_manager=run_manager)
  file ""c:\path\appdata\local\programs\python\python311\lib\site-packages\langchain\chains\sql_database\base.py"", line 181, in _call
    raise exc
  file ""c:\path\appdata\local\programs\python\python311\lib\site-packages\langchain\chains\sql_database\base.py"", line 151, in _call
    result = self.database.run(checked_sql_command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""c:\path\appdata\local\programs\python\python311\lib\site-packages\langchain\sql_database.py"", line 334, in run
    cursor = connection.execute(text(command))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""c:\path\appdata\local\programs\python\python311\lib\site-packages\sqlalchemy\engine\base.py"", line 1413, in execute
    return meth(
           ^^^^^
  file ""c:\path\appdata\local\programs\python\python311\lib\site-packages\sqlalchemy\sql\elements.py"", line 483, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""c:\path\appdata\local\programs\python\python311\lib\site-packages\sqlalchemy\engine\base.py"", line 1637, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  file ""c:\path\appdata\local\programs\python\python311\lib\site-packages\sqlalchemy\engine\base.py"", line 1846, in _execute_context
    return self._exec_single_context(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""c:\path\appdata\local\programs\python\python311\lib\site-packages\sqlalchemy\engine\base.py"", line 1987, in _exec_single_context
    self._handle_dbapi_exception(
  file ""c:\path\appdata\local\programs\python\python311\lib\site-packages\sqlalchemy\engine\base.py"", line 2344, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  file ""c:\path\appdata\local\programs\python\python311\lib\site-packages\sqlalchemy\engine\base.py"", line 1968, in _exec_single_context
    self.dialect.do_execute(
  file ""c:\path\appdata\local\programs\python\python311\lib\site-packages\sqlalchemy\engine\default.py"", line 920, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.operationalerror: (sqlite3.operationalerror) near ""the"": syntax error
[sql: the original query appears to be correct as it doesn't seem to have any of the common mistakes listed. here is the same query:

select ""url"" from browsinghistory where ""title"" like '%news%']
(background on this error at: 

i know that i can try to tell it not to return anything but the query (might be unstable. though...), but why isn't this work for gpt-4, while it works for text-davinci-003?

update:
tried with a different query, and the problem remains:
> entering new sqldatabasechain chain...
list all websites visited by the user.
do not specify timestamp unless query said so.
do not specify limit unless query said so.
sqlquery:the original query seems to be correct. it is simply selecting the ""url"" column from the ""browsinghistory"" table. there is no misuse of any functions, no data type mismatch, no joins, etc.

reproducing the original query:

select ""url"" from browsinghistory
...
...
...","['langchain', 'py-langchain', 'gpt-4']",76848245,"i still have no idea why the query result contains contents other than sql query, but i implemented sqldatabasetoolkit to run the query and worked. sqldatabasetoolkit can help to deal with some parsing errors like this.
toolkit = sqldatabasetoolkit(db=db, llm=self.llm)

agent_executor = create_sql_agent(
    llm=self.llm,
    toolkit=toolkit,
    verbose=true
)

with get_openai_callback() as cb:
    res = agent_executor.run(query)
    print(""sql query result:"")
    print(res)
    print("""")
    print(cb)",https://stackoverflow.com/questions/76800148,langchain,31-07-2023 00:58,2519.0,1.0,1.0,True,07-08-2023 00:41,31-07-2023 02:15
76185813,how to resolve error in seqeval in ner bert finetuning?,"i'm trying to finetune a ner model, (bert/biobert) and after first epoch of training, in evaluation part, i got the following error, any idea what is wrong?
valueerror: predictions and/or references don't match the expected format.
expected format: {'predictions': sequence(feature=value(dtype='string', id='label'), length=-1, id='sequence'), 
'references': sequence(feature=value(dtype='string', id='label'), length=-1, id='sequence')},
input predictions: [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], ..., [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]],
input references: [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], ..., [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]

i am using very standard eval function and if i remove the evaluation from trainer, the model trains without any problem and the results are good, but i have little to no metrics.
def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = seqeval.compute(predictions=true_predictions, references=true_labels)
    return {
        ""precision"": results[""overall_precision""],
        ""recall"": results[""overall_recall""],
        ""f1"": results[""overall_f1""],
        ""accuracy"": results[""overall_accuracy""],
    }

thanks.","['nlp', 'huggingface-transformers', 'named-entity-recognition', 'evaluation', 'huggingface-evaluate']",76190156,"as indicated by the error message, the expected predictions & references should be lists of strings not integers. for seqeval, this makes sense, since the seqeval metric is concerned with matching entity spans exactly (as indicated by the b- & i- prefixes of the tags).
so your label_list should map label identifiers to label tags, such as [""o"", ""b-per"", ""i-per"", ""b-org"", ""i-org"", ""b-loc"", ""i-loc""].",https://stackoverflow.com/questions/76185813,nlp,05-05-2023 20:44,950.0,2.0,2.0,True,10-05-2023 15:05,09-05-2023 00:06
76696698,what is the correct way to save chatgpt api-key for flutter app/firebase,"i have made some research, some said i have to hardcoded but using obfuscation in my client app, i see this may be a easy way. but i am not sure for the following reason:

since i need to check how many tokens one user is using, anyway i need firebase as my backend/server to give the permission to the specific user, in this case saving chatgpt apikey in firebase is better?
the other solution like a. --dart-define; b.envied/.env seems not hundred percent safe, so i do not have such experience how to save such as chatgpt api level, should i follow this solution?

so do we have some official solution or any existing tutorial how to handle the connection between chatgpt api and flutter app?
==============update===============
hi alex i have read your medium article and have following questions:

is it safe to use firebase extension where i input my openai api-key? i am confused where to save this openai api-key in the end: google cloud console/secret manager or chatbot with chatgpt firebase extension?

i know flutter instead of android, from my understanding in your resource, it seems a user can directly connect with firebase and chatgpt since i input my openai apikey in the chatbot with chatgpt firebase extension, so how can we control how many tokens this user is allowed to use?

do you have also some article introducing the third-party-payment api such as stripes or revenuecat, so firebase knows the specific user is a premium user and give more chatgpt quota?


thanks!","['flutter', 'firebase', 'google-cloud-firestore', 'openai-api']",76697198,"if want to save the openai api key on the client, which i recommend against it, you can indeed do it by obfuscating it, but the safest place would be to store it in google cloud. so you should open your google cloud console, select your project, then go to the secret manager at:



and create your secret key. the benefit of using this approach is that the encryption is managed by google. so you don't have to worry about it.",https://stackoverflow.com/questions/76696698,flutter,16-07-2023 04:39,683.0,1.0,1.0,True,18-07-2023 04:39,18-07-2023 04:39
44881999,"word embedding, lookuptable, word embedding visualizations","i need to ask few questions regarding word embeddings.....could be basic.

when we convert a one-hot vector of a word for instance king [0 0 0 1 0] into an embedded vector e = [0.2, 0.4, 0.2, 0.2].... is there any importance for each index in resultant word vector? for instance e[1] which is 0.2.... what specifically e[1] defines (although i know its basically a transformation into another space).... or word vector collectively defines context but not individually...
how the dimension (reduced or increased) of a word vector matters as compared to the original one-hot vector?
how can we define a lookup table in terms of an embedding layer?
is lookup table a kind of random generated table or it already been trained separately with respect to data instance in data and we just use it later on in neural network operations?
is there any method to visualize an embedded vector at hidden layer (as we do have in image based neural network processing)?

thanks in advance","['deep-learning', 'text-mining', 'word2vec', 'word-embedding']",44889219,"1: each element (or a group of element) in embedding vector have some meaning, but mostly unknown for human. depend on what algorithm you use, a word embedding vector may have different meaning, but usually useful. 
for example, glove, similar word 'frog', 'toad' stay near each other in vector space. king - man result in vector similar to queen. 

turn vocab into index. for example, you have a vocabulary list:
[dog, cat, mouse, feed, play, with]
then the sentences: dog play with cat => 0, 4, 5, 1
while, you have embedding matrix as follow
[0.1, 0.1, 0]  # comment: this is dog 
[0.2, 0.5, 0.1] # this is cat 
[...] 
[...] 
[...] 
[...] 

where first row is embedding vector of dog, second row is cat, then so on
then, you use the index (0, 4, 5, 1) after lookup would become a matrix [[0.1, 0.1, 0][...][...][0.2, 0.5, 0.1]]

either or both


you can randomly init embedding vector and training it with gradient descent
you can take pretrained word vector and keep it fixed (i.e: read-only, no change). 
you can train your word vector in model and use it in another model. our you can download pretrained word vector online. example common crawl (840b tokens, 2.2m vocab, cased, 300d vectors, 2.03 gb download): glove.840b.300d.zip on glove
you can init with pretrained word vector and train with your model by  gradient descent


update:
one-hot vector does not contain any information. you can think that one-hot vector is index of that vector in vocabulary. 
for example, dog =>  [1, 0, 0, 0, 0, 0] and cat =>  [0, 1, 0, 0, 0, 0]. there are some different between one-hot vs index: 

if you input a list of index: [0, 4, 5, 1] to your multi-layer perceptron, it cannot learn anything (i tried...).but if you input a matrix of one-hot vector [[...1][1...][...][...]], it learn something. but it costly in term of ram and cpu. 
one-hot cost a lot of memory to store zeros. thus, i suggest randomly init embedding matrix if you don't have one. store dataset as index, and use index to look up embedding vector


""its mean that lookup table is just a matrix of embedded vectors
  (already been trained seperately via word2vec or...) for each word in
  the vocabulary. and while in the process of neural network either we
  can use an embedding layer or we can just refer to embedded vector in
  lookup table for that particular embedded vector against particular
  one-hot vector.""

use the ""index"" to look-up in lookup table. turn dog into 0, cat into 1. one-hot vector and index contain same information, but one-hot cost more memory to store. moreover, a lot of deeplearning framework accept index as input to embedding layer (which, output is a vector represent for a word in that index.)

"". how we get this embedding vector...""

=> read paper. here is paper about word2vec and glove. ask your lecturers for more detail, they are willing to help you.",https://stackoverflow.com/questions/44881999,deep-learning,03-07-2017 09:24,9839.0,7.0,1.0,True,01-12-2023 15:12,01-12-2023 15:12
77681471,unable to tag the pos of the text file,"i want to tag the parts of speech of a sentence. for this task i am using pos-english-fast model. if there was one sentence the model identified the tags for the pos. i created a data file where i kept all my sentences. the name of the data file is 'data1.txt'. now if i try to tag the sentences on the data file it does not work.
my code
from flair.models import sequencetagger
model = sequencetagger.load(""flair/pos-english"")
#read the data from the data.txt 
with open('data1.txt') as f:
  data = f.read().splitlines()
#create a list of sentences from the data 
sentences = [sentence.split() for sentence in data]
#tag each sentence using the model
tagged_sentences = []
for sentence in sentences:
  tagged_sentences.append(model.predict(sentence))
for sentence in tagged_sentences:
  print(sentence)

the error i received
attributeerror                            traceback (most recent call last)
<ipython-input-16-03268ee0d9c9> in <cell line: 10>()
      9 tagged_sentences = []
     10 for sentence in sentences:
---> 11   tagged_sentences.append(model.predict(sentence))
     12 for sentence in tagged_sentences:
     13   print(sentence)

1 frames
/usr/local/lib/python3.10/dist-packages/flair/data.py in set_context_for_sentences(cls, sentences)
   1116         previous_sentence = none
   1117         for sentence in sentences:
-> 1118             if sentence.is_context_set():
   1119                 continue
   1120             sentence._previous_sentence = previous_sentence

attributeerror: 'str' object has no attribute 'is_context_set'

the snapshot of the errors

how could i resolve it?","['python', 'nlp', 'tagging', 'huggingface', 'flair']",77681791,"let's say this is your data:
['not my responsibility is a 2020 american short film written and produced by singer-songwriter billie eilish.',
 ""a commentary on body shaming and double standards placed upon young women's appearances, it features a monologue from eilish about the media scrutiny surrounding her body."",
 'the film is spoken-word and stars eilish in a dark room, where she gradually undresses before submerging herself in a black substance.']

this is what you need to do to do part-of-speech tagging in flair:
from flair.data import sentence
from flair.models import sequencetagger

sentences = list(map(sentence, data))
_ = model.predict(sentences)

now all sentences are correctly tagged. if you want to visualize, for example, the tags for the first sentence, just use print(sentences[0]). this is the output:
sentence[17]: ""not my responsibility is a 2020 american short film written and produced by singer-songwriter billie eilish."" ï¿½ï¿½ï¿½
[""not""/rb, ""my""/prp$, ""responsibility""/nn, ""is""/vbz, ""a""/dt, ""2020""/cd, ""american""/jj, ""short""/jj, ""film""/nn, ""written""/vbn, ""and""/cc, ""produced""/vbn, ""by""/in, ""singer-songwriter""/nn, ""billie""/nnp, ""eilish""/nnp, "".""/.]
``
<",https://stackoverflow.com/questions/77681471,python,18-12-2023 20:02,37.0,1.0,1.0,True,18-12-2023 21:21,18-12-2023 21:21
63973959,extract synonyms using wordnet,"i am having a minor issue with nltk.corpus and wordnet. it can't seem to find synonyms for 'yes', even though thesaurus.com says there is, how i can i extract my 'yes' sysnoyms in order to properly assess input into that bottom section.
import textblob as txtnlp
from nltk.corpus import wordnet
def text_extraction():
    yes_ls = []
    for synset in wordnet.synsets(""yes""):
        for lemma in synset.lemmas():
            yes_ls.append(lemma.name())
    init_conversation = str(input('hello there my name is therpibot 2.0, your name is? '))
    blob_1 = txtnlp.textblob(init_conversation)
    fragments_name = blob_1.words
    print(f'hello there {fragments_name[0]}!', end=' ')
    print('my purpose is to make your day better through some cognitive behavioral therapy.')
    init_response = str(input('would like to engage in a talk session? '))
    if init_response.lower() in yes_ls:
        therapy()
    #ex_1 = list(i.tags for i in fragments)
    #print(ex_1)
def therapy():
    print('hi')

if __name__ in ""__main__"":
    text_extraction()","['python', 'nltk', 'wordnet']",63974229,"i've tried to do something like that before but wordnet wasn't my best choice but you can take a look here:
wordnet find synonyms
and i recommend  for you , it's much better and easier for your task , it's an open data set you can access it for free",https://stackoverflow.com/questions/63973959,python,19-09-2020 22:35,231.0,2.0,1.0,True,24-05-2023 16:50,24-05-2023 16:50
62462878,customize the encode module in huggingface bert model,"i am working on a text classification project using huggingface transformers module. the encode_plus function provides the users with a convenient way of generating the input ids, attention masks, token type ids, etc. for instance:
from transformers import berttokenizer

pretrained_model_name = 'bert-base-cased'
bert_base_tokenizer = berttokenizer.from_pretrained(pretrained_model_name)

sample_text = 'bamboo poles, ï¿½ï¿½ï¿½installation by an unknown building constructor #discoverhongkong #hongkonginsta'

encoding = bert_base_tokenizer.encode_plus(
        cleaned_tweet, hashtag_string,
        max_length=70,
        add_special_tokens=true,  # add '[cls]' and '[sep]'
        return_token_type_ids=true,
        pad_to_max_length=true,
        return_attention_mask=true,
        return_tensors='pt',  # return pytorch tensors
    )

print('*'*20)
print(encoding['input_ids'])
print(encoding['attention_mask'])
print(encoding['token_type_ids'])
print('*'*20)

however, my current project requires me to generate customize/strong> for a given text. for instance, for a list of words [hk, us, uk], i want to generate ids for these words and let other words' ids which do not exist in this list as zero. these ids are used to find embedding in another customized embedding matrix, not from pretrained bert module.
how can i achieve this kind of customized encoder? any suggestions and solutions are welcomed! thanks~","['nlp', 'text-classification', 'huggingface-transformers', 'bert-language-model']",62504210,i think you can use the <unusedxxx> tokens in the bert vocab and add your custom tokens there. so now you can easily refer to them with a valid token id.,https://stackoverflow.com/questions/62462878,nlp,19-06-2020 03:39,745.0,1.0,1.0,True,15-06-2021 20:42,19-06-2020 03:49
69047054,how to use python re to remove all sub-strings starting with letters or numbers and ending with &quot;pm&quot;,"i found some random codes caused by image files in my text file and i want to remove those random codes, which start with letters or numbers but end with ""pm"":
for example, there is a text:
isd08lxjpg2021330401pmï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½60ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½60ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½

ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½usaï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½","['python', 'text', 'nlp', 'python-re']",69047137,"you want to remove every continuous segment of roman letters plus arabic numerals that end with pm. this is achieved by a simple regular expression:
[a-za-z0-9]*pm
a-z describes the range of all lowercase latin letters, equivalent for a-z and 0-9. * indicates any amount of characters since your string can likely have arbitrary length. pm is the fixed end string.
of course, you have to make sure these strings don't contain special characters like ï¿½ï¿½. if they do, add groups of characters as appropriate.
the actual python code would then be
    re.sub(r'[a-za-z0-9]*pm',"""",inputtext)",https://stackoverflow.com/questions/69047054,python,03-09-2021 15:04,153.0,0.0,2.0,True,04-09-2021 00:26,04-09-2021 00:26
55528385,finding the word that corresponds to experience,"suppose i have a sentence like the following, how can i find what the experience corresponds to?
ex: programmer with 5 years of experience wanted.
i want to find what the experience (5 years) corresponds to, in this case programmer.
the code should also be recognize a corresponding verb, ex: 5 years of programming
so how should i go about it? i was thinking of making a pattern that finds the closest noun or verb.","['python', 'spacy', 'named-entity-recognition', 'part-of-speech']",55605413,"as i didn't get any meaningful help, i just came up with a temporary solution of my own. find the sentence with the entity tag ""date"", and remove the stop words.
for example, the sentence 'the ideal candidate must have at least 6 years of experience in the field.' becomes into the following:
'ideal candidate 6 years experience field'
obviously i can refine this further, but for now this will do.",https://stackoverflow.com/questions/55528385,python,05-04-2019 04:43,50.0,-3.0,1.0,True,26-09-2021 04:06,26-09-2021 04:06
69064948,how to import gensim summarize,"i got gensim to work in google collab by following this process:
!pip install gensim
from gensim.summarization import summarize

then i was able to call summarize(some_text)
now i'm trying to run the same thing in vs code:
i've installed gensim:
pip3 install gensim
but when i run
from gensim.summarization import summarize

i get the error
import ""gensim.summarization"" could not be resolvedpylancereportmissingimports

i've also tried from gensim.summarization.summarizer import summarize with same error. regardless i haven't been able to call the function summarize(some_text) outside of google collab.","['python', 'visual-studio-code', 'nlp', 'gensim']",69066880,"the summarization code was removed from gensim 4.0. see:


12. removed gensim.summarization
despite its general-sounding name, the module will not satisfy the
majority of use cases in production and is likely to waste people's
time. see this github
ticket for
more motivation behind this.

if you need it, you could try:

installing an older gensim version (such as 3.8.3, the last official release in which it remained); orï¿½ï¿½ï¿½
copy the source code out to your own local module

however, i expect you'd likely be disappointed by its inflexibility and how little it can do.
it was only extractive summarization - choosing a few key sentences from those that already exist. that only gives impressive results when the source text was already well-written in an expository style mixing high-level overview sentences with separate detail sentences. and, its method of analyzing/ranking words was very crude & hard-to-customize ï¿½ï¿½ï¿½ totally unconnected to the more generic/configurable/swappable approaches used elsewhere in gensim or in other text lib",https://stackoverflow.com/questions/69064948,python,05-09-2021 15:54,26196.0,9.0,2.0,True,06-09-2021 20:34,06-09-2021 20:34
77940890,google semantic retriever example error &#39;credentials&#39; object has no attribute &#39;universe_domain&#39;,"here is the code example and steps to create service account and enable api:

error:
attributeerror                            traceback (most recent call last)
<ipython-input-20-ee7d7add68db> in <cell line: 8>()
      6 
      7 # make the request
----> 8 create_corpus_response = retriever_service_client.create_corpus(create_corpus_request)
      9 
     10 # set the `corpus_resource_name` for subsequent sections.

2 frames
/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/retriever_service/client.py in _compare_universes(client_universe, credentials)
    514         """"""
    515         if credentials:
--> 516             credentials_universe = credentials.universe_domain
    517             if client_universe != credentials_universe:
    518                 default_universe = retrieverserviceclient._default_universe

attributeerror: 'credentials' object has no attribute 'universe_domain'","['google-cloud-platform', 'nlp', 'artificial-intelligence', 'large-language-model', 'google-ai-platform']",77941990,"can you try upgrading your google auth it appears that it has some issues with the some of the versions:
pip install --upgrade google-auth

reference issue:",https://stackoverflow.com/questions/77940890,google-cloud-platform,05-02-2024 12:20,116.0,-1.0,1.0,True,05-02-2024 17:55,05-02-2024 12:26
75587208,key error when importing hugging face model into aws lambda function,"i'm trying to launch a lambda function that uses a hugging face model (biogpt) using the transformers paradigm on an aws lambda function. the infrastructure looks like this:

it more or less follows the setup outlined in this post, except that i am trying to use the biogpt model instead of the models outlined in the link above.
here is my app.py:
""""""
copyright amazon.com, inc. or its affiliates. all rights reserved.
spdx-license-identifier: mit-0
""""""

import os
from pathlib import path
from aws_cdk import (
    aws_lambda as lambda_,
    aws_efs as efs,
    aws_ec2 as ec2
)
from aws_cdk import app, stack, duration, removalpolicy, tags

from constructs import construct

class serverlesshuggingfacestack(stack):
    def __init__(self, scope: construct, id: str, **kwargs) -> none:
        super().__init__(scope, id, **kwargs)

        # efs needs to be setup in a vpc
        vpc = ec2.vpc(self, 'vpc', max_azs=2)

        # creates a file system in efs to store cache models
        fs = efs.filesystem(self, 'filesystem',
                            vpc=vpc,
                            removal_policy=removalpolicy.destroy)
        access_point = fs.add_access_point(
            'mlaccesspoint',
            create_acl=efs.acl(
                owner_gid='1001',
                owner_uid='1001',
                permissions='750'
            ),
            path=""/export/models"",
            posix_user=efs.posixuser(gid=""1001"", uid=""1001"")
        )

        # %%
        # iterates through the python files in the docker directory
        docker_folder = os.path.dirname(os.path.realpath(__file__)) + ""/inference""
        pathlist = path(docker_folder).rglob('*.py')
        for path in pathlist:
            base = os.path.basename(path)
            filename = os.path.splitext(base)[0]
            # lambda function from docker image
            lambda_.dockerimagefunction(
                self, filename,
                code=lambda_.dockerimagecode.from_image_asset(docker_folder,
                                                              cmd=[
                                                                  filename+"".handler""]
                                                              ),
                memory_size=8096,
                timeout=duration.seconds(600),
                vpc=vpc,
                filesystem=lambda_.filesystem.from_efs_access_point(access_point, '/mnt/hf_models_cache'),
                environment={""transformers_cache"": ""/mnt/hf_models_cache""},
            )

app = app()

stack = serverlesshuggingfacestack(app, ""biogptstack"")
tags.of(stack).add(""project"", ""biogpt"")

app.synth()

and here is my dockerfile:
arg function_dir=""/function/""

from huggingface/transformers-pytorch-cpu as build-image


# include global arg in this stage of the build
arg function_dir

# install aws-lambda-cpp build dependencies
run apt-get update && \
  apt-get install -y \
  g++ \
  make \
  cmake \
  unzip \
  libcurl4-openssl-dev


# create function directory
run mkdir -p ${function_dir}

# copy handler function
copy *.py ${function_dir}

# install the function's dependencies
run pip uninstall --yes jupyter
run pip install --target ${function_dir} awslambdaric
run pip install --target ${function_dir} sentencepiece protobuf

from huggingface/transformers-pytorch-cpu

# include global arg in this stage of the build
arg function_dir
# set working directory to function root directory
workdir ${function_dir}

# copy in the built dependencies
copy --from=build-image ${function_dir} ${function_dir}

entrypoint [ ""python3"", ""-m"", ""awslambdaric"" ]

# this will get replaced by the proper handler by the cdk script
cmd [ ""sentiment.handler"" ]

here is the error message i am seeing when i try to test my lambda function:
line 672, in from_pretrained
config_class = config_mapping[config_dict[""model_type""]]
file ""/usr/local/lib/python3.6/dist-packages/transformers/models/auto/configuration_auto.py"", line 387, in __getitem__
raise keyerror(key)
keyerror: 'biogpt'
during handling of the above exception, another exception occurred:
traceback (most recent call last):
file ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
""__main__"", mod_spec)
file ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code","['amazon-web-services', 'aws-lambda', 'huggingface-transformers', 'amazon-efs', 'huggingface']",75594515,"my best guess here on the issue is that i am using an older docker image (huggingface/transformers-pytorch-cpu). if you look on docker, you'll see this image hasn't been updated in over a year, so i'm going to save the model to my local machine...
model.save_pretrained(""path/to/model"")

...then push this to efs, so my lambda can access it from a mounted directory.
hope that works...",https://stackoverflow.com/questions/75587208,amazon-web-services,28-02-2023 01:55,413.0,0.0,1.0,True,28-02-2023 16:04,28-02-2023 16:01
71117138,text manipulation of a comma separated string in python,"i want to read a textfile test.txt where the txt is in the format
'jon, stacy, simon, ..., maverick'

i'd want to save the string into test2.txt as
'jon as t1_jon, stacy as t1_stacy, simon as t1_simon, ..., maverick as t1_maverick'

it could be that there is a linebreak every now and then, i would want to ignore that. how would i do it in an efficient and easy way?
ps: i couldn't come up with a more fitting title, how would you name it?","['python', 'file', 'text', 'nlp']",71117222,"one nice approach is to use the re module.
import re

s_in = 'apple, banana, orange,\n mango, guava'
words = re.split(r'[,\n]\s*',s_in)
s_out = ', '.join([f'{word} as t1_{word}' for word in words])
print(s_out)

result:
apple as t1_apple, banana as t1_banana, orange as t1_orange, mango as t1_mango, guava as t1_guava",https://stackoverflow.com/questions/71117138,python,14-02-2022 19:13,110.0,0.0,2.0,True,14-02-2022 19:46,14-02-2022 19:25
78211318,how can i get the first content of a python synsets list?,"enter image description herei have a scrapped text stored under the variable ""message"".
i have removed the stopwords and stored the result with the variable ""without_stop_words"".
i want to loop through each words in the 'without_stop_words' and get their meanings and pronouns.
currently i am trying to get the meanings but i'm getting an error: ""indexerror: list index out of range""
enter image description here
   
 for writeup in writeups:
        message = writeup.text
        #stop words
        stop_words = set(stopwords.words('english'))
        #print(stop_words)

        tokenized_words = word_tokenize(message)
        #filtering stop words
        without_stop_words = []
        for word in tokenized_words:
            if word not in stop_words:
                without_stop_words.append(word)            
                #word meanings
        word_meanings = []
        for each_word in without_stop_words:
            sync_words = wordnet.synsets(each_word)
            meaning = sync_words[0].definition()
            print(meaning)


i want to get the meaning of each word in the ""without_stop_words"".","['python', 'nlp', 'nltk', 'sentiment-analysis', 'synset']",78211341,"the error comes from this line
meaning = sync_words[0].definition()

and it indicates that sync_words is empty
for each_word in without_stop_words:
    sync_words = wordnet.synsets(each_word)
    if sync_words:
        meaning = sync_words[0].definition()
        word_meanings.append(meaning)
    else:
        # whatever you want to do if it's empty

this will stop the error, but you should try to find out why sync_words is empty in the first place.",https://stackoverflow.com/questions/78211318,python,23-03-2024 14:53,43.0,1.0,1.0,True,23-03-2024 15:00,23-03-2024 14:59
76946217,how to highlight the differences between two strings in python?,"i want to highlight the differences between two strings in a colour using python code.
example 1:
sentence1 = ""i'm enjoying the summer breeze on the beach while i do some pilates.""
sentence2 = ""i am enjoying the summer breeze on the beach while i am doing some pilates.""

expected result (the part marked by asterisks should be in red):
 i *am* enjoying the summer breeze on the beach while i *am doing* some pilates.

example 2:
sentence1: ""my favourite season is autumn while my sister's favourite season is winter.""
sentence2: ""my favourite season is autumn, while my sister's favourite season is winter.""

expected result (the comma is different):
""my favourite season is autumn*,* while my sister's favourite season is winter."" 

i tried this:
sentence1 = ""i'm enjoying the summer breeze on the beach while i do some pilates.""
sentence2 = ""i'm enjoying the summer breeze on the beach while i am doing some pilates.""

# split the sentences into words
words1 = sentence1.split()
words2 = sentence2.split()

# find the index where the sentences differ
index_of_difference = next((i for i, (word1, word2) in enumerate(zip(words1, words2)) if word1 != word2), none)

# highlight differing part ""am doing"" in red
highlighted_words = []
for i, (word1, word2) in enumerate(zip(words1, words2)):
    if i == index_of_difference:
        highlighted_words.append('\033[91m' + word2 + '\033[0m')
    else:
        highlighted_words.append(word2)

highlighted_sentence = ' '.join(highlighted_words)
print(highlighted_sentence)


and i got this:
i'm enjoying the summer breeze on the beach while i *am* doing some

instead of this:
i'm enjoying the summer breeze on the beach while i *am doing* some pilates.

how can i solve this?","['python', 'string', 'colors', 'nlp', 'difference']",76946888,"i believe the main issue with your code was with getting the indexes of the differences. here is a solution that makes use of the built-in python difflib library:
from difflib import differ

# return string with the escape sequences at specific indexes to highlight
def highlight_string_at_idxs(string, indexes):
    # hl = ""\x1b[38;5;160m""  # 8-bit
    hl = ""\x1b[91m""
    reset = ""\x1b[0m""
    words_with_hl = []
    for string_idx, word in enumerate(string.split("" "")):
        if string_idx in indexes:
            words_with_hl.append(hl + word + reset)
        else:
            words_with_hl.append(word)
    return "" "".join(words_with_hl)

# return indexes of the additions to s2 compared to s1
def get_indexes_of_additions(s1, s2):
    diffs = list(differ().compare(s1.split("" ""), s2.split("" "")))
    indexes = []
    adj_idx = 0  # adjust index to compensate for removed words
    for diff_idx, diff in enumerate(diffs):
        if diff[:1] == ""+"":
            indexes.append(diff_idx - adj_idx)
        elif diff[:1] == ""-"":
            adj_idx += 1
    return indexes

sentence1 = ""i'm enjoying the summer breeze on the beach while i do some pilates.""
sentence2 = ""i am enjoying the summer breeze on the beach while i am doing some pilates.""
addition_idxs = get_indexes_of_additions(sentence1, sentence2)
hl_sentence2 = highlight_string_at_idxs(sentence2, addition_idxs)
print(hl_sentence2)

output
*i am* enjoying the summer breeze on the beach while i *am doing* some pilates.",https://stackoverflow.com/questions/76946217,python,21-08-2023 14:23,4320.0,1.0,3.0,True,21-08-2023 15:55,21-08-2023 14:57
78612251,how do we add/modify the normalizer in a pretrained huggingface tokenizer?,"given a huggingface tokenizer that already have a normalizer, e.g. ""mistralai/mistral-7b-v0.1"", we can do this to modify the normalizer
import json

from transformers import autotokenizer
from tokenizers.normalizers import sequence, replace, prepend

tokenizer_name = ""mistralai/mistral-7b-v0.1""
old_tok = autotokenizer.from_pretrained(tokenizer_name)

assert old_tok.backend_tokenizer.normalizer != none

new_normalizer = sequence(
    [prepend('ï¿½ï¿½ï¿½'), replace('ï¿½ï¿½ï¿½', ' '), replace(""foo"", ""bar""), replace('<br>', '\n')]
)

old_tok.backend_tokenizer.normalizer = new_normalizer
new_tokenizdr_name = f""new_tokenizer-{tokenizer_name}""
old_tok.save_pretrained(new_tokenizdr_name)


old_tok = autotokenizer.from_pretrained(tokenizer_name)
new_tok = autotokenizer.from_pretrained(new_tokenizdr_name)

[out]:
>>> print(' '.join(old_tok.batch_decode(old_tok(""i foo you<br>hello wornput_ids'])))
<s> i foo you < br > hello world

>>> print(' '.join(new_tok.batch_decode(new_tok(""i foo you<br>hello world"")['input_ids'])))
<s>  i  bar  you 
 hello  world

but when this hot-plug normalizer modification don't always work, if we change it to ""mistralai/mistral-7b-v0.3"", it fails to work:
import json

from transformers import autotokenizer
from tokenizers.normalizers import sequence, replace, prepend

tokenizer_name = ""mistralai/mistral-7b-v0.3""
old_tok = autotokenizer.from_pretrained(tokenizer_name)

new_normalizer = sequence(
    [prepend('ï¿½ï¿½ï¿½'), replace('ï¿½ï¿½ï¿½', ' '), replace(""foo"", ""bar""), replace('<br>', '\n')]
)

old_tok.backend_tokenizer.normalizer = new_normalizer
new_tokenizdr_name = f""new_tokenizer-{tokenizer_name}""
old_tok.save_pretrained(new_tokenizdr_name)


old_tok = autotokenizer.from_pretrained(tokenizer_name)
new_tok = autotokeretrained(new_tokenizdr_name)

print(' '.join(old_tok.batch_decode(old_tok(""i foo you<br>hello world"")['input_ids'])))
print(' '.join(new_tok.batch_decode(new_tok(""i foo you<br>hello world"")['input_ids'])))

[out]:
<s> i foo you < br > hello world
<s> i foo you < br > hello world

how do we add/modify the normalizer in a pretrained huggingface tokenizer?
can any normalizer from a pretrained tokenizer be modified or just specific ones?
if the latter, why and how do we know if a pretrained tokenizer's normalizer can be extended or modified?","['python', 'nlp', 'large-language-model', 'huggingface-tokenizers']",78624238,"this looks like a bug.  the v0.1 tokenizer has a normalizer by default, which can be seen by looking at the mistral-78-v0.1/tokenizer.json file:
{
 ...
  ""normalizer"": {
    ""type"": ""sequence"",
    ""normalizers"": [
      {
        ""type"": ""prepend"",
        ""prepend"": ""ï¿½ï¿½ï¿½""
      },
      {
        ""type"": ""replace"",
        ""pattern"": {
          ""string"": "" ""
        },
        ""content"": ""ï¿½ï¿½ï¿½""
      }
    ]
  },
...
}

after modifying the .backend_tokenizer.normalizer object, the modification are saved to the tokenizer.json file.
in the v0.3 version, the mistral-78-v0.1/tokenizer.json file has no value for the normalizer:
{
...
  ""normalizer"": null,
...
}

modifying the normalizer and saving the model < write the changes to the json file, but it is not getting picked up on reload using autotokenizer.from_pretrained.  i am not sure why, but it is entirely possible the tokenizer.model file indicates no normalizer is the default and it simply does not load it.
however, you can get the tokenizer to load correctly - with the custom normalizer - by instantiating the matched tokenizer class explicitly and passing in the tokenizer.model and tokenizer.json paths along with the values from the tokenizer_config.json file.  in this case it is the llamatokenizerfast class.
from transformers import autotokenizer, llamatokenizerfast, addedtoken
from tokenizers.normalizers import sequence, replace, prepend

### load, modify, and save
tok = autotokenizer.from_pretrained(""mistralai/mistral-7b-v0.3"")
tok.backend_tokenizer.normalizer = sequence([
    prepend('_'), 
    replace('_', ' '), 
    replace(""foo"", ""bar""), 
    replace('<br>', '\n')
])
tok.save_pretrained(""mistral-7b-v0.3-custom/"")


### read in config, construct the addedtoken objects
with open('mistral-7b-v0.3-custom/tokenizer_config.json') as fp:
    config = json.load(fp)
    config['added_tokens_decoder'] = {
        int(k): addedtoken(**v)
        for k, v in config.pop('added_tokens_decoder').items()
    }

### load from saved files
tok_custom = llamatokenizerfast(
    'mistral-7b-v0.3-custom/tokenizer.model', 
    'mistral-7b-v0.3-custom/tokenizer.json', 
    **config,
)

test_str = ""i foo you<br>hello world""
print(' '.join(tok_custom.batch_decode(tok_custom(test_str)['input_ids'])))
# prints:
#<s> i bar you
# hello world

if you don't want to specify the tokenizer class explicitly, you can load the model the the autotokenizer, and then load it again using from the resulting class.  it is a hacky work-around.
tok_path = ""path/to/mistral-7b-v0.3-custom/""
with open(f'{tok_path}/tokenizer_config.json') as fp:
    config = json.load(fp)
    config['added_tokens_decoder'] = {
        int(k): addedtoken(**v)
        for k, v in config.pop('added_tokens_decoder').items()
    }

tok = autotokenizer.from_pretrained(tok_path).__class__(
    f'{tok_path}/tokenizer.model', 
    f'{tok_path}/tokenizer.json', 
    **config,
)",https://stackoverflow.com/questions/78612251,python,12-06-2024 11:03,312.0,1.0,1.0,True,01-07-2024 12:18,12-06-2024 11:20
67520512,use results of a mysql select as input of a match against natural language query,"i need to make a research by natural language on a mysql table field taking as input the values of another table field. i tried something similar but, as i suspected, it was not correct:
    select id, name, match(name), 
    against 
      (
        select name
        from table2
      ) as score
    from table1
    where match(name), 
    against 
      (
        select name
        from table2
      )

any idea?
update 1
i followed the kind example here below but i got the error ""#1064 - sql query syntax error near 'tb2"". i cannot see this syntax error. here the code i am testing:
select name, match(name) against 
(

    (select name
    from 
    (
        select name 
            from active_ingredients

        union all
        
        select active_ingredients.name as name
        from active_ingredients
        inner join temp_active_ingredients_aliases on temp_active_ingredients_aliases.alias_name = active_ingredients.name

    ) tbl
    group by name
    having count(*) = 1
    order by name) tb2 

) as score
from 
(
    
    select alias_name as name
    from temp_active_ingredients_aliases
    


) 
where match(name) against 
(

    (select name
    from 
    (
        select name 
            from active_ingredients

        union all
        
        select active_ingredients.name as name
        from active_ingredients
        inner join temp_active_ingredients_aliases on temp_active_ingredients_aliases.alias_name = active_ingredients.name

    ) tb3
    group by name
    having count(*) = 1
    order by name) tb4

)

the inner queries return the list of active_ingredients.name that do not exactly match at least one of the temp_active_ingredients_alieases.alias_name fields. so that i then try a match of the not exactly matching name(s) with the alias_name(s) by natural language fulltext research. to be noted that the following inner queries are working properly:
    select name
    from 
    (
        select name 
            from active_ingredients

        union all
        
        select active_ingredients.name as name
        from active_ingredients
        inner join temp_active_ingredients_aliases on temp_active_ingredients_aliases.alias_name = active_ingredients.name

    ) tbl
    group by name
    having count(*) = 1
    order by name

i am quite sure that the syntax error is very stupid, but i cannot see it.
update 2
here the links to the code for generating the two tables (schema and some data)","['mysql', 'nlp', 'match-against', 'against']",67520765,"yiu were almost there
to explain a bit further.

you put after the match(name) a comma that is wrong at that position.
group_concat is there because mysql expects there a list of words and this was the easiest way to achieve that, for further information about fulltextasearch and optimization see the manual as you see in the example a space as separator doesn't change anything
in natural language mode was only a assumption of mine as you wrote research by natural language all other options a also in that link explained


create table table1
(id int,
name text,
fulltext(name))



insert into table1 values(1,'text1')



create table table2
(id int,
name text)



insert into table2 values(1,'text1'),(2,'text2')



select t1.id, t1.name, match(t1.name) 
against 
  (
    (select group_concat(name)
    from table2) in natural language mode
  ) as score
from table1 t1 
where match(t1.name) 
against 
  (
    (select group_concat(name)
    from table2) in natural language mode
  )
  




id | name  |                      score
-: | :---- | -------------------------:
 1 | text1 | 0.000000001885928302414186



select t1.id, t1.name, match(t1.name) 
against 
  (
    (select group_concat(name separator ' ')
    from table2) in natural language mode
  ) as score
from table1 t1 
where match(t1.name) 
against 
  (
    (select group_concat(name separator ' ')
    from table2) in natural language mode
  )
  




id | name  |                      score
-: | :---- | -------------------------:
 1 | text1 | 0.000000001885928302414186


db<>fiddle here
i corrected your mistakes
select 
    name,
    match (name) against ((select 
            name
        from
            (select 
                name
            from
                active_ingredients union all select 
                active_ingredients.name as name
            from
                active_ingredients
            inner join temp_active_ingredients_aliases on temp_active_ingredients_aliases.alias_name = active_ingredients.name) tbl
        group by name
        having count(*) = 1
        order by name) ) as score
from
    (select 
        alias_name as name
    from
        temp_active_ingredients_aliases) db2
where
    match (name) against ((select 
            name
        from
            (select 
                name
            from
                active_ingredients union all select 
                active_ingredients.name as name
            from
                active_ingredients
            inner join temp_active_ingredients_aliases on temp_active_ingredients_aliases.alias_name = active_ingredients.name) tb3
        group by name
        having count(*) = 1
        order by name) )

without proper datqa, i can not test it,  in can only remove syntax error
select 
    name,
    match (name) against ((    select name
    from 
    (
        select name 
            from active_ingredients

        union all
        
        select active_ingredients.name as name
        from active_ingredients
        inner join temp_active_ingredients_aliases on temp_active_ingredients_aliases.alias_name = active_ingredients.name

    ) tbl
    group by name
    having count(*) = 1
    order by name) ) as score
from
    (select 
        alias_name as name
    from
        temp_active_ingredients_aliases) db2
where
    match (name) against ((    select name
    from 
    (
        select name 
            from active_ingredients

        union all
        
        select active_ingredients.name as name
        from active_ingredients
        inner join temp_active_ingredients_aliases on temp_active_ingredients_aliases.alias_name = active_ingredients.name

    ) tbl
    group by name
    having count(*) = 1
    order by name) )",https://stackoverflow.com/questions/67520512,mysql,13-05-2021 13:56,155.0,1.0,1.0,True,17-05-2021 08:35,17-05-2021 08:35
75240863,grouping text data in a corpus by a data frame variable,"i have a data frame in r with a column that i need to do basic text analysis on. i am able to do this modifying the code as needed from this source. however, i now need to do this same analysis but for groups of data. i've included the dput of a small sample here.
structure(list(pad.name = c(""missouri w"", ""missouri w"", ""missouri w"", 
""lee"", ""lee"", ""lee""), message = c(""pump maint"", ""pump maint"", ""pump maintenance"", 
""waiting on wireline"", 
""seating the ball"", ""waiting on wireline"")), row.names = 11:16, class = ""data.frame"")

i want to group by the variable pad.name. i've tried using corpus_group function from the quanteda as well as the corpus function from the same package, setting the parameters as follows: docid_field = dat$pad.name and text_field = dat$message. yet none of these seem to work.
my desired output are the most frequent words, say the top 10 most frequent, and a count of those words, for each unique pad.name. similar something to as follows, however the true counts would work out, obviously:
edit: the table option never seems to work here, so here is a dput and data frame of my desired output
structure(list(pad.name = c(""missouri w"", ""missouri w"", ""lee"", 
""lee""), word = c(""pump"", ""maint"", ""waiting"", ""wireline""), count = c(3, 
2, 2, 2)), class = ""data.frame"", row.names = c(na, -4l))

output <- data.frame(pad.name = c(""missouri w"", ""missouri w"", ""lee"", ""lee""), word = c(""pump"", ""maint"", ""waiting"", ""wireline""), count = c(3,2,2,2))","['r', 'dataframe', 'grouping', 'text-mining', 'corpus']",75241200,"would dplyr and tidytext do?
library(tidytext)
library(dplyr)

as_tibble(data) %>% 
  # split to words
  unnest_tokens(word,message) %>% 
  # filter out stopwords
  anti_join(get_stopwords()) %>% 
  # count by (pad.name, word) groups 
  count(pad.name, word, name = ""count"", sort = t) %>%
  # output is sorted by count, no grouping, keep top-4
  slice_head(n = 4) %>% 
  arrange(pad.name, desc(count))
#> joining, by = ""word""
#> # a tibble: 4 ï¿½ï¿½ 3
#>   pad.name   word     count
#>   <chr>      <chr>    <int>
#> 1 lee        waiting      2
#> 2 lee        wireline     2
#> 3 missouri w pump         3
#> 4 missouri w maint        2

input:
data <- structure(list(pad.name = c(
  ""missouri w"", ""missouri w"", ""missouri w"",
  ""lee"", ""lee"", ""lee""
), message = c(
  ""pump maint"", ""pump maint"", ""pump maintenance"",
  ""waiting on wireline&qu
  ""seating the ball"", ""waiting on wireline""
)), row.names = 11:16, class = ""data.frame"")


created on 2023-01-26 with reprex v2.0.2",https://stackoverflow.com/questions/75240863,r,25-01-2023 23:13,356.0,1.0,2.0,True,26-01-2023 08:49,25-01-2023 23:19
74785846,spacy model .from_disk doesn&#39;t load patterns,"for a named entity recognition task in dutch with spacy, i added entities using entityruler. when i add the ruler to the pipeline in my notebook:
nlp = spacy.load(""nl_core_news_md"")
ruler = nlp.add_pipe(""entity_ruler"", before=""ner"")
patterns  = complete_dicts # this is a list of dictionaries, e.g. [{""label"": ""person"", ""pattern"": ""staf aerts""}, {""label"": ""person"", ""pattern"": ""meyrem almaci""}]
ruler.add_patterns(patterns)

the ner-pipeline works very well. however, when i save it to my disk and then load this model again using
nlp.from_disk(""path/to_model"")

the model misses entities that are added through the entityruler.
i found nothing in the documentation why this would happen. i would be grateful for anyone who has an explanation for this! thanks.","['spacy', 'named-entity-recognition', 'spacy-3']",74794689,"to load a saved model, use spacy.load:
nlp = spacy.load(""/path/to/model"")

more details about how spacy.load works (including nlp.from_disk):",https://stackoverflow.com/questions/74785846,spacy,13-12-2022 13:46,285.0,4.0,1.0,True,14-12-2022 07:10,14-12-2022 04:33
21413256,understanding recall and precision,"i am currently learning information retrieval and i am rather stuck with an example of recall and precision
a searcher uses a search engine to look for information. there are 10 documents on the first screen of results and 10 on the second.
assuming there is known to be 10 relevant documents in the search engines index.
soo... there is 20 searches all together of which 10 are relevant.
can anyone help me make sense of this?
thanks","['search-engine', 'information-retrieval', 'precision-recall']",21413496,"recall and precision measure the quality of your result. to understand them let's first define the types of results. a document in your returned list can either be

classified correctly

a true positive (tp): a document which is relevant (positive) that was indeed returned (true) 
a true negative (tn): a document which is not relevant (negative) that was indeed not returned (true)

misclassified

a false positive (fp): a document which is not relevant but was returned positive
a false negative (fn): a document which is relevant but was not returned negative


the precision is then:
|tp| / (|tp| + |fp|)
i.e. the fraction of retrieved documents which are indeed relevant
the recall is then: 
|tp| / (|tp| + |fn|)
i.e. the fraction of relevant documents which are in your result set
so, in your example 10 out of 20 results are relevant. this gives you a precision of 0.5. if there are no more than these 10 relevant documents, you have got a recall of 1.
(when measuring the performance of an information retrieval system it only makes sense to consider both precision and recall. you can easily get a precision of 100% by returning no result at all (i.e. no spurious returned instance => no fp) or a recall of 100% by returning every instance (i.e. no relevant document was missed => no fn). )",https://stackoverflow.com/questions/21413256,search-engine,28-01-2014 18:00,5779.0,7.0,3.0,True,26-03-2022 06:06,12-03-2016 12:59
75595065,huggingface trainer throws an attributeerror:&#39;namespace&#39; object has no attribute &#39;get_process_log_level,"i am trying to run trainer from hugging face(pytorch) with arguments parser. my code looks like
if __name__ == '__main__':
    parser = hfargumentparser(trainingarguments)
    parser.add_argument('--model_name_or_path', type=str, required=true)
    .
    .
    .   
    .


    training_args = parser.parse_args()

    print('args', training_args)

    os.makedirs(training_args.output_dir, exist_ok=true)

    random.seed(training_args.seed)
    
    set_seed(training_args.seed)

    dataset_train = ...

    .
    .
    .


    trainer = trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataloader,
        eval_dataset=val_dataloader)

    trainer.train()

i am getting the following error:
    traceback (most recent call last):
    file ""main.py"", line 250, in <module>
    eval_dataset=val_dataloader)
    file ""c:\user\transformer\lib\site- 
    packages\transformers\trainer.py"", line 316, in __init__
    log_level = args.get_process_log_level()
    attributeerror: 'namespace' object has no attribute 'get_process_log_level

any idea about this error and how to solved it?","['python', 'deep-learning', 'pytorch', 'nlp', 'huggingface-transformers']",75595490,"first check that this works for you:
from transformers import trainingarguments

args = trainingarguments(output_dir='./')
args.get_process_log_level()

[out]:
20

if it doesn't then most probably the version of transformers you have on c:\user\transformer\lib\site-packages\transformers doesn't match the trainer script you have. then try to upgrade your transformers version pip install -u transformers.

if you get the 20 output but when you run your script you're getting the error, then most probably the version of your transformers is from a previous version that doesn't have the get_process_log_level() in the trainingarguments as a property.
add this line at the top of your code and check that the
import sys; print(sys.executable)

you'll get something like
[out]:
c://something/somewhere/bin/python

and with that do this to upgrade the library to the right site-package location and python binary
c://something/somewhere/bin/python -m pip install -u transformers

after upgrading the script should run.",https://stackoverflow.com/questions/75595065,python,28-02-2023 16:54,1379.0,2.0,1.0,True,07-06-2023 23:48,02-03-2023 17:27
76099661,finding words that indicate time order using nlp in python,"i want to find all words that indicate time using nlp in python. also known as time-order words in english. this includes works like 'evening' 'morning', 'first', '5 o'clock ', etc. i can't find a way to do this without having to make a list of every time order word in english. i need this for an english to american sign language sentence translator that should convert an english sentence to the correct grammatical structure of an asl sentence. in asl, timeorder words should be at the start of the sentence in most cases.
i can't find anything on spacy or nltk that does this for me, if anyone knows any functionalities i missed please let me know. this is my current code:
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
import string
import spacy

nltk.download('punkt', )
nltk.download('averaged_perceptron_tagger')

nlp = spacy.load('en_core_web_sm')
# nlp = spacy.load('en_core_web_lg')  #takess longer to load but is a larger dictionary

sentences=[""the big black cat stared at the small dog."",
           ""i didn't watch her brother in the evenings."",
           ""which car did jane buy?""]
doc = nlp(sentences[1])
# for word in doc
tokens_list = []
signs_list = []
for token in doc:
  # remove auxilaries, punctuation, determiners, prepositions
    if token.pos_ not in ['aux', 'punct'] and token.tag_ not in ['dt','in']:
        tokens_list.append([token.text, token.lemma_, token.pos_, token.tag_,])
        signs_list.append(token.lemma_)
for sign in signs_list:
  signs_list[signs_list.index(sign)] = sign.lower()
signs_list

if 'not' in signs_list: 
  ind = signs_list.index('not')
  signs_list[ind], signs_list[ind+1] = signs_list[ind+1], signs_list[ind]
# not comes after the verb it negates in asl

if 'i' in signs_list:
  ind = signs_list.index('i')
  signs_list[ind] = 'me'

# 'me' is used instead of 'i' in asl

signs_list

solution:
timeorderwords = [synset.lemma_names() for synset in wn.synsets('time_of_day')[0].hyponyms()]
for x in wn.synsets('time_of_day')[0].hyponyms():
  if len(x.hyponyms()) > 0:
    timeorderwords = [timeorderwords.append(y.lemma_names()) for y in x.hyponyms()]
timeorderwords = list(itertools.chain(*timeorderwords))
timeorderwords = [timeorderwords[x].replace('_', "" "") for x in range(len(timeorderwords))]
timeorderwords = [timeorderwords[x].replace('-', "" "") for x in range(len(timeorderwords))]
timeorderwords

the code above returns a flat list of all the words related (synsets) to 'time_of_day' and all the hyponyms of those related words. then all the _ and - are removed from the words
the list looks like this: (i had to add some words myself like afternoon)
timeorderwords = ['aurora', 'afternoon','bedtime', 'break of day', 'break of the day', 'breakfast time','canonical hour','closing time',
 'cockcrow','complin','compline','crepuscle','crepuscule','dawn','dawning','daybreak','dayspring','dinnertime',
 'dusk','early morning hour','evenfall','evening','evensong','fall','first light','gloam','gloaming','happy hour','high noon','late night hour',
 'lights out','lunch period','lunchtime','matins','mealtime','midday','midnight','morning','morning prayer','night','nightfall','none','nones','noon',
 'noonday','noontide','prime','rush hour','sext','small hours','sundown','sunrise','sunset','sunup','suppertime','terce','tierce','twelve noon','twilight','vespers','zero hour']","['python', 'nlp', 'spacy']",76100175,"you could use the knowledgebase of wordnet. it's an organized database of words, grouped into synonym sets (synsets). these are organized into a hierarchy. for example, there is a high-level synset for time of day which includes words you are looking for: 
you can access wordnet via nltk (alternatively omw or wn packages): from nltk.corpus import wordnet.",https://stackoverflow.com/questions/76099661,python,25-04-2023 09:33,192.0,0.0,1.0,True,26-04-2023 07:38,26-04-2023 07:38
68633410,how to create custom ner components in spacy v3,"i m trying to add an entityruler but i keep getting this error :[e002] can't find factory for 'ruler' for language french (fr), i don't know how to create a custom component for v3 and i have only found example for the older version and the documentation kinda of confused me.


pattern = [{""label"": ""org"", ""pattern"": ""neoledge""}]
ruler.add_patterns(pattern)
nlp.add_pipe('ruler')



edit:
@language.component('rulerorg') 
def rulerorg(doc):     
    org = [""..."",]     
    ruler= entityruler(nlp, overwrite_ents=true)     
    for o in org:        
        ruler.add_patterns([{""label"": ""org"", ""pattern"": o}])     
        return doc    

nlp.add_pipe('rulerorg')","['nlp', 'spacy']",68633778,"i guess you're trying to create an entityruler? if so you should write your code like this:
import spacy
nlp = spacy.blank(""en"")
pattern = [{""label"": ""org"", ""pattern"": ""neoledge""}]
ruler = nlp.add_pipe('entity_ruler', config={""overwrite_ents"":true})
ruler.add_patterns(pattern)

the entityruler and ner pipelines are different - ner is statistical, the entityruler is rule-based.
the way components are added to the pipeline changed between v2 and v3 and it looks like you have a mix of code.
you can see an example of the approach i outlined here in this part of the docs.",https://stackoverflow.com/questions/68633410,nlp,03-08-2021 09:19,361.0,1.0,1.0,True,01-09-2021 18:04,01-09-2021 18:04
76137512,"langchain, huggingface: can&#39;t evaluate model with two different inputs","i'm evaluating a llm on huggingface using langchain and python using this code:
# 

from langchain import huggingfacehub, llmchain
import os

hugging_face_write = ""my_key""
os.environ['huggingfacehub_api_token'] = hugging_face_write

from langchain import prompttemplate, huggingfacehub, llmchain

template = """"""question: {question}

answer: let's think step by step.""""""
prompt = prompttemplate(template=template, input_variables=[""question""])
llm_chain = llmchain(prompt=prompt, llm=huggingfacehub(repo_id=""google/flan-t5-xl"", model_kwargs={""temperature"":0, ""max_length"":64}))

question = ""what nfl team won the super bowl in the year justin beiber was born?""

print(llm_chain.run(question))


i get the error
valueerror                                traceback (most recent call last)
g:\meine ablage\python\lang_chain\langchain_huggingface_example.py in line 1
----> 19 print(llm_chain.run(question))

file c:\users\johan\.conda\envs\lang_chain\lib\site-packages\langchain\chains\base.py:213, in chain.run(self, *args, **kwargs)
    211     if len(args) != 1:
    212         raise valueerror(""`run` supports only one positional argument."")
--> 213     return self(args[0])[self.output_keys[0]]
    215 if kwargs and not args:
    216     return self(kwargs)[self.output_keys[0]]

file c:\users\johan\.conda\envs\lang_chain\lib\site-packages\langchain\chains\base.py:116, in chain.__call__(self, inputs, return_only_outputs)
    114 except (keyboardinterrupt, exception) as e:
    115     self.callback_manager.on_chain_error(e, verbose=self.verbose)
--> 116     raise e
    117 self.callback_manager.on_chain_end(outputs, verbose=self.verbose)
    118 return self.prep_outputs(inputs, outputs, return_only_outputs)

file c:\users\johan\.conda\envs\lang_chain\lib\site-packages\langchain\chains\base.py:113, in chain.__call__(self, inputs, return_only_outputs)
    107 self.callback_manager.on_chain_start(
    108     {""name"": self.__class__.__name__},
    109     inputs,
    110     verbose=self.verbose,
    111 )
...
    106 if self.client.task == ""text-generation"":
    107     # text generation return includes the starter text.
    108     text = response[0][""generated_text""][len(prompt) :]

valueerror: error raised by inference api: model google/flan-t5-xl time out

what am i doing incorrectly? i'm a newbie...
many thanks in advance, best regards from paris,
jennie
i ran my python script from above. after some waiting the shown error is given.","['python', 'huggingface', 'langchain']",76683808,"you need to upgrade your hugging face account to pro version to host the large model for inference.
""google/flan-t5-base"" works for the free account.",https://stackoverflow.com/questions/76137512,python,29-04-2023 17:28,3139.0,3.0,1.0,True,08-11-2023 16:40,30-04-2023 06:44
75882872,how to overcome rate limit error while working with gpt3 models using tenacity,"in my situation i am trying to pass a prompt using a helper function to the actual gpt3 models, in my case text-ada-001 and then eventually applying it on a pandas column using the following code. but i am recovering the following error:
    def sentiment_prompt(text):
    return """"""is the sentiment positive, negative or neutral for the following text:
    
    ""{}""
    """""".format(text)
    def sentiment_text(text):
        response = openai.completion.create(
           engine=""text-ada-001"",
           prompt=sentiment_prompt(text),
           max_tokens=1000,
           temperature=0,
           top_p=1,
           frequency_penalty=0,
           presence_penalty=0
    )
    sentiment = response.choices[0].text
    return sentiment

and then eventually applying to my pandas column:
    df['sentiment'] = df['text'].apply(lambda x :sentiment_text(x))

and the error;
    ratelimiterror: rate limit reached for default-global-with-image-limits in organization org-xxxx on requests per min. limit: 60 / min. please try again in 1s. contact support@openai.com if you continue to have issues. please add a payment method to your account to increase your rate limit. visit  to add a payment method.

to overcome this error i was looking into this link and found that tenacity could help resolve my issue. but i am not sure how to structure my code. i am doing the following at the moment
how do i use the code suggested in the link to overcome the rate limit error?","['pandas', 'openai-api', 'gpt-3', 'tenacity']",76323834,"import tenacity at the beginning of your code and then add its decoration where you are calling the openai library with create. so your code would look like this:
from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential,
) 

@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
def sentiment_text(text):
        your_prompt = """"""is the sentiment positive, negative or neutral for the 
                         following text:
    
                         ""{}""
                      """""".format(text)
        response = openai.completion.create(
           engine=""text-ada-001"",
           prompt=your_prompt ,
           max_tokens=1000,
           temperature=0,
           top_p=1,
           frequency_penalty=0,
           presence_penalty=0
        )
        sentiment = response.choices[0].text
        return sentiment",https://stackoverflow.com/questions/75882872,pandas,29-03-2023 23:04,1370.0,1.0,1.0,True,28-05-2023 11:16,01-04-2023 07:42
75475219,how to build an extracter spacy pipeline,"i am currently trying to extract some texts from sentences with spacy, did some courses about it but it is still a bit blur to me.
i have the following sentence:
zzz llc is a limited liability company formed in the uk.
xyz llc is a limited liability company formed in the uk. xyz llc owns a commercial property located in germany known as ï¿½ï¿½ï¿½rentviewï¿½ï¿½ï¿½.
mr x owns 21% of xyz llc, the remaining 79% are own by zzz llc which is the sole director of xyz llc.
what i want to extract are the following:
{""name"": ""xyz llc"", ""type"": ""org"", ""country"": ""uk""},
{""name"": ""zzz llc"", ""type"": ""org"", ""country"": ""uk""},
{""name"": ""xyz llc"", ""type"": ""org"", 
  ""owns"": [{""name"": ""rentview"", ""type"": ""commercial property"", ""country"": ""germany""}], 
  ""owned_by&quotuotname"": ""x"", ""type"": ""person"", ""percent"": 21},
    {""name"":""zzz llc"", ""type"": ""org"", ""percent"": 79}
  ]
}

my approach is first to assign an incorporation country to a company.
then detects owners of companies.
thereafter detects ownerby of companies.
and finally, generate the json object.
but i am already blocking about assigning country to company
my code to do this is the following, but i am pretty sure i'm not using the right approach.
span.set_extension(""incorporation_country"", default=false)

@language.component(""assign_org_country"")
def assign_org_country(doc):
  org_entities = [ent for ent in doc.ents if ent.label_ == ""org""]
  for ent in org_entities:
    head = ent.root.head
    if head.lemma_ in ['be']:
      for child in head.children:
        if child.dep_ == ""attr"" and child.text == ""company"" and child.right_edge.ent_type_ == ""gpe"":
          ent._.incorporation_country = child
          print(f""country of {ent.text} is {ent._.incorporation_country}"")
  return doc

any ideas or tips of how to achieve this?","['python', 'artificial-intelligence', 'spacy']",75549348,"the best approach to the problem is to use the spacy's powerful built-in entityruler to create custom rules
that can detect and assign countries to companies automatically.
here is an example using entityruler:
custom_patterns = [
    {""label"": ""org"", ""pattern"": [
        { ""lower"": {""in"": [""company"", ""corporation"", ""incorporated"", 
                           ""llc"", ""limited liability corporation""]}},
        { ""is_punct"": true },
        { ""ent_type"": ""gpe""}
    ]}
]

ruler = entityruler(nlp)
ruler.add_patterns(custom_patterns)

nlp.add_pipe(ruler)

docs = nlp(""zzz llc is a limited liability company formed in the uk."")

for ent in docs.ents:    
    print(f""country of {ent.text} is {ent._.incorporation_country}"")
    

this custom rule will return country for each entity data found you can cusomtize it as per your wish",https://stackoverflow.com/questions/75475219,python,16-02-2023 16:28,76.0,1.0,1.0,True,23-02-2023 19:14,16-02-2023 16:36
29110950,python concordance command in nltk,"i have a question regarding python concordance command in nltk. first, i came through an easy example: 
from nltk.book import *

text1.concordance(""monstrous"")

which worked just fine. now, i have my own .txt file and i would like to perform the same command. i have a list called ""textlist"" and want to find the word ""cna"" so i put command
textlist.concordance('cna') 

yet, i got the error 
attributeerror: 'list' object has no attribute 'concordance'. 

in the example, is the text1 not a list? i wonder what is going on here.","['python', 'nlp', 'nltk']",29111433,".concordance() is a special nltk function. so you can't just call it on any python object (like your list).
more specifically: .concordance() is a method in the text class of nltk
basically, if you want to use the .concordance(), you have to instantiate a text object first, and then call it on that object.
text

a text is typically initialized from a given document or corpus. e.g.:
import nltk.corpus  
from nltk.text import text  
moby = text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))


.concordance()

concordance(word, width=79, lines=25)
print a concordance for word with the specified context window. word matching is not case-sensitive. 

so i imagine something like this would work (not tested)
import nltk.corpus  
from nltk.text import text  
textlist = text(nltk.corpus.gutenberg.words('your file name here.txt'))
textlist.concordance('cna')",https://stackoverflow.com/questions/29110950,python,17-03-2015 22:31,29843.0,10.0,3.0,True,29-04-2021 17:28,29-04-2021 17:28
67540692,"error updating ner model in spacy 3, any advice?","i am currently updating the ner model from fr_core_news_lg pipeline. the code used to work about 1 or 2 months ago, when i last used it. but now, something happened and i can't run it anymore. i haven't change anything from the code, just wanted to run it again. but i received the following error:
traceback (most recent call last):
file ""../nermodel.py"", line 174, in <module>
ner_model.train(med_label)
file ""../nermodel.py"", line 102, in train
optimizer = self.nlp.entity.create_optimizer()
attributeerror: 'french' object has no attribute 'entity'

the error points to the part of the code where i update my ner model with new examples:
def train(self, label, n_iter=10, batch_size=50):
    # creating an optimizer and selecting a list of pipes not to train
    optimizer = self.nlp.entity.create_optimizer()
    other_pipes = [pipe for pipe in self.nlp.pipe_names if pipe != 'ner']

    # adding a named entity label
    ner = self.nlp.get_pipe('ner')
    ner.add_label(label)

    with self.nlp.disable_pipes(*other_pipes):
        for itn in range(n_iter):
            random.shuffle(self.train_data)
            losses = {}

            # batch the examples and iterate over them
            for batch in spacy.util.minibatch(self.train_data, size=batch_size):
                texts = [text for text, entities in batch]
                annotations = [entities for text, entities in batch]

                # update the model
                self.nlp.update(texts, annotations, sgd=optimizer, losses=losses)
                print(losses)
    print(""final loss: "", losses)

a single training example, so that ner learns that 'consultation' is an entity, goes as follows:
('et la consultation post-rï¿½ï¿½animation', {'entities': [(6, 18, 'medical_term')]})


i've updated spacy to the most recent version, and downloaded again the fr_core_news_lg model, even tried this in a new python environment, to no avail. which makes me think that there's a change in the pipeline or in spacy library. googling around, i wasn't able to find precisely an answer for this. does anybody have a fix for this?
edit: provided more details.<","['python', 'spacy', 'spacy-3']",67552865,"i think this code should work for you:
def train(self, label, n_iter=10, batch_size=50):
    # creating an optimizer and selecting a list of pipes not to train
    optimizer = self.nlp.create_optimizer()
    other_pipes = [pipe for pipe in self.nlp.pipe_names if pipe != 'ner']

    # adding a named entity label
    ner = self.nlp.get_pipe('ner')
    ner.add_label(label)

    with self.nlp.disable_pipes(*other_pipes):
        for itn in range(n_iter):
            random.shuffle(self.train_data)
            losses = {}

            # batch the examples and iterate over them
            for batch in spacy.util.minibatch(self.train_data, size=batch_size):
                for text, annotations in batch:
                    doc = nlp.make_doc(text)
                    example = example.from_dict(doc, annotations)
                    nlp.update([example], drop=0.35, sgd=optimizer, losses=losses)
                print(losses)
    print(""final loss: "", losses)

to break it down a little bit further, in spacy 3 there are two changes:

they got rid of entity in nlp.entity.create_optimizer()
we don't pass texts and annotations directly to nlp.update() but with example",https://stackoverflow.com/questions/67540692,python,14-05-2021 20:39,1191.0,0.0,1.0,True,01-08-2024 18:01,15-05-2021 19:16
76035558,change environment variable set in jupyter notebook with magics,"hope this question is not already answered, i've searched many posts but none of them says how to change an environment variable.
i wanted to load environment variables in .env so followed the steps in this post.
%env ""openai_api_key""=openai_api_key

then i wanted to see my variable with:
%env openai_api_key

everything worked well but my mistake was to erroneously copy the value for my environment variable. so i want to change it.
i tried dotenv to load this variable again but seems not to work.
hope you can help me, please.
i tried doing again the same procedure so that it can be changed/updated.","['jupyter-notebook', 'environment-variables', 'langchain']",76036170,"if you made a mistake writing your apikey:
created a .env with your credentials:
openai_api_key=""...erroneous-api-key...""

you verified:
%env openai_api_key

and you realized it is wrong.
you can:
import os
del os.environ[""openai_api_key""]

then:
os.environ[""openai_api_key""]=""...right-key...""

to verify this:
%env openai_api_key

and the updated api-key should be prompted.",https://stackoverflow.com/questions/76035558,jupyter-notebook,17-04-2023 13:13,2306.0,0.0,1.0,True,25-11-2024 13:23,25-11-2024 13:23
56884020,spacy with joblib library generates _pickle.picklingerror: could not pickle the task to send it to the workers,"i have a large list of sentences (~7 millions), and i want to extract the nouns from them.
i used joblib library to parallelize the extracting process, like in the following:
import spacy
from tqdm import tqdm
from joblib import parallel, delayed
nlp = spacy.load('en_core_web_sm')

class nouns:

    def get_nouns(self, text):
        doc = nlp(u""{}"".format(text))
        return [token.text for token in doc if token.tag_ in ['nn', 'nnp', 'nns', 'nnps']]

    def parallelize(self, sentences):
        results = parallel(n_jobs=1)(delayed(self.get_nouns)(sent) for sent in tqdm(sentences))
        return results

if __name__ == '__main__':
    sentences = ['we went to the school yesterday',
                 'the weather is really cold',
                 'can we catch the dog?',
                 'how old are you john?',
                 'i like diving and swimming',
                 'can the world become united?']
    obj = nouns()
    print(obj.parallelize(sentences))

when n_jobs in parallelize function is more than 1, i get this long error:
100%|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½| 6/6 [00:00<00:00, 200.00it/s""""""
traceback (most recent call last):
  file ""c:\python35\lib\site-packages\joblib\externals\loky\backend\queues.py"", line 150, in _feed
    obj_ = dumps(obj, reducers=reducers)
  file ""c:\python35\lib\site-packages\joblib\externals\loky\backend\reduction.py"", line 243, in dumps
    dump(obj, buf, reducers=reducers, protocol=protocol)
  file ""c:\python35\lib\site-packages\joblib\externals\loky\backend\reduction.py"", line 236, in dump
    _lokypickler(file, reducers=reducers, protocol=protocol).dump(obj)
  file ""c:\python35\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 267, in dump
    return pickler.dump(self, obj)
  file ""c:\python35\lib\pickle.py"", line 408, in dump
    self.save(obj)
  file ""c:\python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  file ""c:\python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  file ""c:\python35\lib\pickle.py"", line 836, in _batch_setitems
    save(v)
  file ""c:\python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  file ""c:\python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  file ""c:\python35\lib\pickle.py"", line 841, in _batch_setitems
    save(v)
  file ""c:\python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  file ""c:\python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  file ""c:\python35\lib\pickle.py"", line 836, in _batch_setitems
    save(v)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 770, in save_list
    self._batch_appends(obj)
  file ""c:\python35\lib\pickle.py"", line 797, in _batch_appends
    save(tmp[0])
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 725, in save_tuple
    save(element)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 718, in save_instancemethod
    self.save_reduce(types.methodtype, (obj.__func__, obj.__self__), obj=obj)
  file ""c:\python35\lib\pickle.py"", line 599, in save_reduce
    save(args)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 725, in save_tuple
    save(element)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 395, in save_function
    self.save_function_tuple(obj)
  file ""c:\python35\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 594, in save_function_tuple
    save(state)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  file ""c:\python35\lib\pickle.py"", line 836, in _batch_setitems
    save(v)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  file ""c:\python35\lib\pickle.py"", line 841, in _batch_setitems
    save(v)
  file ""c:\python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  file ""c:\python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  file ""c:\python35\lib\pickle.py"", line 836, in _batch_setitems
    save(v)
  file ""c:\python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  file ""c:\python35\lib\pickle.py"", line 599, in save_reduce
    save(args)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 740, in save_tuple
    save(element)
  file ""c:\python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  file ""c:\python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 740, in save_tuple
    save(element)
  file ""c:\python35\lib\pickle.py"", line 495, in save
    rv = reduce(self.proto)
  file ""stringsource"", line 2, in preshed.maps.preshmap.__reduce_cython__
typeerror: self.c_map cannot be converted to a python object for pickling
""""""exception in thread queuefeederthread:
traceback (most recent call last):
  file ""c:\python35\lib\site-packages\joblib\externals\loky\backend\queues.py"", line 150, in _feed
    obj_ = dumps(obj, reducers=reducers)
  file ""c:\python35\lib\site-packages\joblib\externals\loky\backend\reduction.py"", line 243, in dumps
    dump(obj, buf, reducers=reducers, protocol=protocol)
  file ""c:\python35\lib\site-packages\joblib\externals\loky\backend\reduction.py"", line 236, in dump
    _lokypickler(file, reducers=reducers, protocol=protocol).dump(obj)
  file ""c:\python35\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 267, in dump
    return pickler.dump(self, obj)
  file ""c:\python35\lib\pickle.py"", line 408, in dump
    self.save(obj)
  file ""c:\python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  file ""c:\python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  file ""c:\python35\lib\pickle.py"", line 836, in _batch_setitems
    save(v)
  file ""c:\python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  file ""c:\python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  file ""c:\python35\lib\pickle.py"", line 841, in _batch_setitems
    save(v)
  file ""c:\python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  file ""c:\python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  file ""c:\python35\lib\pickle.py"", line 836, in _batch_setitems
    save(v)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 770, in save_list
    self._batch_appends(obj)
  file ""c:\python35\lib\pickle.py"", line 797, in _batch_appends
    save(tmp[0])
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 725, in save_tuple
    save(element)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 718, in save_instancemethod
    self.save_reduce(types.methodtype, (obj.__func__, obj.__self__), obj=obj)
  file ""c:\python35\lib\pickle.py"", line 599, in save_reduce
    save(args)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 725, in save_tuple
    save(element)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 395, in save_function
    self.save_function_tuple(obj)
  file ""c:\python35\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 594, in save_function_tuple
    save(state)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  file ""c:\python35\lib\pickle.py"", line 836, in _batch_setitems
    save(v)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  file ""c:\python35\lib\pickle.py"", line 841, in _batch_setitems
    save(v)
  file ""c:\python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  file ""c:\python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  file ""c:\python35\lib\pickle.py"", line 836, in _batch_setitems
    save(v)
  file ""c:\python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  file ""c:\python35\lib\pickle.py"", line 599, in save_reduce
    save(args)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 740, in save_tuple
    save(element)
  file ""c:\python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  file ""c:\python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  file ""c:\python35\lib\pickle.py"", line 475, in save
    f(self, obj) # call unbound method with explicit self
  file ""c:\python35\lib\pickle.py"", line 740, in save_tuple
    save(element)
  file ""c:\python35\lib\pickle.py"", line 495, in save
    rv = reduce(self.proto)
  file ""stringsource"", line 2, in preshed.maps.preshmap.__reduce_cython__
typeerror: self.c_map cannot be converted to a python object for pickling

during handling of the above exception, another exception occurred:

traceback (most recent call last):
  file ""c:\python35\lib\threading.py"", line 914, in _bootstrap_inner
    self.run()
  file ""c:\python35\lib\threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  file ""c:\python35\lib\site-packages\joblib\externals\loky\backend\queues.py"", line 175, in _feed
    onerror(e, obj)
  file ""c:\python35\lib\site-packages\joblib\externals\loky\process_executor.py"", line 310, in _on_queue_feeder_error
    self.thread_wakeup.wakeup()
  file ""c:\python35\lib\site-packages\joblib\externals\loky\process_executor.py"", line 155, in wakeup
    self._writer.send_bytes(b"""")
  file ""c:\python35\lib\multiprocessing\connection.py"", line 183, in send_bytes
    self._check_closed()
  file ""c:\python35\lib\multiprocessing\connection.py"", line 136, in _check_closed
    raise oserror(""handle is closed"")
oserror: handle is closed



the above exception was the direct cause of the following exception:

traceback (most recent call last):
  file "".../playground.py"", line 43, in <module>
    print(obj.paralize(sentences))
  file "".../playground.py"", line 32, in paralize
    results = parallel(n_jobs=2)(delayed(self.get_nouns)(sent) for sent in tqdm(sentences))
  file ""c:\python35\lib\site-packages\joblib\parallel.py"", line 934, in __call__
    self.retrieve()
  file ""c:\python35\lib\site-packages\joblib\parallel.py"", line 833, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  file ""c:\python35\lib\site-packages\joblib\_parallel_backends.py"", line 521, in wrap_future_result
    return future.result(timeout=timeout)
  file ""c:\python35\lib\concurrent\futures\_base.py"", line 405, in result
    return self.__get_result()
  file ""c:\python35\lib\concurrent\futures\_base.py"", line 357, in __get_result
    raise self._exception
_pickle.picklingerror: could not pickle the task to send it to the workers.

what is the problem in my code?","['python', 'python-3.x', 'parallel-processing', 'spacy', 'joblib']",56885830,"q: what is the problem in my code?

well, most probably the issue comes not from the code, but from the ""hidden"" processing, that appears, once n_jobs directs ( and joblib internally orchestrates ) to prepare that many exact copies of the main process, so as to let them work independently one of each other ( effectively thus escaping from gil-locking and mapping the multiple process-flows onto physical hardware resources )
this step is responsible for making copies of all pythonic objects and was known to use pickle for doing this. the pickle module was known for its historical principal limitations on what can be pickled and what cannot.
the error message confirms this:

typeerror: self.c_map cannot be converted to a python object for pickling

one may try a trick to supply mike mckearns dill module instead of pickle and test, if your ""problematic"" python objects will get pickled with this module without throwing this error.
dill has the same api signatures, so a pure import dill as pickle may help with leaving all the other code the same.
i had the same problems, with large models to get distributed into and back from multiple processes and the dill was a way to go. also the performance has increased.

bonus: dill allows to save / restore the full python interpreter state!

this was a cool side-effect of finding dill, once import dill as pickle was done, pickle.dump_session( <afile> ) will save ones complete state-full copy of the python interpreter session. this can be restored, if needed ( post-crash restores, trained trained and optimised ml-model state-fully saved / restored, incremental learning ml-model state-fully saved and re-distributed for remote restores for the deployed user-bases, etc. )",https://stackoverflow.com/questions/56884020,python,04-07-2019 08:48,42252.0,26.0,7.0,True,03-11-2022 06:47,04-07-2019 10:24
72680734,huggingface trainer only doing 3 epochs no matter the trainingarguments,"im new at machine learning and i'm facing an issue where i want to increase the epochs for training but .train() will only do 3 epochs. what am i doing wrong?
this is my dataset:

> datasetdict({ train: dataset({ features: [ï¿½ï¿½ï¿½textï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½labelï¿½ï¿½ï¿½], num_rows:
> 85021 }) test: dataset({ features: [ï¿½ï¿½ï¿½textï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½labelï¿½ï¿½ï¿½], num_rows: 15004
> }) })


and its features:
> {ï¿½ï¿½ï¿½labelï¿½ï¿½ï¿½: classlabel(num_classes=20, names=[ï¿½ï¿½ï¿½01. agriï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½02. alimï¿½ï¿½ï¿½,
> ï¿½ï¿½ï¿½03. chemferï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½04. atexï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½05. machï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½06. marnavï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½07. constï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½08.
> minesï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½09. domï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½10. tranï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½11. arartillï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½12. preelecï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½13.
> cerï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½14. achimiï¿½ï¿½ï¿½de>trainer = trainer(
model=model,
args=training_args,
train_dataset=tokenized_datasets[ï¿½ï¿½ï¿½trainï¿½ï¿½ï¿½],
eval_dataset=tokenized_datasets[ï¿½ï¿½ï¿½testï¿½ï¿½ï¿½],
data_collator=data_collator,
tokenizer=tokenizer,
compute_metrics=compute_metrics,
)

what my .train() is showing:

***** running training ***** num examples = 85021 num epochs = 3 instantaneous batch size per device = 8 total train batch size (w.
parallel, distributed & accumulation) = 8 gradient accumulation steps
= 1 total optimization steps = 31884
|epoch|training loss|validation loss|accuracy|
|1|0.994300|0.972638|0.711610|
|2|0.825400|0.879027|0.736337|
|3|0.660800|0.893457|0.744401|

i would like to continue training beyond the 3 epochs to increase my accuracy and continue to decrease training and validation loss. i tried changing the num_train_epochs=10 as you can see but nothing changes.
this is largely my code:
frominingarguments

training_args = trainingarguments(
    output_dir='./results',          # output directory
    num_train_epochs=10,              # total number of training epochs
    per_device_train_batch_size=8,  # batch size per device during training
    per_device_eval_batch_size=16,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
  logging_steps=10,
)

### metrics
from datasets import load_metric
metric = load_metric(""accuracy"")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

### trainer
from transformers import trainingarguments, trainer

training_args = trainingarguments(output_dir=""test_trainer"", evaluation_strategy=""epoch"")","['machine-learning', 'huggingface-transformers']",72688982,"i found the issue. i had in my code twice defined training_args. the second time was right before the trainer and thus the trainer was reading the args from the one definition where i did not write in the option for several epochs.
code should be:
    training_args = trainingarguments(
        output_dir='./results',          # output directory
        num_train_epochs=10,              # total number of training epochs
        per_device_train_batch_size=8,  # batch size per device during training
        per_device_eval_batch_size=16,   # batch size for evaluation
        warmup_steps=500,                # number of warmup steps for learning rate scheduler
        weight_decay=0.01,               # strength of weight decay
        logging_dir='./logs',            # directory for storing logs
      logging_steps=10,
    )
    
    ### metrics
    from datasets import load_metric
    metric = load_metric(""accuracy"")
    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        return metric.compute(predictions=predictions, references=labels)

after this part you can call the trainer.",https://stackoverflow.com/questions/72680734,machine-learning,19-06-2022 22:31,3575.0,2.0,1.0,True,23-06-2022 14:38,20-06-2022 14:50
67381454,loop through all the values (string) in one column and append the values in another column if not unique-text processing,"i would like to find a solution for the following problem:
import pandas as pd

rows = {'id': ['xb01','nt02','tw02','dt92','tw03','we04','er04','ew06','re07','ti92'],
    'datasetname': ['first label','second label','third     label','fourth label','third 
label','third label','third label','fourth label','first  label','last label'],
    'target': ['first label','second label','the    third labels','fourth label 
set','third    label', 'third label','third label  sets','fourth label    sets','first 
label','last labels']
    }

df = pd.dataframe(rows, columns = ['id', 'datasetname','target'])

print (df)

the dataframe looks like this:
     id      datasetname                      target

   xb01         first label              first label
   nt02        second label             second label
   tw02     third     label      the    third labels
   dt92        fourth label         fourth label set
   tw03         third label           third    label
   we04         third label              third label
   er04         third label        third label  sets
   ew06        fourth label     fourth label    sets
   re07        first  label              first label
   ti92          last label              last labels

pseudo code:
   for i in len(range(df)):
      if datasetname[i].is_unique:
         if datasetname[i]!=target[i]:
            target[i]=datasetname[i]+ '|'+target[i]
      else:
         loop through dataframe and find all labels that belongs to the same datasetname 
         and append all those target names together. (note: if datasetname is not same as 
         target name(s), the dataset name should also append to the target)

here we can see:
   datasetname    appeared   target

   first label    2          first label
   second label   1          second label
   third label    4          the third labels | third label | third label sets
   fourth label   2          fourth label set | fourth label sets|fourth label
   last label     1          last labels | last label

the expect output:
   id                  datasetname                                             target
  
 xb01                  first label                                        first label
 nt02                 second label                                      second  label
 tw02                  third label      the third labels|third label|third label sets
 dt92                 fourth label   fourth label set|fourth label sets |fourth label
 tw03                  third label      the third labels|third label|third label sets
 we04                  third label      the third labels|third label|third label sets
 er04                  third label      the third labels|third label|third label sets
 ew06                 fourth label   fourth label set|fourth label sets| fourth label
 re07                  first label                                        first label
 ti92                   last label                             last labels|last label
            

note: the real dataframe have 100,000 rows. there might be extra spaces still exist in those strings(i have already implemented dataframe-lower case(), removed all extra marks,etc.). it might have some mistakes (typo) in this question(i have copied and pasted several times), but hopefully you get my idea for what 's the solution i 'm looking for. thank you!","['python', 'pandas', 'dataframe', 'nlp']",67384192,"let's try an agg with unique values and merge back:
import pandas as pd

rows = {'id': ['xb01', 'nt02', 'tw02', 'dt92', 'tw03', 'we04',
               'er04', 'ew06', 're07', 'ti92'],
        'datasetname': ['first label', 'second label', 'third     label',
                        'fourth label', 'third label', 'third label',
                        'third label', 'fourth label',
                        'first  label', 'last label'],
        'target': ['first label', 'second label', 'the    third labels',
                   'fourth label set', 'third label',
                   'third label', 'third label  sets',
                   'fourth label    sets', 'first label', 'last labels']
        }

df = pd.dataframe(rows, columns=['id', 'datasetname', 'target'])
# fix spacing in columns names
df = df.replace({r'\s+': ' '}, regex=true)
# get unique matches
matches = df.groupby('datasetname') \
    .apply(lambda x: x['datasetname'].append(x['target']).unique()) \
    .agg('|'.join).rename('target')
# merge back to original dataframe
merged = df.drop(columns=['target']).merge(matches, on='datasetname', how=""left"")

# for display
print(merged.to_string())

output:

     id   datasetname                                           target
0  xb01   first label                                      first label
1  nt02  second label                                     second label
2  tw02   third label    third label|the third labels|third label sets
3  dt92  fourth label  fourth label|fourth label set|fourth label sets
4  tw03   third label    third label|the third labels|third label sets
5  we04   third label    third label|the third labels|third label sets
6  er04   third label    third label|the third labels|third label sets
7  ew06  fourth label  fourth label|fourth label set|fourth label sets
8  re07   first label                                      first label
9  ti92    last label                           last label|last labels",https://stackoverflow.com/questions/67381454,python,04-05-2021 08:42,59.0,2.0,1.0,True,04-05-2021 12:45,04-05-2021 12:41
76465343,huggingface transformers model config reported &quot;this is a deprecated strategy to control generation and will be removed soon&quot;,"i am training a sequence-to-sequence model using huggingface transformers' seq2seqtrainer. when i execute the training process, it reports the following warning:

/path/to/python3.9/site-packages/transformers/generation/utils.py:1219: userwarning: you have modified the pretrained model configuration to control generation. this is a deprecated strategy to control generation and will be removed soon, in a future version. please use a generation configuration file (see 

note the huggingface documentation link is dead.
i use the following codes:
model = bartforconditionalgeneration.from_pretrained(checkpoint)
model.config.output_attentions = true
model.config.output_hidden_states = true

training_args = seq2seqtrainingarguments(
    output_dir = ""output_dir_here"",
    evaluation_strategy = intervalstrategy.steps, #""epoch"",
    optim = ""adamw_torch"", # use new pytorch optimizer
    eval_steps = 1000, # new
    logging_steps = 1000,
    save_steps = 1000,
    learning_rate = 2e-5,
    per_device_train_batch_size = batch_size,
    per_device_eval_batch_size = batch_size,
    weight_decay = 0.01,
    save_total_limit = 3,
    num_train_epochs = 30,
    predict_with_generate=true,
    remove_unused_columns=true,
    fp16 = true,
    push_to_hub = true,
    metric_for_best_model = 'bleu', # new or ""f1""
    load_best_model_at_end = true # new
)

trainer = seq2seqtrainer(
    model = model,
    args = training_args,
    train_dataset = train_ds,
    eval_dataset = eval_ds,
    tokenizer = tokenizer,
    data_collator = data_collator,
    compute_metrics = compute_metrics,
    callbacks = [earlystoppingcallback(early_stopping_patience=3)]
)

trainer.train()

the training process can be completed without any problem, but i am concerned about the deprecation warning. how should i modify the codes to solve the problem?
version:

transformers 4.28.1
python 3.9.7","['python', 'huggingface-transformers']",76514925,"root-cause
this is a warning about using the api in the outdated manner (=unsupported soon). however, as of now, the code is fixing this on its own - hence only a warning not a breaking error.
see these lines in the source code.
remedy
the transformers library encourages the use of config files. in this case, we need to pass a generationconfig object early, rather than to set attributes.

i will first share a clean, simple example:
from transformers import autotokenizer, bartforconditionalgeneration

model = bartforconditionalgeneration.from_pretrained(""facebook/bart-large-cnn"")
tokenizer = autotokenizer.from_pretrained(""facebook/bart-large-cnn"")

article_to_summarize = (
    ""pg&e stated it scheduled the blackouts in response to forecasts for high winds ""
    ""amid dry conditions. the aim is to reduce the risk of wildfires. nearly 800 thousand customers were ""
    ""scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.""
)
inputs = tokenizer([article_to_summarize], max_length=1024, return_tensors=""pt"")

# change config and generate summary

from transformers.generation import generationconfig

model.config.max_new_tokens = 10
model.config.min_length = 1
gen_cfg = generationconfig.from_model_config(model.config)
gen_cfg.max_new_tokens = 10
gen_cfg.min_length = 1

summary_ids = model.generate(inputs[""input_ids""], generation_config=gen_cfg)
tokenizer.batch_decode(summary_ids, skip_special_tokens=true, clean_up_tokenization_spaces=false)[0]

if you try to manipulate the config attributes directly and pass no config, you get a warning. if you pass a generationconfig, you are all good. this example is reproducible as a colab notebook here.

now, to the original question. note that, in general, changing architecture configs of pretrained models is not recommended for incompatibility reasons. this is sometimes possible with extra effort. however, certain config changes are possible upon initialization:
model = bartforconditionalgeneration.from_pretrained(
     ""facebook/bart-large-cnn"", 
     attention_dropout=0.123
)

here is the fully-working code, corrected for reproducibility and see also this notebook
from transformers import autotokenizer, bartforconditionalgeneration
from transformers.generation import generationconfig
from transformers import trainer, trainingarguments
from transformers.models.bart.modeling_bart import shift_tokens_right
from transformers import datacollatorforseq2seq

model = bartforconditionalgeneration.from_pretrained(""facebook/bart-large-cnn"", attention_dropout=0.123)
tokenizer = autotokenizer.from_pretrained(""facebook/bart-large-cnn"")
seq2seq_data_collator = datacollatorforseq2seq(tokenizer, model=model)

def get_features(batch):
    input_encodings = tokenizer(batch[""text""], max_length=1024, truncation=true)
    
    with tokenizer.as_target_tokenizer():
        target_encodings = tokenizer(batch[""summary""], max_length=256, truncation=true)
        
    return {""input_ids"": input_encodings[""input_ids""], 
           ""attention_mask"": input_encodings[""attention_mask""], 
           ""labels"": target_encodings[""input_ids""]}

dataset_ftrs = dataset.map(get_features, batched=true)
columns = ['input_ids', 'labels', 'input_ids','attention_mask',] 
dataset_ftrs.set_format(type='torch', columns=columns)

training_args = trainingarguments(
    output_dir='./models/bart-summarizer',          
    num_train_epochs=1,           
    per_device_train_batch_size=1, 
    per_device_eval_batch_size=1,   
    warmup_steps=500,               
    weight_decay=0.01,              
    logging_dir='./logs',          
)

model.config.output_attentions = true
model.config.output_hidden_states = true

training_args = trainingarguments(
    output_dir='./models/bart-summarizer', 
    num_train_epochs=1, 
    warmup_steps=500,                                  
    per_device_train_batch_size=1, 
    per_device_eval_batch_size=1, 
    weight_decay=0.01, 
    logging_steps=10, 
    push_to_hub=false, 
    evaluation_strategy='steps', 
    eval_steps=500, 
    save_steps=1e6, 
    gradient_accumulation_steps=16,
)

trainer = trainer(
    model=model, 
    args=training_args, 
    tokenizer=tokenizer,                  
    data_collator=seq2seq_data_collator,                  
    train_dataset=dataset_ftrs[""train""],                  
    eval_dataset=dataset_ftrs[""test""],
)

assert model.config.attention_dropout==0.123

#trainer.train()",https://stackoverflow.com/questions/76465343,python,13-06-2023 13:12,13667.0,7.0,1.0,True,13-08-2023 20:29,20-06-2023 13:13
77553068,language confusion using open multilingual wordnet with nltk,"i'm trying to understand nltk's wordnet module. even downloading the omw-1.4 module, when i request the synsets of a word in portuguese it returns terms in english.
when i ask for the languages loaded in the module, only 'eng' appears, but i understand that others should appear, since i loaded open multilingual wordnet (omw-1.4).
the same does not happen when i request synonyms. the synonyms all appear in portuguese.
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')

    [nltk_data] downloading package wordnet to /root/nltk_data...
    [nltk_data]   package wordnet is already up-to-date!
    [nltk_data] downloading package omw-1.4 to /root/nltk_data...
    [nltk_data]   package omw-1.4 is already up-to-date!
    true

from nltk.corpus import wordnet as wnet
sorted(wnet.langs())
    ['eng']

wnet.synsets(""casa"", lang='por')
[synset('apartment.n.01'),
 synset('building.n.01'),
 synset('chalet.n.01'),
 synset('cringle.n.01'),
 synset('detached_house.n.01'),
 synset('dwelling.n.01'),
 synset('house.n.01'),
 synset('house.n.12'),
 synset('housing.n.01'),
 synset('theater.n.01'),
 synset('house.n.06'),
 synset('manufacturer.n.01'),
 synset('family.n.01'),
 synset('residence.n.01'),
 synset('domicile.n.01'),
 synset('home.n.01'),
 synset('home.n.06'),
 synset('home.n.07')]

wnet.synonyms(""casa"",lang='por')

  #output:      
    [['apartamento', 'aposentos'],
         ['edifï¿½ï¿½cio', 'edifï¿½ï¿½cios', 'prï¿½ï¿½dio'],
         ['chalï¿½ï¿½'],
         ['ilhï¿½ï¿½'],
         [],
         ['aposento',
          'domicï¿½ï¿½lio',
          'habitaï¿½ï¿½ï¿½ï¿½o',
          'lar',
          'morada',
          'moradia',
          'residï¿½ï¿½ncia'],
         ['habitaï¿½ï¿½ï¿½ï¿½es',
          'edifï¿½ï¿½cios_residenciais',
          'firma',
          'habitaï¿½ï¿½ï¿½ï¿½o',
          'teatro',
          'vivenda'],
         [],
         ['abrigo', 'abrigos', 'alojamentos'],
         ['teatro'],
         [],
         ['fabricante'],
         ['agregado_familiar',
          '       'pessoa_da_famï¿½ï¿½lia_que_mora_na_mesma_casa'],
         ['domicï¿½ï¿½lio', 'residï¿½ï¿½ncia'],
         ['domicï¿½ï¿½lio'],
         ['lar', 'lar'],
         ['lar'],
         ['lar']","['python', 'nltk', 'wordnet', 'open-multilingual-wordnet']",77559118,"the synset('apartment.n.01'), is the name of a graph node; it has been named after an english word, but you have to do an extra step to get text in english (or any language).
so to get the human language text use lemma_names(), like this:
wnet.synset('apartment.n.01').lemma_names('por')

ref:",https://stackoverflow.com/questions/77553068,python,26-11-2023 19:00,535.0,1.0,1.0,True,27-11-2023 18:19,26-11-2023 19:12
78793654,unable to send a local image that isn&#39;t under  to openai using its new .net library,"i have an asp.net core 6 web api with the new official library from openai (
what i'm trying to do, is to use a local image file to openai. the file isn't under  but under backend/assets/1.jpg.

i've written a basic service to setup all the information needed in order to send a request to openai. but the problem is that i'm not able to send the image.
i keep getting errors like ""url is too long"" or ""invalid image"",
here is my code - openaiservice:
using openai.chat;

namespace backend.services
{
    public class openaiservice
    {
        private readonly chatclient _chatclient;
        private readonly chatcompletionoptions _options;

        public openaiservice(iconfiguration configuration)
        {
            var apikey = configuration.getvalue<string>(""openai:key"");
            _chatclient = new chatclient(""gpt-4o"", apikey);

            _options = new chatcompletionoptions()
            {
                maxtokens = 300,
            };
        }

        public async task<string> extractlistofitems()
        {
            var imagepath = path.combine(directory.getcurrentdirectory(), ""assets"", ""1.jpg"");
            var localurl = $""
            
            var messages = new list<chatmessage>
            {
                new userchatmessage(new list<chatmessagecontentpart>
                {
                    chatmessagecontentpart.createtextmessagecontentpart(""extract the items from the following image and return a list of items including prices and amount.""),
                    chatmessagecontentpart.createimagemessagecontentpart(new uri(localurl))
                })
            };

            var completion = await _chatclient.completechatasync(messages, _options);
            return completion.value.tostring();
        }
    }
}

demo controller for testing:
using microsoft.aspnetcore.mvc;
using system.threading.tasks;
using backend.services;
using openai;
using openai.chat;

namespace backend.controllers;

[apicontroller]
[route(""[controller]"")]
public class openaidemocontroller : controllerbase
{
    private readonly openaiservice _openaiservice;

    public openaidemocontroller(openaiservice openaiservice)
    {
        _openaiservice = openaiservice;
    }
    
    [
    [route(""extract-items"")]
    public async task<iactionresult> completesentence()
    {
        var completion = await _openaiservice.extractlistofitems();
        return ok(completion);
    }
}

program.cs file:
using backend.configurations;
using backend.services;
using microsoft.extensions.fileproviders;

var builder = webapplication.createbuilder(args);

// add services to the container.

builder.services.configure<openaiconfig>(builder.configuration.getsection(""openai""));

//add services
builder.services.addsingleton<openaiservice>();

builder.services.addcontrollers();
builder.services.addendpointsapiexplorer();
builder.services.addswaggergen();

// builder.services.addscoped<iopenaiservice, openaiservice>();

builder.services.addcors(opt =>
{
    opt.addpolicy(""allowall"", builder =>
    {
        builder.allowanyorigin()
            .allowanymethod()
            .allowanyheader();
    });
});

var app = builder.build();

app.usestaticfiles(new staticfileoptions
{
    fileprovider = new physicalfileprovider(
        path.combine(builder.environment.contentrootpath, ""assets"")),
    requestpath = ""/assets""
});

app.usestaticfiles(); // this serves files from 

app.usestaticfiles(new staticfileoptions
{
    fileprovider = new physicalfileprovider(
        path.combine(builder.environment.contentrootpath, ""assets"")),
    requestpath = ""/assets""
});

// configure the http request pipeline.
if (app.environment.isdevelopment())
{
    app.useswagger();
    app.useswaggerui();
}

app.use
app.usecors(""allowall"");
app.useauthorization();

app.mapcontrollers();

app.run();

any idea what i'm doing wrong?","['c#', 'asp.net-core-webapi', 'backend', '.net-6.0', 'openai-api']",78794848,"i was able to solve it on my own.
there's an example in openai official repo that helped me to solve it.


tho, im not sure if my implementation is mostly correct. i will leave this open for any other suggestions.
service:
using openai.chat;


namespace backend.services
{
    public class openaiservice
    {
        private readonly chatclient _chatclient;
        private readonly chatcompletionoptions _options;

        public openaiservice(iconfiguration configuration)
        {
            var apikey = configuration.getvalue<string>(""openai:key"");
            _chatclient = new chatclient(""gpt-4o"", apikey);
            _options = new chatcompletionoptions()
            {
                maxtokens = 300,
            };
        }

        public async task<string> extractlistofitems()
        {
            var imagefilepath = path.combine(""assets"", ""1.jpg"");
            await using stream imagestream = file.openread(imagefilepath);
            var imagebytes = binarydata.fromstream(imagestream);

            var messages = new list<chatmessage>
            {
                new userchatmessage(new list<chatmessagecontentpart>
                {
                    chatmessagecontentpart.createtextmessagecontentpart(""describe the image. ""),
                    chatmessagecontentpart.createimagemessagecontentpart(imagebytes, ""image/png"")
                })
            };

            var completion = await _chatclient.completechatasync(messages, _options);
            return completion.value.tostring();
        }
        
    }
}

usage in controller:
using microsoft.aspnetcore.mvc;
using system.threading.tasks;
using backend.services;
using openai;
using openai.chat;

namespace backend.controllers;

[apicontroller]
[route(""[controller]"")]
public class openaidemocontroller : controllerbase
{
    private readonly openaiservice _openaiservice;

    public openaidemocontroller(openaiservice openaiservice)
    {
        _openaiservice = openaiservice;
    }
    
    [
    [route(""extract-items"")]
    public async task<iactionresult> completesentence()
    {
        var completion = await _openaiservice.extractlistofitems();
        return ok(completion);
    }
    
}

no need to apply for static file middleware.",https://stackoverflow.com/questions/78793654,c#,25-07-2024 13:37,376.0,0.0,1.0,True,25-07-2024 18:20,25-07-2024 18:20
39178154,how to split a string on commas or periods in nltk,"i want to separate a string on commas and/or periods in nltk. i've tried with sent_tokenize() but it separates only on periods.
i've also tried this code
from nltk.tokenize.punkt import punktsentencetokenizer, punktlanguagevars
ex_sent = ""this is an example showing sentence filtration.this is how it is done, in the case of python i want to learn more. so, that i can have some experience over it, by it i mean python.""
class commapoint(punktlanguagevars):
    sent_end_chars = ('.','?','!',',')
tokenizer = punktsentencetokenizer(lang_vars = commapoint())
n_w=tokenizer.tokenize(ex_sent)
print (n_w)

the output for the code above is
['this is an example showing sentence filtration.this is how it is done,' 'in case of python i want to learn more.' 'so,' 'that i can have some experience over it,' 'by it i mean python.\n']

when i try to give '.' without any space it is taking it as a word
i want the output as
['this is an example showing sentence filtration.' 'this is how it is done,' 'in case of python i want to learn more.' 'so,' 'that i can have some experience over it,' 'by it i mean python.']","['python', 'nltk']",39190983,"how about something simpler with re:
>>> import re
>>> sent = ""this is an example showing sentence filtration.this is how it is done, in case of python i want to learn more. so, that i can have some experience over it, by it i mean python.""
>>> re.split(r'[.,]', sent)
['this is an example showing sentence filtration', 'this is how it is done', ' in case of python i want to learn more', ' so', ' that i can have some experience over it', ' by it i mean python', '']

to keep the delimiter, you can use group:
>>> re.split(r'([.,])', sent)
['this is an example showing sentence filtration', '.', 'this is how it is done', ',', ' in case of python i want to learn more', '.', ' so', ',', ' that i can have some experience over it', ',', ' by it i mean python', '.', '']",https://stackoverflow.com/questions/39178154,python,27-08-2016 06:26,7742.0,2.0,2.0,True,03-05-2023 21:16,03-05-2023 21:16
69240815,"i am trying to import:from torchtext.legacy.data import field, bucketiterator,iterator,data, but get error &#39;no module named &#39;torchtext.legacy&#39;","i am trying to execute the following code for a nlp proj
import torchtext
from torchtext.legacy.data import field, bucketiterator, iterator
from torchtext.legacy import data


----> 6 from torchtext.legacy.data import field, bucketiterator, iterator
      7 from torchtext.legacy import data
      8 

modulenotfounderror: no module named 'torchtext.legacy'.

i have tried it on both kaggle notebook and jupyter notebook and found the same error in both.
i even tried to install !pip install -qqq deepmatcher==0.1.1 in kaggle to solve the issue but it still gives the same error.
is there any solution to this?","['python', 'nlp', 'pytorch']",72271924,"before you import torchtext.legacy, you need to !pip install torchtext==0.10.0.
maybe legacy was removed in version 0.11.0.",https://stackoverflow.com/questions/69240815,python,19-09-2021 06:20,39397.0,7.0,3.0,True,19-02-2023 07:21,19-09-2021 06:30
74297347,regex: how to i get the name more the character &quot; : &quot;,"im using python to extract some info
i wanna get the words/names before the charcter :
but the problem is everythig is tied together
from here
morgan stanley.erik woodring: 

i just wanna extract ""erik woodring:""
or from here
market.operator: 

i just wanna extract operator:
sometimes there are questiosn like this
to acquire?tim cook:

i just wanna extract ""tim cook:""
this is what i tried
\w*(?=.*:)
this is not getting what i wanted, its returning a lot of words","['python', 'regex', 'web-scraping', 'nlp']",74300028,"this could be the regex you're looking for:
\b[\w\s]+(?=:)


\b world boundary;
[\w\s]+ matches any word or whitespace (at least one character);
(?=:) positive lookahead that specifies the word must be followed by a punctation mark;


if you want to get the "":"" too you can simply remove the lookahead:
\b[\w\s]+:",https://stackoverflow.com/questions/74297347,python,03-11-2022 01:41,36.0,0.0,1.0,True,03-11-2022 08:28,03-11-2022 01:48
69802895,the inputs into bert are token ids. how do i get the corresponding the input token vectors into bert?,"i am new and learning about transformers.
in a lot of bert tutorials, i see the input is just the token id of the words. but surely we need to convert this token id to a vector representation (it can be one hot encoding, or any initial vector representation for each token id) so that it can be used by the model.
my question is: where cam i find this initial vector representation for each token?","['nlp', 'huggingface-transformers', 'bert-language-model', 'word-embedding']",69807638,"in bert, the input is a string itself. then, bert manages to convert it into a token and then, create its vector. let's see an example:
prep_url = '
enc_url = ' 
bert_preprocess = hub.keraslayer(prep_url)
bert_encoder = hub.keraslayer(enc_url)

text = ['hello i""m new to stack overflow']

# first, you need to preprocess the data

preprocessed_text = bert_preprocess(text)
# this will give you a dict with a few keys such us input_word_ids, that is, the tokenizer

encoded = bert_encoder(preprocessed_text)
# and this will give you the (1, 768) vector with the context value of the previous text. the output is encoded['pooled_output']

# you can play with both dicts, printing its keys()

i recommend you to go to both links above and do a little of research. to recap, bert uses string as inputs and then tokenize it (with its own tokenzer!). if you want to tokenize with the same values, you need the same vocab file, but for a fresh start like you are doing this should be enough.",https://stackoverflow.com/questions/69802895,nlp,01-11-2021 21:19,2723.0,0.0,1.0,True,02-11-2021 08:56,02-11-2021 07:02
61250311,error importing bert: module &#39;tensorflow._api.v2.train&#39; has no attribute &#39;optimizer&#39;,"i tried to use bert-tensorflow in google colab, but i got the following error:

--------------------------------------------------------------------------- attributeerror                            traceback (most recent call
  last)  in ()
        1 import  bert
  ----> 2 from bert import run_classifier_with_tfhub # run_classifier
        3 from bert import optimization
        4 from bert import tokenization
1 frames /usr/local/lib/python3.6/dist-packages/bert/optimization.py
  in ()
       85 
       86 
  ---> 87 class adamweightdecayoptimizer(tf.train.optimizer):
       88   """"""a basic adam optimizer that includes ""correct"" l2 weight decay.""""""
       89 
attributeerror: module 'tensorflow._api.v2.train' has no attribute
  'optimizer'

here is the code i tried:

install the libraries:

!pip install --upgrade --force-reinstall tensorflow
!pip install --upgrade --force-reinstall tensorflow-gpu
!pip install tensorflow_hub
!pip install sentencepiece
!pip install bert-tensorflow

run this code:

from sklearn.model_selection import train_test_split
import pandas as pd
from datetime import datetime
from tensorflow.keras import optimizers
import  bert
from bert import run_classifier
from bert import optimization
from bert import tokenization

i've also tried 
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
but got the same error.","['python', 'tensorflow', 'classification', 'bert-language-model']",61252082,"i did some experimentation in my own colab notebook (please provide a link next time) and i found that in the error message, there was 
class adamweightdecayoptimizer(tf.train.optimizer):

this being the header of the class. but there is nothing like tf.train.optimizer instead it should be :
class adamweightdecayoptimizer(tf.compat.v1.train.optimizer):

the link where there is exact issue with (lol) exact same line is here",https://stackoverflow.com/questions/61250311,python,16-04-2020 12:30,16530.0,10.0,4.0,True,14-10-2021 19:04,16-04-2020 12:36
78461078,how to optimize this function and improve running time?,"i have function aimed at creating a data-frame with three columns; bigram-phrase, count (of the bigram-phrase), and pmi score (for the bigram-phrase). since i want to run this on a large dataset with over a million phrases, the compute time is incredibly long. i recognize that the nested for-loops and matching conditions are contributing to the computation difficulties. is there an alternative way to do the same thing and cut down run-time?
here's my code:
def pmi_count_phrase_create(pmi_tups,freq_list):

    import pandas as pd

    """"""pmi_tups is result of running pmi_tups = [i for i in finder.score_ngrams(bigram_measures.pmi)]  
       freq_list is a result of running freq_list= finder.ngram_fd.items() 
       
       -> df made up of columns for  pmi list, count list, phrase list""""""
    pmi3_list =[]
    count3_list =[]
    phrase3_list =[]
    for phrase, pmi in pmi_tups: #pmi_tups is list of tuples of form:[((phrase),pmi),..]
        for item in freq_list:  
            quadgram,count = item
            if quadgram == phrase:
                pmi3_list.append(pmi)
                count3_list.append(count)
                phrase3_list.append(phrase)

                # create dataframe
    df = pd.dataframe({'phrase':phrase3_list,'pmi':pmi3_list,'count':count3_list})
    return df 

running this code on my pmi_tups and freq_list, it is still running and it's been over 1000 minutes. i'm open to also using a different library to evaluate the bi-gram phrases, pmi's and frequencies.","['python', 'performance', 'optimization', 'nlp', 'nltk']",78461789,"ended up changing my function to convert freq_list to a dictionary and list comprehensions instead of for loops and this code instantly returned a data-frame:
def quicker_func(pmi_tups, freq_list):
    import pandas as pd
    freq_dict = dict(freq_list)  # create a dictionary for faster lookups 

    pmi_list = [pmi for phrase, pmi in pmi_tups if phrase in freq_dict]
    count_list = [freq_dict[phrase] for phrase, pmi in pmi_tups if phrase in freq_dict]
    phrase_list = [phrase for phrase, pmi in pmi_tups if phrase in freq_dict]

    df = pd.dataframe({'phrase': phrase_list, 'pmi': pmi_list, 'count': count_list})
    return df",https://stackoverflow.com/questions/78461078,python,10-05-2024 15:18,47.0,0.0,1.0,True,18-08-2024 20:13,18-08-2024 20:13
69712715,pandas apply two argument with spacy,"df.head()
        lang    text
    0  eng      johnnet went out on the field and felt under her feet..
    1  eng      john was shocked by this statement..
    2  de        heute hat marie kï¿½ï¿½stlich gegessen und..

i have a dataframe with different languages that why i have a dictionary of two languages spacy :
eng_nlp= spacy.load('en_core_web_lg')
de_nlp= spacy.load('de_core_news_lg')

spacy_lang = {
    'de': de_nlp,
    'eng': eng_nlp
}

i wrote a function that looks displays only people in the column depending on the language.
def label_lang(lang,text):
    model = spacy_lang[lang]
    doc = model(text)
    for ent in  doc.ents:
         if  (ent.label_ == 'person'):
                 return ent.text

now i want to apply this to the column df['text'], but i get an error
df.apply( lambda x: label_lang(spacy_lang[x],x['text']),axis = 1)

typeerror: unhashable type: 'series'
i dounderstand what i should use as an argument function (spacy_lang)","['python', 'label', 'apply', 'spacy']",69713059,"x is a series, not a string, thus you can't use spacy_lang[x] as the dictionary key type expected here is a string. in this case, you need to use spacy_lang[x['lang']] instead of spacy_lang[x] in the lambda.
more, if you get the model in the lambda, you need not re-try getting the model inside the label_lang function. look, you have label_lang(spacy_lang[x].... and model = spacy_lang[lang], where the latter already holds the spacy model in the lang variable.
you can use
import spacy
import pandas as pd

eng_nlp= spacy.load('en_core_web_lg')
de_nlp= spacy.load('de_core_news_lg')
spacy_lang = {'de': de_nlp,'eng': eng_nlp}

def label_lang(model,text):
    doc = model(text)
    for ent in  doc.ents:
         if  (ent.label_ == 'person'):
                 return ent.text

df = pd.dataframe({'lang': ['eng','eng', 'de'], 'text':[
    'johnnet went out on the field and felt under her feet..', 
    'john was shocked by this statement..',
    'heute hat marie kï¿½ï¿½stlich gegessen und']})

print(df.apply( lambda x: label_lang(spacy_lang[x['lang']],x['text']),axis = 1))

output:
0    none
1    john
2    none
</p",https://stackoverflow.com/questions/69712715,python,25-10-2021 17:57,173.0,2.0,1.0,True,07-11-2021 14:27,07-11-2021 14:27
76708691,how to do a web scraping getting all the data,"i was doing a web scrapping to get reviews from the play store in order to study nlp, but my script only brought the first reviews.
i noticed that when we clicked on ""show all reviews"" the classes of the elements of the comments on the html page remain the same and the url too, so theoretically all the data should come.
can anyone help me?
import requests
from bs4 import beautifulsoup

def scrape_playstore_simpler_reviews(url):
    reviews_list = []
    response = requests.get(url)

    if response.status_code == 200:
        soup = beautifulsoup(response.content, 'html.parser')
        app_title1 = soup.find_all('h1', class_='fd93bb')

        for app_title in app_title1:
            app_title1 = app_title.find('span')


        reviews = soup.find_all('div', class_='egfghd')

        for review in reviews:
            user_name = review.find('div', class_='x5ppbb')
            comment_text = review.find('div', class_='h3yv2d')
            helpful_count = review.find('div', class_='ajtpzc')
            post_date = review.find('span', class_='bp9aid')
            star_rating = review.find('div', class_='ixrfpc').attrs['aria-label']
            
            review_data = {
                'tï¿½ï¿½tulo do aplicativo': app_title,
                'nome do usuï¿½ï¿½rio': user_name,
                'texto do comentï¿½ï¿½rio': comment_text,
    'qtd apoio ao comentï¿½ï¿½rio': helpful_count,
                'data da postagem': post_date,
                'qtd estrelas': star_rating
            }

            reviews_list.append(review_data)
            print(review_data)
    else:
        print('falha ao fazer requisiï¿½ï¿½ï¿½ï¿½o 

url = '
reviews = scrape_playstore_simpler_reviews(url)

scrape_playstore_simpler_reviews(url)

i expected to see all the app rating comments i loo","['python', 'web-scraping', 'beautifulsoup', 'nlp']",76708755,"it seems that the issue with your web scraping script lies in how you're trying to extract the reviews from the google play store page. the reason you're only getting the first reviews is that the page uses dynamic loading, and the reviews are loaded as you scroll down.
to scrape all the reviews, you'll need to simulate scrolling or use the play store api to get all the reviews. directly scraping the page won't work since the initial response doesn't contain all the reviews.
here's a general outline of how you can scrape all reviews using the play store api:

use the play store api to fetch the reviews data.
parse the json response to extract the necessary information.",https://stackoverflow.com/questions/76708691,python,17-07-2023 23:33,243.0,1.0,3.0,True,17-07-2023 23:55,17-07-2023 23:36
21844546,forming bigrams of words in list of sentences with python,"i have a list of sentences:
text = ['cant railway station','citadel hotel',' police stn']. 

i need to form bigram pairs and store them in a variable. the problem is that when i do that, i get a pair of sentences instead of words. here is what i did: 
text2 = [[word for word in line.split()] for line in text]
bigrams = nltk.bigrams(text2)
print(bigrams)

which yields 
[(['cant', 'railway', 'station'], ['citadel', 'hotel']), (['citadel', 'hotel'], ['police', 'stn'])

can't railway station and citadel hotel form one bigram. what i want is 
[([cant],[railway]),([railway],[station]),([citadel,hotel]), and so on...

the last word of the first sentence should not merge with the first word of second sentence.
what should i do to make it work?","['python', 'list', 'list-comprehension', 'nltk', 'collocation']",21844800,"using list comprehensions and zip:
>>> text = [""this is a sentence"", ""so is this one""]
>>> bigrams = [b for l in text for b in zip(l.split("" "")[:-1], l.split("" "")[1:])]
>>> print(bigrams)
[('this', 'is'), ('is', 'a'), ('a', 'sentence'), ('so', 'is'), ('is', 'this'), ('this',     
'one')]",https://stackoverflow.com/questions/21844546,python,18-02-2014 04:41,119872.0,33.0,10.0,True,02-05-2022 09:07,29-04-2016 21:23
75322813,openai gpt-3 api error: &quot;that model does not exist&quot;,"get ""that model does not exist"" from api call in node.js
const chatgpturl = ""

...

const response = await axios.post(
      chatgpturl,
      {
        prompt,
        max_tokens: 100,
        n: 1,
        stop: """",
      },
      {
        headers: {
          ""content-type"": ""application/json"",
          ""authorization"": `bearer ${chatgptapikey}`,
        },
      }
    );

    const responsetext = response.data.choices[0].text;","['node.js', 'openai-api', 'gpt-3']",75322907,"problem
you didn't set the model parameter. you have to set the model parameter to any openai model that is compatible with the completions api endpoint. it's a required parameter. see the official openai documentation.

solution
set the model parameter. you can choose between the following models:

gpt-3.5-turbo-instruct
babbage-002
davinci-002
text-davinci-003
text-curie-001
text-babbage-001
text-ada-001



also, all engines endpoints are deprecated.

use the completions api endpoint.
change the url from this...


...to this.",https://stackoverflow.com/questions/75322813,node.js,02-02-2023 11:56,4999.0,1.0,2.0,True,13-10-2023 13:46,13-03-2023 14:09
73225121,print strings of one dataframe contained in another dataframe,"i have two dataframes:
one dataframe consists of two columns ('good' and bad')
and another one that contains text data.
df_dictionary = pd.dataframe({'good': ['love', 'like'],
                    'bad': ['dislike', 'hate']})
df_text = pd.dataframe({'col1': ['i love cats', 'i hate dogs']})

now i would like to retrieve exact string matches of words that are in the dictionary and are contained in col1 of df_text and assign the string match to the second column of df_text.
i tried .isin(), however this code only shows exact string matches if the whole phrase matches and not if the word is contained in the sentence.
df_text should then look as follows:




col1
string_match_good
string_match_bad




i love cats
love



i hate dogs

hate




i do not want partial string matches, e.g. if col1 says 'i loved cats', then i do not want a string match.
i found the following:
matches = df_text[df_text['col1'].str.contains(fr""\b(?:{'|'.join(df_dictionary)})\b"")] , however this one does not print the matched words (i.e. good or bad) in the string_match columns.
does anyone have a solution to it?","['python', 'pandas', 'nlp', 'string-matching']",73229043,"i think the data structure is not ideal, specifically because your text values are conceptually several values in one (i.e., lists of tokens/words) but pandas works best with one value per cell. here's how i'd approach it:

explode the strings such that you get one word per cell.

df_text = (
           df_text.col1.str.split() # split into single words
           .explode() # explode them to one word per cell
           .rename_axis(""sent_index"") # rename the index for later
           .reset_index() # set the sent_index as its own column
           )

intermediary result:
   sent_index  col1
0           0     i
1           0  love
2           0  cats
3           1     i
4           1  hate
5           1  dogs


now you can merge col1 with df_dictionary, once for each of the two labels good and bad:

for label in [""good"", ""bad""]:
    df_text = df_text.merge(df_dictionary[label], 
                            left_on=""col1"", 
                            right_on=label, 
                            how=""left"")

now df_text looks like this:
   sent_index  col1  good   bad
0           0     i   nan   nan
1           0  love  love   nan
2           0  cats   nan   nan
3           1     i   nan   nan
4           1  hate   nan  hate
5           1  dogs   nan   nan

afaict, this should already contain all the information you need.

re-combine the words into sentences, using the sent_index we set earlier.

df_final = (df_text.groupby(""sent_index"")
            .agg(list)
            .applymap(lambda s: ' '.join(w for w in s if not pd.isna(w)))
           )

the final result then is:
                   col1  good   bad
sent_index                         
0           i love cats  love      
1           i hate dogs        hate

note that in case of multiple matches, you'd get the labels as joined strings, too. e.g., i dislike dogs but don't hate them would occur as 'dislike hate' in the bad column. whether or not that's alright depends on your next steps. note that this is no problem for the data structure in step 2.",https://stackoverflow.com/questions/73225121,python,03-08-2022 16:59,567.0,2.0,1.0,True,04-08-2022 00:13,04-08-2022 00:13
60121107,pytorch nllloss function target shape mismatch,"i'm training a lstm model using pytorch with batch size of 256 and nllloss() as loss function.
the loss function is having problem with the data shape.
the softmax output from the forward passing has shape of torch.size([256, 4, 1181]) where 256 is batch size, 4 is sequence length, and 1181 is vocab size.
the target is in the shape of torch.size([256, 4]) where 256 is batch size and 4 is the output sequence length.
when i was testing earlier with batch size of 1, the model works fine but when i add batch size, it is breaking. i read that nllloss() can take class target as input instead of one hot encoded target.
am i misunderstanding it? or did i not format the shape of the target correctly?
class lstm(nn.module):

    def __init__(self, embed_size=100, hidden_size=100, vocab_size=1181, embedding_matrix=...):
        super(lstm, self).__init__()
        self.hidden_size = hidden_size
        self.word_embeddings = nn.embedding(vocab_size, embed_size)
        self.word_embeddings.load_state_dict({'weight': torch.tensor(embedding_matrix)})
        self.word_embeddings.weight.requires_grad = false
        self.lstm = nn.lstm(embed_size, hidden_size)
        self.hidden2out = nn.linear(hidden_size, vocab_size)


    def forward(self, tokens):
        batch_size, num_steps = tokens.shape
        embeds = self.word_embeddings(tokens)
        lstm_out, _ = self.lstm(embeds.view(batch_size, num_steps, -1))
        out_space = self.hidden2out(lstm_out.view(batch_size, num_steps, -1))
        out_scores = f.log_softmax(out_space, dim=1)
        return out_scores

model = lstm(self.config.embed_size, self.config.hidden_size, self.config.vocab_size, self.embedding_matrix)
loss_function = nn.nllloss()
optimizer = optim.adam(model.parameters(), lr=self.config.lr)

error:
~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)
   1846         if target.size()[1:] != input.size()[2:]:
   1847             raise valueerror('expected target size {}, got {}'.format(
-> 1848                 out_size, target.size()))
   1849         input = input.contiguous().view(n, c, 1, -1)
   1850         target = target.contiguous().view(n, 1, -1)

valueerror: expected target size (256, 554), got torch.size([256, 4])","['deep-learning', 'nlp', 'pytorch', 'lstm', 'loss-function']",60162037,"your input shape to the loss function is (n, d, c) = (256, 4, 1181) and your target shape is (n, d) = (256, 4), however, according to the docs on nllloss the input should be (n, c, d) for a target of (n, d).
supposing x is your network output and y is the target then you can compute loss by transposing the incorrect dimensions of x as follows:
loss = loss_function(x.transpose(1, 2), y)

alternatively, since nllloss is just averaging all the responses anyway, you can reshape x and y to be (n*d, c) and (n*d). this gives the same result without creating temporary copies of your tensors.
loss = loss_function(x.reshape(n*d, c), y.reshape(n*d))",https://stackoverflow.com/questions/60121107,deep-learning,07-02-2020 21:09,3709.0,5.0,1.0,True,11-06-2022 12:15,07-02-2020 21:31
74889192,why is zappa deploy missing all / most packages present in virtual environment?,"trying to deploy a python script to aws via zappa. script works in local virtual environment (using virtualenv) but zappa deploy fails with multiple missing packages. must be missing something pretty fundamental despite extensive troubleshooting. details below, any ideas appreciated.
error: zappa deploy dev errors. zappa tail shows [error] modulenotfounderror: no module named 'pandas'. it errors on whatever is the first import statement in main.py` suggesting issue isn't specific to pandas pkg.
screenshot of settings json & project directory structure included below.
tried the following with no luck:

added requirements.txt project directory with all localenv packages shown by pip freeze
added an ""include"" setting in zappa_settings.json
confirmed pandas and other dependency packages appear in zip file produced by zappa package

edit resolved: project directory size was too big. aws lambda (and consequently zappa) limit project directory including dependencies to 512mb. exploring alternative cloud infra.","['python', 'python-3.x', 'python-3.8', 'openai-api', 'zappa']",74896388,"resolved: root cause was project directory size too big. aws lambda (and zappa) limit project directory including dependencies to 512mb by default, and zappa's slim_handler doesn't get around it automatically. so it fails to see the libraries because there isn't enough space for them on the lambda at runtime.
this can be fixed in aws lambda console, under functions --> [select your function] --> configuration --> general configuration --> ephemeral storage (edit up to directory size).
saw very little info about this error, so hope this helps someone :)",https://stackoverflow.com/questions/74889192,python,22-12-2022 13:27,669.0,0.0,1.0,True,25-12-2022 08:25,23-12-2022 05:21
13965823,resource &#39;corpora/wordnet&#39; not found on heroku,"i'm trying to get nltk and wordnet working on heroku. i've already done
heroku run python
nltk.download()
  wordnet
pip install -r requirements.txt

but i get this error:
resource 'corpora/wordnet' not found.  please use the nltk
  downloader to obtain the resource:  >>> nltk.download()
  searched in:
    - '/app/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'

yet, i've looked at in /app/nltk_data and it's there, so i'm not sure what's going on.","['python', 'django', 'heroku', 'nltk', 'wordnet']",14869451,"i just had this same problem. what ended up working for me is creating an 'nltk_data' directory in the application's folder itself, downloading the corpus to that directory and adding a line to my code that lets the nltk know to look in that directory. you can do this all locally and then push the changes to heroku.
so, supposing my python application is in a directory called ""myapp/""
step 1: create the directory
cd myapp/
mkdir nltk_data

step 2: download corpus to new directory
python -m nltk.downloader

this'll pop up the nltk downloader. set your download directory to whatever_the_absolute_path_to_myapp_is/nltk_data/. if you're using the gui downloader, the download directory is set through a text field on the bottom of the ui. if you're using the command line one, you set it in the config menu. 
once the downloader knows to point to your newly created nltk_data directory, download your corpus.
or in one step from python code:
nltk.download(""wordnet"", ""whatever_the_absolute_path_to_myapp_is/nltk_data/"")

step 3: let nltk know where to look
ntlk looks for data,resources,etc. in the locations specified in the nltk.data.path variable. all you need to do is add nltk.data.path.append('./nltk_data/') to the python file actually using nltk, and it will look for corpora, tokenizers, and such in there in addition to the default paths.
step 4: send it to heroku
git add nltk_data/
git commit -m 'super useful commit message'
git push heroku master

that should work! it did for me anyway. one thing worth noting is that the path from the python file executing nltk stuff to the nltk_data directory may be different depending on how you've structured your application, so just account for that when you do nltk.data.path.append('path_to_nltk_data')",https://stackoverflow.com/questions/13965823,python,20-12-2012 05:21,65024.0,26.0,15.0,True,28-12-2024 07:03,06-04-2015 19:01
71434804,how to fed last 4 concatenated hidden layers of bert to fc layers,"i am trying to do classification task and i got last 4 layers from bert and concatenate them.
out = model(...)
out=torch.cat([out['hidden_states'][-i] for i in range(1,5)],dim=-1)

now the shape is (12,200,768*4) which is batch,max_length,concatenation layer but for fully connected layer we need to have two dimension. so one way is to average like torch.mean((12,200,768*4),dim=1) and get the output as (12,768*4).
but i am confused what is the original bert approach","['python', 'pytorch', 'huggingface-transformers', 'bert-language-model']",71713963,"there is no ""original"" bert approach for classification with concatenated hidden layers. you have several options to proceed and i will just describe a comment on your approach and suggest an alternative in the following.
preliminary:
import torch.nn as nn
from transformers import berttokenizerfast, bertmodel

t = berttokenizerfast.from_pretrained(""bert-base-cased"")
m = bertmodel.from_pretrained(""bert-base-cased"")
fc = nn.linear(768, 5)

s = [""this is a random sentence"", ""this is another random sentence with more words""]

i = t(s, padding=true,return_tensors=""pt"")

with torch.no_grad():
  o = m(**i, output_hidden_states=true)

print(i)

at first, you should look at your input:
#print(i)
{'input_ids': 
tensor([[ 101, 1188, 1110,  170, 7091, 5650,  102,    0,    0,    0],
        [ 101, 1188, 1110, 1330, 7091, 5650, 1114, 1167, 1734,  102]]), 
'token_type_ids': 
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 
'attention_mask': 
tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])
}

what you should notice here, is that the shorter sentence gets padded. that is relevant because simply pooling the mean with torch.mean, will result in different sentence embeddings for the same sentence depending on the number of padding tokens. of course, the model will learn to handle that to some extent after sufficient training, but you should, however, use a more sophisticated mean function that removes the padding tokens right away :
def mean_pooling(model_output, attention_mask):
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(model_output.size()).float()
    return torch.sum(model_output * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


o_mean = [mean_pooling(o.hidden_states[-x],i.attention_mask) for x in range(1,5)]
#we want a tensor and not a list
o_mean = torch.stack(o_mean, dim=1)
#we want only one tensor per sequence
o_mean = torch.mean(o_mean,dim=1)

print(o_mean.shape)
with torch.no_grad():
  print(fc(o_mean))

output:
torch.size([2, 768])
tensor([[ 0.0677, -0.0261, -0.3602,  0.4221,  0.2251],
        [-0.0328, -0.0161, -0.5209,  0.5825,  0.2405]])

these operations are pretty expensive and people often use an approach called cls pooling as a cheaper alternative with comparable performance:
#we only use the cls token (i.e. first token of the sequence)
#id 101
o_cls = [o.hidden_states[-x][:, 0] for x in range(1,5)]
#we want a tensor and not a list
o_cls = torch.stack(o_cls, dim=1)
#we want only one tensor per sequence
o_cls = torch.mean(o_cls,dim=1)
print(o_cls.shape)
with torch.no_grad():
  print(fc(o_cls))

output:
torch.size([2, 768])
tensor([[-0.3731,  0.0473, -0.4472,  0.3804,  0.4057],
        [-0.3468,  0.0685, -0.5885,  0.4994,  0.4182]])",https://stackoverflow.com/questions/71434804,python,11-03-2022 07:08,1454.0,1.0,1.0,True,02-04-2022 00:26,11-03-2022 07:19
76352628,flutter textfield onsubmitted not working,"i am trying to make it so that when the button is pressed it prints a list fetched from an api, and the code showed no errors but when i tried pressing the button nothing happened. i tried commenting out the section and trying just a debugprint to see if it was the api calling function that was causing the problem, but when i press the button it still doesn't print anything. you can see the code on line 85 of the file.
import 'package:flutter/material.dart';
import 'package:flutter_spinkit/flutter_spinkit.dart';
import 'package:tsa_softwaredev/constants/const.dart';
import 'package:tsa_softwaredev/services/asset_manager.dart';
import 'package:tsa_softwaredev/services/services.dart';
import 'package:tsa_softwaredev/widgets/chat_widget.dart';

class chatscreen extends statefulwidget {
  const chatscreen({key? key}) : super(key: key);

  @override
  state<chatscreen> createstate() => _chatscreenstate();
}

class _chatscreenstate extends state<chatscreen> {
  late texteditingcontroller texteditingcontroller;

  @override
  void initstate() {
    texteditingcontroller = texteditingcontroller();
    super.initstate();
  }

  @override
  void dispose() {
    texteditingcontroller.dispose();
    super.dispose();
  }

  @override
  widget build(buildcontext context) {
    return scaffold(
      appbar: appbar(
        elevation: 2,
        leading: padding(
          padding: const edgeinsets.all(8),
          child: image.asset(assetmanager.openailogoimage),
        ),
        title: const text(""chatgpt""),
        actions: [
          iconbutton(
            onpressed: () async {
              await services.showmodalsheet(context: context);
            },
            icon: const icon(
              icons.more_vert_rounded,
              color: colors.white,
            ),
          ),
        ],
      ),
      body: safearea(
        child: column(
          children: [
            flexible(
              child: listview.builder(
                itemcount: 6,
                itembuilder: (context, index) {
                  return chatwidget(
                    msg: chatmessages[index][""msg""].tostring(),
                    chatindex: int.parse(chatmessages[index][""chatindex""].tostring()),
                  );
                },
              ),
            ),
            const sizedbox(height: 15),
            material(
              color: cardcolor,
              child: padding(
                padding: const edgeinsets.all(8.0),
                child: row(
                  children: [
                    expanded(
                      child: textfield(
                        style: const textstyle(color: colors.white),
                        controller: texteditingcontroller,
                        onsubmitted: (value) {
                          debugprint(""sent"");
                        },
                        decoration: const inputdecoration.collapsed(
                          hinttext: ""how can i help you?"",
                          hintstyle: textstyle(color: colors.grey),
                        ),
                      ),
                    ),
                    iconbutton(
                      onpressed: () {},
                      icon: const icon(
                        icons.send_rounded,
                        color: colors.grey,
                      ),
                    ),
                  ],
                ),
              ),
            ),
          ],
        ),
      ),
    );
  }
}","['flutter', 'dart', 'openai-api']",76353262,"there is nothing being triggered onpressed method
iconbutton(
onpressed: () {},
icon: const icon(
icons.send_rounded,
color: colors.grey,
),
),
when i add print, it prints


iconbutton(
                      onpressed: () {
                        debugprint('hey, im printing debugprint');
                      },
                      icon: const icon(
                        icons.send_rounded,
                        color: colors.grey,
                      ),
                    ),",https://stackoverflow.com/questions/76352628,flutter,28-05-2023 17:06,150.0,0.0,1.0,True,28-05-2023 19:27,28-05-2023 17:52
77005341,how to concatenate a split word using nlp caused by tokenizers after machine translation?,"russian translation produces the following result, is there a nlp function which we can use to concatenate as ""europe's"" in the following string?
""nitzchia protector todibo can go to one of europe ' s top clubs""","['nlp', 'tokenize', 'machine-translation']",77006402,"try detokenizers but because there are rules to process tokens that are expected to change x 's -> x's but not x ' s -> x's, you might have to iteratively apply the detokenizer, e.g. using sacremoses
>>> from sacremoses import mosestokenizer, mosesdetokenizer
>>> md = mosesdetokenizer(lang='en')

>>> md.detokenize(""nitzchia protector todibo can go to one of europe ' s top clubs"".split())
""nitzchia protector todibo can go to one of europe 's top clubs""

>>> md.detokenize(md.detokenize(""nitzchia protector todibo can go to one of europe ' s top clubs"".split()).split())
""nitzchia protector todibo can go to one of europe's top clubs""",https://stackoverflow.com/questions/77005341,nlp,30-08-2023 06:10,257.0,0.0,1.0,True,30-08-2023 08:58,30-08-2023 08:58
67687100,quanteda group documents by multiple variables,"i would like to be able to group documents in my dfm by two variables - speaker and week_start. i was previously able to do this using
dfm(corpus, groups=c(""speaker"",""week_start""). this worked fine and grouped documents by speaker-week.
however, with the recent updates to the quanteda package i seem to be running into a few problems. so i now create the dfm first then i try to group. below is the code
dfm <- dfm(corpus)
dfm <- dfm_group(dfm, groups = c(speaker, week_start))

however, when i do this i get the error:

error: groups must have length ndoc(x)

i have also tried to put the docvars in quotations but this generates the same error.","['r', 'nlp', 'quanteda']",67690670,"we changed the usage of the groups argument in v3 to make it more standard.
from news(version >= ""3.0"", package = ""quanteda""):

we have added non-standard evaluation for by and groups arguments
to access object docvars:

the *_sample() functions' argument by, and groups in the *_group() functions, now take unquoted document variable (docvar)
names directly, similar to the way the subset argument works in the
*_subset() functions.
quoted docvar names no longer work, as these will be evaluated literally.
the by = ""document"" formerly sampled from docid(x), but this functionality is now removed.  instead, use by = docid(x) to
replicate this functionality.
for groups, the default is now docid(x), which is now documented more completely.  see ?groups and ?docid.


so, to get the previous behaviour, you would want to use:
groups = interaction(speaker, week_start)

here's an example:
library(""quanteda"")
## package version: 3.0
## unicode version: 13.0
## icu version: 69.1
## parallel computing: 12 of 12 threads used.
## see  for tutorials and examples.

corp <- corpus(c(
  ""a b c"",
  ""a c d"",
  ""c d d"",
  ""d d e""
),
docvars = data.frame(
  var1 = c(""a"", ""a"", ""b"", ""b""),
  var2 = c(1, 2, 1, 1)
)
)
corp %>%
  tokens() %>%
  dfm() %>%
  dfm_group(groups = interaction(var1, var2))
## document-feature matrix of: 3 documents, 5 features (40.00% sparse) and 2 docvars.
##      features
## docs  a b c d e
##   a.1 1 1 1 0 0
##   b.1 0 0 1 4 1
##   a.2 1 0 1 1 0",https://stackoverflow.com/questions/67687100,r,25-05-2021 11:34,880.0,0.0,1.0,True,25-05-2021 15:02,25-05-2021 11:59
78513041,openai completions api: how do i extract the text from the response?,"i am attempting to extract text from openai, but i need some help with the correct syntax.
my code:
_model = ""gpt-3.5-turbo-instruct""
# summarize the text using an ai model (e.g., openai's gpt)
try:
    summary = openai.completions.create(
        model=_model,
        prompt=f""summarize this text: {text}"",
        max_tokens=150
    )
except exception as e:
    return f""summary: exception {e}""

_summary = summary.choices[0].message.content
#_summary = summary['choices'][0]['text']

i want to get a summary of the text. is this the best way?
however, i find when i debug the code that the text object is located in the following structure within summary['choices'][0]:
completionchoice(finish_reason='stop', index=0, logprobs=none, text='blah blah blah')

how do i extract the text from this in python code?","['python', 'openai-api', 'gpt-3']",78513064,"by using the following method, it means that you're using the openai python sdk >=v1.0.0:
openai.completions.create

the response using the openai python sdk >=v1.0.0 looks like this:
{
  ""id"": ""cmpl-uqkvlqyyk7bgyrrhq0exlwi7"",
  ""object"": ""text_completion"",
  ""created"": 1589478378,
  ""model"": ""gpt-3.5-turbo-instruct"",
  ""system_fingerprint"": ""fp_44709d6fcb"",
  ""choices"": [
    {
      ""text"": ""\n\nthis is indeed a test"",
      ""index"": 0,
      ""logprobs"": null,
      ""finish_reason"": ""length""
    }
  ],
  ""usage"": {
    ""prompt_tokens"": 5,
    ""completion_tokens"": 7,
    ""total_tokens"": 12
  }
}

which means that the following is the correct way to extract the completion using the openai python sdk >=v1.0.0:
summary.choices[0].text",https://stackoverflow.com/questions/78513041,python,21-05-2024 15:56,2560.0,0.0,1.0,True,12-06-2024 12:33,12-06-2024 12:33
77414028,how to do the fusion of two parallel branch in an encoder design?,"it seems i am not designing my encoder correctly, that is why i need the expert opinion on this since i am beginner to transformers and dl model design.
i have two different types of transformers networks in an encoder as follows:

the embedding dimension of each branch is 256 and they are fused by a linear layer
 self.fusion_head = nn.linear(2*self.num_features, self.num_features) #self_num_features = 256

i have a forward feature function in my encoder
def transformer_forward(self,x):
    """"""

    :param x: the embeddings + pos_embed
    :return:
    """"""

    x_t1 = self.transformer_type1(x)  # torch.size([1, 1280, 256])

    x_t2 = self.transformer_type2.forward(x)  # torch.size([1, 1280, 256])

    # x = x_t1 + x_t2
    x = torch.cat([x_t1,x_t2],dim=2)

    x = self.fusion_head(x)

    return x

however, after training the models and loading the checkpoints, i realized that the self.fusion_head is place after transformer_type1 modules
.
... 3.0.fn.to_qkv.weight', 'module.encoder.transformer_type1.3.layers.3.0.fn.to_out.0.weight', 'module.encoder.transformer_type1.3.layers.3.0.fn.to_out.0.bias', 'module.encoder.transformer_type1.3.layers.3.1.norm.weight', 'module.encoder.transformer_type1.3.layers.3.1.norm.bias', 'module.encoder.transformer_type1.3.layers.3.1.fn.net.0.weight', 'module.encoder.transformer_type1.3.layers.3.1.fn.net.0.bias', 'module.encoder.transformer_type1.3.layers.3.1.fn.net.3.weight', 'module.encoder.transformer_type1.3.layers.3.1.fn.net.3.bias', 'module.encoder.mlp_head.0.weight', 'module.encoder.mlp_head.0.bias', 'module.encoder.mlp_head.1.weight', 'module.encoder.mlp_head.1.bias', 'module.encoder.fusion_head.weight', 'module.encoder.fusion_head.bias', 'module.encoder.transformer_type2.pos_embed', 'module.encoder.transformer_type2.patch_embed.proj.weight', 'module.encoder.transformer_type2.patch_embed.proj.bias', 'module.encoder.transformer_type2.patch_embed.norm.weight', 'module.encoder.transformer_type2.patch_embed.norm.bias', 'module.encoder.transformer_type2.blocks.0.norm1.weight', 'module.encoder.transformer_type2.blocks.0.norm1.bias', 'module.encoder.transformer_type2.blocks.0.filter.complex_weight', 'module.encoder.transformer_type2.blocks.0.norm2.weight', 'module.encoder.transformer_type2.blocks.0.norm2.bias', 'module.encoder.transformer_type2.blocks.0.mlp.fc1.weight',  ...
is the placing of this concatenation layer (i.e., fusion_head correct in the forward function? why it is placed after transformet_type1? should not fusion_head layer be after both transformet_type1 and transformer_type2 in terms of order?","['deep-learning', 'pytorch', 'neural-network', 'huggingface-transformers', 'transformer-model']",77421988,"what you see there is the implementation of __repr__ of nn.module. it prints the modules that you registered inside the __init__ method of your network. that the forward method does not define the order, makes sense because you can call modules several times or not at all inside forward.
import torch
from torch import nn
class bla(nn.module):
    def __init__(self):
        super().__init__()
        self.b1 = nn.linear(256, 128)
        self.b2 = nn.gelu()
        self.b3 = nn.linear(128,5)
        self.b0 = nn.embedding(100,256)
    def forward(self, x):
        emb = self.b0(x)
        emb = self.b1(emb)
        emb = self.b2(emb)
        emb = self.b3(emb)
        return emb

net = bla()
print(net)

output:
bla(
  (b1): linear(in_features=256, out_features=128, bias=true)
  (b2): gelu(approximate='none')
  (b3): linear(in_features=128, out_features=5, bias=true)
  (b0): embedding(100, 256)
)",https://stackoverflow.com/questions/77414028,deep-learning,03-11-2023 03:26,242.0,1.0,1.0,True,04-11-2023 12:27,03-11-2023 11:00
62703391,estimate token probability/logits given a sentence without computing the entire sentence,"i have a sentence like:  ""i like sitting in my new chair and _____ about life"".
and i have a specific set of tokens like [""watch"", ""run"", ""think"", ""apple"", ""light""]
i would like to calculate the probability of each of those tokens to appear as the next word in that incomplete sentence. hopefully i should get that the probability of ""think"" is higher that ""apple"" for instance.
i am working with pytorch-transformers (gpt2lmheadmodel specifically), and a possible solution is to evaluate the score of the full sentence with each of the tokens, but when number of tokens to evaluate is on the order of 100 or 1000 then the computation time starts to be too long.
it must be possible to process the sentence only once and somehow use the hidden states to calculate the probabilities of the set of tokens, but i don't know how to do it.
any ideas? thanks in advance

edit:
the actual code looks like the one below (estimating the probability for the full sentence every time). for every sentence it takes about 0.1 seconds to run the score() method, which turns into hours if i want to evaluate some thousands of words.
from pytorch_transformers import gpt2tokenizer, gpt2lmheadmodel
import pandas as pd

model = gpt2lmheadmodel.from_pretrained(""gpt2"")
model.eval()
tokenizer = gpt2tokenizer.from_pretrained(""gpt2"")


def score(sentence):
    tokenize_input = tokenizer.tokenize(sentence)
    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])
    loss = model(tensor_input, labels=tensor_input)
    return -loss[0].item()


candidates = [""watch"", ""run"", ""think"", ""apple"", ""light""]
sent_template = ""i like sitting in my new chair and {} about life""
print({candidate: score(sent_template.format(candidate)) for candidate in candidates})","['python', 'nlp', 'huggingface-transformers']",62980607,"your example produced the following output and took around 48.5 seconds with 282 candidates to finish in my environment (i only conducted 3 runs):
{'watch': -5.406847953796387
, 'run': -5.533411502838135
, 'think': -4.525279521942139
, 'apple': -6.158637046813965
, 'light': -5.835141658782959}

as mentioned in the comments i think you can spare some calculations with the past parameter and the fast tokenizer as shown in the commented example below:
import torch

from  transformers import gpt2tokenizerfast, gpt2lmheadmodel
from torch.nn import crossentropyloss

model = gpt2lmheadmodel.from_pretrained(""gpt2"")
model.eval()
tokenizer = gpt2tokenizerfast.from_pretrained(""gpt2"")

###we calculate the hidden_states and the past of the common left part of the sentence
past = ""i like sitting in my new chair and""
past_tokenize_input = tokenizer.tokenize(past)
past_tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(past_tokenize_input)])

past_last_hidden_state, past = model.transformer(past_tensor_input)

def score(sentence, past, past_last_hidden_state, past_tensor_input):
    tokenize_input = tokenizer.tokenize(sentence, )
    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])

    ###the following code is slightly modified from 
    ###now we calculate the right part of the sentence with the already calculated past
    transformer_outputs = model.transformer(
            tensor_input,
            past=past,
            attention_mask=none,
            token_type_ids=none,
            position_ids=none,
            head_mask=none,
            inputs_embeds=none,
            use_cache=none,
            output_attentions=none,
            output_hidden_states=none,
        )
    ###and concatenate the output of with the hidden_state of the left part of the sentence
    hidden_states = torch.cat((past_last_hidden_state, transformer_outputs[0]), dim=1)
    
    ###the following part is exactly the same as 
    lm_logits = model.lm_head(hidden_states)

    labels_input = torch.cat((past_tensor_input, tensor_input), dim=1)

    # shift so that tokens < n predict n
    shift_logits = lm_logits[..., :-1, :].contiguous()
    shift_labels = labels_input[..., 1:].contiguous()
    # flatten the tokens
    loss_fct = crossentropyloss()
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    return -loss.item()

candidates = [""watch"", ""run"", ""think"", ""apple"", ""light""]

sent_template = "" {} about life""

print({candidate: score(sent_template.format(candidate), past, past_last_hidden_state, past_tensor_input) for candidate in candidates})

output:
{'watch': -5.406846046447754
, 'run': -5.533413887023926
, 'think': -4.525280952453613
, 'apple': -6.158637046813965
, 'light': -5.835141181945801}

the runtime here was 40.5 seconds with 282 candidates (3 cycles again). you also see that i lost some precision.
many thanks to patrickvonplaten who gave me a good explanation about the past implementation.",https://stackoverflow.com/questions/62703391,python,02-07-2020 19:05,3347.0,5.0,1.0,True,03-11-2023 21:23,04-07-2020 07:36
297112,how do i use python libraries in c++?,"i want to use the nltk libraries in c++. 
is there a glue language/mechanism i can use to do this? 
reason:
i havent done any serious programming in c++ for a while  and want to revise nlp concepts at the same time.
thanks","['c++', 'python', 'nltk']",297175,"you can also try the boost.python library; which has this capability. this library is mainly used to expose c++ to python, but can be used the other way around.",https://stackoverflow.com/questions/297112,c++,17-11-2008 22:03,21610.0,12.0,4.0,True,28-06-2021 15:58,17-11-2008 22:18
75864073,"use of unstructuredpdfloader unstructured package not found, please install it with `pip install unstructured","i just have a newly created environment in anaconda (conda 22.9.0 and python 3.10.10). then i proceed to install langchain (pip install langchain if i try conda install langchain it does not work). according to the quickstart guide i have to install one model provider so i install openai (pip install openai).
then i enter to the python console and try to load a pdf using the class unstructuredpdfloader and i get the following error. what the problem could be?
(langchain) c:\users\user>python
python 3.10.10 | packaged by anaconda, inc. | (main, mar 21 2023, 18:39:17) [msc v.1916 64 bit (amd64)] on win32
>>> from langchain.document_loaders import unstructuredpdfloader
>>> loader = unstructuredpdfloader(""c:\\<path-to-data>\\data\\name-of-file.pdf"")
traceback (most recent call last):
  file ""c:\<path-to-anaconda>\envs\langchain\lib\site-packages\langchain\document_loaders\unstructured.py"", line 32, in __init__
    import unstructured  # noqa:f401
modulenotfounderror: no module named 'unstructured'

during handling of the above exception, another exception occurred:

traceback (most recent call last):
  file ""<stdin>"", line 1, in <module>
  file ""c:\<path-to-anaconda>\envs\langchain\lib\site-packages\langchain\document_loaders\unstructured.py"", line 90, in __init__
    super().__init__(mode=mode, **unstructured_kwargs)
  file ""c:\<path-to-anaconda>\envs\langchain\lib\site-packages\langchain\document_loaders\unstructured.py"", line 34, in __init__
    raise valueerror(
valueerror: unstructured package not found, please install it with `pip install unstructured`","['python', 'conda', 'openai-api', 'langchain']",75864224,"run this
pip install unstructured
or this
pip install ""unstructured[local-inference]""",https://stackoverflow.com/questions/75864073,python,28-03-2023 08:44,34474.0,12.0,3.0,True,03-07-2023 17:48,28-03-2023 09:03
74690541,how do i get access to the &quot;last_hidden_state&quot; for code generation models in huggingface?,"i'm trying to obtain the ""last_hidden_state"" (as explained here) for code generation models over here. i am unable to figure out how to proceed, other than manually downloading each code-generation-model and checking if its key has that attribute using the following code -
import numpy as np
from datasets import load_dataset
from transformers import autotokenizer
from transformers import automodel, automodelforcausallm
import torch
from sklearn.linear_model import logisticregression

from transformers import autotokenizer, automodelwithlmhead

tokenizer = autotokenizer.from_pretrained(""codeparrot/codeparrot"")
device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
model = automodelwithlmhead.from_pretrained(""codeparrot/codeparrot"").to(device)

inputs = tokenizer(""def hello_world():"", return_tensors=""pt"")
inputs = {k:v.to(device) for k,v in inputs.items()}
with torch.no_grad():
    outputs = model(**inputs)
print(outputs.keys())

so far, i tried this strategy on codeparrot and incoder with no success. perhaps there is a better way to access the values of the hidden layers?","['pytorch', 'nlp', 'huggingface-transformers']",74692653,"the hidden_states of output from codegenforcausallm is already the last_hidden_state for the codegen model. see: link
where hidden_states = transformer_outputs[0] is the output of codegenmodel (link) and the transformer_outputs[0] is the last_hidden_state
        if not return_dict:
            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not none)

        return basemodeloutputwithpast(
            last_hidden_state=hidden_states,
            past_key_values=presents,
            hidden_states=all_hidden_states,
            attentions=all_self_attentions,
        )",https://stackoverflow.com/questions/74690541,pytorch,05-12-2022 15:28,4024.0,1.0,1.0,True,05-12-2022 18:17,05-12-2022 15:36
63377198,what is the difference between pooled output and sequence output in bert layer?,"everyone! i was reading about bert and wanted to do text classification with its word embeddings. i came across this line of code:
pooled_output, sequence_output = self.bert_layer([input_word_ids, input_mask, segment_ids])   

and then:
clf_output = sequence_output[:, 0, :]
out = dense(1, activation='sigmoid')(clf_output)

but i can't understand the use of pooled output. doesn't sequence output contain all the information including the word embedding of ['cls']? if so, why do we have pooled output?
thanks in advance!","['python-3.x', 'tensorflow', 'neural-network', 'text-classification', 'bert-language-model']",63380542,"if you have given a sequence, ""you are on stackoverflow"". the sequence_output will give 768 embeddings of these four words. but, the pooled output will just give you one embedding of 768, it will pool the embeddings of these four words.",https://stackoverflow.com/questions/63377198,python-3.x,12-08-2020 13:09,6237.0,4.0,3.0,True,26-05-2022 21:11,03-04-2022 07:16
25998742,nltk download url authorization issue,"i tried to update my nltk data with nltk.download() but i got http error 401: authorization required.
when i traced the url in question, i found it in downloader.py

default_url = '

i then copied that url and ran it in my browser to find out that it's asking me for a username and password.
does anyone know how to solve this issue?","['python', 'python-2.7', 'nltk']",25998985,"nltk have moved from their googlecode site.
as noted in this question, nltk's new data server is located at  just update the url to the new location, and it should hopefully work.",https://stackoverflow.com/questions/25998742,python,23-09-2014 15:16,4788.0,4.0,1.0,True,18-06-2022 10:55,18-06-2022 10:55
76307031,sugestions on the best way to work with nlp mixed some numerical and categorical features,"i'm working with a dataset of medicinal products across different countries, with each country having it's own data source. this results in the data not always being quite 'standardized' (for a lack of a better word), so one of the problems i'm trying to solve is to have the dosage in the same format across all countries. i've been doing it 'manually' for each country using regex, while having into account some criteria that i want to use as features in the model. for example: the number of active substances of the product, the pharmaceutical form and if some specific active substance is present in the product. by doing this 'manually' for like 1/3 of the countries, i've got a reasonable amount of records to train a model.
name   activesubstances   numberofactsubst   pharmaceuticalform   dosage        dosagefinal

x      ['y','z']          2                  tablet               '20mg/5mg'    '20 mg + 5 mg'

a      ['b']              1                  tablet               '(50 microg+10mg)/ml''50 ï¿½ï¿½g/ml + 10mg/ml'

i want this dosagefinal field to be filled automatically. what would be the best way to approach this task? i looked into parallel networks and the idea would be to use one nn to get the embeddings of the text variables, and another nn to collect the embeddings of the only numeric feature and later concatenate the embeddings. am i overcomplicating it?<","['python', 'regex', 'string', 'nlp', 'feature-engineering']",76314085,"you would use embeddings to understand the semantic meaning of the text.
for your situation, i would recommend looking at this as a translation task, or a simple text generation.
generation
use any decoder to generate the text in the right format.
use a few-show learning inside the prompt, and it will already understand the pattern.
do a quick test; go to any free ai-chat platform (e.g. hfchat, chatgpt, etc.), instruct it with a few examples, and you would get the right answers.
if you build the prompt correctly you will get sota answers.
some ideas to help the model would be: transform each country independently, or each medication.
also, if you give it a good enough prompt (few-shots) - it will do great.
translation
if you have enough data samples to train an lm - try to use bart, t5, etc.
and you might be able to create a model to generate these texts for you.
good luck.",https://stackoverflow.com/questions/76307031,python,22-05-2023 14:06,52.0,0.0,1.0,True,23-05-2023 11:11,22-05-2023 14:25
21355156,topic models: cross validation with loglikelihood or perplexity,"i'm clustering documents using topic modeling. i need to come up with the optimal topic numbers. so, i decided to do ten fold cross validation with topics 10, 20, ...60.
i have divided my corpus into ten batches and set aside one batch for a holdout set. i have ran latent dirichlet allocation (lda) using nine batches (total 180 documents) with topics 10 to 60. now, i have to calculate perplexity or log likelihood for the holdout set.
i found this code from one of cv's discussion sessions. i really don't understand several lines of code below. i have dtm matrix using the holdout set (20 documents). but i don't know how to calculate the perplexity or log likelihood of this holdout set.

questions:

can anybody explain to me what seq(2, 100, by =1) mean here? also, what associatedpress[21:30] mean? what function(k) is doing here?
 best.model <- lapply(seq(2, 100, by=1), function(k){ lda(associatedpress[21:30,], k) })


if i want to calculate perplexity or log likelihood of the holdout set called dtm, is there better code? i know there are perplexity() and loglik() functions but since i'm new i can not figure out how to implement it with my holdout matrix, called dtm.

how can i do ten fold cross validation with my corpus, containing 200 documents? is there existing code that i can invoke? i found caret for this purpose, but again cannot figure that out either.","['r', 'tm', 'cross-validation', 'topic-modeling']",41473782,"the accepted answer to this question is good as far as it goes, but it doesn't actually address how to estimate perplexity on a validation dataset and how to use cross-validation.
using perplexity for simple validation
perplexity is a measure of how well a probability model fits a new set of data.  in the topicmodels r package it is simple to fit with the perplexity function, which takes as arguments a previously fit topic model and a new set of data, and returns a single number.  the lower the better.
for example, splitting the associatedpress data into a training set (75% of the rows) and a validation set (25% of the rows):
# load up some r packages including a few we'll need later
library(topicmodels)
library(doparallel)
library(ggplot2)
library(scales)

data(""associatedpress"", package = ""topicmodels"")

burnin = 1000
iter = 1000
keep = 50

full_data  <- associatedpress
n <- nrow(full_data)
#-----------validation--------
k <- 5

splitter <- sample(1:n, round(n * 0.75))
train_set <- full_data[splitter, ]
valid_set <- full_data[-splitter, ]

fitted <- lda(train_set, k = k, method = ""gibbs"",
                          control = list(burnin = burnin, iter = iter, keep = keep) )
perplexity(fitted, newdata = train_set) # about 2700
perplexity(fitted, newdata = valid_set) # about 4300

the perplexity is higher for the validation set than the training set, because the topics have been optimised based on the training set.
using perplexity and cross-validation to determine a good number of topics
the extension of this idea to cross-validation is straightforward.  divide the data into different subsets (say 5), and each subset gets one turn as the validation set and four turns as part of the training set.  however, it's really computationally intensive, particularly when trying out the larger numbers of topics.  
you might be able to use caret to do this, but i suspect it doesn't handle topic modelling yet.  in any case, it's the sort of thing i prefer to do myself to be sure i understand what's going on.
the code below, even with parallel processing on 7 logical cpus, took 3.5 hours to run on my laptop:
#----------------5-fold cross-validation, different numbers of topics----------------
# set up a cluster for parallel processing
cluster <- makecluster(detectcores(logical = true) - 1) # leave one cpu spare...
registerdoparallel(cluster)

# load up the needed r package on all the parallel sessions
clusterevalq(cluster, {
   library(topicmodels)
})

folds <- 5
splitfolds <- sample(1:folds, n, replace = true)
candidate_k <- c(2, 3, 4, 5, 10, 20, 30, 40, 50, 75, 100, 200, 300) # candidates for how many topics

# export all the needed r objects to the parallel sessions
clusterexport(cluster, c(""full_data"", ""burnin"", ""iter"", ""keep"", ""splitfolds"", ""folds"", ""candidate_k""))

# we parallelize by the different number of topics.  a processor is allocated a value
# of k, and does the cross-validation serially.  this is because it is assumed there
# are more candidate values of k than there are cross-validation folds, hence it
# will be more efficient to parallelise
system.time({
results <- foreach(j = 1:length(candidate_k), .combine = rbind) %dopar%{
   k <- candidate_k[j]
   results_1k <- matrix(0, nrow = folds, ncol = 2)
   colnames(results_1k) <- c(""k"", ""perplexity"")
   for(i in 1:folds){
      train_set <- full_data[splitfolds != i , ]
      valid_set <- full_data[splitfolds == i, ]

      fitted <- lda(train_set, k = k, method = ""gibbs"",
                    control = list(burnin = burnin, iter = iter, keep = keep) )
      results_1k[i,] <- c(k, perplexity(fitted, newdata = valid_set))
   }
   return(results_1k)
}
})
stopcluster(cluster)

results_df <- as.data.frame(results)

ggplot(results_df, aes(x = k, y = perplexity)) +
   geom_point() +
   geom_smooth(se = false) +
   ggtitle(""5-fold cross-validation of topic modelling with the 'associated press' dataset"",
           ""(ie five different models fit for each candidate number of topics)"") +
   labs(x = ""candidate number of topics"", y = ""perplexity when fitting the trained model to the hold-out set"")

we see in the results that 200 topics is too many and has some over-fitting, and 50 is too few.  of the numbers of topics tried, 100 is the best, with the lowest average perplexity on the five different hold-out sets.",https://stackoverflow.com/questions/21355156,r,25-01-2014 17:52,34970.0,32.0,2.0,True,27-12-2022 04:17,27-12-2022 04:17
72868060,creating a list from a file then checking and printing matching token from the list,"i am trying to achieve a task where i have a file which consists of a sample conversation.
on the other hand i have some action keywords that needs to match the starting of the sentence and print the whole line.
file:
hey salam daniyal, can you hear me alright? hey hey walikumassalam joe, how are you? yes for sure i can hear you. how is it going brother? all good bro all good, so i wanted to discuss something with you, yeah sure shoot. okay so, i want a simple 4 page website for my business on wordpress. i need home page about us services and contact us. can you make it? yeah no problem send me the content and all the images i will start working on it and will show you some samples. great, one more thing. can we use ""one page multipurpose"" theme from themeforest? yeah sure. alright great! sending you the images and content.

to achieve this i've write:
import re

textfile = open(""filepath"", 'r')
filetext = textfile.read()
a = [i[0].strip() for i in re.findall(r""((\w\w+){1,}(?=(\,|\.|\!|\?)))"", filetext)]

print(a)

output:
['salam daniyal', 'can you hear me alright', 'hey hey walikumassalam joe', 'how are you', 'yes for sure i can hear you', 'how is it going brother', 'all good bro all good', 'so i wanted to discuss something with you', 'yeah sure shoot', 'okay so', 'i want a simple 4 page website for my business on wordpress', 'i need home page about us services and contact us', 'can you make it', 'yeah no problem send me the content and all the images i will start working on it and will show you some samples', 'great', 'one more thing', 'theme from themeforest', 'yeah sure', 'alright great', 'sending you the images and content']

another approach can be using nltk which gives a precise list.
from nltk.tokenize import sent_tokenize
  
textfile = open(""filepath"", 'r')
filetext = textfile.read()

textfile.close()

print(sent_tokenize(filetext))

output:
['hey salam daniyal, can you hear me alright?', 'hey hey walikumassalam joe, how are you?', 'yes for sure i can hear you.', 'how is it going brother?', 'all good bro all good, so i wanted to discuss something with you, yeah sure shoot.', 'okay so, i want a simple 4 page website for my business on wordpress.', 'i need home page about us services and contact us.', 'can you make it?', 'yeah no problem send me the content and all the images i will start working on it and will show you some samples.', 'great, one more thing.', 'can we use ""one page multipurpose"" theme from themeforest?', 'yeah sure.', 'alright great!', 'sending you the images and content.']

this one creates a list of whole sentence and regex doesn't. but also in regex i can print list from it's index and in nltk i can't.
in regex:
print(a[11]) //will print list on 11th index

output:
i need home page about us services and contact us

in nltk:
print(sent_tokenize(filetext[11]))

output:
['a']

which one is the better option to create list, now to match action keywords what approach should i take?
as i have a list of action keywords which needs to match from the above list and print the results,
actionkeywords = ""i need"" , ""can we"", ""i want a"", ""we need""
so according to current action keywords i want my code to print these sentences from the list as these sentences starts from my action keywords:
'i need home page about us services and contact us'
'i want a simple 4 page website for my business on wordpress.'
'can we use ""one page multipurpose"" theme from themeforest?'","['python', 'nltk']",72868441,"if you have data like below
a = ['hey salam daniyal, can you hear me alright?', 'hey hey walikumassalam joe, how are you?', 'yes for sure i can hear you.', 'how is it going brother?', 'all good bro all good, so i wanted to discuss something with you, yeah sure shoot.', 'okay so, i want a simple 4 page website for my business on wordpress.', 'i need home page about us services and contact us.', 'can you make it?', 'yeah no problem send me the content and all the images i will start working on it and will show you some samples.', 'great, one more thing.', 'can we use ""one page multipurpose"" theme from themeforest?', 'yeah sure.', 'alright great!', 'sending you the images and content.']

and action_words like so:
action_keywords = [""i need"" , ""can we"", ""i want a"", ""we need""]

you can filter a using python's inbuilt filter method like below
def extract(x):
    for e in action_keywords:
        if e in x:
            return true
    return false
    
ans = filter(extract, a)
print(list(ans))

outputs:
['okay so, i want a simple 4 page website for my business on wordpress.', 'i need home page about us services and contact us.', 'can we use ""one page multipurpose"" theme from themeforest?']

please note that you can modify the logic inside extract based on the size of your data and other conditions",https://stackoverflow.com/questions/72868060,python,05-07-2022 10:42,45.0,-2.0,1.0,True,05-07-2022 11:11,05-07-2022 11:02
74745041,odd behavior for findw in embedded do loops?,"when i use a do loop to iterate through an array of words in sas and see if they exist in a string, it works. when i incorporate a second for daily words, findw does not find words already in the final string.
so this works as expected:
word1=""""
word2=""pancake""
word3=""""
word4=""donut""
word5=""""
array word {5} $ 250 word:;
final_str=""pancake"";

do i = 1 to 5;
final_str_w_removed_hyphens = translate(final_str, "" "", ""-"");

if findw(final_str_w_removed_hyphens,  word[i], "" "") = 0 
     then final_str = catx(""-"", str, word[i]);

it gives me the expected final string of ""pancake-donut"".
however, when i incorporate days into the matter (there can be multiple breakfast names everyday), findw begins to do this weird double counting behavior. the data looks like this, it describes the food we ate for breakfast on a given day:
breakfast_foods_jan1 | breakfast_foods_jan2                   | breakfast_foods_jan3                 | breakfast_foods_jan4               |
---------------------|----------------------------------------|--------------------------------------|------------------------------------|
""breakfast-pancake""  | ""breakfast-donut-breakfast-pancake""    | ""breakfast-donut-breakfast-pancake""  | ""breakfast-donut-breakfast-pancake""|

i want to find all of the unique breakfast items a person ate in a year, here is my solution:
do j=1 to 4          /*january 1st - january 4th*/;
      do i=1 to i=5 /*there can't be more than 5 breakfast items on any day*/;

            if scan(breakfast_foods[j], i, ""-"", ""d"") ne ""breakfast""
              then daily_breakfast_foods[i] = scan(breakfast_foods[j], i, ""-"", ""d"");

            word_find = findw(translate(all_breakfast_foods, "" "", ""-""), daily_breakfast_foods[i], "" "");

             if word_find=0 then all_breakfast_foods =
              catx(""-"", all_breakfast_foods, daily_breakfast_foods[i];

end;
end;

this returns the final all_breakfast_foods of ""pancake-donut-pancake"" it double counts pancake!!! i have no clue why word_find is not finding pancake when it is clearly contained in the all_breakfast_foods string.
here is what is happening in the loop:
daily_breakfast_foods1 | daily_breakfast_foods2 |daily_breakfast_foods3 | daily_breakfast_foods4 | daily_breakfast_foods5 |
-----------------------|------------------------|-----------------------|-----------------------|------------------------|
                       | donut                  |                       |pancake               |                        |```






all_breakfast_foods_debug1
all_breakfast_foods_debug2
all_breakfast_foods_debug3
all_breakfast_foods_debug4
all_breakfast_foods_debug5




pancake
pancake
pancake donut
pancake donut
pancake donut pancake","['loops', 'nlp', 'sas']",74745896,"so your list has some leading spaces (spaces after the - delimiter) that is causing trouble.  you can use the left() function to remove the leading spaces.  and the findw() has the t modifier to trim trailing spaces when hunting for words.
so let's make your test data listing into an actual dataset.
data have ;
  input (breakfast_foods_jan1-breakfast_foods_jan4) ($40./);
cards;
breakfast-pancake
breakfast-donut-pancake
breakfast-donut-pancake
breakfast-donut- pancake
;

now we can loop over the food variables and then loop over each list of foods and build up the list of unique foods.
data want;
  set have;
  array foods breakfast_foods_jan1-breakfast_foods_jan4 ;
  length next_food $30 food_list $200 ;
  do day=1 to dim(foods);
    do item=1 to countw(foods[day],'-');
      next_food = left(scan(foods[day],item,'-'));
      if next_food ne 'breakfast' then do;
        if not findw(food_list,next_food,'-','t') then
          food_list=catx('-',food_list,next_food)
        ;
      end;
    end;
  end;
  drop day item next_food;
run;

result:",https://stackoverflow.com/questions/74745041,loops,09-12-2022 15:20,55.0,0.0,2.0,True,09-12-2022 16:39,09-12-2022 15:58
70449920,run my python script easily on any other laptop,"i have written a script (around 2k lines) for processing text.
it reads the input form my text file, and print the output in another file.
but, i want it can be run on any other laptop (with python installed) easily as well. for example,
other people can run it without installing additional libraries (that i had imported in the script).
how can i realize my purpose? by packaging my script in a library or what else i can do? please provide any hint.
i tried to use the pyinstaller or the py2exe, but i always have a problem of over recursion limit,
and since i have several huge sized libraries being imported, so i guess even i can finally make a .exe file,
it would be in a huge size, so i stopped to using that way. anyone has a comment on it?","['python', 'nlp']",70450077,"if you're sure that every client has python and pip installed and present in path, you can just pip install the libraries in the beginning of your script. something like this:
import subprocess
subprocess.run(['pip', 'install', '--user', 'your', 'libs'])
import your
import libs

this is just a general idea, maybe hacky, and definitely requires additional work with error handling, etc.",https://stackoverflow.com/questions/70449920,python,22-12-2021 13:35,67.0,-3.0,1.0,True,22-12-2021 14:23,22-12-2021 14:23
75882725,can you determine the number of output classes in a huggingface segmentation model?,"i am loading a model like this:
id2label = {
        0: 'background', 
        1: 'cake',
        2: 'donut', 
}
model = mask2formerforuniversalsegmentation.from_pretrained(self.backbone, id2label=self.id2label, ignore_mismatched_sizes=true)  
model.load_state_dict(torch.load('checkpoint.pt', map_location=torch.device('cpu')))

however i don't actually know id2label. (i just have the checkpoint). i don't really care about the names of the classes i just want to know how many there are in the checkpointed model. i can see it in the warning message that appears, but would like to avoid that:
runtimeerror: error(s) in loading state_dict for mask2formerforuniversalsegmentation:
    size mismatch for class_predictor.weight: copying a param with shape torch.size([8, 256]) from checkpoint, the shape in current model is torch.size([20, 256]).
    size mismatch for class_predictor.bias: copying a param with shape torch.size([8]) from checkpoint, the shape in current model is torch.size([20]).
    size mismatch for criterion.empty_weight: copying a param with shape torch.size([8]) from checkpoint, the shape in current model is torch.size([20]).","['python', 'pytorch', 'huggingface-transformers']",75885112,"you could check the state_dict:
import torch

chk = torch.load(checkpoint.pt)
# chk is a dict[str, torch.tensor]
# the layer shape tells you the number of labels +1 (i.e. subtract 1)
chk[""class_predictor.weight""].shape[0] 

the only downside is that you need to know the layer's name, but that is feasible when you only load with mask2formerforuniversalsegmentation.",https://stackoverflow.com/questions/75882725,python,29-03-2023 22:36,229.0,3.0,1.0,True,30-03-2023 07:52,30-03-2023 07:10
71740706,get first element of tokenized words in a row,"using the existing column name, add a new column first_name to df such that the new column splits the name into multiple words and takes the first word as its first name. for example, if the name is elon musk, it is split into two words in the list ['elon', 'musk'] and the first word elon is taken as its first name. if the name has only one word, then the word itself is taken as its first name.
a snippet of the data frame




name




alemsah ozturk


igor arinich


christopher maloney


dj holiday


brian tracy


philip defranco


patrick collison


peter moore


dr.darrell scott


atul gawande


everette taylor


elon musk


nelly_mo




this is what i have so far. i am not sure how to extract the name after i tokenize it
import nltk
first = df.name.apply(lambda x: nltk.word_tokenize(x))
df[""first_name""] = this is where i'm stuck","['python', 'pandas', 'nltk']",71740838,"try this snippet:
df[""first_name""] = df['name'].map(lambda x: x.split(' ')[0])
df[""last_name""] = df['name'].map(lambda x: x.split(' ')[1])",https://stackoverflow.com/questions/71740706,python,04-04-2022 16:33,170.0,1.0,1.0,True,04-04-2022 16:44,04-04-2022 16:38
75394318,python text parsing to split list into chunks including preceding delimiters,"what i have
after ocr'ing some public q&a deposition pdfs which have a q&a form, i have raw text like the following:
text = """"""\na\n\nq so i do first want to bring up exhibit no. 46, which is in the binder 
in front of\nyou.\n\nand that is a letter [to] alston\n& bird...
\n\nis that correct?\n\na this is correct.\n\nq okay.""""""

...which i want to split into the separate questions and answers. each question or answer starts with '\nq ', '\na ', '\nq_' or '\na_' (e.g. matches regex ""\n[qa]_?\s"")
what i've done so far
i can get a list of all questions and answers with the following code:
pattern = ""\n[qa]_?\s""
q_a_list = re.split(pattern, text)
print(q_a_list)

which yields q_a_list:
['\na\n', 
'so i do first want to bring up exhibit no. 46, which is in the binder \nin front of\nyou.\n\nand that is a letter [to] alston\n& bird...\n\n\nis that correct?\n', 
'this is correct.\n', 
'okay.']

what i want
this is close to what i want, but has the following problems:

it's not always clear if a statement is a question or an answer, and
sometimes, such as in this particular example, the first item in the list may be neither a question nor answer, but just random text before the first \q delimiter.

i would like a modified version of the my q_a_list above, but which addresses the two bulleted problems by linking each text chunk to the delimiter that preceded it. something like:
[{'0': '\na\n', 
  '\nq': 'so i do first want to bring up exhibit no. 46, which is in the binder \nin front of\nyou.\n\nand that is a letter [to] alston\n& bird...\n\n\nis that correct?\n',
  '\na': 'this is correct.\n',
  '\nq': 'okay.'}]

or
[{'\nq': 'so i do first want to bring up exhibit no. 46, which is in the binder \nin front of\nyou.\n\nand that is a letter [to] alston\n& bird...\n\n\nis that correct?\n',
  '\na': 'this is correct.\n',
  '\nq': 'okay.'}]

or maybe even just a list with delimiters pre-pended:
['\nq: so i do first want to bring up exhibit no. 46, which is in the binder \nin front of\nyou.\n\nand that is a letter [to] alston\n& bird...\n\n\nis that correct?\n',
'\na: this is correct.\n',
'\nq: okay.'
]","['python', 'regex', 'parsing', 'nlp', 'ocr']",75394630,"this is probably not the most elegant answer, but it seems to work. i won't accept this for the next few days in case someone posts a better answer:
# this gets me the location (index start & end) of each occurrence of my regex pattern 
delims = list(re.finditer(pattern, text))

# now let's iterate through each pair of delimiter and next-delimiter locations
q_a_list = []

for delim, next_delim in zip(delims[:-1], delims[1:]):

    # pull ""q"" or ""a"" out of the current delimiter
    prefix = text[delim.span()[0]:delim.span()[1]].strip()

    # the actual question or answer text spans from the end of this 
    # delimiter to the start of the next delimiter
    text_chunk = text[delim.span()[1]:next_delim.span()[0]]

    q_a_list.append(f""{prefix}: {text_chunk}"")

# q_a_list is missing the final prefix and text_chunk, because
# they have no next_delim, so the zip() above doesn't get to it
final_delim = delims[-1]

final_prefix = text[final_delim.span()[0]: final_delim.span()[1]].strip()
final_text_chunk = text[final_delim.span()[1]:]

q_a_list.append(f""{final_prefix}: {final_text_chunk}"")

now the result:
>>> print(q_a_list)
['q: so i do first want to bring up exhibit no. 46, which is in the binder \nin front of\nyou.\n\nand that is a letter [to] alston\n& bird...\n\n\nis that correct?\n', 
'a: this is correct.\n', 
'q: okay.']",https://stackoverflow.com/questions/75394318,python,09-02-2023 04:49,242.0,1.0,3.0,True,14-03-2023 00:34,09-02-2023 05:09
71909945,how to get the sentence embeddings with deberta.deberta.pooling?,"how to get the sentence embeddings with deberta.deberta.pooling?
hi everyone, i applied a deberta model to analyze sentences, this is what my code looks like:
from transformers import debertatokenizer, debertamodel
import torch
# downloading the models
tokenizer = debertatokenizer.from_pretrained(""microsoft/deberta-base"")
model = debertamodel.from_pretrained(""microsoft/deberta-base"")
# tokenizing the input text and converting it into pytorch tensors
inputs = tokenizer([""the cat cought the mouse"", ""this is the second sentence""], return_tensors=""pt"", padding=true)
# pass through the model 
outputs = model(**inputs)

i realize that one option to get the sentence embeddings is to look at the cls hidden state using
outputs.last_hidden_state[:,0,:]

however, i would prefer to get the pooled output. as i learned, pooled_output is not supported, but there seems to be an implementation in deberta named deberta.deberta.pooling (see  does anyone know how to use it?","['python', 'nlp', 'spyder', 'bert-language-model']",77151507,"first, you need to import pooler for deberta, and then it is better to create a separate class to make it more convenient to work.
from transformers.models.deberta.modeling_deberta import contextpooler
from transformers.models.deberta.modeling_deberta import stabledropout
from transformers import debertatokenizer, debertamodel
import torch

tokenizer = debertatokenizer.from_pretrained(""microsoft/deberta-base"")
model = debertamodel.from_pretrained(""microsoft/deberta-base"")

class custommodel(nn.module): # deberta
    def __init__(self, backbone):
        super(custommodel, self).__init__()
        self.model = backbone
        self.config = self.model.config
        self.pooler = contextpooler(self.config) 
        # output_dim = self.pooler.output_dim
        # self.classifier = nn.linear(output_dim, num_classes)
        # drop_out = getattr(self.config, ""cls_dropout"", none)
        # drop_out = self.config.hidden_dropout_prob if drop_out is none else drop_out
        # self.dropout = stabledropout(drop_out)
   def forward('.....'):
        encoder_layer = outputs[0]
        pooled_output = self.pooler(encoder_layer)#, flat_attention_mask)
        # x = '......'
        return x # pooled_output
model = custommodel(model)

the details depend on your task. a more precise implementation can be found in the model repository. i hope it helps.",https://stackoverflow.com/questions/71909945,python,18-04-2022 09:25,1399.0,2.0,1.0,True,24-09-2023 19:21,18-04-2022 13:19
77001186,how to display the reconstructed image from huggingface vitmaemodel?,"i am using the following code example:
using the autoencoder, i want to display the recontracted image. how to display it?
from transformers import autoimageprocessor, vitmaemodel
from pil import image
import requests

url = ""
image = image.open(requests.get(url, stream=true).raw)

image_processor = autoimageprocessor.from_pretrained(""facebook/vit-mae-base"")
model = vitmaemodel.from_pretrained(""facebook/vit-mae-base"")

inputs = image_processor(images=image, return_tensors=""pt"")
outputs = model(**inputs)
last_hidden_states = outputs.last_hidden_state","['python', 'pytorch', 'huggingface-transformers']",77682434,"i encountered the same issue. according to the official doc of vitmae, please have a look at vit_mae_visualization_demo.ipynb.
import torch
import numpy as np
import matplotlib.pyplot as plt

from transformers import vitfeatureextractor, vitmaeforpretraining
import requests
from pil import image

feature_extractor = vitfeatureextractor.from_pretrained(""facebook/vit-mae-base"")
imagenet_mean = np.array(feature_extractor.image_mean)
imagenet_std = np.array(feature_extractor.image_std)

def show_image(image, title=''):
    # image is [h, w, 3]
    assert image.shape[2] == 3
    plt.imshow(torch.clip((image * imagenet_std + imagenet_mean) * 255, 0, 255).int())
    plt.title(title, fontsize=16)
    plt.axis('off')
    return

def visualize(pixel_values, model):
    # forward pass
    outputs = model(pixel_values)
    y = model.unpatchify(outputs.logits)
    y = torch.einsum('nchw->nhwc', y).detach().cpu()
    
    # visualize the mask
    mask = outputs.mask.detach()
    mask = mask.unsqueeze(-1).repeat(1, 1, model.config.patch_size**2 *3)  # (n, h*w, p*p*3)
    mask = model.unpatchify(mask)  # 1 is removing, 0 is keeping
    mask = torch.einsum('nchw->nhwc', mask).detach().cpu()
    
    x = torch.einsum('nchw->nhwc', pixel_values)

    # masked image
    im_masked = x * (1 - mask)

    # mae reconstruction pasted with visible patches
    im_paste = x * (1 - mask) + y * mask

    # make the plt figure larger
    plt.rcparams['figure.figsize'] = [24, 24]

    plt.subplot(1, 4, 1)
    show_image(x[0], ""original"")

    plt.subplot(1, 4, 2)
    show_image(im_masked[0], ""masked"")

    plt.subplot(1, 4, 3)
    show_image(y[0], ""reconstruction"")

    plt.subplot(1, 4, 4)
    show_image(im_paste[0], ""reconstruction + visible"")

    plt.show()


url = ""
image = image.open(requests.get(url, stream=true).raw)

pixel_values = feature_extractor(image, return_tensors=""pt"").pixel_values

# make random mask reproducible (comment out to make it change)
torch.manual_seed(2)

model = vitmaeforpretraining.from_pretrained(""facebook/vit-mae-base"")

visualize(pixel_values, model)",https://stackoverflow.com/questions/77001186,python,29-08-2023 14:39,443.0,0.0,1.0,True,25-04-2024 09:12,30-08-2023 08:00
74977785,quanteda calculating tokens frequency in dfm including also a customized list of phrases,"i have been wondering if it is possible to perform the feauture_frequency of the powerful quanteda library in r including also a list of phrases or ""words"" to be accounted for, for instance i have the following data set:
library(quanteda)
library(quanteda.textstats)

df_sample<-c(""word record"",
             ""be able to count by word"",
             ""but also include some phrases such as"",
             ""world record super bass mr. president mr. president"")

when i calculate the textstat_frequency of the df_sample i get something like this:
> tokens<-corpus(df_sample) %>% tokens(remove_punct = true)
> dfm<-dfm(tokens)
> 
> quanteda.textstats::textstat_frequency(dfm)
     feature frequency rank docfreq group
1       word         2    1       2   all
2     record         2    1       2   all
3         mr         2    1       1   all
4  president         2    1       1   all
5         be         1    5       1   all
6       able         1    5       1   all
7         to         1    5       1   all
8      count         1    5       1   all
9         by         1    5       1   all
10       but         1    5       1   all
11      also         1    5       1   all
12   include         1    5       1   all
13      some         1    5       1   all
14   phrases         1    5       1   all
15      such         1    5       1   all
16        as         1    5       1   all
17     world         1    5       1   all
18     super         1    5       1   all
19      bass         1    5       1   all
> 

which is correct but i also want to change my code in other to take into account and print in the output the words or phrases ""mr. president"", ""world record"", ""super bass""
key_lookups<-c(""mr. president"", ""world record"", ""super bass"" )

how can i use quanteda funs to have in my output along with the previous counts also the frequency of the previous phrases,for example

""mr. president""  2 ""world record""   2 ""super bass""     1","['r', 'regex', 'text-mining', 'quanteda']",74982575,"first: a warning about your example code: do not create objects that have the same name as functions (like tokens and dfm) this will (eventually) lead to errors and is difficult to debug.
there are probably a few ways of doing this. i created a ""normal"" tokens object and one ngrams tokens object. both turned into dfm's and from the ngrams dfm, i kept the phrases you wanted. then combined the dfm's and you can use textstat_frequency as normal.

note: you can't combine tokens objects like you can combine dfm objects.

library(quanteda)
library(quanteda.textstats)

df_sample<-c(""word record"",
             ""be able to count by word"",
             ""but also include some phrases such as"",
             ""world record super bass mr. president mr. president"")



my_tokens <- corpus(df_sample) %>% tokens(remove_punct = true)
my_dfm <- dfm(my_tokens)

# no points as they are removed in the dfm
key_lookups<-c(""mr president"", ""world record"", ""super bass"" )


my_tokens_ngram <- tokens_ngrams(my_tokens, n = 2, concatenator = "" "")

my_dfm_ngrams <- dfm(my_tokens_ngram)

# only keep the lookups
my_dfm_ngrams <- dfm_keep(my_dfm_ngrams, key_lookups)

# combine both dfms
my_dfms <- rbind(my_dfm, my_dfm_ngrams)

# if necessary uncomment next part
# my_dfms <- dfm_compress(my_dfms) 

outcome:
head(textstat_frequency(my_dfms), 5)
       feature frequency rank docfreq group
1         word         2    1       2   all
2       record         2    1       2   all
3           mr         2    1       1   all
4    president         2    1       1   all
5 mr president         2    1       1   all

tail(textstat_frequency(my_dfms), 5)
        feature frequency rank docfreq group
18        world         1    6       1   all
19        super         1    6       1   all
20         bass         1    6       1   all
21 world record         1    6       1   all
22   super bass         1    6       1   all

do note that using rbind on dfms, creates a new document name like ""text1.1"". if you  want this merged back to the original documents, you can call dfm_compress(my_dfms)  first and then call textstat_frequency.",https://stackoverflow.com/questions/74977785,r,01-01-2023 22:33,240.0,0.0,2.0,True,22-01-2023 11:24,22-01-2023 11:24
47123094,how to extract name from string using nltk,"i am trying to extract name(indian) from unstructured string.
here come my code:
text = ""balaji chandrasekaran bangalore |  senior business analyst/ lead business analyst an accomplished senior business analyst with a track record of handling complex projects in given period of time, exceeding above the expectation. successful at developing product road maps and leading cross-functional software teams from prototype to release. professional competencies systems development life cycle (sdlc) agile methodologies business process improvement requirements gathering & analysis project management uml specification ui & ux (wireframe designing) functional specification test scenario creation sharepoint admin work history senior business analyst (aug 2012 current) youbox technology pvt ltd, chennai translating business goals, feature concepts and customer needs into prioritized product requirements and use cases. expertized in designing innovative wireframes combining user experience analysis and technology models. extensive experience in implementing soft wares for shipping/logistics firms to handle crm, finance, logistics, operations, intermodal, and documentation. strong interpersonal skills, highly adept at diplomatically facilitating discussions and negotiations with stakeholders. education bachelor of engineering: electronics & communication, 2011 ces tech hosur accomplishment successful onsite implementation at various locations around the globe for europe shipping company. - (pre study, general design, and functional specification) organized business analyst forum and conducted various activities to develop skill sets of business analysts.""
if text != """":
    grammar = """"""person: {<nnp>}""""""
    chunkparser = nltk.regexpparser(grammar)
    tagged = nltk.pos_tag(nltk.word_tokenize(text))
    tree = chunkparser.parse(tagged)

    for subtree in tree.subtrees():
        if subtree.label() == ""person"": 
            pronouns.append(' '.join([c[0] for c in subtree]))

    print(pronouns)


['balaji', 'chandrasekaran', 'bangalore', '|','senior', 'business',
  'analys', '/', 'lead', 'business', 'analyst', 'senior', 'business',
  'analyst', 'successful', 'development', 'life', 'cycle', 'sdlc',
  'agile', 'business', 'requirements', 'analysis', 'project',
  'management', 'uml', 'specification', 'ui', 'ux', 'wireframe',
  'designing', 'functional', 'specification', 'test', 'scenario',
  'creation', 'sharepoint', 'admin', 'work', 'history', 'senior',
  'business', 'analyst', 'aug', 'current', 'technology', 'chennai',
  'translating', 'crm', 'finance', 'logistics', 'operations',
  'intermodal', 'education', 'bachelor', 'engineering', 'electronics',
  'communication', 'accomplishment', 'successful', 'mediterranean',
  'ship', 'company', 'msc', 'georgia', 'msc', 'cambodia', 'msc',  'msc',
  'south', 'successful', 'stake', 'msc', 'geneva', 'switzerland', 'pre',
  'study', 'general', 'design', 'functional', 'specification', 'o',
  'business', 'analyst', 'forum', 'business']

but actually i need to get only balaji chandrasekaran , i even try to use standford ner lib.which fails to pick balaji chandrasekaran
can any one help to extract name from the un strcuture string, or suggest me any good tutorial to do that.
thank you in advance.","['python', 'nlp', 'nltk', 'stanford-nlp', 'named-entity-extraction']",47123624,"like i said in the comments, you would have to create your own corpora for indian names and test your text against that. the nltk book teaches you how to do this in chapter 2 (section 1.9 to be exact).
from nltk.corpus import plaintextcorpusreader

# you can use a regular expression to find the files, or pass a list of files
files = "".*\.txt""

new_corpus = plaintextcorpusreader(""/path/"", files)
corpus  = nltk.text(new_corpus.words())

see also: creating a new corpus with nltk",https://stackoverflow.com/questions/47123094,python,05-11-2017 15:09,5068.0,3.0,2.0,True,26-03-2023 10:09,26-03-2023 10:09
72834436,"typeerror: tuple indices must be integers or slices, not str, facing this error in keras model","i am running a keras model, link is here. i have just changed the dataset for this model and when i run my model it throwing this error typeerror: tuple indices must be integers or slices, not str. as it's a image captioning model and the dataset is difficult for me to understand.
see the blow code and read also the location of the error.
`reduce_lr = keras.callbacks.reducelronplateau(
    monitor=""val_loss"", factor=0.2, patience=3
 )
 # create an early stopping callback.
 early_stopping = tf.keras.callbacks.earlystopping(
 monitor=""val_loss"", patience=5, restore_best_weights=true 
 )
 history = dual_encoder.fit(
 train_dataloader,
 epochs=num_epochs,
 #validation_data=val_dataloader,
 #callbacks=[reduce_lr, early_stopping],
 )
 print(""training completed. saving vision and text encoders..."")
 vision_encoder.save(""vision_encoder"")
 text_encoder.save(""text_encoder"")
 print(""models are saved."")


 typeerror                                 traceback (most recent call last)
 <ipython-input-31-745dd79762e6> in <module>()
      15 history = dual_encoder.fit(
      16     train_dataloader,
 ---> 17     epochs=num_epochs,
      18     #validation_data=val_dataloader,
      19     #callbacks=[reduce_lr, early_stopping],

  11 frames
  <ipython-input-26-0696c83bf387> in call(self, features, training)
      16         with tf.device(""/gpu:0""):
      17             # get the embeddings for the captions.
 ---> 18             caption_embeddings = text_encoder(features[""caption""], training=training)
      19             #caption_embeddings = text_encoder(train_inputs, training=training)
      20         with tf.device(""/gpu:1""):

  typeerror: tuple indices must be integers or slices, not str'

the error is pointing to this location caption_embeddings = text_encoder(features[""caption""], training=training)
now i am confused, i don't  know whether this error is due to the data which i am passing to my model like this history = dual_encoder.fit(train_dataloader) or this error is related to caption_embeddings = text_encoder(features[""caption""], training=training) and image_embeddings = vision_encoder(features[""image""], training=training) which is defined in class dualencoder.
because i don't know what are these features[""caption""] and features[""image""] which is defined in class dualencoder as i have not changed these two with my new dataset if you check my code here in this colab file.","['tensorflow', 'keras', 'deep-learning', 'nlp', 'computer-vision']",72851055,"the dataset (train_dataloader) seems to return a tuple of items: link. in particular, model input is a tuple (images, x_batch_input).
however, your code (in dualencoder) seems to assume that it's a dict (with keys like ""caption"", ""image"", etc). i think that's the source of the mismatch.",https://stackoverflow.com/questions/72834436,tensorflow,01-07-2022 20:40,244.0,0.0,1.0,True,04-07-2022 01:50,01-07-2022 20:43
77561864,how to use pdf document in the agent using langchain,"my code uses ""wikipedia"" to search for the relevant content. below is the code
load tools
tools = load_tools(
    [""wikipedia""],
    llm=llm)
agent = initialize_agent(
    tools,
    llm,
    agent=agenttype.chat_zero_shot_react_description,
    handle_parsing_errors=true,
    verbose=false
)
out = agent(f""does {var_1} cause {var_2} or the other way around?."")

instead of ""wikipedia"", i want to use my own pdf document that is available in my local. can anyone help me in doing this?
i have tried using the below code
from langchain.document_loaders import pypdfium2loader
loader = pypdfium2loader(""hunter-350-dual-channel.pdf"")
data = loader.load()

but i am not sure how to include this in the agent.","['python', 'langchain', 'large-language-model', 'pdfium']",77562839,"you can use retrievalqa to generate a tool.
just like below:
from langchain.agents import agenttype, tool, initialize_agent
from langchain.agents import load_tools
from langchain.chains import retrievalqa
from langchain.embeddings.openai import openaiembeddings
from langchain.llms import openai
from langchain.text_splitter import charactertextsplitter
from langchain.vectorstores import chroma
from langchain.document_loaders import textloader

llm = openai()

loader = textloader(""union.txt"", encoding=""utf-8"")
documents = loader.load()
text_splitter = charactertextsplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

embeddings = openaiembeddings(api_key=""sk-pigio******************"", base_url=""
docsearch = chroma.from_documents(texts, embeddings, collection_name=""state-of-union"")

state_of_union = retrievalqa.from_chain_type(
    llm=llm, chain_type=""stuff"", retriever=docsearch.as_retriever()
)

tools = load_tools([""wikipedia""], llm=llm)
tools += [
    tool(
        name=""state of union qa system"",
        func=state_of_union.run,
        description=""useful for when you need to answer questions about the most recent state of the union address. input should be a fully formed question."",
    ),   
]

agent = initialize_agent(
    tools, llm, agent=agenttype.zero_shot_react_description, verbose=true
)
agent.run(
    ""what did biden say about ketanji brown jackson in the state of the union address?""
)",https://stackoverflow.com/questions/77561864,python,28-11-2023 07:00,3512.0,0.0,1.0,True,19-12-2023 12:11,28-11-2023 08:39
75596597,"if i just want to use current version of files, do i still need the objects under .git/lfs?","use an example to explain.
i use these commands to download bart-large from huggingface:
git lfs install
git clone 

the downloaded folder bart-large has size of 11 gb. the size of .git/lfs alone is 5.2 gb.
i remove the objects under .git/lfs and still can load the models from the local bart-large folder. this makes me wonder if i just want to use the current version of models and will not modify the repo, do i still need objects under .git/lfs?","['huggingface-transformers', 'git-lfs']",75610341,"you should not just delete files from under .git/lfs.  if you want to prune all unpushed objects, including those in the working tree, you can use git lfs prune --force to do so, which will free those objects.  on some systems, you can de-duplicate the files in the working tree with git lfs dedup instead of using git lfs prune --force; whether that works depends on your operating system and file system.  (it works on macos with apfs, windows with refs, and linux with xfs and btrfs, possibly among others.)  you can try it and it will inform you whether de-duplication is possible.
note that git lfs prune --force does not delete unpushed objects, since they're not on the server; doing so would cause data loss, which is why you shouldn't just delete files from under .git/lfs, since that does, too.",https://stackoverflow.com/questions/75596597,huggingface-transformers,28-02-2023 19:43,490.0,0.0,1.0,True,02-03-2023 00:33,28-02-2023 22:21
74623127,"problem with for loop, break statement does not do what i thought it would","this is my first time posting here, so be gentle, please.
i have written the following code:
import pandas as pd
import spacy

df = pd.read_csv('../../../data/conll2003.dev.conll', sep='\t', on_bad_lines='skip', header=none)

nlp = spacy.load('en_core_web_sm')
nlp.max_length = 1500000
## 

all_tokens = []

for token in df[0]:
    all_tokens.append(str(token))

string = ' '.join(all_tokens)

doc = nlp(string)

token_tuples = tuple(enumerate(doc))

outfile = open('./conll2003.dev.syntax_corrupt.conll', 'w')
i = 0 ## initiate by looking at the first token in the doc
for x, token in enumerate(df[0]):
    for num, tok in token_tuples[i:]: ## we add this step to ensure that the for loop always looks from the last token that was a match, since doc is longer
        ## than df[0], otherwise it would at some point start looking from earlier tokens since spacy has more tokens and if there is an accidental match, it
        ## would provide the wrong dep and head
        if token == tok.text:
            i = num ## get the number from the token tuples as new starting point
            outfile.write(str(df[0][x]) + '\t' + str(df[1][x]) + '\t' + str(df[2][x]) + '\t' + str(df[3][x]) + '\t' + str(tok.dep_) + '\t' + str(tok.head.text) + '\n')
            break
        else:
            outfile.write(str(df[0][x]) + '\t' + str(df[1][x]) + '\t' + str(df[2][x]) + '\t' + str(df[3][x]) + '\t' + 'no_dep' + '\t' + 'no_head' + '\n')
            break
outfile.close()

the code is supposed to take data from the 2003conll-shared task on ner and first join the individual tokens to a string (as the data comes pre-tokenized) and then feed it into spacy in order to make use of its dependency parsing. after that, i want to write the same lines that were in the original file + two new columns containing the dependency relation and the respective head noun.
spacy obviously tokenizes the text differently than what came pre-tokenized so i had to find a way that the correct relation would be attributed to the correct token as len(doc)!= len(df[0]).
it works fine if i do not include the else statement and it writes the correct relation with the token to the outfile. however, when i do include it, i would expect it to print one line with the values ""no_dep"" and ""no_head"" (for the token spacy did not take into account) and then continue printing the tokens where there is information on the dependency relations (because the break statement should break the loop, yeah?). but it does not. it writes to every following token ""no_dep"" and ""no_head"" instead of going back to writing the actual relations.
in other words:
inputfile (snippet):
london  nnp b-np    b-loc
1996-08-30  cd  i-np    o

west    nnp b-np    b-misc

outputfile without else statement:
london  nnp b-np    b-loc   nmod    simmons
west    nnp b-np    b-misc  nmod    indian

what i want with the else statement:
london  nnp b-np    b-loc   nmod    simmons
1996-08-30      cd      i-np    no_dep  no_head
west    nnp b-np    b-misc  nmod    indian

what i get:
london  nnp b-np    b-loc   no_dep  no_head
1996-08-30  cd  i-np    o   no_dep  no_head
west    nnp b-np    b-misc  no_dep  no_head

(note that the first line in the outputfile does have the correct dependency relation and head noun, the problem starts from the second line.)
any ideas what it is that i'm doing wrong? thanks!","['python', 'pandas', 'nlp', 'spacy', 'conll']",74624208,"you should preserve the original tokenization. to do this, manually create the doc in order to skip the tokenizer in the pipeline:
import spacy
from spacy.tokens import doc

nlp = spacy.load(model)
words = [""here"", ""are"", ""the"", ""original"", ""tokens""]
doc = doc(nlp.vocab, words=words)

# apply the model to the doc (it skips the tokenizer for an input `doc`)
doc = nlp(doc)",https://stackoverflow.com/questions/74623127,python,30-11-2022 05:07,77.0,0.0,1.0,True,30-11-2022 07:29,30-11-2022 05:21
76672343,"openai api, chatcompletion and completion give totally different answers with same parameters. why?","i'm exploring the usage of different prompts on gpt3.5-turbo.
investigating over the differences between ""chatcompletion"" and ""completion"", some references say that they should be more or less the same, for example: 
other sources say, as expected, that chatcompletion is more useful for chatbots, since you have ""roles"" (system, user and assistant), so that you can orchestrate things like few-shot examples and/or memory of previous chat messages. while completion is more useful for summarization, or text generation.
but the difference seems to be much bigger. i can't find references where they explain what is happening under the hood.
the following experiment gives me totally diferent results, even when using the same model with the same parameters.
with chatcompletion
import os
import openai
openai.api_type = ""azure""
openai.api_version = ""2023-03-15-preview""
openai.api_base = ...
openai.api_key = ...

chat_response = openai.chatcompletion.create(
  engine=""my_model"", # gpt-35-turbo
  messages = [{""role"":""user"",""content"":""give me something intresting:\n""}],
  temperature=0,
  max_tokens=800,
  top_p=0.95,
  frequency_penalty=0,
  presence_penalty=0,
  stop=none)

print(chat_response.choices[0]['message']['content'])

result is a fact about a war:
did you know that the shortest war in history was between britain and zanzibar in 1896? it lasted only 38 minutes!

with completion
regular_response = openai.completion.create(
  engine=""my_model"", # gpt-35-turbo
  prompt=""give me something intresting:\n"",
  temperature=0,
  max_tokens=800,
  top_p=0.95,
  frequency_penalty=0,
  presence_penalty=0,
  stop=none)

print(regular_response['choices'][0]['text'])

result is a python code and some explanation of what it does:
    ```
    import random
    import string
    
    def random_string(length):
        return ''.join(random.choice(string.ascii_letters) for i in range(length))
    
    print(random_string(10))
    ```
    output:
    ```
    'jvjvjvjvjv'
    ```
    this code generates a random string of length `length` using `string.ascii_letters` and `random.choice()`. `string.ascii_letters` is a string containing all ascii letters (uppercase and lowercase). `random.choice()` returns a random element from a sequence. the `for` loop generates `length` number of random letters and `join()` concatenates them into a single string. the result is a random string of length `length`. this can be useful for generating random passwords or other unique identifiers.<|im_end|>

notes

i'm using the same parameters (temperature, top_p, etc). the only difference is the chatcompletion/completion api.
the model is the same in both cases, gpt-35-turbo.
i'm keeping the temperature low so i can get more consistent results.
other prompts also give totally different answers, like if i try something like ""what is the definition of song?""

the question

why is this happening?
shouldn't same prompts give similar results given that they are using the same model?
is there any reference material where openai explains what it is doing under the hood?","['python', 'openai-api', 'chatgpt-api', 'azure-openai']",76679485,"i actually found the answer by chance reviewing some old notebooks.
it's all on the hidden tags, or as i found out now, the chat markup language (chatml): 
this prompt with the completion api now returns almost the same answer as the chatcompletion:
prompt = """"""<|im_start|>system
<|im_end|>
<|im_start|>user
give me something intresting:
<|im_end|>
<|im_start|>assistant
""""""

regular_response = openai.completion.create(
  engine=""my_model"", # gpt-35-turbo
  prompt=prompt,
  temperature=0,
  max_tokens=800,
  top_p=0.95,
  frequency_penalty=0,
  presence_penalty=0,
  stop=none)

print(regular_response['choices'][0]['text'])

result now is the same fact about a war (with the ending tag):
did you know that the shortest war in history was between britain and zanzibar in 1896? the war lasted only 38 minutes, with the british emerging victorious.<|im_end|>

it seems that all that the chatcompletion api is doing is adding those tags in between your prompts.",https://stackoverflow.com/questions/76672343,python,12-07-2023 15:43,6955.0,3.0,2.0,True,12-11-2023 01:09,12-07-2023 15:49
74931998,unable to update &#39; and &quot;&quot; in the stop_word list,"i tried to update ' and "" in my stop_word list.

> stop_words.update([""'"",""""""])
> stop_words

i got the following error.

    > file ""<ipython-input-85-54a2b8b08201>"", line 2
    > stop_words
    > 
    > 
    > syntaxerror: eof while scanning triple-quoted string literal--


how to update those characters in the stop_word ?","['nlp', 'nltk', 'stop-words']",74932833,"a literal "" can e.g. be denoted as '""' or ""\"""". your """""" syntactically starts a triple-quoted string (which is never ended, causing the syntaxerror).",https://stackoverflow.com/questions/74931998,nlp,27-12-2022 16:49,35.0,0.0,1.0,True,27-12-2022 18:19,27-12-2022 16:51
77220631,within-string detection of multiple human languages in rust?,"i've found that in python, according to this example on this page, the pycld2 package is able to detect changes of language within strings.
i have tested that example. it works. i then modified that string to put different english and different french. it worked again.
but i want to do this in rust.
on the rust cld2 page it says ""deprecated in favor of whatlang, which is native rust and smaller. if you have a compelling use-case for this code, please open an issue.""
i've now used whatlang in rust. by default it doesn't seem to be able to split strings into sections in different detected languages. and there doesn't seem to be any talk on that page about this capability.
this seems indeed to have been an integral part of the python cld project from 2013 at least:

an option to identify which parts (byte ranges) of the text contain
which language, in case the application needs to do further
language-specific processing. from python, pass the optional
returnvectors=true argument to get the byte ranges, ...

is there any way of accomplishing in-string language differentiation in rust?","['rust', 'language-detection', 'cld2']",77224073,"promising answer with lingua-rs: ""10.6 detection of multiple languages in mixed-language texts"".
said to be an experimental feature, but works out of the box.
first results actually pretty/very impressive.",https://stackoverflow.com/questions/77220631,rust,03-10-2023 08:14,108.0,-2.0,1.0,True,03-10-2023 17:23,03-10-2023 12:06
74065619,nlp bert in r with tensorflow/keras setup,"i am trying to get bert to run in r.
i got other nlp tasks (e.g. word2vec) done with keras, so the general setup should be ok.
i adapted the model code from here: 
the problem is how to insert the inputs (tokens) correctly. i have tried a lot of different ways to transform them (as tensors, various forms of arrays etc), but can't seem to figure out what kind of data structure/type/shape is expected as input.
here is a simplified, replicable example:
#rm(list=ls())
packages <- c(""reticulate"", ""keras"", ""tensorflow"", ""tfdatasets"", ""tidyverse"", ""data.table"")
for (p in packages) if (!(p %in% installed.packages()[,1])) install.packages(p, character.only = true) else require(p, character.only = true)
rm(packages, p)

#reticulate::install_miniconda(force = true) # 1time
reticulate::use_condaenv(""~/.local/share/r-miniconda"") # win? reticulate::use_condaenv(""r-miniconda"")

sys.setenv(tf_keras=1) 
tensorflow::tf_version() # install_tensorflow() if null
reticulate::py_config()

#reticulate::py_install('transformers', pip = true)
#reticulate::py_install('torch', pip = true)
transformer = reticulate::import('transformers')
tf = reticulate::import('tensorflow')
builtins <- import_builtins() #built in python methods

set.tf.repos <- ""distilbert-base-german-cased""

tokenizer <- transformer$autotokenizer$from_pretrained(set.tf.repos)  # 
tokenizer_vocab_size <- length(tokenizer$vocab)

###### load model
model_tf = transformer$tfdistilbertmodel$from_pretrained(set.tf.repos, from_pt = t, trainable = false)
model_tf$config

# set configs
model_tf$config$output_hidden_states = true
summary(model_tf)

###### data & tokens #####
data <- data.table::fread("" encoding = ""latin-1"")
txt <- data$v1
y <- data$v2
table(y, exclude = null)

set.max_length = 100
tokens <- tokenizer(
  txt,
  max_length = set.max_length %>% as.integer(),
  padding = 'max_length', #'longest' #implements dynamic padding
  truncation = true,
  return_attention_mask = true,
  return_token_type_ids = false
)
#tokens[[""input_ids""]] %>% str()
#tokens[[""attention_mask""]] %>% str()

tokens <- list(tokens[[""input_ids""]], tokens[[""attention_mask""]])
str(tokens)



####### model ########
input_word_ids <- layer_input(shape = c(set.max_length), dtype = 'int32', name = ""input_word_ids"")
input_mask <- layer_input(shape = c(set.max_length), dtype = 'int32', name = ""input_attention_mask"")
#input_segment_ids <- layer_input(shape = c(max_len), dtype = 'int32', name=""input_segment_ids"")

last_hidden_state <- model_tf(input_word_ids, attention_mask = input_mask)[[1]]
cls_token <- last_hidden_state[, 1,]

output <- cls_token %>%
  layer_dense(units = 32, input_shape = c(set.max_length, 768), activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')

model <- keras_model(inputs = list(input_word_ids, input_mask), outputs = output)

model %>% compile(optimizer = ""adam"",
                  loss = ""binary_crossentropy""
)

history = model %>%
  keras::fit(
    x = list(input_word_ids = tokens$input_ids, input_mask = tokens$attention_mask),
    y = y,
    epochs = 2,
    batch_size = 256,
    #metrics = ""accuracy"",
    validation_split = .2
  )

error message:
error in py_call_impl(callable, dots$args, dots$keywords) : 
  valueerror: failed to find data adapter that can handle input: (<class 'dict'> containing {""<class 'str'>""} keys and {""<class 'nonetype'>""} values), <class 'numpy.ndarray'>

detailed traceback:
  file ""/home/sz/.local/share/r-miniconda/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from none
  file ""/home/sz/.local/share/r-miniconda/lib/python3.9/site-packages/keras/engine/data_adapter.py"", line 984, in select_data_adapter
    raise valueerror(

many thanks in advance!","['r', 'keras', 'bert-language-model', 'reticulate']",74073265,"your model$inputs shapes don't match the inputs you're feeding it in fit().
it's helpful to create a tf dataset, so you can be explicit about your training dataset tensor shapes, and make sure that those tensor shapes match model$inputs.
changing your fit() call to this makes it work:
x_ds <- tensor_slices_dataset(tokens) 
y_ds <- tensor_slices_dataset(y)

ds <- zip_datasets(x_ds, y_ds) %>% 
  dataset_batch(256)

history = model %>% fit(ds, epochs = 2)",https://stackoverflow.com/questions/74065619,r,14-10-2022 07:14,1036.0,1.0,1.0,True,14-10-2022 18:11,14-10-2022 07:41
78748463,rag using langchain / chroma - unable to save more than 99 records to database,"i'm using the following code to load the content of markdown files (only one file, in my case), split it into chunks and then embed and store the chunks one by one. my file is split into 801 chunks. however, this code is unable to save the embeddings to disk in the vector db.
def load_documents():
loader = directoryloader(data_path, glob=""*.md"")

documents = loader.load()

return documents

def split_text(documents: list[document]):
text_splitter = recursivecharactertextsplitter(

    chunk_size=300,

    chunk_overlap=100,

    length_function=len,

    add_start_index=true,

)

chunks = text_splitter.split_documents(documents)

print(f""split {len(documents)} documents into {len(chunks)} chunks."")



document = chunks[10]

print(document.page_content)

print(document.metadata)



return chunks

def save_to_chroma(chunks: list[document]):
# clear out the database first.

if os.path.exists(chroma_path):

    shutil.rmtree(chroma_path)



# create a new db from the documents.

db = chroma.from_documents(

    chunks, openaiembeddings(), persist_directory=chroma_path

)

while analysing this problem, i attempted to save the chunks one by one instead, using a for loop:
for i, chunk in enumerate(chunks):
db = chroma.from_documents(
chunks, openaiembeddings(), persist_directory=chroma_path
)
i found that the code does save up to 99 chunks / embeddings, but always crashes when it tries to save further data. to investigate further, i opened the underlying database using db browser for sqlite and saw that chroma was saving a max of 99 records in the 'embeddings' table. however, i was able to manually add more records to it i.e. exceeding 99.
i also tried a few variations i.e. by:

saving the chunks in batches of 10 or 30
not deleting the existing database already containing 99 records, and then attempting to write more data
playing around with the chunk size and chunk overlap

however, none of the above made any difference.
does anybody know why this is happening and how to solve this problem?","['database', 'vector', 'langchain', 'chromadb', 'retrieval-augmented-generation']",78774435,"my problem was solved when i re-installed python on my pc due to some other problem. now, when i run the code, it works like magic! all the chunks are being saved now regardless of whether i save them all in one go or in batches.
edit:
the problem was actually due to a conflict in the installed libraries. it gets resolved when you use a virtual environment, so no need to re-install python.",https://stackoverflow.com/questions/78748463,database,15-07-2024 07:06,1987.0,-3.0,2.0,True,07-10-2024 10:43,15-07-2024 11:32
72846463,finding similarity of 1 paragraph in different documents with doc2vec,"how to find one target paragraph or document similar to other lists of documents to the target paragraph that is semantically similar.
import os
import gensim
import smart_open
import random
from nltk.tokenize import word_tokenize
# set file names for train and test data
test_data_dir =('c:\\users\\hamza\\desktop\\')
train_file = os.path.join(test_data_dir, 'read-me.txt')
target_file = os.path.join(test_data_dir, 'read-me2.txt')

def read_file(filename):
    
    try:
        with open(filename, 'r') as f:
            data = f.read()
        return data
    
    except ioerror:
        print(""error opening or reading input file: "", filename)
        sys.exit()
def read_corpus(fname, tokens_only=false):
    with smart_open.open(fname, encoding=""iso-8859-1"") as f:
        for i, line in enumerate(f):
            tokens = gensim.utils.simple_preprocess(line)
            if tokens_only:
                yield tokens
            else:
                # for training data, add tags
                yield gensim.models.doc2vec.taggeddocument(tokens, [i])

train_data = list(read_corpus(train_file))
target_data = word_tokenize(read_file(target_file))

# print(target_data)
# print(test_corpus)
model = gensim.models.doc2vec.doc2vec(vector_size=50, min_count=2, epochs=40)
model.build_vocab(train_data)
# print(f""word 'noise' appeared {model.wv.get_vecattr('noise', 'count')} times in the training corpus."")
model.train(train_data, total_examples=model.corpus_count, epochs=model.epochs)
inferred_vector = model.infer_vector(target_data)
sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))
print(sims)

output
[(1, 0.20419511198997498),
(2, 0.1924923211336136),
(0, 0.10696495324373245)]
now how i can match target data to train data and how i will know how much they are similar is there any way to scale the similarity into percentage?","['python', 'nlp', 'doc2vec', 'opensemanticsearch']",72846857,"despite the class name doc2vec, and the fact that it is based on an algorithm called 'paragraph vectors', this algorithm for modeling text has no inherent idea what 'paragraphs' or 'documents' are.
it simply takes whatever texts you give it ï¿½ï¿½ï¿½ where each text is a list-of-words ï¿½ï¿½ï¿½ & learns a way to plot those texts into a vector-space for comparisons.
so, using it to ""match one target document or paragraph in other documents and bring them as a list how much they are semantically similar"" is one possible application:

train a doc2vec model with your full set of texts.
at the end of training, the model will both (a) have learned a vector for each text; (b) have learned how to infer (via the .infer_vector() method) vectors for new texts that use the same words as the model already knows.
look up the exact vector for one of your training texts with model.dv[tag]. get a  new text with model.infer_vector(list_of_words). compare those vectors using any vector operations you'd like.
get a ranked list of known texts (from the training set) that are closest to some target vector with model.dv.most_similar() - you can either supply a tag (to name one of the training documents) or a raw vector (via the positive argument) as the target point.

if this & the tutorial isn't enough to make progress, you should better explain more about what your data is, what you've tried so far, and where things haven't yet worked ï¿½ï¿½ï¿½ with as much of your code, and precise info about what has and hasn't been achieved yet, as possible.
(it's nearly impossible to give a helpful answer to ""guide me through this generic underspecified project"". but if you say instead ï¿½ï¿½ï¿½ ""i have data d & want to achieve well-described goal g. i've tried x, but only had result or error y so far, when my ideal result woulde z. what would help me get from my progress y so far, to my desired result z?"" ï¿½ï¿½ï¿½ then it is possible to give tangible tips/pointers/explanations.",https://stackoverflow.com/questions/72846463,python,03-07-2022 12:22,245.0,0.0,1.0,True,03-07-2022 15:30,03-07-2022 15:30
76030084,chatgpt completion /v1/chat/completions memorize across multiple requests,"when i use user parameter on  the memory is not persisted across multiple requests. how can we let the model memorize it across multiple requests?
eg. is the message ""my name is xxx"" remembered by the chatgpt api? or do i have to send it every time? then what is the purpose of the ""user"" variable if it is not used to remember things?
{
    ""model"": ""gpt-4"",
    ""messages"": [
        {
            ""role"": ""user"",
            ""content"": ""my name is xxx.""
        }
    ],
    ""user"": ""myuser""
}","['openai-api', 'chatgpt-api']",77690125,"you have 2 options here. use the chat completion api, that will be soon obsolete, or use the new assistant api, the latter is exactly what you need. with the chat completion api you must add all the conversation texts every time you post a question, in that case chatgpt will be able to extract from the previous question/answers the context and history of the conversation. with the new assistant api (at the moment in beta version) that is done more efficiently and with less effort from your side because they have been developed to deal with what you need (test the assistant api playground on openai website to check it). they released the assistant api a couple of weeks ago.",https://stackoverflow.com/questions/76030084,openai-api,16-04-2023 20:07,2446.0,2.0,2.0,True,20-12-2023 08:28,17-04-2023 00:29
78932356,capitalized words in sentiment analysis,"i'm currently working with data of customers reviews on products from sephora. my task to classify them to sentiments : negative, neutral , positive .
a common technique of text preprocessing is to lower case all the words , but in this situation upper case words like 'amazing' can hide significant emotion behind them and turning all the word to lower case can cause information loss. would be happy for your opinion in the subject should i still lower case all the words? i personally think about creating more classes and  distinction between sentiments as good , very good than just positive to include the importance of this upper case words .
this is my current code :
from itertools import chain

def is_upper_case(text):
  return [word for word in text.split() if word.isupper() and word != 'i']

unique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case)))
print(unique_upper_words)","['nlp', 'sentiment-analysis', 'bert-language-model', 'data-preprocessing']",78933236,"if you are using a bert-based model (or any other llm) to do the actual classification i would recommend to not use any preprocessing at all (at least when it comes to capitalization), as these models were pre-trained on non-preprocessed data.
if you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group n-grams and to simplify the analysis.
if you are thinking about having multiple classes to have a better distinction between the prediction, i think it would make most sense if you switch to a sentiment regression instead of a classification, where you predict a value in a continuous range. this comes somewhat natural to the fine-tuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax, so for your needs you can just skip that last step and directly use the model output. many python ml frameworks for fine-tuning or using language models have their own classes for regression tasks, check out this repository as an example.",https://stackoverflow.com/questions/78932356,nlp,30-08-2024 13:49,136.0,-1.0,1.0,True,30-08-2024 17:50,30-08-2024 13:51
69701493,nltk tagging specific words,"i'm new to ntlk and python overall. i understand you can insert a sentence and have all the words tagged to a part of speech, but i'm trying to figure out how to tag for a specific part of speech. for example, i want to only tag nouns and verbs, how would i go about doing that? this is probably a dumb question but i've been researching for a bit and still am very confused. i'd be great if i could get a few lines of code that i can edit to tag for specific parts of speech.","['python', 'nltk']",69701511,"import nltk
from nltk import word_tokenize
nltk.download('punkt')

text = word_tokenize(""and now for something completely different"")
tagged = nltk.pos_tag(text)
print(tagged)
verbs_nouns = [w[0] for w in tagged if w[1]=='vp' or w[1]=='nn']
print(verbs_nouns)

in that case just filter your tagged list for whatever parts you'd like.",https://stackoverflow.com/questions/69701493,python,24-10-2021 23:26,353.0,0.0,1.0,True,24-10-2021 23:49,24-10-2021 23:31
4361971,convert chinese characters to hanyu pinyin,"how to convert from chinese characters to hanyu pinyin?
e.g.
ï¿½ï¿½ï¿½ --> nï¿½ï¿½
ï¿½ï¿½ï¿½ --> mï¿½ï¿½

more info:
either accents or numerical forms of hanyu pinyin are acceptable, the numerical form being my preference.
a java library is preferred, however, a library in another language that can be put in a wrapper is also ok.
i would like anyone who has personally used such a library before to recommend or comment on it, in terms of its quality","language-agnostic, nlp, cjk",4458498,"the problem of converting hanzi to pinyin is a fairly difficult one. there are many hanzi characters which have multiple pinyin representations, depending on context. compare ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ (pinyin: zhang da) to ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ (pinyin: chang cheng). for this reason, single-character conversion is often actually useless, unless you have a system that outputs multiple possibilities. there is also the issue of word segmentation, which can affect the pinyin representation as well. though perhaps you already knew this, i thought it was important to say this.
adso package contains both a segmenter and a probabilistic pinyin annotator, based on the excellent adso library. it takes a while to get used to though, and may be much larger than you are looking for (i have found in the past that it was a bit too bulky for my needs). additionally, there doesn't appear to be a public api anywhere, and its c++ ...
for a recent project, because i was working with place names, i simply used the google translate api (specifically, the unofficial java port, which, for common nouns at least, usually does a good job of translating to pinyin. the problem is commonly-used alternative transliteration systems, such as ""hongkong"" for what should be ""xianggang"". given all of this, google translate is pretty limited, but it offers a start. i hadn't heard of pinyin4j before, but after playing with it just now, i have found that it is less than optimal--while it outputs a list of potential candidate pinyin romanizations it makes no attempt to statistically determine their likelihood. there is a method to return a single representation, but it will soon be phased out, as it currently only returns the first romanization, not the most likely. where the program seems to do well is with conversion between romanizations and general configurability. 
in short then, the answer may be either any one of these, depending on what you need. idiosyncratic proper nouns? google translate. in need of statistics? adso. willing to accept candidate lists without context information? pinyin4j.",https://stackoverflow.com/q/4361971,"language-agnostic, nlp, cjk",05-12-2010 23:27,4651.0,5.0,4.0,True,22-12-2022 21:50,16-01-2018 08:20
78489329,how to filter documents based on a list of metadata in langchain&#39;s chroma vectorstore?,"i'm working with langchain's chroma vectorstore, and i'm trying to filter documents based on a list of document names.
i have a list of document names as follows:
lst = ['doc1', 'doc2', 'doc3']

i also have doc_name metadata in my vectorstore. currently, iï¿½ï¿½ï¿½m using the following code to retrieve documents:
base_retriever = chroma_db.as_retriever(search_kwargs={'k': 10})

however, iï¿½ï¿½ï¿½m not sure how to modify this code to filter documents based on my list of document names. could anyone guide me on how to achieve this? any help would be greatly appreciated","['python', 'metadata', 'langchain', 'chromadb', 'vector-database']",78515796,"# define your list of document names
lst = ['doc1', 'doc2', 'doc3']

# create a filter dictionary to filter by document names
filter_dict = {""name"": {""$in"": lst}}

# modify the as_retriever method to include the filter in search_kwargs
base_retriever = chroma_db.as_retriever(search_kwargs={'k': 10, 'filter': filter_dict})

# now you can use the retriever to search with the filter applied
query = ""your search query""
results = base_retriever.invoke(query)
for result in results:
    print(result)",https://stackoverflow.com/questions/78489329,python,16-05-2024 10:49,9574.0,0.0,2.0,True,22-05-2024 08:38,22-05-2024 08:13
75976909,avoiding trimmed summaries of a pegasus-pubmed huggingface summarization model,"i am new to huggingface.
i am using pegasus - pubmed huggingface model to generate summary of the reserach paper. following is the code for the same. the model gives a trimmed summary.
any way of avoiding the trimmed summaries and getting more concrete results in summarization.?
following is the code that i tried.
#loading pubmed dataset for scientifc articles

dataset_pubmed = load_dataset(""scientific_papers"",""pubmed"")

#taking piece of  train dataset

sample_dataset = dataset_pubmed[""train""]
sample_dataset

#taking first two articles of train dataset
sample_dataset = sample_dataset['article'][:2]
sample_dataset

###import pegasusmodel and tokenizer

from transformers import pipeline, pegasustokenizer, pegasusforconditionalgeneration


model = pegasusforconditionalgeneration.from_pretrained('google/pegasus-pubmed')
tokenizer =pegasustokenizer.from_pretrained('google/pegasus-pubmed')

summerize_pipe = pipeline(""summarization"", model=model, tokenizer=tokenizer)
pipe_out = summerize_pipe(sample_dataset, truncation=true)
pipe_out

as a results of this one of the summary output i get is as follows. the last sentence is not complete it gets trimmed for all the papers. how to avoid this.?
[{'summary_text': ""background : in iran a national free food program ( nffp ) is implemented in elementary schools of deprived areas to cover all poor students . however , this program is not conducted in slums and poor areas of the big cities so many malnourished children with low socio - economic situation are not covered by nffp . therefore , the present study determines the effects of nutrition intervention in an advocacy process model on the prevalence of underweight in school aged children in the poor area of shiraz , iran.materials and methods : this interventional study has been carried out between 2009 and 2010 in shiraz , iran . in those schools all students ( 2897 , 7 - 13 years old ) were screened based on their body mass index ( bmi ) by nutritionists . according to convenience method all students divided to two groups based on their economic situation ; family revenue and head of household 's job and nutrition situation ; the first group were poor and malnourished students and the other group were well nourished or well - off students . for this report , the children 's height and weight were entered into center for disease control and prevention ( cdc ) to calculate bmi and bmi - for -""}","['pytorch', 'nlp', 'huggingface-transformers', 'huggingface-tokenizers', 'huggingface']",75977633,"you should increase the max_length to a larger value, such as 1024 or 2048:
summerize_pipe = pipeline(""summarization"", model=model, tokenizer=tokenizer, max_length=1024)",https://stackoverflow.com/questions/75976909,pytorch,10-04-2023 12:00,112.0,1.0,1.0,True,10-04-2023 13:56,10-04-2023 13:56
77021885,attributeerror: &#39;str&#39; object has no attribute &#39;is_context_set&#39;,"i am trying to implement flair to obtain ner tags for a piece of text (doing this in colab).  encountered this error while trying tagger.predict(text). what should i do to resolve this?
here's my code:
from flair.data import sentence
from flair.models import sequencetagger

text = ""apple is headquartered in cupertino, california.""
tagger = sequencetagger.load(""flair/ner-english"")

tagger.predict(text)

here's what i got:
2023-09-01 09:00:29,790 sequencetagger predicts: dictionary with 20 tags: <unk>, o, s-org, s-misc, b-per, e-per, s-loc, b-org, e-org, i-per, s-per, b-misc, i-misc, e-misc, i-org, b-loc, e-loc, i-loc, <start>, <stop>
---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-17-02be6f83cf90> in <cell line: 8>()
      6 
      7 
----> 8 tagger.predict(text)
      9 print(text)
     10 print('the following ner tags are found:')

1 frames
/usr/local/lib/python3.10/dist-packages/flair/models/sequence_tagger_model.py in predict(self, sentences, mini_batch_size, return_probabilities_for_all_classes, verbose, label_name, return_loss, embedding_storage_mode, force_token_predictions)
    454                 sentences = [sentences]
    455 
--> 456             sentence.set_context_for_sentences(cast(list[sentence], sentences))
    457 
    458             # filter empty sentences

/usr/local/lib/python3.10/dist-packages/flair/data.py in set_context_for_sentences(cls, sentences)
   1087         previous_sentence = none
   1088         for sentence in sentences:
-> 1089             if sentence.is_context_set():
   1090                 continue
   1091             sentence._previous_sentence = previous_sentence

attributeerror: 'str' object has no attribute 'is_context_set'","['python', 'machine-learning', 'nlp', 'named-entity-recognition', 'flair']",77021955,"turn your text into a sentence first so the underlying code has access to the necessary functions and modify the sentence object.
from flair.data import sentence
from flair.models import sequencetagger

text = ""apple is headquartered in cupertino, california.""
sentence = sentence(text) # <---
tagger = sequencetagger.load(""flair/ner-english"")

tagger.predict(sentence ) # <---
print(sentence)
...",https://stackoverflow.com/questions/77021885,python,01-09-2023 09:12,293.0,0.0,1.0,True,01-09-2023 22:14,01-09-2023 22:14
73402366,creating new column using regex if certain keywords are found in other column values,"i have dataframe (df) column called a which is string
 index   a
----------------------------
  0      boy_was_born_in_2010
  1      men_was_born_in_1997
  2      girl_this_is_2022
  3      this_is_a_lady
  4      how_tall_is_this_boy
  5      girl_is_studying

now i wrote a code which would identify specific words like boy, girl, men, lady if it found that keyword in the column a create a new column b, given below
which is if the string a contain boy then b will have kid as a value and contain men->male, girl->kid, lady->female
 index   a                         b
------------------------------------------
  0      boy_was_born_in_2010      kid
  1      men_was_born_in_1997      male
  2      girl_this_is_2022         kid
  3      this_is_a_lady            female
  4      how_tall_is_this_boy      kid
  5      girl_is_studying          kid

i have used the following code
df['b']=df.a.str.findall('boy').transform(''.join).replace('boy','kid')

this is working fine but when i apply it for other rows than the previously applied value which is the above somehow gets undo, moreover i dont think this is a optimized way
also tried this
df['b'] = df['a'].replace(to_replace ='^boy\w', value = 'kid', regex = true) # not working

if you have anyother way of doing it please suggest
i'am a complete beginner just started working on nlp projects","['python', 'pandas', 'regex', 'dataframe', 'nlp']",73402826,"adding an example with multiple matches:
import pandas as pd

data = {'a': ['boy_was_born_in_2010',
  'men_was_born_in_1997',
  'girl_this_is_2022',
  'this_is_a_lady',
  'how_tall_is_this_boy',
  'girl_is_studying',
  'boy meets men']}

df = pd.dataframe(data)
print(df)

                      a
0  boy_was_born_in_2010
1  men_was_born_in_1997
2     girl_this_is_2022
3        this_is_a_lady
4  how_tall_is_this_boy
5      girl_is_studying
6         boy meets men

set up a dict for mapping, use str.extract on pattern with alternatives, sqeeuze the result, and finally apply map:
a_dict = {'boy':'kid',
          'men':'male',
          'girl':'kid',
          'lady':'female'}

pattern = '(' + '|'.join(list(a_dict)) + ')'
df['b'] = df.a.str.extract(pattern).squeeze().map(a_dict)

print(df)

                      a       b
0  boy_was_born_in_2010     kid
1  men_was_born_in_1997    male
2     girl_this_is_2022     kid
3        this_is_a_lady  female
4  how_tall_is_this_boy     kid
5      girl_is_studying     kid
6         boy meets men     kid

above method will only get you the first match. e.g. boy -> kid for last string. if you want to get all matches, we can use something like this:
temp = df.a.str.extractall(pattern).squeeze().map(a_dict)
df['b'] = temp.groupby(level=0).agg(', '.join)

print(df)

                      a          b
0  boy_was_born_in_2010        kid
1  men_was_born_in_1997       male
2     girl_this_is_2022        kid
3        this_is_a_lady     female
4  how_tall_is_this_boy        kid
5      girl_is_studying        kid
6         boy meets men  kid, male",https://stackoverflow.com/questions/73402366,python,18-08-2022 11:33,714.0,0.0,2.0,True,18-08-2022 12:07,18-08-2022 11:35
79021544,removing strange/special characters from outputs llama 3.1 model,"background: i'm using hugging face's transformers package and llama 3.1 8b (instruct).
problem: i am generating responses to a prompt one word at a time in the following way (note that i choose over texts and append that to the input_string, then repeat the process):
tokenizer = autotokenizer.from_pretrained(model_path, use_safetensors=true)
model = automodelforcausallm.from_pretrained(model_path, use_safetensors=true)

input_ids = tokenizer.encode(input_string, return_tensors=""pt"") # tokenize to ids
logits = model(input_ids).logits # call model() to get logits
logits = logits[-1, -1] # only care about the last projection in the last batch
probs = torch.nn.functional.softmax(logits, dim=-1) # softmax() to get probabilities
probs, ids = torch.topk(probs, 5) # keep only the top 5
texts = tokenizer.convert_ids_to_tokens(ids) # convert ids to tokens

but i notice many strange or special characters appearing in the output. for example, the following is the literal string returned from input_string = ""how often should i wear a seatbelt?"":
ï¿½ï¿½always.ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½always,ï¿½ï¿½unlessï¿½ï¿½youï¿½ï¿½areï¿½ï¿½ï¿½ï¿½inï¿½ï¿½ï¿½ï¿½aï¿½ï¿½ï¿½ï¿½carï¿½ï¿½ï¿½ï¿½thatï¿½ï¿½ï¿½ï¿½isï¿½ï¿½ï¿½ï¿½notï¿½ï¿½ï¿½ï¿½moving.

is there any way to easily remove strange special characters?
i've tried using options on the decoder (in every possible t/f combo), such as the following:
mystr = 'ï¿½ï¿½always.ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½always,ï¿½ï¿½unlessï¿½ï¿½youï¿½ï¿½areï¿½ï¿½ï¿½ï¿½inï¿½ï¿½ï¿½ï¿½aï¿½ï¿½ï¿½ï¿½carï¿½ï¿½ï¿½ï¿½thatï¿½ï¿½ï¿½ï¿½isï¿½ï¿½ï¿½ï¿½notï¿½ï¿½ï¿½ï¿½moving.'
t","['python', 'huggingface-transformers', 'tokenize', 'large-language-model', 'llama']",79029236,"tl;dr
use this instead of rolling out your own detokenizer.
tokenizer.batch_decode(input_ids)

in long
the official llama 3.1 has some approval process that might take some time, so this answer will use a proxy model that shares the same tokenizer as llama 3.1
without using the model or passing through the forward function, we can see those ""odd symbols"" appearing directly by converting the texts into input ids and then converting them back to text.
you'll see that consistently there's this ï¿½ï¿½ symbol added.
from transformers import autotokenizer
import torch

model_path = ""neuralmagic/meta-llama-3.1-405b-instruct-fp8""
tokenizer = autotokenizer.from_pretrained(model_path, use_safetensors=true)

input_string = ""always. always, unless you are in a car that is not moving""
input_ids = tokenizer.encode(input_string, return_tensors=""pt"") # tokenize to ids
texts = tokenizer.convert_to_tokens(input_ids.squeeze()) # convert ids to tokens

print(texts)

[out]:
['<|begin_of_text|>',
 'always',
 '.',
 'ï¿½ï¿½always',
 ',',
 'ï¿½ï¿½unless',
 'ï¿½ï¿½you',
 'ï¿½ï¿½are',
 'ï¿½ï¿½in',
 'ï¿½ï¿½a',
 'ï¿½ï¿½car',
 'ï¿½ï¿½that',
 'ï¿½ï¿½is',
 'ï¿½ï¿½not',
 'ï¿½ï¿½moving']

it seems like ï¿½"" rel=""nofollow noreferrer"">sentencepiece uses the ""ï¿½ï¿½ï¿½"" (u+2581) symbol.
so where does that ï¿½ï¿½ come from?
lets first try printing out the vocab, and you'll see these non-natural text characters appearing everywhere:
print(tokenizer.vocab)

[out]:
{'icc': 48738,
 'ï¿½ï¿½carly': 79191,
 'ï¿½ï¿½bot': 83430,
 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½': 118849,
 'depends': 59047,
 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½': 120010,
 'ï¿½ï¿½dolphin': 96096,
 'ï¿½ï¿½datatype': 23082,
 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½': 116811,
 'ï¿½ï¿½me': 757,
 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½': 84659,
 '.secondary': 70156,
 'ï¿½ï¿½axes': 90804,
 'pn': 18378,
 'ï¿½ï¿½flav': 18779,
 'ï¿½ï¿½hp': 21280,
 '(module': 76395,
 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï"" rel=""nofollow noreferrer""> and 
the root of this ï¿½ï¿½evil comes from 
so how do i get the decoded tokens in natural text?
try this:
from transformers import autotokenizer
import torch

tokenizer = autotokenizer.from_pretrained(model_path, use_safetensors=true)

input_string = ""always. always, unless you are in a car that is not moving""
input_ids = tokenizer.encode(input_string, return_tensors=""pt"") # tokenize to ids
texts = tokenizer.convert_ids_to_tokens(input_ids.squeeze()) 

tokenizer.batch_decode(input_ids) # convert ids to natural text.

[out]:
['<|begin_of_text|>always. always, unless you are in a car that is not moving']

and to remove the special bos token,
tokenizer.batch_decode(input_ids, skip_special_tokens=true, clean_up_tokenization_spaces=true)

[out]:
['always. always, unless you are in a car that is not moving']",https://stackoverflow.com/questions/79021544,python,25-09-2024 07:14,1231.0,3.0,1.0,True,26-09-2024 22:12,26-09-2024 22:12
79279995,c# asp.net mvc web app - update view after completion of asynch calls w/ openai,"i have an asp.net mvc app in c# that i'm trying to asynchronously call a bunch of openai requests in parallel and then wait on all results to return.
the following is a snippet from this code:
        foreach (chatmessage msg in messages)
        {
            list<chatmessage> tmpmessage = new list<chatmessage>();
            tmpmessage.add(msg);

            task<clientresult<chatcompletion>> chatresult = client.completechatasync(tmpmessage, chatoptions);

            await chatresult;

            if (chatresult.iscompleted && chatresult.result.value != null)
            {
                output.add(chatresult.result.value.content[0].text);
            }
        }

each string in the output list is a json structured output from openai. i then go through each json output and manipulate it as needed.
my questions are: is this truly asynchronous? i call the completechatasync but then i use await chatresult and i am uncertain if this is the way to do that asynchronously?
second - the view portion of the webpage does not update once all results return and are processed. it just sits there. how do i refresh the view in the asp.net mvc web app in .net?
thanks!","['c#', 'openapi', 'openai-api', 'chatgpt-api']",79280072,"to run several tasks ""in parallel"" (more correctly: concurrently), you can use task.whenall(listoftasks). the steps will be as follow:

create the list of tasks (without awaiting them)
fire them and wait for all of them to finish.
handle the results.

example:
// step 1
var allchatresults = new list<task<clientresult<chatcompletion>>>();
foreach (chatmessage msg in messages)
{
    var chatresult = client.completechatasync(tmpmessage, chatoptions);
    allchatresults.add(chatresult);
}

// step 2
await task.whenall(allchatresults);

// step 3
foreach (task chatresult in allchatresults)
{
    if (chatresult.iscompleted && chatresult.result.value != null)
    {
        output.add(chatresult.result.value.content[0].text);
    }
}",https://stackoverflow.com/questions/79279995,c#,14-12-2024 03:05,81.0,2.0,1.0,True,14-12-2024 05:15,14-12-2024 05:15
76741896,properly count amount of tokens in the whole request payload - openai,"17.10.24: title edited for easier search
original title: what part of openai api request payload is limited by the max amount tokens?

i kinda understand how to count tokens out of characters, but what do i actually have to count?
if i have a payload like this:
{
  ""model"": ""gpt-3.5-turbo"",
  ""temperature"": 1,
  ""max_tokens"": 400,
  ""presence_penalty"": 0.85,
  ""frequency_penalty"": 0.85,
  ""messages"": [
    {
      ""role"": ""system"",
      ""content"": ""prompt""
    },
    {
      ""role"": ""assistant"",
      ""content"": ""message""
    },
    // tens of messages
  ]
}

do i have to count tokens out of it entirely? or do i have to count it in ""messages"" only? if so, do i have to count all the json syntax characters, like spacebars, brackets and commas too? what about ""role"" and ""content"" keys? what about ""role"" value?
or i have to simply concat all the ""content"" values into a single string and count tokens based only on it?","['openai-api', 'chatgpt-api', 'gpt-3', 'gpt-4']",77002084,"from my understanding and calculations, all the tokens in the list provided in ""messages"" are counted. this includes the keys ""role"" and ""content"" and their values but does not include spaces, brackets, commas, and quotes.
i use the following script provided by openai to calculate the number of tokens in my input. i have modified the script to calculate the cost involved with the input for multiple messages (not the output response) and it's been fairly accurate for me.
import json
import os
import tiktoken
import numpy as np
from collections import defaultdict

def num_tokens_from_messages(messages, model=""gpt-3.5-turbo-0613""):
    """"""return the number of tokens used by a list of messages.""""""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except keyerror:
        print(""warning: model not found. using cl100k_base encoding."")
        encoding = tiktoken.get_encoding(""cl100k_base"")
    if model in {
        ""gpt-3.5-turbo-0613"",
        ""gpt-3.5-turbo-16k-0613"",
        ""gpt-4-0613"",
        ""gpt-4-32k-0613"",
        }:
        tokens_per_message = 3
        tokens_per_name = 1
    elif model == ""gpt-3.5-turbo-0301"":
        tokens_per_message = 4  # every message follows <|start|>{role/name}\n{content}<|end|>\n
        tokens_per_name = -1  # if there's a name, the role is omitted
    elif ""gpt-3.5-turbo"" in model:
        print(""warning: gpt-3.5-turbo may update over time. returning num tokens assuming gpt-3.5-turbo-0613."")
        return num_tokens_from_messages(messages, model=""gpt-3.5-turbo-0613"")
    elif ""gpt-4"" in model:
        print(""warning: gpt-4 may update over time. returning num tokens assuming gpt-4-0613."")
        return num_tokens_from_messages(messages, model=""gpt-4-0613"")
    else:
        raise notimplementederror(
            f""""""num_tokens_from_messages() is not implemented for model {model}. see  for information on how messages are converted to tokens.""""""
        )
    num_tokens = 0
    for message in messages:
        num_tokens += tokens_per_message
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
            if key == ""name"":
                num_tokens += tokens_per_name
    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>
    return num_tokens

convo_lens = []

for ex in dataset: #your list of inputs
    messages = ex[""messages""]
    convo_lens.append(num_tokens_from_messages(messages))

n_input_tokens_in_dataset = sum(min(4096, length) for length in convo_lens)
print(f""input portion of the data has ~{n_input_tokens_in_dataset} tokens"")

# costs as of aug 29 2023.
costs = {
    ""gpt-4-0613"": {
        ""input"" : 0.03,
        ""output"": 0.06
    },
    ""gpt-4-32k-0613"": {
        ""input"" : 0.06,
        ""output"": 0.12
    },
    ""gpt-3.5-turbo-0613"": {
        ""input"": 0.0015,
        ""output"": 0.002
    },

    ""gpt-3.5-turbo-16k-0613"": {
        ""input"": 0.003,
        ""output"": 0.004
    }
}

# we select gpt 3.5 turbo here
print(f""cost of inference: ${(n_input_tokens_in_dataset/1000) * costs['gpt-3.5-turbo-0613']['input']}"")",https://stackoverflow.com/questions/76741896,openai-api,22-07-2023 00:38,1062.0,1.0,1.0,True,17-10-2024 13:04,17-10-2024 13:04
61049310,how to avoid reloading ml model every time when i call python script?,"i have two files, file1.py which have ml model size of 1gb and file2.py which calls get_vec() method from file1 and receives vectors in return. ml model is being loaded everytime when file1 get_vec() method is called. this is where it is taking lots of time (around 10s) to load the model from disk.
i want to tell file1 somehow not to reload model every time but utilize loaded model from earlier calls.
sample code is as follows
# file1.py

import spacy
nlp = spacy.load('model')

def get_vec(post):
    doc = nlp(post)
    return doc.vector


file2.py

from file1 import get_vec

df['vec'] = df['text'].apply(lambda x: get_vec(x))


so here, it is taking 10 to 12 seconds in each call. this seems small code but it is a part of a large project and i can not put both in the same file. 
update1:
i have done some research and came to know that i can use redis to store model in cache first time it runs and thereafter i can read the model from cache directly. i tried it for testing with redis as follows
import spacy
import redis

nlp = spacy.load('en_core_web_lg')
r = redis.redis(host = 'localhost', port = 6379, db = 0)
r.set('nlp', nlp)

it throws an error
dataerror: invalid input of type: 'english'. convert to a bytes, string, int or float first.

seems, type(nlp) is english() and it need to convert in a suitable format. so i tried to use pickle as well to convert it. but again, pickle is taking lots of time in encoding and decoding. is there anyway to store this in redis?
can anybody suggest me how can i make it faster? thanks.","['python', 'machine-learning', 'redis', 'nlp', 'spacy']",61057688,"heres how to do it
step 1) create a function in python and load your model in that function
model=none
def load_model():

    global model
    model = resnet50(weights=""imagenet"")

if you carefully observe first i assigned variable model to none. then inside load_model function i loaded a model. 
also i made sure the variable model is made global so that it can be accessed from outside this function. the intuition here is we load model object in a global variable. so that we can access this variable anywhere within the code. 
now that we have our tools ready (i.e we can access the model from anywhere within this code ) lets freeze this model in your computers ram. this is done by:
if __name__ == ""__main__"":
    print((""* loading keras model and flask starting server...""
        ""please wait until server has fully started""))
    load_model()
    app.run()

now what's the use of freezing model in ram without using it. so, to use it i use post request in flask
@app.route(""/predict"", methods=[""post""])
def predict():

    if flask.request.method == ""post"":

            output=model.predict(data)  #what you want to do with frozen model goes here

so using this trick you can freeze model in ram, access it using a global variable. and then use it in your code.",https://stackoverflow.com/questions/61049310,python,05-04-2020 20:49,7425.0,10.0,5.0,True,27-12-2022 10:00,06-04-2020 10:56
74851128,language detection for short user-generated string,"i need to detect the language of text sent in chat, and i am faced with 2 problems:

the length of the message
the errors that may be in it and the noise (emoji etc...)

for the noise, i clean the message and that works fine, but the length of the message is a problem.
for example, if a user writes ""hi"", fasttext detects the language as dutch text, but google translate detects it as english. and most likely it is a message in english.
i try to train my own fasttext model, but how can i adjust the model to have better results with short strings? do i need to train the model with the dictionary of a lot of languages to get a better result?
i use fasttext because it's the most accurate language detector.
here is an exemple of the problem with fasttext:
# wget 

import fasttext

text = ""hi""

pretrained_lang_model = ""lid.176.bin""
model = fasttext.load_model(pretrained_lang_model)

predictions = model.predict(text, k=2)
print(predictions)
# (('__label__de', '__label__en'), array([0.51606238, 0.31865335]))","['python', 'nlp', 'fasttext', 'language-detection']",74972235,"i have found a way to have better results. if you sum all probabilities of all languages on different detectors like fasttext and lingua, and add a dictionary-based detection for short texts, you can have very good results (for my task, i also made a fasttext model trained on my data).",https://stackoverflow.com/questions/74851128,python,19-12-2022 13:20,3171.0,3.0,2.0,True,29-10-2023 09:38,29-10-2023 09:38
65341552,is the word2vec spark implementation distributed?,"i'm relatively new to spark and having some difficulty understanding spark ml.
the problem i have is that i have 3tb of text, which i want to train a word2vec model on. the server i'm running on has around 1tb of ram and so i can't save the file temporarily.
the file is saved as a parquet that i import into spark. the question i have is does the spark ml library distribute the word2vec training? if so is there anything i need to worried about while processing such a large text file? if not, is there anyway to stream this data while training word2vec?","['apache-spark', 'pyspark', 'nlp', 'word2vec', 'apache-spark-mllib']",65346885,"from this   already in 2014 you can glean that parallel processing is possible - per partition.

quote:
to make our implementation more scalable, we train each partition
separately and merge the model of each partition after each iteration.
to make the model more accurate, multiple iterations may be needed.

but you have to have partitoned data.",https://stackoverflow.com/questions/65341552,apache-spark,17-12-2020 13:21,466.0,0.0,1.0,True,26-05-2023 22:11,26-05-2023 22:11
54924582,is it possible to freeze only certain embedding weights in the embedding layer in pytorch?,"when using glove embedding in nlp tasks, some words from the dataset might not exist in glove. therefore, we instantiate random weights for these unknown words.
would it be possible to freeze weights gotten from glove, and train only the newly instantiated weights?
i am only aware that we can set:
model.embedding.weight.requires_grad = false
but this makes the new words untrainable..
or are there better ways to extract semantics of words..","['python', 'nlp', 'pytorch', 'word-embedding', 'glove']",54952825,"1. divide embeddings into two separate objects
one approach would be to use two separate embeddings one for pretrained, another for the one to be trained.
the glove one should be frozen, while the one for which there is no pretrained representation would be taken from the trainable layer.
if you format your data that for pretrained token representations it is in smaller range than the tokens without glove representation it could be done. let's say your pretrained indices are in the range [0, 300], while those without representation are [301, 500]. i would go with something along those lines:
import numpy as np
import torch


class yournetwork(torch.nn.module):
    def __init__(self, glove_embeddings: np.array, how_many_tokens_not_present: int):
        self.pretrained_embedding = torch.nn.embedding.from_pretrained(glove_embeddings)
        self.trainable_embedding = torch.nn.embedding(
            how_many_tokens_not_present, glove_embeddings.shape[1]
        )
        # rest of your network setup

    def forward(self, batch):
        # which tokens in batch do not have representation, should have indices bigger
        # than the pretrained ones, adjust your data creating function accordingly
        mask = batch > self.pretrained_embedding.num_embeddings

        # you may want to optimize it, you could probably get away without copy, though
        # i'm not currently sure how
        pretrained_batch = batch.copy()
        pretrained_batch[mask] = 0

        embedded_batch = self.pretrained_embedding(pretrained_batch)

        # every token without representation has to be brought into appropriate range
        batch -= self.pretrained_embedding.num_embeddings
        # zero out the ones which already have pretrained embedding
        batch[~mask] = 0
        non_pretrained_embedded_batch = self.trainable_embedding(batch)

        # and finally change appropriate tokens from placeholder embedding created by
        # pretrained into trainable embeddings.
        embedded_batch[mask] = non_pretrained_embedded_batch[mask]

        # rest of your code
        ...

let's say your pretrained indices are in the range [0, 300], while those without representation are [301, 500].
2. zero gradients for specified tokens.
this one is a bit tricky, but i think it's pretty concise and easy to implement. so, if you obtain the indices of tokens which got no glove representation, you can explicitly zero their gradient after backprop, so those rows will not get updated.
import torch

embedding = torch.nn.embedding(10, 3)
x = torch.longtensor([[1, 2, 4, 5], [4, 3, 2, 9]])

values = embedding(x)
loss = values.mean()

# use whatever loss you want
loss.backward()

# let's say those indices in your embedding are pretrained (have glove representation)
indices = torch.longtensor([2, 4, 5])

print(""before zeroing out gradient"")
print(embedding.weight.grad)

print(""after zeroing out gradient"")
embedding.weight.grad[indices] = 0
print(embedding.weight.grad)

and the output of the second approach:
before zeroing out gradient
tensor([[0.0000, 0.0000, 0.0000],
        [0.0417, 0.0417, 0.0417],
        [0.0833, 0.0833, 0.0833],
        [0.0417, 0.0417, 0.0417],
        [0.0833, 0.0833, 0.0833],
        [0.0417, 0.0417, 0.0417],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0417, 0.0417, 0.0417]])
after zeroing out gradient
tensor([[0.0000, 0.0000, 0.0000],
        [0.0417, 0.0417, 0.0417],
        [0.0000, 0.0000, 0.0000],
        [0.0417, 0.0417, 0.0417],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0417, 0.0417, 0.0417]])",https://stackoverflow.com/questions/54924582,python,28-02-2019 11:23,8944.0,16.0,1.0,True,15-04-2023 15:37,28-02-2019 11:28
72920750,error getting prediction explanation using shap_values when using scikit-learn pipeline?,"i am building an nlp model to predict language type (c/c++/c#/python...) for a given code.
now i need to provide an explanation for my model prediction. for example the following user_input is written in java and the model is predicting that, but i need to show the users why it predicts so.
i am using shap_values to achieve this.
for some reason, the following code results in an error (i have added the error at the bottom).
please advise how can i get shap_values and plots for my model predictions.
link to data: 
code:
import pandas as pd

from sklearn.feature_extraction.text import tfidfvectorizer
from sklearn.preprocessing import functiontransformer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import randomforestclassifier
from sklearn.pipeline import pipeline

# loading data:
data_path = r""sample.csv""

data = pd.read_csv(data_path, dtype='object')
data = data.convert_dtypes()
data = data.dropna()
data = data.drop_duplicates()

# train/test split
x, y = data.content, data.language
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y)

# model params to match:
# 1. variable and module names, words in a string, keywords: [a-za-z_]\w*\b
# 2. operators: [!\#\$%\&\*\+:\-\./<=>\?@\\\^_\|\~]+
# 3. tabs, spaces and brackets: [ \t\(\),;\{\}\[\]`""']
# with the following regex:
token_pattern = r""""""(\b[a-za-z_]\w*\b|[!\#\$%\&\*\+:\-\./<=>\?@\\\^_\|\~]+|[ \t\(\),;\{\}\[\]`""'])""""""


def preprocess(x):
 """""" clean up single-character variable names or ones constituted of a sequence of the same character """"""
 return pd.series(x).replace(r'\b([a-za-z])\1+\b', '', regex=true)\
 .replace(r'\b[a-za-z]\b', '', regex=true)


# pipe steps:
# define a transformer:
transformer = functiontransformer(preprocess)
# perform tf-idf vectorization with our token pattern:
vectorizer = tfidfvectorizer(token_pattern=token_pattern, max_features=3000)
# create random forest classifier:
clf = randomforestclassifier(n_jobs=4)

pipe_rf = pipeline([
 ('preprocessing', transformer),
 ('vectorizer', vectorizer),
 ('clf', clf)]
)

# setting best params (after performing gridsearchcv)
best_params = {
 'clf__criterion': 'gini',
 'clf__max_features': 'sqrt',
 'clf__min_samples_split': 3,
 'clf__n_estimators': 300
}

pipe_rf.set_params(**best_params)

# fitting
pipe_rf.fit(x_train, y_train)

# evaluation
print(f'accuracy: {pipe_rf.score(x_test, y_test)}')



user_input = ["""""" public class fibonacci {

public static void main(string[] args) {

int n = 10;

system.out.println(fib(n));

}

public static int fib(int n) {

if (n <= 1) {

return n;

}

return fib(n - 1) + fib(n - 2);

}

} """"""]


import shap

shap.initjs()
explainer = shap.treeexplainer(pipe_rf.named_steps['clf'])
observation = pipe_rf[:-1].transform(user_input).toarray()
shap_values = explainer.shap_values(observation)

load the data and run it to get the following error:

explainererror: additivity check failed in treeexplainer! please
ensure the data matrix you passed to the explainer is the same shape
that the model was trained on. if your data shape is correct then
please report this on github. consider retrying with the
feature_perturbation='interventional' option. this check failed
because for one of the samples the sum of the shap values was
46609069202029743624438153216.000000, while the model output was 0.004444. if this difference is acceptable you can set check_additivity=false to disable this check.","['python-3.x', 'scikit-learn', 'nlp', 'pipeline', 'shap']",72929720,"i have figured out how to fix it, posting to help others :)
import pandas as pd

from sklearn.feature_extraction.text import tfidfvectorizer
from sklearn.preprocessing import functiontransformer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import randomforestclassifier
from sklearn.pipeline import pipeline

import re
from lime.lime_text import limetextexplainer

from ipython.core.interactiveshell import interactiveshell
interactiveshell.ast_node_interactivity = ""all""

# loading github repos data containing code and comments from 2.8 million github repositories:
data_path = r""/users/stevesolun/steves_files/data/github_repos_data.csv""

data = pd.read_csv(data_path, dtype='object')
data = data.convert_dtypes()
data = data.dropna()
data = data.drop_duplicates()

# train/test split
x, y = data.content, data.language
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y)

# model params to match:
# 1. variable and module names, words in a string, keywords: [a-za-z_]\w*\b
# 2. operators: [!\#\$%\&\*\+:\-\./<=>\?@\\\^_\|\~]+
# 3. tabs, spaces and brackets: [ \t\(\),;\{\}\[\]`""']
# with the following regex:
token_pattern = r""""""(\b[a-za-z_]\w*\b|[!\#\$%\&\*\+:\-\./<=>\?@\\\^_\|\~]+|[ \t\(\),;\{\}\[\]`""'])""""""


def preprocess(x):
    """""" clean up single-character variable names or ones constituted of a sequence of the same character """"""
    return pd.series(x).replace(r'\b([a-za-z])\1+\b', '', regex=true)\
        .replace(r'\b[a-za-z]\b', '', regex=true)


# pipe steps:
# define a transformer:
transformer = functiontransformer(preprocess)
# perform tf-idf vectorization with our token pattern:
vectorizer = tfidfvectorizer(token_pattern=token_pattern, max_features=1500)
# create random forest classifier:
clf = randomforestclassifier(n_jobs=-1)

pipe_rf = pipeline([
     ('preprocessing', transformer),
     ('vectorizer', vectorizer)]
    )

# setting best params (after performing gridsearchcv)
best_params = {
    'criterion': 'gini',
    'max_features': 'sqrt',
    'min_samples_split': 3,
    'n_estimators': 300
}

clf.set_params(**best_params)
# here i am preprocessing the data:
x_train = pipe_rf.fit_transform(x_train).toarray()
x_test = pipe_rf.transform(x_test).toarray()

# fitting the model outside the pipe - feel free to show if possible to do it inside the pipe + fit_transform the train and test sets.
clf.fit(x_train, y_train)

# evaluation
print(f'accuracy: {clf.score(x_test, y_test)}')


user_input = """""" def fib(n):
                    a,b = 0,1
                    while a < n:
                        print(a, end=' ')
                    a,b = b, a+b
                    print()
                    fib(1000)

   """"""
clf.predict(pipe_rf.transform(user_input))[0]

prediction = clf.predict(pipe_rf.transform(user_input))[0]
predicted_class_idx = list(clf.classes_).index(prediction)

import shap

shap.initjs()
explainer = shap.treeexplainer(clf, x_train)
observation = pipe_rf.transform(user_input).toarray()
shap_values = explainer.shap_values(observation)


shap.force_plot(explainer.expected_value[predicted_class_idx], shap_values[predicted_class_idx], feature_names=vectorizer.get_feature_names_out())",https://stackoverflow.com/questions/72920750,python-3.x,09-07-2022 10:52,1734.0,-1.0,1.0,True,10-07-2022 15:35,09-07-2022 12:16
76943231,azure web app has no access to openai private endpoint in virtual network,"i am trying to host a web application similar to a private chatgpt instance within a secluded virtual network, ensuring that there's no external internet access.
i have developed a web application that serves as the interface for the openai api, where the api itself is utilized for chatgpt functionality. to achieve privacy, i established a virtual network and set up private endpoints for both components. as a result, the webapp resides at 10.0.0.5, while the openai api is located at 10.0.0.6. furthermore, i configured a virtual machine (vm) with a bastion to gain access to my private web application. this setup has been functioning seamlessly thus far. the chat feature within openai studio exclusively operates within the confines of my private network (via the bastion vm), and i can only interact with my web application when connected to the vm. any attempt to access the web app from the public internet results in a ""403 forbidden"" error. similarly, accessing openai studio from the public internet prompts the message ""public access is disabled. please configure private endpoint,"" which is the intended behavior.
however, i've encountered an issue: even with internet access restricted and proper access via the virtual network on my vm, attempting to use the question feature in my web application within the private network leads to the same ""public access is disabled. please configure private endpoint"" error. strangely, the chat functionality in openai studio works flawlessly. there seems to be a specific obstacle affecting my web app's functionality.
so i would like to ask if someone has an idea how to solve this.
//edit:
this is what my private network looks like

now i made a few changes and seems like im a step further. now if im requesting the openai from my webapp, it tells me ""access denied due to virtual network/firewall rules.""","['azure', 'azure-web-app-service', 'openai-api', 'azure-virtual-network', 'azure-webapps']",76958444,the fix was to create another subnet and activate the vnet integration in the web app and link it to the created subnet.,https://stackoverflow.com/questions/76943231,azure,21-08-2023 07:33,6514.0,2.0,1.0,True,23-08-2023 05:16,22-08-2023 05:34
61410527,"oserror: [e050] can&#39;t find model &#39;en&#39;. it doesn&#39;t seem to be a shortcut link, a python package or a valid path to a data directory","i try to set things for rasacore in my system when i run the command

python3 -m rasa_nlu.train -c nlu_config.yml --data data/nlu.md -o models --fixed_model_name nlu --project current --verbose

after it tries to get 

rasa_nlu.utils.spacy_utils  - trying to load spacy model with name 'en'
  it gives error as
  file ""e:\anaconda\envs\botenv\lib\site-packages\spacy\util.py"", line 119, in load_model
      raise ioerror(errors.e050.format(name=name))
  oserror: [e050] can't find model 'en'. it doesn't seem to be a shortcut link, a python package or a valid path to a data directory.","['python', 'conda', 'spacy', 'rasa-core']",61546898,"you need to download and link the spacy model you want to use. 
e.g.
python -m spacy download en_core_web_md
python -m spacy link en_core_web_md en

also, preconfigured pipelines will be deprecated, see the new guide to choosing a pipeline and pass the components you want explicitly",https://stackoverflow.com/questions/61410527,python,24-04-2020 14:18,4743.0,0.0,2.0,True,05-07-2022 17:30,26-04-2020 22:54
68403128,getting nans for gradient,"i am trying to create a search relevance model where i take the dot product between query vector and resulting documents. i add a positional bias term on top to take into account the fact that position 1 is more likely to be clicked on. the final (unnormalised) log likelihood calculation is as follows:
        query = self.query_model(query_input_ids, query_attention_mask)
        docs = self.doc_model(doc_input_ids, doc_attention_mask)
        positional_bias = self.position_model()
        
        if optimizer_idx is not none:
            if optimizer_idx == 0:
                docs = docs.detach()
                positional_bias = positional_bias.clone().detach()
            elif optimizer_idx == 1:
                query = query.detach()
                positional_bias = positional_bias.clone().detach()
            else:
                query = query.detach()
                docs = docs.detach()
                
        similarity = (docs @ query.unsqueeze(-1)).squeeze()

        click_log_lik = (similarity + positional_bias)\
                .reshape(doc_mask.shape)\
                .masked_fill_((1 - doc_mask).bool(), float(""-inf""))

the query and doc model is simply a distilbert model with a projection layer on top of cls token. the models can be seen here: 
when inspecting the first gradient descent step, it has nans, but only for the query model and not the doc model. my hypothesis is that normalizing the return values for doc and query models (return f.normalize(out, dim=-1)) is somehow playing up with the gradients.
does anyone know 1. if my hypothesis is true and more importantly 2. how can i rectify nan gradients?.
additional info:

none of the losses are inf or nan.
query is bs x 768
docs is bs x doc_results x 768
positional_bias is doc_results
doc_results is 10 in my case.
the masked_fill in the last line is because occasionally i have less than 10 data points for a query.

update 1
the following changes made no difference to nans:

changing masked_fill from -inf to 1e5.
changing the projection from f.normalize(out, dim=-1) to out / 100.
removed positional bias altogether with again no luck.","['tensorflow', 'deep-learning', 'pytorch', 'huggingface-transformers']",68616941,"if it helps anyone, and you come across this while using transformers this is what i did:
so in the end the bug was due to the fact that i was masking away nan's. since i had some documents with zero length, the output of the transformer was nan. i was hoping that masked_fill would fix this problem, but it doesn't. the solution in my case was to only put non-zero length sequences through transformers, and then append with zeros to fill the batch size.",https://stackoverflow.com/questions/68403128,tensorflow,16-07-2021 03:39,423.0,0.0,1.0,True,02-08-2021 05:56,16-07-2021 06:35
57128766,using nlp.pipe() with pre-segmented and pre-tokenized text with spacy,"i am trying to tag and parse text that has already been split up in sentences and has already been tokenized. as an example:
sents = [['i', 'like', 'cookies', '.'], ['do', 'you', '?']]

the fastest approach to process batches of text is .pipe(). however, it is not clear to me how i can use that with pre-tokenized, and pre-segmented text. performance is key here. i tried the following, but that threw an error
docs = [nlp.tokenizer.tokens_from_list(sentence) for sentence in sents]
nlp.tagger(docs)
nlp.parser(docs)

trace:
traceback (most recent call last):
  file ""c:\python\python37\lib\multiprocessing\pool.py"", line 121, in worker
    result = (true, func(*args, **kwds))
  file ""c:\python\projects\predict\predicting-wte\build_id_dictionary.py"", line 204, in process_batch
    self.nlp.tagger(docs)
  file ""pipes.pyx"", line 377, in spacy.pipeline.pipes.tagger.__call__
  file ""pipes.pyx"", line 396, in spacy.pipeline.pipes.tagger.predict
  file ""c:\users\bmvroy\.virtualenvs\predicting-wte-ykqw76ba\lib\site-packages\thinc\neural\_classes\model.py"", line 169, in __call__
    return self.predict(x)
  file ""c:\users\bmvroy\.virtualenvs\predicting-wte-ykqw76ba\lib\site-packages\thinc\neural\_classes\feed_forward.py"", line 40, in predict
    x = layer(x)
  file ""c:\users\bmvroy\.virtualenvs\predicting-wte-ykqw76ba\lib\site-packages\thinc\neural\_classes\model.py"", line 169, in __call__
    return self.predict(x)
  file ""c:\users\bmvroy\.virtualenvs\predicting-wte-ykqw76ba\lib\site-packages\thinc\neural\_classes\model.py"", line 133, in predict
    y, _ = self.begin_update(x, drop=none)
  file ""c:\users\bmvroy\.virtualenvs\predicting-wte-ykqw76ba\lib\site-packages\thinc\neural\_classes\feature_extracter.py"", line 14, in begin_update
    features = [self._get_feats(doc) for doc in docs]
  file ""c:\users\bmvroy\.virtualenvs\predicting-wte-ykqw76ba\lib\site-packages\thinc\neural\_classes\feature_extracter.py"", line 14, in <listcomp>
    features = [self._get_feats(doc) for doc in docs]
  file ""c:\users\bmvroy\.virtualenvs\predicting-wte-ykqw76ba\lib\site-packages\thinc\neural\_classes\feature_extracter.py"", line 21, in _get_feats
    arr = doc.doc.to_array(self.attrs)[doc.start : doc.end]
attributeerror: 'list' object has no attribute 'doc'","['python', 'nlp', 'batch-processing', 'tokenize', 'spacy']",57134499,"just replace the default tokenizer in the pipeline with nlp.tokenizer.tokens_from_list instead of calling it separately:
import spacy
nlp = spacy.load('en')
nlp.tokenizer = nlp.tokenizer.tokens_from_list

for doc in nlp.pipe([['i', 'like', 'cookies', '.'], ['do', 'you', '?']]):
    for token in doc:
        print(token, token.pos_)

output:
i pron
like verb
cookies noun
. punct
do verb
you pron
? punct",https://stackoverflow.com/questions/57128766,python,20-07-2019 21:34,2454.0,2.0,4.0,True,20-07-2022 12:11,20-07-2019 22:20
69406937,how to use scibert in the best manner?,"i'm trying to use bert models to do text classification. as the text is about scientific texts, i intend to use the sicbert pre-trained model: 
i have faced several limitations which i want to know if there is any solutions for them:

when i want to do tokenization and batching, it only allows me to use max_length of <=512. is there any way to use more tokens. doen't this limitation of 512 mean that i am actually not using all the text information during training? any solution to use all the text?

i have tried to use this pretrained library with other models such as deberta or roberta. but it doesn't let me. i has only worked with bert. is there anyway i can do that?

i know this is a general question, but any suggestion that i can improve my fine tuning (from data to hyper parameter, etc)? currently, i'm getting ~75% accuracy. thanks


codes:
tokenizer = berttokenizer.from_pretrained('allenai/scibert_scivocab_uncased')

encoded_data_train = tokenizer.batch_encode_plus(
    df_train.text.values, 
    add_special_tokens=true, 
    return_attention_mask=true, 
    padding=true,
    max_length=256
)

input_ids_train = encoded_data_train['input_ids']
attention_masks_train = encoded_data_train['attention_mask']
labels_train = torch.tensor(df_train.label.values)

dataset_train = tensordataset(input_ids_train, attention_masks_train, labels_train)

dataloader_train = dataloader(dataset_train, 
                              sampler=randomsampler(dataset_train), 
                              batch_size=batch_size)

model = bertforsequenceclassification.from_pretrained('allenai/scibert_scivocab_uncased',
                                                      num_labels=len(labels),
                                                      output_attentions=false,
                                                      output_hidden_states=false)

epochs = 1

optimizer = adamw(model.parameters(), lr=1e-5, eps=1e-8)

scheduler = get_linear_schedule_with_warmup(optimizer,
num_training_steps=len(dataloader_train)*epochs)","['nlp', 'pytorch', 'text-classification', 'huggingface-transformers', 'bert-language-model']",69417899,"when i want to do tokenization and batching, it only allows me to use max_length of <=512. is there any way to use more tokens. doen't this limitation of 512 mean that i am actually not using all the text information during training? any solution to use all the text?

yes, you are not using the complete text. and this is one of the limitations of bert and t5 models, which limit to using 512 and 1024 tokens resp. to the best of my knowledge.
i can suggest you to use longformer or bigbird or reformer models, which can handle sequence lengths up to 16k, 4096, 64k tokens respectively. these are really good for processing longer texts like scientific documents.

i have tried to use this pretrained library with other models such as deberta or roberta. but it doesn't let me. i has only worked with bert. is there anyway i can do that?

scibert is actually a pre-trained bert model.
see this issue for more details where they mention the feasibility of converting bert to roberta:
since you're working with a bert model that was pre-trained, you unfortunately won't be able to change the tokenizer now from a wordpiece (bert) to a byte-level bpe (roberta).

i know this is a general question, but any suggestion that i can
improve my fine tuning (from data to hyper parameter, etc)? currently,
i'm getting ~79% accuracy.

i would first try to tune the most important hyperparameter learning_rate. i would then explore different values for hyperparameters of adamw optimizer and num_warmup_steps hyperparamter of the scheduler.",https://stackoverflow.com/questions/69406937,nlp,01-10-2021 13:44,2517.0,0.0,1.0,True,03-10-2021 14:21,02-10-2021 15:11
78404535,langchain python with structured output ollama functions,"i am following this guide to set up a self-rag.
i am not allowed to use openai models at the moment, so i've been using chatollama models instead. i want to pipe outputs using the ""with_structured_output()"" function, with ollamafunctions instead of chatollama. it is demonstrated here.
essentially here is the code:
from langchain_experimental.llms.ollama_functions import ollamafunctions


from langchain_core.prompts import prompttemplate
from langchain_core.pydantic_v1 import basemodel, field


# schema for structured response
class person(basemodel):
    name: str = field(description=""the person's name"", required=true)
    height: float = field(description=""the person's height"", required=true)
    hair_color: str = field(description=""the person's hair color"")


# prompt template
prompt = prompttemplate.from_template(
    """"""alex is 5 feet tall. 
claudia is 1 feet taller than alex and jumps higher than him. 
claudia is a brunette and alex is blonde.

human: {question}
ai: """"""
)

# chain
llm = ollamafunctions(model=""phi3"", format=""json"", temperature=0)
structured_llm = llm.with_structured_output(person)
chain = prompt | structured_llm

i get two errors that bring me to a dead end. the first one is:
validationerror: 1 validation error for ollamafunctions
__root__
  langchain_community.chat_models.ollama.chatollama() got multiple values for keyword argument 'format' (type=type_error)

so i changed
llm = ollamafunctions(model=""phi3"", format=""json"", temperature=0)
to
llm = ollamafunctions(model=""phi3"", temperature=0)
and that brings me to the next line at least. then, the with_structured_output(person) line fails with error:
file ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/language_models/base.py:208, in baselanguagemodel.with_structured_output(self, schema, **kwargs)
    204 def with_structured_output(
    205     self, schema: union[dict, type[basemodel]], **kwargs: any
    206 ) -> runnable[languagemodelinput, union[dict, basemodel]]:
    207     """"""implement this if there is a way of steering the model to generate responses that match a given schema.""""""  # noqa: e501
--> 208     raise notimplementederror()

notimplementederror:

and i don't know where to go from here. anything would help. thanks!","['langchain', 'llama']",78428420,"hobakjuk found the issue: pip, github, webdoc versions of ollama_functions are out of sync. which requires a temp workaround until the pypi version is updated.
the workaround involves:

ctrl+c copy code contents from github ollama_functions.py

make a local ollama_functions.py file, ctrl+v paste code into it

in your python code then import the 'patched' local library by replacing
from langchain_experimental.llms.ollama_functions import ollamafunctions
with
from ollama_functions import ollamafunctions


keep track of your code",https://stackoverflow.com/questions/78404535,langchain,29-04-2024 18:24,10029.0,10.0,2.0,True,31-10-2024 01:10,05-08-2024 13:47
69533961,how to configure and train the model using glove and cnn for text classification?,"i have worked with text classification using glove and cnn and found the problem below:
file ""c:\programfiles_anaconda\anaconda3\envs\math_stat_class\lib\site-packages\tensorflow\python\framework\ops.py"", line 1657, in _create_c_op
    raise valueerror(str(e))

valueerror: negative dimension size caused by subtracting 5 from 1 for '{{node max_pooling1d_9/maxpool}} = maxpool[t=dt_float, data_format=""nhwc"", ksize=[1, 5, 1, 1], padding=""valid"", strides=[1, 5, 1, 1]](max_pooling1d_9/expanddims)' with input shapes: [?,1,1,128].

glove input
embedding_dim = 100
    
embeddings_index = {}
    
f = open(glove_path, encoding='utf-8')  
for line in f:    
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
    f.close()
    
print('found %s word vectors.' % len(embeddings_index))
    
embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))
    
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not none:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

layer input of cnn
# apply embedding matrix into an embedding layer
# trainable=false to prevent the weights from being updated during training
embedding_layer = embedding(len(word_index) + 1,
                            embedding_dim,
                            weights=[embedding_matrix],
                            input_length=max_sequence_length,
                            trainable=false)

training 1d cnn
sequence_input = input(shape=(max_sequence_length,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)

x = conv1d(128, 5, activation='relu')(embedded_sequences)   
print(""x shape = "", x)

x = maxpooling1d(5)(x)  
print(""x shape = "", x)
        
x = conv1d(128, 5, activation='relu')(x)
print(""x shape = "", x)
    
#-----this line below produced error-----
x = maxpooling1d(5)(x) #error this line
#-----this line above produced error-----
        
print(""x shape = "", x)

x = conv1d(128, 5, activation='relu')(x)
print(""x shape = "", x)
    
x = maxpooling1d(35)(x)  # global max pooling
print(""x shape = "", x)
    
x = flatten()(x)
x = dense(128, activation='relu')(x)
    
preds = dense(len(labels_index), activation='softmax')(x)
model = model(sequence_input, preds)
model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['acc'])
    
# learning
model.fit(x_train, y_train, validation_data=(x_val, y_val),
          epochs=2, batch_size=128)

my ideas
1) are there some issues/problems with glove input?
2) conv1d:

change the ""kernel_size"" from 5 to a new value.

3) maxpooling1d:

change pool_size from 5 to a new value.
specify other parameters: strides, padding and so on.

4) i currently use keras on tensorflow 2.20 and python 3.6

do i need to upgrade tensorflow and python?

however, i could not figure out any better way to do. may i have your suggestions?","['python', 'tensorflow', 'conv-neural-network', 'stanford-nlp', 'text-classification']",69535932,"two things that come to my mind: your max-pooling layers are reducing the size of the input to the next convolutional layers every time and eventually the size is too small to run another max-pooling operation. try running
 tf.print(model.summary) 

after each max-pooling operation and you will quickly find out that your tensor cannot be further reduced. you can then consider using a different pool_size in your max-pooling layers.
the second thing i notice (i am not sure if it is intentional), but maxpooling1d != global max pooling. keras supports both operations. take a look at the documentation.
on a side note, sentence classification with cnns was widely popularized by the work of yoon kim. in his work, he shows that global max-pooling operations perform much better than striding max-pooling operations in sentence classification (when using word embeddings, as you are doing).",https://stackoverflow.com/questions/69533961,python,12-10-2021 01:01,263.0,0.0,1.0,True,28-10-2021 21:45,28-10-2021 21:45
78518971,can i dynamically add or remove lora weights in the transformer library like diffusers,"i see that in the diffuser library, there is this feature to dynamically add and remove lora weights based on this article  ï¿½ï¿½ github and using the load_lora_weights and fuse_lora_weights. i want to know if i can do something similar with lora for transformers too?<","['python', 'huggingface-transformers', 'huggingface', 'peft']",78520745,"in peft, when you create or load an adapter, you give it a name.
then you can enable the adapter(s) of your choice dynamically by name with 
see the example of how to do this here:",https://stackoverflow.com/questions/78518971,python,22-05-2024 16:45,1332.0,0.0,1.0,True,23-05-2024 03:18,23-05-2024 03:18
1192768,how to detect the language of a string?,what's the best way to detect the language of a string?,"['c#', 'language-detection']",1192802,"if the context of your code have internet access, you can try to use the google api for language detection.

var text = ""ýýdýýnde estýý el baýýo?"";
google.language.detect(text, function(result) {
  if (!result.error) {
    var language = 'unknown';
    for (l in google.language.languages) {
      if (google.language.languages[l] == result.language) {
        language = l;
        break;
      }
    }
    var container = document.getelementbyid(""detection"");
    container.innerhtml = text + "" is: "" + language + """";
  }
});

and, since you are using c#, take a look at this article on how to call the api from c#.
update:
that c# link is gone, here's a cached copy of the core of it:
string s = textboxtranslateenglishtohebrew.text;
string key = ""your google ajax api key"";
googlelangaugedetector detector =
   new googlelangaugedetector(s, version.one_point_zero, key);

googletranslator gtranslator = new googletranslator(s, version.one_point_zero,
   detector.languagedetected.equals(""iw"") ? language.hebrew : language.english,
   detector.languagedetected.equals(""iw"") ? language.english : language.hebrew,
   key);

textboxtranslation.text = gtranslator.translation;


basically, you need to create a uri and send it to google that looks like:



this tells the api that you want to translate ""hello world"" from english to hebrew, to which google's json response would look like:
{""responsedata"": {""translatedtext"":""ýýýýýýýý ýýýýýýýýýý""}, ""responsedetails"": null, ""responsestatus"": 200}

i chose to make a base class that represents a typical google json response:
[serializable]
public class jsonresponse
{
   public string responsedetails = null;
   public string responsestatus = null;
}

then, a translation object that inherits from this class:
[serializable]
public class translation: jsonresponse
{
   public translationresponsedata responsedata = 
    new translationresponsedata();
}

this translation class has a translationresponsedata object that looks like this:
[serializable]
public class translationresponsedata
{
   public string translatedtext;
}

finally, we can make the googletranslator class:
using system;
using system.collections.generic;
using system.text;

using system.web;
using system.net;
using system.io;
using system.runtime.serialization.json;

namespace googletranslationapi
{

   public class googletranslator
   {
      private string _q = """";
      private string _v = """";
      private string _key = """";
      private string _langpair = """";
      private string _requesturl = """";
      private string _translation = """";

      public googletranslator(string queryterm, version version, language languagefrom,
         language languageto, string key)
      {
         _q = 
         _v = 
         _langpair =
             +
            ""|"" + enumstringutil.getstringvalue(languageto));
         _key = 

         string encodedrequesturlfragment =
            string.format(""?v={0}&q={1}&langpair={2}&key={3}"",
            _v, _q, _langpair, _key);

         _requesturl = enumstringutil.getstringvalue(baseurl.translate) + encodedrequesturlfragment;

         gettranslation();
      }

      public string translation
      {
         get { return _translation; }
         private set { _translation = value; }
      }

      private void gettranslation()
      {
         try
         {
            webrequest request = webrequest.create(_requesturl);
            webresponse response = request.getresponse();

            streamreader reader = new streamreader(response.getresponsestream());
            string json = reader.readline();
            using (memorystream ms = new memorystream(encoding.unicode.getbytes(json)))
            {
               datacontractjsonserializer ser =
                  new datacontractjsonserializer(typeof(translation));
               translation translation = ser.readobject(ms) as translation;

               _translation = translation.responsedata.translatedtext;
            }
         }
         catch (exception) { }
      }
   }
}",https://stackoverflow.com/questions/1192768,c#,28-07-2009 08:47,34421.0,22.0,11.0,True,28-01-2025 23:38,05-09-2013 23:04
77442646,"openai.error.ratelimiterror: you exceeded your current quota, please check your plan and billing details","i recently obtained a free api key from openai and attempted to use it in my script. however, i encountered the following error on my very first attempt:
openai.error.ratelimiterror: you exceeded your current quota, please check your plan and billing details.
i haven't used the api key before, so i'm puzzled by this error. could someone please help me understand what might be causing this issue and how i can resolve it?","['python', 'openai-api', 'chatgpt-api']",77495224,"this also happened to me when i sent a lot of prompts via the api. the rate limit is dependent on the amount of credits paid on your account and how long it has been since you have made your first payment.
according to the openai documentation, there are different rate limits:

rate limits are measured in five ways: rpm (requests per minute), rpd (requests per day), tpm (tokens per minute), tpd (tokens per day), and ipm (images per minute). rate limits can be hit across any of the options depending on what occurs first. for example, you might send 20 requests with only 100 tokens to the chatcompletions endpoint and that would fill your limit (if your rpm was 20), even if you did not send 150k tokens (if your tpm limit was 150k) within those 20 requests.

this limit is bumped up if you upgrade your plan tier. the plans are as follows according to the openai documentation:




tier
qualification




free
user must be in allowed geography


tier 1
$5 paid


tier 2
$50 paid and more than 7 days since first successful payment


tier 3
$100 paid and more than 7 days since first successful payment


tier 4
$250 paid and more than 14 days since first successful payment


tier 5
$1,000 paid and more than 30 days since first successful payment




depending on your tier and the model you use, you can find your limits in the openai doucentation.",https://stackoverflow.com/questions/77442646,python,08-11-2023 03:16,9128.0,1.0,3.0,True,19-04-2025 08:39,08-11-2023 06:47
74228640,which huggingface summarization models support more than 1024 tokens? which model is more suitable for programming related articles?,"if this is not the best place to ask this question, please lead me to the most accurate one.
i am planning to use one of the huggingface summarization models ( to summarize my lecture video transcriptions.
so far i have tested facebook/bart-large-cnn and sshleifer/distilbart-cnn-12-6, but they only support a maximum of 1,024 tokens as input.
so, here are my questions:

are there any summarization models that support longer inputs such as 10,000 word articles?

what are the optimal output lengths for given input lengths? let's say for a 1,000 word input, what is the optimal (minimum) output length (the min. length of the summarized text)?

which model would likely work on programming related articles?","['nlp', 'huggingface-transformers', 'summarization', 'huggingface', 'mlmodel']",74229599,"question 1

are there any summarization models that support longer inputs such as
10,000 word articles?

yes, the longformer encoder-decoder (led) [1] model published by beltagy et al. is able to process up to 16k tokens. various led models are available here on huggingface. there is also pegasus-x [2] published recently by phang et al. which is also able to process up to 16k tokens. models are also available here on huggingface.
alternatively, you can look at either:

extractive followed by abstractive summarisation, or
splitting a large document into chunks of max_input_length (e.g. 1024), summarise each, and then concatenate together. care will have to be taken as to how the documents are chunked as to avoid chunking mid-way through particular topics, or having a relatively short final chunk that may produce an unusable summary.

question 2

what are the optimal output lengths for given input lengths? let's say
for a 1,000 word input, what is the optimal (minimum) output
length (i.e. the min. length of the summarized text)?

this is a very difficult question to answer as it hard to empirically evaluate the quality of a summarisation. i would suggest running a few tests yourself with varied output length limits (e.g. 20, 50, 100, 200) and find what subjectively works best. each model and document genre will be different. anecdotally, i would say 50 words will a good minimum, with 100-150 offering better results.
question 3

which model would likely to work on programming related articles?

i can imagine three possible cases for what constitutes a programming related article.

source code summarisation (which involves producing a natural (informal) language summary of code (formal language)).
traditional abstractive summarisation (i.e. natural language summary of natural language but for articles talking about programming yet have no code).
combination of both 1 and 2.

for case (1), i'm not aware of any implementations on huggingface that focus on this problem. however, it is an active research topic (see [3], [4], [5]).
for case (2), you can use the models you've been using already, and if feasible, fine tune on your own specific dataset of programming related articles.
for case (3), simply look at combining implementations from both (1) and (2) based on whether the input is categorised as either formal (code) or informal (natural) language.
references
[1] beltagy, i., peters, m.e. and cohan, a., 2020. longformer: the long-document transformer. arxiv preprint arxiv:2004.05150.
[2] phang, j., zhao, y. and liu, p.j., 2022. investigating efficiently extending transformers for long input summarization. arxiv preprint arxiv:2208.04347.
[3] ahmad, w.u., chakraborty, s., ray, b. and chang, k.w., 2020. a transformer-based approach for source code summarization. arxiv preprint arxiv:2005.00653.
[4] wei, b., li, g., xia, x., fu, z. and jin, z., 2019. code generation as a dual task of code summarization. advances in neural information processing systems, 32.
[5] wan, y., zhao, z., yang, m., xu, g., ying, h., wu, j. and yu, p.s., 2018, september. improving automatic source code summarization via deep reinforcement learning. in proceedings of the 33rd acm/ieee international conference on automated software engineering (pp. 397-407).",https://stackoverflow.com/questions/74228640,nlp,27-10-2022 21:45,6161.0,10.0,1.0,True,28-10-2022 08:27,28-10-2022 08:27
73459649,bart tokenizer tokenises same word differently?,"i have noticed that if i tokenize a full text with many sentences, i sometimes get a different number of tokens than if i tokenise each sentence individually and add up the tokens. i have done some debugging and have this small reproducible example to show the issue
from transformers import autotokenizer
tokenizer = autotokenizer.from_pretrained('facebook/bart-large-cnn')

print(tokenizer.tokenize(""thames is a river""))
print(tokenizer.tokenize(""we are in london. thames is a river""))

i get the following output
['th', 'ames', 'ï¿½ï¿½is', 'ï¿½ï¿½a', 'ï¿½ï¿½river']
['we', 'ï¿½ï¿½are', 'ï¿½ï¿½in', 'ï¿½ï¿½london', '.', 'ï¿½ï¿½thames', 'ï¿½ï¿½is', 'ï¿½ï¿½a', 'ï¿½ï¿½river']

i would like to understand why the word thames has been split into two tokens when itï¿½ï¿½ï¿½s at the start of sequence, whereas itï¿½ï¿½ï¿½s a single word if itï¿½ï¿½ï¿½s not at the start of sequence. i have noticed this behaviour is very frequent and, assumint tokeniser behaves like this.","['nlp', 'huggingface-transformers', 'bert-language-model', 'huggingface-tokenizers', 'bart']",73459967,"according to 
this tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will be encoded differently whether it is at the beginning of the sentence (without space) or not. you can get around that behavior by passing add_prefix_space=true when instantiating this tokenizer or when you call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.
trying
from transformers import autotokenizer
tokenizer = autotokenizer.from_pretrained('facebook/bart-large-cnn', add_prefix_space=true)

print(tokenizer.tokenize(""thames is a river""))
print(tokenizer.tokenize(""we are in london. thames is a river""))

yields the 'correct' result to me.",https://stackoverflow.com/questions/73459649,nlp,23-08-2022 13:28,996.0,2.0,1.0,True,29-08-2022 09:43,29-08-2022 09:43
75560424,transformers refine-tune with different classes,"i want to fine-tune a bert-based already fine-tuned model for classification with 7 classes another time on a 16 class dataset:
model_name_or_path = 'some pretrained model for 7 class classification on huggingface repo'
model = build_model(model_name_or_path, learning_rate=learning_rate)

def build_model(model_name, learning_rate=3e-5):
    model = tfbertforsequenceclassification.from_pretrained(model_name)

    optimizer = tf.keras.optimizers.adam(learning_rate=learning_rate)
    loss = tf.keras.losses.sparsecategoricalcrossentropy(from_logits=true)
    metric = tf.keras.metrics.sparsecategoricalaccuracy('accuracy')
    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

   return model
r = model.fit(
    train_dataset,
    validation_data=valid_dataset,
    steps_per_epoch=train_steps,
    validation_steps=valid_steps,
    epochs=epochs,
    verbose=1)

as expected the model expects 7 class at the final layer, and produces the following error:
node: 
'sparse_categorical_crossentropy/sparsesoftmaxcrossentropywithlogits/sparsesoftmaxcrossentropywithlogits'
received a label value of 9 which is outside the valid range of [0, 8).  label values: 6 2 0 6 0 9 6 6 0 6 6 0 7 2 2 2
     [[{{node sparse_categorical_crossentropy/sparsesoftmaxcrossentropywithlogits/sparsesoftmaxcrossentropywithlogits}}]] [op:__inference_train_function_43224]

how should one edit the structure of the model?","['python', 'classification', 'huggingface-transformers', 'bert-language-model']",75583418,"for further references, you need to edit the final layer. in my case, as i was using tensorflow:
model.classifier = tf.keras.layers.dense(nunits)",https://stackoverflow.com/questions/75560424,python,24-02-2023 19:03,43.0,0.0,1.0,True,27-02-2023 17:01,25-02-2023 05:37
77240878,tokenizing and summarizing textual data by group efficiently in python,"i have a dataset in python that look like this one:
data = pd.dataframe({
    'id': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'],
    'text': [
        ""mouthwatering bbq ribs cheese, and coleslaw."",
        ""delicious pizza with pepperoni and extra cheese."",
        ""spicy thai curry with cheese and jasmine rice."",
        ""tiramisu dessert topped with cocoa powder."",
        ""sushi rolls with fresh fish and soy sauce."",
        ""freshly baked chocolate chip cookies."",
        ""homemade lasagna with layers of cheese and pasta."",
        ""gourmet burgers with all the toppings and extra cheese."",
        ""crispy fried chicken with mashed potatoes and extra cheese."",
        ""creamy tomato soup with a grilled cheese sandwich.""
    ],
    'date': [
        '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-02',
        '2023-02-02', '2023-02-01', '2023-02-01', '2023-02-02', '2023-02-02'
    ]
})

what i'd like to do is group by date and get the frequency of each token after removing punctuation. i'm very new to the python environment; i come from r, and i have been looking into the gensim library for further reference. it looks quite complicated to me. my desired output would look like this: for each group (date), we'll have the frequency of each unique token.




token
subtotal
date




cheese
5
1/02/2023


and
5
1/02/2023


with
5
1/02/2023


extra
2
1/02/2023


mouthwatering
1
1/02/2023


bbq
1
1/02/2023


ribs
1
1/02/2023


coleslaw
1
1/02/2023


delicious
1
1/02/2023


pizza
1
1/02/2023


pepperoni
1
1/02/2023




in r this can be done very easy with quanteda like this:
corpus_food<-corpus(data,
                  docid_field = ""id"",
                  text_field = ""text"")

corpus_food %>%
  tokens(remove_punct = true) %>% 
  dfm() %>% 
  textstat_frequency(groups = lubridate::date(date)) 

which only creates a corpus and then tokenizes to remove punctuation. later, it creates a document-term matrix and finally summarizes the tokens and their frequencies by group.
i am in no way comparing the two languages, python and r. they are amazing, but at the moment, i'm interested in a very straightforward and fast method to achieve my results in python. if perhaps you don't use the gensim library, i'd still be interested in a way to achieve what i'm looking for in a faster and more efficient way in python. i'm new to python.","['python', 'pandas', 'dataframe', 'nlp', 'gensim']",77242709,"i would simply extractall the words then value_counts :
out = (
    data[[""date""]].join(
        data[""text""].str.extractall(""(\w+)"")[0]
            .droplevel(1).rename(""token"").str.lower()
    ).groupby([""date"", ""token""]).value_counts().reset_index(name=""subtotal"")
        .sort_values([""date"", ""subtotal""], ascending=[true, false])
)

output :
print(out)

          date   token  subtotal
1   2023-02-01     and         5
4   2023-02-01  cheese         5
30  2023-02-01    with         5
10  2023-02-01   extra         2
0   2023-02-01     all         1
..         ...     ...       ...
51  2023-02-02   sauce         1
52  2023-02-02    soup         1
53  2023-02-02     soy         1
54  2023-02-02   sushi         1
55  2023-02-02  tomato         1

[57 rows x 3 columns]",https://stackoverflow.com/questions/77240878,python,05-10-2023 22:16,63.0,1.0,1.0,True,12-10-2023 23:33,12-10-2023 23:33
57886076,python keyerror when using pandas,"i'm following a tutorial on nlp but have encountered a key error error when trying to group my raw data into good and bad reviews. here is the tutorial link: 
#reviews.csv
i am so angry about the service
nothing was wrong, all good
the bedroom was dirty
the food was great

#nlp.py
import pandas as pd

#read data
reviews_df = pd.read_csv(""reviews.csv"")
# append the positive and negative text reviews
reviews_df[""review""] = reviews_df[""negative_review""] + 
reviews_df[""positive_review""]

reviews_df.columns

i'm seeing the following error:
file ""pandas\_libs\hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.pyobjecthashtable.get_item
keyerror: 'negative_review'

why is this happening?","['pandas', 'dataframe', 'nlp']",57889256,"you're getting this error because you did not understand how to structure your data.
when you do df['reviews']=df['positive_reviews']+df['negative_reviews'] you're actually summing the values of positive reviews to negative reviews(which does not exist currently) into the 'reviews' column (chich also does not exist).
your csv is nothing more than a plaintext file with one text in each row. also, since you're working with text, remember to enclose every string in quotation marks(""), otherwise your commas will create fakecolumns.
with your approach, it seems that you'll still tag all your reviews manually (usually, if you're working with machine learning, you'll do this outside code and load it to your machine learning file).
in order for your code to work, you want to do the following:
import pandas as pd

df = pd.read_csv('testfilefolder/57886076.csv', names=['text'])
## fill with placeholder values
df['positive_review']=0
df['negative_review']=1
df.head()

result:
                              text  positive_review  negative_review
0  i am so angry about the service                0                1
1      nothing was wrong, all good                0                1
2            the bedroom was dirty                0                1
3               the food was great                0                1

however, i would recommend you to have a single column (is_review_positive) and have it to true or false. you can easily encode it later on.",https://stackoverflow.com/questions/57886076,pandas,11-09-2019 09:29,4810.0,0.0,2.0,True,02-06-2023 04:20,11-09-2019 16:33
59927844,is it possible to install spacy to raspberry pi 4 raspbian buster,"i have been stuck at installing spacy the entire day.
sudo pip install -u spacy

looking in indexes:  
collecting spacy
  using cached 
  installing build dependencies ... done
    complete output from command python setup.py egg_info:
      failed building wheel for blis
    error: failed to build one or more wheels
    traceback (most recent call last):
      file ""/tmp/pip-build-env-e4fo917j/lib/python3.7/site-packages/setuptools/installer.py"", line 128, in fetch_build_egg
        subprocess.check_call(cmd)
      file ""/usr/lib/python3.7/subprocess.py"", line 347, in check_call
        raise calledprocesserror(retcode, cmd)
    subprocess.calledprocesserror: command '['/usr/bin/python', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/tmp/tmpgp2s1vb0', '--quiet', 'blis<0.5.0,>=0.4.0']' returned non-zero exit status 1.

    during handling of the above exception, another exception occurred:

    traceback (most recent call last):
      file ""<string>"", line 1, in <module>
      file ""/tmp/pip-install-avn68ykf/spacy/setup.py"", line 200, in <module>
        setup_package()
      file ""/tmp/pip-install-avn68ykf/spacy/setup.py"", line 195, in setup_package
        cmdclass={""build_ext"": build_ext_subclass},
      file ""/tmp/pip-build-env-e4fo917j/lib/python3.7/site-packages/setuptools/__init__.py"", line 144, in setup
        _install_setup_requires(attrs)
      file ""/tmp/pip-build-env-e4fo917j/lib/python3.7/site-packages/setuptools/__init__.py"", line 139, in _install_setup_requires
        dist.fetch_build_eggs(dist.setup_requires)
      file ""/tmp/pip-build-env-e4fo917j/lib/python3.7/site-packages/setuptools/dist.py"", line 721, in fetch_build_eggs
        replace_conflicting=true,
      file ""/tmp/pip-build-env-e4fo917j/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 783, in resolve
        replace_conflicting=replace_conflicting
      file ""/tmp/pip-build-env-e4fo917j/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 1066, in best_match
        return self.obtain(req, installer)
      file ""/tmp/pip-build-env-e4fo917j/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 1078, in obtain
        return installer(requirement)
      file ""/tmp/pip-build-env-e4fo917j/lib/python3.7/site-packages/setuptools/dist.py"", line 777, in fetch_build_egg
        return fetch_build_egg(self, req)
      file ""/tmp/pip-build-env-e4fo917j/lib/python3.7/site-packages/setuptools/installer.py"", line 130, in fetch_build_egg
        raise distutilserror(str(e))
    distutils.errors.distutilserror: command '['/usr/bin/python', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/tmp/tmpgp2s1vb0', '--quiet', 'blis<0.5.0,>=0.4.0']' returned non-zero exit status 1.

    ----------------------------------------
command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-install-avn68ykf/spacy/

i just wondering is it possible to install it on arm(armv7l) architecture?

updated: also tried to install from source, here is the error:
sudo pip install -r requirements.txt 

ignoring pathlib: markers 'python_version < ""3.4""' don't match your environment
looking in indexes:  
requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.0.3)
requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.0.2)
collecting thinc==7.4.0.dev0 (from -r requirements.txt (line 4))
  using cached 
requirement already satisfied: murmurhash<1.1.0,>=0.28.0 ...
requirement already satisfied: wasabi<1.1.0,>=0.4.0 ...
requirement already satisfied: srsly<1.1.0,>=0.1.0 ...
requirement already satisfied: catalogue<1.1.0,>=0.0.7 ...
requirement already satisfied: numpy>=1.15.0 in ...
requirement already satisfied: requests<3.0.0,>=2.13.0 ...
requirement already satisfied: plac<1.2.0,>=0.9.6 ...
requirement already satisfied: tqdm<5.0.0,>=4.38.0 ...
collecting jsonschema<3.1.0,>=2.6.0 (from -r requirements.txt (line 17))
  using cached 
requirement already satisfied: cython>=0.25 ...
collecting pytest>=4.6.5 (from -r requirements.txt (line 20))
  using cached 
collecting pytest-timeout<2.0.0,>=1.3.0 (from -r requirements.txt (line 21))
  using cached 
collecting mock<3.0.0,>=2.0.0 (from -r requirements.txt (line 22))
  using cached 
collecting flake8<3.6.0,>=3.5.0 (from -r requirements.txt (line 23))
  using cached 
collecting blis<0.5.0,>=0.4.0 (from thinc==7.4.0.dev0->-r requirements.txt (line 4))
  using cached 
requirement already satisfied: importlib-metadata>=0.20; python_version < ""3.8"" ...
requirement already satisfied: six>=1.11.0 ...
requirement already satisfied: setuptools ...
collecting pyrsistent>=0.14.0 (from jsonschema<3.1.0,>=2.6.0->-r requirements.txt (line 17))
  using cached 
collecting attrs>=17.4.0 (from jsonschema<3.1.0,>=2.6.0->-r requirements.txt (line 17))
  using cached 
collecting pluggy<1.0,>=0.12 (from pytest>=4.6.5->-r requirements.txt (line 20))
  using cached 
requirement already satisfied: wcwidth in /usr/lib/python3/dist-packages (from pytest>=4.6.5->-r requirements.txt (line 20)) (0.1.7)
collecting packaging (from pytest>=4.6.5->-r requirements.txt (line 20))
  using cached 
collecting more-itertools>=4.0.0 (from pytest>=4.6.5->-r requirements.txt (line 20))
  using cached 
collecting py>=1.5.0 (from pytest>=4.6.5->-r requirements.txt (line 20))
  using cached 
collecting pbr>=0.11 (from mock<3.0.0,>=2.0.0->-r requirements.txt (line 22))
  using cached 
collecting pycodestyle<2.4.0,>=2.0.0 (from flake8<3.6.0,>=3.5.0->-r requirements.txt (line 23))
  using cached 
requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /usr/lib/python3/dist-packages (from flake8<3.6.0,>=3.5.0->-r requirements.txt (line 23)) (0.6.1)
collecting pyflakes<1.7.0,>=1.5.0 (from flake8<3.6.0,>=3.5.0->-r requirements.txt (line 23))
  using cached 
requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < ""3.8""->catalogue<1.1.0,>=0.0.7->-r requirements.txt (line 9)) (2.1.0)
requirement already satisfied: pyparsing>=2.0.2 in /usr/lib/python3/dist-packages (from packaging->pytest>=4.6.5->-r requirements.txt (line 20)) (2.2.0)
building wheels for collected packages: blis
  running setup.py bdist_wheel for blis ... error
  complete output from command /usr/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-sh0bcfbq/blis/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d /tmp/pip-wheel-hbtbd3ev --python-tag cp37:
  blis_compiler? none
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.linux-armv7l-3.7
  creating build/lib.linux-armv7l-3.7/blis
  copying blis/benchmark.py -> build/lib.linux-armv7l-3.7/blis
  copying blis/__init__.py -> build/lib.linux-armv7l-3.7/blis
  copying blis/about.py -> build/lib.linux-armv7l-3.7/blis
  creating build/lib.linux-armv7l-3.7/blis/tests
  copying blis/tests/test_gemm.py -> build/lib.linux-armv7l-3.7/blis/tests
  copying blis/tests/__init__.py -> build/lib.linux-armv7l-3.7/blis/tests
  copying blis/tests/test_dotv.py -> build/lib.linux-armv7l-3.7/blis/tests
  copying blis/tests/common.py -> build/lib.linux-armv7l-3.7/blis/tests
  copying blis/cy.pyx -> build/lib.linux-armv7l-3.7/blis
  copying blis/py.pyx -> build/lib.linux-armv7l-3.7/blis
  copying blis/cy.pxd -> build/lib.linux-armv7l-3.7/blis
  copying blis/__init__.pxd -> build/lib.linux-armv7l-3.7/blis
  running build_ext
  /usr/local/lib/python3.7/dist-packages/cython/compiler/main.py:369: futurewarning: cython directive 'language_level' not set, using 2 for now (py2). this will change in a later release! file: /tmp/pip-install-sh0bcfbq/blis/blis/cy.pxd
    tree = parsing.p_module(s, pxd, full_module_name)
  /usr/local/lib/python3.7/dist-packages/cython/compiler/main.py:369: futurewarning: cython directive 'language_level' not set, using 2 for now (py2). this will change in a later release! file: /tmp/pip-install-sh0bcfbq/blis/blis/py.pyx
    tree = parsing.p_module(s, pxd, full_module_name)
  processing blis/cy.pyx
  processing blis/py.pyx
  unix
  py_compiler gcc
  {'blis_arch': 'generic', 'hostname': 'aa9d42588791', 'ssl_cert_file': '/opt/_internal/certs.pem', 'term': 'xterm', 'oldpwd': '/usr/local/repos/cython-blis', 'ld_library_path': '/opt/rh/devtoolset-2/root/usr/lib64:/opt/rh/devtoolset-2/root/usr/lib:/usr/local/lib64:/usr/local/lib', 'ls_colors': 'no=00:fi=00:di=00;34:ln=00;36:pi=40;33:so=00;35:bd=40;33;01:cd=40;33;01:or=01;05;37;41:mi=01;05;37;41:ex=00;32:*.cmd=00;32:*.exe=00;32:*.com=00;32:*.btm=00;32:*.bat=00;32:*.sh=00;32:*.csh=00;32:*.tar=00;31:*.tgz=00;31:*.arj=00;31:*.taz=00;31:*.lzh=00;31:*.zip=00;31:*.z=00;31:*.z=00;31:*.gz=00;31:*.bz2=00;31:*.bz=00;31:*.tz=00;31:*.rpm=00;31:*.cpio=00;31:*.jpg=00;35:*.gif=00;35:*.bmp=00;35:*.xbm=00;35:*.xpm=00;35:*.png=00;35:*.tif=00;35:', 'virtual_env': '/usr/local/repos/cython-blis/env3.6', 'path': '/usr/local/repos/cython-blis/env3.6/bin:/opt/rh/devtoolset-2/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'pwd': '/usr/local/repos/cython-blis/flame-blis', 'blis_arch': 'generic', 'lang': 'en_us.utf-8', 'home': '/root', 'shlvl': '2', 'language': 'en_us.utf-8', 'blis_arc': 'generic', 'auditwheel_plat': 'manylinux1_x86_64', 'pkg_config_path': '/usr/local/lib/pkgconfig', 'lessopen': '|/usr/bin/lesspipe.sh %s', 'g_broken_filenames': '1', '_': '/usr/local/repos/cython-blis/env3.6/bin/python'}
  gcc -c /tmp/pip-install-sh0bcfbq/blis/blis/_src/config/generic/bli_cntx_init_generic.c -o /tmp/tmpday66tmd/bli_cntx_init_generic.o -o3 -fpic -std=c99 -d_posix_c_source=200112l -dblis_version_string=""0.5.1"" -dblis_is_building_library -iinclude/linux-x86_64 -i./frame/3/ -i./frame/ind/ukernels/ -i./frame/1m/ -i./frame/1f/ -i./frame/1/ -i./frame/include -i/tmp/pip-install-sh0bcfbq/blis/blis/_src/include/linux-x86_64
  gcc -c /tmp/pip-install-sh0bcfbq/blis/blis/_src/config/haswell/bli_cntx_init_haswell.c -o /tmp/tmpday66tmd/bli_cntx_init_haswell.o -o3 -fpic -std=c99 -d_posix_c_source=200112l -dblis_version_string=""0.5.1"" -dblis_is_building_library -iinclude/linux-x86_64 -i./frame/3/ -i./frame/ind/ukernels/ -i./frame/1m/ -i./frame/1f/ -i./frame/1/ -i./frame/include -i/tmp/pip-install-sh0bcfbq/blis/blis/_src/include/linux-x86_64
  gcc -c /tmp/pip-install-sh0bcfbq/blis/blis/_src/config/penryn/bli_cntx_init_penryn.c -o /tmp/tmpday66tmd/bli_cntx_init_penryn.o -o3 -fpic -std=c99 -d_posix_c_source=200112l -dblis_version_string=""0.5.1"" -dblis_is_building_library -iinclude/linux-x86_64 -i./frame/3/ -i./frame/ind/ukernels/ -i./frame/1m/ -i./frame/1f/ -i./frame/1/ -i./frame/include -i/tmp/pip-install-sh0bcfbq/blis/blis/_src/include/linux-x86_64
  gcc -c /tmp/pip-install-sh0bcfbq/blis/blis/_src/config/piledriver/bli_cntx_init_piledriver.c -o /tmp/tmpday66tmd/bli_cntx_init_piledriver.o -o3 -fpic -std=c99 -d_posix_c_source=200112l -dblis_version_string=""0.5.1"" -dblis_is_building_library -iinclude/linux-x86_64 -i./frame/3/ -i./frame/ind/ukernels/ -i./frame/1m/ -i./frame/1f/ -i./frame/1/ -i./frame/include -i/tmp/pip-install-sh0bcfbq/blis/blis/_src/include/linux-x86_64
  gcc -c /tmp/pip-install-sh0bcfbq/blis/blis/_src/config/sandybridge/bli_cntx_init_sandybridge.c -o /tmp/tmpday66tmd/bli_cntx_init_sandybridge.o -o3 -fpic -std=c99 -d_posix_c_source=200112l -dblis_version_string=""0.5.1"" -dblis_is_building_library -iinclude/linux-x86_64 -i./frame/3/ -i./frame/ind/ukernels/ -i./frame/1m/ -i./frame/1f/ -i./frame/1/ -i./frame/include -i/tmp/pip-install-sh0bcfbq/blis/blis/_src/include/linux-x86_64
  gcc -c /tmp/pip-install-sh0bcfbq/blis/blis/_src/config/steamroller/bli_cntx_init_steamroller.c -o /tmp/tmpday66tmd/bli_cntx_init_steamroller.o -o3 -fpic -std=c99 -d_posix_c_source=200112l -dblis_version_string=""0.5.1"" -dblis_is_building_library -iinclude/linux-x86_64 -i./frame/3/ -i./frame/ind/ukernels/ -i./frame/1m/ -i./frame/1f/ -i./frame/1/ -i./frame/include -i/tmp/pip-install-sh0bcfbq/blis/blis/_src/include/linux-x86_64
  gcc -c /tmp/pip-install-sh0bcfbq/blis/blis/_src/kernels/zen/1/bli_amaxv_zen_int.c -o /tmp/tmpday66tmd/bli_amaxv_zen_int.o -o3 -mavx2 -mfma -mfpmath=sse -march=core-avx2 -fpic -std=c99 -d_posix_c_source=200112l -dblis_version_string=""0.5.1"" -dblis_is_building_library -iinclude/linux-x86_64 -i./frame/3/ -i./frame/ind/ukernels/ -i./frame/1m/ -i./frame/1f/ -i./frame/1/ -i./frame/include -i/tmp/pip-install-sh0bcfbq/blis/blis/_src/include/linux-x86_64
  gcc: error: unrecognized -march target: core-avx2
  gcc: note: valid arguments are: armv2 armv2a armv3 armv3m armv4 armv4t armv5 armv5t armv5e armv5te armv5tej armv6 armv6j armv6k armv6z armv6kz armv6zk armv6t2 armv6-m armv6s-m armv7 armv7-a armv7ve armv7-r armv7-m armv7e-m armv8-a armv8.1-a armv8.2-a armv8.3-a armv8.4-a armv8-m.base armv8-m.main armv8-r iwmmxt iwmmxt2 native
  gcc: error: missing argument to ï¿½ï¿½ï¿½-march=ï¿½ï¿½ï¿½
  gcc: error: unrecognized command line option ï¿½ï¿½ï¿½-mavx2ï¿½ï¿½ï¿½
  gcc: error: unrecognized command line option ï¿½ï¿½ï¿½-mfmaï¿½ï¿½ï¿½
  gcc: error: unrecognized command line option ï¿½ï¿½ï¿½-mfpmath=sseï¿½ï¿½ï¿""<string>"", line 1, in <module>
    file ""/tmp/pip-install-sh0bcfbq/blis/setup.py"", line 277, in <module>
      ""topic :: scientific/engineering"",
    file ""/usr/local/lib/python3.7/dist-packages/setuptools/__init__.py"", line 145, in setup
      return distutils.core.setup(**attrs)
    file ""/usr/lib/python3.7/distutils/core.py"", line 148, in setup
      dist.run_commands()
    file ""/usr/lib/python3.7/distutils/dist.py"", line 966, in run_commands
      self.run_command(cmd)
    file ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
      cmd_obj.run()
    file ""/usr/lib/python3/dist-packages/wheel/bdist_wheel.py"", line 188, in run
      self.run_command('build')
    file ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
      self.distribution.run_command(command)
    file ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
      cmd_obj.run()
    file ""/usr/lib/python3.7/distutils/command/build.py"", line 135, in run
      self.run_command(cmd_name)
    file ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
      self.distribution.run_command(command)
    file ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
      cmd_obj.run()
    file ""/usr/lib/python3.7/distutils/command/build_ext.py"", line 340, in run
      self.build_extensions()
    file ""/tmp/pip-install-sh0bcfbq/blis/setup.py"", line 103, in build_extensions
      objects = self.compile_objects(compiler.split(""-"")[0], arch, obj_dir)
    file ""/tmp/pip-install-sh0bcfbq/blis/setup.py"", line 188, in compile_objects
      objects.append(self.build_object(env=env, **spec))
    file ""/tmp/pip-install-sh0bcfbq/blis/setup.py"", line 201, in build_object
      subprocess.check_call(command, cwd=blis_dir)
    file ""/usr/lib/python3.7/subprocess.py"", line 347, in check_call
      raise calledprocesserror(retcode, cmd)
  subprocess.calledprocesserror: command '['gcc', '-c', '/tmp/pip-install-sh0bcfbq/blis/blis/_src/kernels/zen/1/bli_amaxv_zen_int.c', '-o', '/tmp/tmpday66tmd/bli_amaxv_zen_int.o', '-o3', '-mavx2', '-mfma', '-mfpmath=sse', '-march=core-avx2', '-fpic', '-std=c99', '-d_posix_c_source=200112l', '-dblis_version_string=""0.5.1""', '-dblis_is_building_library', '-iinclude/linux-x86_64', '-i./frame/3/', '-i./frame/ind/ukernels/', '-i./frame/1m/', '-i./frame/1f/', '-i./frame/1/', '-i./frame/include', '-i/tmp/pip-install-sh0bcfbq/blis/blis/_src/include/linux-x86_64']' returned non-zero exit status 1.

  ----------------------------------------
  failed building wheel for blis
  running setup.py clean for blis
failed to build blis
installing collected packages: blis, thinc, pyrsistent, attrs, jsonschema, pluggy, packaging, more-itertools, py, pytest, pytest-timeout, pbr, mock, pycodestyle, pyflakes, flake8
  running setup.py install for blis ... error
    complete output from command /usr/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-sh0bcfbq/blis/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-record-vq1exsjn/install-record.txt --single-version-externally-managed --compile:
    blis_compiler? none
    running install
    running build
    running build_py
    creating build
    creating build/lib.linux-armv7l-3.7
    creating build/lib.linux-armv7l-3.7/blis
    copying blis/benchmark.py -> build/lib.linux-armv7l-3.7/blis
    copying blis/__init__.py -> build/lib.linux-armv7l-3.7/blis
    copying blis/about.py -> build/lib.linux-armv7l-3.7/blis
    creating build/lib.linux-armv7l-3.7/blis/tests
    copying blis/tests/test_gemm.py -> build/lib.linux-armv7l-3.7/blis/tests
    copying blis/tests/__init__.py -> build/lib.linux-armv7l-3.7/blis/tests
    copying blis/tests/test_dotv.py -> build/lib.linux-armv7l-3.7/blis/tests
    copying blis/tests/common.py -> build/lib.linux-armv7l-3.7/blis/tests
    copying blis/cy.pyx -> build/lib.linux-armv7l-3.7/blis
    copying blis/py.pyx -> build/lib.linux-armv7l-3.7/blis
    copying blis/cy.pxd -> build/lib.linux-armv7l-3.7/blis
    copying blis/__init__.pxd -> build/lib.linux-armv7l-3.7/blis
    running build_ext
    /usr/local/lib/python3.7/dist-packages/cython/compiler/main.py:369: futurewarning: cython directive 'language_level' not set, using 2 for now (py2). this will change in a later release! file: /tmp/pip-install-sh0bcfbq/blis/blis/cy.pxd
      tree = parsing.p_module(s, pxd, full_module_name)
    /usr/local/lib/python3.7/dist-packages/cython/compiler/main.py:369: futurewarning: cython directive 'language_level' not set, using 2 for now (py2). this will change in a later release! file: /tmp/pip-install-sh0bcfbq/blis/blis/py.pyx
      tree = parsing.p_module(s, pxd, full_module_name)
    processing blis/cy.pyx
    processing blis/py.pyx
    unix
    py_compiler gcc
    {'blis_arch': 'generic', 'hostname': 'aa9d42588791', 'ssl_cert_file': '/opt/_internal/certs.pem', 'term': 'xterm', 'oldpwd': '/usr/local/repos/cython-blis', 'ld_library_path': '/opt/rh/devtoolset-2/root/usr/lib64:/opt/rh/devtoolset-2/root/usr/lib:/usr/local/lib64:/usr/local/lib', 'ls_colors': 'no=00:fi=00:di=00;34:ln=00;36:pi=40;33:so=00;35:bd=40;33;01:cd=40;33;01:or=01;05;37;41:mi=01;05;37;41:ex=00;32:*.cmd=00;32:*.exe=00;32:*.com=00;32:*.btm=00;32:*.bat=00;32:*.sh=00;32:*.csh=00;32:*.tar=00;31:*.tgz=00;31:*.arj=00;31:*.taz=00;31:*.lzh=00;31:*.zip=00;31:*.z=00;31:*.z=00;31:*.gz=00;31:*.bz2=00;31:*.bz=00;31:*.tz=00;31:*.rpm=00;31:*.cpio=00;31:*.jpg=00;35:*.gif=00;35:*.bmp=00;35:*.xbm=00;35:*.xpm=00;35:*.png=00;35:*.tif=00;35:', 'virtual_env': '/usr/local/repos/cython-blis/env3.6', 'path': '/usr/local/repos/cython-blis/env3.6/bin:/opt/rh/devtoolset-2/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'pwd': '/usr/local/repos/cython-blis/flame-blis', 'blis_arch': 'generic', 'lang': 'en_us.utf-8', 'home': '/root', 'shlvl': '2', 'language': 'en_us.utf-8', 'blis_arc': 'generic', 'auditwheel_plat': 'manylinux1_x86_64', 'pkg_config_path': '/usr/local/lib/pkgconfig', 'lessopen': '|/usr/bin/lesspipe.sh %s', 'g_broken_filenames': '1', '_': '/usr/local/repos/cython-blis/env3.6/bin/python'}
    gcc -c /tmp/pip-install-sh0bcfbq/blis/blis/_src/config/generic/bli_cntx_init_generic.c -o /tmp/tmp7c_4z8c0/bli_cntx_init_generic.o -o3 -fpic -std=c99 -d_posix_c_source=200112l -dblis_version_string=""0.5.1"" -dblis_is_building_library -iinclude/linux-x86_64 -i./frame/3/ -i./frame/ind/ukernels/ -i./frame/1m/ -i./frame/1f/ -i./frame/1/ -i./frame/include -i/tmp/pip-install-sh0bcfbq/blis/blis/_src/include/linux-x86_64
    gcc -c /tmp/pip-install-sh0bcfbq/blis/blis/_src/config/haswell/bli_cntx_init_haswell.c -o /tmp/tmp7c_4z8c0/bli_cntx_init_haswell.o -o3 -fpic -std=c99 -d_posix_c_source=200112l -dblis_version_string=""0.5.1"" -dblis_is_building_library -iinclude/linux-x86_64 -i./frame/3/ -i./frame/ind/ukernels/ -i./frame/1m/ -i./frame/1f/ -i./frame/1/ -i./frame/include -i/tmp/pip-install-sh0bcfbq/blis/blis/_src/include/linux-x86_64
    gcc -c /tmp/pip-install-sh0bcfbq/blis/blis/_src/config/penryn/bli_cntx_init_penryn.c -o /tmp/tmp7c_4z8c0/bli_cntx_init_penryn.o -o3 -fpic -std=c99 -d_posix_c_source=200112l -dblis_version_string=""0.5.1"" -dblis_is_building_library -iinclude/linux-x86_64 -i./frame/3/ -i./frame/ind/ukernels/ -i./frame/1m/ -i./frame/1f/ -i./frame/1/ -i./frame/include -i/tmp/pip-install-sh0bcfbq/blis/blis/_src/include/linux-x86_64
    gcc -c /tmp/pip-install-sh0bcfbq/blis/blis/_src/config/piledriver/bli_cntx_init_piledriver.c -o /tmp/tmp7c_4z8c0/bli_cntx_init_piledriver.o -o3 -fpic -std=c99 -d_posix_c_source=200112l -dblis_version_string=""0.5.1"" -dblis_is_building_library -iinclude/linux-x86_64 -i./frame/3/ -i./frame/ind/ukernels/ -i./frame/1m/ -i./frame/1f/ -i./frame/1/ -i./frame/include -i/tmp/pip-install-sh0bcfbq/blis/blis/_src/include/linux-x86_64
    gcc -c /tmp/pip-install-sh0bcfbq/blis/blis/_src/config/sandybridge/bli_cntx_init_sandybridge.c -o /tmp/tmp7c_4z8c0/bli_cntx_init_sandybridge.o -o3 -fpic -std=c99 -d_posix_c_source=200112l -dblis_version_string=""0.5.1"" -dblis_is_building_library -iinclude/linux-x86_64 -i./frame/3/ -i./frame/ind/ukernels/ -i./frame/1m/ -i./frame/1f/ -i./frame/1/ -i./frame/include -i/tmp/pip-install-sh0bcfbq/blis/blis/_src/include/linux-x86_64
    gcc -c /tmp/pip-install-sh0bcfbq/blis/blis/_src/config/steamroller/bli_cntx_init_steamroller.c -o /tmp/tmp7c_4z8c0/bli_cntx_init_steamroller.o -o3 -fpic -std=c99 -d_posix_c_source=200112l -dblis_version_string=""0.5.1"" -dblis_is_building_library -iinclude/linux-x86_64 -i./frame/3/ -i./frame/ind/ukernels/ -i./frame/1m/ -i./frame/1f/ -i./frame/1/ -i./frame/include -i/tmp/pip-install-sh0bcfbq/blis/blis/_src/include/linux-x86_64
    gcc -c /tmp/pip-install-sh0bcfbq/blis/blis/_src/kernels/zen/1/bli_amaxv_zen_int.c -o /tmp/tmp7c_4z8c0/bli_amaxv_zen_int.o -o3 -mavx2 -mfma -mfpmath=sse -march=core-avx2 -fpic -std=c99 -d_posix_c_source=200112l -dblis_version_string=""0.5.1"" -dblis_is_building_library -iinclude/linux-x86_64 -i./frame/3/ -i./frame/ind/ukernels/ -i./frame/1m/ -i./frame/1f/ -i./frame/1/ -i./frame/include -i/tmp/pip-install-sh0bcfbq/blis/blis/_src/include/linux-x86_64
    gcc: error: unrecognized -march target: core-avx2
    gcc: note: valid arguments are: armv2 armv2a armv3 armv3m armv4 armv4t armv5 armv5t armv5e armv5te armv5tej armv6 armv6j armv6k armv6z armv6kz armv6zk armv6t2 armv6-m armv6s-m armv7 armv7-a armv7ve armv7-r armv7-m armv7e-m armv8-a armv8.1-a armv8.2-a armv8.3-a armv8.4-a armv8-m.base armv8-m.main armv8-r iwmmxt iwmmxt2 native
    gcc: error: missing argument to ï¿½ï¿½ï¿½-march=ï¿½ï¿½ï¿½
    gcc: error: unrecognized command line option ï¿½ï¿½ï¿½-mavx2ï¿½ï¿½ï¿½
    gcc: error: unrecognized command line option ï¿½ï¿½ï¿½-mfmaï¿½ï¿½ï¿½
    gcc: error: unrecognized command line option ï¿½ï¿½ï¿½-mfpmath=sseï¿½ï¿½ï¿½
  ""<string>"", line 1, in <module>
      file ""/tmp/pip-install-sh0bcfbq/blis/setup.py"", line 277, in <module>
        ""topic :: scientific/engineering"",
      file ""/usr/local/lib/python3.7/dist-packages/setuptools/__init__.py"", line 145, in setup
        return distutils.core.setup(**attrs)
      file ""/usr/lib/python3.7/distutils/core.py"", line 148, in setup
        dist.run_commands()
      file ""/usr/lib/python3.7/distutils/dist.py"", line 966, in run_commands
        self.run_command(cmd)
      file ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
        cmd_obj.run()
      file ""/usr/local/lib/python3.7/dist-packages/setuptools/command/install.py"", line 61, in run
        return orig.install.run(self)
      file ""/usr/lib/python3.7/distutils/command/install.py"", line 589, in run
        self.run_command('build')
      file ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      file ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
        cmd_obj.run()
      file ""/usr/lib/python3.7/distutils/command/build.py"", line 135, in run
        self.run_command(cmd_name)
      file ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      file ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
        cmd_obj.run()
      file ""/usr/lib/python3.7/distutils/command/build_ext.py"", line 340, in run
        self.build_extensions()
      file ""/tmp/pip-install-sh0bcfbq/blis/setup.py"", line 103, in build_extensions
        objects = self.compile_objects(compiler.split(""-"")[0], arch, obj_dir)
      file ""/tmp/pip-install-sh0bcfbq/blis/setup.py"", line 188, in compile_objects
        objects.append(self.build_object(env=env, **spec))
      file ""/tmp/pip-install-sh0bcfbq/blis/setup.py"", line 201, in build_object
        subprocess.check_call(command, cwd=blis_dir)
      file ""/usr/lib/python3.7/subprocess.py"", line 347, in check_call
        raise calledprocesserror(retcode, cmd)
    subprocess.calledprocesserror: command '['gcc', '-c', '/tmp/pip-install-sh0bcfbq/blis/blis/_src/kernels/zen/1/bli_amaxv_zen_int.c', '-o', '/tmp/tmp7c_4z8c0/bli_amaxv_zen_int.o', '-o3', '-mavx2', '-mfma', '-mfpmath=sse', '-march=core-avx2', '-fpic', '-std=c99', '-d_posix_c_source=200112l', '-dblis_version_string=""0.5.1""', '-dblis_is_building_library', '-iinclude/linux-x86_64', '-i./frame/3/', '-i./frame/ind/ukernels/', '-i./frame/1m/', '-i./frame/1f/', '-i./frame/1/', '-i./frame/include', '-i/tmp/pip-install-sh0bcfbq/blis/blis/_src/include/linux-x86_64']' returned non-zero exit status 1.

    ----------------------------------------
command ""/usr/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-sh0bcfbq/blis/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-record-vq1exsjn/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-install-sh0bcfbq/blis/","['python', 'python-3.x', 'raspberry-pi', 'arm', 'spacy']",59957172,"disclaimer
the build might take a long time on a single rpi. if you have a cluster of multiple pi's available, it may be wise to invest into setting up distcc on them. this will also reduce the last step (verifying the build by running the tests) as pytest supports distributed test running on multiple hosts with the pytest-xdist plugin, so running the tests will pass a lot faster on a cluster.
also, although crosscompiling for arm on a x86 system is a lot more faster option, a proper setup may take a lot more time than the slow native compilation, so beware.
get the prebuilt wheels
if you want to save time, i have uploaded the wheels, built using the command sequence in this answer, on github. install using my own index proxy:
$ pip install spacy blis --extra-index-url=

or using direct links:
$ pip install 
$ pip install 

preliminaries
$ uname -a
linux raspberrypi 4.19.57-v7+ #1244 smp thu jul 4 18:45:25 bst 2019 armv7l gnu/linux

$ python3 -v
python 3.7.3

$ pip3 -v
pip 18.1 from /usr/lib/python3/dist-packages/pip (python 3.7)

$ cat /etc/os-release
pretty_name=""raspbian gnu/linux 10 (buster)""
name=""raspbian gnu/linux""
version_id=""10""
version=""10 (buster)""
version_codename=buster
id=raspbian
id_like=debian
home_url=""
support_url=""
bug_report_url=""

preparations
install the compiler, atlas and create a new virtual env for building:
$ sudo apt install build-essential gfortran libatlas-base-dev virtualenv
$ virtualenv spacy-build --python=python3
$ source spacy-build/bin/activate

don't use the builtin venv module for the virtual env creation or you will have to adjust the compilation options yourself, pointing to the env manually.
blis build
this follows (more or less exactly) the steps from the project's readme.
(spacy-build) $ git clone  && cd cython-blis
(spacy-build) $ git pull && git submodule init && git submodule update && git submodule status
(spacy-build) $ pip install -r requirements.txt
(spacy-build) $ export blis_arch=""cortexa57""
(spacy-build) $ ./bin/generate-make-jsonl linux $blis_arch

this last command should run without errors, generating the $blis_arch.jsonl file with system configuration (e.g. cortexa57.jsonl in this example). if this command fails, try an older arch, e.g. cortexa53, cortexa15, cortexa9 etc until it fits (see the list of arch names). for example, for my rpi 3 it's cortexa15. once the compilation is done, build the blis wheel:
(spacy-build) $ python setup.py bdist_wheel

if the command succeeds, check out the contents of the dist dir:
(spacy-build) $ ls dist/
blis-0.4.1-cp37-cp37m-linux_armv7l.whl

spacy build
simply download the source dist and build a wheel from it:
(spacy-build) $ pip wheel spacy --no-binary=spacy --no-build-isolation --find-links=dist/ --wheel-dir=dist/

once the build is done, the dist dir should also contain the spacy wheel:
(spacy-build) $ ls dist/
blis-0.4.1-cp37-cp37m-linux_armv7l.whl
...
spacy-2.2.3-cp37-cp37m-linux_armv7l.whl

you can now copy the wheels somewhere safe and reuse them when in need of reinstalling blis or spacy. to install from the built wheels, issue:
(spacy-build) $ pip install spacy blis --find-links=dist

testing the installation
after you have installed spacy from your built wheel, it may be wise to run the tests to ensure you have actually built something usable. install pytest and test dependencies:
(spacy-build) $ pip install pytest pytest-timeout jsonschema mock
(spacy-build) $ pytest $(python -c ""import spacy; print(spacy.__path__[0])"")

to run the tests distributed on all cores ans speedup the execution, install pytest-xdist in addition and append the -nauto flag to the above command.",https://stackoverflow.com/questions/59927844,python,27-01-2020 09:02,2985.0,2.0,2.0,True,15-08-2021 04:24,28-01-2020 21:22
71335585,"huggingface | valueerror: connection error, and we cannot find the requested files in the cached path. please try again or make sure your internet con","not always, but occasionally when running my code this error appears. at first, i doubted it was a connectivity issue but to do with cashing issue, as discussed on an older git issue.
clearing cache didn't help runtime:
$ rm ~/.cache/huggingface/transformers/*

traceback references:

nltk also gets error loading stopwords: <urlopen error [errno -2] name or service not known.
last 2 lines re cached_path and get_from_cache.


cache (before cleared):
$ cd ~/.cache/huggingface/transformers/
(sdg) me@pf2dcsxd:~/.cache/huggingface/transformers$ ls
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.json
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.lock
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5.json
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5.lock
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.json
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.json
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51.json
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51.lock

code:
from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2')  # error
set_seed(42)

traceback:
2022-03-03 10:18:06.803989: w tensorflow/stream_executor/platform/default/dso_loader.cc:64] could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: no such file or directory
2022-03-03 10:18:06.804057: i tensorflow/stream_executor/cuda/cudart_stub.cc:29] ignore above cudart dlerror if you do not have a gpu set up on your machine.
[nltk_data] error loading stopwords: <urlopen error [errno -2] name or
[nltk_data]     service not known>
2022-03-03 10:18:09.216627: w tensorflow/stream_executor/platform/default/dso_loader.cc:64] could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: no such file or directory
2022-03-03 10:18:09.216700: w tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuinit: unknown error (303)
2022-03-03 10:18:09.216751: i tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pf2dcsxd): /proc/driver/nvidia/version does not exist
2022-03-03 10:18:09.217158: i tensorflow/core/platform/cpu_feature_guard.cc:151] this tensorflow binary is optimized with oneapi deep neural network library (onednn) to use the following cpu instructions in performance-critical operations:  avx2 fma
to enable them in other operations, rebuild tensorflow with the appropriate compiler flags.
2022-03-03 10:18:09.235409: w tensorflow/python/util/util.cc:368] sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
all model checkpoint layers were used when initializing tfgpt2lmheadmodel.

all the layers of tfgpt2lmheadmodel were initialized from the model checkpoint at gpt2.
if your task is similar to the task the model of the checkpoint was trained on, you can already use tfgpt2lmheadmodel for predictions without further training.
traceback (most recent call last):
  file ""/home/me/miniconda3/envs/sdg/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, none,
  file ""/home/me/miniconda3/envs/sdg/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  file ""/mnt/c/users/me/documents/github/project/foo/bar/__main__.py"", line 26, in <module>
    nlp_setup()
  file ""/mnt/c/users/me/documents/github/project/foo/bar/utils/modeling.py"", line 37, in nlp_setup
    generator = pipeline('text-generation', model='gpt2')
  file ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/pipelines/__init__.py"", line 590, in pipeline
    tokenizer = autotokenizer.from_pretrained(
  file ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py"", line 463, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  file ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py"", line 324, in get_tokenizer_config
    resolved_config_file = get_file_from_repo(
  file ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py"", line 2235, in get_file_from_repo
    resolved_file = cached_path(
  file ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py"", line 1846, in cached_path
    output_path = get_from_cache(
  file ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py"", line 2102, in get_from_cache
    raise valueerror(
valueerror: connection error, and we cannot find the requested files in the cached path. please try again or make sure your internet connection is on.


failed attempts

i closed my ide and bash terminal. ran wsl.exe --shutdown in powershell. relaunched ide and bash terminal with same error.
disconnecting and using a different vpn.
clear cache $ rm ~/.cache/huggingface/transformers/*.","['python-3.x', 'tensorflow', 'huggingface-transformers', 'valueerror', 'gpt-2']",71336842,"since i am working in a conda venv and using poetry for handling dependencies, i needed to re-install torch - a dependency for hugging face ï¿½ï¿½ï¿½ï¿½ transformers.

first, install torch:
"" rel=""nofollow noreferrer"">pytorch's website lets you chose your exact setup/ specification for install. in my case, the command was
conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch

then add to poetry:
poetry add torch

both take ages to process. runtime was back to normal :)",https://stackoverflow.com/questions/71335585,python-3.x,03-03-2022 10:28,34753.0,5.0,5.0,True,25-03-2025 15:56,09-09-2024 20:23
76946314,unity using api openai error en la solicitud:  400 bad request,"i am trying to use the openai api in a new unity project. however try whatever i try i always get the same 400 bad request error
i have tried both options interleaving between simple jsom and using  but always the same error.
the reason why i use classes is to be able to use the utility to transform to json because otherwise it can't be done.
i have a two versions of my code:
version1
using unityengine;
using unityengine.networking;
using system.collections;
using system.collections.generic;

public class openaicommunication : monobehaviour
{
    private string apikey = ""sk-.....""; // hide apikey
    private string apiurl = ""

    private void update()
    {
        if(input.getbuttondown(""jump""))
            askquestion(""hola! este es un juego de unity!"");
    }

    ienumerator sendquestion(string question)
    {
        messageslist messages = new messageslist();
        messages.list = new message[] { new message() };
        messages.list[0].role = ""user"";
        messages.list[0].content = question;
        apicall apicall = new apicall();
        apicall.model = ""text-davinci-003"";
        apicall.prompt = question;

        string json = jsonutility.tojson(apicall);
        debug.log(""json: "" + json);

        // configurar el encabezado de autorizaciï¿½ï¿½ï¿½n
        dictionary<string, string> headers = new dictionary<string, string>
        {
            { ""content-type"", ""application/json"" },
            { ""authorization"", ""bearer "" + apikey }
        };
        // headers.add(""authorization"", ""bearer "" + apikey);
        
        // enviar la solicitud post a la api de openai
        using (unitywebrequest www = unitywebrequest.post json))
        {
            foreach (var header in headers)
            {
                debug.log(""header: "" + header.keader.value);
                 header.value);
            }

            yield return 

            if ( == unitywebrequest.result.connectionerror ||  == unitywebrequest.result.protocolerror)
            {
                debug.logerror(""error en la solicitud: "" + 
            }
            else
            {
                string responsejson = 
                // procesar la respuesta json aquï¿½ï¿½ï¿½ (extraer y mostrar la respuesta del asistente)
                debug.log(""response: "" + responsejson);
            }
        }
    }

    public void askquestion(string question)
    {
        startcoroutine(sendquestion(question));
    }
}

[system.serializable]
public class message
{
    public string role;
    public string content;
}
public class messageslist
{
    public message[] list;
}
public class apicall
{
    public string model;
    public string prompt;
}
rsion2
using unityengine;
using unityengine.networking;
using system.collections;
using system.collections.generic;

public class openaicommunication : monobehaviour
{
    private string apikey = ""sk-....""; // hide apikey
    private string apiurl = ""

    private void update()
    {
        if (input.getbuttondown(""jump""))
            askquestion(""hola! este es un juego de unity!"");
    }

    ienumerator sendquestion(string question)
    {
        // configurar el encabezado de autorizaciï¿½ï¿½ï¿½n
        dictionary<string, string> headers = new dictionary<string, string>
        {
            { ""content-type"", ""application/json"" },
            { ""authorization"", ""bearer "" + apikey }
        };
        // headers.add(""authorization"", ""bearer "" + apikey);

        messageslist messages = new messageslist();
        messages.list = new mess{ new message() };
        messages.list[0].role = ""user"";
        messages.list[0].content = question;
        string json = jsonutility.tojson(messages);

        // elimino la parte del string que no corresponde para formatear bien el json.
        // lo eliminado es: { list:  .... y el final }
        json = json.substring(8, json.length - 8 - 1);
        // verifico y es correcto.
        debug.log(""json: "" + json);

        // preparar los datos para la solicitud post utilizo la de chat tambiï¿½ï¿½n para probar pero no cambia nada...
         form = new 
        form.addfield(""model"", ""gpt-3.5-turbo"");
        form.addfield(""messages"", json);

        // enviar la solicitud post a la api de openai
        using (unitywebrequest www = unitywebrequest.post(apiurl, form))
        {
            foreach (var header in headers)
            {
                debug.log(""header: "" + header.key + header.value);
                 header.value);
            }

            yield return 

            if ( == unitywebrequest.result.connectionerror ||  == unitywebrequest.result.protocolerror)
            {
                debug.logerror(""error en la solicitud: "" + 
            }
            else
            {
                string responsejson = 
                // procesar la respuesta json aquï¿½ï¿½ï¿½ (extraer y mostrar la respuesta del asistente)
                debug.log(""response: "" + responsejson);
            }
        }
    }

    public void askquestion(string question)
    {
        startcoroutine(sendquestion(question));
    }
}

[system.serializable]
public class message
{
    public string role;
    public string content;
}
public class messageslist
{
    public message[] list;
}

i use both variations of the code with two of the openai urls but always get the same error.
error:
error en la solicitud:  400 bad request
0x00007ff62ae8229d (unity) stackwalker::getcurrentcallstack
0x00007ff62ae87249 (unity) stackwalker::showcallstack
0x00007ff62be533a1 (unity) getstacktrace
0x00007ff62c514c02 (unity) debugstringtofile
0x00007ff629d98736 (unity) debugloghandler_custom_internal_log
0x0000025ca0dfc363 (mono jit code) (wrapper managed-to-native) unityengine.debugloghandler:internal_log (unityengine.logtype,unityengine.logoption,string,unityengine.object)
0x0000025ca0dfc27b (mono jit code) unityengine.debugloghandler:logformat (unityengine.logtype,unityengine.object,string,object[])
0x0000025ca0dfbfc0 (mono jit code) unityengine.logger:log (unityengine.logtype,object)
0x0000025c7c685ab5 (mono jit code) unityengine.debug:logerror (object)
0x0000025c7c67b363 (mono jit code) openaicommunication/<sendquestion>d__3:movenext () (at d:/projects/unity/ai_project/assets/scripts/openaicommunication.cs:57)
0x0000025c7c67ab00 (mono jit code) unityengine.setupcoroutine:invokemovenext (system.collections.ienumerator,intptr)
0x0000025c7c67ac2f (mono jit code) (wrapper runtime-invoke) <module>:runtime_invoke_void_object_intptr (object,intptr,intptr,intptr)
0x00007ffcf45ee0d4 (mono-2.0-bdwgc) mono_jit_runtime_invoke (at c:/build/output/unity-technologies/mono/mono/mini/mini-runtime.c:3445)
0x00007ffcf452eb74 (mono-2.0-bdwgc) do_runtime_invoke (at c:/build/output/unity-technologies/mono/mono/metadata/object.c:3066)
0x00007ffcf452ed0c (mono-2.0-bdwgc) mono_runtime_invoke (at c:/build/output/unity-technologies/mono/mono/metadata/object.c:3113)
0x00007ff62ad99724 (unity) scripting_method_invoke
0x00007ff62ad77944 (unity) scriptinginvocation::invoke
0x00007ff62ad4041a (unity) coroutine::run
0x00007ff62ad3de0f (unity) coroutine::continuecoroutine
0x00007ff62a9fb653 (unity) asyncoperation::invokecoroutine
0x00007ff62b33d7bc (unity) unitywebrequestasyncoperation::invokecoroutine
0x00007ff62b33d9a1 (unity) unitywebrequestproto<unitywebrequesttransport,atomicrefcounter,redirecthelper,responsehelper,downloadhandler,uploadhandler,certificatehandler,headerhelper,asyncoperation>::job_invokecoroutine
0x00007ff62a9aed9a (unity) backgroundjobqueue::executemainthreadjobs
0x00007ff62aa3127c (unity) `initplayerloopcallbacks'::`2'::earlyupdateexecutemainthreadjobsregistrator::forward
0x00007ff62aa1133a (unity) executeplayerloop
0x00007ff62aa114c6 (unity) executeplayerloop
0x00007ff62aa17d85 (unity) playerloop
0x00007ff62b9dac6f (unity) playerloopcontroller::internalupdatescene
0x00007ff62b9e784d (unity) playerloopcontroller::updatesceneifneededfrommainloop
0x00007ff62b9e5b51 (unity) application::ticktimer
0x00007ff62be59cda (unity) mainmessageloop
0x00007ff62be5f540 (unity) winmain
0x00007ff62d248bae (unity) __scrt_common_main_seh
0x00007ffd46ea7614 (kernel32) basethreadinitthunk
0x00007ffd47c026b1 (ntdll) rtluserthreadstart

i tried the api in my terminal and it worked fine with the -k prefix.
i don't know why i have the 400 error in unity.","['unity-game-engine', 'openai-api']",76954542,"i ended up generating a server with nodejs to see what i got from unity.
then i discovered that the post was sending this information in data:

received data: %7b%22model%22%3a%22gpt-3.5-turbo%22%2c%22messages%22%3a%5b%7b%22role%22%3a%22s
ystem%22%2c%22content%22%3a%22tu%20heres%20el%20master%20en%20un%20juego%20de%20roles%22%7d%2c
%7b%22role%22%3a%22user%22%2c%22content%22%3a%22hola%21%20este%20es%20un%20juego%20de%20unity%
21%22%7d%5d%7d
erorr al formatear el json

then i found a comment on the web that someone said that sometimes it was better to generate the byte string personally and not automatically, the answer was the following:

received data: {""model"":""gpt-3.5-turbo"",""messages"":[{""role"":""system"",""content"":""tu heres el ma
ster en un juego de roles""},{""role"":""user"",""content"":""hola! este es un juego de unity!""}]}

this is the code that finally worked for any of the options in the original post.
using (unitywebrequest www = unitywebrequest.post json))
    {
         = ""post""; //alternative
        //funtional code
        byte[] jsontosend = new system.text.utf8encoding().getbytes(json);
         = (uploadhandler)new uploadhandlerraw(jsontosend);
         = (downloadhandler)new downloadhandlerbuffer();
        //end funtional code

        foreach (var header in headers)
        {
             header.value);
        }

        yield return 

        if ( == unitywebrequest.result.connectionerror ||  == unitywebrequest.result.protocolerror)
        {
            debug.logerror(""error en la solicitud: "" + 
        }
        else
        {
            string responsejson = 
            // procesar la respuesta json aquï¿½ (extraer y mostrar la respuesta del asistente)
            debug.log(""response: "" + responsejson);
        }
    }",https://stackoverflow.com/questions/76946314,unity-game-engine,21-08-2023 14:34,689.0,0.0,1.0,True,19-10-2023 21:28,19-10-2023 21:28
78241665,"how to detect if two sentences are simmilar, not in meaning, but in syllables/words?","here are some examples of the types of sentences that need to be considered ""similar""
there was a most extraordinary noise going on shrinking rapidly she soon made out
there was a most extraordinary noise going on shrinking rapid

that will be a very little alice knew it was just possible it had
thou wilt be very little alice i knew it was possible to add

however at last it sat down and looked very anxiously into her face and
however that lives in sadtown and look very anxiously into him facing it

she went in search of her or of anything to say she simply bowed
she went in the search of her own or of anything to say

and she squeezed herself up on tiptoe and peeped over the wig he did
and she squeezed herself up on the tiptoe and peeped over her wig he did

she had not noticed before and behind it was very glad to find that
she had not noticed before and behind it it was very glad to find that

as soon as the soldiers had to fall a long hookah and taking not
soon as the soldiers have to fall along huka and taking knots

and here are some examples of more difficult edge cases i would be able to like to catch, but are not as necessary
so she tucked it under her arm with its head it would not join
she tucked it under her arm with its head

let me see four times five is twelve and four times five is twelve 
let me see  times  is  and  times  is

let me see four times seven is oh dear run home this moment and 
times  is o dear run home this moment and

in a minute or two she walked sadly down the middle being held up 
and then well see you sidely down the middle in health often

sentences that are somewhat different and have no such similarities need to be marked as dissimilar. if there is an algorithm that exists that outputs a ""score"" versus just a boolean similar or not, i could determine what threshold would be necessary through my own testing.
the top sentence in each example is randomly generated; the bottom sentence is the output of a speech-to-text neural network, from an audio file of someone reading out the top line. if there is some syllabic comparison method that would be much more accurate given that i have the initial source text as well as the audio, i could also employ that instead of this word comparison technique.
my current method involves indexing each word, once forwards, and once reverse, and then checking how many words line up. if at least 10 words match in either indexing order, i count the sentences as similar. however, all of the presented examples are cases where this strategy does not work.","['search', 'nlp', 'full-text-search', 'similarity', 'sentence-similarity']",78259030,"this is exactly similar to my answer above, but this is in nodejs. apart from the language difference, code works exactly the same.
first you need to install the natural module using npm.
    npm install natural


    const natural = require('natural');
    
    function dotproduct(vector1, vector2) {
        return vector1.reduce((acc, val, index) => acc + val * vector2[index], 0);
    }
    
    function magnitude(vector) {
        return math.sqrt(vector.reduce((acc, val) => acc + val * val, 0));
    }
    
    function cosinesimilarity(vector1, vector2) {
        const dotprod = dotproduct(vector1, vector2);
        const mag1 = magnitude(vector1);
        const mag2 = magnitude(vector2);
    
        if (mag1 === 0 || mag2 === 0) {
            return 0; // avoid division by zero
        }
    
        return dotprod / (mag1 * mag2);
    }
    
    function sentencesimilarity(sentence1, sentence2) {
        // tokenizing sentences
        const tokenizer = new natural.wordtokenizer();
        const sentence1tokens = tokenizer.tokenize(sentence1);
        const sentence2tokens = tokenizer.tokenize(sentence2);
    
        // creating a bade of words from tokens
        const bagofwords = new set([...sentence1tokens, ...sentence2tokens]);
    
        // convert tokens to vectors
        const vector1 = array.from(bagofwords).map(word => sentence1tokens.includes(word) ? 1 : 0);
        const vector2 = array.from(bagofwords).map(word => sentence2tokens.includes(word) ? 1 : 0);
    
        // calculate cosine similarity
        const similarity = cosinesimilarity(vector1, vector2);
    
        return similarity;
    }
    
    // example usage
    const sentence1 = ""this is a sentence."";
    const sentence2 = ""this is another sentence."";
    const similarityscore = sentencesimilarity(sentence1, sentence2);
    console.log(""similarity score:"", similarityscore);


function dotproduct(), magnitude(), and cosinesimilarity() were needed to be defined since i was not able to find a library that provides these in node unlike in python. apart from that, all the other logic are similar to the python code above.",https://stackoverflow.com/questions/78241665,search,29-03-2024 02:08,126.0,1.0,2.0,True,02-04-2024 04:54,29-03-2024 02:10
71465239,can&#39;t backward pass two losses in classification transformer model,"for my model i'm using a roberta transformer model and the trainer from the huggingface transformer library.
i calculate two losses:
lloss is a cross entropy loss and dloss calculates the loss inbetween hierarchy layers.
the total loss is the sum of lloss and dloss. (based on this)
when calling total_loss.backwards() however, i get the error:
runtimeerror: trying to backward through the graph a second time, but the buffers have already been freed

any idea why that happens? can i force it to only call backwards once? here is the loss calculation part:
dloss = calculate_dloss(prediction, labels, 3)
lloss = calculate_lloss(predeiction, labels, 3)
total_loss = lloss + dloss 
total_loss.backward()

def calculate_lloss(predictions, true_labels, total_level):
    '''calculates the layer loss.
    '''

    loss_fct = nn.crossentropyloss()

    lloss = 0
    for l in range(total_level):

        lloss += loss_fct(predictions[l], true_labels[l])

    return self.alpha * lloss

def calculate_dloss(predictions, true_labels, total_level):
    '''calculate the dependence loss.
    '''

    dloss = 0
    for l in range(1, total_level):

        current_lvl_pred = torch.argmax(nn.softmax(dim=1)(predictions[l]), dim=1)
        prev_lvl_pred = torch.argmax(nn.softmax(dim=1)(predictions[l-1]), dim=1)

        d_l = self.check_hierarchy(current_lvl_pred, prev_lvl_pred, l)  #just a boolean tensor

        l_prev = torch.where(prev_lvl_pred == true_labels[l-1], torch.floattensor([0]).to(self.device), torch.floattensor([1]).to(self.device))
        l_curr = torch.where(current_lvl_pred == true_labels[l], torch.floattensor([0]).to(self.device), torch.floattensor([1]).to(self.device))

        dloss += torch.sum(torch.pow(self.p_loss, d_l*l_prev)*torch.pow(self.p_loss, d_l*l_curr) - 1)

    return self.beta * dloss","['python', 'neural-network', 'pytorch', 'text-classification']",71465703,"there is nothing wrong with having a loss that is the sum of two individual losses, here is a small proof of principle adapted from the docs:
import torch
import numpy
from sklearn.datasets import make_blobs

class feedforward(torch.nn.module):
    def __init__(self, input_size, hidden_size):
        super(feedforward, self).__init__()
        self.input_size = input_size
        self.hidden_size  = hidden_size
        self.fc1 = torch.nn.linear(self.input_size, self.hidden_size)
        self.relu = torch.nn.relu()
        self.fc2 = torch.nn.linear(self.hidden_size, 1)
        self.sigmoid = torch.nn.sigmoid()
    def forward(self, x):
        hidden = self.fc1(x)
        relu = self.relu(hidden)
        output = self.fc2(relu)
        output = self.sigmoid(output)
        return output

def blob_label(y, label, loc): # assign labels
    target = numpy.copy(y)
    for l in loc:
        target[y == l] = label
    return target

x_train, y_train = make_blobs(n_samples=40, n_features=2, cluster_std=1.5, shuffle=true)
x_train = torch.floattensor(x_train)
y_train = torch.floattensor(blob_label(y_train, 0, [0]))
y_train = torch.floattensor(blob_label(y_train, 1, [1,2,3]))

x_test, y_test = make_blobs(n_samples=10, n_features=2, cluster_std=1.5, shuffle=true)
x_test = torch.floattensor(x_test)
y_test = torch.floattensor(blob_label(y_test, 0, [0]))
y_test = torch.floattensor(blob_label(y_test, 1, [1,2,3]))


model = feedforward(2, 10)
criterion = torch.nn.bceloss()
optimizer = torch.optim.sgd(model.parameters(), lr = 0.01)


model.eval()
y_pred = model(x_test)
before_train = criterion(y_pred.squeeze(), y_test)
print('test loss before training' , before_train.item())

model.train()
epoch = 20
for epoch in range(epoch):
    optimizer.zero_grad()    # forward pass
    y_pred = model(x_train)    # compute loss
    lossce= criterion(y_pred.squeeze(), y_train)
    losssqd = (y_pred.squeeze()-y_train).pow(2).mean()
    loss=lossce+losssqd
    print('epoch {}: train loss: {}'.format(epoch, loss.item()))    # backward pass
    loss.backward()
    optimizer.step()

there must be a real second time that you call directly or indirectly backward on some varaible that then traverses through your graph. it is a bit too much to ask for the complete code here, only you can check this or at least reduce it to a minimal example (while doing so, you might already find the issue). apart from that, i would start checking:

does it already occur in the first iteration of training? if not: are you reusing any calculation results for the second iteration without a detach?
when you do backward on your losses individually lloss.backward() followed by dloss.backward() (this has the same effect as adding them together first as gradients are accumulated): what happens? this will let you track down for which of the two losses the error occurs.",https://stackoverflow.com/questions/71465239,python,14-03-2022 09:06,614.0,0.0,2.0,True,25-01-2023 02:53,14-03-2022 09:15
2696392,i want a machine to learn to categorize short texts,"i have a ton of short stories about 500 words long and i want to categorize them into one of, let's say, 20 categories:

entertainment
food
music
etc

i can hand-classify a bunch of them, but i want to implement machine learning to guess the categories eventually. what's the best way to approach this? is there a standard approach to machine learning i should be using? i don't think a decision tree would work well since it's text data...","machine-learning, nlp, classification",2697080,"a naive bayes will most probably work for you. the method is like this: 

fix a number of categories and get a training data set of (document, category) pairs.
a data vector of your document will be sth like a bag of words. e.g. take the 100 most common words except words like ""the"", ""and"" and such. each word gets a fixed component of your data vector (e.g. ""food"" is position 5). a feature vector is then an array of booleans, each indicating whether that word came up in the corresponding document.

training:

for your training set, calculate the probability of every feature and every class: p(c) = number documents of class c / total number of documents.
calculate the probability of a feature in a class: p(f|c) = number of documents of class with given feature (= word ""food"" is in the text) / number of documents in given class.

decision: 

given an unclassified document, the probability of it belonging to class c is proportional to p(c|f1, ..., f500) = p(c) * p(f1|c) * p(f2|c) * ... * p(f500|c). pick the c that maximizes this term. 
since multiplication is numerically difficult, you can use the sum of the logs instead, which is maximized at the same c: log p(c|f1, ..., f500) = log p(c) + log p(f1|c) + log p(f2|c) + ... + log p(f500|c).",https://stackoverflow.com/q/2696392,"machine-learning, nlp, classification",23-04-2010 05:23,10636.0,20.0,8.0,True,23-05-2023 15:39,23-05-2023 15:39
75812086,how can i stream using chatgpt with delphi?,"i am playing around with chatgpt and delphi, using the openai library at:  it supports streaming, but i can't figure out the chatgpt mechanism for streaming.  i can create a chat, and get all data back in one return message.
however, when i try to use streaming, i get an error. the following console code works fine.  i submit my chat, and i get the entire answer back in one ""event"".  i would like the same behavior as the chatgpt website, so the tokens would be displayed as they are generated. my code is as follows...
var buf : tstringlist;
begin
...
 var chat := openai.chat.create(
           procedure(params: tchatparams)
       begin
          params.messages([tchatmessagebuild.create(tmessagerole.user, buf.text)]);
          params.maxtokens(1024);
         // params.stream(true);
        end);
       try
            for var choice in chat.choices do
              begin

                buf.add(choice.message.content);
                writeln(choice.message.content);
              end;
        finally
         chat.free;
      end;

this code works.  when i try to turn on streaming, i get the econversionerror 'the input value is not a valid object', which causes chatgpt to return 'empty or invalid response'.","['delphi', 'openai-api', 'chatgpt-api']",75812753,"because in this mode, it responds in this case not with a json object, but in its own special format.
data: {""id"": ""cmpl-6wsvxtku0tzrram4xpf5itxyw9ctf"", ""object"": ""text_completion"", ""created"": 1679490597, ""choices"": [{""text"": ""\r"", ""index"": 0, ""logprobs"": null, ""finish_reason"": null}], ""model"": ""text-davinci-003""}

data: {""id"": ""cmpl-6wsvxtku0tzrram4xpf5itxyw9ctf"", ""object"": ""text_completion"", ""created"": 1679490597, ""choices"": [{""text"": ""\n"", ""index"": 0, ""logprobs"": null, ""finish_reason"": null}], ""model"": ""text-davinci-003""}

data: {""id"": ""cmpl-6wsvxtku0tzrram4xpf5itxyw9ctf"", ""object"": ""text_completion"", ""created"": 1679490597, ""choices"": [{""text"": ""1"", ""index"": 0, ""logprobs"": null, ""finish_reason"": null}], ""model"": ""text-davinci-003""}

data: {""id"": ""cmpl-6wsvxtku0tzrram4xpf5itxyw9ctf"", ""object"": ""text_completion"", ""created"": 1679490597, ""choices"": [{""text"": "","", ""index"": 0, ""logprobs"": null, ""finish_reason"": null}], ""model"": ""text-davinci-003""}

data: {""id"": ""cmpl-6wsvxtku0tzrram4xpf5itxyw9ctf"", ""object"": ""text_completion"", ""created"": 1679490597, ""choices"": [{""text"": "" 2"", ""index"": 0, ""logprobs"": null, ""finish_reason"": null}], ""model"": ""text-davinci-003""}
...

i can start working on such a mode for the library.",https://stackoverflow.com/questions/75812086,delphi,22-03-2023 12:10,780.0,1.0,1.0,True,10-04-2023 18:39,10-04-2023 18:39
75282891,how to merge predicted values to original pandas test data frame where x_test has been converted using countvectorizer before splitting,"i want to merge my predicted results of my test data to my x_test. i was able to merge it with y_test but since my x_test is a corpus i'm not sure how i can identify the indexes to merge.
my codes are as below
def lr_model(df):

    from sklearn.feature_extraction.text import countvectorizer
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import logisticregression
    import pandas as pd
   
    # create corpus as a list
    corpus = df['text'].tolist()
    cv = countvectorizer()
    x = cv.fit_transform(corpus).toarray()
    y = df.iloc[:, -1].values

    # splitting to testing and training
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

    # train logistic regression on training set
    classifier = logisticregression(random_state = 0)
    classifier.fit(x_train, y_train)

    # predicting the test set results
    y_pred = classifier.predict(x_test)

    # merge true vs predicted labels
    true_vs_pred = pd.dataframe(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

    return true_vs_pred

this gives me the y_test and y_pred but i'm not sure how i can add the x_test as an original data frame (the ids of the x_test) to this.
any guidance is much appreciated. thanks","['python', 'pandas', 'scikit-learn', 'nlp']",75306378,"using a pipeline can help you link the original x_test with the prediction:
def lr_model(df):

    from sklearn.feature_extraction.text import countvectorizer
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import logisticregression
    import pandas as pd
    from sklearn.pipeline import pipeline

    # defining x and y
    cv = countvectorizer()
    x = df['text']
    y = df.iloc[:, -1].values

    # splitting to testing and training
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

    # create a pipeline
    pipeline = pipeline([
        ('countvectorizer', cv),
        ('logisticregression', logisticregression(random_state = 0)),
    ])

    # train pipeline on training set
    pipeline.fit(x_train, y_train)

    # predicting the test set results
    y_pred = pipeline.predict(x_test)

    return  x_test, y_test, y_pred",https://stackoverflow.com/questions/75282891,python,30-01-2023 10:12,441.0,0.0,1.0,True,03-02-2023 07:45,30-01-2023 19:49
77068901,discord.py v2.0: slash command with cogs integrated with openai api,"i have been making a discord bot using discord.py v2.0 and openai api to make a ai bot for my personal server. so firstly i will share my code.
this the main.py file:
import os
import asyncio

import discord
from discord.ext import commands

intents = discord.intents.all()
ceriumai = commands.bot(
    command_prefix=""<>"",
    intents=intents
)

@ceriumai.event
async def on_ready():
    print(""ceriumai is ready and online!"")

@ceriumai.command()
async def sync(ctx):
    synced = await ceriumai.tree.sync()
    print(f""synced {len(synced)} command(s)."")

async def loadcogs():
    for filename in os.listdir(""./cogs""):
        if filename.endswith("".py""):
            await ceriumai.load_extension(f""cogs.{filename[:-3]}"")
            print(f""loaded the cog: {filename[:-3]}"")

async def main():
    await loadcogs()
    await ceriumai.start(os.getenv(""token""))

asyncio.run(main())

this is the other file/cog, askai.py:
import openai
import discord
from discord import app_commands
from discord.ext import commands

class ask_ai(commands.cog):
    def __init__(self, ceriumai):
        self.ceriumai = ceriumai
        self.openai.api_key = ""my-api-key""
        self.messages = [{""role"": ""system"",""content"":""you are a intelligent assistant.""}]
    
    @app_commands.command(name=""askai"")
    async def ask_ai(self, interaction: discord.interaction, query: str):
        while true:
            message = query
            if message:
                self.messages.append(
                    {""role"": ""user"", ""content"": message}
                )
                chat = openai.chatcompletion.create(
                    model=""gpt-3.5-turbo"", messages=self.messages
                )
            reply = chat.choices[0].message.content
            await interaction.response.send_message(f""{reply}"")
            self.messages.append({""role"": ""assistant"", ""content"": reply})
                

async def setup(bot):
    await bot.add_cog(ask_ai(bot))

so the problem:
basically, the slash command works. the weird part is that when i use the command and add queries like ""hi"", ""hey"", etc. it responds quickly to the user. but when i add a query like ""what is your name?"", etc. or just queries in which chatgpt takes time to respond, the bot shows ""the application did not respond"". my guess is that there is a response time limit of a bot in a slash command which when expires gives the message ""application did not respond"". i would like to know if such a thing exists and if yes, how to extend it for the ai to respond by taking its own time and get the longer reply. if no, then a bypass to this problem. thanks :)","['python', 'python-3.x', 'discord', 'discord.py', 'openai-api']",77069067,"you are correct.
there is a time limit of 3 seconds for the interaction response.
in order to avoid the interaction from timing out and returning the the application did not respond message, you can use the defer() method included in the response class.
when deferring, you get up to 15 minutes to respond instead of the normal 3 seconds.
also, please note that because deferring counts as a response, you can't use the typical response.send_message method. instead you can use the followup method, also included in the response class.
await interaction.response.defer()
[...]
await interaction.followup.send(...)

docs on deferring
docs on followup",https://stackoverflow.com/questions/77068901,python,08-09-2023 17:36,167.0,0.0,1.0,True,08-09-2023 18:12,08-09-2023 17:51
34197917,python fuzzy search and replace,"i need to perfom fuzzy search for sub-string in string and replace that part. for example:
str_a = ""alabama""
str_b = ""replaced""
orig_str = ""flabama is a state located in the southeastern region of the united states.""
print(fuzzy_replace(str_a, str_b, orig_str)) # fuzzy_replace code should be implemented
# output: replaced is a state located in the southeastern region of the united states.

the search itself is simple with fuzzywuzzy module, but it gives me only ratio of difference between strings. are there any ways to find a position in original string where sub-string fuzzy matches to?","['python', 'string', 'nlp', 'fuzzy-search']",34198919,"try this..
from fuzzywuzzy import fuzz

def fuzzy_replace(str_a, str_b, orig_str):
    l = len(str_a.split()) # length to read orig_str chunk by chunk
    splitted = orig_str.split()
    for i in range(len(splitted)-l+1):
        test = "" "".join(splitted[i:i+l])
        if fuzz.ratio(str_a, test) > 75: #using fuzzwuzzy library to test ratio
            before = "" "".join(splitted[:i])
            after = "" "".join(splitted[i+1:])
            return before+"" ""+str_b+"" ""+after #output will be sandwich of these three strings

str_a = ""alabama is a""
str_b = ""replaced""
orig_str = ""flabama is a state located in the southeastern region of the united states.""
print fuzzy_replace(str_a, str_b, orig_str)

this prints
 replaced state located in the southeastern region of the united states.",https://stackoverflow.com/questions/34197917,python,10-12-2015 09:22,2916.0,2.0,1.0,True,12-09-2024 05:46,10-12-2015 09:46
77759618,"typeerror: expected string or buffer - langchain, openai embeddings","i am trying to create rag using the product manuals in pdf which are splitted, indexed and stored in chroma persisted on a disk. when i try the function that classifies the reviews using the documents context, below is the error i get:

from langchain import prompttemplate
from langchain_core.output_parsers import stroutputparser
from langchain_core.runnables import runnablepassthrough
from langchain.embeddings import azureopenaiembeddings
from langchain.chat_models import azurechatopenai
from langchain.vectorstores import chroma

llm = azurechatopenai(
        azure_deployment=""chatgpt-16k"",
        openai_api_version=""2023-05-15"",
        azure_endpoint=endpoint,
        api_key=result[""access_token""],
        temperature=0,
        seed = 100
    )

embedding_model = azureopenaiembeddings(
    api_version=""2023-05-15"",
    azure_endpoint=endpoint,
    api_key=result[""access_token""],
    azure_deployment=""ada002"",
)

vectordb = chroma(
    persist_directory=vector_db_path,
    embedding_function=embedding_model,
    collection_name=""product_manuals"",
)


def format_docs(docs):
    return ""\n\n"".join(doc.page_content for doc in docs)

def classify (review_title, review_text, product_num):

    template = """"""
        
    you are a customer service ai assistant that handles responses to negative product reviews. 

       use the context below and categorize {review_title} and {review_text} into defect, misuse or poor quality categories based only on provided context. if you don't know, say that you do not know, don't try to make up an answer. respond back with an answer in the following format:

        poor quality
        misuse
        defect

        {context}
            
    category: 
    """"""


    rag_prompt = prompttemplate.from_template(template)
    
    retriever = vectordb.as_retriever(search_type=""similarity"", search_kwargs={'filter': {'product_num': product_num}})


    retrieval_chain = (
            {""context"": retriever | format_docs, ""review_title: runnablepassthrough(), ""review_text"": runnablepassthrough()}
            | rag_prompt
            | llm
            | stroutputparser()
    )
    return retrieval_chain.invoke({""review_title"": review_title, ""review_text"": review_text})

classify(review_title=""terrible"", review_text =""this baking sheet is terrible. it stains so easily and i've tried everything to get it clean"", product_num =""8888999"")

error stack:
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
file <command-3066972537097411>, line 1
----> 1 issue_recommendation(
      2     review_title=""terrible"",
      3     review_text=""this baking sheet is terrible. it stains so easily and i've tried everything to get it clean. i've maybe used it 5 times and it looks like it's 20 years old. the side of the pan also hold water, so when you pick it up off the drying rack, water runs out. i would never purchase these again."",
      4     product_num=""8888999""
      5    
      6 )

file <command-3066972537097410>, line 44, in issue_recommendation(review_title, review_text, product_num)
     36 retriever = vectordb.as_retriever(search_type=""similarity"", search_kwargs={'filter': {'product_num': product_num}})
     38 retrieval_chain = (
     39         {""context"": retriever | format_docs, ""review_text"": runnablepassthrough()}
     40         | rag_prompt
     41         | llm
     42         | stroutputparser()
     43 )
---> 44 return retrieval_chain.invoke({""review_title"":review_title, ""review_text"": review_text})

file /local_disk0/.ephemeral_nfs/envs/pythonenv-65a09d8c-062d-4f4f-9c52-1bf534f6511e/lib/python3.10/site-packages/langchain_core/runnables/base.py:1762, in runnablesequence.invoke(self, input, config)
   1760 try:
   1761     for i, step in enumerate(self.steps):
-> 1762         input = step.invoke(
   1763             input,
   1764             # mark each step as a child run
   1765             patch_config(
   1766                 config, callbacks=run_manager.get_child(f""seq:step:{i+1}"")
   1767             ),
   1768         )
   1769 # finish the root run
   1770 except baseexception as e:

file /local_disk0/.ephemeral_nfs/envs/pythonenv-65a09d8c-062d-4f4f-9c52-1bf534f6511e/lib/python3.10/site-packages/langchain_core/runnables/base.py:2327, in runnableparallel.invoke(self, input, config)
   2314     with get_executor_for_config(config) as executor:
   2315         futures = [
   2316             executor.submit(
   2317                 step.invoke,
   (...)
   2325             for key, step in steps.items()
   2326         ]
-> 2327         output = {key: future.result() for key, future in zip(steps, futures)}
   2328 # finish the root run
   2329 except baseexception as e:

file /local_disk0/.ephemeral_nfs/envs/pythonenv-65a09d8c-062d-4f4f-9c52-1bf534f6511e/lib/python3.10/site-packages/langchain_core/runnables/base.py:2327, in <dictcomp>(.0)
   2314     with get_executor_for_config(config) as executor:
   2315         futures = [
   2316             executor.submit(
   2317                 step.invoke,
   (...)
   2325             for key, step in steps.items()
   2326         ]
-> 2327         output = {key: future.result() for key, future in zip(steps, futures)}
   2328 # finish the root run
   2329 except baseexception as e:

file /usr/lib/python3.10/concurrent/futures/_base.py:451, in future.result(self, timeout)
    449     raise cancellederror()
    450 elif self._state == finished:
--> 451     return self.__get_result()
    453 self._condition.wait(timeout)
    455 if self._state in [cancelled, cancelled_and_notified]:

file /usr/lib/python3.10/concurrent/futures/_base.py:403, in future.__get_result(self)
    401 if self._exception:
    402     try:
--> 403         raise self._exception
    404     finally:
    405         # break a reference cycle with the exception in self._exception
    406         self = none

file /usr/lib/python3.10/concurrent/futures/thread.py:58, in _workitem.run(self)
     55     return
     57 try:
---> 58     result = self.fn(*self.args, **self.kwargs)
     59 except baseexception as exc:
     60     self.future.set_exception(exc)

file /local_disk0/.ephemeral_nfs/envs/pythonenv-65a09d8c-062d-4f4f-9c52-1bf534f6511e/lib/python3.10/site-packages/langchain_core/runnables/base.py:1762, in runnablesequence.invoke(self, input, config)
   1760 try:
   1761     for i, step in enumerate(self.steps):
-> 1762         input = step.invoke(
   1763             input,
   1764             # mark each step as a child run
   1765             patch_config(
   1766                 config, callbacks=run_manager.get_child(f""seq:step:{i+1}"")
   1767             ),
   1768         )
   1769 # finish the root run
   1770 except baseexception as e:

file /local_disk0/.ephemeral_nfs/envs/pythonenv-65a09d8c-062d-4f4f-9c52-1bf534f6511e/lib/python3.10/site-packages/langchain_core/retrievers.py:121, in baseretriever.invoke(self, input, config)
    117 def invoke(
    118     self, input: str, config: optional[runnableconfig] = none
    119 ) -> list[document]:
    120     config = ensure_config(config)
--> 121     return self.get_relevant_documents(
    122         input,
    123         callbacks=config.get(""callbacks""),
    124         tags=config.get(""tags""),
    125         metadata=config.get(""metadata""),
    126         run_name=config.get(""run_name""),
    127     )

file /local_disk0/.ephemeral_nfs/envs/pythonenv-65a09d8c-062d-4f4f-9c52-1bf534f6511e/lib/python3.10/site-packages/langchain_core/retrievers.py:223, in baseretriever.get_relevant_documents(self, query, callbacks, tags, metadata, run_name, **kwargs)
    221 except exception as e:
    222     run_manager.on_retriever_error(e)
--> 223     raise e
    224 else:
    225     run_manager.on_retriever_end(
    226         result,
    227         **kwargs,
    228     )

file /local_disk0/.ephemeral_nfs/envs/pythonenv-65a09d8c-062d-4f4f-9c52-1bf534f6511e/lib/python3.10/site-packages/langchain_core/retrievers.py:216, in baseretriever.get_relevant_documents(self, query, callbacks, tags, metadata, run_name, **kwargs)
    214 _kwargs = kwargs if self._expects_other_args else {}
    215 if self._new_arg_supported:
--> 216     result = self._get_relevant_documents(
    217         query, run_manager=run_manager, **_kwargs
    218     )
    219 else:
    220     result = self._get_relevant_documents(query, **_kwargs)

file /local_disk0/.ephemeral_nfs/envs/pythonenv-65a09d8c-062d-4f4f-9c52-1bf534f6511e/lib/python3.10/site-packages/langchain_core/vectorstores.py:654, in vectorstoreretriever._get_relevant_documents(self, query, run_manager)
    650 def _get_relevant_documents(
    651     self, query: str, *, run_manager: callbackmanagerforretrieverrun
    652 ) -> list[document]:
    653     if self.search_type == ""similarity"":
--> 654         docs = self.vectorstore.similarity_search(query, **self.search_kwargs)
    655     elif self.search_type == ""similarity_score_threshold"":
    656         docs_and_similarities = (
    657             self.vectorstore.similarity_search_with_relevance_scores(
    658                 query, **self.search_kwargs
    659             )
    660         )

file /local_disk0/.ephemeral_nfs/envs/pythonenv-65a09d8c-062d-4f4f-9c52-1bf534f6511e/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:348, in chroma.similarity_search(self, query, k, filter, **kwargs)
    331 def similarity_search(
    332     self,
    333     query: str,
   (...)
    336     **kwargs: any,
    337 ) -> list[document]:
    338     """"""run similarity search with chroma.
    339 
    340     args:
   (...)
    346         list[document]: list of documents most similar to the query text.
    347     """"""
--> 348     docs_and_scores = self.similarity_search_with_score(
    349         query, k, filter=filter, **kwargs
    350     )
    351     return [doc for doc, _ in docs_and_scores]

file /local_disk0/.ephemeral_nfs/envs/pythonenv-65a09d8c-062d-4f4f-9c52-1bf534f6511e/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:437, in chroma.similarity_search_with_score(self, query, k, filter, where_document, **kwargs)
    429     results = self.__query_collection(
    430         query_texts=[query],
    431         n_results=k,
   (...)
    434         **kwargs,
    435     )
    436 else:
--> 437     query_embedding = self._embedding_function.embed_query(query)
    438     results = self.__query_collection(
    439         query_embeddings=[query_embedding],
    440         n_results=k,
   (...)
    443         **kwargs,
    444     )
    446 return _results_to_docs_and_scores(results)

file /local_disk0/.ephemeral_nfs/envs/pythonenv-65a09d8c-062d-4f4f-9c52-1bf534f6511e/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:691, in openaiembeddings.embed_query(self, text)
    682 def embed_query(self, text: str) -> list[float]:
    683     """"""call out to openai's embedding endpoint for embedding query text.
    684 
    685     args:
   (...)
    689         embedding for the text.
    690     """"""
--> 691     return self.embed_documents([text])[0]

file /local_disk0/.ephemeral_nfs/envs/pythonenv-65a09d8c-062d-4f4f-9c52-1bf534f6511e/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:662, in openaiembeddings.embed_documents(self, texts, chunk_size)
    659 # note: to keep things simple, we assume the list may contain texts longer
    660 #       than the maximum context and use length-safe embedding function.
    661 engine = cast(str, self.deployment)
--> 662 return self._get_len_safe_embeddings(texts, engine=engine)

file /local_disk0/.ephemeral_nfs/envs/pythonenv-65a09d8c-062d-4f4f-9c52-1bf534f6511e/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:465, in openaiembeddings._get_len_safe_embeddings(self, texts, engine, chunk_size)
    459 if self.model.endswith(""001""):
    460     # see: 
    461     #      issues/418#issuecomment-1525939500
    462     # replace newlines, which can negatively affect performance.
    463     text = text.replace(""\n"", "" "")
--> 465 token = encoding.encode(
    466     text=text,
    467     allowed_special=self.allowed_special,
    468     disallowed_special=self.disallowed_special,
    469 )
    471 # split tokens into chunks respecting the embedding_ctx_length
    472 for j in range(0, len(token), self.embedding_ctx_length):

file /databricks/python/lib/python3.10/site-packages/tiktoken/core.py:116, in encoding.encode(self, text, allowed_special, disallowed_special)
    114     if not isinstance(disallowed_special, frozenset):
    115         disallowed_special = frozenset(disallowed_special)
--> 116     if match := _special_token_regex(disallowed_special).search(text):
    117         raise_disallowed_special_token(match.group())
    119 try:

typeerror: expected string or buffer


embeddings seems to work fine when i test. it also works fine when i remove the context and retriever from the chain. it seems to be related to embeddings. examples on langchain website instantiates retriver from chroma.from_documents() whereas i load chroma vector store from a persisted path. i also tried invoking with review_text only (instead of review title and review text) but the error persists. not sure why this is happening. these are the package versions i work:
name: openai
version: 1.6.1
name: langchain
version: 0.0.354","['openai-api', 'langchain', 'azure-openai', 'openaiembeddings', 'retrieval-augmented-generation']",77763155,"i've come across the same issue, and turned out that langchain pass a key-value pair as an input to the encoding.code() while it requires str type. a work around is by using itemgetter() to get the direct string input. it might be something like this
        retrieval_chain = (
            {
                ""document"": itemgetter(""question"") | self.retriever,
                ""question"": itemgetter(""question""),
            }
            | prompt
            | model
            | stroutputparser()
        )

you can find the reference here",https://stackoverflow.com/questions/77759618,openai-api,04-01-2024 16:01,2348.0,2.0,1.0,True,05-01-2024 07:51,04-01-2024 18:35
71280615,spacy adds words automatically to vocab?,"i loaded regular spacy language, and tries the following code:
import spacy

nlp = spacy.load(""en_core_web_md"")

text = ""xxasdfdsfsdzz is the first u.s. public company""

if 'xxasdfdsfsdzz' in nlp.vocab:
    print(""in"")
else:
    print(""not"")
    
if 'apple' in nlp.vocab:
    print(""in"")
else:
    print(""not"")


# process the text
doc = nlp(text)

if 'xxasdfdsfsdzz' in nlp.vocab:
    print(""in"")
else:
    print(""not"")
    
if 'apple' in nlp.vocab:
    print(""in"")
else:
    print(""not"")

it seems like spacy loaded words after they called to analyze - nlp(text)
can someone explain the output? how can i avoid it? why ""apple"" is not existing in vocab? and why ""xxasdfdsfsdzz"" exists?
output:
not
not
in
not","['python', 'python-3.x', 'nlp', 'spacy']",71290584,"the spacy vocab is mainly an internal implementation detail to interface with a memory-efficient method of storing strings. it is definitely not a list of ""real words"" or any other thing that you are likely to find useful.
the main thing a vocab stores by default is strings that are used internally, such as pos and dependency labels. in pipelines with vectors, words in the vectors are also included. you can read more about the implementation details here.
all words an nlp object has seen need storage for their strings, and so will be present in the vocab. that's what you're seeing with your nonsense string in the example above.",https://stackoverflow.com/questions/71280615,python,26-02-2022 22:03,771.0,2.0,1.0,True,28-02-2022 04:26,27-02-2022 11:16
76961751,next.js 13 build error for private identifiers when using openai 4.0 package,"i added the new 4.0 openai package in my next.js app, i'm using it in the server of the app only and when building i get the error.
- info linting and checking validity of types .failed to compile.

./node_modules/openai/src/core.ts:539:3
type error: private identifiers are only available when targeting ecmascript 2015 and higher.

  537 |
  538 | export abstract class abstractpage<item> implements asynciterable<item> {
> 539 |   #client: apiclient;
      |   ^
  540 |   protected options: finalrequestoptions;
  541 |
  542 |   protected response: response;



i tried changing the tsconfig target to es6,es2015,esnext, but it doesn't seem to work, skiplibcheck is enabled so i'm even more confused as of why it happens
this is my tsconfig:
{
  ""compileroptions"": {
    ""target"": ""es5"",
    ""lib"": [
      ""dom"",
      ""dom.iterable"",
      ""esnext""
    ],
    ""allowjs"": true,
    ""skiplibcheck"": true,
    ""strict"": true,
    ""forceconsistentcasinginfilenames"": true,
    ""noemit"": true,
    ""esmoduleinterop"": true,
    ""module"": ""esnext"",
    ""moduleresolution"": ""node"",
    ""resolvejsonmodule"": true,
    ""isolatedmodules"": true,
    ""jsx"": ""preserve"",
    ""incremental"": true,
    ""plugins"": [
      {
        ""name"": ""next""
      }
    ],
    ""paths"": {
      ""$/*"": [
        ""./src/*""
      ],
      ""$app/*"": [
        ""./src/app/*""
      ],
      ""$cmp/*"": [
        ""./src/components/*""
      ],
      ""$types/*"": [
        ""./src/types/*""
      ],
      ""$assets/*"": [
        ""./src/assets/*""
      ],
      ""$lib/*"": [
        ""./src/lib/*""
      ],
    }
  },
  ""include"": [
    ""next-env.d.ts"",
    ""**/*.ts"",
    ""**/*.tsx"",
    "".next/types/**/*.ts""
  ],
  ""exclude"": [
    ""node_modules""
  ]
}","['node.js', 'reactjs', 'next.js', 'openai-api']",78962495,"use this import (no ""src"" in import):
import { type chatcompletionmessageparam } from ""openai/resources/index.mjs"";

instead of :
import { chatcompletionmessageparam } from ""openai/src/resources/chat/completions"";",https://stackoverflow.com/questions/76961751,node.js,23-08-2023 13:19,525.0,0.0,3.0,True,08-09-2024 13:39,23-08-2023 16:15
44857382,change nltk.download() path directory from default ~/ntlk_data,"i was trying to download/update python nltk packages on a computing server and it returned this [errno 122] disk quota exceeded: error.
specifically:
[nltk_data] downloading package stop words to /home/sh2264/nltk_data...
[nltk_data] error downloading u'stopwords' from
[nltk_data] <
[nltk_data] pages/packages/corpora/stopwords.zip>: [errno 122]
[nltk_data] disk quota exceeded:
[nltk_data] u'/home/sh2264/nltk_data/corpora/stopwords.zip
false

how could i change the entire path for nltk packages, and what other changes should i make to ensure errorless loading of nltk?","['python', 'python-2.7', 'path', 'nltk', 'default']",47082481,"this can be configured both by command-line (nltk.download(..., download_dir=) or by gui. bizarrely nltk seems to totally ignore its own environment variable nltk_data and default its download directories to a standard set of five paths, regardless whether nltk_data is defined and where it points, and regardless whether nltk's five default dirs even exist on the machine or architecture(!). some of that is documented in installing nltk data, although it's incomplete and kinda buried; reproduced below with much clearer formatting:

command line installation
the downloader will search for an existing nltk_data directory to
  install nltk data. if one does not exist it will attempt to create one
  in a central location (when using an administrator account) or
  otherwise in the userï¿½ï¿½ï¿½s filespace. if necessary, run the download
  command from an administrator account, or using sudo. the recommended
  system location is:

c:\nltk_data (windows) ;
/usr/local/share/nltk_data (mac) and
/usr/share/nltk_data (unix).

you can use the -d flag to specify a different location (but if you do this, be sure to set the nltk_data environment variable accordingly).

run the command python -m nltk.downloader all
to ensure central instan, run the command: sudo python -m nltk.downloader -d /usr/local/share/nltk_data all
but really they should say: sudo python -m nltk.downloader -d $nltk_data all


now as to what recommended path nltk_data should use, nltk doesn't really give any proper guidance, but it should be a generic standalone path not under any install tree (so not under <python-install-directory>/lib/site-packages) or any user dir. hence, /usr/local/share, /opt/share or similar. on macos 10.7+, /usr and thus /usr/local/ these days are hidden by default, so /opt/share may well be a better choice. or do chflags nohidden /usr/local/share.",https://stackoverflow.com/questions/44857382,python,01-07-2017 04:42,54181.0,36.0,8.0,True,24-01-2024 18:24,02-11-2017 17:55
59435020,get probability of multi-token word in mask position,"it is relatively easy to get a token's probability according to a language model, as the snippet below shows. you can get the output of a model, restrict yourself to the output of the masked token, and then find the probability of your requested token in the output vector. however, this only works with single-token words, e.g. words that are themselves in the tokenizer's vocabulary. when a word does not exist in the vocabulary, the tokenizer will chunk it up into pieces that it does know (see the bottom of the example). but since the input sentence consists of only one masked position, and the requested token has more tokens than that, how can we get its probability? ultimately i am looking for a solution that works regardless of the number of subword units a word has.
in the code below i have added many comments explaining what is going on, as well as printing out the given output of print statements. you'll see that predicting tokens such as 'love' and 'hate' is straightforward because they are in the tokenizer's vocabulary. 'reprimand' is not, though, so it cannot be predicted in a single masked position - it consists of three subword units. so how can we predict 'reprimand' in the masked position?
from transformers import berttokenizer, bertformaskedlm
import torch

# init model and tokenizer
tokenizer = berttokenizer.from_pretrained('bert-base-uncased')
model = bertformaskedlm.from_pretrained('bert-base-uncased')
model.eval()
# init softmax to get probabilities later on
sm = torch.nn.softmax(dim=0)
torch.set_grad_enabled(false)

# set sentence with mask token, convert to token_ids
sentence = f""i {tokenizer.mask_token} you""
token_ids = tokenizer.encode(sentence, return_tensors='pt')
print(token_ids)
# tensor([[ 101, 1045,  103, 2017,  102]])
# get the position of the masked token
masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero().item()

# forward
output = model(token_ids)
last_hidden_state = output[0].squeeze(0)
# only get output for masked token
# output is the size of the vocabulary
mask_hidden_state = last_hidden_state[masked_position]
# convert to probabilities (softmax)
# giving a probability for each item in the vocabulary
probs = sm(mask_hidden_state)

# get probability of token 'hate'
hate_id = tokenizer.convert_tokens_to_ids('hate')
print('hate probability', probs[hate_id].item())
# hate probability 0.008057191967964172

# get probability of token 'love'
love_id = tokenizer.convert_tokens_to_ids('love')
print('love probability', probs[love_id].item())
# love probability 0.6704086065292358

# get probability of token 'reprimand' (?)
reprimand_id = tokenizer.convert_tokens_to_ids('reprimand')
# reprimand is not in the vocabulary, so it needs to be split into subword units
print(tokenizer.convert_ids_to_tokens(reprimand_id))
# [unk]

reprimand_id = tokenizer.encode('reprimand', add_special_tokens=false)
print(tokenizer.convert_ids_to_tokens(reprimand_id))
# ['rep', '##rim', '##and']
# but how do we now get the probability of a multi-token word in a single-token position?","['python', 'pytorch', 'transformer-model', 'bert-language-model', 'huggingface-transformers']",59520904,"since the split word is not present in the dictionary, bert is simply unaware of its probability, so there is no use of masking it before tokenization.
and you can't get its probability by exploiting the rule of chain, see response by j.devlin. to illustrate it, let's take a more generic example. try to estimate the probability of some bigram in position i. while you can estimate the probability of each word given the sentence and their positions
p(w_i|w_0, w_1... w_i-1, w_i+1, ..., w_n),
p(w_i+1|w_0, w_1... w_i, wi+2, ..., w_n),
there is no way to get the probability of the bigram
p(w_i,w_i+1|w_0, w_1... w_i-1, wi+2, ..., w_n)
because bert does not store such information.
having said all that, you can get a very rough estimate of the probability of your oov word by multiplying probabilities of seeing it's parts. so you will get
p(""reprimand""|...) ~= p(""rep""|...)*p(""##rim""|...)*p(""##and""|...)
since your subwords are not regular words, but a special kind of words, this is not all wrong, because the dependency between them is implicit.",https://stackoverflow.com/questions/59435020,python,21-12-2019 09:24,5096.0,12.0,2.0,True,17-12-2022 22:27,24-12-2019 09:58
72186244,extract all the data within parenthesis using spacy matcher,"i am trying to extract data from within paranthesis using a spacy matcher.
say the text is: ' i am on stackoverflow(for x years) and i ask (technical) questions here about natural language processing (nlp) (information retrieval)'
the desired output of the matcher is : (for x years), (technical) (nlp) (information retrieval)
below is the code which i tried to work with
nlp = spacy.load(""en_core_web_sm"")
text = 'i am on stackoverflow(for x years) and i ask (technical) questions here about natural language processing (nlp) (information retrieval)'
doc = nlp(text)
matcher = matcher(nlp.vocab)
pattern = [{""orth"": '(', }, {""text"": {""regex"": r"".*?""}}, {""orth"": ')'}]
matcher.add('paranthesis_data', none, pattern)
matches = matcher(doc)
for match_id, start, end in matches:
   print(nlp.vocab.strings[match_id], doc[start:end])

the output i am getting is as below:

but i would like the output like :
data (for x years)
data (technical)
data (nlp)
data (information retrieval)
i know i could use regex, but that's not an option in my project. if i use 'op' it is returning very long string matching, something like: (for x years) and i ask (technical)
any help is very much appreciated and i would be very thankful.","['python-3.x', 'nlp', 'spacy', 'text-extraction']",72194948,"in matcher patterns, regex matches a single token, not the text of the whole doc. it isn't doing what you want.
i think you can get what you want with a pattern like this:
pattern = [{""text"": '(', }, {""text"": {""not_in"": ["")""]}, ""op"": ""*""}, {""text"": ')'}]

a couple of other issues...
the string stackoverflow(for (note lack of space) is probably going to be a single token. you'll need to adjust the tokenizer to deal with that if it's a common problem.
you seem to be using v2 style matcher code. spacy v3 has been out for a year, i would recommend upgrading if you're starting a new project.
also see the rule-based matching docs.",https://stackoverflow.com/questions/72186244,python-3.x,10-05-2022 12:22,573.0,1.0,1.0,True,11-05-2022 03:08,10-05-2022 12:29
76010115,how to add retrieval information to openai&#39;s chatcompletion.create api call,"openai provides a tutorial on adding information from a web site to a completion.create api call.  however it appears that this call has been superceded by the chatcompletion.create call which provides better control over context. so how do you add retrieved information to a chatcompletion.create call?
in particular for the completion.create api call, you add the relevant web pages to the prompt field using what looks like a markup language e.g
prompt = f""answer the question based on the context below, and if the question can't be answered based on the context, say \""i don't know\""\n\ncontext: {context}\n\n---\n\nquestion: {question}\nanswer:""f""answer the question based on the context below, and if the question can't be answered based on the context, say \""i don't know\""\n\ncontext: {context}\n\n---\n\nquestion: {question}\nanswer:""

where context is text from different web pages separated by ""\n\n###\n\n"", and question is the actual question to be answered.
the chatcompletion.create call does not have a prompt field, instead it has a messages field and it does not appear to parse the markup. so how do you put such information into the chatcompletion.create call. i have tried using
'messages': ['role': 'user', 'content': prompt]

but this returns a much shorter answer than using the completion.create api.",['openai-api'],76030765,"what i found is that the best approach is to treat it like an adult and give it the necessary information to answer the question properly.  so my messages was
[
  { 'role': 'user', 'content': prompt },
  { 'role': 'user', 'content': question }
]

where prompt is
""please the answer the next question based on the context below, and if the question can't be answered based on the context, say \""i don't know\""\n\n.  the question is about my web site, and the context are the sections of the web site that have the embeddings that have the closest cosine distance to the question's embedding. context: #{context}:""

and question is the text of the question.  in some cases this can give a briefer response than the completion.create api call, but mostly the answers seem to be better.",https://stackoverflow.com/questions/76010115,openai-api,13-04-2023 21:51,1078.0,1.0,2.0,True,08-12-2023 03:07,16-04-2023 01:07
77475671,langchain unstructuredurlloader shows libmagic unavailble,"attempting to use unstructuredurlloader but getting a 'libmagic is unavailable'.
i have:

install langchain
install unstructured libmagic python-magic python-magic-bin
install python-magic-bin==0.4.13
python_magic-0.4.13-py2.py3-none-any.whl (i even tried other versions). i am on an amd64 windows machine.
uninstalled and reinstalled.
google, chatgtp, similar issues on stackoverflow for answers.

code:
from langchain.document_loaders import unstructuredurlloader
loader = unstructuredurlloader(
    urls = [
        ""
        ""
    ]
)
data = loader.load()
len(data)

error:
libmagic is unavailable but assists in filetype detection on file-like objects. please consider installing libmagic for better results.
error fetching or processing  exception: invalid file. the filetype.unk file type is not supported in partition.
libmagic is unavailable but assists in filetype detection on file-like objects. please consider installing libmagic for better results.
error fetching or processing  exception: invalid file. the filetype.unk file type is not supported in partition.","['python', 'loader', 'langchain', 'large-language-model', 'libmagic']",77482447,"resolution: the path to the libmagic.dll folder in the venv has to be added to system variables.
in my instance:
d:\ds_projects\code-basic-llm-finance-domain.venv\lib\site-packages\magic\libmagic
for others, it will likely be:
your_path\ .venv\lib\site-packages\magic\libmagic",https://stackoverflow.com/questions/77475671,python,13-11-2023 17:16,2840.0,1.0,4.0,True,25-04-2024 22:15,13-11-2023 19:30
75528441,trouble parsing interview transcript (q&amp;as) where questioner name is sometimes redacted,"i have the following python script i wrote as a reproducible example of my current pdf-parsing hangup. it:

downloads a pdf transcript from the web (cassidy hutchinson's 9/14/2022 interview transcript with the j6c)
reads/ocrs that pdf to text
attempts to split that text into the series of q&a passages from the interview
runs a series of tests i wrote based on my manual read of the transcript

running the python code below generates the following output:
~/askliz ï¿½ï¿½ï¿½ main !1 ?21 ï¿½ï¿½ï¿½ python stack_overflow_q_example.py                                                     ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ docenv py ï¿½ï¿½ï¿½ 22:41:00 
test for passage0 passed.
test for passage1 passed.
test for passage7 passed.
test for passage8 passed.
traceback (most recent call last):
  file ""/home/max/askliz/stack_overflow_q_example.py"", line 91, in <module>
    assert nltk.edit_distance(passages[10][:len(actual_passage10_start)],table_text_discrepancy, e_msg
assertionerror: failed on passage 10

your mission, should you choose to accept it: get this passage10 test to pass without breaking one of the previous tests. i'm hoping there's a clever regex or other modification in extract_q_a_locations below that will do the trick, but i'm open to any solution that passes all these tests, as i chose these test passages deliberately.
a little background on this transcript text, in case it's not as fun reading to you as it is to me: sometimes a passage starts with a ""q"" or ""a"", and sometimes it starts with a name (e.g. ""ms. cheney.""). the test that's failing, for passage 10, is where a question is asked by a  staff member whose name is then redacted. the only way i've managed to get that test to pass has inadvertently broken one of the other tests, because not all redactions indicate the start of a question. (note: in the pdf/ocr library i'm using, pdfplumber, redacted text usually shows up as just a bunch of extra spaces).
code below:
import nltk
import re
import requests
import pdfplumber


def extract_q_a_locations(examination_text:str)->list:

    # (when parsed by pdfplumber) every q/a starts with a newline, then spaces, 
    # then a line number and more spaces 
    prefix_regex = '\n\s+\d+\s+'

    # sometimes what comes next is a 'q' or 'a' and more spaces
    qa_regex = '[qa]\s+'

    # other times what comes next is the name of a congressperson or lawyer for the witness
    speaker_regex = ""(?:(?:mr\.|ms\.) \w+\.|-\s+)""

    # the combined regex i've been using is looking for the prefix then qa or speaker regex
    pattern = f""{prefix_regex}(?:{speaker_regex}|{qa_regex})""
    delims = list(re.finditer(pattern, text))
    return delims

def get_q_a_passages(qa_delimiters, text):
    q_a_list = []
    for delim, next_delim in zip(qa_delimiters[:-1], qa_delimiters[1:]):
        # prefix is either 'q', 'a', or the name of the speaker
        prefix = text[delim.span()[0]:delim.span()[1]].strip().split()[-1]

        # the text chunk is the actual dialogue text. everything from current delim to next one
        text_chunk = text[delim.span()[1]:next_delim.span()[0]]
        
        # now we want to remove some of the extra cruft from layout=true ocr in pdfplumber
        text_chunk = re.sub(""\n\s+\d+\s+"", "" "", text_chunk)  # remove line numbers
        text_chunk = "" "".join(text_chunk.split())            # remove extra whitespace
        
        q_a_list.append(f""{prefix} {text_chunk}"")

    return q_a_list

if __name__ == ""__main__"":

    # download pdf
    pdf_url = ""
    filename = ""interview_transcript_stackoverflow.pdf""

    response = requests.get(pdf_url)
    with open(filename, ""wb"") as f:
        f.write(response.content)

    # read pdf as text
    with pdfplumber.open(filename) as pdf:
        text = """".join([p.extract_text(layout=true) for p in pdf.pages])

    # i care about the q&a transcript, which starts after the ""examination"" header
    startidx = text.find(""examination"")
    text = text[startidx:]

    # extract q&a passages
    passage_locations = extract_q_a_locations(text)
    passages = get_q_a_passages(passage_locations, text)

    # tests
    acceptable_text_discrepancy = 2

    # the tests below all pass already.
    actual_passage0_start = ""q so i do first want to bring up exhibit""
    assert nltk.edit_distance(passages[0][:len(actual_passage0_start)], actual_passage0_start) <= acceptable_text_discrepancy
    print(""test for passage0 passed."")

    actual_passage1 = ""a this is correct.""
    assert nltk.edit_distance(passages[1][:len(actual_passage1)], actual_passage1) <= acceptable_text_discrepancy
    print(""test for passage1 passed."")

    # (note: for the next two passages/texts, prefix/questioner is captured as ""cheney"" & 
    # ""jordan"", not ""ms. cheney"" & ""mr. jordan"". i'm fine with either way.
    actual_passage7_start = ""cheney. and we also, just as"" 
    assert nltk.edit_distance(passages[7][:len(actual_passage7_start)], actual_passage7_start) <= acceptable_text_discrepancy
    print(""test for passage7 passed."")

    actual_passage8_start = ""jordan. they are pro bono""
    assert nltk.edit_distance(passages[8][:len(actual_passage8_start)], actual_passage8_start) <= acceptable_text_discrepancy
    print(""test for passage8 passed."")

    # here's my problem. 
    # this test fails because my regex fails to capture the question which starts with the 
    # redacted name of the staff/questioner. the only way i've managed to get this test to 
    # pass has also broken at least one of the tests above. 
    actual_passage10_start = "" so at this point, as we discussed earlier, i'm going to""
    e_msg = ""failed on passage 10""
    assert nltk.edit_distance(passages[10][:len(actual_passage10_start)], actual_passage10_start) <= acceptable_text_discrepancy, e_msg","['python', 'regex', 'pdf', 'nlp', 'pdfplumber']",75557728,"i have assumed that the redactions in between the passage are not required. what i have done is replaced the redacted name's spaces with ms. fakename. . this i did because as you have mentioned in your question, the required passages are either starting with a name or q or a. when it starts with a name, you'll notice that the name ends with a period and then starts with a capital letter. when the name is redacted, and that is an answer, there are a lot of spaces before it. combining all these observations, i was able to have all the tests passing by adding the following snippet
    lines = text.splitlines()

    for i in range(len(lines)):
        if re.fullmatch(r"" {10,}\d{1,2} {15,}[a-z].+"", lines[i]):
            lines[i] = re.sub(r"" {15,}"", ""       ms. fakename. "", lines[i], count=1)
    
    text = ""\n"".join(lines)

with the final code as
import nltk
import re
import requests
import pdfplumber


def extract_q_a_locations(examination_text:str)->list:

    # (when parsed by pdfplumber) every q/a starts with a newline, then spaces, 
    # then a line number and more spaces 
    prefix_regex = '\n\s+\d+\s+'

    # sometimes what comes next is a 'q' or 'a' and more spaces
    qa_regex = '[qa]\s+'

    # other times what comes next is the name of a congressperson or lawyer for the witness
    speaker_regex = ""(?:(?:mr\.|ms\.) \w+\.|-\s+)""

    # the combined regex i've been using is looking for the prefix then qa or speaker regex
    pattern = f""{prefix_regex}(?:{speaker_regex}|{qa_regex})""
    delims = list(re.finditer(pattern, text))
    return delims

def get_q_a_passages(qa_delimiters, text):
    q_a_list = []
    for delim, next_delim in zip(qa_delimiters[:-1], qa_delimiters[1:]):
        # prefix is either 'q', 'a', or the name of the speaker
        prefix = text[delim.span()[0]:delim.span()[1]].strip().split()[-1]

        # the text chunk is the actual dialogue text. everything from current delim to next one
        text_chunk = text[delim.span()[1]:next_delim.span()[0]]
        
        # now we want to remove some of the extra cruft from layout=true ocr in pdfplumber
        text_chunk = re.sub(""\n\s+\d+\s+"", "" "", text_chunk)  # remove line numbers
        text_chunk = "" "".join(text_chunk.split())            # remove extra whitespace
        
        q_a_list.append(f""{prefix} {text_chunk}"")

    return q_a_list

if __name__ == ""__main__"":

    # download pdf
    pdf_url = ""
    filename = ""interview_transcript_stackoverflow.pdf""

    response = requests.get(pdf_url)
    with open(filename, ""wb"") as f:
        f.write(response.content)

    # read pdf as text
    with pdfplumber.open(filename) as pdf:
        text = """".join([p.extract_text(layout=true) for p in pdf.pages])
    
    lines = text.splitlines()

    for i in range(len(lines)):
        if re.fullmatch(r"" {10,}\d{1,2} {15,}[a-z].+"", lines[i]):
            lines[i] = re.sub(r"" {15,}"", ""       ms. fakename. "", lines[i], count=1)
    
    text = ""\n"".join(lines)

    # i care about the q&a transcript, which starts after the ""examination"" header
    startidx = text.find(""examination"")
    text = text[startidx:]

    # extract q&a passages
    passage_locations = extract_q_a_locations(text)
    passages = get_q_a_passages(passage_locations, text)

    # tests
    acceptable_text_discrepancy = 2

    # the tests below all pass already.
    actual_passage0_start = ""q so i do first want to bring up exhibit""
    assert nltk.edit_distance(passages[0][:len(actual_passage0_start)], actual_passage0_start) <= acceptable_text_discrepancy
    print(""test for passage0 passed."")

    actual_passage1 = ""a this is correct.""
    assert nltk.edit_distance(passages[1][:len(actual_passage1)], actual_passage1) <= acceptable_text_discrepancy
    print(""test for passage1 passed."")

    # (note: for the next two passages/texts, prefix/questioner is captured as ""cheney"" & 
    # ""jordan"", not ""ms. cheney"" & ""mr. jordan"". i'm fine with either way.
    actual_passage7_start = ""cheney. and we also, just as"" 
    assert nltk.edit_distance(passages[7][:len(actual_passage7_start)], actual_passage7_start) <= acceptable_text_discrepancy
    print(""test for passage7 passed."")

    actual_passage8_start = ""jordan. they are pro bono""
    assert nltk.edit_distance(passages[8][:len(actual_passage8_start)], actual_passage8_start) <= acceptable_text_discrepancy
    print(""test for passage8 passed."")

    # here's my problem. 
    # this test fails because my regex fails to capture the question which starts with the 
    # redacted name of the staff/questioner. the only way i've managed to get this test to 
    # pass has also broken at least one of the tests above. 
    actual_passage10_start = ""fakename so at this point, as we discussed earlier, i'm going to""
    e_msg = ""failed on passage 10""
    assert nltk.edit_distance(passages[10][:len(actual_passage10_start)], actual_passage10_start) <= acceptable_text_discrepancy, e_msg

note that in the last test, i added ""fakename"" as the prefix. if this is not desired, the passages list can be updated to remove the manually added prefix.",https://stackoverflow.com/questions/75528441,python,22-02-2023 05:01,231.0,2.0,1.0,True,24-02-2023 14:27,22-02-2023 05:17
58221470,is this classification model overfitting?,"i am performing a url classification (phishing - nonphishing) and i plotted the learning curves (training vs cross validation score) for my model (gradient boost). 
my view
it seems that these two curves converge and the difference is not significant. tt's normal for the training set to have a slightly higher accuracy). (figure 1)

the question
i have limited experience on machine learning, thus i am asking your opinion. is the way i am approaching the problem right? is this model fine or is it overfitting?
note: the classes are balanced and the features are well chosen 
relevant code
from yellowbrick.model_selection import learningcurve

def plot_learning_curves(ï¿½ï¿½, y, model):

       # create the learning curve visualizer
       cv = stratifiedkfold(n_splits=5)
       sizes = np.linspace(0.1, 1.0, 8)
       visualizer = learningcurve(model, cv=cv, train_sizes=sizes, n_jobs=4)
       visualizer.fit(ï¿½ï¿½, y)  # fit the data to the visualizer
       visualizer.poof()
</code","['machine-learning', 'scikit-learn', 'classification', 'text-classification', 'overfitting-underfitting']",58222979,"firstly, in your graph there are 8 different models.
it's hard to tell if one of them is overfitting because overfitting can be detected with a ""epoch vs performance (train / valid)"" graph (there would be 8 in your case).
overfitting means that, after a certain number of epochs, as the number of epoch increases, training accuracy goes up while validation accuracy goes down. this can be the case, for example, when you have too few data points regarding the complexity of your problem, hence your model is using spurious correlations.
with your graph, what we can say is that the complexity of your problem seems to require a ""high"" number or training instances because your validation performance keep increasing as you add more training instances. there is a chance that the model with <10000 is overfitting but your >50000 could be overiftting too and we don't see that because you are using early stopping!",https://stackoverflow.com/questions/58221470,machine-learning,03-10-2019 14:38,920.0,0.0,1.0,True,16-02-2024 13:24,16-02-2024 13:24
75969974,openai gpt-3 api: why do i get an unexpected response?,"i am connecting to the gpt-3 api through a jupyter notebook. this is the code:
import openai
import os

# set up your api key
openai.api_key = os.environ[""openai_api_key""]

# choose the api endpoint
model_engine = ""davinci""

# create a prompt
prompt = ""hello, chatgpt!""

# a temperature of 0.5 returns gibberish
# generate a response
response = openai.completion.create(
    engine = model_engine,
    prompt = prompt,
    max_tokens = 1024,
    temperature = 0.5,
    frequency_penalty = 0.5,
    presence_penalty = 0.5
)

# print the response
print(response.choices[0].text)

attempting to debug the code led to me playing around with the temperature, frequency_penalty and presence_penalty. i figure i'm doing something wrong if i can't make it work with such a simple prompt.
if you want an example of the unexpected responses i am getting from the simple prompt above, here is the beginning of a few of them:

iï¿½ï¿½ï¿½m here to review a product that i was sent for free. this is not a
paid advertisement and all opinions are my own. i have been using the
new bamboo pen & touch tablet from wacom for about a month now and i
have to say that i am very impressed with this product! the bamboo is
a tablet designed for the everyday user, whether you are an artist or
just someone who likes to sketch on the computer. itï¿½ï¿½ï¿½s also great for
people like me who use their tablets primarily for writing.


chatgpt is a chat bot powered by google assistant. it can handle up to
10,000 messages per month for free and more if you pay for premium
services. this bot can be used for customer support, sales and
marketing, human resources, and more. in this tutorial, i will show
you how to create a chatbot using chatgpt with php. we will use
laravel as a backend framework. if you donï¿½ï¿½ï¿½t have an account at
chatgpt yet, sign up here first.

i was expectihi, how can i assist"". what's incorrect here?","['python', 'jupyter-notebook', 'openai-api', 'gpt-3']",75977040,"you are using a very old gpt-3 model (i.e., davinci). the performance of the openai api is model-related. newer models are more capable.

text-davinci-003 <-- use this one
text-davinci-002
davinci",https://stackoverflow.com/questions/75969974,python,09-04-2023 10:26,506.0,-4.0,1.0,True,14-09-2023 06:53,15-04-2023 10:10
76379440,how to see the embedding of the documents with chroma (or any other db) saved in lang chain?,"i can see everything but the embedding of the documents when i used chroma with langchain and openai embeddings. it always show me none for that
here is the code:
for db_collection_name in tqdm([""class1-sub2-chap3"", ""class2-sub3-chap4""]):
    documents = []
    doc_ids = []

    for doc_index in range(3):
        cl, sub, chap = db_collection_name.split(""-"")
        content = f""this is {db_collection_name}-doc{doc_index}""
        doc = document(page_content=content, metadata={""chunk_num"": doc_index, ""chapter"":chap, ""class"":cl, ""subject"":sub})
        documents.append(doc)
        doc_ids.append(str(doc_index))


    # # initialize a chroma instance with the original document
    db = chroma.from_documents(
         collection_name=db_collection_name,
         documents=documents, ids=doc_ids,
         embedding=embeddings, 
         persist_directory=""./data"")
    
     db.persist()

when i do db.get(), i see everything as expected except embedding is none.
{'ids': ['0', '1', '2'],
 'embeddings': none,
 'documents': ['this is class1-sub2-chap3-doc0',
  'this is class1-sub2-chap3-doc1',
  'this is class1-sub2-chap3-doc2'],
 'metadatas': [{'chunk_num': 0,
   'chapter': 'chap3',
   'class': 'class1',
   'subject': 'sub2'},
  {'chunk_num': 1, 'chapter': 'chap3', 'class': 'class1', 'subject': 'sub2'},
  {'chunk_num': 2, 'chapter': 'chap3', 'class': 'class1', 'subject': 'sub2'}]}

my embeddings is also working fine as it returns:
len(embeddings.embed_documents([""embed this""])[0])
>> 1536

also, in my ./data directory i have embedding file as chroma-embeddings.parquet

i tried the example with example given in document but it shows none too
# import document class
from langchain.docstore.document import document

# initial document content and id
initial_content = ""this is an initial document content""
document_id = ""doc1""

# create an instance of document with initial content and metadata
original_doc = document(page_content=initial_content, metadata={""page"": ""0""})

# initialize a chroma instance with the original document
new_db = chroma.from_documents(
    collection_name=""test_collection"",
    documents=[original_doc],
    embedding=openaiembeddings(),  # using the same embeddings as before
    ids=[document_id],
)

here also new_db.get() gives me none","['python', 'nlp', 'openai-api', 'langchain', 'chromadb']",76386231,"you just need to specify that you want the embeddings as well when using .get
# get all embeddings
db._collection.get(include=['embeddings'])

# get embeddings by document_id
db._collection.get(ids=['doc0', ..., 'docn'], include=['embeddings'])",https://stackoverflow.com/questions/76379440,python,01-06-2023 07:07,25799.0,9.0,2.0,True,14-11-2023 22:02,01-06-2023 07:25
76195972,aspect sentiment analysis using hugging face,"i am new to transformers models and trying to extract aspect and sentiment for a sentence but having issues
from transformers import autotokenizer, automodelforsequenceclassification

model_name = ""yangheng/deberta-v3-base-absa-v1.1""
tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforsequenceclassification.from_pretrained(model_name)
text = ""the food was great but the service was terrible.""
inputs = tokenizer(text, return_tensors=""pt"")
outputs = model(**inputs)



i am able to get the tensor what i need is the output to extract the aspect and sentiment for the overall sentence
i tried this however getting error
sentiment_scores = outputs.logits.softmax(dim=1)
aspect_scores = sentiment_scores[:, 1:-1]

aspects = [tokenizer.decode([x]) for x in inputs[""input_ids""].squeeze()][1:-1]
sentiments = ['positive' if score > 0.5 else 'negative' for score in aspect_scores.squeeze()]

for aspect, sentiment in zip(aspects, sentiments):
    print(f""{aspect}: {sentiment}"")

i am looking for below o/p or similar o/p
i am unable to write the logic as to how extract aspect and sentiment
text -the food was great but the service was terrible

aspect- food ,sentiment positive
aspect - service, sentiment negative


or at overall level

aspect - food, sentiment positive","['python', 'nlp', 'huggingface-transformers', 'sentiment-analysis']",76231431,"the model you are trying to use predicts the sentiment for a given aspect based on a text. that means, it requires text and aspect to perform a prediction. it was not trained to extract aspects from a text. you could use a keyword extraction model to extract aspects (compare this so answer).
import torch
import torch.nn.functional as f
from transformers import autotokenizer, automodelforsequenceclassification

model_name = ""yangheng/deberta-v3-base-absa-v1.1""
tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforsequenceclassification.from_pretrained(model_name)

aspects = [""food"", ""service""]
text = ""the food was great but the service was terrible.""
sentiment_aspect = {}
for aspect in aspects:
  inputs = tokenizer(text, aspect, return_tensors=""pt"")

  with torch.inference_mode():
    outputs = model(**inputs)

  scores = f.softmax(outputs.logits[0], dim=-1)
  label_id = torch.argmax(scores).item()
  sentiment_aspect[aspect] = (model.config.id2label[label_id], scores[label_id].item())

print(sentiment_aspect)

output:
{'food': ('positive', 0.9973154664039612), 'service': ('negative', 0.9935430288314819)}",https://stackoverflow.com/questions/76195972,python,07-05-2023 19:50,1498.0,3.0,1.0,True,26-05-2023 18:07,26-05-2023 18:07
73082185,prediction logits using lxmert with hugging face library,"how can we get the prediction logits in the lxmert model using hugging face library? it's fairly easy to get in visualbert, but i'm not able to get it with the lxmert model. in case of visualbert model, the keys i'm getting are :
['prediction_logits', 'seq_relationship_logits', 'attentions']

and with the help of lxmert mode, the keys are :
['language_output', 'vision_output', 'pooled_output', 'language_attentions', 'vision_attentions', 'cross_encoder_attentions']

even though there's a mention of prediction logits in the documentation i am not able to get them, if someone can help that would be great.
edit : link to colab notebook for lxmert.","['python', 'image-processing', 'huggingface-transformers', 'bert-language-model', 'multimodal']",73128172,"use lxmertforpretraining instead of lxmertmodel:
###colab commands
#pip install transformers
#!git clone 
#cd transformers
#cd examples/research_projects/lxmert
#pip install wget

from ipython.display import clear_output, image, display
import pil.image
import io
import json
import torch
import numpy as np
from processing_image import preprocess
from visualizing_image import singleimageviz
from modeling_frcnn import generalizedrcnn
from utils import config
import utils
import wget
import pickle
import os
import cv2
from copy import deepcopy

torch.cuda.is_available()

url = ""

frcnn_cfg = config.from_pretrained(""unc-nlp/frcnn-vg-finetuned"")
frcnn = generalizedrcnn.from_pretrained(""unc-nlp/frcnn-vg-finetuned"", config=frcnn_cfg)
image_preprocess = preprocess(frcnn_cfg)

# run frcnn
images, sizes, scales_yx = image_preprocess(url)
output_dict = frcnn(
    images,
    sizes,
    scales_yx=scales_yx,
    padding=""max_detections"",
    max_detections=frcnn_cfg.max_detections,
    return_tensors=""pt"",
)

# very important that the boxes are normalized
normalized_boxes = output_dict.get(""normalized_boxes"")
features = output_dict.get(""roi_features"")

from transformers import lxmerttokenizer, lxmertforpretraining
import torch

tokenizer = lxmerttokenizer.from_pretrained(""unc-nlp/lxmert-base-uncased"")
model = lxmertforpretraining.from_pretrained(""unc-nlp/lxmert-base-uncased"")

text_sentence = ""dog and cat are in the room and "" + tokenizer.mask_token + "" is laying on the ground""

inputs = tokenizer(text_sentence, return_token_type_ids=true, return_attention_mask=true, add_special_tokens=true, return_tensors=""pt"")

visual_feats = features
visual_attention_mask = torch.ones(features.shape[:-1], dtype=torch.long)
visual_pos=normalized_boxes

inputs.update(
    {
        ""visual_feats"": visual_feats,
        ""visual_pos"": visual_pos,
        ""visual_attention_mask"": visual_attention_mask,
    }
)

model_outputs = model(**inputs, output_attentions=true)

model_outputs.keys()

output:
odict_keys(['prediction_logits', 'cross_relationship_score', 'question_answering_score', 'language_attentions', 'vision_attentions', 'cross_encoder_attentions'])

p.s.: you can control the pertaining task heads via the configuration fields task_matched, task_mask_lm, task_obj_predict, and task_qa. i assume you are only interested in mask_lm following your comment. that means you should initialize your model as follows:
from transformers import lxmertconfig, lxmertforpretraining

config = lxmertconfig.from_pretrained(""unc-nlp/lxmert-base-uncased"")
config.task_matched = false
config.task_obj_predict=false
config.task_qa= false
model = lxmertforpretraining.from_pretrained(""unc-nlp/lxmert-base-uncased"", config=config)",https://stackoverflow.com/questions/73082185,python,22-07-2022 14:46,338.0,1.0,1.0,True,23-12-2023 01:10,23-07-2022 19:15
75041247,what&#39;s the correct url to test openai api?,"i'm trying to test the gpt-3 api with a request using curl in windows cmd:
curl -x post -h ""content-type: application/json"" -h ""authorization: bearer my_key"" -d ""{\""text\"": \""is this working\""}"" 

given that i did change ""my_key"" for my key.
but i got:
{
  ""error"": {
    ""message"": ""invalid url (post /v1/conversations/text-davinci-003/messages)"",
    ""type"": ""invalid_request_error"",
    ""param"": null,
    ""code"": null
  }
}

i also tried the model name as text-davinci-002 and text-davinci-001, but get the same invalid url error. what's the correct url here? i can't find it on the docs (or in chatgpt itself).","['curl', 'openai-api', 'gpt-3']",75043933,"sending a post request to /v1/conversations/text-davinci-003/messages will not return the result you want, because this url is not used by the openai api.
here's an example of a curl request which completes the message say this is a test
curl  \
-h ""content-type: application/json"" \
-h ""authorization: bearer your_api_key"" \
-d '{""model"": ""text-davinci-003"", ""prompt"": ""say this is a test"", ""temperature"": 0, ""max_tokens"": 7}'

and this is an example of what the api will respond with:
{
    ""id"": ""cmpl-gerzejq4lvqpk8skzu4xmiur"",
    ""object"": ""text_completion"",
    ""created"": 1586839808,
    ""model"": ""text-davinci:003"",
    ""choices"": [
        {
            ""text"": ""this is indeed a test"",
            ""index"": 0,
            ""logprobs"": null,
            ""finish_reason"": ""length""
        }
    ],
    ""usage"": {
        ""prompt_tokens"": 5,
        ""completion_tokens"": 7,
        ""total_tokens"": 12
    }
}

this is the full list of api paths:
instead, you can use the urls listed in the openai documentation:

list models
get 
retrieve model
get 
create completion
post 
create edit
post 
create image
post 
create image edit
post 
create image variation
post 
create embeddings
post 

more found in the openai documentation.",https://stackoverflow.com/questions/75041247,curl,07-01-2023 14:50,25416.0,2.0,2.0,True,31-08-2023 00:15,24-01-2023 18:16
78294720,"error when calling hugging face load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)","i'm following the huggingface tutorial here and it's giving me a strange error. when i run the following code:
from datasets import load_dataset
from transformers import autotokenizer, datacollatorwithpadding
from torch.utils.data import dataloader

raw_datasets = load_dataset(""glue"", ""mrpc"")

here is what i see:
downloading data: 100%|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý| 11.1k/11.1k [00:00<00:00, 6.63mb/s]
downloading data files: 100%|ýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý| 3/3 [00:32<00:00, 10.89s/it]
extracting data files: 100%|ýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý| 3/3 [00:00<00:00, 127.92it/s]
traceback (most recent call last):
  file ""/users/ameenizhac/downloads/transformers_playground.py"", line 5, in <module>
    raw_datasets = load_dataset(""glue"", ""mrpc"")
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""/library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/datasets/load.py"", line 1782, in load_dataset
    builder_instance.download_and_prepare(
  file ""/library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/datasets/builder.py"", line 872, in download_and_prepare
    self._download_and_prepare(
  file ""/library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/datasets/builder.py"", line 967, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  file ""/library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/datasets/builder.py"", line 1709, in _prepare_split
    split_info = self.info.splits[split_generator.name]
                 ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^
  file ""/library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/datasets/splits.py"", line 530, in __getitem__
    instructions = make_file_instructions(
                   ^^^^^^^^^^^^^^^^^^^^^^^
  file ""/library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/datasets/arrow_reader.py"", line 112, in make_file_instructions
    name2filenames = {
                     ^
  file ""/library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/datasets/arrow_reader.py"", line 113, in <dictcomp>
    info.name: filenames_for_dataset_split(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""/library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/datasets/naming.py"", line 70, in filenames_for_dataset_split
    prefix = filename_prefix_for_split(dataset_name, split)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""/library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/datasets/naming.py"", line 54, in filename_prefix_for_split
    if os.path.basename(name) != name:
       ^^^^^^^^^^^^^^^^^^^^^^
  file ""<frozen posixpath>"", line 142, in basename
typeerror: expected str, bytes or os.pathlike object, not nonetype

i don't know where to start because i don't understand where the error is coming from.","['python', 'nlp', 'huggingface', 'huggingface-datasets']",78310683,"i tried on my pc and on google colab. the strange thing is that on colab it works, on my pc it does not.
anyway, a possible workaround is the following:
raw_datasets = load_dataset(""setfit/mrpc"")

if you print it, you will see that the dataset is the same, it just has a different name:
datasetdict({
    train: dataset({
        features: ['text1', 'text2', 'label', 'idx', 'label_text'],
        num_rows: 3668
    })
    test: dataset({
        features: ['text1', 'text2', 'label', 'idx', 'label_text'],
        num_rows: 1725
    })
    validation: dataset({
        features: ['text1', 'text2', 'label', 'idx', 'label_text'],
        num_rows: 408
    })
})",https://stackoverflow.com/questions/78294720,python,08-04-2024 19:21,456.0,0.0,1.0,True,16-04-2024 10:09,16-04-2024 10:09
4951751,creating a new corpus with nltk,"i reckoned that often the answer to my title is to go and read the documentations, but i ran through the nltk book but it doesn't give the answer. i'm kind of new to python.
i have a bunch of .txt files and i want to be able to use the corpus functions that nltk provides for the corpus nltk_data. 
i've tried plaintextcorpusreader but i couldn't get further than:
>>>import nltk
>>>from nltk.corpus import plaintextcorpusreader
>>>corpus_root = './'
>>>newcorpus = plaintextcorpusreader(corpus_root, '.*')
>>>newcorpus.words()

how do i segment the newcorpus sentences using punkt? i tried using the punkt functions but the punkt functions couldn't read plaintextcorpusreader class?
can you also lead me to how i can write the segmented data into text files?","['python', 'nlp', 'nltk', 'corpus']",4952238,"i think the plaintextcorpusreader already segments the input with a punkt tokenizer, at least if your input language is english.
plaintextcorpusreader's constructor
def __init__(self, root, fileids,
             word_tokenizer=wordpuncttokenizer(),
             sent_tokenizer=nltk.data.lazyloader(
                 'tokenizers/punkt/english.pickle'),
             para_block_reader=read_blankline_block,
             encoding='utf8'):

you can pass the reader a word and sentence tokenizer, but for the latter the default already is nltk.data.lazyloader('tokenizers/punkt/english.pickle').
for a single string, a tokenizer would be used as follows (explained here, see section 5 for punkt tokenizer).
>>> import nltk.data
>>> text = """"""
... punkt knows that the periods in mr. smith and johann s. bach
... do not mark sentence boundaries.  and sometimes sentences
... can start with non-capitalized words.  i is a good variable
... name.
... """"""
>>> tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
>>> tokenizer.tokenize(text.strip())",https://stackoverflow.com/questions/4951751,python,09-02-2011 23:19,90877.0,91.0,4.0,True,25-07-2022 10:06,09-06-2019 18:53
74935089,creating a token count by date and co-occurence term proportion by date using quanteda,"i have a quite massive dataset that contains reviews of utilities services from customers all over the uk, this is a small sample of what the data looks like:
df <- data.frame (text  = c(""the investors and their supporters shall invest and do something mostly invest"",
         "" shall we tell the investors to invest ?"",  ""investors shall invest."",
         ""investors may sometimes invest"",""spend what investor do""),
                  date = c(""10/12/2022"", ""10/12/2022"", ""10/12/2022"",""11/12/2022"",""12/12/2022""))

what i want is to be able to count the frequency of terms/words/tokens by date.
for instance, the word invest appears 6 times in total, so for the date 10/12/2022 its word count is 4 i want to be able to use the quanteda library (since it is so powerful) to achieve the count and plot the viz over date.
i also want to plot the association or co-occurence of the word investor & invest over date
for instance, we have in this example 5 reviews in those reviews 4/5 times the word invest and investor were present and i'd like to plot that percentage over date as well. is that is possible? what amazing options does the quantada lib has that can perform this task? will it be possible to also find lets say a min percentage of the 0.25 most frequent words that appear when ""invest"" appears?
to achieve the first point i started with the following code:
df %>% 
  corpus(text_field=""text"") %>% 
  dfm() %>%
  textstat_frequency(10)

which gives:
      feature frequency rank docfreq group
1      invest         6    1       5   all
2   investors         4    2       4   all
3       shall         3    3       3   all
4         the         2    4       2   all
5         and         2    4       1   all
6          do         2    4       2   all
7       their         1    7       1   all
8  supporters         1    7       1   all
9   something         1    7       1   all
10         we         1    7       1   all
warning message:
'dfm.corpus()' is deprecated. use 'tokens()' first. 

how would i go about plotting the frequency of this words over the date column? i read in the documentation that one can group but i had have no luck in doing so.
and for the second question i don't know for sure if what function of the quenteda lib to use but i am trying to mirror the  tm::findassocs() fun from the tm library.","['r', 'nlp', 'text-mining', 'data-wrangling', 'quanteda']",74941047,"answer to your first question:
the dates are put into the docvars part of your corpus. this can be used within the textstat_frequency with the group option.
dat <- data.frame (text  = c(""the investors and their supporters shall invest and do something mostly invest"",
                            "" shall we tell the investors to invest ?"",  ""investors shall invest."",
                            ""investors may sometimes invest"",""spend what investor do""),
                  date = c(""10/12/2022"", ""10/12/2022"", ""10/12/2022"",""11/12/2022"",""12/12/2022""))


library(dplyr)
library(quanteda)
library(quanteda.textstats)

dat %>% 
  corpus(text_field=""text"") %>% 
  tokens() %>%
  dfm() %>% 
  textstat_frequency(groups = date)

      feature frequency rank docfreq      group
1      invest         4    1       3 10/12/2022
2   investors         3    2       3 10/12/2022
3       shall         3    2       3 10/12/2022
4         the         2    4       2 10/12/2022
5         and         2    4       1 10/12/2022
6       their         1    6       1 10/12/2022
7  supporters         1    6       1 10/12/2022
8          do         1    6       1 10/12/2022
9   something         1    6       1 10/12/2022
10     mostly         1    6       1 10/12/2022
11         we         1    6       1 10/12/2022
12       tell         1    6       1 10/12/2022
13         to         1    6       1 10/12/2022
14          ?         1    6       1 10/12/2022
15          .         1    6       1 10/12/2022
16  investors         1    1       1 11/12/2022
17     invest         1    1       1 11/12/2022
18        may         1    1       1 11/12/2022
19  sometimes         1    1       1 11/12/2022
20         do         1    1       1 12/12/2022
21      spend         1    1       1 12/12/2022
22       what         1    1       1 12/12/2022
23   investor         1    1       1 12/12/2022

you now have access to the frequency per day.
as for question 2, i think you can use textstat_simil. something like below. it does give some different answers as using tm::findassoc, usually more features. so i'm not completely sure if this is the correct answer. maybe someone from the quanteda team can confirm or deny this.
my_dfm <- dat %>% 
  corpus(text_field=""text"") %>% 
  tokens() %>%
  dfm()

textstat_simil(my_dfm, 
               my_dfm[, c(""investor"")], 
               method = ""correlation"", 
               margin = ""features"",
               min_simil = 0.7)

textstat_simil object; method = ""correlation""
           investor
the               .
investors         .
and               .
their             .
supporters        .
shall             .
invest            .
do                .
something         .
mostly            .
we                .
tell              .
to                .
?                 .
.                 .
may               .
sometimes         .
spend             1
what              1
investor          1

you can save the outcome of textstat_simil as a data.frame or list if you want to with as.data.frame or as.list.",https://stackoverflow.com/questions/74935089,r,27-12-2022 23:58,101.0,0.0,1.0,True,08-08-2023 19:30,08-08-2023 19:30
67043468,unparsedflagaccesserror: trying to access flag --preserve_unused_tokens before flags were parsed. bert,"i want to use bert language model for training a multi class text classification task.
previously i trained using lstm without any error but bert gives me this error.
i get the following error and i really don't know how to solve it, can anyone help me please?
unfortunately there is very little documentation using bert in keras library.
!wget --quiet 

import tensorflow_hub as hub
from bert import tokenization
module_url = '
bert_layer = hub.keraslayer(module_url, trainable=true)





vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.fulltokenizer(vocab_file, do_lower_case)

def bert_encode(texts, tokenizer, max_len=512):
    all_tokens = []
    all_masks = []
    all_segments = []
    
    for text in texts:
        text = tokenizer.tokenize(text)
            
        text = text[:max_len-2]
        input_sequence = [""[cls]""] + text + [""[sep]""]
        pad_len = max_len - len(input_sequence)
        
        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len
        pad_masks = [1] * len(input_sequence) + [0] * pad_len
        segment_ids = [0] * max_len
        
        all_tokens.append(tokens)
        all_masks.append(pad_masks)
        all_segments.append(segment_ids)
    
    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)



def build_model(bert_layer, max_len=512):
    input_word_ids = tf.keras.input(shape=(max_len,), dtype=tf.int32, name=""input_word_ids"")
    input_mask = tf.keras.input(shape=(max_len,), dtype=tf.int32, name=""input_mask"")
    segment_ids = tf.keras.input(shape=(max_len,), dtype=tf.int32, name=""segment_ids"")

    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])
    clf_output = sequence_output[:, 0, :]
    net = tf.keras.layers.dense(64, activation='softmax')(clf_output)
    net = tf.keras.layers.dropout(0.2)(net)
    net = tf.keras.layers.dense(32, activation='softmax')(net)
    net = tf.keras.layers.dropout(0.2)(net)
    out = tf.keras.layers.dense(3, activation='softmax')(net)
    
    model = tf.keras.models.model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)
    model.compile(tf.keras.optimizers.adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model



max_len = 150
train_input = bert_encode(data.text_cleaned, tokenizer, max_len=max_len)


error as following :

unparsedflagaccesserror                   traceback (most recent call last)
<ipython-input-175-fd64df42591d> in <module>()
      1 import sys
      2 max_len = 150
----> 3 train_input = bert_encode(o.text_cleaned, tokenizer, max_len=max_len)

4 frames
/usr/local/lib/python3.7/dist-packages/absl/flags/_flagvalues.py in __getattr__(self, name)
    496         # get too much noise.
    497         logging.error(error_message)
--> 498       raise _exceptions.unparsedflagaccesserror(error_message)
    499 
    500   def __setattr__(self, name, value):

unparsedflagaccesserror: trying to access flag --preserve_unused_tokens before flags were parsed.","['python', 'nlp', 'bert-language-model']",67250830,"based on this issue you have to downgrade bert-tensorflow to 1.0.1. check this answer to find a solution. if you are following this tutorial downgrade bert-tensorflow and use the !wget --quiet  as suggested because inside the python code the author has made the change from tf.gfile.gfile(vocab_file, ""r"") to tf.io.gfile.gfile(vocab_file, ""r""). after that code compiles successfully. ping me if you want anything else.",https://stackoverflow.com/questions/67043468,python,11-04-2021 09:39,4587.0,4.0,2.0,True,30-03-2023 09:53,12-09-2022 10:34
75866093,"how does huggingface&#39;s zero-shot classification work in production/webapp, do i need to train the model first?","i have already used huggingface's zero-shot classification: i used ""facebook/bart-large-mnli"" model as reported here ( the  accuracy is quite good for my task.

my question is about productionizing the code: in particular i would like to create a gradio (or streamlit) webapp. do i need to train the ""facebook/bart-large-mnli"" model  first, secondly save the model in a pickle file, and then predict a new (unseen) sentence using the pickle file?

or can i simply import the ""facebook/bart-large-mnli"" library and compute the prediction for the production/webapp code?


the latter scenario would be preferable. but i am not sure whether loading the model from scratch would produce the same output as loadingthe pickle file with the saved facebook/bart-large-mnli"" model.
thank you in advance.","['python', 'huggingface-transformers', 'text-classification', 'large-language-model', 'zeroshot-classification']",75873068,"q: how does zero-shot classification work? do i need train/tune the model to use in production?
options:

(i) train the ""facebook/bart-large-mnli"" model first, secondly save the model in a pickle file, and then predict a new (unseen) sentence using the pickle file? or
(ii) can i simply import the ""facebook/bart-large-mnli"" library and compute the prediction for the production/webapp code?

a (human): (ii) you can load up the model with pipeline(""zero-shot-classification"", model=""facebook/bart-large-mnli"") once when the server start, then reuse the pipeline without re-initializing it for each request.
when you use the model off-the-shelf, it'll be zero-shot but if you fine-tune a model with limited training data, people commonly refer to that as ""few-shot""; take a look at  for few-shot learning.

the proof is in the pudding, see if the model you pick fits the task you want. also, there's more than one way to wield the shiny hammer =)
disclaimer: your miles may vary...
zero shot classification
tl;dr: i don't want to train anything, i don't have labeled data, do something with some labels that i come up with.
from transformers import pipeline

classifier = pipeline(""zero-shot-classification"", model=""facebook/bart-large-mnli"")

text = ""catan (base game) | ages 10+ | for 3 to 4 players | average playtime 60 minutes | made by catan studio | trade, build and settle: embark on a quest to settle the isle of catan! guide your settlers to victory by clever trading and cunning development. but beware! someone might cut off your road or buy a monopoly. and you never know when the wily robber might steal some of your precious games!""

candidate_labels = ['beauty & wellness', 'electronics', 'toys & games']

classifier(text, candidate_labels)

[out]:
{'sequence': 'catan (base game) | ages 10+ | for 3 to 4 players | average playtime 60 minutes | made by catan studio | trade, build and settle: embark on a quest to settle the isle of catan! guide your settlers to victory by clever trading and cunning development. but beware! someone might cut off your road or buy a monopoly. and you never know when the wily robber might steal some of your precious games!',
 'labels': ['toys & games', 'electronics', 'beauty & wellness'],
 'scores': [0.511284351348877, 0.38416239619255066, 0.10455326735973358]}

don't classify, translate (or seq2seq)
inspiration: 
from transformers import autotokenizer, automodelforseq2seqlm

model_name = ""google/flan-t5-large""

tokenizer= autotokenizer.from_pretrained(model_name)
model = automodelforseq2seqlm.from_pretrained(model_name)

text = ""catan (base game) | ages 10+ | for 3 to 4 players | average playtime 60 minutes | made by catan studio | trade, build and settle: embark on a quest to settle the isle of catan! guide your settlers to victory by clever trading and cunning development. but beware! someone might cut off your road or buy a monopoly. and you never know when the wily robber might steal some of your precious games!""


prompt=f""""""which category is this product?
query:{text}
options:
 - beauty & wellness
 - electronics
 - toys & games
""""""

input_ids = tokenizer(prompt, return_tensors=""pt"").input_ids

tokenizer.decode(model.generate(input_ids)[0], skip_special_tokens=true)

[out]:
toys & games

and for the fun of it =)
from transformers import autotokenizer, automodelforseq2seqlm

model_name = ""google/flan-t5-large""

tokenizer= autotokenizer.from_pretrained(model_name)
model = automodelforseq2seqlm.from_pretrained(model_name)

prompt=f""""""how does zero-shot classification work? 
query: do i need tune/modify the model to use in production?
options:
 - (i) train the ""facebook/bart-large-mnli"" model first, secondly save the model in a pickle file, and then predict a new (unseen) sentence using the pickle file
 - (ii) can i simply import the ""facebook/bart-large-mnli"" library and compute the prediction for the production/webapp code
""""""

input_ids = tokenizer(prompt, return_tensors=""pt"").input_ids

print(tokenizer.decode(model.generate(input_ids)[0], skip_special_tokens=true))

[out]:
(ii)


q: what if both methods above don't work?
a: try more models from  or try different tasks and be creative in how to use what's available to fit your data to solve the problem
q: what if none of the models/tasks works?
a: then it's time to think about what data you can/need to collect to train the model you need. but before collecting the data, it'll be prudent to first decide how you want to evaluate/measure the success of the model, e.g. f1-score, accuracy, etc.
this is how i'll personally solve nlp problems that fits the frame ""x problem, y approach"" solutions,  (shameless plug)
q: how do i deploy a model after i found the model+task i want?
there're several ways but it'll be out-of-scope of this question, since it's asking about how zero-shot works and more pertinently ""can i use zero-shot classification models off-the-shelf without training?"".
to deploy a model, take a look at:",https://stackoverflow.com/questions/75866093,python,28-03-2023 12:10,6316.0,4.0,1.0,True,29-03-2023 04:27,29-03-2023 04:24
77043020,compare 2 pdf files langchain,"import streamlit as st
import os
import tempfile
from pathlib import path
from pydantic import basemodel, field
import streamlit as st
from langchain.chat_models import chatopenai
from langchain.agents import tool
from langchain.embeddings.openai import openaiembeddings
from langchain.text_splitter import charactertextsplitter
from langchain.vectorstores import faiss
from langchain.document_loaders import pypdfloader
from langchain.chains import retrievalqa
from langchain.agents import initialize_agent
import openai
os.environ[""openai_api_key""] = """"
os.environ['openai_api_type'] = 'azure'
os.environ['openai_api_version'] = '2023-03-15-preview'
os.environ['openai_api_base'] = ""

#api settings for embedding
openai.api_type = ""azure""
openai.api_base = ""
openai.api_version = '2023-03-15-'
openai.api_key = """"
    
    
class documentinput(basemodel):
    question: str = field()

# create a temporary directory in the script's folder
script_dir = path(__file__).resolve().parent
temp_dir = os.path.join(script_dir, ""tempdir"")


def main():
    st.title(""pdf document comparison"")

    # create a form to upload pdf files and enter a question
    st.write(""upload the first pdf file:"")
    pdf1 = st.file_uploader(""choose a pdf file"", type=[""pdf""], key=""pdf1"")

    st.write(""upload the second pdf file:"")
    pdf2 = st.file_uploader(""choose a pdf file"", type=[""pdf""], key=""pdf2"")

    question = st.text_input(""enter your question"")
    submit_button = st.button(""compare pdfs"")

    if submit_button:
        if pdf1 and pdf2:
            if not os.path.exists(temp_dir):
                os.makedirs(temp_dir)
            else:
                # clear the previous contents of the ""tempdir"" folder
                for file in os.listdir(temp_dir):
                    file_path = os.path.join(temp_dir, file)
                    try:
                        if os.path.isfile(file_path):
                            os.unlink(file_path)
                    except exception as e:
                        print(f""error deleting file: {e}"")

            # save the pdf files to the ""tempdir"" directory
            pdf1_path = os.path.join(temp_dir, pdf1.name)
            with open(pdf1_path, 'wb') as f:
                f.write(pdf1.getbuffer())

            pdf2_path = os.path.join(temp_dir, pdf2.name)
            with open(pdf2_path, 'wb') as f:
                f.write(pdf2.getbuffer())



            llm = chatopenai(temperature=0, model=""gpt-3.5-turbo"",engine=""gpt-35-turbo"")

            tools = []
            files = [

                {
                    ""name"": pdf1.name,
                    ""path"": pdf1_path,
                },

                {
                    ""name"": pdf2.name,
                    ""path"": pdf2_path,
                },
            ]

            for file in files:
                loader = pypdfloader(file[""path""])
                pages = loader.load_and_split()
                text_splitter = charactertextsplitter(chunk_size=1000, chunk_overlap=0)
                docs = text_splitter.split_documents(pages)
                embeddings = openaiembeddings()
                retriever = faiss.from_documents(docs, embeddings).as_retriever()

                # wrap retrievers in a tool
                tools.append(
                    tool(
                        args_schema=documentinput,
                        name=file[""name""],
                        description=f""useful when you want to answer questions about {file['name']}"",
                        func=retrievalqa.from_chain_type(llm=llm, retriever=retriever),
                    )
                )
            agent = initialize_agent(
                tools=tools,
                llm=llm,
                verbose=true,
            )

            st.write(agent({""input"": question}))
            # now you have both pdfs saved in the ""tempdir"" folder
            # you can perform your pdf comparison here


if __name__ == ""__main__"":
    main()

i get the following error :
pydantic.v1.error_wrappers.validationerror: 1 validation error for tool
args_schema
subclass of basemodel expected (type=type_error.subclass; expected_class=basemodel) i am following the example from langchain documentation:","['python', 'streamlit', 'pydantic', 'langchain', 'gpt-3']",77150773,"i've run into this issue, i solved it using
from pydantic.v1 import basemodel

or install last v1 version
pip install pydantic==1.10.12

pydantic has released v2 version on june 30, 2023 and langchain integration is not compatible",https://stackoverflow.com/questions/77043020,python,05-09-2023 08:44,2047.0,2.0,1.0,True,26-01-2024 06:37,26-01-2024 06:37
72264935,"neither stemmer nor lemmatizer seem to work very well, what should i do?","i am new to text analysis and am trying to create a bag of words model(using sklearn's countvectorizer method). i have a data frame with a column of text with words like 'acid', 'acidic', 'acidity', 'wood', 'woodsy', 'woody'.
i think that 'acid' and 'wood' should be the only words included in the final output, however neither stemming nor lemmatizing seems to accomplish this.
stemming produces 'acid','wood','woodi',woodsi'
and lemmatizing produces a worse output of  'acid'  'acidic'    'acidity'   'wood'  'woodsy'    'woody'. i assume this is due to the part of speech not being specified accurately although i am not sure where this specification should go. i have included it in the line x = vectorizer.fit_transform(df['text'],'a') (i believe that most of the words should be adjectives) however, it does not make a difference in the output.
what can i do to improve the output?
my full code is below;
!pip install nltk
nltk.download('omw-1.4')  
import nltk
import pandas as pd
from sklearn.feature_extraction.text import countvectorizer
from nltk.stem import wordnetlemmatizer 

data frame:
df = pd.dataframe()
df['text']=['acid', 'acidic', 'acidity', 'wood', 'woodsy', 'woody']

countvectorizer with stemmer:
analyzer = countvectorizer().build_analyzer()
stemmer = nltk.stem.snowballstemmer('english')
lemmatizer = wordnetlemmatizer()

def stemmed_words(doc):
    return (stemmer.stem(w) for w in analyzer(doc))

vectorizer = countvectorizer(stop_words='english',analyzer=stemmed_words)
x = vectorizer.fit_transform(df['text'])
df_bow_sklearn = pd.dataframe(x.toarray(),columns=vectorizer.get_feature_names())
df_bow_sklearn.head()

countvectorizer with lemmatizer:
analyzer = countvectorizer().build_analyzer()
stemmer = nltk.stem.snowballstemmer('english')
lemmatizer = wordnetlemmatizer()

def lemed_words(doc):
    return(lemmatizer.lemmatize(w) for w in analyzer(doc))

vectorizer = countvectorizer(stop_words='english',analyzer=lemed_words)
x = vectorizer.fit_transform(df['text'],'a')
df_bow_sklearn = pd.dataframe(x.toarray(),columns=vectorizer.get_feature_names())
df_bow_sklearn.head()","['python', 'wordnet', 'stemming', 'lemmatization', 'countvectorizer']",72265587,"might be a simple under-performing issue with the wordnetlemmatizer and the stemmer.
try different ones like...
stemmers:

porter ( -> from nltk.stem import porterstemmer)
lancaster (-> from nltk.stem import lancasterstemmer)

lemmatizers:

spacy ( -> import spacy)
iwnlp ( -> from spacy_iwnlp import spacyiwnlp)
hanta ( -> from hanta import hanovertagger /note: is more or less trained for german language)

had the same issue and switching to a different stemmer and lemmatizer solved the issue. for closer instruction on how to propperly implement the stemmers and lemmatizers, a quick search on the web reveals good examples on all cases.",https://stackoverflow.com/questions/72264935,python,16-05-2022 19:59,513.0,3.0,1.0,True,16-05-2022 21:01,16-05-2022 20:39
69025750,how to fine-tune huggingface bert model for text classification,is there a step by step explanation on how to fine-tune huggingface bert model for text classification?,"['machine-learning', 'huggingface-transformers', 'transfer-learning']",69025751,"fine tuning approach
there are multiple approaches to fine-tune bert for the target tasks.

further pre-training the base bert model
custom classification layer(s) on top of the base bert model  being trainable
custom classification layer(s) on top of the base bert model being non-trainable (frozen)

note that the bert base model has been pre-trained only for two tasks as in the original paper.

bert: pre-training of deep bidirectional transformers for language understanding


3.1 pre-training bert ...we pre-train bert using two unsupervised tasks

task #1: masked lm
task #2: next sentence prediction (nsp)


hence, the base bert model is like half-baked which can be fully baked for the target domain (1st way). we can use it as part of our custom model training with the base trainable (2nd) or not-trainable (3rd).

1st approach
how to fine-tune bert for text classification? demonstrated the 1st approach of further pre-training, and pointed out the learning rate is the key to avoid catastrophic forgetting where the pre-trained knowledge is erased during learning of new knowledge.

we find that a lower learning rate, such as 2e-5,
is necessary to make bert overcome the catastrophic forgetting problem. with an aggressive learn rate of 4e-4, the training set fails to converge.


probably this is the reason why the bert paper used 5e-5, 4e-5, 3e-5, and 2e-5 for fine-tuning.

we use a batch size of 32 and fine-tune for 3 epochs over the data for all glue tasks. for each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the dev set

note that the base model pre-training itself used higher learning rate.

bert-base-uncased - pretraining


the model was trained on 4 cloud tpus in pod configuration (16 tpu chips total) for one million steps with a batch size of 256. the sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. the optimizer used is adam with a learning rate of 1e-4, ï¿½ï¿½1=0.9 and ï¿½ï¿½2=0.999, a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.

will describe the 1st way as part of the 3rd approach below.
fyi:
"" rel=""noreferrer"">tfdistilbertmodel is the bare base model with the name distilbert.
model: ""tf_distil_bert_model_1""
_________________________________________________________________
layer (type)                 output shape              param #   
=================================================================
distilbert (tfdistilbertmain multiple                  66362880  
=================================================================
total params: 66,362,880
trainable params: 66,362,880
non-trainable params: 0


2nd approach
huggingface takes the 2nd approach as in fine-tuning with native pytorch/tensorflow where tfdistilbertforsequenceclassification has added the custom classification layer classifier on top of the base distilbert model being trainable. the small learning rate requirement will apply as well to avoid the catastrophic forgetting.
from transformers import tfdistilbertforsequenceclassification

model = tfdistilbertforsequenceclassification.from_pretrained('distilbert-base-uncased')
optimizer = tf.keras.optimizers.adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn
model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)

model: ""tf_distil_bert_for_sequence_classification_2""
_________________________________________________________________
layer (type)                 output shape              param #   
=================================================================
distilbert (tfdistilbertmain multiple                  66362880  
_________________________________________________________________
pre_classifier (dense)       multiple                  590592    
_________________________________________________________________
classifier (dense)           multiple                  1538      
_________________________________________________________________
dropout_59 (dropout)         multiple                  0         
=================================================================
total params: 66,955,010
trainable params: 66,955,010  <--- all parameters are trainable
non-trainable params: 0

implementation of the 2nd approach
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from transformers import (
    distilberttokenizerfast,
    tfdistilbertforsequenceclassification,
)


data_column = 'text'
label_column = 'category_index'
max_sequence_length = 512
learning_rate = 5e-5
batch_size = 16
num_epochs = 3


# --------------------------------------------------------------------------------
# tokenizer
# --------------------------------------------------------------------------------
tokenizer = distilberttokenizerfast.from_pretrained('distilbert-base-uncased')
def tokenize(sentences, max_length=max_sequence_length, padding='max_length'):
    """"""tokenize using the huggingface tokenizer
    args:
        sentences: string or list of string to tokenize
        padding: padding method ['do_not_pad'|'longest'|'max_length']
    """"""
    return tokenizer(
        sentences,
        truncation=true,
        padding=padding,
        max_length=max_length,
        return_tensors=""tf""
    )

# --------------------------------------------------------------------------------
# load data
# --------------------------------------------------------------------------------
raw_train = pd.read_csv(""./train.csv"")
train_data, validation_data, train_label, validation_label = train_test_split(
    raw_train[data_column].tolist(),
    raw_train[label_column].tolist(),
    test_size=.2,
    shuffle=true
)

# --------------------------------------------------------------------------------
# prepare tf dataset
# --------------------------------------------------------------------------------
train_dataset = tf.data.dataset.from_tensor_slices((
    dict(tokenize(train_data)),  # convert batchencoding instance to dictionary
    train_label
)).shuffle(1000).batch(batch_size).prefetch(1)
validation_dataset = tf.data.dataset.from_tensor_slices((
    dict(tokenize(validation_data)),
    validation_label
)).batch(batch_size).prefetch(1)

# --------------------------------------------------------------------------------
# training
# --------------------------------------------------------------------------------
model = tfdistilbertforsequenceclassification.from_pretrained(
    'distilbert-base-uncased',
    num_labels=num_labels
)
optimizer = tf.keras.optimizers.adam(learning_rate=learning_rate)
model.compile(
    optimizer=optimizer,
    loss=tf.keras.losses.sparsecategoricalcrossentropy(from_logits=true),
)
model.fit(
    x=train_dataset,
    y=none,
    validation_data=validation_dataset,
    batch_size=batch_size,
    epochs=num_epochs,
)


3rd approach
basics
please note that the images are taken from a visual guide to using bert for the first time and modified.
tokenizer
tokenizer generates the instance of batchencoding which can be used like a python dictionary and the input to the bert model.

batchencoding


holds the output of the encode_plus() and batch_encode() methods (tokens, attention_masks, etc).

this class is derived from a python dictionary and can be used as a dictionary. in addition, this class exposes utility methods to map from word/character space to token space.
parameters

data (dict) ï¿½ï¿½ï¿½ dictionary of lists/arrays/tensors returned by the encode/batch_encode methods (ï¿½ï¿½ï¿½input_idsï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½attention_maskï¿½ï¿½ï¿½, etc.).


the data attribute of the class is the tokens generated which has input_ids and attention_mask elements.
in"" rel=""noreferrer"">input_ids


the input ids are often the only required parameters to be passed to the model as input. they are token indices, numerical representations of tokens building the sequences that will be used as input by the model.

attention_mask

attention mask


this argument indicates to the model which tokens should be attended to, and which should not.

if the attention_mask is 0, the token id is ignored. for instance if a sequence is padded to adjust the sequence length, the padded words should be ignored hence their attention_mask are 0.
special tokens
berttokenizer addes special tokens, enclosing a sequence with [cls] and [sep]. [cls] represents classification and [sep] separates sequences. for question answer or paraphrase tasks, [sep] separates the two sentences to compare.
berttokenizer


cls_token (str, optional, defaults to ""[cls]"")the classifier token which is used when doing sequence classification (classification of the whole sequence instead of per-token classification). it is the first token of the sequence when built with special tokens.
sep_token (str, optional, defaults to ""[sep]"")the separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for sequence classification or for a text and a question for question answering. it is also used as the last token of a sequence built with special tokens.


a visual guide to using bert for the first time show the tokenization.

[cls]
the embedding vector for [cls] in the output from the base model final layer represents the classification that has been learned by the base model. hence feed the embedding vector of  [cls] token into the classification layer added on top of the base model.

bert: pre-training of deep bidirectional transformers for language understanding


the first token of every sequence is always a special classification token ([cls]). the final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. sentence pairs are packed together into a single sequence. we differentiate the sentences in two ways. first, we separate them with a special token ([sep]). second, we add a learned embedding to every token indicating whether it belongs to sentence a or sentence b.

the model structure will be illustrated as below.


vector size
in the model distilbert-base-uncased, each token is embedded into a vector of size 768. the shape of the output from the base model is (batch_size, max_sequence_length, embedding_vector_size=768). this accords with the bert paper about the bert/base model (as indicated in distilbert-base-uncased).

bert: pre-training of deep bidirectional transformers for language understanding


bert/base (l=12, h=768, a=12, total parameters=110m) and bert/large (l=24, h=1024, a=16, total parameters=340m).

base model - tfdistilbertmodel

hugging face transformers: fine-tuning distilbert for binary classification tasks


tfdistilbertmodel class to instantiate the base distilbert model without any specific head on top (as opposed to other classes such as tfdistilbertforsequenceclassification that do have an added classification head). 
we do not want any task-specific head attached because we simply want the pre-trained weights of the base model to provide a general understanding of the english language, and it will be our job to add our own classification head during the fine-tuning process in order to help the model distinguish between toxic comments.

tfdistilbertmodel generates an instance of tfbasemodeloutput whose last_hidden_state parameter is the output from the model last layer.
tfbasemodeloutput([(
    'last_hidden_state',
    <tf.tensor: shape=(batch_size, sequence_lendgth, 768), dtype=float32, numpy=array([[[...]]], dtype=float32)>
)])


tfbasemodeloutput


parameters

last_hidden_state (tf.tensor of shape (batch_size, sequence_length, hidden_size)) ï¿½ï¿½ï¿½ sequence of hidden-states at the output of the last layer of the model.


implementation
python modules
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from transformers import (
    distilberttokenizerfast,
    tfdistilbertmodel,
)

configuration
timestamp = datetime.datetime.now().strftime(""%y%b%d%h%m"").upper()

data_column = 'text'
label_column = 'category_index'

max_sequence_length = 512   # max length allowed for bert is 512.
num_labels = len(raw_train[label_column].unique())

model_name = 'distilbert-base-uncased'
num_base_model_output = 768

# flag to freeze base model
freeze_base = true

# flag to add custom classification heads
use_custom_head = true
if use_custom_head == false:
    # make te trainable when no classification head exists.
    freeze_base = false


batch_size = 16
learning_rate = 1e-2 if freeze_base else 5e-5
l2 = 0.01

tokenizer
tokenizer = distilberttokenizerfast.from_pretrained(model_name)
def tokenize(sentences, max_length=max_sequence_length, padding='max_length'):
    """"""tokenize using the huggingface tokenizer
    args:
        sentences: string or list of string to tokenize
        padding: padding method ['do_not_pad'|'longest'|'max_length']
    """"""
    return tokenizer(
        sentences,
        truncation=true,
        padding=padding,
        max_length=max_length,
        return_tensors=""tf""
    )

input layer
the base model expects input_ids and attention_mask whose shape is (max_sequence_length,). generate keras tensors for them with input layer respectively.
# inputs for token indices and attention masks
input_ids = tf.keras.layers.input(shape=(max_sequence_length,), dtype=tf.int32, name='input_ids')
attention_mask = tf.keras.layers.input((max_sequence_length,), dtype=tf.int32, name='attention_mask')

base model layer
generate the output from the base model. the base model generates tfbasemodeloutput. feed the embedding of [cls] to the next layer.
base = tfdistilbertmodel.from_pretrained(
    model_name,
    num_labels=num_labels
)

# freeze the base model weights.
if freeze_base:
    for layer in base.layers:
        layer.trainable = false
    base.summary()

# [cls] embedding is last_hidden_state[:, 0, :]
output = base([input_ids, attention_mask]).last_hidden_state[:, 0, :]

classification layers
if use_custom_head:
    # -------------------------------------------------------------------------------
    # classifiation leayer 01
    # --------------------------------------------------------------------------------
    output = tf.keras.layers.dropout(
        rate=0.15,
        name=""01_dropout"",
    )(output)
    
    output = tf.keras.layers.dense(
        units=num_base_model_output,
        kernel_initializer='glorot_uniform',
        activation=none,
        name=""01_dense_relu_no_regularizer"",
    )(output)
    output = tf.keras.layers.batchnormalization(
        name=""01_bn""
    )(output)
    output = tf.keras.layers.activation(
        ""relu"",
        name=""01_relu""
    )(output)

    # --------------------------------------------------------------------------------
    # classifiation leayer 02
    # --------------------------------------------------------------------------------
    output = tf.keras.layers.dense(
        units=num_base_model_output,
        kernel_initializer='glorot_uniform',
        activation=none,
        name=""02_dense_relu_no_regularizer"",
    )(output)
    output = tf.keras.layers.batchnormalization(
        name=""02_bn""
    )(output)
    output = tf.keras.layers.activation(
        ""relu"",
        name=""02_relu""
    )(output)

softmax layer
output = tf.keras.layers.dense(
    units=num_labels,
    kernel_initializer='glorot_uniform',
    kernel_regularizer=tf.keras.regularizers.l2(l2=l2),
    activation='softmax',
    name=""softmax""
)(output)

final custom model
name = f""{timestamp}_{model_name.upper()}""
model = tf.keras.models.model(inputs=[input_ids, attention_mask], outputs=output, name=name)
model.compile(
    loss=tf.keras.losses.sparsecategoricalcrossentropy(from_logits=false),
    optimizer=tf.keras.optimizers.adam(learning_rate=learning_rate),
    metrics=['accuracy']
)
model.summary()
---
layer (type)                    output shape         param #     connected to                     
==================================================================================================
input_ids (inputlayer)          [(none, 256)]        0                                            
__________________________________________________________________________________________________
attention_mask (inputlayer)     [(none, 256)]        0                                            
__________________________________________________________________________________________________
tf_distil_bert_model (tfdistilb tfbasemodeloutput(la 66362880    input_ids[0][0]                  
                                                                 attention_mask[0][0]             
__________________________________________________________________________________________________
tf.__operators__.getitem_1 (sli (none, 768)          0           tf_distil_bert_model[1][0]       
__________________________________________________________________________________________________
01_dropout (dropout)            (none, 768)          0           tf.__operators__.getitem_1[0][0] 
__________________________________________________________________________________________________
01_dense_relu_no_regularizer (d (none, 768)          590592      01_dropout[0][0]                 
__________________________________________________________________________________________________
01_bn (batchnormalization)      (none, 768)          3072        01_dense_relu_no_regularizer[0][0
__________________________________________________________________________________________________
01_relu (activation)            (none, 768)          0           01_bn[0][0]                      
__________________________________________________________________________________________________
02_dense_relu_no_regularizer (d (none, 768)          590592      01_relu[0][0]                    
__________________________________________________________________________________________________
02_bn (batchnormalization)      (none, 768)          3072        02_dense_relu_no_regularizer[0][0
__________________________________________________________________________________________________
02_relu (activation)            (none, 768)          0           02_bn[0][0]                      
__________________________________________________________________________________________________
softmax (dense)                 (none, 2)            1538        02_relu[0][0]                    
==================================================================================================
total params: 67,551,746
trainable params: 1,185,794
non-trainable params: 66,365,952   <--- base bert model is frozen

data allocation
# --------------------------------------------------------------------------------
# split data into training and validation
# --------------------------------------------------------------------------------
raw_train = pd.read_csv(""./train.csv"")
train_data, validation_data, train_label, validation_label = train_test_split(
    raw_train[data_column].tolist(),
    raw_train[label_column].tolist(),
    test_size=.2,
    shuffle=true
)

# x = dict(tokenize(train_data))
# y = tf.convert_to_tensor(train_label)
x = tf.data.dataset.from_tensor_slices((
    dict(tokenize(train_data)),  # convert batchencoding instance to dictionary
    train_label
)).batch(batch_size).prefetch(1)

v = tf.data.dataset.from_tensor_slices((
    dict(tokenize(validation_data)),  # convert batchencoding instance to dictionary
    validation_label
)).batch(batch_size).prefetch(1)

train
# --------------------------------------------------------------------------------
# train the model
# 
# input data x can be a dict mapping input names to the corresponding array/tensors, 
# if the model has named inputs. beware of the ""names"". y should be consistent with x 
# (you cannot have numpy inputs and tensor targets, or inversely). 
# --------------------------------------------------------------------------------
history = model.fit(
    x=x,    # dictionary 
    # y=y,
    y=none,
    epochs=num_epochs,
    batch_size=batch_size,
    validation_data=v,
)

to implement the 1st approach, change the configuration as below.
use_custom_head = false

then freeze_base is changed to false and learning_rate is changed to 5e-5 which will run further pre-training on the base bert model.
saving the model
for the 3rd approach, saving the model will cause issues. the save_pretrained method of the huggingface model cannot be used as the model is not a direct sub class from of huggingface pretrainedmodel.
keras save_model causes an error with the default save_traces=true, or causes a different error with save_traces=true when loading the model with keras load_model.
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
<ipython-input-71-01d66991d115> in <module>()
----> 1 tf.keras.models.load_model(model_directory)
 
11 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/saved_model/load.py in _unable_to_call_layer_due_to_serialization_issue(layer, *unused_args, **unused_kwargs)
    865       'recorded when the object is called, and used when saving. to manually '
    866       'specify the input shape/dtype, decorate the call function with '
--> 867       '`@tf.function(input_signature=...)`.'.format(layer.name, type(layer)))
    868 
    869 
 
valueerror: cannot call custom layer tf_distil_bert_model of type <class 'tensorflow.python.keras.saving.saved_model.load.tfdistilbertmodel'>, because the call function was not serialized to the savedmodel.please try one of the following methods to fix this issue:
 
(1) implement `get_config` and `from_config` in the layer/model class, and pass the object to the `custom_objects` argument when loading the model. for more details, see: 
 
(2) ensure that the subclassed model or layer overwrites `call` and not `__call__`. the input shape and dtype will be automatically recorded when the object is called, and used when saving. to manually specify the input shape/dtype, decorate the call function with `@tf.function(input_signature=...)`.

only keras model save_weights worked as far as i tested.
experiments
as far as i tested with toxic comment classification challenge, the 1st approach gave better recall (identify true toxic comment, true non-toxic comment). code can be accessed as below. please provide correction/suggestion if anything.

code for 1st and 3rd approach


related

bert document classification tutorial with code - fine tuning using tfdistilbertforsequenceclassification and pytorch
hugging face transformers: fine-tuning distilbert for binary classification tasks - fine tuning using tfdistilbertmodel",https://stackoverflow.com/questions/69025750,machine-learning,02-09-2021 07:18,9811.0,3.0,1.0,True,11-11-2021 19:55,11-11-2021 19:55
77285102,how to format a few-shot prompt for gpt4 chat completion api?,"i'm trying to use the gpt4's chat completion api for the following prompt:
for each situation, describe the intent. examples:


situation 1: devin gets the newspaper.

the intent of situation 1: devin intends to read the newspaper.

situation 2: jamie works all night.

the intent of situation 2: jamie intends to meet a deadline.

situation 3: sydney destroys ryan.

the intent of situation 3: sydney intends to punish ryan.

situation 4: lindsay clears her mind.

the intent of situation 4: lindsay intends to be ready for a new task.

situation 5: rowan wants to start a business.

the intent of situation 5: rowan intends to be self sufficient.

situation 6: lee ensures aliï¿½ï¿½ï¿½s safety.

the intent of situation 6: lee intends to be helpful.

situation 7: riley buys lottery tickets.

the intent of situation 7: riley intends to become rich.

situation 8: alex makes chris wait.

the intent of situation 8: alex intends

as you can see, i want to complete the senthat says ""alex intends"". this prompt is intuitive for gpt3's completion api where you only had to put one prompt that has all the few-shots examples.
however, i don't know what is the best practice to perform the same prompting with gpt4's chatcompletion api. i've checked out 
where they provided an example of how to do few-shot prompting, but my prompt is not ""conversational"" as you can see.
i'm not even sure whether the ""name"" parameter impacts the result's quality. does anybody have an answer to this?
what i thought of so far is to format my prompt like this as the content from the above link instructed:
messages=[
        {""role"": ""system"", ""content"": ""for each situation, describe the intent. examples:""},
        {""role"": ""system"", ""name"":""situation 1"", ""content"": ""devin gets the newspaper.""},
        {""role"": ""system"", ""name"": ""the intent of situation 1"", ""content"": ""devin intends to read the newspaper.""},
        {""role"": ""system"", ""name"":""situation 2"", ""content"": ""jamie works all night.""},
        {""role"": ""system"", ""name"": ""the intent of situation 2"", ""content"": ""jamie intends to meet a deadline.""},

...
        {""role"": ""system"", ""name"":""situation 8"", ""content"": ""alex makes chris wait.""},
        {""role"": ""user"", ""name"": ""the intent of situation 8"", ""content"": """"},
    ]

is this a proper way to do few-show with gpt4 chatcompletion api? please let me know if you have a better solution or explanations on why certain parts of my prompt needs work.
so far, i've simply put the original prompt into one user content, just like it is gpt3's completion api:
messages=[
    {""role"": ""user"", ""content"": ""for each situation, describe the intent. examples:


situation 1: devin gets the newspaper.

the intent of situation 1: devin intends to read the newspaper.

situation 2: jamie works all night.

the intent of situation 2: jamie intends to meet a deadline.

situation 3: sydney destroys ryan.

the intent of situation 3: sydney intends to punish ryan.

situation 4: lindsay clears her mind.

the intent of situation 4: lindsay intends to be ready for a new task.

situation 5: rowan wants to start a business.

the intent of situation 5: rowan intends to be self sufficient.

situation 6: lee ensures aliï¿½ï¿½ï¿½s safety.

the intent of situation 6: lee intends to be helpful.

situation 7: riley buys lottery tickets.

the intent of situation 7: riley intends to become rich.

situation 8: alex makes chris wait.

the intent of situation 8: alex intends""}
]

it does work, but i was wondering if i can boost the api's performance if i follow a certain practice","['openai-api', 'chatgpt-api', 'completion', 'gpt-4', 'few-shot-learning']",77293295,you might want to check out our gpt best practices guide which talks about how to do prompting effectively. the reality is that prompt engineering for tasks like this is much more art than science today. i suggest trying both approaches but my hunch is the one with each example as a user message / assistant response separately will perform better based on experiments i have seen.,https://stackoverflow.com/questions/77285102,openai-api,13-10-2023 04:45,7935.0,2.0,2.0,True,01-11-2023 04:53,13-10-2023 04:49
77774499,how to save keras textvectorization layer configuration with custom standardization function into a pickle file and reload it?,"i have a keras textvectorization layer which uses a custom standardization function.
def custom_standardization(input_string, preserve=['[', ']'], add=['ï¿½ï¿½']):

    strip_chars = string.punctuation
    for item in add:
        strip_chars += item
    
    for item in preserve:
        strip_chars = strip_chars.replace(item, '')

    lowercase = tf.strings.lower(input_string)
    output = tf.strings.regex_replace(lowercase, f'[{re.escape(strip_chars)}]', '')

    return output

target_vectorization = keras.layers.textvectorization(max_tokens=vocab_size,
                                                output_mode='int',
                                                output_sequence_length=sequence_length + 1,
                                                standardize=custom_standardization)

target_vectorization.adapt(train_spanish_texts)

i want to save the adapted configuration for an inference model to make uf.
one way, as described here, is to save the weights and config separately as a pickle file and reload them.
however, target_vectorization.get_config()  returns
{'name': 'text_vectorization_5',
 'trainable': true,
 ...
 'standardize': <function __main__.custom_standardization(input_string, preserve=['[', ']'], add=['ï¿½ï¿½'])>,
 ...
 'vocabulary_size': 15000}

which is being saved into the pickle file.
trying to load this config using keras.layers.textvectorization.from_config(pickle.load(open('ckpts/spanish_vectorization.pkl', 'rb'))['config']) results in typeerror: could not parse config: <function custom_standardization at 0x2a1973a60>, because the file does not have any information about this custom standardization function.
what is a good way to save the textvectorization weights and configuration for an inference model to make use of, in this scenario?<","['python', 'tensorflow', 'keras', 'nlp']",77788582,"the solution here was to define a wrapper around the textvectorization object and use the custom standardizer as a method. moreover, we needed to exclude callable objects while saving configuration to the pickle file. here's the fixed code:
@keras.utils.register_keras_serializable(package='custom_layers', name='textvectorizer')
class textvectorizer(layers.layer):
    '''english - spanish text vectorizer'''

    def __init__(self, max_tokens=none, output_mode='int', output_sequence_length=none, standardize='lower_and_strip_punctuation', vocabulary=none, config=none):
        super().__init__()
        if config:
            self.vectorization = layers.textvectorization.from_config(config)

        else:
            self.max_tokens = max_tokens
            self.output_mode = output_mode
            self.output_sequence_length = output_sequence_length
            self.vocabulary = vocabulary
            if standardize != 'lower_and_strip_punctuation':
                self.vectorization = layers.textvectorization(max_tokens=self.max_tokens,
                                                              output_mode=self.output_mode,
                                                              output_sequence_length=self.output_sequence_length,
                                                              vocabulary=self.vocabulary,
                                                              standardize=self.standardize)
            else:
                self.vectorization = layers.textvectorization(max_tokens=self.max_tokens,
                                                              output_mode=self.output_mode,
                                                              output_sequence_length=self.output_sequence_length,
                                                              vocabulary=self.vocabulary)


    def standardize(self, input_string, preserve=['[', ']'], add=['ï¿½ï¿½']) -> str:
        strip_chars = string.punctuation
        for item in add:
            strip_chars += it   
        for item in preserve:
            strip_chars = strip_chars.replace(item, '')

        lowercase = tf.strings.lower(input_string)
        output = tf.strings.regex_replace(lowercase, f'[{re.escape(strip_chars)}]', '')

        return output

    def __call__(self, *args, **kwargs):
        return self.vectorization.__call__(*args, **kwargs)

    def get_config(self):
        return {key: value if not callable(value) else none for key, value in self.vectorization.get_config().items()}

    def from_config(config):
        return textvectorizer(config=config)

    def set_weights(self, weights):
        self.vectorization.set_weights(weights)

    def adapt(self, dataset):
        self.vectorization.adapt(dataset)

    def get_vocabulary(self):
        return self.vectorization.get_vocabulary()

to adapt and save weights [training phase]:
vocab_size = 15000
sequence_length = 20

source_vectorization = textvectorizer(max_tokens=vocab_size,
                                      output_mode='int',
                                      output_sequence_length=sequence_length)

target_vectorization = textvectorizer(max_tokens=vocab_size,
                                      output_mode='int',
                                      output_sequence_length=sequence_length + 1,
                                      standardize='spanish')

train_english_texts = [pair[0] for pair in train_pairs]
train_spanish_texts = [pair[1] for pair in train_pairs]
source_vectorization.adapt(train_english_texts)
target_vectorization.adapt(train_spanish_texts)

pickle.dump({'config': source_vectorization.get_config(),
             'weights': source_vectorization.get_weights()}, open('ckpts/english_vectorization.pkl', 'wb'))

pickle.dump({'config': target_vectorization.get_config(),
             'weights': target_vectorization.get_weights()}, open('ckpts/spanish_vectorization.pkl', 'wb'))

to load and use them [inference phase]:
vectorization_data = pickle.load(open('ckpts/english_vectorization.pkl', 'rb'))
source_vectorization = textvectorizer.from_config(vectorization_data['config'])
source_vectorization.set_weights(vectorization_data['weights'])

vectorization_data = pickle.load(open('ckpts/spanish_vectorization.pkl', 'rb'))
target_vectorization = textvectorizer.from_config(vectorization_data['config'])
target_vectorization.set_weights(vectorization_data['weights'])",https://stackoverflow.com/questions/77774499,python,07-01-2024 19:21,464.0,1.0,2.0,True,09-01-2024 17:16,09-01-2024 11:42
72211841,how to specify a column for prediction in haystack?,"i am using an opensearchdocumentstore to store my data an feed it to my haystack pipeline, the data includes uuids and other information, that are not relevant for the prediction but need to stack in the documentstore (i was told). now i wonder if there is a way or need to specify a special inputtext ""column"" for the retriever and reader.","['python', 'rename', 'opensearch', 'nlp-question-answering', 'haystack']",72226225,"other information that are not relevant for retrieval should stay in the meta field of documents. for example you could create documents like:
doc = document(
    content=""this is relevant for retrieval"", 
    meta={""uuid"": ""1234"",  ""comment"": ""this won't be used for retrieval""}
)",https://stackoverflow.com/questions/72211841,python,12-05-2022 07:48,239.0,2.0,1.0,True,08-07-2022 16:13,08-07-2022 16:13
68760136,attributeerror: caught attributeerror in dataloader worker process 0. - fine tuning pre-trained transformer model,"can anyone help me to resolve this error?
---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-4-aaa58b106c77> in <module>()
     25     output_path='fine_tuned_bert',
     26     save_best_model= true,
---> 27     show_progress_bar= true
     28     )

4 frames
/usr/local/lib/python3.7/dist-packages/torch/_utils.py in reraise(self)
    423             # have message field
    424             raise self.exc_type(message=msg)
--> 425         raise self.exc_type(msg)
    426 
    427 

attributeerror: caught attributeerror in dataloader worker process 0.
original traceback (most recent call last):
  file ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py"", line 287, in _worker_loop
    data = fetcher.fetch(index)
  file ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py"", line 47, in fetch
    return self.collate_fn(data)
  file ""/usr/local/lib/python3.7/dist-packages/sentence_transformers/sentencetransformer.py"", line 518, in smart_batching_collate
    num_texts = len(batch[0].texts)
attributeerror: 'str' object has no attribute 'texts'

code:
import pandas as pd
# initialise data of lists.
data = {'input':[
          ""alpro, cioccolato bevanda a base di soia 1 ltr"", #alpro, chocolate soy drink 1 ltr
          ""milka  cioccolato al latte 100 g"", #milka milk chocolate 100 g
          ""danone, hipro 25g proteine gusto cioccolato 330 ml"", #danone, hipro 25g protein chocolate flavor 330 ml
         ]
        }
 
# creates pandas dataframe.
x_sample = pd.dataframe(data)
print(x_sample['input'])

# load model
from sentence_transformers import sentencetransformer, sentencesdataset, inputexample, losses, evaluation
from torch.utils.data import dataloader

embedder = sentencetransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1') # or any other pretrained model
print(""embedder loaded..."")

# define your train dataset, the dataloader, and the train loss
train_dataset = sentencesdataset(x_sample[""input""].tolist(), embedder)
train_dataloader = dataloader(train_dataset, shuffle=false, batch_size=4, num_workers=1)
train_loss = losses.cosinesimilarityloss(embedder)

# dummy evaluator to make the api work
sentences1 = ['latte al cioccolato', 'latte al cioccolato','latte al cioccolato']
sentences2 = ['alpro, cioccolato bevanda a base di soia 1 ltr', 'danone, hipro 25g proteine gusto cioccolato 330 ml','milka  cioccolato al latte 100 g']
scores = [0.99,0.95,0.4]
evaluator = evaluation.embeddingsimilarityevaluator(sentences1, sentences2, scores)

# tune the model
embedder.fit(train_objectives=[(train_dataloader, train_loss)], 
    epochs=5, 
    warmup_steps=500, 
    evaluator=evaluator, 
    evaluation_steps=1,
    output_path='fine_tuned_bert',
    save_best_model= true,
    show_progress_bar= true
    )","['python', 'machine-learning', 'nlp', 'bert-language-model']",68797928,"[updated]
i skimmed several lines of documentation here about how to use the fit() method and i realized there is a simpler solution to do what you desired. the only changes you need to consider are to define proper inputexample for constructing a dataloader & create a loss!
import pandas as pd
# initialise data of lists.
data = {'input':[
          ""alpro, cioccolato bevanda a base di soia 1 ltr"", #alpro, chocolate soy drink 1 ltr
          ""milka  cioccolato al latte 100 g"", #milka milk chocolate 100 g
          ""danone, hipro 25g proteine gusto cioccolato 330 ml"", #danone, hipro 25g protein chocolate flavor 330 ml
         ]
        }
 
# creates pandas dataframe.
x_sample = pd.dataframe(data)
print(x_sample['input'])

# load model
from sentence_transformers import sentencetransformer, sentencesdataset, inputexample, losses, evaluation
from torch.utils.data import dataloader

embedder = sentencetransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1') # or any other pretrained model
print(""embedder loaded..."")

# define your train dataset, the dataloader, and the train loss
# train_dataset = sentencesdataset(x_sample[""input""].tolist(), embedder)
# train_dataloader = dataloader(train_dataset, shuffle=false, batch_size=4, num_workers=1)
# train_loss = losses.cosinesimilarityloss(embedder)

# dummy evaluator to make the api work
sentences1 = ['latte al cioccolato', 'latte al cioccolato','latte al cioccolato']
sentences2 = ['alpro, cioccolato bevanda a base di soia 1 ltr', 'danone, hipro 25g proteine gusto cioccolato 330 ml','milka  cioccolato al latte 100 g']
scores = [0.99,0.95,0.4]
evaluator = evaluation.embeddingsimilarityevaluator(sentences1, sentences2, scores)

examples = []
for s1,s2,l in zip(sentences1, sentences2, scores):
  examples.append(inputexample(texts=[s1, s2], label=l))
train_dataloader = dataloader(examples, shuffle=false, batch_size=4, num_workers=1)
train_loss = losses.cosinesimilarityloss(embedder)
# tune the model
embedder.fit(train_objectives=[(train_dataloader, train_loss)], 
    epochs=5, 
    warmup_steps=500, 
    evaluator=evaluator, 
    evaluation_steps=1,
    output_path='fine_tuned_bert',
    save_best_model= true,
    show_progress_bar= true
    )",https://stackoverflow.com/questions/68760136,python,12-08-2021 15:19,1774.0,0.0,1.0,True,17-08-2021 13:41,13-08-2021 00:11
74642594,why does stablediffusionpipeline return black images when generating multiple images at once?,"i am using the stablediffusionpipeline from the hugging face diffusers library in python 3.10.2, on an m2 mac (i tagged it because this might be the issue). when i try to generate 1 image from 1 prompt, the output looks fine, but when i try to generate multiple images using the same prompt, the images are all either black squares or a random image (see example below). what could be the issue?
my code is as follows (where i change n_imgs from 1 to more than 1 to break it):
from diffusers import stablediffusionpipeline

pipe = stablediffusionpipeline.from_pretrained(""runwayml/stable-diffusion-v1-5"")
pipe = pipe.to(""mps"")  # for m1/m2 chips
pipe.enable_attention_slicing()

prompt = ""a photo of an astronaut driving a car on mars""

# first-time ""warmup"" pass (because of weird m1 behaviour)
_ = pipe(prompt, num_inference_steps=1)

# generate images
n_imgs = 1
imgs = pipe([prompt] * n_imgs).images

i also tried setting num_images_per_prompt instead of creating a list of repeated prompts in the pipeline call, but this gave the same bad results.
example output (for multiple images):

[edit/update]: when i generate the images in a loop surrounding the pipe call instead of passing an iterable to the pipe call, it does work:
# generate images
n_imgs = 3
for i in range(n_imgs):
    img = pipe(prompt).images[0]
    # do something with img

but it is still a mystery to me as to why.","['python', 'pytorch', 'apple-m1', 'huggingface-transformers', 'stable-diffusion']",74642804,"apparently it is indeed an apple silicon (m1/m2) issue, of which hugging face is not yet sure why this is happening, see this github issue for more details.",https://stackoverflow.com/questions/74642594,python,01-12-2022 13:23,3460.0,4.0,3.0,True,05-07-2023 02:27,02-12-2022 08:59
68691450,how can i check a confusion_matrix after fine-tuning with custom datasets?,"this question is the same with how can i check a confusion_matrix after fine-tuning with custom datasets?, on data science stack exchange.
background
i would like to check a confusion_matrix, including precision, recall, and f1-score like below after fine-tuning with custom datasets.
fine tuning process and the task are sequence classification with imdb reviews on the fine-tuning with custom datasets tutorial on hugging face.
after finishing the fine-tune with trainer, how can i check a confusion_matrix in this case?
an image of confusion_matrix, including precision, recall, and f1-score original site: just for example output image
predictions = np.argmax(trainer.test(test_x), axis=1)

# confusion matrix and classification report.
print(classification_report(test_y, predictions))

            precision    recall  f1-score   support

          0       0.75      0.79      0.77      1000
          1       0.81      0.87      0.84      1000
          2       0.63      0.61      0.62      1000
          3       0.55      0.47      0.50      1000
          4       0.66      0.66      0.66      1000
          5       0.62      0.64      0.63      1000
          6       0.74      0.83      0.78      1000
          7       0.80      0.74      0.77      1000
          8       0.85      0.81      0.83      1000
          9       0.79      0.80      0.80      1000

avg / total       0.72      0.72      0.72     10000

code
from transformers import distilbertforsequenceclassification, trainer, trainingarguments

training_args = trainingarguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
)

model = distilbertforsequenceclassification.from_pretrained(""distilbert-base-uncased"")

trainer = trainer(
    model=model,                         # the instantiated ï¿½ï¿½ï¿½ï¿½ transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=vet             # evaluation dataset
)

trainer.train()

what i did so far
data set preparation for sequence classification with imdb reviews, and i'm fine-tuning with trainer.
from pathlib import path

def read_imdb_split(split_dir):
    split_dir = path(split_dir)
    texts = []
    labels = []
    for label_dir in [""pos"", ""neg""]:
        for text_file in (split_dir/label_dir).iterdir():
            texts.append(text_file.read_text())
            labels.append(0 if label_dir is ""neg"" else 1)

    return texts, labels

train_texts, train_labels = read_imdb_split('aclimdb/train')
test_texts, test_labels = read_imdb_split('aclimdb/test')

from sklearn.model_selection import train_test_split
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)

from transformers import distilberttokenizerfast
tokenizer = distilberttokenizerfast.from_pretrained('distilbert-base-uncased')

train_encodings = tokenizer(train_texts, truncation=true, padding=true)
val_encodings = tokenizer(val_texts, truncation=true, padding=true)
test_encodings = tokenizer(test_texts, truncation=true, padding=true)

import torch

class imdbdataset(torch.utils.data.dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = imdbdataset(train_encodings, train_labels)
val_dataset = imdbdataset(val_encodings, val_labels)
test_dataset = imdbdataset(test_encodings, test_labels)","['python-3.x', 'machine-learning', 'pytorch', 'nlp', 'huggingface-transformers']",68933588,"what you could do in this situation is to iterate on the validation set(or on the test set for that matter) and manually create a list of y_true and y_pred.
import torch
import torch.nn.functional as f
from sklearn import metrics
 
y_preds = []
y_trues = []
for index,val_text in enumerate(val_texts):
     tokenized_val_text = tokenizer([val_text], 
                                    truncation=true,
                                    padding=true,
                                    return_tensor='pt')
     logits = model(tokenized_val_text)
     prediction = f.softmax(logits, dim=1)
     y_pred = torch.argmax(prediction).numpy()
     y_true = val_labels[index]
     y_preds.append(y_pred)
     y_trues.append(y_true)

finally,
confusion_matrix = metrics.confusion_matrix(y_trues, y_preds, labels=[""neg"", ""pos""]))
print(confusion_matrix)

observations:

the output of the model are the logits, not the probabilities normalized.
as such, we apply softmax on dimension one to transform to actual probabilities (e.g. 0.2% class 0, 0.8% class 1).
we apply the .argmax() operation to get the index of the class.",https://stackoverflow.com/questions/68691450,python-3.x,07-08-2021 10:19,6583.0,7.0,1.0,True,12-05-2023 14:11,12-05-2023 14:11
73148372,training my spacy models gives &quot;could not load dynamic library &#39;libcudart.so.11.0&#39;&quot; but continues anyways,"my question is actually a quick one. i am using spacy in a venv, following an online tutorial. i've just trained my model and whilst the training itself concluded, it also returns these errors:
(venv) [annemarie@annemarie-linux trainingdata]$ python3.8 -m spacy train config/config.cfg --paths.train corpus/spacy-docbins/train.spacy --paths.dev corpus/spacy-docbins/test.spacy --output output/models --training.eval_frequency 10 --training.max_steps 300
2022-07-28 08:35:25.488466: w tensorflow/stream_executor/platform/default/dso_loader.cc:64] could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: no such file or directory
2022-07-28 08:35:25.488481: i tensorflow/stream_executor/cuda/cudart_stub.cc:29] ignore above cudart dlerror if you do not have a gpu set up on your machine.
2022-07-28 08:35:26.409531: i tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful numa node read from sysfs had negative value (-1), but there must be at least one numa node, so returning numa node zero
2022-07-28 08:35:26.409662: w tensorflow/stream_executor/platform/default/dso_loader.cc:64] could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: no such file or directory
2022-07-28 08:35:26.409694: w tensorflow/stream_executor/platform/default/dso_loader.cc:64] could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: no such file or directory
2022-07-28 08:35:26.409721: w tensorflow/stream_executor/platform/default/dso_loader.cc:64] could not load dynamic library 'libcublaslt.so.11'; dlerror: libcublaslt.so.11: cannot open shared object file: no such file or directory
2022-07-28 08:35:26.409747: w tensorflow/stream_executor/platform/default/dso_loader.cc:64] could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: no such file or directory
2022-07-28 08:35:26.409773: w tensorflow/stream_executor/platform/default/dso_loader.cc:64] could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: no such file or directory
2022-07-28 08:35:26.409799: w tensorflow/stream_executor/platform/default/dso_loader.cc:64] could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: no such file or directory
2022-07-28 08:35:26.409824: w tensorflow/stream_executor/platform/default/dso_loader.cc:64] could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: no such file or directory
2022-07-28 08:35:26.409850: w tensorflow/stream_executor/platform/default/dso_loader.cc:64] could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: no such file or directory
2022-07-28 08:35:26.409857: w tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] cannot dlopen some gpu libraries. please make sure the missing libraries mentioned above are installed properly if you would like to use gpu. follow the guide at  for how to download and setup the required libraries for your platform.
skipping registering gpu devices...
ï¿½ï¿½ï¿½ saving to output directory: output/models
ï¿½ï¿½ï¿½ using cpu

=========================== initializing pipeline ===========================
[2022-07-28 08:35:26,788] [info] set up nlp object from config
[2022-07-28 08:35:26,794] [info] pipeline: ['ner']
[2022-07-28 08:35:26,797] [info] created vocabulary
[2022-07-28 08:35:26,797] [info] finished initializing nlp object
[2022-07-28 08:35:26,845] [info] initialized pipeline components: ['ner']
ï¿½ï¿½ï¿½ initialized pipeline

============================= training pipeline =============================
ï¿½ï¿½ï¿½ pipeline: ['ner']
ï¿½ï¿½ï¿½ initial learn rate: 0.001
e    #       loss ner  ents_f  ents_p  ents_r  score 
---  ------  --------  ------  ------  ------  ------
  0       0     68.44    0.00    0.00    0.00    0.00
  3      10    615.49    0.00    0.00    0.00    0.00
  6      20    295.06   28.57   57.14   19.05    0.29
 10      30    210.32   46.67   77.78   33.33    0.47
 13      40    154.60   55.00   57.89   52.38    0.55
  84.21   76.19    0.80
 20      60     20.64   92.68   95.00   90.48    0.93
 23      70      2.85   92.68   95.00   90.48    0.93
 26      80      0.01   92.68   95.00   90.48    0.93
 30      90      0.00   92.68   95.00   90.48    0.93
 33     100      0.00   87.80   90.00   85.71    0.88
 37     110      0.00   92.68   95.00   90.48    0.93
 41     120      0.00   92.68   95.00   90.48    0.93
 45     130      0.00   92.68   95.00   90.48    0.93
 50     140      0.00   92.68   95.00   90.48    0.93
 55     150      0.00   92.68   95.00   90.48    0.93
 60     160      0.00   92.68   95.00   90.48    0.93
 65     170      0.00   92.68   95.00   90.48    0.93
 70     180      0.00   92.68   95.00   90.48    0.93
 75     190      0.00   92.68   95.00   90.48    0.93
 80     200      0.00   92.68   95.00   90.48    0.93
 85     210      0.00   92.68   95.00   90.48    0.93
 90     220      0.00   92.68   95.00   90.48    0.93
 95     230      0.00   87.80   90.00   85.71    0.88
100     240      0.00   87.80   90.00   85.71    0.88
105     250      0.00   87.80   90.00   85.71    0.88
110     260      0.00   87.80   90.00   85.71    0.88
115     270      0.00   92.68   95.00   90.48    0.93
120     280      0.00   92.68   95.00   90.48    0.93
125     290      0.00   92.68   95.00   90.48    0.93
130     300      0.00   92.68   95.00   90.48    0.93
ï¿½ï¿½ï¿½ saved pipeline to output directory
output/models/model-last

now, i understand i could probably just install 'libcudart.so.11.0'. however, neither the tutorial nor any others (or the doc) i looked into about spacy mentioned i'd need to separately install this, so i am confused as to maybe i have done something wrong or installed something faulty.
i installed spacy using pip and i am using python3.8, i'm on manjaro (arch). should i just install libcudart? i've never worked much with python and this is my first time using spacy so i apologize if this question is weird.

edit: which makes it more confusing, testing the modes it functions just as i trained it. however, it'll give the same block of warnings as above for spacy.load(""pathtomodel""), though.","['python', 'tensorflow', 'pip', 'spacy']",73150399,"these warnings are related to tensorflow, which isn't being used spacy or spacy train for these pipelines.
spacy automatically imports tensorflow if it's available and that's why you see the warnings, but they shouldn't affect anything for typical spacy usage.
if you work in a venv where tensorflow is not installed, the warnings should go away.",https://stackoverflow.com/questions/73148372,python,28-07-2022 06:47,404.0,1.0,1.0,True,28-07-2022 09:25,28-07-2022 06:55
47295316,importerror: no module named &#39;spacy.en&#39;,"i'm working on a codebase that uses spacy. i installed spacy using:
sudo pip3 install spacy

and then 
sudo python3 -m spacy download en

at the end of this last command, i got a message:
    linking successful
/home/rayabhik/.local/lib/python3.5/site-packages/en_core_web_sm -->
/home/rayabhik/.local/lib/python3.5/site-packages/spacy/data/en

you can now load the model via spacy.load('en')

now, when i try running my code, on the line:
    from spacy.en import english

it gives me the following error:
importerror: no module named 'spacy.en'

i've looked on stackexchange and the closest is:  import error with spacy: ""no module named en""
which does not solve my problem.
any help would be appreciated. thanks.
edit: i might have solved this by doing the following:
 python 3.5.2 (default, sep 14 2017, 22:51:06) 
[gcc 5.4.0 20160609] on linux
type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import spacy
>>> spacy.load('en')
<spacy.lang.en.english object at 0x7ff414e1e0b8>

and then using:
from spacy.lang.en import english

i'm still keeping this open in case there are any other answers.","['python', 'spacy']",47297686,"yes, i can confirm that your solution is correct. the version of spacy you downloaded from pip is v2.0, which includes a lot of new features, but also a few changes to the api. one of them is that all language data has been moved to a submodule spacy.lang to keep thing cleaner and better organised. so instead of using spacy.en, you now import from spacy.lang.en.
- from spacy.en import english
+ from spacy.lang.en import english

however, it's also worth mentioning that what you download when you run spacy download en is not the same as spacy.lang.en. the language data shipped with spacy includes the static data like tokenization rules, stop words or lemmatization tables. the en package that you can download is a shortcut for the statistical model en_core_web_sm. it includes the language data, as well as binary weight to enable spacy to make predictions for part-of-speech tags, dependencies and named entities.
instead of just downloading en, i'd actually recommend using the full model name, which makes it much more obvious what's going on:
python -m spacy download en_core_web_sm

nlp = spacy.load(""en_core_web_sm"")

when you call spacy.load, spacy does the following:

find the installed model named ""en_core_web_sm"" (a package or shortcut link).
read its meta.json and check which language it's using (in this case, spacy.lang.en), and how its processing pipeline should look (in this case, tagger, parser and ner).
initialise the language class and add the pipeline to it.
load in the binary weights from the model data so pipeline components (like the tagger, parser or entity recognizer) can make predictions.

see this section in the docs for more details.",https://stackoverflow.com/questions/47295316,python,14-11-2017 21:15,154769.0,55.0,14.0,True,09-08-2022 11:47,28-03-2018 08:05
69941156,the `glibc_2.29 not found` problem of the installation of transformers?,"to run transformers i installed it on centos 8 by
conda install -c conda-forge transformers=4.12.2

following the method on this page, but i still encountered the same error:

version `glibc_2.29ï¿½ï¿½ï¿½ not found

then i tried to install that from the huggingface channel and got stuck by lots of conflicts.
conda install -c huggingface transformers=4.12.2

at last i tried to install the glibc231 myself, and downloaded the rpm from this link, but i saw the following error, leading me to think that i would be in the wrong direction:

error:  problem: conflicting requests

nothing provides glibc-common = 2.31-3.gf.el7 needed by glibc231-2.31-3.gf.el7.x86_64
nothing provides glibc-langpack = 2.31-3.gf.el7 needed by glibc231-2.31-3.gf.el7.x86_64


any suggestions? thanks in advance.","['python', 'deep-learning', 'nlp', 'glibc', 'huggingface-transformers']",70469240,"i had the same issues, and i downgraded to the following version:
tokenizers=0.10.1 
transformers=4.6.1",https://stackoverflow.com/questions/69941156,python,12-11-2021 10:09,2440.0,3.0,2.0,True,01-02-2022 11:47,27-12-2021 03:29
79288703,creating a &#39;for&#39; or &#39;if-else&#39; selector for checking dynamically sized python list,"i'm trying to mask 8752 images with transformers like this
from transformers import pipeline
from pil import image
import requests
import cv2

import numpy as np
from matplotlib import pyplot as plt


semantic_segmentation_nvidia = pipeline(""image-segmentation"", ""nvidia/segformer-b0-finetuned-ade-512-512"")
jpeg_im = none
a = 0
mask_i = 0

f = open(""masking_log.txt"", ""w"")


for im in large_image_stack_512:
    i=0 
    while(i == 0):
        
        jpeg_im = image.open(os.path.join(root_dir,im))
        print(os.path.join(root_dir,im))
        # semantic segmentation
        segmentation = semantic_segmentation_nvidia(jpeg_im)
        print(""the length of current segmentation labels are: "", len(segmentation))
        water_mask_label = segmentation[mask_i][""label""]
        print(water_mask_label)
        print(""here"")    
        if (water_mask_label == ""water""):
            print(""successful labelling at: "", mask_i)
            water_mask = segmentation[mask_i][""mask""]
            print(""here"")
            imar = np.asarray(water_mask)
            print(water_mask_label)
            print(""type im (array)"", type(imar))
            f.write(""image "" + str(a) + ""\nsuccess-label at "" + str(mask_i) + ""\nwith dir: "" + str(im)  + ""\n with mask labeled as: "" + str(water_mask_label) + '\n\n')
            plt.imsave('d:\..\data\'+'img_'+str(a)+'.jpg', imar, cmap=""gray"")  
            i=1
            a+=1
            mask_i= 0
            semantic_jpeg = none
            imar = none
            water_mask = none
            water_mask_label = none
            segmentation = none
            water_mask_label = none
        else:       
            print(""not water"")
            if (mask_i < len(segmentation)):
                mask_i += 1
            else:
                f.write(""image "" + str(a) + ""\n unsuccess-labelling (has no 'water' label)"" + ""final mask_i value: "" + str(mask_i) + ""\nwith dir: "" + str(im)  + ""\n check later "" +  + '\n\n')
                print(""masking fails, check later image"" + im)
                i = 1
                continue
            
            
        #plt.imshow(water_mask)
        #plt.show()
        #print(""type jpeg_im (jpeg)"", type(water_mask))
    continue   
       
#print(len(cropped))
f.close()

each of segmentation = semantic_segmentation_nvidia(jpeg_im) will have different size of array, for example with this image i have 11 items inside it like this:
the code (did this in jupyter notebook rows)
a_512 = semantic_segmentation_nvidia(image_512)
a_512

the output, a_512 variable is a list with 11 items inside it
[{'score': none,
  'label': 'wall',
  'mask': <pil.image.image image mode=l size=512x512>},
 {'score': none,
  'label': 'building',
  'mask': <pil.image.image image mode=l size=512x512>},
 {'score': none,
  'label': 'sky',
  'mask': <pil.image.image image mode=l size=512x512>},
 {'score': none,
  'label': 'tree',
  'mask': <pil.image.image image mode=l size=512x512>},
 {'score': none,
  'label': 'earth',
  'mask': <pil.image.image image mode=l size=512x512>},
 {'score': none,
  'label': 'water',
  'mask': <pil.image.image image mode=l size=512x512>},
 {'score': none,
  'label': 'fence',
  'mask': <pil.image.image image mode=l size=512x512>},
 {'score': none,
  'label': 'railing',
  'mask': <pil.image.image image mode=l size=512x512>},
 {'score': none,
  'label': 'bridge',
  'mask': <pil.image.image image mode=l size=512x512>},
 {'score': none,
  'label': 'ship',
  'mask': <pil.image.image image mode=l size=512x512>},
 {'score': none,
  'label': 'pier',
  'mask': <pil.image.image image mode=l size=512x512>}]

to access things i need, probably the 'label', i need to access the pil.image.imagedata from each list item like this
(a_512[5][""mask""]

in the codes above, i used a variable to represent 5 in this code because apparently each image has different order of when it detects 'water' as the label and thus the required mask.
because python can compare strings, i'm trying to make a for loop to check every items in the list of every 8752 images. let's say some of the images can have 10-12 items like this output of the final run.
d:\..\data\cropped_512\img_2541.jpg
the length of current segmentation labels are:  15
car
here
not water
d:\..\data\cropped_512\img_2541.jpg
the length of current segmentation labels are:  15
water
here
successful labelling at:  7
here
water
type im (array) <class 'numpy.ndarray'>
d:\..\data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
wall
here
not water
d:\..\data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
building
here
not water
d:\..\data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
sky
here
not water
d:\..\data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
tree
here
not water
d:\..\data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
road
here
not water
d:\..\data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
mountain
here
not water
d:\..\data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
car
here
not water
d:\..\data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
sea
here
not water
d:\..\data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
fence
here
not water
d:\..\data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
bridge
here
not water
d:\..\data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
boat
here
not water
d:\..\data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
ship
here
not water
d:\..\data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12

---------------------------------------------------------------------------
indexerror                                traceback (most recent call last)
cell in[16], line 17
     15 segmentation = semantic_segmentation_nvidia(jpeg_im)
     16 print(""the length of current segmentation labels are: "", len(segmentation))
---> 17 water_mask_label = segmentation[mask_i][""label""]
     18 print(water_mask_label)
     19 print(""here"")    

indexerror: list index out of range

as you can see the index keeps increasing and didn't touch the last else of the if-else comparator, so it went out of bounds. i have tried this and using range on the for-loop but it didn't work and still went out of bounds. i put breaks and it stops the loop, i put continue it keeps went out of bounds.which part did i do wrong and understood incorrectly about python comparator behavior?
i also tried logging it into .txt file or give status with print but it didn't go that way and always go out of bounds. will gladly add more details if needed. the image is 512 x 512 size.","['python', 'arrays', 'list', 'loops', 'huggingface-transformers']",79293048,"here is some code using oldboy's suggestion to move the increment:
mask_i = 0
a = 0

f = open(""masking_log.txt"", ""w"")

for im in large_image_stack_512:
    image_success_flag = 0
    mask_i= 0
    
    while(image_success_flag < 1):
        jpeg_im = image.open(os.path.join(root_dir,im))
        print(os.path.join(root_dir,im))
        # semantic segmentation
        segmentation = semantic_segmentation_nvidia(jpeg_im)
        print(""the length of current segmentation labels are: "", len(segmentation))
        while(mask_i < len(segmentation)):
           image_mask = segmentation[mask_i][""label""]
           print(image_mask)
           if(image_mask == ""water""):
               print(""correct mask"")
               water_mask = segmentation[mask_i][""mask""]
               imar = np.asarray(water_mask)
               plt.imsave('d:/semester_12/data_irredeano/'+'img_'+str(a)+'.jpg', imar, cmap=""gray"") 
               print(""here"")
               f.write(""image "" + str(a) + ""\nsuccess-label at "" + str(mask_i) + ""\nwith dir: "" + str(im)  + ""\n with mask labeled as: "" + str(water_mask_label) + '\n\n')
               print(""mask-succesfully saved"")
               mask_i = 0 
               break
           elif(image_mask != ""water""):
               mask_i+=1
               print(""number of mask: "", mask_i)
               if(mask_i == len(segmentation)):
                    print(""this image has no correct mask, check later"")
                    f.write(""image "" + str(a) + ""\n unsuccess-labelling (has no 'water' label) final \n mask_i value: "" + str(mask_i) + ""\nwith dir: "" + str(im)  + ""\n check later "" + '\n\n')
               image_success_flag=+1  
        a+=1


f.close()

instead of selecting the mask by checking the segmentation[mask_i][""label] i check if the 'cursor' or mask_i if it's smaller than the  length of the list (len(segmentation)). the += also contributes to the problem since it's adding first then change the number, as implied here and here, because of that my 'cursor variable' can move beyond the array size before checking the segmentation[mask_i][""label]. but i don't think i have other choice other than that to increment since =+ is just redefining itself.
other than that i also added another while condition to make sure the code runs while the mask_i is below the size of the list, so the program become ""if it's still below the list size, check whether mask is ""water"" or not ""water"".
although the program is finished and can mask most of the images, i still have to log several different images since not all of them had ""water"" as the label, but something close like ""sea"" that we humans can intuitively say they are basically the same but computer that can only compare strings not.",https://stackoverflow.com/questions/79288703,python,17-12-2024 16:41,70.0,0.0,1.0,True,05-03-2025 14:50,17-12-2024 17:17
45426215,how to remove stop phrases/stop ngrams (multi-word strings) using pandas/sklearn?,"i want to prevent certain phrases for creeping into my models. for example, i want to prevent 'red roses' from entering into my analysis. i understand how to add individual stop words as given in adding words to scikit-learn's countvectorizer's stop list by doing so:
from sklearn.feature_extraction import text
additional_stop_words=['red','roses']

however, this also results in other ngrams like 'red tulips' or 'blue roses' not being detected. 
i am building a tfidfvectorizer as part of my model, and i realize the processing i need might have to be entered after this stage but i am not sure how to do this.
my eventual aim is to do topic modelling on a piece of text. here is the piece of code (borrowed almost directly from  ) that i am working on:
from sklearn import decomposition

from sklearn.feature_extraction import text
additional_stop_words = ['red', 'roses']

sw = text.english_stop_words.union(additional_stop_words)
mod_vectorizer = text.tfidfvectorizer(
    ngram_range=(2,3),
    stop_words=sw,
    norm='l2',
    min_df=5
)

dtm = mod_vectorizer.fit_transform(df[col]).toarray()
vocab = np.array(mod_vectorizer.get_feature_names())
num_topics = 5
num_top_words = 5
m_clf = decomposition.latentdirichletallocation(
    n_topics=num_topics,
    random_state=1
)

doctopic = m_clf.fit_transform(dtm)
topic_words = []

for topic in m_clf.components_:
    word_idx = np.argsort(topic)[::-1][0:num_top_words]
    topic_words.append([vocab[i] for i in word_idx])

doctopic = doctopic / np.sum(doctopic, axis=1, keepdims=true)
for t in range(len(topic_words)):
    print(""topic {}: {}"".format(t, ','.join(topic_words[t][:5])))

edit
sample dataframe (i have tried to insert as many edge cases as possible),  df:
   content
0  i like red roses as much as i like blue tulips.
1  it would be quite unusual to see red tulips, but not red roses
2  it is almost impossible to find blue roses
3  i like most red flowers, but roses are my favorite.
4  could you buy me some red roses?
5  john loves the color red. roses are mary's favorite flowers.","['python', 'pandas', 'scikit-learn', 'nlp']",45580161,"tfidfvectorizer allows for a custom preprocessor.  you can use this to make any needed adjustments.
for example, to remove all occurrences of consecutive ""red"" + ""roses"" tokens from your example corpus (case-insensitive), use:
import re
from sklearn.feature_extraction import text

cases = [""i like red roses as much as i like blue tulips."",
         ""it would be quite unusual to see red tulips, but not red roses"",
         ""it is almost impossible to find blue roses"",
         ""i like most red flowers, but roses are my favorite."",
         ""could you buy me some red roses?"",
         ""john loves the color red. roses are mary's favorite flowers.""]

# remove_stop_phrases() is our custom preprocessing function.
def remove_stop_phrases(doc):
    # note: this regex considers ""... red. roses..."" as fair game for removal.
    #       if that's not what you want, just use [""red roses""] instead.
    stop_phrases= [""red(\s?\\.?\s?)roses""]
    for phrase in stop_phrases:
        doc = re.sub(phrase, """", doc, flags=re.ignorecase)
    return doc

sw = text.english_stop_words
mod_vectorizer = text.tfidfvectorizer(
    ngram_range=(2,3),
    stop_words=sw,
    norm='l2',
    min_df=1,
    preprocessor=remove_stop_phrases  # define our custom preprocessor
)

dtm = mod_vectorizer.fit_transform(cases).toarray()
vocab = np.array(mod_vectorizer.get_feature_names_out())

now vocab has all red roses references removed.
print(sorted(vocab))

['could buy',
 'it impossible',
 'it impossible blue',
 'it quite',
 'it quite unusual',
 'john loves',
 'john loves color',
 'mary favorite',
 'mary favorite flowers',
 'blue roses',
 'blue tulips',
 'color mary',
 'color mary favorite',
 'favorite flowers',
 'flowers roses',
 'flowers roses favorite',
 'impossible blue',
 'impossible blue roses',
 'like blue',
 'like blue tulips',
 'like like',
 'like like blue',
 'like red',
 'like red flowers',
 'loves color',
 'loves color mary',
 'quite unusual',
 'quite unusual red',
 'red flowers',
 'red flowers roses',
 'red tulips',
 'roses favorite',
 'unusual red',
 'unusual red tulips']

update (per comment thread):
to pass in desired stop phrases along with custom stop words to a wrapper function, use:
desired_stop_phrases = [""red(\s?\\.?\s?)roses""]
desired_stop_words = ['could', 'buy']

def wrapper(stop_words, stop_phrases):

    def remove_stop_phrases(doc):
        for phrase in stop_phrases:
            doc = re.sub(phrase, """", doc, flags=re.ignorecase)
        return doc
    
    sw = text.english_stop_words.union(stop_words)
    mod_vectorizer = text.tfidfvectorizer(
        ngram_range=(2,3),
        stop_words=sw,
        norm='l2',
        min_df=1,
        preprocessor=remove_stop_phrases
    )
    
    dtm = mod_vectorizer.fit_transform(cases).toarray()
    vocab = np.array(mod_vectorizer.get_feature_names_out())
    
    return vocab

wrapper(desired_stop_words, desired_stop_phrases)",https://stackoverflow.com/questions/45426215,python,31-07-2017 22:25,4035.0,2.0,4.0,True,07-02-2024 14:03,07-08-2017 01:52
68155102,how to implement text metadata to spacy output?,"i am trying to build a corpus and parse it with spacy. corpus consists of over 2000 individual text files each has specific document meta such as filename, gender, nationally, etc.stored in each row in the data frame.
what i did so far is to create a excel file consisting of metadata and text_field. text_field is where the actual texts are stored. then i imported it as pandas, parsed text_field with spacy via following codes;
import spacy
import pandas as pd
nlp = spacy.load('en_core_web_sm')
df = pd.read_excel('c:/users/desktop/data.xlsx')
df['docs'] = list(nlp.pipe(df.text_field))

however, i would like to iterate over all docs stored in data frame and extract outputs with metadata provided in data frame as well.
for instance, common spacy output is like this;
doc = nlp('this is a test sentence')
for token in doc:
print(token.lemma_, token.text, token.pos_)

lemma / text / pos_
this   this   det
be     is     aux
a      a      det
test   test   noun
sentence   sentence   noun

expected out is like this;
 file(text/doc metadata) /  lemma / text / pos_
    text1                   this    this   det
    text1                   be      is     aux
    text1                   a       a      det
    text1                   test    test   noun
    text1                   sentence sentence noun
    text2                   this     this     det
    text2                   be       is       aux
    text2                   another  another  det
    text2                   sentence sentence noun


when i apply the df['docs'] = list(nlp.pipe(df.text_field)), docs column only consists of text, not doc objects.
how should i proceed to get the expected output? this is possible in r with quanteda package, creating corpus and tokenizing etc, but is there a way to do the same on python with spacy?","['nlp', 'spacy']",68156090,"following the tutorial here i managed to create a corpus with metadata integrated. for those who might need;
import pandas as pd
import spacy
import textacy
nlp = spacy.load('en_core_web_sm')
df = pd.read_excel('e:/test.xlsx')
native_language = (df
               .sort_values(['native_language', 
                             'gender'])
               .groupby('native_language')['text_field']
               .agg(lambda col: '\n'.join(col)))
docs = list(native_language.apply(lambda x: nlp(x)))
corpus = textacy.corpus.corpus(nlp, data=docs)

in the original answer, corpus = textacy.corpus.corpus(nlp, **docs**=docs) was provided however, textacy now requires data rather docs, therefore the correct code is corpus = textacy.corpus.corpus(nlp, **data**=docs)",https://stackoverflow.com/questions/68155102,nlp,27-06-2021 20:41,404.0,-1.0,1.0,True,30-03-2022 21:00,30-03-2022 21:00
67447256,distribution of topics over time with lda,"my goal is to identify topics of tweets and visualize how the distribution of topics changed over time. as far as i know, the best way to do it is with the stm package but i have some problems with it. so, my only option is to do a simple lda.
based on the topic shares for each of the tweets, i aggregated the shares of topics per year and compared each topic share versus the total of each year (the same way it is done here  the final visualization looks similar to this:
topics over time
my question is f it is possible to visualize topics over time with lda what is the point of doing it in stm? are there any important differences?","['r', 'lda', 'topic-modeling', 'mallet']",67463623,"post hoc analysis is a fine way to measure topic prevalence over time. lda doesn't explicitly learn parameters that represent the relationship between years and topics, but as you've found, that doesn't mean that there isn't a relationship.
lda often doesn't work well for short documents like tweets. you might also try k-means.
stm is helpful if you want to make an argument about the relationship in the way that you would with any other regression model. the advantage would be that you might get topics that are more aligned with your covariates, but it's usually not necessary.
visually, i really don't like streamplots for topic over time results. for example, because of the spike in art in 2000, you can't tell whether any of the other topics is changing in that year because they're all ""shoved aside"" from above. giving each topic its own area chart makes it much easier to see individual trends.",https://stackoverflow.com/questions/67447256,r,08-05-2021 11:54,1352.0,1.0,1.0,True,10-05-2021 01:09,08-05-2021 17:06
68058647,initialize huggingface bert with random weights,"how is it possible to initialize bert with random weights? i want to compare the performance of multilingual vs monolingual vs randomly initialized bert in a masked language modeling task. while in the former cases it is very straightforward:
from transformers import berttokenizer, bertformaskedlm

tokenizer_multi = berttokenizer.from_pretrained('bert-base-multilingual-cased')
model_multi = bertformaskedlm.from_pretrained('bert-base-multilingual-cased')
model_multi.eval()

tokenizer_mono = berttokenizer.from_pretrained('bert-base-cased')
model_mono = bertformaskedlm.from_pretrained('bert-base-cased')
model_mono.eval()

i don't know how to load random weights.
thanks in advance!","['bert-language-model', 'huggingface-transformers']",76215841,"you can initialize a random bert model using the hugginface capabilites (from the documentation 
from transformers import bertconfig, bertmodel

# initializing a bert bert-base-uncased style configuration
configuration = bertconfig()

# initializing a model (with random weights) from the bert-base-uncased style configuration
model = bertmodel(configuration)

# accessing the model configuration
configuration = model.config",https://stackoverflow.com/questions/68058647,bert-language-model,20-06-2021 17:57,4668.0,6.0,2.0,True,10-05-2023 07:18,23-10-2021 07:45
72801555,"typeerror: expected `trainable` argument to be a boolean, but got: bert","i got this error when implementing my model. i think the erros come from the bert model which i have imported.
def create_text_encoder(
    num_projection_layers, projection_dims, dropout_rate, trainable=false
):
    # load the bert preprocessing module.
    preprocess = hub.keraslayer(
        ""
        name=""text_preprocessing"",
    )
    # load the pre-trained bert model to be used as the base encoder.
    bert = hub.keraslayer(
        ""
        ""bert"",
    )
    # set the trainability of the base encoder.
    bert.trainable = trainable
    # receive the text as inputs.
    inputs = layers.input(shape=(), dtype=tf.string, name=""text_input"")
    # preprocess the text.
    bert_inputs = preprocess(inputs)
    # generate embeddings for the preprocessed text using the bert model.
    embeddings = bert(bert_inputs)[""pooled_output""]
    # project the embeddings produced by the model.
    outputs = project_embeddings(
        embeddings, num_projection_layers, projection_dims, dropout_rate
    )
    # create the text encoder model.
    return keras.model(inputs, outputs, name=""text_encoder"")

the error is showing in below code but i think problem is in above part.
num_epochs = 5  # in practice, train for at least 30 epochs
batch_size = 256

vision_encoder = create_vision_encoder(
    num_projection_layers=1, projection_dims=256, dropout_rate=0.1
)
text_encoder = create_text_encoder(
    num_projection_layers=1, projection_dims=256, dropout_rate=0.1
)
dual_encoder = dualencoder(text_encoder, vision_encoder, temperature=0.05)
dual_encoder.compile(
    optimizer=tfa.optimizers.adamw(learning_rate=0.001, weight_decay=0.001)
)

thanks.","['machine-learning', 'deep-learning', 'nlp', 'data-science', 'bert-language-model']",72966418,"i saw this question again after facing this issue in same code, now i am writing an answer as i have solved it.
there is a keyword name missing in the above code
 bert = hub.keraslayer(
    ""
    ""bert"",
)

i changed it to
bert = hub.keraslayer(
    ""
    name = ""bert"",
)

by just putting name = ""bert"" and now it works.",https://stackoverflow.com/questions/72801555,machine-learning,29-06-2022 12:37,1834.0,1.0,1.0,True,13-07-2022 12:36,29-06-2022 16:08
76031163,how to inject a link to chatgpt left menu bar using chrome plugin/extension,"i am trying to inject a link to chatgpt left menu bar using chrome extension similar to aiprm for chatgpt chrome extension is inserting additional menu items.
enter image description here
i am using the following but not working.
menifest.json:
{
  ""name"": ""auto snippet ai"",
  ""version"": ""1.0"",
  ""description"": ""inject a url called ' to the left navigation bar of '
  ""manifest_version"": 3,
  ""permissions"": [
    ""activetab"",
    ""tabs""
  ],
  ""icons"": {
    ""16"": ""icon.png"",
    ""32"": ""icon.png"",
    ""48"": ""icon.png"",
    ""128"": ""icon.png""
  },
  ""content_scripts"": [
    {
      ""js"": [
        ""content.js""
      ],
      ""matches"": [
        ""
      ]
    }
  ]
}

content.js:
document.addeventlistener('domcontentloaded', function () {
    // find the nav div
    const nav = document.queryselector(""#__next > div.overflow-hidden.w-full.h-full.relative.flex > div.dark.hidden.bg-gray-900.md\\:flex.md\\:w-\\[260px\\].md\\:flex-col > div > div > nav"");

    // check if the nav div exists
    if (nav) {
        // create a new link for the auto snippet ai website
        const a = document.createelement('a');
        a.classlist.add('nav-item');
        a.href = '
        a.target = '_blank';
        a.innertext = 'auto snippet ai';

        // insert the link into the nav div
        nav.insertbefore(a, nav.lastchild);
    }
}); 

so, i want to inject a link ""auto snippet ai"" in left menu bar similar to aiprm is injecting a link ""aiprm for seo powered"" when installed. any suggestion and help will be highly appreciable.","['javascript', 'html', 'openai-api']",76032760,"looks like domcontentloaded  event isn't firing.

or you can add ""run_at"" : ""document_end"" in the manifest.json
...
  ""content_scripts"": [
    {
      ""js"": [""content.js""],
      ""matches"": [""
      ""run_at"": ""document_end""
    }
  ]
...

so the content.js would look like this without the addeventlistener
const nav = document.queryselector(
    ""#__next > div.overflow-hidden.w-full.h-full.relative.flex > div.dark.hidden.bg-gray-900.md\\:flex.md\\:w-\\[260px\\].md\\:flex-col > div > div > nav""
  );

  // check if the nav div exists
  if (nav) {
    // create a new link for the auto snippet ai website
    const a = document.createelement(""a"");
    a.classlist.add(""nav-item"");
    a.href = ""
    a.target = ""_blank"";
    a.innertext = ""auto snippet ai"";

    // insert the link into the nav div
    nav.insertbefore(a, nav.lastchild);
  }",https://stackoverflow.com/questions/76031163,javascript,17-04-2023 01:31,186.0,2.0,1.0,True,20-04-2023 13:33,20-04-2023 13:33
71374635,"wav2vec2forctc were not initialized from the mode, pass &#39;sampling_rate&#39; argument","my code kind of working (it is listening and capturing my voice) but is warning me!! i am keeping having this issue with the code as:
1)
userwarning: positional arguments and argument ""destination"" are deprecated. nn.module.state_dict will not accept them in the future. refer to  for details.
warnings.warn(
some weights of wav2vec2forctc were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']
you should probably train this model on a down-stream task to be able to use it for predictions and inference.

listening now..
2)
it is strongly recommended to pass the ``sampling_rate`` argument to this function. failing to do so can result in silent errors that might be hard to debug.


import torch
import speech_recognition as sr
import io
from pydub import audiosegment
from transformers import wav2vec2forctc, wav2vec2processor

tokenizer = wav2vec2processor.from_pretrained(""facebook/wav2vec2-base-960h"")
model = wav2vec2forctc.from_pretrained(""facebook/wav2vec2-base-960h"")
recognizer = sr.recognizer()

while true:
    audio = recognizer.listen(source)
    data = io.bytesio(audio.get_wav_data())
    clip = audiosegment.from_file(data)
    tensor = torch.floattensor(clip.get_array_of_samples())

    inputs = tokenizer(tensor, sample_rate=16000, return_tensors=""pt"", padding=""longest"").input_values
    logits = model(inputs).logits
    tokens = torch.argmax(logits, dim=-1)
    text = tokenizer.batch_decode(tokens)

    print(str(text).lower())","['python', 'pytorch', 'torch', 'huggingface-transformers']",72696177,"import soundfile as sf
import torch       
ds, samplerate = sf.read(audio_file)
input_values = speech_tokenizer(ds  , return_tensors=""pt"", sampling_rate = samplerate, padding=""longest""  )

pass sampling_rate to speechtokenizer to solve sampling_rate warning
and make sure your ""sampling_rate"": 16000",https://stackoverflow.com/questions/71374635,python,06-03-2022 22:16,3756.0,0.0,1.0,True,21-06-2022 06:25,07-03-2022 06:19
74069129,openai response text extraction,"i'm trying to fix the response that open ai gpt 3 davinci-text-002 is giving me. in the console i am getting the following response text:
{
  ""id"": ""cmpl-61dshxu43ecbrqir187yilz9mdhsj"",
  ""object"": ""text_completion"",
  ""created"": 1665749707,
  ""model"": ""text-davinci-002"",
  ""choices"": [{
    ""text"": ""?\n\nthere is no simple answer to these questions. each person's individual experiences and perspectives will shape their understanding of who they are and what life is. in general, however, people often think of themselves as unique individuals with specific talents, interests, and goals. they may also think of life as a journey full of challenges and opportunities for growth."",
    ""index"": 0,
    ""logprobs"": null,
    ""finish_reason"": ""stop""
  }],
  ""usage"": {
    ""prompt_tokens"": 7,
    ""completion_tokens"": 71,
    ""total_tokens"": 78
  }
}

the text seems to start at \n\n and end at "",""index"". what would be the best way to isolate this text to use as an output? here is my current code:


let open_ai_response;

openai_test();

async function openai_test() {
  var prompt_text = ""who am i?""
  var prompt_text2 = ""what is life""
  var url = ""

  var xhr = new xml
  xhr.open(""post"", url);
  xhr.setrequestheader(""content-type"", ""application/json"");
  xhr.setrequestheader(""authorization"", ""bearer sk-00x0x0x0x0x0x0x0x0x0x0x0x0x"");

  xhr.onreadystatechange = function() {
    if (xhr.readystate === 4) {
      console.log(xhr.status);
      console.log(xhr.responsetext);
      var open_ai_response = xhr.responsetext;
      console.log(open_ai_response);
      var edit = open_ai_response[0].touppercase() + open_ai_response.slice(1).tolowercase();
      console.log(edit)
    }
  };

  var data = `{
    ""prompt"": ""${prompt_text + prompt_text2}"",
    ""temperature"": 0.7,
    ""max_tokens"": 256,
    ""top_p"": 1,
    ""frequency_penalty"": 0.75,
    ""presence_penalty"": 0
  }`;

  xhr.send(data);
}","['javascript', 'html', 'openai-api']",74069174,"the response you're receiving is json formatted. you need to parse it in the onreadystatechange handler using json.parse(xhr.responsetext), then you can retrieve the text by accessing the choices[0].text property.
xhr.onreadystatechange = function() {
  if (xhr.readystate === 4) {
    var response = json.parse(xhr.responsetext);
    let text = response.choices[0].text
    console.log(text);    
  }
};

note that this only reads the text from the first element in the choices array. if you want to handle all content in that array, then you can amend the above logic to loop through them instead.",https://stackoverflow.com/questions/74069129,javascript,14-10-2022 12:20,3876.0,2.0,1.0,True,14-10-2022 12:25,14-10-2022 12:25
74798182,subword vector in fasttext?,"i can't figure out what a subword input vector is. i read in the newspaper that the subword is hashed, the subword is the hash code, hash code is a number, not a vector
ex: input vector of word eating is [0,0,0,1,0,0,0,0,0]
so what is the input vector of subwords ""eat"", ""ati"", ""ing"",...?
link paper: 
enter image description here
the subword is the hash code, hash code is a number, not a vector","['nlp', 'word-embedding', 'fasttext']",74803074,"the fasttext subwords are, as you've suggested, fragments of the full word. for the purposes of subword creation, fasttext will also prepend/append special start-of-word and end-of-word characters. (if i recall correctly, it uses < & >.)
so, for the full word token 'eating', it is considered as '<eating>'.
all the 3-character subwords would be '<ea', 'eat', 'ati', 'tin', 'ing', 'ng>'.
all the 4-character subwords would be '<eat', 'atin', 'ting', 'ing>'.
all the 5-character subwords would be '<eati', 'ating', 'ting>'.
i see you've written out a ""one-hot"" representation of the full word 'eating' ï¿½ï¿½ï¿½ [0,0,0,1,0,0,0,0,0] ï¿½ï¿½ï¿½ as if 'eating' is the 4th word iocabulary. while diagrams & certain ways of thnking about the underlying model may consider such a one-hot vector, it's useful to realize that in actual code implementations, such a sparse one-hot vector for words is never actually created.
instead, it's just represented as a single number ï¿½ï¿½ï¿½ the index to the non-zero number. that's used as a lookup into an array of vectors of the configured 'dense' size, returning one input word-vector of that size for the word.
for example, imagine you have a model with a 1-million word known vocabulary, which offers 100-dimensional 'dense embedding' word-vectors. the word 'eating' is the 543,210th word.
that model will have an array of input-vectors that's has one million slots, and each slot has a 100-dimensional vector in it. we could call it word_vectors_in. the word 'eating''s vector will be at word_vectors_in[543209] (beccause the 1st vector is at word_vectors_in[0]).
no point during the creation/training/use of this model will an actual 1-million-long one-hot vector for 'eating' be created. most often, it'll just be referred-to inside the code as the word-index 543209. the model will have a helper lookup dictionary/hashmap, let's call it word_index that lets code find the right slot for a word. so word_index['eating'] will be 543209.
ok, now to your actual question, about the subwords. i've detailed how the the single vectors per one known full word are stored, above, in order to contrast it with the different way subwords are handled.
subwords are also stored in a big array of vectors, but that array is treated as a collision-oblivious hashtable. that is, by design, many subwords can and do all reuse the same slot.
let's call that big array of subword vectors subword_vector_in. let's also make it 1 million slots long, where each slot has a 100-dimensional vector.
but now, there is no dictionary that remembers which subwords are in which slots - for example, remembering that subword '<eat' is in arbitrary slot 78789.
instead, the string '<eat' is hashed to a number, that number is restricted to the possible indexes into the subwords, and the vector at that index, let's say it's 12344, is used for the subword.
and then when some other subword comes along, maybe '<dri', it might hash to the exact-same 12344 slot. and that same vector then gets adjusted for that other subword (during training), or returned for both those subwords (and possibly many others) during later fasttext-vector synthesis from the finali model.
notably, now even if there are far more than 1-million unique subwords, they can all be represented inside that single 1-million slot array, albeit with collisions/interference.
in practice, the collisions are tolerable because many collisions from very-rare subwords essentially just fuzz slots with lots of random noise that mostly cancels out. for the most-common subwords, that tend to carry any unique meaning because of the way word-roots/prefixes/suffixes hint at word meaning in english & similar langauges, those very-common examples overpower the other noise, and ensure that slot, for at least one or more of its most-common subwords, carries at least some hint of the subword's implied meaning(s).
so when fasttext assembles its final word-vector, by adding:
word_vector_in[word_index['eating']]  # learned known-word vector
+ subword_vector_in[slot_hash('<ea')]  # 1st 3-char subword
+ subword_vector_in[slot_hash('eat')]
+ subword_vector_in[slot_hash('ati')]
... # other 3-char subwords
... # every 4-char subword
... # other 5-char subwords
+ subword_vector_in[slot_hash('ting>')]  # last 5-char subword

ï¿½ï¿½ï¿½it gets something that's dominated by the (likely stronger-in-magnitude) known full-word vectoh some useful hints of meaning also contributed by the (probably lower-magnitude) many noisy subword vectors.
and then if we were to imagine that some other word that's not part of the known 1-million word vocabulary comes along, say 'eatery', it has nothing from word_vector_in for the full word, but it can still do:
subword_vector_in[slot_hash('<ea')]  # 1st 3-char subword
+ subword_vector_in[slot_hash('eat')]  
+ subword_vector_in[slot_hash('ate')]
... # other 3-char subwords
... # every 4-char subword
... # other 5-char subwords
+ subword_vector_in[slot_hash('tery>')]  # last 5-char subword

because at least a few of those subwords likely include some meaningful hints of the meaning of the word 'eatery' ï¿½ï¿½ï¿½ especially meanings around 'eat' or even the venue/vendor aspects of the suffix -tery, this synthesized guess for an out-of-vocabulary (oov) word will be better than a random vector, &ften better than ignoring the word entirely in whatever upper-level process is using the fasttext vectors.",https://stackoverflow.com/questions/74798182,nlp,14-12-2022 12:18,499.0,-1.0,1.0,True,14-12-2022 18:59,14-12-2022 12:21
78284637,attributeerror: module &#39;torch&#39; has no attribute &#39;version&#39;,"os: ubuntu 18.04 lts
cuda: 11.3
gpu: nvidia p5000 quadro
ide: jupyter notebook
environment: virtualenv (venv)

code:
# importing the required libraries
import torch

from transformers import automodelforcausallm, autotokenizer, bitsandbytesconfig

# defining the name of the falcon model
model_name = ""ybelkada/falcon-7b-sharded-bf16""

# configuring the bitsandbytes quantization
bnb_config = bitsandbytesconfig(
load_in_4bit=true,
bnb_4bit_quant_type=""nf4"",
bnb_4bit_compute_dtype=torch.float16,
)

# loading the falcon model with quantization configuration
model = automodelforcausallm.from_pretrained(
model_name,
quantization_config=bnb_config,
trust_remote_code=true
)

# disabling cache usage in the model configuration
model.config.use_cache = false

error:
---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
file ~/fyp_chatbot/test02/src/myenv-test002-02/lib/python3.9/site-packages/torch/cuda/__init__.py:242, in _lazy_init()
    241 try:
--> 242     queued_call()
    243 except exception as e:

file ~/fyp_chatbot/test02/src/myenv-test002-02/lib/python3.9/site-packages/torch/cuda/__init__.py:122, in _check_capability()
    116 old_gpu_warn = """"""
    117 found gpu%d %s which is of cuda capability %d.%d.
    118 pytorch no longer supports this gpu because it is too old.
    119 the minimum cuda capability supported by this library is %d.%d.
    120 """"""
--> 122 if torch.version.cuda is not none:  # on rocm we don't want this check
    123     cuda_version = torch._c._cuda_getcompiledversion()

attributeerror: module 'torch' has no attribute 'version'

the above exception was the direct cause of the following exception:

deferredcudacallerror                     traceback (most recent call last)
cell in[10], line 17
     10 bnb_config = bitsandbytesconfig(
     11 load_in_4bit=true,
     12 bnb_4bit_quant_type=""nf4"",
     13 bnb_4bit_compute_dtype=torch.float16,
     14 )
     16 # loading the falcon model with quantization configuration
---> 17 model = automodelforcausallm.from_pretrained(
     18 model_name,
     19 quantization_config=bnb_config,
     20 trust_remote_code=true
     21 )
     23 # disabling cache usage in the model configuration
     24 model.config.use_cache = false

file ~/fyp_chatbot/test02/src/myenv-test002-02/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:563, in _baseautomodelclass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    561 elif type(config) in cls._model_mapping.keys():
    562     model_class = _get_model_class(config, cls._model_mapping)
--> 563     return model_class.from_pretrained(
    564         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    565     )
    566 raise valueerror(
    567     f""unrecognized configuration class {config.__class__} for this kind of automodel: {cls.__name__}.\n""
    568     f""model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.""
    569 )

file ~/fyp_chatbot/test02/src/myenv-test002-02/lib/python3.9/site-packages/transformers/modeling_utils.py:3053, in pretrainedmodel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)
   3049 hf_quantizer.validate_environment(
   3050     torch_dtype=torch_dtype, from_tf=from_tf, from_flax=from_flax, device_map=device_map
   3051 )
   3052 torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)
-> 3053 device_map = hf_quantizer.update_device_map(device_map)
   3055 # force-set to `true` for more mem efficiency
   3056 if low_cpu_mem_usage is none:

file ~/fyp_chatbot/test02/src/myenv-test002-02/lib/python3.9/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:246, in bnb4bithfquantizer.update_device_map(self, device_map)
    244 def update_device_map(self, device_map):
    245     if device_map is none:
--> 246         device_map = {"""": torch.cuda.current_device()}
    247         logger.info(
    248             ""the device_map was not initialized. ""
    249             ""setting device_map to {'':torch.cuda.current_device()}. ""
    250             ""if you want to use the model for inference, please set device_map ='auto' ""
    251         )
    252     return device_map

file ~/fyp_chatbot/test02/src/myenv-test002-02/lib/python3.9/site-packages/torch/cuda/__init__.py:552, in current_device()
    550 def current_device() -> int:
    551     r""""""returns the index of a currently selected device.""""""
--> 552     _lazy_init()
    553     return torch._c._cuda_getdevice()

file ~/fyp_chatbot/test02/src/myenv-test002-02/lib/python3.9/site-packages/torch/cuda/__init__.py:246, in _lazy_init()
    243         except exception as e:
    244             msg = (f""cuda call failed lazily at initialization with error: {str(e)}\n\n""
    245                    f""cuda call was originally invoked at:\n\n{orig_traceback}"")
--> 246             raise deferredcudacallerror(msg) from e
    247 finally:
    248     delattr(_tls, 'is_initializing')

deferredcudacallerror: cuda call failed lazily at initialization with error: module 'torch' has no attribute 'version'

environment packages:
accelerate==0.29.1 
bitsandbytes==0.43.0 
datasets==2.18.0 
einops==0.7.0 
fsspec==2023.10.0 
peft @ git+ torch==1.13.0 transformers==4.39.3 
trl==0.8.1 
wandb==0.16.6

i encountered the error after downgrading pytorch 2.2.2 to pytorch 1.13.0. i had to downgrade pytorch 2.2.2 because of the fact that i have cuda toolkit of version 11.3 which was not compatible with the later versions of pytorch. i downgraded pytorch to version 1.13.0 specifically because i am using ""transformers"" library from huggingface which requires pytorch version >= 1.13.0.
nvidia graphics cards details (nvidia-smi):
sat apr  6 22:40:45 2024       
+-----------------------------------------------------------------------------+
| nvidia-smi 470.182.03   driver version: 470.182.03   cuda version: 11.4     |
|-------------------------------+----------------------+----------------------+
| gpu  name        persistence-m| bus-id        disp.a | volatile uncorr. ecc |
| fan  temp  perf  pwr:usage/cap|         memory-usage | gpu-util  compute m. |
|                               |                      |               mig m. |
|===============================+======================+======================|
|   0  quadro p5000        off  | 00000000:01:00.0  on |                  off |
| 27%   44c    p8     6w / 180w |    295mib / 16275mib |      3%      default |
|                               |                      |                  n/a |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| processes:                                                                  |
|  gpu   gi   ci        pid   type   process name                  gpu memory |
|        id   id                                                   usage      |
|=============================================================================|
|    0   n/a  n/a       938      g   /usr/lib/xorg/xorg                103mib |
|    0   n/a  n/a      1150      g   /usr/bin/gnome-shell               37mib |
|    0   n/a  n/a      1986      g   /usr/lib/firefox/firefox          150mib |
+-----------------------------------------------------------------------------+

nvcc -v:
nvcc: nvidia (r) cuda compiler driver
copyright (c) 2005-2021 nvidia corporation
built on sun_mar_21_19:15:46_pdt_2021
cuda compilation tools, release 11.3, v11.3.58
build cuda_11.3.r11.3/compiler.29745058_0","['python', 'pytorch', 'huggingface-transformers']",78285527,"according to the information given in your post, you have two requirements:

the transformers package you are trying to use requires a pytorch version of >= 1.13.0

the maximum cuda version supported by your gpu is 11.4, so the cuda toolkit  installed via pytorch must be <= 11.4


that cuda version was decommissioned on the release of pytorch 1.13, see release notes. so a viable solution for you would be to see whether you can update your cuda driver to 11.6 or 11.7. with that version, you will be able to install pytorch 1.13 with the appropriate cuda toolkit.",https://stackoverflow.com/questions/78284637,python,06-04-2024 13:53,5990.0,0.0,2.0,True,28-07-2024 21:55,28-07-2024 21:55
79231978,why do layernorm layers in bert base have 768 (and not 512) weight and bias parameters?,"the following will print 768 weight and bias parameters for each layernorm layer.
from transformers import bertmodel
model = bertmodel.from_pretrained('bert-base-uncased')
for name, param in model.named_parameters():
    if 'layernorm' in name:
        print(f""layer: {name}, parameters: {param.numel()}"")

as per this video, mean and std values are computed for each token in the input. and each mean, std pair has its own learned weight and bias in layer normalization. since bert takes in a max of 512 tokens, i'd expect a total of 512 weight and bias parameters in layernorm layers.
so why is it 768? is the video incorrect? is normalization performed for each of the 768 dimensions across all tokens, meaning mean and std values are computed across a max of 512 values?","['pytorch', 'bert-language-model']",79238368,"after quite a bit of research and code review, i was able to isolate details of layer normalization (ln), an aspect of transformers that's confusing a lot of people.
tl;dr: the assumption i made in my original question that each mean, std pair has its own weight, bias pair is incorrect. in ln, mean and std stats are computed across embedding dimensions of each token, i.e., there are as many mean, std pairs as there are tokens. but the weight and bias values are learned per embedding dimension, i.e., tokens share the weight and bias values during ln. this means, in the case of bert base, there are a max of 512 mean and std values, and there are 768 weight and bias values.
for complete details, see my answer to this question.",https://stackoverflow.com/questions/79231978,pytorch,27-11-2024 21:48,266.0,0.0,2.0,True,29-11-2024 21:06,27-11-2024 21:54
76976251,openai chat completions api: how do i make a fine-tuned gpt-3.5 model only answer from the fine-tuned data?,"openai now allows us to fine-tune gpt-3.5 models. i have tested and fine-tuned the model with my own dataset but the problem is the fine-tuned model generates the answer randomly, not correct based on my custom dataset.
is there any way to make the model only answer from my own fine-tuned dataset?","['openai-api', 'chatgpt-api', 'fine-tuning']",76976428,"this is a completely wrong approach, as you've already figured out.
why doesn't the fine-tuned openai model answer a specific question with a specific answer (i.e., fact) from the fine-tuning dataset?
as stated in the official openai documentation:

some common use cases where fine-tuning can improve results:

setting the style, tone, format, or other qualitative aspects
improving reliability at producing a desired output
correcting failures to follow complex prompts
handling many edge cases in
specific ways
performing a new skill or task thatï¿½ï¿½ï¿½s hard to articulate
in a prompt


fine-tuning is not about answering a specific question with a specific answer (i.e., fact) from the fine-tuning dataset.
what's the correct approach, then?
you need to implement a vector similarity search, as stated in the official openai documentation:

when should i use fine-tuning vs embeddings with retrieval?
embeddings
with retrieval is best suited for cases when you need to have a large
database of documents with relevant context and information.
by default openaiï¿½ï¿½ï¿½s models are trained to be helpful generalist
assistants. fine-tuning can be used to make a model which is narrowly
focused, and exhibits specific ingrained behavior patterns. retrieval
strategies can be used to make new information available to a model by
providing it with relevant context before generating its response.
retrieval strategies are not an alternative to fine-tuning and can in
fact be complementary to it.

a term that you most likely stumbled upon to this point if you're into ai is rag (i.e., retrieval-augmented generation). read nvidia's rag explanation to better understand what rag is:

to understand the latest advance in generative ai, imagine a
courtroom.
judges hear and decide cases based on their general understanding of
the law. sometimes a case ï¿½ï¿½ï¿½ like a malpractice suit or a labor dispute
ï¿½ï¿½ï¿½ requires special expertise, so judges send court clerks to a law
library, looking for precedents and specific cases they can cite.
like a good judge, large language models (llms) can respond to a wide
variety of human queries. but to deliver authoritative answers that
cite sources, the model needs an assistant to do some research.
the court clerk of ai is a process called retrieval-augmented
generation, or rag for short.

wait, what does a rag have to do with vector similarity search?
rags use vector similarity search under the hood. take a look at the visual representation of a rag process below:"" rel=""nofollow noreferrer"">
image source: an introduction to rag and simple/ complex rag by chia jeng yang
information is extracted from data sources (a), slashed into chunks (b), transformed into vectors (c), and inserted into a vector database (d). when a user asks a question, its question is transformed into a vector (1). this vector is then compared with vectors that are inside the vector database (2). the most similar vectors (3) are passed to an llm (4), which then returns an answer to the user (5).
this is how rags work. they use a vector similarity search under the hood.
so, how do i achieve my goal?
you have at least the following three options if you want your llm to answer a specific question with a specific answer (i.e., fact):

custom solution (see my past stackoverflow answer).
using llamaindex rag or langchain rag (see my youtube tutorial with corresponsing code).
using the openai assistants api (see my youtube tutorial with corresponding code).",https://stackoverflow.com/questions/76976251,openai-api,25-08-2023 10:11,2427.0,2.0,1.0,True,12-06-2024 16:43,12-06-2024 16:43
73931787,sklearn_crfsuite.crf unicodeencodeerror,"python version: 3.6
os: windows

i am trying to train a chinese ner model with sklearn_crfsuite.crf with ner_dataset.
after i cleanup the dataset and fit the model, it shows the error message:
60loading training data to crfsuite:   0%|                                                                                                                                                                     | 0/700 [00:00<?, ?it/s]
traceback (most recent call last):
  file ""main_script.py"", line 22, in <module>
    crf_pipeline.model.fit(x_train, y_train, x_test, y_test)
  file ""c:\users\weber\pycharmprojects\demo-insurance-backend\venv\lib\site-packages\sklearn_crfsuite\estimator.py"", line 314, in fit
    trainer.append(xseq, yseq)
  file ""pycrfsuite\_pycrfsuite.pyx"", line 312, in pycrfsuite._pycrfsuite.basetrainer.append
  file ""stringsource"", line 48, in vector.from_py.__pyx_convert_vector_from_py_std_3a__3a_string
  file ""stringsource"", line 15, in string.from_py.__pyx_convert_string_from_py_std__in_string
unicodeencodeerror: 'ascii' codec can't encode characters in position 2-6: ordinal not in range(128)

the data format is in .txt seperated with \n, with originaltext storing the text data and entities storing the entities information.
below is the code i preprocess the dataset:
import ast
from opencc import opencc
import sklearn_crfsuite

from sklearn.model_selection import train_test_split
from tqdm import tqdm

tag_dictionary = {
    'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½': 'i-ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½',
    'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½': 's-ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½',
    'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½': 'e-ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½',
    'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½': 'b-ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½',
    'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½': 'd-ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½'
}

def check_entity(entities):
    return [
        entity
        for entity in entities
        if enti    tag = tag_dictionary[entity['label_type']]
        tag_list[entity['start_pos']] = f'{tag}-b'
        for i in range(entity['start_pos']+1, entity['end_pos']):
            tag_list[i] = f'{tag}-i'
    return tag_list

def data_coverter(data):
    cc = opencc('s2t')  # ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
    data_dict = ast.literal_eval(cc.convert(data))  # txtï¿½ï¿½ï¿½dict
    return data_dict

def process_data(data):
    data_dict = data_coverter(data)
    text = data_dict['originaltext']
    entities = data_dict['entities']
    entities = check_entity(entities)
    tag_seq = build_tag_seq(text, entities)
    return text, tag_seq

def load_txt_data(stop=-1):
    data_x = list()  # ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(tokenï¿½ï¿½ï¿½ï¿½ï¿½ï¿½)
    data_y = list()  # ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½tokenï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½tagï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
    for path in ['subtask1_training_part1.txt']:
        with open(path, 'r', encoding='utf-8') as f:
            for i, line in tqdm(enumerate(f.readlines())):
                text = line.strip()
 )

                    data_x.append(temp_x)
                    data_y.append(temp_y)
                    if i == stop:
                        break
    return data_x, data_y

x, y = load_txt_data()

model = sklearn_crfsuite.crf(
    algorithm='l2sgd',
    c2=1.0,
    max_iterations=1000,
    all_possible_transitions=true,
    all_possible_states=true,
    verbose=true
)

model.fit(x, y)

below is pkgs list i used:
pip install opencc sklearn sklearn_crfsuite 

does anyone get the similiar error message before and solved it? please, any help would be appreciated.","['python', 'scikit-learn', 'named-entity-recognition']",73932273,"i figure out that i cannot use the chinese symbols in ner tag from reference.
after changing the tag_dictionary with int in the value, it works.",https://stackoverflow.com/questions/73931787,python,03-10-2022 06:26,163.0,1.0,1.0,True,03-10-2022 07:24,03-10-2022 06:35
75224828,how can i improve the quality of documentai document-ocr processor result,i have this image (first image) which i want to process using the document-ocr processor. the output i got looks something like the second image printed onto the python console. this output has been badly process in inclusion with most of my files. how can i make document-ocr understand and yield a perfect result.,"['google-cloud-platform', 'nlp', 'cloud-document-ai']",75226162,"to set expectations, no machine learning model can give ""perfect results"" consistently.
results will greatly depend on the quality of the input files. in this case, the document is a scan of a handwritten file and handwriting can vary greatly from document to document.
in this particular example, some of these words could be difficult for humans to read, so the performance for document ai could be inconsistent.
in general for improving quality of ocr output, higher quality for the source material results in higher accuracy of ocr detected text. so scanning at a higher dpi can improve results.
if the pdf file has embedded text already, then you can also use the native pdf parsing feature in the pretrained-ocr-v1.2-2022-11-10 processor version. this repository has some sample code for how to use it.",https://stackoverflow.com/questions/75224828,google-cloud-platform,24-01-2023 17:03,386.0,2.0,1.0,True,25-01-2023 13:36,25-01-2023 13:36
77524569,importerror: dll load failed while importing numpy_ops: the specified module could not be found. when importing spacy in python,"i was trying to import spacy because i wanted to remove stop words.
when i tried running the import line, it gave me the error mentioned in the title.
i had installed spacy using
python -m pip install spacy

because pip install spacy gave me an error saying:
fatal error in launcher: unable to create process using '""c:\users\nikun\appdata\local\programs\python\python39\python.exe""  ""c:\users\nikun\appdata\local\programs\python\python39\scripts\pip.exe"" install spacy': the system cannot find the file specified.

how do i fix this so that it imports spacy without error?
i would appreciate any help and am new to coding so please forgive for any silly errors i am making.","['python', 'spacy', 'importerror']",77526554,"you just need to run the following line on the command prompt:

python -m pip install msvc-runtime

then the error goes away",https://stackoverflow.com/questions/77524569,python,21-11-2023 16:49,434.0,0.0,1.0,True,21-11-2023 23:39,21-11-2023 17:02
74966513,extracting text data from files in different sub-directories raises &quot;valueerror: substring not found&quot;,"i was trying to extract text data from files in different sub-directories and put the extracted data into pandas dataframes.
an example of the text data is given below:

""examination: chest pa and lat indication: history: f with shortness of breath technique: chest pa and lateral comparison: findings: the cardiac mediastinal and hilar contours are normal. pulmonary vasculature is normal. lungs are clear. no pleural effusion or pneumothorax is present. multiple clips are again seen projecting over the left breast. remote leftsided rib fractures are also re demonstrated. impression: no acute cardiopulmonary abnormality.""

however, when attempting to execute the code given below, it produced the following error, how do i resolve this?
error
valueerror                                traceback (most recent call last)
<ipython-input-108-bbeeb452bdef> in <module>
     48         df = pd.dataframe(columns=keywords)
     49         # extract text
---> 50         result = extract_text_using_keywords(text, keywords)
     51         # append list of extracted text to the end of the pandas df
     52         df.loc[len(df)] = result

<ipython-input-108-bbeeb452bdef> in extract_text_using_keywords(clean_text, keyword_list)
     39             for prev_kw, current_kw in zip(keyword_list, keyword_list[1:]):
     40                 prev_kw_index = clean_text.index(prev_kw)
---> 41                 current_kw_index = clean_text.index(current_kw)
     42                 extracted_texts.append(clean_text[prev_kw_index + len(prev_kw) + 2:current_kw_index])
     43                 if current_kw == keyword_list[-1]:

valueerror: substring not found

code
out = []
result = {}

for filename in glob.iglob('/content/sample_data/**/*.txt', recursive = true):
    
    out.append(filename)

print('file names: ',out)

for file in out:
      
        with open(file) as f:
          data = f.read()
          
    
        import re
        text = re.sub(r""[-_()\n\""#//@;<>{}=~|?,]*"", """", data)
        text = re.sub(r'final report', '', text)
        text = re.sub(r'\s+', ' ', text)
        print(text)

        keywords = [""indication"", ""technique"", ""comparison"", ""findings"", ""impression""]

        # create function to extract text between each of the keywords
        # assumption
        def extract_text_using_keywords(clean_text, keyword_list):
            extracted_texts = []
            for prev_kw, current_kw in zip(keyword_list, keyword_list[1:]):
                prev_kw_index = clean_text.index(prev_kw)
                current_kw_index = clean_text.index(current_kw)
                extracted_texts.append(clean_text[prev_kw_index + len(prev_kw) + 2:current_kw_index])
                if current_kw == keyword_list[-1]:
                    extracted_texts.append(clean_text[current_kw_index + len(current_kw) + 2:len(clean_text)])
            return extracted_texts

        # create empty pandas df with keywords as column names
        df = pd.dataframe(columns=keywords)
        # extract text
        result = extract_text_using_keywords(text, keywords)
        # append list of extracted text to the end of the pandas df
        df.loc[len(df)] = result

        #print(df)

        with pd.option_context('display.max_colwidth', none): # for diplaying full columns
          display(df)","['python', 'machine-learning', 'nlp', 'data-science']",74966878,"the valueerror is raised by the function call index() in the line current_kw_index = clean_text.index(current_kw) because clean_text does not contain the current_kw that the code is attempting to find.
it is likely that in one of your files, the data and therefore the text that your inputting to result = extract_text_using_keywords(text, keywords) does not contain either ""indication"", ""technique"", ""comparison"", ""findings"", or ""impression"". so the easiest way to resolve this is to check which file is causing the issue and add the necessary keyword.
to make this debugging easier, you can update the extract_text_using_keywords() function to include a try except block to give you a more useful output for the valueerror. you can also update other parts of the code to deal with the subsequent issues that will follow as a result of being unable to find the keyword. a complete solution is as follows:
import glob
import pandas as pd
import re

# get & print all .txt file names with directory information
out = []
for filename in glob.iglob('content/sample_data/**/*.txt', recursive = true):
    out.append(filename)
print('file names: ', out)

# define keywords
keywords = [""indication"", ""technique"", ""comparison"", ""findings"", ""impression""]
# create empty pandas df with keywords as column names
df = pd.dataframe(columns=keywords)


# create function to extract text between each of the keywords
def extract_text_using_keywords(clean_text, keyword_list):
    extracted_texts = []
    for prev_kw, current_kw in zip(keyword_list, keyword_list[1:]):
        try:            
            prev_kw_index = clean_text.index(prev_kw)
        except valueerror:
            print(""keyword {} was not found in the text."".format(prev_kw))
        try:
            current_kw_index = clean_text.index(current_kw)
        except valueerror:
            print(""keyword {} was not found in the text."".format(current_kw))
        try:
            extracted_texts.append(clean_text[prev_kw_index + len(prev_kw) + 2:current_kw_index])
            if current_kw == keyword_list[-1]:
                extracted_texts.append(clean_text[current_kw_index + len(current_kw) + 2:len(clean_text)])
        except unboundlocalerror:
            print(""an index was not assigned for a particular keyword."")
    return extracted_texts


# iterate over all .txt files
for file in out:
    with open(file) as f:
      data = f.read()

    text = re.sub(r""[-_()\n\""#//@;<>{}=~|?,]*"", """", data)
    text = re.sub(r'final report', '', text)
    text = re.sub(r'\s+', ' ', text)
    # print(text)

    # extract text
    result = extract_text_using_keywords(text, keywords)

    # if all keywords and their results were found
    if len(result) == len(keywords):
        # append list of extracted text to the end of the pandas df
        df.loc[len(df)] = result
    else:
        print(""\nfailed to extract text for one or more keywords.\
        \nplease check that {} are all present in the following text:\n\n{}\n"".format(keywords, text))

# display results
print(df)
# with pd.option_context('display.max_colwidth', none): # for diplaying full columns
#     display(df)

produces the following error output when a keyword is not included (e.g. ""technique""):
keyword technique was not found in the text.
an index was not assigned for a particular keyword.
keyword technique was not found in the text.

failed to extract text for one or more keywords.
please check that ['examination', 'technique', 'comparison', 'findings', 'impression'] are all present in the following text:

 examination: chest pa and lat indication: f with new onset ascites eval for infection : chest pa and lateral comparison: none findings: there is no focal consolidation pleural effusion or pneumothorax bilateral nodular opacities that most likely represent nipple shadows the cardiomediastinal silhouette is normal clips project over the left lung potentially within the breast the imaged upper abdomen is unremarkable chronic deformity of the posterior left sixth and seventh ribs are noted impression: no acute cardiopulmonary process

empty dataframe
columns: [indication, technique, comparison, findings, impression]
index: []

and produces the desired output when all keywords are included:
file names:  ['content/sample_data\\my_data.txt', 'content/sample_data\\my_data2.txt']
                                     indication              technique comparison                                           findings                        impression
0  f with new onset ascites eval for infection   chest pa and lateral       none   there is no focal consolidation pleural effusi...  no acute cardiopulmonary process
1   chronic pain noted in lower erector spinae                palpate       none   upper iliocostalis thoracis triggers pain alon...                               nil",https://stackoverflow.com/questions/74966513,python,30-12-2022 23:17,128.0,1.0,1.0,True,03-01-2023 22:48,31-12-2022 13:58
76847246,how to save and load a peft/lora finetune (star-chat),"i am trying to further finetune starchat-beta, save my progress, load my progress, and continue training. but whatever i do, it doesn't come together. whenever i load my progress and continue training, my loss starts back from zero (3.xxx in my case).
i'll run you through my code and then the problem.
tokenizer = autotokenizer.from_pretrained(basepath)
model = automodelforcausallm.from_pretrained(
    ""/notebooks/starbaseplus""
    ...
)
# i get both the tokenizer and the foundation model from the starbaseplus repo (which i have locally). 

peftconfig = loraconfig(
    ""/notebooks/starchat-beta"" 
    base_model_name_or_path = ""/notebooks/starbaseplus"",
    ...
)
model = get_peft_model(model, peftconfig)
# all gucci so far, the model and the lora fine-tune are loaded from the starchat-beta repo (also local).

# important for later:
print_trainable_parameters(model)
# trainable params: 306 million || all params: 15 billion || trainable: 1.971%


trainer = trainer(
    model=model,
    ...
)
trainer.train()
# i train, loss drops. from 3.xx to 1.xx.

# now, either i follow the hugginface docks:
model.save_pretrained(""./huggingface_model"") 
# -> saves /notebooks/huggingface_model/adapter_model.bin 16mb.

# or an alternative i found on so:
trainer.save_model(""./torch_model"") 
# -> saves /notebooks/torch_model/pytorch_model.bin 60gb.

i have two alternatives saved to disk. lets restart and try either of these approaches
first the huggingface docs approach:
i now have three sets of weights.

the foundation model - starbase plus
the chat finetune - starchat-beta
the 16mb saved bin - adapter_model.bin

but i only have two opportunities to load weights.

automodelforcausallm.from_pretrained
either get_peft_model or peftmodel.from_pretrained

neither works. training restarts at a loss of 3.x.
second approach:
load the 60bg instead of the old starchat-beta repo model.
get_peft_model(""/notebooks/torch_model/pytorch_model.bin"", peftconfig)
also doesn't work. the print_trainable_parameters(model) drops to trainable: 0.02% and training restarts at a loss of 3.x","['python', 'pytorch', 'huggingface-transformers', 'torch']",77099970,the problem was the the save_pretrained and from_pretrained from hf were broken for a time. they fixed it with the newest versions.,https://stackoverflow.com/questions/76847246,python,06-08-2023 18:23,9227.0,1.0,2.0,True,18-09-2023 14:10,06-08-2023 18:38
79264911,openai assistants api: is the whole thread with all the past messages sent to the api every time i add a new message to the thread?,"while i was using the chat completions api, i learned that you need to include the trail of the questions from the user and answers from the openai api (including the system messages) when asking a new question if you want the chat completions api to be able to have the chat history included.
with the assistants api, you don't need to do that, and it remembers the chat history.
my question is, what happens to token consumption in the case of the assistants api? would all the past messages be included in the token consumption?","['openai-api', 'chatgpt-api', 'openai-assistants-api']",79265018,"token consumption in the assistants api can be very, very high if you use the same thread for a long time because the thread is storing message history and passing the whole thread to the api every time you ask a new question using the existing thread.
after some time, a single message you ask the assistants api can cost a lot, even if the message is short. see the past discussion:

/ ... /
the message contains around 1000 tokens, checked via

/ ... /
this code takes around 250,000 tokens to complete. the image shows
today's token usage for three requests.


what the developer didn't understand is that your recent message might contain 1,000 tokens, but you also need to keep in mind that hundreds of messages that were either asked by you or answered by the assistant in the past were also sent to the assistants api.
there is, however, a limit of 100,000 messages per thread. as stated in the official openai documentation:

the contents of the messages your users or applications create are
added as message objects to the thread. messages can contain both text
and files. there is a limit of 100,000 messages per thread and we
smartly truncate any context that does not fit into the model's
context window.",https://stackoverflow.com/questions/79264911,openai-api,09-12-2024 12:07,515.0,1.0,1.0,True,09-12-2024 12:50,09-12-2024 12:50
77993164,runtimeerror: cuda error: no kernel image is available for execution on the device for cuda 11.8 and torch 2.0.0,"i wanted to use meta-llama/llama-2-13b-chat-hf, but i am having this error:
runtimeerror: cuda error: no kernel image is available for execution on the device
cuda kernel errors might be asynchronously reported at some other api call, so the stacktrace below might be incorrect.

for debugging consider passing cuda_launch_blocking=1.

compile with `torch_use_cuda_dsa` to enable device-side assertions.

the output of nvidia-smi is:
| nvidia-smi 465.19.01    driver version: 465.19.01    cuda version: 11.3     |

nvcc:
nvcc: nvidia (r) cuda compiler driver
copyright (c) 2005-2022 nvidia corporation

built on wed_sep_21_10:33:58_pdt_2022

cuda compilation tools, release 11.8, v11.8.89

build cuda_11.8.r11.8/compiler.31833905_0

torch version:
torch==2.0.0+cu118
torchaudio==2.0.1+cu118
torchvision==0.15.1+cu118

transformers version:
transformers==4.37.2

i have a some 2080ti and a 710 and am using ubuntu 16.
i also got this in my output:
found gpu9 nvidia geforce gt 710 which is of cuda capability 3.5.
    pytorch no longer supports this gpu because it is too old.
    the minimum cuda capability supported by this library is 3.7.

i was downloading torch versions from here.
i tried building from the source as well but it gave me the same output.
the output from bitsandbytes:

++++++++++++++++++++++++++ other +++++++++++++++++++++++++++
compiled_with_cuda = true
compute_capabilities_per_gpu = ['7.5', '7.5', '7.5', '7.5', '7.5', '7.5', '7.5', '7.5', '7.5', '3.5']
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++ debug info end ++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

running a quick check that:
    + library is importable
    + cuda function is callable


warning: please be sure to sanitize sensible info from any such env vars!

success!
installation was successful!


i ran this code to test torch
import torch
import sys
print('a', sys.version)
print('b', torch.__version__)
print('c', torch.cuda.is_available())
print('d', torch.backends.cudnn.enabled)
device = torch.device('cuda')
print('e', torch.cuda.get_device_properties(device))
print('f', torch.tensor([1.0, 2.0]).cuda())

it gave me this output:
a 3.11.7 (main, dec 15 2023, 18:12:31) [gcc 11.2.0]
b 2.0.0+cu118
c true
d true
    userwarning: 
    found gpu9 nvidia geforce gt 710 which is of cuda capability 3.5.
    pytorch no longer supports this gpu because it is too old.
    the minimum cuda capability supported by this library is 3.7.
    
  warnings.warn(old_gpu_warn % (d, name, major, minor, min_arch // 10, min_arch % 10))
e _cudadeviceproperties(name='nvidia geforce rtx 2080 ti', major=7, minor=5, total_memory=11019mb, multi_processor_count=68)
f tensor([1., 2.], device='cuda:0')

what should i do to fix this?","['python', 'pytorch', 'cuda', 'huggingface-transformers', 'llama']",77998841,"as mentioned in the warning:
    userwarning: 
    found gpu9 nvidia geforce gt 710 which is of cuda capability 3.5.
    pytorch no longer supports this gpu because it is too old.
    the minimum cuda capability supported by this library is 3.7.


the main issue was that torch was trying to run code on the gt710 and due to it being not supported, the program crashed. to fix this i added an environment variable:
export cuda_visible_devices=0,1,2,3,4,5,6,7,8

so torch just ignores the gt710 now and it runs perfectly fine for me now.",https://stackoverflow.com/questions/77993164,python,14-02-2024 09:16,510.0,0.0,1.0,True,15-02-2024 06:51,15-02-2024 06:51
75568896,outputs from openai api is delayed and random,"when i type less than 3-5 characters, openai api replies with random output and delayed for like 5 seconds (based on how long openai output is). tried changing model and lowering max_tokens to 500.
#main.js

const userinput = document.queryselector('.user-input');
const inputtext = userinput.value;
      const data = {
        model: 'text-davinci-003',
        prompt: inputtext,
        temperature: 0.7,
        max_tokens: 2048,
        top_p: 1,
        frequency_penalty: 0,
        presence_penalty: 0
      };
  
      const options = {
        method: 'post',
        headers: {
          'content-type': 'application/json',
          'authorization': `bearer ${openaiapikey}`
        },
        body: json.stringify(data)
      };
  
      try {
        const response = await fetch(' options);
        const data = await response.json();
        const choices = data.choices;
        const text = choices[0].text.trim();
        addmessage(text, 'bot');
      } catch (error) {
        console.error(error);
      }

#index.html
<textarea class=""user-input"" id=""user-input""></textarea>

what i expecting","['javascript', 'openai-api']",75812326,"use ""gpt-3.5-turbo"" model",https://stackoverflow.com/questions/75568896,javascript,25-02-2023 22:42,338.0,0.0,2.0,True,20-12-2024 08:10,20-12-2024 08:10
78836024,"efficiently maintaining chat context with genai api (gpt, claude) without resending all prompts","i'm using the genai api (gpt, claude) to create a conversational ai that handles multi-turn dialogues. my goal is to maintain the context of the conversation without resending all previous prompts with each new request, as this approach quickly becomes expensive due to token usage.
what i have tried
currently, i append each new user message and the assistant's responses to a list and send this entire list with each new api call:
conversation_history.append({""role"": ""user"", ""content"": user_message})
response = openai.chatcompletion.create(model=""gpt-3.5-turbo"", messages=conversation_history)
conversation_history.append({""role"": ""assistant"", ""content"": response['choices'][0]['message']['content']})
......

my question
is there a way to maintain the context of a conversation with the api without needing to resend all previous prompts and responses in each request? ideally, i'm looking for a method to retain the conversational state on the server side or a more efficient way to manage the context.

using gpt or claude api.
the primary concern is minimizing token usage to reduce costs.","['python', 'artificial-intelligence', 'chat', 'openai-api']",78836280,"unfortunately, you are going to have to resend all the context each time. requests to openai conform to rest standards, meaning calls are stateless.",https://stackoverflow.com/questions/78836024,python,05-08-2024 18:54,757.0,4.0,1.0,True,05-08-2024 20:09,05-08-2024 20:01
72594048,how can i extract some contents in the cells of web-scraped csv file?,"i am struggling with dealing with a csv file that scraped one crowdfunding website.
my goal is successfully load all information as separate columns, but i found some information are mixed in a single column when i load it using 1) r, 2) stata, and 3) python.
since the real data is really dirty, let me suggest abbreviate version of current dataset.




id
pledge
creator




000001
13.7
{""urls"":{""web"":{""user"":"" ""name"":john"",""id"":709510333}


000002
26.4
{""urls"":{""web"":{""user"":"" ""name"":kellen"",""id"":703514812}


000003
7.6
{""urls"":{""web"":{""user"":"" ""name"":jach"",""id"":609542647}




my goal was extracting the ""name"" and ""id"" as separate columns, though they are all mixed with urls in the creator column.
is there any way that i can extract names (john, kellen, jach) and ids as separate columns?
i prefer r, but stata and python would also be helpful!
thank you so much for considering this.","['csv', 'web-scraping', 'nlp', 'opencsv']",72597436,"if you want to extract the name and id without any other values you can simply replace the code that is setting the creator column with
replace the creator with what ever variable that holds the dictionary
{""name"": creator[""name""], ""id"": creator[""id""]}


also if the json data is not formatted correctly (like missing a quote) you can try using regular expressions",https://stackoverflow.com/questions/72594048,csv,12-06-2022 16:34,63.0,-1.0,1.0,True,14-06-2022 22:44,14-06-2022 22:44
65197853,"i need to perform a stemming operation in python, without nltk . using pipeline methods","i have a list of words and a list of stem rules.
i need to stem the words that their suffixes are in the stem rules list.i got a hint from a friend that i can use pipeline methods
for example if i have :
stem = [ 'less', 'ship', 'ing', 'les', 'ly', 'es', 's' ]
text = [ 'friends', 'friendly', 'keeping', 'friendship' ]

i should get:
'friend', 'friend', 'keep', 'friend'","['pipeline', 'stemming']",75614557,"rules = {'ness': '',
'ational': 'ate',
'ing': '',
'sses': 'ss'}
def stemx(inp:str):
for x in rules:
if inp[len(inp) - len(x):] == x:
return inp[0:len(inp) - len(x)] + rules[x]
return inp
print(stemx('singfds'))",https://stackoverflow.com/questions/65197853,pipeline,08-12-2020 11:13,2133.0,0.0,2.0,True,23-03-2025 19:55,23-03-2025 19:55
72099620,how to solve missing words in nltk.corpus.words.words()?,"i have tried to remove non-english words from a text. problem many other words are absent from the nltk words corpus.
my code:
import pandas as pd
    
lst = ['i have equipped my house with a new [xxx] hp203x climatisation unit']
df = pd.dataframe(lst, columns=['sentences'])
    
import nltk 
nltk.download('words')
words = set(nltk.corpus.words.words())
    
df['sentences'] = df['sentences'].apply(lambda x: "" "".join(w for w in nltk.wordpunct_tokenize(x) if w.lower() in (words)))
df

input: i have equipped my house with a new [xxx] hp203x climatisation unit
result: i have my house with a new unit
should have been: i have equipped my house with a new climatisation unit
i can't figure out how to complete nltk.corpus.words.words() to avoid words like equipped, climatisation to be remouved from the sentences.","['nlp', 'nltk', 'tokenize', 'corpus']",72213312,"you can use
words.update(['climatisation', 'equipped'])

here, words is a set, that is why .extend(word_list) did not work.",https://stackoverflow.com/questions/72099620,nlp,03-05-2022 12:49,716.0,3.0,1.0,True,12-05-2022 09:43,12-05-2022 09:43
62776477,how to extract sentences with key phrases in spacy,"i have worked with spacy and so far, found very intuitative and robust in nlp.
i am trying to make out of text sentences search which is both ways word base as well as content type base search but so far, i would not find any solution with spacy.
i have the text like:

in computer science, artificial intelligence (ai), sometimes called
machine intelligence, is intelligence demonstrated by machines, unlike
the natural intelligence displayed by humans and animals. leading ai
textbooks define the field as the study of ""intelligent agents"": any
device that perceives its environment and takes actions that maximize
its chance of successfully achieving its goals.[1] colloquially, the
term ""artificial intelligence"" is often used to describe machines (or
computers) that mimic ""cognitive"" functions that humans associate with
the human mind, such as ""learning"" and ""problem solving"".[2]
as machines become increasingly capable, tasks considered to require
""intelligence"" are often removed from the definition of ai, a
phenomenon known as the ai effect.[3] a quip in tesler's theorem says
""ai is whatever hasn't been done yet.""[4] for instance, optical
character recognition is frequently excluded from things considered to
be ai,[5] having become a routine technology.[6] modern machine
capabilities generally classified as ai include successfully
understanding human speech,[7] competing at the highest level in
strategic game systems (such as chess and go),[8] autonomously
operating cars, intelligent routing in content delivery networks, and
military simulations[9].
artificial intelligence was founded as an academic discipline in 1955,
and in the years since has experienced several waves of
optimism,[10][11] followed by disappointment and the loss of funding
(known as an ""ai winter""),[12][13] followed by new approaches, success
and renewed funding.[11][14] for most of its history, ai research has
been divided into sub-fields that often fail to communicate with each
other.[15] these sub-fields are based on technical considerations,
such as particular goals (e.g. ""robotics"" or ""machine learning""),[16]
the use of particular tools (""logic"" or artificial neural networks),
or deep philosophical differences.[17][18][19] sub-fields have also
been based on social factors (particular institutions or the work of
particular researchers).[15]

now, i want to extract the sentences complete in multiple with multiple words or string matching. e.g., i want to search intelligent and machine learning. and it prints all complete sentences which contain this single or both given strings.
is there any way that importing model of spacy with spacy can sense the phrase match ..  like it finds all the intelligent and machine learning containing words and print that ? and also with other option, can it also finds as with search machine learning, also suggests deep learning, artificial intelligence, pattern recognition etc?
import spacy
nlp = spacy.load(""en_core_web_sm"")
from spacy.matcher import phrasematcher
phrase_matcher = phrasematcher(nlp.vocab)

phrases = ['machine learning', ''intelligent, 'human']

patterns = [nlp(text) for text in phrases]

phrase_matcher.add('ai', none, *patterns)

sentence = nlp (processed_article)

matched_phrases = phrase_matcher(sentence)

for match_id, start, end in matched_phrases:
    string_id = nlp.vocab.strings[match_id]  
    span = sentence[start:end]                   
    print(match_id, string_id, start, end, span.text)

i tried this which is not providing the complete sentence but only the word with matching id number.
in short,

i am trying to search with multiple words input and find complete sentences which contain either out of input single string or all
i am trying to use the trained model to also find suggested sentence out of input.","['python', 'nlp', 'spacy']",62780351,"part 1:

i want to search intelligent and machine learning. and it prints all complete sentences which contain this single or both given strings.

this is how you can find complete sentences that contain your keywords that you are looking for. keep in mind that sentence boundaries are determined statistically, and hence, and it would work fine if the incoming paragraphs are from news or wikipedia, but it wouldn't work as well if the data is coming from social media.
import spacy
from spacy.matcher import phrasematcher

text = """"""i like tomtom and i cannot lie. in computer science, artificial intelligence (ai), sometimes called machine intelligence, is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals.  leading ai textbooks define the field as the study of ""intelligent agents"": any device that perceives its  environment and takes actions that maximize its chance of successfully achieving its goals.[1] colloquially,  the term ""artificial intelligence"" is often used to describe machines (or computers) that mimic ""cognitive""  functions that humans associate with the human mind, such as ""learning"" and ""problem solving"".[2] """"""

nlp = spacy.load(""en_core_web_sm"")

phrase_matcher = phrasematcher(nlp.vocab)
phrases = ['machine learning', 'artificial intelligence']
patterns = [nlp(text) for text in phrases]
phrase_matcher.add('ai', none, *patterns)

doc = nlp(text)

for sent in doc.sents:
    for match_id, start, end in phrase_matcher(nlp(sent.text)):
        if nlp.vocab.strings[match_id] in [""ai""]:
            print(sent.text)

output
in computer science, artificial intelligence (ai), sometimes called machine intelligence, is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals.  
colloquially,  the term ""artificial intelligence"" is often used to describe machines (or computers)


part 2:

can it also finds as with search machine learning, also suggests deep learning, artificial intelligence, pattern recognition etc?

yes. that is very much possible, you would need to utilize a word2vec or sense2vec in order to do that.",https://stackoverflow.com/questions/62776477,python,07-07-2020 13:31,10343.0,4.0,2.0,True,23-10-2023 06:13,08-07-2020 15:57
76782909,how do i pass arguments to an async lambda function through api gateway?,"we have an aws lambda function that makes a call to the openai api to generate page_content for a page based on a transcript. once a response is received, the respective page element in a dynamodb table is updated.
import boto3
import json
import openai
    
    def lambda_handler(event, context):
       dynamodb = boto3.client('dynamodb')
       openai.api_key = ""xxxxx""

       d = json.loads(event['body'])
       pageid = d['page_id']
       transcript = d['transcript']
           
       conversation = [
          {""role"": ""system"", ""content"": ""assistant""},
          {""role"": ""user"", ""content"": transcript},
          {""role"": ""system"", ""content"":""provide a blog post for the content above.""}
       ]

       response = openai.chatcompletion.create(
          model=""gpt-3.5-turbo"",
          messages=conversation,
          temperature=1,
          max_tokens=2000,
          top_p=1,
          frequency_penalty=0,
          presence_penalty=0
       )
    
       page_content = response['choices'][0]['message']['content']
       state = ""complete""
    
       response = dynamodb.update_item(
          tablename='pagedb-xxxxxx-dev',
          key={'id': {'s': pageid}},
          updateexpression='set content = :val1, contentstate = :val2',
          expressionattributevalues={
             ':val1': {'s': page_content},
             ':val2': {'s': state}
          },
       )
    
       return {
          'statuscode': 200,
          'headers': {
             'content-type': 'application/json', 
             'access-control-allow-origin': '*',
             'access-control-allow-headers': 'content-type',
             'access-control-allow-origin': '*',
             'access-control-allow-methods': 'options,post,get'
           }
        }

this lambda function is invoked through api gateway from a react application.
    const response = await axios.post(
          '
          {page_id: page.id, transcript: page.transcript},
        );

the issue: most of our calls exceed the 30 second maximum timeout for api gateway and will cause an error, despite the lambda function going to successful completion. to work around this, we made the call asynchronous by disabling ""use lambda proxy integration"" and adding an http header x-amz-invocation-type mapped from 'event'. this allows our lambda function to run beyond 30 seconds, however we cannot pass details through 'event' of the handler function anymore, as lambda proxy allowed for this.
question: how can we pass details from our react application to the lambda function through api gateway if lambda proxy integration is disabled?
update - implemented fix
with mark's feedback, i setup a mapping template for the integration request of my method in api gateway doing the following:

integration request -> mapping templates -> set ""request body passthrough"" to ""when there are no templates defined""
""add mapping template"" -> entered ""application/json""
generate template: method request passthrough
modified to the following:

#set($allparams = $input.params())
{
   ""method"" : ""$context. ## api method
   ""authcontext"" : ""$context.authorizer.stringkey"", ## optional output from lambda authorizers

   ## passthrough body
   ""body"" : $input.json('$'),

   ## passthrough headers, querystrings and path parameters
   ""params"" : {
      #foreach($type in $allparams.keyset())
      #set($params = $allparams.get($type))
      ""$type"" : {
         #foreach($paramname in $params.keyset())
         ""$paramname"" : ""$util.escapejavascript($params.get($paramname))""
         #if($foreach.hasnext),#end
         #end
      }
      #if($foreach.hasnext),#end
      #end
   }
}


updated my lambda function to set the variables as such

    pageid = event['body']['page_id']
    transcript = event['body']['transcript']","['amazon-web-services', 'asynchronous', 'aws-lambda', 'aws-api-gateway', 'openai-api']",76783101,"when you disabled the proxy integration, you also disabled the default proxy request mapping. you need to enable integration passthrough request mapping to get that request data showing up in the lambda function's event parameter again.",https://stackoverflow.com/questions/76782909,amazon-web-services,27-07-2023 19:36,184.0,0.0,1.0,True,28-07-2023 14:04,28-07-2023 14:04
75570409,openai api error: &quot;attributeerror: module &#39;openai&#39; has no attribute &#39;embedding&#39;&quot;,"according to openai's documentation and a large number of demonstrations i found online, the following code should run without a problem in python:
import openai
response = openai.embedding.create(
  input=""porcine pals say"",
  model=""text-embedding-ada-002""
)

however, when i run this code on my local jupyter instance, i receive the following error:
attributeerror                            traceback (most recent call last)
>! <ipython-input-209-e3e908b35b81> in <module>
1 import openai
2 response = openai.embedding.create(
3   input=""porcine pals say"",
4   model=""text-embedding-ada-002""
5 )

attributeerror: module 'openai' has no attribute 'embedding'

this is unique only for embedding, as other engines (like completion) run fine on my local machine.
i upgraded my openai library to the newest version, but the error remained.  i also asked chatgpt for help, but its response appeared to be nothing more than a work-around using completion (not embedding).  this did not work.
my question is whether others have encountered the same problem?  if so, how did you resolve it?  i presently don't have a workaround to retrieve embeddings from openai's new 'text-embedding-ada-oo2' model.  so even if there is a workaround i could use- that would be great.","['python', 'embedding', 'openai-api', 'azure-openai', 'text-embedding-ada-002']",75571226,"i tested your code with:

python 3.11.1 and openai python library 0.26.3
python 3.11.1 and openai python library 0.26.5 (latest version)

in both cases i ran test.py and the openai api returned the embedding:

[-0.02801201120018959, 0.022157097235322, -0.011196479201316833,
0.005577428266406059, 0.012320289388298988, 0.007221521344035864, 0.00034121860517188907, -0.020603187382221222, -0.011182605288922787, -0.011349095962941647, -0.007270080968737602, 0.03884775936603546, -0.016232814639806747, 7.668747275602072e-05, -0.018938282504677773, 0.040873393416404724, 0.01576109230518341, 0.032798610627651215, 0.0067047071643173695, -0.03257662057876587, -0.01071088295429945, -0.002186920726671815, 0.018535930663347244, -0.0074435085989534855, -0.0016180785605683923, -0.009108412079513073, 0.023946870118379593, -0.03690537437796593, -0.024030115455389023, -0.007582250516861677, 0.015539104118943214, -0.02534816414117813, -0.008275960572063923, -0.015261620283126831, -0.019853981211781502, 0.0053346301428973675, 0.0011671670945361257, -0.02440471760928631, 0.05225023627281189, -0.010988366790115833, -0.004113700240850449, 0.020686432719230652, ...]

test.py
import openai

response = openai.embedding.create(
  input = 'create embedding for this text',
  model = 'text-embedding-ada-002'
)

content = response['data'][0]['embedding']

print(content)

solution
step 1: upgrade python
see the instructions.
step 2: upgrade openai python library
pip install --upgrade openai",https://stackoverflow.com/questions/75570409,python,26-02-2023 06:48,12516.0,1.0,1.0,True,14-09-2023 11:08,11-04-2023 09:37
70387763,"spacy library to extract noun phrase - valueerror: [e866] expected a string or &#39;doc&#39; as input, but got: &lt;class &#39;float&#39;&gt;","currently i'm trying to extract noun phrase from sentences.
the sentences were stored in a column in excel file.
here the code using python:
import pandas as pd
import spacy

df = pd.read_excel(""xxx.xlsx"")

nlp = spacy.load(""en_core_web_md"")
for row in range(len(df)):
    doc = nlp(df.loc[row, ""title""])
    for np in doc.noun_chunks:
        print(np.text)

but i got this error:
traceback (most recent call last):
  file ""/users/pusinov/pycharmprojects/textsummarizer/paper_term_extraction.py"", line 10, in <module>
    doc = nlp(df.loc[row, ""title""])
  file ""/users/pusinov/pycharmprojects/textsummarizer/venv/lib/python3.9/site-packages/spacy/language.py"", line 1002, in __call__
    doc = self._ensure_doc(text)
  file ""/users/pusinov/pycharmprojects/textsummarizer/venv/lib/python3.9/site-packages/spacy/language.py"", line 1093, in _ensure_doc
    raise valueerror(errors.e866.format(type=type(doc_like)))
valueerror: [e866] expected a string or 'doc' as input, but got: <class 'float'>.

can anyone help me to make better code?
thank you very much.
p.s. i'm still newbie in python","['python', 'spacy', 'phrase']",72169544,"i faced a similar issue and i fixed it using
df['title']= df['title'].astype(str)

the use of this code will fix the problem. as you have to convert all the data values to str format (usually it happens as comment might be number, or nan or null).",https://stackoverflow.com/questions/70387763,python,17-12-2021 02:26,8587.0,4.0,3.0,True,08-10-2022 15:18,18-12-2021 15:49
65667929,invalidrequesterror: must provide an &#39;engine&#39; parameter while invoking openai api for text generation,"i was trying this code given in openai.
link:- api for text generation
code
import openai

prompt = """"""weï¿½ï¿½ï¿½re releasing an api for accessing new ai models developed by openai. unlike most ai systems which are designed for one use-case, the api today provides a general-purpose ï¿½ï¿½ï¿½text in, text outï¿½ï¿½ï¿½ interface, allowing users to try it on virtually any english language task. you can now request access in order to integrate the api into your product, develop an entirely new application, or help us explore the strengths and limits of this technology.""""""

response = openai.completion.create(model=""davinci"", prompt=prompt, stop=""\n"", temperature=0.9, max_tokens=100)

print(response)

i'm getting an error
error

""must provide an 'engine' parameter to create a %s"" % cls, ""engine"".
openai.error.invalidrequesterror: must provide an 'engine' parameter to create a <class 'op.completion.completion'>

i'm using python 3.7.6","['python', 'openai-api']",65880052,"it seems you have confused the engine parameter with the model parameter. please have a look at this documentation for the correct way to call: 
please change model = ""davinci"" to engine = ""davinci""
and you should be good to go.",https://stackoverflow.com/questions/65667929,python,11-01-2021 13:39,38783.0,6.0,4.0,True,13-06-2023 14:52,15-01-2023 17:48
51476682,training a model to identify names appearing in a sentence,"i have a dataset containing the names of about 238583 people. the names can contain more than one word for example:
 willie enriquez , james j johnson, d.j. khaled.
 my problem is to identify these names when it appears in a sentence. i am trying to create a machine learning model that can identify if the input is a name or not. my trouble is figuring the input and output of this model. since i have a bunch of names i can train a model which can recognise a name when the input is a name, but what about the other words that are part of this sentence. the model should also be able to identify words that are not names. assuming the sentences can have any other words in it, what would be the ideal dataset for this purpose? does it make sense to train a model on a random bunch of words and tag it as nonnames?
(the entire sentences in which the names appear is not available. the user can type absolutely anything he/she wants)   
thankyou.","['machine-learning', 'nlp', 'named-entity-recognition']",51476962,"the specifics of the answer may vary according to which model you are using, but the general idea is more or less the following:
you are trying to solve a classification task, precisely a binary classification task where you want to distinguish between proper names (assuming from your example) from other expressions. 
the input to your model, in the most general case, are the features of the example that you want to classify: you should decide what features you think are useful to distinguish such names (e.g., number of words, contains capital letter, every word is capitalized, contains dotted letters, contains any word that you already have in your dataset, etc...). the output is the class, that is 0/1 for non-names/names. 
you then train your model with positive examples from the dataset that you have and negative examples (i.e. non-names) taken from random words for non-names.
if the use can enter full sentences then you will need to do a preprocessing step where you extract all sequences of length n (word n-grams) and classify each of them individually with your previously trained model.",https://stackoverflow.com/questions/51476682,machine-learning,23-07-2018 10:30,2037.0,1.0,1.0,True,23-03-2025 22:45,23-07-2018 11:05
76219536,can&#39;t install flash attention in azure databricks gpu (for hugging face model),"i can successfully run the following code on a cpu cluster in databricks.
import torch
import transformers
model =  transformers.automodelforcausallm.from_pretrained(
    ""mosaicml/mpt-7b"",
    trust_remote_code=true,
    torch_dtype=torch.bfloat16,
)

on the cpu databricks cluster, i first installed pytorch 2.0.1 ;transformers 4.28.1 & einops 0.6.1.
however, the same python code fails on a gpu cluster - with the following error:
importerror: this modeling file requires the following packages that were not found in your environment: flash_attn. run pip install flash_attn
i then tried to install the required package pip install flash-attn on the databricks gpu cluster. (based on instructions here
however, i have been unable to install flash attention on the gpu cluster.
on gpu i tried the following:

attempted to install 'flash-attn' library on the gpu cluster. pip install flash-attn resulted in the following error:

collecting flash-attn
  downloading flash_attn-1.0.4.tar.gz (2.0 mb)
     ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 2.0/2.0 mb 28.1 mb/s eta 0:00:0000:010:01
  preparing metadata (setup.py) ... done
requirement already satisfied: torch in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (from flash-attn) (1.13.1)
requirement already satisfied: einops in /local_disk0/.ephemerrsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging->flash-attn) (3.0.9)
requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (from torch->flash-attn) (11.10.3.66)
requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (from torch->flash-attn) (8.5.0.96)
requirement already satisfied: typing-extensions in /databricks/python3/lib/python3.10/site-packages (from torch->flash-attn) (4.3.0)
requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (from torch->flash-attn) (11.7.99)
requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (from torch->flash-attn) (11.7.99)
requirement already satisfied: setuptools in /databricks/python3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->flash-attn) (63.4.1)
requirement already satisfied: wheel in /databricks/python3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->flash-attn) (0.37.1)
building wheels for collected packages: flash-attn
  building wheel for flash-attn (setup.py) ... error
  error: subprocess-exited-with-error
  
  ï¿½ï¿½ python setup.py bdist_wheel did not run successfully.
  ï¿½ï¿½ï¿½ exit code: 1
  ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½> [317 lines of output]
      
      
      torch.__version__  = 1.13.1+cu117
      
      
      fatal: not a git repository (or any of the parent directories): .git
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build/lib.linux-x86_64-cpython-310
      creating build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/flash_blocksparse_attn_interface.py -> build/lib.linux-x86_64-cpython-3pying flash_attn/rotary.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/flash_attn_triton_tmp_og.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/flash_blocksparse_attention.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/flash_attn_triton.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/bert_padding.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/flash_attn_triton_og.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/flash_attention.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/fused_softmax.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/flash_attn_triton_varlen.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/flash_attn_interface.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/flash_attn_triton_tmp.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/attention_kernl.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/flash_attn_triton_single_query.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      creating build/lib.linux-x86_64-cpython-310/flash_attn/modules
      copying flash_attn/modules/embedding.py -> build/lib.linux-x86_64-cpython-310/flash_attn/modules
      copying flash_attn/modules/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn/modules
      copying flash_attn/modules/mlp.py -> build/lib.linux-x86_64-cpython-310/flash_attn/modules
      copying flash_attn/modules/block.py -> build/lib.linux-x86_64-cpython-310/flash_attn/modules
      copying flash_attn/modules/mha.py -> build/lib.linux-x86_64-cpython-310/flash_attn/modules
      creating build/lib.linux-x86_64-cpython-310/flash_attn/utils
      copying flash_attn/utils/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn/utils
      copying flash_attn/utils/generation.py -> build/lib.linux-x86_64-cpython-310/flash_attn/utils
      copying flash_attn/utils/benchmark.py -> build/lib.linux-x86_64-cpython-310/flash_attn/utils
      copying flash_attn/utils/distributed.py -> build/lib.linux-x86_64-cpython-310/flash_attn/utils
      copying flash_attn/utils/pretrained.py -> build/lib.linux-x86_64-cpython-310/flash_attn/utils
      creating build/lib.linux-x86_64-cpython-310/flash_attn/layers
      copying flash_attn/layers/rotary.py -> build/lib.linux-x86_64-cpython-310/flash_attn/layers
      copying flash_attn/layers/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn/layers
      copying flash_attn/layers/patch_embed.py -> build/lib.linux-x86_64-cpython-310/flash_attn/layers
      creating build/lib.linux-x86_64-cpython-310/flash_attn/triton
      copying flash_attn/triton/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn/triton
      copying flash_attn/triton/fused_attention.py -> build/lib.linux-x86_64-cpython-310/flash_attn/triton
      creating build/lib.linux-x86_64-cpython-310/flash_attn/losses
      copying flash_attn/losses/cross_entropy_apex.py -> build/lib.linux-x86_64-cpython-310/flash_attn/losses
      copying flash_attn/losses/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn/losses
      copying flash_attn/losses/cross_entropy_parallel.py -> build/lib.linux-x86_64-cpython-310/flash_attn/losses
      copying flash_attn/losses/cross_entropy.py -> build/lib.linux-x86_64-cpython-310/flash_attn/losses
      creating build/lib.linux-x86_64-cpython-310/flash_attn/ops
      copying flash_attn/ops/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops
      copying flash_attn/ops/layer_norm.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops
      copying flash_attn/ops/gelu_activation.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops
      copying flash_attn/ops/rms_norm.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops
      copying flash_attn/ops/fused_dense.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops
      copying flash_attn/ops/activations.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops
      creating build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/gptj.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/vit.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/llama.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/gpt.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/bert.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/gpt_j.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/opt.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/gpt_neox.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      running build_ext
      building 'flash_attn_cuda' extension
      creating /tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/build/temp.linux-x86_64-cpython-310
      creating /tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/build/temp.linux-x86_64-cpython-310/csrc
      creating /tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/build/temp.linux-x86_64-cpython-310/csrc/flash_attn
      creating /tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src
      emitting ninja build file /tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/build/temp.linux-x86_64-cpython-310/build.ninja...
      compiling objects...
      allowing ninja to set a default number of workers... (overridable by setting the environment variable max_jobs=n)
      [1/9] /usr/local/cuda/bin/nvcc  -i/tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/csrc/flash_attn -i/tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/csrc/flash_attn/src -i/tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/csrc/flash_attn/cutlass/include -i/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/torch/include -i/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -i/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/torch/include/th -i/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/torch/include/thc -i/usr/local/cuda/include -i/local_disk0/.ephemeral_nfs/envs/pythonenv-69bd3443-3436-4892-a827-1f1b494c1c35/include -i/usr/include/python3.10 -c -c /tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/csrc/flash_attn/src/fmha_bwd_hdim32.cu -o /tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/fmha_bwd_hdim32.o -d__cuda_no_half_operators__ -d__cuda_no_half_conversions__ -d__cuda_no_bfloat16_conversions__ -d__cuda_no_half2_operators__ --expt-relaxed-constexpr --compiler-options ''""'""'-fpic'""'""'' -o3 -std=c++17 -u__cuda_no_half_operators__ -u__cuda_no_half_conversions__ -u__cuda_no_half2_operators__ -u__cuda_no_bfloat16_conversions__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 --threads 4 -dtorch_api_include_extension_h '-dpybind11_compiler_type=""_gcc""' '-dpybind11_stdlib=""_libstdcpp""' '-dpybind11_build_abi=""_cxxabi1011""' -dtorch_extension_name=flash_attn_cuda -d_glibcxx_use_cxx11_abi=0
      failed: /tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/fmha_bwd_hdim32.o
      /usr/local/cuda/bin/nvcc  -i/tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/csrc/flash_attn -i/tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/csrc/flash_attn/src -i/tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/csrc/flash_attn/cutlass/include -i/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/torch/include -i/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -i/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/torch/include/th -i/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/torch/include/thc -i/usr/local/cuda/include -i/local_disk0/.ephemeral_nfs/envs/pythonenv-69bd3443-3436-4892-a827-1f1b494c1c35/include -i/usr/include/python3.10 -c -c /tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/csrc/flash_attn/src/fmha_bwd_hdim32.cu -o /tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/fmha_bwd_hdim32.o -d__cuda_no_half_operators__ -d__cuda_no_half_conversions__ -d__cuda_no_bfloat16_conversions__ -d__cuda_no_half2_operators__ --expt-relaxed-constexpr --compiler-options ''""'""'-fpic'""'""'' -o3 -std=c++17 -u__cuda_no_half_operators__ -u__cuda_no_half_conversions__ -u__cuda_no_half2_operators__ -u__cuda_no_bfloat16_conversions__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v -lineinfo -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 --threads 4 -dtorch_api_include_extension_h '-dpybind11_compiler_type=""_gcc""' '-dpybind11_stdlib=""_libstdcpp""' '-dpybind11_build_abi=""_cxxabi1011""' -dtorch_extension_name=flash_attn_cuda -d_glibcxx_use_cxx11_abi=0
      in file included from /tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/csrc/flash_attn/src/fmha.h:39,
                       from /tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/csrc/flash_attn/src/fmha_bwd_launch_template.h:6,
                       from /tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/csrc/flash_attn/src/fmha_bwd_hdim32.cu:5:
      /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/torch/include/aten/cuda/cudacontext.h:6:10: fatal error: cusparse.h: no such file or directory
          6 | #include <cusparse.h>
            |          ^~~~~~~~~~~~
      compilation terminated.
      in file included from /tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/csrc/flash_attn/src/fmha.h:39,
                       from /tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/csrc/flash_attn/src/fmha_bwd_launch_template.h:6,
                       from /tmp/pip-install-nluf5697/flash-attn_79c1dfb03cc4482ba86c435b2db7a8b6/csrc/flash_attn/src/fmha_bwd_hdim32.cu:5:
      /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/torch/include/aten/cuda/cudacontext.h:6:10: fatal error: cusparse.h: no such file or directory
          6 | #include <cusparse.h>
            |          ^~~~~~~~~~~~
      compilation terminated.


......................................... [ removed middle of error message] 

          cmd_obj.run()
        file ""/databricks/python/lib/python3.10/site-packages/setuptools/command/build.py"", line 24, in run
          super().run()
        file ""/databricks/python/lib/python3.10/site-packages/setuptools/_distutils/command/build.py"", line 132, in run
          self.run_command(cmd_name)
        file ""/databricks/python/lib/python3.10/site-packages/setuptools/_distutils/cmd.py"", line 319, in run_command
          self.distribution.run_command(command)
        file ""/databricks/python/lib/python3.10/site-packages/setuptools/dist.py"", line 1217, in run_command
          super().run_command(command)
        file ""/databricks/python/lib/python3.10/site-packages/setuptools/_distutils/dist.py"", line 992, in run_command
          cmd_obj.run()
        file ""/databricks/python/lib/python3.10/site-packages/setuptools/command/build_ext.py"", line 79, in run
          _build_ext.run(self)
        file ""/databricks/python/lib/python3.10/site-packages/cython/distutils/old_build_ext.py"", line 186, in run
          _build_ext.build_ext.run(self)
        file ""/databricks/python/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py"", line 346, in run
          self.build_extensions()
        file ""/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/torch/utils/cpp_extension.py"", line 843, in build_extensions
          build_ext.build_extensions(self)
        file ""/databricks/python/lib/python3.10/site-packages/cython/distutils/old_build_ext.py"", line 195, in build_extensions
          _build_ext.build_ext.build_extensions(self)
        file ""/databricks/python/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py"", line 466, in build_extensions
          self._build_extensions_serial()
        file ""/databricks/python/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py"", line 492, in _build_extensions_serial
          self.build_extension(ext)
        file ""/databricks/python/lib/python3.10/site-packages/setuptools/command/build_ext.py"", line 202, in build_extension
          _build_ext.build_extension(self, ext)
        file ""/databricks/python/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py"", line 547, in build_extension
          objects = self.compiler.compile(
        file ""/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/torch/utils/cpp_extension.py"", line 658, in unix_wrap_ninja_compile
          _write_ninja_file_and_compile_objects(
        file ""/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/torch/utils/cpp_extension.py"", line 1573, in _write_ninja_file_and_compile_objects
          _run_ninja_build(
        file ""/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/torch/utils/cpp_extension.py"", line 1916, in _run_ninja_build
          raise runtimeerror(message) from e
      runtimeerror: error compiling objects for extension
      [end of output]
  
  note: this error originates from a subprocess, and is likely not a problem with pip.
error: legacy-install-failure

ï¿½ï¿½ encountered error while trying to install package.
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½> flash-attn

note: this is an issue with the package mentioned above, not pip.
hint: see above for output from the failure.

second, i tried the following:
2. updated torch library on gpu to version 2.0.1 (to match the successful approach on cpu) , tried again to-attn . this still didn't work.
my suspicion is that the code works fine on cpu cluster because it is not prepackaged with an earlier version of pytorch. i install pytorch 2.0.1 on databricks cpu and it works fine. however, the gpu cluster is preinstalled with an earlier version of pytorch, and flash attention has not installed on gpu.","['pytorch', 'azure-databricks', 'huggingface-transformers']",76280603,"this is likely related to missing cuda dependencies.
please try restarting the cluster, running the following on a notebook cell and reinstalling flash_attn - and then give it another shot:
!wget  -o /tmp/libcusparse-dev-11-7_11.7.3.50-1_amd64.deb && \
  wget  -o /tmp/libcublas-dev-11-7_11.10.1.25-1_amd64.deb && \
  wget  -o /tmp/libcusolver-dev-11-7_11.4.0.1-1_amd64.deb && \
  wget  -o /tmp/libcurand-dev-11-7_10.2.10.91-1_amd64.deb && \
  dpkg -i /tmp/libcusparse-dev-11-7_11.7.3.50-1_amd64.deb && \
  dpkg -i /tmp/libcublas-dev-11-7_11.10.1.25-1_amd64.deb && \
  dpkg -i /tmp/libcusolver-dev-11-7_11.4.0.1-1_amd64.deb && \
  dpkg -i /tmp/libcurand-dev-11-7_10.2.10.91-1_amd64.deb

update - added both the init script and general instructions as part of this repo:",https://stackoverflow.com/questions/76219536,pytorch,10-05-2023 14:19,9154.0,2.0,2.0,True,02-09-2023 12:40,10-05-2023 15:01
77661152,sentence similarity between a phrase with 2-3 words and documents with multiple sentences,"what i want to achieve: i have thousands of documents (descriptions of incidents) and i would like to find the documents which match a phrase or are similar to the words in the phrase. an example, for an input phrase, ""electric vehicle"", i would like to find all the documents that has any discussion related to anything happening with any type of electric vehicle or conveyance, the documents in the corpus might not have the word ""vehicle"", but may have the specific vehicle type mentioned, like ""scooter"", ""bicycle"", ""hoverboard"" etc,. and document may have the word ""electrical"" or even something like ""lithium battery of a "". so, from an input phrase like ""an electric vehicle"" or ""an electric automobile"" or ""vehicle powered by a lithium-ion battery"", i need to find out all the documents that has related mentions to that term. but, i don't want to capture the documents with ""automobile"", ""scooter"" that doesn't have any mention of ""electric"" or ""lithium-ion"". so, from a phrase with 1 to 4 words, i must find matching documents containing anywhere from 2 to 100 words used for 1 to 7 sentences in each document.
and the list of input phrases (that are used to find matching documents) will vary, hence something like siamese-networks or even training a classification model can't be done i suppose.
and the count of documents will also keep increasing by day and each of the document is independent of each other.
here's what i have done till now:
i have used sentence-transformers (tried the pre-trained models, multi-qa-mpnet-base-dot-v1, all-minilm-l12-v2, all-minilm-l16-v2 and all-mpnet-base-v2), to get normalized embeddings for all the documents, then my input phrase. and then computed cosine-similarity between my input phrase's embeddings with all the documents, then get the top 20 sentences with highest values.
the matched documents were barely relevant. for ex, for input phrase ""an electrical vehicle"" matches documents, with highest cosine-similarity, containing nothing but the word ""electrical"", followed by documents with only ""vehicle"", then documents with only ""electrical vehicle"" or a bit more words or the same 2 words in different forms, followed by documents just a bit more words but having mentions only of ""vehicle"" without ""electrical"" and vice-versa. i presume, because of the less count of words in the input phrase.
how do i counter this and find documents that actually mention all the words in my input phrase instead of just using one word to find the matching documents?","['nlp', 'word-embedding', 'sentence-similarity', 'sentence-transformers']",77666884,"in general your approach so far seem sensible and you should see more relevant search results. i suggest these improvements:

use models for asymmetric semantic search. have a look at this part of sentence transformer's documentation. here you'll find a selection of ms marco models specifically trained for short queries (e.g. few words) and search for larger text passages (e.g. your documents). at the moment you seem to use suboptimal pretrained models.
have a look at hybrid search. combining lexical with semantic search might yield better results for your use case. weaviate has a nice implementation that you quickly can whip up and try out.

you could also provide a minimal reproducible example. this might help to give more detailed recommendations.",https://stackoverflow.com/questions/77661152,nlp,14-12-2023 15:09,306.0,1.0,1.0,True,29-01-2024 10:14,29-01-2024 10:14
72349165,grammar parser for parsing parliamentary debates?,"i'm looking to parse the plain text from a transcription tool (the goal is to render it into legaldocml).
my issue is that i do not know where to start and learning a grammar parser is quite a steep learning curve. i'm looking for guidance as to what kind of parser would be appropriate for the problem.
my gut feel is that the below is a candidate for lr grammar tools as there might be some clear delimiters? (all caps for speaker, brackets for speaker role, square brackets to speech time) but also some nlp needs - for grievances  the person the speech is addressed to is often loosely in the first sentence of the speech..
any advice would be appreciated
as a sample:
legislative assembly
thursday, 19 may 2022
               
the speaker (mrs m.h. roberts) took the chair at 9.00 am, acknowledged country and read prayers.
paper tabled
a paper was tabled and ordered to lie upon the table of the house.
small business assistance grants
statement by minister for small business
statement
mr d.t. punch (bunbury ï¿½ï¿½ï¿½ minister for small business) [9.01 am]: i would like to bring to the attention of the house some recent changes made by the mcgowan government to the small business assistance grants. as i have previously advised the house, in february the state government announced a $67 million level 1 covid-19 business assistance package, and more recently a $72 million package for businesses impacted by level 2 public health and social measures, taking the total committed to covid-19 business support to almost $1.7 billion over the past two years. the level 1 package includes $42 million in rent relief assistance and the level 2 package includes a $66.8 million small business hardship grants program.
last month, a revision and expansion of the small business hardship grants program was announced.
.
.
.
home indemnity insurance
grievance
mr r.s. love (moore ï¿½ï¿½ï¿½ deputy leader of the opposition) [9.06 am]: i grieve today to the parliamentary secretary to the minister for behalf of western australian residents who have had their","['python', 'nlp', 'grammar', 'peg', 'lr-grammar']",72370632,"this problem is indeed in an awkward wasteland between context-free parsing, which is far too precise to handle unstructured discourse, and natural language parsing, which (as i understand the current state of the art) is not designed to take advantage of subtle printed clues.
my recommendation, for what it's worth, is that you use a collection of ad hoc regular expressions to attempt to capture the printed style and the boilerplate phrases. (""a paper was tabled and ordered to lie upon the table of the house."") that's what i did when i tried to do something like this a couple of decades ago with the canadian equivalent (in the days in which perl was state of the art), and it mostly worked, although a certain amount of manual intervention was required. (my style is to use sanity checks to try to detect cases which are mishandled and log them to allow future improvements.) how much work all that is will depend on how precise you need the results to be.
it's quite possible that you could build a machine learning model which did a reasonable job, if you have access to enough computational resources. but you'll still need to do a lot of verification and recalibration, unless you can tolerate errors.",https://stackoverflow.com/questions/72349165,python,23-05-2022 13:12,66.0,-1.0,1.0,True,24-05-2022 23:54,24-05-2022 01:25
78486966,langchaing text splitter &amp; docs saving issue,"i'm trying to use the langchain text splitters library fun to ""chunk"" or divide a massive str file that has sci-fi books i want to split it into n_chunks with a n_lenght of overlaping
this is my code:
from langchain_text_splitters import charactertextsplitter

text_splitter = charactertextsplitter(
    chunk_size=30,
    chunk_overlap=5
)

text_raw = """"""
water is life's matter and matrix, mother, and medium. there is no life without water.
save water, secure the future.
conservation is the key to a sustainable water supply.
every drop saved today is a resource for tomorrow.
let's work together to keep our rivers flowing and our oceans blue.
""""""

chunks=text_splitter.split_text(text_raw)

print(chunks)

print(f'\n\n {len(chunks)}')

but this is my output:
[""water is life's matter and matrix, mother, and medium. there is no life without water.\nsave water, secure the future.\nconservation is the key to a sustainable water supply.\nevery drop saved today is a resource for tomorrow.\nlet's work together to keep our rivers flowing and our oceans blue.""]


 1

my intention is to split at every 30 characters and overlap the last/leading 5
for instance if this is one chunk:
'this is one chunk after text splitting abc'

then i want my following chunk to be something like :
'splitting abc this is my second chunk ---'' 

notice how the beginning of the next chunk overlaps the last characters of the previous chunks?
that's what i'm looking for but it is obvious that that is not how that function works. i am very new to langchanin. i have checked the official documentation but haven't found an example or tutorial like the one i'm looking for.
i am also wanting to write a function to save locally the chunks from langchain. or do we have to stick to base python to do that?","['python', 'split', 'langchain', 'py-langchain']",78487317,"i think the subclass charactertextsplitter does not quite solve this problem, because it splits long string based on manually specified seperators such as ""\n"". it seems a better thing to use is the tokentextsplitter, which splits a long string based on tokens. here is what i tried based on your example:
from langchain_text_splitters import tokentextsplitter

text_splitter = tokentextsplitter.from_tiktoken_encoder(
    chunk_size=20,
    chunk_overlap=4
)

text_raw = """"""water is life's matter and matrix, mother, and medium. there is no life without water.
save water, secure the future.
conservation is the key to a sustainable water supply.
every drop saved today is a resource for tomorrow.
let's work together to keep our rivers flowing and our oceans blue.
""""""

chunks = text_splitter.split_text(text_raw)

# chunks is a list[str]
for line in chunks:
    print(line.replace(""\n"", "" ""))

this is the output i got:
water is life's matter and matrix, mother, and medium. there is no life without water.
 life without water. save water, secure the future. conservation is the key to a
 the key to a sustainable water supply. every drop saved today is a resource for tomorrow. 
 for tomorrow. let's work together to keep our rivers flowing and our oceans blue.

i noticed that for tokentextsplitter, chunk_size and chunk_overlap refer to number of words, unlike in some other subclasses where they refer to number of characters. so i guess this would be a better choice. you do have to replace any mechanical seperators like ""\n"" into something natrual though.",https://stackoverflow.com/questions/78486966,python,15-05-2024 23:43,2190.0,0.0,2.0,True,19-05-2024 22:50,19-05-2024 22:50
4600612,extracting noun+noun or (adj|noun)+noun from text,"is it possible to extract noun+noun or (adj|noun)+noun using the r package opennlp? that is, i would like to use linguistic filtering to extract candidate noun phrases. could you direct me how to do?
many thanks.

thanks for the responses.
here is the code:
library(""opennlp"")

acq <- ""gulf applied technologies inc said it sold its subsidiaries engaged in
        pipeline and terminal operations for 12.2 mln dlrs. the company said 
        the sale is subject to certain post closing adjustments, 
        which it did not explain. reuter."" 

acqtag <- tagpos(acq)    
acqtagsplit = strsplit(acqtag,"" "")
acqtagsplit

qq = 0
tag = 0

for (i in 1:length(acqtagsplit[[1]])){
    qq[i] <-strsplit(acqtagsplit[[1]][i],'/')
    tag[i] = qq[i][[1]][2]
}

index = 0

k = 0

for (i in 1:(length(acqtagsplit[[1]])-1)) {
    
    if ((tag[i] == ""nn"" && tag[i+1] == ""nn"") | 
        (tag[i] == ""nns"" && tag[i+1] == ""nns"") | 
        (tag[i] == ""nns"" && tag[i+1] == ""nn"") | 
        (tag[i] == ""nn"" && tag[i+1] == ""nns"") | 
        (tag[i] == ""jj"" && tag[i+1] == ""nn"") | 
        (tag[i] == ""jj"" && tag[i+1] == ""nns""))
    {      
            k = k +1
            index[k] = i
    }

}

index


reader can refer index on acqtagsplit to do noun+noun or (adj|noun)+noun extraction. (the code is not optimal, but it works. if you have any idea, please let me know.)
i have an additional problem:
justeson and katz (1995) proposed another linguistic filtering to extract candidate noun phrases:
((adj|noun)+|((adj|noun)*(noun-prep)?)(adj|noun)*)noun

i cannot understand its meaning well. could you do me a favor and explain it? or show how to code the filtering rule in the r language?
many thanks.","r, nlp, opennlp, pos-tagger",4600660,"it is possible. 
edit:
you got it. use the pos tagger and split on spaces: ll <- strsplit(acqtag,' '). from there iterate on the length of the input list (length of ll) like:
for (i in 1:37){qq <-strsplit(ll[[1]][i],'/')} and get the part of speech sequence you're looking for. 
after splitting on spaces it is just list processing in r.",https://stackoverflow.com/q/4600612,"r, nlp, opennlp, pos-tagger",05-01-2011 03:34,5617.0,8.0,2.0,True,31-10-2022 12:09,31-10-2022 12:09
49634536,"python, stemmer not found","i got this code from github and this code will execute on windows machine 64 bit.
here's the error i get:
traceback (most recent call last):
  file ""new.py"", line 2, in 
    import stemmer
modulenotfounderror: no module named 'stemmer'
import math
import stemmer

def irange(sequence):
   return zip(range(len(sequence)), sequence)

class cosinescore(object):
    def __init__(self,all_docs):
      self.documents = all_docs #list all docs [doc1,doc2..]
       self.ndocs = len(all_docs)
    self.posting_list = {} #term frequency list, don't care about term position
     #term => {docid => freq}
       self.pstemmer = stemmer.porterstemmer()

    self._term_indexer()

def _term_indexer(self):
    #create term frequency dict
    #run each word through stemmer
    for doc_id,document in irange(self.documents):
        for word in document.split(' '):
            s_word = self.pstemmer.stem(word)
            if self.posting_list.has_key(s_word):
                doc_id_mapping = self.posting_list[s_word]
                if doc_id_mapping.has_key(doc_id):
                    doc_id_mapping[doc_id] += 1
                else:
                    doc_id_mapping[doc_id] = 1
            else:
                self.posting_list[s_word] = {doc_id: 1}

def _term_frequency(self,term):
    if self.posting_list.has_key(term):
        return self.posting_list[term]
    else:
        return -1

def _listtostring(self,arg):
    if isinstance(arg,basestring):
        return arg.split(' ')

def __qtermfrequency(self,term,bwords):
    count =0
    for i,bwordsobj in irange(bwords):
        if bwordsobj == term:
            count = count +1
    return count

def _doclistweights(self) :

    all_terms = self.posting_list.keys()
    doclist_weights = [0.0] * self.ndocs 

    #for all terms in the corpus
    for i,term in irange(all_terms):
        #for all docs in corpus that contain this term
        docs = self.posting_list[term].keys()
        for j,doc_id in irange(docs):
            tf = self.posting_list[term][doc_id]
            tfsquared = (tf * tf)
            doclist_weights[doc_id] += tfsquared 

        for k in range(self.ndocs):
            doclist_weights[k] = math.sqrt(doclist_weights[k])
    return doclist_weights

def compute(self,query,midf=0):
    '''
    dft - document term frequency
    idf - inverse document frequency
    wtq - weights for each query term
    midf - max tf normalization
    '''

    scores = [0.0] * self.ndocs
    bwords = self._listtostring(query)
    normalizationfactor = self._doclistweights() 

    for qterm in bwords:
        term = self.pstemmer.stem(qterm)
        #calculate wt
        #dft =  __qtermfrequency(queryterm,bwords)
        #wtq = math.log10(int(n)/dft) 

        term_posting_doclist = []
        if self._term_frequency(term) != -1:
            #find all documents with this query term 

            term_posting_doclist = self.posting_list[term].keys()
            #total_term_frequency_in_corpus = sum(self.posting_list[term].values())

            if(midf!=0):
                dft = midf
            else:
                dft = len(term_posting_doclist) 

            _wtq = float(self.ndocs)/float(dft)
            wtq = math.log10(float(_wtq)) #idf

        #cosinescore algorithm
        for doc_id in term_posting_doclist:
            if normalizationfactor[doc_id] != 0:
                #wftd = termdocfrequencylist/ normalizationfactor(doc_id) 
                wftd = self.posting_list[term][doc_id] / float(normalizationfactor[doc_id])    
            else:
                wftd = 0.0

            scores[doc_id] +=  (wtq * wftd)
    return scores

if __name__ == ""__main__"":
    docs = [  ""mallya"",""mallya mallya in hawaii"", ""sunil"" ]
    q = ""hawaii mallya""
    cs = cosinescore(docs)
    print (cs.compute(q))","['python-3.x', 'python-import', 'stemming']",49634768,"most probably it is nltk , you can install it using : 
pip install nltk

change import stemmer to import nltk.stem as stemmer
and run the code. please do take note this code is in python 2.7 and will not run if u have python3",https://stackoverflow.com/questions/49634536,python-3.x,03-04-2018 16:05,17226.0,4.0,4.0,True,23-03-2022 12:53,03-04-2018 16:09
77127234,using delphi application to ask question to chatgpt,"i'm facing on chatgpt to create an application using delphi 11, where the purpose is send data to ai then get a response related to it.
this is the code (but i get ssl protocol error on  as error:1409442e:ssl routines:ssl3_read_bytes:tlsv1 alert protocol version)
i think that the problem is related to indy 10 components and its ssl.
any suggestion?
function senddatatochatgpt(data: tjsonobject): string;
var
   tid
  requesturl: string;
  requestbody: tstringstream;
  idssliohandlersocketopenssl: tidssliohandlersocketopenssl ;
begin
   := tid
  try
    idssliohandlersocketopenssl := tidssliohandlersocketopenssl.create( ;
     := idssliohandlersocketopenssl;
    idssliohandlersocketopenssl.ssloptions.sslversions :=[sslvtlsv1_2];

    requesturl := '
     := 'application/json';
     bearer myapi_key'); //myapi_key is my personal token

    requestbody := tstringstream.create(data.tostring, tencoding.utf8);
    try
      result :=  requestbody);
    finally
      requestbody.free;
    end;
  finally
    
  end;
end;


procedure tform10.button5click(sender: tobject);
var
  data: tjsonobject;
  dataarray: tjsonarray;
  dataitem: tjsonobject;
  response: string;
begin
  // prepare your data as a json object
  data := tjsonobject.create;
  try
    data.addpair('prompt', 'comment data and trend during interval date');
    data.addpair('max_tokens', '50');

    // create a json array to hold the data items
    dataarray := tjsonarray.create;

    // create a json object for each data item and add it to the array
    dataitem := tjsonobject.create;
    dataitem.addpair('acid', 'acetico');
    dataitem.addpair('value', '10');
    dataitem.addpair('data', '10/02/2003');
    dataarray.addelement(dataitem);

    dataitem := tjsonobject.create;
    dataitem.addpair('acid', 'acetico');
    dataitem.addpair('value', '11');
    dataitem.addpair('data', '12/02/2003');
    dataarray.addelement(dataitem);

    dataitem := tjsonobject.create;
    dataitem.addpair('acid', 'acetico');
    dataitem.addpair('value', '8.5');
    dataitem.addpair('data', '15/02/2003');
    dataarray.addelement(dataitem);

    dataitem := tjsonobject.create;
    dataitem.addpair('acid', 'acetico');
    dataitem.addpair('value', '10.7');
    dataitem.addpair('data', '22/02/2003');
    dataarray.addelement(dataitem);

    // add the data array to the main data object
    data.addpair('data', dataarray);

    // send data to chatgpt
    response := senddatatochatgpt(data);

    // process the response as needed
    memo1.lines.text := response;
  finally
    data.free;
  end;
end;","['delphi', 'openai-api']",77130519,"searching around, it seems that  may require tls 1.3 only, which tidssliohandlersocketopenssl does not support, it can only do tls 1.2 and lower.
to use tls 1.3 in indy, you will have to use this wip ssliohandler instead, or an alternative solution, such as this schannel ssliohandler 1.
1: note that tls 1.3 is supported in schannel only on windows 11, windows server 2022, and later.",https://stackoverflow.com/questions/77127234,delphi,18-09-2023 12:30,615.0,1.0,3.0,True,27-09-2023 12:14,18-09-2023 12:44
72183136,saved machine learning model using pickle won&#39;t predict text values properly,"i currently have a machine learning model which would predict what part of speech does a current word belong to
penn_results = penn_crf.predict_single(features)

and then, i made a code wherein it makes a print making a (word, pos) style print;
penn_tups = [(sent.split()[idx], penn_results[idx]) for idx in range(len(sent.split()))]

and when i try to run this, it gives me this output.
[('the', 'dt'), ('quick', 'jj'), ('brown', 'nn'), ('fox', 'nn'), ('jumps', 'nns'), ('over', 'in')] [('the', 'det'), ('quick', 'noun'), ('brown', 'adj'), ('fox', 'noun'), ('jumps', 'noun'), ('over', 'adp')]
and so  i saved this model using
penn_filename = 'ptcp.sav'
pickle.dump(penn_crf, open(penn_filename, 'wb'))

upon trying to run the model by loading hte saved pickle file with this
test = ""the quick brown fox jumps over the head""
pickled_model = pickle.load(open('penn_treebank_crf_postagger.sav', 'rb'))
pickled_model.predict(test)
print(pickled_model.predict(test))

it prints this
[['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp'], ['nnp']]
how can i make it print the accurate predicted values like this
[('the', 'dt'), ('quick', 'jj'), ('brown', 'nn'), ('fox', 'nn'), ('jumps', 'nns'), ('over', 'in')] [('the', 'det'), ('quick', 'noun'), ('brown', 'adj'), ('fox', 'noun'), ('jumps', 'noun'), ('over', 'adp')]","['python', 'machine-learning', 'pickle', 'text-classification']",72204218,"caution: this code was not tested.
replace the last line
print(pickled_model.predict(test))

with something like this:
tokens_test = test.split()
predictions_test = pickled_model.predict(test)
pairs_test = [(tokens_test[idx], predictions_test[idx]) for idx in range(len(tokens_test))]
print(pairs_test)",https://stackoverflow.com/questions/72183136,python,10-05-2022 08:38,732.0,0.0,2.0,True,19-05-2022 16:12,11-05-2022 00:50
78858561,how to calculate embedding vectors using the openai api for mongodb vector search?,"i am trying to follow the example from the official mongodb documentation.
it mentioned that the search vector was based on the string ""historical heist"" using the openai text-embedding-ada-002 model. however, when i tried to validate this with my own openai api key, i got different results.
why?
curl  \
  -h ""content-type: application/json"" \
  -h ""authorization: bearer sk-xxxxx"" \
  -d '{
    ""input"": ""historical heist"",
    ""model"": ""text-embedding-ada-002"",
    ""encoding_format"": ""float""
  }'

my results are the following:
   ""data"": [
     {
108      ""object"": ""embedding"",
        ""index"": 0,
19      ""embedding"": [
8        -0.02273331,
97         -0.027646419,
          0.0030520316,
 3        -0.033631727,
1        -0.026207773,
4  0:00:01 --:-        0.0016854883,
-:--  0:0        -0.017983066,
0:0        0.006311038,
1         -0.0013012275,
201        -0.024823416,
54        0.015227924,
        0.005751188,
        0.0004003777,
        -0.008095773,

the example results are the following:
'queryvector': [-0.020156775, -0.024996493, 0.010778184, ....","['mongodb', 'mongodb-query', 'openai-api', 'mongodb-atlas', 'vector-search']",78872635,"there was a discussion on the official openai forum. people are saying that they get a different embedding vector for the identical input.
the first reply from curt kennedy suggests that there seems to be a difference between the openai api endpoint and the azure api endpoint for the same model.

embeddings only return vectors. the vector is the same for the same
input, same model, and the same api endpoint. but we have seen
differences between the openai endpoint and the azure endpoint for the
same model. so a pick an endpoint and stick with it to avoid any
differences.

later on, curt kennedy adds that some variance is expected.

some variation is expected because of the random timing in the gpuï¿½ï¿½ï¿½s,
and that floating point is not associative, and they are likely taking
the last hidden layer and scaling out to the unit hyper-sphere, which
would magnify the error for hidden states close to the origin.

i wrote the following python code to make a test:
# imports
import os
import numpy as np
from openai import openai

# initialize openai client
client = openai(
    api_key=os.getenv(""openai_api_key""),
)

# vector from mongodb documentation
vector_from_mongodb = np.array([-0.020156775, -0.024996493, 0.010778184, -0.030058576, -0.03309321, 0.0031229265, -0.022772837, 0.0028351594, 0.00036870153, -0.02820117, 0.016245758, 0.0036232488, 0.0020519753, -0.0076454473, 0.0073380596, -0.007377301, 0.039267123, -0.013433489, 0.01428371, -0.017279103, -0.028358135, 0.0020160044, 0.00856761, 0.009653277, 0.0107912645, -0.0266 0.009594415, -0.020182934, 0.018077003, -0.015709465, 0.003310956, 0.0014878864, -0.015971072, -0.002411684, -0.029561523, -0.030450987, -0.013106481, -0.005385822, -0.018652538, 0.012642129, -0.005189617, 0.018835662, -0.0048102876, -0.0261214, -0.016167276, -0.007972456, 0.0023381072, -0.010058766, -0.009012341, 0.008358325, 0.018665617, 0.02163485, -0.012975678, -0.010745483, -0.002571918, -0.014479915, 0.007226877, 0.015003128, 0.013165343, -0.028279653, 0.0053727417, -0.020588424, -0.017383745, 0.023518417, 0.01262905, -0.011922712, 0.007638907, -0.0073249796, -0.014859244, -0.00001101736, 0.017043658, 0.010111088, 0.0074623227, 0.009555174, 0.008338705, -0.002240005, -0.0010603234, -0.004973792, 0.003391073, 0.021543289, 0.013341927, 0.0005980159, 0.010693162, 0.005336771, 0.016062634, 0.005768421, 0.005186347, 0.039790336, 0.0021942237, -0.0026275094, 0.010431555, 0.0042151334, -0.0050359233, 0.025768232, -0.021451725, 0.01833861, -0.01836477, -0.013433489, 0.030006256, -0.014793842, 0.017475309, 0.0020585153, -0.012975678, -0.017266022, -0.01593183, -0.014257549, 0.0010676811, -0.007887433, -0.0045911926, 0.00012303676, -0.0014976967, 0.03552615, 0.0065630507, -0.037435878, 0.011929252, -0.00939167, 0.016768971, 0.01223664, 0.007789331, -0.037200432, 0.013145722, 0.00896002, 0.021857215, 0.010333453, 0.021582529, -0.007089534, -0.007154935, -0.02485261, 0.0040254686, -0.00088864425, 0.023466095, -0.020719228, -0.006690584, -0.021006994, -0.018286288, 0.025545865, -0.0096598165, 0.008803056, -0.023021365, -0.040078104, 0.015408617, 0.017043658, -0.011242535, 0.0063537657, -0.026618453, 0.0071614753, -0.014623798, 0.00067322777, -0.00083427917, -0.028070368, 0.03714811, -0.004529061, 0.0054087127, 0.0028727653, 0.008384486, 0.010026066, -0.006190262, -0.0002493436, 0.0029953935, -0.026226042, -0.018417092, 0.009941043, 0.0036494094, -0.00982332, 0.013551212, 0.02574207, -0.0022645304, -0.0006004685, 0.012805633, -0.024303235, 0.008194821, -0.014179068, -0.02977081, 0.003095131, -0.0015941641, 0.029953934, 0.0052680993, 0.025388902, -0.031392768, -0.021386323, 0.014898485, 0.022419669, 0.00897964, 0.013243824, 0.006854088, 0.0066415328, -0.003839074, -0.01877026, 0.021216279, -0.015055449, -0.0015508354, 0.013211124, -0.008783435, 0.0052157775, -0.68938524, -0.01221702, -0.04125533, -0.016232677, 0.020039052, -0.0026422248, -0.0037050007, 0.0064682183, -0.0047579664, 0.0032749851, -0.0035382267, 0.031942144, -0.00035643874, -0.011628405, -0.043086577, -0.0196074, -0.0066088317, -0.014872325, 0.028331975, 0.010294212, -0.013930541, 0.031994462, -0.018626377, 0.017462227, 0.026343765, -0.010274592, 0.0046827546, -0.029430721, -0.011746128, 0.0024362097, 0.0023054064, 0.0027730279, -0.002406779, 0.003917556, 0.059436977, 0.008665713, -0.0018901062, 0.06037876, 0.017880797, 0.05185039, 0.0067102043, -0.020300657, 0.005604917, 0.018704858, 0.012073136, 0.0144145135, 0.012413224, -0.0074819434, 0.015801027, -0.0061412104, 0.008613391, -0.0039077457, -0.0036232488, 0.008469507, 0.014087505, 0.0124066835, 0.019267311, -0.002573553, 0.005055544, -0.009417831, -0.009103903, 0.011150973, -0.012046975, 0.0058567133, -0.0053727417, 0.018260127, -0.005588567, 0.015591742, 0.007495024, -0.02567667, 0.024211673, 0.021386323, -0.012890656, -0.016114954, 0.009515933, 0.009679437, 0.025532786, -0.0076454473, -0.02575515, 0.008319084, -0.0068410076, -0.017082898, -0.026173722, -0.0049901423, 0.01918883, -0.008646091, -0.031759016, 0.014820003, 0.011850771, 0.01836477, 0.012700991, -0.0011437106, 0.005058814, 0.0151993325, -0.0060692686, 0.027416352, 0.0037344315, 0.0013546307, 0.018325528, -0.03152357, -0.008809595, 0.014649959, -0.008345244, 0.0066415328, -0.005523165, 0.0043492066, -0.0015892589, 0.0048855, 0.034453563, -0.03837766, 0.0068410076, -0.0042151334, -0.0067429054, 0.0055689462, -0.011733048, -0.0212032, 0.016847452, -0.0022220195, 0.0059351954, -0.00449963, 0.02251123, -0.01020265, 0.023361452, -0.0032455544, 0.016180357, 0.0049443613, -0.0064747585, -0.03259616, 0.012321662, 0.020104453, 0.009954124, -0.019411195, 0.0048102876, -0.000392614, 0.012184318, 0.0044276887, 0.005634348, -0.020562263, 0.015722545, -0.005179807, -0.0067952266, 0.0027861083, 0.0024198592, -0.0020585153, 0.0018525004, -0.045100946, -0.010176489, -0.012956058, 0.0013497255, 0.0105361985, 0.003796563, -0.0106016, -0.013126101, 0.0050359233, 0.015003128, -0.0075800456, -0.015722545, -0.01755379, -0.00978408, -0.02940456, 0.017606111, 0.016612006, -0.016912855, 0.025441224, 0.0054741143, 0.00448001, 0.009470152, 0.015382457, -0.008332164, -0.019123428, 0.024564842, 0.016860534, 0.008286383, -0.007141855, 0.006559781, 0.016625088, -0.01840401, -0.011602244, -0.00489858, -0.0073184394, -0.008809595, -0.0018459603, -0.01629808, -0.005542786, 0.0064257076, 0.010379234, 0.014663039, 0.034872133, -0.013355007, 0.027285548, 0.011654565, -0.004032009, 0.02323065, -0.02653997, -0.0009941043, 0.002946342, 0.010667001, 0.008345244, 0.018626377, 0.04821406, 0.031392768, 0.010281132, 0.026069079, 0.002735422, 0.01182461, -0.01593183, 0.006585941, -0.010071847, 0.024564842, -0.0025261368, 0.004293615, -0.0068606283, -0.0066448026, -0.0074100015, -0.0014347476, 0.021530207, -0.010418476, 0.018495573, -0.0034924455, -0.014165987, -0.004784127, -0.012472086, 0.004417878, -0.0030313642, -0.010084927, -0.010954768, 0.01508161, 0.0010047321, 0.0042347535, -0.03345946, -0.00027346043, 0.014793842, -0.019882087, 0.012772933, 0.021490967, 0.0031932332, 0.0093589695, 0.00090172456, 0.0048102876, 0.0070045115, -0.0045584915, 0.015840268, 0.024342475, -0.0091300635, 0.0039796876, 0.003796563, 0.025022654, -0.008103259, -0.025022654, 0.03021554, -0.008201361, -0.0070502926, 0.0011821339, 0.021072397, 0.004849529, -0.02495725, 0.012184318, 0.0019228071, -0.007226877, 0.020562263, 0.018861823, -0.0017593032, 0.01345965, 0.0022727058, 0.003023189, -0.026971621, -0.0030558899, 0.017723834, -0.01998673, -0.010608139, 0.011491061, -0.025179617, 0.0069652707, 0.003924096, 0.021177039, 0.0045650317, -0.0009973744, 0.007586586, -0.004032009, -0.008129419, -0.010091467, -0.04279881, 0.019790525, 0.01595799, 0.0044309585, -0.0033747226, -0.018665617, -0.012818714, -0.016206518, 0.014113666, -0.0020912162, 0.01427063, -0.020248337, -0.0112752365, -0.020588424, -0.011039791, 0.008744194, -0.015147011, 0.0022269245, -0.010438096, -0.0017772885, -0.028750544, -0.008861917, -0.016991336, 0.033668745, 0.034636687, 0.009888723, 0.0023953337, 0.006991431, -0.003346927, 0.003103306, -0.0044571194, 0.011249076, 0.0033779927, 0.00012446742, -0.0027027212, -0.025859794, -0.011942333, 0.02694546, 0.028227331, 0.0064289775, -0.03385187, -0.020719228, 0.00489531, 0.10663077, 0.041752383, -0.021700252, -0.008103259, 0.0049574412, -0.01675589, -0.020182934, -0.006585941, 0.007684688, -0.002859685, 0.027023941, 0.00856107, 0.0037017306, 0.016978256, 0.025885954, -0.010372694, 0.0025964435, 0.011706887, 0.021360163, -0.021674091, -0.024983412, 0.0034074234, 0.0032030435, 0.022262705, -0.01266829, -0.002249815, 0.032779284, -0.0034303141, -0.016101874, -0.005156916, -0.0212032, 0.005362931, 0.009077743, -0.013917461, -0.0017315074, 0.010980929, -0.019450437, 0.013865139, 0.028227331, -0.008757275, -0.0033649125, -0.012857955, 0.011039791, 0.009764459, 0.00029594224, -0.026317604, 0.025048813, 0.037749805, -0.025807472, -0.005425063, 0.021791814, -0.010012985, -0.00066995766, -0.016952096, 0.0031147513, -0.016598927, 0.0084368065, 0.004787397, -0.0064355177, 0.0015164997, -0.021216279, -0.023845425, 0.013969782, -0.011255615, 0.0042576445, -0.024250913, -0.009908343, -0.02289056, -0.023361452, -0.010987469, -0.013394248, 0.0032553647, -0.019018786, 0.021438645, 0.029587684, -0.010490417, 0.01263559, -0.018417092, -0.008731114, 0.01875718, -0.0072399573, -0.029090632, -0.017736914, -0.04031355, -0.019712042, 0.012772933, -0.030320182, -0.022341188, -0.02041838, 0.011752668, 0.028829027, -0.017043658, 0.024996493, 0.006334145, -0.0024263994, -0.0077370093, 0.017802317, 0.017396826, 0.030398665, 0.011464901, 0.03016322, -0.014558396, -0.0036690298, -0.009954124, -0.006703664, -0.00035705187, -0.014519156, 0.0075342646, -0.00896656, 0.040078104, 0.024420958, -0.016886694, -0.00092543266, -0.0017494928, 0.01672973, 0.016533526, 0.002648765, 0.0187441, -0.0055460557, 0.004735076, 0.03186366, 0.0003435628, 0.007495024, 0.023453014, -0.012504786, -0.0074557825, -0.0027844731, -0.04570264, 0.010477337, 0.0030101088, -0.015670223, 0.03351178, -0.020261416, 0.00050849747, -0.009653277, -0.023466095, -0.007396921, -0.011909632, 0.003436854, -0.02979697, -0.039031677, -0.014584557, 0.0019555078, 0.0042216736, -0.0060594585, -0.023400694, -0.00023462824, -0.017763074, -0.016180357, 0.0132372845, -0.020496862, -0.007390381, -0.0058697937, -0.0096598165, 0.0039796876, -0.019306554, -0.012622509, -0.0012287326, 0.010863206, 0.024368636, 0.027730279, 0.016795132, 0.019908248, -0.006343955, 0.0014592733, -0.005425063, 0.019450437, 0.004532331, -0.031889822, 0.008476048, 0.019712042, -0.00047906674, -0.0028286192, 0.011883471, -0.012426305, 0.0041497317, 0.001756033, -0.0013603533, -0.008031317, -0.010281132, -0.0071222344, -0.026330685, -0.007920134, -0.026866978, -0.03026786, -0.0015328501, 0.027442513, -0.005922115, 0.005186347, 0.003436854, 0.036703378, -0.0053204205, 0.013165343, 0.0016939015, -0.0041431915, -0.017213702, -0.012439385, -0.015212413, 0.014532236, 0.0093589695, -0.0053400407, 0.017422987, -0.028881347, -0.014179068, 0.011307937, 0.040104263, -0.007593126, -0.000631943, -0.0003404971, -0.0055198953, -0.00063030794, -0.004852799, -0.0024214943, -0.029718488, 0.023322212, 0.011079031, 0.012988758, 0.0071614753, -0.034034993, -0.01551326, 0.004012388, 0.006442058, 0.032386873, 0.0076519875, 0.0465921, 0.01757995, -0.0135381315, -0.016978256, 0.024983412, 0.0003280299, 0.0026209692, 0.022380428, -0.010640841, 0.0027648527, -0.007959375, -0.005922115, 0.0075342646, -0.03597088, -0.018874902, 0.03510758, -0.015356296, 0.004597733, -0.0015328501, -0.019947488, -0.013446569, 0.020614585, -0.0056016473, 0.035186063, 0.0005248479, -0.030712591, -0.019136509, 0.004202053, -0.010339993, 0.014754602, 0.0072922786, -0.015460939, 0.027494833, -0.02974465, -0.0033616424, 0.0105819795, -0.028881347, 0.01720062, -0.0073707607, 0.0054479535, -0.0019522378, -0.018103164, -0.009110443, -0.024630243, 0.005624538, 0.01879642, -0.019345794, -0.0027681228, -0.015971072, 0.022354268, -0.0038194535, 0.018901063, -0.017357586, -0.02493109, 0.006703664, -0.0021173768, -0.005667049, -0.004535601, -0.016441964, 0.0034172337, -0.02447328, -0.003310956, -0.02078463, -0.011589164, 0.013263445, -0.014728441, -0.0187441, -0.019476596, 0.013224204, 0.015238573, -0.012380524, 0.00019058435, 0.010778184, 0.025022654, -0.036127847, 0.01470228, -0.007671608, 0.032857765, 0.002982313, 0.009829861, 0.0072203367, -0.0028237142, 0.025990596, -0.029012151, 0.0016955365, 0.012033895, -0.0049901423, -0.013629694, 0.0072464976, 0.0012704261, 0.0018868363, 0.017043658, 0.00448001, -0.009555174, -0.016520444, 0.02570283, -0.00939167, 0.01998673, 0.002001289, -0.023662299, 0.0041072206, -0.024839528, -0.007396921, -0.0034793653, -0.032020625, -0.0036003583, -0.010719323, 0.022995204, -0.01757995, -0.0043851775, -0.023884665, -0.018430172, -0.009018881, 0.00091562246, -0.0055689462, -0.012537487, 0.016455043, 0.03264848, 0.018560974, 0.014623798, 0.0025555675, -0.0060986993, 0.0058272826, -0.008462967, -0.012720612, -0.0042576445, -0.027207067, 0.014152907, -0.0029610575, 0.010241891, -0.011222915, -0.01140604, -0.022197304, -0.003433584, -0.0056899395, 0.004372097, 0.061896075, -0.005846903, -0.011863851, 0.004535601, -0.0074819434, 0.016847452, -0.0012647035, 0.021085477, 0.02409395, -0.030137058, -0.0012197399, 0.009607496, -0.008220982, -0.007893973, -0.007893973, 0.007972456, 0.010012985, 0.009143144, 0.0044734697, 0.015264734, -0.0032520946, 0.002208939, 0.011968493, -0.0012998568, -0.0114322, -0.056454662, -0.013217663, 0.0017593032, -0.00244275, -0.021399405, -0.010732403, 0.00694565, 0.0033207664, 0.0025539326, 0.01102671, -0.012589809, 0.010706242, -0.012413224, 0.01427063, -0.000049970913, -0.0056016473, 0.027965724, 0.018652538, -0.009535554, 0.0068867886, 0.004699105, -0.001245083, -0.009071202, -0.0032946058, -0.03756668, 0.034453563, -0.00408106, 0.013361547, -0.0065107294, 0.009300108, -0.016415803, 0.0059973267, -0.017422987, 0.0048822295, 0.022158062, -0.025611266, 0.01022227, -0.0061771814, -0.014218308, -0.00044636594, -0.019110348, -0.013747416, -0.013629694, -0.021896457, -0.0051634563, -0.020509942, -0.018731019, 0.0043328563, -0.032386873, -0.023086766, 0.0196074, 0.20614585, -0.014649959, -0.009712138, 0.01345965, -0.010928608, 0.0196074, 0.015814107, 0.017383745, -0.0024656404, 0.021399405, 0.013668935, -0.0063864663, -0.0015303975, -0.0012924991, -0.0030575248, -0.015539421, -0.009692517, -0.012190859, -0.02287748, 0.002936532, 0.00069325697, 0.013158802, -0.0070110518, -0.013629694, 0.01585335, -0.019829765, 0.013747416, 0.016036473, 0.011693806, 0.0071483953, -0.010156869, -0.013799738, -0.00034703725, -0.010706242, -0.02289056, 0.0039339066, -0.0015835363, -0.014532236, 0.012445925, -0.00009779583, 0.0053335004, 0.0055329753, -0.005281179, -0.007475403, 0.00040385488, -0.012942977, -0.015277814, 0.012956058, 0.00006162057, 0.007056833, -0.02571591, -0.018731019, -0.0061771814, 0.034427404, 0.0010570535, 0.0079528345, 0.024172433, 0.021386323, -0.019803606, -0.006821387, -0.011262156, 0.026605371, -0.0036951904, -0.008207901, -0.019698963, 0.042981934, -0.026212962, 0.00856761, 0.015173172, 0.0024149541, -0.0008036222, -0.005752071, -0.02898599, -0.008443347, -0.0064224373, -0.014479915, 0.036467932, -0.00086820626, 0.026396086, 0.002001289, -0.0074361623, -0.0086918725, -0.007835112, 0.021464806, 0.0008984545, -0.02489185, 0.019515838, 0.026644614, -0.0137212565, 0.00448982, 0.004211863, -0.022380428, -0.014100585, -0.01629808, 0.0074884836, 0.02652689, 0.011634945, 0.049626734, -0.023583818, -0.0021958589, -0.015735626, 0.02733787, 0.0036428692, -0.031261966, -0.012674831, 0.006196802, -0.009535554, 0.016886694, 0.010771644, -0.021490967, 0.014100585, -0.007063373, 0.00043778197, -0.012151618, -0.0058894143, 0.009182385, -0.005768421, -0.013995943, 0.004725266, -0.01347273, -0.020797709, -0.018037762, 0.020274498, 0.011595704, 0.0017364125, -0.02248507, 0.005954816, 0.0062196925, -0.014257549, -0.025127295, 0.015356296, 0.005179807, 0.021726413, -0.0034499345, -0.017082898, 0.019803606, 0.005209238, 0.0005939283, -0.0035807376, -0.011661106, 0.006559781, 0.0033207664, 0.0017233322, -0.00059924216, -0.000341519, -0.0140221035, 0.00084286317, -0.003306051, -0.005634348, -0.00816212, -0.009319728, -0.024447119, -0.014950806, -0.024564842, 0.0137212565, -0.010084927, 0.000044886958, -0.0033943432, 0.0025359471, 0.012478625, -0.023086766, 0.014519156, 0.020876192, -0.023282971, -0.0030804155, -0.014545316, -0.16805595, 0.01262905, 0.020719228, -0.012413224, 0.026592292, -0.0024198592, 0.041072205, 0.002658575, -0.013708176, -0.0068867886, -0.0018639456, 0.000031627806, -0.043452825, -0.028018046, -0.0105819795, 0.01266829, -0.009450532, 0.008292923, 0.0058534434, -0.006782146, 0.032229908, 0.0005955633, -0.0023103117, 0.003140912, 0.00037687673, -0.0049247406, -0.008070557, 0.017279103, -0.012759852, -0.011608784, -0.019450437, 0.016167276, 0.02248507, 0.030529467, 0.015905669, 0.0061150496, -0.016834373, 0.017344505, 0.006667693, -0.005461034, 0.0066742334, 0.01998673, 0.024591003, -0.007717389, 0.0096598165, 0.03225607, 0.018626377, -0.020248337, 0.0017740185, 0.012589809, 0.0014927916, -0.040235065, 0.01713522, 0.016206518, 0.017776156, 0.024734886, 0.0040516295, -0.009627116, 0.002001289, -0.010496957, -0.0121058365, -0.017266022, 0.008279843, -0.02122936, -0.01349889, -0.02251123, 0.004820098, -0.000071533, -0.022628954, 0.015238573, -0.01833861, -0.016572766, -0.0031523572, -0.008064018, 0.019973649, 0.0089207785, -0.03228223, 0.0040647094, -0.004784127, -0.0017920039, -0.0013775212, 0.047246117, 0.0030804155, -0.010660461, 0.02982313, 0.006088889, -0.019371955, -0.024447119, -0.011687267, -0.013708176, 0.017187541, -0.018286288, 0.019267311, 0.0011960318, 0.0046271635, 0.016886694, 0.0069129495, 0.00029062838, 0.013629694, -0.016494283, -0.017069818, 0.0058240127, 0.013943622, 0.001675916, 0.01347273, 0.023335291, 0.008129419, 0.0047187256, 0.032099105, 0.0007701039, 0.0068344674, 0.0004672127, -0.00610851, 0.026396086, -0.010738943, 0.024591003, 0.008220982, -0.019908248, 0.024682565, -0.009404751, 0.0594893, -0.009731758, -0.022628954, 0.013865139, -0.016049553, 0.0033371167, -0.107572556, -0.022341188, 0.008050937, -0.0089731, 0.004983602, 0.010771644, -0.013034539, -0.013368088, -0.0071287747, 0.0091758445, -0.017409906, -0.022118822, -0.011170594, -0.010908987, 0.050490037, 0.014584557, 0.018312449, 0.0014968792, -0.0057161, 0.024342475, -0.02699778, 0.020091372, -0.00094587065, -0.021347083, -0.003711541, 0.0016677409, -0.030738752, 0.040208906, 0.008109799, -0.017527629, -0.0009058122, 0.017776156, 0.0052779093, -0.0046206233, 0.0067952266, -0.01226934, -0.009162764, -0.01595799, 0.021582529, -0.027390191, -0.00011210243, -0.003145817, 0.01672973, -0.009999905, 0.003832534, -0.01793312, -0.0004868332, 0.027573315, 0.001756033, -0.012112376, -0.009718678, 0.0025473924, -0.027547155, -0.019084187, 0.010693162, 0.025558947, -0.02168717, -0.0068802484, -0.010869746, -0.028698223, -0.0051634563, -0.012131997, -0.014963887, 0.022210384, 0.01510777, -0.0026504, -0.013577373, 0.0058599836, 0.011281776, -0.0009393305, -0.00204053, 0.030110897, -0.029326078, 0.006491109, -0.01671665, 0.0006049648, -0.024342475, -0.008325624, 0.03722659, -0.007710849, -0.0055656764, -0.02043146, -0.015317055, -0.015212413, 0.002815539, 0.022262705, 0.00818828, 0.021778734, -0.0037409717, -0.02485261, 0.0033779927, 0.013217663, -0.0059319255, -0.018940303, 0.02409395, 0.015761785, -0.009672897, 0.011301396, -0.011582624, 0.0029725027, -0.015343216, -0.00735114, -0.075761214, 0.016821291, 0.0028040938, 0.0017233322, 0.01595799, -0.0054741143, -0.007096074, -0.011641486, -0.003554577, 0.009829861, -0.037828285, 0.024983412, 0.003793293, -0.010895907, -0.011916172, -0.017893879, 0.029640006, 0.0027452323, 0.004977062, 0.0138913, 0.0132830655, 0.010725862, 0.014205228, -0.003839074, 0.020470701, 0.0048626093, -0.010967849, 0.035343025, -0.004568302, -0.007665068, 0.0040091183, -0.02367538, -0.006821387, 0.012112376, -0.0012475356, -0.02041838, -0.030869557, -0.004865879, 0.036127847, 0.019528918, 0.00087147637, 0.0016366751, -0.006072539, -0.012380524, -0.016886694, 0.0014224849, 0.0058632535, 0.0053138803, 0.024525601, -0.008227522, 0.016167276, 0.021373244, -0.019855926, -0.011602244, -0.012223559, 0.009116983, 0.00448001, 0.0027027212, 0.0112294555, -0.025048813, 0.005958086, 0.005578757, 0.012040435, -0.019528918, -0.008096718, -0.023439934, 0.00047497914, 0.0073315194, 0.025061894, -0.016455043, 0.003992768, 0.002038895, -0.0003484679, 0.004444039, -0.014846164, 0.0018263398, 0.017305264, -0.0047154557, -0.006729825, 0.011288317, -0.009764459, -0.03220375, -0.015369376, 0.009594415, 0.031078842, 0.020967754, -0.007802411, 0.022354268, -0.010778184, 0.01833861, 0.004581382, 0.0072399573, 0.010673542, -0.012112376, -0.023073684, 0.0066448026, -0.027887244, 0.0063504954, 0.012956058, 0.032151427, -0.018103164, 0.0048855, -0.018286288, -0.036938824, -0.012354363, 0.020039052, 0.004921471, -0.03790677, 0.0212686, 0.02982313, 0.015434778, 0.0041039507, -0.016245758, 0.012171238, -0.006415897, 0.0072464976, -0.0024362097, -0.025218857, -0.021399405, 0.036860343, 0.0056572384, 0.017004417, 0.03432276, -0.013825899, 0.028724384, 0.008528369, 0.018652538, -0.02443404, -0.025637427, 0.006497649, -0.015447859, 0.01917575, -0.016520444, -0.008678793, -0.021072397, 0.015840268, -0.006324335, 0.025925195, -0.03594472, 0.0384823, 0.01308032, 0.0054217926, 0.00448328, -0.027207067, -0.016847452, 0.0036003583, 0.01061468, -0.019816685, -0.004659864, 0.023387613, -0.005461034, 0.004326316, 0.0037278912, -0.007540805, 0.00860031, 0.0015524705, 0.020039052, -0.0028367946, 0.0049509015, 0.009162764, 0.009705598, 0.013982862, 0.004852799, 0.0061869915, -0.0083910255, 0.012975678, -0.034558207, -0.029064473, -0.03058179, -0.019450437, 0.01062122, -0.014179068, -0.010012985, 0.007874353, -0.014126746, -0.009731758, -0.03398267, -0.000115883464, -0.0029725027, -0.024290156, 0.012864495, -0.00937859, -0.035264544, 0.0027959184, 0.012982218, -0.012609429, 0.0065270797, 0.010712783])

# loop to generate embeddings and calculate dot products
for _ in range(3):
    # generate an embedding
    response = client.embeddings.create(
        model=""text-embedding-ada-002"",
        input=""historical heist"",
        encoding_format=""float"",
    )

    # extract the embedding vector from the response
    vector = response.data[0].embedding

    # calculate the dot product between the generated embedding and the vector from mongodb
    dot_product = np.dot(vector, vector_from_mongodb)
    
    # print the first 5 elements of the embedding
    print(f""\nembedding vector (first 5 elements): {vector[:5]}"")

    # print the dot product value
    print(f""dot product: {dot_product}\n"")
        
    print(""------------------------------------"")

after i ran it, i got the following output:
embedding vector (first 5 elements): [-0.02273331, -0.027646419, 0.0030520316, -0.033631727, -0.026207773]
dot product: 0.9853400963855683

------------------------------------

embedding vector (first 5 elements): [-0.02273331, -0.027646419, 0.0030520316, -0.033631727, -0.026207773]
dot product: 0.9853400963855683

------------------------------------

embedding vector (first 5 elements): [-0.02273331, -0.027646419, 0.0030520316, -0.033631727, -0.026207773]
dot product: 0.9853400963855683

------------------------------------

as you can see:

the embedding vector is identical all three times.
the dot product is identical all three times (as a consequence of the first point) and very high. this means that the embedding vectors (i.e., the one from mongodb documentation and generated ones) are very similar.

the discussion on the forum went on about whether this small difference between embedding vectors for the identical input is problematic or not. in other words, should you care about the difference or not? i don't think you should, as long as you get meaningful solutions using them.
moreover, i would care even less if you got identical embedding vectors for a given api endpoint, model, and input. don't compare an embedding vector you get when using the openai api endpoint and mongodb api endpoint. compare an embedding vector you get when using the openai api endpoint multiple times. compare an embedding vector you get when using the mongodb api multiple times. the difference in embedding vectors between the openai api endpoint and mongodb api endpoint might be of a technical nature, as curt kennedy explained on the official openai forum.",https://stackoverflow.com/questions/78858561,mongodb,11-08-2024 14:26,109.0,1.0,1.0,True,21-09-2024 16:13,21-09-2024 16:13
78231239,meta-llama/llama-2-7b-hf returning tensor instead of modeloutput,"i am using the llama-2-7b-hf model with the model.generate function from the transformers library (v4.38.2) and it's returning the outputs as a single tensor, instead of the modeloutput i expected.
i have a copy of the model stored locally:
[llama-2-7b-hf]$ ls -1
config.json
generation_config.json
license.txt
model-00001-of-00002.safetensors
model-00002-of-00002.safetensors
model.safetensors.index.json
readme.md
responsible-use-guide.pdf
special_tokens_map.json
tokenizer_config.json
tokenizer.json
tokenizer.model
use_policy.md

this is the code where the model is initialized and then called:
model_path = ""llama-2-7b-hf""
model = automodelforcausallm.from_pretrained(model_path, return_dict_in_generate=true, local_files_only=true).to(device)
tokenizer = autotokenizer.from_pretrained(engine, local_files_only=true)

input_ids = tokenizer(model_path, return_tensors=""pt"").input_ids.to(device)
outputs = model.generate(input_ids, top_k=1, max_length=max_len, num_return_sequences=1, output_scores=true)
sequences, scores = outputs.sequences, outputs.scores

i have used this code with several other models like mistral and occiglot and they return modeloutput objects with these attributes, sequences and scores, but not llama. can anyone help me understand what is wrong?","['huggingface-transformers', 'llama']",78261886,"i managed to solve it by passing the return_dict_in_generate and output_scores parameters in the call to model.generate instead of in the initialization of the model.
model = automodelforcausallm.from_pretrained(engine, local_files_only=true).to(device)
outputs = model.generate(input_ids, top_k=1, max_length=max_len, num_return_sequences=1, output_scores=true, return_dict_in_generate=true)",https://stackoverflow.com/questions/78231239,huggingface-transformers,27-03-2024 11:13,231.0,1.0,1.0,True,02-04-2024 14:07,27-03-2024 12:55
163923,methods for geotagging or geolabelling text content,"what are some good algorithms for automatically labeling text with the city / region  or origin?  that is, if a blog is about new york, how can i tell programatically.  are there packages / papers that claim to do this with any degree of certainty?  
i have looked at some tfidf based approaches, proper noun intersections, but so far, no spectacular successes, and i'd appreciate ideas!  
the more general question is about assigning texts to topics, given some list of topics.
simple / naive approaches preferred to full on bayesian approaches, but i'm open.","['algorithm', 'statistics', 'nlp', 'named-entity-recognition']",164722,"you're looking for a named entity recognition system, or short ner. there are several good toolkits available to help you out. lingpipe in particular has a very decent tutorial. cageclass seems to be oriented around ner on geographical place names, but i haven't used it yet.
if you're going with java, i'd recommend using the lingpipe ner classes. opennlp also has some, but the former has a better documentation.
if you're looking for some theoretical background, chavez et al. (2005) have constructed an interesting system and documented it.",https://stackoverflow.com/questions/163923,algorithm,02-10-2008 18:44,6592.0,9.0,2.0,True,16-08-2023 14:40,02-11-2009 16:30
77388920,error: could not build wheels for aio which is required to install pyproject.toml-based,"newbie here.
i have been trying to installen the openai library into python, but i keep running into problems. i have already installed c++ libraries.
it seems to have problems specific with aio  and i get the error below.
i a running a windows 11 laptop without admin restrictions.
error
""c:\program files (x86)\microsoft visual studio\2022\buildtools\vc\tools\msvc\14.37.32822\bin\hostx86\x64\cl.exe"" /c /nologo /o2 /w3 /gl /dndebug /md -ic:\users\sande\appdata\local\programs\python\python312\include -ic:\users\sande\appdata\local\programs\python\python312\include ""-ic:\program files (x86)\microsoft visual studio\2022\buildtools\vc\tools\msvc\14.37.32822\include"" ""-ic:\program files (x86)\microsoft visual studio\2022\buildtools\vc\auxiliary\vs\include"" ""-ic:\program files (x86)\windows kits\10\include\10.0.22621.0\ucrt"" ""-ic:\program files (x86)\windows kits\10\\include\10.0.22621.0\\um"" ""-ic:\program files (x86)\windows kits\10\\include\10.0.22621.0\\shared"" ""-ic:\program files (x86)\windows kits\10\\include\10.0.22621.0\\winrt"" ""-ic:\program files (x86)\windows kits\10\\include\10.0.22621.0\\cppwinrt"" /tcaio /fobuild\temp.win-amd64-cpython-312\release\aio
      _websocket.c
      aio warning c4996: 'py_optimizeflag': deprecated in 3.12
      aio error c2039: 'ob_digit': is not a member of '_longobject'

[end of output]

note: this error originates from a subprocess, and is likely not a problem with pip.

error: failed building wheel for aiohttp

failed to build aiohttp

error: could not build wheels for aio which is required to install pyproject.toml-based projects","['python', 'python-3.x', 'pip', 'openai-api', 'aiohttp']",77388956,"either use python 3.11
or
pip install aio


installs their current beta release that supports python 3.12.x

then try openai installation

link to git :",https://stackoverflow.com/questions/77388920,python,30-10-2023 13:08,5788.0,10.0,2.0,True,05-05-2024 08:22,05-05-2024 08:22
77906460,codegpt sublime text plugin,"i recently developed a chatgpt plugin for sublime text, which is designed to enhance the coding experience by enabling users to directly send snippets of code along with specific questions to chatgpt. this integration aims to provide quick and intelligent responses, facilitating smoother coding workflows.
however i am encountering display issues in sublime text where the formatting of chatgpt's responses is not rendered correctly.
import os
import sublime
import sublime_plugin
import 
import requests
import json

openai_api_key = """"

class sendtogptcommand(sublime_plugin.textcommand):
    def run(self, edit):
        # capture the selected text
        self.selected_text = ''
        for region in self.view.sel():
            if not region.empty():
                self.selected_text = self.view.substr(region)
        
        # prompt the user for additional input
        self.view.window().show_input_panel(""enter your question:"", """", self.on_done, none, none)

    def on_done(self, user_input):
        code = textwrap.dedent(self.selected_text)

        # combine the user's question with the selected text
        data = {
            'code': code,
            'question': user_input
        }

        # send the data as an http request
        self.send_

    def send_ data):
        url = '
        headers = {
            'content-type': 'application/json',
            'authorization': f'bearer {openai_api_key}'
        }

        req = {
            'model': 'gpt-3.5-turbo',
            'messages': [
                {
                    'role': 'system',
                    'content': 'you are a helpful coding assistant.'
                },
                {
                    'role': 'system',
                    'content': f""context code:```{data['code']}```""
                },
                {
                    'role': 'user',
                    'content': f""question: {data['question']}""
                }
            ]
        }

        # initialize result
        result = """"
        
        try:
            response = requests.post(url, headers=headers, data=json.dumps(req))
            response.raise_for_status()  # raise exception if the request was not successful
            response_json = response.json()

            # get the http response and extract the completion
            if 'choices' in response_json:
                choices = response_json['choices']
                messages = []  # list to store messages

                for choice in choices:
                    # extract 'message' from each choice
                    if 'message' in choice:
                        messages.append(choice['message']['content'])
                    else:
                        messages.append(""message field not found in choice"")
                
                # join messages into a single string
                result = "" "".join(messages)

            else:
                result = ""choices field not found in response""

        except (requests.requestexception, json.jsondecodeerror) as err:
            result = f""error occurred during request: {str(err)}""
        
        self.display_result(data['question'], data['code'], result)

    def display_result(self, question, code, result):
        # create a new buffer (tab)
        new_view = self.view.window().new_file()

        # set the syntax to markdown
        new_view.set_syntax_file(""packages/markdown/markdown.sublime-syntax"")
        
        # insert the question, code, and response
        content = f""""""question asked:\n\n{question}\n\n""""""
        new_view.run_command('insert', {'characters': content})

        content = f""""""code:\n{code}\n\n""""""
        new_view.run_command('insert', {'characters': content})

        content = f""""""response:\n{result}\n\n""""""
        new_view.run_command('insert', {'characters': content})

here is the response:
question asked:

lets test this code ?

code:
def display_result(self, question, code, result):
    print(code)
        code = textwrap.dedent(code)
            print(code)
                # create a new buffer (tab)
                    new_view = self.view.window().new_file()

                        # set the syntax to markdown
                            new_view.set_syntax_file(""packages/markdown/markdown.sublime-syntax"")

                                # insert the question, code, and response
                                    content = f""""""question asked:\n\n{question}\n\n""""""
                                        new_view.run_command('insert', {'characters': content})

                                            content = f""""""code:\n{code}\n\n""""""
                                                new_view.run_command('insert', {'characters': content})

                                                    content = f""""""response:\n{result}\n\n""""""
                                                        new_view.run_command('insert', {'characters': content})

                                                        response:
                                                        sure, please provide the code that you would like to test.

                                                        

this is what the response should look like.
question asked:

lets test this code ?

code:
def display_result(self, question, code, result):
    print(code)
    code = textwrap.dedent(code)
    print(code)
    # create a new buffer (tab)
    new_view = self.view.window().new_file()

    # set the syntax to markdown
    new_view.set_syntax_file(""packages/markdown/markdown.sublime-syntax"")

    # insert the question, code, and response
    content = f""""""question asked:\n\n{question}\n\n""""""
    new_view.run_command('insert', {'characters': content})

    content = f""""""code:\n{code}\n\n""""""
    new_view.run_command('insert', {'characters': content})

    content = f""""""response:\n{result}\n\n""""""
    new_view.run_command('insert', {'characters': content})

response:
sure, please provide the code that you would like to test.


im not sure why there are all these extra spaces/tabs.
thanks for the help.","['python', 'sublimetext', 'openai-api', 'sublime-text-plugin']",77906576,"use 'append' instead of 'insert' or turn auto indentation off: new_view.settings().set('auto_indent', false)",https://stackoverflow.com/questions/77906460,python,30-01-2024 12:46,560.0,0.0,1.0,True,30-01-2024 13:08,30-01-2024 13:05
78977010,next.js edge function: module not found error (&#39;path&#39;) and internal server error when using openai integration,"description
i'm encountering an issue where the path module is not being found in my node.js environment. this problem occurs when trying to send message in the chatbox.
error message
module not found: can't resolve 'path'



import trace for requested module:
./node_modules/dotenv/config.js
./src/lib/db/index.ts
./src/app/api/chat/route.ts
./node_modules/next/dist/build/webpack/loaders/next-edge-app-route-loader/index.js?absolutepagepath=c%3a%5cusers%5cdell%5conedrive%5cdesktop%5c100xdevs%5csummarize-my-pdf-ai%5csrc%5capp%5capi%5cchat%5croute.ts&page=%2fapi%2fchat%2froute&appdirloader=bmv4dc1hchatbg9hzgvyp25hbwu9yxbwjtjgyxbpjtjgy2hhdcuyrnjvdxrljnbhz2u9jtjgyxbpjtjgy2hhdcuyrnjvdxrljmfwcfbhdghzpszwywdlugf0ad1wcml2yxrllw5lehqtyxbwlwrpciuyrmfwasuyrmnoyxqlmkzyb3v0zs50cyzhchbeaxi9qyuzqsu1q1vzzxjzjtvdrgvsbcu1q09uzuryaxzljtvdrgvza3rvccu1qzewmhhezxzzjtvdc3vtbwfyaxpllw15lxbkzi1hasu1q3nyyyu1q2fwcczwywdlrxh0zw5zaw9ucz10c3gmcgfnzuv4dgvuc2lvbnm9dhmmcgfnzuv4dgvuc2lvbnm9ann4jnbhz2vfehrlbnnpb25zpwpzjnjvb3reaxi9qyuzqsu1q1vzzxjzjtvdrgvsbcu1q09uzuryaxzljtvdrgvza3rvccu1qzewmhhezxzzjtvdc3vtbwfyaxpllw15lxbkzi1haszpc0rldj10cnvljnrzy29uzmlnugf0ad10c2nvbmzpzy5qc29ujmjhc2vqyxropszhc3nldfbyzwzped0mbmv4denvbmzpz091dhb1dd0mchjlzmvycmvkumvnaw9upsztawrkbgv3yxjlq29uzmlnpwuzmcuzrce%3d&nextconfigoutput=&preferredregion=&middlewareconfig=e30%3d!

environment

node.js version: v18.17.1
framework: next.js
operating system: windows 11

code
route.ts
import { getcontext } from ""@/lib/context"";
import { db } from ""@/lib/db"";
import { chats, messages as _messages } from ""@/lib/db/schema"";
import { openai } from ""@ai-sdk/openai"";
import { streamtext } from ""ai"";
import { eq } from ""drizzle-orm"";
import { nextresponse } from ""next/server"";
import { message } from ""ai/react"";

export const runtime = ""edge"";

export async function post(req: request) {
  console.log(""called api"");

  try {
    const { messages,chatid } = await req.json();
    console.log(""messages:"", messages);
    console.log(""chat id:"", chatid);
    const _chats = await db.select().from(chats).where(eq(chats.id, chatid));
    // log retrieved chats
    console.log(""_chats:"", _chats);
    if (_chats.length != 1) {
      return nextresponse.json({ error: "" chat not found"" }, { status: 404 });
    }
    const filekey = _chats[0].filekey;
    const lastmessage = messages[messages.length - 1];
    // log filekey and lastmessage
    console.log(""file key:"", filekey);
    console.log(""last message:"", lastmessage);
    const context = await getcontext(lastmessage.content, filekey);
    console.log(""context:"", context);
    const prompt = {
      role: ""system"",
      content: `ai assistant is a brand new, powerful, human-like artificial intelligence.
      the traits of ai include expert knowledge, helpfulness, cleverness, and articulateness.
      ai is a well-behaved and well-mannered individual.
      ai is always friendly, kind, and inspiring, and he is eager to provide vivid and thoughtful responses to the user.
      ai has the sum of all knowledge in their brain, and is able to accurately answer nearly any question about any topic in conversation.
      ai assistant is a big fan of pinecone and vercel.
      start context block
      ${context}
      end of context block
      ai assistant will take into account any context block that is provided in a conversation.
      if the context does not provide the answer to question, the ai assistant will say, ""i'm sorry, but i don't know the answer to that question"".
      ai assistant will not apologize for previous responses, but instead will indicated new information was gained.
      ai assistant will not invent anything that is not drawn directly from the context.
      `,
    };
    const response = await streamtext({
      model: openai(""gpt-4o-mini""),
      messages: [
        prompt,
        ...messages.filter((message: message) => message.role === ""user""),
      ],
    });
    return response.todatastreamresponse();
  } catch (error) {
    console.log(error);
    return nextresponse.json(
      { error: ""internal server error"" },
      { status: 500 }
    );
  }
}

chatcomponent.tsx
""use client"";
import react from ""react"";
import { input } from ""./ui/input"";
import { usechat } from ""ai/react"";
import { button } from ""./ui/button"";
import { sendicon } from ""lucide-react"";
import messagelist from ""./messagelist"";

type props = { chatid: number };

const chatcomponent = ({ chatid }: props) => {
  console.log(""chat id in chatcomponent:"", chatid);

  const { input, handleinputchange, handlesubmit, messages } = usechat({
    api: ""/api/chat"",

    body: {
      chatid,
    },
  });
  // react.useeffect(() => {
  //   const messagecontainer = document.getelementbyid(""message-container"");
  //   if (messagecontainer) {
  //     messagecontainer.scrollto({
  //       top: messagecontainer.scrollheight,
  //       behavior: ""smooth"",
  //     });
  //   }
  // }, [messages]);

  return (
    <div
      classname=""relative max-h-screen overflow-scroll""
      id=""message-container""
    >
      {/* header */}
      <div classname=""sticky top-0 inset-x-0 p-2 bg-white h-fit"">
        <h3 classname=""text-xl font-bold"">chat</h3>
      </div>
      {/* message list */}
      <messagelist messages={messages} />
      <form
        onsubmit={handlesubmit}
        classname=""sticky  bottom-0 px-2 py-4 inset-x-0 bg-white""
      >
        <div classname=""flex"">
          <input
            value={input}
            onchange={handleinputchange}
            placeholder=""ask any question...""
            classname=""w-full""
          />
          <button classname=""bg-gradient-to-r from-sky-400 to-blue-500 ml-2"">
            <sendicon classname=""h-4 w-4"" />
          </button>
        </div>
      </form>
    </div>
  );
};

export default chatcomponent;


logs
ps c:\users\dell\onedrive\desktop\100xdevs\summarize-my-pdf-ai> node -v
v18.17.1
ps c:\users\dell\onedrive\desktop\100xdevs\summarize-my-pdf-ai>

this is console get /chat/9?_rsc=a12k2 200 in 227ms
get /chat/8?_rsc=18zah 200 in 383ms
ï¿½ï¿½ï¿½ compiling /api/chat ...
ï¿½ï¿½ï¿½ ./node_modules/dotenv/lib/main.js:2:1
module not found: can't resolve 'path'



import trace for requested module:
./node_modules/dotenv/config.js
./src/lib/db/index.ts
./src/app/api/chat/route.ts
./node_modules/next/dist/build/webpack/loaders/next-edge-app-route-loader/index.js?absolutepagepath=c%3a%5cusers%5cdell%5conedrive%5cdesktop%5c100xdevs%5csummarize-my-pdf-ai%5csrc%5capp%5capi%5cchat%5croute.ts&page=%2fapi%2fchat%2froute&appdirloader=bmv4dc1hchatbg9hzgvyp25hbwu9yxbwjtjgyxbpjt

jgy2hhdcuyrnjvdxrljnbhz2u9jtjgyxbpjtjgy2hhdcuyrnjvdxrljmfwcfbhdghzpszwywdlugf0ad1wcml2yxrllw5lehqtyxbwlwrpciuyrmfwasuyrmnoyxqlmkzyb3v0zs50cyzhchbeaxi9qyuzqsu1q1vzzxjzjtvdrgvsbcu1q09uzuryaxzljtvdrgvza3rvccu1qzewmhhezxzzjtvdc3vtbwfyaxpllw15lxbkzi1hasu1q3nyyyu1q2fwcczwywdlrxh0zw5zaw9ucz10c3gmcgfnzuv4dgvuc2lvbnm9dhmmc2lvbnm9ann4jnbhz2vfehrlbnnpb25zpwpzjnjvb3reaxi9qyuzqsu1q1vzzxjzjtvdrgvsbcu1q09uzuryaxzljtvdrgvza3rvccu1qzewmhhezxzzjtvdc3vtbwfyaxpllw15lxbkzi1haszpc0rldj10cnvljnrzy29uzmlnugf0ad10c2nvbmzpzy5qc29ujmjhc2vqyxropszhc3nldfbyzwzped0mbmv4denvbmzpz091dhb1dd0mchjlzmvycmvkumvnaw9upsztawrkbgv3yxjlq29uzmlnpwuzmcuzrce%3d&nextconfigoutput=&preferredregion=&middlewareconfig=e30%3d!


working code
when using the following simplified code, i am able to get a response from the chatbox:
import { openai } from ""@ai-sdk/openai"";
import { streamtext } from ""ai"";
import { nextresponse } from ""next/server"";

export const runtime = ""edge"";

export async function post(req: request) {
  try {
    const { messages } = await req.json();
    const response = await streamtext({
      model: openai(""gpt-4o-mini""),
      messages,
    });
    return response.todatastreamresponse();
  } catch (error) {
    console.log(error);
    return nextresponse.json(
      { error: ""internal server error"" },
      { status: 500 }
    );
  }
}


this should provide a clear context for the issue and indicate that the problem might be related to the configuration or dependencies rather than the core functionality of fetching chat responses.
steps to reproduce

run the application with the provided route.ts and chatcomponent.tsx code.
observe the error when attempting to start or compile the project.

i attempted to implement a chat functionality in my next.js application using the streamtext function from the ai package and the openai model. specifically, i used the following code to handle post requests and fetch chat responses:
import { openai } from ""@ai-sdk/openai"";
import { streamtext } from ""ai"";
import { nextresponse } from ""next/server"";

export const runtime = ""edge"";

export async function post(req: request) {
  try {
    const { messages } = await req.json();
    const response = await streamtext({
      model: openai(""gpt-4o-mini""),
      messages,
    });
    return response.todatastreamresponse();
  } catch (error) {
    console.log(error);
    return nextresponse.json(
      { error: ""internal server error"" },
      { status: 500 }
    );
  }
}

i expected this implementation to correctly process incoming messages and return a valid response from the chatbox.
what actually resulted?
although the code runs without errors and returns a response in a simplified setup, i encountered issues with the more complex implementation, which includes additional logic such as database interactions and context handling. in those cases, i am receiving a 500 internal server error and the response is not as expected. the simplified code works as intended and provides the expected chat responses.","['next.js', 'path', 'chatbot', 'openai-api', 'vercel-ai']",78977220,"i removed this and it worked!!!!!
export const runtime = ""edge"";",https://stackoverflow.com/questions/78977010,next.js,12-09-2024 07:38,121.0,0.0,1.0,True,12-09-2024 08:30,12-09-2024 07:57
77766747,how to display spacy named entities in plain text?,spacy visualizers render named entities into html or svg.  is there any easy way to get quick debug output in plaintext that looks good?  i'm using spacy 3.7.2 and python 3.11.,"['spacy', 'displacy']",77766874,"i ended up making a simple utility function:
def char_span_string(span: span, extra: str = """") -> str:
    return (
            (span.start_char * "" "")
            + ((span.end_char - span.start_char) * ""-"")
            + "" ""
            + span.label_
            + "" ""
            + str(span.start_char)
            + ""..""
            + str(span.end_char)
            + extra
    )

for ent in doc.ents:
    print(char_span_string(ent))

example output:
3 tablespoons minced scallions
  ----------- unit 2..13
                     --------- ingredient 21..30",https://stackoverflow.com/questions/77766747,spacy,05-01-2024 19:19,119.0,0.0,2.0,True,05-01-2024 19:48,05-01-2024 19:48
53023382,print all the tokens in the file that are labelled with the morphological tag,"i want to print all the tokens which are labellad with the morphological tag in a file. so far i wrote the code shown below.
def index(filepath, string):

    import re
    pattern = re.compile(r'(\w+)+')
    stringlist = []
    stringlist.append(string)

    with open(filepath) as f:
        for lineno, line in enumerate(f, start=1):
            words = set(m.group(1) for m in pattern.finditer(line))
            matches = [keyword for keyword in stringlist if keyword in words]
            if matches:
                result = ""{:<15} {}"".format(','.join(matches), lineno)
                print(result)

    stringlist.clear()



index('deneme.txt', '+noun')

the output is like this, i can find the noun in the token and the line number but can't print the part which i wanted. i only want the word part which is before + sign.
noun            1
noun            2
noun            3
noun            4
noun            5
noun            6
noun            7

the lines in my file is like this:
tï¿½ï¿½rkiye+noun ,+punc terï¿½ï¿½rizm+noun+gen ve+conj kitle+noun imha+noun silah+noun+a3pl+p3sg+gen kï¿½ï¿½resel+adj dï¿½ï¿½zey+noun+loc oluï¿½ï¿½+verb+caus+pastpart+p3sg tehdit+noun+gen boyut+noun+p3sg karï¿½ï¿½ï¿½ï¿½+adj+p3sg+loc ,+punc tï¿½ï¿½m+det ï¿½ï¿½lke+noun+a3pl+gen yay+verb+pass+inf2+gen ï¿½ï¿½nle+verb+pass+inf2+p3sg hedef+noun+a3pl+p3sg+acc paylaï¿½ï¿½+verb+pastpart+p3pl ,+punc daha+noun gï¿½ï¿½ven+noun+with ve+conj istikrar+noun+with bir+num dï¿½ï¿½nya+noun dï¿½ï¿½zen+noun+p3sg iï¿½ï¿½it+p3pl bir+num aï¿½ï¿½ama+noun+dat gel+verb+pass+inf2+p3sg+acc samimi+adj ol+verb+bydoingso arzula+verb+prog2+cop .+punc 
tï¿½ï¿½rkiye+noun+gen ekonomik+adj ve+conj insani+adj potansiyel+noun+p3sg ,+punc gï¿½ï¿½ï¿½ï¿½+noun+with savun+verb+inf2 kapasite+noun+p3sg ,+punc ulus+noun+a3pl+inbetween ï¿½ï¿½atï¿½ï¿½ï¿½ï¿½+verb+inf2+a3pl+gen ï¿½ï¿½nle+verb+pass+inf2+p3sg ve+conj barï¿½ï¿½ï¿½ï¿½+noun+p3sg inï¿½ï¿½a+noun ï¿½ï¿½aba+noun+a3pl+p3sg+dat aktif+adj katï¿½ï¿½lï¿½ï¿½m+noun+p3sg+gen yanï¿½ï¿½sï¿½ï¿½ra+postppcgen ,+punc fark+noun+with kï¿½ï¿½ltï¿½ï¿½r+noun ve+conj gelenek+noun+a3pl+dat ait+postppcdat seï¿½ï¿½kin+adj ï¿½ï¿½zellik+noun+a3pl+acc birleï¿½ï¿½+verb+caus+prespart bir+num bï¿½ï¿½nye+noun+dat sahip+noun ol+verb+inf2+p3sg ,+punc kendi+pron+p3sg bï¿½ï¿½lge+noun+p3sg+loc ve+conj ï¿½ï¿½te+noun+p3sg+loc ï¿½ï¿½nem+noun+with rol+noun oyna+verb+inf2+p3sg+acc saï¿½ï¿½la+verb+fut deï¿½ï¿½er+noun+with ï¿½ï¿½zellik+noun+a3pl+cop .+punc 
tï¿½ï¿½rkiye+noun ,+punc bu+det ï¿½ï¿½nem+noun+with katkï¿½ï¿½+noun+acc yap+verb+pl iï¿½ï¿½in+postppcgen daha+noun i+noun+acc bir+num dï¿½ï¿½nya+noun oluï¿½ï¿½+verb+caus+inf1 amaï¿½ï¿½+noun+p3sg+ins ,+punc dost+noun+a3pl+p3pl ve+conj mï¿½ï¿½ttefik+adj+a3pl+p3sg+ins yakï¿½ï¿½n+noun bir+num biï¿½ï¿½im+noun+loc ï¿½ï¿½alï¿½ï¿½ï¿½ï¿½+verb+inf2+dat devam+noun et+verb+fut+cop .+punc 
ab+noun ile+postppcnom gï¿½ï¿½mrï¿½ï¿½k+noun alan+noun+p3sg+loc+rel kurumsal+adj iliï¿½ï¿½ki+noun+a3pl 
club+noun toplantï¿½ï¿½+noun+a3pl+p3sg 
tï¿½ï¿½rkiye+noun -+punc at+noun gï¿½ï¿½mrï¿½ï¿½k+noun iï¿½ï¿½birlik+noun+p3sg komite+noun+p3sg ,+punc ankara+noun anlaï¿½ï¿½ma+noun+p3sg+gen 6+num madde+noun+p3sg uyar+verb+when ortaklï¿½ï¿½k+noun rejim+noun+p3sg+gen uygula+verb+pass+inf2+p3sg+acc ve+conj geliï¿½ï¿½+verb+inf2+p3sg+acc saï¿½ï¿½la+verb+inf1 ï¿½ï¿½zere+postppcnom ortaklï¿½ï¿½k+noun konsey+noun+p3sg+gen 2+num /+punc 69+num sayï¿½ï¿½lï¿½ï¿½+adj karar+noun+p3sg ile+conj teknik+noun komite+noun mahiyet+noun+p3sg+loc kur+verb+pass+narr+cop .+punc 
club+noun toplantï¿½ï¿½+noun+a3pl+p3sg 
nispi+adj 
nisbi+adj 
gï¿½b+orexample when i write +adj i want to get all the tokens which include +adj (nispi, izafi .... (forexample)).","['python', 'regex', 'nlp', 'morphological-analysis']",53024784,"i think, your concept how to use regexes needs some improvement.
note that each input line contains a number of ""tokens"", e.g. terï¿½ï¿½rizm+noun+gen.
as you can see, it contains:

the first word - actual word from text,
a number of classification symbols, each preceded with a + char.

so:

each line should be split into tokens, on a sequence of blank chars,
each token should be split into words, on + char,
the first from these words is ""actual"" word,
the remaining words (without +) are classification symbols.

a good habit it to strip the terminating blank chars (at least \n).
note also that your code contains stringlist, so you are aware of the
case that this function may look for one or more of multiple
classification words.
i programmed it a slightly different way:

the second parameter (lookfor) is a list of words, which is
converted into a set (lookforset).
the set of words (result of splitting of a token, minus the first word)
is also converted into a set.

the decision whether to print a word (the first word from a token) is based on
whether at least one of its classification symbols can be found in lookforset.
to put it another way - whether lookforset and wordset have some
common elements (set intersection).
so the whole script can look like below:
import re

def index(filename, lookfor):
    lookforset = set(lookfor)  # set of classification symbols to look for
    pat1 = re.compile(r'\s+')  # regex to split line into tokens
    pat2 = re.compile(r'\+')   # regex to split a token into words
    with open(filename) as f:
        for lineno, line in enumerate(f, start=1):
            line = line.rstrip()
            tokens = pat1.split(line)
            for token in tokens:
                words = pat2.split(token)
                word1 = words.pop(0)  # initial word
                wordset = set(words)  # classification words
                commonwords = lookforset.intersection(wordset)
                if commonwords:
                    print(""{:3}: {:<15} {}"".format(lineno, word1, ', '.join(commonwords)))

index('lines.txt', ['noun', 'gen'])

a piece of output from it, for my input data (slightly shortened version of your)
is like below:
1: tï¿½ï¿½rkiye         noun
1: terï¿½ï¿½rizm        noun, gen
1: kitle           noun
1: imha            noun
2: tï¿½ï¿½rkiye         noun, gen
2: potansiyel      noun

it contains:

the number of source line,
the fist word of a token,
which classification words from lookfor have been found in this token.",https://stackoverflow.com/questions/53023382,python,27-10-2018 15:20,319.0,-1.0,2.0,True,09-10-2024 21:24,09-10-2024 21:24
21552518,using scikit-learn vectorizers and vocabularies with gensim,"i am trying to recycle scikit-learn vectorizer objects with gensim topic models. the reasons are simple: first of all, i already have a great deal of vectorized data; second, i prefer the interface and flexibility of scikit-learn vectorizers; third, even though topic modelling with gensim is very fast, computing its dictionaries (dictionary()) is relatively slow in my experience.
similar questions have been asked before, especially here and here, and the bridging solution is gensim's sparse2corpus() function which transforms a scipy sparse matrix into a gensim corpus object.
however, this conversion does not make use of the vocabulary_ attribute of sklearn vectorizers, which holds the mapping between words and feature ids. this mapping is necessary in order to print the discriminant words for each topic (id2word in gensim topic models, described as ""a a mapping from word ids (integers) to words (strings)"").
i am aware of the fact that gensim's dictionary objects are much more complex (and slower to compute) than scikit's vect.vocabulary_ (a simple python dict)...
any ideas to use vect.vocabulary_ as id2word in gensim models?
some example code:
# our data
documents = [u'human machine interface for lab abc computer applications',
        u'a survey of user opinion of computer system response time',
        u'the eps user interface management system',
        u'system and human system engineering testing of eps',
        u'relation of user perceived response time to error measurement',
        u'the generation of random binary unordered trees',
        u'the intersection graph of paths in trees',
        u'graph minors iv widths of trees and well quasi ordering',
        u'graph minors a survey']

from sklearn.feature_extraction.text import countvectorizer
# compute vector space with sklearn
vect = countvectorizer(min_df=1, ngram_range=(1, 1), max_features=25000)
corpus_vect = vect.fit_transform(documents)
# each doc is a scipy sparse matrix
print vect.vocabulary_
#{u'and': 1, u'minors': 20, u'generation': 9, u'testing': 32, u'iv': 15, u'engineering': 5, u'computer': 4, u'relation': 28, u'human': 11, u'measurement': 19, u'unordered': 37, u'binary': 3, u'abc': 0, u'for': 8, u'ordering': 23, u'graph': 10, u'system': 31, u'machine': 17, u'to': 35, u'quasi': 26, u'time': 34, u'random': 27, u'paths': 24, u'of': 21, u'trees': 36, u'applications': 2, u'management': 18, u'lab': 16, u'interface': 13, u'intersection': 14, u'response': 29, u'perceived': 25, u'in': 12, u'widths': 40, u'well': 39, u'eps': 6, u'survey': 30, u'error': 7, u'opinion': 22, u'the': 33, u'user': 38}

import gensim
# transform sparse matrix into gensim corpus
corpus_vect_gensim = gensim.matutils.sparse2corpus(corpus_vect, documents_columns=false)
lsi = gensim.models.lsimodel(corpus_vect_gensim, num_topics=4)
# i instead would like something like this line below
# lsi = gensim.models.lsimodel(corpus_vect_gensim, id2word=vect.vocabulary_, num_topics=2)
print lsi.print_topics(2)
#['0.622*""21"" + 0.359*""31"" + 0.256*""38"" + 0.206*""29"" + 0.206*""34"" + 0.197*""36"" + 0.170*""33"" + 0.168*""1"" + 0.158*""10"" + 0.147*""4""', '0.399*""36"" + 0.364*""10"" + -0.295*""31"" + 0.245*""20"" + -0.226*""38"" + 0.194*""26"" + 0.194*""15"" + 0.194*""39"" + 0.194*""23"" + 0.194*""40""']","['python', 'scikit-learn', 'topic-modeling', 'gensim']",21553171,"gensim doesn't require dictionary objects. you can use your plain dict as input to id2word directly, as long as it maps ids (integers) to words (strings).
in fact anything dict-like will do (including dict, dictionary, sqlitedict...).
(btw gensim's dictionary is a simple python dict underneath.
not sure where your remarks on dictionary performance come from, you can't get a mapping much faster than a plain dict in python. maybe you're confusing it with text preprocessing (not part of gensim), which can indeed be slow.)",https://stackoverflow.com/questions/21552518,python,04-02-2014 12:25,10277.0,21.0,6.0,True,24-05-2022 06:30,23-05-2017 12:26
76969521,"nlp, how to fix that pretrained model paraphrase-multilingual-mpnet-base-v2 isn&#39;t accurate on some examples?","i use sentence-transformer model paraphrase-multilingual-mpnet-base-v2 from huggingface:

my target is to find similarity between phrases.
i launch the model exactly as noted at the model page:
from sentence_transformers import sentencetransformer
sentences = [""this is an example sentence"", ""each sentence is converted""]
model = sentencetransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')
embeddings = model.encode(sentences)

there is a problem that model counts some phrases very similar although they really aren't. in most cases the model works fine, but sometimes it is reaaly wrong.
i see that the model is pretrained and i don't know how to retrain it adding some new examples slightly change the weights.
is it possible to add a few new examples to model and retrain it if the model is pretrained? will it need full model train process probably with gpu?
or how model errors could be fixed in other way?
or the only way is to find another model that would work better?","['python', 'machine-learning', 'nlp', 'huggingface-transformers']",76969658,"what you're looking for is called fine-tuning and hugging face does provide a trainer api:  
the general steps are from the above tutorial (i'll be using the bert-base-cased pre-trained model and the yelp_review_full dataset to fine-tune, both can be found on huggingface, as well as pytorch trainer for the actual fine-tuning).
# logging
import logging
# used to load the dataset
from datasets import load_dataset
# tokenizer, to be used on dataset
from transformers import autotokenizer
# gets the model
from transformers import automodelforsequenceclassification
# contains hyperparameters and the trainer
from transformers import trainingarguments, trainer
# used in evaluation
import numpy as np
import evaluate

# init the logger
logger_file_handler = rotatingfilehandler(u'./retrain.log')
logger_file_handler.setlevel(logging.debug)
logging.capturewarnings(true)
root_logger = logging.getlogger()
#root_logger.addhandler(logger_file_handler)
root_logger.setlevel(logging.debug)

# load the dataset to use for fine-tuning
dataset = load_dataset(""yelp_review_full"")
# log an example of what we have
logging.debug(dataset[""train""][100])

# create tokenizer
tokenizer = autotokenizer.from_pretrained(""bert-base-cased"")

the map function requires a function which it uses on every item, in this case we want to tokenize only the ""text"" key in each item.  the logging statement above will show 2 keys: ""label"" and ""text"".  to keep things simple we don't want the label, but we could use it and concat the text onto the end of it.
def tokenize_function(examples):
    return tokenizer(examples[""text""], padding=""max_length"", truncation=true)

tokenized_datasets = dataset.map(tokenize_function, batched=true)

# we split the data to reduce retrain time, this isn't necessary however
small_train_dataset = tokenized_datasets[""train""].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets[""test""].shuffle(seed=42).select(range(1000))

# load up the bert-base-cased model
model = automodelforsequenceclassification.from_pretrained(""bert-base-cased"", num_labels=5)

as per the tutorial, the above line to load the model will display a warning:

you will see a warning about some of the pretrained weights not being used and some weights being randomly initialized. donï¿½ï¿½ï¿½t worry, this is completely normal! the pretrained head of the bert model is discarded, and replaced with a randomly initialized classification head. you will fine-tune this new model head on your sequence classification task, transferring the knowledge of the pretrained model to it.

# we want to evaluate our performance based on accuracy, but others could be used like precision or perplexity.  check the tutorial for links to more information on both.
metric = evaluate.load(""accuracy"")

# define function to compute the metrics (transformers return logits so input will beredictions converted to logits.
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

here we create the class that contains all hyperparameters. output_dir says where to save checkpoints from training and evaluation_strategy sets when to report evaluation metrics, in this case we want to do this at the end of every epoch
training_args = trainingarguments(output_dir=""test_trainer"", evaluation_strategy=""epoch"")

# we create the trainer object using the model, training args,training dataset, test dataset and the function used to compute metrics
trainer = trainer(
model=model,
args=training_args,
train_dataset=small_train_dataset,
eval_dataset=small_eval_dataset,
compute_metrics=compute_metrics,

)
# finally, we call train to start the fine-tuning
trainer.train()

while this covers the basics of fine-tuning there are many ways to optimize and customize every step in the process.  for a tutorial with an indepth look into things like the tokenizer and preprocessing, adding layers ontop of tasks, batch/distributed training, hyperparameter customization, experimentation and much more you can find it over at",https://stackoverflow.com/questions/76969521,python,24-08-2023 12:57,1017.0,0.0,1.0,True,28-08-2023 13:58,24-08-2023 21:53
64284835,replace personal pronoun with previous person mentioned (noisy coref),"i want to do a noisy resolution such that given a personal prounoun, that pronoun is replace by the previous(nearest) person.
for example:
alex is looking at buying a u.k. startup for $1 billion. he is very confident that this is going to happen. sussan is also in the same situation. however, she has lost hope.
the output is:
alex is looking at buying a u.k. startup for $1 billion. alex is very confident that this is going to happen. sussan is also in the same situation. however, susan has lost hope.
another example,
peter is a friend of gates. but gates does not like him. 
in this case, the output would be :
peter is a friend of gates. but gates does not like gates.
yes! this is super noisy.
using spacy:
i have extracted the person using ner, but how can i replace pronouns appropriately?
code:
import spacy
nlp = spacy.load(""en_core_web_sm"")
for ent in doc.ents:
  if ent.label_ == 'person':
    print(ent.text, ent.label_)","['python', 'python-3.x', 'nlp', 'spacy', 'coreference-resolution']",64288809,"i have written a function that works for your two examples:
consider using a larger model such as en_core_web_lg for more accurate tagging.
import spacy
from string import punctuation

nlp = spacy.load(""en_core_web_lg"")

def pronoun_coref(text):
    doc = nlp(text)
    pronouns = [(tok, tok.i) for tok in doc if (tok.tag_ == ""prp"")]
    names = [(ent.text, ent[0].i) for ent in doc.ents if ent.label_ == 'person']
    doc = [tok.text_with_ws for tok in doc]
    for p in pronouns:
        replace = max(filter(lambda x: x[1] < p[1], names),
                      key=lambda x: x[1], default=false)
        if replace:
            replace = replace[0]
            if doc[p[1] - 1] in punctuation:
                replace = ' ' + replace
            if doc[p[1] + 1] not in punctuation:
                replace = replace + ' '
            doc[p[1]] = replace
    doc = ''.join(doc)
    return doc",https://stackoverflow.com/questions/64284835,python,09-10-2020 17:40,1786.0,4.0,3.0,True,03-04-2024 04:44,10-10-2020 10:18
3182268,nltk and language detection,"how do i detect what language a text is written in using nltk?
the examples i've seen use nltk.detect, but when i've installed it on my mac, i cannot find this package.","['python', 'nlp', 'nltk', 'detection']",3384659,"have you come across the following code snippet?
english_vocab = set(w.lower() for w in nltk.corpus.words.words())
text_vocab = set(w.lower() for w in text if w.lower().isalpha())
unusual = text_vocab.difference(english_vocab) 

from 
or the following demo file?",https://stackoverflow.com/questions/3182268,python,05-07-2010 21:30,61015.0,43.0,5.0,True,13-02-2023 15:16,27-09-2017 09:30
71792841,how to get the dimensions of a word2vec vector?,"i have run a word2vec model on my data list_of_sentence:
from gensim.models import word2vec

w2v_model=word2vec(list_of_sentence,min_count=5, workers=4)

print(type(w2v_model))

<class 'gensim.models.word2vec.word2vec'>

i would like to know the dimensionality of w2v_model vectors. how can i check it?","['python', 'machine-learning', 'nlp', 'gensim', 'word2vec']",71793855,"the vector dimensionality is included as an argument in word2vec:

in gensim versions up to 3.8.3, the argument was called size (docs)
in the latest gensim versions (4.0 onwards), the relevant argument is renamed to vector_size (docs)

in both cases, the argument has a default value of 100; this means that, if you do not specify it explicitly (as you do here), the dimensionality will be 100.
here is a reproducible example using gensim 3.6:
import gensim
gensim.__version__
# 3.6.0

from gensim.test.utils import common_texts
from gensim.models import word2vec

model = word2vec(sentences=common_texts, window=5, min_count=1, workers=4) # do not specify size, leave the default 100

wv = model.wv['computer']  # get numpy vector of a word in the corpus
wv.shape # verify the dimension of a single vector is 100
# (100,)

if you want to change this dimensionality to, say, 256, you should call word2vec with the argument size=256 (for gensim versions up to 3.8.3) or vector_size=256 (for gensim versions 4.0 or later).",https://stackoverflow.com/questions/71792841,python,08-04-2022 06:53,2019.0,1.0,1.0,True,22-04-2022 02:03,22-04-2022 02:03
73204460,how to pretrain bart using custom dataset(not fine tuning!!),"i'm using huggingface transformers 4.19.0
i want to pretrain bart model using my custom dataset.
to make it clear, i'm not asking about fine tuning bart to down stream task but asking about ""pre training bart"".
but i can't find method or class for this job on huggingface docs page (
is it impossible to pretrain bart using transformers package?
do i have to make bart model layer by layer from the scratch?
if anyone knows how to pretrain bart model using custom data please help me...","['huggingface-transformers', 'pre-trained-model', 'huggingface', 'bart']",73238126,"you need to initialize a random model with the architecture of your choice.
from transformers import bartconfig, bartmodel

configuration = bartconfig() # default bart config
model = bartmodel(configuration) # default randomly initialised bart

then you need to train said model, the easiest way being using a trainer (doc), to which you provide your model, training sets, evaluation sets, etc.",https://stackoverflow.com/questions/73204460,huggingface-transformers,02-08-2022 08:40,2069.0,1.0,1.0,True,04-08-2022 14:58,02-08-2022 08:42
72841219,rule based classification based on a dictionary,"apologies in advance if this has already been questioned/answered, but i couldn't find any answer close to my problem. i am also somewhat new to python, so sorry for any wrong formatting or wording.  i am struggling with a task to label/classify a list of documents based on a list of dictionary.
the list of dictionary looks like this:
| category | keywords       |
| -------- | -------------- |
| reg      | a, b, c        |
| gov      | d,e,f,g        |


document1 = [sentences1, sentences 2,...]
document2 = [sentences3, sentences 4,...]

i would like to label documents per category if keywords belong to this category appear in the document. i have searched through stackoverflow and github but have not found similar problem.","['python', 'text-classification', 'topic-modeling']",72841462,"your question seems to ask this:

for a collection of documents, each with multiple sentences, mark each document as belonging to one or more categories if, for any of the given categories, one or more sentences in the document contains at least one keyword from the category.

here is a way to do that:
categories = {'reg':['a', 'b', 'c'], 'gov':['d', 'e', 'f', 'g']}
document1 = ['a quick brown fox.', 'he got a b in calculus.']
document2 = ['my next job will be in the c suite.', 'you can get there on the d train.']
docs = {'doc1':document1, 'doc2':document2}
docsbycategory = {}
for doc, sentences in docs.items():
    cats = set()
    for sentence in sentences:
        words = sentence.split()
        for category, keywords in categories.items():
            if any(keyword in words for keyword in keywords):
                cats.add(category)
    docsbycategory[doc] = list(cats)
print(docsbycategory)

output:
{'doc1': ['reg'], 'doc2': ['gov', 'reg']}

explanation:

iterate over the document names and contents using docs.items()
iterate over the list of sentences the make up each document's contents
split each sentence into words
iterate over the category names and keywords using categories.items()
use any() with a comprehension (loop) over keywords to see if any of them are found in the sentence's words, in which case add the category name to the variable cats which is a set, so that multiple calls to add() for the same category will result in only a single entry in cats for that category
convert cats to a list and add it to the result dictionary docsbycategory for the current document name.

an alternative solution is this:
categories = {'reg':['a', 'b', 'c'], 'gov':['d', 'e', 'f', 'g']}
document1 = ['a quick brown fox.', 'he got a b in calculus.']
document2 = ['my next job will be in the c suite.', 'you can get there on the d train.']
docs = {'doc1':document1, 'doc2':document2}
docsbycategory = {}
for doc, sentences in docs.items():
    sentencewordsets = [set(sentence.split()) for sentence in sentences]
    cats = set()
    for category, keywords in categories.items():
        keywordset = set(keywords)
        if any(wordset & keywordset for wordset in sentencewordsets):
            cats.add(category)
    docsbycategory[doc] = list(cats)
print(docsbycategory)",https://stackoverflow.com/questions/72841219,python,02-07-2022 17:40,822.0,1.0,1.0,True,02-07-2022 18:34,02-07-2022 18:28
75840329,it&#39;s a good idea in spacy use set_extension into new component function?,"i'm following the spacy nlp course. in the chapter 3 section 12, there is an example for adding a new component to a blank pipeline and, also they add an attribute extension with the set_extension function, but they add the attribute out of the component function.
it,s not a good idea set a new extension in a new component function?
i've tested the idea:
import json
import spacy
from spacy.language import language
from spacy.tokens import span
from spacy.matcher import phrasematcher

with open(""countries.json"", encoding=""utf8"") as f:
    countries = json.loads(f.read())

with open(""capitals.json"", encoding=""utf8"") as f:
    capitals = json.loads(f.read())

nlp = spacy.blank(""en"")
matcher = phrasematcher(nlp.vocab)
matcher.add(""country"", list(nlp.pipe(countries)))


def is_country(span):
    return span.label_==""country""

@language.component(""countries_component"")
def countries_component(doc):
    #add de attribute extension
    span.set_extension(""capital"",default=none, validate = ""is_country"", force = true)
    #creates an span wit ""country"" label for all matches
    matches = matcher(doc)
    ents = []
    for id, start, end in matches:
      #marc the span with the label country
      span_country = span(doc, start, end, label=""country"")
      #save the capital in the new attribute
      span_country._.capital = capitals.get(span_country.text)
      ents.append(span_country)

    doc.ents = tuple(ents)
    return doc

# adding component to pipeline
nlp.add_pipe(""countries_component"")
print(nlp.pipe_names)

in my version, i'm setting the attribute extension in the component. but without the force  = true, the component generates an error when creating a new second and later doc because it detects there already is an ._.capital atribute seted. it's solved setting the force = true argument. but it has made me doubt.",['spacy'],75855439,"it's fine to do this.
instead of force=true, another option is to check if it's already been set and skip otherwise:
if not span.has_extension(""capital""):
    span.set_extension(...)

in contrast to force=true, you don't know for sure whether ""capital"" is your version with your defaults+validation, since it could technically have been set by another user component/library, but in most applications this isn't a concern and it could be a bit faster.",https://stackoverflow.com/questions/75840329,spacy,25-03-2023 07:55,362.0,0.0,1.0,True,19-07-2023 17:13,19-07-2023 17:13
76651059,how does placing the output (word) labels on the initial transitions of the words in an fst lead to effective composition?,"i am going through hbka.pdf (wfst paper). 
a wfst figure for reference
here the input label i, the output label o, and weight w of a transition are marked on the corresponding directed arc by i: o/w.
it does not make sense as to how a transducer can output the entire word at the initial transition itself. if the entire word was outputted at the final transition, it is sensible to me.
later i saw the following in page 19,
""in order for this transducer to efficiently compose with g, the output (word) labels must be placed on the initial transitions of the words; other locations would lead to delays in the composition matching, which could consume significant time and space.""
chatgpt answers that ""placing the output labels on the initial transitions of the words in the word bigram transducer enables more efficient composition with another transducer by optimizing the matching and combination of transitions.""
but how exactly does it happen?
""placing the output labels on the initial transitions ensures that the word transitions in the word bigram transducer align directly with the transitions in the other transducer.""
but still, the entire word which the finite state transducer has to figure out using phones as input symbols like d,ey,dx,ax, how can it be the output of the initial transition?","['nlp', 'speech-recognition', 'state-machine', 'finite-state-automaton', 'automatic-speech-recognition']",76652173,"as far as i understand it, while the output is determined in the first transition, it is only actually produced once a final state is reached. so in a way the output is hypothesised, and subsequent transitions are used to test the hypothesis. if no final state is reached, the output so far is discarded.
on advantage is that multiple paths with identical output (the upper path in your example image), do not have to repeat the output in every final transition. also, if you have inputs with similar endings you can merge the paths later; which might make the fst more efficient. imaging how many english words end in /ing/ or /ed/ or /s/ -- these can all point to the same identical final states, but not if the output is generated at the end.
i guess a further reason is that this makes it easier to manipulate the fst when it is combined with other fsts. it is always easier to push the output generation further backwards if you merge two fsts, rather than deal with it when it is already at the end of the path.",https://stackoverflow.com/questions/76651059,nlp,10-07-2023 06:11,30.0,0.0,1.0,True,10-07-2023 08:53,10-07-2023 08:02
75148919,how to extract different patterns in string in r?,"i want to extract a pattern of phrases from the following sentences.
text1 <- ""on a year-on-year basis, the number of subscribers of netflix increased 1.15% in november last year.""

text2 <- ""there is no confirmed audited number of subscribers in the netflix's earnings report.""

text3 <- ""netflix's unaudited number of subscribers has grown more than 1.50% at the last quarter.""


the pattern is number of subscribers or audited number of subscribers or unaudited number of subscribers.
i am using the following pattern \\bnumber\\s+of\\s+subscribers?\\b from a previous problem (thanks to @wiktor-stribiï¿½ï¿½ew) and then extracting the phrases.
find_words <- function(text){
  
  pattern <- ""\\bnumber\\s+of\\s+subscribers?\\b"" # something like this

  str_extract(text, pattern)

}

however, this extracts the exact number of subscribeode> not the other patterns.
desired output:
find_words(text1)

'number of subscribers'

find_words(text2)

'audited number of subscribers'

find_words(text3)

'unaudited number of subscribers'","['r', 'regex', 'string', 'nlp']",75149017,"see if this works
find_words <- function(text){

pattern <- ""(audited |unaudited )?number\\s+of\\s+subscribers""

str_extract(text, pattern)

}

you can test it with the sample texts you provided:
find_words(text1)
# 'number of subscribers'
find_words(text2)
# 'audited number of subscribers'
find_words(text3)
# 'unaudited number of subscribers'",https://stackoverflow.com/questions/75148919,r,17-01-2023 15:48,46.0,1.0,1.0,True,17-01-2023 15:55,17-01-2023 15:55
74494620,spacy doc.char_span raises error whenever there is any number in string,"i was trying to train a model from spacy. i have strings and their token offsets saved into the json file.
i have read that file using utf-8 encoding and there is no special character in it. but it raises typeerror: object of type 'nonetype' has no len()
# code for reading file
with open(""data/results.json"", ""r"", encoding=""utf-8"") as file:
    training_data = json.loads(file.read())

i have also tried changing alignment_type from strict to contract & expand. the expand works but shows incorrect spans.
span = doc.char_span(start, end, label, alignment_mode=""contract"")

the code that i'm using
import spacy
from spacy.tokens import docbin

nlp = spacy.blank(""en"")
db = docbin()
training_dataset = [[
        ""department of chemistry,central university of las villas,santa clara,villa clara,54830,cuba."",
        [
            [
                57,
                68,
                ""city_name""
            ],
            [
                87,
                91,
                ""country_name""
            ]
        ]
    ]]
for text, annotations in training_dataset:
    doc = nlp(text)
    ents = []
    for start, end, label in annotations:
        span = doc.char_span(start, end, label)
        ents.append(span)
    doc.ents = ents
    db.add(doc)

i have pasted the json object that is read from the file, directly into the program for debugging purposes.
when i tried after removing the 54830, part, the program runs successfully.
i have also referred to this issue, but that issue has a special character. but this string doesn't have any special character.
can anyone know why this is happening with all strings that contain a number in them?","['python', 'json', 'nlp', 'spacy', 'spacy-3']",74503239,"the error typeerror: object of type 'nonetype' has no len() occurs in line doc.ents = ents when one of the entries in ents is none.
the reason for having a none in the list is that doc.char_span(start, end, label) returns none when the start and end provided don't align with token boundaries.
the tokenizer of the model (spacy.blank(""en"")) doesn't behave as needed for this use case. it seems that it doesn't produce an end of token after a comma  that follows a number without space after the comma.
examples:
tokenizing a number with decimals:
>>> import spacy
>>> nlp = spacy.blank(""en"")
>>> nlp.tokenizer.explain(""5,1"")
[('token', '5,1')]

one single token.
tokenizing a number + comma + letter:
>>> nlp.tokenizer.explain(""5,a"")
[('token', '5,a')]

one single token.
tokenizing a letter + comma + letter:
>>> nlp.tokenizer.explain(""a,a"")
[('token', 'a'), ('infix', ','), ('token', 'a')]

three tokens.
tokenizing a number + comma + space + letter:
>>> nlp.tokenizer.explain(""5, a"")
[('token', '5'), ('suffix', ','), ('token', 'a')]

three tokens.
tokenizing a number + comma + space + number:
>>> nlp.tokenizer.explain(""5, 1"")
[('token', '5'), ('suffix', ','), ('token', '1')]

three tokens.
therefore, with the default tokenizer, a space is needed after a comma following a number so the comma is used to create the token boundaries.
workarounds:

preprocess your text to add a space after the commas you desire to split tokens by. this would also require to update the start and end values of the annotations.
create your custom tokenizer as described in spacy documentation:",https://stackoverflow.com/questions/74494620,python,18-11-2022 19:45,1268.0,2.0,2.0,True,21-08-2024 16:36,18-11-2022 19:54
75277690,getting a weird behaviour when using matcher from spacy several times,"i would like to use matcher from spacy on a list of span (sents)
class chunker:
    def __init__(self, nlp, matcher):
        self.nlp = nlp
        self.matcher = matcher
        self.matcher.add(""np"", np_pattern, on_match=self.on_match_callback, greedy=""longest"")
        self.matcher.add(""vp"", vp_pattern, on_match=self.on_match_callback, greedy=""longest"")
        self.matcher.add(""vvp"", vvp_pattern, on_match=self.on_match_callback, greedy=""longest"")

    def on_match_callback(self, matcher, doc, i, matches):
        match_id, start, end = matches[i]
        string_id = self.nlp.vocab.strings[match_id]
        span = doc[start:end]
        print(""("", span, "")"")
        self.phrase[string_id].append(span)

    def chunk(self, text):
        self.phrases = []
        doc = self.nlp(text)
        sents = list(doc.sents)
        for sent in sents:
            self.phrase = {
                ""np"": [],
                ""vp"": [],
                ""vvp"": []
            }
            self.phrases.append(self.phrase)
            print(""["", sent, ""]"")
            self.matcher(sent)

            for phrase in self.phrase.values():
                phrase.sort(key=lambda x: x.start)

        return self.phrases

nlp = spacy.load(""en_core_web_sm"")
matcher = matcher(nlp.vocab)
chunker = chunker(nlp, matcher)

phrases = chunker.chunk(""pytables is built on top of the hdf5 library, using the python language and the numpy package.\ni love pdf, it is wonderfull."")
print(phrases)

but it seems confused and give me this response
[ pytables is built on top of the hdf5 library, using the python language and the numpy package.
 ]
( the hdf5 library )
( the python language )
( the numpy package )
( pytables )
( top )
( is built on )
( using )
[ i love pdf, it is wonderfull. ]
( is )
( of )
( built )
[{'np': [pytables, top, the hdf5 library, the python language, the numpy package], 'vp': [is built on, using], 'vvp': []}, {'np': [built], 'vp': [is, of], 'vvp': []}]

the first element is good but not the second {'np': [built], 'vp': [is, of], 'vvp': []}
is there a problem if we use the matcher several times with different text ?","['python', 'nlp', 'spacy']",75279046,"instead of using multiple sentence, i check the sentence id on the callback function, it work but looks a bit gross
class chunker:
    def __init__(self, nlp, matcher):
        self.nlp = nlp
        self.matcher = matcher
        self.matcher.add(""np"", np_pattern, on_match=self.on_match_callback, greedy=""longest"")
        self.matcher.add(""vp"", vp_pattern, on_match=self.on_match_callback, greedy=""longest"")
        self.matcher.add(""vvp"", vvp_pattern, on_match=self.on_match_callback, greedy=""longest"")

    def on_match_callback(self, matcher, doc, i, matches):
        match_id, start, end = matches[i]
        string_id = self.nlp.vocab.strings[match_id]
        span = doc[start:end]
        sents = list(doc.sents)
        sent_id = sents.index(span.sent)
        print(""("", span, "")"")
        print(""sentence number: "", sent_id)

        self.phrases[sent_id][string_id].append(span)

    def chunk(self, text):
        self.phrases = []
        doc = self.nlp(text)
        self.phrases = [{""np"": [], ""vp"": [], ""vvp"": []} for _ in doc.sents]
        self.matcher(doc)

        for phrases in self.phrases:
            for phrase in phrases.values():
                phrase.sort(key=lambda x: x.start)

        return self.phrases",https://stackoverflow.com/questions/75277690,python,29-01-2023 19:18,67.0,0.0,1.0,True,04-03-2023 08:44,04-03-2023 08:44
60905016,how do i find most frequent words by each observation in r?,"i am very new to nlp. please, don't judge me strictly.
i have got a very big data-frame on customers' feedback, my goal is to analyze feedbacks. i tokenized words in feedbacks, deleted stop-words (smart). now, i need to receive a table of most and less frequent used words.
the code looks like this:
library(tokenizers)
library(stopwords)
words_as_tokens <- 
     tokenize_words(dat$description, 
                    stopwords = stopwords(language = ""en"", source = ""smart""))

the dataframe looks like this: there are lots of feedbacks (variable ""description"") and customers by whom the feedbacks were given (each customer is not unique, they can be repeated). i want to receive a table with 3 columns: a) customer name b) word c) its frequency. this ""ranking"" should be in a decreasing order.","['r', 'nlp', 'text-mining']",60909486,"try this
library(tokenizers)
library(stopwords)
library(tidyverse)

# count freq of words
words_as_tokens <- setnames(lapply(sapply(dat$description, 
                                 tokenize_words, 
                                 stopwords = stopwords(language = ""en"", source = ""smart"")), 
                          function(x) as.data.frame(sort(table(x), true), stringsasfactors = f)), dat$name)

# tidyverse's job
df <- words_as_tokens %>%
  bind_rows(, .id = ""name"") %>%
  rename(word = x)

# output
df

#    name          word freq
# 1  john    experience    2
# 2  john          word    2
# 3  john    absolutely    1
# 4  john        action    1
# 5  john        amazon    1
# 6  john     amazon.ae    1
# 7  john     answering    1
# ....
# 42 alex         break    2
# 43 alex          nice    2
# 44 alex         times    2
# 45 alex             8    1
# 46 alex        accent    1
# 47 alex        africa    1
# 48 alex        agents    1
# ....

data
dat <- data.frame(name = c(""john"", ""alex""),
                  description = c(""unprecedented. the perfect word to describe amazon. in every positive sense of that word! all because of one man - jeff bezos. what an entrepreneur! what a vision! this is from personal experience. let me explain. i had given up all hope, after a horrible experience with amazon.ae (formerly souq.com) - due to a herculean effort to get an order cancelled and the subsequent refund issued. i have never faced such a feedback-resistant team in my life! they were robotically answering my calls and sending me monotonous, unhelpful emails, followed by absolutely zero action!"",
                                 ""not only does amazon have great products but their customer service for the most part is wonderful. although most times you are outsourced to a different country, i personally have found that when i call it's either south africa or philippines and they speak so well, understand me and my ny accent and are quite nice. letï¿½ï¿½ï¿½s face it. most times you are calling cs with a problem or issue. these agents have to listen to 8 hours of complaints so they themselves need a break. no matter how annoyed i am i try to be on my best behavior and as nice as can be because they too need a break with how nasty we as a society c""), stringsasfactors = f)",https://stackoverflow.com/questions/60905016,r,28-03-2020 18:18,3242.0,0.0,2.0,True,07-05-2022 20:27,29-03-2020 02:16
65612062,how do i subtract and add vectors with gensim keyedvectors?,"i need to add and subtract word vectors, for a project in which i use gensim.models.keyedvectors (from the word2vec-google-news-300 model)
unfortunately, i've tried but can't manage to do it correctly.
let's look at the poular example queen ~= king - man + woman.
when i want to subtract man from king and add woman,
i can do this with gensim by
# model is loaded using gensim.models.keyedvectors.load()
model.wv.most_similar(positive=[""king"", ""woman""], negative=[""man""])[0]

which, as expected, returns ('queen', 0.7118192911148071) for the model i use.
now, to achieve the same with adding and subtracting vectors (all of them are unit-normed), i've tried the following code:
 vec_king, vec_man, vec_woman = model.wv[""king""], model.wv[""man""], model.wv[""woman""]
 result = model.similar_by_vector(vec_king - vec_man + vec_woman)[0]

result in the code above is ('king', 0.7992597222328186) which is not what i'd expect.
what is my mistake?","['python', 'nlp', 'gensim', 'word2vec', 'vector-space']",65617717,"you're generally doing the right thing, but note:

the most_similar() method also disqualifies from its results any of the named words provided - so even if 'king' is (still) the closest word to the result, it will be ignored. your formulation might very well have 'queen' as the next-closest word, after ignoring the input words - which is all that the 'analogy' tests need.

the most_similar() method also does its vector-arithmetic on versions of the vectors that are normalized to unit length, which can result in slightly different answers. if you change your uses of model.wv['king'] to model.get_vector('king', norm=true), you'll get the unit-normed vectors instead.


see also similar earlier answer:",https://stackoverflow.com/questions/65612062,python,07-01-2021 12:03,1497.0,2.0,1.0,True,29-07-2022 09:29,29-07-2022 09:29
72358012,notimplementederror: the lemmatize parameter is no longer supported,"i have run the code for my own similar gpt2 model, but the below error was got it. how to solve this implement error in python.
corpus = wikicorpus(file_path, lemmatize=false, lower=false, tokenizer_func=tokenizer_func)
  file ""c:\rayi\python\text-generate\text-gene\lib\site-packages\gensim\corpora\wikicorpus.py"", line 619, in __init__
    raise notimplementederror(
notimplementederror: the lemmatize parameter is no longer supported. if you need to lemmatize, use e.g. < perform lemmatization as part of your tokenization function and pass it as the tokenizer_func parameter to this initializer.

import tensorflow as tf
from gensim.corpora import wikicorpus
import os
import argparse

# lang = 'bn'

def store(corpus, lang):
    base_path = os.getcwd()
    store_path = os.path.join(base_path, '{}_corpus'.format(lang))
    if not os.path.exists(store_path):
        os.mkdir(store_path)
    file_idx=1
    for text in corpus.get_texts():
        current_file_path = os.path.join(store_path, 'article_{}.txt'.format(file_idx))
        with open(current_file_path, 'w' , encoding='utf-8') as file:
            file.write(bytes(' '.join(text), 'utf-8').decode('utf-8'))
        #endwith
        file_idx += 1
    #endfor

def tokenizer_func(text: str, token_min_len: int, token_max_len: int, lower: bool) -> list:
    return [token for token in text.split() if token_min_len <= len(token) <= token_max_len]

def run(lang):
    origin='
    fname='{}wiki-latest-pages-articles.xml.bz2'.format(lang)
    file_path = tf.keras.utils.get_file(origin=origin, fname=fname, untar=false, extract=false)
    corpus = wikicorpus(file_path, lemmatize=true, lower=false, tokenizer_func=tokenizer_func)
    store(corpus, lang)

if __name__ == '__main__':
    args_parser = argparse.argumentparser()
    args_parser.add_argument(
        '--lang',
        default='en',
        type=str,
        help='language code to download from wikipedia corpus'
    )
    args = args_parser.parse_args()
    run(**vars(args))","['python', 'machine-learning', 'text', 'nlp', 'artificial-intelligence']",75508457,"i couldn't find any actual documentation for this function, just some example page.
what i did was just calling:
corpus = wikicorpus(file_path, tokenizer_func=tokenizer_func)

if you still want to lemmatize, call some lemmatization function in tokenizer_func, as described in error message.
and now wait around 8h to process :d",https://stackoverflow.com/questions/72358012,python,24-05-2022 06:17,689.0,0.0,1.0,True,20-02-2023 11:18,24-05-2022 08:14
71705218,bert prediction shape not equal to num_samples,"i have a text classification that i am trying to do using bert. below is the code i am using. the model training code(below) works fine but i am facing issue with the prediction part
from transformers import tfbertforsequenceclassification
import tensorflow as tf

# recommended learning rate for adam 5e-5, 3e-5, 2e-5
learning_rate = 5e-5
nlabels = 26

# we will do just 1 epoch for illustration, though multiple epochs might be better as long as we will not overfit the model
number_of_epochs = 1


# model initialization
model = tfbertforsequenceclassification.from_pretrained('bert-base-uncased', num_labels=nlabels,
                                                      output_attentions=false,
                                                      output_hidden_states=false)

# optimizer adam
optimizer = tf.keras.optimizers.adam(learning_rate=learning_rate, epsilon=1e-08)

# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy
loss = tf.keras.losses.sparsecategoricalcrossentropy(from_logits=true)
metric = tf.keras.metrics.sparsecategoricalaccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

bert_history = model.fit(ds_tr_encoded, epochs=number_of_epochs)

i am getting the output using the following
preds = model.predict(ds_te_encoded)
pred_labels_idx = np.argmax(preds['logits'], axis=1)

the issue i am facing is that the shape of pred_labels_idx is not the same as ds_te_encoded
len(pred_labels_idx) #426820
tf.data.experimental.cardinality(ds_te_encoded) #<tf.tensor: shape=(), dtype=int64, numpy=21341>

not sure why this is happening.","['python', 'tensorflow', 'tensorflow-datasets', 'bert-language-model', 'transformer-model']",71705603,"since ds_te_encoded is of type tf.data.dataset and you call cardinality(...), the cardinality in your case is simply the rounded number of batches and not the number of samples. so i am assuming you are using a batch size of 20, because 426820/20 = 21341. that is probably what is causing the confusion.",https://stackoverflow.com/questions/71705218,python,01-04-2022 10:06,208.0,1.0,1.0,True,01-04-2022 11:02,01-04-2022 11:02
75713161,finetuning vision encoder decoder models with huggingface causes valueerror: expected sequence of length 11 at dim 2 (got 12),"input code that causes code failing:
from transformers import automodelforseq2seqlm, seq2seqtrainingarguments, seq2seqtrainer, vitfeatureextractor, autotokenizer
from transformers import vitimageprocessor, berttokenizer, visionencoderdecodermodel, default_data_collator
from datasets import load_dataset, datasetdict
encoder_checkpoint = ""google/vit-base-patch16-224-in21k""
decoder_checkpoint = ""bert-base-uncased""


model = visionencoderdecodermodel.from_encoder_decoder_pretrained(
    encoder_checkpoint, decoder_checkpoint
)
# set special tokens used for creating the decoder_input_ids from the labels
model.config.decoder_start_token_id = tokenizer.bos_token_id
model.config.pad_token_id = tokenizer.pad_token_id
# make sure vocab size is set correctly
model.config.vocab_size = model.config.decoder.vocab_size

# set beam search parameters
model.config.eos_token_id = tokenizer.sep_token_id
model.config.max_length = 512
model.config.early_stopping = true
model.config.no_repeat_ngram_size = 3
model.config.length_penalty = 2.0
model.config.num_beams = 4
model.decoder.resize_token_embeddings(len(tokenizer))

feature_extractor = vitfeatureextractor.from_pretrained(encoder_checkpoint)
tokenizer = autotokenizer.from_pretrained(decoder_checkpoint)

preparing dataset
dataset = load_dataset(""svjack/pokemon-blip-captions-en-zh"").remove_columns(""zh_text"")
dataset = dataset.map(lambda example: {'pixel_values': feature_extractor(example['image'], return_tensors='pt').pixel_values})
dataset = dataset.remove_columns(""image"")
dataset = dataset.map(lambda example: {'labels': tokenizer(example['en_text'], return_tensors='pt').input_ids })
dataset = dataset.remove_columns(""en_text"")
""""""
dataset = datasetdict({
train: dataset({
    features: ['pixel_values', 'labels'],
    num_rows: 833
})
""""""
train_testvalid = dataset[""train""].train_test_split(0.1)
test_valid = train_testvalid['test'].train_test_split(0.5)
train_test_valid_dataset = datasetdict({
    'train': train_testvalid['train'],
    'test': test_valid['test'],
    'valid': test_valid['train']})

setting parameters:
for param in model.encoder.parameters():
    param.requires_grad = false

output_dir = ""./checkpoints""
training_args = seq2seqtrainingarguments(
    predict_with_generate=true,
    evaluation_strategy=""steps"",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    overwrite_output_dir=true,
    fp16=true,
    run_name=""first_run"",
    load_best_model_at_end=true,
    output_dir=output_dir,
    logging_steps=2000,
    save_steps=2000,
    eval_steps=2000,
)

trying to finetune models:
trainer = seq2seqtrainer(
        model=model,
        tokenizer=tokenizer,
        args=training_args,
        train_dataset=train_test_valid_dataset['train'],
        eval_dataset=train_test_valid_dataset['valid'],
        data_collator=default_data_collator,
        
    )
trainer.train()

output error:
/usr/local/lib/python3.9/dist-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1541             self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1542         )
-> 1543         return inner_training_loop(
   1544             args=args,
   1545             resume_from_checkpoint=resume_from_checkpoint,

/usr/local/lib/python3.9/dist-packages/transformers/trainer.py in _inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1763 
   1764             step = -1
-> 1765             for step, inputs in enumerate(epoch_iterator):
   1766 
   1767                 # skip past any already trained steps if resuming training

/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py in __next__(self)
    626                 # todo(
    627                 self._reset()  # type: ignore[call-arg]
--> 628             data = self._next_data()
    629             self._num_yielded += 1
    630             if self._dataset_kind == _datasetkind.iterable and \

/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py in _next_data(self)
    669     def _next_data(self):
    670         index = self._next_index()  # may raise stopiteration
--> 671         data = self._dataset_fetcher.fetch(index)  # may raise stopiteration
    672         if self._pin_memory:
    673             data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)

/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)
     59         else:
     60             data = self.dataset[possibly_batched_index]
---> 61         return self.collate_fn(data)

/usr/local/lib/python3.9/dist-packages/transformers/data/data_collator.py in default_data_collator(features, return_tensors)
     68 
     69     if return_tensors == ""pt"":
---> 70         return torch_default_data_collator(features)
     71     elif return_tensors == ""tf"":
     72         return tf_default_data_collator(features)

/usr/local/lib/python3.9/dist-packages/transformers/data/data_collator.py in torch_default_data_collator(features)
    134                 batch[k] = torch.tensor(np.stack([f[k] for f in features]))
    135             else:
--> 136                 batch[k] = torch.tensor([f[k] for f in features])
    137 
    138     return batch

valueerror: expected sequence of length 11 at dim 2 (got 12)

how to fix the code?","['python', 'huggingface-transformers']",76004702,"update :
in my case. i solved it by passing trunction= true inside the tokenizer.
tokenizer(text, 
            padding='max_length',  
            truncation=true,  
            max_length=self.max_target_length).input_ids

for more check",https://stackoverflow.com/questions/75713161,python,12-03-2023 12:37,1178.0,1.0,1.0,True,13-04-2023 14:20,12-03-2023 12:59
74313676,attribute error: openai has no attribute image,"so i was getting my hands-on on the new dall.e api for python which has been made public. i was getting the attribution error as it was not detecting the image model in open ai after running the following code:
response = openai.image.create(
  prompt=""a white siamese cat"",
  n=1,
  size=""1024x1024""
)
image_url = response['data'][0]['url']

why am i getting this error?","['python', 'openai-api']",74313677,"i just found the solution. you have to upgrade your openai library with code:
pip install --upgrade openai

restart the kernel, and enjoy :)",https://stackoverflow.com/questions/74313676,python,04-11-2022 07:45,3688.0,2.0,3.0,True,20-06-2024 00:22,15-01-2023 17:53
73916950,replacing abbreviations in text using python without changing white space,"i am trying to replace abbreviations in the text using python without changing the sentence structure including whitespace.
i have created a data dictionary with the abbreviations and the replacers;
replacers = {
'aaa': 'abdominal aortic aneurysm',
'taa' : 'thoracic aortic aneurysm',
'clti': 'chronic limb threatening ischaemia',

i have my text coming from a text area in a form called 'note'.
if request. method == ""post"":
        text = request.post.get(""note"")

i have created this function to remove the abbreviations.
# remove abbreviations function
def acronym(replacers, text):
     return ' '.join([replacers.get(i, i) for i in text.split()])

it works well but it removes all new-lines and tabs and makes the text difficult to read.
is there an elegant way to write the above function?
many thanks.","['python', 'string', 'split', 'nlp']",73918464,"i found out that using regular expression works for this situation.
# remove abbreviations function
def acronym(replacers, text):
    for i, x in replacers.items():
        text = re.sub(rf'\b{i}\b', x, text)
    return text",https://stackoverflow.com/questions/73916950,python,01-10-2022 08:40,399.0,1.0,2.0,True,01-10-2022 13:00,01-10-2022 11:26
3788870,how to check if a word is an english word with python?,"i want to check in a python program if a word is in the english dictionary.
i believe nltk wordnet interface might be the way to go but i have no clue how to use it for such a simple task.
def is_english_word(word):
    pass # how to i implement is_english_word?

is_english_word(token.lower())

in the future, i might want to check if the singular form of a word is in the dictionary (e.g., properties -> property -> english word). how would i achieve that?","['python', 'nltk', 'wordnet']",3789057,"for (much) more power and flexibility, use a dedicated spellchecking library like pyenchant. there's a tutorial, or you could just dive straight in:
>>> import enchant
>>> d = enchant.dict(""en_us"")
>>> d.check(""hello"")
true
>>> d.check(""helo"")
false
>>> d.suggest(""helo"")
['he lo', 'he-lo', 'hello', 'helot', 'help', 'halo', 'hell', 'held', 'helm', 'hero', ""he'll""]
>>>

pyenchant comes with a few dictionaries (en_gb, en_us, de_de, fr_fr), but can use any of the openoffice ones if you want more languages.
there appears to be a pluralisation library called inflect, but i've no idea whether it's any good.",https://stackoverflow.com/questions/3788870,python,24-09-2010 16:01,306517.0,195.0,12.0,True,20-07-2023 12:29,20-09-2015 22:13
70544499,how to get the best merger from symspellpy word segmentation of many languages in python?,"the following code uses symspell in python, see the symspellpy guide on word_segmentation.
it uses ""de-100k.txt"" and ""en-80k.txt"" frequency dictionaries from a github repo, you need to save them in your working directory. as long as you do not want to use any symspell logic, you do not need to install and run this script to answer the question, take just the output of the two language's word segmentations and go on.
import pkg_resources
from symspellpy.symspellpy import symspell

input_term = ""sonnenempfindlichkeitsunoil farbpalettesuncreme""

# german:
# set max_dictionary_edit_distance to 0 to avoid spelling correction
sym_spell = symspell(max_dictionary_edit_distance=0, prefix_length=7)
dictionary_path = pkg_resources.resource_filename(
    ""symspellpy"", ""de-100k.txt""
)
# term_index is the column of the term and count_index is the
# column of the term frequency
sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)
result = sym_spell.word_segmentation(input_term)
print(f""{result.corrected_string}, {result.distance_sum}, {result.log_prob_sum}"")

# english:
# reset the sym_spell object
sym_spell = symspell(max_dictionary_edit_distance=0, prefix_length=7)
dictionary_path = pkg_resources.resource_filename(
    ""symspellpy"", ""en-80k.txt""
)
sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)
result = sym_spell.word_segmentation(input_term)
print(f""{result.corrected_string}, {result.distance_sum}, {result.log_prob_sum}"")

out:
sonnen empfindlichkeit s uno i l farb palette sun creme, 8, -61.741842760725255
sonnen empfindlichkeit sun oil farb palette sun creme, 6, -45.923471400632884

the aim is to find out the most relevant words by some logic: most frequent ngram neighours and/or word frequency, longest word, and the like. the logic is free of choice.
in this example with two languages, the two outputs need to be compared so that only the best segments are kept while dropping the rest, without interceptions of parts of words. in the outcome, each letter is used one time and uniquely.
if there are spaces between words in the input_term, these words should not be joined to become a new segment. for example, if you have 'cr eme' with a wrong space in it, that should still not be allowed to become 'creme'. it is just likely that the space is right more often than the errors that would appear from taking neighoured letters.
array('sonnen', 'empfindlichkeit', 'sun', 'oil', 'farb', 'palette', 'sun', 'creme')
array(['de'], ['de'], ['en'], ['en'], ['de'], ['de', 'en'], ['en'], ['de', 'en'])

the 'de/en' tag is just an optional idea to show that the word exists in german and english, you can also choose 'en' over 'de' in this example. the language tags are a  bonus, you can also answer without that.
there is probably a fast solution that uses numpy arrays and/or dictionaries instead of lists or dataframes, but choose as you like.
how to use many languages in symspell word segmentation and combine them to one chosen merger? the aim is a sentence of words built from all letters, using each letter once, keeping all original spaces.","['python', 'nlp', 'text-segmentation', 'symspell']",70546123,"simspell way
this is the recommended way. i found this out only after doing the manual way. you can easily use the same frequency logic that is used for one language for two languages instead: just load two languages or more into the sym_spell object!
import pkg_resources
from symspellpy.symspellpy import symspell

input_term = ""sonnenempfindlichkeitsunoil farbpalettesuncreme""

# set max_dictionary_edit_distance to 0 to avoid spelling correction
sym_spell = symspell(max_dictionary_edit_distance=0, prefix_length=7)
dictionary_path = pkg_resources.resource_filename(
    ""symspellpy"", ""de-100k.txt""
)

# term_index is the column of the term and count_index is the
# column of the term frequency
sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)

result = sym_spell.word_segmentation(input_term)
print(f""{result.corrected_string}, {result.distance_sum}, {result.log_prob_sum}"")

# do not reset the sym_spell object at this line so that
# english is added to the german frequency dictionary
# not: #reset the sym_spell object
# not: #sym_spell = symspell(max_dictionary_edit_distance=0, prefix_length=7)
dictionary_path = pkg_resources.resource_filename(
    ""symspellpy"", ""en-80k.txt""
)
sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)
result = sym_spell.word_segmentation(input_term)
print(f""{result.corrected_string}, {result.distance_sum}, {result.log_prob_sum}"")

out:
sonnen empfindlichkeit s uno i l farb palette sun creme, 8, -61.741842760725255
sonnen empfindlichkeit sun oil farb palette sun creme, 6, -45.923471400632884

manual way
in this manual way, the logic is: the longer word of two languages wins, logging the winner language tag. if they are at the same length, both languages are logged.
as in the question, the input_term = ""sonnenempfindlichkeitsunoil farbpalettesuncreme"", using a reset object for each language segmentation in symspell, leads to s1 for german and s2 for english.
import numpy as np

s1 = 'sonnen empfindlichkeit s uno i l farb palette sun creme'
s2 = 'son ne ne mp find li ch k e it sun oil far b palette sun creme'

num_letters = len(s1.replace(' ',''))
list_w1 = s1.split()
list_w2 = s2.split()
list_w1_len = [len(x) for x in list_w1]
list_w2_len = [len(x) for x in list_w2]

lst_de = [(x[0], x[1], x[2], 'de', x[3], x[4]) for x in zip(list_w1, list_w1_len, range(len(list_w1)), np.cumsum([0] + [len(x)+1 for x in list_w1][:-1]), np.cumsum([0] + [len(x) for x in list_w1][:-1]))]
lst_en = [(x[0], x[1], x[2], 'en', x[3], x[4]) for x in zip(list_w2, list_w2_len, range(len(list_w2)), np.cumsum([0] + [len(x)+1 for x in list_w2][:-1]), np.cumsum([0] + [len(x) for x in list_w2][:-1]))]

idx_word_de = 0
idx_word_en = 0
lst_words = []
idx_letter = 0

# stop at num_letters-1, else you check the last word 
# also on the last idx_letter and get it twice
while idx_letter <= num_letters-1:
lst_de[idx_word_de][5], idx_letter)
    while(lst_de[idx_word_de][5]<idx_letter):
        idx_word_de +=1
    while(lst_en[idx_word_en][5]<idx_letter):
        idx_word_en +=1

    if lst_de[idx_word_de][1]>lst_en[idx_word_en][1]:
        lst_word_stats = lst_de[idx_word_de]
        str_word = lst_word_stats[0]
#         print('de:', lst_de[idx_word_de])
        idx_letter += len(str_word) #lst_de[idx_word_de][0])
    elif lst_de[idx_word_de][1]==lst_en[idx_word_en][1]:
        lst_word_stats = (lst_de[idx_word_de][0], lst_de[idx_word_de][1], (lst_de[idx_word_de][2], lst_en[idx_word_en][2]), (lst_de[idx_word_de][3], lst_en[idx_word_en][3]), (lst_de[idx_word_de][4], lst_en[idx_word_en][4]), lst_de[idx_word_de][5])
        str_word = lst_word_stats[0]
#         print('de:', lst_de[idx_word_de], 'en:', lst_en[idx_word_en])
        idx_letter += len(str_word) #lst_de[idx_word_de][0])        
    else:
        lst_word_stats = lst_en[idx_word_en]
        str_word = lst_word_stats[0]
#         print('en:', lst_en[idx_word_en][0])
        idx_letter += len(str_word)
    lst_words.append(lst_word_stats)

out lst_words:
[('sonnen', 6, 0, 'de', 0, 0),
 ('empfindlichkeit', 15, 1, 'de', 7, 6),
 ('sun', 3, 10, 'en', 31, 21),
 ('oil', 3, 11, 'en', 35, 24),
 ('farb', 4, 6, 'de', 33, 27),
 ('palette', 7, (7, 14), ('de', 'en'), (38, 45), 31),
 ('sun', 3, (8, 15), ('de', 'en'), (46, 53), 38),
 ('creme', 5, (9, 16), ('de', 'en'), (50, 57), 41)]

legend of the output:
chosen word | len | word_idx_of_lang | lang | letter_idx_lang_with_spaces | letter_idx_no_spaces",https://stackoverflow.com/questions/70544499,python,31-12-2021 17:40,1639.0,1.0,1.0,True,01-01-2022 17:52,01-01-2022 17:32
12184304,extracting text from garbled pdf,"i have a pdf file with valuable textual information.
the problem is that i cannot extract the text, all i get is a bunch of garbled symbols. the same happens if i copy and paste the text from the pdf reader to a text file. even file -> save as text in acrobat reader fails.
i have used all tools i could get my hands on and the result is the same. i believe that this has something to do with fonts embedding, but i don't know what exactly?
my questions:

what is the culprit of this weird text garbling?
how to extract the text content from the pdf (programmatically, with a tool, manipulating the bits directly, etc.)?
how to fix the pdf to not garble on copy?","['pdf', 'file-format', 'text-analysis']",12219980,i went to a lot of people for help and ocr is the only solution to this problem,https://stackoverflow.com/questions/12184304,pdf,29-08-2012 18:30,40182.0,12.0,3.0,True,25-07-2022 01:50,17-02-2017 14:07
34232190,scikit learn tfidfvectorizer : how to get top n terms with highest tf-idf score,"i am working on keyword extraction problem. consider the very general case
from sklearn.feature_extraction.text import tfidfvectorizer

tfidf = tfidfvectorizer(tokenizer=tokenize, stop_words='english')

t = """"""two travellers, walking in the noonday sun, sought the shade of a widespreading tree to rest. as they lay looking up among the pleasant leaves, they saw that it was a plane tree.

""how useless is the plane!"" said one of them. ""it bears no fruit whatever, and only serves to litter the ground with leaves.""

""ungrateful creatures!"" said a voice from the plane tree. ""you lie here in my cooling shade, and yet you say i am useless! thus ungratefully, o jupiter, do men receive their blessings!""

our best blessings are often the least appreciated.""""""

tfs = tfidf.fit_transform(t.split("" ""))
str = 'tree cat travellers fruit jupiter'
response = tfidf.transform([str])
feature_names = tfidf.get_feature_names()

for col in response.nonzero()[1]:
    print(feature_names[col], ' - ', response[0, col])

and this gives me
  (0, 28)   0.443509712811
  (0, 27)   0.517461475101
  (0, 8)    0.517461475101
  (0, 6)    0.517461475101
tree  -  0.443509712811
travellers  -  0.517461475101
jupiter  -  0.517461475101
fruit  -  0.517461475101

which is good. for any new document that comes in, is there a way to get the top n terms with the highest tfidf score?","['python', 'scikit-learn', 'nlp', 'nltk', 'tf-idf']",34236002,"you have to do a little bit of a song and dance to get the matrices as numpy arrays instead, but this should do what you're looking for:
feature_array = np.array(tfidf.get_feature_names())
tfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]

n = 3
top_n = feature_array[tfidf_sorting][:n]

this gives me:
array([u'fruit', u'travellers', u'jupiter'], 
  dtype='<u13')

the argsort call is really the useful one, here are the docs for it. we have to do [::-1] because argsort only supports sorting small to large. we call flatten to reduce the dimensions to 1d so that the sorted indices can be used to index the 1d feature array. note that including the call to flatten will only work if you're testing one document at at time.
also, on another note, did you mean something like tfs = tfidf.fit_transform(t.split(""\n\n""))? otherwise, each term in the multiline string is being treated as a ""document"". using \n\n instead means that we are actually looking at 4 documents (one for each line), which makes more sense when you think about tfidf.",https://stackoverflow.com/questions/34232190,python,11-12-2015 20:39,70797.0,54.0,3.0,True,07-11-2022 23:31,16-03-2021 00:24
77073059,how can i change from openai to chatopenai in langchain and flask?,"this is an implementation based on langchain and flask and refers to an implementation to be able to stream responses from the openai server in langchain to a page with javascript that can show the streamed response.
i tried all ways to modify the code below to replace the langchain library from openai to chatopenai without success, i upload below both implementations (the one with openai working) and the one chatopenai with error.
thank you to all the community and those who can help me to understand the problem, it would be very helpful if you could also show me how to solve it since i have been trying for days and the error it shows has really no significance.
code version with library that works but reports as deprecated:
from flask import flask, response
import threading
import queue

from langchain.llms import openai
from langchain.callbacks.base import basecallbackmanager
from langchain.callbacks.streaming_stdout import streamingstdoutcallbackhandler

app = flask(__name__)

@app.route('/')
def index():
    return response('''<!doctype html>
<html>
<head><title>flask streaming langchain example</title></head>
<body>
    <div id=""output""></div>
    <script>
const outputel = document.getelementbyid('output');

(async function() {
    try {
        const controller = new abortcontroller();
        const signal = controller.signal;
        const timeout = 120000; // imposta il timeout su 120 secondi

        settimeout(() => controller.abort(), timeout);

        const response = await fetch('/chain', {method: 'post', signal});
        const reader = response.body.getreader();
        const decoder = new textdecoder();
        let buffer = '';

        while (true) {
            const { done, value } = await reader.read();
            if (done) { break; }

            const text = decoder.decode(value, {stream: true});
            outputel.innerhtml += text;
        }
    } catch (err) {
        console.error(err);
    }
})();

    </script>
</body>
</html>''', mimetype='text/html')


class threadedgenerator:
    def __init__(self):
        self.queue = queue.queue()

    def __iter__(self):
        return self

    def __next__(self):
        item = self.queue.get()
        if item is stopiteration: raise item
        return item

    def send(self, data):
        self.queue.put(data)

    def close(self):
        self.queue.put(stopiteration)

class chainstreamhandler(streamingstdoutcallbackhandler):
    def __init__(self, gen):
        super().__init__()
        self.gen = gen

    def on_llm_new_token(self, token: str, **kwargs):
        self.gen.send(token)

def llm_thread(g, prompt):
    try:
        llm = openai(
            model_name=""gpt-4"",
            verbose=true,
            streaming=true,

            callback_manager=basecallbackmanager([chainstreamhandler(g)]),
            temperature=0.7,
        )
        llm(prompt)
    finally:
        g.close()


def chain(prompt):
    g = threadedgenerator()
    threading.thread(target=llm_thread, args=(g, prompt)).start()
    return g


@app.route('/chain', methods=['post'])
def _chain():
    return response(chain(""create a poem about the meaning of life \n\n""), mimetype='text/plain')

if __name__ == '__main__':
    app.run(threaded=true, debug=true)


version with error (openai replaced with chatopenai)
from flask import flask, response
import threading
import queue

from langchain.chat_models import chatopenai
from langchain.callbacks.base import basecallbackmanager
from langchain.callbacks.streaming_stdout import streamingstdoutcallbackhandler

app = flask(__name__)

@app.route('/')
def index():
    return response('''<!doctype html>
<html>
<head><title>flask streaming langchain example</title></head>
<body>
    <div id=""output""></div>
    <script>
const outputel = document.getelementbyid('output');

(async function() {
    try {
        const controller = new abortcontroller();
        const signal = controller.signal;
        const timeout = 120000; // imposta il timeout su 120 secondi

        settimeout(() => controller.abort(), timeout);

        const response = await fetch('/chain', {method: 'post', signal});
        const reader = response.body.getreader();
        const decoder = new textdecoder();
        let buffer = '';

        while (true) {
            const { done, value } = await reader.read();
            if (done) { break; }

            const text = decoder.decode(value, {stream: true});
            outputel.innerhtml += text;
        }
    } catch (err) {
        console.error(err);
    }
})();

    </script>
</body>
</html>''', mimetype='text/html')


class threadedgenerator:
    def __init__(self):
        self.queue = queue.queue()

    def __iter__(self):
        return self

    def __next__(self):
        item = self.queue.get()
        if item is stopiteration: raise item
        return item

    def send(self, data):
        self.queue.put(data)

    def close(self):
        self.queue.put(stopiteration)

class chainstreamhandler(streamingstdoutcallbackhandler):
    def __init__(self, gen):
        super().__init__()
        self.gen = gen

    def on_llm_new_token(self, token: str, **kwargs):
        self.gen.send(token)

    def on_chat_model_start(self, token: str):
        print(""started"")

def llm_thread(g, prompt):
    try:
        llm = chatopenai(
            model_name=""gpt-4"",
            verbose=true,
            streaming=true,

            callback_manager=basecallbackmanager([chainstreamhandler(g)]),
            temperature=0.7,
        )
        llm(prompt)
    finally:
        g.close()


def chain(prompt):
    g = threadedgenerator()
    threading.thread(target=llm_thread, args=(g, prompt)).start()
    return g


@app.route('/chain', methods=['post'])
def _chain():
    return response(chain(""parlami dei 5 modi di dire in inglese che gli italiani conoscono meno \n\n""), mimetype='text/plain')

if __name__ == '__main__':
    app.run(threaded=true, debug=true)



error showing the console at startup and at the time i enter the web page:
error in chainstreamhandler.on_chat_model_start callback: chainstreamhandler.on_chat_model_start() got an unexpected keyword argument 'run_id'
exception in thread thread-4 (llm_thread):
127.0.0.1 - - [09/sep/2023 18:09:29] ""post /chain  200 -
traceback (most recent call last):
  file ""c:\users\user22\desktop\work\testproj\env\lib\site-packages\langchain\callbacks\manager.py"", line 300, in _handle_event
    getattr(handler, event_name)(*args, **kwargs)
  file ""c:\users\user22\desktop\work\testproj\env\lib\site-packages\langchain\callbacks\base.py"", line 168, in on_chat_model_start
    raise notimplementederror(
notimplementederror: stdoutcallbackhandler does not implement `on_chat_model_start`

during handling of the above exception, another exception occurred:

traceback (most recent call last):
  file ""c:\users\user22\appdata\local\programs\python\python311\lib\threading.py"", line 1038, in _bootstrap_inner    
    self.run()
  file ""c:\users\user22\appdata\local\programs\python\python311\lib\threading.py"", line 975, in run
    self._target(*self._args, **self._kwargs)
  file ""c:\users\user22\desktop\work\testproj\streamresp.py"", line 90, in llm_thread
    llm(prompt)
  file ""c:\users\user22\desktop\work\testproj\env\lib\site-packages\langchain\chat_models\base.py"", line 552, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  file ""c:\users\user22\desktop\work\testproj\env\lib\site-packages\langchain\chat_models\base.py"", line 293, in generate
    run_managers = callback_manager.on_chat_model_start(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""c:\users\user22\desktop\work\testproj\env\lib\site-packages\langchain\callbacks\manager.py"", line 1112, in on_chat_model_start
    _handle_event(
  file ""c:\users\user22\desktop\work\testproj\env\lib\site-packages\langchain\callbacks\manager.py"", line 304, in _handle_event
    message_strings = [get_buffer_string(m) for m in args[1]]
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""c:\users\user22\desktop\work\testproj\env\lib\site-packages\langchain\callbacks\manager.py"", line 304, in <listcomp>
    message_strings = [get_buffer_string(m) for m in args[1]]
                       ^^^^^^^^^^^^^^^^^^^^
  file ""c:\users\user22\desktop\work\testproj\env\lib\site-packages\langchain\schema\messages.py"", line 52, in get_buffer_string
    raise valueerror(f""got unsupported message type: {m}"")
valueerror: got unsupported message type: p

thank you very much for the support!","['flask', 'openai-api', 'langchain', 'py-langchain']",77097281,"thanks to python273 user on github i've resolved:
import os
os.environ[""openai_api_key""] = """"

from flask import flask, response, request
import threading
import queue

from langchain.chat_models import chatopenai
from langchain.callbacks.streaming_stdout import streamingstdoutcallbackhandler
from langchain.schema import aimessage, humanmessage, systemmessage

app = flask(__name__)

@app.route('/')
def index():
    # just for the example, html is included directly, move to .html file
    return response('''
<!doctype html>
<html>
<head><title>flask streaming langchain example</title></head>
<body>
    <form id=""form"">
        <input name=""prompt"" value=""write a short koan story about seeing beyond""/>
        <input type=""submit""/>
    </form>
    <div id=""output""></div>
    <script>
        const formel = document.getelementbyid('form');
        const outputel = document.getelementbyid('output');

        let aborter = new abortcontroller();
        async function run() {
            aborter.abort();  // cancel previous request
            outputel.innertext = '';
            aborter = new abortcontroller();
            const prompt = new formdata(formel).get('prompt');
            try {
                const response = await fetch(
                    '/chain', {
                        signal: aborter.signal,
                        method: 'post',
                        headers: {'content-type': 'application/json'},
                        body: json.stringify({
                            prompt
                        }),
                    }
                );
                const reader = response.body.getreader();
                const decoder = new textdecoder();
                while (true) {
                    const { done, value } = await reader.read();
                    if (done) { break; }
                    const decoded = decoder.decode(value, {stream: true});
                    outputel.innertext += decoded;
                }
            } catch (err) {
                console.error(err);
            }
        }
        run();  // run on initial prompt
        formel.addeventlistener('submit', function(event) {
            event.preventdefault();
            run();
        });
    </script>
</body>
</html>
''', mimetype='text/html')

class threadedgenerator:
    def __init__(self):
        self.queue = queue.queue()

    def __iter__(self):
        return self

    def __next__(self):
        item = self.queue.get()
        if item is stopiteration: raise item
        return item

    def send(self, data):
        self.queue.put(data)

    def close(self):
        self.queue.put(stopiteration)

class chainstreamhandler(streamingstdoutcallbackhandler):
    def __init__(self, gen):
        super().__init__()
        self.gen = gen

    def on_llm_new_token(self, token: str, **kwargs):
        self.gen.send(token)

def llm_thread(g, prompt):
    try:
        chat = chatopenai(
            verbose=true,
            streaming=true,
            callbacks=[chainstreamhandler(g)],
            temperature=0.7,
        )
        chat([humanmessage(content=prompt)])
    finally:
        g.close()


def chain(prompt):
    g = threadedgenerator()
    threading.thread(target=llm_thread, args=(g, prompt)).start()
    return g


@app.route('/chain', methods=['post'])
def _chain():
    return response(chain(request.json['prompt']), mimetype='text/plain')

if __name__ == '__main__':
    app.run(threaded=true, debug=true)

link to the original reply:",https://stackoverflow.com/questions/77073059,flask,09-09-2023 16:17,4616.0,1.0,2.0,True,13-09-2023 12:57,09-09-2023 16:36
67699354,are these normal speed of bert pretrained model inference in pytorch,"i am testing bert base and bert distilled model in huggingface with 4 scenarios of speeds, batch_size = 1:
1) bert-base-uncased: 154ms per request
2) bert-base-uncased with quantifization: 94ms per request
3) distilbert-base-uncased: 86ms per request
4) distilbert-base-uncased with quantifization: 69ms per request

i am using the imdb text as experimental data and set the max_length=512, so it's quite long. the cpu on ubuntu 18.04 info is below:
cat /proc/cpuinfo  | grep 'name'| uniq
model name  : intel(r) xeon(r) platinum 8163 cpu @ 2.50ghz

the machine has 3 gpu available for use:
tesla v100-sxm2

it seems quite slow for realtime application. are those speeds normal for bert base model?
the testing code is below:
import pandas as pd
import torch.quantization

from transformers import autotokenizer, automodel, distilberttokenizer, distilbertmodel

def get_embedding(model, tokenizer, text):
    inputs = tokenizer(text, return_tensors=""pt"", max_length=512, truncation=true)
    outputs = model(**inputs)
    output_tensors = outputs[0][0]
    output_numpy = output_tensors.detach().numpy()
    embedding = output_numpy.tolist()[0]

def process_text(model, tokenizer, text_lines):
    for index, line in enumerate(text_lines):
        embedding = get_embedding(model, tokenizer, line)
        if index % 100 == 0:
            print('current index: {}'.format(index))

import time
from datetime import timedelta
if __name__ == ""__main__"":

    df = pd.read_csv('../data/train.csv', sep='\t')
    df = df.head(1000)
    text_lines = df['review']
    text_line_count = len(text_lines)
    print('text size: {}'.format(text_line_count))

    start = time.time()

    tokenizer = autotokenizer.from_pretrained(""bert-base-uncased"")
    model = automodel.from_pretrained(""bert-base-uncased"")
    process_text(model, tokenizer, text_lines)

    end = time.time()
    print('total time spent with bert base: {}'.format(str(timedelta(seconds=end - start))))

    model = torch.quantization.quantize_dynamic(model, {torch.nn.linear}, dtype=torch.qint8)
    process_text(model, tokenizer, text_lines)

    end2 = time.time()
    print('total time spent with bert base quantization: {}'.format(str(timedelta(seconds=end2 - end))))

    tokenizer = distilberttokenizer.from_pretrained(""distilbert-base-uncased"")
    model = distilbertmodel.from_pretrained(""distilbert-base-uncased"")
    process_text(model, tokenizer, text_lines)

    end3 = time.time()
    print('total time spent with distilbert: {}'.format(str(timedelta(seconds=end3 - end2))))

    model = distilbertmodel.from_pretrained(""distilbert-base-uncased"")
    model = torch.quantization.quantize_dynamic(model, {torch.nn.linear}, dtype=torch.qint8)
    process_text(model, tokenizer, text_lines)

    end4 = time.time()
    print('total time spent with distilbert quantization: {}'.format(str(timedelta(seconds=end4 - end3))))

edit: based on suggestion i changed to the following:
inputs = tokenizer(text_batch, padding=true, return_tensors=""pt"")
outputs = model(**inputs)

where text_batch is a list of text as input.","['bert-language-model', 'huggingface-transformers', 'transformer-model', 'huggingface-tokenizers']",67712401,"no, you can speed it up.
first, why are you testing it with batch size 1?
both tokenizer and model accept batched inputs. basically, you can pass a 2d array/list that contains a single sample at each row. see the documentation for tokenizer:  the same applies for the models.
also, your for loop is sequential even if you use batch size larger than 1. you can create a test data and then use trainer class with trainer.predict()
also see this discussion of mine at the hf forums:",https://stackoverflow.com/questions/67699354,bert-language-model,26-05-2021 06:07,1766.0,2.0,1.0,True,26-05-2021 23:43,26-05-2021 23:43
71196936,how do i put my scraped data into a data frame,"please i need help, am having trouble trying to put my scraped data into a data frame that has 3 columns i.e. date, source and keywords extracted from each scraped website for further text analysis, my code is borrowed from  and is given below:
from newspaper import article
import nltk
nltk.download('punkt')
urls = [' ' ' ' '
results = {}
for url in urls:
    article = article(url)
    article.download()
    article.parse()
    article.nlp()
    results[url] = article
for url in urls:
    print(url)
    article = results[url]
    print(article.authors)
    print(article.publish_date)
    print(article.keywords)","['text', 'nlp']",71200810,"i played around with it and here is how you can make it into a data frame. assuming that you wanted to use pandas in the first place:
import nltk
import pandas as pd

from newspaper import article


nltk.download('punkt')


urls = [' ' ' ' '

    # create a data frame with the needed columns
saved_data = pd.dataframe(columns=['date', 'source', 'keywords'])
    # put into a data frame that has 3 columns i.e. date, source and keywords
def add_data_to_df(urls, saved_data):
    for url in urls: # process each url separately
        article = article(url)
        article.download()
        article.parse()
        article.nlp()
        # create a row with the data you need using attributes
        record = {'date': article.publish_date, 'source': url, 'keywords': article.keywords}
        # append info about each url as a new row
        saved_data = saved_data.append(record, ignore_index = true)

    return saved_data

now, when you run this function
add_data_to_df(urls, saved_data), you should see a data frame with contents similar to the ones i got below during testing:
date    source  keywords
0   2022-02-02 00:00:00    [nigeria, securing, oyo, state, senator, prote...
1   2021-09-30 04:25:24+00:00      [shutdown, terrorists, nigeria, guardian, decl...
2   2021-10-24 00:00:00    [terrorists, nigerian, declare, state, militar...
3   2021-10-05 14:41:48+00:00      [president, buhari, house, national, lawmaker,...
4   2021-07-31 00:30:47+00:00      [plans, congress, deal, nigeria, nigerian, rig...
(sorry for the format, i am showing the output as plain text since i am not allowed to attach screenshots, but you will have a nice pandas format)
edit: adding a function to save the data frame to a csv file. note that this one of the shortest ways of doing this and it will save the file to the current working directory, i.e., where you are executing your code:
# this function saves given data to csv    
def save_to_csv(saved_data):
        saved_data.to_csv('output.csv', index=false, sep=',')

# process the articles and create a csv
save_to_csv(add_data_to_df(urls, saved_data))",https://stackoverflow.com/questions/71196936,text,20-02-2022 17:27,67.0,0.0,1.0,True,24-02-2022 02:17,20-02-2022 17:59
78597097,azure ai search multiindex / conditional semantic search,"i'm working with azure openai to create a chatbot for retrieving data from massive enterprise textual data. at my editorial company, i'm facing a challenge with azure ai search. initially, all the data was in a single index, but now i need to separate it into three different indices due to conditional search requirements. here are the details:

index 1: biology index (private, fr)
index 2: engineering and technology index (en)
index 3: art and architecture index (usa, uk)

these indices contain various data sources and publications, and there is overlap in topics across them. for example, when querying about anatomy-related topics like eyesight, cardiovascular diseases, or growth hormone therapy, i want these queries, and related biological topics, to exclusively retrieve data from the biology index (index 2).
my python code effectively retrieves accurate data (with a one single index), but i'm looking for a solution within azure ai search to prioritize specific indices based on query context.
for example:

queries related to biology should exclusively retrieve data from indices 1 and 2.

queries related to technology, data science, and ai should exclusively retrieve data from index 2.


i haven't come across a service or github repository that directly addresses this specific requirement. i know that azure does not allow multi-index search.
how can i find a solution or workaround?
this is the code i use to rag
index_name = 'indx-editorials-bio-fr-old'

# query to execute
query = 'please retrieve publications from editorial certified houses covering cardiovascular diseases'

# function to execute the query with semantic ranking
def execute_query_with_semantic_ranking():
    try:
        # create a searchclient for the index
        credential = azurekeycredential(admin_key)
        client = searchclient(endpoint=endpoint, index_name=index_name, credential=credential)
        
        # execute the query with semantic ranking
        results = client.search(search_text=query, semantic_fields=[""content"", ""title""])
        
        # print the results
        print(f""results from index '{index_name}' with semantic ranking:"")
        for result in results:
            print(result)
        print()
    
    except exception as e:
        print(f""error querying index '{index_name}' with semantic ranking: {e}"")

# execute the query with semantic ranking
execute_query_with_semantic_ranking()

index definition:
{
  ""@odata.context"": ""search.windows.net"",
  ""@odata.etag"": ""\""123547858wrf\"""",
  ""name"": ""all_articles_index"",
  ""defaultscoringprofile"": null,
  ""fields"": [
    {
      ""name"": ""content"",
      ""type"": ""edm.string"",
      ""searchable"": true,
      ""filterable"": true,
      ""retrievable"": true,
      ""stored"": true,
      ""sortable"": true,
      ""facetable"": false,
      ""key"": false,
      ""indexanalyzer"": null,
      ""searchanalyzer"": null,
      ""analyzer"": null,
      ""normalizer"": null,
      ""dimensions"": null,
      ""vectorsearchprofile"": null,
      ""vectorencoding"": null,
      ""synonymmaps"": []
    },
    {
      ""name"": ""title"",
      ""type"": ""edm.string"",
      ""searchable"": true,
      ""filterable"": true,
      ""retrievable"": true,
      ""stored"": true,
      ""sortable"": true,
      ""facetable"": false,
      ""key"": false,
      ""indexanalyzer"": null,
      ""searchanalyzer"": null,
      ""analyzer"": null,
      ""normalizer"": null,
      ""dimensions"": null,
      ""vectorsearchprofile"": null,
      ""vectorencoding"": null,
      ""synonymmaps"": []
    },
    {
      ""name"": ""doi"",
      ""type"": ""edm.string"",
      ""searchable"": false,
      ""filterable"": false,
      ""retrievable"": true,
      ""stored"": true,
      ""sortable"": false,
      ""facetable"": false,
      ""key"": false,
      ""indexanalyzer"": null,
      ""searchanalyzer"": null,
      ""analyzer"": null,
      ""normalizer"": null,
      ""dimensions"": null,
      ""vectorsearchprofile"": null,
      ""vectorencoding"": null,
      ""synonymmaps"": []
    },
    {
      ""name"": ""editorial_house"",
      ""type"": ""edm.string"",
      ""searchable"": false,
      ""filterable"": false,
      ""retrievable"": true,
      ""stored"": true,
      ""sortable"": false,
      ""facetable"": false,
      ""key"": false,
      ""indexanalyzer"": null,
      ""searchanalyzer"": null,
      ""analyzer"": null,
      ""normalizer"": null,
      ""dimensions"": null,
      ""vectorsearchprofile"": null,
      ""vectorencoding"": null,
      ""synonymmaps"": []
    },
    {
      ""name"": ""metadata_storage_path"",
      ""type"": ""edm.string"",
      ""searchable"": false,
      ""filterable"": false,
      ""retrievable"": true,
      ""stored"": true,
      ""sortable"": false,
      ""facetable"": false,
      ""key"": true,
      ""indexanalyzer"": null,
      ""searchanalyzer"": null,
      ""analyzer"": null,
      ""normalizer"": null,
      ""dimensions"": null,
      ""vectorsearchprofile"": null,
      ""vectorencoding"": null,
      ""synonymmaps"": []
    }
  ],
  ""scoringprofiles"": [],
  ""corsoptions"": null,
  ""suggesters"": [],
  ""analyzers"": [],
  ""normalizers"": [],
  ""tokenizers"": [],
  ""tokenfilters"": [],
  ""charfilters"": [],
  ""encryptionkey"": null,
  ""similarity"": {
    ""@odata.type"": ""bm25similarity"",
    ""k1"": null,
    ""b"": null
  },
  ""semantic"": {
    ""defaultconfiguration"": null,
    ""configurations"": [
      {
        ""name"": ""article-semantic"",
        ""prioritizedfields"": {
          ""titlefield"": {
            ""fieldname"": ""title""
          },
          ""prioritizedcontentfields"": [
            {
              ""fieldname"": ""content""
            }
          ],
          ""prioritizedkeywordsfields"": []
        }
      }
    ]
  },
  ""vectorsearch"": null
}

sample data
[
  {
    ""content"": ""this article explores the potential of ai to revolutionize genomics, highlighting recent breakthroughs and future prospects."",
    ""title"": ""the impact of ai on genomics: recent breakthroughs and future prospects"",
    ""doi"": ""10.1234/ai-bio-2024-001"",
    ""editorial_house"": ""biotech publishers"",
    ""metadata_storage_path"": ""/articles/2024/ai-bio-2024-001""
  },
  {
    ""content"": ""in this study, we discuss the integration of machine learning in drug discovery processes, focusing on its benefits and challenges."",
    ""title"": ""machine learning in drug discovery: benefits and challenges"",
    ""doi"": ""10.1234/ai-bio-2024-002"",
    ""editorial_house"": ""biotech publishers"",
    ""metadata_storage_path"": ""/articles/2024/ai-bio-2024-002""
  },
  {
    ""content"": ""this paper examines the role of ai in ecological monitoring, presenting case studies on wildlife conservation efforts."",
    ""title"": ""ai in ecological monitoring: wildlife conservation case studies"",
    ""doi"": ""10.1234/ai-bio-2024-003"",
    ""editorial_house"": ""biotech publishers"",
    ""metadata_storage_path"": ""/articles/2024/ai-bio-2024-003""
  },
  {
    ""content"": ""the article reviews advances in bioinformatics driven by ai, with a focus on data analysis techniques and their applications."",
    ""title"": ""advances in bioinformatics: ai-driven data analysis techniques"",
    ""doi"": ""10.1234/ai-bio-2024-004"",
    ""editorial_house"": ""biotech publishers"",
    ""metadata_storage_path"": ""/articles/2024/ai-bio-2024-004""
  },
  {
    ""content"": ""this study highlights the use of ai in personalized medicine, detailing the technology's impact on treatment plans and patient outcomes."",
    ""title"": ""personalized medicine: ai's role in tailoring treatment plans"",
    ""doi"": ""10.1234/ai-bio-2024-005"",
    ""editorial_house"": ""biotech publishers"",
    ""metadata_storage_path"": ""/articles/2024/ai-bio-2024-005""
  }
]","['azure', 'indexing', 'openai-api', 'azure-openai', 'azure-ai-search']",78692023,"yes as you said multi index query is not possible.
and for your problem below is the possible approach you can follow.
you said you are creating 3 new index, along with that you also need to have 4th index with all of your content, topic and index name as fields.
sample data
{
""index_name"":""biology index"",
""content"":""all of your content having the topic about biology""
},
{
""index_name"":""engineering and technology index"",
""content"":""all of your content having the topic about engineering and technology""
},
{
""index_name"":""art and architecture index"",
""content"":""all of your content having the topic about art and architecture index""
}

so, create a 4th index with above kind of sample data, if you have more than 1 documents for each of the topic then combine them and add it in the content field.
next, do query with the input on this 4th index and get the index name from the results which is having highest search.score in the results and use that in your python code for further querying.",https://stackoverflow.com/questions/78597097,azure,08-06-2024 22:31,696.0,0.0,1.0,True,01-07-2024 11:42,22-06-2024 20:46
68624392,running out of memory with pytorch,"i am trying to train a model using huggingface's wav2vec for audio classification. i keep getting this error:
the following columns in the training set  don't have a corresponding argument in `wav2vec2forspeechclassification.forward` and have been ignored: name, emotion, path.
***** running training *****
  num examples = 2708
  num epochs = 1
  instantaneous batch size per device = 4
  total train batch size (w. parallel, distributed & accumulation) = 64
  gradient accumulation steps = 2
  total optimization steps = 42
 [ 2/42 : < :, epoch 0.02/1]
step    training loss   validation loss

runtimeerror: caught runtimeerror in replica 0 on device 0.
original traceback (most recent call last):
  file ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 61, in _worker
    output = module(*input, **kwargs)
  file ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  file ""<ipython-input-81-dd9fe3ea0f13>"", line 77, in forward
    return_dict=return_dict,
  file ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  file ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py"", line 1073, in forward
    return_dict=return_dict,
  file ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  file ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py"", line 732, in forward
    hidden_states, attention_mask=attention_mask, output_attentions=output_attentions
  file ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  file ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py"", line 574, in forward
    hidden_states = hidden_states + self.feed_forward(self.final_layer_norm(hidden_states))
  file ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  file ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py"", line 510, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)
  file ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/functional.py"", line 1555, in gelu
    return torch._c._nn.gelu(input)
runtimeerror: cuda out of memory. tried to allocate 20.00 mib (gpu 0; 11.17 gib total capacity; 10.49 gib already allocated; 11.44 mib free; 10.68 gib reserved in total by pytorch)

i'm on an aws ubuntu deep learning ami ec2.
i've been researching this a lot. i've already tried:

reducing the batch size (i want 4, but i've gone down to 1 with no change in error)
adding:
import gc
gc.collect()
torch.cuda.empty_cache()


removing all wav files in my dataset that are longer than 6 seconds

is there anything else i can do? i'm on a p2.8xlarge dataset with 105 gib mounted.
running torch.cuda.memory_summary(device=none, abbreviated=false)  gives me:
|===========================================================================|\n|                  pytorch cuda memory summary, device id 0                 |\n|---------------------------------------------------------------------------|\n|            cuda ooms: 3            |        cudamalloc retries: 4         |\n|===========================================================================|\n|        metric         | cur usage  | peak usage | tot alloc  | tot freed  |\n|---------------------------------------------------------------------------|\n| allocated memory      |    7550 mb |   10852 mb |  209624 mb |  202073 mb |\n|       from large pool |    7544 mb |   10781 mb |  209325 mb |  201780 mb |\n|       from small pool |       5 mb |      87 mb |     298 mb |     293 mb |\n|---------------------------------------------------------------------------|\n| active memory         |    7550 mb |   10852 mb |  209624 mb |  202073 mb |\n|       from large pool |    7544 mb |   10781 mb |  209325 mb |  201780 mb |\n|       from small pool |       5 mb |      87 mb |     298 mb |     293 mb |\n|---------------------------------------------------------------------------|\n| gpu reserved memory   |   10936 mb |   10960 mb |   63236 mb |   52300 mb |\n|       from large pool |   10928 mb |   10954 mb |   63124 mb |   52196 mb |\n|       from small pool |       8 mb |      98 mb |     112 mb |     104 mb |\n|---------------------------------------------------------------------------|\n| non-releasable memory |  443755 kb |    1309 mb |  155426 mb |  154992 mb |\n|       from large pool |  443551 kb |    1306 mb |  155081 mb |  154648 mb |\n|       from small pool |     204 kb |      12 mb |     344 mb |     344 mb |\n|---------------------------------------------------------------------------|\n| allocations           |    1940    |    2622    |   32288    |   30348    |\n|       from large pool |    1036    |    1618    |   21855    |   20819    |\n|       from small pool |     904    |    1203    |   10433    |    9529    |\n|---------------------------------------------------------------------------|\n| active allocs         |    1940    |    2622    |   32288    |   30348    |\n|       from large pool |    1036    |    1618    |   21855    |   20819    |\n|       from small pool |     904    |    1203    |   10433    |    9529    |\n|---------------------------------------------------------------------------|\n| gpu reserved segments |     495    |     495    |    2169    |    1674    |\n|       from large pool |     491    |     491    |    2113    |    1622    |\n|       from small pool |       4    |      49    |      56    |      52    |\n|---------------------------------------------------------------------------|\n| non-releasable allocs |     179    |     335    |   15998    |   15819    |\n|       from large pool |     165    |     272    |   12420    |   12255    |\n|       from small pool |      14    |      63    |    3578    |    3564    |\n|===========================================================================|\n'

after reducing data only to inputs that are less tahn 2 seconds in length, it trains a lot further but still errors with this:
the following columns in the training set  don't have a corresponding argument in `wav2vec2forspeechclassification.forward` and have been ignored: path, emotion, name.
***** running training *****
  num examples = 1411
  num epochs = 1
  instantaneous batch size per device = 4
  total train batch size (w. parallel, distributed & accumulation) = 64
  gradient accumulation steps = 2
  total optimization steps = 22
/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/_tensor.py:575: userwarning: floor_divide is deprecated, and will be removed in a future version of pytorch. it currently rounds toward 0 (like the 'trunc' function not 'floor'). this results in incorrect rounding for negative values.
to keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (triggered internally at  /pytorch/aten/src/aten/native/binaryops.cpp:467.)
  return torch.floor_divide(self, other)
/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: userwarning: was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('was asked to gather along dimension 0, but all '
 [11/22 01:12 < 01:28, 0.12 it/s, epoch 0.44/1]
step    training loss   validation loss accuracy
10  2.428100    2.257138    0.300283
the following columns in the evaluation set  don't have a corresponding argument in `wav2vec2forspeechclassification.forward` and have been ignored: path, emotion, name.
***** running evaluation *****
  num examples = 353
  batch size = 32
saving model checkpoint to trainingargs/checkpoint-10
configuration saved in trainingargs/checkpoint-10/config.json
model weights saved in trainingargs/checkpoint-10/pytorch_model.bin
configuration saved in trainingargs/checkpoint-10/preprocessor_config.json
---------------------------------------------------------------------------
oserror                                   traceback (most recent call last)
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py in save(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)
    378             with _open_zipfile_writer(opened_file) as opened_zipfile:
--> 379                 _save(obj, opened_zipfile, pickle_module, pickle_protocol)
    380                 return

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py in _save(obj, zip_file, pickle_module, pickle_protocol)
    498         num_bytes = storage.size() * storage.element_size()
--> 499         zip_file.write_record(name, storage.data_ptr(), num_bytes)
    500 

oserror: [errno 28] no space left on device

during handling of the above exception, another exception occurred:

runtimeerror                              traceback (most recent call last)
<ipython-input-25-3435b262f1ae> in <module>
----> 1 trainer.train()

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1334                     self.control = self.callback_handler.on_step_end(args, self.state, self.control)
   1335 
-> 1336                     self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
   1337                 else:
   1338                     self.control = self.callback_handler.on_substep_end(args, self.state, self.control)

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/trainer.py in _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)
   1441 
   1442         if self.control.should_save:
-> 1443             self._save_checkpoint(model, trial, metrics=metrics)
   1444             self.control = self.callback_handler.on_save(self.args, self.state, self.control)
   1445 

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/trainer.py in _save_checkpoint(self, model, trial, metrics)
   1531         elif self.args.should_save and not self.deepspeed:
   1532             # deepspeed.save_checkpoint above saves model/optim/sched
-> 1533             torch.save(self.optimizer.state_dict(), os.path.join(output_dir, ""optimizer.pt""))
   1534             with warnings.catch_warnings(record=true) as caught_warnings:
   1535                 torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, ""scheduler.pt""))

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py in save(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)
    378             with _open_zipfile_writer(opened_file) as opened_zipfile:
    379                 _save(obj, opened_zipfile, pickle_module, pickle_protocol)
--> 380                 return
    381         _legacy_save(obj, opened_file, pickle_module, pickle_protocol)
    382 

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py in __exit__(self, *args)
    257 
    258     def __exit__(self, *args) -> none:
--> 259         self.file_like.write_end_of_file()
    260         self.buffer.flush()
    261 

runtimeerror: [enforce fail at inline_container.cc:298] . unexpected pos 1849920000 vs 1849919888

when i run !free in the notebook, i get:
the history saving thread hit an unexpected error (operationalerror('database or disk is full')).history will not be written to the database.
              total        used        free      shared  buff/cache   available
mem:      503392908     6223452   478499292      346492    18670164   492641984
swap:             0           0           0

for training code, i am essentially running this colab notebook as an example:

all that i am changing is the incoming data/labels, which i have intentionally fit into the same directory structure used in the tutorial notebook. the tutorial notebook runs fine for some reason, even though my data has comparable size/num classes.","['deep-learning', 'pytorch', 'huggingface-transformers']",68893434,"you might use the dataparallel or distributeddataparallel framework in pytorch
model = model(input_size, output_size)
if torch.cuda.device_count() > 1:
  print(""let's use"", torch.cuda.device_count(), ""gpus!"")
  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 gpus
  model = nn.dataparallel(model)

model.to(device)

in this approach the model get replicated on each device (gpu) and the data is distributed across devices

dataparallel splits your data automatically and sends job orders to
multiple models on several gpus. after each model finishes their job,
dataparallel collects and merges the results before returning it to
you.

further examples here 
if the model does not fit in the memory of one gpu, then a model parallel approach should be resorted to.
from your existing model you might tell which layer sits on which gpu with .to('cuda:0'), .to('cuda:1') etc.
class modelparallelresnet50(resnet):
    def __init__(self, *args, **kwargs):
        super(modelparallelresnet50, self).__init__(
            bottleneck, [3, 4, 6, 3], num_classes=num_classes, *args, **kwargs)

        self.seq1 = nn.sequential(
            self.conv1,
            self.bn1,
            self.relu,
            self.maxpool,

            self.layer1,
            self.layer2
        ).to('cuda:0')

        self.seq2 = nn.sequential(
            self.layer3,
            self.layer4,
            self.avgpool,
        ).to('cuda:1')

        self.fc.to('cuda:1')

    def forward(self, x):
        x = self.seq2(self.seq1(x).to('cuda:1'))
        return self.fc(x.view(x.size(0), -1))

since you might lose performance, a pipelining approach might be of use, i.e. further chunking input data into batches which are run in parallel on the different devices.",https://stackoverflow.com/questions/68624392,deep-learning,02-08-2021 15:39,2664.0,1.0,1.0,True,23-08-2021 13:17,02-08-2021 19:38
31143015,docker nltk download,"i am building a docker container using the following dockerfile:
from ubuntu:14.04

run apt-get update

run apt-get install -y python python-dev python-pip

add . /app

run apt-get install -y python-scipy

run pip install -r /arrc/requirements.txt

expose 5000

workdir /app

cmd python app.py

everything goes well until i run the image and get the following error:
**********************************************************************
  resource u'tokenizers/punkt/english.pickle' not found.  please
  use the nltk downloader to obtain the resource:  >>>
  nltk.download()
  searched in:
    - '/root/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
    - u''
**********************************************************************

i have had this problem before and it is discussed here however i am not sure how to approach it using docker. i have tried:
cmd python
cmd import nltk
cmd nltk.download()

as well as:
cmd python -m nltk.downloader -d /usr/share/nltk_data popular

but am still getting the error.","['python', 'docker', 'nltk']",31467809,"in your dockerfile, try adding instead:

run python -m nltk.downloader punkt

this will run the command and install the requested files to //nltk_data/
the problem is most likely related to using cmd vs. run in the dockerfile. documentation for cmd:

the main purpose of a cmd is to provide defaults for an executing container.

which is used during docker run <image>, not during build. so other cmd lines probably were overwritten by the last cmd python app.py line.",https://stackoverflow.com/questions/31143015,python,30-06-2015 15:56,35631.0,52.0,5.0,True,02-10-2024 05:51,27-07-2017 16:10
69660201,"inputting some data for bert model, using tf.data.dataset.from_tensor_slices","here's my model:
def build_classifier_model():
    text_input = tf.keras.layers.input(shape=(), dtype=tf.string, name='features')
    preprocessing_layer = hub.keraslayer(tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.keraslayer(tfhub_handle_encoder, trainable=true, name='bert_encoder')
    outputs = encoder(encoder_inputs)
    net = outputs['pooled_output']
    net = tf.keras.layers.dropout(0.1)(net)
    net = tf.keras.layers.dense(3, activation=""softmax"", name='classifier')(net)
    return tf.keras.model(text_input, net)

in the preprocessing layer, i'm using a bert preprocessor from tf-hub.
i've already divided the data into corpus_train, corpus_test, labels_train, labels_test.
the corpuses are panda dataframes with the texts that will be used as features, and the labels are numpy arrays.
corpus=df_speech_en_merged[""contents""]
corpus.shape
(1768,)

labels=np.asarray(df_speech_en_merged[""classes""].astype(""int""))
labels.shape
(1768,)

to create the train and test data set, i've used the following:
train_dataset = (
    tf.data.dataset.from_tensor_slices(
        {
            ""features"":tf.cast(corpus_train.values, tf.string),
            ""labels"":tf.cast(labels_train, tf.int32) #labels is already an array, no need for .values
        }
    )
test_dataset = tf.data.dataset.from_tensor_slices(
    {""features"":tf.cast(corpus_test.values, tf.string),
     ""labels"":tf.cast(labels_test, tf.int32)
    } #labels is already an array, no need for .values
    )
)

after building and compiling the model without any error message, when i fit the model with:
classifier_model.fit(x=train_dataset,
                               validation_data=test_dataset,
                               epochs=2)

i get the following error:
valueerror: could not find matching function to call loaded from the savedmodel. got:
      positional arguments (3 total):
        * tensor(""inputs:0"", shape=(), dtype=string)
        * false
        * none
      keyword arguments: {}

expected these arguments to match one of the following 4 option(s):

option 1:
  positional arguments (3 total):
    * tensorspec(shape=(none,), dtype=tf.string, name='sentences')
    * false
    * none
  keyword arguments: {}

option 2:
  positional arguments (3 total):
    * tensorspec(shape=(none,), dtype=tf.string, name='sentences')
    * true
    * none
  keyword arguments: {}

option 3:
  positional arguments (3 total):
    * tensorspec(shape=(none,), dtype=tf.string, name='inputs')
    * false
    * none
  keyword arguments: {}

option 4:
  positional arguments (3 total):
    * tensorspec(shape=(none,), dtype=tf.string, name='inputs')
    * true
    * none
  keyword arguments: {}

i think this error occurs because i'm either building train_dataset/test_dataset wrong or because the text_input layer is expecting the wrong type of data. any help would be appreciated.","['tensorflow', 'nlp', 'bert-language-model']",69661276,"when using tf.data.dataset.from_tensor_slices, try providing a batch_size, since the bert preprocessing layer expects a very specific shape. here is a simplified, working example based on the bert models used in this tutorial and your specific details:
def build_classifier_model():
    text_input = tf.keras.layers.input(shape=(), dtype=tf.string, name='features')
    preprocessing_layer = hub.keraslayer(tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.keraslayer(tfhub_handle_encoder, trainable=true, name='bert_encoder')
    outputs = encoder(encoder_inputs)
    net = outputs['pooled_output']
    net = tf.keras.layers.dropout(0.1)(net)
    net = tf.keras.layers.dense(3, activation=""softmax"", name='classifier')(net)
    return tf.keras.model(text_input, net)

sentences = tf.constant([
""improve the physical fitness of your goldfish by getting him a bicycle"",
""you are unsure whether or not to trust him but very thankful that you wore a turtle neck"",
""not all people who wander are lost"", 
""there is a reason that roses have thorns"",
""charles ate the french fries knowing they would be his last meal"",
""he hated that he loved what she hated about hate"",
])

labels = tf.random.uniform((6, ), minval=0, maxval=2, dtype=tf.dtypes.int32)

classifier_model = build_classifier_model()
classifier_model.compile(optimizer=tf.keras.optimizers.adam(),
                         loss=tf.keras.losses.sparsecategoricalcrossentropy(),
                         metrics=tf.keras.metrics.sparsecategoricalaccuracy())
batch_size = 1
train_dataset = tf.data.dataset.from_tensor_slices(
        (sentences, labels)).shuffle(
        sentences.shape[0]).batch(
        batch_size)
    
classifier_model.fit(x=train_dataset, epochs=2)

epoch 1/2
6/6 [==============================] - 7s 446ms/step - loss: 2.4348 - sparse_categorical_accuracy: 0.5000
epoch 2/2
6/6 [==============================] - 3s 447ms/step - loss: 1.3977 - sparse_categorical_accuracy: 0.5000",https://stackoverflow.com/questions/69660201,tensorflow,21-10-2021 10:24,649.0,1.0,1.0,True,21-10-2021 12:51,21-10-2021 11:00
69247049,gensim word2vec vocabulary size fluctuates up &amp; down as corpus grows despite `max_vocab_size` setting,"i am training word embeddings using gensim word2vec model with a multi-million sentence corpus that is made of 3 million unique tokens with max_vocab_size = 32_000.
even though i set min_count = 1, model creates a vocabulary of far less than 32_000. when i use a subset of the corpus, vocabulary size increases!
in order to troubleshoot, i set up an experiment where i control the size of vocabulary with different sized subcorpus. the size of the vocabulary flactuates!
you can re-produce with the code below:
import string
import numpy as np
from gensim.models import word2vec

letters = list(string.ascii_lowercase)

# creating toy sentences
sentences = []
number_of_sentences = 100_000

for _ in range(number_of_sentences):
    number_of_tokens = np.random.randint(1, 15, 1)[0]
    sentence = []
    for i in range(number_of_tokens):
        token = """"
        len_of_token = np.random.randint(1, 5, 1)[0]
        for j in range(len_of_token):
            token += np.random.choice(letters)
        sentence.append(token)
    sentences.append(sentence)

# sanity check to ensure that input data is a list of list of strings(tokens)
for _ in range(4):
    print(np.random.choice(sentences))

# collecting some statistics about tokens
flattened = []
for sublist in sentences:
    for item in sublist:
        flattened.append(item)
        
unique_tokens = {}
for token in flattened:
    if token not in unique_tokens:
        unique_tokens[token] = len(unique_tokens)

print('number of tokens:', f'{len(flattened):,}')
print('number of unique tokens:', f'{len(unique_tokens):,}')


# gensim model
vocab_size = 32_000
min_count = 1
collected_data = []
for num_sentence in range(5_000, number_of_sentences + 5_000, 5_000):
    model = word2vec(min_count=min_count, max_vocab_size= vocab_size)
    model.build_vocab(sentences[:num_sentence])

    collected_data.append((num_sentence, len(model.wv.key_to_index)))

for duo in collected_data:
    print('vocab size of', duo[1], 'for', duo[0], 'number of sentences!')

output:
['cpi', 'bog', 'df', 'tgi', 'xck', 'kkh', 'ktw', 'ay']
['z', 'h', 'w', 'jek', 'w', 'dqm', 'wfb', 'agq', 'egrg']
['kgwb', 'lahf', 'kzx', 'd', 'qdok', 'xka', 'hbiz', 'bjo', 'fvk', 'j', 'hx']
['old', 'c', 'ik', 'n', 'e', 'n', 'o', 'r', 'ehx', 'dlud', 'd']

number of tokens: 748,383
number of unique tokens: 171,485

vocab size of 16929 for 5000 number of sentences!
vocab size of 30314 for 10000 number of sentences!
vocab size of 19017 for 15000 number of sentences!
vocab size of 31394 for 20000 number of sentences!
vocab size of 19564 for 25000 number of sentences!
vocab size of 31831 for 30000 number of sentences!
vocab size of 19543 for 35000 number of sentences!
vocab size of 31744 for 40000 number of sentences!
vocab size of 19536 for 45000 number of sentences!
vocab size of 31642 for 50000 number of sentences!
vocab size of 18806 for 55000 number of sentences!
vocab size of 31255 for 60000 number of sentences!
vocab size of 18497 for 65000 number of sentences!
vocab size of 31166 for 70000 number of sentences!
vocab size of 18142 for 75000 number of sentences!
vocab size of 30886 for 80000 number of sentences!
vocab size of 17693 for 85000 number of sentences!
vocab size of 30390 for 90000 number of sentences!
vocab size of 17007 for 95000 number of sentences!
vocab size of 30196 for 100000 number of sentences!

i tried increasing min_count but it did not help this flactuation of vocabulary size. what am i missing?","['python', 'nlp', 'gensim', 'word2vec', 'word-embedding']",69248527,"in gensim, the max_vocab_size parameter is a very crude mechanism to limit ram usage during the initial scan of the training corpus to discover the vocabulary. you should only use this parameter if it's the only way to work around ram problems.
essentially: try without using max_vocab_size. if you want control over which words are retained, use alternate parameters like min_count (to discard words less-frequent than a certain threshold) or max_final_vocab (to take no more than a set number of the most-frequent words).
if and only if you hit out-of-memory errors (or massive virtual-memory swapping), then consider using max_vocab_size.
but even then, because of the way it works, you still wouldn't want to set max_vocab_size to the actual final size you want. instead, you should set it to some value much, much larger - but just small enough to not exhaust your ram.
this allows the most accurate possible word-counts before other parameters (like min_count & max_final_vocab) are applied.
if you instead use a low max_vocab_size, the running survey will prematurely trim the counts any time the number of known words reaches that value. that is, as soon as the interim count reaches that many entries, say max_vocab_size=32000, many of the least-frequent counts are forgotten to cap memory usage (and more each time the threshold is reached).
that makes all final counts approximate (based on how often a term missed the cutoff), and means the final number of unique tokens in the full survey will be some value even less than max_vocab_size, somewhat arbitrarily based on how recently a forgetting-trim was triggered. (hence, the somewhat random, but always lower than max_vocab_size, counts seen in your experiment output.)
so: max_vocab_size is unlikely to do what most people want, or in a predictable way. still, it can help a fuzzy survey complete for extreme corpora where unique terms would otherwise overflow ram.
separately: min_count=1 is usually a bad idea in word2vec, as words that lack sufficient varied usage examples won't themselves get good word-vectors, but leaving all such poorly-represented words in the training data tends to serve as noise that dilutes (& delays) what can be learned about adequately-frequent words.",https://stackoverflow.com/questions/69247049,python,19-09-2021 21:01,683.0,0.0,1.0,True,20-09-2021 02:31,20-09-2021 02:31
70325758,mapping values from a dictionary&#39;s list to a string in python,"i am working on some sentence formation like this:
sentence = ""person is adjective""
dictionary = {""person"": [""alice"", ""bob"", ""carol""], ""adjective"": [""cute"", ""intelligent""]}

i would now need all possible combinations to form this sentence from the dictionary, like:
alice is cute
alice is intelligent
bob is cute
bob is intelligent
carol is cute
carol is intelligent

the above use case was relatively simple, and it was done with the following code
dictionary = {""person"": [""alice"", ""bob"", ""carol""], ""adjective"": [""cute"", ""intelligent""]}

for i in dictionary[""person""]:
    for j in dictionary[""adjective""]:
        print(f""{i} is {j}"")

but can we also make this scale up for longer sentences?
example:
sentence = ""person is adjective and is from country"" 
dictionary = {""person"": [""alice"", ""bob"", ""carol""], ""adjective"": [""cute"", ""intelligent""], ""country"": [""usa"", ""japan"", ""china"", ""india""]}

this should again provide all possible combinations like:
alice is cute and is from usa
alice is intelligent and is from usa
.
.
.
.
carol is intelligent and is from india

i tried to use  , but the sentence are all are mixed up - but how can we make a few words fixed, like in this example the words ""and is from"" is fixed
essentially if any key in the dictionary is equal to the word in the string, then the word should be replaced by the list of values from the dictionary.
any thoughts would be really helpful.","['python', 'python-3.x', 'list', 'python-2.7', 'nlp']",70326469,"i would base my answer off of two building blocks itertools.product and zip.
itertools.product will allow us to get the various combinations of our dictionary list values
zip with the original keys and the combinations above will allow us to create a list of tuples that we can use with replace.
import itertools

sentence = ""person is adjective and is from country""
dictionary = {""person"": [""alice"", ""bob"", ""carol""], ""adjective"": [""cute"", ""intelligent""], ""country"": [""usa"", ""japan"", ""china"", ""india""]}

keys = dictionary.keys()
for values in itertools.product(*dictionary.values()):
    new_sentence = sentence
    for tpl in zip(keys, values):
        new_sentence = new_sentence.replace(*tpl)
    print(new_sentence)

if you happen to have the ability to control the ""sentence"" template, and you can do:
sentence = ""{person} is {adjective} and is from {country}""

then you can simplify this to:
sentence = ""{person} is {adjective} and is from {country}""
dictionary = {""person"": [""alice"", ""bob"", ""carol""], ""adjective"": [""cute"", ""intelligent""], ""country"": [""usa"", ""japan"", ""china"", ""india""]}

keys = dictionary.keys()
for values in itertools.product(*dictionary.values()):
    new_sentence = sentence.format(**dict(zip(keys, values)))
    print(new_sentence)

both should give you the results like:
alice is cute and is from usa
alice is cute and is from japan
...
carol is intelligent and is from china
carol is intelligent and is from india

note that the order of appearance in the template is not important and both solutions should work with a template of:
sentence = ""person is from country and is adjective""

or in case 2
sentence = ""{person} is from {country} and is {adjective}""

followup:
what happens if there is a chance that the dictionary contains items that are not in the sentence template? at the moment, that is not ideal as the way the sentences are generated with product() assumes that all keys are and we currently would generate duplicates.
the easiest fix for that would be to just ensure that the dictionary only has keys of interest...
in the first case this might do that.
dictionary = {key: value for key, value in dictionary.items() if key in sentence}

or in the second case:
dictionary = {key: value for key, value in dictionary.items() if f""{{{key}}}"" in sentence}",https://stackoverflow.com/questions/70325758,python,12-12-2021 17:01,767.0,3.0,2.0,True,21-12-2021 16:45,12-12-2021 17:18
75299615,openai api: can i remove the line break from the response with a parameter?,"i've starting using openai api in r. i downloaded the openai package. i keep getting a double linebreak in the text response. here's an example of my code:

library(openai)

vector = create_completion(
  model = ""text-davinci-003"",
  prompt = ""tell me what the weather is like in london, uk, in celsius in 5 words."",
  max_tokens = 20,
  temperature = 0,
  echo = false
)


vector_2 = vector$choices[1]

vector_2$text


[1] ""\n\nrainy, mild, cool, humid.""


is there a way to get rid of this without 'correcting' the response text using other functions?","['r', 'openai-api', 'gpt-3']",75300061,"no, it's not possible.
the openai api returns the completion with a starting \n\n by default. there's no parameter for the completions endpoint to control this.
you need to remove the line break manually.
an example response looks like this:
{
  ""id"": ""cmpl-uqkvlqyyk7bgyrrhq0exlwi7"",
  ""object"": ""text_completion"",
  ""created"": 1589478378,
  ""model"": ""text-davinci-003"",
  ""choices"": [
    {
      ""text"": ""\n\nthis is indeed a test"",
      ""index"": 0,
      ""logprobs"": null,
      ""finish_reason"": ""length""
    }
  ],
  ""usage"": {
    ""prompt_tokens"": 5,
    ""completion_tokens"": 7,
    ""total_tokens"": 12
  }
}",https://stackoverflow.com/questions/75299615,r,31-01-2023 15:46,6369.0,4.0,4.0,True,13-05-2024 05:06,13-03-2023 14:04
63888551,illegalargumentexception: ptblexer: invalid options key in constructor: asciiquotes stanford nlp,"i'm trying to test the hello word of stanford pos tagger api in java (i used the same .jar in python and it worked well) on french sentences.
here is my code
public class textpreprocessor {

    private static maxenttagger tagger=new maxenttagger(""../stanford-tagger-4.1.0/stanford-postagger-full-2020-08-06/models/french-ud.tagger"");

    public static void main(string[] args) {
        
        string taggedstring = tagger.tagstring(""salut ï¿½ï¿½ tous, je suis coincï¿½ï¿½"");
        system.out.println(taggedstring);
    }
}


but i get the following exception:
loading pos tagger from c:/users/_nprime496_/downloads/compressed/stanford-tagger-4.1.0/stanford-postagger-full-2020-08-06/models/french-ud.tagger ... done [0.3 sec].
exception in thread ""main"" java.lang.illegalargumentexception: ptblexer: invalid options key in constructor: asciiquotes
    at edu.stanford.nlp.process.ptblexer.<init>(ptblexer.java)
    at edu.stanford.nlp.process.ptbtokenizer.<init&gtenizer.java:285)
    at edu.stanford.nlp.process.ptbtokenizer$ptbtokenizerfactory.gettokenizer(ptbtokenizer.java:698)
    at edu.stanford.nlp.process.documentpreprocessor$plaintextiterator.<init>(documentpreprocessor.java:271)
    at edu.stanford.nlp.process.documentpreprocessor.iterator(documentpreprocessor.java:226)
    at edu.stanford.nlp.tagger.maxent.maxenttagger.tokenizetext(maxenttagger.java:1148)
    at edu.stanford.nlp.tagger.maxent.maxenttagger$taggerwrapper.apply(maxenttagger.java:1332)
    at edu.stanford.nlp.tagger.maxent.maxenttagger.tagstring(maxenttagger.java:999)
    at modules.generation.preprocessing.textpreprocessor.main(textpreprocessor.java:19)

can you help me?","['java', 'stanford-nlp', 'pos-tagger', 'french']",63894011,"you can use this code and the full corenlp package:
package edu.stanford.nlp.examples;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.*;

import java.util.*;


public class pipelineexample {

  public static string text = ""paris est la capitale de la france."";

  public static void main(string[] args) {
    // set up pipeline properties
    properties props = stringutils.argstoproperties(""-props"", ""french"");
    // set the list of annotators to run
    props.setproperty(""annotators"", ""tokenize,ssplit,mwt,pos"");
    // build pipeline
    stanfordcorenlp pipeline = new stanfordcorenlp(props);
    // create a document object
    coredocument document = pipeline.processtocoredocument(text);
    // display tokens
    for (corelabel tok : document.tokens()) {
      system.out.println(string.format(""%s\t%s"", tok.word(), tok.tag()));
    }
  }

}

you can download corenlp here: 
make sure to download the latest french models.
i am not sure why your example with the standalone tagger does not work. what jars were you using?",https://stackoverflow.com/questions/63888551,java,14-09-2020 16:44,326.0,0.0,1.0,True,01-09-2022 09:13,01-09-2022 09:13
77310403,how to post a fine-tuning job to openai api using the correct file format?,"i am new to the openai api and i am trying to understand fine-tuning to use the api.
i checked what finetuning files i have available using this request in vs code rest client:
get 
authorization: bearer {{key}}  

here is part of the response:
{
  ""object"": ""list"",
  ""data"": [
    {
      ""object"": ""file"",
      ""id"": ""file-2flvf7vjsbseofswjaf1xxxx"",
      ""purpose"": ""fine-tune"",
      ""filename"": ""mydata.jsonl"",
      ""bytes"": 493,
      ""created_at"": 1697459301,
      ""status"": ""processed"",
      ""status_details"": null
    },
    {
....


apparently, i have a file object with id ""file-2flvf7vjsbseofswjaf1xxxx"" (last four characters i want to keep secret) for a ""purpose"" named ""fine-tune"". sounds great, so let's use that in a call.
post 
content-type: application/json
authorization: bearer {{key}}

{
  ""training_file"": ""file-2flvf7vjsbseofswjaf1xxxx"",
  ""model"": ""gpt-3.5-turbo""
}

this is the result:
{
  ""error"": {
    ""message"": ""file 'file-2flvf7vjsbseofswjaf1xxxx' is in prompt-completion format. the model gpt-3.5-turbo-0613 requires data in the chat-completion format."",
    ""type"": ""invalid_request_error"",
    ""param"": null,
    ""code"": ""invalid_file_format""
  }
}

i got the error. that data is indeed in prompt-completion instead of chat completion format:
{""prompt"": ""what is the closest planet to the sun?"", ""completion"": ""the closest planet to the sun is mercury.""}
{""prompt"": ""how many moons does mars have?"", ""completion"": ""mars has two moons.""}
{""prompt"": ""what is the largest planet in our solar system?"", ""completion"": ""the largest planet in our solar system is jupiter.""}
{""prompt"": ""what is the main component of saturn's rings?"", ""completion"": ""the main component of saturn's rings is ice particles, with some rocky debris and dust.""}

however, it is still not clear how to fix this. the chat completion format is simply not accepted. here is how i discovered this using this jsonl file
{""chat"": ""translate the following english text to french: 'hello, how are you?'"", ""completion"": ""bonjour, comment ï¿½ï¿½a va?""}
{""chat"": ""what's the capital of france?"", ""completion"": ""paris""}
{""chat"": ""solve for x: 2x + 5 = 11"", ""completion"": ""x = 3""}

which i tried to upload by running this in my wsl (ubuntu) console:
curl    -h ""authorization: bearer [my_key]""   -f purpose=""fine-tune""   -f file=""@finetune.jsonl""

giving this error:
{
  ""error"": {
    ""message"": ""unexpected file format, expected either prompt/completion pairs or chat messages."",
    ""type"": ""invalid_request_error"",
    ""param"": null,
    ""code"": null
  }
}

so how can i possibly solve this?
all i want to is to get a call like this:
post 
content-type: application/json
authorization: bearer {{key}}

{
  ""training_file"": ""file-2flvf7vjsbseofswjaf1xxxx"",
  ""model"": ""gpt-3.5-turbo""
}

working..... but i don't succeed. so please help me getting this working. please come up with a fully working example.

update
appararently, i need to use gpt-3.5-turbo-0613 instead but that is still not solving the problem.
to help me, i need a fully working example. i do not need an explanation. it is clear for me how to do it in theory. however, when i try it in reality with real curl calls. it is not working at all.
all i want is to get a http statuscode 200 from this call. so please give my a fully working example with curl calls.
curl  \
  -h ""content-type: application/json"" \
  -h ""authorization: bearer {{key}}"" \
  -d '{
    ""training_file"": ""file-2flvf7vjsbseofswjaf1xxxx"",
    ""model"": ""gpt-3.5-turbo-0613""
  }'

but this always gives me errors like this.

{
  ""error"": {
    ""message"": ""file 'file-2flvf7vjsbseofswjaf1xxxx' is in prompt-completion format. the model gpt-3.5-turbo-0613 requires data in the chat-completion format."",
    ""type"": ""invalid_request_error"",
    ""param"": null,
    ""code"": ""invalid_file_format""
  }
}

logically, i need to upload a file to do so. i do this with this call first:
curl  \
  -h ""authorization: bearer {{key}}"" \
  -f purpose=""fine-tune"" \
  -f file=""@chatcompletion.jsonl""

i tried these jsonl files:
{""messages"": [{""role"": ""system"", ""content"": ""you are a helpful assistant.""}]}
{""messages"": [{""role"": ""system"", ""content"": ""you are a helpful assistant.""}, {""role"": ""user"", ""content"": ""who won the world series in 2020?""}]}
{""messages"": [{""role"": ""system"", ""content"": ""you are a helpful assistant.""}, {""role"": ""user"", ""content"": ""who won the world series in 2020?""}, {""role"": ""assistant"", ""content"": ""the los angeles dodgers won the world series in 2020.""}]}
{""messages"": [{""role"": ""system"", ""content"": ""you are a helpful assistant.""}, {""role"": ""user"", ""content"": ""tell me a joke.""}]}

{  ""role"": ""user"", ""content"": ""what is the average temperature in new york city in january?"" }
{  ""role"": ""assistant"", ""content"": ""the average temperature in new york city in january is around 32ï¿½ï¿½f (0ï¿½ï¿½c).""}
{  ""role"": ""user"",     ""content"": ""is that safe for humans"" }

{""messages"": [{""role"": ""user"", ""content"": ""tell me about the eiffel tower.""}]}
{""messages"": [{""role"": ""user"", ""content"": ""tell me about the eiffel tower.""}, {""role"": ""assistant"", ""content"": ""the eiffel tower is a wrought-iron lattice tower located in paris, france. it was constructed in 1889 as the entrance arch for the 1889 world's fair and has since become one of the most recognizable structures in the world.""}]}
{""messages"": [{""role"": ""user"", ""content"": ""how tall is it?""}]}
{""messages"": [{""role"": ""user"", ""content"": ""how tall is it?""}, {""role"": ""assistant"", ""content"": ""the eiffel tower is approximately 324 meters (1,063 feet) tall, including its antennas.""}]}

{""messages"": [{""role"": ""user"", ""content"": ""how does photosynthesis work?""}]}
{""messages"": [{""role"": ""user"", ""content"": ""what is the capital of france?""}, {""role"": ""assistant"", ""content"": ""the capital of france is paris.""}, {""role"": ""user"", ""content"": ""how tall is the eiffel tower?""}]}
{""messages"": [{""role"": ""user"", ""content"": ""can you give me a recipe for chocolate chip cookies?""}]}","['rest', 'curl', 'openai-api']",77310633,"the conversation chat format is required to fine-tune gpt-3.5-turbo

{""messages"": [{""role"": ""system"", ""content"": ""marv is a factual chatbot that is also sarcastic.""}, {""role"": ""user"", ""content"": ""what's the capital of france?""}, {""role"": ""assistant"", ""content"": ""paris, as if everyone doesn't know that already.""}]}

curl -x post  \
-h 'authorization: bearer <token>' \
-f file=""@file.jsonl"" \
-f purpose=""fine-tune""

after the file is processed, you can create a fine-tuning model.",https://stackoverflow.com/questions/77310403,rest,17-10-2023 15:40,2834.0,1.0,2.0,True,06-02-2024 11:18,17-10-2023 18:08
69963924,tensorflow keras multiple input model,"i need to adapt this model for two text columns input (instead one column)
tfhub_handle_encoder = \
    ""
tfhub_handle_preprocess = \
""

def build_classifier_model():

text_input = tf.keras.layers.input(
    shape=(), dtype=tf.string, name='text')

preprocessing_layer = hub.keraslayer(
    tfhub_handle_preprocess, name='preprocessing')

encoder_inputs = preprocessing_layer(text_input)
encoder = hub.keraslayer(
    tfhub_handle_encoder, trainable=true, name='bert_encoder')

outputs = encoder(encoder_inputs)
net = outputs['pooled_output']
net = tf.keras.layers.dropout(0.1)(net)
net = tf.keras.layers.dense(
    6, activation='softmax', name='classifier')(net)
model = tf.keras.model(text_input, net)

loss = tf.keras.losses.categoricalcrossentropy(from_logits=false) # (from_logits=true)
metric = tf.metrics.categoricalaccuracy('accuracy')
optimizer = adam(
    learning_rate=5e-05, epsilon=1e-08, decay=0.01, clipnorm=1.0)
model.compile(
    optimizer=optimizer, loss=loss, metrics=metric)
model.summary()
return model

history = classifier_model.fit(
    x=x_train['f'].values,
    y=y_train_c,
    validation_data=(x_valid['f'].values, y_valid_c),
    epochs=15)

seems like this is model from tutorial: 
i have tried modify code for two input layer, but get error because after concatenate there is wrong tensor dimensions:
def build_classifier_model():

input1 = tf.keras.layers.input(
    shape=(), dtype=tf.string, name='text')

input2 = tf.keras.layers.input(
    shape=(), dtype=tf.string, name='text1')
text_input = tf.keras.layers.concatenate([input1, input2], axis=-1)


preprocessing_layer = hub.keraslayer(
    tfhub_handle_preprocess, name='preprocessing')

encoder_inputs = preprocessing_layer(text_input)
encoder = hub.keraslayer(
    tfhub_handle_encoder, trainable=true, name='bert_encoder')

outputs = encoder(encoder_inputs)
net = outputs['pooled_output']
net = tf.keras.layers.dropout(0.1)(net)
net = tf.keras.layers.dense(
    6, activation='softmax', name='classifier')(net)
model = tf.keras.model([input1, input2], net)

loss = tf.keras.losses.categoricalcrossentropy(from_logits=false) # (from_logits=true)
metric = tf.metrics.categoricalaccuracy('accuracy')
optimizer = adam(
    learning_rate=5e-05, epsilon=1e-08, decay=0.01, clipnorm=1.0)
model.compile(
    optimizer=optimizer, loss=loss, metrics=metric)
model.summary()
return model

error:
invalidargumenterror:  logits and labels must be broadcastable: logits_size=[64,6] labels_size=[32,6]
     [[node categorical_crossentropy/softmax_cross_entropy_with_logits (defined at tmp/ipykernel_39/1837193519.py:5) ]] [op:__inference_train_function_271676]

if use concatenate with another dimension then model doensn't compile","['python', 'tensorflow', 'keras', 'deep-learning', 'nlp']",69965171,"weirdly enough, replacing your concatenation layer with tf.strings.join inside your model seems to work:
def build_classifier_model():

  input1 = tf.keras.layers.input(
      shape=(), dtype=tf.string, name='text')

  input2 = tf.keras.layers.input(
      shape=(), dtype=tf.string, name='text1')
  text_input = tf.strings.join([input1, input2])

  preprocessing_layer = hub.keraslayer(
      tfhub_handle_preprocess, name='preprocessing')

  encoder_inputs = preprocessing_layer(text_input)
  encoder = hub.keraslayer(
      tfhub_handle_encoder, trainable=true, name='bert_encoder')

  outputs = encoder(encoder_inputs)
  net = outputs['pooled_output']
  net = tf.keras.layers.dropout(0.1)(net)
  output = tf.keras.layers.dense(
      6, activation='softmax', name='classifier')(net)
  model = tf.keras.model([input1, input2], output)

  loss = tf.keras.losses.categoricalcrossentropy(from_logits=false) # (from_logits=true)
  metric = tf.metrics.categoricalaccuracy('accuracy')
  optimizer = adam(
      learning_rate=5e-05, epsilon=1e-08, decay=0.01, clipnorm=1.0)
  model.compile(
      optimizer=optimizer, loss=loss, metrics=metric)
  model.summary()
  return model

epoch 1/5
 497/1094 [============>.................] - eta: 2:14 - loss: 1.8664 - accuracy: 0.1641

you could also consider simply doing  text_input = input1 + input2 , since the concatenation layer seems to mess up the batch dimension. or you could feed each input to your encoder and concatenate the results afterwards:
def build_classifier_model():

  input1 = tf.keras.layers.input(
      shape=(), dtype=tf.string, name='text')

  input2 = tf.keras.layers.input(
      shape=(), dtype=tf.string, name='text1')

  preprocessing_layer = hub.keraslayer(
      tfhub_handle_preprocess, name='preprocessing')

  encoder_input1 = preprocessing_layer(input1)
  encoder_input2 = preprocessing_layer(input2)
  encoder = hub.keraslayer(
      tfhub_handle_encoder, trainable=true, name='bert_encoder')

  output1 = encoder(encoder_input1)
  output2 = encoder(encoder_input2)

  net = tf.keras.layers.concatenate(axis=-1)([output1['pooled_output'], output2['pooled_output']])
  net = tf.keras.layers.dropout(0.1)(net)
  output = tf.keras.layers.dense(
      6, activation='softmax', name='classifier')(net)
  model = tf.keras.model([input1, input2], output)

  loss = tf.keras.losses.categoricalcrossentropy(from_logits=false) # (from_logits=true)
  metric = tf.metrics.categoricalaccuracy('accuracy')
  optimizer = adam(
      learning_rate=5e-05, epsilon=1e-08, decay=0.01, clipnorm=1.0)
  model.compile(
      optimizer=optimizer, loss=loss, metrics=metric)
  model.summary()
  return model",https://stackoverflow.com/questions/69963924,python,14-11-2021 14:17,1264.0,2.0,1.0,True,02-12-2021 09:05,02-12-2021 09:05
69744611,creating a nested dictionary in python,"i have a dictionary in python and it currently looks like this:
{'apple': ['file1.txt', 'file2.txt', 'file3.txt'], 'banana': ['file1.txt', 'file2.txt'],
'carrot': ['file3.txt'],.....................................}

i have the contents of each file stored in a list of lists that contains the words from that file and also a general list of the files used:
[['hello', 'apple', 'test', 'banana'], ['weird', 'apple', 'tester', 'banana', 'apple'],........]]

['file1.txt', 'file2.txt', .....]

i would now like to create a new nested dictionary that contains all the information from the previous one but also the position in which the term appears in each document (if it exists in that document).
for example:
i'd want print(dictionary['apple']) to return [{'file1.txt': [1]}, {'file2.txt': [1,4]},...... ] (it tells me the document it appears in and its position in that document)
my existing code for creating the dictionary i already have is:

dict = {}
for i in range(len(textfile_list)): #list of textfiles used
    check = file_contents  #contents of file in form [['word1',..],['word2','wordn',...]]
    for item in words:#a list of every word from every file ['word1','wordn','word3',...]
  
        if item in check:
            if item not in dict:
                dict[item] = []
  
            if item in dict:
                dict[item].append(textfile_list[i])

dict = {k: list(set(v)) for k, v in dict.items()}

how would i do this??","['python', 'list', 'dictionary', 'nlp']",69750983,"i could organize your workflow like the following. use this as a source of inspiration:
content = [['hello', 'apple', 'test', 'banana'], ['weird', 'apple', 'tester', 'banana', 'banana', 'apple']]
files = ['file1.txt', 'file2.txt']
index = {k:v for k, v in zip(files, content)}
words = set([word for words in index.values() for word in words])
expected_dict = {}
for word in words:
    expected_dict[word]=[]
    for key, value in index.items():
        if word in value:
            expected_dict[word].append({key:[idx for idx in range(len(value)) if value[idx]==word]})

output:
{'test': [{'file1.txt': [2]}],
 'apple': [{'file1.txt': [1]}, {'file2.txt': [1, 5]}],
 'banana': [{'file1.txt': [3]}, {'file2.txt': [3, 4]}],
 'tester': [{'file2.txt': [2]}],
 'hello': [{'file1.txt': [0]}],
 'weird': [{'file2.txt': [0]}]}",https://stackoverflow.com/questions/69744611,python,27-10-2021 19:47,77.0,0.0,1.0,True,28-10-2021 08:49,27-10-2021 19:55
26323371,how to make my python code more effective?,"i am constructing my word n-gram training vector later to be used by svm. i ran my code, but it took me too much time, above 10 hours. do you have any method to make it faster?
def wordngrams(s,n):
    """"""calculate word n-grams and return a dictionary""""""
    input = s.split(' ')
    output = {}
    for i in xrange(len(input)-n+1):
        g = "" "".join(input[i:i+n]).lower()
        output.setdefault(g,0)
        output[g] += 1
    return output

res is a list of 10000 strings, whereas each string contains 300 words on average. global_vector is a sorted list of 200000 2-grams.
x = []
for i in xrange(10000):
     word_dic = wordngrams(res[i], 2)
     vector1 = {key : 1 if key in word_dic else 0 for key in global_vector}
     od = ordereddict(sorted(vector1.items(), key = lambda t : t[0]))
     x.append(od.values())

to be honest, it is common if it takes 2 or 3 hours. but it took me over 10 hours. i really do not what i should do. please help me.","['python', 'nlp', 'n-gram']",26323834,"i think the sorted call you are running in your loop is the only thing that can be optimized out. everything else appears to be asymptotically optimal (it may be possible to improve things by small factors in various places, but not by huge amounts).
if your global_vector is already sorted, you can avoid sorting the results by creating the results list directly, rather than going through an unordered dictionary first. here's a quick version that uses a list comprehension:
x = []
for text in res:
    word_dic = wordngrams(text, 2)
    x.append([1 if bigram in word_dic else 0 for bigram in global_vector])

this should be o(m*n) + o(m*p) where m is the length of res, n is the length of global_vector and p is the average length of the texts. your original code included an extra log(n) factor in the first term, due to the extra sorting.",https://stackoverflow.com/questions/26323371,python,12-10-2014 08:46,110.0,1.0,1.0,True,05-03-2024 23:04,05-03-2024 23:04
70391922,how to i update my trained space ner model with new training dataset?,"i'm new to nlp, i started learning how to train the custom ner in spacy.
train_data = [
          ('what is the price of polo?', {'entities': [(21, 25, 'product')]}), 
          ('what is the price of ball?', {'entities': [(21, 25, 'product')]}), 
          ('what is the price of jegging?', {'entities': [(21, 28, 'product')]}), 
          ('what is the price of t-shirt?', {'entities': [(21, 28, 'product')]}), 
          ('what is the price of jeans?', {'entities': [(21, 26, 'product')]}), 
          ('what is the price of bat?', {'entities': [(21, 24, 'product')]}), 
          ('what is the price of shirt?', {'entities': [(21, 26, 'product')]}), 
          ('what is the price of bag?', {'entities': [(21, 24, 'product')]}), 
          ('what is the price of cup?', {'entities': [(21, 24, 'product')]}), 
          ('what is the price of jug?', {'entities': [(21, 24, 'product')]}), 
          ('what is the price of plate?', {'entities': [(21, 26, 'product')]}), 
          ('what is the price of glass?', {'entities': [(21, 26, 'product')]}), 
          ('what is the price of moniter?', {'entities': [(21, 28, 'product')]}), 
          ('what is the price of desktop?', {'entities': [(21, 28, 'product')]}), 
          ('what is the price of bottle?', {'entities': [(21, 27, 'product')]}), 
          ('what is the price of mouse?', {'entities': [(21, 26, 'product')]}), 
          ('what is the price of keyboad?', {'entities': [(21, 28, 'product')]}), 
          ('what is the price of chair?', {'entities': [(21, 26, 'product')]}), 
          ('what is the price of table?', {'entities': [(21, 26, 'product')]}), 
          ('what is the price of watch?', {'entities': [(21, 26, 'product')]})
]

training the blank spacy model for the first time:
def train_spacy(data,iterations):
    train_data = data
    nlp = spacy.blank('en')  # create blank language class
    # create the built-in pipeline components and add them to the pipeline
    # nlp.create_pipe works for built-ins that are registered with spacy
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=true)
   

    # add labels
    for _, annotations in train_data:
         for ent in annotations.get('entities'):
         ner.add_label(ent[2])

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train ner
        optimizer = nlp.begin_training()
        for itn in range(iterations):
            print(""statring iteration "" + str(itn))
            random.shuffle(train_data)
            losses = {}
            for text, annotations in train_data:
                nlp.update(
                    [text],  # batch of texts
                    [annotations],  # batch of annotations
                    drop=0.2,  # dropout - make it harder to memorise data
                    sgd=optimizer,  # callable to update weights
                    losses=losses)
            print(losses)
    return nlp


start_training = train_spacy(train_data, 20)

saving my trained spacy model:
# saveing the trained model
start_training.to_disk(""spacy_start_model"")

my question here is how to update the saved model with new training data?
new training data:
train_data_2 = [('who is chaka khan?', {""entities"": [(7, 17, 'person')]}),
            ('i like london and berlin.', {""entities"": [(7, 13, 'loc')]})]

could any one help me with your solution and tip for this?
thanks in advance!","['python', 'spacy', 'spacy-3']",70393614,"as far as i know, you could retrain your model using your new data examples, but instead of starting from a blank model, you would now start from your existing model.
in order to achieve this, it will first remove the following line from your train_spacy method, and may be receives the model as a parameter:
nlp = spacy.blank('en')  # create blank language class

then to retrain your model instead of loading a spacy blank model and pass to your training method, load your existing model using the load method and then call your training method (read more about spacy save/load here).
start_training = spacy.load(""spacy_start_model"") 

one final suggestion, in my practice i have obtained better results by retraining a spacy ner model from an existing one such as en_core_web_md or en_core_web_lg, adding my custom entities, than training from scratch from a spacy blank model.
all together:

method update

def train_spacy(data, iterations, nlp):  # <-- add model as nlp parameter
    train_data = data
    # create the built-in pipeline components and add them to the pipeline
    # nlp.create_pipe works for built-ins that are registered with spacy
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=true)
    else:
        ner = nlp.get_pipe('ner')
   

    # add labels
    for _, annotations in train_data:
         for ent in annotations.get('entities'):
         ner.add_label(ent[2])

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train ner
        optimizer = nlp.begin_training()
        for itn in range(iterations):
            print(""statring iteration "" + str(itn))
            random.shuffle(train_data)
            losses = {}
            for text, annotations in train_data:
                nlp.update(
                    [text],  # batch of texts
                    [annotations],  # batch of annotations
                    drop=0.2,  # dropout - make it harder to memorise data
                    sgd=optimizer,  # callable to update weights
                    losses=losses)
            print(losses)
    return nlp

nlp = spacy.blank('en')  # create blank language class
start_training = train_spacy(train_data, 20, nlp)


retrain your model

train_data_2 = [('who is chaka khan?', {""entities"": [(7, 17, 'person')]}),
            ('i like london and berlin.', {""entities"": [(7, 13, 'loc')]})]

nlp = spacy.load(""spacy_start_model"")  # <-- now your base model is your custom model
start_training = train_spacy(train_data_2, 20, nlp)

i hopethis works for you!",https://stackoverflow.com/questions/70391922,python,17-12-2021 10:42,1873.0,4.0,1.0,True,16-02-2023 22:48,17-12-2021 10:48
76403814,what is the best approach to creating a question generation model using gpt and bert architectures?,"i want to make a question generation model from questions as well as context. should i make use of gpt based models or bert based architectures.
gpt is able to perform the tasks but sometimes returns with vague questions that were not in the context itself. when i made use of wizardlm(7b), i was able to get generalized questions from the context itself which sounded more natural and were nearly to the point when kept within limit of 3.","['python', 'open-source', 'huggingface-transformers', 'huggingface', 'gpt-3']",76411286,"when dealing with text generation, it is more straightforward to work with transformer decoder models such as gpt-* models. although bert-like models are also capable of text generation, it is a quite convoluted process and not something that follows naturally from the tasks for which these models have been pretrained.
i assume you are comparing gpt-2 and wizardlm (7b). the performance of the model on this task is expected to improve as you scale up the number of parameters by using larger models. i would recommend you to try llms such as alpaca-lora, dolly or gpt-j ( see here how to run gpt-j on colab pro ).",https://stackoverflow.com/questions/76403814,python,05-06-2023 05:58,912.0,0.0,1.0,True,06-06-2023 03:32,05-06-2023 06:14
77494171,using curl to request chatgpt api on windows cmd,"this is my command using windonws cmd:
curl -x 127.0.0.1:33210 
  -h ""content-type: application/json"" ^
  -h ""authorization: bearer %openai_api_key%"" ^
  -d '{ ^
     ""model"": ""gpt-3.5-turbo"", ^
     ""messages"": [{""role"": ""user"", ""content"": ""say this is a test!""}], ^
     ""temperature"": 0.7 ^
   }'

and cmd returned the error  reply below:
{
    ""error"": {
        ""message"": ""we could not parse the json body of your request. (hint: this likely means you aren't using your http library correctly. the openai api expects a json payload, but what was sent was not valid json. if you have trouble figuring out how to fix this, please contact us through our help center at help.openai.com.)"",
        ""type"": ""invalid_request_error"",
        ""param"": null,
        ""code"": null
    }
}

i can't find the error on the json body.i used the examples from openai's documention.
after solving the format problemï¿½ï¿½ï¿½the command is as belowï¿½ï¿½ï¿½
curl  -x 127.0.0.1:33210 
-h ""content-type: application/json"" ^
-h ""authorization: bearer %openai_api_key%"" ^
-d ""{""model"": ""gpt-3.5-turbo"", ""messages"": [{""role"": ""user"", ""content"": ""say this is a test!""}], ""temperature"": 0.7 }""

but a problem remains:
{
    ""error"": {
        ""message"": ""we could not parse the json body of your request. (hint: this likely means you aren't using your http library correctly. the openai api expects a json payload, but what was sent was not valid json. if you have trouble figuring out how to fix this, please contact us through our help center at help.opent;,
        ""type"": ""invalid_request_error"",
        ""param"": null,
        ""code"": null
    }
}
curl: (3) url using bad/illegal format or missing url
curl: (3) bad range specification in url position 12:
messages: [{role: user, content: say

it seems that the curl doesn't package all the sentance which i wish to package, i don't understand why.","['openai-api', 'chatgpt-api']",77495493,"this is caused by the ^ characters you parse in your json. the json data is parsed as a string, so we don't need the newline characters. a valid json command would be
curl -x 127.0.0.1:33210 
  -h ""content-type: application/json"" ^
  -h ""authorization: bearer %openai_api_key%"" ^
  -d '{""model"": ""gpt-3.5-turbo"", ""messages"": [{""role"": ""user"", ""content"": ""say this is a test!""}], ""temperature"": 0.7 }'

edit
the format problem is caused by using double quotes ("") instead of single quotes (') around the json body. in this link, the following can be found in the first paragraph:

json data is passed as a string. double quotes in json must be escaped with the backslash ""\ "" on windows computers.

in your case, that means that the data parsed after the data switch -d should be formatted as follows:
-d '{""model"": ""gpt-3.5-turbo"", ""messages"": [{""role"": ""user"", ""content"": ""say this is a test!""}], ""temperature"": 0.7 }'

note the single quotes around the whole json body, but the double quotes around the properties and values!",https://stackoverflow.com/questions/77494171,openai-api,16-11-2023 10:47,5429.0,1.0,1.0,True,21-11-2023 08:52,17-11-2023 05:28
75026054,simple ner - indexerror: string index out of range error,"here is a simple example of named entity recognition (ner) using the named entity recognition tool in the natural language toolkit (nltk) library in python:
import nltk
input text
text = ""barack obama was born in hawaii. he was the 44th president of the united states.""
tokenize the text
tokens = nltk.word_tokenize(text)
perform named entity recognition
entities = nltk.ne_chunk(tokens)
print the named entities
print(entities)
when i run this code in my jupyter notebook, i get this error.
""indexerror: string index out of range""
am i missing any installation? please advise.
expected output:
(person barack/nnp obama/nnp)
(gpe hawaii/nnp)
(organization united/nnp states/nnps)",['named-entity-recognition'],75026180,"nltk.ne_chunk expects its input to be tagged tokens rather than just plain tokens, so i would recommend adding a tagging step between the tokenization and ne chunking via nltk.pos_tag. ne chunking still would give you every token, chunked by entities if there are any detected. since you want only the entities, you can check for if there is a tree in a particular chunk. like the following:
text = ""barack obama was born in hawaii. he was the 44th president of the united states.""
tokens = nltk.word_tokenize(text)
tagged_tokens = nltk.pos_tag(tokens)
entities = [chunk for chunk in nltk.ne_chunk(tagged_tokens) if isinstance(chunk, nltk.tree)]

for entity in entities:
    print(entity)

please note that this code doesn't give exactly the output you want. instead it gives:
(person barack/nnp)
(person obama/nnp)
(gpe hawaii/nnp)
(gpe united/nnp states/nnps)",https://stackoverflow.com/questions/75026054,named-entity-recognition,06-01-2023 01:24,111.0,0.0,1.0,True,06-01-2023 01:57,06-01-2023 01:26
77602701,openai whisper api: &quot;invalid file format&quot; on aws lambda,"i am trying to do transcription on the voice of the user which is sent from the client as webm bytes with opus codecs. when running locally i have no issues in saving the bytes and reading them as file to use in the api.
when running the same code on lambda i get this error:

openai.badrequesterror: error code: 400 - {'error': {'message': ""invalid file format. supported formats: ['flac', 'm4a', 'mp3', 'mp4', 'mpeg', 'mpga', 'oga', 'ogg', 'wav', 'webm']"", 'type': 'invalid_request_error', 'param': none, 'code': none}}.

this is my code:
@api_bp.post(""/audio"")
def transcriberoute():
    audio_file = request.files.get('audio', none)

    if audio_file:
        print(audio_file) # <filestorage: 'blob' ('audio/webm;codecs=opus')>


        old_work_dir = os.getcwd()

        with tempfile.temporarydirectory() as tmp_dir:
            os.chdir(tmp_dir)

            try:
                input_file_path = 'recording.webm'
                audio_file.save(input_file_path)

                audio_file = open(input_file_path, ""rb"")
                print(audio_file) # <_io.bufferedreader name='recording.webm'>

                transcript = client.audio.transcriptions.create(model=""whisper-1"",file=audio_file).text

            finally:
                os.remove(input_file_path)
                os.chdir(old_work_dir)

        print(f""transcribed audio: {transcript}"")
        return {""transcription"": transcript}, 200

    print(""no audio file found"")
    return {""transcription"": ""error""}, 400

my first thought was that this is an issue with lambda's temporary storage. however it seems that /tmp works and has 500mb of storage. printing the file also confirms that it is saved and can be read.
any help is greatly appreciated!
edit 1
i now just use the raw bytes instead of saving it first to remove the possibility of it being an error in the filesystem.
@api_bp.post(""/audio"")
def transcriberoute():
    audio_file = request.files.get('audio', none)

    if audio_file:
        print(audio_file) # <filestorage: 'blob' ('audio/webm;codecs=opus')>

        buffer = bytesio(audio_file.read())
        buffer.name = ""test.webm""

        transcript = client.audio.transcriptions.create(model=""whisper-1"", file=buffer).text

        print(f""transcribed audio: {transcript}"")
        return {""transcription"": transcript}, 200

    print(""no audio file found"")
    return {""transcription"": ""error""}, 400

i am still investigating this thread, which made me think, that maybe this is something with a dependency on the host machine. to find out i rehosted the code to lambda with lambda docker image which threw the same error. running the program locally in the official python:3.9 image works with no problems. when deploying this to lambda again it starts throwing errors once again.
my current theory is that the binary-data somehow gets corrupted when sending it over the api-gateway, since when i try to read that data and transform it into another format it starts breaking (only on lambda, local is no problem). this is what i get:
output from ffmpeg/avlib:
2023-12-06t15:14:21.776+01:00   ffmpeg version 5.1.4-0+deb12u1 copyright (c) 2000-2023 the ffmpeg developers
built with gcc 12 (debian 12.2.0-14)
configuration: --prefix=/usr --extra-version=0+deb12u1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librist --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --disable-sndio --enable-libjxl --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-libplacebo --enable-librav1e --enable-shared

libavutil 57. 28.100 / 57. 28.100
libavcodec 59. 37.100 / 59. 37.100
libavformat 59. 27.100 / 59. 27.100
ibavdevice 59. 7.100 / 59. 7.100
libavfilter 8. 44.100 / 8. 44.100
libswscale 6. 7.100 / 6. 7.100
libswresample 4. 7.100 / 4. 7.100
libpostproc 56. 6.100 / 56. 6.100
[matroska,webm @ 0x55802d5b9e00] element at 0x44 ending at 0x83 exceeds containing master element ending at 0x74
[matroska,webm @ 0x55802d5b9e00] ebml header parsing failed
pipe:: invalid data found when processing input","['python', 'aws-lambda', 'openai-api', 'openai-whisper']",77616053,"after much trying and researching the problem was a mix of 2 issues:
a) in order for the whisper api to work, the buffer with the audio-bytes has to have a name (which happens automatically when you write and read it to the file, just make sure you have the right extension).
b) the aws api-gateway doesn't support binary data in requests by default, and you have to manually allow it. f.e. if you are using serverless framework for deployment add this:
provider:
  apigateway:
    binarymediatypes:
      - '*/*'",https://stackoverflow.com/questions/77602701,python,04-12-2023 21:36,1613.0,0.0,1.0,True,06-12-2023 20:15,06-12-2023 14:32
78435465,getting nltk certificate verify failed error with visual studio code with python3,"i got this error. as you can see i have import nltk and nltk.download in my code per their guide.:
[nltk_data] error loading words: <urlopen error [ssl:
[nltk_data]     certificate_verify_failed] certificate verify failed:
[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>

my code:
import re # to remove regular expressions. like ? ! . ,
import tkinter as tk # this is a graphical ui
from tkinter.scrolledtext import scrolledtext # widget

import nltk
nltk.download('words') # check wether a word is valid
from nltk.corpus import words



class spellingchecker:

    def __init__(self):
        self.root = tk.tk() # tk.tk refers to a class within the tkinter module, which is a standard gui
        self.root.geometry(""600x500"")

        self.text = scrolledtext(self.root, font=(""helvetica"", 14))
        self.text.bind(""<keyrelease>"", self.check) # to check words whenever we release a key
        self.text.pack()

        self.old_spaces = 0 # by default we have 0 whitespaces

        self.root.mainloop() # to get the gui running


    def check(self, event):
        content = self.text.get(""1.0"", tk.end)  # 1.0 is the first character, 1.1 is the second character, 1.2 is the third character etc.  # tk.end this gives the full content of the text box
        space_count = content.count("" "") # count the white spaces

        if space_count != self.old_spaces: # if space count is not the same, != as self.old_spaces
            self.old_spaces = space_count

            for tag in self.text.tag_names():
               self.text.tag_delete(tag)
            
            for word in content.split("" ""):
                if re.sub(r""[^\w]"", """", word.lower()) not in words.words():
                    position = content.find(word)
                    self.text.tag_add(word, f""1.{position}"", f""1.{position + len(word)}"")
                    self.text.tag_config(word, foreground=""red"")
        
spellingchecker()


i'm on macos and i got python3 installed.
searching for answers.
i tried to re add them, say nltk3, check forums, yt videos. nothing.","['python', 'nltk', 'errno']",78519231,used a vpn and then added some certificate code at the top.,https://stackoverflow.com/questions/78435465,python,06-05-2024 08:41,51.0,0.0,1.0,True,22-05-2024 17:50,06-05-2024 08:45
75482382,compare each string with all other strings in a dataframe,"i have this dataframe:
mylist = [
    ""ï¿½ï¿½ï¿½67.00 to rupam sweets using bank account xxxxxxxx5343<br>11 feb 2023, 20:42:25"",
    ""ï¿½ï¿½ï¿½66.00 to rupam sweets using bank account xxxxxxxx5343<br>10 feb 2023, 21:09:23"",
    ""ï¿½ï¿½ï¿½32.00 to nagori sajjad mohammed sayyed using bank account xxxxxxxx5343<br>9 feb 2023, 07:06:52"",
    ""ï¿½ï¿½ï¿½110.00 to vikram manohar jsohi using bank account xxxxxxxx5343<br>9 feb 2023, 06:40:08"",
    ""ï¿½ï¿½ï¿½120.00 to winner dinesh gupta using bank account xxxxxxxx5343<br>30 jan 2023, 06:23:55"",
]
import pandas as pd

df = pd.dataframe(mylist)
df.columns = [""full_text""]
ndf = df.full_text.str.split(""to"", expand=true)
ndf.columns = [""amt"", ""full_text""]
ndf2 = ndf.full_text.str.split(""using bank account xxxxxxxx5343<br>"", expand=true)
ndf2.columns = [""client"", ""date""]
df = ndf.join(ndf2)[[""date&qtamt""]]

i have created embeddings for each client name:
from openai.embeddings_utils import get_embedding, cosine_similarity
import openai

openai.api_key = 'xxx'
embedding_model = ""text-embedding-ada-002""
embeddings = df.client.apply([lambda x: get_embedding(x, engine=embedding_model)])
df[""embeddings""] = embeddings

i can now calculate the similarity index for a given string. for e.g. ""rupam sweet"" using:
query_embedding = get_embedding(""rupam sweet"", engine=""text-embedding-ada-002"")
df[""similarity""] = df.embeddings.apply(lambda x: cosine_similarity(x, query_embedding))

but i need the similarity score of each client across all other clients. in other words, the client names will be in rows as well as in columns and the score will be the data. how do i achieve this?","['python', 'numpy', 'nlp', 'vectorization', 'similarity']",75581674,"if you have a vectorized similarity function f(x, y) and want to apply it to all pairs of a series, you can make use of numpy broadcasting. if f is not a vectorized function, you can turn it into one by calling f_vec = np.vectorize(f) on it. in the example below, i'm using the ratio function from the fuzzywuzzy module for illustration purposes, but it works the same way with any other comparison function.
from fuzzywuzzy.fuzz import ratio
import numpy as np

ratio_vec = np.vectorize(ratio)
s = pd.series(mylist)
df = pd.dataframe(ratio_vec(s, s[:, none]))

the result is a similarity matrix:
     0    1    2    3    4
0  100   92   74   76   71
1   92  100   74   73   72
2   70   74  100   74   67
3   73   73   73  100   72
4   71   72   64   74  100",https://stackoverflow.com/questions/75482382,python,17-02-2023 09:26,363.0,0.0,2.0,True,01-03-2023 05:35,01-03-2023 05:32
75305169,decoding hidden layer embeddings in t5,"i'm new to nlp (pardon the very noob question!), and am looking for a way to perform vector operations on sentence embeddings (e.g., randomization in embedding-space in a uniform ball around a given sentence) and then decode them. i'm currently attempting to use the following strategy with t5 and huggingface transformers:

encode the text with t5tokenizer.
run a forward pass through the encoder with model.encoder. use the last hidden state as the embedding. (i've tried .generate as well, but it doesn't allow me to use the decoder separately from the encoder.)
perform any desired operations on the embedding.
the problematic step: pass it through model.decoder and decode with the tokenizer.

i'm having trouble with (4). my sanity check: i set (3) to do nothing (no change to the embedding), and i check whether the resulting text is the same as the input. so far, that check always fails.
i get the sense that i'm missing something rather important (something to do with the lack of beam search or some other similar generation method?). i'm unsure of whether what i think is an embedding (as in (2)) is even correct.
how would i go about encoding a sentence embedding with t5, modifying it in that vector space, and then decoding it into generated text? also, might another model be a better fit?
as a sample, below is my incredibly broken code, based on this:
t5_model = transformers.t5forconditionalgeneration.from_pretrained(""t5-large"")
t5_tok = transformers.t5tokenizer.from_pretrained(""t5-large"")
text = ""foo bar is typing some words.""
input_ids = t5_tok(text, return_tensors=""pt"").input_ids
encoder_output_vectors = t5_model.encoder(input_ids, return_dict=true).last_hidden_state
# the rest is what i think is problematic:
decoder_input_ids = t5_tok(""<pad>"", return_tensors=""pt"", add_special_tokens=false).input_ids
decoder_output = t5_model.decoder(decoder_input_ids, encoder_hidden_states=encoder_output_vectors)
t5_tok.decode(decoder_output.last_hidden_state[0].softmax(0).argmax(1))","['python', 'machine-learning', 'nlp', 'huggingface-transformers', 'transformer-model']",77034052,"much easier than anticipated! for anyone else looking for an answer, this page in huggingface's docs wound up helping me the most. below is an example with code based heavily on that page.

first, to get the hidden layer embeddings:
encoder_input_ids = self.tokenizer(encoder_input_str, return_tensors=""pt"").input_ids
embeds = self.model.get_encoder()(
    encoder_input_ids.repeat_interleave(num_beams, dim=0),
    return_dict=true
)

note that using repeat_interleave above is only necessary for decoding methods such as beam search. otherwise, no repetition in the hidden layer embedding is necessary.
huggingface provides many methods for decoding, just as would be accessible via generate()'s options. these are documented in the article linked above. to provide an example of decoding using beam search with num_beams beams:
model_kwargs = {
    ""encoder_outputs"": encoder_outputs
}

# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# instantiate three configuration objects for scoring
beam_scorer = beamsearchscorer(
    batch_size=1,
    num_beams=num_beams,
    device=model.device,
)

logits_processor = logitsprocessorlist(
    [
        minlengthlogitsprocessor(5, eos_token_id=model.config.eos_token_id),
    ]
)

stopping_criteria = stoppingcriterialist([
    maxlengthcriteria(max_length=max_length),
])

outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, stopping_criteria=stopping_criteria, **model_kwargs)

results = tokenizer.batch_decode(outputs, skip_special_tokens=true)

similar approaches can be taken for greedy and contrastive search, with different parameters. similarly, different stopping criteria can be used.",https://stackoverflow.com/questions/75305169,python,01-02-2023 02:49,2169.0,4.0,1.0,True,03-09-2023 20:23,01-02-2023 02:55
77464554,why am i receiving &quot;attributeerror: &#39;str&#39; object has no attribute &#39;page_content&#39;&quot; when trying to add my embeddings to azure cognitive search,"i am extracting text from pdf documents and load it to azure cognitive search for a rag approach. unfortunately this does not work. i am receiving the error message

attributeerror: 'str' object has no attribute 'page_content'

what i want to do is

extract text from pdf via pymupdf - works
upload it to azuer vector search as embeddings with vectors and `filename``
query this through chatgpt model

this is my code:
!pip install cohere tiktoken
!pip install openai==0.28.1
!pip install pymupdf
!pip install azure-storage-blob azure-identity
!pip install azure-search-documents --pre --upgrade
!pip install langchain

import fitz
import time
import uuid
import os
import openai

from pil import image
from io import bytesio
from ipython.display import display

from azure.identity import defaultazurecredential
from azure.storage.blob import blobserviceclient, blobclient, containerclient

from langchain.embeddings import openaiembeddings
from langchain.text_splitter import recursivecharactertextsplitter

from langchain.chat_models import azurechatopenai
from langchain.vectorstores import azuresearch
from langchain.document_loaders import directoryloader
from langchain.document_loaders import textloader
from langchain.text_splitter import tokentextsplitter
from langchain.chains import conversationalretrievalchain
from langchain.prompts import prompttemplate

from google.colab import drive

openai_api_base = ""
openai_api_key = ""xxx""
openai_api_version = ""2023-05-15""

openai.api_type = ""azure""
openai.api_key = openai_api_key
openai.api_base = openai_api_base
openai.api_version = openai_api_version

azure_cognitive_search_service_name = ""
azure_cognitive_search_api_key = ""xxx""
azure_cognitive_search_index_name = ""test""

llm = azurechatopenai(deployment_name=""gpt35"", openai_api_key=openai_api_key, openai_api_base=openai_api_base, openai_api_version=openai_api_version)
embeddings = openaiembeddings(deployment_id=""ada002"", chunk_size=1, openai_api_key=openai_api_key, openai_api_base=openai_api_base, openai_api_version=openai_api_version)

acs = azuresearch(azure_search_endpoint=azure_cognitive_search_service_name,
                  azure_search_key = azure_cognitive_search_api_key,
                  index_name = azure_cognitive_search_index_name,
                  embedding_function = embeddings.embed_query)
    
def generate_tokens(s):
  text_splitter = recursivecharactertextsplitter(chunk_size=1000, chunk_overlap=100)
  splits = text_splitter.split_text(s)

  return splits

drive.mount('/content/drive')
folder = ""/content/drive/.../pdf/""

page_content = ''
doc_content = ''
    
for filename in os.listdir(folder):
    file_path = os.path.join(folder, filename)
    if os.path.isfile(file_path):
        print(f""processing file: {file_path}"")

        doc = fitz.open(file_path)
        for page in doc: # iterate the document pages
          page_content += page.get_text() # get plain text encoded as utf-8
          doc_content += page_content

          d = generate_tokens(doc_content)

          # the following line throws the error
          # how can i add the chunks + filename to 
          # azure cognitive search?

          acs.add_documents(documents=d)
    
        print(metadatas)
        print(""----------"")
        print(doc_content)
        count = len(doc_content.split())
        print(""number of tokens: "", count)


---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-33-d9eaff7ee027> in <cell line: 10>()
     31           all_texts.extend(d)
     32 
---> 33           acs.add_documents(documents=d)
     34 
     35           metadatas = [{""source"": f""{i}-pl""} for i in range(len(all_texts))]

1 frames
/usr/local/lib/python3.10/dist-packages/langchain/schema/vectorstore.py in <listcomp>(.0)
    118         """"""
    119         # todo: handle the case where the user doesn't provide ids on the collection
--> 120         texts = [doc.page_content for doc in documents]
    121         metadatas = [doc.metadata for doc in documents]
    122         return self.add_texts(texts, metadatas, **kwargs)

attributeerror: 'str' object has no attribute 'page_content'","['python', 'azure-cognitive-services', 'azure-cognitive-search', 'langchain', 'azure-openai']",77464710,"i believe the issue is in your generate_tokens method. instead of returning a list of documents, it is returning a list of string. please see the documentation for split_text here: 
i believe the fix should be to convert this list of string into a list of document objects. can you please try the following:
def generate_tokens(s):
  text_splitter = recursivecharactertextsplitter(chunk_size=1000, chunk_overlap=100)
  splits = text_splitter.split_text(s)

  return text_splitter.create_documents(splits) #this should return the list of documents.",https://stackoverflow.com/questions/77464554,python,11-11-2023 09:33,9524.0,0.0,1.0,True,13-08-2024 14:36,13-08-2024 14:36
76633836,what does langchain charactertextsplitter&#39;s chunk_size param even do?,"my default assumption was that the chunk_size parameter would set a ceiling on the size of the chunks/splits that come out of the split_text method, but that's clearly not right:
from langchain.text_splitter import recursivecharactertextsplitter, charactertextsplitter

chunk_size = 6
chunk_overlap = 2

c_splitter = charactertextsplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)

text = 'abcdefghijklmnopqrstuvwxyz'

c_splitter.split_text(text)

prints: ['abcdefghijklmnopqrstuvwxyz'], i.e. one single chunk that is much larger than chunk_size=6.
so i understand that it didn't split the text into chunks because it never encountered the separator. but so then the question is what is the chunk_size even doing?
i checked the documentation page for langchain.text_splitter.charactertextsplitter here but did not see an answer to this question. and i asked the ""mendable"" chat-with-langchain-docs search functionality, but got the answer ""the chunk_size parameter of the charactertextsplitter determines the maximum number of characters in each chunk of text.""...which is not true, as the code sample above shows.","['python', 'machine-learning', 'text', 'nlp', 'langchain']",76747135,"charactertextsplitter will only split on separator (which is '\n\n' by default). chunk_size is the maximum chunk size that will be split if splitting is possible. if a string starts with n characters, has a separator, and has m more characters before the next separator then the first chunk size will be n if chunk_size < n + m + len(separator).
your example string has no matching separators so there's nothing to split on.
basically, it attempts to make chunks that are <= chunk_size, but will still produce chunks > chunk_size if the minimum size chunks that can be created are > chunk_size.",https://stackoverflow.com/questions/76633836,python,07-07-2023 03:50,36274.0,19.0,3.0,True,18-12-2023 04:01,07-07-2023 12:53
7594056,need to create a histogram in python for a corpus,"import nltk
from nltk.book import *
from nltk.corpus import brown
corpus_text = brown.words()
word_freq = freqdist(corpus_text)
word_hist = dict()

for k,v in word_freq.iteritems():
   if key in word_hist:
      word_hist[v] = word_hist[v] + 1
   else:
      word_hist[v] = 1 

print word_hist.viewkeys()
print word_hist.viewvalues()

i'm making a mistake at the dictionary handling here. need to create a dictionary that has
it's keys as the words from the freqdict and the values as the number of the corresponding word. how do i perform this increment?
i'm certain that  
      word_hist[v] = word_hist[v] + 1
   else:
      word_hist[v] = 1

has a bug.","['python', 'nltk', 'dictionary']",7594250,"of course. it seems you are replacing the word_hist dict with one of its values (plus 1). try 
word_hist[v] = word_hist[v] + 1

or even better
word_hist[v] += 1

instead.
edit: there is another bug:
for k,v in word_freq.iteritems():
   if key in word_hist:
      word_hist[v] = word_hist[v] + 1
   else:
      word_hist[v] = 1

makes no sense. key is tested for presence in word_hist, but then v is used.
i don't know what key is, but either use k or v for both.",https://stackoverflow.com/questions/7594056,python,29-09-2011 07:46,2028.0,1.0,3.0,True,03-12-2021 14:37,29-09-2011 15:38
62111614,scispacy in google colab,"i am trying to build  ner model of clinical data using scispacy in colab. i have installed packages like this.
!pip install spacy
!pip install scispacy
!pip install        #pip install <model url>```

then i imported both using
import scispacy
import spacy
import en_core_sci_md

then used following code to display sentences and entities
nlp = spacy.load(""en_core_sci_md"")
text =""""""myeloid derived suppressor cells (mdsc) are immature myeloid cells with immunosuppressive activity. they accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (hcc)"""""" 
doc = nlp(text)
print(list(doc.sents))
print(doc.ents)

i am getting the following error
oserror: [e050] can't find model 'en_core_sci_md'. it doesn't seem to be a shortcut link, a python package or a valid path to a data directory.

i don't know why this error is coming, i followed all codes from the official github post of scispacy. any help would be appreciated.
thanks in advance.","['python', 'nlp', 'spacy', 'named-entity-recognition']",63382322,"i hope i am not too late... i believe you are very close to the correct approach.
i will write my answer in steps and you can choose where to stop.
step 1)
#install en_core_sci_lg package from the website of spacy  (large corpus), but you can also use en_core_sci_md for the medium corpus.
       
!pip install  

step 2)
# import the large dataset
import en_core_sci_lg

step 3)
# identify entities
nlp = en_core_sci_lg.load()
doc = nlp(text)
displacy_image = displacy.render(doc, jupyter = true, style = ""ent"")

step 4)
#print only the entities
print(doc.ents)

step 5)
# save the result 
save_res = [doc.ents]
save_res

step 6)
#save the results to a dataframe
df_save_res = pd.dataframe(save_res)
df_save_res

step 7)
# in case that you want to visualise the dependency parse
  displacy_image = displacy.render(doc, jupyter = true, style = ""dep"")",https://stackoverflow.com/questions/62111614,python,31-05-2020 04:33,2670.0,3.0,1.0,True,23-09-2024 00:56,31-05-2020 06:05
79007855,inquiry about intent feature in openai and azure ai search integration,"i am interested in understanding the logic behind the intent feature in the integration between openai's chat completions and azure ai search. specifically, i noticed that when appending and sending chat history for retrieval-augmented generation (rag), there is a feature in the response named ""intent."" this output appears to be a reformulated query that is sent to the ai search to ensure it receives a meaningful request, allowing the search engine to complete its task effectively.
take a look:
completion = client.chat.completions.create(
    messages=[
        {""role"": ""user"", ""content"": ""who is the first man to land on the moon?""},
        {""role"": ""assistant"", ""content"": ""the first man to land on the moon was neil armstrong.""},
        {""role"": ""user"", ""content"": ""how old was he at that time?""}
    ],
    model=deployment,
    extra_body={
        ""datasources"": [
            {
                ""type"": ""azurecognitivesearch"",
                ""parameters"": {
                    ""endpoint"": os.environ[""search_endpoint""],
                    ""key"": os.environ[""search_key""],
                    ""indexname"": os.environ[""search_index_name""],
                }
            }
        ]
    }
)

not only the chat completion returns the correct answer which is 38 years old but also return an 'intent' feature :
print(completion.choices[0].message.context['intent'])

[""how old was neil armstrong when he landed on the moon?"", ""what was neil armstrong's age when he landed on the moon?"", ""how old was neil armstrong when he was on the moon?""]

iï¿½ï¿½ï¿½m very interested in understanding this mechanism, as iï¿½ï¿½ï¿½m working with a stateless langchain agent, and i would love to implement something sim>i would like to know what prompt openai uses to reformulate the query and send it to azure ai search using the ""intent"" feature. i want to be able to replicate this functionality using a prompt or some tools.
i am looking for a solution that allows me to summarize user inputs and send those reformulated queries to a langchain agent. is there any documentation or report that explains how to implement this intent feature in a standard chat completion? additionally, are there any prompts used by openai to reformulate and query the questions?","['azure', 'openai-api', 'azure-openai']",79013695,"it is the detected intent from chat history, check this
getting intent considering all the chat history you are giving.
azure openai did not provided how it is creating intent publicly.
so, you either use intent detection model or a prompt with examples to create intent.
here is an example using prompt.
import json

messages=[
        {""role"": ""user"", ""content"": ""who is the first man to land on the moon?""},
        {""role"": ""assistant"", ""content"": ""the first man to land on the moon was neil armstrong.""},
        {""role"": ""user"", ""content"": ""how old was he at that time?""}
    ]

userquery = ','.join([json.dumps(i) for i in messages])

reformulation_prompt = """"""
you are an intelligent assistant tasked with helping users retrieve information from a database. the user may ask ambiguous, incomplete, or vague questions. your task is to reformulate their query in a clear and precise way that can be sent to a search engine to retrieve the most relevant documents.

here are some examples:
- user input: {""role"": ""user"",""content"": ""what is vector profiles?""}
  reformulated queries: ""what are vector profiles?"", ""definition of vector profiles"", ""how do vector profiles work?""

now, please reformulate the following user input, also give 2 to 3 reformulated queries:
user input:"""""" + userquery + """"""
reformulated query:
""""""

t = client.completions.create(model=deployment,prompt=reformulation_prompt)
t.choices[0].text

and output:

now you use this response in your langchain agent.",https://stackoverflow.com/questions/79007855,azure,20-09-2024 18:08,417.0,1.0,1.0,True,23-09-2024 07:59,22-09-2024 15:28
70260427,valueerror: tf.function-decorated function tried to create variables on non-first call while using custom loss function,"i am trying to create a triplet loss function to calculate the similarity between two sentences as follows:
def tripletloss(y_true,y_pred, margin=0.25,batch_size = 64):
    v1, v2 = y_pred[:,:128],y_pred[:,-128:]
    scores = k.dot(v1, k.transpose(v2))
    positive = tf.linalg.diag_part(scores)
    negative_without_positive = scores - 2 * k.eye(batch_size)

    closest_negative = tf.reduce_max(negative_without_positive, axis=1)

    negative_zero_on_duplicate = scores * (1.0 - k.eye(batch_size))
    
    mean_negative = k.sum(negative_zero_on_duplicate, axis=1) / (batch_size-1)
    
    triplet_loss1 = k.maximum(0.0, margin - positive + closest_negative)
    
    triplet_loss2 = k.maximum(0.0, margin - positive + mean_negative)
    
    triplet_loss = k.mean(triplet_loss1 + triplet_loss2)

    return triplet_loss

my model is as follows:
input1 = keras.input(shape=(train_data1.shape[1],))
input2 = keras.input(shape=(train_data1.shape[1],))

encoding1 = base_model(input1)
encoding2 = base_model(input2)

merged = layers.concatenate()([encoding1, encoding2])

model = models.model(inputs = [input1, input2], outputs = merged)

where the base model is:
def calculate_mean(x, axis=1):
    return k.mean(x, axis=axis)

def normalize(x):
        return x / k.sqrt(k.sum(x * x, axis=-1, keepdims=true))

base_model = models.sequential()
base_model.add(layers.embedding(input_dim=len(vocab)+2, output_dim=128))
base_model.add(layers.lstm(128, return_sequences=true))
base_model.add(layers.lambda(calculate_mean, name='mean'))
base_model.add(layers.lambda(normalize, name='normalize'))

now when i use that loss function to compile the model with
model.compile(
    optimizer = adam(0.001),
    loss = tripletloss
)

it does not give any error. but when i train it using the fit method it gives me errors as:
valueerror: tf.function-decorated function tried to create variables on non-first call.

if i use other losses it works perfectly. i don't know what is wrong with the loss function here.","['python', 'tensorflow', 'keras', 'deep-learning', 'nlp']",70260530,"maybe try the following with tf.eye:
import tensorflow as tf

from tensorflow.keras import backend as k

def tripletloss(margin=0.25):
    def triplet(y_true,y_pred):
      batch_size = tf.cast(tf.shape(y_true)[0], dtype=tf.float32)
      v1, v2 = y_pred[:,:128],y_pred[:,-128:]
      scores = k.dot(v1, k.transpose(v2))
      positive = tf.linalg.diag_part(scores)
      negative_without_positive = scores - 2 * tf.eye(batch_size)

      closest_negative = tf.reduce_max(negative_without_positive, axis=1)

      negative_zero_on_duplicate = scores * (1.0 - tf.eye(batch_size))
      
      mean_negative = k.sum(negative_zero_on_duplicate, axis=1) / (batch_size-1)
      
      triplet_loss1 = k.maximum(0.0, margin - positive + closest_negative)
      
      triplet_loss2 = k.maximum(0.0, margin - positive + mean_negative)
      
      triplet_loss = k.mean(triplet_loss1 + triplet_loss2)

      return triplet_loss
    return triplet

triplet_loss = tripletloss()

def calculate_mean(x, axis=1):
    return k.mean(x, axis=axis)

def normalize(x):
        return x / k.sqrt(k.sum(x * x, axis=-1, keepdims=true))

base_model = tf.keras.sequential()
base_model.add(tf.keras.layers.embedding(input_dim=50, output_dim=128))
base_model.add(tf.keras.layers.lstm(128, return_sequences=true))
base_model.add(tf.keras.layers.lambda(calculate_mean, name='mean'))
base_model.add(tf.keras.layers.lambda(normalize, name='normalize'))

input1 = tf.keras.layers.input(shape=(50,))
input2 = tf.keras.layers.input(shape=(50,))

encoding1 = base_model(input1)
encoding2 = base_model(input2)

merged = tf.keras.layers.concatenate()([encoding1, encoding2])

model = tf.keras.model(inputs = [input1, input2], outputs = merged)
model.compile(
    optimizer = tf.keras.optimizers.adam(0.001),
    loss = triplet_loss
)

x = tf.random.uniform((500, 50), maxval=50, dtype=tf.int32)
y = tf.random.uniform((500, 256))
model.fit([x, x], y, epochs=2, batch_size=64)

epoch 1/2
8/8 [==============================] - 6s 237ms/step - loss: 0.0037
epoch 2/2
8/8 [==============================] - 2s 233ms/step - loss: 5.4691e-04
<keras.callbacks.history at 0x7fd249072d50>",https://stackoverflow.com/questions/70260427,python,07-12-2021 12:49,1453.0,0.0,1.0,True,08-12-2021 08:30,08-12-2021 08:30
65852264,using xlnet for sentiment analysis - setting the correct reshape parameters,"following  this link, i am trying to use my own data to do sentiment analysis. but i get this error:
---------------------------------------------------------------------------
runtimeerror                              traceback (most recent call last)
<timed exec> in <module>

<ipython-input-41-5f2f35b7976e> in train_epoch(model, data_loader, optimizer, device, scheduler, n_examples)
      7 
      8     for d in data_loader:
----> 9         input_ids = d[""input_ids""].reshape(4,64).to(device)
     10         attention_mask = d[""attention_mask""].to(device)
     11         targets = d[""targets""].to(device)

runtimeerror: shape '[4, 64]' is invalid for input of size 64

when i try to run this code
history = defaultdict(list)
best_accuracy = 0

for epoch in range(epochs):
    print(f'epoch {epoch + 1}/{epochs}')
    print('-' * 10)

    train_acc, train_loss = train_epoch(
        model,
        train_data_loader,     
        optimizer, 
        device, 
        scheduler, 
        len(df_train)
    )

    print(f'train loss {train_loss} train accuracy {train_acc}')

    val_acc, val_loss = eval_model(
        model,
        val_data_loader, 
        device, 
        len(df_val)
    )

    print(f'val loss {val_loss} val accuracy {val_acc}')
    print()

    history['train_acc'].append(train_acc)
    history['train_loss'].append(train_loss)
    history['val_acc'].append(val_acc)
    history['val_loss'].append(val_loss)

i know this error has something to do with the shape of my data but i am not sure how to find the correct reshape parameters in order to make this work.","['python', 'machine-learning', 'nlp', 'sentiment-analysis', 'huggingface-transformers']",66360105,"shape [4,64] in your example is actually [batch size, max_sequence_length]
so maybe you could replace them with your values...",https://stackoverflow.com/questions/65852264,python,22-01-2021 20:22,276.0,1.0,2.0,True,24-02-2021 23:07,29-01-2021 23:05
68850657,how can i merge values in one dictionary as an item to another dictionary created inside a &quot;for&quot; loop?,"i am struggling with lists, loops and dictionaries.
basically, i have two 'levels' of dictionary. the first has as its key a word, and as the value the number of occurrences of that word within a given sentence. so like this:
wordcount={'sort': 3, 'count': 3, 'wrap': 3, 'coin': 11}
next, i have a for loop that goes through a list of just these words, and for each one creates a dictionary with a number of additional attributes, namely that word's occurrence according to google n-grams:
for word in wordlist:

      url =f"" 
      {word}&year_start=1965&year_end=1975&corpus=26&smoothing=3""
      sleep(1)
      resp = requests.get(url)

            if resp.ok:
                results = json.loads(resp.content)[0]
                results_clean = {key: val for key, val in results.items() if key == ""ngram"" or key ==""timeseries""}
                timeseries = {key: results_clean[key] for key in results_clean.keys() & {'timeseries'}}
                timeseriesvalues= list(timeseries.values())
                timeseriesmean=np.mean(timeseriesvalues)
                ngramsonly = {key: results_clean[key] for key in results_clean.keys() & {'ngram'}}
                ngramsvalues = list(ngramsonly.values())
                results_nouns_final={""word"": ngramsvalues, ""occurrence_mean"": timeseriesmean}

basically, i want to append to results_noun_final that word's occurrence value from before. however, when i try to do so by adding the word's value from wordcount as a third item to this dictionary (as follows):
results_nouns_final={""word"": ngramsvalues, ""occurrence_mean"": timeseriesmean, ""count"": wordcount.items()}

it is appending all words' counts, and giving me something like the following:
{'word': ['sort'], 'occurrence_mean': 5.319996372468642e-05, 'count': dict_items([('sort', 3), ('count', 3), ('wrap', 3), ('coin', 11)}
{'word': ['count'], 'occurrence_mean': 4.5438979543294875e-05, 'count': dict_items([('sort', 3), ('count', 3), ('wrap', 3), ('coin', 11)}
...etc.

could anybody let me know where i am going wrong? my desired output would be something like the following:
{'word': ['sort'], 'occurrence_mean': 5.319996372468642e-05, 'count': 3}
{'word': ['count'], 'occurrence_mean': 4.5438979543294875e-05, 'count': 3}","['python', 'loops', 'dictionary', 'nlp', 'n-gram']",68850765,"when you use wordcount.items(), you are getting all the items in the wordcount dictionary. you only want to access the count of word.
try replacing the last line in your loop with:
results_nouns_final={""word"": ngramsvalues, ""occurrence_mean"": timeseriesmean, ""count"": wordcount.get(word)}

wordcount.get(word) gives you the count of word from your dictionary. using .get() returns none if the word is not in wordcount.
if you're sure every word exists in your dictionary, you could just use wordcount[word].",https://stackoverflow.com/questions/68850657,python,19-08-2021 15:31,69.0,0.0,1.0,True,12-06-2022 03:23,12-06-2022 03:23
63302027,how to avoid double-extracting of overlapping patterns in spacy with matcher?,"i need to extract item combination from 2 lists by means of python spacy matcher. the problem is following:
let us have 2 lists:
colors=['red','bright red','black','brown','dark brown']
animals=['fox','bear','hare','squirrel','wolf']

i match the sequences by the following code:
first_color=[]
last_color=[]
only_first_color=[]
for color in colors:
    if ' ' in color:
        first_color.append(color.split(' ')[0])
        last_color.append(color.split(' ')[1])
    else:
        only_first_color.append(color)
matcher = matcher(nlp.vocab)

pattern1 = [{""text"": {""in"": only_first_color}},{""text"":{""in"": animals}}]
pattern2 = [{""text"": {""in"": first_color}},{""text"": {""in"": last_color}},{""text"":{""in"": animals}}]

matcher.add(""animals"", none, pattern1,pattern2)

doc = nlp('bright red fox met black wolf')

matches = matcher(doc)

for match_id, start, end in matches:
    string_id = nlp.vocab.strings[match_id]  # get string representation
    span = doc[start:end]  # the matched span
    print(start, end, span.text)

it gives the output:
0 3 bright red fox
1 3 red fox
4 6 black wolf

how can i extract only 'bright red fox' and 'black wolf'? should i change the patterns rules or post-process the matches?
any thoughts appreciate!","['python', 'nlp', 'spacy', 'matcher']",63303480,"you may use spacy.util.filter_spans:

filter a sequence of span objects and remove duplicates or overlaps.
useful for creating named entities (where one token can only be part
of one entity) or when merging spans with retokenizer.merge. when
spans overlap, the (first) longest span is preferred over shorter
spans.

python code:
matches = matcher(doc)
spans = [doc[start:end] for _, start, end in matches]
for span in spacy.util.filter_spans(spans):
    print(span.start, span.end, span.text)

output:
0 3 bright red fox
4 6 black wolf",https://stackoverflow.com/questions/63302027,python,07-08-2020 12:37,4687.0,15.0,2.0,True,01-05-2023 10:40,07-08-2020 14:02
76422222,how to do tokenizer batch processing? - huggingface,"in the tokenizer documentation from huggingface, the call fuction accepts list[list[str]] and says:

text (str, list[str], list[list[str]], optional) ï¿½ï¿½ï¿½ the sequence or batch of sequences to be encoded. each sequence can be a string or a list of strings (pretokenized string). if the sequences are provided as list of strings (pretokenized), you must set is_split_into_words=true (to lift the ambiguity with a batch of sequences).

things run normally if i run:
 test = [""hello this is a test"", ""that transforms a list of sentences"", ""into a list of list of sentences"", ""in order to emulate, in this case, two batches of the same lenght"", ""to be tokenized by the hf tokenizer for the defined model""]
 tokenizer = autotokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')
 tokenized_test = tokenizer(text=test, padding=""max_length"", is_split_into_words=false, truncarue, return_tensors=""pt"")

but if i try to emulate batches of sentences:
 test = [""hello this is a test"", ""that transforms a list of sentences"", ""into a list of list of sentences"", ""in order to emulate, in this case, two batches of the same lenght"", ""to be tokenized by the hf tokenizer for the defined model""]
 test = [test, test]
 tokenizer = autotokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')
 tokenized_test = tokenizer(text=test, padding=""max_length"", is_split_into_words=false, truncation=true, return_tensors=""pt"")

i get:
traceback (most recent call last):
  file ""/users/lucazeve/coding/wxcc_sentiment_analysis/modify_scores.py"", line 53, in <module>
    tokenized_test = tokenizer(text=test, padding=""max_length"", is_split_into_words=false, truncation=true, return_tensors=""pt"")
  file ""/users/lucazeve/coding/wxcc_sentiment_analysis/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py"", line 2548, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  file ""/users/lucazeve/coding/wxcc_sentiment_analysis/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py"", line 2634, in _call_one
    return self.batch_encode_plus(
  file ""/users/lucazeve/coding/wxcc_sentiment_analysis/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py"", line 2825, in batch_encode_plus
    return self._batch_encode_plus(
  file ""/users/lucazeve/coding/wxcc_sentiment_analysis/venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py"", line 428, in _batch_encode_plus
    encodings = self._tokenizer.encode_batch(
typeerror: textencodeinput must be union[textinputsequence, tuple[inputsequence, inputsequence]]

is the documentation wrong? i just need a way to tokenize and predict using batches, it shouldn't be that hard.
is it something to do with the is_split_into_words arguments?

contextualizing
i will feed that into a sentiment score model (the one defined in the code snippets). i am facing oom problems when predicting it so i need to feed the data in batches to the model.
the documentation (refered above) stated that i can feed list[list[str]] in the tokenizer which is not the case. the question remains the same: how to tokenize batches of sentences?
note: i don't need the tokenizing process to be in batches (although it would yield batches of tokens/attention_tokens), which would solve my problem: using the model for prediction with batches like this:
with torch.no_grad():
    logits = model(**tokenized_test).logits","['pytorch', 'batch-processing', 'tokenize', 'huggingface-transformers', 'huggingface-tokenizers']",76456769,"use pipelines, but there is a catch.
because you are passing all the processing steps, you need to pass the args for each one of them - when needed. for the tokenizer, we define:
 tokenizer = autotokenizer.from_pretrained(selected_model)
 tokenizer_kwargs = {'padding':true,'truncation':true,'max_length':512}
 

the model is straight forward:
model = automodelforsequenceclassification.from_pretrained(""distilbert-base-uncased-finetuned-sst-2-english"")

then finally:
classifier = pipeline(""text-classification"", model=model, batch_size=32, tokenizer=tokenizer)


specific to my application:
since i need the logits and not the predicted classes, i will have to modify the pipeline class. documentation says that in order to create a custom pipeline class, i need to define four mandatory methods: implement preprocess, _forward, postprocess, and _sanitize_parameters... or i can overwrite postprocess method from the textclassificationpipeline:
class mypipeline(textclassificationpipeline):
     def postprocess(self, model_outputs):
         return model_outputs[""logits""][0]

and modify the call:
classifier = pipeline(""text-classification"", model=model, batch_size=32, tokenizer=tokenizer, pipeline_class=mypipeline)

logits = classifier(text, **tokenizer_kwargs)",https://stackoverflow.com/questions/76422222,pytorch,07-06-2023 10:15,20115.0,9.0,2.0,True,12-06-2023 12:40,07-06-2023 13:42
76221016,find similarity in a very precise numerical environment,"i have a list of 100+ sentences, and i need to find which is the closest to a user prompt.
the thing is that we are dealing with very precise, nuanced prompt, because we analyze numeric data.
eaxmple:
1. did variable x changed at least 5% in the past week ?
2. show me variable x change in the past week

in this example, sentence 1 and 2 are totally different in the context of a chart but similar in the global context, but most simple models like spacy will rate them as very similar (0.9+) because they have many similar words.
what is the way to go to be able to train a model or to use a trained model, to find similarity in a very precise numerical environment like this, where sentences have many similar words but have total different meaning ?
i used this spacy model:
prompt_doc = nlp(user_prompt)
similarities = []

 
for sentence in sentences:
    sentence_doc = nlp(sentence)
    similarity = prompt_doc.similarity(sentence_doc)
    similarities.append(similarity)
    print(""sentence:"", sentence)
    print(""similarity rating:"", similarity)
    print()

the result for 100 sentences like the above, was that all of them have around 0.8-0.9 similarity. which is very wrong.","['python', 'nlp', 'spacy']",76221666,"have you tried using google's universal sentence encoder?
here's a code snippet which uses the encoder (i used google colab to run the code). for your example it returns a similarity of 0.6643186, much lower than the similarity you get with spacy.
some additional info: looking at the spacy documentation about similarity, it seems like spacy uses non-contextualized word vectors in conjunction with vector averaging to compute the similarities, which is the reason why it doesn't work for you: spacy's implementation does neither consider word order nor context of a word. in contrast, google's universal sentence encoder will give you one single vector for an entire sentence, thereby considering both word order and context of the word.
let me know if this helps!
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub

module_url = ""
model = hub.load(module_url)

def embed(input):
  return model(input)


sentence_1 = ""did variable x changed at least 5% in the past week ?""
sentence_2 = ""show me variable x change in the past week""

embeddings = embed([sentence_1, sentence_2])
corr = np.inner(embeddings[0], embeddings[1])

print(corr)  # 0.6643186",https://stackoverflow.com/questions/76221016,python,10-05-2023 17:11,46.0,0.0,1.0,True,10-05-2023 18:50,10-05-2023 17:18
73082256,how to create a list of tokenized words from dataframe column using spacy?,"i'm trying to apply spacys tokenizer on dataframe column to get a new column containing list of tokens.
assume we have the following dataframe:
import pandas as pd
details = {
    'text_id' : [23, 21, 22, 21],
    'text' : ['all roads lead to rome', 
              'all work and no play makes jack a dull buy', 
              'any port in a storm', 
              'avoid a questioner, for he is also a tattler'],
}
  
# creating a dataframe object 
example_df = pd.dataframe(details)

the code below aims to tokenize text column:
import spacy

nlp = spacy.load(""en_core_web_sm"")

example_df[""tokens""] = example_df[""text""].apply(lambda x: nlp.tokenizer(x))

example_df

the results looks like:

now, we have a new column tokens, which returns doc object for each sentence.
how could we change the code to get a python list of tokenized words?
i've tried the following line:
example_df[""tokens""] = example_df[""text""].apply(token.text for token in (lambda x: nlp.tokenizer(x)))


but i have the following error:
typeerror                                 traceback (most recent call last)
/tmp/ipykernel_33/3712416053.py in <module>
     14 nlp = spacy.load(""en_core_web_sm"")
     15 
---> 16 example_df[""tokens""] = example_df[""text""].apply(token.text for token in (lambda x: nlp.tokenizer(x)))
     17 
     18 example_df

typeerror: 'function' object is not iterable

thank you in advance!
update: i have a solution, but i still have another problem. i want to count words using built-in class counter, which takes a list as input and can be incrementally updated with a list of tokens of other document using update function. the below code should returns the number of occurences for each word in dataframe:
from collections import counter
# instantiate counter object
counter_df = counter()

# call update function of the counter object in update the counts
example_df[""tokens""].map(counter_df.update)

however, the output is:
0    none
1    none
2    none
3    none
name: tokens, dtype: object

the expected output must be like:
counter({'all': 2, 'roads': 1, 'lead': 1, 'to': 1, 'rome': 1, 'work': 1, 'and': 1, 'no': 1, 'play': 1, 'makes': 1, 'a': 4, 'dull':1, 'buy':1, 'any':1, 'port':1, 'in': 1, 'storm':1, 'avoid':1, 'questioner':1, ',':1, 'for':1, 'he':1})

thank you again :)","['python', 'pandas', 'nlp', 'spacy', 'tokenize']",73082425,"you can use
example_df[""tokens""] = example_df[""text""].apply(lambda x: [t.text for t in nlp.tokenizer(x)])

see the pandas test:
import pandas as pd
details = {
    'text_id' : [23, 21, 22, 21],
    'text' : ['all roads lead to rome', 
              'all work and no play makes jack a dull buy', 
              'any port in a storm', 
              'avoid a questioner, for he is also a tattler'],
}
  
# creating a dataframe object 
example_df = pd.dataframe(details)
import spacy

nlp = spacy.load(""en_core_web_sm"")

example_df[""tokens""] = example_df[""text""].apply(lambda x: [t.text for t in nlp.tokenizer(x)])

print(example_df.to_string())

output:
   text_id                                          text                                                    tokens
0       23                        all roads lead to rome                              [all, roads, lead, to, rome]
1       21    all work and no play makes jack a dull buy     [all, work, and, no, play, makes, jack, a, dull, buy]
2       22                           any port in a storm                                 [any, port, in, a, storm]
3       21  avoid a questioner, for he is also a tattler  [avoid, a, questioner, ,, for, he, is, also, a, tattler]",https://stackoverflow.com/questions/73082256,python,22-07-2022 14:50,2100.0,2.0,2.0,True,25-07-2022 12:01,22-07-2022 15:47
47666699,using word2vec to classify words in categories,"background
i have vectors with some sample data and each vector has a category name (places,colors,names).
['john','jay','dan','nathan','bob']  -> 'names'
['yellow', 'red','green'] -> 'colors'
['tokyo','bejing','washington','mumbai'] -> 'places'

my objective is to train a model that take a new input string and predict which category it belongs to. for example if a new input is ""purple"" then i should be able to predict 'colors' as the correct category. if the new input is ""calgary"" it should predict 'places' as the correct category.
approach
i did some research and came across word2vec. this library has a ""similarity"" and ""mostsimilarity"" function which i can use. so one brute force approach i thought of is the following:

take new input.
calculate it's similarity with each word in each vector and take an average.

so for instance for input ""pink"" i can calculate its similarity with words in vector ""names"" take a average and then do that for the other 2 vectors also. the vector that gives me the highest similarity average would be the correct vector for the input to belong to.
issue
given my limited knowledge in nlp and machine learning i am not sure if that is the best approach and hence i am looking for help and suggestions on better approaches to solve my problem. i am open to all suggestions and also please point out any mistakes i may have made as i am new to machine learning and nlp world.","['python', 'machine-learning', 'nlp', 'word2vec', 'gensim']",47758616,"if you're looking for the simplest / fastest solution then i'd suggest you take the pre-trained word embeddings (word2vec or glove) and just build a simple query system on top of it. the vectors have been trained on a huge corpus and are likely to contain good enough approximation to your domain data.
here's my solution below:
import numpy as np

# category -> words
data = {
  'names': ['john','jay','dan','nathan','bob'],
  'colors': ['yellow', 'red','green'],
  'places': ['tokyo','bejing','washington','mumbai'],
}
# words -> category
categories = {word: key for key, words in data.items() for word in words}

# load the whole embedding matrix
embeddings_index = {}
with open('glove.6b.100d.txt') as f:
  for line in f:
    values = line.split()
    word = values[0]
    embed = np.array(values[1:], dtype=np.float32)
    embeddings_index[word] = embed
print('loaded %s word vectors.' % len(embeddings_index))
# embeddings for available words
data_embeddings = {key: value for key, value in embeddings_index.items() if key in categories.keys()}

# processing the query
def process(query):
  query_embed = embeddings_index[query]
  scores = {}
  for word, embed in data_embeddings.items():
    category = categories[word]
    dist = query_embed.dot(embed)
    dist /= len(data[category])
    scores[category] = scores.get(category, 0) + dist
  return scores

# testing
print(process('pink'))
print(process('frank'))
print(process('moscow'))

in order to run it, you'll have to download and unpack the pre-trained glove data from here (careful, 800mb!). upon running, it should produce something like this:
{'colors': 24.655489603678387, 'names': 5.058711671829224, 'places': 0.90213905274868011}
{'colors': 6.8597321510314941, 'names': 15.570847320556641, 'places': 3.5302454829216003}
{'colors': 8.2919375101725254, 'names': 4.58830726146698, 'places': 14.7840416431427}

... which looks pretty reasonable. and that's it! if you don't need such a big model, you can filter the words in glove according to their tf-idf score. remember that the model size only depends on the data you have and words you might want to be able to query.",https://stackoverflow.com/questions/47666699,python,06-12-2017 04:16,14459.0,19.0,2.0,True,12-04-2024 10:22,11-12-2017 22:23
68534365,saving the custom model,"i have a built a custom  model and planned to store the model in \ s3 which suits my project.
we have 2 different ways to save a model
nlp.to_bytes()
nlp.to_disk()

nlp.to_disk() needs a path in the argument. so i choose to go with nlp.to_bytes(),
code:
to_bytes_model = custom_nlp.to_bytes()
in: type(to_bytes_model) 
out: bytes

s3 = boto3.resource('s3')    
s3.bucket('customregex').upload_file('to_bytes_model','new_folder')

the above code for boto3 gives me error saying there is no file to_bytes_model
do need help on saving the nlp model directly to s3. thanks.","['python', 'amazon-s3', 'boto3', 'spacy', 'named-entity-recognition']",68538179,"to_bytes_model is an in-memory object. it is not written to a file on disk. using upload_file() which looks for a file on disk based on the name of the file you pass in. see the documentation here.
one way to do this is to use nlp.to_disk() followed by using some library to create zip/tar and then use upload_file() to upload the zip/tar. if you wish to skip the zip/tar you will need iterate over all the files in the folder to upload them one by one. i personally prefer option 1.",https://stackoverflow.com/questions/68534365,python,26-07-2021 17:46,535.0,1.0,1.0,True,28-07-2021 11:04,28-07-2021 11:04
76319917,&quot;python&quot; unable to import spacy and download en_core_web_sm,"what i want to achieve:
import spacy and use it.
what i've tried:
when i try to import spacy on python i get importerror: cannot import name util error (detail on error1)
spacy is sucessfully installed to my device.

following article i operated
pip uninstall en_core_web_sm then i got
warning: skipping en_core_web_sm as it is not installed.
operate python -m spacy download en_core_web_sm give me
typeerror: issubclass() arg 1 must be a class error (detail in error2)
error1:
importerror                               traceback (most recent call last)
~\appdata\local\temp/ipykernel_11524/513823458.py in <module>
      1 import sys
----> 2 import spacy

~\appdata\roaming\python\python39\site-packages\spacy\__init__.py in <module>
     12 from thinc.api import config
     13 
---> 14 from . import pipeline  # noqa: f401
     15 from .cli.info import info  # noqa: f401
     16 from .glossary import explain  # noqa: f401

~\appdata\roaming\python\python39\site-packages\spacy\pipeline\__init__.py in <module>
----> 1 from .attributeruler import attributeruler
      2 from .dep_parser import dependencyparser
      3 from .edit_tree_lemmatizer import edittreelemmatizer
      4 from .entity_linker import entitylinker
      5 from .ner import entityrecognizer

~\appdata\roaming\python\python39\site-packages\spacy\pipeline\attributeruler.py in <module>
      4 from pathlib import path
      5 
----> 6 from .pipe import pipe
      7 from ..errors import errors
      8 from ..training import example

~\appdata\roaming\python\python39\site-packages\spacy\pipeline\pipe.pyx in init spacy.pipeline.pipe()

~\appdata\roaming\python\python39\site-packages\spacy\vocab.pyx in init spacy.vocab()

~\appdata\roaming\python\python39\site-packages\spacy\tokens\__init__.py in <module>
----> 1 from .doc import doc
      2 from .token import token
      3 from .span import span
      4 from .span_group import spangroup
      5 from ._serialize import docbin

~\appdata\roaming\python\python39\site-packages\spacy\tokens\doc.pyx in init spacy.tokens.doc()

importerror: cannot import name util

error2:
traceback (most recent call last):
  file ""c:\users\akira\anaconda3\lib\runpy.py"", line 188, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _error)
  file ""c:\users\akira\anaconda3\lib\runpy.py"", line 147, in _get_module_details
    return _get_module_details(pkg_main_name, error)
  file ""c:\users\akira\anaconda3\lib\runpy.py"", line 111, in _get_module_details
    __import__(pkg_name)
  file ""c:\users\akira\appdata\roaming\python\python39\site-packages\spacy\__init__.py"", line 14, in <module>
    from . import pipeline  # noqa: f401
  file ""c:\users\akira\appdata\roaming\python\python39\site-packages\spacy\pipeline\__init__.py"", line 1, in <module>
    from .attributeruler import attributeruler
  file ""c:\users\akira\appdata\roaming\python\python39\site-packages\spacy\pipeline\attributeruler.py"", line 6, in <module>
    from .pipe import pipe
  file ""spacy\pipeline\pipe.pyx"", line 1, in init spacy.pipeline.pipe
  file ""spacy\vocab.pyx"", line 1, in init spacy.vocab
  file ""c:\users\akira\appdata\roaming\python\python39\site-packages\spacy\tokens\__init__.py"", line 1, in <module>
    from .doc import doc
  file ""spacy\tokens\doc.pyx"", line 36, in init spacy.tokens.doc
  file ""c:\users\akira\appdata\roaming\python\python39\site-packages\spacy\schemas.py"", line 222, in <module>
    class tokenpattern(basemodel):
  file ""pydantic\main.py"", line 205, in pydantic.main.modelmetaclass.__new__
  file ""pydantic\fields.py"", line 491, in pydantic.fields.modelfield.infer
  file ""pydantic\fields.py"", line 421, in pydantic.fields.modelfield.__init__
  file ""pydantic\fields.py"", line 537, in pydantic.fields.modelfield.prepare
  file ""pydantic\fields.py"", line 634, in pydantic.fields.modelfield._type_analysis
  file ""pydantic\fields.py"", line 641, in pydantic.fields.modelfield._type_analysis
  file ""c:\users\akira\anaconda3\lib\typing.py"", line 847, in __subclasscheck__
    return issubclass(cls, self.__origin__)
typeerror: issubclass() arg 1 must be a class","['python', 'spacy']",76322191,this has been reported. see the suggested workaround:,https://stackoverflow.com/questions/76319917,python,24-05-2023 03:27,3047.0,4.0,1.0,True,24-05-2023 09:48,24-05-2023 03:43
78020395,how to specify nested json using langchain,"i am using structuredparser of langchain library. i am getting flat dictionary from parser. please guide me to get a list of dictionaries from output parser.
prompt_template = """""" 
you are an android developer. 
parse this error message and provide me identifiers & texts mentioend in error message. 
--------
error message is {msg}
--------
{format_instructions}
""""""

def get_output_parser():
    missing_id = responseschema(name=""identifier"", description=""this is missing identifier."")
    missing_text = responseschema(name=""text"", description=""this is missing text."")

    response_schemas = [missing_id, missing_text]
    output_parser = structuredoutputparser.from_response_schemas(response_schemas)
    return output_parser


def predict_result(msg):
    model = chatopenai(open_api_key="""", openai_api_base="""", model=""llama-2-70b-chat-hf"", temperature=0, max_tokens=2000)
    output_parser = get_output_parser()
    format_instructions = output_parser.get_format_instructions()
    
    prompt = chatprompttemplate.from_template(template=prompt_template)
    message = prompt.format_messages(msg=msg, format_instructions=format_instructions)
    response = model.invoke(message)

    response_as_dict = output_parser.parse(response.content)
    print(response_as_dict)


predict_result(""objectnotfoundexception anyof(allof(withid:identifier1, withtext:text1),allof(withid:identifier2, withtext:text1),allof(withid:identifier3, withtext:text1))"")

the output i get is
{
    ""identifier"":""identifier1"",
    ""text"":""text1""
}


expected output is
[
    {
        ""identifier"":""identifier1"",
        ""text"":""text1""
    },
    {
        ""identifier"":""identifier2"",
        ""text"":""text1""
    },
    {
        ""identifier"":""identifier3"",
        ""text"":""text1""
    }
]

how to specify such nested json in outputparser","['python', 'langchain']",78020976,"i updated my responseschema by specifying json format in description and it gives me expected result. below is the code snippet that is working.
formatted_json = responseschema(name=""assertions"", description='''[{""text"":"""",""identifier"":""""}]'''
response_schemas = [format_json]

i get output as
{""assertions"":[
    {
        ""identifier"":""identifier1"",
        ""text"":""text1""
    },
    {
        ""identifier"":""identifier2"",
        ""text"":""text1""
    },
    {
        ""identifier"":""identifier3"",
        ""text"":""text1""
    }
]}",https://stackoverflow.com/questions/78020395,python,19-02-2024 11:44,1023.0,1.0,2.0,True,19-02-2024 13:23,19-02-2024 11:52
72202295,how to apply max_length to truncate the token sequence from the left in a huggingface tokenizer?,"in the huggingface tokenizer, applying the max_length argument specifies the length of the tokenized text. i believe it truncates the sequence to max_length-2 (if truncation=true) by cutting the excess tokens from the right. for the purposes of utterance classification, i need to cut the excess tokens from the left, i.e. the start of the sequence in order to preserve the last tokens. how can i do that?
from transformers import autotokenizer

train_texts = ['text 1', ...]
tokenizer = autotokenizer.from_pretrained('xlm-roberta-base')
encodings = tokenizer(train_texts, max_length=128, truncation=true)","['python', 'pytorch', 'huggingface-transformers', 'bert-language-model', 'huggingface-tokenizers']",72798443,"tokenizers have a truncation_side parameter that should set exactly this.
see the docs.",https://stackoverflow.com/questions/72202295,python,11-05-2022 13:52,10646.0,5.0,3.0,True,25-05-2023 09:26,11-05-2022 14:46
64640843,understanding the model of openai 5 (1024 unit lstm reinforcement learning),"i recently came across openai 5. i was curious to see how their model is built and understand it. i read in wikipedia that it ""contains a single layer with a 1024-unit lstm"". then i found this pdf containing a scheme of the architecture.
my questions
from all this i don't understand a few things:

what does it mean to have a 1024-unit lstm layer? does this mean we have 1024 time steps with a single lstm cell, or does this mean we have 1024 cells. could you show me some kind of graph visualizing this? i'm especially having a hard time visualizing 1024 cells in one layer. (i tried looking at several so questions such as 1, 2, or the openai 5 blog, but they didn't help much).

how can you do reinforcement learning on such model? i'm used to rl being used with q-tables and them being updated during training. does this simply mean that their loss function is the reward?

how come such large model doesn't suffer from vanishing gradients or something? haven't seen in the pdf any types of normalizations or so.

in the pdf you can see a blue rectangle, seems like it's a unit and there are n of those. what does this mean? and correct me please if i'm mistaken, the pink boxes are used to select the best move/item(?)



in general all of this can be summarized to ""how does the openai 5 model work?","['tensorflow', 'machine-learning', 'deep-learning', 'lstm', 'openai-api']",64645229,"it means that the size of the hidden state is 1024 units, which is essentially that your lstm has 1024 cells, in each timestep. we do not know in advance how many timesteps we will have.

the state of the lstm (hidden state) represents the current state that is observed by the agent. it gets updated every timestep using the input received. this hidden state can be used to predict the q-function (as in deep q-learning). you don't have an explicit table of (state, action) -> q_value, instead you have a 1024 sized vector which represents the state and feeds into another dense layer, which will output the q_values for all possible actions.

lstms are the mechanism which help stop vanishing gradients, as the long range memory also allows the gradients to flow back easier.

if you are referring to the big blue and pink boxes, then the pink ones seem like they are the input values which are put through a network and pooled, over each pickup or modifier. the blue space seems to be the same thing over each unit. the terms pickup, modifier, unit, etc., should be meaningful in the context of the game they are playing.


here is an image of the lstm - the yellow nodes at each step are the n:

the vector h is the hidden state of the lstm which is being passed to both the next timestep and being used as the output of that timestep.",https://stackoverflow.com/questions/64640843,tensorflow,02-11-2020 06:36,651.0,2.0,1.0,True,02-11-2020 12:11,02-11-2020 06:42
73797902,gpt-3 api invalid_request_error: you must provide a model parameter,"i'm new to apis and i'm trying to understand how to get a response from a prompt using openai's gpt-3 api (using api.openai.com/v1/completions). i'm using postman to do so.
the documentation says that there is only one required parameter, which is the ""model."" however, i get an error saying that ""you must provide a model parameter,"" even though i already provided it.
what am i doing wrong?","['rest', 'postman', 'http-post', 'openai-api', 'gpt-3']",73853263,"you can get this to work the following way in postman with the post setting:

leave all items in the params tab empty

in the authorization tab, paste your openai api token as the type bearer token (as you likely already did)

in the headers tab, add key ""content-type"" with value ""application/json""

in the body tab, switch to raw, and add e.g.
 {  
     ""model"":""text-davinci-002"",
     ""prompt"":""albert einstein was""
 }


hit send. you'll get back the completions for your prompt.


note alternatively, you can add the model into the post url, like 
while above works, it might not be using the postman ui to its full potential -- after all, we're raw-editing json instead of utilizing nice key-value input boxes. if you find out how to do the latter, let us know.",https://stackoverflow.com/questions/73797902,rest,21-09-2022 08:49,38529.0,17.0,4.0,True,11-05-2023 08:16,11-01-2023 20:14
64898002,apertium + python: pos-tagger not providing surface form,"i'm trying to pos-tag some sentences in italian with apertium's tagger.
while according to the apertium github page i am supposed to get as output also the surface form in addition to the morphological analysis, i only get the analysis. i want also the surface form. i cannot infer it since the tagger doesn't necessarily tag a single token, so i cannot simply tokenize the original sentence and loop over it or zip it with the tagger's output.
according to the github page:
in [1]: import apertium
in [2]: tagger = apertium.tagger('ita')
in [3]: tagger.tag('gatti').
out[3]: [gatti/gatto<n><m><pl>]

what i got:
in [1]: import apertium
in [2]: tagger = apertium.tagger('ita')
in [3]: tagger.tag('gatti') # 'gatti' is the surface form
out[3]: [gatto<n><m><pl>]

how can i get the surface form? if i provided one token at a time this would not be a problem since i would know what the token is. but in a sentence i cannot know how the tagger creates chunks.","['python', 'nlp', 'pos-tagger', 'morphological-analysis', 'apertium']",64901691,"by default, when creating a tagger of language ita it looks for /usr/share/apertium/modes/ita-tagger.mode. this is a shell script that calls various apertium commands. the command for the italian tagger script happens to be configured to not include surface commands (it's missing the -p option).
a quick and dirty solution is to just sudo vim /usr/share/apertium/modes/ita-tagger.mode (or sudo nano or whatever your editor is) and add -p to the end of the last command, so the file looks like
lt-proc -w '/usr/share/apertium/apertium-ita/ita.automorf.bin' | cg-proc '/usr/share/apertium/apertium-ita/ita.rlx.bin' | apertium-tagger -g $2 '/usr/share/apertium/apertium-ita/ita.prob' -p

and do tagger = apertium.tagger('ita') again.

a sudo-less solution would be to copy the mode file, edit, and add it to the search path, see",https://stackoverflow.com/questions/64898002,python,18-11-2020 17:15,97.0,1.0,1.0,True,09-10-2024 21:26,09-10-2024 21:26
68907519,bert with padding and masked token predicton,"i am playing around with bert pretrained models (bert-large-uncased-whole-word-masking)
i used huggingface to try it i first used this piece of code
m = tfbertlmheadmodel.from_pretrained(""bert-large-cased-whole-word-masking"")
logits = m(tokenizer(""hello world [mask] like it"",return_tensors=""tf"")[""input_ids""]).logits

i then used argmax to get max probabilities after applying softmax,
things works fine until now.
when i used padding with max_length = 100 the model started making false prediction and not working well and all predicted tokens were the same i.e 119-token id
code i used for argmax
tf.argmax(tf.keras.activations.softmax(m(tokenizer(""hello world [mask] like it"",return_tensors=""tf"",max_length=,padding=""max_length"")[""input_ids""]).logits)[0],axis=-1)

output before using padding
<tf.tensor: shape=(7,), dtype=int64, numpy=array([ 9800, 19082,  1362,   146,  1176,  1122,   119])>

output after using padding with max_length of 100
<tf.tensor: shape=(100,), dtype=int64, numpy=
array([119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119])>

i wonder if this problem prevail even training a new model as it is mandatory to set input shape for training new model i padded and tokenized the data but, now i want to know if this problem continues with it too.","['tensorflow', 'keras', 'bert-language-model', 'huggingface-transformers', 'language-model']",69064907,"as already mentioned in the comments, you forgot to pass the attention_mask to bert and it, therefore, treated the added padding tokens like ordinary tokens.
you also asked in the comments how you can rid of the padding token prediction. there are several ways to do it depending on your actual task. one of them is removing them with boolean_mask and the attention_mask as shown below:
import tensorflow as tf
from transformers import tfbertlmheadmodel, berttokenizerfast

ckpt = ""bert-large-cased-whole-word-masking""

t = berttokenizerfast.from_pretrained(ckpt)
m = tfbertlmheadmodel.from_pretrained(ckpt)

e = t(""hello world [mask] like it"",return_tensors=""tf"")
e_padded = t(""hello world [mask] like it"",return_tensors=""tf"", padding=""max_length"", max_length = 100)

def prediction(encoding):
  logits = m(**encoding).logits
  token_mapping = tf.argmax(tf.keras.activations.softmax(logits),axis=-1)
  return tf.boolean_mask(token_mapping, encoding[""attention_mask""])

token_predictions = prediction(e) 
token_predictions_padded = prediction(e_padded) 

print(token_predictions)
print(token_predictions_padded)

output:
tf.tensor([ 9800 19082  1362   146  1176  1122   119], shape=(7,), dtype=int64)
tf.tensor([ 9800 19082  1362   146  1176  1122   119], shape=(7,), dtype=int64)",https://stackoverflow.com/questions/68907519,tensorflow,24-08-2021 12:23,1746.0,2.0,1.0,True,05-09-2021 15:49,24-08-2021 21:08
76199653,"valueerror: `run` not supported when there is not exactly one input key, got [&#39;question&#39;, &#39;documents&#39;]","getting the error while trying to run a langchain code.
valueerror: `run` not supported when there is not exactly one input key, got ['question', 'documents'].
traceback:
file ""c:\users\aviparna.biswas\appdata\local\programs\python\python37\lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py"", line 565, in _run_script
    exec(code, module.__dict__)
file ""d:\python projects\poc\radium\ana\app.py"", line 49, in <module>
    answer = question_chain.run(formatted_prompt)
file ""c:\users\aviparna.biswas\appdata\local\programs\python\python37\lib\site-packages\langchain\chains\base.py"", line 106, in run
    f""`run` not supported when there is not exactly one input key, got ['question', 'documents'].""

my code is as follows.
import os
from apikey import apikey

import streamlit as st
from langchain.llms import openai
from langchain.prompts import prompttemplate
from langchain.chains import llmchain, sequentialchain
#from langchain.memory import conversationbuffermemory
from docx import document

os.environ['openai_api_key'] = apikey

# app framework
st.title('ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ colab ana answering bot..')
prompt = st.text_input('plug in your question here')


# upload multiple documents
uploaded_files = st.file_uploader(""choose your documents (docx files)"", accept_multiple_files=true, type=['docx'])
document_text = """"

# read and combine word documents
def read_docx(file):
    doc = document(file)
    full_text = []
    for paragraph in doc.paragraphs:
        full_text.append(paragraph.text)
    return '\n'.join(full_text)

for file in uploaded_files:
    document_text += read_docx(file) + ""\n\n""

with st.expander('contextual prompt'):
    st.write(document_text)

# prompt template
question_template = prompttemplate(
    input_variables=['question', 'documents'],
    template='given the following documents: {documents}. answer the question: {question}'
)

# llms
llm = openai(teuestion_chain = llmchain(llm=llm, prompt=question_template, verbose=true, output_key='answer')

# show answer if there's a prompt and documents are uploaded
if prompt and document_text:
    formatted_prompt = question_template.format(question=prompt, documents=document_text)
    answer = question_chain.run(formatted_prompt)
    st.write(answer['answer'])

i have gone through the documentations and even then i am getting the same error. i have already seen demos where multiple prompts are being taken by langchain.","['python', 'langchain']",76203995,"for a prompt with multiple inputs, use predict() instead of run(), or just call the chain directly. (note: requires python 3.8+)
prompt_template = ""tell me a {adjective} joke and make it include a {profession}""
llm_chain = llmchain(
    llm=openai(temperature=0.5),
    prompt=prompttemplate.from_template(prompt_template)
)

# option 1
llm_chain(inputs={""adjective"": ""corny"", ""profession"": ""plumber""})

# option 2
llm_chain.predict(adjective=""corny"", profession=""plumber"")

also note that you only need to assign the prompttemplate at the moment you're instantiating the llmchain - after that you're just passing in the template variables - in your case, documents and question (instead of passing in the formatted template, as you have currently).",https://stackoverflow.com/questions/76199653,python,08-05-2023 10:24,7706.0,4.0,2.0,True,12-05-2023 12:55,08-05-2023 11:17
62743531,using gensim fasttext model with lstm nn in keras,"i have trained fasttext model with gensim over the corpus of very short sentences (up to 10 words). i know that my test set includes words that are not in my train corpus, i.e some of the words in my corpus are like ""oxytocin"" ""lexitocin"", ""ematrophin"",'betaxitocin""
given a new word in the test set, fasttext knows pretty well to generate a vector with high cosine-similarity to the other similar words in the train set by using the characters level n-gram
how do i incorporate the fasttext model inside a lstm keras network without losing the fasttext model to just a list of vectors in the vocab? because then i won't handle any oov even when fasttext do it well.
any idea?","['tensorflow', 'keras', 'nlp', 'gensim', 'word-embedding']",62747179,"here the procedure to incorporate the fasttext model inside an lstm keras network
# define dummy data and precproces them

docs = ['well done',
        'good work',
        'great effort',
        'nice work',
        'excellent',
        'weak',
        'poor effort',
        'not good',
        'poor work',
        'could have done better']

docs = [d.lower().split() for d in docs]

# train fasttext from gensim api

ft = fasttext(size=10, window=2, min_count=1, seed=33)
ft.build_vocab(docs)
ft.train(docs, total_examples=ft.corpus_count, epochs=10)

# prepare text for keras neural network

max_len = 8

tokenizer = tf.keras.preprocessing.text.tokenizer(lower=true)
tokenizer.fit_on_texts(docs)

sequence_docs = tokenizer.texts_to_sequences(docs)
sequence_docs = tf.keras.preprocessing.sequence.pad_sequences(sequence_docs, maxlen=max_len)

# extract fasttext learned embedding and put them in a numpy array

embedding_matrix_ft = np.random.random((len(tokenizer.word_index) + 1, ft.vector_size))

pas = 0
for word,i in tokenizer.word_index.items():
    
    try:
        embedding_matrix_ft[i] = ft.wv[word]
    except:
        pas+=1

# define a keras model and load the pretrained fasttext weights matrix

inp = input(shape=(max_len,))
emb = embedding(len(tokenizer.word_index) + 1, ft.vector_size, 
                weights=[embedding_matrix_ft], trainable=false)(inp)
x = lstm(32)(emb)
out = dense(1)(x)

model = model(inp, out)

model.predict(sequence_docs)

how to deal unseen text
unseen_docs = ['asdcs work','good nxsqa zajxa']
unseen_docs = [d.lower().split() for d in unseen_docs]

sequence_unseen_docs = tokenizer.texts_to_sequences(unseen_docs)
sequence_unseen_docs = tf.keras.preprocessing.sequence.pad_sequences(sequence_unseen_docs, maxlen=max_len)

model.predict(sequence_unseen_docs)",https://stackoverflow.com/questions/62743531,tensorflow,05-07-2020 16:39,4100.0,6.0,1.0,True,16-01-2021 17:16,16-01-2021 17:16
71269432,how to load data for only certain label of spacy&#39;s ner entities?,"i just started to explore spacy and need it only for gpe (global political entities) of the name entity recognition (ner) component.
so, to save time on loading i keep only 'ner':
    nlp = spacy.load('en_core_web_sm', disable=['tok2vec','tagger','parser', 'senter', 'attribute_ruler', 'lemmatizer'])

then i create a set of cities / states / countries that exist in the text by running:
doc = nlp(txt) 
geo_ents = {str(word) for word in doc.ents if word.label_=='gpe'}

that means i only need a small subset of the entities with the label_=='gpe'.
i didn't find a way yet to iterate only within that component of the whole model to reduce runtime on big loads of texts.
would you please guide me to how to load only certain label of spacy's ner entities? that might be helpful for others in order to get only selected types of entities.
thank you very much!","['python', 'nlp', 'spacy', 'named-entity-recognition']",71290608,"it isn't possible to do this. the ner model is classifying each token/span between all the labels it knows about, and the knowledge is not separable.
additionally, the ner component requires a tok2vec. depending on the pipeline architecture you may be able to disable the top-level tok2vec. (edit: i incorrectly stated the top-level tok2vec was required for the small english model; it is not. see here for details.)
it may be possible to train a smaller model that only recognizes gpes with similar accuracy, but i wouldn't be too optimistic about it. it also wouldn't be faster.",https://stackoverflow.com/questions/71269432,python,25-02-2022 17:18,1191.0,1.0,1.0,True,01-03-2022 04:03,25-02-2022 17:31
72695297,difference between from_config and from_pretrained in huggingface,"num_labels = 3 if task.startswith(""mnli"") else 1 if task==""stsb"" else 2
preconfig = distilbertconfig(n_layers=6)
    
model1 = automodelforsequenceclassification.from_config(preconfig)
model2 = automodelforsequenceclassification.from_pretrained(model_checkpoint, num_labels=num_labels)

i am modifying this code (modified code is provided above) to test distilbert transformer layer depth size via from_config since from my knowledge from_pretrained uses 6 layers because in the paper section 3 they said:

we initialize the student from the teacher by taking one layer out of two

while what i want to test is various sizes of layers. to test whether both are the same, i tried running the from_config
with n_layers=6 because based on the documentation distilbertconfig the n_layers is used to determine the transformer block depth. however as i run model1 and model2 i found that with  sst-2 dataset, in accuracy:

model1 achieved only 0.8073
model2 achieved 0.901

if they both behave the same i expect the result to be somewhat similar but 10% drop is a significant drop, therefore i believe there ha to be a difference between the functions. is there a reason behind the difference of the approach (for example model1 has not yet applied hyperparameter search) and is there a way to make both functions behave the same? thank you!","['huggingface-transformers', 'transformer-model', 'distilbert']",72744660,"the two functions you described, from_config and from_pretrained, do not behave the same. for a model m, with a reference r:

from_config allows you to instantiate a blank model, which has the same configuration (the same shape) as your model of choice: m is as r was before training
from_pretrained allows you to load a pretrained model, which has already been trained on a specific dataset for a given number of epochs: m is as r after training.

to cite the doc, note: loading a model from its configuration file does not load the model weights. it only affects the modelï¿½ï¿½ï¿½s configuration. use from_pretrained() to load the model weights.</code",https://stackoverflow.com/questions/72695297,huggingface-transformers,21-06-2022 04:20,5215.0,2.0,1.0,True,24-06-2022 13:14,21-06-2022 04:30
67821111,download nltk corpus as cmdclass in setup.py files not working,"there are some parts of the nltk corpus that i'd like to add to the setup.py file. i followed the response here by setting up a custom cmdclass. my setup file looks like this.

from setuptools import setup
from setuptools.command.install import install as _install


class downloadnltk(install):
    def run(self):
        self.do_egg_install()
        import nltk
        nltk.download('wordnet')
        nltk.download('punkt')
        nltk.download('stopwords')
        nltk.download('vader_lexicon')

setup(
    install_requires=requirements,
    python_requires='>=3.7',
    cmdclass={'download_nltk': downloadnltk()}
)


however, running it, i get this error:
traceback (most recent call last):
  file ""setup.py"", line 15, in <module>
    'install': downloadnltk()}
typeerror: __init__() missing 1 required positional argument: 'dist'

i tried to better understand what's needed, but i have to say that the documentation i found here on dist is not very clear to me. can someone help with a solution approach? thanks!","['python', 'nltk', 'setuptools', 'setup.py']",67821277,"pass the class, not its instance:
    cmdclass={'download_nltk': downloadnltk}

(no () to avoid instantiating the class)",https://stackoverflow.com/questions/67821111,python,03-06-2021 12:01,280.0,2.0,1.0,True,19-12-2022 15:25,03-06-2021 12:13
79241735,unexpected transformer&#39;s dataset structure after set_transform or with_transform,"i am using the feature extractor from vit like explained here.
and noticed a weird behaviour i cannot fully understand.
after loading the dataset as in that colab notebook, i see:
ds['train'].features

{'image_file_path': value(dtype='string', id=none),  'image':
image(mode=none, decode=true, id=none),  'labels':
classlabel(names=['angular_leaf_spot', 'bean_rust', 'healthy'],
id=none)}

and we can assess the features in both ways:
ds['train']['labels'][0:5]

[0, 0, 0, 0, 0]

ds['train'][0:2]

{'image_file_path':
['/home/albert/.cache/huggingface/datasets/downloads/extracted/967f0d9f61a7a8de58892c6fab6f02317c06faf3e19fba6a07b0885a9a7142c7/train/angular_leaf_spot/angular_leaf_spot_train.0.jpg',
'/home/albert/.cache/huggingface/datasets/downloads/extracted/967f0d9f61a7a8de58892c6fab6f02317c06faf3e19fba6a07b0885a9a7142c7/train/angular_leaf_spot/angular_leaf_spot_train.1.jpg'],
'image': [<pil.jpegimageplugin.jpegimagefile image mode=rgb
size=500x500>,   <pil.jpegimageplugin.jpegimagefile image mode=rgb
size=500x500>],  'labels': [0, 0]}

but after
from transformers import vitfeatureextractor

model_name_or_path = 'google/vit-base-patch16-224-in21k'
feature_extractor = vitfeatureextractor.from_pretrained(model_name_or_path)
ds = load_dataset('beans')

def transform(example_batch):
    inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')
    inputs['labels'] = example_batch['labels']
    return inputs

prepared_ds = ds.with_transform(transform)

we see the features are kept:
prepared_ds['train'].features

{'image_file_path': value(dtype='string', id=none),  'image':
image(mode=none, decode=true, id=none),  'labels':
classlabel(names=['angular_leaf_spot', 'bean_rust', 'healthy'],
id=none)}

prepared_ds['train'][0:2]

{'pixel_values': tensor([[[[-0.5686, -0.5686, -0.5608,  ..., -0.0275, 
0.1843, -0.2471],
...,
[-0.5843, -0.5922, -0.6078,  ...,  0.2627,  0.1608,  0.2000]],

         [[-0.7098, -0.7098, -0.7490,  ..., -0.3725, -0.1608, -0.6000],
          ...,
          [-0.8824, -0.9059, -0.9216,  ..., -0.2549, -0.2000, -0.1216]]],

        [[[-0.5137, -0.4902, -0.4196,  ..., -0.0275, -0.0039, -0.2157],
          ...,
          [-0.5216, -0.5373, -0.5451,  ..., -0.1294, -0.1529, -0.2627]],

         [[-0.1843, -0.2000, -0.1529,  ...,  0.2157,  0.2078, -0.0902],
          ...,
          [-0.7725, -0.7961, -0.8039,  ..., -0.3725, -0.4196, -0.5451]],

         [[-0.7569, -0.8510, -0.8353,  ..., -0.3255, -0.2706, -0.5608],
          ...,
          [-0.5294, -0.5529, -0.5608,  ..., -0.1686, -0.1922, -0.3333]]]]), 'labels': [0, 0]}

but when i try to access the labels directly
prepared_ds['train']['labels']

i got a key error message:
```
--------------------------------------------------------------------------- 
keyerror                                  traceback (most recent call last) cell in[32], line 1
----> 1 prepared_ds['train']['labels']

file ~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/arrow_dataset.py:2872, in dataset.__getitem__(self, key)    2870 def __getitem__(self, key): 
# noqa: f811    2871     """"""can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).""""""
-> 2872     return self._getitem(key)

file ~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/arrow_dataset.py:2857, in dataset._getitem(self, key, **kwargs)    2855 formatter = get_formatter(format_type, features=self._info.features,
**format_kwargs)    2856 pa_subtable = query_table(self._data, key, indices=self._indices)
-> 2857 formatted_output = format_table(    2858     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns    2859 )    2860 return formatted_output

file ~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/formatting/formatting.py:639, in format_table(table, key, formatter, format_columns, output_all_columns)
    637 python_formatter = pythonformatter(features=formatter.features)
    638 if format_columns is none:
--> 639     return formatter(pa_table, query_type=query_type)
    640 elif query_type == ""column"":
    641     if key in format_columns:

file ~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/formatting/formatting.py:405, in formatter.__call__(self, pa_table, query_type)
    403     return self.format_row(pa_table)
    404 elif query_type == ""column"":
--> 405     return self.format_column(pa_table)
    406 elif query_type == ""batch"":
    407     return self.format_batch(pa_table)

file ~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/formatting/formatting.py:501, in customformatter.format_column(self, pa_table)
    500 def format_column(self, pa_table: pa.table) -> columnformat:
--> 501     formatted_batch = self.format_batch(pa_table)
    502     if hasattr(formatted_batch, ""keys""):
    503         if len(formatted_batch.keys()) > 1:

file ~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/formatting/formatting.py:522, in customformatter.format_batch(self, pa_table)
    520 batch = self.python_arrow_extractor().extract_batch(pa_table)
    521 batch = self.python_features_decoder.decode_batch(batch)
--> 522 return self.transform(batch)

cell in[12], line 5, in transform(example_batch)
      3 def transform(example_batch):
      4     # take a list of pil images and turn them to pixel values
----> 5     inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')
      7     # don't forget to include the labels!
      8     inputs['labels'] = example_batch['labels']

keyerror: 'image'
```

it sounds like the error is because the feature extractor added 'pixel_values' but the feature is kept as 'image'
but it also appears to imply an attempt to re-apply transform...
also: it is not possible to save the dataset to the disk
    prepared_ds.save_to_disk(img_path)
```
--------------------------------------------------------------------------- 
typeerror                                 traceback (most recent call last) cell in[21], line 1
----> 1 dataset.save_to_disk(img_path)

file ~/anaconda3/envs/llm/lib/python3.13/site-packages/datasets/arrow_dataset.py:1503, in dataset.save_to_disk(self, dataset_path, max_shard_size, num_shards, num_proc, storage_options)    1501         json.dumps(state[""_format_kwargs""][k])    1502     except typeerror as e:
-> 1503         raise typeerror(    1504             str(e) + f""\nthe format kwargs must be json serializable, but key '{k}' isn't.""    1505 ) from none    1506 # get json serializable dataset info    1507 dataset_info = asdict(self._info)

typeerror: object of type function is not json serializable the format kwargs must be json serializable, but key 'transform' isn't.
```

note the original codes in that notebook work perfectly (training, evaluation, etc). i just got this error because i tried to inspect the dataset, try to save the generated dataset, etc. to explore the dataset object...
shouldn't the dataset structure be accessible in a similar way after with_transform() or set_transform()? why does it call the transform function again if we just attempt to access one of the features?
iï¿½ï¿½ï¿½m hoping you can shed some light on this behaviour..","['python', 'machine-learning', 'neural-network', 'huggingface-transformers', 'huggingface-datasets']",79247093,"this is not the way how you pick up the dataset items. first you need to indicate the slice:
prepared_ds_batch = prepared_ds['train'][0:10]

by using indexing.
then you can use the key labels
prepared_ds_batch['labels']
[out]: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

regarding the second issue with saving the data: you are not able to save it because of the known issue with transform functions: 
you might however save the dataset as prepared_ds.with_format(none).save_to_disk('test_path'). but after loading it again from disk you need to launch again the transform function.
edited: you cannot use prepared_ds['train']['labels'] as 'labels' is expected to be integers representing indices of the items.",https://stackoverflow.com/questions/79241735,python,01-12-2024 14:07,75.0,0.0,1.0,True,03-12-2024 13:46,02-12-2024 23:38
70421012,how to define a new tensor with a dynamic shape to support batching in a custom layer,"i am trying to implement a custom layer that would preprocess a tokenized sequence of words into a matrix with a predefined number of elements equal to the size of vocabulary. essentially, i'm trying to implement a 'bag of words' layer. this is the closest i could come up with:
    def get_encoder(vocab_size=args.vocab_size):
       encoder = textvectorization(max_tokens=vocab_size)
       encoder.adapt(train_dataset.map(lambda text, label: text))
       return encoder

    class bagofwords(tf.keras.layers.layer):
        def __init__(self, vocab_size=args.small_vocab_size, batch_size=args.batch_size):
            super(bagofwords, self).__init__()
            self.vocab_size = vocab_size
            self.batch_size = batch_size

        def build(self, input_shape):
            super().build(input_shape)

        def call(self, inputs):
            if inputs.shape[-1] == none:
                return tf.constant(np.zeros([self.batch_size, self.vocab_size])) # 32 is the batch size
            outputs = tf.zeros([self.batch_size, self.vocab_size])
            if inputs.shape[-1] != none:
                for i in range(inputs.shape[0]):
                    for ii in range(inputs.shape[-1]):
                        ouput_idx = inputs[i][ii]
                        outputs[i][ouput_idx] = outputs[i][ouput_idx] + 1
            return outputs

    model = keras.models.sequential()
    model.add(encoder)
    model.add(bag_of_words)
    model.add(keras.layers.dense(64, activation='relu'))
    model.add(keras.layers.dense(1, activation='sigmoid'))


no surprise that i get an error when calling fit() on the model: ""incompatible shapes: [8,1] vs. [32,1]"". this happens on  the last steps, when the batch size is less than 32.
my question is: putting aside performance, how do i define the outputs tensor for my bag of words matrix so that it has a dynamic shape for batching and get my code working?
edit 1
after the comment, i realised that the code doesn't work indeed because it never goes to the 'else' branch.
i edited it a bit so that it uses only tf functions:
 class bagofwords(tf.keras.layers.layer):
        def __init__(self, vocab_size=args.small_vocab_size, batch_size=args.batch_size):
            super(bagofwords, self).__init__()
            self.vocab_size = vocab_size
            self.batch_size = batch_size
            self.outputs = tf.variable(tf.zeros([batch_size, vocab_size]))

        def build(self, input_shape):
            super().build(input_shape)

        def call(self, inputs):
            if tf.shape(inputs)[-1] == none:
                return tf.zeros([self.batch_size, self.vocab_size])
            self.outputs.assign(tf.zeros([self.batch_size, self.vocab_size]))
            for i in range(tf.shape(inputs)[0]):
                for ii in range(tf.shape(inputs)[-1]):
                    output_idx = inputs[i][ii]
                    if output_idx >= tf.constant(self.vocab_size, dtype=tf.int64):
                        output_idx = tf.constant(1, dtype=tf.int64)
                    self.outputs[i][output_idx].assign(self.outputs[i][output_idx] + 1)                        
            return outputs


it didn't help though:  attributeerror: 'tensor' object has no attribute 'assign'.","['python', 'tensorflow', 'keras', 'nlp', 'tensorflow2.0']",70439644,"here is an example of a bag-of-words custom keras layer without using any additional preprocessing layers:
import tensorflow as tf

class bagofwords(tf.keras.layers.layer):
   def __init__(self, vocabulary_size):
       super(bagofwords, self).__init__()
       self.vocabulary_size = vocabulary_size

   def call(self, inputs):  
       batch_size = tf.shape(inputs)[0]
       outputs = tf.tensorarray(dtype=tf.float32, size=0, dynamic_size=true)
       for i in range(batch_size):
         string = inputs[i]
         string_length = tf.shape(tf.where(tf.math.not_equal(string, b'')))[0]
         string = string[:string_length]
         string_array = tf.tensorarray(dtype=tf.float32, size=0, dynamic_size=true)
         for s in string:
           string_array = string_array.write(string_array.size(), tf.where(tf.equal(s, self.vocabulary_size), 1.0, 0.0))
         outputs = outputs.write(i, tf.cast(tf.reduce_any(tf.cast(string_array.stack(), dtype=tf.bool), axis=0), dtype=tf.float32))
       return outputs.stack()

and here are the manual preprocessing steps and the model:
labels = [[1], [0], [1], [0]]

texts  = ['all my cats in a row',
          'when my cat sits down, she looks like a furby toy!',
          'the cat from the outer space',
          'sunshine loves to sit like this for some reason.']

default_strip_regex = r'[!""#$%&()\*\+,-\./:;<=>?@\[\\\]^_`{|}~\']'
tensor_of_strings = tf.constant(texts)
tensor_of_strings = tf.strings.lower(tensor_of_strings)
tensor_of_strings = tf.strings.regex_replace(tensor_of_strings, default_strip_regex, """")
split_strings = tf.strings.split(tensor_of_strings).to_tensor()
flattened_split_strings = tf.reshape(split_strings, (split_strings.shape[0] * split_strings.shape[1]))
unique_words, _ = tf.unique(flattened_split_strings)
unique_words = tf.random.shuffle(unique_words)

bag_of_words = bagofwords(vocabulary_size = unique_words)
train_dataset = tf.data.dataset.from_tensor_slices((split_strings, labels))
model = tf.keras.sequential()
model.add(bag_of_words)
model.add(tf.keras.layers.dense(64, activation='relu'))
model.add(tf.keras.layers.dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss = tf.keras.losses.binarycrossentropy())
model.fit(train_dataset.batch(2), epochs=2)

epoch 1/2
4/4 [==============================] - 2s 7ms/step - loss: 0.7081
epoch 2/2
4/4 [==============================] - 0s 6ms/step - loss: 0.7008
<keras.callbacks.history at 0x7f5ba844bad0>

and this is what the 4 encoded sentences look like:
print(bag_of_words(split_strings))

tf.tensor(
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.
  1. 1. 1. 0.]
 [1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.
  0. 1. 1. 0.]
 [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0.
  0. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
  0. 0. 0. 1.]], shape=(4, 28), dtype=float32)",https://stackoverflow.com/questions/70421012,python,20-12-2021 11:19,1375.0,1.0,3.0,True,13-01-2022 12:09,13-01-2022 12:09
69720846,when should i train my own models and when should i use pretrained models?,"is it recommended to train my own models for things like sentiment analysis, despite only having a very small dataset (5000 reviews), or is it best to use pretrained models which were trained on way larger datasets, however aren't ""specialized"" on my data.
also, how could i train my model on my data and then later use it on it too? i was thinking of an iterative approach where the training data would be randomly selected subset of my total data for each learning epoch.","['machine-learning', 'nlp', 'artificial-intelligence']",69721236,"i would go like this:

try the pre-trained model and see how it goes
if results are non satisfactory, you can fine tune it (see this tutorial). basically, you are using your own examples to change the weights of the pre-trained model. this should improve the results, but it depends on how your data is and how many examples you can provide. the more you have, the better it should be (i would try to use 10-20k at least)


also, how could i train my model on my data and then later use it on it too?

be careful to distinguish between pre-train and fine-tuning.
for pre-training you need a huge amount of text (like billions of characters), it is very resource demanding, and tipically you don't want to do that, unless for a very good reason (for example, a model for your target language does not exist).
fine-tuning requires much much less examples (some tents of thousands), it take tipycally less than a day on a single gpu and allow you to exploit pre-trained model created by someone else.
from what you write, i would go with fine-tune.
of course you can save the model for later, as you can see in the tutorial i linked above:
model.save_pretrained(""my_imdb_model"")",https://stackoverflow.com/questions/69720846,machine-learning,26-10-2021 09:46,3963.0,2.0,1.0,True,30-06-2022 09:54,28-10-2021 17:20
70606847,assigning true/false if a token is present in a data-frame,"my current data-frame is:
     |articleid | keywords                                               | 
     |:-------- |:------------------------------------------------------:| 
0    |58b61d1d  | ['second avenue (manhattan, ny)']                      |     
1    |58b6393b  | ['crossword puzzles']                                  |          
2    |58b6556e  | ['workplace hazards and violations', 'trump, donald j']|            
3    |58b657fa  | ['trump, donald j', 'speeches and statements'].        |  

i want a data-frame similar to the following, where a column is added based on whether a trump token, 'trump, donald j' is mentioned in the keywords and if so then it is assigned true :
     |articleid | keywords                                               | trumpmention |
     |:-------- |:------------------------------------------------------:| ------------:|
0    |58b61d1d  | ['second avenue (manhattan, ny)']                      | false        |      
1    |58b6393b  | ['crossword puzzles']                                  | false        |          
2    |58b6556e  | ['workplace hazards and violations', 'trump, donald j']| true         |           
3    |58b657fa  | ['trump, donald j', 'speeches and statements'].        | true         |       

i have tried multiple ways using df functions. but cannot achieve my wanted results. some of the ways i've tried are:
df['trumpmention'] = np.where(any(df['keywords']) == 'trump, donald j', true, false) 

or
df['trumpmention'] = df['keywords'].apply(lambda x: any(token == 'trump, donald j') for token in x) 

or
lst = ['trump, donald j']  
df['trumpmention'] = df['keywords'].apply(lambda x: ([ true for token in x if any(token in lst)]))   

raw input:
df = pd.dataframe({'articleid': ['58b61d1d', '58b6393b', '58b6556e', '58b657fa'],
                   'keywords': [['second avenue (manhattan, ny)'],
                                ['crossword puzzles'],
                                ['workplace hazards and violations', 'trump, donald j'],
                                ['trump, donald j', 'speeches and statements']],
                   'trumpmention': [false, false, true, true]})","['python', 'pandas', 'dataframe', 'text', 'nlp']",70606928,"try
df[""trumpmention""] = df[""keywords""].apply(lambda x: ""trump, donald j"" in x)",https://stackoverflow.com/questions/70606847,python,06-01-2022 12:05,125.0,3.0,4.0,True,06-01-2022 12:38,06-01-2022 12:38
68704499,finding associations in dataset with list of string data in each cell in r,"i am looking for finding a method to find the association between words in the table (or list). in each cell of the table, i have several words separated by "";"".
lets say i have a table as below; some words are 'af' or 'aa' belong to one cell.
df<-read.table(text=""
a           b            c           d
af;aa;az    bf;bb        c;cc       df;dd
aa;az       bf;bc        c          dc;dd
ah;al;aa    bb           c;cd       dd
af;aa       bf           cc         dd"",header=t,stringsasfactors = f)

i want to find associations between all words in the entire dataset, between cells(not interested in within cell association). for example, how many times aa and dd appear in one row, or show me which words have the highest association (e.g. aa with bb, aa with dd,....).
expected output: (the numbers can be inaccurate and association rep does not have be shown with '--')
2 pairs association (numbers can be counts, probability or normalized association)
association    number of associations 
aa--dd          3
aa--c           3
bb--dd          2
...
3 pairs association
aa--bb--dd      3
aa--bb--c       3
...

4 pairs association
aa--bb--c--dd   2
aa--bf--c--dd   2
...

can you help me to implement it in r?
tx","['r', 'associations', 'text-mining']",68749386,"i am not sure if you have something like the approach below in mind. it is basically a custom function which we use in a nested purrr::map call. the outer call loops over the number of pairs: 2,3, 4 and the inner call uses combn to create all possible combinations as input and uses the custom function to create the desired output.
library(tidyverse)

count_pairs <- function(x) {
s <- seq(x)
 df[, x] %>% 
    reduce(s, separate_rows, .init = ., sep = "";"")  
    group_by(across()) %>% 
    count() %>% 
    rename(set_names(s))
}

map(2:4,
    ~ map_dfr(combn(1:4, .x, simplify = false),
                    count_pairs) %>% arrange(-n))
#> [[1]]
#> # a tibble: 50 x 3
#> # groups:   1, 2 [50]
#>    `1`   `2`       n
#>    <chr> <chr> <int>
#>  1 aa    dd        4
#>  2 aa    bf        3
#>  3 aa    c         3
#>  4 bf    dd        3
#>  5 c     dd        3
#>  6 aa    bb        2
#>  7 af    bf        2
#>  8 az    bf        2
#>  9 aa    cc        2
#> 10 af    cc        2
#> # ... with 40 more rows
#> 
#> [[2]]
#> # a tibble: 70 x 4
#> # groups:   1, 2, 3 [70]
#>    `1`   `2`   `3`       n
#>    <chr> <chr> <chr> <int>
#>  1 aa    bf    dd        3
#>  2 aa    c     dd        3
#>  3 aa    bb    c         2
#>  4 aa    bf    c         2
#>  5 aa    bf    cc        2
#>  6 af    bf    cc        2
#>  7 az    bf    c         2
#>  8 aa    bb    dd        2
#>  9 af    bf    dd        2
#> 10 az    bf    dd        2
#> # ... with 60 more rows
#> 
#> [[3]]
#> # a tibble: 35 x 5
#> # groups:   1, 2, 3, 4 [35]
#>    `1`   `2`   `3`   `4`       n
#>    <chr> <chr> <chr> <chr> <int>
#>  1 aa    bb    c     dd        2
#>  2 aa    bf    c     dd        2
#>  3 aa    bf    cc    dd        2
#>  4 af    bf    cc    dd        2
#>  5 az    bf    c     dd        2
#>  6 aa    bb    c     df        1
#>  7 aa    bb    cc    dd        1
#>  8 aa    bb    cc    df        1
#>  9 aa    bb    cd    dd        1
#> 10 aa    bc    c     dc        1
#> # ... with 25 more rows

# the data
df<-read.table(text=""
a           b            c           d
af;aa;az    bf;bb        c;cc       df;dd
aa;az       bf;bc        c          dc;dd
ah;al;aa    bb           c;cd       dd
af;aa       bf           cc         dd"",header=t,stringsasfactors = f)


created on 2021-08-11 by the reprex package (v2.0.1)",https://stackoverflow.com/questions/68704499,r,08-08-2021 20:27,194.0,3.0,3.0,True,17-01-2022 20:11,08-08-2021 20:45
71193790,hot to remove one letter token with tf-idf vectorizer,"i'm working on a small project to calculate the tf_idf in this document which basically contains book titles and their abstracts. so far i only managed to remove stopwords and numbers, now my goal is to select words that contain at least three letters and up and do a lemmatization of the words.
this is the code i have written:
from sklearn.feature_extraction.text import tfidfvectorizer
tf_idf = tfidfvectorizer(stop_words='english', token_pattern=r'(?u)\b[a-za-z]+\b')
tfidf_matrix = tf_idf.fit_transform(doc)
print(tfidf_matrix)

if i print ""tf_idf.vocabulary_"" i get all words that occur in the document as well as letters such as r,s,t,m etc. as far as lemmatization is concerned, i don't know how to go about it and i still don't understand how it works, if someone can give me a hand i thank you in advance.","['python', 'regex', 'text-mining', 'tf-idf', 'stop-words']",71194575,"token_patternstr, default=rï¿½ï¿½ï¿½(?u)\b\w\w+\bï¿½ï¿½ï¿½
regular expression denoting what constitutes a ï¿½ï¿½ï¿½tokenï¿½ï¿½ï¿½, only used if analyzer == 'word'. the default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).

to select words that contain at least three letters change you""lang-py prettyprint-override"">tf_idf = tfidfvectorizer(stop_words='english', token_pattern=r'(?u)\b[a-za-z]+\b')

to regex quantifer {3,}, which match its preceding element at least n times.
tf_idf = tfidfvectorizer(stop_words='english', analyzer='word', token_pattern=r'(?u)\b[a-za-z]{3,}\b')

# doc used as sample text.
doc = """"""hi lucia. how are you? it was so nice to meet you last week in sydney at the sales meeting. how was the rest of your trip? did you see any kangaroos? i hope you got home to mexico city ok.
anyway, i have the documents about the new berlin offices. we're going to be open in three months. i moved here from london just last week. they are very nice offices, and the location is perfect.
there are lots of restaurants, cafï¿½ï¿½s and banks in the area. there's also public transport; we are next to an u-bahn (that is the name for the metro here). maybe you can come and see them one day? i would love to show you berlin, especially in the winter. you said you have never seen snow ï¿½ï¿½ï¿½ you will see lots here! here's a photo of you and me at the restaurant in sydney. that was a very fun night! remember the singing englishman? crazy! please send me any other photos you have of that night. good memories.
please give me your email address and i will send you ths. bye for now. mikel""""""

print(tf_idf.vocabulary_)
{
   ""lucia"": 27,
   ""nice"": 38,
   ""meet"": 29,
   ""week"": 59,
   ""sydney"": 56,
   ""sales"": 51,
   ""meeting"": 30,
   ""rest"": 47,
   ""trip"": 58,
   ""did"": 10,
   ""kangaroos"": 22,
   ""hope"": 20,
   ""got"": 18,
   ...
   ...",https://stackoverflow.com/questions/71193790,python,20-02-2022 11:09,772.0,0.0,1.0,True,20-02-2022 14:01,20-02-2022 14:01
59343997,how and where to set the environment variable spacy_warning_ignore=w008,"i have the below code which is trying to find similar words between the two lists. for this purpose i am using spacy's .similarity function.
import en_vectors_web_lg
nlp = en_vectors_web_lg.load()
listx =['hsbc', 'jp morgan',......] #500 words lists
listy = ['currency','blockchain'.......] #1000 words lists
s_words = []
for token1 in listy:
    list_to_sort = [] 
    for token2 in listx:    
        list_to_sort.append((token1, token2,nlp(str(token1)).similarity(nlp(str(token2)))))
        sorted_list = sorted(list_to_sort, key = itemgetter(2), reverse=true)[0][:2]
        s_words.append(sorted_list)

on running the above code i keep getting the warning: userwarning: [w008] evaluating doc.similarity based on empty vectors""
i came across solutions like setting an environment variable ""spacy_warning_ignore=w008"" to suppress the error ""userwarning: [w008] evaluating doc.similarity based on empty vectors""
my question is where and how to set up this environment variable? 
i am using windows 10, spyder ide, python 3.7 and spacy 2.2.5.","['python-3.x', 'nlp', 'anaconda', 'spyder', 'spacy']",59345941,"you need to add the following lines to your code to set that environment variable:
import os
os.environ['spacy_warning_ignore'] = 'w008'",https://stackoverflow.com/questions/59343997,python-3.x,15-12-2019 12:42,1064.0,1.0,3.0,True,02-03-2021 10:55,15-12-2019 12:52
60422206,machine learning entity candidate scoring,"i am trying to understand the machine learning part behind google's smart linkify. the article states the following regarding their generate candidate entities model.

a given input text is first split into words (based on space
  separation), then all possible word subsequences of certain maximum
  length (15 words in our case) are generated, and for each candidate
  the scoring neural net assigns a value (between 0 and 1) based on
  whether it represents a valid entity:



next, the generated entities that overlap are removed, favoring the
  ones with the higher score over the conflicting ones with a lower
  score.

if i understand correctly the model tries every word in the sentence and a combination of that word up to 15 words total?
how can you train such model? i assume it's supervised learning but don't understand how such data could be labeled. is it similar to ner where the entity is specified by character position? and there are only 2 entities in the data entity and non-entity.
and for the output of the model, the so called ""candidate score"", how can a a neural network return a single numerical value? (the score). or is the output layer just a single node?
a more detailed explanation on:

possible word subsequences of certain maximum length means it considers every word with the 7 words before and 7 after the word?
how can the neural net generate a score when its a binary classification entity and non-entity? or do they mean the probability score for entity?
how to train a binary ner? like any other ner except replace all entities to type 'entity' and then generate negative samples for non-entity?
how can this model be fast, as they claim, when it processes every word in the text plus 7 words before and after said word?

is what i'm looking for, to understand.","['python', 'tensorflow', 'machine-learning', 'neural-network', 'nlp']",60476780,"possible word subsequences of certain maximum length means it considers every word with the 7 words before and 7 after the word?
as i understand it from the documentation, your description is not quite right. since every possible sequence up to 15 words in length is evaluated, this would include a word with 7 words before and after it, but also that word with 5 words before and 3 after it, etc. (i.e. every possible n-gram between len(1) and len(15). initial probabilities are derived, overlapping strings are compared and any overlaps with lower probabilities are discarded so that the final candidates are non-overlapping.  
how can the neural net generate a score when its a binary classification entity and non-entity? or do they mean the probability score for entity?
according to the google ai blog, ""for each candidate the scoring neural net assigns a value (between 0 and 1) based on whether it represents a valid entity."" so that would be a probability. 
how to train a binary ner? like any other ner except replace all entities to type 'entity' and then generate negative samples for non-entity?
yes but, because this is a perceptron model, many binary classifiers will be trained and each will function as neuron in the model. it is important to note that the classifier only classifies entity/non-entity, not what type of entity it is. the post also discusses automatically generating negative samples by taking a positive sample (marked by a start token and end token in a string) and deliberately including the token before or after that entity. this technique would greatly increase the size of the training data.
how can this model be fast, as they claim, when it processes every word in the text plus 7 words before and after said word?
the computational cost of taking relatively small string (len 15) and fitting it to a model is small. the computational cost of dividing a longer string into substrings of this length is also quite small. even if the text is 5000 words long (which would be huge for a query of this sort), that's only about 600,000 n-grams to evaluate, and most of those will have very low entity scores. as i understand it, the most significant computational cost of these approaches is training the model. this is where the ""hashed charactergram embedding"" technique discussed in the post is utilized.",https://stackoverflow.com/questions/60422206,python,26-02-2020 20:41,273.0,5.0,1.0,True,29-05-2023 12:28,29-05-2023 12:28
58777613,getting a specific element from a list of tuples,"in using spacy, i have the following:
import spacy

nlp = spacy.load('en_core_web_lg')

sentence = ""a quick john jumps over the lazy dog""

tag_entities = [(x, x.ent_iob_, x.ent_type_) for x in nlp(sentence)]
inputlist = tag_entities

print (inputlist)

[(a, 'o', ''), (quick, 'o', ''), (john, 'b', 'person'), (jumps, 'o', ''), (over, 'o', ''), (the, 'o', ''), (lazy, 'o', ''), (dog, 'o', '')]

it is a list of tuples. i want to extract the person element. this is what i do:
for i in inputlist:
  if (i)[2] == ""person"":
    print ((i)[0])

john

what would be a better way?","['python', 'tuples', 'spacy']",58777814,"to keep all first element if second element is person from first list use a list comprehension notation with a if at the end
filtered_taglist = [x for x,_,type in tag_entities if type == ""person""]

this corresponds to
filtered_taglist = []
for x,_,type in inputlist:
    if type == ""person"":
        filtered_taglist.append(x)",https://stackoverflow.com/questions/58777613,python,09-11-2019 08:29,86.0,0.0,2.0,True,08-06-2022 22:01,08-06-2022 22:01
2645706,is there any lib for python that will get me the synonyms of a word?,"is there any api/lib for python that will get me the synonyms of a word?
for example if i have the word ""house"" it will return ""building, domicile, mansion, etc...""","python, nlp, synonym",2645764,"nltk and wordnet can help: e.g., per this article,
from nltk.corpus import wordnet

dog = wordnet.synset('dog.n.01')
print(dog.lemma_names())

prints:
['dog', 'domestic_dog', 'canis_familiaris']",https://stackoverflow.com/q/2645706,"python, nlp, synonym",15-04-2010 13:37,16470.0,13.0,2.0,True,11-02-2022 21:26,15-04-2010 13:53
79501178,store images instead of showing in a server,"i am running the code found on this site in my server and i would like to store images instead of showing them since i have connected remotely with an ssh connection to my server via an ssh connection.
the code is for instance this one:
skip_tokens = [1]  # skip the special token for the start of the text <s>
inp = texttokeninput(
  eval_prompt, 
  tokenizer,
  skip_tokens=skip_tokens,
)

target = ""playing guitar, hiking, and spending time with his family.""
attr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)
attr_res.plot_token_attr(show=true)

how to store the files locally instead of showing them?","['python', 'nlp', 'large-language-model']",79501337,"i can't test it but ...
i checked source code and it uses matplotlib for this.
if you remove show=true then it shouldn't show it but it should only get fig, ax.
i think you could use matplotlib.pyplot.savefig(filename) to save it in file.
import matplotlib.pyplot as plt

# ... code  ...

attr_res.plot_token_attr()  # without `show=true
plt.savefig(""output.png"")
#plt.show()  # eventually show it after saving


probably you can also use fig for this
fig, ax = attr_res.plot_token_attr()  # without `show=true
fig.savefig(""output.png"")",https://stackoverflow.com/questions/79501178,python,11-03-2025 14:50,40.0,0.0,1.0,True,04-04-2025 23:40,04-04-2025 23:40
73506939,attempt to convert a value (&lt;pil.bmpimageplugin.bmpimagefile image mode=l size=190x100 at 0x7f35c52ad210&gt;) with an unsupported type to a tensor,"i tried to execute the vit model from image classification with hugging face transformers and keras, i got an error, particularly in this instruction:
processed_dataset = ds.map(augmentation, batched=true)

the error :

valueerror: exception encountered when calling layer ""resizing_8""
(type resizing).
attempt to convert a value (<pil.bmpimageplugin.bmpimagefile image
mode=l size=190x100 at 0x7f35c52ad210>) with an unsupported type
(<class 'pil.bmpimageplugin.bmpimagefile'>) to a tensor.
call arguments received:   ï¿½ï¿½ï¿½ inputs=<pil.bmpimageplugin.bmpimagefile
image mode=l size=190x100 at 0x7f35c52ad210>

i tried the answer in this link arrowtypeerror: could not convert <pil.pngimageplugin.pngimagefile image mode=rgb size=32x32 at 0x7f2223b6ed10>, where i added 'img': image(decode=true, id=none) to my features in create_image_folder_dataset() and i still have the same problem except for a small change in this part

valueerror: exception encountered when calling layer ""resizing_13""
(type resizing).

what i should do to solve this problem?
create_image_folder_dataset function:
def create_image_folder_dataset(root_path):
  """"""creates `dataset` from image folder structure""""""
  
  # get class names by folders names
  _class_names= os.listdir(root_path)
  # defines `datasets` features`
  features=datasets.features({
                      ""img"": datasets.image(decode=true, id=none),
                      #""img"": datasets.image(),
                      ""label"": datasets.features.classlabel(names=_class_names),
                      
                  })
  #print(_class_names)
  # temp list holding datapoints for creation
  img_data_files=[]
  label_data_files=[]
  # load images into list for creation
  for img_class in os.listdir(root_path):
    for img in os.listdir(os.path.join(root_path,img_class)):
      path_=os.path.join(root_path,img_class,img)
      img_data_files.append(path_)
      label_data_files.append(img_class)
  # create dataset
  ds = datasets.dataset.from_dict({""img"":img_data_files,""label"":label_data_files},features=features)
  return ds
ds = create_image_folder_dataset(""/content/drive/mydrive/final_dataset"")
ds[0] """""" return: 
{'img': <pil.bmpimageplugin.bmpimagefile image mode=l size=190x100 at 0x7f35c54ecc10>,
 'label': 0}""""""

my augmentation function :
from transformers import vitfeatureextractor
from tensorflow import keras 
from tensorflow.keras import layers


model_id = ""google/vit-base-patch16-224-in21k""
#google/vit-base-patch32-384
feature_extractor = vitfeatureextractor.from_pretrained(model_id)

# learn more about data augmentation here: 
data_augmentation = keras.sequential(
    [
        layers.resizing(feature_extractor.size, feature_extractor.size),
        layers.rescaling(1./255),
        layers.randomflip(""horizontal""),
        layers.randomrotation(factor=0.02),
        layers.randomzoom(
            height_factor=0.2, width_factor=0.2
        ),
    ],
    name=""data_augmentation"",
)
# use keras image data augementation processing
def augmentation(examples):
    print(examples[""img""])
    examples[""pixel_values""] = [data_augmentation(image) for image in examples[""img""]]
    return examples


# basic processing (only resizing)
def process(examples):
    examples.update(feature_extractor(examples['img'], ))
    return examples


# we are also renaming our label col to labels to use `.to_tf_dataset` later
#ds = ds.rename_column(""label"", ""labels"")","['python', 'keras', 'huggingface-transformers', 'feature-extraction', 'image-classification']",73540692,"now it's working, i convert my dataset from ""l"" to ""rgb"".",https://stackoverflow.com/questions/73506939,python,26-08-2022 22:16,994.0,1.0,1.0,True,30-08-2022 10:09,26-08-2022 22:35
75211491,increasing efficiency for zero shot classification,"i am using zero shot classification to label large amounts of data. i have written a simple function to assist me with this and am wondering if there is a better way for this to run. my current logic was to take the highest score and label and append this label into a dataframe.
def labeler(input_df,output_df):
    labels = ['fruit','vegetable','meat','other']


    for i in tqdm(range(len(input_df))):
        temp = classifier(input_df['description'][i],labels)
        output ={'work_order_num':input_df['order_num'][i],
                 'work_order_desc':input_df['description'][i],
                'label':temp['labels'][0],
                'score':temp['scores'][0]}
        output_df.append(output)

in terms of speed and resources would it be better to shape this function with lambda?","['python', 'pandas', 'performance', 'loops', 'nlp']",75250052,"your problem boils down to iteration over the pandas dataframe input_df. doing that with a for loop is not the most efficient way (see: how to iterate over rows in a dataframe in pandas).
i suggest doing something like this:
output_df['work_order_num', 'work_order_desc'] = input_df['order_num', 'description']  # these columns can be copied as whole.

def classification(df_desc):
    temp = classifier(df_desc, labels)
    return temp['labels'][0], temp['scores'][0]
    
output_df['label'], output_df['score'] = zip(*input_df.apply(classification))

classification function returns tuples of values that need to be unpacked so i used the zip trick from this question.
also, building a dataframe by concatenation is a very slow process too. so with the solution above you omit two potentially prohibitively slow operations: slow for-loop and appending rows to a dataframe.",https://stackoverflow.com/questions/75211491,python,23-01-2023 15:18,247.0,0.0,1.0,True,26-01-2023 18:05,26-01-2023 07:31
78568527,how to send large videos to gemini ai api 1.5 pro for inference?,"i'm currently working with the gemini ai api 1.5 pro (latest version) and need to send large video files for inference. these videos are several hundred megabytes each (~700mb) but are within the api's constraints (e.g., less than 1 hour in length). i want to upload them once and perform inference without re-uploading.
in gpt-4o, there was an option to use image_urls to reference images. is there a similar method or best practice for handling large video files with the gemini ai api 1.5 pro?
the videos are too large to send repeatedly, so an efficient method for uploading and referencing them is crucial.
any guidance on api endpoints, required parameters, or example code snippets would be greatly appreciated.","['python', 'server', 'openai-api', 'google-gemini']",78568879,"in your situation, how about the following sample script?
sample script 1:
before you test the following script, please update google-generativeai to the latest version.
import google.generativeai as genai
import time

apikey = ""###"" # please set your api key.
video_file_name = ""sample.mp4"" # please set your video file with the path.
display_name = ""sampledisplayname"" # please set the display name of the uploaded file on gemini. the file is searched from the file list using this value.

genai.configure(api_key=apikey)

# get file list in gemini
filelist = genai.list_files(page_size=100)

# check uploaded file.
video_file = next((f for f in filelist if f.display_name == display_name), none)
if video_file is none:
    print(f""uploading file..."")
    video_file = genai.upload_file(path=video_file_name, display_name=display_name, resumable=true)
    print(f""completed upload: {video_file.uri}"")
else:
    print(f""file uri: {video_file.uri}"")

# check the state of the uploaded file.
while video_file.state.name == ""processing"":
    print(""."", end="""")
    time.sleep(10)
    video_file = genai.get_file(video_file.name)

if video_file.state.name == ""failed"":
    raise valueerror(video_file.state.name)

# generate content using the uploaded file.
prompt = ""describe this video.""
model = genai.generativemodel(model_name=""models/gemini-1.5-pro-latest"")
print(""making llm inference request..."")
response = model.generate_content([video_file, prompt], request_options={""timeout"": 600})
print(response.text)

in this sample script, when the file has already been uploaded, the existing file is used. on the other hand, when the file is not found, the file is uploaded and the uploaded file is used. in order to search the file, in this sample, display_name is used.
sample script 2:
as another approach, when the value of name can be directly given, the following sample script can be also used. in this case, the value of name is required to be the unique value in the uploaded files.
import google.generativeai as genai
import time

apikey = ""###"" # please set your api key.
video_file_name = ""sample.mp4"" # please set your video file with the path.
name = ""sample-name-1"" # please set the name of the uploaded file on gemini. the file is searched from the file list using this value.

genai.configure(api_key=apikey)

# check uploaded file.
try:
    video_file = genai.get_file(f""files/{name}"")
    print(f""file uri: {video_file.uri}"")
except:
    print(f""uploading file..."")
    video_file = genai.upload_file(path=video_file_name, name=name, resumable=true)
    print(f""completed upload: {video_file.uri}"")

# check the state of the uploaded file.
while video_file.state.name == ""processing"":
    print(""."", end="""")
    time.sleep(10)
    video_file = genai.get_file(video_file.name)

if video_file.state.name == ""failed"":
    raise valueerror(video_file.state.name)

# generate content using the uploaded file.
prompt = ""describe this video.""
model = genai.generativemodel(model_name=""models/gemini-1.5-pro-latest"")
print(""making llm inference request..."")
response = model.generate_content([video_file, prompt], request_options={""timeout"": 600})
print(response.text)

this script is the same result with the above script.
note:

this is a simple sample script. so, please modify this to your actual situation.

reference:

prompting with media files",https://stackoverflow.com/questions/78568527,python,03-06-2024 05:27,2100.0,1.0,1.0,True,20-07-2024 05:45,03-06-2024 12:31
76997109,using langchain for text to sql using custom llm api,"i am trying to use my llama2 model (exposed as an api using ollama). i want to chat with the llama agent and query my postgres db (i.e. generate text to sql). i was able to find langchain code that uses open ai to do this. however, i am unable to find anything out there which fits my situation.
any pointers will be of great help.
code with openai
# create connection to postgres
import psycopg2  # import the library

database = 'postgres'
username = 'postgres'
password = 'password'
server = 'localhost'
port = '5432'

# establish the connection
conn = psycopg2.connect(
    dbname=database,
    user=username,
    password=password,
    host=server,
    port=port
)

db = sqldatabase.from_uri(
    ""postgresql://postgres:password@localhost:5432/postgres"")
toolkit = sqldatabasetoolkit(db=db, llm=openai(temperature=0))

agent_executor = create_sql_agent(
    llm=openai(temperature=0),
    toolkit=toolkit,
    verbose=true,
    agent_type=agenttype.zero_shot_react_description,
)

agent_executor.run(""describe the transaction table"")

i want to make the above code work for my llama2 model exposed via an api at localhost:11434/api/generate","['openai-api', 'langchain', 'large-language-model', 'llama']",76999065,"load your llm like mentioned here 
and then use that inplace of openai. you'll most probably have to change the prompts to fit llama2 desired format",https://stackoverflow.com/questions/76997109,openai-api,29-08-2023 03:19,4789.0,2.0,1.0,True,31-08-2023 21:41,31-08-2023 21:41
71338307,query() of generator `max_length` being succeeded,"goal: set min_length and max_length in hugging face transformers generator query.
i've passed 50, 200 as these parameters. yet, the length of my outputs are much higher...
there's no runtime failure.
from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2')
set_seed(42)

def query(payload, multiple, min_char_len, max_char_len):
    print(min_char_len, max_char_len)
    list_dict = generator(payload, min_length=min_char_len, max_length=max_char_len, num_return_sequences=multiple)
    test = [d['generated_text'].split(payload)[1].strip() for d in list_dict]
    for t in test: print(len(t))
    return test

query('example', 1, 50, 200)

output:
50 200
setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
1015","['python-3.x', 'huggingface-transformers']",71349357,"explanation:
as explained by narsil on hugging face ï¿½ï¿½ï¿½ï¿½ transformers "" rel=""nofollow noreferrer"">git issue response

models, don't ingest the text one character at a time, but one token
at a time. there are different algorithms to achieve this but
basically ""my name is nicolas"" gets transformers into [""my"", "" name"",
"" is"", "" nic"", ""olas""] for instance, and each of those tokens have a
number.
so when you are generating tokens, they can contain themselves 1 or
more characters (usually several and almost any common word for
instance). that's why you are seeing 1015 instead of your expected 200
(the tokens here have an average of 5 chars)

solution:
as i resolved...

rename min_char_len, max_char_len to min_tokens, max_tokens and
simply reduce their values by a ~1/4 or 1/5.",https://stackoverflow.com/questions/71338307,python-3.x,03-03-2022 13:50,137.0,-1.0,1.0,True,04-03-2022 10:30,03-03-2022 14:12
76551067,how to create a langchain doc from an str?,"i've searched all over langchain documentation on their official website but i didn't find how to create a langchain doc from a str variable in python so i searched in their github code and i found this :
  doc=document(
                page_content=""text"",
                metadata={""source"": ""local""}
            )


ps: i added the metadata attribute
then i tried using that doc with my chain:
memory and chain:
memory = conversationbuffermemory(memory_key=""chat_history"", input_key=""human_input"")
chain = load_qa_chain(
    llm, chain_type=""stuff"", memory=memory, prompt=prompt
)


the call method:
  chain({""input_documents"": doc, ""human_input"": query})

prompt template:
template = """"""you are a senior financial analyst analyzing the below document and having a conversation with a human.
{context}
{chat_history}
human: {human_input}
senior financial analyst:""""""

prompt = prompttemplate(
    input_variables=[""chat_history"", ""human_input"", ""context""], template=template
)

but i am  getting the following error:
attributeerror: 'tuple' object has no attribute 'page_content'


when i tried to check the type and the page content of the document object before using it with the chain i got this
print(type(doc))
<class 'langchain.schema.document'>
print(doc.page_content)
""text""","['python', 'nlp', 'langchain', 'large-language-model']",77737781,"i had a similar issue and i noticed the api calls for a list, so try
    doc = document(page_content=input,
            metatdata={
                ""source"": ""userinput""
            }
        )
    #db.add_documents(doc)
    db.add_documents([doc])",https://stackoverflow.com/questions/76551067,python,25-06-2023 15:09,50817.0,28.0,5.0,True,12-03-2024 05:20,13-02-2024 02:58
78596555,openai assistants api v2: should i attach files to the thread or to the assistant?,"i am using the openai assistants api, and i have the option to add my pdf files as a knowledge base.
should i attach files to the thread or to the assistant?
i am super confused here.
or should i add them both times?
what would work best?","['openai-api', 'gpt-4', 'openai-assistants-api']",78596993,"when using the openai assistants api v2, you attach files when you add a user message to the thread by using the attachments parameter.
in the code examples below, i passed a file with the file_search tool.
python:
my_thread_message = client.beta.threads.messages.create(
    thread_id=my_thread.id,
    role=""user"",
    content=user_input,
    attachments=[  # ï¿½ï¿½ï¿½ï¿½ add files by using the attachments parameter
        {""file_id"": file_id, ""tools"": [{""type"": ""file_search""}]}
    ],
)

node.js:
const mythreadmessage = await openai.beta.threads.messages.create(
  (thread_id = mythread.id),
  {
    role: ""user"",
    content: userinput,
    attachments: [ // ï¿½ï¿½ï¿½ï¿½ add files by using the attachments parameter
      {
        file_id: fileid,
        tools: [{ type: ""file_search"" }],
      },
    ],
 re>",https://stackoverflow.com/questions/78596555,openai-api,08-06-2024 18:11,1855.0,1.0,2.0,True,12-06-2024 12:26,12-06-2024 12:26
75462898,using an nlp vectorized output for subsequent model?,"i use spacy to output a vectorized array of my text field. i'm having issues plugging this output into my random forest and could use some guidance. i label encoded other fields so my pandas dataframe looks something like:
import pandas as pd
import numpy as np
from sklearn.ensemble import randomforestclassifier
from sklearn.model_selection import train_test_split

d = {'le1': [0,1,2,1], 'le2': [3,0,2,1], 'spacy_output':[[0.12,0.14,3.5],[1.21,0.84,1.92],[0.34,0.85,2.43],[0.09,0.18,2.21]], 'response':[0,1,1,0]}

df = pd.dataframe(d)

then i try to plug this into my model:
x = np.array(df.drop('response', axis=1))
y = df['response'].values.ravel()
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 23)

clf = randomforestclassifier(min_samples_split=4, n_estimators=100, criterion='entropy')
clf.fit(x_train,y_train)

confused on how to pass this dataframe to my model. i get the following errors:
typeerror: only size-1 arrays can be converted to python scalars
valueerror: setting an array element with a sequence.","['python', 'machine-learning', 'nlp']",75510226,"spacy_output column in your dataframe is a list of lists, so when you convert the dataframe to a numpy array using np.array, you end up with a 2d array where each element is a list. this causes problems when you try to pass this array to the randomforestclassifier because the classifier expects a 2d array of numerical values.
d = {'le1': [0,1,2,1], 'le2': [3,0,2,1], 'spacy_output':[[0.12,0.14,3.5],[1.21,0.84,1.92],[0.34,0.85,2.43],[0.09,0.18,2.21]], 'response':[0,1,1,0]}

df = pd.dataframe(d)

x = np.concatenate([np.array(df[['le1', 'le2']]), np.array(df['spacy_output'].tolist())], axis=1)
y = df['response'].values.ravel()",https://stackoverflow.com/questions/75462898,python,15-02-2023 16:42,64.0,0.0,1.0,True,20-02-2023 14:03,16-02-2023 06:56
77061667,resume from checkpoint gives device error in huggingface transformers trainer,"i have pretrained a bert model using huggingface transformers library and currently it's working as expected but i stopped the training midway. now i have a problem resuming the training.
for the training i used:
from transformers import berttokenizerfast
tokenizer = berttokenizerfast(tokenizer_file=""tokenizer.json"",max_len=512)

from transformers import bertconfig
from transformers import bertformaskedlm    
config = bertconfig(vocab_size=50_000)
model = bertformaskedlm(config=config)
model.num_parameters()

from transformers import datacollatorforwholewordmask

data_collator_wwm = datacollatorforwholewordmask(
    tokenizer=tokenizer, mlm=true, mlm_probability=0.15
)


from transformers import trainer, trainingarguments

training_args = trainingarguments(
    output_dir=""./mybert"",
    overwrite_output_dir=false,
    num_train_epochs=6,
    per_gpu_train_batch_size=32,

    logging_steps = 0.001,
    save_strategy = 'steps',
    save_steps= 0.05,
    save_total_limit=20,
    evaluation_strategy='steps',
    eval_steps = 0.05,

    tf32 = true,
    optim = ""adamw_torch_fused"",
    group_by_length = true,

    prediction_loss_only=true,
    #resume_from_checkpoint=true,

    hub_model_id = 'mybert',
    push_to_hub = true,
    hub_strategy = 'every_save',
    hub_private_repo = true,
    
)

trainer = trainer(
    model=model,
    args=training_args,
    data_collator=data_collator_wwm,
    train_dataset=dataset['train'],
    eval_dataset=dataset['test']
    
)

trainer.train()
# trainer.train(resume_from_checkpoint=true)

now after training for the resume i changed the commented line to enable resume_from_checkpoint. but i get device error.
---------------------------------------------------------------------------
runtimeerror                              traceback (most recent call last)
cell in[11], line 45
     35 trainer = trainer(
     36     model=model,
     37     args=training_args,
   (...)
     41     
     42 )
     44 # trainer.train()
---> 45 trainer.train(resume_from_checkpoint=true)

file c:\users\user01\documents\mybert\myvenv\lib\site-packages\transformers\trainer.py:1539, in trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1534     self.model_wrapped = self.model
   1536 inner_training_loop = find_executable_batch_size(
   1537     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1538 )
-> 1539 return inner_training_loop(
   1540     args=args,
   1541     resume_from_checkpoint=resume_from_checkpoint,
   1542     trial=trial,
   1543     ignore_keys_for_eval=ignore_keys_for_eval,
   1544 )

file c:\users\user01\documents\mybert\myvenv\lib\site-packages\transformers\trainer.py:1888, in trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1886     optimizer_was_run = scale_before <= scale_after
   1887 else:
-> 1888     self.optimizer.step()
   1889     optimizer_was_run = not self.accelerator.optimizer_step_was_skipped
   1891 if optimizer_was_run:
   1892     # delay optimizer scheduling until metrics are generated

file c:\users\user01\documents\mybert\myvenv\lib\site-packages\accelerate\optimizer.py:142, in acceleratedoptimizer.step(self, closure)
    139     self._last_scale = scale_after
    141 else:
--> 142     self.optimizer.step(closure)

file c:\users\user01\documents\mybert\myvenv\lib\site-packages\torch\optim\lr_scheduler.py:69, in lrscheduler.__init__..with_counter..wrapper(*args, **kwargs)
     67 instance._step_count += 1
     68 wrapped = func.__get__(instance, cls)
---> 69 return wrapped(*args, **kwargs)

file c:\users\user01\documents\mybert\myvenv\lib\site-packages\torch\optim\optimizer.py:280, in optimizer.profile_hook_step..wrapper(*args, **kwargs)
    276         else:
    277             raise runtimeerror(f""{func} must return none or a tuple of (new_args, new_kwargs),""
    278                                f""but got {result}."")
--> 280 out = func(*args, **kwargs)
    281 self._optimizer_step_code()
    283 # call optimizer step post hooks

file c:\users\user01\documents\mybert\myvenv\lib\site-packages\torch\optim\optimizer.py:33, in _use_grad_for_differentiable.._use_grad(self, *args, **kwargs)
     31 try:
     32     torch.set_grad_enabled(self.defaults['differentiable'])
---> 33     ret = func(self, *args, **kwargs)
     34 finally:
     35     torch.set_grad_enabled(prev_grad)

file c:\users\user01\documents\mybert\myvenv\lib\site-packages\torch\optim\adamw.py:171, in adamw.step(self, closure)
    158     beta1, beta2 = group[""betas""]
    160     self._init_group(
    161         group,
    162         params_with_grad,
   (...)
    168         state_steps,
    169     )
--> 171     adamw(
    172         params_with_grad,
    173         grads,
    174         exp_avgs,
    175         exp_avg_sqs,
    176         max_exp_avg_sqs,
    177         state_steps,
    178         amsgrad=amsgrad,
    179         beta1=beta1,
    180         beta2=beta2,
    181         lr=group[""lr""],
    182         weight_decay=group[""weight_decay""],
    183         eps=group[""eps""],
    184         maximize=group[""maximize""],
    185         foreach=group[""foreach""],
    186         capturable=group[""capturable""],
    187         differentiable=group[""differentiable""],
    188         fused=group[""fused""],
    189         grad_scale=getattr(self, ""grad_scale"", none),
    190         found_inf=getattr(self, ""found_inf"", none),
    191     )
    193 return loss

file c:\users\user01\documents\mybert\myvenv\lib\site-packages\torch\optim\adamw.py:321, in adamw(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)
    318 else:
    319     func = _single_tensor_adamw
--> 321 func(
    322     params,
    323     grads,
    324     exp_avgs,
    325     exp_avg_sqs,
    326     max_exp_avg_sqs,
    327     state_steps,
    328     amsgrad=amsgrad,
    329     beta1=beta1,
    330     beta2=beta2,
    331     lr=lr,
    332     weight_decay=weight_decay,
    333     eps=eps,
    334     maximize=maximize,
    335     capturable=capturable,
    336     differentiable=differentiable,
    337     grad_scale=grad_scale,
    338     found_inf=found_inf,
    339 )

file c:\users\user01\documents\mybert\myvenv\lib\site-packages\torch\optim\adamw.py:615, in _fused_adamw(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)
    613     device_found_inf = found_inf_dict[device]
    614 torch._foreach_add_(device_state_steps, 1)
--> 615 torch._fused_adamw_(
    616     device_params,
    617     device_grads,
    618     device_exp_avgs,
    619     device_exp_avg_sqs,
    620     device_max_exp_avg_sqs,
    621     device_state_steps,
    622     amsgrad=amsgrad,
    623     lr=lr,
    624     beta1=beta1,
    625     beta2=beta2,
    626     weight_decay=weight_decay,
    627     eps=eps,
    628     maximize=maximize,
    629     grad_scale=device_grad_scale,
    630     found_inf=device_found_inf,
    631 )
    632 if device_found_inf is not none:
    633     torch._foreach_sub_(device_state_steps, [device_found_inf] * len(device_state_steps))

runtimeerror: expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument state_steps in method wrapper_cuda___fused_adamw_)

things i tried:

loading model from checkpoint (though it should load last checkpoint automatically):
model = bertformaskedlm.from_pretrained('mybert\checkpoint-5410440')

sending model to cuda device using model = model.to(""cuda"").

removing resume_from_checkpoint from args


edit:
4. tried upgrading transformer from 4.31.0 to 4.33.1 (not recommended because training was done in 4.31.0) , but got a somewhat similar error:
`fused=true` requires all the params to be cuda, floating point tensor","['pytorch', 'huggingface-transformers']",77064023,"as pointed in huggingface github this is a known issue of loading optimizer.
changing the line 2542 in trainer.py from
map_location = self.args.device if self.args.world_size > 1 else ""cpu""

to
map_location = self.args.device

will fix the issue.
(huggingface transformer v4.30.1, pytorch v2.0.1)",https://stackoverflow.com/questions/77061667,pytorch,07-09-2023 17:22,1707.0,1.0,1.0,True,09-09-2023 01:03,09-09-2023 01:03
72981649,overfitting on lstm text classification using keras,"i am trying to develop an lstm model using keras, following this tutorial. however, i am implementing it with a different dataset of u.s. political news articles with the aim of classifying them based on a political bias (labels: left, centre and right). i have gotten a model to run with the tutorial, but the loss and accuracy would look very off, like this:

i tried to play around with different dropout probabilities (i.e. 0.5 instead of 0.2), adding/removing hidden layers (and making them less dense), and decreasing/increasing the max number of words and max sequence length.
i have managed to get the graphs to align a bit more, however, that has led to the model having less accuracy with the training data (and the problem of overfitting is still bad):

additionally, i am not sure why the validation accuracy always seems to be higher than the model accuracy in the first epoch (shouldn't it usually be lower)?
here is some code that is being used when tokenizing, padding, and initializing variables:
# the maximum number of words to be used. (most frequent)
max_nb_words = 500

# max number of words in each news article
max_sequence_length = 100 # i am aware this may be too small

# this is fixed.
embedding_dim = 64

tokenizer = tokenizer(num_words=max_nb_words, filters='!""#$%&()*+,-./:;<=>?@[\]^_`{|}~', 
lower=true)
tokenizer.fit_on_texts(df_raw['titletext'].values)
word_index = tokenizer.word_index
print('found %s unique tokens.' % len(word_index))


x = tokenizer.texts_to_sequences(df_raw['titletext'].values)
x = pad_sequences(x, maxlen=max_sequence_length)
print('shape of data tensor:', x.shape)

y = pd.get_dummies(df_raw['label']).values
print('shape of label tensor:', y.shape)

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.20)
print(x_train.shape,y_train.shape)
print(x_test.shape,y_test.shape)

x_train.view()

when i look at what is shown when x_train.view() is executed, i am also not sure why all the arrays start with zeros like this:

i also did a third attempt that was just a second attempt with the number of epochs increased, it looks like this:

here is the code of the actual model:
model = sequential()
model.add(embedding(max_nb_words, embedding_dim, input_length=x.shape[1]))
# model.add(spatialdropout1d(0.2)) ---> commented out
# model.add(lstm(100, dropout=0.2, recurrent_dropout=0.2)) ---> commented out
model.add(lstm(64, dropout=0.2, recurrent_dropout=0.2))
model.add(dropout(0.5))
model.add(dense(8))
model.add(dropout(0.5))
model.add(dense(3, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

epochs = 25
batch_size = 64

history = model.fit(x_train, y_train, epochs=epochs, 
batch_size=batch_size,validation_split=0.2,callbacks=[earlystopping(monitor='val_loss', patience=3, min_delta=0.0001)])

here is the link to the full code, including the dataset
any help would be greatly appreciated!","['python', 'machine-learning', 'keras', 'nlp', 'lstm']",72982833,"hyperparameter adjustments for reducing overfitting in neural networks

identify and ascertain overfitting. the first attempt shows largely overfitting, with early divergence of your test & train loss. i would try a lower learning rate here (in addition to the steps you took for regularisation with dropout layers). using the default rate does not guarantee best results.

allowing your model to find the global mimima / not being stuck in a local minima. on the second attempt, it looks better. however, if the x-axis shows the number of epochs -- it could be that your early stopping is too strict? ie. increase the threshold. consider other optimisers, including sgd with a learning rate scheduler.

too large network leads to overfitting on the trainset and difficulty in generalisation. too many neurons may cause the network to 'memorize' all you trainset and overfit. i would try out 8, 16 or 24 neurons in your lstm layer for example.

data preprocessing & cleaning. check your padding_sequences. it is probably padding the start of each text with zeros. i would pad post text.

dataset. depending on the size of your current dataset, i would suggest data augmentation to get to a sizable amount of text of training (empirically >=1m words). i would also try several techniques including feature engineering / improving data quality such as, spell checks. are the classes imbalanced? you may need to balance them out by over/undersampling.

consider using transfer learning and incorporate trained language models as your embeddings layer instead of training one from scratch. ie.",https://stackoverflow.com/questions/72981649,python,14-07-2022 13:51,871.0,0.0,1.0,True,15-07-2022 04:11,15-07-2022 00:45
77654210,openai api invalid_request_error: you must provide a model parameter,"maybe someone can help me, i got this api request, and its working with
""gpt-3.5-turbo"" as
the ""model"" with this api endpoint "" but as soon as i change it to ""gpt-4 turbo"", i got this error message, (look down) and i don't understand why, because it should be good with the same api endpoint(
""{ ""error"": { ""message"": ""invalid model id"", ""type"": ""invalid_request_error"", ""param"": null, ""code"": null } } ""
            try {
                using (var  = new  {
                    vorlagentext = vorlagentext + selektiertertext;
                    var settings = chatgptsettings;
                    validateparameter((chatgptsettings)settings);
                     $""bearer {chatgpt_apikey}"");
                    var requestdata = new {
                        max_tokens = chatgptsettings.maxtokens,
                        temperature = chatgptsettings.temperatur,
                        model = ""gpt-4 turbo"",
                        messages = new[] {
                                new { role = ""user"", content = vorlagentext }
                        }
                    };
                    var requestcontent = new stringcontent(jsonconvert.serializeobject(requestdata), encoding.utf8, ""application/json"");
                    var response = await  requestcontent);
                    var responsebody = await response.content.readasstringasync();

maybe someone can help me, thank you even if just trying :d
i'm expecting an answer from chatgpt as i get it with the model ""gpt-3.5-turbo"" because it has the same api endpoint, i tried to change the url, the api endpoint to the older one, but thats not working, and this is the only new one i can find.","['c#', 'visual-studio-2019', 'openai-api', 'chatgpt-api']",77658538,"the model name you want to use right now to get the gpt4 turbo model is gpt-4-1106-preview. once that model is promoted from preview to production, it will be called something different, but check out this link for a list of other model names you might choose.",https://stackoverflow.com/questions/77654210,c#,13-12-2023 14:10,2779.0,0.0,1.0,True,14-12-2023 07:55,14-12-2023 07:55
42489,how to implement a &quot;related&quot; degree measure algorithm?,"i was going to ask a question earlier today when i was presented to a surprising functionality in stackoverflow. when i wrote my question title stackoverflow suggested me several related questions and i found out that there was already two similar questions. that was stunning! 
then i started thinking how i would implement such function. how i would order questions by relatedness:

question that have higher number of
words matchs with the new question
if the number of matchs are the
same, the order of words is considered
words that appears in the title has
higher relevancy

that would be a simple workflow or a complex score algortithm?
some stemming to increase the recall, maybe?
is there some library the implements this function?
what other aspects would you consider?
maybe jeff could answer himself! how did you implemented this in stackoverflow? :)","algorithm, machine-learning, indexing, nlp, full-text-search",42532,"one such way to implement such an algorithm would involve ranking the questions as per a heuristic function which assigns a 'relevance' weight factor using the following steps:

apply a noise filter to the 'new' question to remove words that are common across a large number of objects such as: 'the', 'and', 'or', etc.
get the number of words contained in the 'new' question which match the words the set of questions already posted on the website. [a]
get the number of tag matches between the words in the 'new' question and the available. [b]
compute the 'relevance weight' based on [a] and [b] as 'x[a] + y[b]', where x and y are weight multipliers (assign a higher weight multiplier to [b] as tagging is more relevant than simple word search)
get the top 5 questions which have the highest 'relevance weight'.

the heuristic might require tweaking to get optimal results, but it should work.",https://stackoverflow.com/q/42489,"algorithm, machine-learning, indexing, nlp, full-text-search",03-09-2008 20:21,458.0,8.0,2.0,True,29-03-2025 20:26,29-03-2025 20:26
67372903,access openai (json) api from r,"i want to access the openai api with the following curl command from r:
curl  \
-h ""content-type: application/json"" \
-h ""authorization: bearer your_api_key"" \
-d '{""prompt"": ""this is a test"", ""max_tokens"": 5}'

i think the curl package (on cran) will be the best option(?). i have never used this package so can anyone help me getting started with this simple call?","['r', 'json', 'curl', 'openai-api', 'gpt-3']",75949356,"i created an r package named ""openapi"" ( which supports all openai apis and can generate streaming returns (currently, other packages do not have good solutions), chatgpt app, and various rstudio add-ins. welcome to use it.",https://stackoverflow.com/questions/67372903,r,03-05-2021 17:01,2548.0,1.0,2.0,True,06-04-2023 12:08,24-01-2023 18:23
77614213,transformer.js model fails to parse json in client-side next.js example,"on brave browser version 1.61.101 chromium: 120.0.6099.71 (official build) (x86_64) (on chrome version 120.0.6099.62 (official build) (x86_64) i don't see this or any other errors) i run into a json parsing error when trying to run in development mode the client-side example from hugging face's next.js tutorial:  because of this error the text classification cannot run.
the error is as follows:

this is my transformerworker.js which is taken directly from the tutorial linked above, no changes:
import { pipeline, env } from ""@xenova/transformers"";

// skip local model check and cache
env.allowlocalmodels = false;

// use the singleton pattern to enable lazy construction of the pipeline.
class pipelinesingleton {
    static task = 'text-classification';
    static model = 'xenova/distilbert-base-uncased-finetuned-sst-2-english';
    static instance = null;

    static async getinstance(progress_callback = null) {
        if (this.instance === null) {
            this.instance = pipeline(this.task, this.model, { progress_callback });
        }
        return this.instance;
    }
}

// listen for messages from the main thread
self.addeventlistener('message', async (event) => {
    // retrieve the classification pipeline. when called for the first time,
    // this will load the pipeline and save it for future use.
    let classifier = await pipelinesingleton.getinstance(x => {
        // we also add a progress callback to the pipeline so that we can
        // track model loading.
        self.postmessage(x);
    });

    // actually perform the classification
    let output = await classifier(event.data.text);

    // send the output back to the main thread
    self.postmessage({
        status: 'complete',
        output: output,
    });
});","['javascript', 'next.js', 'huggingface-transformers', 'brave']",77614214,"looking through the github issues for transformers.js i found  in which one user suggested adding the following line to the worker file:
env.usebrowsercache = false;
turning the browser cache option off i no longer receive this error in the brave browser and, in fact, the next.js client-side example code hugging face provides in their tutorial works without any further issues for me.
that said, browser caching is a significant feature, so this could indicate an underlying bug that needs to be fixed in transformers.js. see the link to the transformers.js github issue above.",https://stackoverflow.com/questions/77614213,javascript,06-12-2023 15:05,1125.0,0.0,1.0,True,09-12-2024 20:41,07-12-2023 18:16
31023099,how can i easily draw a parse tree from stanford parsing data in python?,"so i have this stanford-style parsing of an english sentence:
""there is a tree behind a car""
parse: [s [np there_ex np] [vp is_vbz [np [np a_dt tree_nn np] [pp behind_in [np a_dt car_nn np] pp] np] vp] s]

i want to use some of the tree drawing methods in python to draw a parsing tree from the data.
is there an easy way to use that parsing representation to draw a tree with python or should i change the representation somehow?","['python', 'parsing', 'nlp', 'parse-tree']",31023937,nltk has a tree module. you can use it to parse the representation you get out of stanford (see this related question). then you can use nltk.tree.draw to display it.,https://stackoverflow.com/questions/31023099,python,24-06-2015 09:42,3742.0,5.0,3.0,True,24-01-2023 13:12,24-06-2015 10:39
6629165,k-fold cross validation for determining k in k-means?,"in a document clustering process, as a data pre-processing step, i first applied singular vector decomposition to obtain u, s and vt and then by choosing a suitable number of eigen values i truncated vt, which now gives me a good document-document correlation from what i read here. now i am performing clustering on the columns of the matrix vt to cluster similar documents together and for this i chose k-means and the initial results looked acceptable to me (with k = 10 clusters) but i wanted to dig a bit deeper on choosing the k value itself. to determine the number of clusters k in k-means, i was suggested to look at cross-validation. 
before implementing it i wanted to figure out if there is a built-in way to achieve it using numpy or scipy. currently, the way i am performing kmeans is to simply use the function from scipy.
import numpy, scipy

# preprocess the data and compute svd
u, s, vt = svd(a) # a is the tfidf representation of the original term-document matrix

# obtain the document-document correlations from vt
# this 50 is the threshold obtained after examining a scree plot of s
docvectors = numpy.transpose(self.vt[0:50, 0:]) 

# prepare the data to run k-means
whitened = whiten(docvectors)
res, idx = kmeans2(whitened, 10, iter=20)

assuming my methodology is correct so far (please correct me if i am missing some step), at this stage, what is the standard way of using the output to perform cross-validation? any reference/implementations/suggestions on how this would be applied to k-means would be greatly appreciated.","python, statistics, numpy, nlp, machine-learning",6631714,"to run k-fold cross validation, you'd need some measure of quality to optimize for. this could be either a classification measure such as accuracy or f1, or a specialized one such as the v-measure.
even the clustering quality measures that i know of need a labeled dataset (""ground truth"") to work; the difference with classification is that you only need part of your data to be labeled for the evaluation, while the k-means algorithm can make use all the data to determine the centroids and thus the clusters.
v-measure and several other scores are implemented in scikit-learn, as well as generic cross validation code and a ""grid search"" module that optimizes according to a specified measure of evaluation using k-fold cv. disclaimer: i'm involved in scikit-learn development, though i didn't write any of the code mentioned.",https://stackoverflow.com/q/6629165,"python, statistics, numpy, nlp, machine-learning",08-07-2011 19:00,22179.0,5.0,3.0,True,06-10-2023 20:57,23-05-2017 10:28
77444332,openai python package error: &#39;chatcompletion&#39; object is not subscriptable,"after updating my openai package to version 1.1.1, i got this error when trying to read the chatgpt api response:

'chatcompletion' object is not subscriptable

here is my code:
messages = [
        {""role"": ""system"", ""content"": '''you answer question about some service'''
        },
        {""role"": ""user"", ""content"": 'the user question is ...'},
    ]
response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0
    )
response_message = response[""choices""][0][""message""][""content""]

how can i resolve this error?","['python', 'typeerror', 'openai-api', 'chatgpt-api']",77444334,"in the latest openai package the response.choices object type is changed and in this way you must read the response:
print(response.choices[0].message.content)

the complete working code:
from openai import openai

client = openai(api_key='yourkey')
gpt_model = ""gpt-4-1106-preview"" #""gpt-3.5-turbo-1106""
messages = [
        {""role"": ""system"", ""content"": 'you answer question about web  services.'
        },
        {""role"": ""user"", ""content"": 'the user message'},
    ]
response = client.chat.completions.create(
        model=gpt_model,
        messages=messages,
        temperature=0
    )
response_message = response.choices[0].message.content
print(response_message )

see this example in the project readme.",https://stackoverflow.com/questions/77444332,python,08-11-2023 09:35,65116.0,51.0,2.0,True,03-12-2024 06:56,04-09-2024 01:56
18391602,what does generate() do when using nltk in python?,"i've been working with nltk for the past three days to get familiar and reading the ""natural language processing"" book to understand what's going on. i'm curious if someone could clarify for me the following:

note that the first time you run this command, it is slow because it
  gathers statistics about word sequences. each time you run it, you
  will get different output text. now try generating random text in the
  style of an inaugural address or an internet chat room. although the
  text is random, it re-uses common words and phrases from the source
  text and gives us a sense of its style and content. (what is lacking
  in this randomly generated text?)

this part of the text, chapter 1, simply says that it ""gathers statistics"" and it will get ""different output text""
what specifically does generate do and how does it work? 
this example of generate() uses text3, which is the bible's genesis:

in the beginning , between me and thee and in the garden thou mayest
  come in unto noah into the ark , and mibsam , and said , is there yet
  any portion or inheritance for us , and make thee as ephraim and as
  the sand of the dukes that came with her ; and they were come . also
  he sent forth the dove out of thee , with tabret , and wept upon them
  greatly ; and she conceived , and called their names , by their names
  after the end of the womb ? and he

here, the generate() function seems to simply output phrases created by cutting off text at punctuation and randomly reassembling it but it has a bit of readability to it.","['nlp', 'nltk']",18392258,"type(text3) will tell you that text3 is of type nltk.text.text. 
to cite the documentation of text.generate():

print random text, generated using a trigram language model.

that means that nltk has created an n-gram model for the genesis text, counting each occurence of sequences of three words so that it can predict the most likely successor of any given two words in this text. n-gram models will be explained in more detail in chapter 5 of the nltk book.
see also the answers to this question.",https://stackoverflow.com/questions/18391602,nlp,22-08-2013 22:06,7530.0,2.0,1.0,True,03-06-2022 17:10,25-10-2013 19:55
71318599,"bert classifier valueerror: target size (torch.size([4, 1])) must be the same as input size (torch.size([4, 2]))","i'm training a classifier model but it's a few days that i cannot overcame a problem!
i have the valueerror: target size (torch.size([4, 1])) must be the same as input size (torch.size([4, 2])) error but actually it seems correct to me ! indeed i used unsqueeze(1) to put them of the same size. what else i can try? thank you!
class sequenceclassifier(nn.module):

  def __init__(self, n_classes):
    super(sequenceclassifier, self).__init__()
    self.bert = bertmodel.from_pretrained(pre_trained_model_name,return_dict=false)
    self.drop = nn.dropout(p=0.3)
    self.out = nn.linear(self.bert.config.hidden_size, n_classes)
  
  def forward(self, input_ids, attention_mask):
    _, pooled_output = self.bert(
      input_ids=input_ids,
      attention_mask=attention_mask
    ) 
    output = self.drop(pooled_output)
    return self.out(output)

model = sequenceclassifier(len(class_names))
model = model.to(device)

epochs = 10

optimizer = adamw(model.parameters(), lr=2e-5, correct_bias=false)
total_steps = len(train_data_loader) * epochs

scheduler = get_linear_schedule_with_warmup(
  optimizer,
  num_warmup_steps=0,
  num_training_steps=total_steps
)
weights=[0.5,1]
pos_weight=torch.floattensor(weights).to(device)
loss_fn=nn.bcewithlogitsloss(pos_weight=pos_weight)

def train_epoch(
  model, 
  data_loader, 
  loss_fn, 
  optimizer, 
  device, 
  scheduler, 
  n_examples
):
  model = model.train()

  losses = []
  correct_predictions = 0
  
  for d in data_loader:
    input_ids = d[""input_ids""].to(device)
    attention_mask = d[""attention_mask""].to(device)
    targets = d[""targets""].to(device)

    outputs = model(
      input_ids=input_ids,
      attention_mask=attention_mask
    )

    _, preds = torch.max(outputs, dim=1)
    
    targets = targets.unsqueeze(1)
    loss = loss_fn(outputs, targets)
    

    correct_predictions += torch.sum(preds == targets)
    losses.append(loss.item())

    loss.backward()
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()

  return correct_predictions.double() / n_examples, np.mean(losses)
  

%%time

history = defaultdict(list)
best_accuracy = 0

for epoch in range(epochs):

  print(f'epoch {epoch + 1}/{epochs}')
  print('-' * 10)

  train_acc, train_loss = train_epoch(
    model,
    train_data_loader,    
    loss_fn, 
    optimizer, 
    device, 
    scheduler, 
    len(df_train)
  )

  print(f'train loss {train_loss} accuracy {train_acc}')

  val_acc, val_loss = eval_model(
    model,
    val_data_loader,
    loss_fn, 
    device, 
    len(df_val)
  )

  print(f'val   loss {val_loss} accuracy {val_acc}')
  print()

  history['train_acc'].append(train_acc)
  history['train_loss'].append(train_loss)
  history['val_acc'].append(val_acc)
  history['val_loss'].append(val_loss)

  if val_acc > best_accuracy:
    torch.save(model.state_dict(), 'best_model_state.bin')
    best_accuracy = val_acc

valueerror: target size (torch.size([4, 1])) must be the same as input size (torch.size([4, 2]))

edit
i have a binary classifier problem, indeed i have 2 classes encoded 0 (""bad"") and 1 (""good"").","['python', 'machine-learning', 'pytorch', 'huggingface-transformers', 'bert-language-model']",72744048,"in case anyone stumbles on this like i did, i'll write out an answer since there aren't a lot of google hits for this target size/input size error and the previous answer has some factual inaccuracies.
unlike the previous answer would suggest, the real problem isn't with the loss function but with the output of the model.nn.bcewithlogitsloss is completely fine for multi-label and multi-class applications. chiara updated her post saying that in fact she has a binary classification problem, but even that should not be a problem for this loss function. so why the error?
the original code has:
outputs = model(
  input_ids=input_ids,
  attention_mask=attention_mask
)
_, preds = torch.max(outputs, dim=1)

this means ""run the model, then create preds with the row indeces of the highest output of the model"". obviously, there is only a ""index of highest"" if there are multiple  predicted values. multiple output values usually means multiple input classes, so i can see why shai though this was multi-class. but why would we get multiple outputs from a binary classifier?
as it turns out, bert (or huggingface anyway) for binary problems expects that n_classes is set to 2 -- setting classes to 1 puts the model in regression mode. this means that under the hood, binary problems are treated like a two-class problem, outputting predictions with the size [2, batch size] -- one column predicting the chance of it being a 1 and one for the chance of it being 0. the loss fucntion throws an error because it is supplied with only one row of one-hot encoded labels: targets = d[""targets""].to(device) so the  labels have dimensions [batch size] or after the unsqueeze, [1, batch size]. either way, the dimensions don't match up.
some loss functions can deal with this fine, but others require the exact same dimensions. to make things more frustrating, for version 1.10, nn.bcewithlogitsloss requires matching dimensions but later versions do not.
one solution may therefore be to update your pytorch (version 1.11 would work for example).
for me, this was not an option, so i ended up going with a different loss function. nn.crossentropyloss, as suggested by shai, indeed does the trick as it accepts any input with the same length. in other words, they had a working solution for the wrong reasons.",https://stackoverflow.com/questions/71318599,python,02-03-2022 07:02,4453.0,0.0,2.0,True,30-06-2022 11:30,02-03-2022 09:53
73415504,error importing layoutlmv2fortokenclassification from huggingface,"i am trying to run this demo notebook on colab and i am getting the following pytorch error when importing layoutlmv2fortokenclassification:
from transformers import layoutlmv2fortokenclassification

error:
modulenotfounderror                       traceback (most recent call last)
~\pycharmprojects\playground\camelot\camelotenv\lib\site-packages\transformers\utils\import_utils.py in _get_module(self, module_name)
   1029         try:
-> 1030             return importlib.import_module(""."" + module_name, self.__name__)
   1031         except exception as e:

~\appdata\local\programs\python\python39\lib\importlib\__init__.py in import_module(name, package)
    126             level += 1
--> 127     return _bootstrap._gcd_import(name[level:], package, level)
    128 

~\appdata\local\programs\python\python39\lib\importlib\_bootstrap.py in _gcd_import(name, package, level)

~\appdata\local\programs\python\python39\lib\importlib\_bootstrap.py in _find_and_load(name, import_)

~\appdata\local\programs\python\python39\lib\importlib\_bootstrap.py in _find_and_load_unlocked(name, import_)

~\appdata\local\programs\python\python39\lib\importlib\_bootstrap.py in _load_unlocked(spec)

~\appdata\local\programs\python\python39\lib\importlib\_bootstrap_external.py in exec_module(self, module)

~\appdata\local\programs\python\python39\lib\importlib\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)

~\pycharmprojects\playground\camelot\camelotenv\lib\site-packages\transformers\models\layoutlmv2\modeling_layoutlmv2.py in <module>
     48     import detectron2
---> 49     from detectron2.modeling import meta_arch_registry
     50 

~\pycharmprojects\playground\camelot\camelotenv\lib\site-packages\detectron2\modeling\__init__.py in <module>
     19 )
---> 20 from .meta_arch import (
     21     meta_arch_registry,

~\pycharmprojects\playground\camelot\camelotenv\lib\site-packages\detectron2\modeling\meta_arch\__init__.py in <module>
      5 
----> 6 from .panoptic_fpn import panopticfpn
      7 

~\pycharmprojects\playground\camelot\camelotenv\lib\site-packages\detectron2\modeling\meta_arch\panoptic_fpn.py in <module>
     13 from .build import meta_arch_registry
---> 14 from .rcnn import generalizedrcnn
     15 from .semantic_seg import build_sem_seg_head

~\pycharmprojects\playground\camelot\camelotenv\lib\site-packages\detectron2\modeling\meta_arch\rcnn.py in <module>
     17 from ..proposal_generator import build_proposal_generator
---> 18 from ..roi_heads import build_roi_heads
     19 from .build import meta_arch_registry

~\pycharmprojects\playground\camelot\camelotenv\lib\site-packages\detectron2\modeling\roi_heads\__init__.py in <module>
     14 )
---> 15 from .roi_heads import (
     16     roi_heads_registry,

~\pycharmprojects\playground\camelot\camelotenv\lib\site-packages\detectron2\modeling\roi_heads\roi_heads.py in <module>
     16 from ..matcher import matcher
---> 17 from ..poolers import roipooler
     18 from ..proposal_generator.proposal_utils import add_ground_truth_to_proposals

~\pycharmprojects\playground\camelot\camelotenv\lib\site-packages\detectron2\modeling\poolers.py in <module>
      9 from detectron2.structures import boxes
---> 10 from detectron2.utils.tracing import assert_fx_safe
     11 

~\pycharmprojects\playground\camelot\camelotenv\lib\site-packages\detectron2\utils\tracing.py in <module>
      3 import torch
----> 4 from torch.fx._symbolic_trace import _orig_module_call
      5 from torch.fx._symbolic_trace import is_fx_tracing as is_fx_tracing_current

modulenotfounderror: no module named 'torch.fx._symbolic_trace'

the above exception was the direct cause of the following exception:

runtimeerror                              traceback (most recent call last)
<ipython-input-36-b5a73c6d310a> in <module>
      4 device_ids = [0,1]
      5 
----> 6 from transformers import layoutlmv2fortokenclassification, trainingarguments, trainer
      7 from datasets import load_metric
      8 import numpy as np

~\appdata\local\programs\python\python39\lib\importlib\_bootstrap.py in _handle_fromlist(module, fromlist, import_, recursive)

~\pycharmprojects\playground\camelot\camelotenv\lib\site-packages\transformers\utils\import_utils.py in __getattr__(self, name)
   1019         elif name in self._class_to_module.keys():
   1020             module = self._get_module(self._class_to_module[name])
-> 1021             value = getattr(module, name)
   1022         else:
   1023             raise attributeerror(f""module {self.__name__} has no attribute {name}"")

~\pycharmprojects\playground\camelot\camelotenv\lib\site-packages\transformers\utils\import_utils.py in __getattr__(self, name)
   1018             value = self._get_module(name)
   1019         elif name in self._class_to_module.keys():
-> 1020             module = self._get_module(self._class_to_module[name])
   1021             value = getattr(module, name)
   1022         else:

~\pycharmprojects\playground\camelot\camelotenv\lib\site-packages\transformers\utils\import_utils.py in _get_module(self, module_name)
   1030             return importlib.import_module(""."" + module_name, self.__name__)
   1031         except exception as e:
-> 1032             raise runtimeerror(
   1033                 f""failed to import {self.__name__}.{module_name} because of the following error (look up to see its""
   1034                 f"" traceback):\n{e}""

runtimeerror: failed to import transformers.models.layoutlmv2.modeling_layoutlmv2 because of the following error (look up to see its traceback):
no module named 'torch.fx._symbolic_trace'

can anyone please help?
thanks!","['pytorch', 'huggingface-transformers']",73424368,"this is issue from importing torch.fix flag check for symbolic trace and new commit error of detectron (use 5aeb252b194b93dc2879b4ac34bc51a31b5aee13 this checkout and install )
for temporary work
or clone pytorch with new commit",https://stackoverflow.com/questions/73415504,pytorch,19-08-2022 10:39,1069.0,1.0,1.0,True,23-08-2022 06:53,19-08-2022 13:08
72496831,"in r, chop off column after n words","i have a df with a text column, and a column with a wordcount value.
how can i delete the last n words of the text (specified in the 'wc' column) and save the output to a third column?
in other words, i need the ""introductory"" part of a bunch of texts, and i know when the intro ends, so i want to cut the text off at that point and save the intro in a new column.
df <- data.frame(text = c(""this is a long text"",""this is also a long text"", ""another long text""),wc=c('1','2','1'))

desired output:




text
wc
chopped_off_text




this is a long text
1
this is a long


this is also a long text
2
this is also a


another long text
1
another long","['r', 'text', 'nlp', 'data-wrangling', 'quanteda']",72496897,"you can use the word function from the stringr package to extract ""words"" in a sentence. str_count(text, ""\\s"") + 1 counts the number of words present in the sentence.
library(stringr)
library(dplyr)

df %>% 
  mutate(chopped_off_text = 
           word(text, 1, end = str_count(text, ""\\s"") + 1 - as.integer(wc)))

                      text wc chopped_off_text
1      this is a long text  1   this is a long
2 this is also a long text  2   this is also a
3        another long text  1     another long",https://stackoverflow.com/questions/72496831,r,04-06-2022 03:22,79.0,1.0,2.0,True,04-06-2022 06:43,04-06-2022 03:33
58858431,pythonpath error [windows 123] with pyspark when use import lazy like nltk or pattern duplicate label disk &#39;c://c://..spark-core_2.11-2.3.2.jar&#39;,"the problem is that the windows path and libraries are imported lazily, like nltk, means that nltk and pattern import their libraries when they are used, and at this moment the module importlib_metada.py and pathlib.py try to read pythonpath with a incorrect value in the path (d: / d: /), then the code explodes.
first, we have a simple function like this
import nltk
def print_stopwords():
  print(nltk.corpus.stopwords)

in local mode, you can run this and you get all the stop words, ok.
if you want to use this function inside a map from spark, to implements a pyspark workflow, the code above is not working. why? i really dont know...
i think the reason why it doesn't work is due to the spark java libraries that use and modify the pythonpath when they do a map function like this:
import nltk
from pyspark.sql import sqlcontext, sparksession

spark = (sparksession
         .builder
         .master(""local[*]"")
         .appname(""nueva"")
         .getorcreate())

sc = spark.sparkcontext
sqlcontext = sqlcontext(sc)

def print_stopwords(x):
    print(""\n"",x)
    print(nltk.corpus.stopwords.words('english'))
    return x

prueba = sc.parallelize([0,1,2,3])
r = prueba.map(print_stopwords)
r.take(1)

i get the error
  file ""c:\programdata\anaconda3\lib\site-packages\nltk\__init__.py"", line 143, in <module>
    from nltk.chunk import *
  file ""c:\programdata\anaconda3\lib\site-packages\nltk\chunk\__init__.py"", line 157, in <module>
    from nltk.chunk.api import chunkparseri
  file ""c:\programdata\anaconda3\lib\site-packages\nltk\chunk\api.py"", line 13, in <module>
    from nltk.parse import parseri
  file ""c:\programdata\anaconda3\lib\site-packages\nltk\parse\__init__.py"", line 100, in <module>
    from nltk.parse.transitionparser import transitionparser
  file ""c:\programdata\anaconda3\lib\site-packages\nltk\parse\transitionparser.py"", line 22, in <module>
    from sklearn.datasets import load_svmlight_file
  file ""c:\programdata\anaconda3\lib\site-packages\sklearn\datasets\__init__.py"", line 22, in <module>
    from .twenty_newsgroups import fetch_20newsgroups
  file ""c:\programdata\anaconda3\lib\site-packages\sklearn\datasets\twenty_newsgroups.py"", line 44, in <module>
    from ..feature_extraction.text import countvectorizer
  file ""c:\programdata\anaconda3\lib\site-packages\sklearn\feature_extraction\__init__.py"", line 10, in <module>
    from . import text
  file ""c:\programdata\anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"", line 28, in <module>
    from ..preprocessing import normalize
  file ""c:\programdata\anaconda3\lib\site-packages\sklearn\preprocessing\__init__.py"", line 6, in <module>
    from ._function_transformer import functiontransformer
  file ""c:\programdata\anaconda3\lib\site-packages\sklearn\preprocessing\_function_transformer.py"", line 5, in <module>
    from ..utils.testing import assert_allclose_dense_sparse
  file ""c:\programdata\anaconda3\lib\site-packages\sklearn\utils\testing.py"", line 718, in <module>
    import pytest
  file ""c:\programdata\anaconda3\lib\site-packages\pytest.py"", line 6, in <module>
    from _pytest.assertion import register_assert_rewrite
  file ""c:\programdata\anaconda3\lib\site-packages\_pytest\assertion\__init__.py"", line 7, in <module>
    from _pytest.assertion import rewrite
  file ""c:\programdata\anaconda3\lib\site-packages\_pytest\assertion\rewrite.py"", line 26, in <module>
    from _pytest.assertion import util
  file ""c:\programdata\anaconda3\lib\site-packages\_pytest\assertion\util.py"", line 8, in <module>
    import _pytest._code
  file ""c:\programdata\anaconda3\lib\site-packages\_pytest\_code\__init__.py"", line 2, in <module>
    from .code import code  # noqa
  file ""c:\programdata\anaconda3\lib\site-packages\_pytest\_code\code.py"", line 23, in <module>
    import pluggy
  file ""c:\programdata\anaconda3\lib\site-packages\pluggy\__init__.py"", line 16, in <module>
    from .manager import pluginmanager, pluginvalidationerror
  file ""c:\programdata\anaconda3\lib\site-packages\pluggy\manager.py"", line 11, in <module>
    import importlib_metadata
  file ""c:\programdata\anaconda3\lib\site-packages\importlib_metadata\__init__.py"", line 549, in <module>
    __version__ = version(__name__)
  file ""c:\programdata\anaconda3\lib\site-packages\importlib_metadata\__init__.py"", line 511, in version
    return distribution(distribution_name).version
  file ""c:\programdata\anaconda3\lib\site-packages\importlib_metadata\__init__.py"", line 482, in distribution
    return distribution.from_name(distribution_name)
  file ""c:\programdata\anaconda3\lib\site-packages\importlib_metadata\__init__.py"", line 183, in from_name
    dist = next(dists, none)
  file ""c:\programdata\anaconda3\lib\site-packages\importlib_metadata\__init__.py"", line 425, in <genexpr>
    for path in map(cls._switch_path, paths)
  file ""c:\programdata\anaconda3\lib\site-packages\importlib_metadata\__init__.py"", line 449, in _search_path
    if not root.is_dir():
  file ""c:\programdata\anaconda3\lib\pathlib.py"", line 1351, in is_dir
    return s_isdir(self.stat().st_mode)
  file ""c:\programdata\anaconda3\lib\pathlib.py"", line 1161, in stat
    return self._accessor.stat(self)
oserror: [winerror 123] the file name, directory name or volume label syntax is not correct: 'c:\\c:\\enviroments\\spark-2.3.2-bin-hadoop2.7\\jars\\spark-core_2.11-2.3.2.jar'

i print the environment variables from pathlib.py and importlib_metadata.py and get the pythonpath values ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½like this:
'pythonpath': 'c:\\enviroments\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip;c:\\enviroments\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip;/c:/enviroments/spark-2.3.2-bin-hadoop2.7/jars/spark-core_2.11-2.3.2.jar'

i try to edit the path inside the function, outside, and all the ways...but in some moment spark serialize the function and edit the pythonpath... no in python files, in java files, and i cant debug this code because the spark runs inside a container, with an ip and port that i cant enter for a lot of complex reason of my ide (intellij idea).
the reason not wse of this slash -> /c:/enviroments/spark-2.3.2-bin-hadoop2.7/jars/spark-core_2.11-2.3.2.jar'. the python interprets this slash in windows like an absolute path and add the disk label at the beginning of the path, /c: => c:/c:/. then in execution it generates the error that this route, obviously, does not exist.
please help me!
thanks in advance :)","['python', 'windows', 'apache-spark', 'pyspark', 'nltk']",58861455,"it works if use new conda environment in another drive letter, maybe because the importlib package is installed and used in the base conda env, but is not executed in new conda environments. i don't know why.
python with conda environments do not work properly with the base conda env, it needs to use another conda environment in another dirve (for example d: if have conda installed on c: drive)
to make this, you can change the pyspark_python enviroments variables:
os.environ[""pyspark_python""]=""d:\\conda_envs\\new_environment\\python.exe""
os.environ[""spark_driver_python""]=""d:\\conda_envs\\new_environment\\python.exe""

make sure that you have the spark_home directory in the same drive letter as the python, or anaconda environment.
ï¿½ï¿½\(ï¿½ï¿½ï¿½)/ï¿½ï¿½
whatever
finally res",https://stackoverflow.com/questions/58858431,python,14-11-2019 13:54,383.0,-1.0,2.0,True,18-03-2022 13:59,14-11-2019 16:18
74944314,text data extraction between keywords in a string,"i have text data that looks like the following after extracting from a file and cleaning. i want to put the data into a pandas dataframe where the columns are ('examination', 'technique', 'comparison', 'findings', 'impression'), and each cell in each row contains the extracted data related to the column name (i.e. the keyword).

'final report examination: chest pa and lat indication: f with new onset ascites eval for infection technique: chest pa and lateral comparison: none findings: there is no focal consolidation pleural effusion or pneumothorax bilateral nodular opacities that most likely represent nipple shadows the cardiomediastinal silhouette is normal clips project over the left lung potentially within the breast the imaged upper abdomen is unremarkable chronic deformity of the posterior left sixth and seventh ribs are noted impression: no acute cardiopulmonary process'

for example, under the column technique there should be a cell containing ""chest pa and lateral"", and under the column impression, there should be a cell containing ""no acute cardiopulmonary process"".","['dataframe', 'machine-learning', 'nlp', 'data-science', 'data-cleaning']",74945513,"solution as follows, please note the following assumptions:

keywords as presented are located in that order within the sample text.
the keywords are not contained within the text to be extracted.
each keyword is followed by a "": "" (the colon and whitespace is removed).

solution
import pandas as pd

sample = ""final report examination: chest pa and lat indication: f with new onset ascites eval for infection technique: chest pa and lateral comparison: none findings: there is no focal consolidation pleural effusion or pneumothorax bilateral nodular opacities that most likely represent nipple shadows the cardiomediastinal silhouette is normal clips project over the left lung potentially within the breast the imaged upper abdomen is unremarkable chronic deformity of the posterior left sixth and seventh ribs are noted impression: no acute cardiopulmonary process""

keywords = [""examination"", ""technique"", ""comparison"", ""findings"", ""impression""]


# create function to extract text between each of the keywords
def extract_text_using_keywords(clean_text, keyword_list):
    extracted_texts = []
    for prev_kw, current_kw in zip(keyword_list, keyword_list[1:]):
        prev_kw_index = clean_text.index(prev_kw)
        current_kw_index = clean_text.index(current_kw)
        extracted_texts.append(clean_text[prev_kw_index + len(prev_kw) + 2:current_kw_index])
        # extract the text after the final keyword in keyword_list (i.e. ""impression"")
        if current_kw == keyword_list[-1]:
            extracted_texts.append(clean_text[current_kw_index + len(current_kw) + 2:len(clean_text)])
    return extracted_texts


# extract text
result = extract_text_using_keywords(sample, keywords)
# create pandas dataframe
df = pd.dataframe([result], columns=keywords)

print(df)

# to append future results to the end of the pandas df you can use
# df.loc[len(df)] = result

output
   examination                                        technique                  comparison    findings                                           impression
0  chest pa and lat indication: f with new onset ...  chest pa and lateral       none          there is no focal consolidation pleural effusi...  no acute cardiopulmonary process",https://stackoverflow.com/questions/74944314,dataframe,28-12-2022 19:35,81.0,2.0,2.0,True,29-12-2022 21:18,29-12-2022 21:18
73869397,retrieve a list of model-specific pos tags using spacy,"i am looking for a way to get a list of all possibly usable pos tags for a specific language model in spacy.
in an answer to another question, spacy's tag_map has been referenced to, but i am not sure how to access this. the documentation of spacy says that this attribute has been replaced. since spacy only uses a specific subset of all pos tags for a specific language, i would like to retrieve a list of all pos tags that are currently used with the initialized language model.
i did currently just set up a model this way:
import spacy

tagger = spacy.load(""de_dep_news_trf"")

# todo print(pos_tags)

now, how do i print a list of all possible pos tags for this model?","['python', 'python-3.x', 'nlp', 'pos-tagger', 'spacy-3']",73876748,"for pretrained pipelines, you can check the labels on the model page, under the ""label scheme"" entry.
if your pipeline has a tagger, like the german one does, you can do this:
tagger = nlp.get_pipe(""tagger"")
print(tagger.labels)",https://stackoverflow.com/questions/73869397,python,27-09-2022 14:34,867.0,1.0,1.0,True,28-09-2022 05:55,27-09-2022 15:08
21445659,use r to convert pdf files to text files for text mining,"i have nearly one thousand pdf journal articles in a folder. i need to text mine on all article's abstracts from the whole folder. now i am doing the following:
dest <- ""~/a1.pdf""

# set path to pdftotxt.exe and convert pdf to text
exe <- ""c:/program files (x86)/xpdfbin-win-3.03/bin32/pdftotext.exe""
system(paste(""\"""", exe, ""\"" \"""", dest, ""\"""", sep = """"), wait = f)

# get txt-file name and open it
filetxt <- sub("".pdf"", "".txt"", dest)
shell.exec(filetxt)

by this, i am converting one pdf file to one .txt file and then copying the abstract in another .txt file and compile it manually. this work is troublesome. 
how can i read all individual articles from the folder and convert them into .txt file which contain only the abstract from each article. it can be done by limiting the content between abstract and introduction in each article; but i am not able to do so. any help is appreciated.","['r', 'text-mining', 'tm', 'pdftotext']",21449040,"yes, not really an r question as ishouldbuyaboat notes, but something that r can do with only minor contortions...
use r to convert pdf files to txt files...
# folder with 1000s of pdfs
dest <- ""c:\\users\\desktop""

# make a vector of pdf file names
myfiles <- list.files(path = dest, pattern = ""pdf"",  full.names = true)

# convert each pdf file that is named in the vector into a text file 
# text file is created in the same directory as the pdfs
# note that my pdftotext.exe is in a different location to yours
lapply(myfiles, function(i) system(paste('""c:/program files/xpdf/bin64/pdftotext.exe""', 
             paste0('""', i, '""')), wait = false) )

extract only abstracts from txt files...
# if you just want the abstracts, we can use regex to extract that part of
# each txt file, assumes that the abstract is always between the words 'abstract'
# and 'introduction'
mytxtfiles <- list.files(path = dest, pattern = ""txt"",  full.names = true)
abstracts <- lapply(mytxtfiles, function(i) {
  j <- paste0(scan(i, what = character()), collapse = "" "")
  regmatches(j, gregexpr(""(?<=abstract).*?(?=introduction)"", j, perl=true))
})

write abstracts into separate txt files...
# write abstracts as txt files 
# (or use them in the list for whatever you want to do next)
lapply(1:length(abstracts),  function(i) write.table(abstracts[i], file=paste(mytxtfiles[i], ""abstract"", ""txt"", sep="".""), quote = false, row.names = false, col.names = false, eol = "" "" ))

and now you're ready to do some text mining on the abstracts.",https://stackoverflow.com/questions/21445659,r,30-01-2014 00:33,38675.0,21.0,2.0,True,07-08-2022 07:34,16-02-2018 01:13
78128795,why does langchain sqldatabase connection detects only a few of existing tables?,"i have successfully connected to a redshift database like below and got all the table names;
conn = psycopg2.connect(host,db,port,username,password)
cursor.execute(""select tablename from pg_tables group by tablename order by tablename"")

however, when i connect using langchain and sqlalchemy like below, get_usable_table_names returns few of many tables in the database;
pg_url = f""postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{port_}/{db_}""
db_engine = create_engine(pg_url)
db = sqldatabase(db_engine)
llm = openai(temperature=0.0, openai_api_key=openai_api_key, model='gpt-3.5-turbo')

table_names = ""\n"".join(db.get_usable_table_names())

anyone has any suggestions on what might be the issue?
i have tried querying a missing table by;
db.run(""select * from db_schema.missing_table_name"") 

and this works. however, i need sqldatabase from langchain.sql_database module to detect the tables right without specifying one by one. (because i would like to chat with sql database using langchain & openai)","['sql', 'sqlalchemy', 'amazon-redshift', 'openai-api', 'langchain']",78315141,"thanks for all the answers.
i managed to get it running by replacing
db_engine = create_engine(pg_uri) db=sqldatabase(db_engine)
with this
db = sqldatabase.from_uri(pg_uri,schema=""schema_name"", view_support=true)
i believe i wasn't specifying the schema correctly.",https://stackoverflow.com/questions/78128795,sql,08-03-2024 15:41,676.0,0.0,1.0,True,12-04-2024 08:49,11-03-2024 01:13
56968434,bleu score in python from scratch,"after watching andrew ng's video about bleu score i wanted to implement one from scratch in python. i wrote the code full in python with numpy sparingly. this is the full code
import numpy as np

def n_gram_generator(sentence,n= 2,n_gram= false):
    '''
    n-gram generator with parameters sentence
    n is for number of n_grams
    the n_gram parameter removes repeating n_grams 
    '''
    sentence = sentence.lower() # converting to lower case
    sent_arr = np.array(sentence.split()) # split to string arrays
    length = len(sent_arr)

    word_list = []
    for i in range(length+1):
        if i < n:
            continue
        word_range = list(range(i-n,i))
        s_list = sent_arr[word_range]
        string = ' '.join(s_list) # converting list to strings
        word_list.append(string) # append to word_list
        if n_gram:
            word_list = list(set(word_list))
    return word_list

def bleu_score(original,machine_translated):
    '''
    bleu score function given a orginal and a machine translated sentences
    '''
    mt_length = len(machine_translated.split())
    o_length = len(original.split())

    # brevity penalty 
    if mt_length>o_length:
        bp=1
    else:
        penality=1-(mt_length/o_length)
        bp=np.exp(penality)

    # calculating precision
    precision_score = []
    for i in range(mt_length):
        original_n_gram = n_gram_generator(original,i)
        machine_n_gram = n_gram_generator(machine_translated,i)
        n_gram_list = list(set(machine_n_gram)) # removes repeating strings

        # counting number of occurence 
        machine_score = 0
        original_score = 0
        for j in n_gram_list:
            machine_count = machine_n_gram.count(j)
            original_count = original_n_gram.count(j)
            machine_score = machine_score+machine_count
            original_score = original_score+original_count

        precision = original_score/machine_score
        precision_score.append(precision)
    precisions_sum = np.array(precision_score).sum()
    avg_precisions_sum=precisions_sum/mt_length
    bleu=bp*np.exp(avg_precisions_sum)
    return bleu

if __name__ == ""__main__"":
    original = ""this is a test""
    bs=bleu_score(original,original)
    print(""bleu score original"",bs)

i tried to test my score with nltk's 
from nltk.translate.bleu_score import sentence_bleu
reference = [['this', 'is', 'a', 'test']]
candidate = ['this', 'is', 'a', 'test']
score = sentence_bleu(reference, candidate)
print(score)

the problem is my bleu score is about 2.718281 and nltk's is 1. what am i doing wrong? 
here are some possible reason's:
1) i calculated ngrams with respect to the length of the machine translated sentence. here from 1 to 4
2) n_gram_generator function which i wrote myself and not sure about its accuracy
3) some how i used wrong function or miscalculated bleu score
can some one look my code up and tell me where i did the mistake?","['python', 'machine-learning', 'nlp', 'nltk']",56971701,"your bleu score calculation is wrong.
issue:

you have to use clipped precision 
sklearn uses weights for each n gram 
sklearn uses ngrams for n = 1,2,3,4 

corrected code
def bleu_score(original,machine_translated):
    '''
    bleu score function given a orginal and a machine translated sentences
    '''
    mt_length = len(machine_translated.split())
    o_length = len(original.split())

    # brevity penalty 
    if mt_length>o_length:
        bp=1
    else:
        penality=1-(mt_length/o_length)
        bp=np.exp(penality)

    # clipped precision
    clipped_precision_score = []
    for i in range(1, 5):
        original_n_gram = counter(n_gram_generator(original,i))
        machine_n_gram = counter(n_gram_generator(machine_translated,i))

        c = sum(machine_n_gram.values())
        for j in machine_n_gram:
            if j in original_n_gram:
                if machine_n_gram[j] > original_n_gram[j]:
                    machine_n_gram[j] = original_n_gram[j]
            else:
                machine_n_gram[j] = 0

        #print (sum(machine_n_gram.values()), c)
        clipped_precision_score.append(sum(machine_n_gram.values())/c)

    #print (clipped_precision_score)

    weights =[0.25]*4

    s = (w_i * math.log(p_i) for w_i, p_i in zip(weights, clipped_precision_score))
    s = bp * math.exp(math.fsum(s))
    return s

original = ""it is a guide to action which ensures that the military alwasy obeys the command of the party""
machine_translated = ""it is the guiding principle which guarantees the military forces alwasy being under the command of the party""

print (bleu_score(original, machine_translated))
print (sentence_bleu([original.split()], machine_translated.split()))

output:
0.27098211583470044
0.27098211583470044",https://stackoverflow.com/questions/56968434,python,10-07-2019 10:09,4194.0,5.0,4.0,True,10-10-2023 20:32,10-07-2019 11:53
75961596,where are the different projection matrices for huggingface transformer model?,"my question is very simple. i have a pre-trained transformer model i'm loading using pytorch and huggingface. using collab, i run the following code and print out the keys of the state dict:
model = distilbertmodel.from_pretrained(""distilbert-base-uncased"")
model.state_dict().keys()

the output of this is:

odict_keys(['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.layernorm.weight', 'embeddings.layernorm.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.output_layer_norm.bias'])

it seems at first glance that i'm missing the weights for different heads. where are the weights for different heads?
my second question is a yes or no: it seems the answer to my first question may be that the weights for different heads have been concatenated. on inspection, the projection matrices for example are 768x768. is this really 12 768x64 projection matrices concatenated?
where is the documentation for this? i can't find any explanation of these state_dict keys anywhere on huggingface.
edit:
i tried loading a pre-trained bert model using tensorflow instead, but its the same issue. the wq and wk matrices are both 768x768. my hunch is that since each of the wq matrices for the 12 different were intended to be 64xdim, this matrix stacks the projection matrices for each of the heads row-by-row. but how do i know i'm not getting it backwards or transposed without any documentation for either tensorflow or pytorch on how this state is defined?","['pytorch', 'huggingface-transformers']",76409374,"the weights for the heads are concatenated; not only that, but it is also customary to concatenate the weights for q, k, and v into a single matrix (for self-attention).
the easiest (most common) way to make the split is along the last dimension, as the resulting matrix would still be contiguous in memory. but ultimately, that depends on the implementation.
take for instance this code from mingpt
# on init
c_attn = nn.linear(n_embed, 3*n_embed)
...
# on forward
b, t, c = x.size() # batch size, time sequence length, n_embed
q, k, v = c_attn(x).split(n_embed, dim=2) 
q = q.view(b, t, n_head, n_embed/n_head) #.transpose(1,2)

as you can understand, both the qkv and heads are concatenated along the last dimension, and were split/reshaped accordingly.
for your distilbert, you can look at the source code of hugginface transformers multiheadattention
def shape(x: torch.tensor) -> torch.tensor:
            """"""separate heads""""""
            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)
# query: torch.tensor(bs, seq_length, dim)
# self.q_lin(query) still has shape (bs, seq_length, dim)
q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)
# the result is already transpose for the next matrix product

is this really 12 768x64 projection matrices concatenated? yes, the split/concat is along the last dimension.
the best documentation is the source.",https://stackoverflow.com/questions/75961596,pytorch,07-04-2023 20:02,473.0,1.0,1.0,True,05-06-2023 19:10,10-04-2023 05:04
79131873,kernel crash while excecuting vector embedding operation in chromadb,"i'm trying to build a chatbot using ollama locally. and i'm stuck with my embedding process (chromadb).
when i provide a full pdf, the kernel crashes during the embedding process.
it works fine when i provide a single chapter from the book.
problem block:
# create the chroma vector store
from langchain_chroma.vectorstores import chroma
try:
    vector_db = chroma.from_documents(
        documents=chunked_document,
        embedding=embedding_model,
        collection_name=""local-rag"",
        persist_directory=""./db/db_nomic""
    )
    print(""embedded documents stored in chromadb successfully!"")
except exception as e:
    print(f""an error occurred: {e}"")

output
note
embedding_model = ollamaembeddings(model=""nomic-embed-text"")

chunked_document = [document(metadata={'source': 'xxx', 'page': 1, 'math_expressions': 'xxx'}, page_content=''), .... ]

additional info:
python version = 3.12.7
what i've tried so far:

i tried moving the code to a python file instead of using jupyter notebook. the execution stops at the same block.
execution stops after chunking

i tried to embed in batches. the same issue occurs


from langchain_chroma.vectorstores import chroma
vector_db = chroma(
    collection_name=""local-rag"",
    persist_directory=""./dtbs/db_nomic"",
    embedding_function=embedding_model
)
texts = [chunk.page_content for chunk in chunked_document]
metadatas = [chunk.metadata for chunk in chunked_document]
batch_size = 100
for i in range(0, len(texts), batch_size):
    batch_texts = texts[i:i+batch_size]
    batch_metadatas = metadatas[i:i+batch_size]
    vector_db.add_texts(texts=batch_texts, metadatas=batch_metadatas)


i tried to merge all the databases in a single database(as it is working when i provide single chapters). no luck there either

chapter_paths = [
    ""./partial_databases/db_nomic/ch1"",
    ""./partial_databases/db_nomic/ch2"",
    ...,
]
vector_db = chroma(
    collection_name = ""local-rag"",
    persist_directory = ""./db/db_nomic"",
    embedding_function = embedding_model
)
# merge documents from each chapter database into main_db
for path in chapter_paths:
    chapter_db = chroma(
        collection_name = ""local-rag"",
        persist_directory=path,
        embedding_function=embedding_model
    )
    
    # retrieve all documents (vectors) from the current chapter database
    chapter_data = chapter_db.get()    
    
    # extract documents and metadatas
    docs = chapter_data['documents']
    metadatas = chapter_data['metadatas']
        
    vector_db.add_texts(texts=docs, metadatas=metadatas)

print(""documents successfully merged into main database."")

i'm expecting to create a vector database using chromadb to store the whole pdf (246 pages)","['chatbot', 'langchain', 'chromadb', 'ollama', 'rag']",79150319,"these steps solved my issue:

created a virtual environment
moved all the code from jupyter notebook to a python file
installed necessary dependencies with pip
ran the python file

as the problem was solved by fresh installation of the dependencies, most probably i faced the issue because of some internal dependency conflict.",https://stackoverflow.com/questions/79131873,chatbot,28-10-2024 01:53,605.0,0.0,2.0,True,02-11-2024 09:12,28-10-2024 07:04
74686939,creating rasa chatbot with some specific conditions,"i want to create a chatbot where,
i have a branch of questions and answers like,
q1. how are you
a1. - fine.
    - good.
q2. do you like travelling?
a2  - yes
    - absolutely
    - i like it.
q3. did you ever use rasa chatbot?
a3  - yes
    - almost 2 years.

now,

if i ask questions to the chatbot but miss some questions like forget to ask q2, the chatbot will say that end of the conversion that i miss that questions,

after the chatbot say i miss some questions have to go throw all the questions from start again but,

when i ask those questions again the chatbot will give different answers than it gives 1st time.
example: 1st time when i ask q1 and q3 it gives answers -fine and -yes but when 2nd time i go throw the process for q1 and q3 it has to give different answers than -fine and -yes.


now my questions are,

is it possible to make a chatbot with rasa where it will follow the three conditions up there?
if not then is any sort of alternative?","['python', 'nlp', 'chatbot', 'rasa', 'rasa-core']",74742706,"yes, all three conditions are possible.
please correct me as i understood your question,
i am adding follow solution as per the below understanding:
you want your assistant to ask how are you? and end-user answers fine/good,
and so as q2 and q3 do.
solution
for this to work, you have to use forms in rasa. below is a simple example for using this functionality.

make sure you have form for these questions and slots added.(facing issue, following this tutorial)
go to your actions.py
add validations for each slot you have. here, you will find just one example here as an example.

import os
from pathlib import path
from typing import any, text, dict, list

from rasa_sdk import action, tracker, formvalidationaction
from rasa_sdk.executor import collectingdispatcher

from utils.utils import get_html_data, send_email


class validatecontactusform(formvalidationaction):
    def name(self) -> text:
        return ""validate_contact_us_form""

    def validate_name(
        self,
        value: text,
        dispatcher: ""collectingdispatcher"",
        tracker: ""tracker"",
        domain: ""domaindict"",
    ) -> dict[str, str]:
        if value in [""fine"", ""good""]:
            return {""name"": value}
        else:
            # return {""name"": none, ""requested_slot"": ""name""}
            return {""requested_slot"": ""name""} # to request the same slot again, 
            # assistant will ask same question again.


note: return {""name"": none, ""requested_slot"": ""name""} this will close the form and restart the whole form, in this case, the form has just one slot, if you have more slots you have to nullify them too.",https://stackoverflow.com/questions/74686939,python,05-12-2022 10:48,389.0,0.0,1.0,True,09-12-2022 11:48,05-12-2022 11:30
10700289,is this cyk parser result correct?,"i am trying to learn the cyk parsing algorithm.
for this set of grammar rules, are the resulting tables correct for the two given sentences?
s -> np vp
vp -> vb np
np -> dt nn
pp -> in np
np -> np pp
np -> nn
vp -> vp pp
in -> with
nn -> dog
nn -> cat
vb -> ate
nn -> mouse
dt -> the


['s']
[none, none]
[none, none, 'vp']
['np', none, none, 'np']
['dt', 'nn', 'vb', 'dt', 'nn']
['the', 'cat', 'ate', 'the', 'dog']


['s']
['np', none]
['np', none, 'vp']
['np', none, none, 'np']
[none, none, 'vp', none, none]
[none, none, 'vp', none, none, 'pp']
['np', none, none, 'np', none, none, 'np']
['dt', 'nn', 'vb', 'dt', 'nn', 'in', 'dt', 'nn']
['the', 'cat', 'ate', 'the', 'dog', 'with', 'the', 'cat']","['nlp', 'compiler-construction', 'dynamic-programming', 'chomsky-normal-form', 'cyk']",38434201,"you can try to minimize your grammar first, because there are some unnecessary rules and furthermore that's why it is not in cnf. 
looking at it more concisely, you happen to have none on the first example, second row, second column. there it is actually possible to have a s, but since the logic in cyk cannot do further optimizations such as np->nn. from there s -> np vp for the mentioned none cell goes missing. because of cyk's inability to perform those, the grammar must be in cnf. so, basically  it is roughly like you are trying to apply a c-compiler on a c++ programm (with no c++ libraries). and you got lucky to even get the right result at the top.
with that being said, i am not going to indulge in the second example of yours. 
just to clarify, a grammar is in cnf if it has rules only of these two forms:

s -> ab
  a -> a

so clearly something like np -> nn is not in cnf.",https://stackoverflow.com/questions/10700289,nlp,22-05-2012 10:32,688.0,2.0,2.0,True,10-12-2021 12:13,10-12-2021 12:13
70502457,do i need to do any text cleaning for spacy ner?,"i am new to ner and spacy. trying to figure out what, if any, text cleaning needs to be done. seems like some examples i've found trim the leading and trailing whitespace and then muck with the start/stop indexes. i saw one example where the guy did a bunch of cleaning and his accuracy was really bad because all the indexes were messed up.
just to clarify, the dataset was annotated with dataturks, so you get json like this:
        ""content"": <original text>
        ""label"": [
            ""skills""
        ],
        ""points"": [
            {
                ""start"": 1295,
                ""end"": 1621,
                ""text"": ""\nï¿½ï¿½ï¿½ programming language...

so by ""mucking with the indexes"", i mean, if you strip off the leading \n, you need to update the start index, so it's still aligned properly.
so that's really the question, ifrt removing characters from the beginning, end or middle, i need to apply the rule to the content attribute and adjust start/end indexes to match, no? i'm guessing an obvious ""yes"" :), so i was wondering how much cleaning needs to be done.
so you would remove the \ns, bullets, leading / trailing whitespace, but leave standard punctuation like commas, periods, etc?
what about stuff like lowercasing, stop words, lemmatizing, etc?
one concern i'm seeing with a few samples i've looked at, is the start/stop indexes do get thrown off by the cleaning they do because you kind of need to update every annotation as you remove characters to keep them in sync.
i.e.
a 0 -> 100
b 101 -> 150

if i remove a char at position 50, then i need to adjust b to 100 -> 149.","['python', 'spacy', 'named-entity-recognition']",70502883,"first, spacy does no transformation of the input - it takes it literally as-is and preserves the format. so you don't lose any information when you provide text to spacy.
that said, input to spacy with the pretrained pipelines will work best if it is in natural sentences with no weird punctuation, like a newspaper article, because that's what spacy's training data looks like.
to that end, you should remove meaningless white space (like newlines, leading and trailing spaces) or formatting characters (maybe a line of ----?), but that's about all the cleanup you have to do. the spacy training data won't have bullets, so they might get some weird results, but i would leave them in to start. (also, bullets are obviously printable characters - maybe you mean non-ascii?)
i have no idea what you mean by ""muck with the indexes"", but for some older nlp methods it was common to do more extensive preprocessing, like removing stop words and lowercasing everything. doing that will make things worse with spacy because it uses the information you are removing for clues, just like a human reader would.
note that you can train your own models, in which case they'll learn about the kind of text you show them. in that case you can get rid of preprocessing entirely, though for actually meaningless things like newlines / leading and following spaces you might as well remove them anyway.

to address your new info briefly...
yes, character indexes for ner labels must be updated if you do preprocessing. if they aren't updated they aren't usable.
it looks like you're trying to extract ""skills"" from a resume. that has many bullet point lists. the spacy training data is newspaper articles, which don't contain any lists like that, so it's hard to say what the right thing to do is. i don't think the bullets matter much, but you can try removing or not removing them.

what about stuff like lowercasing, stop words, lemmatizing, etc?

i already addressed this, but do not do this. this was historically common practice for nlp models, but for modern neural models, including spacy, it is actively unhelpful.",https://stackoverflow.com/questions/70502457,python,28-12-2021 03:15,2605.0,7.0,1.0,True,28-12-2021 11:42,28-12-2021 11:42
72112204,"getting a valueerror: shapes (none, 1) and (none, 5) are incompatible","x_train = df_train[""base_reviews""].values
x_test  = df_test[""base_reviews""].values

y_train = df_train['category'].values
y_test  = df_test['category'].values

num_words = 20000 #max. workds to use per toxic comment
max_features = 15000 #max. number of unique words in embeddinbg vector
max_len = 200 #max. number of words per toxic comment to be use
embedding_dims = 128 #embedding vector output dimension 
num_epochs = 5 # (before 5)number of epochs (number of times that the model is exposed to the training dataset)
val_split = 0.2
batch_size2 = 256

tokenizer = tokenizer = tokenizer(num_words = num_words, lower = false)
tokenizer.fit_on_texts(list(x_train))


x_train = tokenizer.texts_to_sequences(x_train)
x_test = tokenizer.texts_to_sequences(x_test)
x_train = sequence.pad_sequences(x_train, max_len)
x_test  = sequence.pad_sequences(x_test,  max_len)
print('x_train shape:', x_train.shape)
print('x_test shape: ', x_test.shape)

and this is the shape of our dataset: x_train shape: (11419, 200), x_test shape:  (893, 200)
x_tra, x_val, y_tra, y_val = train_test_split(x_train, y_train, train_size =0.8, random_state=233)
early = earlystopping(monitor=""val_loss"", mode=""min"", patience=4)

nn_model = sequential([
    embedding(input_dim=max_features, input_length=max_len, output_dim=embedding_dims),
    globalmaxpool1d(),
    dense(50, activation = 'relu'),
    dropout(0.2),
    dense(5, activation = 'softmax')
])

def mean_pred(y_true, y_pred):
return k.mean(y_pred)
nn_model.compile(loss=""categorical_crossentropy"", optimizer=adam(0.01), metrics=['accuracy', mean_pred, fmeasure, precision, auroc, recall])

when i run the below code i get the above error.
nn_model.compile(loss=""categorical_crossentropy"", optimizer=adam(0.01), metrics=['accuracy', mean_pred, fmeasure, precision, auroc, recall])

when i feed the data to nn model the above error i get. how can i resolve the error? this is the error:
valueerror                               


traceback (most recent call last)
<ipython-input-51-a3721a91aa0b> in <module>
----> 1 nn_model_fit = nn_model.fit(x_tra, y_tra, batch_size=batch_size2, epochs=num_epochs, validation_data=(x_val, y_val), callbacks=[early])

~\anaconda3\lib\site-packages\keras\utils\traceback_utils.py in error_handler(*args, **kwargs)
     65     except exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67       raise e.with_traceback(filtered_tb) from none
     68     finally:
     69       del filtered_tb

~\anaconda3\lib\site-packages\tensorflow\python\framework\func_graph.py in autograph_handler(*args, **kwargs)
   1145           except exception as e:  # pylint:disable=broad-except
   1146             if hasattr(e, ""ag_error_metadata""):
-> 1147               raise e.ag_error_metadata.to_exception(e)
   1148             else:
   1149               raise

valueerror: in user code:
**valueerror: shapes (none, 1) and (none, 5) are incompatible**","['python', 'tensorflow', 'keras', 'text-classification', 'multiclass-classification']",72124041,"you have to map your labels to integer values:
import numpy as np

labels_index = dict(zip([""issue"", ""supporting"", ""decision"", ""neutral"", ""attacking""], np.arange(5)))

y_train = [labels_index[y] for y in y_train]",https://stackoverflow.com/questions/72112204,python,04-05-2022 11:29,83.0,1.0,1.0,True,05-05-2022 08:34,05-05-2022 08:34
69408535,how to assign new observations to cluster using distance matrix and kmedoids?,"i have a dataframe that holds the word mover's distance between each document in my dataframe. i am running kmediods on this to generate clusters.
       1      2     3      4      5   
  1  0.00   0.05  0.07   0.04   0.05
  2  0.05   0.00  0.06   0.04   0.05
  3. 0.07   0.06  0.00   0.06   0.06
  4  0.04   0.04. 0.06   0.00   0.04
  5  0.05   0.05  0.06   0.04   0.00

  kmed = kmedoids(n_clusters= 3, random_state=123, method  ='pam').fit(distance)

after running on this initial matrix and generating clusters, i want to add new points to be clustered. after adding a new document to the distance matrix i end up with:
       1      2     3      4      5      6
  1  0.00   0.05  0.07   0.04   0.05   0.12
  2  0.05   0.00  0.06   0.04   0.05   0.21 
  3. 0.07   0.06  0.00   0.06   0.06   0.01
  4  0.04   0.04. 0.06   0.00   0.04   0.05
  5  0.05   0.05  0.06   0.04   0.00   0.12
  6. 0.12   0.21  0.01   0.05   0.12   0.00

i have tried using kmed.predict on the new row.
kmed.predict(new_distance.loc[-1: ])

however, this gives me an error of incompatible dimensions x.shape[1] == 6 while y.shape[1] == 5.
how can i use this distance of the new document to determine which cluster it should be a part of? is this even possible, or do i have to recompute clusters every time? thanks!","['python', 'nlp', 'cluster-analysis', 'distance', 'k-means']",69440014,"the source code for k-medoids says the following:
def transform(self, x):
    """"""transforms x to cluster-distance space.

    parameters
    ----------
    x : {array-like, sparse matrix}, shape (n_query, n_features), \
            or (n_query, n_indexed) if metric == 'precomputed'
        data to transform.
   """"""

i assume that you use the precomputed metric (because you compute the distances outside the classifier), so in your case n_query is the number of new documents, and n_indexed is the number of the documents for which the fit method was called.
in your particular case when you fit the model on 5 documents and then want to classify the 6'th one, the x for classification should have shape (1,5), that can be computed as
kmed.predict(new_distance.loc[-1: , :-1])",https://stackoverflow.com/questions/69408535,python,01-10-2021 15:36,628.0,4.0,2.0,True,11-10-2021 12:38,11-10-2021 12:38
74198404,torch filter multidimensional tensor by start and end values,"i have a list of sentences and i am looking to extract contents between two items.
if the start or end item does not exist, i want it to return a row with padding only.
i already have the sentences tokenized and padded with 0 to a fixed length.
i figured a way to do this using for loops, but it is extremely slow, so would like to
know what is the best way to solve this, probably by using tensor operations.
import torch
start_value, end_value = 4,9

data = torch.tensor([
[3,4,7,8,9,2,0,0,0,0], 
[1,5,3,4,7,2,8,9,10,0],
[3,4,7,8,10,0,0,0,0,0], # does not contain end value
[3,7,5,9,2,0,0,0,0,0], # does not contain start value
])

# expected output
[
[7,8,0,0,0,0,0,0,0,0],
[7,2,8,0,0,0,0,0,0,0],
[0,0,0,0,0,0,0,0,0,0],
[0,0,0,0,0,0,0,0,0,0],
]
# or 
[
[0,0,7,8,0,0,0,0,0,0], 
[0,0,0,0,7,2,8,0,0,0],
[0,0,0,0,0,0,0,0,0,0], 
[0,0,0,0,0,0,0,0,0,0], 
]

the current solution that i have, which uses a for loop. it does not produce a symmetric array like i want in the expected output.
def _get_part_from_tokens(
    self,
    data: torch.tensor,
    s_id: int,
    e_id: int,
) -> list[str]:
    input_ids = []
    for row in data:
        try:
            s_index = (row == s_id).nonzero(as_tuple=true)[0][0]
            e_index = (row == e_id).nonzero(as_tuple=true)[0][0]
        except indexerror:
            input_ids.append(torch.tensor([]))
            continue
        if s_index is none or e_index is none or s_index > e_index:
            input_ids.append(torch.tensor([]))
            continue
        ind = torch.arange(s_index + 1, e_index)
        input_ids.append(row.index_select(0, ind))
    return input_ids","['python', 'pytorch', 'nlp', 'tensor']",74248425,"a possible loop-free approach is this:
import torch

# using the provided sample data
start_value, end_value = 4,9
data = torch.tensor([
    [3,4,7,8,9,2,0,0,0,0], 
    [1,5,3,4,7,2,8,9,10,0],
    [3,4,7,8,10,0,0,0,0,0], # does not contain end value
    [3,7,5,9,2,0,0,0,0,0], # does not contain start value
    [3,7,5,8,2,0,0,0,0,0], # does not contain start or end value
])


first, check which rows contain only a start_value or an end_value and fill these rows with 0.
# fill 'invalid' rows with 0
starts = (data == start_value)
ends = (data == end_value)
invalid = ((starts.sum(axis=1) - ends.sum(axis=1)) != 0)
data[invalid] = 0

then set the values up to (and including) the start_value and after (and including) the end_value to 0 in each row. this step targets mainly the 'valid' rows. nevertheless, all other rows will (again) be overwritten with zeros.
# set values in the start and end of 'valid rows' to 0
row_length = data.shape[1]
start_idx = starts.long().argmax(axis=1)
start_mask = (start_idx[:,none] - torch.arange(row_length))>=0
data[start_mask] = 0
end_idx = row_length - ends.long().argmax(axis=1)
end_mask = (end_idx[:,none] + torch.arange(row_length))>=row_length 
data[end_mask] = 0

note: this works also, if a row contains neither a start_value nor an end_value (i added such a row to the sample data). still, there are many more edge cases that one could think of (e.g. multiple start and end values in one row, start value after end value, ...). not sure if they are of relevance for the specific problem.

comparison of execution time
using timeit and randomly generated data to compare the execution time of the different approaches suggests, that the approach without loops is considerably faster than the approach from the question. if the data is converted to numpy first and converted back to pytorch afterwards some further (very minor) time savings are possible.

each dot (execution time) in the plot is the minimum value of 3 trials each with 100 repetitions.",https://stackoverflow.com/questions/74198404,python,25-10-2022 18:06,633.0,1.0,2.0,True,01-11-2022 14:49,31-10-2022 20:20
69715683,checking if words are within n space of one another (using nltk or otherwise) in python,"i have a list file contents that contains lists consisting of word tokens as its elements. i would like to create a function that takes two strings of length one and size as its input and returns instances where these two terms appear within size words of each other.
i have so far tokenized the words for each element in the list using nltk but i'm not sure where to go from here, could anyone refer me to an nltk method/python code that could do this?
here is the what the function should look like
file_contents = [['man', 'once', 'upon', 'time', 'love', 'princess'], ['python', 'code', 'cool', 'uses, 'java'],['man', 'help', 'test', 'weird', 'love'], .............]


def check_words_within(string: word1, string:word2, int: size) -> list:
                    #how to implement?


check_words_within('man','love', 4) would return [[man', 'once', 'upon', 'time', 'love'],['man', 'help', 'test', 'weird', 'love']]
check_words_within('man','upon', 1) would return [['man', 'once', 'upon']]
check_words_within('man','document',4) would return []
does nltk have a function to help me do this?","['python', 'nlp', 'nltk', 'tokenize']",69715762,"create a list of dicts to look up values from.*

dat = [{ind: val for val, ind in enumerate(el)} for el in file_contents]

def foo(w1, w2, dist, f, fdat):
    arr = []
    for i, v in enumerate(fdat):
        i1 = v.get(w1)
        i2 = v.get(w2)
        if (i1 is not none) and (i2 is not none) and (i2 - i1 <= dist + 1):
            arr.append(f[i][i1:i2+1])
    return arr

foo(""man"", ""upon"", 1, file_contents, dat)
# [['man', 'once', 'upon']]


create a class

class search:
    def __init__(self, words_list):
        self.__words_list = words_list
        self.__words_dict = self.__get_dict()        
    
    def __get_dict(self):
        d = {}
        for ind, arr in enumerate(self.__words_list):
            for pos, word in enumerate(arr):
                if not d.get(word):
                    d[word] = {}
                d[word][ind] = pos
        
        return d    
    
    def check_words_within(self, w1, w2, dist):
        arr = []
        if self.__words_dict.get(w1) and self.__words_dict.get(w2):
            wl_inds = self.__words_dict[w1].keys()
            for wl_ind in wl_inds:
                pos1 = self.__words_dict[w1][wl_ind]
                pos2 = self.__words_dict[w2].get(wl_ind, pos1 - 1)
                if (pos2 - pos1 > 0) and (pos2 - pos1 <= dist + 1):
                    arr.append(self.__words_list[wl_ind][pos1:pos2 + 1])
        
        return arr


foo = search(file_contents)
foo.check_words_within(""man"", ""love"", 4)
# [['man', 'once', 'upon', 'time', 'love'],
#  ['man', 'help', 'test', 'weird', 'love']]",https://stackoverflow.com/questions/69715683,python,25-10-2021 23:03,99.0,0.0,1.0,True,26-10-2021 16:02,25-10-2021 23:20
6709830,fuzzy string searching with whoosh in python,"i've built up a large database of banks in mongodb. i can easily take this information and create indexes with it in whoosh.  for example i'd like to be able to match the bank names 'eagle bank & trust co of missouri' and 'eagle bank and trust company of missouri'. the following code works with simple fuzzy such, but cannot achieve a match on the above:
from whoosh.index import create_in
from whoosh.fields import *

schema = schema(name=text(stored=true))
ix = create_in(""indexdir"", schema)
writer = ix.writer()

test_items = [u""eagle bank and trust company of missouri""]

writer.add_document(name=item)
writer.commit()

from whoosh.qparser import queryparser
from whoosh.query import fuzzyterm

with ix.searcher() as s:
    qp = queryparser(""name"", schema=ix.schema, termclass=fuzzyterm)
    q = qp.parse(u""eagle bank & trust co of missouri"")
    results = s.search(q)
    print results

gives me: 
<top 0 results for and([fuzzyterm('name', u'eagle', boost=1.000000, minsimilarity=0.500000, prefixlength=1), fuzzyterm('name', u'bank', boost=1.000000, minsimilarity=0.500000, prefixlength=1), fuzzyterm('name', u'trust', boost=1.000000, minsimilarity=0.500000, prefixlength=1), fuzzyterm('name', u'co', boost=1.000000, minsimilarity=0.500000, prefixlength=1), fuzzyterm('name', u'missouri', boost=1.000000, minsimilarity=0.500000, prefixlength=1)]) runtime=0.00166392326355>

is it possible to achieve what i want with whoosh? if not what other python based solutions do i have?","['python', 'information-retrieval', 'fuzzy-search', 'whoosh']",30502661,"you could match co with company using fuzzy search in whoosh but you shouldn't do  because the difference between co and company is large. co is similar to company as be is similar to beast and ny to company, you can imagine how bad and how large will be the search results.
however, if you want to match compan or compani or companee to company you could do it by using a personalized class of fuzzyterm with default maxdist equal to 2 or more :

maxdist ï¿½ï¿½ï¿½ the maximum edit distance from the given text.

class myfuzzyterm(fuzzyterm):
     def __init__(self, fieldname, text, boost=1.0, maxdist=2, prefixlength=1, constantscore=true):
         super(myfuzzyterm, self).__init__(fieldname, text,, maxdist, prefixlength, constantscore)

then:
 qp = queryparser(""name"", schema=ix.schema, termclass=myfuzzyterm)

you could match co with company by setting maxdist to 5 but this as i said give bad search results. i suggest to keep maxdist from 1 to 3.  
if you are looking for matching a word linguistic variations, you better use whoosh.query.variations. 
note: older whoosh versions has minsimilarity instead of maxdist.",https://stackoverflow.com/questions/6709830,python,15-07-2011 15:55,7220.0,13.0,5.0,True,21-06-2022 18:43,05-11-2015 21:15
58215855,"how to get full list of pos, tag, and dep in spacy?","the pos, tag, and dep values used in spacy are common ones of nlp, but i believe there are some differences depending on the corpus database.
for example, universal dependencies contributors has listed 37 syntactic dependencies. does spacy use all of these 37 dependencies? and nothing more?
is there a command to output all pos, tag, and dep values, spacy may provide?","['nlp', 'spacy']",58220014,"a specific pipeline component can show its labels:
nlp = spacy.load('en')
nlp.get_pipe(""tagger"").labels
# ('$', ""''"", ',', '-lrb-', '-rrb-', '.', ':', 'add', 'afx', 'cc', 'cd', 'dt', 'ex', 'fw', 'hyph', 'in', 'jj', 'jjr', 'jjs', 'ls', 'md', 'nfp', 'nn', 'nnp', 'nnps', 'nns', 'pdt', 'pos', 'prp', 'prp$', 'rb', 'rbr', 'rbs', 'rp', 'sym', 'to', 'uh', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'vbz', 'wdt', 'wp', 'wp$', 'wrb', 'xx', '_sp', '``')

spacy.explain(label) will give a short definition for most labels, e.g.:
spacy.explain(""nn"")
# 'noun, singular or mass'
spacy.explain(""person"")
# 'people, including fictional'

the detailed documentation is here:",https://stackoverflow.com/questions/58215855,nlp,03-10-2019 09:10,9614.0,3.0,5.0,True,27-04-2023 14:15,03-10-2019 11:52
479825,problem trimming japanese string in java,"i have the following string (japanese) ""ýýýýýýýýýýýýýýýýýý"" , the first character is ""like"" whitespace but its number in unicode is 12288, so if i do ""ýýýýýýýýýýýýýýýýýý"".trim() i get the same string (trim doesn't work).
if i do trim in c++ it works ok.
does anyone know how to solve this issue in java?
is there a special trim method for unicode?","java, string, nlp",479946,"as an alternative to the stringutils class mentioned by mike, you can also use a unicode-aware regular expression, using only java's own libraries:
""ýýýýýýýýýýýýýýýýýý"".replaceall(""\\p{z}"", """")

or, to really only trim, and not remove whitespace inside the string:
""ýýýýýýýýýýýý ýýýýýý "".replaceall(""(^\\p{z}+|\\p{z}+$)"", """")",https://stackoverflow.com/q/479825,"java, string, nlp",26-01-2009 13:39,6023.0,5.0,6.0,True,03-04-2023 10:26,28-01-2009 00:06
65549200,error in installing spacy en_core_web_lg on heroku app,"am deploying my ml model on heroku using django, i need en_core_web_lg for my application but couldn't install it
my requirements.txt is like:
..
..
djangorestframework==3.12.2
en-core-web-lg==2.3.1
en-core-web-sm==2.3.1
gunicorn==20.0.4
heroku==0.1.4
..


the error is:
 error: could not find a version that satisfies the requirement en-core-web-lg==2.3.1 (from -r /tmp/build_c3075f3c_/requirements.txt (line 14)) (from versions: none)
       error: no matching distribution found for en-core-web-lg==2.3.1 (from -r /tmp/build_c3075f3c_/requirements.txt (line 14))
 !     push rejected, failed to compile python app.
 !     push failed","['python', 'django', 'heroku', 'spacy']",65550922,"i don't know why it is not installing direct from requirements.txt file. there is another way it is not right way but the work will go on.
first you have to remove the package from requirements.txt file  for which the error is coming.
then push the app on heroku, once your app have come to heroku then write this code either from the terminal or from the heroku dashboard.
if you are using terminal then:
heroku run bash

then run:
pip install spacy

and then install which requiremts you want from spacy
python -m spacy download en_core_web_lg

if you are using heroku dashboard then :
first go to your heroku dashboard click on your app and then at top right click on more and select the run console
example image:

than this interface will come up

in this you have to click on bash or type bash and run
after this you can put same cammand and install.",https://stackoverflow.com/questions/65549200,python,03-01-2021 11:17,3448.0,2.0,2.0,True,18-06-2022 17:09,03-01-2021 11:53
60142937,huggingface transformers for text generation with ctrl with google colab&#39;s free gpu,"i wanted to test textgeneration with ctrl using pytorch-transformers, before using it for fine-tuning. but it doesn't prompt anything like it does with gpt-2 and other similar language generation models. i'm very new for this and am stuck and can't figure out what's going on.
this is the procedure i followed in my colab notebook,
!pip install transformers

!git clone 

!python pytorch-transformers/examples/run_generation.py \
    --model_type=ctrl \
    --length=100 \
    --model_name_or_path=ctrl \
    --temperature=0.2 \
    --repetition_penalty=1.2 \

and this is what i get after running the script
02/10/2020 01:02:31 - info - transformers.tokenization_utils -   loading file  from cache at /root/.cache/torch/transformers/a858ad854d3847b02da3aac63555142de6a05f2a26d928bb49e881970514e186.285c96a541cf6719677cfb634929022b56b76a0c9a540186ba3d8bbdf02bca42
02/10/2020 01:02:31 - info - transformers.tokenization_utils -   loading file  from cache at /root/.cache/torch/transformers/aa2c569e6648690484ade28535a8157aa415f15202e84a62e82cc36ea0c20fa9.26153bf569b71aaf15ae54be4c1b9254dbeff58ca6fc3e29468c4eed078ac142
02/10/2020 01:02:31 - info - transformers.configuration_utils -   loading configuration file  from cache at /root/.cache/torch/transformers/d6492ca334c2a4e079f43df30956acf935134081b2b3844dc97457be69b623d0.1ebc47eb44e70492e0c20494a084f108332d20fea7fe5ad408ef5e7a8f2baef4
02/10/2020 01:02:31 - info - transformers.configuration_utils -   model config ctrlconfig {
  ""architectures"": null,
  ""attn_pdrop"": 0.1,
  ""bos_token_id"": 0,
  ""dff"": 8192,
  ""do_sample"": false,
  ""embd_pdrop"": 0.1,
  ""eos_token_ids"": 0,
  ""finetuning_task"": null,
  ""from_tf"": false,
  ""id2label"": {
    ""0"": ""label_0""
  },
  ""initializer_range"": 0.02,
  ""is_decoder"": false,
  ""label2id"": {
    ""label_0"": 0
  },
  ""layer_norm_epsilon"": 1e-06,
  ""length_penalty"": 1.0,
  ""max_length"": 20,
  ""model_type"": ""ctrl"",
  ""n_ctx"": 512,
  ""n_embd"": 1280,
  ""n_head"": 16,
  ""n_layer"": 48,
  ""n_positions"": 50000,
  ""num_beams"": 1,
  ""num_labels"": 1,
  ""num_return_sequences"": 1,
  ""output_attentions"": false,
  ""output_hidden_states"": false,
  ""output_past"": true,
  ""pad_token_id"": 0,
  ""pruned_heads"": {},
  ""repetition_penalty"": 1.0,
  ""resid_pdrop"": 0.1,
  ""summary_activation"": null,
  ""summary_first_dropout"": 0.1,
  ""summary_proj_to_labels"": true,
  ""summary_type"": ""cls_index"",
  ""summary_use_proj"": true,
  ""temperature"": 1.0,
  ""top_k"": 50,
  ""top_p"": 1.0,
  ""torchscript"": false,
  ""use_bfloat16"": false,
  ""vocab_size"": 246534
}

02/10/2020 01:02:31 - info - transformers.modeling_utils -   loading weights file  from cache at /root/.cache/torch/transformers/c146cc96724f27295a0c3ada1fbb3632074adf87e9aef8269e44c9208787f8c8.b986347cbab65fa276683efbb9c2f7ee22552277bcf6e1f1166557ed0852fdf0
tcmalloc: large alloc 1262256128 bytes == 0x38b92000 @  0x7fe1900bdb6b 0x7fe1900dd379 0x7fe139843b4a 0x7fe1398455fa 0x7fe13bb7578a 0x7fe13bdbe30b 0x7fe13be05b37 0x7fe184c8cad5 0x7fe184c8d17b 0x7fe184c91160 0x7fe184ade496 0x551b15 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x5096b7 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50c5b9
tcmalloc: large alloc 1262256128 bytes == 0x19fdda000 @  0x7fe1900bdb6b 0x7fe1900dd379 0x7fe139843b4a 0x7fe1398455fa 0x7fe13bb7578a 0x7fe13bdbe30b 0x7fe13be05b37 0x7fe184c8cad5 0x7fe184c8d17b 0x7fe184c91160 0x7fe184ade496 0x551b15 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50d390 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5a067e 0x50d966 0x508245
^c

and then terminates. could this be because of a gpu problem?","['python', 'deep-learning', 'nlp', 'pytorch', 'huggingface-transformers']",60481050,"the solution was to increase the ram. since i was using the google colab's free gpu, i was going through this: github issue
and found this useful: solution
the following piece of code will crash the session in colab and select 'get more ram', which will increase the ram up to 25.51gb
d=[]
while(1):
  d.append('1')",https://stackoverflow.com/questions/60142937,python,10-02-2020 01:14,2670.0,4.0,1.0,True,10-12-2024 01:27,10-12-2024 01:27
78344850,how to tag words that not include one specific symbol in spacy?,"i'm trying to tag one word in spacy using regex, but i want to add one condition: it can't contain symbol '/' in any place inside. my code looks like this:
[{'lower': {""regex"": ""^.*(word).*?""}}]

i tried using ^ to exclude this but it didn't work.
so examples:

'subwordw' tagged: 'subword'
'subword/w' tagged nothing","['python', 'nlp', 'pattern-matching', 'spacy']",78348925,"try this:
{'lower': {'regex': ""^([^\/]*word[^\/]*)$""}}",https://stackoverflow.com/questions/78344850,python,18-04-2024 04:21,31.0,1.0,1.0,True,18-04-2024 16:12,18-04-2024 04:26
57340142,skkearn.tfidfvectorizer user warning: your stop_words may be inconsistent with your preprocessing,"i am following this document clustering tutorial. as an input i give a txt file which can be downloaded here. it's a combined file of 3 other txt files divided with a use of \n. after creating a tf-idf matrix i received this warning:
,,userwarning: your stop_words may be inconsistent with your preprocessing. 
tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.
  'stop_words.' % sorted(inconsistent))"". 

i guess it has something to do with the order of lemmatization and stop words removal, but as this is my first project in txt processing, i am a bit lost and i don't know how to fix this...
import pandas as pd
import nltk
from nltk.corpus import stopwords
import re
import os
import codecs
from sklearn import feature_extraction
import mpld3
from nltk.stem.snowball import snowballstemmer
from sklearn.feature_extraction.text import tfidfvectorizer


stopwords = stopwords.words('english')
stemmer = snowballstemmer(""english"")

def tokenize_and_stem(text):
    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token
    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]
    filtered_tokens = []
    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)
    for token in tokens:
        if re.search('[a-za-z]', token):
            filtered_tokens.append(token)
    stems = [stemmer.stem(t) for t in filtered_tokens]
    return stems


def tokenize_only(text):
    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token
    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]
    filtered_tokens = []
    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)
    for token in tokens:
        if re.search('[a-za-z]', token):
            filtered_tokens.append(token)
    return filtered_tokens


totalvocab_stemmed = []
totalvocab_tokenized = []
with open('shortresultlist.txt', encoding=""utf8"") as synopses:
    for i in synopses:
        allwords_stemmed = tokenize_and_stem(i)  # for each item in 'synopses', tokenize/stem
        totalvocab_stemmed.extend(allwords_stemmed)  # extend the 'totalvocab_stemmed' list
        allwords_tokenized = tokenize_only(i)
        totalvocab_tokenized.extend(allwords_tokenized)

vocab_frame = pd.dataframe({'words': totalvocab_tokenized}, index = totalvocab_stemmed)
print ('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')
print (vocab_frame.head())

#define vectorizer parameters
tfidf_vectorizer = tfidfvectorizer(max_df=0.8, max_features=200000,
                                 min_df=0.2, stop_words='english',
                                 use_idf=true, tokenizer=tokenize_and_stem, ngram_range=(1,3))

with open('shortresultlist.txt', encoding=""utf8"") as synopses:
    tfidf_matrix = tfidf_vectorizer.fit_transform(synopses) #fit the vectorizer to synopses

print(tfidf_matrix.shape)","['vectorization', 'text-processing', 'tf-idf', 'stop-words', 'stemming']",58304161,"i faced this problem because of pt-br language. 
tl;dr: remove the accents of your language.
# special thanks for the user humberto diogenes from python list (answer from aug 11, 2008)
# link: 

# i found the issue by chance (i swear, haha) but this guy gave the tip before me
# link: 

import spacy
nlp = spacy.load('pt_core_news_sm')

# define default stopwords list
stoplist = spacy.lang.pt.stop_words.stop_words

def replace_ptbr_char_by_word(word):
  """""" will remove the encode token by token""""""
    word = str(word)
    word = normalize('nfkd', word).encode('ascii','ignore').decode('ascii')
    return word

def remove_pt_br_char_by_text(text):
  """""" will remove the encode using the entire text""""""
    text = str(text)
    text = "" "".join(replace_ptbr_char_by_word(word) for word in text.split() if word not in stoplist)
    return text

df['text'] = df['text'].apply(remove_pt_br_char_by_text)

i put the solution and references in this gist.",https://stackoverflow.com/questions/57340142,vectorization,03-08-2019 16:23,23414.0,21.0,4.0,True,23-02-2025 01:17,23-02-2025 01:17
74345615,unity connecting to dall-e api when using image param?,"how would one use the dall-e text-to-image api's image and mask parameters in unity c#?
for background, something like the following works for the other parameters like prompt (full code on github):
string apimode = ""generations"";
string apiurl = "" + apimode;

unitywebrequest www = unitywebrequest.post(apiurl, """");
 ""application/json"");
 ""bearer "" + key);

string jsonstring = jsonconvert.serializeobject(aiparams, formatting.none, serializersettings);

 = new uploadhandlerraw(system.text.encoding.utf8.getbytes(jsonstring));
 = new downloadhandlerbuffer();
yield return 

when using apimode ""edits"" or ""variations"" though, as per dall-e's documentation, it will return an api error suggesting to switch to content-type ""multipart/form-data"". how would one use this and add the binary data for image or mask, given some byte[] for the png? thanks!","['c#', 'unity-game-engine', 'openai-api', 'text2image']",74345650,"unitywebrequest.post -> overload that takes list<imultipartformsection> will automatically use this content type header and there you can provide multipartformfilesection and multipartformdatasection sections.
the example is unfortunately useless but it would probably look somewhat like e.g.
 var jsonstring = jsonconvert.serializeobject(aiparams, formatting.none, serializersettings);
 var jsonbytes = encoding.utf8.getbytes(jsonstring);
 var formparts = new list<imultipartformsection>();
 formparts.add(new multipartformdatasection(""data"", jsonbytes, ""application/json""));
 formparts.add(new multipartformfilesection(""image"", yourfilecontentbytes, ""yourfilename.png"", ""image/png""));

 using (unitywebrequest www = unitywebrequest.post("" formparts))
 {
      ""bearer "" + key);
     yield return 

     if ( != unitywebrequest.result.success)
     {
         debug.log(
     }
     else
     {
         debug.log(""form upload complete!"");
     }
 }",https://stackoverflow.com/questions/74345615,c#,07-11-2022 11:07,743.0,0.0,1.0,True,02-03-2023 03:48,02-03-2023 03:48
78677155,connecting weaviate with langflow across docker containers,"i am trying to build a local rag application using langflow. for my vectore store, i want to use a local weaviate instance, hosted in a separate docker container on the same network. building the weaviate component in langflow fails, and throws an error:
error building component

    error building component weaviate:  port=8080): max retries exceeded with url: /v1/.well-known/openid-configuration (caused by newconnectionerror('<urllib3.connection. object at 0x7fb2a2db9fa0>: failed to establish a new connection: [errno 111] connection refused'))

my setup:
docker for desktop v4.31.1 on windows
compose file:
version: ""3.9""
name: proto_rag

services:
  langflow:
    image: langflowai/langflow:1.0-alpha
    ports:
      - ""7860:7860""
    depends_on:
      - postgres
    environment:
      - langflow_database_url=postgresql://langflow:langflow@postgres:5432/langflow
      # this variable defines where the logs, file storage, monitor data and secret keys are stored.
      - langflow_config_dir=app/langflow
    volumes:
      - langflow-data:/app/langflow

  postgres:
    image: postgres:16
    environment:
      postgres_user: langflow
      postgres_password: langflow
      postgres_db: langflow
    ports:
      - ""5432:5432""
    volumes:
      - langflow-postgres:/var/lib/postgresql/data

  weaviate:
    command:
    - --host
    - 0.0.0.0
    - --port
    - '8080'
    - --scheme
    - http
    image: cr.weaviate.io/semitechnologies/weaviate:1.25.4
    ports:
    - 8080:8080
    - 50051:50051
    volumes:
    - weaviate_data:/var/lib/weaviate
    restart: on-failure:0
    environment:    
      authentication_anonymous_access_enabled: 'true'
      disable_telemetry: 'true'
      persistence_data_path: '/var/lib/weaviate'
      default_vectorizer_module: 'text2vec-ollama'
      enable_modules: 'text2vec-ollama'
      cluster_hostname: 'node1'  

  ollama:
    volumes:
     - ollama:/root/.ollama
    ports:
     - 11434:11434
    container_name: ollama
    image: ollama/ollama

volumes:
  langflow-postgres:
  langflow-data:
  ollama:
  weaviate_data:

my weaviate component in langflow calls  and has the variable ""index name"" set to ""vector_store"".
connecting to ollama on localhost works, so it doesn't seem to be a network problem. however since authentication_anonymous_access_enabled is set true i can't see a reason why weaviate would refuse the connection.
i tried setting up users and api keys in the compose file and using the key in the langflow component, this had no effect.
[...]
      authentication_apikey_enabled: 'true'
      authentication_apikey_allowed_keys: 'key206339812'
      authentication_apikey_users: 'user206339812'
[...]

i also tried raising the query limit, which also had no effect.
      query_defaults_limit: 1000

this thread suggests to use the service/container name for reference instead of localhost, this failed aswell.","['docker-compose', 'langchain', 'weaviate']",78694581,"indeed, your error message is consistent with that thread's one. looks like your langflow is trying to connect to weaviate at 
considering you are using docker, this is not the place where langflow will find weaviate, but 
i have used your docker-compose, and was able to ingest data using this simple flow in langflow:

let me know if this helps.",https://stackoverflow.com/questions/78677155,docker-compose,27-06-2024 10:49,509.0,0.0,1.0,True,02-07-2024 00:55,27-06-2024 11:07
66561880,weights of pre-trained bert model not initialized,"i am using the language interpretability toolkit (lit) to load and analyze a bert model that i pre-trained on an ner task.
however, when i'm starting the lit script with the path to my pre-trained model passed to it, it fails to initialize the weights and tells me:
    modeling_utils.py:648] loading weights file bert_remote/examples/token-classification/data/models/results_21_03_04_cleaned_annotations/04.03._8_16_5e-5_cleaned_annotations/04-03-2021 (15.22.23)/pytorch_model.bin
    modeling_utils.py:739] weights of bertfortokenclassification not initialized from pretrained model: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']
    modeling_utils.py:745] weights from pretrained model not used in bertfortokenclassification: ['bert.embeddings.position_ids']


it then simply uses the bert-base-german-cased version of bert, which of course doesn't have my custom labels and thus fails to predict anything. i think it might have to do with pytorch, but i can't find the error.
if relevant, here is how i load my dataset into conll 2003 format (modification of the dataloader scripts found here):
    def __init__(self):

        # read conll test files

        self._examples = []

        data_path = ""lit_remote/lit_nlp/examples/datasets/ner_data""
        with open(os.path.join(data_path, ""test.txt""), ""r"", encoding=""utf-8"") as f:
            lines = f.readlines()

        for line in lines[:2000]:
            if line != ""\n"":
                token, label = line.split("" "")
                self._examples.append({
                    'token': token,
                    'label': label,
                })
            else:
                self._examples.append({
                    'token': ""\n"",
                    'label': ""o""
                })

    def spec(self):
        return {
            'token': lit_types.tokens(),
            'label': lit_types.sequencetags(align=""token""),
        }

and this is how i initialize the model and start the lit server (modification of the simple_pytorch_demo.py script found here):
    def __init__(self, model_name_or_path):
        self.tokenizer = transformers.autotokenizer.from_pretrained(
            model_name_or_path)
        model_config = transformers.autoconfig.from_pretrained(
            model_name_or_path,
            num_labels=15,  # fixme change
            output_hidden_states=true,
            output_attentions=true,
        )
        # this is a just a regular pytorch model.
        self.model = _from_pretrained(
            transformers.automodelfortokenclassification,
            model_name_or_path,
            config=model_config)
        self.model.eval()

## some omitted snippets here

    def input_spec(self) -> lit_types.spec:
        return {
            ""token"": lit_types.tokens(),
            ""label"": lit_types.sequencetags(align=""token"")
        }

    def output_spec(self) -> lit_types.spec:
        return {
            ""tokens"": lit_types.tokens(),
            ""probas"": lit_types.multiclasspreds(parent=""label"", vocab=self.labels),
            ""cls_emb"": lit_types.embeddings()","['tensorflow', 'nlp', 'pytorch', 'bert-language-model', 'huggingface-transformers']",70580076,"this actually seems to be expected behaviour. in the documentation of the gpt models the huggingface team writes:

this will issue a warning about some of the pretrained weights not being used and some weights being randomly initialized. thatï¿½ï¿½ï¿½s because we are throwing away the pretraining head of the bert model to replace it with a classification head which is randomly initialized.

so it seems to not be a problem for the fine-tuning. in my use case described above it worked despite the warning as well",https://stackoverflow.com/questions/66561880,tensorflow,10-03-2021 09:30,1367.0,0.0,1.0,True,04-01-2022 14:11,10-03-2021 09:56
78472764,langchain workaround for with_structured_output using chatbedrock,"i'm working with the langchain library to implement a document analysis application.  especifically i want to use the routing technique described in this documentation. i wanted to follow along the example, but my environment is restricted to aws, and i am using chatbedrock instead of chatopenai due to limitations with my deployment.
according to this overview the with_structured_output method, which i need, is not (yet) implemented for models on aws bedrock, which is why i am looking for a workaround or any method to replicate this functionality.
the key functionality i am looking for is shown in this example:
from typing import list
from typing import literal

from langchain_core.prompts import chatprompttemplate
from langchain_core.pydantic_v1 import basemodel, field
from langchain_openai import chatopenai



class routequery(basemodel):
    """"""route a user query to the most relevant datasource.""""""

    datasources: list[literal[""python_docs"", ""js_docs"", ""golang_docs""]] = field(
        ...,
        description=""given a user question choose which datasources would be most relevant for answering their question"",
    )

system = """"""you are an expert at routing a user question to the appropriate data source.

based on the programming language the question is referring to, route it to the relevant data source.""""""
prompt = chatprompttemplate.from_messages(
    [
        (""system"", system),
        (""human"", ""{question}""),
    ]
)

llm = chatopenai(model=""gpt-3.5-turbo-0125"", temperature=0)
structured_llm = llm.with_structured_output(routequery)
router = prompt | structured_llm
router.invoke(
    {
        ""question"": ""is there feature parity between the python and js implementations of openai chat models""
    }
)

the output would be:
routequery(datasources=['python_docs', 'js_docs'])

the most important fact for me is that it just selects items from the list without any additional overhead, which makes it possible to setup the right follow up questions.
did anyone find a workaround how to resolve this issue?","['python', 'langchain', 'amazon-bedrock', 'claude']",78547931,"i found a solution in these two blog posts: here and here.
the key is to use the instructor package, which is a wrapper around pydantic. this means langchain is not necessary.
here is an example based on the blog posts:
from typing import list
import instructor
from anthropic import anthropicbedrock
from loguru import logger
from pydantic import basemodel
import enum

class user(basemodel):
    name: str
    age: int

class multilabels(str, enum.enum):
    tech_issue = ""tech_issue""
    billing = ""billing""
    general_query = ""general_query""

class multiclassprediction(basemodel):
    """"""
    class for a multi-class label prediction.
    """"""
    class_labels: list[multilabels]

if __name__ == ""__main__"":
    # initialize the instructor client with anthropicbedrock configuration
    client = instructor.from_anthropic(
        anthropicbedrock(
            aws_region=""eu-central-1"",
        )
    )

    logger.info(""hello world example"")

    # create a message and extract user data
    resp = client.messages.create(
        model=""anthropic.claude-instant-v1"",
        max_tokens=1024,
        messages=[
            {
                ""role"": ""user"",
                ""content"": ""extract jason is 25 years old."",
            }
        ],
        response_model=user,
    )

    print(resp)
    logger.info(""classification example"")

    # classify a support ticket
    text = ""my account is locked and i can't access my billing info.""

    _class = client.chat.completions.create(
        model=""anthropic.claude-instant-v1"",
        max_tokens=1024,
        response_model=multiclassprediction,
        messages=[
            {
                ""role"": ""user"",
                ""content"": f""classify the following support ticket: {text}"",
            },
        ],
    )

    print(_class)",https://stackoverflow.com/questions/78472764,python,13-05-2024 13:56,3333.0,3.0,2.0,True,19-12-2024 17:16,25-06-2024 07:48
70619634,error(s) in loading state_dict for robertaforsequenceclassification,"i am using a fine-tuned roberta model that is unbiased-toxic-roberta trained on jigsaw data:

it is fine-tuned on 16 classes.
i am writing my code for binary classification:
metrics to calculate loss on binary labels as accuracy
def compute_metrics(eval_pred):
    
    logits, labels = eval_pred
   

    predictions = np.argmax(logits, axis=-1)
    
    acc = np.sum(predictions == labels) / predictions.shape[0]
    
    return {""accuracy"" : acc}

import torch.nn as nn
model = tr.robertaforsequenceclassification.from_pretrained(""/home/pc/unbiased_toxic_roberta"",num_labels=2)

model.to(device)



training_args = tr.trainingarguments(
#     report_to = 'wandb',
    output_dir='/home/pc/1_proj_hate_speech/results_roberta',          # output directory
    overwrite_output_dir = true,
    num_train_epochs=20,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=32,   # batch size for evaluation
    learning_rate=2e-5,
    warmup_steps=1000,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs3',            # directory for storing logs
    logging_steps=1000,
    evaluation_strategy=""epoch""
    ,save_strategy=""epoch""
    ,load_best_model_at_end=true
)


trainer = tr.trainer(
    model=model,                         # the instantiated ï¿½ï¿½ï¿½ï¿½ transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_data,         # training dataset
    eval_dataset=val_data,             # evaluation dataset
    compute_metrics=compute_metrics

)
when i run this, i get an error:
loading weights file /home/pc/unbiased_toxic_roberta/pytorch_model.bin
runtimeerror: error(s) in loading state_dict for robertaforsequenceclassification:
    size mismatch for classifier.oweight: copying a param with shape torch.size([16, 768]) from checkpoint, the shape in current model is torch.size([2, 768]).
    size mismatch for classifier.out_proj.bias: copying a param with shape torch.size([16]) from checkpoint, the shape in current model is torch.size([2]).

how can i add a linear layer and solve this error ?","['python-3.x', 'pytorch', 'huggingface-transformers', 'roberta']",70625621,"load with ignore_mismatched_sizes=true:
model = tr.robertaforsequenceclassification.from_pretrained(
    ""/home/pc/unbiased_toxic_roberta"",
    num_labels=2,
    ignore_mismatched_sizes=true)

then you can finetune the model.",https://stackoverflow.com/questions/70619634,python-3.x,07-01-2022 10:11,2088.0,2.0,1.0,True,07-01-2022 18:25,07-01-2022 11:28
37700765,delete hyphen (special chars) while processing text in rapidminer,"i would like to delete all hyphens in the text document which i analyze in rapidminer. for that i use operator ""process documents from files"" to analyze large pdf-files. each file contains a lot of hyphens which i would like to delete before i'll tokenize the text into pieces (non letters). i've used operator ""replace token"". with it i can replace hyphens with other symbols, but i cannot replace them with nothing or empty string("" ""). i've tried also to use my own customized dictionary of stopwords(non-letters, -). this operator does no work at all. i've saved my dictionary containing the chars and words i want to delete as a text file (each in the new line). can anybody help on this issue?","['replace', 'text-mining', 'rapidminer']",37710943,"you can use replace tokens with the following parameters. 
replace what ()[-] 
replace by $1
it's a bit of a hack but it works because the first capturing group between the brackets will always be empty and the whole regular expression will match a single hyphen. the $1 is the result of the first capture group and it's always empty.
here's an example process that shows this working.
<?xml version=""1.0"" encoding=""utf-8"" standalone=""no""?>
<process version=""7.0.000"">
  <context>
    <input/>
    <output/>
    <macros/>
  </context>
  <operator activated=""true"" class=""process"" compatibility=""7.0.000"" expanded=""true"" name=""process"">
    <process expanded=""true"">
      <operator activated=""true"" class=""text:create_document"" compatibility=""7.0.000"" expanded=""true"" height=""68"" name=""create document"" width=""90"" x=""246"" y=""187"">
        <parameter key=""text"" value=""some text &#10;with&#10;some-text-with-hyphens-in&#10;hyphens in&#10;""/>
      </operator>
      <operator activated=""true"" class=""text:replace_tokens"" compatibility=""7.0.000"" expanded=""true"" height=""68"" name=""replace tokens"" width=""90"" x=""447"" y=""187"">
        <list key=""replace_dictionary"">
          <parameter key=""()[-]"" value=""$1""/>
        </list>
      </operator>
      <operator activated=""true"" class=""text:process_documents"" compatibility=""7.0.000"" expanded=""true"" height=""103"" name=""process documents"" width=""90"" x=""648"" y=""187"">
        <parameter key=""vector_creation"" value=""term occurrences""/>
        <process expanded=""true"">
          <operator activated=""true"" class=""text:tokenize"" compatibility=""7.0.000"" expanded=""true"" height=""68"" name=""tokenize"" width=""90"" x=""179"" y=""85""/>
          <connect from_port=""document"" to_op=""tokenize"" to_port=""document""/>
          <connect from_op=""tokenize"" from_port=""document"" to_port=""document 1""/>
          <portspacing port=""source_document"" spacing=""0""/>
          <portspacing port=""sink_document 1"" spacing=""0""/>
          <portspacing port=""sink_document 2"" spacing=""0""/>
        </process>
      </operator>
      <connect from_op=""create document"" from_port=""output"" to_op=""replace tokens"" to_port=""document""/>
      <connect from_op=""replace tokens"" from_port=""document"" to_op=""process documents"" to_port=""documents 1""/>
      <connect from_op=""process documents"" from_port=""example set"" to_port=""result 1""/>
      <portspacing port=""source_input 1"" spacing=""0""/>
      <portspacing port=""sink_result 1"" spacing=""0""/>
      <portspacing port=""sink_result 2"" spacing=""0""/>
    </process>
  </operator>
</process>

hope that helps as a basis.",https://stackoverflow.com/questions/37700765,replace,08-06-2016 11:06,758.0,1.0,1.0,True,03-07-2022 05:45,03-07-2022 05:45
62538079,hugginface transformers module not recognized by anaconda,"i am using anaconda, python 3.7, windows 10.
i tried to install transformers by  on my env.
i am aware that i must have either pytorch or tf installed, i have pytorch installed - as seen in anaconda navigator environments.
i would get many kinds of errors, depending on where (anaconda / prompt) i uninstalled and reinstalled pytorch and transformers. last attempt using
conda install pytorch torchvision cpuonly -c pytorch and
conda install -c conda-forge transformers
i get an error:
from transformers import berttokenizer
bert_tokenizer = berttokenizer.from_pretrained('bert-base-uncased', do_lower_case=true)

def tok(dataset):
    input_ids = []
    attention_masks = []
    sentences = dataset.answer2en.values
    labels = dataset.class.values
    for sent in sentences:
        encoded_sent = bert_tokenizer.encode(sent, 
                                             add_special_tokens=true,
                                             max_length = 64,
                                             pad_to_max_length =true)


typeerror: _tokenize() got an unexpected keyword argument
'pad_to_max_length'

does anyone know a secure installation of transformers using anaconda?
thank you","['python', 'python-3.x', 'anaconda', 'pytorch', 'huggingface-transformers']",62542474,"the problem is that conda only offers the transformers library in version 2.1.1 (repository information) and this version didn't have a pad_to_max_length argument. i'm don't want to look it up if there was a different parameter, but you can simply pad the result (which is just a list of integers):
from transformers import berttokenizer
bert_tokenizer = berttokenizer.from_pretrained('bert-base-uncased', do_lower_case=true)

sentences = ['this is just a test', 'this is another test']

max_length = 64

for sent in sentences:
    encoded_sent = bert_tokenizer.encode(sent, 
                                         add_special_tokens=true,
                                         max_length = max_length)
    encoded_sent.extend([0]* (max_length - len(encoded_sent)))

    ###your other stuff

the better option in my opinion is to create a new conda environment and install everything via pip and not via conda. this will allow you to work with the most recent transformers version (2.11).",https://stackoverflow.com/questions/62538079,python,23-06-2020 15:12,4291.0,2.0,3.0,True,29-04-2022 02:46,23-06-2020 19:37
79295026,langchain runnableeach invoke error: response is not valid json,"i am trying to use runnableeach to make parallel requests to openai. the responses are supposed to be ""yes/no"" plus a motivation. i want the response to be in json format, my code is like the following:
from langchain_core.prompts import chatprompttemplate
from langchain_core.runnables.base import runnableeach
from langchain_openai import chatopenai

class classification(basemodel):
   flag: bool = field(description=""my descr"",enum=[true, false])
   answer: str = field(description=""my descr"")

llm = chatopenai(temperature=0, model=""gpt-4o-mini"", api_key=model_key).with_structured_output(
            classification)

prompt = my_prompt

tagging_chain = prompt | llm
runnable_each = runnableeach(bound=tagging_chain)
input_list = [{""input"": val} for val in mydata]
res = runnable_each.invoke(input_list)

i have around 25k elements in input list, that is 25k requests have to be processed. this has worked fine until today, it is failing because of an error in one request:
function classification arguments:

{""flag"":false,""motivation"":""the passage does not contain any information relevant to products

are not valid json. received jsondecodeerror unterminated string starting at: line 1 column 34 (char 33)

i understand that the call to the llm model returned a malformed string that when parsed as json (since i have constrained output), generates an error. i would like to know whether this kind of error can be handled in a way such that the entire process does not fail but only that request. thanks in advance for your help.
update
i saw in the langchain documentation that a retry logic is implemented in the library: 
by defining
tagging_chain = prompt | llm.with_retry()
runnable_each = runnableeach(bound=tagging_chain)

would it retry the single input that is failing or the entire input sequence?","['langchain', 'large-language-model', 'gpt-4o-mini']",79295069,"i found the below in the documentation
return_exceptions (bool) ï¿½ï¿½ï¿½ whether to return exceptions instead of raising them. defaults to false.

so basically you just have to pass return_exceptions as true to the runnableeach and it will just return it and not break the whole thing.
reference",https://stackoverflow.com/questions/79295026,langchain,19-12-2024 16:45,159.0,0.0,1.0,True,20-12-2024 13:55,20-12-2024 13:55
33289820,noun phrases with spacy,"how can i extract noun phrases from text using spacy?
i am not referring to part of speech tags. 
in the documentation i cannot find anything about noun phrases or regular parse trees.","['python', 'python-3.x', 'spacy']",33512175,"if you want base nps, i.e. nps without coordination, prepositional phrases or relative clauses, you can use the noun_chunks iterator on the doc and span objects:
>>> from spacy.en import english
>>> nlp = english()
>>> doc = nlp(u'the cat and the dog sleep in the basket near the door.')
>>> for np in doc.noun_chunks:
>>>     np.text
u'the cat'
u'the dog'
u'the basket'
u'the door'

if you need something else, the best way is to iterate over the words of the sentence and consider the syntactic context to determine whether the word governs the phrase-type you want. if it does, yield its subtree:
from spacy.symbols import *

np_labels = set([nsubj, nsubjpass, dobj, iobj, pobj]) # probably others too
def iter_nps(doc):
    for word in doc:
        if word.dep in np_labels:
            yield word.subtree",https://stackoverflow.com/questions/33289820,python,22-10-2015 20:12,43271.0,38.0,5.0,True,19-01-2023 07:07,19-01-2023 07:07
71078218,valueerror: no gradients provided for any variable (tfcamembert),"currently i am working on named entity recognition in the medical domain using camembert, precisely using the model: tfcamembert.
however i have some problems with the fine-tuning of the model for my task as i am using a private dataset not available on hugging face.
the data is divided into text files and annotation files. the text file contains for example:
le cas prï¿½ï¿½sentï¿½ï¿½ concerne un homme ï¿½ï¿½gï¿½ï¿½ de 61 ans (71 kg, 172 cm, soit un indice de masse corporelle de 23,9 kg/mï¿½ï¿½) admissible ï¿½ï¿½ une transplantation pulmonaire en raison dï¿½ï¿½ï¿½une insuffisance respiratoire chronique terminale sur emphysï¿½ï¿½me post-tabagique, sous oxygï¿½ï¿½nothï¿½ï¿½rapie continue (1 l/min) et ventilation non invasive nocturne. il prï¿½ï¿½sente, comme principaux antï¿½ï¿½cï¿½ï¿½dents, une dyslipidï¿½ï¿½mie, une hypertension artï¿½ï¿½rielle et un tabagisme sevrï¿½ï¿½ estimï¿½ï¿½ ï¿½ï¿½ 21 paquets-annï¿½ï¿½es (facteurs de risque cardiovasculaires). le bilan prï¿½ï¿½opï¿½ï¿½ratoire a rï¿½ï¿½vï¿½ï¿½lï¿½ï¿½ une hypertension artï¿½ï¿½rielle pulmonaire essentiellement postcapillaire conduisane dï¿½ï¿½ï¿½un elispot (enzyme-linked immunospot) positif pour la tuberculose a motivï¿½ï¿½ lï¿½ï¿½ï¿½introduction dï¿½ï¿½ï¿½un traitement prophylactique par lï¿½ï¿½ï¿½association rifampicine-isoniazide (600-300 mg par jour) pour une durï¿½ï¿½e de trois mois.
deux mois aprï¿½ï¿½s le bilan prï¿½ï¿½opï¿½ï¿½ratoire, le patient a bï¿½ï¿½nï¿½ï¿½ficiï¿½ï¿½ dï¿½ï¿½ï¿½une transplantation mono-pulmonaire gauche sans dysfonction primaire du greffon5,6. le donneur et le receveur prï¿½ï¿½sentaient tous deux un statut sï¿½ï¿½rologique positif pour cytomegalovirus (cmv) et epstein barr virus (ebv). une sï¿½ï¿½rologie positive de la toxoplasmose a ï¿½ï¿½tï¿½ï¿½ mise en ï¿½ï¿½vidence uniquement chez le receveur. le traitement immunosuppresseur dï¿½ï¿½ï¿½induction associait la mï¿½ï¿½thylprednisolone (500 mg ï¿½ï¿½ jour 0 et 375 mg ï¿½ï¿½ jour +1 post-transplantation) et le basiliximab, anticorps monoclonal dirigï¿½ï¿½ contre lï¿½ï¿½ï¿½interleukine-2 (20 mg ï¿½ï¿½ jour 0 et jour +4 posttransplantation). ï¿½ï¿½ partir de jour mus ï¿½ï¿½ une posologie initiale de 5 mg par jour, le mofï¿½ï¿½til mycophï¿½ï¿½nolate (mmf) 2000 mg par jour et la prednisone 20 mg par jour. les traitements associï¿½ï¿½s sont prï¿½ï¿½sentï¿½ï¿½s dans le tableau i.
lï¿½ï¿½ï¿½ï¿½ï¿½volution est marquï¿½ï¿½e par la survenue, au jour +5 posttransplantation, dï¿½ï¿½ï¿½une dï¿½ï¿½gradation respiratoire sur ï¿½ï¿½dï¿½ï¿½me pulmonaire gauche de reperfusion, avec possible participation cardiogï¿½ï¿½nique. le rejet aigu de grade iii, ï¿½ï¿½voquï¿½ï¿½ par la prï¿½ï¿½sence dï¿½ï¿½ï¿½infiltrats lymphocytaires aux biopsies transbronchiques, a ï¿½ï¿½tï¿½ï¿½ confirmï¿½ï¿½ par lï¿½ï¿½ï¿½anatomopathologie.

while the annotation file looks like:
t1 genre 28 33 homme
t2 age 41 47 61 ans
a1 genre t1 masculin
t3 origine 127 326 une transplantation pulmonaire en raison dï¿½ï¿½ï¿½une insuffisance respiratoire chronique terminale sur emphysï¿½ï¿½me post-tabagique, sous oxygï¿½ï¿½nothï¿½ï¿½rapie continue (1 l/min) et ventilation non invasive nocturne
cipation cardiogï¿½ï¿½nique. le rejet aigu de grade iii
a2 issue t4 dï¿½ï¿½tï¿½ï¿½rioration

more details about the prepossessing"" rel=""nofollow noreferrer"">the data can be found in this notebook.
the things is that the internal loss of my model does not work, if i run the training of the model without declaring a loss, it does not work, i have to define a loss to be able to run the training!
here is my train_data converted to tensor slice dataset.
train_label_encodings = tf.convert_to_tensor(train_label_encodings, dtype=tf.int32)
train_label_encodings.data

train_dataset = tf.data.dataset.from_tensor_slices((
    dict(train_text_encodings.data),
    train_label_encodings.data
))
train_dataset

<tensorslicedataset shapes: ({input_ids: (512,), offset_mapping: (512, 2)}, (512,)), types: ({input_ids: tf.int32, offset_mapping: tf.int32}, tf.int32)>

i define the model:
# import the model and define an optimizer
from transformers import tfautomodelfortokenclassification, tfcamembertmodel, create_optimizer
import tensorflow as tf

num_train_steps = len(train_dataset) * 5
optimizer, lr_schedule = create_optimizer(
    init_lr = 5e-6,
    num_train_steps = num_train_steps,
    weight_decay_rate = 0.01,
    num_warmup_steps = 0
)

metric = tf.keras.metrics.sparsecategoricalaccuracy('accuracy')

model = tfautomodelfortokenclassification.from_pretrained(model_id, num_labels=len(unique_labels), label2id=label2id, id2label=id2label)

model.compile(optimizer=optimizer, metrics=['accuracy'])

here is the summary of the model:
model: ""tf_camembert_for_token_classification_2""
_________________________________________________________________
 layer (type)                output shape              param #   
=================================================================
 roberta (tfrobertamainlayer  multiple                 110031360 
 )                                                               
                                                                 
 dropout_113 (dropout)       multiple                  0         
                                                                 
 classifier (dense)          multiple                  25377     
                                                                 
=================================================================
total params: 110,056,737
trainable params: 110,056,737
non-trainable params: 0
_________________________________________________________________

when i try to launch the training, i get the following error:
import os
from tensorflow.keras.callbacks import tensorboard

callbacks = []
callbacks.append(tensorboard(log_dir=os.path.join(output_dir,""logs"")))

model.fit(
    train_dataset,
    callbacks = callbacks,
    epochs = 3,
)

epoch 1/3
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
<ipython-input-71-54e2d25b9415> in <module>()
      8     train_dataset,
      9     callbacks = callbacks,
---> 10     epochs = 3,
     11 )

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)
   1127           except exception as e:  # pylint:disable=broad-except
   1128             if hasattr(e, ""ag_error_metadata""):
-> 1129               raise e.ag_error_metadata.to_exception(e)
   1130             else:
   1131               raise

valueerror: in user code:

    file ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 878, in train_function  *
        return step_function(self, iterator)
    file ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 867, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    file ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 860, in run_step  **
        outputs = model.train_step(data)
    file ""/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py"", line 911, in train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    file ""/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py"", line 532, in minimize
        return self.apply_gradients(grads_and_vars, name=name)
    file ""/usr/local/lib/python3.7/dist-packages/transformers/optimization_tf.py"", line 232, in apply_gradients
        return super(adamweightdecay, self).apply_gradients(zip(grads, tvars), name=name, **kwargs)
    file ""/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py"", line 633, in apply_gradients
        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)
    file ""/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/utils.py"", line 73, in filter_empty_gradients
        raise valueerror(f""no gradients provided for any variable: {variable}. ""

    valueerror: no gradients provided for any variable: (['tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/query/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/query/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/key/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/key/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/value/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/self/value/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/output/layernorm/gamma:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/attention/output/layernorm/beta:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/output/dense/kernel:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/output/dense/bias:0', 'tf_camembert_for_token_classification_2/roberta/encoder/layer_._0/output/la...

any clues to solve this gradient issue? thanks in advance!","['python', 'tensorflow', 'nlp', 'huggingface-transformers', 'named-entity-recognition']",71078985,"try transforming your data into the correct format, before feeding it to model.fit:
def map_func(x, y):
  return {'input_ids': x['input_ids'], 'attention_mask': x['attention_mask'], 'labels':y}

train_dataset = train_dataset.map(map_func)

the model seems to run after this step.",https://stackoverflow.com/questions/71078218,python,11-02-2022 10:02,431.0,1.0,1.0,True,11-02-2022 13:37,11-02-2022 13:37
79455504,load phi 3 model extract attention layer and visualize it,"i would like to visualize the attention layer of a phi-3-medium-4k-instruct (or mini) model downloaded from hugging-face. in particular, i am using the following model, tokenizer:
import torch
from transformers import automodelforcausallm, autotokenizer, pipeline
import pdb

tokenizer = autotokenizer.from_pretrained(""microsoft/phi-3-medium-4k-instruct"")

model = automodelforcausallm.from_pretrained(
    ""microsoft/phi-3-meduium-4k-instruct"",
    device_map = ""auto"",
    torch_dtype = ""auto"",
    trust_remote_code = true
)

# create a pipeline
generator = pipeline(
    ""text-generation"",
    model = model,
    tokenizer = tokenizer,
    return_full_text= false,
    max_new_tokens = 50,
    do_sample = false
)

prompt = ""...""
input_ids = tokenizer(prompt, return_tensors = ""pt"").input_ids
# tokenize the input prompt
input_ids = input_ids.to(""cuda:0"")
# get the output of the model
model_output = model.model(input_ids)

# extract the attention layer
attention = model_output[-1] 

firstly, i am wondering if that is the correct way to extract attention from my model. what should expect from this model and how can i visualize it properly? isn't that i should expect a matrix n_tokens x n_tokens?
the attention variable i have extracted has a size of 1x40x40x15x15 (or 1x12x12x15x15 in the case of mini model), where the first dimension corresponds to different layers the second for the different heads, and the final two for the attention matrix. that is actually my assumption and i am not sure whether it is correct. when i am visualizing the attention i am getting some very weird matrices like:

what we see in this figure, i assume is all the heads for one layer. however, most of the heads distribute the attention equally to all the tokens. does that make sense?
edit: for the visualization i am doing sth like:
# save attention visualization code 
def save_attention_image(attention, tokens, filename='attention.png'):
    """"""
    save the attention weights for a specific layer and head as an image.
    
    :param attention: the attention weights from the model.
    :param tokens: the tokens corresponding to the input.
    :param layer_num: the layer number to visualize.
    :param head_num: the head number to visualize.
    :param filename: the filename to save the image.
    """"""

    attn = attention[0].detach().cpu().float().numpy()    
    num_heads = attn.shape[0]
    fig, axes = plt.subplots(3, 4, figsize=(20, 15))  # adjust the grid size as needed
    
    for i, ax in enumerate(axes.flat):
        if i < num_heads:
            cax = ax.matshow(attn[i], cmap='viridis')
            ax.set_title(f'head {i + 1}')
            ax.set_xticks(range(len(tokens)))
            ax.set_yticks(range(len(tokens)))
            ax.set_xticklabels(tokens, rotation=90)
            ax.set_yticklabels(tokens)
        else:
            ax.axis('off')
    
    fig.colorbar(cax, ax=axes.ravel().tolist())
    plt.suptitle(f'layer {1}')
    plt.savefig(filename)
    plt.close()","['python', 'pytorch', 'huggingface-transformers', 'attention-model']",79476998,"here is what you need to know:
running colab code - 
you want see attention weights from your phi3 model. first thing: you must tell model to output attentions. usually you do
outputs = model(input_ids, output_attentions=true)

then outputs.attentions will be tuple with one element per layer. each element is tensor shape (batch, num_heads, seq_len, seq_len) ï¿½ï¿½ï¿½ that is what you expect, a matrix n_tokens x n_tokens per head.
what you did using
model_output = model.model(input_ids)
attention = model_output[-1]

may or may not be correct ï¿½ï¿½ï¿½ depends on how model.forward is coded. better use output_attentions flag so you get proper attention weights.
about the shape you see, e.g. 1x40x40x15x15 (or 1x12x12x15x15) ï¿½ï¿½ï¿½ this likely means:

1 is batch size,
next dimension is number of layers (40 for medium, 12 for mini),
next is number of heads per layer,
and last two are the attention matrices (each head gets a 15x15 attention matrix if you have 15 tokens).

if many heads show nearly uniform attention it can be normal ï¿½ï¿½ï¿½ sometimes heads do that, not focusing on any token particularly.
for proper visualization, select one layer and one head lik""lang-py prettyprint-override"">attn = outputs.attentions[layer][0, head]  # shape (seq_len, seq_len)

and then use your plotting code (imshow or matshow) to visualize.
so summary: use model(..., output_attentions=true) to get correct attention, then each attention tensor will be (batch, heads, seq_len, seq_len) ï¿½ï¿½ï¿½ that is the matrix you expect. if you see extra dimensions then check if you are calling the right forward method. and yes, many heads may show uniform distribution ï¿½ï¿½ï¿½ that can be normal in transformer models.
hope this helps, and you can put my code in your colab as is.
note that
when using hugging face transformers, the recommended approach is to run:""lang-py prettyprint-override"">outputs = model(
    input_ids=inputs,
    output_attentions=true,
    # possibly also output_hidden_states=true if you want hidden states
)

then outputs.attentions will be a tuple with one entry per layer, each entry shaped (batch_size, num_heads, seq_len, seq_len).
if you call model.model(input_ids) directly (as in your code snippet), you might be accessing a lower-level forward function that returns a different structure. instead, call the top-level model with output_attentions=true. that yields attention shapes more in line with standard hugging face conventions.
ok so basically you want see attention. you pass output_attentions=true when calling model, then get outputs.attentions. that is standard shape (batch, heads, seq_len, seq_len). then pick layer and head to plot. some heads look uniform, that is normal. if you do model.model(input_ids) directly, might not give the standard shape. safer is:
# !pip install transformers torch

import torch
import matplotlib.pyplot as plt
from transformers import automodelforcausallm, autotokenizer

# load tokenizer and model (make sure you have a valid license for the model)
tokenizer = autotokenizer.from_pretrained(""microsoft/phi-3-medium-4k-instruct"")
model = automodelforcausallm.from_pretrained(
    ""microsoft/phi-3-medium-4k-instruct"",  # note: check spelling if you get error
    device_map=""auto"",
    torch_dtype=torch.float16,            # or torch.float32 if preferred
    trust_remote_code=true
)

# prepare a prompt
prompt = ""the quick brown fox jumps over the lazy dog.""
inputs = tokenizer(prompt, return_tensors=""pt"")
inputs = inputs.to(""cuda:0"")  # send inputs to cuda

# run the model with attention outputs enabled
# make sure to pass output_attentions=true
outputs = model(input_ids=inputs.input_ids, output_attentions=true)

# outputs.attentions is a tuple with one element per layer
# each element is a tensor of shape (batch_size, num_heads, seq_len, seq_len)
attentions = outputs.attentions

# for example, choose layer 0 and head 0 to visualize
layer = 0
head = 0
attn = attentions[layer][0, head].detach().cpu().numpy()  # shape (seq_len, seq_len)

# get tokens for labeling the axes
tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])

# visualize the attention matrix using matplotlib
plt.figure(figsize=(8,8))
plt.imshow(attn, cmap=""viridis"")
plt.colorbar()
plt.xticks(range(len(tokens)), tokens, rotation=90)
plt.yticks(range(len(tokens)), tokens)
plt.title(f""attention matrix (layer {layer}, head {head})"")
plt.show()


now you see nice n_tokens by n_tokens matrix. if model has 12 layers, you see 12 in outputs.attentions. if ï¿½ï¿½ï¿½mediumï¿½ï¿½ï¿½ is 40 layers, you see 40. each head is shape 15ï¿½ï¿½15 if your input is 15 tokens. some heads do uniform attention, that is normal. that is basically all.
note -
when you do something like:
model_output = model.model(input_ids)
attention = model_output[-1]

youï¿½ï¿½ï¿½re relying on how the internal forward method organizes its return. some models do return (hidden_states, present, attentions, ...) but some do not. itï¿½ï¿½ï¿½s safer to rely on the official hugging face usage:
outputs = model(..., output_attentions=true)
attention = outputs.attentions

thatï¿½ï¿½ï¿½s guarant",https://stackoverflow.com/questions/79455504,python,20-02-2025 18:25,162.0,3.0,1.0,True,01-03-2025 01:22,28-02-2025 12:09
73745607,how to pass arguments to huggingface tokenclassificationpipeline&#39;s tokenizer,"i've finetuned a huggingface bert model for named entity recognition. everything is working as it should. now i've setup a pipeline for token classification in order to predict entities out the text i provide. even this is working fine.
i know that bert models are supposed to be fed with sentences less than 512 tokens long. since i have texts longer than that, i split the sentences in shorter chunks and i store the chunks in a list chunked_sentences. to make it brief my tokenizer for training looks like this:
from transformers import berttokenizerfast
tokenizer = berttokenizerfast.from_pretrained('bert-base-uncased')
tokenized_inputs = tokenizer(chunked_sentences, is_split_into_words=true, padding='longest')

i pad everything to the longest sequence and avoid truncation so that if a sentence is tokenized and goes beyond 512 tokens i receive a warning that i won't be able to train. this way i know that i have to split the sentences in smaller chunks.
during inference i wanted to achieve the same thing, but i haven't found a way to pass arguments to the pipeline's tokenizer. the code looks like this:
from transformers import pipeline
ner_pipeline = pipeline('token-classification', model=model_folder, tokenizer=model_folder)
out = ner_pipeline(text, aggregation_strategy='simple')

i'm pretty sure that if a sentence is tokenized and surpasses the 512 tokens, the extra tokens will be truncated and i'll get no warning. i want to avoid this.
i tried passing arguments to the tokenizer like this:
tokenizer_kwargs = {'padding': 'longest'}
out = ner_pipeline(text, aggregation_strategy='simple', **tokenizer_kwargs)

i got that idea from this answer, but it seems not to be working, since i get the following error:
traceback (most recent call last):
  file ""...\inference.py"", line 42, in <module>
    out = ner_pipeline(text, aggregation_strategy='simple', **tokenizer_kwargs)
  file ""...\venv\lib\site-packages\transformers\pipelines\token_classification.py"", line 191, in __call__
    return super().__call__(inputs, **kwargs)
  file ""...\venv\lib\site-packages\transformers\pipelines\base.py"", line 1027, in __call__
    preprocess_params, forward_params, postprocess_params = self._sanitize_parameters(**kwargs)
typeerror: tokenclassificationpipeline._sanitize_parameters() got an unexpected keyword argument 'padding'

process finished with exit code 1

any ideas? thanks.","['python', 'huggingface-transformers', 'named-entity-recognition', 'huggingface-tokenizers', 'huggingface']",74522080,"i took a closer look at  it seems you can override preprocess() to disable truncation and add padding to longest.
from transformers import tokenclassificationpipeline

class mytokenclassificationpipeline(tokenclassificationpipeline):
    def preprocess(self, sentence, offset_mapping=none):
        truncation = false
        padding = 'longest'
        model_inputs = self.tokenizer(
            sentence,
            return_tensors=self.framework,
            truncation=truncation,
            padding=padding,
            return_special_tokens_mask=true,
            return_offsets_mapping=self.tokenizer.is_fast,
        )
        if offset_mapping:
            model_inputs[""offset_mapping""] = offset_mapping
    
        model_inputs[""sentence""] = sentence
        return model_inputs
    
ner_pipeline = mytokenclassificationpipeline(model=model_folder, tokenizer=model_folder)
out = ner_pipeline(text, aggregation_strategy='simple')",https://stackoverflow.com/questions/73745607,python,16-09-2022 13:32,3892.0,5.0,1.0,True,21-11-2022 16:35,16-09-2022 14:29
78920095,cannot import name &#39;split_torch_state_dict_into_shards&#39; from &#39;huggingface_hub&#39;,"i've been using llama 2 for research for a few months now and i import as follows:
from transformers import automodelforcausallm, autotokenizer
device = torch.device(""cuda"")
tokenizer = autotokenizer.from_pretrained(""meta-llama/llama-2-7b-chat-hf"",token = ""token_key"",torch_dtype=""auto"")
model = automodelforcausallm.from_pretrained(""meta-llama/llama-2-7b-chat-hf"",token = ""token_key"", torch_dtype=""auto"", load_in_4bit=true)

it has always worked. however, today it is showing the following error:
runtimeerror: failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
failed to import transformers.generation.utils because of the following error (look up to see its traceback):
cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/opt/conda/lib/python3.10/site-packages/huggingface_hub/init.py)
recreated the hugging face token, but it didn't work. i am using google colab and kaggle notebook.","['python', 'nlp', 'huggingface-transformers', 'transformer-model', 'llama']",78920098,"the error you're encountering is due to the split_torch_state_dict_into_shards function not being available in huggingface-hub version < 0.23.0.
this function is included starting from version 0.23.0.
to resolve this issue, update the huggingface-hub library to version 0.23.0 or later
also please install accelerate:
pip install accelerate==0.31.0

here is a git link:",https://stackoverflow.com/questions/78920095,python,27-08-2024 17:20,6165.0,1.0,1.0,True,27-10-2024 23:35,27-08-2024 17:52
72954156,regular expression to recognize digits written as words in python?,"it is easy to recognize numbers as digits or integers from the text but not when numbers are written as words in natural language text.
for recognizing the digits using regex one can just the following regular expression.
digits_recognize = r'[0-9]+'

how can one develop a pattern to recognize digits written as numbers?","['python-3.x', 'regex', 'nlp']",72954157,"the following solution is applicable only to the versions after python 3.6.
one_to_9 = '((f(ive|our)|s(even|ix)|[tt](hree|wo)|(ni|o)ne|eight))'

ten_to_19 = '((([ss](even|ix)|[ff](our|if)|[nn]ine)[tt][ee]|[ee](ighte|lev))en|[tt]((hirte)?en|welve))'

two_digit_prefix = '((s(even|ix)|[tt](hir|wen)|f(if|or)|eigh|nine)ty)'

one_to_99 = fr'({two_digit_prefix}([- ]{one_to_9})?|{ten_to_19}|{one_to_9})' 

one_to_999 = fr'({one_to_9}[ ]hundred([ ](and[ ])?{one_to_99})?|{one_to_99})'

compiled_pattern = re.compile(one_to_999)

the answer is adapted from here.",https://stackoverflow.com/questions/72954156,python-3.x,12-07-2022 14:40,441.0,2.0,1.0,True,12-07-2022 19:46,12-07-2022 15:57
71712866,"error in .rect_dendrogram(dend, k = k, palette = rect_border, rect_fill = rect_fill, : k must be between 2 and 97","i am trying to estimate a cluster dendrogram in r for a structural topic model i produced with 98 topics.
i first ran the following which worked well:
res.hc <- eclust(scale(out_corr$cor), ""hclust"", nboot = 500)

i then attempting to visualize the dendrogram using the following syntax:
fviz_dend(res.hc, rect = true)

here, i received the following error:
error in .rect_dendrogram(dend, k = k, palette = rect_border, rect_fill = rect_fill, :
k must be between 2 and 97
is this because the number of topics in my model is 98? if so, is there a way to still visualize the dendrogram without reducing my topics to 97?
thank you!","['r', 'hierarchical-clustering', 'topic-modeling', 'dendrogram', 'dendextend']",71759470,"the following steps helped to resolve the issue:

estimate cluster dendrogram

res.hc <- eclust(scale(out_corr$cor), ""hclust"", nboot = 500)


install dendextend

install.packages(""dendextend"")
library(dendextend)


install dplyr

install.packages(""dplyr"")
library(dplyr)


save cluster estimate as a dendrogram

dend<-as.dendrogram(res.hc)


color in cluster levels

par(mar=c(1,1,1,7))
dend %>%
  set(""labels_col"", value = c(""skyblue"", ""red"", ""grey"", ""blue""), k=4) %>%
  set(""branches_k_color"", value = c(""skyblue"", ""red"", ""grey"", ""blue""), k = 4) %>%
  plot(horiz=false, axes=false)
abline(v = 350, lty = 2)",https://stackoverflow.com/questions/71712866,r,01-04-2022 21:21,331.0,0.0,1.0,True,05-04-2022 23:22,02-04-2022 20:35
60888125,automatic generation of question in python,"im currently working on a simple chatbot in python. the goal of this chatbot is to discriminate some products of a product list in order replace the search bar on some website.
so im currently working on the automatic generation of question to ask to the user, using a keyword.
do you have any algorithm in mind or some keyword in order to help me in my research ?
thank you !
ps: an example of use.
questiongeneration(""colour"") -> ""what color is the product you are looking for ?""","['python', 'nlp', 'nltk']",66449359,"i am not sure if question generation can be done properly with just one keyword. maybe i am not very informed on that matter. however, here is a service you can use to generate questions automatically from any text.",https://stackoverflow.com/questions/60888125,python,27-03-2020 14:33,573.0,0.0,1.0,True,05-12-2021 11:01,27-03-2020 14:41
74196558,how do i retrieve phrases from a nltk.tree using custom node labels?,"given a nltk tree produced using the code below, how do i retrieve the leaf values (phrases) that potentially match all of the node labels assigned using the nltk.regexparser (e.g. those phrases which match the present_indefinite or present_perfect tense)?
from nltk import word_tokenize, pos_tag
import nltk

text = ""#novavax has produced the #nuvaxovid vaccine.\
 will that provide a new rally? we see biotechnology\
  stock $nvax entering the buying area.""
tokenized = word_tokenize(text) # tokenize text
tagged = pos_tag(tokenized) # tag tokenized text with pos tags

my_grammar = r""""""
future_perfect_continuous: {<md><vb><vbn><vbg>}
future_continuous:         {<md><vb><vbg>}
future_perfect:            {<md><vb><vbn>}
past_perfect_continuous:   {<vbd><vbn><vbg>}
present_perfect_continuous:{<vbp|vbz><vbn><vbg>}
future_indefinite:         {<md><vb>}
past_continuous:           {<vbd><vbg>}
past_perfect:              {<vbd><vbn>}
present_continuous:        {<vbz|vbp><vbg>}
present_perfect:           {<vbz|vbp><vbn>}
past_indefinite:           {<vbd>}
present_indefinite:        {<vbz>|<vbp>}""""""


def check_grammar(grammar, tags):
    cp = nltk.regexpparser(grammar)
    result = cp.parse(tags)
    return result

# apply regex parser and create parse tree
result = check_grammar(my_grammar, tagged)
print(type(result))
# output: <class 'nltk.tree.tree.tree'>

more specifically, given that the output of print(result) is as shown below, how can i retrieve the phrases labelled as present_perfect and present_indefinite, or more generally, any other phrases which match the labels in my grammar?
(s
  #/#
  novavax/nnp
  (present_perfect has/vbz produced/vbn)
  the/dt
  #/#
  nuvaxovid/nnp
  vaccine/nn
  ./.
  will/md
  that/wdt
  provide/vb
  a/dt
  new/jj
  rally/nn
  ?/.
  we/prp
  (present_indefinite see/vbp)
  biotechnology/nnp
  stock/nnp
  $/$
  nvax/nnp
  entering/nnp
  the/dt
  buying/nnp
  area/nnp
  ./.)","['python', 'nlp', 'nltk', 'part-of-speech', 'parse-tree']",74201199,"i've created a get_phrases_using_tense_label() function which takes:

the parse tree returned from your check_grammar() function (i've renamed it to get_parse_tree() as this is more meaningful in terms of what the function is doing), and
a list of tense labels based on your grammar.

the tense labels are retrieved using the get_labels_from_grammar() function i created, which iterates over the lines in your grammar and splits the string at the "":"" retrieving the tense label.
the function then returns the list of phrases (along with their tags) for those nodes in the nltk tree which match any of your tense_labels (e.g. ""present_indefinite"" and present_perfect"" in the solution below). i've used a smaller text as input as an example.

solution
from nltk import word_tokenize, pos_tag
import nltk

text = ""#novavax produces #nuvaxovid vaccine.\
 will that provide a new rally? we see biotechnology\
  stock $nvax entering the buying area.""

# smaller text for testing
textsmall = ""we see a surge in sales. it has been a great year.""

tokenized = word_tokenize(textsmall)  # tokenize text
tagged = pos_tag(tokenized)  # tag tokenized text with pos tags

my_grammar = r""""""
future_perfect_continuous: {<md><vb><vbn><vbg>}
future_continuous:         {<md><vb><vbg>}
future_perfect:            {<md><vb><vbn>}
past_perfect_continuous:   {<vbd><vbn><vbg>}
present_perfect_continuous:{<vbp|vbz><vbn><vbg>}
future_indefinite:         {<md><vb>}
past_continuous:           {<vbd><vbg>}
past_perfect:              {<vbd><vbn>}
present_continuous:        {<vbz|vbp><vbg>}
present_perfect:           {<vbz|vbp><vbn>}
past_indefinite:           {<vbd>}
present_indefinite:        {<vbz>|<vbp>}""""""


def get_parse_tree(grammar, pos_tagged_text):
    cp = nltk.regexpparser(grammar)
    parse_tree = cp.parse(pos_tagged_text)
    # parse_tree.draw()  # visualise parse tree
    return parse_tree


# function to get labels from grammar:
# takes line separated nltk regexp grammar rules
def get_labels_from_grammar(grammar):
    labels = []
    for line in grammar.splitlines()[1:]:
        labels.append(line.split("":"")[0])
    return labels


# function takes parse tree & list of nltk custom grammar labels as input
# returns phrases which match
def get_phrases_using_tense_labels(parse_tree, tense_labels_to_get):
    matching_phrases = []
    for node in parse_tree.subtrees(filter=lambda x: any(x.label() == tense_lab for tense_lab in tense_labels_to_get)):
        matching_phrases.append(node.leaves()[0])
    return matching_phrases


# function takes parse tree & list of nltk custom grammar labels as input
# returns the tense labels present in the parse tree
def get_tense_labels_in_tree(parse_tree, tense_labels_to_get):
    matching_labels = []
    for node in parse_tree.subtrees(filter=lambda x: any(x.label() == tense_lab for tense_lab in tense_labels_to_get)):
        matching_labels.append(node.label())
    return matching_labels


text_parse_tree = get_parse_tree(my_grammar, tagged)
# print(text_parse_tree)  # view parse tree output
tense_labels = get_labels_from_grammar(my_grammar)
phrases = get_phrases_using_tense_labels(text_parse_tree, tense_labels)
labels = get_tense_labels_in_tree(text_parse_tree, tense_labels)

print(phrases)
# output: [('see', 'vbp'), ('has', 'vbz')]
print([phrase[0] for phrase in phrases])
# output: ['see', 'has']
print(labels)
# ['present_perfect', 'present_indefinite']",https://stackoverflow.com/questions/74196558,python,25-10-2022 15:29,126.0,1.0,1.0,True,27-10-2022 20:59,27-10-2022 20:59
43649359,machine learning libraries for android,"i am trying build a small text mining tool for my android app. i am checking for a machine learning library that will allow me to cluster, classify etc.
are there any machine learning libraries for android? i came across tensorflow but i need a bit more access to common ml functions.","['android', 'machine-learning', 'nlp', 'artificial-intelligence', 'text-mining']",43812916,"you can try these ports of weka for android (they do not use the latest weka version, but it may be sufficient for your needs):",https://stackoverflow.com/questions/43649359,android,27-04-2017 05:33,5482.0,10.0,3.0,True,04-05-2024 12:44,01-05-2017 22:52
69983684,list index out of range when saving a fine tuned bert model,"model is created using the following function definition
def create_model(max_length = 256):
  bert_model = tfbertmodel.from_pretrained('bert-base-uncased')
  for layer in bert_model.layers:
    layer.trainable = false
  input_ids = tf.keras.input(shape = (max_length, ), dtype = tf.int32, name = 'input_ids')
  attention_masks = tf.keras.input(shape = (max_length, ), dtype = tf.int32, name = 'attention_masks')
  x = bert_model.bert([input_ids, attention_masks])
  x = x.pooler_output
  x = tf.keras.layers.dropout(0.2)(x)
  x = tf.keras.layers.dense(256, activation = 'relu')(x)
  x = tf.keras.layers.dropout(0.2)(x)
  x = tf.keras.layers.dense(33)(x)
  out = tf.keras.layers.activation('sigmoid')(x)
  model = tf.keras.model(inputs = [input_ids, attention_masks], outputs = out)
  model.compile(optimizer = tf.keras.optimizers.adam(learning_rate=3e-5),
                loss = tf.keras.losses.binarycrossentropy(),
                metrics = tf.metrics.binaryaccuracy())
  return model

on trying to save the model using tf.keras.models.save_model, i run into the following error:
indexerror: exception encountered when calling layer 'bert' (type tfbertmainlayer). list index out of range","['tensorflow', 'huggingface-transformers']",71388255,"sorry i meant to write answer,
it resolved for me when i changed from
sequence_output = bert_layer.bert([input_ids, input_masks_ids])[""last_hidden_state""]
to
sequence_output = bert_layer.bert(input_ids, input_masks_ids)[""last_hidden_state""]
please check this below,",https://stackoverflow.com/questions/69983684,tensorflow,16-11-2021 04:14,310.0,0.0,1.0,True,07-03-2022 23:05,27-11-2021 15:23
69476453,how to detect the address form the text in node.js,"i have a raw text
exmp 1:
order pickup details>>> >>> pick up before the store closes on wed, apr 11>>> >>> 
scan in-store for order pickup>>> >>> >>> 9019560>>>    warrenville target store>>> 28201 diehl rd, warrenville, il 60555

exmp 2:
    come to collect your order in the next 2 days (after that it'll be cancelled). your payment will be processed as soon as you collect your order.>> >>  >> 
pickup store:>> >> lush naperville < 
119 s. main street , naperville, il 60540>> >> choose between curbside or in-store pickup.>>

how the address part can be extracted from the above text in node.js. how to solve this?
actually, what happening here is. i am getting different emails from different stores after order confirmation. i need to get the store address from the email. here each store using a different format for their email order confirmation.
i am getting this raw text after converting the email template into text format.
below one is the related question to my problem. but it is in python.
how can i extract address from raw text using nltk in python?
is there any way to detect the address from the text? i am new to this.","['node.js', 'regex', 'nlp', 'street-address']",69565437,"the regexp for the above type of address format in node.js is
var text = ""pick up before the store closes on wed, apr 11>>> >>> 
scan in-store for order pickup>>> >>> >>> 9019560>>>    warrenville target store>>> 28201 diehl rd, warrenville, il 60555""

var regex = ""[0-9]{1,5} .+, .+, [a-z]{2} [0-9]{5}"";

var address = text.match(regex);
console.log(""address"",address);

// address = 28201 diehl rd, warrenville, il 60555

explanation:
[0-9]{1,3}: 1 to 3 digits, the address number
(space): a space between the number and the street name
.+: street name, any character for any number of occurrences
,: a comma and a space before the city
.+: city, any character for any number of occurrences
,: a comma and a space before the state
[a-z]{2}: exactly 2 uppercase chars from a to z
[0-9]{5}: 5 digits
text.match(regex) will return an array with all the occurrences found.
however this regex is only used for particular type of address format.",https://stackoverflow.com/questions/69476453,node.js,07-10-2021 06:37,876.0,0.0,1.0,True,14-10-2021 05:18,07-10-2021 11:31
78149859,is there a way to integrate vector embeddings in a langhcain agent,"i'm trying to use the langchain react agents and i want to give them my pinecone index for context. i couldn't find any interface that let me provide the llm that uses the react chain my vector embeddings as well.
here i set up the llm and retrieve my vector embedding.
llm = chatopenai(temperature=0.1, model_name=""gpt-4"")
retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})

here i start my react chain.
prompt = hub.pull(""hwchase17/structured-chat-agent"")
agent = create_structured_chat_agent(llm, tools, prompt)
agent_executor = agentexecutor(agent=agent, tools=tools, verbose=true)
result = agent_executor.invoke(
    {
        ""input"": question,
        ""chat_history"": chat_history
    }
)

before using the react agent, i used the vector embedding like this.
crc = conversationalretrievalchain.from_llm(llm, retriever)
result = crc.invoke({'question': systemprompt, 'chat_history': chat_history})
chat_history.append((question, result['answer']))

is there any way to combine both methods and have a react agent that also uses vector embeddings?","['python', 'openai-api', 'langchain']",78150091,"you can specify the retriever as a tool for the agent. example:
from langchain.tools.retriever import create_retriever_tool


retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})

retriever_tool = create_retriever_tool(
    retriever,
    ""retriever_name"",
    ""a detailed description of the retriever and when the agent should use it."",
)

tools = [retriever_tool]

agent_executor = agentexecutor(agent=agent, tools=tools, verbose=true)

references:

agent > retriever (langchain)",https://stackoverflow.com/questions/78149859,python,12-03-2024 20:27,730.0,2.0,1.0,True,13-03-2024 01:49,13-03-2024 01:49
34831551,ensure the gensim generate the same word2vec model for different runs on the same data,"in lda model generates different topics everytime i train on the same corpus , by setting the np.random.seed(0), the lda model will always be initialized and trained in exactly the same way. 
is it the same for the word2vec models from gensim? by setting the random seed to a constant, would the different run on the same dataset produce the same model?
but strangely, it's already giving me the same vector at different instances. 
>>> from nltk.corpus import brown
>>> from gensim.models import word2vec
>>> sentences = brown.sents()[:100]
>>> model = word2vec(sentences, size=10, window=5, min_count=5, workers=4)
>>> model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
>>> model = word2vec(sentences, size=10, window=5, min_count=5, workers=4)
>>> model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
>>> model = word2vec(sentences, size=20, window=5, min_count=5, workers=4)
>>> model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)
>>> model = word2vec(sentences, size=20, window=5, min_count=5, workers=4)
>>> model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)
>>> exit()
alvas@ubi:~$ python
python 2.7.11 (default, dec 15 2015, 16:46:19) 
[gcc 4.8.4] on linux2
type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from nltk.corpus import brown
>>> from gensim.models import word2vec
>>> sentences = brown.sents()[:100]
>>> model = word2vec(sentences, size=10, window=5, min_count=5, workers=4)
>>> word0 = sentences[0][0]
>>> model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
>>> model = word2vec(sentences, size=20, window=5, min_count=5, workers=4)
>>> model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)

is it true then that the default random seed is fixed? if so, what is the default random seed number? or is it because i'm testing on a small dataset? 
if it's true that the the random seed is fixed and different runs on the same data returns the same vectors, a link to a canonical code or documentation would be much appreciated.","['python', 'random', 'gensim', 'word2vec', 'word-embedding']",34849797,"yes, default random seed is fixed to 1, as described by the author in  vectors for each word are initialised using a hash of the concatenation of word + str(seed).
hashing function used, however, is pythonï¿½ï¿½ï¿½s rudimentary built in hash function and can produce different results if two machines differ in

32 vs 64 bit, reference
python versions, reference
different operating systems/ interpreters, reference1, reference2

above list is not exhaustive. does it cover your question though?
edit
if you want to ensure consistency, you can provide your own hashing function as an argument in word2vec
a very simple (and bad) example would be:
def hash(astring):
   return ord(astring[0])

model = word2vec(sentences, size=10, window=5, min_count=5, workers=4, hashfxn=hash)

print model[sentences[0][0]]",https://stackoverflow.com/questions/34831551,python,16-01-2016 20:05,9931.0,15.0,6.0,True,25-04-2022 02:15,23-05-2017 12:18
76447153,how to use a llama model with langchain? it gives an error: pipeline cannot infer suitable model classes from: &lt;model_name&gt; - huggingface,"finetuned a model ( using peft and lora and saved as  now im getting pipeline cannot infer suitable model classes from when trying to use it along with with langchain and chroma vectordb:
from langchain.embeddings import huggingfacehubembeddings
from langchain import prompttemplate, huggingfacehub, llmchain
from langchain.chains import retrievalqa
from langchain.prompts import prompttemplate
from langchain.vectorstores import chroma

repo_id = ""sentence-transformers/all-mpnet-base-v2""
embedder = huggingfacehubembeddings(
    repo_id=repo_id,
    task=""feature-extraction"",
    huggingfacehub_api_token=""xxxxx"",
)
comments = [""foo"", ""bar""]
embeddings = embedder.embed_documents(texts=comments)
docsearch = chroma.from_texts(comments, embedder).as_retriever()
#docsearch = chroma.from_documents(texts, embeddings)

llm = huggingfacehub(repo_id='lucas0/empath-llama-7b', huggingfacehub_api_token='xxxxx')
qa = retrievalqa.from_chain_type(llm=llm, chain_type=""stuff"", retriever=docsearch, return_source_documents=false)

q = input(""input your query:"")
result = qa.run(query=q)

print(result[""result""])


is anyone able to tell me how to fix this? is it an issue with the model card? i was facing issues with the lack of the config.json file and ended up just placing the same config.json as the model i used as base for the lora fine-tuning. could that be the origin of the issue? if so, how to generate the correct config.json without having to get the original llama weights?
also, is there a way of loading several sentences into a custom hf model (not only openai, as the tutorial show) without using vector dbs?
thanks!

the same issue happens when trying to run the api on the model's hf page:","['python', 'huggingface-transformers', 'langchain', 'chromadb', 'large-language-model']",76469646,"before using the langchain api to the huggingface model, you should try to load the model in huggingface:
from transformers import automodel

model = automodel.from_pretrained('lucas0/empath-llama-7b')

and that'll throw some errors:
---------------------------------------------------------------------------
oserror                                   traceback (most recent call last)
<ipython-input-2-1b9ce76f5421> in <cell line: 3>()
      1 from transformers import automodel
      2 
----> 3 model = automodel.from_pretrained('lucas0/empath-llama-7b')

1 frames
/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   2553                             )
   2554                         else:
-> 2555                             raise environmenterror(
   2556                                 f""{pretrained_model_name_or_path} does not appear to have a file named""
   2557                                 f"" {_add_variant(weights_name, variant)}, {tf2_weights_name}, {tf_weights_name} or""

oserror: lucas0/empath-llama-7b does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

then looking into the model files, it looks like only the adapter model is saved and not the model,  so the automodel is throwing tantrums.
to load an adapted model, you have to the base model and the peft (adapter model separated, first the installs (restart after installs, if needed):
! pip install -u peft accelerate
! pip install -u sentencepiece
! pip install -u transformers

then to load the model, take a look at the guanaco example, trying to install guanaco (pip install guanaco) for a text classification model but getting error (you will need a gpu runtime)
import torch
from peft import peftmodel    
from transformers import automodelforcausallm, autotokenizer, llamatokenizer, stoppingcriteria, stoppingcriterialist, textiteratorstreamer

model_name = ""decapoda-research/llama-7b-hf""
adapters_name = 'lucas0/empath-llama-7b'

print(f""starting to load the model {model_name} into memory"")

m = automodelforcausallm.from_pretrained(
    model_name,
    #load_in_4bit=true,
    torch_dtype=torch.bfloat16,
    device_map={"""": 0}
)
m = peftmodel.from_pretrained(m, adapters_name)
m = m.merge_and_unload()
tok = llamatokenizer.from_pretrained(model_name)
tok.bos_token_id = 1

stop_token_ids = [0]

print(f""successfully loaded the model {model_name} into memory"")


now you can load the model that you've adapted/fine-tuned in huggingface transformers, you can try it with langchain, before that we have to dig the langchain code, to use a prompt with hf model, users are told to do this:
from langchain import prompttemplate, llmchain, huggingfacehub

template = """""" hey llama, you like to eat quinoa. whatever question i ask you, you reply with ""waffles, waffles, waffles!"".
 question: {input} answer: """"""
prompt = prompttemplate(template=template, input_variables=[""input""])


model = huggingfacehub(repo_id=""facebook/mbart-large-50"",
                       model_kwargs={""temperature"": 0, ""max_length"":200},
chain = llmchain(prompt=prompt, llm=model)

but when we look at the huggingfacehub object it isn't just a vanilla automodel from transformers huggingface.
when we look at  we see that it's trying to load the llm=... argument with some wrapper class, so we dig deeper into langchain's huggingfacehub object at 
the huggingfacehub object wraps over the huggingface_hub.inference_api.inferenceapi for the text-generation, text2text-generation or summarization tasks
and huggingfacehub looks like some spaghetti like object that inherits from llm object 
to summarize this a little, we want to:

load a huggingfacehub with langchain api,
and the huggingfacehub is actually a wrapper over the huggingface_hub.inference_api.inferenceapi
and the huggingfacehub object is a subclass of llm.base.llm


given that knowledge on the huggingfacehub object, now, we have several options:
opinion: the easiest way around it is to totally avoid langchain, since it's wrapper around things, you can write your customized wrapper that skip the levels of inheritance created in langchain to wrap around as many tools as it can/need
ideally: ask the langchain developer/maintainer to load peft/adapter model and write another subclass for them
practical:* lets hack the thing and write our own llm subclass.

practical solution:
lets try to hack up a new llm subclass
from typing import any, dict, list, mapping, optional

from pydantic import extra, root_validator

from langchain.callbacks.manager import callbackmanagerforllmrun
from langchain.llms.base import llm
from langchain.llms.utils import enforce_stop_tokens

from langchain import prompttemplate, llmchain

class huggingfacehugs(llm):
  pipeline: any
  class config:
    """"""configuration for this pydantic object.""""""
    extra = extra.forbid

  def __init__(self, model, tokenizer, task=""text-generation""):
    super().__init__()
    self.pipeline = pipeline(task, model=model, tokenizer=tokenizer)

  @property
  def _llm_type(self) -> str:
    """"""return type of llm.""""""
    return ""huggingface_hub""

  def _call(self, prompt, stop: optional[list[str]] = none, run_manager: optional[callbackmanagerforllmrun] = none,):
    # runt the inference.
    text = self.pipeline(prompt, max_length=100)[0]['generated_text']
    
    # @alvas: i've totally no idea what this in langchain does, so i copied it verbatim.
    if stop is not none:
      # this is a bit hacky, but i can't figure out a better way to enforce
      # stop tokens when making calls to huggingface_hub.
      text = enforce_stop_tokens(text, stop)
    print(text)
    return text[len(prompt):]


template = """""" hey llama, you like to eat quinoa. whatever question i ask you, you reply with ""waffles, waffles, waffles!"".
 question: {input} answer: """"""
prompt = prompttemplate(template=template, input_variables=[""input""])


hf_model = huggingfacehugs(model=m, tokenizer=tok)

chain = llmchain(prompt=prompt, llm=hf_model)

chain(""who is princess momo?"")

phew, langchain didn't complain... and here's the output:
{'input': 'who is princess momo?',
 'text': ' she is a princess.  she is a princess.  she is a princess.  she is a princess.  she is a princess.  she is a princess.  she is a princess.  she is'}

epilogue: apparently this llama model doesn't understand that all it needs to do is to reply waffles, waffles, waffles

tl;dr
see",https://stackoverflow.com/questions/76447153,python,10-06-2023 16:38,13028.0,2.0,1.0,True,14-06-2023 02:44,14-06-2023 02:44
74345093,how to get predictions for new data from multinomialnb?,"i'm venturing into a new topic and experimenting with categorising product names. without deeper knowledge, the use of multinomialnb (superficially) already yielded quite good results for my use case.
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import countvectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import multinomialnb


df = pd.dataframe({
    'title':['short shirt', 'long shirt','green shoe','cool sneaker','heavy ballerinas'],
    'label':['shirt','shirt','shoe','shoe','shoe']
})

count_vec = countvectorizer()
bow = count_vec.fit_transform(df['title'])
bow = np.array(bow.todense())

x = bow
y = df['label']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, stratify=y)
model = multinomialnb().fit(x_train, y_train)

model.predict(x_test)

based on the trainigs of the above simplified example, i would like to categorise completely new titles and output them with the predicted labels:
new = pd.dataframe({
    'title':['long top', 'super shirt','white shoe','super cool sneaker','perfect fit ballerinas'],
    'label': np.nan
})

unfortunately, i am not sure of the next steps and would hope for some support.
...
count_vec = countvectorizer()
bow = count_vec.fit_transform(new['title'])
bow = np.array(bow.todense())
model.predict(bow)","['scikit-learn', 'nlp', 'classification', 'prediction', 'naivebayes']",74354303,"it's a mistake to fit countvectorizer on the whole dataset, because the test set should not be used at all during training. this discipline not only follows proper ml principles (to prevent data leakage), it also avoids this practical problem: when the test set is prepared together with the training set, it gets confusing to apply the model to another test set.
the clean way to proceed is to always split the data first between training and test set, this way one is forced to correctly transform the test set independently from the training set. then it's easy to apply the model on another test set.
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import countvectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import multinomialnb
from sklearn import preprocessing

df = pd.dataframe({
    'title':['short shirt', 'long shirt','green shoe','cool sneaker','heavy ballerinas'],
    'label':['shirt','shirt','shoe','shoe','shoe']
})

x = df['title']
y = df['label']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, stratify=y)

# 1) training: use only training set!

# labels should be encoded
le = preprocessing.labelencoder()
y_train_enc = le.fit_transform(y_train)

count_vec = countvectorizer()
x_train_bow = count_vec.fit_transform(x_train)
x_train_bow = np.array(x_train_bow.todense())

model = multinomialnb().fit(x_train_bow, y_train_enc)



# 2) testing: apply previous transformation to test set before applying model
x_test_bow = count_vec.transform(x_test)
x_test_bow = np.array(x_test_bow.todense())
y_test_enc = model.predict(x_test_bow)

print(""predicted labels test set 1:"",le.inverse_transform(y_test_enc))



# 3) apply to another dataset = just another step of testing, same as above
new = pd.dataframe({
    'title':['long top', 'super shirt','white shoe','super cool sneaker','perfect fit ballerinas'],
    'label': np.nan
})
x_new = new['title']
x_new_bow = count_vec.transform(x_new)
x_new_bow = np.array(x_new_bow.todense())
y_new_enc = model.predict(x_new_bow)

print(""predicted labels test set 2:"", le.inverse_transform(y_new_enc))

notes:

this point is not specific to multinomialnb, this is the correct method for any classifier.
with real data it's often a good idea to use the min_df argument with countvectorizer, because rare words increase the number of features, don't help predicting the label and cause overfitting.",https://stackoverflow.com/questions/74345093,scikit-learn,07-11-2022 10:25,868.0,2.0,1.0,True,07-11-2022 23:57,07-11-2022 19:34
73881130,python regex - extract all the matching text between two patterns,"i want to extract all the text in the bullet points numbered as 1.1, 1.2, 1.3 etc. sometimes the bullet points can have space like 1. 1, 1. 2, 1 .3, 1 . 4
sample text
    text = ""some text before pattern 1.1 text_1_here  1.2 text_2_here  1 . 3 text_3_here  1. 4 text_4_here  1 .5 text_5_here 1.10 last_text_here 1.23 text after pattern""

for the text above, the output should be
[' text_1_here  ', ' text_2_here  ', ' text_3_here  ', ' text_4_here  ', ' text_5_here ', ' last_text_here  ']
i tried regex findall but not getting the required output. it is able to identify and extract 1.1 & 1.2 and then 1.3 & 1.4. it is skipping text between 1.2 & 1.3.
    import re
    re.findall(r'[0-9].\s?[0-9]+(.*?)[0-9].\s?[0-9]+', text)","['python', 'regex', 'text', 'text-mining']",73881270,"i'm unsure about the exact rule why you'd want to exclude the last bit of text but based on your comments it seems we could also just split the entire text on the bullits and simply exclude the 1st and last element from the resulting array:
re.split(r'\s+\d(?:\s*\.\s*\d+)+\s+', text)[1:-1]

which would output:
['text_1_here', 'text_2_here', 'text_3_here', 'text_4_here', 'text_5_here', 'last_text_here']",https://stackoverflow.com/questions/73881130,python,28-09-2022 12:07,496.0,0.0,1.0,True,28-09-2022 12:28,28-09-2022 12:28
51369858,spacy - nlp.pipe() returns generator,"i am using spacy for nlp in python. i am trying to use nlp.pipe() to generate a list of spacy doc objects, which i can then analyze. oddly enough, nlp.pipe() returns an object of the class <generator object pipe at 0x7f28640fefa0>. how can i get it to return a list of docs, as intended?
import spacy
nlp = spacy.load('en_depent_web_md', disable=['tagging', 'parser'])
matches = ['one', 'two', 'three']
docs = nlp.pipe(matches)
docs","['python', 'nlp', 'spacy']",51369937,"for iterating through docs just do 
for item in docs

or do
 list_of_docs = list(docs)",https://stackoverflow.com/questions/51369858,python,16-07-2018 20:45,14057.0,9.0,3.0,True,23-09-2023 13:38,26-06-2020 05:05
76835392,openai api in r: error in chat completion - how to locate the issue?,"i'm trying to classify a large number of newspaper articles with the openai api's chat completion function in r. usually, it works quite well but with a number of articles, i get an error that i don't understand.
this is the error i get:

error in is.na(output) || !is.character(output) :
'length = 3' in coercion to 'logical(1)'

one example is:
# libraries loaded at that moment
library(tidyverse)
library(httr)
library(tidytext)
library(purrr)

system_task <- ""you will be provided one article per prompt by the austrian newspaper der standard. the article is written in german. each prompt only contains one article. all of these mention the justice system (or actors in the justice system such as staatsanwalt, wksta, richter or gericht) or the austrian constitutional court (called vfgh or verfassungsgerichtshof). we want you to take multiple steps for each of these articles: 1) summarize the article in 3 sentences.  answer this question with the prefix '1)'. 2) does the article mention that politicians critisize the judiciary or the vfgh? answer this question with the prefix '2)'.3) does the article mention that politicians attack the judiciary or the vfgh?  answer this question with the prefix '3)' 4) if 2) or 3) is answered with yes: who is criticized or attacked by whom, name the actors. remember that we are not interested in conflict between politicians, only between politicians and the judiciary. answer this question with the prefix '4)'""

article <- 'in der adventzeit rï¿½ï¿½cken armut, flucht, krankheit und obdachlosigkeit vermehrt in den fokus der ï¿½ï¿½ffentlichkeit. nicht nur in der weihnachtsgeschichte geht es um armut, flucht, krankheit und obdachlosigkeit. im advent rï¿½ï¿½cken diese themen wiederholt in das blickfeld der ï¿½ï¿½ffentlichkeit, denn fï¿½ï¿½r viele familien ist weihnachten aus finanziellen und gesundheitlichen grï¿½ï¿½nden keine besinnliche zeit. rund 140.000 menschen in ï¿½ï¿½sterreich kssreichend heizen. das sagte caritas-prï¿½ï¿½sident michael landau am sonntagvormittag in der orf-""pressestunde"". das ziel der neuen regierung mï¿½ï¿½sse sein, kinderarmut und altersarmut zu senken. landau hatte der tï¿½ï¿½rkis-blauen regierung wï¿½ï¿½hrend der vergangenen legislaturperiode eine ""demontage"" des sozialstaats vorgeworfen. im orf-studio betonte er wiederholt seine ""erleichterung"" ï¿½ï¿½ber den entscheid des verfassungsgerichtshofs, die kernpunkte der neuen sozialhilfe von tï¿½ï¿½rkis-blau zu kippen. die hï¿½ï¿½chstrichter hatten am dienstag sowohl die starken kï¿½ï¿½rzungen fï¿½ï¿½r kinderreiche familien als auch fï¿½ï¿½r menschen mit schlechten deutsch- oder englischkenntnissen gekippt. ""die mindestsicherung muss armut vermeiden"", sagte landau. eine kï¿½ï¿½rzung der familienzuschlï¿½ï¿½ge auf 44 euro ab dem dritten kind ï¿½ï¿½ï¿½ wie in der neuen sozialhilfe von tï¿½ï¿½rkis-blau vorgesehen ï¿½ï¿½ï¿½ ""entspricht nicht der lebenswirklichkeit von menschee familien seien sowieso schon ""in besonderer weise armutsgefï¿½ï¿½hrdet"". die vfgh-entscheidung sei eine chance, landau plï¿½ï¿½diert fï¿½ï¿½r eine neuregelung unter einbindung der praktiker und hilfsorganisationen. auch im pflegebereich brauche man dringend neue lï¿½ï¿½sungen. die vorige bundesregierung habe vï¿½ï¿½llig zu recht erkannt, dass dieses thema viele menschen berï¿½ï¿½hre. ï¿½ï¿½sterreich ist pflegebedï¿½ï¿½rftig, so landau ï¿½ï¿½ï¿½ er hoffe auf die neue regierung, egal wie sie aussehe. es brauche einen vergleichbaren qualitï¿½ï¿½ts-, versorgungs- und finanzierungsrahmen. der zugang zur pflege mï¿½ï¿½sse fï¿½ï¿½r alle flï¿½ï¿½chendeckend leistbar sein und es brauche auch anstrengungen, damit sich menschen fï¿½ï¿½r den pflegeberuf entscheiden. die zahl der pflegebedï¿½ï¿½rftigen menschen in ï¿½ï¿½sterreich ist gestiegen, damit haben sich laut statistik austria auch die kosten fï¿½ï¿½r die betreuung krï¿½ï¿½ftig erhï¿½ï¿½ht. die pflegekosten des staats sind seit 2013 um ein drittel angeseimen und pflegehï¿½ï¿½usern untergebracht. bei den hausbesuchen durch pflegefachkrï¿½ï¿½fte war der anstieg geringer. neben dem plus der ï¿½ï¿½lteren, pflegebedï¿½ï¿½rftigen menschen habe sich aber auch die abschaffung des pflegeregress ausgewirkt. sozialministerin brigitte zarfl hatte im november zwei studien zur zukunft der pflege in ï¿½ï¿½sterreich prï¿½ï¿½sentiert. die zahlen deuteten auf einen bevorstehenden personalmangel hin. (red, 22.12.2019)'

expr <- post(
      # api link
      url = ""
      # authorizatiob
      add_headers(authorization = paste(""bearer"", chatgpt_api)),
      # output in json
      content_type_json(),
      # encode the value to json format
      encode = ""json"",
      # low randomness of answers
      temperature = 0,
      stop = none,
      # controlling what to show as the output
      body = list(
        model = ""gpt-3.5-turbo"",
        messages = list(list(role = ""user"", conte               list(role = ""system"", content = system_task))))

does anyone have the idea where the issue might lie and how to fix it?
i tried wrapping it in a trycatch but that didn't work either:
chatgpt_response <- trycatch(
    
    # try to retrieve answer for openai api
    expr = {
      post(
      # api link
      url = ""
      # authorizatiob
      add_headers(authorization = paste(""bearer"", chatgpt_api)),
      # output in json
      content_type_json(),
      # encode the value to json format
      encode = ""json"",
      # low randomness of answers
      temperature = 0,
      stop = none,
      # controlling what to show as the output
      body = list(
        model = ""gpt-3.5-turbo"",
         messages = list(list(role = ""user"", content = article),
                        list(role = ""system"", content = system_task)
                        )))
      },
    # in case an error occurs while using api
    error = function(e){
      message(paste(""for article there was an error.""))
      return(na)
    },
    # notify about a possible warning while using api
    warning = function(w){
      message(paste(""for article there was a warning.""))
    }
  )","['r', 'openai-api', 'chatgpt-api', 'gpt-3']",76852302,"what helped in the end was to switch to the openai package.
library(tidyverse)
library(openai)
library(tidytext)
library(purrr)

system_task <- ""you will be provided one article per prompt by the austrian newspaper der standard. the article is written in german. each prompt only contains one article. all of these mention the justice system (or actors in the justice system such as staatsanwalt, wksta, richter or gericht) or the austrian constitutional court (called vfgh or verfassungsgerichtshof). we want you to take multiple steps for each of these articles: 1) summarize the article in 3 sentences.  answer this question with the prefix '1)'. 2) does the article mention that politicians critisize the judiciary or the vfgh? answer this question with the prefix '2)'.3) does the article mention that politicians attack the judiciary or the vfgh?  answer this question with the prefix '3)' 4) if 2) or 3) is answered with yes: who is criticized or attacked by whom, name the actors. remember that we are not interested in conflict between politicians, only between politicians and the judiciary. answer this question with the prefix '4)'""

article <- 'in der adventzeit rï¿½ï¿½cken armut, flucht, krankheit und obdachlosigkeit vermehrt in den fokus der ï¿½ï¿½ffentlichkeit. nicht nur in der weihnachtsgeschichte geht es um armut, flucht, krankheit und obdachlosigkeit. im advent rï¿½ï¿½cken diese themen wiederholt in das blickfeld der ï¿½ï¿½ffentlichkeit, denn fï¿½ï¿½r viele familien ist weihnachten aus finanziellen und gesundheitlichen grï¿½ï¿½nden keine besinnliche zeit. rund 140.000 menschen in ï¿½ï¿½sterreich kï¿½ï¿½nnten ihre wohnung nicht ausreichend heizen. das sagte caritas-prï¿½ï¿½sident michael landau am sonntagvormittag in der orf-""pressestunde"". das ziel der neuen regierung mï¿½ï¿½sse sein, kinderarmut und altersarmut zu senken. landau hatte der tï¿½ï¿½rkis-blauen regierung wï¿½ï¿½hrend der vergangenen legislaturperiode eine ""demontage"" des sozialstaats vorgeworfent;erleichterung"" ï¿½ï¿½ber den entscheid des verfassungsgerichtshofs, die kernpunkte der neuen sozialhilfe von tï¿½ï¿½rkis-blau zu kippen. die hï¿½ï¿½chstrichter hatten am dienstag sowohl die starken kï¿½ï¿½rzungen fï¿½ï¿½r kinderreiche familien als auch fï¿½ï¿½r menschen mit schlechten deutsch- oder englischkenntnissen gekippt. ""die mindestsicherung muss armut vermeiden"", sagte landau. eine kï¿½ï¿½rzung der familienzuschlï¿½ï¿½ge auf 44 euro ab dem dritten kind ï¿½ï¿½ï¿½ wie in der neuen sozialhilfe von tï¿½ï¿½rkis-blau vorgesehen ï¿½ï¿½ï¿½ ""entspricht nicht der lebenswirklichkeit von menschen"". alleinerzieherinnen und kinderreiche, einkommensschwache familien seien sowieso schon ""in besonderer weise armutsgefï¿½ï¿½hrdet"". die vfgh-entscheidung sei eine chance, landau plï¿½ï¿½diert fï¿½ï¿½r eine neuregelung unter einbindung der praktiker und hilfsorganisationen. auch im pflegebereich brauche man dringend neue lï¿½ï¿½sungen. die vorige bundesregierung habe vï¿½ï¿½llig zu recst pflegebedï¿½ï¿½rftig, so landau ï¿½ï¿½ï¿½ er hoffe auf die neue regierung, egal wie sie aussehe. es brauche einen vergleichbaren qualitï¿½ï¿½ts-, versorgungs- und finanzierungsrahmen. der zugang zur pflege mï¿½ï¿½sse fï¿½ï¿½r alle flï¿½ï¿½chendeckend leistbar sein und es brauche auch anstrengungen, damit sich menschen fï¿½ï¿½r den pflegeberuf entscheiden. die zahl der pflegebedï¿½ï¿½rftigen menschen in ï¿½ï¿½sterreich ist gestiegen, damit haben sich laut statistik austria auch die kosten fï¿½ï¿½r die betreuung krï¿½ï¿½ftig erhï¿½ï¿½ht. die pflegekosten des staats sind seit 2013 um ein drittel angestiegen, zeigen aktuelle zahlen. 13.000 menschen mehr als im vorjahr waren in heimen und pflegehï¿½ï¿½usern untergebracht. bei den hausbesuchen durch pflegefachkrï¿½ï¿½fte war der anstieg geringer. neben dem plus der ï¿½ï¿½lteren, pflegebedï¿½ï¿½rftigen menschen habe sich aber auch die abschaffung des pflegeregress ausgewirkt. sozialministerin brigitte zarfl hatte im november zwei studien zur zukunft der pflege inenden personalmangel hin. (red, 22.12.2019)'

gpt_response <- create_chat_completion(
            model = ""gpt-3.5-turbo"",
            messages = list(
              list(
                ""role"" = ""system"",
                ""content"" = system_task
              ),
              list(
                ""role"" = ""user"",
                ""content"" = article
              )
            )
          )

response_text <- gpt_response |> 
      pluck('choices') |> 
      pull(message.content)",https://stackoverflow.com/questions/76835392,r,04-08-2023 11:15,225.0,0.0,1.0,True,07-08-2023 13:40,04-08-2023 12:36
74029113,"given an r dataframe with two columns with strings of words in each, remove words repeating in between the first and second column in each row","i have an r data frame like the example below (but with ten of thousands of rows).
a1 <- c(""ab ac ad ae af ag"",""ab ad ah ai aj"")
q1 <- c(""ab ac ae"",""ad aj ai"")
id <- 1:2
df <- data.frame(id,a1,q1)

i would like the result to look like this:
df$a_clean <- c(""ad af ag"", ""ab ah"")

i have tried using the ""str_split"" and ""exclude"" function that is part of the qdap package but this seems to work on the whole column at once rather than on a row-by-row basis in the data frame and just gives me the unique words in each row of a1 excluding all the words in the q1 column.","['r', 'string', 'data-cleaning', 'text-mining']",74029291,"in base r, with intersect:
mapply(\(x, y) paste(setdiff(x, y), collapse = "" ""), 
       strsplit(df$a1, "" ""), strsplit(df$q1, "" ""))
#[1] ""ad af ag"" ""ab ah""",https://stackoverflow.com/questions/74029113,r,11-10-2022 13:40,51.0,1.0,3.0,True,11-10-2022 14:09,11-10-2022 14:01
78505822,chromadb retrieval with metadata filtering is very slow,"i have a collection that contains about 300k chunks. i don't think it is a huge amount but the retrieval process is very slow when a metadata filter is applied. it sometimes take up to 180 seconds to retrieve 10 documents, while taking only 2 seconds without filter.
i am using bgelarge as my embedding model and langchain retriever.invoke() as my retrieval function.
does anyone encounter same situation? i wonder if it is possible to speed up the filtering process such as setting up an index, like what we can do in mongodb.
i looked up on the internet but it seems that although some people do complain about chromadb being slow, so far no one has it as slow as i do.
below is my code of setting up the retriever
def set_retriever(self, search_type='mmr', search_kwargs={'k':3}):
  """"""
  used for document retrieval
  search type is defaulted as 'mmr'
  use retriever.invoke(query) to retrieve documents
  """"""
  self.retriever = self.langchain_chroma.as_retriever(search_type=search_type, search_kwargs=search_kwargs)
    
  return self.retriever

below is my code for the retrieval
retriever = db.set_retriever(search_kwargs={
  ""filter"":{
    ""publishdate"":{'$gte':start_date.timestamp()}
    }
  }
)
    
t1 = time.perf_counter()
results = retriever.invoke(""some question"")
t2 = time.perf_counter()
print(f""total time taken: "", round(t2-t1,3))
print(results)

i also used chromadb.collection.query(where={""some filter""}) but it didn't help.","['information-retrieval', 'chromadb', 'vector-database', 'retrieval-augmented-generation']",78757686,"chroma db does not currently create indices on metadata. this is still an open issue in their repo as far as i can see. a workaround is to apply filtering manually after performing vector search. i had similar performance issues with only ~50k documents. personally i would advise using milvus or pinecone for non-trivially-sized collections.
see the thread and the open pr",https://stackoverflow.com/questions/78505822,information-retrieval,20-05-2024 09:43,1462.0,1.0,1.0,True,17-07-2024 04:51,20-05-2024 10:00
78053868,using showdown js with openai streaming response,"i tried using showdownjs to translate streamed markdown from openai to html
i changed the code given at  and just added the showdown part
the system prompt to openai includes returning responses using markdown, which it does
after the showdownjs parsed , the results are weird. each chunk is in a separate line and the markdown isn't parsed!

                        let converter = new showdown.converter({smoothlivepreview: true});
                        let parsedhtml = converter.makehtml(txt);

                        div.innerhtml += parsedhtml;


the data does come back from the backend as a stream
am totally flummoxed. what am i doing wrong here? i have the references all included and the data does come back from the php file in the backend.
enter image description here
thanks in advance
edited
i just realized that showdown is adding a ""html p"" tag around every word in every stream:-( and, the text with markdown (sometimes incomplete in the chunk), do not get processed and converted to html :-(","['javascript', 'stream', 'openai-api']",78056159,"i finally figured out a very simple solution. duh.
i decided to use the markdown-it library from 
and in the above code, rather than applying markdown when the text is streamed, i do it at the end on getting ""done""
am reproducing just the relevant part of the code here for brevity, with my solution. works like charm. should have thought of it before! ideally, i would like it to happen when the data streams, but looks like that is either not possible or too much hard work !!!
                if (e.data == ""[done]"") {
                    msgersendbtn.disabled = false;
                    document.getelementbyid(""usersendbuttonav"").value=""send"";
                    var elemb = document.getelementbyid(""usersendbuttonav"");
                    elemb.value = ""send"";
                
                    const md = window.markdownit();
                    const renderedtext = md.render(div.innertext);
                    div.innerhtml = renderedtext;


                    document.getelementbyid('usermessageav').placeholder='enter your message now...';
                    document.getelementbyid('usermessageav').disabled = false;
                

                    eventsource.close();
                } else {
                    
                    //original code  let txt = json.parse(e.data).choices[0].delta.content
                    if (isjsonstring(e.data)) {
                        let txt = json.parse(e.data).choices[0].delta.content;

                    if (txt !== undefined) {
                        div.innerhtml += txt.replace(/(?:\r\n|\r|\n)/g, '<br>');
                    }
                } 
                        
            }",https://stackoverflow.com/questions/78053868,javascript,24-02-2024 20:23,1421.0,0.0,1.0,True,25-02-2024 13:42,25-02-2024 09:57
73391230,how to run an end to end example of distributed data parallel with hugging face&#39;s trainer api (ideally on a single node multiple gpus)?,"i've extensively look over the internet, hugging face's (hf's) discuss forum & repo but found no end to end example of how to properly do ddp/distributed data parallel with hf (links at the end).
this is what i need to be capable of running it end to end:

do we wrap the hf model in ddp? (script needs to know how to synchronize stuff at some point somehow somewhere, otherwise just launching torch.distributed from the command line)
do we change the args to trainer or trainer args in anyway?
wrap the optimizer in any distributed trainer (like cherry? cherry is a pytorch lib for things like this)
do we do the usual init group that is usually needed for ddp?
what is the role of local rank?
terminal launch script e.g. python -m torch.distributed.launch --nproc_per_node=2 distributed_maml.py
how do we use the world size to shard the data at each loop e.g. see 

given answers to those i think i could write my own notebook and share it widely.
this is my starter code that i want to complete but unsure if i am doing it right (especially since i don't know which args to trainer to change):
""""""

- training on multiple gpus: 
- data paralelism, dp vs ddp: 
- github example: 
    - above came from hf discuss: 

ï¿½ï¿½ï¿½ single node / multi-gpu

model fits onto a single gpu:

ddp - distributed dp
zero - may or may not be faster depending on the situati configuration used.

...

python -m torch.distributed.launch \
    --nproc_per_node number_of_gpu_you_have path_to_script.py \
    --all_arguments_of_the_script

python -m torch.distributed.launch --nproc_per_node 2 main_data_parallel_ddp_pg.py
python -m torch.distributed.launch --nproc_per_node 2 ~/ultimate-utils/tutorials_for_myself/my_hf_hugging_face_pg/main_data_parallel_ddp_pg.py

e.g.
python -m torch.distributed.launch \
    --nproc_per_node 8 pytorch/text-classification/run_glue.py \

    --model_name_or_path bert-large-uncased-whole-word-masking \
    --task_name mnli \
    --do_train \
    --do_eval \
    --max_seq_length 128 \
    --per_device_train_batch_size 8 \
    --learning_rate 2e-5 \
    --num_train_epochs 3.0 \
    --output_dir /tmp/mnli_output/
""""""
# %%

# - init group
# - set up processes a la l2l
# local_rank: int = local_rank: int = int(os.environ[""local_rank""]) # get_local_rank()
# print(f'{local_rank=}')
## init_process_group_l2l(args, local_rank=local_rank, world_size=args.world_size, init_method=args.init_method)
# init_process_group_l2l bellow
# if is_running_parallel(rank):
#     print(f'----> setting up rank={rank} (with world_size={world_size})')
#     # master_addr = 'localhost'
#     master_addr = '127.0.0.1'
#     master_port = master_port
#     # set up the master's ip address so this child process can coordinate
#     os.environ['master_addr'] = master_addr
#     print(f""---> {master_addr=}"")
#     os.environ['master_port'] = master_port
#     print(f""---> {master_port=}"")
#
#     # - use nccl if you are using gpus: 
#     if torch.cuda.is_available():
#         backend = 'nccl'
#         # you need to call torch_uu.cuda.set_device(rank) before init_process_group is called. 
#         torch.cuda.set_device(
#             args.device)  # is this right if we do parallel cpu? # you need to call torch_uu.cuda.set_device(rank) before init_process_group is called. 
#     print(f'---> {backend=}')
# rank: int = torch.distributed.get_rank() if is_running_parallel(local_rank) else -1

# 
import datasets
from datasets import load_dataset, datasetdict

books: datasetdict = load_dataset(""opus_books"", ""en-fr"")
print(f'{books=}')

books: datasetdict = books[""train""].train_test_split(test_size=0.2)
print(f'{books=}')
print(f'{books[""train""]=}')

print(books[""train""][0])
""""""
{'id': '90560',
 'translation': {'en': 'but this lofty plateau measured only a few fathoms, and soon we reentered our element.',
  'fr': 'mais ce plateau ï¿½ï¿½levï¿½ï¿½ ne mesurait que quelques toises, et bientï¿½ï¿½t nous fï¿½ï¿½mes rentrï¿½ï¿½s dans notre ï¿½ï¿½lï¿½ï¿½ment.'}}
""""""

formers import autotokenizer, pretrainedtokenizerfast, pretrainedtokenizer

tokenizer: pretrainedtokenizerfast = autotokenizer.from_pretrained(""t5-small"")
print(f'{isinstance(tokenizer, pretrainedtokenizer)=}')
print(f'{isinstance(tokenizer, pretrainedtokenizerfast)=}')

source_lang = ""en""
target_lang = ""fr""
prefix = ""translate english to french: ""


def preprocess_function(examples):
    inputs = [prefix + example[source_lang] for example in examples[""translation""]]
    targets = [example[target_lang] for example in examples[""translation""]]
    model_inputs = tokenizer(inputs, max_length=128, truncation=true)

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=128, truncation=true)

    model_inputs[""labels""] = labels[""input_ids""]
    return model_inputs


# then create a smaller subset of the dataset as previously shown to speed up the fine-tuning: (hack to seep up tutorial)
books['train'] = books[""train""].shuffle(seed=42).select(range(100))
books['test'] = books[""test""].shuffle(seed=42).select(range(100))

# # use ï¿½ï¿½ï¿½ï¿½ datasets map method to apply a preprocessing function over the entire dataset:
# tokenized_datasets = dataset.map(tokenize_function, batched=true, batch_size=2)

# todo - would be nice to remove this since gpt-2/3 size you can't preprocess the entire data set...or can you?
# tokenized_books = books.map(preprocess_function, batched=true, batch_size=2)
from uutils.torch_uu.data_uu.hf_uu_data_preprocessing import preprocess_function_translation_tutorial

preprocessor = lambda examples: preprocess_function_translation_tutorial(examples, tokenizer)
tokenized_books = books.map(preprocessor, batched=true, batch_size=2)
print(f'{tokenized_books=}')

# - load model
from transformers import automodelforseq2seqlm

model = automodelforseq2seqlm.from_pretrained(""t5-small"")

# - to ddp
# model = model().to(rank)
# from torch.nn.parallel import distributeddlel as ddp
# ddp_model = ddp(model, device_ids=[rank])

# use datacollatorforseq2seq to create a batch of examples. it will also dynamically pad your text and labels to the
# length of the longest element in its batch, so they are a uniform length.
# while it is possible to pad your text in the tokenizer function by setting padding=true, dynamic padding is more efficient.

from transformers import datacollatorforseq2seq

# data collator that will dynamically pad the inputs received, as well as the labels.
data_collator: datacollatorforseq2seq = datacollatorforseq2seq(tokenizer=tokenizer, model=model)

""""""
at this point, only three steps remain:

- define your training hyperparameters in seq2seqtrainingarguments.
- pass the training arguments to seq2seqtrainer along with the model, dataset, tokenizer, and data collator.
- call train() to fine-tune your model.
""""""
report_to = ""none""
if report_to != 'none':
    import wandb
    wandb.init(project=""playground"", entity=""brando"", name='run_name', group='expt_name')

from transformers import seq2seqtrainingarguments, seq2seqtrainer

# fp16 = true # cuda
# fp16 = false # cpu
import torch

fp16 = torch.cuda.is_available()  # true for cuda, false for cpu
training_args = seq2seqtrainingarguments(
    output_dir=""./results"",
    evaluation_strategy=""epoch"",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=1,
    fp16=fp16,
    report_to=report_to,
)

trainer = seq2seqtrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_books[""train""],
    eval_dataset=tokenized_books[""test""],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

print('\n ----- success\a')


all references i consulted when writing this question:




setting hugging face dataloader_num_workers for multi-gpu training
using huggingface trainer with distributed data parallel
why, using huggingface trainer, single gpu training is faster than 2 gpus?




dist maml: 
cross:","['python', 'machine-learning', 'pytorch', 'huggingface-transformers', 'huggingface-datasets']",73393805,"you don't need to setup anything, just do:
python -m torch.distributed.launch --nproc_per_node 2 ~/src/main_debug.py

or
torchrun --nproc_per_node=2 --nnodes=2 --use_env ~/src/main_debug.py

then monitor the gpus with nvidia-smi see:

example from alpaca:
torchrun --nproc_per_node=4 --master_port=<your_random_port> train.py \
    --model_name_or_path <your_path_to_hf_converted_llama_ckpt_and_tokenizer> \
    --data_path ./alpaca_data.json \
    --bf16 true \
    --output_dir <your_output_dir> \
    --num_train_epochs 3 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps 8 \
    --evaluation_strategy ""no"" \
    --save_strategy ""steps"" \
    --save_steps 2000 \
    --save_total_limit 1 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type ""cosine"" \
    --logging_steps 1 \
    --fsdp ""full_shard auto_wrap"" \
    --fsdp_transformer_layer_cls_to_wrap 'llamadecoderlayer' \
    --tf32 true

ref:",https://stackoverflow.com/questions/73391230,python,17-08-2022 15:35,3397.0,7.0,1.0,True,12-07-2023 21:54,17-08-2022 15:43
4512590,latin to english alphabet hashing,"i have to convert all the latin characters to their corresponding english alphabets. can i use python to do it? or is there a mapping available? 
unicode values to non-unicode characters 
ramï¿½ï¿½rez sï¿½ï¿½nchez should be converted to ramirez sanchez</cod","python, nlp",4512721,"it looks like what you want is accent removal.  you can do this with:
def strip_accents(text):
    return ''.join(char for char in
                   unicodedata.normalize('nfkd', text)
                   if unicodedata.category(char) != 'mn')

>>> strip_accents('ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½')
'aeinouu'
>>> strip_accents('ramï¿½ï¿½rez sï¿½ï¿½nchez')
'ramirez sanchez'

this works fine for spanish, but note that it doesn't always work for other languages.
>>> str",https://stackoverflow.com/q/4512590,"python, nlp",22-12-2010 18:52,2991.0,5.0,1.0,True,04-09-2023 08:54,10-05-2013 11:59
72701918,is `sklearn.pipeline` with regex really more performant than `spacy` for preprocessing huge volumes of text?,"tl;dr
i need help selecting between spacy and sklearn for processing a huge text corpus. i ran a test to measure the performance of each, but the results were unexpected. moreover, because i'm new-ish to the frameworks involved, i lack confidence that my test is completely valid. i'd really appreciate some guidance.

code

results


background
i'm doing a project that involves preprocessing 35 million reddit comments. this is a pretty massive amount of text. so i'm searching for the most efficient framework to accomplish this with.
currently, i am considering using either spacyï¿½ï¿½ï¿½s nlp.pipe with several custom components, or a sklearn.pipeline with a ton of regex-based data transformers. since (1) spacy is optimized for text and (2) regex in python is slow, i figured the spacy option is the way to go. but i wanted to test my assumptions before proceeding.
the test
so i wrote a quick and dirty script to do just that. it seems like a lot of code at first skim, but it's actually not. it's very modular, mostly consisting of simple classes. skip to if __name__ ... at the end to see the overall logic.
anyway, this script defines what i think are broadly equivalent pipelines, one spacy-based and one sklearn-based, that simply remove (1) punctuation and (2) inline code like this. these pipelines subclass an additional class which actually carries out the test. so the script loads a ~7.5k-comment sample from r/languagetechnology as a dask.dataframe (for parallelization), applies the same preprocessing 100 times using each pipeline, then averages out the results.
to be clear, my actual pipeline will do several more things than just remove punctuation and inline code. i only chose those particular transformations for testing purposes, to keep my tests simple and to the point.
results
my findings (in seconds) are as follows, illustrated graphically here:




pipeline
mean
standard dev




spacy
13.49772
1.182763


sklearn
6.853291
0.127701




clearly, spacy was massively slower. this contradicted my expectations, and leaves me unable to draw firm conclusions.
is sklearn.pipeline with regex truly the more efficient framework for this? or was there an issue with my test, or how i structured my pipelines? the latter seems plausible because almost everything the script uses is new-ish to me - dask.dataframe, spacy with custom components, and sklearn.pipeline with custom transformers. so it may very well be that e.g., i'm just using spacy wrong, or there's something about my script that renders the comparison apples to oranges instead of apples to apples.
cry for help
in light of this uncertainty, i'd sincerely appreciate some input from anyone familiar with these frameworks. i'd also appreciate some eyes on my code, if possible, just to check that i've actually used everything properly.
any and all input is welcome. thank you!","['performance', 'nlp', 'spacy', 'dask-dataframe', 'scikit-learn-pipeline']",72709520,"your regexes are faster here because they're only doing the work you need. spacy is also doing tokenization, which for your preprocessing described here is not necessary, so it's not surprising it's slower.
since it's likely you'll want tokens for whatever downstream processing you have, your current comparison may not be useful.",https://stackoverflow.com/questions/72701918,performance,21-06-2022 13:43,314.0,1.0,1.0,True,28-06-2022 06:59,28-06-2022 06:59
75608318,react memoizing the fetch result and fetch again only if new item added to the list,"i have a function that makes an api call with the product titles to openai to generate a recipe when the button is clicked. i am getting the list of products from the basket and if no new product is added to the basket i just want to show the same result when the button is clicked instead of fetching again. i want to only fetch again when a new item is added to the product list(cart) and when the button is clicked.
i have tried using usememo but even though i added the product list to the dependency array it fetches every time.
here is my code so far;
const handleclicked = async (event) => {
    event.preventdefault();
   try {
      setloading(true)
      setdisplayedmessage(""recipe is generating.."");
      let productslist = """";

      cartitems.products.foreach((prod, index) => {
         productslist = productslist + prod.title + "" "";
      });
      
      const res = await fetch(""api_call"",
      {
        method: ""post"",
        headers: {
          ""content-type"": ""application/json""
        },
        body: json.stringify({ prompt: ""generate recipe with this items "" + productslist })
      });

      const { response: data } = await res.json();
      setresult(data.trimstart());

      if (data === """") {
         throw data.error || new error(`request failed with status ${data}`);
      }

      setdisplayedmessage("""");
      openmodal();

   }catch (e) {
     console.log(e)
   }
}","['reactjs', 'react-hooks', 'openai-api']",75608813,"you guess it right, usememo is designed for render optimization, but not quite as you thought
there is even an equivalent hook for functions called usecallback, but in your case, it won't fix your issue, because usememo and usecallback keep reference between renders so children components that gets those references won't update over renders (it will if the reference will update, which happens when one of the expressions in the dependency array -the second arg of those two hooks- updates)
your issue is not about references but about the execution of your event handler handleclicked. even tho the function reference is the same through renders, at every click it will be executed.
what you need, is a reference that you can compare with productslist to check if you want to fetch, and if so update this reference. to do so, you can use useref
using useref i would have done something like that
const productslistref = useref('');

const handleclicked = async (event) => {
  event.preventdefault();

  // just a little bonus on how build your productlist variable
  const productlist = cartitems.products
  // here we sort the array in order to make sure the order is always the same
  // so we don't trigger a fetch when the order is different
  .sort((proda, prodb) => {
    return proda.title > prodb.title ? 1 : -1;
  })
  // thanks to array.prototype.reduce we can build a string
  // without reassigning the productlist variable, so the code is less error prone
  .reduce((acc, prod) => {
    return `${acc}${prod.title} `;
  }, '');

  const shouldfetch = productslistref.current !== productlist;

  if (shouldfetch) {
    // fetch your data and do all your computation
    // [...]
    // then update the ref
    productslistref.current = productlist;
  }
}",https://stackoverflow.com/questions/75608318,reactjs,01-03-2023 19:51,261.0,0.0,1.0,True,01-03-2023 20:43,01-03-2023 20:19
76980840,error with node.js api for gpt3.5 on aws lambda: &quot;cannot read properties of undefined (reading &#39;completions&#39;)&quot;,"i'm attempted to allow myself to text gpt3.5 and cannot figure out why i keep getting carious errors for this line.
when i use openai.chat.completions.create, as the latest documentation suggests,i get the following error:
cannot read properties of undefined (reading 'completions')
if i use the prior format of invoking completions with openai.createcompletion, i get the following error instead:
typeerror: openai.createchatcompletion is not a function
i've read every piece of documentation i can, and tried even older ways of invoking completions, but cannot get anything to work.
i've already tried getting a new api key and doing npm update, and still get the same issue. here's my full code, i must be missing something:
const openai = require('openai');

const accountsid = process.env.twilio_account_sid;
const authtoken = process.env.twilio_auth_token;

const client = require('twilio')(accountsid, authtoken);

openai.apikey = process.env.openai_auth,

exports.handler = async (event, context) => {
  try {
    const buff = buffer.from(event.body, ""base64"");
    const formencodedparams = buff.tostring(""utf-8"");
    const urlsearchparams = new urlsearchparams(formencodedparams);

    const body = urlsearchparams.get(""body"");
    const from = urlsearchparams.get(""from"");

    const completion = await openai.chat.completions.create({
      engine: ""gpt-3.5-turbo"", // gpt-3.5 turbo
      messages: [
        { role: ""system"", content: body },
      ],
      max_tokens: 100, // you can adjust this as needed
    });

    await sendmessageback(completion.data.choices[0].message.content, from);
    return {
      statuscode: 200,
      body: json.stringify('message sent successfully'),
    };
  } catch (error) {
    console.error(error);
    return {
      statuscode: 500,
      body: json.stringify('error sending message'),
    };
  }
};

async function sendmessageback(msg, to) {
  try {
    await client.messages.create({
      body: msg,
      to: to,
      from: process.env.twilio_phone_num,
    });
    console.log('message sent:', msg);
  } catch (e) {
    console.error('error sending message:', e);
  }
}

i have tried updating the openai package, new invocations of completions, old invocations, reading the documentation, but aws  lambda keeps throwing errors on that same line.
edit: the solutions below led to the following rewrite. including it here in case it's helpful to anyone else:
import openai from 'openai';

const openai = new openai({
  apikey: process.env.openai_auth, 
});

import twilio from 'twilio';

const accountsid = process.env.twilio_account_sid;
const authtoken = process.env.twilio_auth_token;

const client = twilio(accountsid, authtoken);

export const handler = async (event, context) => {
  try {
    const buff = buffer.from(event.body, ""base64"");
    const formencodedparams = buff.tostring(""utf-8"");
    const urlsearchparams = new urlsearchparams(formencodedparams);

    const msgbody = urlsearchparams.get(""body"");
    const from = urlsearchparams.get(""from"");

    const chatcompletion = await openai.chat.completions.create({
      model: ""gpt-3.5-turbo"",
      messages: [{ role: ""system"", content: msgbody }],
      max_tokens: 100,
    });

    await sendmessageback(chatcompletion.choices[0].message.content, from);
    return {
      statuscode: 200,
      body: json.stringify('message sent successfully'),
    };
  } catch (error) {
    console.error(error);
    return {
      statuscode: 500,
      body: json.stringify('error sending message'),
    };
  }
};

async function sendmessageback(msg, to) {
  try {
    await client.messages.create({
      body: msg,
      to: to,
      from: process.env.twilio_phone_num,
    });
    console.log('message sent:', msg);
  } catch (e) {
    console.error('error sending message:', e);
  }
}","['node.js', 'aws-lambda', 'openai-api', 'gpt-3']",76981323,"the reason why your code is throwing cannot read properties of undefined (reading 'completions') is because openai.chat was not even defined, so it's trying to access a variable or a property that has not been declared yet. that makes sense because usually libraries like this provide a base class to construct everything internally, like the openai one. so you must instantiate it's original class first, like this:
using openai:4.2.0:
import openai from 'openai';

const openai = new openai({
  apikey: 'your api key', // defaults to process.env[""openai_api_key""]
});

rather than:
const openai = require('openai');

openai.apikey = process.env.openai_auth;

because the later just imports the openai package, but doesn't instantiate it, so just sets the apikey as a variable there. the package base class knows how to do it internally, and it helps them to better maintain it.
also, i'd like to recommend changing engine: ""gpt-3.5-turbo"", to model: ""gpt-3.5-turbo"" at line 18, as model is the new way of specifiing it.",https://stackoverflow.com/questions/76980840,node.js,25-08-2023 22:29,1087.0,1.0,1.0,True,26-08-2023 18:40,26-08-2023 18:40
79005985,seq2seq trainer.train() keeps giving indexing error,"i am trying to do a machine translation from hindi to sanskrit using nllb model. but i keep getting the error:

indexerror: invalid key: 39463 is out of bounds for size 0.


the error is coming when training the pretrained nllb model `facebook/nllb-200-1.3b
the input data is ~40k hindi sentences. the same error arises when i tried training with a sample data also.

detailed error message:
traceback (most recent call last):
  file ""nllbtrain.py"", line 273, in <module>
    print(trainer.train())
  file ""/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py"", line 1645, in train
    return inner_training_loop(
  file ""/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py"", line 1907, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  file ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 631, in __next__
    data = self._next_data()
  file ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise stopiteration
  file ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py"", line 49, in fetch
    data = self.dataset.__getitems__(possibly_batched_index)
  file ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2814, in __getitems__
    batch = self.__getitem__(keys)
  file ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2810, in __getitem__
    return self._getitem(key)
  file ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2794, in _getitem
    pa_subtable = query_table(self._data, key, indices=self._indices)
  file ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 583, in query_table
    _check_valid_index_key(key, size)
  file ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 536, in _check_valid_index_key
    _check_valid_index_key(int(max(key)), size=size)
  file ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 526, in _check_valid_index_key
    raise indexerror(f""invalid key: {key} is out of bounds for size {size}"")
indexerror: invalid key: 39463 is out of bounds for size 0
  0%|

the code of the preprocessing done for the data:
def preprocess_function(examples):
        inputs = [example + ' </s>' + f' <2{s_lang}>' for example in examples[source_lang]]
        targets = [f'<2{t_lang}> ' + example + ' </s>' for example in examples[target_lang]]

        model_inputs = tokenizer.batch_encode_plus(inputs, max_length=max_input_length, truncation=true, padding='max_length')
        # model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=true)

        with tokenizer.as_target_tokenizer():
            # labels = tokenizer(targets, max_length=max_target_length, truncation=true)
            labels = tokenizer.batch_encode_plus(targets, max_length=max_input_length, truncation=true, padding='max_length')

        model_inputs['labels'] = labels['input_ids']

        return model_inputs

data after preprocessing:
datasetdict({
    train: dataset({
        features: ['hindi', 'sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 39729
    })
    val: dataset({
        features: ['hindi', 'sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 2210
    })
    test: dataset({
        features: ['hindi', 'sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 2214
    })
})

the code of model params and training:
model_path = 'facebook/nllb-200-1.3b'
model = automodelforseq2seqlm.from_pretrained(pretrained_model_name_or_path =model_path)
tokenizer = autotokenizer.from_pretrained('facebook/nllb-200-1.3b', do_lower_case=false, use_fast=false, truncation=true, xkeep_accents=true, src_lang=""hin_deva"", tgt_lang=""san_deva"", max_length = 500)

training_args = seq2seqtrainingarguments(
    evaluation_strategy=""epoch"",
    save_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    output_dir=""./output_dir"",
    weight_decay=0.01,
    save_total_limit=1,
    num_train_epochs=4,
    predict_with_generate=true,
    fp16=false,
    push_to_hub=false,
)
trainer = seq2seqtrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=dataset['train'],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)
print(trainer.train())


any idea why this error is persisting?","['python', 'nlp', 'huggingface-transformers', 'huggingface-trainer']",79007590,"size 0 indicates that the dataset your trainer gets when the fine-tuning starts is empty. looking at this ( and this ( thread suggests adding remove_unused_columns = false to your training_args might resolve the issue, so you could give that a try.",https://stackoverflow.com/questions/79005985,python,20-09-2024 08:43,57.0,0.0,1.0,True,20-09-2024 16:38,20-09-2024 09:56
78516046,why do i get a &quot;resource not found&quot; error when switching resource groups for azure openai?,"when i  use my second resource group in west us (my first one is germany), i always get the following error:

error code: 404 - {'error': {'code': '404', 'message': 'resource not found'}}

my python code:
from openai import azureopenai


client = azureopenai(
  azure_endpoint = "" 
  api_key=""xxxxx"",  
  api_version=""2024-05-13""
)


message_text = [{""role"":""system"",""content"":""you are an ai assistant that helps people find information.""},{""role"":""user"",""content"":""was ist 4x6?""}]

completion = client.chat.completions.create(
  model=""gpt-4o"", # model = ""deployment_name""
  messages = message_text,
  temperature=0.7,
  max_tokens=800,
  top_p=0.95,
  frequency_penalty=0,
  presence_penalty=0,
  stop=none
)

print(completion.choices[0].message.content)

model deployment:

i already checked double network, api, and endpoint.
in my first resource group the code works, and in second, it doesn't.
i want to use us west because there are always the new models like gpt-4o.","['python', 'azure', 'openai-api', 'azure-openai']",78516243,"the error is because of the api version you are using. the version given, 2024-05-13, is a model version and not the api version.

use the api version 2024-02-15-preview and it should work:

code in",https://stackoverflow.com/questions/78516046,python,22-05-2024 07:54,4354.0,2.0,1.0,True,19-07-2024 20:46,19-07-2024 20:46
75882988,openai gpt-3 api error: &quot;invalid url (post /v1/chat/completions)&quot;,"here is my code snippet:
const { configuration, openai, openaiapi } = require (""openai"");
const configuration = new configuration({
    apikey: 'my key'
})

const openai = new openaiapi(configuration)

async function start() {
    const response = await openai.createchatcompletion({
        model:""text-davinci-003"",
        prompt: ""write a 90 word essay about family guy"",
        temperature: 0,
        max_tokens: 1000
    })

    console.log(response.data.choices[0].text)
}

start()

when i run: node index
i run into this issue:
data: {
      error: {
        message: 'invalid url (post /v1/chat/completions)',
        type: 'invalid_request_error',
        param: null,
        code: null
      }
    }
  },
  isaxioserror: true,
  tojson: [function: tojson]
}

node.js v18.15.0
i've looked all over the internet and tried some solutions but nothing seems to work. please help!
usually others have some link attached to their code when i look up this problem online. very much a beginner at this stuff so any help would be much appreciated","['node.js', 'openai-api', 'gpt-3']",75886728,"tl;dr: treat the text-davinci-003 as a gpt-3 model (i.e., completions api). see the code under option 1.
introduction
at first glance, as someone who's been using the openai api for the past few months, i thought the answer was straight and simple if you read the official openai documentation. well, i read the documentation once again, and now i understand why you're confused.
confusion number 1
you want to use the text-davinci-003 model. this model is originally from the gpt-3 model family. but if you take a look at the openai models overview and click gpt-3, you won't find text-davinci-003 listed as a gpt-3 model. this is unexpected.

confusion number 2
moreover, the text-davinci-003 is listed as a gpt-3.5 model.

confusion number 3
as if this isn't confusing enough, if you take a look at the openai model endpoint compatibility, you'll find the text-davinci-003 listed under the /v1/completions endpoint. this api endpoint is used for the gpt-3 model family.


wait, what?
theï¿½ï¿½text-davinci-003 isn't listed as a gpt-3 model (i.e., completions api). it's listed as a gpt-3.5 model (i.e., chat completions api) but is compatible with the gpt-3 api endpoint. this doesn't make any sense.

test
either the text-davinci-003 could be treated as a gpt-3 model or a gpt-3.5 model, or perhaps both? let's make a test.
note: openai nodejs sdk v4 was released on august 16, 2023, and is a complete rewrite of the sdk. the code below differs depending on the version you currently have. see the v3 to v4 migration guide.
option 1: treat the text-davinci-003 as a gpt-3 model --> it works ï¿½ï¿½ï¿½
if you treat the text-davinci-003 as a gpt-3 model, then run test-1.js, and the openai will return the following completion:

this is indeed a test

ï¿½ï¿½ï¿½ if you have the openai nodejs sdk v3:
test-1.js
const { configuration, openaiapi } = require('openai');

const configuration = new configuration({
  apikey: process.env.openai_api_key,
});

const openai = new openaiapi(configuration);

async function getcompletionfromopenai() {
  const completion = await openai.createcompletion({
    model: 'text-davinci-003',
    prompt: 'say this is a test',
    max_tokens: 7,
    temperature: 0,
  });

  console.log(completion.data.choices[0].text);
}

getcompletionfromopenai();

ï¿½ï¿½ï¿½ if you have the openai nodejs sdk v4:

import openai from 'openai';

const openai = new openai({
  apikey: process.env.openai_api_key,
});

async function getcompletionfromopenai() {
  const completion = await openai.completions.create({
    model: 'text-davinci-003',
    prompt: 'say this is a test',
    max_tokens: 7,
    temperature: 0,
  });

  console.log(completion.choices[0].text);
}

getcompletionfromopenai();

option 2: treat the text-davinci-003 as a gpt-3.5 model --> it doesn't work ï¿½ï¿½ï¿½
if you treat the text-davinci-003 as a gpt-3.5 model, then run test-2.js, and the openai will return the following error:
data: {
  error: {
    message: 'invalid url (post /v1/chat/completions)',
    type: 'invalid_request_error',
    param: null,
    code: null
  }
}

ï¿½ï¿½ï¿½ if you have the openai nodejs sdk v3:
test-2.js
const { configuration, openaiapi } = require('openai');

uration = new configuration({
  apikey: process.env.openai_api_key,
});

const openai = new openaiapi(configuration);

async function getchatcompletionfromopenai() {
  const chatcompletion = await openai.createchatcompletion({
    model: 'text-davinci-003',
    messages: [{ role: 'user', content: 'hello!' }],
    temperature: 0,
  });

  console.log(chatcompletion.data.choices[0].message.content);
}

getchatcompletionfromopenai();

ï¿½ï¿½ï¿½ if you have the openai nodejs sdk v4:
test-2.js
import openai from 'openai';

const openai = new openai({
  apikey: process.env.openai_api_key,
});

async function getchatcompletionfromopenai() {
  const chatcompletion = await openai.chat.completions.create({
    model: 'gpt-3.5-turbo',
    messages: [{role: 'user', content: 'hello!'}],
    temperature: 0,
  });

  console.log(chatcompletion.choices[0].message.content);
}

getchatcompletionfromopenai();

conclusion
treat the  as a gpt-3 model. see the code under option 1.",https://stackoverflow.com/questions/75882988,node.js,29-03-2023 23:32,16143.0,5.0,2.0,True,07-10-2023 06:18,31-03-2023 07:31
75585069,why is bert storing cache even after caching is disabled?,"i am trying to extract hidden state features from a fine-tuned bert model, but each text entry consumes memory and does not free it up after the next call. i can only run 20-30 sentences with 24 gb of ram memory.
from transformers import berttokenizer, bertmodel
import numpy as np

data = pd.read_csv(' + 
                   '1cfyujdpfc3gpqsqjnc4d8zcxbamd_pcpu8slrsjav-q' +
                   '/export?gid=0&format=csv',
                  )
data = data.messages


# i will be using my own fine-tuned model, but with
# bert-base-uncased, i get the same problem
tokenizer = berttokenizer.from_pretrained(""bert-base-uncased"")
model = bertmodel.from_pretrained(
    ""bert-base-cased"",
    from_tf=true,
    output_hidden_states=true,
    use_cache=false)

sentences = data[0:].tolist()
inputs = tokenizer(sentences, return_tensors='pt', padding=true,truncation=true)
featuresineed = model(inputs['input_ids'])['pooler_output']

in the case above, i run out of memory. i tried breaking it into chunks and using torch.cuda.empty_cache(), but it doesn't seem to clear all the memory. i tried both with and without gpu. in my case, i am using a dataset of size 60,000 (possibly larger in the future) and using a fine-tuned model of bert large. i will have a 24gb gpu available for me.
any suggestions?
to keep in mind, my main goal is to have 1 language model predict the next token and extract features of the current sentence.","['caching', 'huggingface-transformers', 'torch', 'bert-language-model', 'transformer-model']",75601400,"the code snippet has several issues.

it does not use gpu at all. you need to send both model and the data to a gpu explicitly, e.g., by doing the following:

device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
model.to(device)
inputs.to(device)


pytorch automatically prepares for computing the gradients, which requires storing intermediate results. you can wrap the call in with torch.no_grad() to ensure no gradients are collected.

60k sentences is too much, regardless of your gpu memory. in any case, you need to split it into more batches. you can use the dataset interface from transformers for that.",https://stackoverflow.com/questions/75585069,caching,27-02-2023 20:04,611.0,0.0,1.0,True,01-03-2023 08:50,01-03-2023 08:38
78638576,error runtimeerror: cuda error: operation not supported when tried to locate something into cuda,"here is my code:
from transformers import automodelforcausallm, autotokenizer, quantoconfig
import torch
device = ""cuda:0""
model_id = ""bigscience/bloom-560m""
quantization_config = quantoconfig(weights=""int8"")

model = automodelforcausallm.from_pretrained(model_id, torch_dtype=torch.float32,  device_map=device)

tokenizer = autotokenizer.from_pretrained(model_id)

text = ""hello my name is""
inputs = tokenizer(text, return_tensors=""pt"").to(device)
outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0], skip_special_tokens=true))

when i run, i obtain the next error:

runtimeerror: cuda error: operation not supported cuda kernel errors
might be asynchronously reported at some other api call, so the
stacktrace below might be incorrect. for debugging consider passing
cuda_launch_blocking=1. compile with torch_use_cuda_dsa to enable
device-side assertions.

however, when i check if cuda is available i obtain:
print('-------------------------------')
print(torch.cuda.is_available())
print(torch.cuda.device_count())
print(torch.cuda.current_device())
print(torch.cuda.device(0))
print(torch.cuda.get_device_name(0))
print('memory usage:')
print('allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'gb')
print('cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'gb')


true 1 0 <torch.cuda.device object at 0x7f8bf6d4a9b0> grid t4-16q
memory usage: allocated: 0.0 gb cached: 0.0 gb

i run this code on colab, and i do not have any issues. i also run the code on another machine with another gpu, and it runs as expected.
the configuration of the machine where i need to run it fails.

and the libraries:
accelerate 0.31.0 aiohttp 3.9.5 aiosignal 1.3.1 async-timeout 4.0.3 attrs 23.2.0 certifi 2024.6.2 charset-normalizer 3.3.2 datasets 2.20.0 dill 0.3.8 filelock 3.15.1 frozenlist 1.4.1 fsspec 2024.5.0 huggingface-hub 0.23.4 idna 3.7 jinja2 3.1.4 markupsafe 2.1.5 mpmath
1.3.0 multidict 6.0.5 multiprocess 0.70.16 networkx 3.3 ninja 1.11.1.1 numpy 2.0.0 nvidia-cublas-cu12 12.1.3.1 nvidia-cuda-cupti-cu12
12.1.105 nvidia-cuda-nvrtc-cu12 12.1.105 nvidia-cuda-runtime-cu12 12.1.105 nvidia-cudnn-cu12 8.9.2.26 nvidia-cufft-cu12 11.0.2.54 nvidia-curand-cu12 10.3.2.106 nvidia-cusolver-cu12 11.4.5.107 nvidia-cusparse-cu12 12.1.0.106 nvidia-nccl-cu12 2.20.5 nvidia-nvjitlink-cu12 12.5.40 nvidia-nvtx-cu12 12.1.105 packaging 24.1 pandas 2.2.2 pip 24.0 psutil 5.9.8 pyarrow 16.1.0 pyarrow-hotfix 0.6 python-dateutil 2.9.0.post0 pytz 2024.1 pyyaml 6.0.1 quanto 0.2.0 regex 2024.5.15 requests 2.32.3 safetensors 0.4.3 setuptools 65.5.0 six 1.16.0 sympy 1.12.1 tokenizers 0.19.1 torch 2.3.1 tqdm 4.66.4 transformers 4.42.0.dev0 triton 2.3.1 typing_extensions 4.12.2 tzdata
2024.1 urllib3 2.2.2 xxhash 3.4.1 yarl 1.9.4

i do not know if this affect you, but the machine is a virtual machine with wmware under a vgpu. also, i tried to run a simple nn, just for check if the problem was with the transformers library, but i obtained the same error when i tried to locate info on the gpu.
 import torch
    import torch.nn as nn
    dev = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")
    t1 = torch.randn(1,2)
    t2 = torch.randn(1,2).to(dev)
    print(t1)  # tensor([[-0.2678,  1.9252]])
    print(t2)  # tensor([[ 0.5117, -3.6247]], device='cuda:0')
    t1.to(dev)
    print(t1)  # tensor([[-0.2678,  1.9252]])
    print(t1.is_cuda) # false
    t1 = t1.to(dev)
    print(t1)  # tensor([[-0.2678,  1.9252]], device='cuda:0')
    print(t1.is_cuda) # true

    class m(nn.module):
        def __init__(self):        
            super().__init__()        
            self.l1 = nn.linear(1,2)

        def forward(self, x):                      
            x = self.l1(x)
            return x
    model = m()   # not on cuda
    model.to(dev) # is on cuda (all parameters)
    print(next(model.parameters()).is_cuda) # true


traceback (most recent call last): file
ï¿½ï¿½ï¿½/home/admin/llm/modelsservice/test.pyï¿½ï¿½ï¿½, line 14, in t2 =
torch.randn(1,2).to(dev) runtimeerror: cuda error: operation not
supported cuda kernel errors might be asynchronously reported at some
other api call, so the stacktrace below might be incorrect. for
debugging consider passing cuda_launch_blocking=1. compile with
torch_use_cuda_dsa to enable device-side assertions.

by the way here info about my cuda

(test310) admin@appdev-llm-lnx1:~/llm/modelsservice$ nvcc --version
nvcc: nvidia (r) cuda compiler driver copyright (c) 2005-2019 nvidia
corporation built on sun_jul_28_19:07:16_pdt_2019 cuda compil release 10.1, v10.1.243

regards","['python', 'pytorch', 'huggingface-transformers']",78649591,"ok i will responds myself, if someone has a similar eror, just create the next environment variables:

export cuda_home=/usr/local/cuda
export path=${cuda_home}/bin:${path}
export ld_library_path=${cuda_home}/lib64:$ld_library_path

after that pytorch start working correctly",https://stackoverflow.com/questions/78638576,python,18-06-2024 16:15,2568.0,0.0,2.0,True,11-02-2025 14:03,19-06-2024 01:25
71338131,why spacy morphologizer doesn&#39;t work when we use a custom tokenizer?,"i don't understand why when i'm doing this
import spacy
from copy import deepcopy
nlp = spacy.load(""fr_core_news_lg"")

class mytokenizer:
    def __init__(self, tokenizer):
        self.tokenizer = deepcopy(tokenizer)
    def __call__(self, text):
        return self.tokenizer(text)

nlp.tokenizer = mytokenizer(nlp.tokenizer)
doc = nlp(""un texte en franï¿½ï¿½ais."")

tokens don't have any morph assigned
print([tok.morph for tok in doc])
> ['','','','','']

is this behavior expected? if yes, why ? (spacy v3.0.7)","['spacy', 'spacy-3']",71379203,"the pipeline expects nlp.vocab and nlp.tokenizer.vocab to refer to the exact same vocab object, which isn't the case after running deepcopy.
i admit that i'm not entirely sure off the top of my head why you end up with empty analyses instead of more specific errors, but i think the morphanalysis objects, which are stored centrally in the vocab in vocab.morphology, end up out-of-sync between the two vocabs.",https://stackoverflow.com/questions/71338131,spacy,03-03-2022 13:37,438.0,4.0,1.0,True,07-03-2022 09:52,03-03-2022 14:02
79298368,inspect all probabilities of bertopic model,"say i build a bertopic model using
from bertopic import bertopic
topic_model = bertopic(n_gram_range=(1, 1), nr_topics=20)
topics, probs = topic_model.fit_transform(docs)

inspecting probs gives me just a single value for each item in docs.
probs
array([0.51914467, 0.        , 0.        , ..., 1.        , 1.        ,
       1.        ])

i would like the entire probability vector across all topics (so in this case, where nr_topics=20, i want a vector of 20 probabilities for each item in docs). in other words, if i have n items in docs and k topics, i would like an nxk output.","['python', 'nlp', 'topic-modeling']",79299703,"for individual topic probability across each document you need to add one more argument.
topic_model = bertopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=true)

note: this calculate_probabilities = true will only work if you are using hdbscan clustering embedding model. and bertopic by default uses all-minilm-l6-v2.
official documentation: 
they have mentioned the same in document as well.",https://stackoverflow.com/questions/79298368,python,20-12-2024 20:49,63.0,1.0,1.0,True,21-12-2024 16:03,21-12-2024 00:01
75427004,cannot import name &#39;generationmixin&#39; from &#39;transformers.generation&#39; while running executable file creating using pyinstaller,"i am trying to create a application which checks for sentence similarity. exe file got created.
getting the below error message while executing .exe file after giving required inputs.
code
model = pickle.load(open(r""minilm.sav"", ""rb"")) 
sentences_embeddings = model.encode(desc_corpus) 
c_matrix = cosine_similarity(sentences_embeddings, sentences_embeddings)

error
cannot import name 'generationmixin' from 'transformers.generation' (c:\users\username\appdata\local\temp\_mei198962\transformers\generation\__init__.pyc)

i have the latest version of transformers - 4.26.1
requirements.txt
absl-py==1.4.0
altgraph==0.17.3
astunparse==1.6.3
cachetools==5.3.0
certifi==2022.12.7
charset-normalizer==3.0.1
click==8.1.3
colorama==0.4.6
et-xmlfile==1.1.0
filelock==3.9.0
flatbuffers==23.1.21
gast==0.4.0
google-auth==2.16.0
google-auth-oauthlib==0.4.6
google-pasta==0.2.0
grpcio==1.51.1
h5py==3.8.0
huggingface-hub==0.12.0
idna==3.4
importlib-metadata==6.0.0
joblib==1.2.0
keras==2.11.0
libclang==15.0.6.1
markdown==3.4.1
markupsafe==2.1.2
nltk==3.8.1
numpy==1.24.2
oauthlib==3.2.2
openpyxl==3.1.0
opt-einsum==3.3.0
packaging==23.0
pandas==1.5.3
pefile==2023.2.7
pillow==9.4.0
protobuf==3.19.6
pyasn1==0.4.8
pyasn1-modules==0.2.8
pyinstaller==5.7.0
pyinstaller-hooks-contrib==2022.15
pyqt5==5.15.9
pyqt5-qt5==5.15.2
pyqt5-sip==12.11.1
python-dateutil==2.8.2
python-version==0.0.2
pytz==2022.7.1
pywin32-ctypes==0.2.0
pyyaml==6.0
regex==2022.10.31
requests==2.28.2
requests-oauthlib==1.3.1
rsa==4.9
scikit-learn==1.2.1
scipy==1.10.0
sentence-transformers==2.2.2
sentencepiece==0.1.97
six==1.16.0
tensorboard==2.11.2
tensorboard-data-server==0.6.1
tensorboard-plugin-wit==1.8.1
tensorflow==2.11.0
tensorflow-estimator==2.11.0
tensorflow-intel==2.11.0
tensorflow-io-gcs-filesystem==0.30.0
termcolor==2.2.0
threadpoolctl==3.1.0
tokenizers==0.13.2
torch==1.13.1
torchvision==0.14.1
tqdm==4.64.1
transformers==4.26.1
typing_extensions==4.4.0
urllib3==1.26.14
werkzeug==2.2.2
wrapt==1.14.1
zipp==3.13.0


spec file
# -*- mode: python ; coding: utf-8 -*-
from pyinstaller.utils.hooks import copy_metadata

datas = [('config\\\\favicon.ico', '.'), ('config\\\\minilm.sav', '.')]
datas += copy_metadata('tqdm')
datas += copy_metadata('regex')
datas += copy_metadata('requests')
datas += copy_metadata('packaging')
datas += copy_metadata('filelock')
datas += copy_metadata('numpy')
datas += copy_metadata('tokenizers')
datas += copy_metadata('importlib_metadata')
datas += copy_metadata('tensorflow')


block_cipher = none


a = analysis(
    ['render_ui.py'],
    pathex=[],
    binaries=[],
    datas=datas,
    hiddenimports=['sklearn.metrics._pairwise_distances_reduction._datasets_pair', 'sklearn.metrics._pairwise_distances_reduction._middle_term_computer', 'sklearn.metrics._pairwise_distances_reduction._argkmin', 'sklearn.metrics._pairwise_distances_reduction._base', 'sklearn.metrics._pairwise_distances_reduction._radius_neighbors', 'sentence_transformers.sentencetransformer', 'tensorflow'],
    hookspath=[],
    hooksconfig={},
    runtime_hooks=[],
    excludes=[],
    win_no_prefer_redirects=false,
    win_private_assemblies=false,
    cipher=block_cipher,
    noarchive=false,
)
pyz = pyz(a.pure, a.zipped_data, cipher=block_cipher)

exe = exe(
    pyz,
    a.scripts,
    a.binaries,
    a.zipfiles,
    a.datas,
    [],
    name='app',
    debug=false,
    bootloader_ignore_signals=false,
    strip=false,
    upx=true,
    upx_exclude=[],
    runtime_tmpdir=none,
    console=true,
    disable_windowed_traceback=false,
    argv_emulation=false,
    target_arch=none,
    codesign_identity=none,
    entitlements_file=none,
    icon='config\\favicon.ico'
)

downgraded transformers to - 4.25.1
issue still persists.
i am not sure what is causing this problem. can someone please help me out on this?","['python', 'sentence-transformers']",75472389,"below code changes were made
model = sentencetransformer('sentence-transformers/all-minilm-l6-v2')
sentences_embeddings = model.encode(desc_corpus)
c_matrix = cosine_similarity(sentences_embeddings, sentences_embeddings)

spec file
# -*- mode: python ; coding: utf-8 -*-
from pyinstaller.utils.hooks import copy_metadata

datas = [('config\\\\favicon.ico', '.'), ('config\\\\minilm.sav', '.')]
datas += copy_metadata('tqdm')
datas += copy_metadata('regex')
datas += copy_metadata('requests')
datas += copy_metadata('packaging')
datas += copy_metadata('filelock')
datas += copy_metadata('numpy')
datas += copy_metadata('tokenizers')
datas += copy_metadata('importlib_metadata')
datas += copy_metadata('torch')

block_cipher = none


a = analysis(
    ['render_ui.py'],
    pathex=[],
    binaries=[],
    datas=datas,
    hiddenimports=['sklearn.metrics._pairwise_distances_reduction._datasets_pair', 'sklearn.metrics._pairwise_distances_reduction._middle_term_computer', 'sklearn.metrics._pairwise_distances_reduction._argkmin', 'sklearn.metrics._pairwise_distances_reduction._base', 'sklearn.metrics._pairwise_distances_reduction._radius_neighbors', 'sentence_transformers.sentencetransformer', 'transformers', 'torch'],
    hookspath=[],
    hooksconfig={},
    runtime_hooks=[],
    excludes=[],
    win_no_prefer_redirects=false,
    win_private_assemblies=false,
    cipher=block_cipher,
    noarchive=false,
)
pyz = pyz(a.pure, a.zipped_data, cipher=block_cipher)

exe = exe(
    pyz,
    a.scripts,
    a.binaries,
    a.zipfiles,
    a.datas,
    [],
    name='app',
    debug=false,
    bootloader_ignore_signals=false,
    strip=false,
    upx=true,
    upx_exclude=[],
    runtime_tmpdir=none,
    console=true,
    disable_windowed_traceback=false,
    argv_emulation=false,
    target_arch=none,
    codesign_identity=none,
    entitlements_file=none,
    icon='config\\favicon.ico'
)

used below pip commands
pip install torch torchvision torchaudio
pip install -u sentence-transformers",https://stackoverflow.com/questions/75427004,python,12-02-2023 12:31,4315.0,0.0,1.0,True,16-02-2023 12:37,13-02-2023 12:28
76388280,how does langchain help to overcome the limited context size of chatgpt?,"it's not possible to pass long documents to chatgpt directly due to its limited context size. so for example question answering or summarization of long documents is not possible at first sight. i've learned how chatgpt can in principle ""know"" larger contexts -- basically by summarizing a sequence of previous contexts from the chat history -- but will this suffice to detect really long-range dependencies (bearing ""meaning"") inside really long texts?
langchain seems to offer an solution, making use of openai's api and  vectorstores. i'm looking for a high-level description what's going on when langchain makes accessible long documents or even corpora of long documents to chatgpt and then makes use of chatgpt's nlp abilities by clever automated prompting, e.g. question answering or summarization. let's assume that the documents are already formatted as langchain document objects.","['document', 'openai-api', 'langchain']",76391205,"so you have you document(s) that contain a lot of text. langchain can load these documents and get the text from these documents. as the text is too big to be used for the context it needs to be splitted into several chunks.
this can for example be done with langchain's recursivecharactertextsplitter. you'll have to determine the size of the chunks, so it again won't be to large to be used for the context size. by setting a chunk overlap you can keep context between the chunks.
after you have your desired chunks you'll need to create embeddings for each of these chunks, for example by using the openaiembeddings. these embeddings will be used to find to most relevant chunks of texts based to answer the query. this means that it will not use the entire content of the documents for the context, but only the most relevant parts of the document. these chunks and embeddings are stored into the vector database.
when the relevant chunks are found these can be passed into a prompt template. for example a default prompt template in langchain's retrievalqa looks like this:
use the following pieces of context to answer the question at the end. if you don't know the answer, just say that you don't know, don't try to make up an answer.

{{context}}

question: {{question}}?
helpful answer:

this is then passed to the llm to generate an answer.",https://stackoverflow.com/questions/76388280,document,02-06-2023 08:05,6761.0,4.0,1.0,True,02-06-2023 14:50,02-06-2023 08:31
70601433,having error after deploying django in heroku. lookuperror,"i just deployed my sentiment analysis django app to heroku. the deployment was successful and i can access the features like login register etc. but i am unable to analyze the text because apparently, nltk cannot be found. but i have installed and imported it in my local host and it already worked before.
this is my first time deploying in heroku so i am unfamiliar with everything.
this is how it looks in my local host:

this is the error in the herokuapp live webapp when i try to analyze the sentiment.
    lookuperror at /sentiment/type/
**********************************************************************
  resource e[93mpunkte[0m not found.
  please use the nltk downloader to obtain the resource:

  e[31m>>> import nltk
  >>> nltk.download('punkt')
  e[0m
  for more information see: 

  attempted to load e[93mtokenizers/punkt/py3/english.picklee[0m

  searched in:
    - '/app/nltk_data'
    - '/app/.heroku/python/nltk_data'
    - '/app/.heroku/python/share/nltk_data'
    - '/app/.heroku/python/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
    - ''
**********************************************************************
request method: post
request url:    
django version: 4.0
exception type: lookuperror
exception value:    
**********************************************************************
  resource e[93mpunkte[0m not found.
  please use the nltk downloader to obtain the resource:

  e[31m>>> import nltk
  >>> nltk.download('punkt')
  e[0m
  for more information see: 

  attempted to load e[93mtokenizers/punkt/py3/english.picklee[0m

  searched in:
    - '/app/nltk_data'
    - '/app/.heroku/python/nltk_data'
    - '/app/.heroku/python/share/nltk_data'
    - '/app/.heroku/python/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
    - ''
**********************************************************************
exception location: /app/.heroku/python/lib/python3.8/site-packages/nltk/data.py, line 583, in find
python executable:  /app/.heroku/python/bin/python
python version: 3.8.9
python path:    
['/app/.heroku/python/bin',
 '/app',
 '/app/.heroku/python/lib/python38.zip',
 '/app/.heroku/python/lib/python3.8',
 '/app/.heroku/python/lib/python3.8/lib-dynload',
 '/app/.heroku/python/lib/python3.8/site-packages']
server time:    wed, 05 jan 2022 20:16:57 +0000

setting.py
    """"""
django settings for sentiment_emotion_analysis project.

generated by 'django-admin startproject' using django 2.1.7.

for more information on this file, see


for the full list of settings and their values, see

""""""


import os
# os.environ.setdefault(""django_settings_module"", ""sentiment.settings"")

# import django
# django.setup()

# from django.core.management import call_command

from django.contrib.messages import constants as messages
message_tags = {
        messages.debug: 'alert-debug',
        messages.info: 'alert-info',
        messages.success: 'success',
        messages.warning: 'alert-warning',
        messages.error: 'danger',
}


# build paths inside the project like this: os.path.join(base_dir, ...)
base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


# quick-start development settings - unsuitable for production
# see 

# security warning: keep the secret key used in production secret!
secret_key = 'g&x!0fjh8c8)e_-z@gs1^lbngvqwk2(o3s(5zg!o&woxdsu_un'

# security warning: don't run with debug turned on in production!
debug = true

allowed_hosts = ['sentymeter.herokuapp.com' , '127.0.0.1']


# application definition

installed_apps = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'sentiment',
    'user',
]

middleware = [
    'django.middleware.security.securitymiddleware',
    
    'whitenoise.middleware.whitenoisemiddleware',
   
    'django.contrib.sessions.middleware.sessionmiddleware',
    'django.middleware.common.commonmiddleware',
    'django.middleware.csrf.csrfviewmiddleware',
    'django.contrib.auth.middleware.authenticationmiddleware',
    'django.contrib.messages.middleware.messagemiddleware',
    'django.middleware.clickjacking.xframeoptionsmiddleware',
]

root_urlconf = 'sentiment_emotion_analysis.urls'

templates = [
    {
        'backend': 'django.template.backends.django.djangotemplates',
        'dirs': [os.path.join(base_dir, ""templates"")],
        'app_dirs': true,
        'options': {
            'context_processors': [
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
            ],
        },
    },
]

wsgi_application = 'sentiment_emotion_analysis.wsgi.application'


# database
# 

databases = {
    'default': {
        'engine': 'django.db.backends.sqlite3',
        'name': os.path.join(base_dir, 'db.sqlite3'),
    }
}


# password validation
# 

auth_password_validators = [
    {
        'name': 'django.contrib.auth.password_validation.userattributesimilarityvalidator',
    },
    {
        'name': 'django.contrib.auth.password_validation.minimumlengthvalidator',
    },
    {
        'name': 'django.contrib.auth.password_validation.commonpasswordvalidator',
    },
    {
        'name': 'django.contrib.auth.password_validation.numericpasswordvalidator',
    },
]


# internationalization
# 

language_code = 'en-us'

time_zone = 'utc'

use_i18n = true

use_l10n = true

use_tz = true


# static files (css, javascript, images)
# 

project_root = os.path.dirname(os.path.abspath(__file__))
static_url = '/static/'
static_root = os.path.join(project_root, 'static')

models = os.path.join(base_dir, 'sentiment/models')

can anyone help me to solve this issue?","['django', 'heroku', 'nltk']",70601925,"it looks like you installed the nltk python package but not any data for it.
if you want a quick fix, you can connect to your heroku (heroku ps:exec ) and run a django shell python manage.py shell and then install the relevant nltk modules from the python shell:
import nltk
nltk.download('punkt')

since you want to be able to redeploy it, better create a management command to do this and run it similar to how you run migrations. management command should be under {app}/management/commands/
a command like this:
from django.core.management.base import basecommand, commanderror

class command(basecommand):
    help = 'install nltk stuff'
   
    def handle(self, *args, **options):
        import nltk
        nltk.download(..) # replace the dots with the relevant corpora",https://stackoverflow.com/questions/70601433,django,06-01-2022 01:16,75.0,0.0,1.0,True,08-01-2022 14:01,06-01-2022 02:49
76141118,not sure why my python code that uses spacy to add a phone_number entity is not working,"the pattern works with matcher.  but not as an entity? here is my code:
import spacy
from spacy.pipeline import entityruler

nlp = spacy.load(""en_core_web_sm"")

patterns = [
    {
        ""label"": ""phone_number"",
        ""pattern"": [
            {""orth"": ""(""},
            {""shape"": ""ddd""},
            {""orth"": "")""},
            {""is_space"": true, ""op"": ""?""},
            {""shape"": ""ddd""},
            {""orth"": ""-""},
            {""shape"": ""dddd""},
        ],
    }
]

entity_ruler = entityruler(nlp, patterns=patterns, overwrite_ents=true)
nlp.add_pipe(""entity_ruler"", before=""ner"")

doc = nlp(""you can reach me at (111) 111-1111."")

for ent in doc.ents:
    print(ent.text, ent.label_)

this returns:
111 cardinal
111 cardinal

advice/help needed and appreciated.  thank you.","['nlp', 'spacy', 'spacy-3']",76156251,"the problem was two seperate componentsï¿½ï¿½ï¿½one constructed with the class entityruler and one constructed with nlp.add_pipe. the component created with the add_pipe method wasn't aware of your patterns. using just one method and then adding the patterns to that component did the trick.
import spacy

nlp = spacy.load(""en_core_web_sm"")
patterns = [
    {
        ""label"": ""phone_number"",
        ""pattern"": [
            {""orth"": ""(""},
            {""shape"": ""ddd""},
            {""orth"": "")""},
            {""is_space"": true, ""op"": ""?""},
            {""shape"": ""ddd""},
            {""orth"": ""-""},
            {""shape"": ""dddd""},
        ],
    }
]

ruler = nlp.add_pipe(""entity_ruler"", before=""ner"")
ruler.add_patterns(patterns)

doc = nlp(""you can reach me at (111) 111-1111."")

for ent in doc.ents:
    print(ent.text, ent.label_)

(111) 111-1111 phone_number

i read about the different ways to initialize the component here:",https://stackoverflow.com/questions/76141118,nlp,30-04-2023 11:54,178.0,0.0,1.0,True,02-05-2023 15:03,30-04-2023 12:36
68225948,explanation/interpretation of the parameters in the spacy config file,"i have a couple of questions regarding the parameters that we define in the config.cfg file. although spacy's docs do try to explain them, i feel that the explanation isn't really descriptive enough and that lots of things are scattered around the docs, making it difficult to find exactly what you need, especially with spacy v3, (unless i'm looking at wrong parts of the website) which is recent and hence has really less question/answers in the forums.
i'm basically building a named entity recognition (ner) model along with a transformer component. my questions are as follows:

in the following part (same question for corpora.train also), what is the difference between max_length and limit?
for max_length the docs say ""limitations on training document length""
for limit, the docs say ""limitation on number of training examples""
aren't they both more or less the same thing? i mean i can limit the number of training examples by limiting the document's length itself, right?


[corpora.dev]
@readers = ""spacy.corpus.v1""
path = ${paths.dev}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null


in the below snippet, what is the meaning of one 'step'? i understand max_steps=0 means infinite steps. but how do i know how many such 'steps' make one epoch? also how many example sentences are covered in 1 such step?

[training]
train_corpus = ""corpora.train""
dev_corpus = ""corpora.dev""
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_epochs = 10
max_steps = 0
eval_frequency = 200
frozen_components = []
before_to_disk = null


how exactly is the learn_rate being modified in the below snippet of code, during the training process? more specifically, what do total_steps and warmup_steps mean?

[training.optimizer.learn_rate]
@schedules = ""warmup_linear.v1""
warmup_steps = 250
total_steps = 200
initial_rate = 0.00005


finally, in the cli output of the training process, what exactly is this '#'? it was mentioned in one of github discussions that ""the # column is the number of optimization steps (= batches processed)"" , but what exactly is this 1 batch or 'optimization step'? if the training process shows me the scores for after 200 such 'batches' how do i interpret it (as in how many example sentences have been processed till that point)?","['python', 'nlp', 'spacy', 'named-entity-recognition', 'spacy-3']",68235408,"in the following part (same question for corpora.train also), what is the difference between max_length and limit?
for max_length the docs say ""limitations on training document length""
for limit, the docs say ""limitation on number of training examples""
aren't they both more or less the same thing? i mean i can limit the number of training examples by limiting the document's length itself, right?

these are different things, you seem to be confused about what a ""document"" is. you can think of a ""doc"" as being a single object in spacy. different docs don't know anything about each other. a doc is based on a single string.  using normal python strings as an example:
[""cat"", ""dog"", ""fish""] # this is three strings
[""cat dog fish""] # this is one string

you can see that ""take three strings from the list"" and ""take strings not more than three characters long"" are very different things. the values in spacy are like that.

in the below snippet, what is the meaning of one 'step'? i understand max_steps=0 means infinite steps. but how do i know how many such 'steps' make one epoch? also how many example sentences are covered in 1 such step?

a ""step"" is a ""batch"". a ""batch"" is running training over some number of examples and updating the model weights once. you can control the size of a batch so it can be any number of examples. an ""epoch"" is how long it takes the training to see every example once, so if you have 5 documents per batch and 30 training documents then 6 steps would be one epoch.
spacy doesn't necessarily know anything about ""sentences"" in training, docs are the basic unit of a batch. your training examples might all be single sentences but that's not a requirement.
these terms are not spacy-specific, they are widely used in machine learning.

how exactly is the learn_rate being modified in the below snippet of code, during the training process? more specifically, what do total_steps and warmup_steps mean?

this is from thinc, see the docs there.
to quote:

generate a series, starting from an initial rate, and then with a warmup period, and then a linear decline. used for learning rates.

at the end of total_steps the learning rate stops changing.

finally, in the cli output of the training process, what exactly is this '#'? it was mentioned in one of github discussions that ""the # column is the number of optimization steps (= batches processed)"" , but what exactly is this 1 batch or 'optimization step'? if the training process shows me the scores for after 200 such 'batches' how do i interpret it (as in how many example sentences have been processed till that point)?

a step is the same thing as in #2, it's one batch. batch size is expressed in docs, not in sentences.",https://stackoverflow.com/questions/68225948,python,02-07-2021 13:20,1911.0,3.0,1.0,True,03-07-2021 10:34,03-07-2021 09:21
74769552,classic king - man + woman = queen example with pretrained word-embedding and word2vec package in r,"i am really desperate, i just cannot reproduce the allegedly classic example of king - man + woman = queen with the word2vec package in r and any (!) pre-trained embedding model (as a bin file).
i would be very grateful if anybody could provide working code to reproduce this example... including a link to the necessary pre-trained model which is also downloadable (many are not!).
thank you very much!","['r', 'nlp', 'word2vec', 'word-embedding']",74927758,"an overview of using word2vec with r is available at  which even shows an example of king - man + woman = queen.
just following the instructions there and downloading the first english 300-dim embedding word2vec model from  ran on the british national corpus which i encountered, downloaded and unzipped the model.bin on my drive and next inspecting the terms in the model (words are there apparently appended with pos tags), getting the word vectors, displaying the vectors, getting the king - man + woman and finding the closest vector to that vector gives ... queen.
> library(word2vec)
> model <- read.word2vec(""c:/users/jwijf/onedrive/bureaublad/model.bin"", normalize = true)
> head(summary(model, type = ""vocabulary""), n = 10)
 [1] ""vintage-style_adj"" ""sinopoli_propn""    ""yarrell_propn""     ""en-1_num""          ""74ï¿½ï¿½ï¿½ï¿½ï¿½78ï¿½ï¿½f_x""       
 [6] ""bursa_noun""        ""uni-male_adj""      ""37541_num""         ""menuetto_propn""    ""saxena_propn""     
> wv <- predict(model, newdata = c(""king_noun"", ""man_noun"", ""woman_nou = ""embedding"")
> head(t(wv), n = 10)
       king_noun    man_noun  woman_noun
 [1,] -0.4536242 -0.47802860 -1.03320265
 [2,]  0.7096733  1.40374041 -0.91597748
 [3,]  1.1509652  2.35536361  1.57869458
 [4,] -0.2882653 -0.59587735 -0.59021348
 [5,] -0.2110678 -1.05059254 -0.64248675
 [6,]  0.1846713 -0.05871651 -1.01818573
 [7,]  0.5493720  0.13456300  0.38765019
 [8,] -0.9401053  0.56237948  0.02383301
 [9,]  0.1140556 -0.38569298 -0.43408644
[10,]  0.3657919  0.92853492 -2.56553030
> wv <- wv[""king_noun"", ] - wv[""man_noun"", ] + wv[""woman_noun"", ]
> predict(model, newdata = wv, type = ""nearest"", top_n = 4)
             term similarity rank
1       king_noun  0.9332663    1
2      queen_noun  0.7813236    2
3 coronation_noun  0.7663506    3
4   kingship_noun  0.7626975    4

do you prefer to build your own model based on your own text or a more larger corpus e.g. the text8 file. follow the instructions shown at 
get a text file and use r package word2vec to build the model, wait untill the model finished training and next interact with it.
download.file("" ""text8.zip"")
unzip(""text8.zip"", files = ""text8"")

> library(word2vec)
> set.seed(123456789)
> model <- word2vec(x = ""text8"", type = ""cbow"", dim = 100, window = 10, lr = 0.05, iter = 5, hs = false, threads = 2)
> wv    <- predict(model, newdata = c(""king"", ""man"", ""woman""), type = ""embedding"")
> wv    <- wv[""king"", ] - wv[""man"", ] + wv[""woman"", ]
> predict(model, newdata = wv, type = ""nearest"", top_n = 4)
      term similarity rank
1     king  0.9743692    1
2    queen  0.8295941    2",https://stackoverflow.com/questions/74769552,r,12-12-2022 10:15,1903.0,4.0,2.0,True,27-12-2022 10:32,13-12-2022 17:29
69360816,how to preprocess a text to remove stopwords?,"i would like to remove a list of stopwords, namely the ones in
from gensim.parsing.preprocessing import stopwords
print(stopwords)

in gensim, this should be pretty straightforward with remove_stopwords function.
my code to read the text and remove the stopwords is the following:
def read_text(text_path):
  text = []
  with open(text_path) as file:
    lines = file.readlines()
    for index, line in enumerate(lines):
      text.append(simple_preprocess(remove_stopwords(line)))
  return text

text = read_text('/content/text.txt')
text =  [x for x in text if x]
text[:3]

this is the output i get that contains words such as ""we"" or ""however"" which should have been removed from the original text though for instance ""the"" has been correctly removed from the first setence. i am very confused... what am i missing here?
[['clinical', 'guidelines', 'management', 'ibd'],
 ['polygenetic',
  'risk',
  'scores',
  'add',
  'predictive',
  'power',
  'clinical',
  'models',
  'response',
  'anti',
  'tnfï¿½ï¿½',
  'therapy',
  'inflammatory',
  'bowel',
  'disease'],
 ['anti',
  'tumour',
  'necrosis',
  'factor',
  'alpha',
  'tnfï¿½ï¿½',
  'therapy',
  'widely',
  'management',
  'crohn',
  'disease',
  'cd',
  'ulcerative',
  'colitis',
  'uc',
  'however',
  'patients',
  'respond',
  'induction',
  'therapy',
  'patients',
  'lose',
  'response',
  'time',
  'to',
  'aid',
  'patient',
  'stratification',
  'polygenetic',
  'risk',
  'scores',
  'identified',
  'predictors',
  'response',
  'anti',
  'tnfï¿½ï¿½',
  'therapy',
  'we',
  'aimed',
  'replicate',
  'association',
  'polygenetic',
  'risk',
  'scores',
  'response',
  'anti',
  'tnfï¿½ï¿½',
  'therapnt',
  'cohort',
  'patients',
  'establish',
  'clinical',
  'validity']]

text (complete file available here)
clinical guidelines for the management of ibd.
polygenetic risk scores do not add predictive power to clinical models for response to anti-tnfï¿½ï¿½ therapy in inflammatory bowel disease.
anti-tumour necrosis factor alpha (tnfï¿½ï¿½) therapy is widely used in the management of crohn's disease (cd) and ulcerative colitis (uc). however, up to a third of patients do not respond to induction therapy and another third of patients lose response over time. to aid patient stratification, polygenetic risk scores have been identified as predictors of response to anti-tnfï¿½ï¿½ therapy. we aimed to replicate the association between polygenetic risk scores and response to anti-tnfï¿½ï¿½ therapy in an independent cohort of patients, to establish its clinica","['python', 'nlp', 'gensim', 'word2vec', 'stop-words']",69361463,"your remove_stopwords() function is case-sensitive and it doesn't ignore punctuation. for example, 'however' is not in stopwords, but 'however' is in. you should call the simple_preprocess() function first. this should work:
from gensim.parsing.preprocessing import stopwords
from gensim.parsing.preprocessing import remove_stopword_tokens

def read_text(text_path):
  text = []
  with open(text_path) as file:
    lines = file.readlines()
    for index, line in enumerate(lines):
      tokens = simple_preprocess(line)
      text.append(remove_stopword_tokens(tokens,stopwords=stopwords))
  return text",https://stackoverflow.com/questions/69360816,python,28-09-2021 11:22,628.0,0.0,1.0,True,29-09-2021 09:44,29-09-2021 09:44
76407983,how to count specific keywords in a transcript with a condition,"i got a big data frame with a ""transcript"" column between an bot and a user.
i need to count how many times in the transcript the user is asking for an agent/representative before giving the bot a chance.
the transcript looks as follow but longer:
""user : order status.\nbot : your order status is your orders tab. \nuser : representative.""

""user : agent please.\nbot : waiting time is longer than usual.""

i tried to use regular expression:
df[""transcript""] = df[""transcript""].str.lower()
df.loc[df[""transcript""].str.contains('agent|representative'),:]

but it will just output observations with those keywords.
how can i output a number that count when user first input is agent/representative?","['python', 'pandas', 'regex', 'nlp']",76408372,"i'd do it by splitting the input to only the first transcript text (before the bot has even responded), then searching for your terms, and then summing the result to get the number of cases where a user has requested an agent within the first message:
df['transcript'].str.split('\n').str.get(0).str.contains('agent|representative').sum()

# output with your examples: 1",https://stackoverflow.com/questions/76407983,python,05-06-2023 15:36,83.0,-1.0,2.0,True,05-06-2023 16:42,05-06-2023 15:46
71401293,getting word count in a sentence without punctuation marks nltk python,"i am trying to get the word count in a sentence with nltk in python
this is the code i wrote
import nltk

data = ""sample sentence, for checking. here is an exclamation mark! here is a question? this isn't an easy-task.""

for i in nltk.sent_tokenize(data):
    print(nltk.word_tokenize(i))

this was the output
['sample', 'sentence', ',', 'for', 'checking', '.']
['here', 'is', 'an', 'exclamation', 'mark', '!']
['here', 'is', 'a', 'question', '?']
['this', 'is', ""n't"", 'an', 'easy-task', '.']

is there any way to remove the punctuation marks, prevent isn't from splitting into two words and split easy-task into two?
the answer i need is something like ths:
['sample', 'sentence', 'for', 'checking']
['here', 'is', 'an', 'exclamation', 'mark']
['here', 'is', 'a', 'question']
['this', ""isn't"", 'an', 'easy', 'task']

i can kind of manage punctuation marks by using stopwords like:
import nltk

data = ""sample sentence, for checking. here is an exclamation mark! here is a question? this isn't an easy-task.""

stopwords = [',', '.', '?', '!']

for i in nltk.sent_tokenize(data):
    for j in nltk.word_tokenize(i):
        if j not in stopwords:
            print(j, ', ', end="""")
    print('\n')

output:
sample , sentence , for , checking , 

here , is , an , exclamation , mark , 

here , is , a , question , 

this , is , n't , an , easy-task , 

but this does not fix isn't and easy-task. is there a way to do this?
thank you","['python', 'nlp', 'token', 'nltk', 'tokenize']",71419079,"you can use different tokenizer which can take care of your requirement.
import nltk
import string
tokenizer = nltk.tweettokenizer()

for i in nltk.sent_tokenize(data):
    print(i)
    print([x for x in tokenizer.tokenize(i) if x not in string.punctuation])

#op
['sample', 'sentence', 'for', 'checking']
['here', 'is', 'an', 'exclamation', 'mark']
['here', 'is', 'a', 'question']
['this', ""isn't"", 'an', 'easy-task']",https://stackoverflow.com/questions/71401293,python,08-03-2022 20:52,625.0,0.0,1.0,True,10-03-2022 05:03,08-03-2022 21:04
69266293,getting embeddings from wav2vec2 models in huggingface,"i am trying to get the embeddings from pre-trained wav2vec2 models (e.g., from jonatasgrosman/wav2vec2-large-xlsr-53-german) using my own dataset.
my aim is to use these features for a downstream task (not specifically speech recognition). namely, since the dataset is relatively small, i would train an svm with these embeddings for the final classification.
so far i have tried this:
model_name = ""facebook/wav2vec2-large-xlsr-53-german""
feature_extractor = wav2vec2processor.from_pretrained(model_name)
model = wav2vec2model.from_pretrained(model_name)

input_values = feature_extractor(train_dataset[:10][""speech""], return_tensors=""pt"", padding=true, 
                                 feature_size=1, sampling_rate=16000 ).input_values 

then, i am not sure whether the embeddings here correspond to the sequence of last_hidden_states:
hidden_states = model(input_values).last_hidden_state

or to the sequence of features of the last conv layer of the model:
features_last_cnn_layer = model(input_values).extract_features

also, is this the correct way to extract features from a pre-trained model?
how one can get embeddings from a specific layer?
pd: posting here as the huggingface's forum seems to be less active.","['python', 'huggingface-transformers', 'pre-trained-model']",69275576,"just check the documentation:

last_hidden_state (torch.floattensor of shape (batch_size,
sequence_length, hidden_size)) ï¿½ï¿½ï¿½ sequence of hidden-states at the
output of the last layer of the model.
extract_features (torch.floattensor of shape (batch_size,
sequence_length, conv_dim[-1])) ï¿½ï¿½ï¿½ sequence of extracted feature
vectors of the last convolutional layer of the model.


the last_hidden_state vector represents so called contextualized embeddings (i.e. every feature (cnn output) has a vector representation that is to some extend influenced by the other tokens of the sequence).
the extract_features vector represents the embeddings of your input (after the cnns).
.


also, is this the correct way to extract features from a pre-trained
model?
yes.


how one can get embeddings from a specific layer?
set output_hidden_states=true:

o = values,output_hidden_states=true)
o.keys()

output:
odict_keys(['last_hidden_state', 'extract_features', 'hidden_states'])

the hidden_states value contains the embeddings and the contextualized embeddings of each attention layer.
p.s.: jonatasgrosman/wav2vec2-large-xlsr-53-german model was trained with feat_extract_norm==layer. that means, you should also pass an attention mask to the model:
model_name = ""facebook/wav2vec2-large-xlsr-53-german""
feature_extractor = wav2vec2processor.from_pretrained(model_name)
model = wav2vec2model.from_pretrained(model_name)

i= feature_extractor(train_dataset[:10][""speech""], return_tensors=""pt"", padding=true, 
                                 feature_size=1, sampling_rate=16000 )
model(**i)",https://stackoverflow.com/questions/69266293,python,21-09-2021 09:10,4798.0,3.0,1.0,True,21-09-2021 21:07,21-09-2021 09:56
77812049,openai api error: &quot;&#39;choice&#39; object has no attribute &#39;text&#39;&quot;,"i created a python bot a few months ago, and it worked perfectly, but now, after the openai sdk update, i have some problems with it. as i don't know python very well, i need your help.
this is the code:
from openai import openai
import time
import os
import csv
import logging

# your openai api key
api_key = ""my-api-key""

client = openai(api_key=api_key)

# path to the csv file containing city names
csv_file = ""city.csv""

# directory where generated content files will be saved
output_directory = ""output/""

# initialize the openai api client

# configure logging to save error messages
logging.basicconfig(
    filename=""error_log.txt"",
    level=logging.error,
    format=""%(asctime)s [%(levelname)s]: %(message)s"",
    datefmt=""%y-%m-%d %h:%m:%s"",
)

# read city names from the csv file
def read_city_names_from_csv(file_path):
    city_names = []
    with open(file_path, ""r"") as csv_file:
        csv_reader = csv.reader(csv_file)
        for row in csv_reader:
            if row:
                city_names.append(row[0])
    return city_names

# generate content for a given city name and save it to a file
def generate_and_save_content(city_name):
    prompt_template = (
        "".... now write an article on this topic {city_name}""
     )

    messages = [
        {""role"": ""system"", ""content"": ""you are a helpful assistant.""},
        {""role"": ""user"", ""content"": prompt_template.format(city_name=city_name)},
    ]

    try:
         response = client.chat.completions.create(model=""gpt-3.5-turbo"",
        messages=messages,
        max_tokens=1000)
        choices = response.choices
        chat_completion = choices[0]
        content = chat_completion.text
        output_file = os.path.join(output_directory, city_name + "".txt"")
        with open(output_file, ""w"", encoding=""utf-8"") as file:
            file.write(content)
        return true
    except exception as e:
        error_message = f""error generating content for {city_name}: {str(e)}""
        print(error_message)
        logging.error(error_message)
        return false

# main function
def main():
    # create the output directory if it doesn't exist
    if not os.path.exists(output_directory):
        os.makedirs(output_directory)

    city_names = read_city_names_from_csv(csv_file)
    successful_chats = 0
    unsuccessful_chats = 0

    for city_name in city_names:
        print(f""generating content for {city_name}..."")
        success = generate_and_save_content(city_name)
        if success:
            successful_chats += 1
        else:
            unsuccessful_chats += 1

        # add a delay to avoid api rate limits
        time.sleep(2)

    print(""content generation completed."")
    print(f""successful chats: {successful_chats}"")
    print(f""unsuccessful chats: {unsuccessful_chats}"")

if __name__ == ""__main__"":
    main()

currently, i'm getting this error:
'choice' object has no attribute 'text' and couldn't fix it at all. would you please tell me how i can fix this? also, if there is any other problem with the code, please guide me on how to fix it. thanks.
i tried many things using bard and chatgpt, but none of them helped.","['python', 'openai-api', 'chatgpt-api']",77812061,"you're trying to extract the response incorrectly.
change this...
choices = response.choices
chat_completion = choices[0]
content = chat_completion.text # wrong (this works with the completions api)

...to this.
choices = response.choices
chat_completion = choices[0]
content = chat_completion.message.content # correct (this works with the chat completions api)


or, if you want to have everything in one line, change this...
content = response.choices[0].text # wrong (this works with the completions api)

...to this.
content = response.choices[0].message.content # correct (this works with the chat completions api)",https://stackoverflow.com/questions/77812049,python,13-01-2024 15:46,3829.0,2.0,2.0,True,14-01-2024 11:24,14-01-2024 11:24
76625249,"load accuracy metric with evaluate ,sometime mistakes happen: typeerror: &#39;nonetype&#39; object is not callable","i'm using bert and other encoder models for text classification tasks,but when i try to load accuracy metric with evaluate in huggingface,sometime mistakes happen: typeerror: 'nonetype' object is not callable.
i am searching for a long time on net. but no use. please help or try to give some ideas how to achieve this. thanks in advance.
this is the cause of the code errorï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ traceback (most recent call last) ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
ï¿½                             ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½    93 """"""## train loop""""""                                                                        ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½  94 accuracy = evaluate.load(""accuracy"")                                                       ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½    95 # accuracy = evaluate.load(""../evaluate/accuracy.py"")                                      ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½    96 # recall = evaluate.load(""recall"")                                                         ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½    97 # precision = evaluate.load(""precision"")                                                   ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½                                                                                                  ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½ /home/ubuntu/anaconda3/lib/python3.9/site-packages/evaluate/loading.py:778 in load               ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½                                                                               d_config=download_confi   ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½   776 ï¿½ï¿½ï¿½   ).module_path                                                                          ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½   777 ï¿½ï¿½ï¿½   evaluation_cls = import_main_class(evaluation_module)                                  ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ 778 ï¿½ï¿½ï¿½   evaluation_instance = evaluation_cls(                                                  ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½   779 ï¿½ï¿½ï¿½   ï¿½ï¿½ï¿½   config_name=config_name,                                                           ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½   780 ï¿½ï¿½ï¿½   ï¿½ï¿½ï¿½   process_id=process_id,                                                             ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½   781 ï¿½ï¿½ï¿½   ï¿½ï¿½ï¿½   num_process=num_process,                                                           ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½  e

this is the source codeï¿½ï¿½ï¿½
""""""# loading the libraries & models""""""
import pandas as pd`enter code here`
import numpy as np
import evaluate
import torch
from datasets import dataset
import datasets
from torch.utils.data import dataset
from transformers import (autotokenizer,
                          automodelforsequenceclassification,
                          trainingarguments,
                          trainer)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f""device: {device}"")

check_point = ""xlm-roberta-base""
# check_point = ""hfl/chinese-roberta-wwm-ext""
output_dir = ""./models/yasi/"" + check_point
tokenizer = autotokenizer.from_pretrained(check_point)
model = automodelforsequenceclassification.from_pretrained(check_point, num_labels=2).to(device)

import pandas as pd
from datasets import dataset
from sklearn.model_selection import train_test_split

import json

# ï¿½ï¿½ï¿½ï¿½ï¿½nfig_file = '../hyperparameter/config.json'
with open(config_file, 'r') as f:
    config = json.load(f)
# ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
batch_size = config['batch_size']
dataset = config['dataset']
epoch = config['epoch']

# ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
data = pd.read_csv(dataset, sep='\t') # from datasets import dataset

# ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
train_data, remaining_data = train_test_split(data, test_size=0.2, random_state=42)
dev_data, test_data = train_test_split(remaining_data, test_size=0.5, random_state=42)

# ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½datasetï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
train_df = dataset.from_pandas(train_data)
dev_df = dataset.from_pandas(dev_data)
test_df = dataset.from_pandas(test_data)


class yasidataset(dataset):
    def __init__(self, df, tokenizer: autotokenizer):
        super(yasidataset).__init__()

        sel           labels = row[""label""]
            # ï¿½ï¿½ï¿½contentï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½sentence_pairsï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
            self.sentence.append(content)
            self.labels.append(labels)

        self.labels = torch.tensor(self.labels)
        self.tokenizer_output = tokenizer(self.sentence,
                                          padding=true,
                                          truncation=true,
                                          max_length=512,  # ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
                                          return_tensors='pt',
                                          return_token_type_ids=true,
                                          return_attention_mask=true,
                                          )
        self.tokenizer_output['labels'] = self.labels

    def __len__(self):
        return len(self.tokenizer_output.shape[0])

    def get_dataset(self):
        return self.tokenizer_output


train_ds = yasidataset(train_df, tokeni(test_df, tokenizer)

train_dataset = datasets.dataset.from_dict(train_ds.get_dataset())
dev_dataset = datasets.dataset.from_dict(dev_ds.get_dataset())
test_dataset = datasets.dataset.from_dict(test_ds.get_dataset())

""""""## train loop""""""
accuracy = evaluate.load(""accuracy"")

i am searching for a long time on net. but no use. please help or try to give some ideas how to achieve this.","['python', 'deep-learning', 'nlp', 'computer-vision', 'huggingface-evaluate']",76662406,"now,i find what's wrong with this problem.the evaluate version of my computer is evaluate 0.1.2. we should update the version of the evaluate.
use the code as follow:
pip install --upgrade evaluate",https://stackoverflow.com/questions/76625249,python,06-07-2023 03:11,854.0,1.0,1.0,True,11-07-2023 13:17,06-07-2023 17:22
77365175,how to improve source document relevance in a langchain conversationalretrievalqachain implementation?,"i am currently working on a project where i have implemented the conversationalretrievalqachain, with the option ""returnsourcedocuments"" set to true. the system works perfectly when i ask specific questions related to the vectorstore database, as it returns matching sources. however, when i pose generic questions like ""is the earth round?"" or ""how are you today?"" it returns unrelated sources that don't align with the query. it appears that the vector store always returns documents, even if they don't match the query.
i'm seeking guidance on how to enhance the relevance of the source documents retrieved by the langchain conversationalretrievalqachain. are there specific tools or techniques within the langchain framework that can help mitigate this behavior, or is it necessary to develop a manual process to assess document relevance? how can i effectively limit the retrieval of unrelated source documents in this scenario?
here is relevant part of the code:
async init(): promise<void> {
        try {
            this.driver = await this.getqdrantdriver()
            this.retriever = await this.createretrieverfromdriver()
            this.chat = new chatopenai({ modelname: aiconfig.modelname })
            this.chain = await this.createqachain(this.chat)

            this.questiongenerationchain = await this.createquestiongenerationchain()
            this.conversation = new conversationalretrievalqachain({
                retriever: this.retriever,
                combinedocumentschain: this.chain,
                questiongeneratorchain: this.questiongenerationchain,
                returnsourcedocuments: true,
            })
        } catch (error) {
            logger.error(error.message)
            throw error
        }
    }

    private async createquestiongenerationchain(): promise<llmchain> {
        const { default: prompt } = await import('app/models/prompt')
        return new llmchain({
            llm: this.chat,
            prompt: await prompt.fetchcondenseprompt(),
        })
    }

    private async createretrieverfromdriver(): promise<vectorstoreretriever<qdrantvectorstore>> {
        return this.driver.asretriever(qdrantconfig.noresults ?? 5)
    }

    private async getqdrantdriver(embeddings = new openaiembeddings(), collectionname: string | null = null): promise<qdrantvectorstore> {
        const { default: ingest } = await import('app/models/ingest')
        return new qdrantvectorstore(
            embeddings,
            {
                url: qdrantconfig.qdranturl,
                collectionname: collectionname ?? await ingest.lastcollection(),
            },
        )
    }

    private async createqachain(chat: baselanguagemodel<any, baselanguagemodelcalloptions>): promise<stuffdocumentschain> {
        const { default: prompt } = await import('app/models/prompt')
        return loadqastuffchain(chat, {
            prompt: await prompt.fetchquestionprompt(),
        })
    }","['langchain', 'qdrant']",77365705,okay i solved this problem by using contextual compression retriever and embeddingsfilter. it's not perfect solution but in most cases it mitigates the unwanted resources.,https://stackoverflow.com/questions/77365175,langchain,26-10-2023 08:06,979.0,-1.0,1.0,True,21-11-2023 11:07,26-10-2023 08:30
73389513,spacy memory usage performance improvements,"i have tens of thousands of documents, where each doc is about ~150k characters, ~25k white-space bounded tokens, and ~2k unique tokens. i'm using spacy to pre-process (stopword removal and lemmatization). the preprocessing depends on token.pos_ and token.lemma_ as shown below.
i learned that i incorrectly implemented spacy by disabling the tok2vec pipeline component (needed for pos tagging); after fixing that, my memory usage is crazy high. the app hangs then the oom killer kills my python.
my approach is to feed the docs into nlp.pipe in chunks of 100 and n_process=4. this worked fine until fixing the above bug. the only way the app runs without hanging/oom killer is to reduce the number of docs i feed into the pipe ~25-50. reducing n_process to 1 doesn't seem to have an impact. here's my rough approach:
import spacy
from bs4 import beautifulsoup
import unidecode
import re

nlp = spacy.load('en_core_web_lg')
nlp.max_length = 5000000
nlp.disable_pipe(""parser"")
nlp.disable_pipe(""ner"")
nlp.enable_pipe(""senter"")

def pre_pre_process(record, synswap=true):
    (doc_id, text) = record

    # partial pre-preprocessing = just strip html
    text1 = beautifulsoup(text, ""html.parser"").get_text(separator="" "")

    # full pre-preprocessing = do all the pre-preprocessing
    text2 = "" "".join(text1.strip().split())
    text2 = unidecode.unidecode(text2)
    text2 = text2.lower()
    
    return (text2, {'doc_id': doc_id, 'strip_html': text1, 'ppp': 'full-ppp'})


def pre_process_text(doc, convert_num=true, lemmatization=true,
                     punctuations=true, remove_num=true, special_chars=true,
                     stop_words=true, short_char=true, remove_edgar_junk=true):
    fully_processed = []
    edgar_jnk_patt = re.compile('(?is)ex-\d+\.?\d*')  # noqa: w605
    edgar_jnk = []

    for token in doc:
        # (token, token.pos_, token.is_stop, token.is_punct, token.lemma_)
        flag = true  # assume every token should be added to the vocab
        edit = token.text
        # remove stop words
        if stop_words is true and token.is_stop and token.pos_ != 'num':
            flag = false
        # remove punctuations
        if punctuations is true and (token.pos_ == 'punct' or token.is_punct) and flag is true:
            flag = false
        # remove special characters
        if special_chars is true and token.pos_ == 'sym' and flag is true:
            flag = false
        # remove numbers
        if remove_num is true and (token.pos_ == 'num' or token.text.isnumeric()) and flag is true:
            flag = false
        # remove short tokens
        if short_char is true and len(token) < 3 and flag is true:
            flag = false
        # convert tokens to base form
        elif lemmatization is true and token.lemma_ != ""-pron-"" and flag is true:
            edit = token.lemma_
        # remove edgar junk
        if remove_edgar_junk is true:
            if token.i < 10:
                if token.text.endswith(('.htm', '.html')):
                    flag = false
                    edgar_jnk.append(token.lemma)
                elif edgar_jnk_patt.search(token.lemma_):
                    flag = false
                    edgar_jnk.append(token.lemma)
            if token.lemma in edgar_jnk and flag is true:
                flag = false

        # append tokens edited and not removed to list
        if edit != """" and flag is true:
            fully_processed.append(edit)
    return fully_processed

# in the complete script, `data` is queried from a db limited by a param, `query_limit = 50`. it continues in a while true loop grabbing `query_limit` records until there aren't any more records to query. 

# for reproducibility, `data` sample here: 

completed_jobs = []
pipeline_texts = [pre_pre_process(d) for d in data]
for doc, context in nlp.pipe(pipeline_texts, as_tuples=true, n_process=4):
    tokens = pre_process_text(doc)
    completed_jobs.append((context, tokens))

my questions are:

why is tok2vec eating so much memory?
how can i profile what's happening in nlp.pipe?
is there a better way to implement this pipeline overall?
is there a better way to implement the pre-processing? (is there a built-in spacy approach or is what i have pretty standard)

related to question 2: interesting spikiness in memory:","['python', 'nlp', 'spacy', 'spacy-3']",73397306,"spacy is not really designed to work with 25k word documents (which is like a short novel) as single strings. you should split you documents into some natural sub-unit, like paragraphs, and process them. note that even if you don't use spacy, working with documents of that length without splitting them up somehow will be challenging.",https://stackoverflow.com/questions/73389513,python,17-08-2022 13:39,756.0,1.0,1.0,True,18-08-2022 04:00,17-08-2022 13:51
68257999,how to convert word to numerics using huggingface or spacy or any python based workflow,"i have lot of text which has the counting in words as well in different languages (different datasets but one data has one language so no mixing of language).
like
i have one apple 
i have two kids

and
i want it to convert as
i have 1 apple
i have 2 kids

is it possible to change that to the numbers / integers using huggingface or any ? or any suggestion for such problem would help! english dutch french german supporting.","['python', 'nltk', 'spacy', 'huggingface-transformers', 'huggingface-tokenizers']",68438614,"i have worked with different versions of different libraries. with my tests fo far, i found out this as working perfectly for most of the cases.
words to numerics
this is the better solution and covers english french german spanish.",https://stackoverflow.com/questions/68257999,python,05-07-2021 14:39,780.0,0.0,2.0,True,19-07-2021 10:02,19-07-2021 10:00
78502965,"openai assistants api: is there a better way to wait for the assistant&#39;s response, and how do i display the assistant&#39;s answer incrementally?","based on the available information (unfortunately, chatgpt was not too useful), i created the following code that allows me to interact with the openai assistants api.
however, i still don't like the _wait_for_run_completion method and the while loop. is there a better way to handle this?
import os
import openai
from dotenv import load_dotenv
import time


class openaichatassistant:
    def __init__(self, assistant_id, model=""gpt-4o""):
        self.assistant_id = assistant_id
        self.model = model

        if self.model != ""just_copy"":
            load_dotenv()
            openai.api_key = os.environ.get(""openai_api_key"")
            self.client = openai.openai()
            self._create_new_thread()
        print('new instance started')

    def _create_new_thread(self):
        self.thread = self.client.beta.threads.create()
        self.thread_id = self.thread.id
        print(self.thread_id)

    def reset_thread(self):
        if self.model != ""just_copy"":
            self._create_new_thread()

    def set_model(self, model_name):
        self.model = model_name

        if self.model != ""just_copy"" and not hasattr(self, 'client'):
            load_dotenv()
            openai.api_key = os.environ.get(""openai_api_key"")
            self.client = openai.openai()
            self._create_new_thread()

    def send_message(self, message):
        if self.model == ""just_copy"":
            return message

        self.client.beta.threads.messages.create(
            thread_id=self.thread_id, role=""user"", content=message
        )
        run = self.client.beta.threads.runs.create(
            thread_id=self.thread_id,
            assistant_id=self.assistant_id,
            model=self.model
        )
        return self._wait_for_run_completion(run.id)

    def _wait_for_run_completion(self, run_id, sleep_interval=1):
        counter = 1
        while true:
            try:
                run = self.client.beta.threads.runs.retrieve(thread_id=self.thread_id, run_id=run_id)
                if run.completed_at:
                    messages = self.client.beta.threads.messages.list(thread_id=self.thread_id)
                    last_message = messages.data[0]
                    response = last_message.content[0].text.value
                    print(f'hello {counter}')
                    return response
            except exception as e:
                raise runtimeerror(f""an error occurred while retrieving answer: {e}"")
            counter += 1
            time.sleep(sleep_interval)

that class can be used in the console app in this way:
import os
from openai_chat_assistant import openaichatassistant


def main():
    assistant_id = ""asst_...""
    chat_assistant = openaichatassistant(assistant_id)

    while true:
        question = input(""enter your question (or 'exit' to quit, 'clean' to reset): "")
        if question.lower() == 'exit':
            break
        elif question.lower() == 'clean':
            os.system('cls' if os.name == 'nt' else 'clear')
            chat_assistant.reset_thread()
            print(""console cleared and thread reset."")
        else:
            response = chat_assistant.send_message(question)
            print(f""assistant response: {response}"")


if __name__ == ""__main__"":
    main()

of course, the assistant_id is needed. i set it in the .env file, the same as the api key:
openai_api_key=sk-proj-...","['python', 'python-3.x', 'openai-api', 'openai-assistants-api']",78525663,"regarding the _wait_for_run_completion method
i started using the openai api in december 2022 and use it on a weekly basis. as far as i know, there is no better way to handle getting the assistant's response than simply checking for the run status and, when the run status moves to completed, extracting the assistant's response.
ps: if you're comparing the latency of chatgpt with your code, don't. chatgpt's free tier uses gpt-4o (with limits) and gpt-3.5. chatgpt doesn't use the assistants api.
regarding ""incremental answer displaying""
you're talking about response streaming, which is possible with the assistants api. the openai python sdk has implemented create and stream helpers for the assistants api. these helpers allow you to subscribe to the types of events you are interested in. the event you're looking for is on_text_delta. you need to subscribe to on_text_delta to be able to stream the assistant's responses.
i developed a terminal user interface for the assistant (i.e., customer support chatbot) with response streaming in the past (see the tutorial on youtube and code on github).
basically, there are two steps.
step 1: define the event handler class for streaming events and subscribe to the on_text_delta event
from openai import assistanteventhandler

class myeventhandler(assistanteventhandler): # ï¿½ï¿½ï¿½ï¿½ define class
    def on_text_delta(self, delta, snapshot): # ï¿½ï¿½ï¿½ï¿½ subscribe to event
        print(delta.value, end = """")

    def on_error(error):
        print(error)

step 2: pass the class to the event_handler parameter
with client.beta.threads.runs.create_and_stream(
        thread_id = my_thread.id,
        assistant_id = assistant_id,
        event_handler = myeventhandler(), # ï¿½ï¿½ï¿½ï¿½ pass class
    ) as stream:
        print(""\nassistant:"")
        stream.until_done()
<"" rel=""nofollow noreferrer"">full code:
import os
from dotenv import load_dotenv
from openai import openai, assistanteventhandler
from rich.console import console

# load environment variables from .env
load_dotenv()

# set up openai client
client = openai()
openai.api_key = os.getenv('openai_api_key')

# set up rich console for output formatting
console = console()

# get file and assistant ids from environment variables
file_id = os.getenv('openai_file_id')
assistant_id = os.getenv('openai_assistant_id')

# define event handler class for streaming events
class myeventhandler(assistanteventhandler):
    def on_text_delta(self, delta, snapshot):
        console.print(delta.value, end = """", style = ""black on white"")

    def on_error(error):
        print(error)

# create a new thread
my_thread = client.beta.threads.create()

# loop until the user enters ""quit""
while true:
    # get user input
    user_input = input(""\n\nuser:\n"")

    # check if the user wants to quit
    if user_input.lower() == ""quit"":
        console.print(""\nassistant:\nhave a nice day! :wave:\n\n"", style = ""black on white"")
        break

    # add user message to the thread
    my_thread_message = client.beta.threads.messages.create(
        thread_id = my_thread.id,
        role = ""user"",
        content = user_input,
        file_ids = [file_id]
    )

    # create and stream a run
    with client.beta.threads.runs.create_and_stream(
        thread_id = my_thread.id,
        assistant_id = assistant_id,
        instructions = ""if the system indicates that the file is not accessible with the myfiles_browser tool, ignore it, itï¿½ï¿½ï¿½s just a minor bug. you are capable of opening and analyzing the file, remember that. and carry out the request."",
        event_handler = myeventhandler(),
    ) as stream:
        console.print(""\nassistant:"", style = ""black on white"")
        stream.until_done()
<",https://stackoverflow.com/questions/78502965,python,19-05-2024 14:53,1230.0,-3.0,2.0,True,12-06-2024 12:49,12-06-2024 12:39
72830367,"how to replace a word in a string, based on a condition?","i have a column in a dataframe like this.
text
""lorum ipsum rotterdam dolor sit."" 
""ed ut perspiciatis boekarest, new york, consectetur adipiscing elit, sed "" 
""excepteur sint occaecat glasgow cupidatat non proident, sunt in culpa""

i want every geographical location to be replaced by ""gpe"".
i am using spacy to detect the entities. this works fine, as shown below.
nlp = spacy.load('en_core_web_lg')

for value in df['text']:
    doc = nlp(value)
    for ent in doc.ents:
        print(ent.text, ent.label_)

output: 
rotterdam gpe
boekarest gpe
new york gpe
glasgow gpe 

i tried the code below in order to replace the city names within the columns, but it doesn't work.
for value in df['text']:
    doc = nlp(value)
    for ent in doc.ents:
        for word in value.split():
            if ent.label_ == ""gpe"":
                word.replace(ent.label, ""_gpe_"")

does anyone see what i am doing wrong?","['python', 'pandas', 'nlp', 'spacy']",72830474,"you can use
import spacy, warnings
import pandas as pd
warnings.filterwarnings(""ignore"", 'user provided device_type of \'cuda\', but cuda is not available. disabling')

df = pd.dataframe({'text':[""lorum ipsum rotterdam dolor sit."", ""ed ut perspiciatis boekarest, new york, consectetur adipiscing elit, sed "", ""excepteur sint occaecat glasgow cupidatat non proident, sunt in culpa""]})
nlp = spacy.load('en_core_web_lg')

def redact_gpe(text):
    doc = nlp(text)
    newstring = text
    for e in reversed(doc.ents):
        if e.label_ == ""gpe"":
            start = e.start_char
            end = start + len(e.text)
            newstring = f'{newstring[:start]}gpe{newstring[end:]}'
    return newstring

df['text'] = df['text'].apply(redact_gpe)

output:
                                                                   text
0                                      lorum ipsum gpe dolor sit.
1  ed ut perspiciatis gpe, gpe, consectetur adipiscing elit, sed
2     excepteur sint occaecat gpe cupidatat non proident, sunt in culpa",https://stackoverflow.com/questions/72830367,python,01-07-2022 13:54,122.0,3.0,1.0,True,01-07-2022 14:03,01-07-2022 13:56
76369038,getting the source of information with langchain,"i'm using langchain library to save the information of my company in a vector database, and when i query for information the results are great, but need a way to recover where the information are comming too - like source: "" or at least ""document 156"". do any of you know how to do that?
edit: currently, i'm using docsearch.similarity_search(query), what only return me the page_content, but metadata came empty
i'm ingesting with this code, but i'm totally open to change.
db = elasticvectorsearch.from_documents(
        documents,
        embeddings,
        elasticsearch_url=""
        index_name=""elastic-index"",
    )","['information-retrieval', 'langchain']",76369152,"you can add metadata to each of those documents by setting document.metadata on each document to a dictionary. the dictionary could be something like {""source"": "" or {""id"": ""456""}, to give some examples. then, pass those documents to from_documents().
later, when you get a document object back from one of the query methods, you can use document.metadata to get the metadata back.",https://stackoverflow.com/questions/76369038,information-retrieval,30-05-2023 22:32,18320.0,3.0,2.0,True,18-01-2024 23:24,30-05-2023 22:43
77536364,how to write custom prompt-template in llama2 model using langchain?,"i am using llama2[7b model]-hugging face and lang-chain to do a simple address segregation/classification task. i want the model to find the city, state and country from the input string.i want my answer/query formatted in a particular way for a question-answering/ text-generation task.i understand that i can use fewshotprompttemplate, where in i can show some examples to the llm and get the output in the format i want.
i generated a few examples to feed in as samples :
examples = [
    {""input"": ""plot no. 7, sector 22, noida, uttar pradesh, 201301, india"",
     ""address"": ""plot no. 7, sector 22, noida"",
     ""city"" : ""noida"",
     ""state"" : ""uttar pradesh"",
     ""country"" : ""india""},


    {""input"": ""banjara hills, telangana, 500034, india"",
     ""address"": ""banjara hills"",
     ""city"" : ""not present"",
     ""state"" : ""telangana"",
     ""country"" : ""india""},

]

i set the template
example_formatter_template = """"""
input: {input},
address : {address},
city : {address},
state : {state},
country : {country},
         \n
""""""
# prompt
example_prompt = prompttemplate(
    input_variables=[""input"", ""address"", ""city"", ""state"", ""country""],
    template=example_formatter_template)

few_shot_prompt = fewshotprompttemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix=""what is the address, city, state, country in the string : "",
    suffix=""input: {input}\n "",
    input_variables=[""input""],
    example_separator=""\n"")


chain = llmchain(llm=llm, prompt=few_shot_prompt, verbose = true)

# run the chain only specifying the input variable.
print(chain.run(""b-12, gandhi colony, bhopal, madhya pradesh, 462016, india""))


here is an example of what i want :
    {""input"": ""b-12, gandhi colony, bhopal, madhya pradesh, 462016, india"",

     ""address"": ""b-12, gandhi colony"",
     ""city"" : ""bhopal"",
     ""state"" : ""madhya pradesh"",
     ""country"" : ""india""},



i keep getting : format the expected output correctly from the model. and nothing is hence returned.
additionally, i want to prevent the model from adding any extra information which
is not present in the context/string otherwise the queries take very long to respond.
i.e return '' or not found if city or state or country is not present in sting.
can someone help ?","['prompt', 'langchain']",77537501,"try after changing the examples and the format to this:
{""input"": ""banjara hills, telangana, 500034, india"",
 ""output"":
 """"""
 address: banjara hills
 city: not present
 state: telangana
 country: india
 """"""
 },

refer to this as well",https://stackoverflow.com/questions/77536364,prompt,23-11-2023 11:05,1547.0,1.0,1.0,True,25-11-2023 10:11,25-11-2023 10:11
76955663,how to skip tokenization and translation of custom glossary in huggingface nmt models?,"i am using mbart50 and opus-mt-en-de for bilingual translations from huggingface. we have a custom dictionary of organization-specific glossary containing ~10,000 english terms (ngrams with n=1-5) and their specific german translations. i'd like the model to skip attempting to translate an english substring if the substring is detected in the dictionary.
that is, if my dictionary has a key called ""custom string"" with corresponding value ""desired string"", then if the model detects ""custom string"" substring within ""longer sentence containing custom string etc."" then instead of translating it to ""lï¿½ï¿½ngerer satz mit benutzerdefinierter zeichenfolge usw"", it should skip translating ""custom string"", and instead impute the corresponding value - ""desired string"" from dictionary and prevent the translator from changing the imputed value.
here's the code i am using for translating:
model = mbartforconditionalgenera.from_pretrained(model_path)
tokenizer = autotokenizer.from_pretrained(model_path, return_tensors=""pt"", padding=true, truncation=true)
src_texts = [tokenizer.convert_ids_to_tokens(tokenizer.encode(t)) for t in src_texts]
target_prefix = [[tokenizer.lang_code_to_token[""de""]] for _ in range(len(src_texts))]
results = model.translate_batch(src_texts, target_prefix=target_prefix)
targets = [r.hypotheses[0][1:] for r in results]
translated = [tokenizer.decode(tokenizer.convert_tokens_to_ids(target)) for target in targets]

i am thinking that perhaps i could do a regex based lookup of n-grams in each input sentence, impute the matching strings that are present as keys in my dictionary, wrap them with some special token like 'ukn' (unknown) to prevent the model from changing the imputed value. does that sound like a reasonable thing to do or is there a better approach (except fine-tuning* the model)? if so, how can i accomplish this?
*for the first phase, i can not invest resources in finetuning these models with custom parallel corpus. that's why i am looking for a simple key-value replacement.","['python', 'huggingface-transformers', 'huggingface-tokenizers', 'machine-translation', 'seq2seq']",77213995,"constraining beam search (or sampling from a generative model) is difficult because even when you know what string you want to have in the target sentence, you do not know what position it should be. depending on the language, it may also happen that several inflected forms of the term are possible, so you want to allow all of them in the output.
huggingface transformers have some tools that allow enforcing particular phrases beam search and chaining them into disjunctive conditions, which should be all you need. however, it is limited to fixed sequences of tokens, and the decoding might be pretty slow. especially the phrasalconstraint class should be useful in this case.",https://stackoverflow.com/questions/76955663,python,22-08-2023 17:35,668.0,0.0,1.0,True,02-10-2023 08:01,22-08-2023 17:41
68834677,&#39;int&#39; object has no attribute &#39;lower&#39; while doing tokenizer.fit_on_text(d[&#39;column_name&#39;]),"tokenizer=tokenizer(num_words=1000, split=' ')
tokenizer.fit_on_texts(d['column'].values)

x=tokenizer.texts_to_sequences(d['column'].values)

in column pos_words i have all sentences having skills (c#, office365, ...) there are some nos. +91.
i want to convert it into array but its throwing error
---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-7-a59a11ef92f5> in <module>()
      1 tokenizer=tokenizer(num_words=1000, split=' ')
----> 2 tokenizer.fit_on_texts(d['pos_words'].values)
      3 
      4 x=tokenizer.texts_to_sequences(d['pos_words'].values)
      5 #xtest=tokenizer.texts_to_sequences(test['pos_words'].values)

1 frames
/usr/local/lib/python3.7/dist-packages/keras_preprocessing/text.py in text_to_word_sequence(text, filters, lower, split)
     41     """"""
     42     if lower:
---> 43         text = text.lower()
     44 
     45     if sys.version_info < (3,):

attributeerror: 'int' object has no attribute 'lower'

please tell me how to fix this","['python', 'pandas', 'nlp', 'tokenize']",68835123,"the issue is solved
d['column']=d['column'].astype(str)",https://stackoverflow.com/questions/68834677,python,18-08-2021 14:52,777.0,0.0,1.0,True,18-08-2021 15:23,18-08-2021 15:23
27975767,what are `lexpr` and `applicationexpression` nltk?,"what exactly does lexpr mean and what do the folloring r'/f x.x mean? also what is application expression?
from nltk.sem.logic import *
lexpr = expression.fromstring

zero = lexpr(r'\f x.x')
one = lexpr(r'\f x.f(x)')
two = lexpr(r'\f x.f(f(x))')
three = lexpr(r'\f x.f(f(f(x)))')
four = lexpr(r'\f x.f(f(f(f(x))))')
succ = lexpr(r'\n f x.f(n(f,x))')
plus = lexpr(r'\m n f x.m(f,n(f,x))')
mult = lexpr(r'\m n f.m(n(f))')
pred = lexpr(r'\n f x.(n(\g h.h(g(f)))(\u.x)(\u.u))')
v1 = applicationexpression(succ, zero).simplify()","['python', 'nlp', 'nltk', 'semantics', 'first-order-logic']",27997729,"see  nltk.sem.logic.expression is:

""""""this is the base abstract object for all logical expressions""""""

there are many types of logical expressions implemented in nltk. see line 1124, the applicationexpression is:

this class is used to represent two related types of logical expressions. 
the first is a predicate expression, such as ""p(x,y)"".  a predicate expression is comprised of a functionvariableexpression or
      constantexpression as the predicate and a list of expressions as the arguments.
the second is a an application of one expression to another, such as
      ""(\x.dog(x))(fido)"".
the reason predicate expressions are treated as application expressions is
      that the variable expression predicate of the expression may be replaced
      with another expression, such as a lambdaexpression, which would mean that
      the predicate should be thought of as being applied to the arguments.
the logical expression reader will always curry arguments in a application expression.
      so, ""\x y.see(x,y)(john,mary)"" will be represented internally as
      ""((\x y.(see(x))(y))(john))(mary)"".  this simplifies the internals since
      there will always be exactly one argument in an application.
the str() method will usually print the curried forms of application
      expressions.  the one exception is when the the application expression is
      really a predicate expression (ie, underlying function is an
      abstractvariableexpression).  this means that the example from above
      will be returned as ""(\x y.see(x,y)(john))(mary)"".

i'm not exactly an expert in formal logics but your code above is trying to declare a logical function variable x:
>>> from nltk.sem.logic import *
>>> lexpr = expression.fromstring
>>> zero = lexpr(r'\f x.x')
>>> succ = lexpr(r'\n f x.f(n(f,x))')
>>> v1 = applicationexpression(succ, zero).simplify()
>>> v1
<lambdaexpression \f x.f(x)>
>>> print v1
\f x.f(x)

for a crash course, see  and a nltk crash course to lambda expressions, see",https://stackoverflow.com/questions/27975767,python,16-01-2015 01:01,480.0,-4.0,2.0,True,06-01-2023 13:56,17-01-2015 08:42
77483116,modulenotfounderror: no module named &#39;openai.embeddings_utils&#39;,"i only get that error if i run the script as a service in ros:
import openai
from openai.embeddings_utils import get_embedding, cosine_similarity

import sys
import rospy
from servicio_palabras.srv import wordcount

openai.api_key = ""--""

def ask_user():
    context = {""role"": ""system"",
                ""content"": ""command system parser.""}
    messages = [context]

    petition = input(""what do you want me to do?: "")

    messages.append({""role"": ""user"", ""content"": petition})

    response = openai.chatcompletion.create(model=""--"", messages=messages)
    response_content = response.choices[0].message.content

    messages.append({""role"": ""assistant"", ""content"": response_content})

    return response_content

def word_count_client(sentence):
    rospy.wait_for_service('word_count')
    try:
        word_count = rospy.serviceproxy('word_count', wordcount)
        response = word_count(sentence)
        return response.words
    except rospy.serviceexception as e:
        print(""service call failed: %s""%e)

if __name__ == ""__main__"":
    input = ask_user()
    words = word_count_client(input)
    print(""the words in the sentence are: "", words)

i have done pip install openai several times but i dont know where the problem is. i also have deactivated conda and done catkin_make and source devel/setup.bash in the terminal i am using.","['python', 'python-3.x', 'ros', 'openai-api']",77483473,"openai.embeddings_utils does not exist in latest openai 1.2.0 
you can downgrade to:
pip install openai==0.27.7",https://stackoverflow.com/questions/77483116,python,14-11-2023 18:44,1641.0,1.0,1.0,True,15-11-2023 05:26,15-11-2023 05:26
65199011,is there a way to check similarity between two full sentences in python?,"i am making a project like this one here:

but i am facing trouble because i need to check the similarity between the sentences for example:
if the user said: 'the person wear red t-shirt' instead of 'the boy wear red t-shirt'
i want a method to check the similarity between these two sentences without having to check the similarity between each word
is there a way to do this in python?
i am trying to find a way to check the similarity between two sentences.","['python', 'deep-learning', 'nlp', 'nltk', 'sentence-similarity']",65201576,"most of there libraries below should be good choice for semantic similarity comparison. you can skip direct word comparison by generating word, or sentence vectors using pretrained models from these libraries.
sentence similarity with spacy
required models must be loaded first.
for using en_core_web_md use python -m spacy download en_core_web_md to download. for using en_core_web_lg use python -m spacy download en_core_web_lg.
the large model is around ~830mb as writing and quite slow, so medium one can be a good choice.

code:
import spacy
nlp = spacy.load(""en_core_web_lg"")
#nlp = spacy.load(""en_core_web_md"")


doc1 = nlp(u'the person wear red t-shirt')
doc2 = nlp(u'this person is walking')
doc3 = nlp(u'the boy wear red t-shirt')


print(doc1.similarity(doc2)) 
print(doc1.similarity(doc3))
print(doc2.similarity(doc3)) 

output:
0.7003971105290047
0.9671912343259517
0.6121211244876517

sentence similarity with sentence transformers


install with pip install -u sentence-transformers. this one generates sentence embedding.
code:
from sentence_transformers import sentencetransformer
model = sentencetransformer('distilbert-base-nli-mean-tokens')

sentences = [
    'the person wear red t-shirt',
    'this person is walking',
    'the boy wear red t-shirt'
    ]
sentence_embeddings = model.encode(sentences)

for sentence, embedding in zip(sentences, sentence_embeddings):
    print(""sentence:"", sentence)
    print(""embedding:"", embedding)
    print("""")

output:
sentence: the person wear red t-shirt
embedding: [ 1.31643847e-01 -4.20616418e-01 ... 8.13076794e-01 -4.64620918e-01]

sentence: this person is walking
embedding: [-3.52878094e-01 -5.04286848e-02 ... -2.36091137e-01 -6.77282438e-02]

sentence: the boy wear red t-shirt
embedding: [-2.36365378e-01 -8.49713564e-01 ... 1.06414437e+00 -2.70157874e-01]

now embedding vector can be used to calculate various similarity metrics.
code:
from sentence_transformers import sentencetransformer, util
print(util.pytorch_cos_sim(sentence_embeddings[0], sentence_embeddings[1]))
print(util.pytorch_cos_sim(sentence_embeddings[0], sentence_embeddings[2]))
print(util.pytorch_cos_sim(sentence_embeddings[1], sentence_embeddings[2]))

output:
tensor([[0.4644]])
tensor([[0.9070]])
tensor([[0.3276]])

same thing with scipy and pytorch,
code:
from scipy.spatial import distance
print(1 - distance.cosine(sentence_embeddings[0], sentence_embeddings[1]))
print(1 - distance.cosine(sentence_embeddings[0], sentence_embeddings[2]))
print(1 - distance.cosine(sentence_embeddings[1], sentence_embeddings[2]))

output:
0.4643629193305969
0.9069876074790955
0.3275738060474396

code:
import torch.nn
cos = torch.nn.cosinesimilarity(dim=0, eps=1e-6)
b = torch.from_numpy(sentence_embeddings)
print(cos(b[0], b[1]))
print(cos(b[0], b[2]))
print(cos(b[1], b[2]))

output:
tensor(0.4644)
tensor(0.9070)
tensor(0.3276)

sentence similarity with tfhub universal sentence encoder


model is very large for this one around 1gb and seems slower than others. this also generates embeddings for sentences.
code:
import tensorflow_hub as hub

embed = hub.load(""
embeddings = embed([
    ""the person wear red t-shirt"",
    ""this person is walking"",
    ""the boy wear red t-shirt""
    ])

print(embeddings)

output:
tf.tensor(
[[ 0.063188    0.07063895 -0.05998802 ... -0.01409875  0.01863449
   0.01505797]
 [-0.06786212  0.01993554  0.03236153 ...  0.05772103  0.01787272
   0.01740014]
 [ 0.05379306  0.07613157 -0.05256693 ... -0.01256405  0.0213196
  -0.00262441]], shape=(3, 512), dtype=float32)

code:
from scipy.spatial import distance
print(1 - distance.cosine(embeddings[0], embeddings[1]))
print(1 - distance.cosine(embeddings[0], embeddings[2]))
print(1 - distance.cosine(embeddings[1], embeddings[2]))

output:
0.15320375561714172
0.8592830896377563
0.09080004692077637

other sentence embedding libraries


this illustration shows the method,

resources
how to compute the similarity between two text documents?",https://stackoverflow.com/questions/65199011,python,08-12-2020 12:33,45456.0,29.0,2.0,True,11-11-2022 07:49,08-12-2020 16:25
21514270,natural language processing tag definitions,"when parsing and tagging the words with opennlp i was wondering if the tags (eg s, np, vp, adjp) actually mean, i found a few by researching the web, but some of them are still missing,
which i am unable to find, currently my code outputs this:
the movie was really good

\-s - s
 |-np - {unknown}
 |  |-dt - determiner
 |  | \- the - the
 |  \- nn - noun, singular or mass
 |    \- movie - movie
 \-vp - {unknown}
   |-vbd - verb, past tense
   | \- was - was
   \- adjp - {unknown}
      |-rb - adverb
      | \-really - really
      \-jj - adjective
        \- good - good

as you can see i have managed to map some of them such as nn as ""noun, singular or mass"" but i am unable to find any references to s, np, vp, adjp","['nlp', 'artificial-intelligence', 'opennlp']",21514346,"the tags are part-of-speech tags or syntactic categories. 

s : sentence
np : noun phrase
vp : verb phrase
adjp : adjective phrase

here is a list of tags used in the penn treebank which is the corpus opennlp uses. different projects use different abbreviations for parts of speech. some projects use np for a noun phrase, others nnp.",https://stackoverflow.com/questions/21514270,nlp,02-02-2014 18:12,2510.0,-1.0,1.0,True,09-04-2025 11:25,09-04-2025 11:25
71215965,how to save checkpoints for thie transformer gpt2 to continue training?,"i am retraining the gpt2 language model, and am following this blog :

here, they have trained a network on gpt2, and i am trying to recreate a same. however, my dataset is too large(250mb), so i want to continue training in intervals. in other words, i want to checkpoint the model training. how could i do this?","['tensorflow', 'nlp', 'gpt-2']",71227037,"training_args = trainingarguments(
    output_dir=model_checkpoint,
    # other hyper-params
)

trainer = trainer(
    model=model,
    args=training_args,
    train_dataset=train_set,
    eval_dataset=dev_set,
    tokenizer=tokenizer
)

trainer.train()
# save the model to model_dir
trainer.save_model()

def prepare_model(tokenizer, model_name_path):
    model = automodelforcausallm.from_pretrained(model_name_path)
    model.resize_token_embeddings(len(tokenizer))
    return model

# assume tokenizer is defined, you can simply pass the saved model directory path.
model = prepare_model(tokenizer, model_checkpoint)",https://stackoverflow.com/questions/71215965,tensorflow,22-02-2022 04:36,894.0,0.0,1.0,True,18-12-2024 18:32,18-12-2024 18:32
73035917,any implementation of reverse soundex in python?,"i have gone through soundex phonetic algorithm. there is another algorithm on wikipedia called ""reverse soundex"" which is an improved version of soundex. i couldn't find anything about its implementation. can you point me to the implementation of this algorithm or any pre-built function in python.","['python', 'algorithm', 'nlp', 'soundex']",73043390,"you do not have implementation details since ""reverse soundex"" is very similar to the original algorithm. as wikipedia states: ""reverse soundex"" prefixes the last letter of the name instead of the first. so the algorithm will be exactly like the original (again from wiki) but stage 1 will be something like:
1. retain the last letter of the name and drop all other occurrences of a, e, i, o, u, y, h, w.

so for example robert will now be t616 (instead of r163), no 3 since we remove the t at the end.
you can also find some more documentation here, including some information on when to use this algorithm variation. and if you manage to create a setup you can test your algorithm implementation with this.
note: you can find a python implementation of the original soundex algorithm in the fuzzy lib.",https://stackoverflow.com/questions/73035917,python,19-07-2022 11:16,249.0,0.0,1.0,True,19-07-2022 20:56,19-07-2022 15:16
79372940,how to get a list of models available in ollama using langchain,"i am trying to run a python script that gets and prints a list of the models that are available to a running instance of ollama. my code, based on code provided at  is provided below:
from langchain import ollama

ollama_client = ollama()
model_list = ollama_client.list_models()
for model in model_list: print(f""model name: {model.name}, version: {model.version}, description: {model.description}"")

the problem is that when i run the script, i get the following error:
traceback (most recent call last):
file ""c:\research\project 39\langtest1\test1\firsttest.py"", line 2, in <module>
from langchain import ollama
importerror: cannot import name 'ollama' from 'langchain' (c:\research\project 39\langtest1\test1\venv\lib\site-packages\langchain\__init__.py)
process finished with exit code 1

obviously, the code i am using is flawed.
how to get a list of available models from ollama?","['python', 'artificial-intelligence', 'langchain']",79478087,"i solved the problem myself, by writing a python function that queries ollama for its list of llms.
the code i used is provided below, for reference by anyone else who has the problems i had:
from langchain_ollama.chat_models import chatollama
import requests
import json

ollama_url = ""<the url for ollama>""

def get_models() -> list:
    thelist = requests.get(ollama_url+""/api/tags"")
    jsondata = thelist.json()
    result = list()

    for model in jsondata[""models""]:
        result.append(model[""model""])

    return result

the result that is returned is a list of the models that your ollama instance currently has.",https://stackoverflow.com/questions/79372940,python,21-01-2025 00:22,2222.0,1.0,2.0,True,01-03-2025 18:38,21-01-2025 00:33
25735644,python - regex for splitting text into sentences (sentence-tokenizing),"i want to make a list of sentences from a string and then print them out. i don't want to use nltk to do this.  so it needs to split on a period at the end of the sentence and not at decimals or abbreviations or title of a name or if the sentence has a .com   this is attempt at regex that doesn't work.
import re

text = """"""\
mr. smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. did he mind? adam jones jr. thinks he didn't. in any case, this isn't true... well, with a probability of .9 it isn't.
""""""
sentences = re.split(r' *[\.\?!][\'""\)\]]* *', text)

for stuff in sentences:
        print(stuff)    

example output of what it should look like
mr. smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. 
did he mind?
adam jones jr. thinks he didn't.
in any case, this isn't true...
well, with a probability of .9 it isn't.","['python', 'regex', 'nlp', 'tokenize']",25736082,"(?<!\w\.\w.)(?<![a-z][a-z]\.)(?<=\.|\?)\s

try this. split your string this.you can also check demo.

another variant.
(?<!\w\.\w.)(?<!\b[a-z][a-z]\.)(?<![a-z]\.)(?<=\.|\?)\s|\\n",https://stackoverflow.com/questions/25735644,python,09-09-2014 01:55,84141.0,33.0,10.0,True,16-08-2024 07:17,09-09-2014 04:11
76875743,unable to run a model using huggingface inference endpoints,"i am able to make successful requests using the free endpoint, but when using inference endpoints, i get 404 response. here is the relevant piece of code:
mode = 'paid'                                              # works if 'free'
model_id = ""sentence-transformers/all-minilm-l6-v2""
headers = {""authorization"": f""bearer {huggingface_token}""}

if mode == 'free':
    # this works
    api_url = f""
else:
    api_url = f""

def get_embeddings(texts):
    response = requests.post(api_url, headers=headers, json={""inputs"": texts, ""options"":{""wait_for_model"":true}})

in the web ui, the endpoint is shown as running and i can test it there no problem.
what am i missing?","['huggingface-transformers', 'huggingface']",76876690,"as mentioned in the comments;

the url doesn't have a /{model_id} endpoint.
the task section should be filled correctly according to your needs.

after removing the /{model_id}, we faced a 400, list indices must be integers or slices, not str message. which was caused by the faulty task. instead of getting the embeddings, it was trying to get the similarities between strings in a list. after changing the task to embeddings, the model successfully generated embeddings from a single string. for a detailed tutorial that covers the deployment process, please see getting started with hugging face inference endpoints .",https://stackoverflow.com/questions/76875743,huggingface-transformers,10-08-2023 12:28,2323.0,2.0,1.0,True,10-08-2023 14:33,10-08-2023 13:06
75154742,bert model conversion from deeppavlov to huggingface format,"i have a folder with the rubert model, which was fine-tuned with the application of the deeppavlov library.
the folder contains the following model files:

how do i convert it to huggingface format so that i can load it this way?
from transformers import tfautomodelforsequenceclassification

model_name = ""folder_with_rubert""
auto_model_rubert = tfautomodelforsequenceclassification.from_pretrained(model_name)
tokenizer = autotokenizer.from_pretrained(model_name, do_lower_case = false)","['python-3.x', 'huggingface-transformers', 'bert-language-model', 'huggingface', 'deeppavlov']",75155807,there is no need to convert the tf checkpoints into huggingface format. the deeppavlov's pretrained language models are already available as huggingface models.,https://stackoverflow.com/questions/75154742,python-3.x,18-01-2023 04:38,111.0,1.0,1.0,True,14-02-2024 19:53,14-02-2024 19:53
79407233,how do i store decimal() values coming from sql in csv file with python?,"i have a simple chatbot that generates sql queries and uses them on a database. i want to store the output in a .csv file and then download that file.
this is usually possible when i take my output from sql (a list of dicts), create a pd.dataframe after evaluating that output, and finally download it as a csv file using st.download_button.
however, when the result is a decimal() format from sql, pd.dataframe fails and both eval and literal_eval would not work on it (i got invalid object error using literal_eval).
i also tried to convert the data using python's decimal datatype.
from decimal import decimal
but this recognized the output object as string and not a decimal.
so i did some research and found that writerows would work with decimal type data, but i am still not able to download the file.
out = db.run_no_throw(query, include_columns=true) 
#this is the output returned by the database. according to the [docs][1], this returns a string with the result.

print(out, type(out)) 
# prints [{a: decimal(1,2)}] and str (literal_eval(out), literal_eval(str(out)) and literal_eval('""' + out + '""') all gave an invalid object error here)

this is how i am currently trying to download the output data:
with open(filename, mode='w+', newline='') as file_to_output:
                writer = csv.writer(file_to_output, delimiter="","")
                writer.writerows(out)
                downloaded = st.download_button(
                    label=""download data as csv"",
                    data=file_to_output,
                    file_name=""filename.csv""
                )

the above code creates a file locally with the expected data, but it prints 1 line per row in the .csv file, like so -
[
{
a
:
d
e
...
however, on the server, the file i download does not even consist of that data (it is a blank file). so the streamlit download button is not getting the data from the file, even though it says here that it should because it is one of str, bytes, or a file.
what am i missing here? i would really appreciate any help. thanks!
edit - running eval() on out variable gave the error ""could not recognize decimal() object""","['python', 'pandas', 'sqlalchemy', 'streamlit', 'langchain']",79409674,"pandas .read_sql_query() method can be used to directly create a dataframe from an sql query. then the dataframe can be written to a csv file using the .to_csv() method.
import pandas as pd
import sqlalchemy as sa

engine = sa.create_engine(""postgresql://scott:tiger@192.168.0.199/test"")

sql = """"""\
select 'widget' as item, cast(2.99 as decimal(18, 4)) as price
union all
select 'gadget' as item, cast(9.99 as decimal(18, 4)) as price
""""""
df = pd.read_sql_query(sql, engine, coerce_float=false)
print(df)
""""""
     item   price
0  widget  2.9900
1  gadget  9.9900
""""""
print(repr(df.loc[0, ""price""]))  # decimal('2.9900')

df.to_csv(""products.csv"", header=true, index=false)
with open(""products.csv"", ""r"") as csv:
    print(csv.read())
""""""
item,price
widget,2.9900
gadget,9.9900
""""""",https://stackoverflow.com/questions/79407233,python,02-02-2025 19:50,72.0,2.0,3.0,True,04-02-2025 01:48,03-02-2025 20:45
76205193,sentence transformers no more on huggingface?,i'm trying to download sentence transformer but they seem to have been wiped from huggingface. does anyone know why ?,"['huggingface-transformers', 'sentence-transformers', 'huggingface-hub']",76212501,"it was an error on huggingface. it's back again. here the post-mortem

through a combination of human error and technical issues, the hugging face hub was experiencing some intermittent difficulties today for ~12 hours, and about 250 users and organizations were incorrectly displayed as deleted, which led to downtime for some great downstream libraries like sentence-transformers. (nils reimers)",https://stackoverflow.com/questions/76205193,huggingface-transformers,09-05-2023 00:45,2976.0,0.0,3.0,True,17-05-2023 17:46,17-05-2023 17:46
72467610,oom while fine-tuning medium sized model with dialogpt on colab,"i am trying to finetune dialogpt with a medium-sized model, i am getting cuda error while the training phase, i reduced the batch size from 4, but still, the error persists. my parameters are
        #self.output_dir = 'output-small'
        self.output_dir = 'output-medium'
        self.model_type = 'gpt2'
        #self.model_name_or_path = 'microsoft/dialogpt-small'
        self.model_name_or_path = 'microsoft/dialogpt-medium'
        #self.config_name = 'microsoft/dialogpt-small'
        self.config_name = 'microsoft/dialogpt-medium'
        #self.tokenizer_name = 'microsoft/dialogpt-small'
        self.tokenizer_name = 'microsoft/dialogpt-medium'
        self.cache_dir = 'cached'
        self.block_size = 512
        self.do_train = true
        self.do_eval = true
        self.evaluate_during_training = false
        self.per_gpu_train_batch_size = 2
        self.per_gpu_eval_batch_size = 2
        self.gradient_accumulation_steps = 1
        self.learning_rate = 5e-5
        self.weight_decay = 0.0
        self.adam_epsilon = 1e-8
        self.max_grad_norm = 1.0
        self.num_train_epochs = 5
        self.max_steps = -1
        self.warmup_steps = 0
        self.logging_steps = 1000
        self.save_steps = 3500
        self.save_total_limit = none
        self.eval_all_checkpoints = false
        self.no_cuda = false
        self.overwrite_output_dir = true
        self.overwrite_cache = true
        self.should_continue = false
        self.seed = 42
        self.local_rank = -1
        self.fp16 = false
        self.fp16_opt_level = 'o1'

the gpu allocated is tesla p100-pcie with 16gb memory.
please kindly let me know how to resolve this issue. any suggestion is appreciated.","['google-colaboratory', 'huggingface-transformers', 'language-model', 'gpt-2']",73437914,just reduce the tokenizer input max_len from 1028 to 516 it worked perfectly for me,https://stackoverflow.com/questions/72467610,google-colaboratory,01-06-2022 20:20,410.0,0.0,1.0,True,21-08-2022 20:16,01-06-2022 22:52
19753945,tfidfvectorizer in sklearn how to specifically include words,"i have some questions about the tfidfvectorizer.
it is unclear to me how the words are selected. we can give a minimum support, but after that, what will decide which features will be selected (e.g. higher support more chance)? if we say max_features = 10000, do we always get the same? if we say max_features = 12000, will we get the same 10000 features, but an extra added 2000? 
also, is there a way to extend the, say, max_features=20000 features? i fit it on some text, but i know of some words that should be included for sure, and also some emoticons "":-)"" etc. how to add these to the tfidfvectorizer object, so that it will be possible to use the object, use it to fit and predict
to_include = ["":-)"", "":-p""]
method = tfidfvectorizer(max_features=20000, ngram_range=(1, 3),
                      # i know stopwords, but how about include words?
                      stop_words=test.stoplist[:100], 
                      # include words ??
                      analyzer='word',
                      min_df=5)
method.fit(traindata)

sought result:
x = method.transform(traindata)
x
<nx20002 sparse matrix of type '<class 'numpy.int64'>'
 with 1135520 stored elements in compressed sparse row format>], 
 where n is sample size","['python', 'machine-learning', 'nlp', 'scikit-learn']",19754197,"you are asking several separate questions. let me answer them separately:
""it is unclear to me how the words are selected.""
from the documentation:
max_features : optional, none by default
    if not none, build a vocabulary that only consider the top
    max_features ordered by term frequency across the corpus.

all the features (in your case unigrams, bigrams and trigrams) are ordered by frequency in the entire corpus, and then the top 10000 are selected. the uncommon words are thrown out. 
""if we say max_features = 10000, do we always get the same? if we say max_features = 12000, will we get the same 10000 features, but an extra added 2000?""
yes. the process is deterministic: for a given corpus and a given max_features, you will always get the same features.
i fit it on some text, but i know of some words that should be included for sure, [...] how to add these to the tfidfvectorizer object?
you use the vocabulary parameter to specify what features should be used. for example, if you want only emoticons to be extracted, you can do the following:
emoticons = {"":)"":0, "":p"":1, "":("":2}
vect = tfidfvectorizer(vocabulary=emoticons)
matrix = vect.fit_transform(traindata)

this will return a <nx3 sparse matrix of type '<class 'numpy.int64'>' with m stored elements in compressed sparse row format>]. notice there are only 3 columns, one for each feature. 
if you want the vocabulary to include the emoticons as well as the n most common features, you could calculate the most frequent features first, then merge them with the emoticons and re-vectorize like so:
# calculate the most frequent features first
vect = tfidfvectorizer(vocabulary=emoticons, max_features=10)
matrix = vect.fit_transform(traindata)
top_features = vect.vocabulary_
n = len(top_features)

# insert the emoticons into the vocabulary of common features
emoticons = {"":)"":0, "":p"":1, "":("":2)}
for feature, index in emoticons.items():
    top_features[feature] = n + index

# re-vectorize using both sets of features
# at this point len(top_features) == 13
vect = tfidfvectorizer(vocabulary=top_features)
matrix = vect.fit_transform(traindata)",https://stackoverflow.com/questions/19753945,python,03-11-2013 14:19,8363.0,8.0,1.0,True,20-03-2024 08:54,06-08-2014 11:57
75116397,how to define pos_pattern for extracting nouns followed by zero or more sequence of nouns or adjectives for keyphrasecountvectorizer?,"i'm trying to extract arabic keywords from tweets. i'm using keybert with keyphrasecountvectorizer
vectorizer = keyphrasecountvectorizer(pos_pattern='< n.*>*')
i'm trying to write more custom pos patterns regexp to select nouns followed by zero or more sequence of nouns or adjectives but not verbs.
can you please help me to write the right regexp?
thank you","['nlp', 'part-of-speech', 'keyword-extraction']",75121673,"i interpret your requirement to match ""nouns followed by zero or more sequence of nouns or adjectives"" as matching at least one or more sequential nouns (i.e. <n.*>+), followed by zero or more adjectives (i.e. <j.*>*). so putting these together you get the full regexp as follows:
vectorizer = keyphrasecountvectorizer(pos_pattern=""<n.*>+<j.*>*"")

as a side point, you note that you are attempting to extract arabic keywords. from my understanding the keyphrase_vectorizers package relies on the text being annotated with spacy pos tags, and so to change languages from the default (english) you have to load a corresponding pipeline/model in the desired language and set the stop words to those of the new language. for example, if using the keyphrase vectorizer for german:
vectorizer = keyphrasecountvectorizer(spacy_pipeline='de_core_news_sm', stop_words='german')

however, at present spacy does not have a pipeline trained for arabic text, which means that using keyphrasecountvectorizer in a straightforward manner with arabic text is not possible without workarounds (something you may have already solved but i just thought i'd mention it).",https://stackoverflow.com/questions/75116397,nlp,14-01-2023 07:17,331.0,2.0,1.0,True,14-01-2023 22:26,14-01-2023 07:50
61852870,how to fix lda model coherence score runtime error?,"text='alice is a student.she likes studying.teachers are giving a lot of homewok.'
i am trying to get topics from a simple text(like above) with coherance score.this is my lda model:
id2word = corpora.dictionary(data_lemmatized)
texts = data_lemmatized
corpus = [id2word.doc2bow(text) for text in texts]

lda_model = gensim.models.ldamodel.ldamodel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=5, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=true)
# print the keyword in the 10 topics
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

when i try to run this coherance model:
coherence_model_lda = coherencemodel(model=lda_model, texts=data_lemmatized, dictionary=id2word, 
coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\ncoherence score: ', coherence_lda)

i am  supposed to get this king of output-> coherence score:  0.532947587081
i get this error:
raise runtimeerror('''
runtimeerror:
        an attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.
    this probably means that you are not using fork to start your
    child processes and you have forgotten to use the proper idiom
    in the main module:

        if __name__ == '__main__':
            freeze_support()
            ...

    the ""freeze_support()"" line can be omitted if the program
    is not going to be frozen to produce an executable.

what should i do to fix this?","['python', 'nlp', 'runtime-error', 'lda', 'topic-modeling']",61966747,"i have faced the same issue. adding 'coherence model' inside if__name__==""main"" resolved the issue for me.
if __name__ == ""__main__"":

     coherence_model_lda = coherencemodel(model=lda_model, texts=data_lemmatized, 
                                                          dictionary=id2word, 
                                                              coherence='c_v')
     coherence_lda = coherence_model_lda.get_coherence()
     print('\ncoherence score: ', coherence_lda)",https://stackoverflow.com/questions/61852870,python,17-05-2020 13:50,2078.0,3.0,3.0,True,25-04-2022 19:24,17-05-2020 16:45
66064503,in huggingface tokenizers: how can i split a sequence simply on spaces?,"i am using distilberttokenizer tokenizer from huggingface.
i would like to tokenize my text by simple splitting it on space:
[""don't"", ""you"", ""love"", ""ï¿½ï¿½ï¿½ï¿½"", ""transformers?"", ""we"", ""sure"", ""do.""]

instead of the default behavior, which is like this:
[""do"", ""n't"", ""you"", ""love"", ""ï¿½ï¿½ï¿½ï¿½"", ""transformers"", ""?"", ""we"", ""sure"", ""do"", "".""]

i read their documentatio"" rel=""nofollow noreferrer"">tokenization in general as well as about bert tokenizer specifically, but could not find an answer to this simple question :(
i assume that it should be a parameter when loading tokenizer, but i could not find it among the parameters list ...
edit:
minimal code example to reproduce:
from transformers import berttokenizer

tokenizer = berttokenizer.from_pretrained('distilbert-base-cased')

tokens = tokenizer.tokenize(""don't you love ï¿½ï¿½ï¿½ï¿½ transformers? we sure do."")
print(""tokens: "", tokens)
</code","['split', 'tokenize', 'huggingface-transformers', 'huggingface-tokenizers']",66074573,"that is not how it works. the transformers library provides different types of tokenizers. in the case of distilbert it is a wordpiece tokenizer that has a defined vocabulary that was used to train the corresponding model and therefore does not offer such modifications (as far as i know). something you can do is using the split() method of the python string:
text = ""don't you love ï¿½ï¿½ï¿½ï¿½ transformers? we sure do.""
tokens = text.split()
print(""tokens: "", tokens)

output:
tokens:  [""don't"", 'you', 'love', 'ï¿½ï¿½ï¿½ï¿½', 'transformers?', 'we', 'sure', 'do.']

in case you are looking for a bit more complex tokenization that also takes the punctuation into account, you can utilize the basic_tokenizer:""lang-py prettyprint-override"">from transformers import distilberttokenizer

tokenizer = distilberttokenizer.from_pretrained('distilbert-base-cased')
tokens = tokenizer.basic_tokenizer.tokenize(text)
print(""tokens: "", tokens)

output:
tokens:  ['don', ""'"", 't', 'you', 'love', 'ï¿½ï¿½ï¿½ï¿½', 'transformers', '?', 'we', 'sure', 'do', '.']
</code",https://stackoverflow.com/questions/66064503,split,05-02-2021 13:51,7216.0,2.0,2.0,True,10-11-2023 00:10,05-02-2021 14:42
59319207,ner - entity recognition - country filter,"i want to extract geo-relevant info from an excel file with spacy. it works to extract all entities, but i just need the geo-data and donï¿½ï¿½t find a way to filter the entities.
import pandas as pd
import spacy

sp = spacy.load(""en_core_web_sm"")
df = pd.read_excel(""test.xlsx"", usecols=[""bio"", ""author""])
df.head(1)
df=df.fillna('')
#df['bio']
doc = df.values.tolist()
#print (doc)
#sp(', '.join(doc[0])).ents
for entry in doc:
    #print('current entry\n {}'.format(entry))
    for entity in sp(', '.join(entry)).ents:
        print(entity.text, entity.label)

currently, the output looks like:
munich 384

germany 384

venezuela 384

london 384

portrait | 9191306739292312949

ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ 383

ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 394

visited:ï""location"" in the csv.

i would appreciate your help very much, with kind regards","['python', 'entity', 'spacy', 'named-entity-recognition']",59326494,"as mentioned, you can filter for the ""loc"" or ""gpe"" entity provided by the spacy language model. however, be aware that the ner language model needs to have a sentence contex to be able to predict the location entities.
sp = spacy.load(""en_core_web_sm"")
# loop over every row in the 'bio' column
for text in df['bio'].tolist():
    # use spacy to extract the entities
    doc = sp(text)
    for ent in doc.ents:    
        # check if entity is equal 'loc' or 'gpe'
        if ent.label_ in ['loc', 'gpe']:
            print(ent.text, ent.label_)   

here the link to the spacy ner documentation: 
edit
here is the full list of english spacy entity types from the documentation: 

person   people, including fictional.  norp  nationalities or religious or political groups.  
fac  buildings, airports, highways, bridges, etc.    
org  companies, agencies, institutions, etc.  
gpe  countries, cities, states.  
loc  non-gpe locations, mountain ranges, bodies of water.
product  objects, vehicles, foods, etc. (not services.)  
event    named hurricanes, battles, wars, sports events, etc.
work_of_art  titles of books, songs, etc. 
law  named documents made
into laws. 
language any named language.  
date absolute or relative dates or periods.
time times smaller than a day. 
percent  percentage, including ï¿½ï¿½ï¿½%ï¿½ï¿½ï¿½.
money    monetary values, including unit.  
quantity measuof weight or distance. 
ordinal  ï¿½ï¿½ï¿½firstï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½secondï¿½ï¿½ï¿½, etc.  
cardinal numerals that do not fall under another type.",https://stackoverflow.com/questions/59319207,python,13-12-2019 09:04,3896.0,1.0,3.0,True,15-07-2022 22:18,13-12-2019 19:21
78137158,can distilled whisper models be used as a drop-in replacement for openai whisper?,"i have a working video transcription pipeline working using a local openai whisper model. i would like to use the equivalent distilled model (""distil-small.en""), which is smaller and faster.
transcribe(self):
    file = ""/path/to/video""

    model = whisper.load_model(""small.en"")          # works
    model = whisper.load_model(""distil-small.en"")   # does not work 

    transcript = model.transcribe(word_timestamps=true, audio=file)
    print(transcript[""text""])

however, i get an error that the model was not found:
runtimeerror: model distil-small.en not found; available models = ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large-v1', 'large-v2', 'large-v3', 'large']

i installed my dependencies in poetry (which used pip under the hood) as follows:
[tool.poetry.dependencies]
python = ""^3.11""
openai-whisper = ""*""
transformers  = ""*"" # distilled whisper models
accelerate  = ""*"" # distilled whisper models
datasets = { version = ""*"", extras = [""audio""] } # distilled whisper models

the github distilled whisper documentation appears to use a different approach to installing and using these models.
is it possible to use a distilled model as a drop-in replacement for a regular whisper model?","['python', 'openai-api', 'openai-whisper']",78188618,"load_model with a string parameter will only work with openai's known list of models. if you want to use your own model, you will need to download it from the huggingface hub or elsewhere first.
see: 
import torch
from datasets import load_dataset
from huggingface_hub import hf_hub_download
from whisper import load_model, transcribe

distil_small_en = hf_hub_download(repo_id=""distil-whisper/distil-small.en"", filename=""original-model.bin"")
model = load_model(distil_small_en)

dataset = load_dataset(""hf-internal-testing/librispeech_asr_dummy"", ""clean"", split=""validation"")
sample = dataset[0][""audio""][""array""]
sample = torch.from_numpy(sample).float()

pred_out = transcribe(model, audio=sample)
print(pred_out[""text""])


you can also see where openai checks the string parameter of load_model that it only checks the known models (as described in the error you showed)",https://stackoverflow.com/questions/78137158,python,10-03-2024 19:07,314.0,-1.0,1.0,True,19-03-2024 17:34,10-03-2024 19:29
31668493,get indices of original text from nltk word_tokenize,"i am tokenizing a text using nltk.word_tokenize and i would like to also get the index in the original raw text to the first character of every token, i.e.
import nltk
x = 'hello world'
tokens = nltk.word_tokenize(x)
>>> ['hello', 'world']

how can i also get the array [0, 7] corresponding to the raw indices of the tokens?","['python', 'text', 'nltk', 'tokenize']",31672954,"update: this is now supported in the default tokenizer, based on the following stackoverflow answer

since nltk 3.4 treebankwordtokenizer supports span_tokenize


i think you are looking for is the span_tokenize() method.
apparently this is not supported by the default tokenizer.
here is a code example with another tokenizer.
from nltk.tokenize import whitespacetokenizer
s = ""good muffins cost $3.88\nin new york.""
span_generator = whitespacetokenizer().span_tokenize(s)
spans = [span for span in span_generator]
print(spans)

which gives:
[(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36)]

just getting the offsets:
offsets = [span[0] for span in spans]
[0, 5, 13, 18, 24, 27, 31]

for further information (on the different tokenizers available) see the tokenize api docs",https://stackoverflow.com/questions/31668493,python,28-07-2015 06:05,9804.0,11.0,3.0,True,27-01-2025 10:18,28-07-2015 07:34
78670044,concurrent api call to openai is returning empty response,"i need to process a batch of video items, format them, and call an asynchronous api to get additional data in a yaml format. however, i'm encountering an issue where the api responses are mostly empty, despite the tasks completing successfully. below is the relevant code.
var fetchtasks = videos.select(async item =>
{
    try
    {
        var formatteditem = new videoitem
        {
            title = item.title,
            subtitle = _extractservice.getcleansubtitle(item.subtitle ?? """"),
            description = _extractservice.getcleansubtitle(item.description ?? """"),
        };

        if (string.isnullorempty(formatteditem.subtitle) && string.isnullorempty(formatteditem.description))
        {
            _logger.logwarning($""video {item.id} has empty subtitle or description. marking as not processed."");
            item.notprocess = true;
            return new { item, processstatus = false, request = string.empty, response = (string)null };
        }
        else
        {
            var builder = new requestbuilder(formatteditem).withyamlformatpath(yamlpath);
            var yamlrequest = builder.build();

            _logger.loginformation($""built yaml request: {yamlrequest}"");

            var response = await _apiservice.callapiasync(yamlrequest);

            _logger.loginformation($""response for video {item.id}: {response}"");

            return new { item, processstatus = true, request = yamlrequest, response };
        }
    }
    catch (exception ex)
    {
        _logger.logerror($""error fetching details for video {item.id}: {ex.message}"");
        return null;
    }
});

var fetcheddetails = (await task.whenall(fetchtasks)).where(result => result != null);

callapiasync method
public async task<string> callapiasync(string response)
{
    try
    {
        var completionresult = await _openaiservice.chatcompletion.createcompletion(new chatcompletioncreaterequest
        {
            messages = new list<chatmessage>
            {
                chatmessage.fromsystem(""you are a helpful assistant.""),
                chatmessage.fromuser(response)
            },
            model = models.gpt_3_5_turbo,
        });
        if (completionresult.successful)
        {
            console.writeline(completionresult.choices.first().message.content);
            return completionresult.choices.first().message.content;
        }
        return """";
    }
    catch (exception ex)
    {
        // log or handle exception as neede
        console.writeline($""error calling chatgpt api: {ex.message}"");
        throw;
    }
}

after the task has been completed to process the response.
foreach (var item in fetcheddetails)
{
    var video = item.video;
    var request = item.request;
    var response = item.response;
     var dbresponse = new response
     {
         request = request,
         ytvideoid = item .id,
         response = response
     };
}

here while saving the response to the database i am often getting the item.response as empty string ("""").

when i put breakpoint and debug in can see the empty string """".
it seems like the call to api has been started but not awaited properly within the batch. the tasks appear to complete without waiting for the response from callapiasync.
does it might be the issue that
var completionresult = await _openaiservice.chatcompletion.createcompletion(new chatcompletioncreaterequest

this code from  could be the reason of empty response?
how can i refactor this code to wait to the external api call to get the response correctly and process it in parallel?","['c#', 'asynchronous', '.net-core', 'openai-api', 'asp.net-core-8']",78672367,"the reason for the empty value on response was caused by open ai itself.
i was getting the following error while making the api call, which i hadn't noticed at first.

rate limit reached for gpt-3.5-turbo in organization on tokens per min
(tpm): limit 60000, used 57140, requested 4647. please try again in
1.787s. visit platform.openai.com/account/rate-limits to learn more.

which means technically it won't allow concurrent operation.",https://stackoverflow.com/questions/78670044,c#,26-06-2024 01:40,539.0,5.0,1.0,True,26-06-2024 12:18,26-06-2024 12:15
56927602,unable to load the spacy model &#39;en_core_web_lg&#39; on google colab,"i am using spacy in google colab to build an ner model for which i have downloaded the spacy 'en_core_web_lg' model using
import spacy.cli
spacy.cli.download(""en_core_web_lg"")

and i get a message saying
ï¿½ï¿½ï¿½ download and installation successful
you can now load the model via spacy.load('en_core_web_lg')

however then when i try to load the model
nlp = spacy.load('en_core_web_lg')

the following error is printed:
oserror: [e050] can't find model 'en_core_web_lg'. it doesn't seem to be a shortcut link, a python package or a valid path to a data directory.

could anyone help me with this problem","['python', 'nlp', 'google-colaboratory', 'spacy']",56949134,"running
import spacy.cli
spacy.cli.download(""en_core_web_lg"")
nlp = spacy.load(""en_core_web_lg"")

shouldn't yield any errors anymore with recent spacy versions.
if running the code still gives errors, you should be all set with running in one cell (takes a while, but gives you visual feedback about progress, differently from spacy.cli)
!python -m spacy download en_core_web_lg

then, *** restart the colab runtime *** via

the colab menu runtime > restart runtime, or
use the keyboard shortcut ctrl+m .

after that, executing
import spacy
nlp = spacy.load('en_core_web_lg')

should work flawlessly.",https://stackoverflow.com/questions/56927602,python,08-07-2019 02:08,37212.0,32.0,4.0,True,20-08-2022 17:39,11-11-2020 12:28
76268588,"yield not returning result immediately to caller api - c#8, iasyncenumerable","i am using .net 6. my use case is to return stream data from ""myapi"" to a ""middle api(bff)"" to client app in react.
i have a code in ""myapi"" endpoint that should yield a result as soon as it receives it -
myapi code -

public async iasyncenumerable<string> getstreamingresponse()
        {            
            var rawazureopenairequest = new completionsrequest();
            rawazureopenairequest.modeltouse = defaulttextmodeltouse;
            completionsoptions optns = new completionsoptions();
            optns.prompts.add(""add 6+1 :"");
            optns.prompts.add(""below is the summary of technical consultant role in software"");
var azresponse = await _openairepository.getstreamingresponse(rawazureopenairequest.model, optns,
                cantoken);

            await foreach (var choice in azresponse.value.getchoicesstreaming())
            {
                await foreach (var message in choice.gettextstreaming())
                {
                    yield return message;
                    await task.delay(10000);
                }
            }
}

my consuming ""middle bff api"" is as below, it is not hitting the breakpoint in consuming api after each yield return which is my issue, ie, control does not return to consuming api after each yield return. i want as soon as a message is yielded returned from the first api above, the consuming api should receive it.
consuming api code -
[
[route(""v1/testendpoint"")]
        public async task get()
        {            
            using  client = new();
            using  response = await client.getasync(
                ""
                
            ).configureawait(false);

            response.ensuresuccessstatuscode();

           stream responsestream = await response.content.readasstreamasync().configureawait(false);

            iasyncenumerable<object> messages = jsonserializer.deserializeasyncenumerable<object>(responsestream,
            new jsonserializeroptions
            {
                propertynamecaseinsensitive = true,
                defaultbuffersize = 10
            });

            response.headers.add(""content-type"", ""text/event-stream"");

            await foreach (var message in messages)
            {
                debugger;
                byte[] messagebytes = asciiencoding.ascii.getbytes(""data:"" + message + ""\n\n"");
                await response.body.writeasync(messagebytes, 0, messagebytes.length);
                await response.body.flushasync();
            }
}

could someone please explain why is it happening?
i have tried to add a delay to check if the control is returning to consuming api after yielding a return, but it is not.
i also tried hitting the first api that yields with below client-side code and it yields in batches.
fetch("" config)
      .then(async response => {
        const reader = response.body?.getreader();
        if (!reader) {
          return;
        }
        const decoder = new textdecoder();
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;
          var item = decoder.decode(value).replace(/\[|]/g, '').replace(/^,/, '');

          var parseditem = json.parse(item);
          console.log(item + ""\n"");
          debugger;

        }
        reader.releaselock();
      }, (reason) => {
        console.log(reason);
        debugger;
      });

in the first sending api, the gettextstreaming method has the following definition -

update:
trying to return stream directly now -
myapi code
public async task<stream> getrawstreamingcompletionresponse()
            {            
                var rawazureopenairequest = new completionsrequest();
                rawazureopenairequest.modeltouse = defaulttextmodeltouse;
                completionsoptions optns = new completionsoptions();
                optns.prompts.add(""add 6+1 :"");
                optns.prompts.add(""below is the summary of technical consultant role in software"");
    
                var azresponse = await _openairepository
                    .getstreamingresponse(rawazureopenairequest.modeltouse, optns,
                    cantoken);
    
                return azresponse.getrawresponse().contentstream;
            }

in consuming api -
public async task get() {
                var stream = await client.getstreamasync(""
                response.headers.add(""content-type"", ""text/event-stream"");
                stream.copytoasync(this.response.body);
                await response.body.flushasync();            
}","['c#', 'asynchronous', 'azure-cognitive-services', 'openai-api', 'azure-openai']",76279209,"i think i found some reasoning behind it. it depends on when the system.text.json library flushes the reponse body when serializing an iasyncenumerable. it does not flush the response body after every step of async enumeration, rather it flushes at a buffer size limit that's internal to the json serializer.
on their page the mentioned default buffer size, in bytes, is 16,384.
i handled it by flushing response after every yield.",https://stackoverflow.com/questions/76268588,c#,17-05-2023 04:36,871.0,1.0,1.0,True,26-06-2023 12:13,19-06-2023 06:31
63666456,spacy add custom component with rewrite doc.text,"i'm trying to create a custom component on spacy's pipeline. i want to transform my text to lower.
my code :
nlp = spacy.load('en_core_web_sm')
def lower_component(doc):
    doc.text = doc.text.lower
    return doc

nlp.add_pipe(lower_component, first=true)
print('pipeline:', nlp.pipe_names)

doc = nlp(""hello world!"")
doc

i have an

attributeerror : attribute 'text' of 'spacy.tokens.doc.doc' objects is not writable

do you have a solution for my problem?","['python', 'spacy']",63690915,"i found ! just pass a class :
class lower(object):
    name = ""lower""

    nlp: language

    def __init__(self, nlp: language):
        self.nlp = nlp

    def __call__(self, doc: doc) -> doc:
        text = doc.text
        return self.nlp.make_doc(text.lower())

and following :
nlp.add_pipe(lower(nlp), first=true)",https://stackoverflow.com/questions/63666456,python,31-08-2020 07:22,450.0,1.0,3.0,True,16-04-2024 05:47,31-08-2020 08:01
72036646,converting h5 to tflite,"i have been trying to get this zero-shot text classification joeddav / xlm-roberta-large-xnli to convert from h5 to tflite file ( but this error pops up and i cant find it described online, how is it fixed? if it can't, is there another zero-shot text classifier i can use that would produce similar accuracy even after becoming tflite?
attributeerror: 't5forconditionalgeneration' object has no attribute 'call'

i have been trying a few different tutorials and the current google colab file i have is an amalgam of a couple of them.","['tensorflow', 'tensorflow-lite', 'huggingface-transformers', 'text-classification', 'huggingface']",72039808,"[ convert tflite from saved .h5 model to tflite model ]
conversion using tflite convert there are multiple ways by

tf-lite convertor tf-lite convertor
tf.lite.tfliteconverter or else

from the provided links currently they try to convert from saved model .h5 to tflite, to confirm their question.
[ sample ]:
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
: model initialize
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
model = tf.keras.models.sequential([
    tf.keras.layers.inputlayer(input_shape=( 32, 32, 3 )),
    tf.keras.layers.dense(128, activation='relu'),
])
model.compile(optimizer='sgd', loss='mean_squared_error') # compile the model
model.summary()

model.save_weights(checkpoint_path)

""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
: filewriter
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
if exists(checkpoint_path) :
    model.load_weights(checkpoint_path)
    print(""model load: "" + checkpoint_path)


tf_lite_model_converter = tf.lite.tfliteconverter.from_keras_model(
    model
) # <tensorflow.lite.python.lite.tflitekerasmodelconverterv2 object at 0x0000021095194e80>
tflite_model = tf_lite_model_converter.convert()

# save the model.
with open(checkpoint_dir + '\\model.tflite', 'wb') as f:
    f.write(tflite_model)",https://stackoverflow.com/questions/72036646,tensorflow,28-04-2022 00:27,784.0,-1.0,1.0,True,21-12-2022 17:16,05-05-2022 12:45
72396844,write generator function for lstm text generation model,"i have a lstm model for text generation but when trying to increase the amount of data to input, i run into ram issues so i found out that i can use fit_generator function to load the data in step by step.
the problem is currently that keras.utils.to_categorical takes to much space when the amount of unique words increases.
so i want to convert this code block into a generator function:
x_values, labels = input_seqs[:, :-1], input_seqs[:, -1]
y_values = tf.keras.utils.to_categorical(labels, num_classes=total_unique_words)

#shape of x_values: (152250, 261)
#shape of y_values: (152250, 4399)

and i got something like this but i'm not sure how to assign the right values to batch_x and batch_y
def generator(input_seq, batch_size):

    index = 0 
    while true:
      batch_x = np.zeros((batch_size, max_seq_length-1))
      batch_y = np.zeros((batch_size, total_unique_words))
      for i in range(batch_size):
        batch_x[i] = input_seqs[:, :-1][i]
        batch_y[i] = tf.keras.utils.to_categorical(input_seqs[:, -1][i], num_classes=total_unique_words)
        index = index + 1
        if index == len(input_seq):
          index = 0

      yield batch_x, batch_y



full code for better overview:
tokenizer = tokenizer()
tokenizer.fit_on_texts(review_list)
word_index = tokenizer.word_index
total_unique_words = len(tokenizer.word_index) + 1 

input_sequences = []
for line in review_list:
  token_list = tokenizer.texts_to_sequences([line])[0]
  for i in range(1, len(token_list)):
    n_gram_seqs = token_list[:i+1]
    input_sequences.append(n_gram_seqs)

max_seq_length = max([len(x) for x in input_sequences])
input_seqs = np.array(pad_sequences(input_sequences, maxlen=max_seq_length, padding='pre'))

x_values, labels = input_seqs[:, :-1], input_seqs[:, -1]
y_values = tf.keras.utils.to_categorical(labels, num_classes=total_unique_words)

callback = tf.keras.callbacks.earlystopping(monitor='loss', patience=3)
k.clear_session()
model = tf.keras.sequential([
tf.keras.layers.embedding(input_dim = total_unique_words, output_dim=100, input_length=max_seq_length-1),
tf.keras.layers.lstm(256, return_sequences=true), 
tf.keras.layers.dropout(0.2), 
tf.keras.layers.lstm(256), 
tf.keras.layers.dropout(0.2),
tf.keras.layers.dense(128, activation='relu'),
tf.keras.layers.dense(total_unique_words , activation='softmax')])
model.compile(optimizer=adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])","['python', 'tensorflow', 'keras', 'deep-learning', 'nlp']",72397781,"you could try something like this:
def generator(input_seq, batch_size, dataset_size):

    no_batches = int(dataset_size/batch_size)
    batch_x = np.zeros((batch_size, max_seq_length-1))
    batch_y = np.zeros((batch_size, total_unique_words))

    for i in range(no_batches):
        batch_x = input_seqs[:, :-1][(i*batch_size) : ((i+1)*batch_size)]
        batch_y = tf.keras.utils.to_categorical(input_seqs[:, -1][(i*batch_size) : ((i+1)*batch_size)], num_classes=total_unique_words)

        yield batch_x, batch_y

    return

i added the dataset_size(152250 in your case) argument so that the number of batches could be calculated.",https://stackoverflow.com/questions/72396844,python,26-05-2022 19:08,155.0,0.0,1.0,True,26-05-2022 20:37,26-05-2022 19:39
76661527,openai function calling error ---- openai.error.invalidrequesterror: &lt;exception str() failed&gt;,"i am creating a chatbot which can query all 'views' in my database based on user query.
i tried many other methods but didn't succeed so now i thought i should try openai's function calling.
what i did:
i created a function for one of the view. in that, i am calling gpt3 to create a sql query based on the user question that i provide in the parameter. i have given instructions and schema to the model so it can create correct query. below is the function.
def get_rent_details(user_query):
    """"""get the current weather in a given location""""""
    response = openai.chatcompletion.create(
        model=""gpt-3.5-turbo-0613"",
        prompt=""""""user will ask you the question regarding their properties, assets and finance. 

        follow below steps to get correct answer:

        1. understand the user question and prepare a syntactically correct sql query to retrieve the correct data.
        2. if you don't find the data in the table, just type ""no answer found"".
        3. do not make up any answer by your own.
        4. instead of '=', always use 'like' statement with 'where' statement.
        5. the user will mention either property name or tenant name. so to make sure the query is correct, use both columns 'tenantname' and 'propertyname' with 'where' statement. for example: select propertycode from viewrentroll where propertyname like 'younger, 3003' or tenantname like 'younger, 3003'.
        6. do not create any dml query like update, insert, delete, add.
        7. below is the table schema to run query on:

        create table [dbo].[viewrentroll] (            
       [propertypkid] [bigint]
      ,[propertycode] [nvarchar]
      ,[propertyname] [nvarchar]
      ,[propertylist] [nvarchar]
      ,[leasecode] [nvarchar]
      ,[tenantname] [nvarchar]
      ,[snp rating] [nvarchar]
      ,[unit number] [nvarchar]
      ,[lease status] [nvarchar]
      ,[lease start date] [datetime]
      ,[lease expiration date] [datetime]
      ,[unit square feet] [bigint]
      ,[remaining lease term] [bigint]
      ,[currently monthly base rent] [bigint]
      ,[rent psf] [bigint]
      ,[abr] [bigint]
      ,[local tenant] [nvarchar]
      ,[current annualized base rent psf] [bigint]
      ,[createdleaseexpirationdate] [datetime]
      ,[tenantcategory] [nvarchar]
  )
    """""" + user_query,
        max_tokens=200,
        temperature=0,
    )
    return (response['choices'][0]['text'])

i am thinking to create such functions for each view.
after this i got the code from openai function calling documentation and modified it as per my need. below is the 'function calling' function:
def run_conversation(user_query):
    # step 1: send the conversation and available functions to gpt

    print(""running run_conversion............\n\n"")

    messages = [{""role"": ""user"", ""content"": user_query}]
    functions = [
        {
            ""name"": ""get_rent_details"",
            ""description"": ""get the details of rent of tenants or properties"",
            ""parameters"": {
                ""type"": ""object"",
                ""user_query"" : {
                    ""type"" : ""string"",
                    ""description"" : ""user's question regarding the rent of tenant or properties""
                }
            }
        }
    ]
    response = openai.chatcompletion.create(
        model=""gpt-3.5-turbo-0613"",
        messages=messages,
        functions=functions,
        function_call=""auto"",  # auto is default, but we'll be explicit
    )
    response_message = response[""choices""][0][""message""]

    # step 2: check if gpt wanted to call a function
    if response_message.get(""function_call""):
        # step 3: call the function
        # note: the json response may not always be valid; be sure to handle errors
        available_functions = {
            ""get_rent_details"": get_rent_details,
        }  # only one function in this example, but you can have multiple
        function_name = response_message[""function_call""][""name""]
        fuction_to_call = available_functions[function_name]
        function_args = json.loads(response_message[""function_call""][""arguments""])
        function_response = fuction_to_call(
            user_query=function_args.get(""user_query""),
        )

        # step 4: send the info on the function call and function response to gpt
        messages.append(response_message)  # extend conversation with assistant's reply
        messages.append(
            {
                ""role"": ""function"",
                ""name"": function_name,
                ""content"": function_response,
            }
        )  # extend conversation with function response
        second_response = openai.chatcompletion.create(
            model=""gpt-3.5-turbo-0613"",
            messages=messages,
        )  # get a new response from gpt where it can see the function response
        return second_response

this is the first time i am trying function calling so i am not hundred percent sure if this will work.
when i run this code, i am getting this error: openai.error.invalidrequesterror: <exception str() failed>
for response = openai.chatcompletion.create() in run_conversation(user_query) function.
can anyone please guide me where i am making mistakes?
i am providing whole code below:
import openai
import json
import os

user_query = ""what is the monthly rent of good neighbor homes, inc.""

openai.api_key=os.environ['openai_api_key']

def run_conversation(user_query):
    # step 1: send the conversation and available functions to gpt

    print(""running run_conversion............\n\n"")

    messages = [{""role"": ""user"", ""content"": user_query}]
    functions = [
        {
            ""name"": ""get_rent_details"",
            ""description"": ""get the details of rent of tenants or properties"",
            ""parameters"": {
                ""type"": ""object"",
                ""user_query"" : {
                    ""type"" : ""string"",
                    ""description"" : ""user's question regarding the rent of tenant or properties""
                }
            }
        }
    ]
    response = openai.chatcompletion.create(
        model=""gpt-3.5-turbo-0613"",
        messages=messages,
        functions=functions,
        function_call=""auto"",  # auto is default, but we'll be explicit
    )
    response_message = response[""choices""][0][""message""]

    # step 2: check if gpt wanted to call a function
    if response_message.get(""function_call""):
        # step 3: call the function
        # note: the json response may not always be valid; be sure to handle errors
        available_functions = {
            ""get_rent_details"": get_rent_details,
        }
        function_name = response_message[""function_call""][""name""]
        fuction_to_call = available_functions[function_name]
        function_args = json.loads(response_message[""function_call""][""arguments""])
        function_response = fuction_to_call(
            user_query=function_args.get(""user_query""),
        )

        # step 4: send the info on the function call and function response to gpt
        messages.append(response_message)  # extend conversation with assistant's reply
        messages.append(
            {
                ""role"": ""function"",
                ""name"": function_name,
                ""content"": function_response,
            }
        )  # extend conversation with function response
        second_response = openai.chatcompletion.create(
            model=""gpt-3.5-turbo-0613"",
            messages=messages,
        )  # get a new response from gpt where it can see the function response
        return second_response


def get_rent_details(user_query):
    """"""get the current weather in a given location""""""
    response = openai.chatcompletion.create(
        model=""gpt-3.5-turbo-0613"",
        prompt=""""""user will ask you the question regarding their properties, assets and finance. 

        follow below steps to get correct answer:

        1. understand the user question and prepare a syntactically correct sql query to retrieve the correct data.
        2. if you don't find the data in the table, just type ""no answer found"".
        3. do not make up any answer by your own.
        4. instead of '=', always use 'like' statement with 'where' statement.
        5. the user will mention either property name or tenant name. so to make sure the query is correct, use both columns 'tenantname' and 'propertyname' with 'where' statement. for example: select propertycode from viewrentroll where propertyname like 'younger, 3003' or tenantname like 'younger, 3003'.
        6. do not create any dml query like update, insert, delete, add.
        7. below is the table schema to run query on:

        create table [dbo].[viewrentroll] (            
       [propertypkid] [bigint]
      ,[propertycode] [nvarchar]
      ,[propertyname] [nvarchar]
      ,[propertylist] [nvarchar]
      ,[leasecode] [nvarchar]
      ,[tenantname] [nvarchar]
      ,[snp rating] [nvarchar]
      ,[unit number] [nvarchar]
      ,[lease status] [nvarchar]
      ,[lease start date] [datetime]
      ,[lease expiration date] [datetime]
      ,[unit square feet] [bigint]
      ,[remaining lease term] [bigint]
      ,[currently monthly base rent] [bigint]
      ,[rent psf] [bigint]
      ,[abr] [bigint]
      ,[local tenant] [nvarchar]
      ,[current annualized base rent psf] [bigint]
      ,[createdleaseexpirationdate] [datetime]
      ,[tenantcategory] [nvarchar]
  )

""""""+user_query+""?"",
        max_tokens=200,
        temperature=0,
    )
    print(response['choices'][0]['text'])
    return (response['choices'][0]['text'])

run_conversation(user_query)","['artificial-intelligence', 'chatbot', 'openai-api', 'chatgpt-api', 'gpt-3']",76865710,"try modifying the function to something like this:
{
    ""name"": ""get_rent_details"",
    ""description"": ""get the details of rent of tenants or properties"",
    ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
            ""user_query"": {
                ""type"": ""string"",
                ""description"": ""user's question regarding the rent of tenant or properties""
            }
        },
        ""required"": [""user_query""]
    }
}

i.e. add properties and required.
i was facing a similar problem and what worked for me was to start with an official open ai function that i verified to work, and then i modified it line by line to verify that no change breaks it. i had missed one of the required attributes.
i built myself also a validation function to check that i pass only valid functions to open ai in the future. it's not perfect but has already helped me catch a few bugs.
def validate_function(function):
    # example func
    """"""
    function = {
        ""name"": ""get_current_weather"",
        ""description"": ""get the current weather in a given location"",
        ""parameters"": {
            ""type"": ""object"",
            ""properties"": {
                ""location"": {
                    ""type"": ""string"",
                    ""description"": ""the city and state, e.g. san francisco, ca"",
                },
                ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
            },
            ""required"": [""location""],
        },
    }
    """"""

    # check the presence of required keys and their types
    assert ""name"" in function and isinstance(
        function[""name""], str
    ), ""'name' must be a string.""
    assert ""description"" in function and isinstance(
        function[""description""], str
    ), ""'description' must be a string.""
    assert ""parameters"" in function and isinstance(
        function[""parameters""], dict
    ), ""'parameters' must be a dictionary.""

    # check the structure of 'parameters' key
    params = function[""parameters""]

    assert (
        ""type"" in params and params[""type""] == ""object""
    ), ""'type' must be 'object' in parameters.""
    assert ""properties"" in params and isinstance(
        params[""properties""], dict
    ), ""'properties' must be a dictionary.""
    assert ""required"" in params and isinstance(
        params[""required""], list
    ), ""'required' must be a list.""

    # check the structure of 'properties' in 'parameters'
    for key, prop in params[""properties""].items():
        assert ""type"" in prop and isinstance(
            prop[""type""], str
        ), f""'type' must be a string in properties of {key}.""

        if prop[""type""] == ""array"":
            assert (
                ""items"" in prop
            ), f""'items' must be present in properties of {key} when type is 'array'.""

        # enum check only if it exists
        if ""enum"" in prop:
            assert isinstance(
                prop[""enum""], list
            ), f""'enum' must be a list in properties of {key}.""

    # check 'required' properties are in 'properties'
    for key in params[""required""]:
        assert (
            key in params[""properties""]
        ), f""'{key}' mentioned in 'required' must exist in 'properties'.""",https://stackoverflow.com/questions/76661527,artificial-intelligence,11-07-2023 11:30,752.0,2.0,1.0,True,09-08-2023 07:58,12-07-2023 02:05
21128689,how to get pmi scores for trigrams with nltk collocations? python,"i know how to get bigram and trigram collocations using nltk and i apply them to my own corpora. the code is below. 
my only problem is how to print out the birgram with the pmi value? i search nltk documentation multiple times. it's either i'm missing something or it's not there. 
import nltk
from nltk.collocations import *

myfile = open(""large.txt"", 'r').read()
mylist = myfile.split()
mycorpus = nltk.text(mylist)
trigram_measures = nltk.collocations.trigramassocmeasures()
finder = trigramcollocationfinder.from_words((mycorpus))

finder.apply_freq_filter(3)
print finder.nbest(trigram_measures.pmi, 500000)","['python', 'nlp', 'nltk', 'collocation']",21130857,"if you take a look at the source code for nlkt.collocations.trigramcollocationfinder (see  you'll find that it returns a trigramcollocationfinder().score_ngrams:
def nbest(self, score_fn, n):
    """"""returns the top n ngrams when scored by the given function.""""""
    return [p for p,s in self.score_ngrams(score_fn)[:n]]

so you could call the score_ngrams() directly without getting the nbest since it returns a sorted list anyways.:
def score_ngrams(self, score_fn):
    """"""returns a sequence of (ngram, score) pairs ordered from highest to
    lowest score, as determined by the scoring function provided.
    """"""
    return sorted(self._score_ngrams(score_fn),
                  key=_itemgetter(1), reverse=true)

try:
import nltk
from nltk.collocations import *
from nltk.tokenize import word_tokenize

text = ""this is a foo bar bar black sheep  foo bar bar black sheep foo bar bar black sheep shep bar bar black sentence""

trigram_measures = nltk.collocations.trigramassocmeasures()
finder = trigramcollocationfinder.from_words(word_tokenize(text))

for i in finder.score_ngrams(trigram_measures.pmi):
    print i

[out]:
(('this', 'is', 'a'), 9.047123912114026)
(('is', 'a', 'foo'), 7.46216141139287)
(('black', 'sheep', 'shep'), 5.46216141139287)
(('black', 'sheep', 'foo'), 4.877198910671714)
(('a', 'foo', 'bar'), 4.462161411392869)
(('sheep', 'shep', 'bar'), 4.462161411392869)
(('bar', 'black', 'sheep'), 4.047123912114026)
(('bar', 'black', 'sentence'), 4.047123912114026)
(('sheep', 'foo', 'bar'), 3.877198910671714)
(('bar', 'bar', 'black'), 3.047123912114026)
(('foo', 'bar', 'bar'), 3.047123912114026)
(('shep', 'bar', 'bar'), 3.047123912114026)",https://stackoverflow.com/questions/21128689,python,15-01-2014 03:38,10065.0,3.0,3.0,True,20-08-2022 14:38,16-01-2014 15:16
65772335,finding words within paragraph using python,"let say i have the following words, test_wrds = ['she', 'her','women'] that i would like to see whether any one of them present in the following str paragraph,
text= ""what recent discussions she has had with the secretary of state for work and pensions on the effect of that departmentï¿½ï¿½ï¿½s welfare policies on women.""

the question is,  how to find these test_wrds in text and bold them in different colours as well as count them how many times test_wrds appeared in para. so i am expecting output something like this,
text= "" what recent discussions **she** has had with the secretary of state for work and pensions on the effect of that departmentï¿½ï¿½ï¿½s welfare policies on **women**.

so far, i have written the following codes:
text="" q: what recent discussions she has had with the secretary of state for work and pensions on the et departmentï¿½ï¿½ï¿½s welfare policies on women.""
test_wrds = ['she', 'her','women']

import spacy 
nlp = spacy.load(""en_core_web_sm"") 
doc = nlp(text)
# word split
wrd_token=[token.orth_ for token in doc]

i am not getting an idea on how to proceed further after this.  i used spacy as i found to be powerful and easy for my future coding.
thanks in advance","['python', 'nlp', 'spacy']",65773547,"first of all in order to count how many times each word from test_wrds list exists in text you can use orth which is an id of the verbatim text content (see here).
import spacy
from spacy.lang.en import english
from spacy.attrs import orth

text="" q: what recent discussions she has had with the secretary of state for work and pensions on the effect of that departmentï¿½ï¿½ï¿½s welfare policies on women.""
test_wrds = ['she', 'her','women']

nlp = english()

doc = nlp(text)

# dictionairy with keys each word's id representation and values the number of times this word appears in your text string
count_number = doc.count_by(orth)

for wid, number in sorted(count_number.items(), key=lambda x: x[1]):
    # nlp.vocap.strings[wid] gives the word corresponding to id
    if nlp.vocab.strings[wid] in test_wrds:
        print(number, nlp.vocab.strings[wid])

output:
1 she
1 women

second, in order to replace each word with bold you can try
import re

# avoid words followed by '.' without empty space
text = text.replace('.', ' .')

lista = text.split()

for word in test_wrds:
    if w lista:
        indices = [i for i,j in enumerate(lista) if j==word] # find list indices
        for index in indices:
            lista[index] = re.sub(lista[index], '**'+word+'**', lista[index])
            
new_text = ' '.join(lista)

output :
>>> new_text
'q: what recent discussions **she** has had with the secretary of state for work and pensions on the effect of that departmentï¿½ï¿½ï¿½s welfare policies on **women** .'
<",https://stackoverflow.com/questions/65772335,python,18-01-2021 10:01,254.0,0.0,1.0,True,18-01-2021 13:26,18-01-2021 10:25
66528474,java stdin and stdout ii why is it necessary to give scan.nextline(),"problem statement
in this challenge, you must read an integer, a double, and a string from stdin, then print the values according to the instructions in the output format section below. to make the problem a little easier, a portion of the code is provided for you in the editor.
note: we recommend completing java stdin and stdout i before attempting this challenge.
input format
there are three lines of input:

the first line contains an integer.
the second line contains a double.
the third line contains a string.

output format
there are three lines of output:

on the first line, print string: followed by the unaltered string read from stdin.
on the second line, print double: followed by the unaltered double read from stdin.
on the third line, print int: followed by the unaltered integer read from stdin.

to make the problem easier, a portion of the code is already provided in the editor.
note: if you use the nextline() method immediately following the nextint() method, recall that nextint() reads integer tokens; because of this, the last newline character for that line of integer input is still queued in the input buffer and the next nextline() will be reading the remainder of the integer line (which is empty).
sample input
42
3.1415
welcome to hackerrank's java tutorials!

sample output
string: welcome to hackerrank's java tutorials!
double: 3.1415
int: 42


import java.util.scanner;

public class solution {

    public static void main(string[] args) {
        scanner scan = new scanner(system.in);
        int i = scan.nextint();
        double d = scan.nextdouble();
        //scan.nextline();
        string s = scan.nextline();

        // write your code here.

        system.out.println(""string: "" + s);
        system.out.println(""double: "" + d);
        system.out.println(""int: "" + i);
    }
}

in the above code if i comment scan.nextline() i couldn't read the string input in the next line. why is it necessary to give scan.nextline() before the actual placeholder of string s?
this is the output i got.
string: 
double: 3.1415
int: 42","['java', 'input', 'output', 'java.util.scanner', 'nlp-question-answering']",66528755,"because there is a newline character after pressing enter which is not consumed by nextint() and it has to be consumed by scan.nextline().
why is nextline() returning an empty string?",https://stackoverflow.com/questions/66528474,java,08-03-2021 10:53,8730.0,1.0,1.0,True,08-03-2021 11:16,08-03-2021 11:16
52120580,n-gram vectorization using tfidfvectorizer,"i am using tfidfvectorizer 
with following parameters: 
smooth_idf=false, sublinear_tf=false, norm=none, analyzer='word', ngram_range=(1,2)

i am vectorizing following text: ""red sun, pink candy. green flower.""
here is output of get_feature_names():
['candy', 'candy green', 'coffee', 'flower', 'green', 'green flower', 'hate', 'icecream', 'like', 'moon', 'pink', 'pink candy', 'red', 'red sun', 'sun', 'sun pink']

since ""candy"" and ""green"" are part of the separate sentences, why is ""candy green"" n-gram created?
is there a way to prevent creation of n-grams spawning multiple sentences?","['scikit-learn', 'tf-idf']",52120828,"depends on how you are passing that to tfidfvectorizer!
if passed as a single document, tfidfvectorizer will only keep words which contain 2 or more alphanumeric characters. punctuation is completely ignored and always treated as a token separator. so your sentence becomes:
['red', 'sun', 'pink', 'candy', 'green', 'flower'] 

now from these tokens, ngrams are generated.
since tfidfvectorizer is a bag-of-words technique, working on words appearing in a document, it does not keep any information about the structure or order of words in a single document.
if you want them to be treated separately, then you should detect the sentences yourself and pass them as different documents.
or else, pass your own analyzer and ngram generator to the tfidfvectorizer.
for more information on how tfidfvectorizer actually works, see my other answer:

sklearn tfidfvectorizer : generate custom ngrams by not removing stopword in them",https://stackoverflow.com/questions/52120580,scikit-learn,31-08-2018 17:54,7706.0,3.0,1.0,True,06-04-2022 14:46,31-08-2018 18:02
72340801,huggingface load_dataset() function throws &quot;valueerror: couldn&#39;t cast&quot;,"my goal is to train a classifier able to do sentiment analysis in slovak language using loaded slovakbert model and huggingface library. code is executed on google colaboratory.
my test dataset is read from this csv file:

and train dataset:

data has two columns: column of slovak sentences and 2nd column of labels which indicate sentiment of the sentence. labels have values -1, 0 or 1.
load_dataset() function throws this error:

valueerror: couldn't cast
vrtuï¿½ï¿½nï¿½ï¿½k je veï¿½ï¿½mi zraniteï¿½ï¿½nï¿½ï¿½ pri dobre mierenej streï¿½ï¿½be zo zeme. brï¿½ï¿½niï¿½ï¿½ sa, unikaï¿½ï¿½, alebo vedieï¿½ï¿½ zneï¿½ï¿½kodniï¿½ï¿½ nepriateï¿½ï¿½a je vecou sekï¿½ï¿½nd, ak nie stotï¿½ï¿½n, kedy ide ï¿½ï¿½ivot. : string
-1: int64
-- schema metadata --
pandas: '{""index_columns"": [{""kind"": ""range"", ""name"": null, ""start"": 0, ""' + 954
to
{'priestorovo a vybavenim ok.': value(dtype='string', id=none), '1': value(dtype='int64', id=none)}
because column names don't match

code:
!pip install transformers==4.10.0 -qqq
!pip instalts import load_metric, load_dataset, dataset
from transformers import trainingarguments, trainer, automodelforsequenceclassification, autotokenizer, datacollatorwithpadding
import pandas as pd
from textblob import textblob
from textblob.sentiments import naivebayesanalyzer

#links to dataset
test = '
train = '


model_name = 'gerulata/slovakbert'


#load data
dataset = load_dataset('csv', data_files={'train': train, 'test': test})

what is done wrong while loading the dataset?","['machine-learning', 'nlp', 'sentiment-analysis', 'huggingface-tokenizers', 'huggingface']",72342130,"the reason is since delimiter is used in first column multiple times the code fails to automatically determine number of columns ( some time segment a sentence into multiple columns as it cannot automatically determine , is a delimiter or a part of sentence.
but, the solution is simple: (just add column names)
dataset = load_dataset('csv', data_files={'train': train,'test':test},column_names=['sentence','label'])

output:
datasetdict({
    train: dataset({
        features: ['sentence', 'label'],
        num_rows: 89
    })
    test: dataset({
        features: ['sentence', 'label'],
        num_rows: 91
    })
})",https://stackoverflow.com/questions/72340801,machine-learning,22-05-2022 19:43,9850.0,2.0,1.0,True,23-05-2022 08:04,23-05-2022 08:04
71038063,use a dictionary to find/replace exact terms in a tokenized pandas series,"using a dictionary, i need to find and replace terms in a pandas series according to the following criteria:

dictionary values are replacing dictionary keys wherever they are found in the pandas series (e.g., for example, in 'mastersphd': 'masters phd', the replacement outcome would be 'masters phd' wherever 'mastersphd' occurs)
maintain record integrity (i.e., can't use a bag of words approach because i need the unique records to remain intact.)
only exact matches should be replaced (for example, if key:value is 'rf': 'random forest', the replacement should not turn 'performance' into 'perandom forestormance'); so regex=true is causing this, obviously)

the data: term_fixes is the dictionary, and df['job_description'] is the tokenized series of interest
term_fixes = {'rf': 'random forest',
              'mastersphd': 'masters phd',
              'curiosity': 'curious',
              'trustworthy': 'ethical',
              'realise': 'realize'}


df = pd.dataframe(data={'job_description': [['knowledge', 'of', 'algorithm', 'like', 'rf'],
                                            ['must', 'have', 'a', 'mastersphd'],
                                            ['trustworthy', 'and', 'possess', 'curiosity'],
                                            ['we', 'realise', 'performance', 'is', 'key']]})

**note: i have also (unsuccessfully) tried an untokenized data structure, but prefer tokenized since i have more nlp to do
df = pd.dataframe(data={'job_description': ['knowledge of algorithm like rf',
                                            'must have a mastersphd',
                                            'must be trustworthy and possess curiosity',
                                            'we realise performance is critical']})

**desired outcome (note that the 'rf' in performance is not replaced by 'random forest'):
df['job_description']
0    ['knowledge' 'of' 'algorithm' 'like' 'random' 'forest']
1                        ['must' 'have' 'a' 'masters' 'phd']
2          ['must' 'be' 'ethical' 'and' 'possess' 'curious']
3             ['we' 'realize' 'performance' 'is' 'critical']

i have tried a number of approaches.
fail: df['job_description'].replace(list(term_fixes.keys()), list(term_fixes.values()), regex=false, inplace=true)
fail: df['job_description'].replace(dict(zip(list(term_fixes.keys()), list(term_fixes.values()))), regex=false, inplace=true)
fail: df['job_description'] = df['job_description'].str.replace(term_fixes, regex=false)
fail: df['job_description'] = df['job_description'].str.replace(str(term_fixes.keys()), str(term_fixes.values()), regex=true)
the closest i have come is:
df['job_description'] = df_jobs['job_description'].replace(term_fixes, regex=true)

however, the regex=true flags on any match (like the 'rf' and 'performance' example above). changing the flag to regex=false fails to replace anything, unfortunately. i looked in the documentation for another argument i could use, but no luck. note that this uses the untokenized structure.
any help would be much appreciated. thanks!","['python', 'pandas', 'dataframe', 'nlp', 'nltk']",71041923,"using the ""tokenized"" version of your df.
df['job_description'] = df['job_description'].explode().replace(term_fixes).groupby(level=-1).agg(list)

# explode to get single terms per ""cell""
# replace to replace the terms in ""term_fixes""
# groupby to reverse the previous explode and return to a column of lists

                                   job_description
0  [knowledge, of, algorithm, like, random forest]
1                     [must, have, a, masters phd]
2                 [ethical, and, possess, curious]
3              [we, realize, performance, is, key]

if you need the new terms also split on white space, then you can add another intermediate step .str.split().explode() before the final groupby
df['job_description'] = df['job_description'].explode().replace(term_fixes).str.split().explode().groupby(level=-1).agg(list)

                                    job_description
0  [knowledge, of, algorithm, like, random, forest] # random forest is now split
1                     [must, have, a, masters, phd] # masters phd is now split
2                  [ethical, and, possess, curious]
3               [we, realize, performance, is, key]",https://stackoverflow.com/questions/71038063,python,08-02-2022 17:18,500.0,1.0,2.0,True,08-02-2022 23:00,08-02-2022 20:30
20773200,python nltk naive bayes probabilities,"is there a way to get at the individual probabilities using nltk.naivebayesclassifier.classify?  i want to see the probabilities of classification to try and make a confidence scale. obviously with a binary classifier the decision is going to be one or the other, but is there some way to see the inner workings of how the decision was made? or, do i just have to write my own classifier?
thanks","['python', 'text', 'classification', 'nltk']",20773513,"how about nltk.naivebayesclassifier.prob_classify?

classify calls this function:
def classify(self, featureset):
    return self.prob_classify(featureset).max()

edit: something like this should work (not tested):
dist = classifier.prob_classify(features)
for label in dist.samples():
    print(""%s: %f"" % (label, dist.prob(label)))",https://stackoverflow.com/questions/20773200,python,25-12-2013 13:17,6345.0,11.0,2.0,True,27-10-2021 22:29,25-12-2013 13:40
73655387,spacy example object format for spancategorizer,"i am having an issue with spancategorizer that i believe is due to my example object format and possible its initialization.
can someone provide a very simple example object with the correct format? just an example with two docs and two labels will make it for me.
i am not getting how the prediction and the reference should look like. there is a gold standard mentioned in spacy documentation, but it looks out-of-date because the line reference = parse_gold_doc(my_data) doesn't work. thanks so much for your help!
here is the code i am using to annotate the docs:
``` phrase_matches = phrase_matcher(doc)
    
    # initializing spangroups
    for label in labels:
        doc.spans[label]=[]
    
    # phrase_matches detection and labeling of spans, and generation of spangrups for each doc
    for match_id, start, end in phrase_matches:
            match_label = nlp.vocab.strings[match_id]
            span        = doc[start:end]
            span        = span(doc, start, end, label = match_label)
            
            # set up of the spangroup for each doc, for the different labels
            doc.spans[match_label].append(span) ```

however spacy is not recognizing my labels.","['initialization', 'spacy']",73693942,"if you want/need to create example objects directly, the easiest way to do so is to use the function example.from_dict, which takes a predicted doc and a dict. predicted in this context is a doc with partial annotations, representing data from previous components. for many use-cases, it can just be a ""clean"" doc created with nlp.make_doc(text):
from spacy.training import example
from spacy.lang.en import english

nlp = english()
text = ""i like london and berlin""
span_dict = {""spans"": {""my_spans"": [(7, 13, ""loc""), (18, 24, ""loc""), (7, 24, ""double_loc"")]}}
predicted = nlp.make_doc(text)
eg = example.from_dict(predicted, span_dict)

what this function does, is taking the annotations from the dict and using those to define the gold-standard that is now stored in the example object eg.
if you print this object (using spacy >= 3.4.2), you'll see the internal representation of those gold-standard annotations:

{'doc_annotation': {'cats': {}, 'entities': ['o', 'o', 'o', 'o', 'o'], 'spans': {'my_spans': [(7, 13, 'loc', ''), (18, 24, 'loc', ''), (7, 24, 'double_loc', '')]}, 'links': {}}, 'token_annotation': {'orth': ['i', 'like', 'london', 'and', 'berlin'], 'spacy': [true, true, true, true, false], 'tag': ['', '', '', '', ''], 'lemma': ['', '', '', '', ''], 'pos': ['', '', '', '', ''], 'morph': ['', '', '', '', ''], 'head': [0, 1, 2, 3, 4], 'dep': ['', '', '', '', ''], 'sent_start': [1, 0, 0, 0, 0]}}

ps: the parse_gold_doc function in the docs is just a placeholder/dummy function. we'll clarify that in the docs to avoid confusion!",https://stackoverflow.com/questions/73655387,initialization,08-09-2022 21:54,727.0,2.0,1.0,True,15-09-2022 15:11,13-09-2022 22:23
72393897,keep model made with tfidf for predicting new content using scikit for python,"this is a sentiment analysis model made with tf-idf for feature extraction
i want to know how can i save this model and reuse it.
i tried saving it this way but when i load it , do same pre-processing on the test text and fit_transform on it it gave an error that the model expected x numbers of features but got y
this is how i saved it
filename = ""model.joblib""
joblib.dump(model, filename)

and this is the code for my tf-idf model
import pandas as pd
import re
import nltk
from sklearn.ensemble import randomforestclassifier
from sklearn.naive_bayes import bernoullinb
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import tfidfvectorizer
nltk.download('stopwords')
from nltk.corpus import stopwords

processed_text = ['list of pre-processed text'] 
y = ['list of labels']
tfidfconverter = tfidfvectorizer(max_features=10000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))
x = tfidfconverter.fit_transform(processed_text).toarray()

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)

text_classifier = bernoullinb()
text_classifier.fit(x_train, y_train)

predictions = text_classifier.predict(x_test)
print(confusion_matrix(y_test, predictions))
print(classification_report(y_test, predictions))
print(accuracy_score(y_test, predictions))

edit:
just to exact where to put every line
so after:
tfidfconverter = tfidfvectorizer(max_features=10000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))

then
tfidf_obj = tfidfconverter.fit(processed_text)//this is what will be used again
joblib.dump(tfidf_obj, 'tf-idf.joblib')

then you do the rest of the steps you will save the classifier after training as well so after:
text_classifier.fit(x_train, y_train)

put
joblib.dump(model, ""classifier.joblib"")
now when you want to predict any text
tf_idf_converter = joblib.load(""tf-idf.joblib"")
classifier = joblib.load(""classifier.joblib"")

now u have list of sentences to predict
sent = []
classifier.predict(tf_idf_converter.transform(sent))

now print that for a list of sentiments for each sentece","['python', 'machine-learning', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']",72394054,"you can first fit tfidf to your training set using:
tfidfconverter = tfidfvectorizer(max_features=10000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))
tfidf_obj = tfidfconverter.fit(processed_text)

then find a way to store the tfidf_obj for instance using pickle or joblib e.g:
joblib.dump(tfidf_obj, filename)

then load the saved tfidf_obj and apply transform only on your test set
loaded_tfidf = joblib.load(filename)
test_new = loaded_tfidf.transform(x_test)",https://stackoverflow.com/questions/72393897,python,26-05-2022 15:00,1460.0,2.0,1.0,True,26-05-2022 20:37,26-05-2022 20:37
57641504,detecting questions in text,"i have a project where i need to analyze a text to extract some information if the user who post this text need help in something or not, i tried to use sentiment analysis but it didn't work as expected, my idea was to get the negative post and extract the main words in the post and suggest to him some articles about that subject, if there is another way that can help me please post it below and thanks.
for the dataset i useed, it was a dataset for sentiment analyze, but now i found that it's not working and i need a dataset use for this subject.","['python-3.x', 'machine-learning', 'text', 'nlp', 'analytics']",57712429,"for this topic i found that this field in machine learning is called ""natural language questions"" it's a field where machine learning models trained to detect questions in text and suggesting answer for them based on data set you are working with, check this article for more detail.",https://stackoverflow.com/questions/57641504,python-3.x,24-08-2019 20:59,1240.0,1.0,2.0,True,19-05-2021 08:09,25-08-2019 12:14
36952763,how to return history of validation loss in keras,"using anaconda python 2.7 windows 10.
i am training a language model using the keras exmaple:
print('build model...')
model = sequential()
model.add(gru(512, return_sequences=true, input_shape=(maxlen, len(chars))))
model.add(dropout(0.2))
model.add(gru(512, return_sequences=false))
model.add(dropout(0.2))
model.add(dense(len(chars)))
model.add(activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

def sample(a, temperature=1.0):
    # helper function to sample an index from a probability array
    a = np.log(a) / temperature
    a = np.exp(a) / np.sum(np.exp(a))
    return np.argmax(np.random.multinomial(1, a, 1))


# train the model, output generated text after each iteration
for iteration in range(1, 3):
    print()
    print('-' * 50)
    print('iteration', iteration)
    model.fit(x, y, batch_size=128, nb_epoch=1)
    start_index = random.randint(0, len(text) - maxlen - 1)

    for diversity in [0.2, 0.5, 1.0, 1.2]:
        print()
        print('----- diversity:', diversity)

        generated = ''
        sentence = text[start_index: start_index + maxlen]
        generated += sentence
        print('----- generating with seed: ""' + sentence + '""')
        sys.stdout.write(generated)

        for i in range(400):
            x = np.zeros((1, maxlen, len(chars)))
            for t, char in enumerate(sentence):
                x[0, t, char_indices[char]] = 1.

            preds = model.predict(x, verbose=0)[0]
            next_index = sample(preds, diversity)
            next_char = indices_char[next_index]

            generated += next_char
            sentence = sentence[1:] + next_char

            sys.stdout.write(next_char)
            sys.stdout.flush()
        print()

according to keras documentation, the model.fit method returns a history callback, which has a history attribute containing the lists of successive losses and other metrics.
hist = model.fit(x, y, validation_split=0.2)
print(hist.history)

after training my model, if i run print(model.history) i get the error:
 attributeerror: 'sequential' object has no attribute 'history'

how do i return my model history after training my model with the above code?
update
the issue was that:
the following had to first be defined:
from keras.callbacks import history 
history = history()

the callbacks option had to be called
model.fit(x_train, y_train, nb_epoch=5, batch_size=16, callbacks=[history])

but now if i print
print(history.history)

it returns
{}

even though i ran an iteration.","['python', 'neural-network', 'nlp', 'deep-learning', 'keras']",36956440,"it's been solved.
the losses only save to the history over the epochs. i was running iterations instead of using the keras built in epochs option.
so instead of doing 4 iterations i now have
model.fit(......, nb_epoch = 4)

now it returns the loss for each epoch run:
print(hist.history)
{'loss': [1.4358016599558268, 1.399221191623641, 1.381293383180471, 1.3758836857303727]}",https://stackoverflow.com/questions/36952763,python,30-04-2016 08:45,211502.0,78.0,13.0,True,01-03-2024 15:03,10-03-2017 15:21
64435099,pandas: split and convert series of alphanumeric texts to columns and rows,"current data frame: i have a pandas data frame where each employee has a text code(all codes start with t) and an associated frequency right next to the code. all text codes have 8 characters.
+----------+-------------------------------------------------------------+
|  emp_id  |   text                                                      |
+----------+-------------------------------------------------------------+
|   e0001  | [t0431516,-8,t0401531,-12,t0517519,12]                      |
|   e0002  | [t0701540,-1,t0431516,-2]                                   |
|   e0003  | [t0517519,-1,t0421531,-7,t0516319,9,t0500371,-6,t0309711,-3]|
|   e0004  | [t0516319,-3]                                               |
|   e0005  | [t0431516,2]                                                |
+----------+-------------------------------------------------------------+

expected data frame: i am trying to make the text codes present in the data frame as individual columns and if an employee has a frequency for that code then populate frequency else 0.
+----------+----------------------------------------------------------------------------------------+
|  emp_id  | t0431516 | t0401531 | t0517519 | t0701540 | t0421531 |  t0516319 | t0500371 | t0309711 |                                      
+----------+----------------------------------------------------------------------------------------+
|   e0001  | -8       | -12      | 12       | 0        | 0        | 0         | 0        | 0        |
|   e0002  | -2       | 0        | 0        | -1       | 0        | 0         | 0        | 0        |
|   e0003  | 0        | 0        | -1       | 0        | -7       | 9         | -6       | -3       |
|   e0004  | 0        | 0        | 0        | 0        | 0        | -3        | 0        | 0        |
|   e0005  | 2        | 0        | 0        | 0        | 0        | 0         | 0        | 0        |
+----------+----------------------------------------------------------------------------------------+

sample data:
pd.dataframe({'emp_id' : {0: 'e0001', 1: 'e0002', 2: 'e0003', 3: 'e0004', 4: 'e0005'},
                'text' :  {0: '[t0431516,-8,t0401531,-12,t0517519,12]', 1: '[t0701540,-1,t0431516,-2]', 2: '[t0517519,-1,t0421531,-7,t0516319,9,t0500371,-6,t0309711,-3]', 3: '[t0516319,-3]', 4: '[t0431516,2]'}
                })

so, far my attempts were unsuccessful. any pointers/help is much appreciated!","['python', 'pandas', 'parsing', 'nlp', 'text-mining']",64435408,"you can explode the dataframe and then create a pivot_table:
df = pd.dataframe({'emp_id' : ['e0001', 'e0002', 'e0003', 'e0004', 'e0005'],
                  'text' : [['t0431516',-8,'t0401531',-12,'t0517519',12],
                 ['t0701540',-1,'t0431516',-2],['t0517519',-1,'t0421531',-7,'t0516319',9,'t0500371',-6,'t0309711',-3],
                 ['t0516319',-3], ['t0431516',2]]})
df = df.explode('text')
df['freq'] = df['text'].shift(-1)
df = df[df['text'].str[0] == 't']
df['freq'] = df['freq'].astype(int)
df = pd.pivot_table(df, index='emp_id', columns='text', values='freq',aggfunc = 'sum').fillna(0).astype(int)
df
out[1]: 
text    t0309711  t0401531  t0421531  t0431516  t0500371  t0516319  t0517519  \
emp_id                                                                         
e0001          0       -12         0        -8         0         0        12   
e0002          0         0         0        -2         0         0         0   
e0003         -3         0        -7         0        -6         9        -1   
e0004          0         0         0         0         0        -3         0   
e0005          0         0         0         2         0         0         0   

text    t0701540  
emp_id            
e0001          0  
e0002         -1  
e0003          0  
e0004          0  
e0005          0",https://stackoverflow.com/questions/64435099,python,19-10-2020 21:06,154.0,0.0,1.0,True,28-10-2020 19:39,20-10-2020 15:57
38982423,opennlp lemmatization example,"does anyone know where i can find an example of how to use the simplelemmatizer class in the opennlp library, and where i can find a sample english dictionary? it appears to be missing from the documentation.","['nlp', 'opennlp']",38985983,"you can download dictionary from here - en-lemmatizer.dict
example : 
import opennlp.tools.lemmatizer.simplelemmatizer;

private static simplelemmatizer lemmatizer;

private string lemmatize(string word, string postag) throws ioexception {
    if (lemmatizer == null) {
        inputstream is = getclass().getresourceasstream(""/models/en-lemmatizer.dict"");
        lemmatizer = new simplelemmatizer(is);
        is.close();
    }
    string lemma = lemmatizer.lemmatize(word, postag);
    return lemma;
}

sample code taken from here - documenttaggerservice",https://stackoverflow.com/questions/38982423,nlp,16-08-2016 18:51,7247.0,7.0,2.0,True,09-12-2023 07:24,09-12-2023 07:24
49774738,how to do an approx match and replace with correct word in r?,"list1 <- c(""prmum"",""prum"",""primium"",""prm"",""prim"",""primum"",""prem"",""premum"",
           ""wrng"",""wng"",
           ""hug"",""hung"",
           ""amut"",
           ""chq"",""chquked"",""cheuq"",""chek"",""cheq"",
           ""cus"",""cust"",
           ""cbk"",""cb"",
           ""ringirng"",""rining"",""rigirigi"")


list2 <- c(""premium"",""wrong"",""hang"",""amount"",""cheque"",""customer"",""callback"",""ringing"")
dat <- as.data.frame(list1)
for(i in length(list1)){
t <- agrep(list1[i],list2,value=false)
 dat[t] <- list2[i]
}

i have two lists one having wrong_words and other correct_words.
i am trying to do the following:
1)take first word.
2)do approx match from list of correct_words and get the index location.
3)replace the wrong word with the correct word at that particular location 
in the dataframe or a list.","['r', 'text-mining']",49775050,"you can do it using stringdistmatrix from package stringdist. it uses levenshtein distance just like agrep. you find which word has the closest match, and replace it in your original list.
 library(stringdist)
 dist_mat <- stringdistmatrix(list1, list2)
 clean_list1 <- list2[apply(dist_mat, 1, which.min)]
 clean_list1

now this solution may be inappropriate if you have very long lists (assume they are of length l1 and l2, you will get a matrix of size l1*l2). you may need to go through a loop to reduce memory consumption.
clean_list1 <- list1
for (i in length(list1)){
     dist_vect <- stringdistmatrix(list1[i],list2)
     clean_list1 <- list2[which.min(dist_vect)]
}",https://stackoverflow.com/questions/49774738,r,11-04-2018 12:07,59.0,0.0,1.0,True,05-08-2020 10:17,05-08-2020 10:17
72468237,matching multiple strings in r/nlp/spacyr,"i have a data frame
mydataframe <- data.frame(keyword = c(c(""meeting"", ""laptop""),c(""attend a meeting"", ""fan"")))
description <- ""i have to attend a meeting.""

i have to match the description with the column keyword in the dataframe and return ""attend a meeting"".
is there any solution to get a particular output","['r', 'nlp', 'spacy', 'string-matching']",72468728,"using grepl will return the keywords in mydataframe which contains in description
mydataframe <- data.frame(keyword = c(c(""meeting"", ""laptop""),c(""attend a meeting"", ""fan"")))
description <- ""i have to attend a meeting.""

found <- as.logical(lapply(mydataframe$keyword ,
                       function(x) grepl(x , description)))

mydataframe[found , ]
#> [1] ""meeting""          ""attend a meeting""

created on 2022-06-02 by the reprex package (v2.0.1)",https://stackoverflow.com/questions/72468237,r,01-06-2022 21:28,94.0,0.0,1.0,True,01-06-2022 22:36,01-06-2022 22:10
