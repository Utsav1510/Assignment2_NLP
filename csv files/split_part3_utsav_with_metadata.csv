question_id,title,body,tags,accepted_answer_id,accepted_answer_body,link,tag,creation_date,view_count,score,answer_count,is_answered,last_activity_date,last_edit_date
74497075,"extract two specified words from the dataframe and place them in a new column, then delete the rows","this is the dataframe:
data = {""company"" : [[""consensys""] , [""cognizant""], [""ibm""], [""ibm""], [""reddit, inc""], [""reddit, inc""], [""ibm""]],
""skills"" : [['services', 'scientist technical expertise', 'databases'], ['datacomputing tools experience', 'deep learning models', 'cloud services'], ['quantitative analytical projects', 'financial services', 'field experience'],
['filesystems server architectures', 'systems', 'statistical analysis', 'data analytics', 'workflows', 'aws cloud services'], ['aws services'], ['data mining statistics', 'statistical analysis', 'aws cloud', 'services', 'data discovery', 'visualization'], ['communication skills experience', 'services', 'manufacturing environment', 'sox compliance']]}

dff = pd.dataframe(data)
dff


i need to create a new column, and i want to start by taking specific
words out of the skills column.
the row that does not include those specific words should then be
deleted.
specific words: 'services', 'statistical analysis'

expected output:





company
skills
new_col




0
[consensys]
[services, scientist technical expertise, databases]
[services]


1
[ibm]
[filesystems server architectures, systems, statistical analysis, data analytics, workflows, aws cloud services]
[services, statistical analysis]


2
[reddit, inc]
[data mining statistics, statistical analysis, aws cloud, services, data discovery, visualization]
[statistical analysis]


3
[ibm]
['communication skills experience', 'services', 'manufacturing environment', 'sox compliance']
[services]




i tried quite a lot of code in an effort to extract a specific word from the one that was available on stack overflow, but i was unsuccessful.","['python', 'pandas', 'dataframe', 'nlp']",74497192,"word = ['services', 'statistical analysis']
s1 = df['skills'].apply(lambda x: [i for i in word if i in x])

output(s1):
0                          [services]
1                                  []
2                                  []
3              [statistical analysis]
4                                  []
5    [services, statistical analysis]
6                          [services]
name: skills, dtype: object

make s1 to new_col and boolean indexing
df.assign(new_col=s1)[lambda x: x['new_col'].astype('bool')]

result:
    company skills  new_col
0   [consensys] [services, scientist technical expertise, data...   [services]
3   [ibm]   [filesystems server architectures, systems, st...   [statistical analysis]
5   [reddit, inc]   [data mining statistics, statistical analysis,...   [services, statistical analysis]
6   [ibm]   [communication skills experience, services, ma...   [services]

i think you should make more simple example",https://stackoverflow.com/questions/74497075,python,19-11-2022 02:14,35.0,0.0,2.0,True,19-11-2022 05:12,19-11-2022 02:35
49564176,python (nltk) - more efficient way to extract noun phrases?,"i've got a machine learning task involving a large amount of text data. i want to identify, and extract, noun-phrases in the training text so i can use them for feature construction later on in the pipeline. 
i've extracted the type of noun-phrases i wanted from text but i'm fairly new to nltk, so i approached this problem in a way where i can break down each step in list comprehensions like you can see below. 
but my real question is, am i reinventing the wheel here? is there a faster way to do this that i'm not seeing?
import nltk
import pandas as pd

mydata = pd.read_excel(""\user\train_.xlsx"")
texts = mydata['message']

# defining a grammar & parser
np = ""np: {(<v\w+>|<nn\w?>)+.*<nn\w?>}""
chunkr = nltk.regexpparser(np)

tokens = [nltk.word_tokenize(i) for i in texts]

tag_list = [nltk.pos_tag(w) for w in tokens]

phrases = [chunkr.parse(sublist) for sublist in tag_list]

leaves = [[subtree.leaves() for subtree in tree.subtrees(filter = lambda t: t.label == 'np')] for tree in phrases]

flatten the list of lists of lists of tuples that we've ended up with, into
just a list of lists of tuples
leaves = [tupls for sublists in leaves for tupls in sublists]

join the extracted terms into one bigram
nounphrases = [unigram[0][1]+' '+unigram[1][0] in leaves]","['python-3.x', 'pandas', 'nlp', 'nltk', 'text-chunking']",49584275,"take a look at why is my nltk function slow when processing the dataframe?, there's no need to iterate through all rows multiple times if you don't need intermediate steps. 
with ne_chunk and solution from 

nltk named entity recognition to a python list and
how can i extract gpe(location) using nltk ne_chunk?

[code]:
from nltk import word_tokenize, pos_tag, ne_chunk
from nltk import regexpparser
from nltk import tree
import pandas as pd

def get_continuous_chunks(text, chunk_func=ne_chunk):
    chunked = chunk_func(pos_tag(word_tokenize(text)))
    continuous_chunk = []
    current_chunk = []

    for subtree in chunked:
        if type(subtree) == tree:
            current_chunk.append("" "".join([token for token, pos in subtree.leaves()]))
        elif current_chunk:
            named_entity = "" "".join(current_chunk)
            if named_entity not in continuous_chunk:
                continuous_chunk.append(named_entity)
                current_chunk = []
        else:
            continue

    return continuous_chunk

df = pd.dataframe({'text':['this is a foo, bar sentence with new york city.', 
                           'another bar foo washington dc thingy with bruce wayne.']})

df['text'].apply(lambda sent: get_continuous_chunks((sent)))

[out]:
0                   [new york]
1    [washington, bruce wayne]
name: text, dtype: object

to use the custom regexpparser :
from nltk import word_tokenize, pos_tag, ne_chunk
from nltk import regexpparser
from nltk import tree
import pandas as pd

# defining a grammar & parser
np = ""np: {(<v\w+>|<nn\w?>)+.*<nn\w?>}""
chunker = regexpparser(np)

def get_continuous_chunks(text, chunk_func=ne_chunk):
    chunked = chunk_func(pos_tag(word_tokenize(text)))
    continuous_chunk = []
    current_chunk = []

    for subtree in chunked:
        if type(subtree) == tree:
            current_chunk.append("" "".join([token for token, pos in subtree.leaves()]))
        elif current_chunk:
            named_entity = "" "".join(current_chunk)
            if named_entity not in continuous_chunk:
                continuous_chunk.append(named_entity)
                current_chunk = []
        else:
            continue

    return continuous_chunk


df = pd.dataframe({'text':['this is a foo, bar sentence with new york city.', 
                           'another bar foo washington dc thingy with bruce wayne.']})


df['text'].apply(lambda sent: get_continuous_chunks(sent, chunker.parse))

[out]:
0                  [bar sentence, new york city]
1    [bar foo washington dc thingy, bruce wayne]
name: text, dtype: object",https://stackoverflow.com/questions/49564176,python-3.x,29-03-2018 20:04,22648.0,12.0,4.0,True,16-01-2023 22:23,31-03-2018 04:35
33676526,pos-tagger is incredibly slow,"i am using nltk to generate n-grams from sentences by first removing given stop words. however, nltk.pos_tag() is extremely slow taking up to 0.6 sec on my cpu (intel i7).
the output:
['the first time i went, and was completely taken by the live jazz band and atmosphere, i ordered the lobster cobb salad.']
0.620481014252
[""it's simply the best meal in nyc.""]
0.640982151031
['you cannot go wrong at the red eye grill.']
0.644664049149

the code:
for sentence in source:

    nltk_ngrams = none

    if stop_words is not none:   
        start = time.time()
        sentence_pos = nltk.pos_tag(word_tokenize(sentence))
        print time.time() - start

        filtered_words = [word for (word, pos) in sentence_pos if pos not in stop_words]
    else:
        filtered_words = ngrams(sentence.split(), n)

is this really that slow or am i doing something wrong here?","['python', 'nlp', 'nltk', 'pos-tagger']",33677051,"use pos_tag_sents for tagging multiple sentences:
>>> import time
>>> from nltk.corpus import brown
>>> from nltk import pos_tag
>>> from nltk import pos_tag_sents
>>> sents = brown.sents()[:10]
>>> start = time.time(); pos_tag(sents[0]); print time.time() - start
0.934092998505
>>> start = time.time(); [pos_tag(s) for s in sents]; print time.time() - start
9.5061340332
>>> start = time.time(); pos_tag_sents(sents); print time.time() - start 
0.939551115036",https://stackoverflow.com/questions/33676526,python,12-11-2015 16:32,4397.0,8.0,4.0,True,04-01-2024 03:14,12-11-2015 16:59
67964793,how to append tokenized sentences as row to a csv,"i am trying to do sentence tokenization several .txt files from a path, and then append each tokenized sentence to a new row with the *.txt document id as csv.
there are several *txt files in the path (work_dir)
in the below example, the first column needs to be the file name (wltw_5_2016_02_29), and the next column tokenized sentence. such that, if there are 40 sentences in a document, i would expect 40 rows with the same file name in the first column and the second column sentences.  i also attached a picture to show how the csv output is expected.
import nltk
work_dir='/content/drive/my drive/deneme'
filename = 'wltw_5_2016_02_29.txt'
file = open(filename, 'rt')
text = file.read()
#file.close()
# split into sentences
from nltk import sent_tokenize
sentences = sent_tokenize(text)
print(sentences)
import csv

with open('writedata.csv', mode='w') as file:
    writer = csv.writer(file, delimiter=',', quotechar='""', quoting=csv.quote_minimal)
    writer.writerow((""filename"", ""sentence""))
    writer.writerow((filename, sentences))

i tried this approach but i couldnot manage it.  here

with the above code, it writes everything to the same row. however as seen in the above example, i want to write them to the same column by appending as row.","['python', 'csv', 'nlp', 'nltk', 'tokenize']",67973798,"i think my issue was with the sequence of the code:
here comes the working one, in case anyone has the same issue, feel free to use it:
import nltk, glob, csv
from nltk import sent_tokenize
files = glob.glob(""/content/drive/my drive/deneme/*.txt"")

with open('writedata.csv', mode='w') as new_file:
  writer = csv.writer(new_file, delimiter=',', quotechar='""', quoting=csv.quote_minimal)
  for filename in files:

    # take all sentences from a given file
    file = open(filename, 'rt')
    text = file.read()
    file.close()
 
    sentences = sent_tokenize(text)
    print(sentences)

    for sentence in sentences:
      writer.writerow((filename, sentence))",https://stackoverflow.com/questions/67964793,python,14-06-2021 04:28,423.0,0.0,1.0,True,14-06-2021 16:15,14-06-2021 05:11
36241051,apache lucene doesn&#39;t filter stop words despite the usage of stopanalyzer and stopfilter,"i have a module based on apache lucene 5.5 / 6.0 which retrieves keywords. everything is working fine except one thing ï¿½ï¿½ï¿½ lucene doesn't filter stop words.
i tried to enable stop word filtering with two different approaches.
approach #1:
tokenstream = new stopfilter(new asciifoldingfilter(new classicfilter(new lowercasefilter(stdtoken))), englishanalyzer.getdefaultstopset());
tokenstream.reset();

approach #2:
tokenstream = new stopfilter(new classicfilter(new lowercasefilter(stdtoken)), stopanalyzer.english_stop_words_set);
tokenstream.reset();

the full code is available here:

my questions:

why lucene doesn't filter stop words?

how can i enable the stop words filtering in lucene 5.5 / 6.0?","['java', 'apache', 'lucene', 'information-retrieval', 'stop-words']",36246089,"the pitfall was in the default lucene's stop words list, i expected, it is much more broader.
here is the code which by default tries to load the customized stop words list and if it's failed then uses the standard one:
chararrayset stopwordsset;

try {
    // use customized stop words list
    string stopwordsdictionary = fileutils.readfiletostring(new file(%path_to_file%));
    stopwordsset = wordlistloader.getwordset(new stringreader(stopwordsdictionary));
} catch (filenotfoundexception e) {
    // use standard stop words list
    stopwordsset = chararrayset.copy(standardanalyzer.stop_words_set);
}

tokenstream = new stopfilter(new asciifoldingfilter(new classicfilter(new lowercasefilter(stdtoken))), stopwordsset);
tokenstream.reset();",https://stackoverflow.com/questions/36241051,java,26-03-2016 21:22,2242.0,3.0,2.0,True,22-11-2022 16:47,22-11-2022 16:47
71279997,can&#39;t access indexed tuple,"i'm trying to traverse through a dictionary that essentially contains tuples and keys for tuples like this:
(101940039, 'yoel'): 0.0016034940264139383, 
(101940039, 'yossi'): 0.004810482079241815, 
(101940039, 'youngmen'): 0.0016034940264139383}

i need to access the value of the key, i.e., the string of the tuple. i tried many things, like converting to the dictionary, using key[0] just gives me ""'int' object is not subscribable""..
    def matching_score(k, tokens, tf_idf_score):
    print(""matching score"")
    query_weights = {}
    for word in tokens:
        for key, value in tf_idf_score.items():
            **if key in word**:
                try:
                    query_weights[key[0]] += tf_idf_score[key]
                except:
                    query_weights[key[0]] = tf_idf_score[key]
        
        query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=true)
    print("""")
    
    l = []
    
    for i in query_weights[:10]:
        l.append(i[0])
    
    print(l)","['python', 'tuples', 'key', 'tf-idf']",71280418,"first, this is a recreation of your data as a dictionary:
d1 = {(101940039, 'yoel'): 0.0016034940264139383, 
      (101940039, 'yossi'): 0.004810482079241815, 
      (101940039, 'youngmen'): 0.0016034940264139383}

with keys() it is possible to access the keys. at the same time, we want to convert them into a list.
list(d1.keys())

the result is a list of tuples.
[(101940039, 'yoel'), (101940039, 'yossi'), (101940039, 'youngmen')]
to access individual items in this nested list: first, use the index of the list to select the desired list, and second, use the index of the tuple to select the desired item within.
list(d1.keys())[0][1]

'yoel'
to get all the string elements of the key tuples:
for i in range(len(d1)):
    print(list(d1.keys())[i][1])

yoel
yossi
youngmen",https://stackoverflow.com/questions/71279997,python,26-02-2022 20:17,632.0,0.0,1.0,True,26-02-2022 21:28,26-02-2022 20:33
77178335,configure multitenancy with langchain and qdrant,"i'm creating a q&a chatbot and i'm using langchain and qdrant.
i'm trying to configure langchain to be able to use qdrant in a multitenant environment.
the doc from qdrant says that the best approach in my case is to use a ""partition by payload"" and use a group_id = oneclient inside the payload of each element of a collection, so that then it's possible to filter on that group_id (which in my case will be the client).
that's the link to the doc 
i'm using langchain and i have added to the documents that i'm saving inside qdrant a ""group_id"" metadata field.
i'd like to understand how to filter on group_id when i use langchain.
this is how i'm using langchain to retrieve the answer to a question:
qdrant = qdrant(
    client=qdrantclient(...),
    collection_name=""collection1"",
    embeddings=embeddings
)
prompt = ...
llm = chatopenai(...) 
qa_chain = retrievalqawithsourceschain.from_chain_type(
     llm=llm,
     chain_type=""stuff"",
     return_source_documents=true,
     retriever=qdrant.as_retriever(),
     chain_type_kwargs = {""prompt"": prompt}
 )
result = qa_chain({""question"": question})

the group_id will represent the client and it is known before the question.
any help is much appreciated, thanks.","['python', 'chatbot', 'langchain', 'qdrant', 'qdrantclient']",77314317,"i have found the answer. thanks for all the suggestions.
to filter on an attribute ""group_id"" which is the client_id, i'm adding a metadata group_id = client when i load some data with ""vectorestore.from_documents"" and i'm using the ""as_retriever"" function to pass the search filter and return only the sources with that group_id:
chain = retrievalqawithsourceschain.from_chain_type(
    llm=llm,
    chain_type=chain_type,
    max_tokens_limit=max_tokens_limit,
    return_source_documents=true,
    retriever=vectorstore.as_retriever(
        search_kwargs={'filter': {'group_id': client}}
    ),
    reduce_k_below_max_tokens=false,
    chain_type_kwargs = {""prompt"": prompt}
)",https://stackoverflow.com/questions/77178335,python,26-09-2023 08:37,1285.0,1.0,2.0,True,18-10-2023 07:37,27-09-2023 19:10
76546004,openai fine-tuning api: why&#160;would i use llamaindex&#160;or&#160;langchain instead of fine-tuning a model?,"i'm just getting started with working with llms, particularly openais and other oss models. there are a lot of guides on using llamaindex to create a store of all your documents and then query on them. i tried it out with a few sample documents, but discovered that each query gets super expensive quickly. i think i used a 50-page pdf document, and a summarization query cost me around 1.5usd per query. i see there's a lot of tokens being sent across, so i'm assuming it's sending the entire document for every query. given that someone might want to use thousands of millions of records, i can't see how something like llamaindex can really be that useful in a cost-effective manner.
on the other hand, i see openai allows you to train a chatgpt model. wouldn't that, or using other custom trained llms, be much cheaper and more effective to query over your own data? why would i ever want to set up llamaindex?","['openai-api', 'langchain', 'chatgpt-api', 'language-model', 'llama-index']",76558875,"tl;dr: use llamaindex or langchain to get an exact answer (i.e., a fact) to a specific question from existing data sources.
why choose llamaindex or langchain over fine-tuning a model?
the answer is simple, but you couldn't answer it yourself because you were only looking at the costs. there are other aspects as well, not just costs. take a look at the usability side of the question.
fine-tuning a model will give the model additional general knowledge, but the fine-tuned model will not give you an exact answer (i.e., a fact) to a specific question.
people train an openai model with some data, but when they ask it something related to the fine-tuning data, they are surprised that the model doesn't answer with the knowledge gained by fine-tuning. see an example explanation on the official openai forum by @juan_olano:

i fine-tuned a 70k-word book. my initial expectation was to have the
desired qa, and at that point i didnï¿½ï¿½ï¿½t know any better. but this
fine-tuning showed me the limits of this approach. it just learned the
style and stayed more or less within the corpus, but hallucinated a
lot.
then i split the book into sentences and worked my way through
embeddings, and now i have a very decent qa system for the book, but
for narrow questions. it is not as good for questions that need the
context of the entire book.

also, see the official openai documentation:

some common use cases where fine-tuning can improve results:

setting the style, tone, format, or other qualitative aspects
improving reliability at producing a desired output
correcting failures to follow complex prompts
handling many edge cases in specific ways
performing a new skill or task thatï¿½ï¿½ï¿½s hard to articulate in a prompt


llamaindex or langchain enable you to connect openai models with your existing data sources. for example, a company has a bunch of internal documents with various instructions, guidelines, rules, etc. llamaindex or langchain can be used to query all those documents and give an exact answer to an employee who needs an answer.
openai models (gpt-3, gpt-3.5, gpt-4, etc.) can't query their knowledge. querying requires calculating embedding vectors from a resource and then calculating cosine similarity, which openai moan't do. an openai model simply gives an answer based on the statistical probability of which word should follow the previous one.
i strongly suggest you read my previous answer regarding semantic search. you'll understand this answer better.",https://stackoverflow.com/questions/76546004,openai-api,24-06-2023 12:12,4977.0,4.0,1.0,True,12-06-2024 15:57,17-01-2024 09:04
20907909,stanford corenlp remove/stop red information print outs,"i'm using stanford's corenlp java api and while running it prints out information in red.
it just fills up the command lines when i don't want to see it.
is there anyway of disabling this feature?
example of the red info lines:
searching for resource: stanfordcorenlp.properties
searching for resource: edu/stanford/nlp/pipeline/stanfordcorenlp.properties
adding annotator tokenize
adding annotator ssplit
adding annotator pos
reading pos tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.2 sec].
adding annotator lemma
adding annotator ner
loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [3.0 sec].
loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [2.7 sec].
loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [2.0 sec].
initializing jollydayholiday for sutime
reading tokensregex rules from edu/stanford/nlp/models/sutime/defs.sutime.txt
reading tokensregex rules from edu/stanford/nlp/models/sutime/english.sutime.txt
jan 03, 2014 3:52:37 pm edu.stanford.nlp.ling.tokensregex.coremapexpressionextractor appendrules
info: ignoring inactive rule: temporal-composite-8:ranges
reading tokensregex rules from edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
adding annotator parse
loading parser from serialized file edu/stanford/nlp/models/lexparser/englishpcfg.ser.gz ... done [0.8 sec].
adding annotator dcoref
searching for resource: stanfordcorenlp.properties
searching for resource: edu/stanford/nlp/pipeline/stanfordcorenlp.properties
adding annotator tokenize
adding annotator ssplit
adding annotator pos
adding annotator lemma
adding annotator ner
adding annotator parse
adding annotator dcoref
searching for resource: stanfordcorenlp.properties
searching for resource: edu/stanford/nlp/pipeline/stanfordcorenlp.properties
adding annotator tokenize
adding annotator ssplit
adding annotator pos
adding annotator lemma
adding annotator ner
adding annotator parse
adding annotator dcoref","['java', 'nlp', 'stanford-nlp']",21406307,"depending on your program context, you can just drop all text from the error stream's output during corenlp execution.

// this is your print stream, store the reference
printstream err = system.err;

// now make all writes to the system.err stream silent 
system.seterr(new printstream(new outputstream() {
    public void write(int b) {
    }
}));

// your code here

// set everything bck to its original state afterwards
system.seterr(err);",https://stackoverflow.com/questions/20907909,java,03-01-2014 16:09,2215.0,6.0,1.0,True,19-07-2022 01:08,19-07-2022 01:08
66715423,distance between strings by similarity of sound,"is the a quantitative descriptor of similarity between two words based on how they sound/are pronounced, analogous to levenshtein distance?
i know soundex gives same id to similar sounding words, but as far as i undestood it is not a quantitative descriptor of difference between the words.
from jellyfish import soundex

print(soundex(""two""))
print(soundex(""to""))","['python', 'audio', 'nlp', 'linguistics']",66716881,"you could combine phonetic encoding and string comparison algorithm. as a matter of fact jellyfish supplies both.
setting up the libraries examples
from jellyfish import soundex, metaphone, nysiis, match_rating_codex,\
    levenshtein_distance, damerau_levenshtein_distance, hamming_distance,\
    jaro_similarity
from itertools import groupby
import pandas as pd
import numpy as np


datalist = ['two','too','to','fourth','forth','dessert',
            'desert','byrne','boern','smith','smyth','catherine','kathryn']

sounds_encoding_methods = [soundex, metaphone, nysiis, match_rating_codex]

let compare different phonetic encoding
report = pd.dataframe([datalist]).t
report.columns = ['word']
for i in sounds_encoding_methods:
    print(i.__name__)
    report[i.__name__]= report['word'].apply(lambda x: i(x))
print(report)
          soundex metaphone   nysiis match_rating_codex
word                                                   
two          t000        tw       tw                 tw
too          t000         t        t                  t
to           t000         t        t                  t
fourth       f630       fr0     fart               frth
forth        f630       fr0     fart               frth
dessert      d263      tsrt    dasad               dsrt
desert       d263      tsrt    dasad               dsrt
byrne        b650       brn     byrn               byrn
boern        b650       brn     barn                brn
smith        s530       sm0     snat               smth
smyth        s530       sm0     snyt              smyth
catherine    c365      k0rn  cataran              cthrn
kathryn      k365      k0rn   catryn             kthryn

you can see that phonetic encoding is doing a pretty good job making comparable the words. you could see different cases and prefer one or other depending on your case.
now i will just take the above and try to find the closest match using levenshtein_distance, but i could you any other too.
""""""select the closer by algorithm
for instance levenshtein_distance""""""
report2 = pd.dataframe([datalist]).t
report2.columns = ['word']

report.set_index('word',inplace=true)
report2 = report.copy()
for sounds_encoding in sounds_encoding_methods:
    report2[sounds_encoding.__name__] = np.nan
    matched_words = []
    for word in datalist:
        closest_list = []
        for word_2 in datalist:
            if word != word_2:
                closest = {}
                closest['word'] =  word_2
                closest['similarity'] = levenshtein_distance(report.loc[word,sounds_encoding.__name__],
                                     report.loc[word_2,sounds_encoding.__name__])
                closest_list.append(closest)

        report2.loc[word,sounds_encoding.__name__] = pd.dataframe(closest_list).\
            sort_values(by = 'similarity').head(1)['word'].values[0]

print(report2)
             soundex  metaphone     nysiis match_rating_codex
word                                                         
two              too        too        too                too
too              two         to         to                 to
to               two        too        too                too
fourth         forth      forth      forth              forth
forth         fourth     fourth     fourth             fourth
dessert       desert     desert     desert             desert
desert       dessert    dessert    dessert            dessert
byrne          boern      boern      boern              boern
boern          byrne      byrne      byrne              byrne
smith          smyth      smyth      smyth              smyth
smyth          smith      smith      smith              smith
catherine    kathryn    kathryn    kathryn            kathryn
kathryn    catherine  catherine  catherine          catherine

as from above you could clearly see that combinations between  phonetic encoding and string comparison algorithm can be very straight forward.",https://stackoverflow.com/questions/66715423,python,19-03-2021 20:50,2676.0,4.0,1.0,True,19-03-2021 23:42,19-03-2021 20:56
78591465,unexpected string validation error in langchain pydantic output parser,"i do not understand why the below use of the pydanticoutputparser is erroring.
the docs do not seem correct - if i follow this exactly (i.e. use with_structured_output exclusively, without an output parser) then the output is a dict, not pydantic class. so i thought i modified it consistently with so so answers e.g. this
from langchain.prompts import prompttemplate
from langchain_openai import chatopenai
from langchain.output_parsers import pydanticoutputparser

from uuid import uuid4
from pydantic import basemodel, field

class testsummary(basemodel):
    """"""represents a summary of the concept""""""

    id: str = field(default_factory=lambda: str(uuid4()), description=""unique identifier"")
    summary: str = field(description=""succinct summary"")
 
llm = chatopenai(model=""gpt-3.5-turbo"", temperature=0).with_structured_output(testsummary)
parser = pydanticoutputparser(pydantic_object=testsummary)
prompt = prompttemplate(
    template=""you are an ai summarizing long texts. text: {stmt}"",
    input_variables=[""stmt""]
)
runnable = prompt | llm | parser 
result = runnable.invoke({""stmt"": ""this is a really long piece of literature i'm too lazy to read""})

the error is
validationerror: 1 validation error for generation
text
  str type expected (type=type_error.str)

as discussed, if i omit the output parser, i get a dict:
runnable = prompt | llm #| parser 
result = runnable.invoke({""stmt"": ""this is a really long piece of literature i'm too lazy to read""})
type(result)
dict","['python', 'pydantic', 'langchain', 'large-language-model']",78592816,"output parsers in langchain receive a string, not structured data. they are used to do what you are already doing with with_structured_output, parse some input string into structured data, or possibly change its format.
from the documentation:

output parsers are classes that help structure language model responses. there are two main methods an output parser must implement:

""get format instructions"": a method which returns a string containing instructions for how the output of a language model should be formatted.
""parse"": a method which takes in a string (assumed to be the response from a language model) and parses it into some structure.


now you have the structured data, you just need to fill the model with it. 
runnable = prompt | llm
result_dict = runnable.invoke({""stmt"": ""this is a really long piece of literature i'm too lazy to read""})
result = testsummary.parse_obj(result_dict)",https://stackoverflow.com/questions/78591465,python,07-06-2024 10:53,4584.0,0.0,1.0,True,07-06-2024 15:27,07-06-2024 11:21
24647400,what is the best stemming method in python?,"i tried all the nltk methods for stemming but it gives me weird results with some words. 
examples
it often cut end of words when it shouldn't do it :

poodle => poodl
article articl

or doesn't stem very good : 

easily and easy are not stemmed in the same word
leaves, grows, fairly are not stemmed

do you know other stemming libs in python, or a good dictionary?
thank you","['python', 'nltk', 'stemming']",24648116,"python implementations of the porter, porter2, paice-husk, and lovins stemming algorithms for english are available in the stemming package",https://stackoverflow.com/questions/24647400,python,09-07-2014 07:12,93249.0,46.0,7.0,True,19-03-2022 23:36,09-07-2014 07:19
67378194,loss is nan when using keras bert for classification,"i'm using keras-bert for classification. on some datasets, it runs well and calculates the loss, while on others the loss is nan.
the different datasets are similar in that they are augmented versions of the original one. working with keras-bert, the original data and some augmented versions of the data run well while the other augmented versions of data don't run well.
when i use a regular one-layer bilstm on the augmented versions of data that don't run well with keras-bert, it works out fine which means i can rule out the possibility of the data being faulty or containing spurious values that may affect the way the loss is calculated.
the data in working with has three classes.
i'm using bert based uncased
!wget -q 

can anyone give me pointers as to why the loss is nan?
inputs = model.inputs[:2]
dense = model.layers[-3].output
outputs = keras.layers.dense(3, activation='sigmoid', kernel_initializer=keras.initializers.truncatednormal(stddev=0.02),name = 'real_output')(dense)
decay_steps, warmup_steps = calc_train_steps(train_y.shape[0], batch_size=batch_size,epochs=epochs,)
#(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=lr)
model = keras.models.model(inputs, outputs)
model.compile(adamwarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=lr), loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])
sess = tf.compat.v1.keras.backend.get_session()
uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.compat.v1.report_uninitialized_variables ())])
init_op = tf.compat.v1.variables_initializer([v for v in tf.compat.v1.global_variables() if v.name.split(':')[0] in uninitialized_variables])
sess.run(init_op)
model.fit(train_x,train_y,epochs=epochs,batch_size=batch_size)

 train on 20342 samples
epoch 1/10
20342/20342 [==============================] - 239s 12ms/sample - loss: nan - sparse_categorical_accuracy: 0.5572
epoch 2/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
epoch 3/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2081
epoch 4/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
epoch 5/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
epoch 6/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
epoch 7/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
epoch 8/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2081
epoch 9/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
epoch 10/10
20342/20342 [==============================] - 225s 11ms/sample - loss: nan - sparse_categorical_accuracy: 0.2082
<tensorflow.python.keras.callbacks.history at 0x7f1caf9b0f90>

also, i'm running this on google colab with tensorflow 2.3.0 and keras 2.4.3
update

i looked the data that was causing this issue again and i realised that one of the target labels were missing. i might have mistakenly edited it. once i fixed it, the loss is nan problem dissappeared. however, i'll be awarding the 50 points to the answer i got because it got me to think better about my code. thanks.","['tensorflow', 'keras', 'deep-learning', 'bert-language-model']",67449580,"i noticed one issue in your code but i'm not sure if this the main cause; better if you can possibly provide some reproducible code.
in your above code snippet, you set sigmoid in your last layer activation with unit < 1 which indicate the problem dataset is probably multi-label and that's why the loss function should be binary_crossentropy but you set sparse_categorical_crossentropy which is typical uses multi-class problem and with integer labels.
outputs = keras.layers.dense(3, activation='sigmoid',
                   kernel_initializer=keras.initializers.truncatednormal(stddev=0.02),
                   name = 'real_output')(dense)

model = keras.models.model(inputs, outputs)
model.compile(adamwarmup(decay_steps=decay_steps, 
                            warmup_steps=warmup_steps, lr=lr),
                 loss='sparse_categorical_crossentropy',
                 metrics=['sparse_categorical_accuracy'])
  

so, if your problem data set is a multi-label with the last layer unit = 3, then the set-up should be more like
outputs = keras.layers.dense(3, activation='sigmoid',
                   kernel_initializer=keras.initializers.truncatednormal(stddev=0.02),
                   name = 'real_output')(dense)
model.compile(adamwarmup(decay_steps=decay_steps, 
                                warmup_steps=warmup_steps, lr=lr),
                     loss='binary_crossentropy',
                     metrics=['accuracy'])

but if the problem set is a multi-class problem and your target labels are integer (unit = 3) then the set-up should more like as follows:
outputs = keras.layers.dense(3, activation='softmax',
                   kernel_initializer=keras.initializers.truncatednormal(stddev=0.02),
                   name = 'real_output')(dense)
model.compile(adamwarmup(decay_steps=decay_steps, 
                                warmup_steps=warmup_steps, lr=lr),
                     loss='sparse_categorical_crossentropy',
                     metrics=['sparse_categorical_accuracy'])",https://stackoverflow.com/questions/67378194,tensorflow,04-05-2021 02:53,664.0,1.0,1.0,True,12-05-2021 02:14,12-05-2021 02:14
34870614,what does tf.nn.embedding_lookup function do?,"tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=none)

i cannot understand the duty of this function. is it like a lookup table? which means to return the parameters corresponding to each id (in ids)?
for instance, in the skip-gram model if we use tf.nn.embedding_lookup(embeddings, train_inputs), then for each train_input it finds the correspond embedding?","['python', 'tensorflow', 'deep-learning', 'word-embedding', 'nlp']",34877590,"embedding_lookup function retrieves rows of the params tensor. the behavior is similar to using indexing with arrays in numpy. e.g.
matrix = np.random.random([1024, 64])  # 64-dimensional embeddings
ids = np.array([0, 5, 17, 33])
print matrix[ids]  # prints a matrix of shape [4, 64] 

params argument can be also a list of tensors in which case the ids will be distributed among the tensors. for example, given a list of 3 tensors [2, 64], the default behavior is that they will represent ids: [0, 3], [1, 4], [2, 5]. 
partition_strategy controls the way how the ids are distributed among the list. the partitioning is useful for larger scale problems when the matrix might be too large to keep in one piece.",https://stackoverflow.com/questions/34870614,python,19-01-2016 07:14,79200.0,174.0,9.0,True,26-08-2022 13:05,13-07-2018 12:19
67256474,lost git changes on local copy,"i cloned a repo, then made some changes on the local copy,
i wanted to upload to a new branch.
so i run the following commands:
git add .
git checkout -b new-branch
git add .
git stash
git push origin new-branch

all of the sudden ,my changes are gone , the repo has the same structure as i cloned it.
how to retrieve my changes??","['git', 'github', 'information-retrieval']",67256640,"it sounds like you stashed your changes, which puts them in a temporary stack data structure. you can get them back by using git stash apply, which applies the top of the stack back to your files.
see git help stash for more info. in particular, checkout git stash list and git stash show for commands to see what's in the stash stack.
tbh, something like sourcetree can also be nice for quickly glancing through the stash to see what's there.",https://stackoverflow.com/questions/67256474,git,25-04-2021 17:44,47.0,2.0,1.0,True,25-04-2021 18:00,25-04-2021 17:46
73879459,documenttermmatrix misses some words,"i am using documenttermmatrix to find a list of keywords in a long text. most of the words in my list are correctly found, but there are a couple that are missing. now, i would love to post here a minimal working example, but the problem is: there is one of the words (""insolvency"", so not a short word as in the problem here) in a document of 32 pages which is missed. now, this word is actually in page 7 of the text. but if i reduce my text with text <- text[7], then documenttermmatrix actually finds it! so i am not able to reproduce this with a minimal working example...
do you have any ideas?
below a sketch of my script:
library(fastpipe)
library(openxlsx)
library(tm)

`%>>%` <- fastpipe::`%>>%`

source(""cleantext.r"") # custom function to clean up the text from reports

keywords_xlsx <- read.xlsx(paste0(getwd(),""/keywords.xlsx""),
                           sheet = ""all"",
                           startrow = 1,
                           colnames = false,
                           skipemptyrows = true,
                           skipemptycols = true)

keywords <- keywords_xlsx[1] %>>%
  tolower(as.character(.[,1]))

# custom function to read pdfs
read <- readpdf(control = list(text = ""-layout""))

# extract text from pdf
report <- ""my_report.pdf""
document <- corpus(urisource(paste0(""./annual reports/"", report)), readercontrol = list(reader = read))
text <- content(document[[1]]) 

text <- cleantext(report, text) # this is a custom function to clean up the texts

# text <- text[7] # if i do this, my word is found! otherwise it is missed

# create a corpus  
text_corpus <- corpus(vectorsource(text))


matrix <- t(as.matrix(inspect(documenttermmatrix(text_corpus,
                                                 list(dictionary = keywords,
                                                      list(wordlengths=c(1, inf))
                                                 )
))))
  
  
words <- sort(rowsums(matrix),decreasing=true) 
df <- data.frame(word = names(words),freq=words)","['r', 'nlp', 'tm']",73881206,"the problem lies in your use of inspect. only use inspect to check if your code is working and to see if a dtm has any values. never use inspect inside functions / transformations, because inspect by default only shows the firs 10 rows and 10 columns of a document term matrix.
also if you want to transpose the outcome of a dtm, use termdocumentmatrix.
your last line should be:
mat <- as.matrix(termdocumentmatrix(text_corpus,
                                    list(dictionary = keywords,
                                         list(wordlengths=c(1, inf)))))

note that turning a dtm / tdm into a matrix will use a lot more memory than having the data inside a sparse matrix.",https://stackoverflow.com/questions/73879459,r,28-09-2022 09:52,38.0,1.0,1.0,True,28-09-2022 12:12,28-09-2022 10:13
75252308,why smote raise &quot;found input variables with inconsistent numbers of samples&quot;?,"i try to classify emotion from tweet with dataset of 4401 tweet, when i use smaller sample of data (around 15 tweet) everything just work fine, but when i use the full dataset it raise the error of
found input variables with inconsistent numbers of samples: [7, 3520]

the error happen when i try to oversampling the data using smote after transforming the data using countvectorizer.
this is the code where the error raise
# n-gram feature and term frequency
vectorizer = countvectorizer(ngram_range=(1,3))
x_train_tf = vectorizer.fit_transform(str(x_train).split('\n')).toarray()
x_test_tf = vectorizer.transform(str(x_test).split('\n')).toarray()
df_output = pd.dataframe(data =x_train_tf, columns = vectorizer.get_feature_names_out())
display(df_output)
# the print shape is (7 rows ï¿½ï¿½ 250 columns)

smote = smote(random_state=42, k_neighbors=5)
x_smote, y_smote = smote.fit_resample(x_train_tf, y_train)
print(""total train data smote : &;,x_smote.shape), print(""total train label smote : "",y_smote)

i did not understand why this is happening so some explanation could really help.
i already tried to solve it using answers from other similiar question but nothing have worked.
this is the full code
import nltk
import re
#nltk.download()
import string
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
from nltk import everygrams
from collections import counter
from sklearn import preprocessing
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from imblearn.over_sampling import smote
from sklearn.naive_bayes import gaussiannb
from sklearn.preprocessing import labelencoder
from sklearn.model_selection import train_test_split
from sastrawi.stemmer.stemmerfactory import stemmerfactory
from sklearn.feature_extraction.text import countvectorizer
from sklearn.feature_extraction.text import tfidfvectorizer
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix

dataset = pd.read_csv(""g:/ta/program/dataset/twitter_emotion_dataset.csv"", encoding='latin-1')
# preprocessing
dataset['case_folding_tweet'] = dataset['tweet'].str.casefold()
dataset['only_alphabet_tweet'] = [re.sub('[^a-za-z]+\s*', ' ', s) for s in dataset['case_folding_tweet']]
dataset['data_cleaning_tweet'] = dataset['only_alphabet_tweet'].str.replace(r'\b\w{1}\b','').str.replace(r'\s+', ' ')

slangword_dictionary = (""g:/ta/program/dataset/kamus_singkatan.csv"")

deslang = {}
list_slangword = open(slangword_dictionary).readlines()
for line in list_slangword:
    slang, unslang = line.strip().split(';')
    deslang[slang] = unslang
deslang[slang] = {r""\b{}\b"".format(k): v for k, v in deslang.items()}

dataset['data_cleaning_tweet'] = dataset['data_cleaning_tweet'].replace(deslang[slang], regex=true)
dataset['convert_slang_tweet'] = dataset['data_cleaning_tweet']

replace_dictionary = {'tidak ': 'tidak', 'bukan ': 'bukan', 'jangan ': 'jangan', 'belum ': 'belum'}
dataset['convert_negation_tweet'] = dataset['convert_slang_tweet'].replace(replace_dictionary, regex=true)
dataset['tokenization_tweet'] = dataset['convert_negation_tweet'].apply(word_tokenize) 
list_stopwords = set(stopwords.words(""indonesian""))
list_stopwords.add('username')
list_stopwords.add('url')
dataset['stopword_removal_tweet'] = dataset['tokenization_tweet'].apply(lambda x: [item for item in x if item not in list_stopwords])

factory = stemmerfactory()
stemmer = factory.create_stemmer()
dataset['stemmed_tweet'] = dataset['stopword_removal_tweet'].apply(lambda x: [stemmer.stem(y) for y in x]) 

# split data
x = dataset[""stemmed_tweet""].values
y = dataset[""label""].values
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state= 42)

# get n-gram and tf
vectorizer = countvectorizer(ngram_range=(1,3))
x_train_tf = vectorizer.fit_transform(str(x_train).split('\n')).toarray()
x_test_tf = vectorizer.transform(str(x_test).split('\n')).toarray()

# oversampling
smote = smote(random_state=42, k_neighbors=5)
x_smote, y_smote = smote.fit_resample(x_train_tf, y_train)

print(""total train data smote : "",x_smote.shape), print(""total train label smote : "",y_smote)

gnb_classifier = gaussiannb()
gnb_classifier.fit(x_smote, y_smote)
print(gnb_classifier)
y_pred = gnb_classifier.predict(x_test_tf)
print(""emotion predicted :"", y_pred)

link to the dataset","['python', 'machine-learning', 'classification', 'text-classification', 'countvectorizer']",75304406,"i fix the problem using the answer from this post answer
by joining all the train data column before vectorizing.
df_train = pd.dataframe(data=x_train)
df_test = pd.dataframe(data=x_test)

series = pd.series(df_train['stemmed_tweet'])
corpus = series.apply(lambda series: ' '.join(series))
vectorizer = countvectorizer(ngram_range=(1,3), lowercase=false)
x_train_tf = vectorizer.fit_transform(corpus).toarray()
x_test_tf = vectorizer.transform(str(df_test.values).split(""\n"")).toarray()",https://stackoverflow.com/questions/75252308,python,26-01-2023 22:30,143.0,2.0,2.0,True,01-02-2023 00:09,01-02-2023 00:02
77651770,how to load the pre-trained word embeddings in .npy files,"i'm trying to use the word embeddings pre-trained in histwords project by the stanford nlp team. but when i ran the document example.py from the github website, there was an error: modulenotfounderror: no module named 'representations.sequentialembedding'.
how can i solve this problem?
i've installed the ""representations"" module, but it doesn't work. the pre-trained word embeddings are of "".npy"" format, is there any python-based method for uploading them?","['python', 'nlp', 'word-embedding']",77654014,"to set it up you can do the following in your shell:
cd <path you want to store your project>
git clone 
cd histwords

# ----- set up python2.7 -----
## python2.7 via conda is quite easy
conda create -y -n <env_name> python=2.7
conda activate <env_name>

## else install/locate a python 2 version
### check python -v (maybe its python2.7)
### maybe you have python2 or python2.7 executables
### on linux you could look for ls /usr/bin/python* 
python2 -m venv <env_name>
source <env_name>/bin/activate # if you use linux
bin/scripts/activate # on windows

# ---- now with python2.7 ----
pip install -r requirements.txt

# ---- download & move your embedding from 
# to embeddings/<category> subfolders

python examples.py
# outputs the similarities",https://stackoverflow.com/questions/77651770,python,13-12-2023 07:34,368.0,0.0,1.0,True,13-12-2023 13:37,13-12-2023 12:38
76104878,whisper api from a recorded audio blob,"i am creating a transcriber using openai whisper api in nodejs and react. i want the user to be able to record an audio file in the browser and transcribe their recording. i am doing this by saving the buffer data of the audio blob they have recorded into an mp3 file, then using the createtranscription() api call i input the fs.createreadstream(recorded_audio_file.mp3) which outputs a 400 error. when i record an audio file using the windows recorder and input that file the api call works just fin.  here is my recorder component in react
import react, { usestate, useeffect, useref } from ""react"";

import microphone from ""./microphone/microphone"";
const tsst = () => {
  const base_url = process.env.react_app_server_url || ""

  const mediarecorder = useref(null);
  const [stream, setstream] = usestate(null);
  const [audiochunks, setaudiochunks] = usestate([]);
  const [audio, setaudio] = usestate(null);
  const [audiofile, setaudiofile] = usestate(null);
  const [transcribtion, settranscription] = usestate("""");
  const [audioblob, setaudioblob] = usestate("""");
  const [audiobuffer, setaudiobuffer] = usestate("""");

  useeffect(() => {
    const initializemediarecorder = async () => {
      if (""mediarecorder"" in window) {
        try {
            const streamdata = await navigator.mediadevices.getusermedia({ audio: true });
            setstream(streamdata);
        } catch (err) {
            console.log(err.message);
        }
      } else {
          console.log(""the mediarecorder api is not supported in your browser."");
      }
    }

    initializemediarecorder();
  }, [])

  const handlestartrecording = () => {
    const media = new mediarecorder(stream, { type: ""audio/mp3"" });

    mediarecorder.current = media;
    mediarecorder.current.start();

    let chunks = [];
    mediarecorder.current.ondataavailable = (e) => {
       chunks.push(e.data);
    };
    setaudiochunks(chunks);
  }
  const handlestoprecording = () => {
    mediarecorder.current.stop();
    mediarecorder.current.onstop = () => {
      const audioblob = new blob(audiochunks, { type: ""audio/mp3"" });
      const audiourl = url.createobjecturl(audioblob);

      setaudioblob(audioblob)
      setaudio(audiourl);
      setaudiochunks([]);

      let file = new file([audiourl], ""recorded_audio.mp3"",{type:""audio/mp3"", lastmodified:new date().gettime()});
      let container = new datatransfer();
      container.items.add(file);
      document.getelementbyid(""audiofile"").files = container.files;
      setaudiofile(container.files[0]);

      console.log(file);
    };
  }

  const handlesubmitrecording = async () => {
    try {
      // assuming you have an audio blob called 'audioblob'

      // convert the audio blob to a base64 string
      const reader = new filereader();
      reader.onloadend = async () => {
        const base64string = reader.result.split(',')[1]; // extract base64 data from the result
        const res = await fetch(`${base_url}/api/openai/transcriber`, {
          method: ""post"",
          headers: {
            ""content-type"": ""application/json"",
          },
          body: json.stringify({ audiobuffer: base64string, lang: ""en"" })
        })
        const data = await res.json();
        settranscription(data);
      };
      reader.readasdataurl(audioblob);

    } catch (error) {
      console.log(error);

    } finally {
    }
  }

    return (
      <div classname=""h-[calc(100vh-73px)] flex justify-center items-center"">
        <div classname=""w-[40%] flex justify-between items-center"">
          <div classname=""flex flex-col"">
            <microphone startfunction={ handlestartrecording } stopfunction={ handlestoprecording } />
            <button onclick={handlestartrecording} classname=""w-fit my-10 p-5 bg-gray-200 rounded-lg"">start recording</button>
            <button onclick={handlestoprecording} classname=""w-fit mb-10 p-5 bg-gray-200 rounded-lg"">stop recording</button>

            <audio classname=""mb-10"" src={audio && audio} controls></audio>
            <input id=""audiofile"" type=""file"" onchange={ (e) => {setaudiofile(e.target.files[0])}}/>
          </div>
          
          <div>
            <button classname=""p-10 bg-yellow-500 rounded-xl"" onclick={ handlesubmitrecording } >submit</button>
          </div>
        </div>

        <div classname=""w-[40%] flex justify-center items-center"">
          <textarea value={transcribtion} readonly classname=""w-[60%] aspect-square resize-none shadow-lg shadow-black""></textarea>
        </div>
      </div>
    );
};
export default tsst;

here is the api:
export const transcribe = async (req, res) => {
    // const lang = json.parse(req.body.json).lang;
    // const audiobuffer = req.file;
    const { audiobuffer, lang} = req.body;

    const audiobufferbase64 = buffer.from(audiobuffer, 'base64');

    const filename = ""test.mp3"";
    const foldername = `./audio/${filename}`

    const writablestream = fs.createwritestream(foldername); // replace with your desired file path and extension
    writablestream.write(audiobufferbase64);

    const readstream = fs.createreadstream(foldername);

    readstream.on('data', (data) => {
        console.log('read stream data:', data);
    });

    try {
        const whisperres = await openai.createtranscription(
            readstream,
            ""whisper-1"",
        )

        const chatresponse = whisperres.data.text;
        console.log(chatresponse)

        res.status(200).json({ chatresponse: chatresponse });
    } catch (error) {
        //console.log(error);
        res.status(500).json({ message: error });
    }
}

and here is the server call:
import express from ""express"";
import cors from ""cors"";
import * as dotenv from ""dotenv"";
import mongoose from ""mongoose"";
import multer from ""multer"";

import { dalle, chatgpt, summarize, translate, transcribe } from ""./api/openai.js"";
import { getimages, postimage } from ""./api/imageshowcase.js"";
import { login, signup } from ""./api/user.js"";

dotenv.config();

const app = express();
const upload = multer();
const storage = multer.memorystorage();
const uploadmiddleware = multer({ storage: storage });

app.use(cors());
app.use(express.json({limit: '50mb'}));

const atlasurl = process.env.mongodb_url;    
const port = process.env.port || 5000;

mongoose.connect(atlasurl)
    .then(() => app.listen(port, () => console.log(`successfully connected to port ${port}`)))
    .catch(error => console.log(""there was an error: "", error));

app.get(""/"", async (req, res) => {
    res.send(""server is running"");
})

app.post(""/api/openai/transcriber"",(req, res) => transcribe(req, res));


the saved mp3 file is working just fine.
the apikey is correct.
when i record my own mp3 using windows recorder and use the createreadstream of that it works just fine.
the saved file data is a buffer of the form 
i tried changing the way i save a file, using different formatting methods for the buffer, binary hex, base64. tried uploading the buffer directly to whisper api. tried using axios to post to the api url directly. tried making a promise out of the saving of the mp3 file and then createreadstream and a lot of other little changes. tried to make a readable out of the buffer directly. i view all the similar questions with answers with no avail.","['node.js', 'reactjs', 'openai-api', 'recorder', 'openai-whisper']",76499252,"just call transcribeaudio function in your try, catch of transcribe function.
also, make sure you are able to create the .mp3 file locally and try to play it. sometimes, the audio file is not correct which causes problems while executing the code.
try {
        const whisperres = await transcribeaudio(readstream);

        const chatresponse = whisperres.data.text;
        console.log(chatresponse)

        res.status(200).json({ chatresponse: chatresponse });
    } catch (error) {
        //console.log(error);
        res.status(500).json({ message: error });
    }

import formdata from ""form-data"";
import axios from 'axios'

const transcribeaudio = async (file) => {
  let data = new formdata();

  data.append(""file"", fs.createreadstream(file));
  data.append(""model"", ""whisper-1"");
  data.append(""language"", ""en"");

  let config = {
    method: ""post"",
    maxbodylength: infinity,
    url: ""
    headers: {
      authorization:
        `bearer ${process.env.openai_api_key}`,
      ""content-type"": ""multipart/form-data"",
      ...data.getheaders(),
    },
    data: data,
  };

  try {
    const response = await axios.request(config);
    const data = response.data;

    return { data };
  } catch (error) {
    return {};
  }
};",https://stackoverflow.com/questions/76104878,node.js,25-04-2023 19:46,4300.0,5.0,1.0,True,18-06-2023 06:09,08-05-2023 15:58
72700395,&#39;meanembeddingvectorizer&#39; object has no attribute &#39;transform&#39;,"hello i'm working with text classification.
i've a dataset with 2 columns one made of text and the other one is the label.
since i'm a beginner i'm following step by step a tutorial on w2vec trying to understand if it can work for my usecase but i keep getting this error.
this is my code
class meanembeddingvectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        self.dim = len(next(iter(word2vec.values())))
def fit(self, x, y):
        return self
def transform(self, x):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for words in x
        ])

train_df['clean_text_tok']=[nltk.word_tokenize(i) for i in train_df['clean_text']]
model = word2vec(train_df['clean_text_tok'],min_count=1)
w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))
modelw = meanembeddingvectorizer(w2v)
# converting text to numerical data using word2vec
x_train_vectors_w2v = modelw.transform(x_train_tok)
x_val_vectors_w2v = modelw.transform(x_test_tok)

the error i'm getting is :
dimension:  100
---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-127-289141692350> in <module>
      4 modelw = meanembeddingvectorizer(w2v)
      5 # converting text to numerical data using word2vec
----> 6 x_train_vectors_w2v = modelw.transform(x_train_tok)
      7 x_val_vectors_w2v = modelw.transform(x_test_tok)

attributeerror: 'meanembeddingvectorizer' object has no attribute 'transform'","['python', 'nlp', 'word2vec']",72705368,"if your meanembeddingvectorizer is defined in your code exactly as its shows here, the failure-to-indent the .fit() and .transform() functions means they're not part of the class, as you likely intended.
indenting those each an extra 4 spaces ï¿½ï¿½ï¿½ as was likely the intent of any source you copied this code from! ï¿½ï¿½ï¿½ will put them ""inside"" the meanembeddingvectorizer class, as class methods. then, objects of that class won't give the same ""no attribute"" error.
for example:""lang-py prettyprint-override"">class meanembeddingvectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        self.dim = len(next(iter(word2vec.values())))
    def fit(self, x, y):
        return self
    def transform(self, x):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for words in x
        ])",https://stackoverflow.com/questions/72700395,python,21-06-2022 12:00,1133.0,0.0,1.0,True,21-06-2022 18:34,21-06-2022 17:29
61826824,can you train a bert model from scratch with task specific architecture?,"bert pre-training of the base-model is done by a language modeling approach, where we mask certain percent of tokens in a sentence, and we make the model learn those missing mask. then, i think in order to do downstream tasks, we add a newly initialized layer and we fine-tune the model.
however, suppose we have a gigantic dataset for sentence classification. theoretically, can we initialize the bert base architecture from scratch, train both the additional downstream task specific layer + the base model weights form scratch with this sentence classification dataset only, and still achieve a good result?","['machine-learning', 'nlp', 'bert-language-model']",61839719,"bert can be viewed as a language encoder, which is trained on a humongous amount of data to learn the language well. as we know, the original bert model was trained on the entire english wikipedia and book corpus, which sums to 3,300m words. bert-base has 109m model parameters. so, if you think you have large enough data to train bert, then the answer to your question is yes. 
however, when you said ""still achieve a good result"", i assume you are comparing against the original bert model. in that case, the answer lies in the size of the training data.
i am wondering why do you prefer to train bert from scratch instead of fine-tuning it? is it because you are afraid of the domain adaptation issue? if not, pre-trained bert is perhaps a better starting point.
please note, if you want to train bert from scratch, you may consider a smaller architecture. you may find the following papers useful.

well-read students learn better: on the importance of pre-training compact models
albert: a lite bert for self-supervised learning of language representations",https://stackoverflow.com/questions/61826824,machine-learning,15-05-2020 19:21,6813.0,6.0,2.0,True,01-02-2024 15:10,01-02-2024 15:10
77107744,use nlp (nltk) to identify groups of phrases in a python dataframe,"i have a table containing diagnosis information for a large group of patients. i would like to determine what the most common groupings of those diagnoses are, for example is it ""bloaty head syndrome"" and ""slack tongue"", or ""broken wind"", ""chronic nosehair"" and ""corrugated ankles""... or some other combination.
data is structured like so:
import pandas as pd
import numpy as np

# list of ids
ids = ['id1', 'id2', 'id3','id4','id5'] 

# list of sample sentences 
diagnosis = [""broken wind"",""chronic nosehair"",""corrugated ankles"",""discrete itching""]

# create dataframe
df = pd.dataframe({'id': ids})

# generate list of sentences for each id
df['diagnosis'] = df['id'].apply(lambda x: np.random.choice(diagnosis, 5).tolist())

# explode into separate rows
df = df.explode('diagnosis')

print(df)

for example if both id2 and id5 contain ""broken wind"" and chronic nosehair"" that would be 2 of that combination. if id1, id3 and id4 contain ""chronic nosehair"",""corrugated ankles"", and ""discrete itching"" that would be 3 of that combination.
with the goal of determining which combination is most common.
i'm wondering is there an nlp library such as nltk, or a method, that can be used to process data stored like this in a pandas dataframe? most of what i have been able to find so far is geared toward sentiment analysis or analyzing single words as opposed to phrases...","['python', 'pandas', 'machine-learning', 'nlp', 'nltk']",77112903,"i would offer that what you are trying to do here is not necessarily an nlp problem, but a much more general frequent pattern mining problem which is typically seen in recommendation.
you can find the most frequent diagnosis combinations of any size by using the fpgrowth algorithm in the mlxtend library and looking at the support for each symptom or combinations thereof:
import pandas as pd
from mlxtend.preprocessing import transactionencoder
from mlxtend.frequent_patterns import fpgrowth

# create list of diagnoses for each patient
x = df.groupby('id').apply(lambda x:list(x['diagnosis']))

# encode to wide dataframe with column for each symptom
te = transactionencoder()
te_ary = te.fit(x).transform(x)
te_df = pd.dataframe(te_ary, columns=te.columns_)

# calculate most frequent diagnosis co-occurrences
fp_df = fpgrowth(te_df, min_support=0.01, use_colnames=true)

# sort and show
fp_df.sort_values(by='support', ascending=false)

the resultant table is a list of tuples with the support, the percentage of ""transactions"" (here, patients) for which the combinations occur:
| support | itemsets                                                 |
| ------- | -------------------------------------------------------- |
| 0.8     | {'broken wind'}                                          |
| 0.6     | {'corrugated ankles'}                                    |
| 0.6     | {'chronic nosehair'}                                     |
| 0.6     | {'discrete itching'}                                     |
| 0.6     | {'corrugated ankles', 'broken wind'}                     |
| 0.4     | {'chronic nosehair', 'broken wind'}                      |
| 0.4     | {'discrete itching', 'chronic nosehair'}                 |
| 0.4     | {'discrete itching', 'broken wind'}                      |
| 0.2     | {'corrugated ankles', 'discrete itching'}                |
| 0.2     | {'discrete itching', 'corrugated ankles', 'broken wind'} |
| 0.2     | {'corrugated ankles', 'chronic nosehair'}                |
| 0.2     | {'chronic nosehair', 'discrete itching', 'broken wind'}  |
| 0.2     | {'chronic nosehair', 'corrugated ankles', 'broken wind'} |",https://stackoverflow.com/questions/77107744,python,14-09-2023 19:04,333.0,1.0,1.0,True,15-09-2023 15:27,15-09-2023 15:27
76752935,error while peforming tf-idfvectorizer() on the training values,"#model creation

x = df.drop(columns='v1', axis=1)
y = df['v1']

from sklearn.feature_extraction.text import tfidfvectorizer


x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)


pipe_gnb = pipeline([
    ('vect', tfidfvectorizer()),
    ('gnb', gaussiannb())
])


params_gnb = {
        # 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],
        # 'vect__max_df': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
        # 'vect__min_df': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
        # 'vect__max_features': [3000, 4000, 5000, none],
        # 'vect__binary': [true, false],
        'vect__sublinear_tf': [true, false]
}

gs_gnb = gridsearchcv(pipe_gnb, params_gnb, verbose=10, cv=5, n_jobs=-1, scoring='accuracy')
gs_gnb.fit(x_train, y_train)

i have been trying to do tfidfvectorizeration on 'v2' column, that is the messages column in the attached dataset but i have been getting the below error(mainly an error to convert the fiited ""x, y(train values)"" to a 'dense array').
code link->
dataset link->

error log->
fitting 5 folds for each of 2 candidates, totalling 10 fits
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
<ipython-input-23-f27f59ee7f63> in <cell line: 45>()
     43 
     44 gs_gnb = gridsearchcv(pipe_gnb, params_gnb, verbose=10, cv=5, n_jobs=-1, scoring='accuracy')
---> 45 gs_gnb.fit(x, y)
     46 
     47 print('best accuracy: ', gs_gnb.best_score_, end='\n')

3 frames
/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py in _warn_or_raise_about_fit_failures(results, error_score)
    365                 f""below are more details about the failures:\n{fit_errors_summary}""
    366             )
--> 367             raise valueerror(all_fits_failed_message)
    368 
    369         else:

valueerror: 
all the 10 fits failed.
it is very likely that your model is misconfigured.
you can try to debug the error by setting error_score='raise'.

below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
traceback (most recent call last):
  file ""/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py"", line 686, in _fit_and_score
    estimator.fit(x_train, y_train, **fit_params)
  file ""/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py"", line 405, in fit
    self._final_estimator.fit(xt, y, **fit_params_last_step)
  file ""/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py"", line 267, in fit
    return self._partial_fit(
  file ""/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py"", line 428, in _partial_fit
    x, y = self._validate_data(x, y, reset=first_call)
  file ""/usr/local/lib/python3.10/dist-packages/sklearn/base.py"", line 584, in _validate_data
    x, y = check_x_y(x, y, **check_params)
  file ""/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py"", line 1106, in check_x_y
    x = check_array(
  file ""/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py"", line 845, in check_array
    array = _ensure_sparse_format(
  file ""/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py"", line 522, in _ensure_sparse_format
    raise typeerror(
typeerror: a sparse matrix was passed, but dense data is required. use x.toarray() to convert to a dense numpy array.","['python', 'machine-learning', 'scikit-learn', 'nlp']",76755926,"you need to add a specific step after the ""tfidfvectorizer"" because the output is a sparse matrix. you can create a densetransformer from transformermixin and add it in the pipeline :
import numpy as np
from sklearn.base import transformermixin

class densetransformer(transformermixin):
    def fit(self, x, y=none, **fit_params):
        return self

    def transform(self, x, y=none, **fit_params):
        return np.array(x.todense())

you need to make two modifications in your code. first, you need to select only the ""v2"" as feature :
x = df['v2']
y = df['v1']

and you need to modify the pipeline :
pipe_gnb = pipeline([
    ('vect', tfidfvectorizer()),
    ('to_dense', densetransformer()), 
    ('gnb', gaussiannb()),
])",https://stackoverflow.com/questions/76752935,python,24-07-2023 09:07,51.0,0.0,1.0,True,24-07-2023 15:24,24-07-2023 10:41
70251993,invalid_argument: assertion failed: [predictions must be &lt;= 1] [condition x &lt;= y did not hold element-wise:],"i have the following model, and i want to make use of the standard metric functions to report on true/false positives, and true/false negatives.
from transformers import tfrobertaforsequenceclassification

model = tfrobertaforsequenceclassification.from_pretrained('roberta-base', num_labels=1)

optimizer = tf.keras.optimizers.adam(learning_rate=5e-5)
model.compile(
    optimizer=optimizer, 
    loss=tf.keras.losses.binarycrossentropy(from_logits=false),
    metrics = [
      'accuracy',
      tf.keras.metrics.truepositives(),
      tf.keras.metrics.truenegatives(),
      tf.keras.metrics.falsenegatives(),
      tf.keras.metrics.falsepositives()
    ]) # can also use any keras loss fn
history = model.fit(train_dataset.shuffle(1000).batch(16), epochs=10, batch_size=16, validation_data = test_dataset.batch(1))

but i am getting the following error, and not sure how to troubleshoot. how can it be that some predictions are greater than 1?
invalid_argument:  assertion failed: [predictions must be <= 1] [condition x <= y did not hold element-wise:] [x (tf_roberta_for_sequence_classification_5/classifier/out_proj/biasadd:0) = ] [[0.375979185][0.340960771][0.41201663]...] [y (cast_9/x:0) = ] [1]
     [[node assert_less_equal/assert/assertguard/assert
 (defined at /usr/local/lib/python3.7/dist-packages/keras/utils/metrics_utils.py:615)","['python', 'tensorflow', 'keras', 'deep-learning', 'nlp']",70257476,"this a known issue with these metrics due to their predefined thresholds and the fact that y_pred is not being squished between 0 and 1. check out this issue for more information. here is a simple working example based on the workaround posted in the linked issue.
from transformers import robertatokenizer, tfrobertaforsequenceclassification
import tensorflow as tf
import pandas as pd

class truepositives(tf.keras.metrics.truepositives):
    def __init__(self, from_logits=false, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._from_logits = from_logits

    def update_state(self, y_true, y_pred, sample_weight=none):
        if self._from_logits:
            super(truepositives, self).update_state(y_true, tf.nn.sigmoid(y_pred), sample_weight)
        else:
            super(truepositives, self).update_state(y_true, y_pred, sample_weight)


class falsepositives(tf.keras.metrics.falsepositives):
    def __init__(self, from_logits=false, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._from_logits = from_logits

    def update_state(self, y_true, y_pred, sample_weight=none):
        if self._from_logits:
            super(falsepositives, self).update_state(y_true, tf.nn.sigmoid(y_pred), sample_weight)
        else:
            super(falsepositives, self).update_state(y_true, y_pred, sample_weight)


class truenegatives(tf.keras.metrics.truenegatives):
    def __init__(self, from_logits=false, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._from_logits = from_logits

    def update_state(self, y_true, y_pred, sample_weight=none):
        if self._from_logits:
            super(truenegatives, self).update_state(y_true, tf.nn.sigmoid(y_pred), sample_weight)
        else:
            super(truenegatives, self).update_state(y_true, y_pred, sample_weight)


class falsenegatives(tf.keras.metrics.falsenegatives):
    def __init__(self, from_logits=false, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._from_logits = from_logits

    def update_state(self, y_true, y_pred, sample_weight=none):
        if self._from_logits:
            super(falsenegatives, self).update_state(y_true, tf.nn.sigmoid(y_pred), sample_weight)
        else:
            super(falsenegatives, self).update_state(y_true, y_pred, sample_weight)


d = {'text': ['you are fishy', 'fishy people are fishy'], 'label': [1, 0]}
train = pd.dataframe(data=d)
train_text = list(train['text'].values)
train_label = list(train['label'].values)

val = pd.dataframe(data=d)
val_text = list(val['text'].values)
val_label = list(val['label'].values)

tokenizer = robertatokenizer.from_pretrained('roberta-base')
model = tfrobertaforsequenceclassification.from_pretrained('roberta-base')

train_encodings = tokenizer(train_text, truncation=true, padding=true)
val_encodings = tokenizer(val_text, truncation=true, padding=true)

train_dataset = tf.data.dataset.from_tensor_slices((
    dict(train_encodings),
    train_label
))
val_dataset = tf.data.dataset.from_tensor_slices((
    dict(val_encodings),
    val_label
))
model = tfrobertaforsequenceclassification.from_pretrained('roberta-base', num_labels=1)

optimizer = tf.keras.optimizers.adam(learning_rate=5e-5)
model.compile(
    optimizer=optimizer, 
    loss=tf.keras.losses.binarycrossentropy(from_logits=false),
    metrics = [
      'accuracy',
      truepositives(from_logits=true),
      truenegatives(from_logits=true),
      falsenegatives(from_logits=true),
      falsepositives(from_logits=true)
    ]) # can also use any keras loss fn
history = model.fit(train_dataset.shuffle(2).batch(1), epochs=2, validation_data = val_dataset.batch(1))

epoch 1/2
2/2 [==============================] - 81s 6s/step - loss: 7.7125 - accuracy: 0.5000 - true_positives_16: 0.0000e+00 - true_negatives_15: 1.0000 - false_negatives_15: 1.0000 - false_positives_15: 0.0000e+00 - val_loss: 7.7125 - val_accuracy: 0.5000 - val_true_positives_16: 0.0000e+00 - val_true_negatives_15: 1.0000 - val_false_negatives_15: 1.0000 - val_false_positives_15: 0.0000e+00
epoch 2/2
2/2 [==============================] - 3s 1s/step - loss: 7.7125 - accuracy: 0.5000 - true_positives_16: 0.0000e+00 - true_negatives_15: 1.0000 - false_negatives_15: 1.0000 - false_positives_15: 0.0000e+00 - val_loss: 7.7125 - val_accuracy: 0.5000 - val_true_positives_16: 0.0000e+00 - val_true_negatives_15: 1.0000 - val_false_negatives_15: 1.0000 - val_false_positives_15: 0.0000e+00",https://stackoverflow.com/questions/70251993,python,06-12-2021 21:10,1242.0,1.0,1.0,True,08-12-2021 08:29,08-12-2021 08:29
78122293,can my code be wrapped and served in databricks model serving?,"import mlflow.pyfunc
import mlflow
from langchain.agents import create_sql_agent
from langchain.agents.agent_toolkits import sqldatabasetoolkit
from langchain.sql_database import sqldatabase
from langchain import openai

llm = openai(temperature=0)

class ucbot():

    def __init__(self, llm):
        self.llm = llm
        self.toolkit = sqldatabasetoolkit(db=sqldatabase.from_databricks(catalog=""samples"", schema=""nyctaxi""), llm=llm)
        self.agent = create_sql_agent(llm=self.llm, toolkit=self.toolkit, verbose=true, top_k=1)

    def get_answer(self, question):
        return self.agent.run(question)
    
class mlflowucbot(mlflow.pyfunc.pythonmodel):
    def __init__(self, llm):
        self.llm = llm

    def predict(self, context, input):
        ucbot = ucbot(self.llm)
        return ucbot.get_answer(input)


# persist model to mlflow
with mlflow.start_run():
    mlflow.pyfunc.log_model(
        python_model=mlflowucbot(llm),
        extra_pip_requirements=['langchain', 'databricks-sql-connector', 'sqlalchemy', 'openai'],
        artifact_path='model',
        registered_model_name=""my_model"",
        input_example={""input"":""how many tables?""}
    )


the code is able to create a model and predict
when i try to create a model serve i get this error:
an error occurred while loading the model. no module named 'openai'
after adding openai in dependency i get the following error
an error occurred while loading the model. no module named 'openai.api_resources'","['python-3.x', 'databricks', 'openai-api', 'mlflow', 'databricks-unity-catalog']",78147915,"installed openai==0.27.8 and it is working now
also added this code
conda_env = mlflow.pyfunc.get_default_conda_env()

# define packages required by model
packages = ['langchain', 'langchain_community', 'databricks-sql-connector', 'sqlalchemy', 'openai==0.27.8']

# add required packages to environment configuration
conda_env['dependencies'][-1]['pip'] += packages

# persist model to mlflow
with mlflow.start_run():
    mlflow.pyfunc.log_model(
        python_model=mlflowucbot(llm),
        artifact_path='model',
        conda_env=conda_env,
        registered_model_name=""my_model"",
        signature=signature
    )",https://stackoverflow.com/questions/78122293,python-3.x,07-03-2024 14:54,170.0,0.0,1.0,True,12-03-2024 14:27,07-03-2024 15:21
76749071,openai chat completions api error in wordpress php code: &quot;invalid url (post /v4/engines/davinci-codex/completions)&quot;,"i've asked chatgpt to generate a code to be incorporated in wordpress function.php to generate tags automatically for all my newly created or updated posts.
here is the code:
function add_tags_with_gpt($post_id) {
    // vï¿½ï¿½rifier si le contenu de l'article a rï¿½ï¿½ellement changï¿½ï¿½
    $post = get_post($post_id);
    $old_content = get_post_meta($post_id, '_old_content', true);
    $new_content = $post->post_content;
    if ($old_content === $new_content) {
        return; // le contenu n'a pas changï¿½ï¿½, donc on ne fait rien
    }
    update_post_meta($post_id, '_old_content', $new_content);

    // utiliser l'api openai pour gï¿½ï¿½nï¿½ï¿½rer des tags
    $ch = curl_init();
    curl_setopt($ch, curlopt_url, '
    curl_setopt($ch, curlopt_returntransfer, 1);
    curl_setopt($ch, curlopt_post, 1);
    curl_setopt($ch, curlopt_postfields, json_encode(array(
        'prompt' => 'generate tags forllowing content: '.$new_content,
        'max_tokens' => 60
    )));
    curl_setopt($ch, curlopt_ array(
        'authorization: bearer your_openai_api_key', // remplacez 'your_openai_api_key' par votre vï¿½ï¿½ritable clï¿½ï¿½ api
        'content-type: application/json'
    ));
    $response = curl_exec($ch);
    if (!$response) {
        // enregistrer l'erreur dans le fichier de log de dï¿½ï¿½bogage de wordpress
        error_log('erreur lors de la gï¿½ï¿½nï¿½ï¿½ration de tags: ' . curl_error($ch));
        return;
    }
    curl_close($ch);

    // enregistrer la rï¿½ï¿½ponse de l'api dans le fichier de log de dï¿½ï¿½bogage de wordpress
    error_log('rï¿½ï¿½ponse de l\'api openai : ' . $response);

    $response_data = json_decode($response, true);
    if(!isset($response_data['choices'][0]['text'])) {
        error_log('erreur: la rï¿½ï¿½ponse de l\'api ne contient pas de tags');
        return;
    }
    $tags = explode(',', $response_data['choices'][0]['text']);
    $tags = array_slice(toyer les tags
    $tags = array_map('sanitize_text_field', $tags);
    $tags = array_map('wp_strip_all_tags', $tags);
    $tags = array_filter($tags, function($tag) {
        return strlen($tag) > 2 && strlen($tag) <= 20; // exclure les tags de moins de 3 caractï¿½ï¿½res et de plus de 20 caractï¿½ï¿½res
    });

    // ajouter les tags ï¿½ï¿½ l'article
    wp_set_post_tags($post_id, $tags, true);
}
add_action('save_post', 'add_tags_with_gpt');

the problem is that the code doesn't work and returns this error message in debug.log:
[23-jul-2023 15:16:27 utc] rï¿½ï¿½ponse de l'api openai : {
  ""error"": {
    ""message"": ""invalid url (post /v4/engines/davinci-codex/completions)"",
    ""type"": ""invalid_request_error"",
    ""param"": null,
    ""code"": null
  }
}

[23-jul-2023 15:16:27 utc] erreur: la rï¿½ï¿½ponse de l'api ne contient pas de tags
[23-jul-2023 15:16:34 utc] php deprecated:  strnull to parameter #3 ($subject) of type array|string is deprecated in /home/***/webapps/***/wp-includes/formatting.php on line 4303

any idea what the issue is and how to fix it? the problem doesn't come from the api key that i pass from my openai account into the code.
many thanks!","['php', 'openai-api', 'chatgpt-api']",76757100,"all engines api endpoints are deprecated.

use the chat completions api endpoint.
change the url from this...


...to this.


there are 4 required parameters for the chat completions api:

model
messages

role
content



see my past answer for a working example in php using the gpt-3.5-turbo model.",https://stackoverflow.com/questions/76749071,php,23-07-2023 15:36,319.0,0.0,1.0,True,12-06-2024 16:52,12-06-2024 16:52
73112216,apply name-entity recognition on specific dataframe columns,"i have the following dataframe:
df = pd.dataframe({'source': ['paul', 'paul'],
                   'target': ['google', 'ferrari'],
                   'edge': ['works at', 'drive']
                   })

df
    source  target  edge
0   paul    google  works at
1   paul    ferrari drive

i want to apply name-entity recognition(ner) on the columns.
expected outcome:
    source  target        edge
0   person  organization  works at
1   person  car           drive

i tried the following function:
!python -m spacy download en_core_web_sm

import spacy
nlp = spacy.load('en_core_web_sm')

def ner(df):
    df['source_entities'] = df['source'].apply(lambda x: nlp(x).label_)
    df['target_entities'] = df['target'].apply(lambda x: nlp(x).label_)
    return df

but when i call the function ner(df) i get back an error:
attributeerror: 'spacy.tokens.doc.doc' object has no attribute 'label_'

any ideas on how to reach the expected outcome?","['python', 'python-3.x', 'dataframe', 'nlp', 'named-entity-recognition']",73142098,"you are trying to get label_ attribute from list as nlp(x) return list of object. because of which you are getting that error.
replace
def ner(df):
  df['source_entities'] = df['source'].apply(lambda x: nlp(x).label_)
  df['target_entities'] = df['target'].apply(lambda x: nlp(x).label_)
  return df

with
def ner(df):
  df['source_entities'] = df['source'].apply(lambda x: [ent.label_ for ent in nlp(x).ents])
  df['target_entities'] = df['target'].apply(lambda x: [ent.label_ for ent in nlp(x).ents])
  return df",https://stackoverflow.com/questions/73112216,python,25-07-2022 16:03,1299.0,2.0,2.0,True,27-07-2022 17:14,25-07-2022 16:23
73848586,how to lemmatize a single word in swift,"how do you get the stem form of a single word token? here is my code. it works for some words, but not others.
let text = ""people"" // works
// let text = ""geese"" // doesn't work
let tagger = nltagger(tagschemes: [.lemma])
tagger.string = text
let (tag, range) = tagger.tag(at: text.startindex, unit: .word, scheme: .lemma)
let stemform = tag?.rawvalue ?? string(text[range])

however, if i lemmatize the entire text it's able to find all the stem forms of words.
let text = ""this is text with plurals such as geese, people, and millennia.""
let tagger = nltagger(tagschemes: [.lemma])
tagger.string = text

var words: [string] = []
tagger.enumeratetags(in: text.startindex..<text.endindex, unit: .word, scheme: .lemma, options: [.omitwhitespace, .omitpunctuation]) { tag, range in
    let stemform = tag?.rawvalue ?? string(text[range])
    words += [stemform]
    return true
}

// this be text with plural such as goose person and millennium
words.joined(separator: "" "")

also, is it possible to reverse the process and find the plural version of a stem word?","['swift', 'string', 'nlp']",73848909,"if you set the language of the text before tagging it, it works:
tagger.string = text
tagger.setlanguage(.english, range: text.startindex..<text.endindex)
let (tag, range) = tagger.tag(at: text.startindex, unit: .word, scheme: .lemma)

without setting a language, the tagger guesses the language. apparently, just ""geese"" alone is too little information for it to guess that it is english. if you check dominantlanguage without setting the language explicitly, it is apparently dutch.",https://stackoverflow.com/questions/73848586,swift,26-09-2022 00:02,185.0,2.0,1.0,True,26-09-2022 01:36,26-09-2022 00:27
68083466,how to use spacy train to add entities to an existing custom ner model? (spacy v3.0),"i am currently implementing a custom ner model interface where a user can interact with a frontend application to add custom entities to train a spacy model.
i want to use spacy train (cli) to take an existing model (custom ner model) and add the keyword and entity specified by the user, to that model. (instead of training the whole model again). i can't find this anywhere in the documentation.
for example, let's say i have a model that is already trained for a custom entity of food. (pizza, pasta, bread, etcï¿½ï¿½ï¿½). now i want to take this existing model, and train it for a new entity called drinks with keywords like coca-cola, pepsi, juice, etcï¿½ï¿½ï¿½ using spacy train command for spacy v3.0.
the spacy train command that i am using currently is as follows:
> python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy

i load the model for prediction using:
> nlp1 = spacy.load(r""el-best"")

as of now, i was training the model for new entities manually. below is the code to find keywords in my training data and output a json format for training data (old format).
import re

keyword = [""outages"",""updates"",""negative star"",""worst""]
entity = [""problem"",""problem"",""complaint"",""complaint""]

train = []

for text in df.text:

    for n in range(0,len(keyword)):
    
        start_index = []
        end_index = []

        start_index = [m.start() for m in re.finditer(keyword[n], str(text))]

        if(start_index):

            end_index = [m+len(keyword[n]) for m in start_index]

            for i in range(0,len(start_index)):

                train.append((text,{""entities"": [(start_index[i],end_index[i],entity[n])]}))

train

after this, i converted my json format into .spacy format with below code.
from tqdm import tqdm
from spacy.tokens import docbin

db = docbin() # create a docbin object

for text, annot in tqdm(train): # data in previous format
    doc = nlp.make_doc(text) # create doc object from text
    ents = []
    for start, end, label in annot[""entities""]: # add character indexes
        span = doc.char_span(start, end, label=label, alignment_mode=""contract"")
        if span is none:
            print(""skipping entity"")
        else:
            ents.append(span)
    doc.ents = ents # label the text with the ents
    db.add(doc)

db.to_disk(""./train.spacy"")","['python', 'machine-learning', 'nlp', 'spacy', 'named-entity-recognition']",68093963,"i want to use spacy train (cli) to take an existing model (custom ner model) and add the keyword and entity specified by the user, to that model. (instead of training the whole model again). i can't find this anywhere in the documentation.

what you are describing is called ""online learning"" and the default spacy models don't support it. most modern neural ner methods, even outside of spacy, have no support for it at all.
you cannot fix this by using a custom training loop.
your options are to use rule-based matching, so you can only match things explicitly in a list, or to retrain models on the fly.
rule-based matching should be easy to set up but has the obvious issue that it can't learn things not explicitly in the list.
training things on the fly may sound like it'll take too long, but you can train a small model quite quickly. what you can do is train a small model for a small number of iterations while the user is working interactively, and after they've confirmed the model is more or less working correctly you can use the same training data for a larger model with longer training.",https://stackoverflow.com/questions/68083466,python,22-06-2021 12:21,1702.0,3.0,1.0,True,20-07-2021 19:11,20-07-2021 19:10
78262843,openai api error: &quot;configuration is not a constructor&quot;,"i'm trying to build a discord bot that generates images, but i faced a problem. i'm getting the following error:

configuration is not a constructor

i'm following a youtube tutorial. i have the following code:
const { slashcommandbuilder, embedbuilder} = require(`discord.js`);
const { configuration, openaiapi } = require(""openai"");

const configuration = new configuration({
  apikey: 'my key'
});
const openai = new openaiapi(configuration);

can anyone help me solve the problem?","['javascript', 'discord', 'discord.js', 'bots', 'openai-api']",78262873,"problem
the code you have works with the openai node.js sdk <v4, but you're using >=v4.
solution
the following is the correct initialization if you're using the openai node.js sdk >=v4:
import openai from ""openai"";

const client = new openai(
  {apikey: ""sk-xxxxxxxxxxxxxxxxxxxxxxxx""}
);

then you can use, for example, the images api as follows:
 import openai from ""openai"";

 const client = new openai(
  { apikey: ""sk-xxxxxxxxxxxxxxxxxx""}
 );

async function main() {
  const image = await client.images.generate({ model: ""dall-e-3"", prompt: ""a cute baby sea otter"" });

  console.log(image.data);
}

main();",https://stackoverflow.com/questions/78262843,javascript,02-04-2024 16:41,1629.0,0.0,1.0,True,24-12-2024 11:42,15-04-2024 09:57
17447045,java library for keywords extraction from input text,"i'm looking for a java library to extract keywords from a block of text.
the process should be as follows:
stop word cleaning -> stemming -> searching for keywords based on english linguistics statistical information - meaning if a word appears more times in the text than in the english language in terms of probability than it's a keyword candidate.
is there a library that performs this task?","['java', 'nlp', 'extract', 'keyword', 'stemming']",17453157,"here is a possible solution using apache lucene. i didn't use the last version but the 3.6.2 one, since this is the one i know the best. besides the /lucene-core-x.x.x.jar, don't forget to add the /contrib/analyzers/common/lucene-analyzers-x.x.x.jar from the downloaded archive to your project: it contains the language-specific analyzers (especially the english one in your case).
note that this will only find the frequencies of the input text words based on their respective stem. comparing these frequencies with the english language statistics shall be done afterwards (this answer may help by the way).

the data model
one keyword for one stem. different words may have the same stem, hence the terms set. the keyword frequency is incremented every time a new term is found (even if it has been already found - a set automatically removes duplicates).
public class keyword implements comparable<keyword> {

  private final string stem;
  private final set<string> terms = new hashset<string>();
  private int frequency = 0;

  public keyword(string stem) {
    this.stem = stem;
  }

  public void add(string term) {
    terms.add(term);
    frequency++;
  }

  @override
  public int compareto(keyword o) {
    // descending order
    return integer.valueof(o.frequency).compareto(frequency);
  }

  @override
  public boolean equals(object obj) {
    if (this == obj) {
      return true;
    } else if (!(obj instanceof keyword)) {
      return false;
    } else {
      return stem.equals(((keyword) obj).stem);
    }
  }

  @override
  public int hashcode() {
    return arrays.hashcode(new object[] { stem });
  }

  public string getstem() {
    return stem;
  }

  public set<string> getterms() {
    return terms;
  }

  public int getfrequency() {
    return frequency;
  }

}


utilities
to stem a word:
public static string stem(string term) throws ioexception {

  tokenstream tokenstream = null;
  try {

    // tokenize
    tokenstream = new classictokenizer(version.lucene_36, new stringreader(term));
    // stem
    tokenstream = new porterstemfilter(tokenstream);

    // add each token in a set, so that duplicates are removed
    set<string> stems = new hashset<string>();
    chartermattribute token = tokenstream.getattribute(chartermattribute.class);
    tokenstream.reset();
    while (tokenstream.incrementtoken()) {
      stems.add(token.tostring());
    }

    // if no stem or 2+ stems have been found, return null
    if (stems.size() != 1) {
      return null;
    }
    string stem = stems.iterator().next();
    // if the stem has non-alphanumerical chars, return null
    if (!stem.matches(""[a-za-z0-9-]+"")) {
      return null;
    }

    return stem;

  } finally {
    if (tokenstream != null) {
      tokenstream.close();
    }
  }

}

to search into a collection (will be used by the list of potential keywords):
public static <t> t find(collection<t> collection, t example) {
  for (t element : collection) {
    if (element.equals(example)) {
      return element;
    }
  }
  collection.add(example);
  return example;
}


core
here is the main input method:
public static list<keyword> guessfromstring(string input) throws ioexception {

  tokenstream tokenstream = null;
  try {

    // hack to keep dashed words (e.g. ""non-specific"" rather than ""non"" and ""specific"")
    input = input.replaceall(""-+"", ""-0"");
    // replace any punctuation char but apostrophes and dashes by a space
    input = input.replaceall(""[\\p{punct}&&[^'-]]+"", "" "");
    // replace most common english contractions
    input = input.replaceall(""(?:'(?:[tdsm]|[vr]e|ll))+\\b"", """");

    // tokenize input
    tokenstream = new classictokenizer(version.lucene_36, new stringreader(input));
    // to lowercase
    tokenstream = new lowercasefilter(version.lucene_36, tokenstream);
    // remove dots from acronyms (and ""'s"" but already done manually above)
    tokenstream = new classicfilter(tokenstream);
    // convert any char to ascii
    tokenstream = new asciifoldingfilter(tokenstream);
    // remove english stop words
    tokenstream = new stopfilter(version.lucene_36, tokenstream, englishanalyzer.getdefaultstopset());

    list<keyword> keywords = new linkedlist<keyword>();
    chartermattribute token = tokenstream.getattribute(chartermattribute.class);
    tokenstream.reset();
    while (tokenstream.incrementtoken()) {
      string term = token.tostring();
      // stem each term
      string stem = stem(term);
      if (stem != null) {
        // create the keyword or get the existing one if any
        keyword keyword = find(keywords, new keyword(stem.replaceall(""-0"", ""-"")));
        // add its corresponding initial token
        keyword.add(term.replaceall(""-0"", ""-""));
      }
    }

    // reverse sort by frequency
    collections.sort(keywords);

    return keywords;

  } finally {
    if (tokenstream != null) {
      tokenstream.close();
    }
  }

}


example
using the guessfromstring method on the java wikipedia article introduction part, here are the first 10 most frequent keywords (i.e. stems) that were found:
java         x12    [java]
compil       x5     [compiled, compiler, compilers]
sun          x5     [sun]
develop      x4     [developed, developers]
languag      x3     [languages, language]
implement    x3     [implementation, implementations]
applic       x3     [application, applications]
run          x3     [run]
origin       x3     [originally, original]
gnu          x3     [gnu]

iterate over the output list to know which were the original found words for each stem by getting the terms sets (displayed between brackets [...] in the above example).

what's next
compare the stem frequency / frequencies sum ratios with the english language statistics ones, and keep me in the loop if your managed it: i could be quite interested too :)",https://stackoverflow.com/questions/17447045,java,03-07-2013 11:43,25872.0,34.0,3.0,True,26-01-2022 18:11,03-07-2013 16:21
48989629,input parameter for model as string in text classification,"i am building document classification system using scikit-learn and it works fine. i am converting the model to core ml model format. but the model format excepts the input parameter as multiarraytype. i want make it to excepts string or array of string so that i can easily predict from ios application.i have tried following way:
from sklearn.linear_model import logisticregression

logreg = logisticregression()
logreg.fit(x_train_dtm, y_train)

#testing a value
docs_new = ['get exclusive prize offer']
docs_pred_class = nb.predict(count_vect.transform(docs_new))

#exporting to coremodel
import coremltools

coreml_model = coremltools.converters.sklearn.convert(logreg)
#print model
coreml_model

printing the coreml model gives following output:
 input {
     name: ""input""
     type {
     multiarraytype {
      shape: 7505
      datatype: double
    }
  }
}
output {
  name: ""classlabel""
  type {
    int64type {
    }
  }
}
output {
  name: ""classprobability""
  type {
    dictionarytype {
      int64keytype {
      }
    }
  }
  }
  predictedfeaturename: ""classlabel""
predictedprobabilitiesname: ""classprobability"" 

i checked the core ml model in github library, i can see there is different input and output. 

how can i achieve this, so that i can pass a simple parameter from ios app to make prediction.","['python', 'scikit-learn', 'text-classification', 'coreml', 'coremltools']",48991924,"it sounds like that other mlmodel you found uses a dictvectorizer to turn the strings into indexes (possibly followed by a onehotencoder). 
you can do this by making a pipeline in sklearn and converting that pipeline to core ml.",https://stackoverflow.com/questions/48989629,python,26-02-2018 13:24,858.0,1.0,1.0,True,05-10-2023 10:15,26-02-2018 13:48
45403390,lemmatizing italian sentences for frequency counting,"i would like to lemmatize some italian text in order to perform some frequency counting of words and further investigations on the output of this lemmatized content.
i am preferring lemmatizing than stemming because i could extract the word meaning from the context in the sentence (e.g. distinguish between a verb and a noun) and obtain words that exist in the language, rather than roots of those words that don't usually have a meaning.
i found out this library called pattern (pip2 install pattern) that should complement nltk in order to perform lemmatization of the italian language, however i am not sure the approach below is correct because each word is lemmatized by itself, not in the context of a sentence.
probably i should give pattern the responsibility to tokenize a sentence (so also annotating each word with the metadata regarding verbs/nouns/adjectives etc), then retrieving the lemmatized word, but i am not able to do this and i am not even sure it is possible at the moment?
also: in italian some articles are rendered with an apostrophe so for example ""l'appartamento"" (in english ""the flat"") is actually 2 words: ""lo"" and ""appartamento"". right now i am not able to find a way to split these 2 words with a combination of nltk and pattern so then i am not able to count the frequency of the words in the correct way.
import nltk
import string
import pattern

# dictionary of italian stop-words
it_stop_words = nltk.corpus.stopwords.words('italian')
# snowball stemmer with rules for the italian language
ita_stemmer = nltk.stem.snowball.italianstemmer()

# the following function is just to get the lemma
# out of the original input word (but right now
# it may be loosing the context about the sentence
# from where the word is coming from i.e.
# the same word could either be a noun/verb/adjective
# according to the context)
def lemmatize_word(input_word):
    in_word = input_word#.decode('utf-8')
    # print('something: {}'.format(in_word))
    word_it = pattern.it.parse(
        in_word, 
        tokenize=false,  
        tag=false,  
        chunk=false,  
        lemmata=true 
    )
    # print(""input: {} output: {}"".format(in_word, word_it))
    the_lemmatized_word = word_it.split()[0][0][4]
    # print(""returning: {}"".format(the_lemmatized_word))
    return the_lemmatized_word

it_string = ""ieri sono andato in due supermercati. oggi volevo andare all'ippodromo. stasera mangio la pizza con le verdure.""

# 1st tokenize the sentence(s)
word_tokenized_list = nltk.tokenize.word_tokenize(it_string)
print(""1) nltk tokenizer, num words: {} for list: {}"".format(len(word_tokenized_list), word_tokenized_list))

# 2nd remove punctuation and everything lower case
word_tokenized_no_punct = [string.lower(x) for x in word_tokenized_list if x not in string.punctuation]
print(""2) clean punctuation, num words: {} for list: {}"".format(len(word_tokenized_no_punct), word_tokenized_no_punct))

# 3rd remove stop words (for the italian language)
word_tokenized_no_punct_no_sw = [x for x in word_tokenized_no_punct if x not in it_stop_words]
print(""3) clean stop-words, num words: {} for list: {}"".format(len(word_tokenized_no_punct_no_sw), word_tokenized_no_punct_no_sw))

# 4.1 lemmatize the words
word_tokenize_list_no_punct_lc_no_stowords_lemmatized = [lemmatize_word(x) for x in word_tokenized_no_punct_no_sw]
print(""4.1) lemmatizer, num words: {} for list: {}"".format(len(word_tokenize_list_no_punct_lc_no_stowords_lemmatized), word_tokenize_list_no_punct_lc_no_stowords_lemmatized))

# 4.2 snowball stemmer for italian
word_tokenize_list_no_punct_lc_no_stowords_stem = [ita_stemmer.stem(i) for i in word_tokenized_no_punct_no_sw]
print(""4.2) stemmer, num words: {} for list: {}"".format(len(word_tokenize_list_no_punct_lc_no_stowords_stem), word_tokenize_list_no_punct_lc_no_stowords_stem))

# difference between stemmer and lemmatizer
print(
    ""for original word(s) '{}' and '{}' the stemmer: '{}' '{}' (count 1 each), the lemmatizer: '{}' '{}' (count 2)""
    .format(
        word_tokenized_no_punct_no_sw[1],
        word_tokenized_no_punct_no_sw[6],
        word_tokenize_list_no_punct_lc_no_stowords_stem[1],
        word_tokenize_list_no_punct_lc_no_stowords_stem[6],
        word_tokenize_list_no_punct_lc_no_stowords_lemmatized[1],
        word_tokenize_list_no_punct_lc_no_stowords_lemmatized[1]
    )
)

gives this output:
1) nltk tokenizer, num words: 20 for list: ['ieri', 'sono', 'andato', 'in', 'due', 'supermercati', '.', 'oggi', 'volevo', 'andare', ""all'ippodromo"", '.', 'stasera', 'mangio', 'la', 'pizza', 'con', 'le', 'verdure', '.']
2) clean punctuation, num words: 17 for list: ['ieri', 'sono', 'andato', 'in', 'due', 'supermercati', 'oggi', 'volevo', 'andare', ""all'ippodromo"", 'stasera', 'mangio', 'la', 'pizza', 'con', 'le', 'verdure']
3) clean stop-words, num words: 12 for list: ['ieri', 'andato', 'due', 'supermercati', 'oggi', 'volevo', 'andare', ""all'ippodromo"", 'stasera', 'mangio', 'pizza', 'verdure']
4.1) lemmatizer, num words: 12 for list: [u'ieri', u'andarsene', u'due', u'supermercato', u'oggi', u'volere', u'andare', u""all'ippodromo"", u'stasera', u'mangiare', u'pizza', u'verdura']
4.2) stemmer, num words: 12 for list: [u'ier', u'andat', u'due', u'supermerc', u'oggi', u'vol', u'andar', u""all'ippodrom"", u'staser', u'mang', u'pizz', u'verdur']
for original word(s) 'andato' and 'andare' the stemmer: 'andat' 'andar' (count 1 each), the lemmatizer: 'andarsene' 'andarsene' (count 2)


how to effectively lemmatize some sentences with pattern using their tokenizer? (assuming lemmas are recognized as nouns/verbs/adjectives etc.)
is there a python alternative to pattern to use for italian lemmatization with nltk?  
how to split articles that are bound to the next word using apostrophes?","['python-2.7', 'nlp', 'nltk', 'stemming', 'lemmatization']",45555676,"i'll try to answer your question, knowing that i don't know a lot about italian!
1) as far as i know, the main responsibility for removing apostrophe is the tokenizer, and as such the nltk italian tokenizer seems to have failed.
3) a simple thing you can do about it is call the replace method (although you probably will have to use the re package for more complicated pattern), an example:
word_tokenized_no_punct_no_sw_no_apostrophe = [x.split(""'"") for x in word_tokenized_no_punct_no_sw]
word_tokenized_no_punct_no_sw_no_apostrophe = [y for x in word_tokenized_no_punct_no_sw_no_apostrophe for y in x]

it yields:
['ieri', 'andato', 'due', 'supermercati', 'oggi', 'volevo', 'andare', 'all', 'ippodromo', 'stasera', 'mangio', 'pizza', 'verdure']

2) an alternative to pattern would be treetagger, granted it is not the easiest install of all (you need the python package and the tool itself, however after this part it works on windows and linux).
a simple example with your example above:
import treetaggerwrapper 
from pprint import pprint

it_string = ""ieri sono andato in due supermercati. oggi volevo andare all'ippodromo. stasera mangio la pizza con le verdure.""
tagger = treetaggerwrapper.treetagger(taglang=""it"")
tags = tagger.tag_text(it_string)
pprint(treetaggerwrapper.make_tags(tags))

the pprint yields:
[tag(word=u'ieri', pos=u'adv', lemma=u'ieri'),
 tag(word=u'sono', pos=u'ver:pres', lemma=u'essere'),
 tag(word=u'andato', pos=u'ver:pper', lemma=u'andare'),
 tag(word=u'in', pos=u'pre', lemma=u'in'),
 tag(word=u'due', pos=u'adj', lemma=u'due'),
 tag(word=u'supermercati', pos=u'nom', lemma=u'supermercato'),
 tag(word=u'.', pos=u'sent', lemma=u'.'),
 tag(word=u'oggi', pos=u'adv', lemma=u'oggi'),
 tag(word=u'volevo', pos=u'ver:impf', lemma=u'volere'),
 tag(word=u'andare', pos=u'ver:infi', lemma=u'andare'),
 tag(word=u""all'"", pos=u'pre:det', lemma=u'al'),
 tag(word=u'ippodromo', pos=u'nom', lemma=u'ippodromo'),
 tag(word=u'.', pos=u'sent', lemma=u'.'),
 tag(word=u'stasera', pos=u'adv', lemma=u'stasera'),
 tag(word=u'mangio', pos=u'ver:pres', lemma=u'mangiare'),
 tag(word=u'la', pos=u'det:def', lemma=u'il'),
 tag(word=u'pizza', pos=u'nom', lemma=u'pizza'),
 tag(word=u'con', pos=u'pre', lemma=u'con'),
 tag(word=u'le', pos=u'det:def', lemma=u'il'),
 tag(word=u'verdure', pos=u'nom', lemma=u'verdura'),
 tag(word=u'.', pos=u'sent', lemma=u'.')]

it also tokenized pretty nicely the all'ippodromo to al and ippodromo (which is hopefully correct) under the hood before lemmatizing. now we just need to apply the removal of stop words and punctuation and it will be fine.
the doc for installing the treetaggerwrapper library for python",https://stackoverflow.com/questions/45403390,python-2.7,30-07-2017 18:41,12964.0,11.0,3.0,True,05-10-2023 12:46,02-08-2017 08:54
43912195,re-training spacy&#39;s ner v1.8.2 - training volume and mix of entity types,"i'm in the process of (re-) training spacy's named entity recognizer and have a couple of doubts that i hope a more experienced researcher/practitioner can help me figure out:

if a few hundred examples are considered 'a good starting point', then what would be a reasonable number to aim for? is 100 000 entity/label excessive?
if i introduce a new label, is it best if the number of the entities of that labeled are roughly the same (balanced) during training?
regarding the mixing in 'examples of other entity types':

do i just add random known categories/labels to my training set eg: ('the business standard published in its recent issue on crude oil and natural gas ...', [(4,21, 'org')], )?

can i use the same text for various labels? e.g. ('the business standard published in its recent issue on crude oil and natural gas ...', [(55,64, 'commodity')], )?





on a similar note let's assume i want spacyto also recognize a second commodity could i then just use the same sentence and label a different region e.g. ('the business standard published in its recent issue on crude oil and natural gas ...', [(69,80, 'commodity')], )? is that how it's supposed to be done?

what ratio between new and other (old) labels is considered reasonable



i'm working with python2.7 in ubuntu 16.04 using spacy 1.8.2","['python-2.7', 'nlp', 'named-entity-recognition', 'spacy']",44159573,"for a full answer by matthew honnibal check out issue 1054 on spacy's github page. below are the most important points as they relate to my questions:

question(q) 1: if a few hundred examples are considered 'a good starting point', then what would be a reasonable number to aim for? is 100 000 entity/label excessive?

answer(a): every machine learning problem will have a different examples/accuracy curve. you can get an idea for this by training with less data than you have, and seeing what the curve looks like. if you have 1,000 examples, then try training with 500, 750, etc, and see how that affects your accuracy.

q 2: if i introduce a new label, is it best if the number of the entities of that label are roughly the same (balanced) during training?

a: there's trade-off between making the gradients too sparse, and making the learning problem too unrepresentative of what the actual examples will look like.

q 3: regarding the mixing in 'examples of other entity types':

do i just add random known categories/labels to my training set:


a: no, one should annotate all the entities in that text, so the example above: ('the business standard published in its recent issue on crude oil and natural gas ...', [(4,21, 'org')], ) should be ('the business standard published in its recent issue on crude oil and natural gas ...', [(4,21, 'org'), (55,64, 'commodity'), (69,80, 'commodity')], )


can i use the same text for various labels?:


a: not in the way the examples were given. see previous answer.


what ratio between new and other (old) labels is considered reasonable?:


a: see answer q 2.

ps: double citations are direct quotes from the github issue answer.",https://stackoverflow.com/questions/43912195,python-2.7,11-05-2017 09:58,821.0,3.0,1.0,True,16-03-2025 17:02,16-03-2025 17:02
70484237,what to use in place of predict_classes() in a jupyter notebook with tensorflow? (nlp text generation),"i am trying to follow a tensorflow nlp tutorial to train a neural network to generate poetry/lyric-like outputs using my own compiled sources. i only know basic python, so this is definitely far above my level of competence. it seems that the tutorial is slightly outdated as i am receiving this error code:
attributeerror: 'sequential' object has no attribute 'predict_classes'

i understand that the attribute 'predict_classes' is deprecated and no longer used in current versions of tensorflow.
this was a line of code suggested in an answer but i don't understand how to include it into my code:
 = np.argmax(model.predict(x_test), axis=-1)
any help would be appreciated. here is the section of code that is giving me trouble, along with the error code.
i included a link to the full jupyter notebook as well.
seed_text = ""vernal sunlight""
next_words = 100
  
for _ in range(next_words):
    token_list = tokenizer.texts_to_sequences([seed_text])[0]
    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
    predicted = model.predict_classes(token_list, verbose=0)
    output_word = """"
    for word, index in tokenizer.word_index.items():
        if index == predicted:
            output_word = word
            break
    seed_text += "" "" + output_word
print(seed_text)
---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-16-0539a42e927b> in <module>()
      5         token_list = tokenizer.texts_to_sequences([seed_text])[0]
      6         token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
----> 7         predicted = model.predict_classes(token_list, verbose=0)
      8         output_word = """"
      9         for word, index in tokenizer.word_index.items():

attributeerror: 'sequential' object has no attribute 'predict_classes'

here is the link to the video i was following!
and here is the copy of the jupyter notebook!","['python', 'tensorflow', 'jupyter-notebook', 'nlp']",70484326,"you can find the predicted class by using argmax with the predicted tensor as a parameter. define predicted as the following :
predicted = np.argmax(model.predict(x), axis=-1)",https://stackoverflow.com/questions/70484237,python,26-12-2021 04:37,154.0,-1.0,1.0,True,28-12-2021 13:42,28-12-2021 13:42
75792678,count words in a sentence controlling for negations,"i am trying to count the number of times some words occur in a sentence while controlling for negations. in the example below, i write a very basic code where i count the number of times ""w"" appear in ""txt"". yet, i fail to control for negations like ""don't"" and/or ""not"".
w = [""hello"", ""apple""]

for word in w:
    txt = ""i love apples, apple are my favorite fruit. i don't really like apples if they are too mature. i do not like apples if they are immature either.""
    print(txt.count(word))

the code should say that it finds ""apple"" only times and not 4. so, i would like to add: if, n words before or after the words in ""w"" there is a negation, then don't count, and otherwise.
n.b. negations here are words like ""don't"" and ""not"".
can anyone help me with this?
thanks a lot for your help!","['python', 'nlp']",75792902,"firstly, before you consider the negations/negatives, str.count might not be doing what you're expecting.
text = ""i love apples, apple are my favorite fruit. i don't really like apples if they are too mature. i do not like apples if they are immature either.""

text.count('apple') # outputs: 4

but if you do:
text = ""the thief grappled the pineapples and ran away with a basket of apples""

text.count('apple') # outputs: 3

if you want to count the words, you would need to do some tokenization first to change the string into a list of strings, e.g.
from collections import counter

import nltk
from nltk import word_tokenize

nltk.download('punkt')

text = ""the thief grappled the pineapples and ran away with a basket of apples""

counter(word_tokenize(text))['apple'] # output: 0
counter(word_tokenize(text))['apples'] # output: 1

then you would need to ask yourself does plural matters when you want to count the no. of times apple/apples occur? if so, then you would have to do some stemming or lemmatization, stemmers vs lemmatizers
this tutorial might be helpful: 

assuming that you adopt lemmas and tokenizers and consider whatever you need to define what is a ""word"" and how to count them, you have to define what is negation and what do you want to do with the counts ultimately?
lets go with

i want to break the text down into ""chunks"" or clauses that have positive and negative sentiment towards some object/nouns.

then you would have to define what does negative/positive means, in the simplest terms you might say

anything negation words that comes near the window of the focus noun we consider as ""negative"" and in any other case, positive.

and if we try to code up the simplest terms of quantifying negation as above, you would first, have to

identify the focus word, lets take the word apple and
then the window, lets say 5 words before and 5 words after.

in code:
import nltk
from nltk import word_tokenize, ngrams

text = ""i love apples, apple are my favorite fruit. i don't really like apples if they are too mature. i do not like apples if they are immature either.""

negative_words = [""don't"", ""do not"", ""not""]
# add all the forms of tokenized negative words
negative_words += [word_tokenize(w) for w in negative_words]

def count_negation(tokens):
    return sum(1 for word in tokens if word in negative_words)

for window in ngrams(word_tokenize(text), 5): 
  if ""apple"" in window or ""apples"" in window:
    print(count_negation(window), window)

[out]:
0 ('i', 'love', 'apples', ',', 'apple')
0 ('love', 'apples', ',', 'apple', 'are')
0 ('apples', ',', 'apple', 'are', 'my')
0 (',', 'apple', 'are', 'my', 'favorite')
0 ('apple', 'are', 'my', 'favorite', 'fruit')
0 ('do', ""n't"", 'really', 'like', 'apples')
0 (""n't"", 'really', 'like', 'apples', 'if')
0 ('really', 'like', 'apples', 'if', 'they')
0 ('like', 'apples', 'if', 'they', 'are')
0 ('apples', 'if', 'they', 'are', 'too')
1 ('i', 'do', 'not', 'like', 'apples')
1 ('do', 'not', 'like', 'apples', 'if')
1 ('not', 'like', 'apples', 'if', 'they')
0 ('like', 'apples', 'if', 'they', 'are')
0 ('apples', 'if', 'they', 'are', 'immature')

q: but isn't that kind of over-counting when i do not like apples get counted 3 times even though the sentence/clause appears once in the text?
yes, it is over-counting, so it goes back to the question of what is the ultimate goal of counting the negations?
if the ultimate goal is to have a sentiment classifier then i think lexical approaches might not be as good as state-of-the-art language models, like:
from transformers import autotokenizer, automodelforseq2seqlm

model_name = ""google/flan-t5-large""

tokenizer= autotokenizer.from_pretrained(model_name)
model = automodelforseq2seqlm.from_pretrained(model_name)

text = ""i love apples, apple are my favorite fruit. i don't really like apples if they are too mature. i do not like apples if they are immature either.""


prompt=f""""""do i like apples or not?
query:{text}
options:
 - yes, i like apples
 - no, i hate apples
""""""

input_ids = tokenizer(prompt, return_tensors=""pt"").input_ids
tokenize.decode(model.generate(input_ids)[0], skip_special_tokens=true)

[out]:
yes, i like apples

q: but what if i want to explain why the model assumes positive/negative sentiments towards apple? how can i do it without counting negations?
a: good point, it's an active research area to explain the outputs, so definitely, there's no clear answer yet but take a look at",https://stackoverflow.com/questions/75792678,python,20-03-2023 16:08,100.0,0.0,1.0,True,20-03-2023 16:50,20-03-2023 16:43
63705803,merge related words in nlp,"i'd like to define a new word which includes count values from two (or more) different words. for example:
words frequency
0   mom 250
1   2020    151
2   the 124
3   19  82
4   mother  81
... ... ...
10  london  6
11  life    6
12  something   6

i would like to define mother as mom + mother:
words frequency
0   mother  331
1   2020    151
2   the 124
3   19  82
... ... ...
9   london  6
10  life    6
11  something   6

this is a way to alternative define group of words having some meaning (at least for my purpose).
any suggestion would be appreciated.","['python', 'nlp', 'cluster-analysis', 'word2vec', 'wordnet']",63771196,"update 10-21-2020
i decided to build a python module to handle the tasks that i outlined in this answer. the module is called wordhoard and can be downloaded from pypi

i have attempted to use word2vec and wordnet in projects where i needed to determine the frequency of a keyword (e.g. healthcare) and the keyword's synonyms (e.g., wellness program, preventive medicine).  i found that most nlp libraries didn't produce the results that i needed, so i decided to build my own dictionary with custom keywords and synonyms.  this approached has worked for both analyzing and classification text in multiple projects.
i'm sure that someone that is versed in nlp technology might have a more robust solution, but the one below is similar ones that have worked for me time and time again.
i coded my answer to match the words frequency data you had in your question, but it can be modified to use any keyword and synonyms dataset.
import string

# python dictionary
# i manually created these word relationship - primary_word:synonyms
word_relationship = {""father"": ['dad', 'daddy', 'old man', 'pa', 'pappy', 'papa', 'pop'],
          ""mother"": [""mamma"", ""momma"", ""mama"", ""mammy"", ""mummy"", ""mommy"", ""mom"", ""mum""]}

# this input text is from various poems about mothers and fathers
input_text = 'the hand that rocks the cradle also makes the house a home. it is the prayers of the mother ' \
         'that keeps the family strong. when i think about my mum, i just cannot help but smile; the beauty of ' \
         'her loving heart, the easy grace in her style. i will always need my mom, regardless of my age. she ' \
         'has made me laugh, made me cry. her love will never fade. if i could write a story, it would be the ' \
         'greatest ever told. i would write about my daddy, for he had a heart of gold. for my father, my friend, ' \
         'this to me you have always been. through the good times and the bad, your understanding i have had.'

# converts the input text to lowercase and splits the words based on empty space.
wordlist = input_text.lower().split()

# remove all punctuation from the wordlist
remove_punctuation = [''.join(ch for ch in s if ch not in string.punctuation) 
for s in wordlist]

# list for word frequencies
wordfreq = []

# count the frequencies of a word
for w in remove_punctuation:
wordfreq.append(remove_punctuation.count(w))

word_frequencies = (dict(zip(remove_punctuation, wordfreq)))

word_matches = []

# loop through the dictionaries
for word, frequency in word_frequencies.items():
   for keyword, synonym in word_relationship.items():
      match = [x for x in synonym if word == x]
      if word == keyword or match:
        match = ' '.join(map(str, match))
        # append the keywords (mother), synonyms(mom) and frequencies to a list
        word_matches.append([keyword, match, frequency])

# used to hold the final keyword and frequencies
final_results = {}

# list comprehension to obtain the primary keyword and its frequencies
synonym_matches = [(keyword[0], keyword[2]) for keyword in word_matches]

# iterate synonym_matches and output total frequency count for a specific keyword
for item in synonym_matches:
  if item[0] not in final_results.keys():
    frequency_count = 0
    frequency_count = frequency_count + item[1]
    final_results[item[0]] = frequency_count
  else:
    frequency_count = frequency_count + item[1]
    final_results[item[0]] = frequency_count

 
print(final_results)
# output
{'mother': 3, 'father': 2}

other methods
below are some other methods and their out-of-box output.

nltk wordnet
in this example, i looked up the synonyms for the word 'mother.' note that wordnet does not have the synonyms 'mom' or 'mum' linked to the word mother.  these two words are within my sample text above.  also note that the word 'father' is listed as a synonym for 'mother.'
from nltk.corpus import wordnet

synonyms = []
word = 'mother'
for synonym in wordnet.synsets(word):
   for item in synonym.lemmas():
      if word != synonym.name() and len(synonym.lemma_names()) > 1:
        synonyms.append(item.name())

print(synonyms)
['mother', 'female_parent', 'mother', 'fuss', 'overprotect', 'beget', 'get', 'engender', 'father', 'mother', 'sire', 'generate', 'bring_forth']

pydictionary
in this example, i looked up the synonyms for the word 'mother' using pydictionary, which queries synonym.com. the synonyms in this example include the words 'mom' and 'mum.' this example also includes additional synonyms that wordnet did not generate.
but, pydictionary also produced a synonym list for 'mum.' which has nothing to do with the word 'mother.'  it seems that pydictionary pulled this list from the adjective section of the page instead of the noun section.  it's hard for a computer to distinguish between the adjective mum and the noun mum.
from pydictionary import pydictionary
dictionary_mother = pydictionary('mother')

print(dictionary_mother.getsynonyms())
# output 
[{'mother': ['mother-in-law', 'female parent', 'supermom', 'mum', 'parent', 'mom', 'momma', 'para i', 'mama', 'mummy', 'quadripara', 'mommy', 'quintipara', 'ma', 'puerpera', 'surrogate mother', 'mater', 'primipara', 'mammy', 'mamma']}]

dictionary_mum = pydictionary('mum')

print(dictionary_mum.getsynonyms())
# output 
[{'mum': ['incommunicative', 'silent', 'uncommunicative']}]

some of the other possible approaches are using the oxford dictionary api or querying thesaurus.com. both these methods also have pitfalls. for instance the oxford dictionary api requires an api key and a paid subscription based on query numbers. and thesaurus.com is missing potential synonyms that could be useful in grouping words.

synonyms: mom, parent, ancestor, creator, mommy, origin, predecessor, progenitor, source, child-bearer, forebearer, procreator

update
producing a precise synonym lists for each potential word in your corpus is hard and will require a multiple prong approach.  the code below using
wordnet and pydictionary to create a superset of synonyms.  like all the other answers, this combine methods also leads to some over counting of word frequencies. i've been trying to reduce this over-counting by combining key and value pairs within my final dictionary of synonyms.  the latter problem is much harder than i anticipated and might require me to open my own question to solve.  in the end, i think that based on your use case you need to determine, which approach works best and will likely need to combine several approaches.
thanks for posting this question, because it allowed me to look at other methods for solving a complex problem.
from string import punctuation
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from pydictionary import pydictionary

input_text = """"""the hand that rocks the cradle also makes the house a home. it is the prayers of the mother
         that keeps the family strong. when i think about my mum, i just cannot help but smile; the beauty of
         her loving heart, the easy grace in her style. i will always need my mom, regardless of my age. she
         has made me laugh, made me cry. her love will never fade. if i could write a story, it would be the
         greatest ever told. i would write about my daddy, for he had a heart of gold. for my father, my friend,
         this to me you have always been. through the good times and the bad, your understanding i have had.""""""


def normalize_textual_information(text):
   # split text into tokens by white space
   token = text.split()

   # remove punctuation from each token
   table = str.maketrans('', '', punctuation)
   token = [word.translate(table) for word in token]

   # remove any tokens that are not alphabetic
   token = [word.lower() for word in token if word.isalpha()]

   # filter out english stop words
   stop_words = set(stopwords.words('english'))

   # you could add additional stops like this
   stop_words.add('cannot')
   stop_words.add('could')
   stop_words.add('would')

   token = [word for word in token if word not in stop_words]

   # filter out any short tokens
   token = [word for word in token if len(word) > 1]
   return token


def generate_word_frequencies(words):
   # list to hold word frequencies
   word_frequencies = []

   # loop through the tokens and generate a word count for each token
   for word in words:
      word_frequencies.append(words.count(word))

   # aggregates the words and word_frequencies into tuples and coverts them into a dictionary
   word_frequencies = (dict(zip(words, word_frequencies)))

   # sort the frequency of the words from low to high
   sorted_frequencies = {key: value for key, value in 
   sorted(word_frequencies.items(), key=lambda item: item[1])}

 return sorted_frequencies


def get_synonyms_internet(word):
   dictionary = pydictionary(word)
   synonym = dictionary.getsynonyms()
   return synonym

 
words = normalize_textual_information(input_text)

all_synsets_1 = {}
for word in words:
  for synonym in wordnet.synsets(word):
    if word != synonym.name() and len(synonym.lemma_names()) > 1:
      for item in synonym.lemmas():
        if word != item.name():
          all_synsets_1.setdefault(word, []).append(str(item.name()).lower())

all_synsets_2 = {}
for word in words:
  word_synonyms = get_synonyms_internet(word)
  for synonym in word_synonyms:
    if word != synonym and synonym is not none:
      all_synsets_2.update(synonym)

 word_relationship = {**all_synsets_1, **all_synsets_2}

 frequencies = generate_word_frequencies(words)
 word_matches = []
 word_set = {}
 duplication_check = set()

 for word, frequency in frequencies.items():
    for keyword, synonym in word_relationship.items():
       match = [x for x in synonym if word == x]
       if word == keyword or match:
         match = ' '.join(map(str, match))
         if match not in word_set or match not in duplication_check or word not in duplication_check:
            duplication_check.add(word)
            duplication_check.add(match)
            word_matches.append([keyword, match, frequency])

 # used to hold the final keyword and frequencies
 final_results = {}

 # list comprehension to obtain the primary keyword and its frequencies
 synonym_matches = [(keyword[0], keyword[2]) for keyword in word_matches]

 # iterate synonym_matches and output total frequency count for a specific keyword
 for item in synonym_matches:
    if item[0] not in final_results.keys():
      frequency_count = 0
      frequency_count = frequency_count + item[1]
      final_results[item[0]] = frequency_count
 else:
    frequency_count = frequency_count + item[1]
    final_results[item[0]] = frequency_count

# do something with the final results",https://stackoverflow.com/questions/63705803,python,02-09-2020 12:44,10091.0,23.0,6.0,True,16-05-2021 11:49,06-09-2020 02:49
78317989,is it possible to fine-tune a pretrained word embedding model like vec2word?,"i'm working on semantic matching in my search engine system. i saw that word embedding can be used for this task. however, my dataset is very limited and small, so i don't think that training a word embedding model such as word2vec from scratch will yield good results. as such, i decided to fine-tune a pre-trained model with my data.
however, i can't find a lot of information, such as articles or documentation, about fine-tuning. some people even say that it's impossible to fine-tune a word embedding model.
this raises my question: is fine-tuning a pre-trained word embedding model possible and has anyone tried this before? currently, i'm stuck and looking for more information. should i try to train a word embedding model from scratch or are there other approaches?","['python', 'nlp', 'artificial-intelligence', 'word2vec', 'word-embedding']",78323755,"as has been pointed out before, there is no ""go-to"" way for fine-tuning word2vec type models.
i would suggest training your own model from scratch, combining your data with other available data from a similar domain. word2vec models are fairly quick to train and this would probably give you the best results. if you do not need static word-level embeddings, i would recommend considering contextualized embeddings, for example through the use of sentence-transformers or similar frameworks, which has a wide selection of already pre-trained models you can choose from. you can fine-tune these types of models on your specific data rather easily, and there are tons of resources online on how to do that.
for your use case, you can embed all the documents into dense vector representations using the abovementioned library, and then construct a searchable index over this semantic space. in order to match queries, all you have to do then is to embed the query using the same model and then retrieve the documents with the highest approximate inner product, often referred to as a mips search. an example library to take a look at would be faiss.",https://stackoverflow.com/questions/78317989,python,12-04-2024 17:57,1355.0,0.0,2.0,True,15-04-2024 16:46,12-04-2024 19:56
72196425,identifying near duplicate keywords and replacing them,"i have a dataframe like as shown below
id,name,year,output
1,test level,2021,1
2,test lvele,2022,1
2,dummy inc,2022,1
2,dummy pvt inc,2022,1
3,dasho ltd,2022,1
4,dasho pvt ltd,2021,0
5,delphi ltd,2021,1
6,delphi pvt ltd,2021,1

df = pd.read_clipboard(sep=',')

my objective is
a) to replace near duplicate strings using a common string.
for example - let's pick couple of strings from name column. we have dummy inc and dummy pvt inc. these both have to be replaced as dummy
i manually prepared a mapping df map_df like as below (but can't do this for big data)
  name,correct_name
  test level,test
  test lvele,test
  dummy inc,dummy
  dummy pvt inc,dummy
  dasho ltd,dasho
  dasho pvt ltd,dasho
  delphi ltd,delphi
  delphi pvt ltd,delphi

so, i tried the below
map_df = map_df.set_index(name)
df['name'] = df['name'].map(map_df) # but this doesn't work and throws error

is creating mapping table the only way or is there any nlp based approach?
i expect my output to be like as below
id,name,year,output
1,test,2021,1
2,test,2022,1
2,dummy,2022,1
2,dummy,2022,1
3,dasho,2022,1
4,dasho,2021,0
5,delphi,2021,1
6,delphi,2021,1","['python', 'pandas', 'dataframe', 'nlp', 'nltk']",72200732,"i suggest using a dict instead of a pandas.dataframe for map_df.
id,name,year,output
1,test level,2021,1
2,test lvele,2022,1
2,dummy inc,2022,1
2,dummy pvt inc,2022,1
3,dasho ltd,2022,1
4,dasho pvt ltd,2021,0
5,delphi ltd,2021,1
6,delphi pvt ltd,2021,1

df = pd.read_clipboard(sep=',')

map_dict = dict(s.strip().split(',') for s in '''  test level,test
  test lvele,test
  dummy inc,dummy
  dummy pvt inc,dummy
  dasho ltd,dasho
  dasho pvt ltd,dasho
  delphi ltd,delphi
  delphi pvt ltd,delphi'''.split('\n'))

df['name'] = df['name'].map(map_dict.get)

results:
df.to_clipboard(sep=',')

,id,name,year,output
0,1,test,2021,1
1,2,test,2022,1
2,2,dummy,2022,1
3,2,dummy,2022,1
4,3,dasho,2022,1
5,4,dasho,2021,0
6,5,delphi,2021,1
7,6,delphi,2021,1

if map_df is already a dataframe with two columns and you want to turn these two columns into a dict, this related question: how to create a dictionary of two pandas dataframe columns? suggests a few methods:
map_dict = dict(zip(map_df['name'], map_df['correct_name']))

map_dict = pd.series(map_df['correct_name'].values,index=map_df['name']).to_dict()

map_dict = map_df.set_index('name').to_dict()['correct_name']

map_dict = dict(map_df.to_records(index=false))",https://stackoverflow.com/questions/72196425,python,11-05-2022 06:36,75.0,1.0,2.0,True,31-05-2022 06:03,11-05-2022 06:56
7870554,extracting clause from a penn treebank-formatted text,"say i have a sentence:
after he had eaten the cheese, bill went to the grocery.

in my program, i get the following output:
---parse tree---
(root
  (s
    (sbar (in after)
      (s
        (np (prp he))
        (vp (vbd had)
          (vp (vbn eaten)
            (np (dt the) (nn cheese))))))
    (, ,)
    (np (nnp bill))
    (vp (vbd went)
      (pp (to to)
        (np (dt the) (nn grocery))))
    (. .)))

how would i merge the stuff not within a clause to become an independent clause? like this:
s clause {
    sbar clause {
         after he had eaten the cheese,
    }

    s clause {
        bill went to the grocery.
    }
}

i'm pretty sure that i'm not clear, but basically i want to extract the independent and dependent clauses of the sentence, and the subclauses of those clauses.","['nlp', 'stanford-nlp']",7883386,here is a demonstration code from the nltk guide (it doesn't explicitly show how to extract a clause):,https://stackoverflow.com/questions/7870554,nlp,24-10-2011 01:27,1525.0,4.0,1.0,True,19-01-2024 23:13,04-05-2012 18:57
68693030,doc2vec model not producing expected similarity scores,"i'm trying to compare two sentences and get the cosine similarity between them.
i have about 50 sentences, and i used genism's pre-trained doc2vec and trained the model on these 50 sentences to just tweak the weights a little bit. however, the cosine similarity between two sentences is not truly reflecting the similarity. for example, sentence1 is not in english close to sentence2 but their embeddings are very similar.
my question is, how do i go about generally comparing 2 sentences for similarities (as doc2vec is not working for me). it seems to be due to the low amount of training inputs to tweak the weights, but i wonder if there is another technique to achieve this task.
e.g. rough implementation so far
s1 = ""this is a sentence""
s2 = ""this is also a sentence""
...
s50 =""this is the last sentence

list = [s1,s2..s50]

tagged_data = [taggeddocument(words=word_tokenize(_d.lower()), tags=[
                                      str(i)]) for i, _d in enumerate(list)]
model = doc2vec(vector_size=vec_size,
                        alpha=alpha,
                        min_alpha=0.00025,
                        min_count=1,
                        dm=1)

model.build_vocab(tagged_data)

for epoch in range(max_epochs):
   print('iteration {0}'.format(epoch))
   model.train(tagged_data,
   total_examples=model.corpus_count,
   epochs=100)
   # decrease the learning rate
   model.alpha -= 0.0002
   # fix the learning rate, no decay
   model.min_alpha = model.alpha


i then loop through each sentence and perform model.infer_vector(sent_tokens) to get the embeddings. but as i said, they are not even close to being correct when using similarities.
if i am doing something wrong please let me know.","['python', 'nlp', 'doc2vec']",68694040,"there is no ""gensim's pre-trained doc2vec"", so if in fact you're using some pre-trained model from some 3rd party, you'd need to descriobe the source to know what's in play here. (however, your code seems to show a new model trained up from only 50 sentences.)
50 sentences is not enough to train doc2vec (or related algorithms like word2vec or fasttext). they need bulk data, with many sublty-varying, realistic usage examples of every word of any interest, to create useful vectors.
it is almost always a bad idea to use min_count=1 with doc2vec & similar algorithms, as they depend on the influence of multiple varied contexts for a word. if there's only 1 use, or a few, of a word then any vector learned for tha word is likely to be idiosyncratic to that appearance and not of generalizable usefulness. plus, the existence of many such rare words (in usual natural-language corpora) can mean such junk words serve as noise in the model to dilute and interfere-with the training of other words for which there are suitable examples. the models usually work better if you discard such infrequent words entirely - and that's why the default is min_count=5.
i've not seen any good write-up of someone doing tiny followup tuning, with a small amount of new data, on a pretrained doc2vec model ï¿½ï¿½ï¿½ so i wouldn't recommend attempting that to someone just starting out with doc2vec. (if it works at all, it'll require expert experimentation & tuning.)
it's also almost always a misguided & error-prone idea to be calling .train() more than once in a loop, and adjusting alpha/min_alpha outside the usual default and automatic management. see this answer for more details: 
if you train properly with a good-sized corpus, then check pairwise similarities of texts that the training data was representative-of, you should see more sensible similarity values.",https://stackoverflow.com/questions/68693030,python,07-08-2021 14:01,206.0,0.0,1.0,True,07-08-2021 20:22,07-08-2021 20:22
78622400,embedding token limit overpass by chunking concatenation and dimensionality reduction,"if you want to generate embeddings for documents using azure openai with ada-002 model then you should sent maximum 8192 tokens to this api. if one document has more than 8k tokens then in order to process it we should follow specific steps as per my investigation.

prepare document text, clean, normalize, remove-stop-words to be able to count tokens as azure openai ada-002 counts them.
tokenize document text into words by splitting on space ("" "")
if document's tokens are more than 8k then split it into more sub-documents with maximum 8k tokens
pass these 8k sub-documents from the azure openai ada-002 endpoint and get embeddings for each sub-document.
combine those float embeddings (by appending) into one single vector to represent the original document.
then in order to be able to find similar documents based on question, question vector and document vectors should have same length, so we need obviously to reduce dimensionality on documents which were spitted and them re-embedded into single vector.

as an example, if a document (10k tokens) is split into two sub documents (8k and 2k) each sub-document embedding will have 1536 dimensions and therefore the complete document will have 1536 x 2 = 3072. the question which is not exceed the 8k tokens will have 1536 and therefore cannot be compared with all documents.
so is there any way to reduce properly the dimensions of those documents of 3072 dims back to 1536 dims?
according to my research this can be done using pca, i have found the following example in c#, but here the data are [][] instead of []:
double[][] data = new double[][]
{
// ... your combined embedding vectors here
};

// create a new principal component analysis
var pca = new principalcomponentanalysis()
{
method = principalcomponentmethod.center,
whiten = false
};

// learn the pca model
pca.learn(data);

// transform the data into the reduced dimensionality space
double[][] reduceddata = pca.transform(data, 3); // reducing to 3 dimensions

any ideas?","['c#', 'token', 'openai-api', 'embedding', 'dimensions']",78666476,"found the answer, there are multiple approaches to approach this issue:
first, split the document in chunks (important notice is the way the document is spitted in chunks, we can split the document by sentences or per symbols or per fixed number of tokens), if we use specific number of tokens based on the model used, for example splitting the document into 256-tokens, or 512-tokens, or 1k-tokens, is good for ada-002 performance. then embed each chunk using selected model, for example ada-002, and then gather all embedded chunks of the document. in most of the cases token overlap in chunks can increase the quality of the solution.
dimensionality reduction can be implemented with multiple ways:
one, good approach, is that after all chunks of the document are embedded we can take the average of each dimension beside the chunks. if a chunk of ada-002 has 1532 dimensions, then we will have multiple chunks of 1532 dimensions. taking the average of each dimension we will have again same dimension vector.  this method is fast and easy to implement.
second, approach, is that after all chunks of the document are embedded we can combine the document embeddings. if a chunk of ada-002 has 1532 dimensions, then we will have 1532 x number_of_embeddings dimensions. then we can use pca to reduce dimensions back to original shape.
just tested splitting document in fixed number of tokens set to 1k, with token overlap to 100 and taking the average as reduction method, seems to work pretty well.",https://stackoverflow.com/questions/78622400,c#,14-06-2024 10:26,1007.0,0.0,1.0,True,27-06-2024 09:45,27-06-2024 09:45
5907296,java api for plural forms of english words,are there any java api(s) which will provide plural form of english words (e.g. cacti for cactus)?,"java, dictionary, nlp, lexical, pluralize",6288368,"wolfram|alpha return a list of inflection forms for a given word.
see this as an example:

and here is their api:",https://stackoverflow.com/q/5907296,"java, dictionary, nlp, lexical, pluralize",06-05-2011 05:49,22869.0,34.0,7.0,True,10-12-2022 17:55,10-12-2022 17:55
2683506,wikipedia text download,"i am looking to download full wikipedia text for my college project. do i have to write my own spider to download this or is there a public dataset of wikipedia available online?
to just give you some overview of my project, i want to find out the interesting words of few articles i am interested in. but to find these interesting words, i am planning to apply tf/idf to calculate term frequency for each word and pick the ones with high frequency. but to calculate the tf, i need to know the total occurrences in whole of wikipedia.
how can this be done?","['text', 'wikipedia', 'web-crawler', 'information-retrieval']",2683522,"from wikipedia:  

wikipedia offers free copies of all available content to interested users. these databases can be used for mirroring, personal use, informal backups, offline use or database queries (such as for wikipedia:maintenance). all text content is multi-licensed under the creative commons attribution-sharealike 3.0 license (cc-by-sa) and the gnu free documentation license (gfdl). images and other files are available under different terms, as detailed on their description pages. for our advice about complying with these licenses, see wikipedia:copyrights.

seems that you are in luck too.  from the dump section:

as of 12 march 2010, the latest complete dump of the english-language wikipedia can be found at   this is the first complete dump of the english-language wikipedia to have been created since 2008.
please note that more recent dumps (such as the 20100312 dump) are incomplete.

so the data is only 9 days old :)
edit: new link as old is broken:",https://stackoverflow.com/questions/2683506,text,21-04-2010 13:56,44033.0,29.0,8.0,True,01-12-2023 13:45,13-04-2017 22:04
75313457,openai api: openai.api_key = os.getenv() not working,"i am just trying some simple functions in python with openai apis but running into an error:
i have a valid api secret key which i am using.
code:
>>> import os
>>> import openai
>>> openai.api_key = os.getenv(""i have placed the key here"")
>>> response = openai.completion.create(model=""text-davinci-003"", prompt=""say this is a test"", temperature=0, max_tokens=7)","['python', 'openai-api', 'chatgpt-api', 'gpt-3', 'gpt-4']",75313682,"option 1: openai api key not set as an environment variable
change this...
openai.api_key = os.getenv('sk-xxxxxxxxxxxxxxxxxxxx')
...to this.
openai.api_key = 'sk-xxxxxxxxxxxxxxxxxxxx'

option 2: openai api key set as an environment variable (recommended)
there are two ways to set the openai api key as an environment variable:

using an .env file (easier, but don't forget to create a .gitignore file) or
using windows environment variables.

way 1: using an .env file
change this...
openai.api_key = os.getenv('sk-xxxxxxxxxxxxxxxxxxxx')
...to this...
openai.api_key = os.getenv('openai_api_key')
also, don't forget to use the python-dotenv package. your final python file should look as follows:
# main.py

import os
from dotenv import load_dotenv
from openai import openai

# load environment variables from the .env file
load_dotenv()

# initialize openai client with the api key from environment variables
client = openai(
    api_key=os.getenv(""openai_api_key""),
)

it's crucial that you create a .gitignore file not to push the .env file to your github/gitlab and leak your openai api key!
# .gitignore

.env

way 2: using windows environment variables (source)
step 1: open system properties and select advanced system settings

step 2: select environment variables

step 3: select new
step 4: add your name/key value pair
variable name: openai_api_key

variable value: sk-xxxxxxxxxxxxxxxxxxxx

step 5: restart your computer (important!)
your final python file should look as follows:
# main.py

import os
from dotenv import load_dotenv
from openai import openai

# initialize openai client
# it will automatically use your openai api key set via windows environment variables
client = openai()",https://stackoverflow.com/questions/75313457,python,01-02-2023 16:44,41799.0,3.0,2.0,True,16-10-2024 12:07,12-08-2023 15:03
63157909,how to determine if two sentences talk about similar topics?,"i would like to ask you a question. is there any algorithm/tool which can allow me to do some association between words?
for example: i have the following group of sentences:
(1)
    ""my phone is on the table""
    ""i cannot find the charger"". # no reference on phone
(2) 
    ""my phone is on the table""
    ""i cannot find the phone's charger"". 

what i would like to do is to find a connection, probably a semantic connection, which can allow me to say that the first two sentences are talking about a topic (phone) as two terms (phone and charger) are common within it (in general). same for the second sentence.
i should have something that can connect phone to charger, in the first sentence.
i was thinking of using word2vec, but i am not sure if this is something that i can do with it.
do you have any suggestions about algorithms that i can use to determine similarity of topics (i.e. sentence which are formulated in a different way, but having same topic)?","['python', 'algorithm', 'nlp', 'sentence-similarity']",63159498,"in python i'm pretty sure you have a sequence matcher that you can usee
from difflib import sequencematcher

def similar(a, b):
    return sequencematcher(none, a, b).ratio()

if you want your own algorithm i would suggest a levenstains distance (it calculates how many operations you need to turn one string(sentance) into another. might be usefull.). i coded it myself in like this for two strings
    edits = [[x for x in range(len(str1) + 1)] for y in range(len(str2)+ 1)]
    for i in range(len(str2) + 1):
        edits[i][0] = i
    for i in range(1, len(str2) + 1):
        for j in range(1,  len(str1) + 1):
            if str2[i-1] == str1[j-1]:
                edits[i][j] = edits[i-1][j-1]
            else:
                edits[i][j] = 1 + min(edits[i-1][j-1], edits[i-1][j],
                                     edits[i][j-1])
    return edits[-1][-1]

[edit] for you, you want to compare if the sentances are about the similar topic. i would suggest any of the following algorithms (all are pretty easy)

jaccary similarity
k-means and hierarchical clustering dendrogram
cosine similarity",https://stackoverflow.com/questions/63157909,python,29-07-2020 16:11,3995.0,7.0,2.0,True,26-06-2023 07:18,29-07-2020 17:41
72619329,how does one extract the verb phrase in spacy?,"for example:

ultimate swirly ice cream scoopers are usually overrated when one considers all of the scoopers one could buy.

here i'd like to pluck:

subject: ""ultimate swirly ice cream scoopers""
adverbial clause: ""when one considers all of the scoopers one could buy""
verb phrase: ""are usually overrated""


i have the following functions for subject, object, and adverbial clause:
def get_subj(decomp):
    for token in decomp:
        if (""subj"" in token.dep_):
            subtree = list(token.subtree)
            start = subtree[0].i
            end = subtree[-1].i + 1
            return str(decomp[start:end])

def get_obj(decomp):
    for token in decomp:
        if (""dobj"" in token.dep_ or ""pobr"" in token.dep_):
            subtree = list(token.subtree)
            start = subtree[0].i
            end = subtree[-1].i + 1
            return str(decomp[start:end])

def get_advcl(decomp):
    for token in decomp:
        # print(f""pos: {token.pos_}; lemma: {token.lemma_}; dep: {token.dep_}"")
        if (""advcl"" in token.dep_):
            subtree = list(token.subtree)
            start = subtree[0].i
            end = subtree[-1].i + 1
            return str(decomp[start:end])

phrase = ""ultimate swirly ice cream scoopers are usually overrated when one considers all of the scoopers one could buy.""

nlp = spacy.load(""en_core_web_sm"")
decomp = nlp(phrase)

subj = get_subj(decomp)
obj = get_obj(decomp)
advcl = get_advcl(decomp)

print(""subj: "", subj)
print(""obj: "", obj)
print(""advcl: "", advcl)

output:
subj:  ultimate swirly ice cream scoopers
obj:  all of the scoopers
advcl:  when one considers all of the scoopers one could buy


however, the actual depenency type .dep_ for the final word of the vp, ""are usually overrated"", is ""root"".
so, the subtree technique fails, as the subtree of root returns the entire sentence.","['nlp', 'nltk', 'stanford-nlp', 'spacy']",72624250,"you are wanting to construct something more like a ï¿½ï¿½ï¿½verb groupï¿½ï¿½ï¿½ where you keep with the root verb only certain close dependents like aux, cop, and advmod but not ones like nsubj, obj, or advcl<",https://stackoverflow.com/questions/72619329,nlp,14-06-2022 15:06,693.0,1.0,1.0,True,14-06-2022 23:01,14-06-2022 15:52
37526550,removing stopwords from a user-defined corpus in r,"i have a set of documents:
documents = c(""she had toast for breakfast"",
 ""the coffee this morning was excellent"", 
 ""for lunch let's all have pancakes"", 
 ""later in the day, there will be more talks"", 
 ""the talks on the first day were great"", 
 ""the second day should have good presentations too"")

in this set of documents, i would like to remove the stopwords. i have already removed punctuation and converted to lower case, using:
documents = tolower(documents) #make it lower case
documents = gsub('[[:punct:]]', '', documents) #remove punctuation

first i convert to a corpus object:
documents <- corpus(vectorsource(documents))

then i try to remove the stopwords:
documents = tm_map(documents, removewords, stopwords('english')) #remove stopwords

but this last line results in the following error:
the_process_has_forked_and_you_cannot_use_this_corefoundation_functionality___you_must_exec() to debug.
this has already been asked here but an answer was not given. what does this error mean?
edit
yes, i am using the tm package.
here is the output of sessioninfo():
r version 3.0.2 (2013-09-25)
platform: x86_64-apple-darwin10.8.0 (64-bit)","['r', 'tm', 'topic-modeling']",37526926,"when i run into tm problems i often end up just editing the original text. 
for removing words it's a little awkward, but you can paste together a regex from tm's stopword list. 
stopwords_regex = paste(stopwords('en'), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
documents = stringr::str_replace_all(documents, stopwords_regex, '')

> documents
[1] ""     toast  breakfast""             "" coffee  morning  excellent""      
[3] "" lunch lets   pancakes""            ""later   day  will   talks""        
[5] "" talks   first day  great""         "" second day   good presentations """,https://stackoverflow.com/questions/37526550,r,30-05-2016 13:11,53112.0,5.0,4.0,True,03-07-2023 22:30,23-05-2017 11:54
71915952,why does huggingface hang on list input for pipeline sentiment-analysis?,"with python 3.10 and latest version of huggingface.
for simple code likes this
from transformers import pipeline

input_list = ['how do i test my connection? (windows)', 'how do i change my payment method?', 'how do i contact customer support?']

classifier = pipeline('sentiment-analysis')
results = classifier(input_list)

the program hangs and returns error messages:
file "".......env/lib/python3.10/multiprocessing/spawn.py"", line 134, in _check_not_importing_main
    raise runtimeerror('''
runtimeerror: 
        an attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        this probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...


but replace the list input with a string, it works
from transformers import pipeline
classifier = pipeline('sentiment-analysis')
result = classifier('how do i test my connection? (windows)')",['huggingface-transformers'],71926664,"it needs to define a main function to run multitask that the list input depends on. following update works
from transformers import pipeline

def main():
    input_list = ['how do i test my connection? (windows)', 
    'how do i change my payment method?',
    'how do i contact customer support?']

    classifier = pipeline('sentiment-analysis')
    results = classifier(input_list)

if __name__ == '__main__':
    main()

the question is reduced to where to put freeze_support() in a python script?",https://stackoverflow.com/questions/71915952,huggingface-transformers,18-04-2022 19:00,406.0,0.0,1.0,True,19-04-2022 14:28,19-04-2022 12:08
60166302,"filtering entities based on the type &quot;person&quot;, &quot;org&quot; etc in spacy","after creating a nlp pipeline from spacy. passed the doc into the pipeline.
am trying to filter the entities based on the type of it.
for ent in doc.ents:
    print(ent.text)

what would be the code to filter just the person or org from the ents list",['spacy'],60167194,"for ent in doc.ents:
    if ent.label_ in [""person"", ""org""]:
        print(ent.text)

see also  for an overview of the relevant properties available from ner.",https://stackoverflow.com/questions/60166302,spacy,11-02-2020 10:02,1996.0,0.0,2.0,True,03-01-2022 11:24,11-02-2020 10:26
74593644,how to fix no token found error while downloading hugging face?,"i am trying to test the hugging face's prithivida/parrot_paraphraser_on_t5 model but getting token not found error.
from parrot import parrot
import torch
import warnings
warnings.filterwarnings(""ignore"")
parrot = parrot(model_tag=""prithivida/parrot_paraphraser_on_t5"", use_gpu=false)

the error i am getting
oserror                                   traceback (most recent call last)
cell in [10], line 2
      1 #init models (make sure you init only once if you integrate this to your code)
----> 2 parrot = parrot(model_tag=""prithivida/parrot_paraphraser_on_t5"", use_gpu=false)

file ~/.local/lib/python3.10/site-packages/parrot/parrot.py:10, in parrot.__init__(self, model_tag, use_gpu)
      8 from parrot.filters import fluency
      9 from parrot.filters import diversity
---> 10 self.tokenizer = autotokenizer.from_pretrained(model_tag, use_auth_token=true)
     11 self.model     = automodelforseq2seqlm.from_pretrained(model_tag, use_auth_token=true)
     12 self.adequacy_score = adequacy()

file ~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:560, in autotokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    557     return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
    559 # next, let's try to use the tokenizer_config file to get the tokenizer class.
--> 560 tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
    561 if ""_commit_hash"" in tokenizer_config:
    562     kwargs[""_commit_hash""] = tokenizer_config[""_commit_hash""]

file ~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:412, in get_tokenizer_config(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)
    353 """"""
    354 loads the tokenizer configuration from a pretrained model tokenizer configuration.
    355 
   (...)
    409 tokenizer_config = get_tokenizer_config(""tokenizer-test"")
    410 ```""""""
    411 commit_hash = kwargs.get(""_commit_hash"", none)
--> 412 resolved_config_file = cached_file(
    413     pretrained_model_name_or_path,
    414     tokenizer_config_file,
    415     cache_dir=cache_dir,
    416     force_download=force_download,
    417     resume_download=resume_download,
    418     proxies=proxies,
    419     use_auth_token=use_auth_token,
    420     revision=revision,
    421     local_files_only=local_files_only,
    422     _raise_exceptions_for_missing_entries=false,
    423     _raise_exceptions_for_connection_errors=false,
    424     _commit_hash=commit_hash,
    425 )
    426 if resolved_config_file is none:
    427     logger.info(""could not locate the tokenizer configuration file, will try to use the model config instead."")

file ~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:409, in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)
    406 user_agent = 
    407 try:
    408     # load from url or cache if already cached
--> 409     resolved_file = hf_hub_download(
    410         path_or_repo_id,
    411         filename,
    412         subfolder=none if len(subfolder) == 0 else subfolder,
    413         revision=revision,
    414         cache_dir=cache_dir,
    415         user_agent=user_agent,
    416         force_download=force_download,
    417         proxies=proxies,
    418         resume_download=resume_download,
    419         use_auth_token=use_auth_token,
    420         local_files_only=local_files_only,
    421     )
    423 except repositorynotfounderror:
    424     raise environmenterror(
    425         f""{path_or_repo_id} is not a local folder and is not a valid model identifier ""
    426         ""listed on ' this is a private repository, make sure to ""
    427         ""pass a token having permission to this repo with `use_auth_token` or log in with ""
    428         ""`huggingface-cli login` and pass `use_auth_token=true`.""
    429     )

file ~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:124, in validate_hf_hub_args.<locals>._inner_fn(*args, **kwargs)
    119 if check_use_auth_token:
    120     kwargs = smoothly_deprecate_use_auth_token(
    121         fn_name=fn.__name__, has_token=has_token, kwargs=kwargs
    122     )
--> 124 return fn(*args, **kwargs)

file ~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1052, in hf_hub_download(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)
   1048         return pointer_path
   1050 url = hf_hub_url(repo_id, filename, repo_type=repo_type, revision=revision)
-> 1052 headers = build_hf_headers(
   1053     token=token,
   1054     library_name=library_name,
   1055     library_version=library_version,
   1056     user_agent=user_agent,
   1057 )
   1059 url_to_download = url
   1060 etag = none

file ~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:124, in validate_hf_hub_args.<locals>._inner_fn(*args, **kwargs)
    119 if check_use_auth_token:
    120     kwargs = smoothly_deprecate_use_auth_token(
    121         fn_name=fn.__name__, has_token=has_token, kwargs=kwargs
    122     )
--> 124 return fn(*args, **kwargs)

file ~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_headers.py:117, in build_hf_headers(token, is_write_action, library_name, library_version, user_agent)
     44 """"""
     45 build headers dictionary to send in a hf hub call.
     46 
   (...)
    114         if `token=true` but token is not saved locally.
    115 """"""
    116 # get auth token to send
--> 117 token_to_send = get_token_to_send(token)
    118 _validate_token_to_send(token_to_send, is_write_action=is_write_action)
    120 # combine headers

file ~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_headers.py:149, in get_token_to_send(token)
    147 if token is true:
    148     if cached_token is none:
--> 149         raise environmenterror(
    150             ""token is required (`token=true`), but no token found. you""
    151             "" need to provide a token or be logged in to hugging face with""
    152             "" `huggingface-cli login` or `huggingface_hub.login`. see""
    153             "" 
    154         )
    155     return cached_token
    157 # case implicit use of the token is forbidden by env variable

oserror: token is required (`token=true`), but no token found. you need to provide a token or be logged in to hugging face with `huggingface-cli login` or `huggingface_hub.login`. see 

i have the secret token downloaded but not sure where to pass and how?
the stack trace after updating the token inside class parrot in ~/.local/lib/python3.10/site-packages/parrot/parrot.py
traceback (most recent call last):
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/pp.py"", line 8, in <module>
    parrot = parrot(model_tag=""prithivida/parrot_paraphraser_on_t5"", use_gpu=false)
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/parrot/parrot.py"", line 10, in __init__
    self.tokenizer = autotokenizer.from_pretrained(model_tag, use_auth_token=true)
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py"", line 560, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py"", line 412, in get_tokenizer_config
    resolved_config_file = cached_file(
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/transformers/utils/hub.py"", line 409, in cached_file
    resolved_file = hf_hub_download(
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py"", line 124, in _inner_fn
    return fn(*args, **kwargs)
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/huggingface_hub/file_download.py"", line 1052, in hf_hub_download
    headers = build_hf_headers(
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py"", line 124, in _inner_fn
    return fn(*args, **kwargs)
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/huggingface_hub/utils/_headers.py"", line 117, in build_hf_headers
    token_to_send = get_token_to_send(token)
  file ""/media/chinmay/new volume/myworks/git_hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/huggingface_hub/utils/_headers.py"", line 149, in get_token_to_send
    raise environmenterror(
oserror: token is required (`token=true`), but no token found. you need to provide a token or be logged in to hugging face with `huggingface-cli login` or `huggingface_hub.login`. see","['python-3.x', 'pytorch', 'huggingface-transformers']",74646499,"use generate token from  and past it
install python lib huggingface_hub
pip install huggingface_hub


python -c ""from huggingface_hub.hf_api import hffolder; hffolder.save_token('your_token_here')""

if you are using notebooke
from huggingface_hub import notebook_login
notebook_login()

past your genrated token",https://stackoverflow.com/questions/74593644,python-3.x,27-11-2022 20:36,26706.0,7.0,2.0,True,06-07-2024 22:23,30-11-2022 05:51
53772907,no such file or directory &#39;nltk_data/corpora/stopwords/english&#39; when using colab,"first of all i am using google colab for the work and
i have downloaded nltk stopwords for english with following:
nltk.download('stopwords')

the download was successful
[nltk_data] downloading package stopwords to /root/nltk_data...

but when i run stop = stopwords.words('english')
i am getting oserror: no such file or directory: '/root/nltk_data/corpora/stopwords/english'","['python', 'nlp', 'nltk', 'google-colaboratory']",53774017,"tl;dr
the english should be in lowercase =)
see:  
in code
# downloads the data.
import nltk
nltk.download('stopwords')


# using the stopwords.
from nltk.corpus import stopwords

# initialize the stopwords
stoplist = stopwords.words('english')",https://stackoverflow.com/questions/53772907,python,14-12-2018 02:56,11404.0,5.0,2.0,True,07-09-2024 13:18,14-12-2018 05:36
54334304,spacy: can&#39;t find model &#39;en_core_web_sm&#39; on windows 10 and python 3.5.3 :: anaconda custom (64-bit),"what is the difference between spacy.load('en_core_web_sm') and spacy.load('en')? this link explains different model sizes. but i am still not clear how spacy.load('en_core_web_sm') and spacy.load('en') differ
spacy.load('en') runs fine for me. but the spacy.load('en_core_web_sm') throws error
i have installed spacyas below. when i go to jupyter notebook and run command nlp = spacy.load('en_core_web_sm') i get the below error
---------------------------------------------------------------------------
oserror                                   traceback (most recent call last)
<ipython-input-4-b472bef03043> in <module>()
      1 # import spacy and load the language library
      2 import spacy
----> 3 nlp = spacy.load('en_core_web_sm')
      4 
      5 # create a doc object

c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder\lib\site-packages\spacy\__init__.py in load(name, **overrides)
     13     if depr_path not in (true, false, none):
     14         deprecation_warning(warnings.w001.format(path=depr_path))
---> 15     return util.load_model(name, **overrides)
     16 
     17 

c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder\lib\site-packages\spacy\util.py in load_model(name, **overrides)
    117     elif hasattr(name, 'exists'):  # path or path-like to model data
    118         return load_model_from_path(name, **overrides)
--> 119     raise ioerror(errors.e050.format(name=name))
    120 
    121 

oserror: [e050] can't find model 'en_core_web_sm'. it doesn't seem to be a shortcut link, a python package or a valid path to a data directory.

how i installed spacy ---
(c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder) c:\users\nikhizzz>conda install -c conda-forge spacy
fetching package metadata .............
solving package specifications: .

package plan for installation in environment c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder:

the following new packages will be installed:

    blas:           1.0-mkl
    cymem:          1.31.2-py35h6538335_0    conda-forge
    dill:           0.2.8.2-py35_0           conda-forge
    msgpack-numpy:  0.4.4.2-py_0             conda-forge
    murmurhash:     0.28.0-py35h6538335_1000 conda-forge
    plac:           0.9.6-py_1               conda-forge
    preshed:        1.0.0-py35h6538335_0     conda-forge
    pyreadline:     2.1-py35_1000            conda-forge
    regex:          2017.11.09-py35_0        conda-forge
    spacy:          2.0.12-py35h830ac7b_0    conda-forge
    termcolor:      1.1.0-py_2               conda-forge
    thinc:          6.10.3-py35h830ac7b_2    conda-forge
    tqdm:           4.29.1-py_0              conda-forge
    ujson:          1.35-py35hfa6e2cd_1001   conda-forge

the following packages will be updated:

    msgpack-python: 0.4.8-py35_0                         --> 0.5.6-py35he980bc4_3 conda-forge

the following packages will be downgraded:

    freetype:       2.7-vc14_2               conda-forge --> 2.5.5-vc14_2

proceed ([y]/n)? y

blas-1.0-mkl.t 100% |###############################| time: 0:00:00   0.00  b/s
cymem-1.31.2-p 100% |###############################| time: 0:00:00   1.65 mb/s
msgpack-python 100% |###############################| time: 0:00:00   5.37 mb/s
murmurhash-0.2 100% |###############################| time: 0:00:00   1.49 mb/s
plac-0.9.6-py_ 100% |###############################| time: 0:00:00   0.00  b/s
pyreadline-2.1 100% |###############################| time: 0:00:00   4.62 mb/s
regex-2017.11. 100% |###############################| time: 0:00:00   3.31 mb/s
termcolor-1.1. 100% |###############################| time: 0:00:00 187.81 kb/s
tqdm-4.29.1-py 100% |###############################| time: 0:00:00   2.51 mb/s
ujson-1.35-py3 100% |###############################| time: 0:00:00   1.66 mb/s
dill-0.2.8.2-p 100% |###############################| time: 0:00:00   4.34 mb/s
msgpack-numpy- 100% |###############################| time: 0:00:00   0.00  b/s
preshed-1.0.0- 100% |###############################| time: 0:00:00   0.00  b/s
thinc-6.10.3-p 100% |###############################| time: 0:00:00   5.49 mb/s
spacy-2.0.12-p 100% |###############################| time: 0:00:10   7.42 mb/s

(c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder) c:\users\nikhizzz>python -v
python 3.5.3 :: anaconda custom (64-bit)

(c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder) c:\users\nikhizzz>python -m spacy download en
collecting en_core_web_sm==2.0.0 from 
  downloading  (37.4mb)
    100% |################################| 37.4mb ...
installing collected packages: en-core-web-sm
  running setup.py install for en-core-web-sm ... done
successfully installed en-core-web-sm-2.0.0

    linking successful
    c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder\lib\site-packages\en_core_web_sm
    -->
    c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder\lib\site-packages\spacy\data\en

    you can now load the model via spacy.load('en')


(c:\users\nikhizzz\appdata\local\conda\conda\envs\tensorflowspyder) c:\users\nikhizzz>","['python', 'python-3.x', 'nlp', 'spacy']",54409674,"the answer to your misunderstanding is a unix concept, softlinks which we could say that in windows are similar to shortcuts. let's explain this.
when you spacy download en, spacy tries to find the best small model that matches your spacy distribution. the small model that i am talking about defaults to en_core_web_sm which can be found in different variations which correspond to the different spacy versions (for example spacy, spacy-nightly have en_core_web_sm of different sizes).
when spacy finds the best model for you, it downloads it and then links the name en to the package it downloaded, e.g. en_core_web_sm. that basically means that whenever you refer to en you will be referring to en_core_web_sm. in other words, en after linking is not a ""real"" package, is just a name for en_core_web_sm.
however, it doesn't work the other way. you can't refer directly to en_core_web_sm because your system doesn't know you have it installed. when you did spacy download en you basically did a pip install. so pip knows that you have a package named en installed for your python distribution, but knows nothing about the package en_core_web_sm. this package is just replacing package en when you import it, which means that package en is just a softlink to en_core_web_sm.
of course, you can directly download en_core_web_sm, using the command: python -m spacy download en_core_web_sm, or you can even link the name en to other models as well. for example, you could do python -m spacy download en_core_web_lg and then python -m spacy link en_core_web_lg en. that would make
en a name for en_core_web_lg, which is a large spacy model for the english language.",https://stackoverflow.com/questions/54334304,python,23-01-2019 19:24,473947.0,178.0,34.0,True,15-03-2025 14:57,13-03-2025 21:33
56161468,why is part-of-speech tag for adjectives &#39;jj&#39;?,"what is the etymology for jj tag denoting pos for adjectives?  i am unable to find any references online. there are several resources listing all the tags, but none describing the reason.",['nlp'],78987349,"i am a ""still-living colleague"" of henry kucera and others, maybe i should print that on a t-shirt! i worked on the lob corpus tagging, the british english equivalent/copy of the us brown corpus of american english.  our tagged lob corpus user manual gives some explnation, see section ""an
overview""  
""... the tags consist of a base, which is very often followed by 'suffixes' marking subclass and/or inflection ..."" base is first letter of tag: n for nouns, v for verbs, a for article/determiner, etc.  as ""a"" was taken, we used j for adjective, r for adverb.  appendix 4 list of tags 
lisrs the subclasses in lob tagset: jj, jjb, jjr, jjt, jnp.
admittedly this dos not explain ""double j"" - why not simply use tags j, jb, jr, jt, jnp? i think we decided all tags must have at least 2 letters, hence jj rather tha just j for standard adjective, and then variants attrbute-only comaprative and superlativ also started jj
eric atwell,",https://stackoverflow.com/questions/56161468,nlp,16-05-2019 05:35,1248.0,3.0,2.0,True,09-10-2024 16:22,02-06-2019 20:21
77933640,how to calculate the weighted sum of last 4 hidden layers using roberta?,"the table from this paper that explains various approaches to obtain the embedding, i think these approaches are also applicable to roberta too:

i'm trying to calculate the weighted sum of last 4 hidden layers using roberta to obtain token embedding, but i don't know if this is the correct way to do, this is the code i have tried:
from transformers import robertatokenizer, robertamodel
import torch

tokenizer = robertatokenizer.from_pretrained('roberta-base')
model = robertamodel.from_pretrained('roberta-base')
caption = ['this is a yellow bird', 'example caption']

tokens = tokenizer(caption, return_tensors='pt', padding=true)

input_ids = tokens['input_ids']
attention_mask = tokens['attention_mask']

output = model(input_ids, attention_mask, output_hidden_states=true)

states = output.hidden_states
token_emb = torch.stack([states[i] for i in [-4, -3, -2, -1]]).sum(0).squeeze()","['python', 'machine-learning', 'deep-learning', 'nlp', 'huggingface-transformers']",77952168,"first, lets do some digging from the og bert code, 
if we just do a quick search for ""sum"" on the github repo, we find this 
  # the transformer performs sum residuals on all layers so the input needs
  # to be the same as the hidden size.
  if input_width != hidden_size:
    raise valueerror(""the width of the input tensor (%d) != hidden size (%d)"" %
                     (input_width, hidden_size))

then a quick search on stackoverflow reveals how to get intermediate layers' output of pre-trained bert model in huggingface transformers library?

now, lets validate if your code logic works by working backwards a little:
from transformers import robertatokenizer, robertamodel
import torch

tokenizer = robertatokenizer.from_pretrained('roberta-base')
model = robertamodel.from_pretrained('roberta-base')
caption = ['this is a yellow bird', 'example caption']

tokens = tokenizer(caption, return_tensors='pt', padding=true)

input_ids = tokens['input_ids']
attention_mask = tokens['attention_mask']

output = model(input_ids, attention_mask, output_hidden_states=true)

then:
>>>len(output.hidden_states)

out:
13

why 13?
12 encoder (hidden) layer output + final pooler output
import torchinfo
torchinfo.summary(model)

[out]:
================================================================================
layer (type:depth-idx)                                  param #
================================================================================
robertamodel                                            --
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertaembeddings: 1-1                                --
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½embedding: 2-1                                   38,603,520
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½embedding: 2-2                                   394,752
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½embedding: 2-3                                   768
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½layernorm: 2-4                                   1,536
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½dropout: 2-5                                     --
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertaencoder: 1-2                                   --
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½modulelist: 2-6                                  --
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-1                           7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½=                    7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-5                           7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-6                           7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-7                           7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-8                           7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-9                           7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-10                          7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-11                          7,087,872
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertalayer: 3-12                          7,087,872
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½robertapooler: 1-3                                    --
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½linear: 2-7                                      590,592
ï¿½ï¿½ï¿½    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½tanh: 2-8       =====================================

to validate that the last layer output is the last layer in the hidden_states:
assert(
  true for x in 
    torch.flatten(
      output[0] == output.hidden_states[-1]
    )
)

lets check if the size for each layer's output matches:
first_hidden_shape = output.hidden_states[0].shape

for x in output.hidden_states:
  assert x.shape == first_hidden_shape

checks out!
>>>first_hidden_shape

[out]:
torch.size([2, 7, 768])

why [2, 7, 768]?
it's (batch_size, sequence_length, hidden_size)

2 sentences = batch size of 2
7 longest sequence length = no. of tokens (i.e. 5 in the case of your longest example sentence + <s> and </s> from len(input_ids[0]))
768 outputs = fixed for all hidden layers output

bï¿½ï¿½ddu aï¿½ï¿½eins! (wait a minute!), does that mean i've sequence_length * 768 outputs for each batch? and if my batches are not equal lengths, the output size are different?
yes that is correct! and to get some sense of ""equality"" for all inputs, it'll be good to pad/truncate all outputs to a fixed length if you're still going to use the feature-based bert approaches.
soooo, is my torch.stack approach right?
yes, it seems so, but it depends on whether you consider the pooler output to be last or second to last.
if second to last:
torch.stack(output.hidden_states[-5:-1]).sum(0)

if you consider the pooler to be the last:
torch.stack(output.hidden_states[-4:]).sum(0)

minor nitpicking, you can access the output.hidden_states through slices because it's a tuple object. next you won't need to she stacked output because the the outer most layer tensor is non-empty.
this is a special case for stack, in nlp where the 1st dimension is batch size and 2nd is token length, so summing the hidden dimensions up ends up the same when you're not explicitly stating which dimension you stack.
to be a little more explicit:
# 2nd dimension is where our hidden states are 
# and that's where we want to do our sum too.
torch.stack(output.hidden_states[-4:], dim=2).sum(2)

but in practice, you can do this to comfort yourself:
assert(
 true for x in torch.flatten(
    torch.stack(output.hidden_states[-4:]).sum(0)
    == 
    torch.stack(output.hidden_states[-4:], dim=2).sum(2)
  )
)

interesting, what about ""concat last four hidden""?
in the case of concat you'll need to be explicit when in the dimensions
>>> torch.cat(output.hidden_states[-4:], dim=2).shape

[out]:
torch.size([2, 7, 3072])

but note, you still ends with sequence_length * hidden_size * 4, which makes batches with unequal lengths a pain.
since you've covered almost everything on the table, what about the ""embeddings"" output?
this is the interesting part, it's actually not accessible through the model(inputs_ids) directly, you'll need to do this:
model.embeddings(input_ids)

finally, why didn't you just answer ""yes, you are right""?
if i did, would that convince you more than you proving the above for yourself?",https://stackoverflow.com/questions/77933640,python,03-02-2024 20:34,329.0,2.0,1.0,True,07-02-2024 04:52,03-02-2024 20:52
71598259,typeerror: len() of unsized object,"i am trying random forest classifier from sklearn, when i want to print the classifier report, it is give me an error.
this was the code :
randomforestmodel = randomforestclassifier()
randomforestmodel.fit(train_vectors, data_train['label'])
predict_rfmodel = randomforestmodel.predict(test_vectors)

print(""classification with randomforest"")
print(metrics.classification_report(test_vectors, predict_rfmodel))

and the error was like this :
    classification with randomforest
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
<ipython-input-34-f976cec884e4> in <module>()
      1 print(""classification with randomforest"")
----> 2 print(metrics.classification_report(test_vectors, predict_rfmodel))

2 frames
/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py in classification_report(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)
   2108     """"""
   2109 
-> 2110     y_type, y_true, y_pred = _check_targets(y_true, y_pred)
   2111 
   2112     if labels is none:

/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py in _check_targets(y_true, y_pred)
     83     """"""
     84     check_consistent_length(y_true, y_pred)
---> 85     type_true = type_of_target(y_true)
     86     type_pred = type_of_target(y_pred)
     87 

/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py in type_of_target(y)
    308 
    309     # invalid inputs
--> 310     if y.ndim > 2 or (y.dtype == object and len(y) and not isinstance(y.flat[0], str)):
    311         return ""unknown""  # [[[1, 2]]] or [obj_1] and not [""label_1""]
    312 

typeerror: len() of unsized object","['scikit-learn', 'classification', 'random-forest', 'text-classification']",71601330,"you're providing the test instances features (test_vectors) instead of the true test instances labels to classification_report.
as per the documentation, the first parameter should be:

y_true: 1d array-like, or label indicator array / sparse matrix.
ground truth (correct) target values.",https://stackoverflow.com/questions/71598259,scikit-learn,24-03-2022 07:05,1612.0,1.0,1.0,True,24-03-2022 11:05,24-03-2022 07:28
75723102,how does gensim calculate sentence embeddings when using a pretrained fasttext model?,"according to this answer, sentence similarity for fasttext is calculated with one of two ways (depending if the embeddings are created superviser or unsupervised)

the mean of the normalized word vectors (unsupervised)
the mean of the word vectors (supervised)

but i cannot make either of those give the same answer as the sentence embedding
from gensim.models import fasttext
import numpy as np

wv = fasttext.load_facebook_vectors(""transtotag/cc.da.300.bin"")

w1 = wv[""til""]
norm_w1 = np.linalg.norm(wv[""til""], ord=2)
s1 = w1/norm_w1

w2 = wv[""skat""]
norm_w2 = np.linalg.norm(wv[""skat""], ord=2)
s2 = w2/norm_w2

w3 = wv[""til skat""]

# using ""raw"" embeddings
((w1+w2)/2-w3).max() #0.25
((w1+w2)-w3).max() # 0.5

# using normalized embeddings
((s1+s2)/2-w3).max() # 0.18
((s1+s2)-w3).max() # 0.37


i even tried to add the eos (as stated in the answer) aswell
nl = wv[""</s>""]
norm_nl = np.linalg.norm(wv[""</s>""],2)
snl = nl/norm_nl

w3 = wv[""til skat""]

((s1+s2+snl)/3-w3).max() #0.12

if we look in the source code, then wv[] just returns vstack([self.get_vector(key) for key in key_or_keys]) i.e it treats til skat a single word.
i cannot find anyting about how sentence embeddings are created in the docs aswell.","['gensim', 'word-embedding', 'fasttext']",75724874,"in gensim, you should use get_sentence_vector method, which was recently added.
please read the docs and notice that this method expects a list of words specified by string or int ids.",https://stackoverflow.com/questions/75723102,gensim,13-03-2023 14:19,906.0,0.0,2.0,True,17-03-2023 17:39,13-03-2023 14:36
1077600,converting words to numbers in php,"i am trying to convert numerical values written as words into integers. for example,
iphone has two hundred and thirty thousand seven hundred and eighty three apps

would become
iphone has 230783 apps

is there any library or function that does this?","php, nlp, numbers",1077651,"there are lots of pages discussing the conversion from numbers to words. not so many for the reverse direction. the best i could find was some pseudo-code on ask yahoo. see  for a nice algorithm:

well, overall you are doing two things: finding tokens (words that translates to numbers) and applying grammar. in short, you are building a parser for a very limited language.
the tokens you would need are: 
power: thousand, million, billion
  hundred: hundred
  ten: twenty, thirty... ninety
  unit: one, two, three, ... nine,
  special: ten, eleven, twelve, ... nineteen
(drop any ""and""s as they are meaningless. break hyphens into two tokens. that is sixty-five should be processed as ""sixty"" ""five"")
once you've tokenized your string, move from right to left.

grab all the tokens from the right until you hit a power or the whole string.
parse the tokens after the stop point for these patterns:
special
  ten
  unit
  ten unit
  unit hundred
  unit hundred special
  unit hundred ten
  unit hundred unit
  unit hundred ten unit
(this assumes that ""seventeen hundred"" is not allowed in this grammar)
this gives you the last three digits of your number.
if you stopped at the whole string you are done.
if you stopped at a power, start again at step 1 until you reach a higher power or the whole string.",https://stackoverflow.com/q/1077600,"php, nlp, numbers",03-07-2009 02:55,25206.0,22.0,6.0,True,10-12-2022 18:39,10-12-2022 18:39
75373129,openai gpt-3 api error: &quot;this model&#39;s maximum context length is 2049 tokens&quot;,"i have two issues relating to the response result from openai completion.
the following result doesn't return back the full text when i give a content of 500 words and prompt with ""fix grammar mistakes"" (is tokens issue?)

the second issue is when the text sometimes have some double quotes or single quotes. it messes with the json format. so i delete any type of quotes from the content (i am not sure if it's the best solution, but i may prefer doing it in javascript, not php).
curl_setopt($ch, curlopt_postfields, ""{\n  \""model\"": \""text-davinci-001\"",\n  \""prompt\"": \"""" . $open_ai_prompt  . "":nn"" . $content_text  . ""\"",\n  \""temperature\"": 0,\n  \""top_p\"": 1.0,\n  \""frequency_penalty\"": 0.0,\n  \""presence_penalty\"": 0.0\n}"");


""message"": ""we could not parse the json body of your request. (hint:
this likely means you aren't using your http library correctly. the
openai api expects a json payload, but what was sent was not valid
json.","['javascript', 'php', 'json', 'openai-api', 'gpt-3']",75373214,"regarding token limits
first of all, i think you don't understand how tokens work: 500 words is more than 500 tokens. use the tokenizer to calculate the number of tokens.
as stated in the official openai article:

depending on the model used, requests can use up to 4097 tokens shared
between prompt and completion. if your prompt is 4000 tokens, your
completion can be 97 tokens at most.
the limit is currently a technical limitation, but there are often
creative ways to solve problems within the limit, e.g. condensing your
prompt, breaking the text into smaller pieces, etc.

switch text-davinci-001 for a gpt-3 model because the token limits are higher.
gpt-3 models:


regarding double quotes in json
you can escape double quotes in json by using \ in front of double quotes like this:
""this is how you can escape \""double quotes\"" in json.""

but... this is more of a quick fix. for proper solution, see @adyson's comment above:

don't build your json by hand like that. make a php object / array
with the correct structure, and then use json_encode() to turn it into
valid json, it will automatically handle any escaping etc which is
needed, and you can also use the options to tweak certain things about
the output - check the php documentation.


edit 1
you need to set the max_tokens parameter higher. otherwise, the output will be shorter than your input. you will not get the whole fixed text back, but just a part of it.

edit 2
now you set the max_tokens parameter too high! if you set max_tokens = 5000, this is too much even for the most capable gpt-3 model (i.e., text-davinci-003). the prompt and the completion together can be 4097 tokens.
you can figure this out if you take a look at the error you got:
""error"": {""message"": ""this model's maximum context length is 4097 tokens, however you requested 6450 tokens (1450 in your prompt; 5000 for the completion). please reduce your prompt; or completion length.""}",https://stackoverflow.com/questions/75373129,javascript,07-02-2023 12:07,3087.0,0.0,2.0,True,10-06-2024 12:17,15-07-2023 22:21
74473271,attributeerror: &#39;numpy.float64&#39; object has no attribute &#39;cpu&#39;,"i am trying to run bert and train a model using pytorch.
i am not sure why i am getting this error after finishing the first epoch.
i am using this code link
history = defaultdict(list)
best_accuracy = 0

for epoch in range(epochs):
    
    # show details 
    print(f""epoch {epoch + 1}/{epochs}"")
    print(""-"" * 10)
    
    train_acc, train_loss = train_epoch(
        model,
        train_data_loader,
        loss_fn,
        optimizer,
        device,
        scheduler,
        len(df_train)
    )
    
    print(f""train loss {train_loss} accuracy {train_acc}"")
    
    # get model performance (accuracy and loss)
    val_acc, val_loss = eval_model(
        model,
        val_data_loader,
        loss_fn,
        device,
        len(df_val)
    )
    
    print(f""val   loss {val_loss} accuracy {val_acc}"")
    print()
    
    history['train_acc'].append(train_acc.cpu())
    history['train_loss'].append(train_loss.cpu())
    history['val_acc'].append(val_acc.cpu())
    history['val_loss'].append(val_loss.cpu())
    
    # if we beat prev performance
    if val_acc > best_accuracy:
        torch.save(model.state_dict(), 'best_model_state.bin')
        best_accuracy = val_acc

here is the output and the error message
image
it is a first time for me to work with pytorch. any ideas how to fix the error>","['python', 'pytorch', 'bert-language-model']",74473473,"i checked kaggle link and i see that there is no cpu() reference as you have posted in your code. it should simply be:
history['train_acc'].append(train_acc)
history['train_loss'].append(train_loss)
history['val_acc'].append(val_acc)
history['val_loss'].append(val_loss)",https://stackoverflow.com/questions/74473271,python,17-11-2022 09:47,1418.0,1.0,1.0,True,17-11-2022 10:01,17-11-2022 09:50
54323427,how to fill in the blank using bidirectional rnn and pytorch?,"i am trying to fill in the blank using a bidirectional rnn and pytorch. 
the input will be like: the dog is _____, but we are happy he is okay.
the output will be like: 
1. hyper (perplexity score here) 
2. sad (perplexity score here) 
3. scared (perplexity score here)

i discovered this idea here: 
import torch, torch.nn as nn
from torch.autograd import variable

text = ['bos', 'how', 'are', 'you', 'eos']
seq_len = len(text)
batch_size = 1
embedding_size = 1
hidden_size = 1
output_size = 1

random_input = variable(
    torch.floattensor(seq_len, batch_size, embedding_size).normal_(), requires_grad=false)

bi_rnn = torch.nn.rnn(
    input_size=embedding_size, hidden_size=hidden_size, num_layers=1, batch_first=false, bidirectional=true)

bi_output, bi_hidden = bi_rnn(random_input)

# stagger
forward_output, backward_output = bi_output[:-2, :, :hidden_size], bi_output[2:, :, hidden_size:]
staggered_output = torch.cat((forward_output, backward_output), dim=-1)

linear = nn.linear(hidden_size * 2, output_size)

# only predict on words
labels = random_input[1:-1]

# for language models, use cross-entropy :)
loss = nn.mseloss()
output = loss(linear(staggered_output), labels)

i am trying to reimplement the code above found at the bottom of the blog post. i am new to pytorch and nlp, and can't understand what the input and output to the code is.
question about the input: i am guessing the input are the few words that are given. why does one need beginning of sentence and end of sentence tags in this case? why don't i see the input being a corpus on which the model is trained like other classic nlp problems? i would like to use the enron email corpus to train the rnn.
question about the output: i see the output is a tensor. my understanding is the tensor is a vector, so maybe a word vector in this case. how can you use the tensor to output the words themselves?","['python', 'nlp', 'pytorch']",54719164,"as this question is rather open-ended i will start from the last parts, moving towards the more general answer to the main question posed in the title.
quick note: as pointed in the comments by @qusai alothman, you should find a better resource on the topic, this one is rather sparse when it comes to necessary informations.
additional note: full code for the process described in the last section would take way too much space to provide as an exact answer, it would be more of a blog post. i will highlight possible steps one should take to create such a network with helpful links as we go along.
final note: if there is anything dumb down there below (or you would like to expand the answer in any way or form, please do correct me/add info by posting a comment below).
question about the input
input here is generated from the random normal distribution and has no connection to the actual words. it is supposed to represent word embeddings, e.g. representation of words as numbers carrying semantic (this is important!) meaning (sometimes depending on the context as well (see one of the current state of the art approaches, e.g. bert)).
shape of the input
in your example it is provided as:
seq_len, batch_size, embedding_size,
where

seq_len - means length of a single sentence (varies across your
dataset), we will get to it later.
batch_size - how many sentences
should be processed in one step of forward pass (in case of
pytorch it is the forward method of class inheriting from
torch.nn.module)
embedding_size - vector with which one word is represented (it
might range from the usual 100/300 using word2vec up to 4096 or
so using the more recent approaches like the bert mentioned
above)

in this case it's all hard-coded of size one, which is not really useful for a newcomer, it only outlines the idea that way.
why does one need beginning of sentence and end of sentence tags in this case?
correct me if i'm wrong, but you don't need it if your input is separated into sentences. it is used if you provide multiple sentences to the model, and want to indicate unambiguously the beginning and end of each (used with models which depend on the previous/next sentences, it seems to not be the case here). those are encoded by special tokens (the ones which are not present in the entire corpus), so neural network ""could learn"" they represent end and beginning of sentence (one special token for this approach would be enough).
if you were to use serious dataset, i would advise to split your text using libraries like spacy or nltk (the first one is a pleasure to use imo), they do a really good job for this task.
you dataset might be already splitted into sentences, in those cases you are kind of ready to go.
why don't i see the input being a corpus on which the model is trained like other classic nlp problems?
i don't recall models being trained on the corpuses as is, e.g. using strings. usually those are represented by floating-points numbers using:

simple approaches, e.g. bag of
words or
tf-idf
more sophisticated ones, which provide some information about word
relationships (e.g. king is more semantically related to queen
than to a, say, banana). those were already linked above, some
other noticeable might be
glove or
elmo and tons of other creative
approaches.

question about the output
one should output indices into embeddings, which in turn correspond to words represented by a vector (more sophisticated approach mentioned above).
each row in such embedding represents a unique word and it's respective columns are their unique representations (in pytorch, first index might be reserved for the words for which a representation is unknown [if using pretrained embeddings], you may also delete those words, or represent them as aj average of sentence/document, there are some other viable approaches as well).
loss provided in the example
# for language models, use cross-entropy :)
loss = nn.mseloss()

for this task it makes no sense, as mean squared error is a regression metric, not a classification one.
we want to use one for classification, so softmax should be used for multiclass case (we should be outputting numbers spanning [0, n], where n is the number of unique words in our corpus).
pytorch's crossentropyloss already takes logits (output of last layer without activation like softmax) and returns loss value for each example. i would advise this approach as it's numerically stable (and i like it as the most minimal one).
i am trying to fill in the blank using a bidirectional rnn and pytorch
this is a long one, i will only highlight steps i would undertake in order to create a model whose idea represents the one outlined in the post.
basic preparation of dataset
you may use the one you mentioned above or start with something easier like 20 newsgroups from scikit-learn.
first steps should be roughly this:

scrape the metadata (if any) from your dataset (those might be html tags, some headers etc.)
split your text into sentences using a pre-made library (mentioned above)

next, you would like to create your target (e.g. words to be filled) in each sentence.
each word should be replaced by a special token (say <target-token>) and moved to target.
example:

sentence: neural networks can do some stuff.

would give us the following sentences and it's respective targets:

sentence: <target-token> networks can do some stuff. target: neural
sentence: neural <target-token> can do some stuff. target: networks
sentence: neural networks <target-token> do some stuff. target: can
sentence: neural networks can <target-token> some stuff. target: do
sentence: neural networks can do <target-token> stuff. target: some
sentence: neural networks can do some <target-token>. target: some
sentence: neural networks can do some stuff <target-token> target: .

you should adjust this approach to the problem at hand by correcting typos if there are any, tokenizing, lemmatizing and others, experiment!
embeddings
each word in each sentence should be replaced by an integer, which in turn points to it embedding.
i would advise you to use a pre-trained one. spacy provides word vectors, but another interesting approach i would highly recommend is in the open source library flair.
you may train your own, but it would take a lot of time + a lot of data for unsupervised training, and i think it is way beyond the scope of this question.
data batching
one should use pytorch's torch.utils.data.dataset and torch.utils.data.dataloader.
in my case, a good idea is was to provide custom collate_fn to dataloader, which is responsible for creating padded batches of data (or represented as torch.nn.utils.rnn.packedsequence already).
important: currently, you have to sort the batch by length (word-wise) and keep the indices able to ""unsort"" the batch into it's original form, you should remember that during implementation. you may use torch.sort for that task. in future versions of pytorch, there is a chance, one might not have to do that, see this issue.
oh, and remember to shuffle your dataset using dataloader, while we're at it.
model
you should create a proper model by inheriting from torch.nn.module. i would advise you to create a more general model, where you can provide pytorch's cells (like gru, lstm or rnn), multilayered and bidirectional (as is described in the post).
something along those lines when it comes to model construction:
import torch


class filler(torch.nn.module):
    def __init__(self, cell, embedding_words_count: int):
        self.cell = cell
        # we want to output vector of n
        self.linear = torch.nn.linear(self.cell.hidden_size, embedding_words_count)

    def forward(self, batch):
        # assuming batch was properly prepared before passing into the network
        output, _ = self.cell(batch)
        # batch shape[0] is the length of longest already padded sequence
        # batch shape[1] is the length of batch, e.g. 32
        # here we create a view, which allows us to concatenate bidirectional layers in general manner
        output = output.view(
            batch.shape[0],
            batch.shape[1],
            2 if self.cell.bidirectional else 1,
            self.cell.hidden_size,
        )

        # here outputs of bidirectional rnns are summed, you may concatenate it
        # it makes up for an easier implementation, and is another often used approach
        summed_bidirectional_output = output.sum(dim=2)
        # linear layer needs batch first, we have to permute it.
        # you may also try with batch_first=true in self.cell and prepare your batch that way
        # in such case no need to permute dimensions
        linear_input = summed_bidirectional_output.permute(1, 0, 2)
        return self.linear(embedding_words_count)

as you can see, information about shapes can be obtained in a general fashion. such approach will allow you to create a model with how many layers you want, bidirectional or not (batch_first argument is problematic, but you can get around it too in a general way, left it out for improved clarity), see below:
model = filler(
    torch.nn.gru(
        # size of your embeddings, for bert it could be 4096, for spacy's word2vec 300
        input_size=300,
        hidden_size=100,
        num_layers=3,
        batch_first=false,
        dropout=0.4,
        bidirectional=true,
    ),
    # how many unique words are there in your dataset
    embedding_words_count=10000,
)

you may pass torch.nn.embedding into your model (if pretrained and already filled), create it from numpy matrix or plethora of other approaches, it's highly dependent how your structure your code exactly. still, please, make your code more general, do not hardcode shapes unless it's totally necessary (usually it's not).
remember it's only a showcase, you will have to tune and fix it on your own.
this implementation returns logits and no softmax layer is used. if you wish to calculate perplexity, you may have to add it in order to obtain a correct probability distribution across all possible vectors.
btw: here is some info on concatenation of bidirectional output of rnn.
model training
i would highly recommend pytorch ignite as it's quite customizable, you can log a lot of info using it, perform validation and abstract cluttering parts like for loops in training.
oh, and split your model, training and others into separate modules, don't put everything into one unreadable file.
final notes
this is the outline of how i would approach this problem, you may have more fun using attention networks instead of merely using the last output layer as in this example, though you shouldn't start with that.
and please check pytorch's 1.0 documentation and do not follow blindly tutorials or blog posts you see online as they might be out of date really fast and quality of the code varies enormously. for example torch.autograd.variable is deprecated as can be seen in the link.",https://stackoverflow.com/questions/54323427,python,23-01-2019 09:00,2384.0,8.0,1.0,True,11-12-2021 09:40,13-02-2019 18:52
52047163,use natural language processing to to split bad &amp; good comments from an employee survey,"so bit of a long shot here, and i apologize for the lack of information. however, i'm struggling to even know where to look now. 
so i'm trying to split good and bad comments from a made-up survey of employees at a random company. all i have is a dataframe consisting of the comment an employee has made along with their managers id code. the idea is to try and see how many good and/or bad comments are associated with a manager via their id.
import pandas as pd 
trial_text=pd.read_csv(""trial.csv"")
trial_text.head()

   managercode              comment
0        ab123  great place to work
1        ab123  need more training
2        ab123  hate working here
3        ab124  always late home
4        ab124  manager never listens

i've used nltk quite a lot for data sets that include a lot more information so anything nltk based won't be a problem. like i say, with what i have, ""google"" has far too much information that i don't know where to begin (or that is useful)! if there's anyone that might just have a suggestion that could put me on track that would be great!
thanks","['python', 'nlp', 'nltk']",52056089,"you need sentiment analysis. i don't think you will get amazing results with an off-the-shelf model though, because your responses are quite short and quite domain specific. in case you want to try anyway, here is an example of how to use the vader model with nltk:
from nltk.sentiment.vader import sentimentintensityanalyzer
sid = sentimentintensityanalyzer()
sid.polarity_scores('great place to work')
>>> {'neg': 0.0, 'neu': 0.423, 'pos': 0.577, 'compound': 0.6249}
sid.polarity_scores('manager never listens')
>>> {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}

as you can see, your mileage may vary.
if you have lots of responses (thousands), a more viable strategy would be to manually label a sample of e.g. a few tens to a few hundred and to train your own sentiment classifier. here are some good tutorials of how to do this with either nltk or sklearn",https://stackoverflow.com/questions/52047163,python,27-08-2018 21:23,384.0,0.0,3.0,True,20-09-2024 11:43,29-08-2018 07:23
61158024,typeerror: lemmatize() missing 1 required positional argument: &#39;word,"i have an array of for each row in a csv file as followed:
[['thxx'], ['too', 'late', 'now', 'dumbass'], ['you', 'ï¿½ï¿½ï¿½', 're', 'so', 'dumb', '?', '?'], ['thxxx'], ['i', 'ï¿½ï¿½ï¿½', 'd', 'be', 'fucked']]

when i try to pass this on to the lemmatizer like this:
from nltk.stem import wordnetlemmatizer
lemmatized_words = [wordnetlemmatizer.lemmatize(word) for word in tokened_text]
print(lemmatized_words)

i get the following error:
typeerror: lemmatize() missing 1 required positional argument: 'word'

why is that?
as a side question: do i need to do this before passing this for vectorization? i am building an machine learning model and saw the function countvectorizer in sci kit learn but could not find any information that it does lemmatization and so on beforehand a","['machine-learning', 'nlp', 'nltk']",61158876,"there are some things wrong in your code:

wordnetlemmatizer is a class, you need to instanciate it first 
tokened_text is a nested list, hence you need a nested list-comprehension to preserve the structure. also lemmatize is expecting a string.

here's how you could do this:
from nltk.stem import wordnetlemmatizer

wnl = wordnetlemmatizer()

lemmatized_words = [[wnl.lemmatize(word) for word in l] for l in tokened_text]",https://stackoverflow.com/questions/61158024,machine-learning,11-04-2020 14:03,6963.0,1.0,4.0,True,11-08-2021 09:58,11-04-2020 14:08
77792137,how to fix the learning-rate for huggingface&#180;s trainer?,"i'm training model with the following parameters:
seq2seqtrainingarguments(
    output_dir                   = ""./out"", 
    overwrite_output_dir         = true,
    do_train                     = true,
    do_eval                      = true,
    
    per_device_train_batch_size  = 2, 
    gradient_accumulation_steps  = 4,
    per_device_eval_batch_size   = 8, 
    
    learning_rate                = 1.25e-5,
    warmup_steps                 = 1,
    
    save_total_limit             = 1,
       
    evaluation_strategy          = ""epoch"",
    save_strategy                = ""epoch"",
    logging_strategy             = ""epoch"",  
    num_train_epochs             = 5,   
    
    gradient_checkpointing       = true,
    fp16                         = true,    
        
    predict_with_generate        = true,
    generation_max_length        = 225,
          
    report_to                    = [""tensorboard""],
    load_best_model_at_end       = true,
    metric_for_best_model        = ""wer"",
    greater_is_better            = false,
    push_to_hub                  = false,
)

i assume that warmup_steps=1 fixes the learning rate.
however, after finished training i'm looking on the file trainer_state.json, and it seems that the learning rate is not fixed.
here are the values of learning_rate and step:
learning_rate,     steps
1.0006 e-05       1033
7.5062 e-06       2066
5.0058 e-06       3099
2.5053 e-06       4132
7.2618 e-09       5165

it seems that the learning rate is not fixed on 1.25e-5 (after step 1). what am i missing? how to i fix the learning rate.","['machine-learning', 'deep-learning', 'huggingface-transformers', 'huggingface-trainer', 'learning-rate']",77793731,"a warm-up is in general an increase of the learning rate. it starts at 0 and then increases linearly over 1(here) step to the specified learning rate of 1.25e-5.
afterwards by default a linear (in other cases a cosine) learning-rate scheduler decays your learning-rate.
to disable the decay add lr_scheduler_type='constant'.
if i recall correctly, this also disables the warmup.
if you want warmup and afterwards a constant rate use constant_with_warmup instead.
edit: valid scheduler types are defined in trainer_utils.py, in the class schedulertype:
class schedulertype(explicitenum):
    """"""
    scheduler names for the parameter `lr_scheduler_type` in [`trainingarguments`].
    by default, it uses ""linear"". internally, this retrieves `get_linear_schedule_with_warmup` scheduler from [`trainer`].
    scheduler types:
       - ""linear"" = get_linear_schedule_with_warmup
       - ""cosine"" = get_cosine_schedule_with_warmup
       - ""cosine_with_restarts"" = get_cosine_with_hard_restarts_schedule_with_warmup
       - ""polynomial"" = get_polynomial_decay_schedule_with_warmup
       - ""constant"" =  get_constant_schedule
       - ""constant_with_warmup"" = get_constant_schedule_with_warmup
       - ""inverse_sqrt"" = get_inverse_sqrt_schedule
       - ""reduce_lr_on_plateau"" = get_reduce_on_plateau_schedule
       - ""cosine_with_min_lr"" = get_cosine_with_min_lr_schedule_with_warmup
       - ""warmup_stable_decay"" = get_wsd_schedule
    """"""

    linear = ""linear""
    cosine = ""cosine""
    cosine_with_restarts = ""cosine_with_restarts""
    polynomial = ""polynomial""
    constant = ""constant""
    constant_with_warmup = ""constant_with_warmup""
    inverse_sqrt = ""inverse_sqrt""
    reduce_on_plateau = ""reduce_lr_on_plateau""
    cosine_with_min_lr = ""cosine_with_min_lr""
    warmup_stable_decay = ""warmup_stable_decay""",https://stackoverflow.com/questions/77792137,machine-learning,10-01-2024 09:14,4614.0,1.0,2.0,True,16-02-2025 22:20,11-06-2024 07:42
68080447,error calling adapt in textvectorization keras,"i have the following code, with a custom standardization definition.
def custom_standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    regex = tf.strings.regex_replace(lowercase, r'[^\w]', ' ')
    regex = tf.strings.regex_replace(regex, ' +', ' ')

    return tf.strings.split(regex)

vectorize_layer = tf.keras.layers.experimental.preprocessing.textvectorization(
    standardize=custom_standardization,
    max_tokens=50000,
    output_mode=""int"",
    output_sequence_length=100,
)

but when i call adapt, like this, i got the next error
vectorize_layer.adapt(['the cat'])
# error:
invalidargumenterror: expected 'tf.tensor(false, shape=(), dtype=bool)' to be true. summarized data: b'the given axis (axis = 2) is not squeezable!'

according to their explication,

when using a custom callable for split, the data received by the callable will have the 1st dimension squeezed out - instead of [[""string to split""], [""another string to split""]], the callable will see [""string to split"", ""another string to split""]. the callable should return a tensor with the first dimension containing the split tokens - in this example, we should see something like [[""string"", ""to"", ""split""], [""another"", ""string"", ""to"", ""split""]]. this makes the callable site natively compatible with tf.strings.split().

blockquote source
but i can't see where the error is
edit: i've done some research in my code
when i pass an array like ['the other day was raining', 'please call me later'], the function custom_standardization() returns something like this
[['the', 'other', 'day', 'was', 'raining'], ['pleasse', 'call', 'me', 'later']]

so it seems that it is not respecting to have same shape. why it changes thought?","['python', 'tensorflow', 'keras', 'nlp']",68944712,"i referred the document you shared earlier. following was mentioned for custom standardize

when using a custom callable for standardize, the data received by
the callable will be exactly as passed to this layer. the callable
should return a tensor of the same shape as the input.

so i changed replaced the return tf.strings.split(regex) with return regex (as splitting is changing the shape here). please try like this.
import tensorflow as tf

def custom_standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    regex = tf.strings.regex_replace(lowercase, r'[^\w]', ' ')
    regex = tf.strings.regex_replace(regex, ' +', ' ')

    return regex

vectorize_layer = tf.keras.layers.experimental.preprocessing.textvectorization(
    standardize=custom_standardization,
    max_tokens=50000,
    output_mode=""int"",
    output_sequence_length=100,
)

#checking input shape and output shape are shape or not 
input = tf.constant([[""foo !  @ qux  #bar""], [""qux baz""]])
print(input)
print(custom_standardization(input))

vectorize_layer.adapt([""foo qux bar""])

providing gist for reference.",https://stackoverflow.com/questions/68080447,python,22-06-2021 08:51,89.0,0.0,1.0,True,09-09-2021 21:25,22-06-2021 09:09
76187256,"importerror: urllib3 v2.0 only supports openssl 1.1.1+, currently the &#39;ssl&#39; module is compiled with libressl 2.8.3","after pip install openai, when i try to import openai, it shows this error:

the 'ssl' module of urllib3 is compile with libressl not openssl

i just followed a tutorial on a project about using api of openai. but when i get to the first step which is the install and import openai, i got stuck. and i tried to find the solution for this error but i found nothing.
here is the message after i try to import openai:
python 3.9.6 (default, mar 10 2023, 20:16:38)
[clang 14.0.3 (clang-1403.0.22.14.1)] on darwin
type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

>>> import openai

traceback (most recent call last):
  file ""<stdin>"", line 1, in <module>
  file ""/users/yule/library/python/3.9/lib/python/site-packages/openai/__init__.py"", line 19, in <module>
    from openai.api_resources import (
  file ""/users/mic/library/python/3.9/lib/python/site-packages/openai/api_resources/__init__.py"", line 1, in <module>
    from openai.api_resources.audio import audio  # noqa: f401
  file ""/users/mic/library/python/3.9/lib/python/site-packages/openai/api_resources/audio.py"", line 4, in <module>
    from openai import api_requestor, util
  file ""/users/mic/library/python/3.9/lib/python/site-packages/openai/api_requestor.py"", line 22, in <module>
    import requests
  file ""/users/mic/library/python/3.9/lib/python/site-packages/requests/__init__.py"", line 43, in <module>
    import urllib3
  file ""/users/mic/library/python/3.9/lib/python/site-packages/urllib3/__init__.py"", line 38, in <module>
    raise importerror(
importerror: urllib3 v2.0 only supports openssl 1.1.1+, currently the 'ssl' module is compiled with libressl 2.8.3. see: 

i tried to --upgrade the urllib3, but it is still not working. the result is:
pip3 install --upgrade urllib3
defaulting to user installation because normal site-packages is not writeable
requirement already satisfied: urllib3 in ./library/python/3.9/lib/python/site-packages (2.0.2)","['python', 'openai-api', 'urllib3']",76187415,"the reason why the error message mentioned openssl 1.1.1+ and libressl 2.8.3 is that urllib3 v2.0 (the version you've installed) requires openssl 1.1.1+ to work properly, as it relies on some new features of openssl 1.1.1.
the issue is that the version of the 'ssl' module that is currently installed in your environment is compiled with libressl 2.8.3, which is not compatible with urllib3 v2.0.
to use urllib3 v2.0, you need an 'ssl' module compiled with openssl 1.1.1 or later, by trying:
brew install openssl@1.1

or you could use an older version of urllib3 that is compatible suc. for example urllib3 v1.26.6, which does not have a strict openssl version requirement.
you can force the version installing with this command:
pip install urllib3==1.26.6",https://stackoverflow.com/questions/76187256,python,06-05-2023 05:11,414804.0,175.0,19.0,True,05-12-2024 12:05,14-08-2023 00:20
70069026,how to use files in the answer api of openai,"as finally openai opened the gpt-3 related api publicly,
i am playing with it to explore and discover his potential.
i am trying the answer api, the simple example that is in the documentation:

i upload the .jsonl file as indicated, and i can see it succesfully uploaded with the openai.file.list() api.
when i try to use it, unfortunately, i always get the same error:
>>> openai.file.create(purpose='answers', file=open('example.jsonl') )
<file file id=file-xxx at 0x7fbc9eca5e00> json: {
  ""bytes"": 140,
  ""created_at"": 1637597242,
  ""filename"": ""example.jsonl"",
  ""id"": ""file-xxx"",
  ""object"": ""file"",
  ""purpose"": ""answers"",
  ""status"": ""uploaded"",
  ""status_details"": null
}

#use the file in the api:
openai.answer.create(
    search_model=""ada"", 
    model=""curie"", 
    question=""which puppy is happy?"", 
    file=""file-xxx"", 
    examples_context=""in 2017, u.s. life expectancy was 78.6 years."", 
    examples=[[""what is human life expectancy in the united states?"", ""78 years.""]], 
    max_rerank=10,
    max_tokens=5,
    stop=[""\n"", ""<|endoftext|>""]
)
<some exception, then>
openai.error.invalidrequesterror: file is still processing.  check back later.


i have waited several hours, and i do not think this content deserve such a long wait...
do you know if it is a normal behaviour, or if i miss something?
thanks","['python', 'openai-api', 'gpt-3']",70157536,"after a few hours (the day after) the file metadata status changed from uploaded to processed and the file could be used in the answer api as stated in the documentation.
i think this need to be better documented in the original openai api reference.",https://stackoverflow.com/questions/70069026,python,22-11-2021 16:19,6853.0,4.0,1.0,True,15-01-2023 17:50,15-01-2023 17:50
77373522,how does an instance of pytorch&#39;s `nn.linear()` process a tuple of tensors?,"in the annotated transformer's implementation of multi-head attention, three tensors (query, key, value) are all passed to a nn.linear(d_model, d_model):
# some class definition ...
self.linears = clones(nn.linear(d_model, d_model), 4) # deep-copied list of nn.linear-modules concatenated via nn.modulelist
# more code ...
query, key, value = [
  lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
  for lin, x in zip(self.linears, (query, key, value))
]

my question: what happens at lin(x), when an instance of nn.linear() is called on the tuple (query, key, value)? is the tuple somehow concatenated to a tensor? if so, how - on which dimension are the tensors concatenated?","['python', 'machine-learning', 'pytorch', 'nlp', 'transformer-model']",77373814,"self.linears = clones(nn.linear(d_model, d_model), 4) # deep-copied list of nn.linear-modules concatenated via nn.modulelist
# more code ...
query, key, value = [
  lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
  for lin, x in zip(self.linears, (query, key, value))
]

actually, the nn.linear does not process input as a tuple of a q,k,v.
in your code, the result similar like this
out_q = self.linears[0](q)
out_k = self.linears[1](k)
out_v = self.linears[2](v)

when you use zip(iterable a, iterable b)
so you will get the pairs (a[0], b[0]) (a[1], b[1]) ,... independently
or more specific
query = self.linears[0](query)
key = self.linears[1](key)
value = self.linears[2](value)",https://stackoverflow.com/questions/77373522,python,27-10-2023 10:43,178.0,0.0,1.0,True,27-10-2023 13:33,27-10-2023 13:33
70051704,efficient way to find an approximate string match and replacing with predefined string,"i need to build a ner system (named entity recognition). for simplicity, i am doing it by using approximate string matching as input can contain typos and other minor modifications. i have come across some great libraries like: fuzzywuzzy or even faster rapidfuzz. but unfortunately i didn't find a way to return the position where the match occurs. as, for my purpose i not only need to find the match, but also i need to know where the match happened. as for ner, i need to replace those matches with some predefined string.
for example, if any one of the line is found in input string i want to replace them with the string company_name:
google
microsoft
facebook
international business machine

like, input: s/he works at google will be transformed to s/he works at company_name.
you can safely assume that, all the input and the pattern to match are already preprocessed and most importantly they are in lower-case now. so, there is no problem with case-sensitivity.
currently, i have approached with a sliding window technique. and a sliding window is passed over the input string from left to right and this window has exactly the size of the pattern we want to match. for example, when i want to match with international business machine, i run a sliding window of size 3 from left to right and try to find the best match by observing each 3 consecutive tokens at the same time with a stride of 1. i do believe, it is not the best way to do it, also it cannot find the best match.
so, what is the efficient way to find the best possible match along with the quantification on the found match (how much they are similar) and the position of the match(es), such that we can replace them with a given fixed string (if the calculated similarity is not less than a threshold)? obviously, a single input may contain multiple portions to be replaced, each of them will be replaced separately, like: google and microsoft are big companies will become company_name and company_name are big companies etc.
edit: fixed link to rapidfuzz","['python', 'nlp', 'named-entity-recognition', 'fuzzy-search', 'fuzzywuzzy']",70091430,"it seems modules fuzzywuzzy and rapidfuzz don't have function for this. you could try to use process.extract() or process.extractone() but it would need to split text in smaller parts (ie. words) and check every part separatelly. for longer words like international business machine it would need to split in part with 3 words - so it would need even more work.

i think you need rather module fuzzysearch
import fuzzysearch

words = ['google', 'microsoft', 'facebook', 'international business machine']

text = 'google and microsoft are big companies like international business machine'

print(' text:', text)
print('---')
    
for word in sorted(words, key=len, reverse=true):
    print(' word:', word)
    
    results = fuzzysearch.find_near_matches(word, text, max_l_dist=1)
    print('found:', results)
    
    for item in reversed(results):
        text = text[:item.start] + 'company' + text[item.end:]
    print(' text:', text)
    
    print('---')

result:
 text: google and microsoft are big companies like facebook international business machine
---
 word: international business machine
found: [match(start=53, end=83, dist=0, matched='international business machine')]
 text: google and microsoft are big companies like facebook company
---
 word: microsoft
found: [match(start=11, end=20, dist=1, matched='microsoft')]
 text: google and company are big companies like facebook company
---
 word: facebook
found: [match(start=42, end=50, dist=0, matched='facebook')]
 text: google and company are big companies like company company
---
 word: google
found: [match(start=0, end=6, dist=1, matched='google')]
 text: company and company are big companies like company company

if it finds many results for one word then it is better to start replacing at last position to keep other words in the same place. and this is why i use reversed().
i would start also with the longest word/name so later it still can search shorter words like business. and this is why i use sorted(..., key=len, reverse=true)
but i'm not sure if it works exactly as you want. maybe it will have problem when words are more incorrect.

edit:
i tried to use fuzzywuzzy for this and created this version but only for names with single word. for international business machine it would need some other idea.
it split full text into words and compare words. later replace word wich have ration > 80
words = ['google', 'microsoft', 'facebook', 'international business machine']

text = 'google and microsoft are big companies like international business machine'

# ---

import fuzzywuzzy.fuzz as fuzz
#import fuzzywuzzy.process

new_words = []

for part in text.split():

    matches = []

    for word in words:
        result = fuzz.token_sort_ratio(part, word)
        matches.append([result, part, word])
        #print([result, part, word])

    matches = sorted(matches, reverse=true)

    if matches and matches[0][0] > 80:
        new_words.append('company')
    else:
        new_words.append(matches[0][1])
        
print("" "".join(new_words))

result:
[100, 'google', 'google']
[27, 'google', 'microsoft']
[29, 'google', 'facebook']
[17, 'google', 'international business machine']
[0, 'and', 'google']
[0, 'and', 'microsoft']
[18, 'and', 'facebook']
[12, 'and', 'international business machine']
[27, 'microsoft', 'google']
[100, 'microsoft', 'microsoft']
[35, 'microsoft', 'facebook']
[15, 'microsoft', 'international business machine']
[22, 'are', 'google']
[17, 'are', 'microsoft']
[36, 'are', 'facebook']
[12, 'are', 'international business machine']
[22, 'big', 'google']
[17, 'big', 'microsoft']
[18, 'big', 'facebook']
[12, 'big', 'international business machine']
[27, 'companies', 'google']
[33, 'companies', 'microsoft']
[24, 'companies', 'facebook']
[26, 'companies', 'international business machine']
[40, 'like', 'google']
[15, 'like', 'microsoft']
[17, 'like', 'facebook']
[18, 'like', 'international business machine']
[21, 'international', 'google']
[27, 'international', 'microsoft']
[19, 'international', 'facebook']
[60, 'international', 'international business machine']
[14, 'business', 'google']
[24, 'business', 'microsoft']
[12, 'business', 'facebook']
[42, 'business', 'international business machine']
[15, 'machine', 'google']
[25, 'machine', 'microsoft']
[40, 'machine', 'facebook']
[38, 'machine', 'international business machine']
company and company are big companies like international business machine


edit:
second version which check also names with many words
all_names = ['google', 'microsoft', 'facebook', 'international business machine']

text = 'google and microsoft are big companies like international business machine'

# ---

import fuzzywuzzy.fuzz as fuzz


for name in all_names:

    length = len(name.split(' ')) # how many words has name 
    print('name length:', length, '|', name)

    words = text.split()  # split text into words

    # compare name with all words in text
    
    matches = []
    
    for index in range(0, len(words)-length+1):
        # join words if name has more then 1 word
        part = "" "".join(words[index:index+length])
        #print('part:', part)
        
        result = fuzz.token_sort_ratio(part, name)
        matches.append([result, name, part, [index, index+length]])

        print([result, name, part, [index, index+length]])
        
    # reverse to start at last position
    matches = list(reversed(matches))

    max_match = max(x[0] for x in matches)
    print('max match:', max_match)

    # replace
    if max_match > 80:
        for match in matches:
            if  match[0] == max_match:
                idx = match[3]  
                words = words[:idx[0]] + ['company'] + words[idx[1]:]

    text = "" "".join(words)
    print('text:', text)
    print('---')

result:
ame length: 1 | google
[100, 'google', 'google', [0, 1]]
[0, 'google', 'and', [1, 2]]
[27, 'google', 'microsoft', [2, 3]]
[22, 'google', 'are', [3, 4]]
[22, 'google', 'big', [4, 5]]
[27, 'google', 'companies', [5, 6]]
[40, 'google', 'like', [6, 7]]
[21, 'google', 'international', [7, 8]]
[14, 'google', 'business', [8, 9]]
[15, 'google', 'machine', [9, 10]]
max match: 100
text: company and microsoft are big companies like international business machine
---
name length: 1 | microsoft
[25, 'microsoft', 'company', [0, 1]]
[0, 'microsoft', 'and', [1, 2]]
[100, 'microsoft', 'microsoft', [2, 3]]
[17, 'microsoft', 'are', [3, 4]]
[17, 'microsoft', 'big', [4, 5]]
[33, 'microsoft', 'companies', [5, 6]]
[15, 'microsoft', 'like', [6, 7]]
[27, 'microsoft', 'international', [7, 8]]
[24, 'microsoft', 'business', [8, 9]]
[25, 'microsoft', 'machine', [9, 10]]
max match: 100
text: company and company are big companies like international business machine
---
name length: 1 | facebook
[27, 'facebook', 'company', [0, 1]]
[18, 'facebook', 'and', [1, 2]]
[27, 'facebook', 'company', [2, 3]]
[36, 'facebook', 'are', [3, 4]]
[18, 'facebook', 'big', [4, 5]]
[24, 'facebook', 'companies', [5, 6]]
[17, 'facebook', 'like', [6, 7]]
[19, 'facebook', 'international', [7, 8]]
[12, 'facebook', 'business', [8, 9]]
[40, 'facebook', 'machine', [9, 10]]
max match: 40
text: company and company are big companies like international business machine
---
name length: 3 | international business machine
[33, 'international business machine', 'company and company', [0, 3]]
[31, 'international business machine', 'and company are', [1, 4]]
[31, 'international business machine', 'company are big', [2, 5]]
[34, 'international business machine', 'are big companies', [3, 6]]
[38, 'international business machine', 'big companies like', [4, 7]]
[69, 'international business machine', 'companies like international', [5, 8]]
[88, 'international business machine', 'like international business', [6, 9]]
[100, 'international business machine', 'international business machine', [7, 10]]
max match: 100
text: company and company are big companies like company


edit:
version with fuzzywuzzy.process
this time i don't have positions and i simply use standard text.replace(item[0], 'company').
i think in most situations it will work correctly and it doesn't need better method.
this time i check it on text with mistakes:
'gogle and mikro-soft are big companies like fasebok and internat. businnes machin'


all_names = ['google', 'microsoft', 'facebook', 'international business machine']

text = 'google and microsoft are big companies like facebook and international business machine'

# text with mistakes
text = 'gogle and mikro-soft are big companies like fasebok and internat. businnes machin'

# ---

import fuzzywuzzy.process
#import fuzzywuzzy.fuzz

for name in sorted(all_names, key=len, reverse=true):
    lenght = len(name.split())

    words = text.split()
    words = ["" "".join(words[i:i+lenght]) for i in range(0, len(words)-lenght+1)]
    #print(words)

    #result = fuzzywuzzy.process.extractbests(name, words, scorer=fuzzywuzzy.fuzz.token_sort_ratio, score_cutoff=80)
    result = fuzzywuzzy.process.extractbests(name, words, score_cutoff=80)
    print(name, result)

    for item in result:
        text = text.replace(item[0], 'company')

print(text)",https://stackoverflow.com/questions/70051704,python,21-11-2021 03:54,7532.0,1.0,1.0,True,10-10-2023 12:37,10-10-2023 12:37
75176667,openai gpt-3 api error: &quot;cannot specify both model and engine&quot;,"so i'm working on some python code that works with chatgpt3. what it does is it sends a request with a prompt and then gets the reply, but i keep getting errors. the error is
traceback (most recent call last):
  file ""main.py"", line 16, in <module>
    print(response_json['choices'][0]['text'])
keyerror: 'choices'

here is my code:
import json
import requests
import os
data = {
    ""prompt"": ""what is the meaning of life?"",
    ""model"": ""text-davinci-002""
}

response = requests.post("" json=data, headers={
    ""content-type"": ""application/json"",
    ""authorization"": f""bearer {apikey}"",
})

response_json = json.loads(response.text)

print(response_json['choices'][0]['text'])


i do have an api key that is valid and the json code i don't get the json code.
{'error': {'message': 'cannot specify both model and engine', 'type': 'invalid_request_error', 'param': none, 'code': none}}

i have tried different api keys and that didn't work. i even looked up all the different models for chatgpt and it still doesn't work","['python', 'json', 'python-3.x', 'openai-api', 'gpt-3']",75182746,"all engines api endpoints are deprecated.

change the url from this...


...to this.


if you run test.py the openai api will return a completion. you'll get a different completion because the temperature parameter is not set to 0. i got the following completion:

the meaning of life is to find out and fulfil the purpose and meaning...

test.py
import json
import requests
import os

data = {
    ""prompt"": ""what is the meaning of life?"",
    ""model"": ""text-davinci-003""
}

response = requests.post("" json=data, headers={
    ""content-type"": ""application/json"",
    ""authorization"": f""bearer {apikey}""
})

response_json = json.loads(response.text)

print(response_json[""choices""][0][""text""])",https://stackoverflow.com/questions/75176667,python,19-01-2023 18:19,8006.0,1.0,2.0,True,12-08-2023 21:49,13-03-2023 13:49
9538425,"is there a database, api, or parsable text for getting verb conjugations?","this isn't directly a programming question, so i apologize in advance. i've been working on a grammar-free random sentence generator for a typing game i'd like to make, and i've been having a difficult time finding any parsable (or callable) data for getting verb conjugations. ultimately, if i can't find anything like this, i'm going to have to go through the dictionary i've created and add first-person singular and plural, second-person singular and plural, third-person singular and plural, simple past, past participle, and present participle forms for every irregular verb.
this wouldn't be a problem in many languages, but there are so many irregular english verbs that this could take a long, long time to do manually. i'm not against the worse option, but i want to make sure i'm not going to be wasting obscene hours doing it myself when there is some database i can use instead.
i've seen  and spoken with the creator, but he doesn't release his exact dictionary (just the classes for it). i've also seen sites like  which would be great for scraping, but that's a bit of a pain as well.
this question has been asked here before ( verb conjugations database ), but the question was left unanswered, and the asker alluded to solving the problem but never said what the solution was.",['nlp'],9543779,"morphadorner (java) has a simple verb conjugator (with online demo).
but if you are interested with an exhaustive listing you can check lexical tools' inflection variants. after downloading lexical tools, you will be importing the data to your database server. then you can just query the database using their library (java).
simplenlg also has this feature, and is very much related to lexical tool.",https://stackoverflow.com/questions/9538425,nlp,02-03-2012 18:24,10023.0,13.0,1.0,True,15-01-2025 04:58,23-05-2017 12:33
77533488,nlp pre-processing on two columns in data frame gives error,"i have the following data frame:
gmedatedf.head(2)





title
score
id
url
comms_num
body
timestamp




it's not about the money, it's about sending a...
55.0
l6ulcx

6.0
nan
2021-01-28 21:37:41


math professor scott steiner says the numbers ...
110.0
l6uibd

23.0
nan
2021-01-28 21:32:10




i have the following function to pre-process the text (with the proper libraries imported and so on):
def preprocess_text(text):
  # tokenize words
  tokens = word_tokenize(text.lower())

  # remove stopwords and non-alphabetic words, and lemmatize
  processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]

  return processed_tokens

then calling it on specific column:
gmedatedf.loc[:, 'body'] = gmedatedf['body'].fillna('nan').astype(str)
gmedatedfprocessed = gmedatedf['body'].apply(preprocess_text)

that works properly as expected. however, when iï¿½ï¿½ï¿½try to do it on two columns, like so:
gmedatedf.loc[:, 'title','body'] = gmedatedf['title', 'body'].fillna('nan').astype(str)
gmedatedfprocessed = gmedatedf['title', 'body'].apply(preprocess_text)
>
iï¿½ï¿½ï¿½get the following error:
   3802                 return self._engine.get_loc(casted_key)
   3803             except keyerror as err:
-> 3804                 raise keyerror(key) from err
   3805             except typeerror:
   3806                 # if we have a listlike key, _check_indexing_error will raise

keyerror: ('title', 'body')

iï¿½ï¿½ï¿½ve looked around, asked chatgptï¿½ï¿½ï¿½for some help, but iï¿½ï¿½ï¿½canï¿½ï¿½ï¿½t figure it out.
please, bear with me as iï¿½ï¿½ï¿½m still learning the basics of python.
why canï¿½ï¿½ï¿½t iï¿½ï¿½ï¿½give it a listlike key? and why is it a listlike key only when iï¿½ï¿½ï¿½have the two columns? when i call it only on gmedatedf.loc[:, 'body'] it is some kind of listlike key, no? so why would it not work otherwise?
iï¿½ï¿½ï¿½m confused, and donï¿½ï¿½ï¿","['python', 'pandas', 'dataframe', 'nlp', 'nltk']",77538239,"as the error suggests, using gmedatedf['title', 'body'] attempts to find a column in the dataframe under the following key: ('title', 'body'). no column in your dataframe is called that, therefore the code fails.
if you wish to select multiple columns at once, you need to provide them in a list, like so: gmedatedf[['title', 'body']]. for more information, head to the documentation page on data selection from a dataframe.
given your specific example, you will need to fix the data selection, and then use some string vectorisation, something like:
gmedatedfprocessed[['title', 'body']] = gmedatedf[['title', 'body']].apply(lambda x: preprocess_text(x.str))",https://stackoverflow.com/questions/77533488,python,22-11-2023 22:54,131.0,0.0,1.0,True,23-11-2023 16:02,23-11-2023 08:33
22159351,handling same words but from different documents,"i'm making a python class which calculates the tfidf weight of each word in a document. now in my dataset i have 50 documents. in these documents many words intersect, thus having multiple same word features but with different tfidf weight. so the question is how do i sum up all the weights into one singular weight?","['python', 'machine-learning', 'text-classification', 'tf-idf']",22159481,"first, let's get some terminology clear.  a term is a word-like unit in a corpus.  a token is a term at a particular location in a particular document.  there can be multiple tokens that use the same term.  for example, in my answer, there are many tokens that use the term ""the"".  but there is only one term for ""the"".
i think you are a little bit confused.  tf-idf style weighting functions specify how to make a per term score out of the term's token frequency in a document and the background token document frequency in the corpus for each term in a document.  tf-idf converts a document into a mapping of terms to weights.  so more tokens sharing the same term in a document will increase the corresponding weight for the term, but there will only be one weight per term.  there is no separate score for tokens sharing a term inside the doc.",https://stackoverflow.com/questions/22159351,python,03-03-2014 22:58,794.0,0.0,1.0,True,04-01-2024 20:05,04-01-2024 20:05
5107371,document analysis and tagging,"let's say i have a bunch of essays (thousands) that i want to tag, categorize, etc.  ideally, i'd like to train something by manually categorizing/tagging a few hundred, and then let the thing loose.
what resources (books, blogs, languages) would you recommend for undertaking such a task?  part of me thinks this would be a good fit for a bayesian classifier or even latent semantic analysis, but i'm not really familiar with either other than what i've found from a few ruby gems.
can something like this be solved by a bayesian classifier?  should i be looking more at semantic analysis/natural language processing?  or, should i just be looking for keyword density and mapping from there?","machine-learning, nlp, classification, bayesian, tagging",5191602,"wow, that's a pretty huge topic you are venturing into :)
there is definitely a lot of books and articles you can read about it but i will try to provide a short introduction. i am not a big expert but i worked on some of this stuff.
first you need to decide whether you are want to classify essays into predefined topics/categories (classification problem) or you want the algorithm to decide on different groups on its own (clustering problem). from your description it appears you are interested in classification.
now, when doing classification, you first need to create enough training data. you need to have a number of essays that are separated into different groups. for example 5 physics essays, 5 chemistry essays, 5 programming essays and so on. generally you want as much training data as possible but how much is enough depends on specific algorithms. you also need verification data, which is basically similar to training data but completely separate. this data will be used to judge quality (or performance in math-speak) of your algorithm.
finally, the algorithms themselves. the two i am familiar with are bayes-based and tf-idf based. for bayes, i am currently developing something similar for myself in ruby, and i've documented my experiences in my blog. if you are interested, just read this -  and if you have any follow up questions i will try to answer.
the tf-idf is a short for termfrequence - inversedocumentfrequency. basically the idea is for any given document to find a number of documents in training set that are most similar to it, and then figure out it's category based on that. for example if document d is similar to t1 which is physics and t2 which is physics and t3 which is chemistry, you guess that d is most likely about physics and a little chemistry.
the way it's done is you apply the most importance to rare words and no importance to common words. for instance 'nuclei' is rare physics word, but 'work' is very common non-interesting word. (that's why it's called inverse term frequency). if you can work with java, there is a very very good lucene library which provides most of this stuff out of the box. look for api for 'similar documents' and look into how it is implemented. or just google for 'tf-idf' if you want to implement your own",https://stackoverflow.com/q/5107371,"machine-learning, nlp, classification, bayesian, tagging",24-02-2011 16:20,2054.0,2.0,2.0,True,27-02-2025 12:57,27-02-2025 12:57
73257704,get contrastive_logits_per_image with flava model using huggingface library,"i have used a code of flava model from this link:


but i am getting the following error:
'flavamodeloutput' object has no attribute 'contrastive_logits_per_image'

i tried using flavaforpretraining model instead, so updated code was :
from pil import image
import requests
from transformers import flavaprocessor, flavaforpretraining

model = flavaforpretraining.from_pretrained(""facebook/flava-full"")
processor = flavaprocessor.from_pretrained(""facebook/flava-full"")

url = ""
image = image.open(requests.get(url, stream=true).raw)

inputs = processor(text=[""a photo of a cat""], images=image, return_tensors=""pt"", padding=true, return_codebook_pixels = true)

inputs.update(
    {
        ""input_ids_masked"": inputs.input_ids,
    }
)

outputs = model(**inputs)

logits_per_image = outputs.contrastive_logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities

but i'm still getting this as error:
/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py:714: futurewarning: the `device` argument is deprecated and will be removed in v5 of transformers.
  ""the `device` argument is deprecated and will be removed in v5 of transformers."", futurewarning

---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-44-bdb428b8184a> in <module>()
----> 1 outputs = model(**inputs)

2 frames
/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1128         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1129                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1130             return forward_call(*input, **kwargs)
   1131         # do not call functions when jit is used
   1132         full_backward_hooks, non_full_backward_hooks = [], []

/usr/local/lib/python3.7/dist-packages/transformers/models/flava/modeling_flava.py in forward(self, input_ids, input_ids_masked, pixel_values, codebook_pixel_values, attention_mask, token_type_ids, bool_masked_pos, position_ids, image_attention_mask, skip_unmasked_multimodal_encoder, mlm_labels, mim_labels, itm_labels, output_attentions, output_hidden_states, return_dict, return_loss)
   1968             if mim_labels is not none:
   1969                 mim_labels = self._resize_to_2d(mim_labels)
-> 1970                 bool_masked_pos = self._resize_to_2d(bool_masked_pos)
   1971                 mim_labels[bool_masked_pos.ne(true)] = self.ce_ignore_index
   1972 

/usr/local/lib/python3.7/dist-packages/transformers/models/flava/modeling_flava.py in _resize_to_2d(self, x)
   1765 
   1766     def _resize_to_2d(self, x: torch.tensor):
-> 1767         if x.dim() > 2:
   1768             x = x.view(x.size(0), -1)
   1769         return x

attributeerror: 'nonetype' object has no attribute 'dim'

can anyone provide suggestions with what's going wrong?","['python-3.x', 'image-processing', 'huggingface-transformers', 'bert-language-model', 'multimodal']",73284664,"flava's author here.
can you please add the following arguments to your processor call:
return_codebook_pixels=true, return_image_mask=true

here is an example colab if you want to see how to call flava model:",https://stackoverflow.com/questions/73257704,python-3.x,06-08-2022 06:40,347.0,1.0,1.0,True,08-08-2022 22:25,08-08-2022 14:19
76470759,transfer learning distillation model loss not decreasing,"currently i'm trying to reproduce paper ""a deep transfer learning method for cross-lingual natural language inference"" (bandyopadhyay et al., lrec 2022) for cross-lingual natural language inference task. but, the model i'm trying to reproduce is not learning any parameters which demonstrated by the model's loss not decreasing.
the dataset i'm using is indonli with hypothesis sentences translated into javanese. but, as you can read on the paper, you can also use xnli for this task.
for this experiment, i'm using pytorch, huggingface transformers, pandas, numpy, and wandb for logging.
first, i construct my dataset as follows:
class compdataset(dataset):
    def __init__(self, df_teacher, df_student):
        self.df_data_teacher = df_teacher
        self.df_data_student = df_student
        
    def __getitem__(self, index):
        # teacher
        sentence_teacher_1 = self.df_data_teacher.loc[index, 'premise']
        sentence_teacher_2 = self.df_data_teacher.loc[index, 'hypothesis']
        
        encoded_dict_teacher = tokenizer.encode_plus(
            sentence_teacher_1,
            sentence_teacher_2,
            add_special_tokens = true,
            max_length = max_len,
            truncation='longest_first',
            padding = 'max_length',
            return_attention_mask = true,
            return_tensors = 'pt'
        )
        
        padded_token_list_teacher = encoded_dict_teacher['input_ids'][0]
        att_mask_teacher = encoded_dict_teacher['attention_mask'][0]
        tok_type_id_teacher = encoded_dict_teacher['token_type_ids'][0]
        
        target_teacher = torch.tensor([self.df_data_teacher.loc[index, 'label']])
        lt_target_teacher = torch.longtensor(target_teacher)
        onehot_encoded_lbl_teacher = f.one_hot(lt_target_teacher, num_classes=3) # 3 classes: entails, neutral, contradict
        
        # student
        sentence_student_1 = self.df_data_student.loc[index, 'premise']
        sentence_student_2 = self.df_data_student.loc[index, 'hypothesis']
        
        encoded_dict_student = tokenizer.encode_plus(
            sentence_student_1,
            sentence_student_2,
            add_special_tokens = true,
            max_length = max_len,
            truncation='longest_first',
            padding = 'max_length',
            return_attention_mask = true,
            return_tensors = 'pt'
        )
        
        padded_token_list_student = encoded_dict_student['input_ids'][0]
        att_mask_student = encoded_dict_student['attention_mask'][0]
        tok_type_id_student = encoded_dict_student['token_type_ids'][0]
        
        target_student = torch.tensor([self.df_data_student.loc[index, 'label']])
        lt_target_student = torch.longtensor(target_student)
        onehot_encoded_lbl_student = f.one_hot(lt_target_student, num_classes=3) # 3 classes: entails, neutral, contradict
        
        output = {
            ""input_ids_teacher"": padded_token_list_teacher, 
            ""attention_mask_teacher"": att_mask_teacher,
            ""token_type_ids_teacher"": tok_type_id_teacher,
            ""lbl_teacher"": onehot_encoded_lbl_teacher,
            ""input_ids_student"": padded_token_list_student, 
            ""attention_mask_student"": att_mask_student,
            ""token_type_ids_student"": tok_type_id_student,
            ""lbl_student"": onehot_encoded_lbl_student
        }
        
        return output
    
    def __len__(self):
        return len(self.df_data_teacher)

then, i build the transformers' dataset & dataloader. the df_train_t and df_train_student being dataframe for teacher dataset (indonesian premise-indonesian hypothesis) and student dataset (indonesian premise-javanese hypothesis).
train_data_cmp = compdataset(df_train_t, df_train_student)
valid_data_cmp = compdataset(df_valid_t, df_valid_student)
test_data_cmp = compdataset(df_test_t, df_test_student)

train_dataloader = dataloader(train_data_cmp, batch_size = batch_size)
valid_dataloader = dataloader(valid_data_cmp, batch_size = batch_size)
test_dataloader = dataloader(test_data_cmp, batch_size = batch_size)

after that, i try to build the model using the schematic and algorithm of transfer learning method provided on the paper. as you can see on the code below, i tried to freeze the mbert model for teacher, and update only the student model parameters.
class transferlearningpaper(pretrainedmodel):
    def __init__(self, config, lambda_kld, learningrate_student, batchnorm_epsilon = 1e-5):
        super(transferlearningpaper, self).__init__(config)
        
        self.bert_model_teacher = bertmodel.from_pretrained(
            model_teacher_type, # using already pretrained mbert in ina language
            num_labels = 3,
            output_hidden_states=true
        )
        
        # freeze teacher mbert parameters
        for params_teacher in self.bert_model_teacher.parameters():
            params_teacher.requires_grad = false
    
        self.bert_model_student = bertmodel.from_pretrained(
            mbert_type,
            num_labels = 3,
            output_hidden_states=true
        )
        
        self.optimizer_student = adamw(
            self.bert_model_student.parameters(), 
            lr=learningrate_student
        )
        
        self.linear = nn.linear(config.hidden_size, 3)  # linear layer
        self.batchnorm = nn.batchnorm1d(config.hidden_size, eps=batchnorm_epsilon)
        self.softmax = nn.softmax(dim=1)  # softmax activation
        
        self.cross_entropy = nn.crossentropyloss()
        self.kld = nn.kldivloss(reduction='batchmean')
        
        # initialize the weights of the linear layer
        self.linear.weight.data.normal_(mean=0.0, std=0.02)
        self.linear.bias.data.zero_()
        
        self.lambda_kld = lambda_kld
    
    def forward(self, input_ids_teacher, attention_mask_teacher, token_type_ids_teacher, lbl_teacher, input_ids_student, attention_mask_student, token_type_ids_student, lbl_student):
        # assume the label is already one-hot encoded
        
        self.bert_model_teacher.eval()
        self.bert_model_student.eval()
        
        with torch.no_grad():
            outputs_teacher = self.bert_model_teacher(
                input_ids=input_ids_teacher, 
                attention_mask=attention_mask_teacher, 
                token_type_ids=token_type_ids_teacher
            )
            outputs_student = self.bert_model_student(
                input_ids=input_ids_student, 
                attention_mask=attention_mask_student, 
                token_type_ids=token_type_ids_student
            )
        
            # take cls token of the last hidden state
            pooled_output_teacher = outputs_teacher[0][:, 0, :]
            pooled_output_student = outputs_student[0][:, 0, :]
        
        batchnormed_logits = self.batchnorm(pooled_output_student)
        linear_output = self.linear(batchnormed_logits) # the output's logits
        softmax_linear_output = f.log_softmax(linear_output, dim=1)
        
        lbl_student = lbl_student[:,0,:].float()
        lbl_teacher = lbl_teacher[:,0,:].float()
        softmax_linear_output = softmax_linear_output.float()
        
        cross_entropy_loss = self.cross_entropy(softmax_linear_output, lbl_student)
        total_kld = self.kld(f.log_softmax(pooled_output_student, dim=1), f.softmax(pooled_output_teacher, dim=1))
        
        joint_loss = cross_entropy_loss + (self.lambda_kld * total_kld )
        
        return {""loss"": joint_loss, ""logits"": softmax_linear_output}
    
    def update_param_student_model(self, loss):
        # doing customized backpropagation for student's model
        self.bert_model_student.train()
        
        self.optimizer_student.zero_grad()
        loss.backward()
        self.optimizer_student.step()

then, i instantiate the model and its configurations and hyperparameters:
config = pretrainedconfig(
    problem_type = ""single_label_classification"",
    id2label = {
        ""0"": ""entail"",
        ""1"": ""neutral"",
        ""2"": ""contradiction""
    },
    label2id = {
        ""entail"": 0,
        ""neutral"": 1,
        ""contradiction"": 2
    },
    num_labels = 3,
    hidden_size = 768,
    name_or_path = ""indojavanesenli-transfer-learning"",
    finetuning_task = ""indonesian-javanese natural language inference""
)
print(config)
transferlearning_model = transferlearningpaper(
    config = config,
    lambda_kld = 0.011, # antara 0.01-0.5
    learningrate_student = student_lrate,
    batchnorm_epsilon = batch_norm_epsilon
)
transferlearning_model = transferlearning_model.to(device)

after that, i create functions to train and validate my model:
def train(the_model, train_data):
    the_model.train()
    
    batch_loss = 0
    
    for batch, data in enumerate(train_data):
        input_ids_teacher = data[""input_ids_teacher""].to(device)
        attention_mask_teacher = data[""attention_mask_teacher""].to(device)
        token_type_ids_teacher = data[""token_type_ids_teacher""].to(device)
        lbl_teacher = data[""lbl_teacher""].to(device)
        input_ids_student = data[""input_ids_student""].to(device)
        attention_mask_student = data[""attention_mask_student""].to(device)
        token_type_ids_student = data[""token_type_ids_student""].to(device)
        lbl_student = data[""lbl_student""].to(device)
        
        output = the_model(
            input_ids_teacher = input_ids_teacher, 
            attention_mask_teacher = attention_mask_teacher, 
            token_type_ids_teacher = token_type_ids_teacher, 
            lbl_teacher = lbl_teacher, 
            input_ids_student = input_ids_student, 
            attention_mask_student = attention_mask_student, 
            token_type_ids_student = token_type_ids_student, 
            lbl_student = lbl_student
        )
        
        loss_model = output[""loss""]
        batch_loss += loss_model
        wandb.log({""train/loss"": loss_model})
        
        # backpropagation
        the_model.update_param_student_model(loss_model)
    
    training_loss = batch_loss / batch_size
    
    return training_loss

def validate(the_model, valid_data):
    the_model.eval()
    
    batch_loss = 0
    
    with torch.no_grad():
        for batch, data in enumerate(valid_data):
            input_ids_teacher = data[""input_ids_teacher""].to(device)
            attention_mask_teacher = data[""attention_mask_teacher""].to(device)
            token_type_ids_teacher = data[""token_type_ids_teacher""].to(device)
            lbl_teacher = data[""lbl_teacher""].to(device)
            input_ids_student = data[""input_ids_student""].to(device)
            attention_mask_student = data[""attention_mask_student""].to(device)
            token_type_ids_student = data[""token_type_ids_student""].to(device)
            lbl_student = data[""lbl_student""].to(device)

            output = the_model(
                input_ids_teacher = input_ids_teacher, 
                attention_mask_teacher = attention_mask_teacher, 
                token_type_ids_teacher = token_type_ids_teacher, 
                lbl_teacher = lbl_teacher, 
                input_ids_student = input_ids_student, 
                attention_mask_student = attention_mask_student, 
                token_type_ids_student = token_type_ids_student, 
                lbl_student = lbl_student
            )

            logits = output[""logits""].cpu().detach().numpy()
            packed_val = logits, lbl_student.cpu().detach().numpy()
            metrics = compute_metrics(packed_val)

            loss_model = output[""loss""]
            batch_loss += loss_model
            wandb.log({
                ""eval/loss"": loss_model, 
                ""eval/f1_score"": metrics[""f1_score""], 
                ""eval/accuracy"": metrics[""accuracy""],
                ""eval/precision"": metrics[""precision""],
                ""eval/recall"": metrics[""recall""]
            })
    
        eval_loss = batch_loss / batch_size
    
    return eval_loss, metrics

def training_sequence(the_model, train_data, valid_data, epochs):
    track_train_loss = []
    track_val_loss = []
    
    t = trange(epochs, colour=""green"", position=0, leave=true)
    for ep in t:
        training_loss = train(the_model, train_data)
        valid_loss, _ = validate(the_model, valid_data)
        
        track_train_loss.append(training_loss)
        track_val_loss.append(valid_loss)
        
        t.set_description(f""epoch [{ep + 1}/{epochs}] - train loss: {training_loss:.2f} valid loss: {valid_loss:.2f}"")
        
        if valid_loss < min(track_val_loss) or ep + 1 == 1:
            the_model.save_pretrained(
                save_directory = model_path + ""indojavanesenli-transfer-learning""
            )
            
        wandb.log({
            ""train_loss/epoch"": training_loss,
            ""validation_loss/epoch"": valid_loss
        })
        
    return {
        ""training_loss"": track_train_loss,
        ""validation_loss"": track_val_loss
    }

finally, i train my model by using:
training_result = training_sequence(transferlearning_model, train_dataloader, valid_dataloader, num_epochs)

but the problem is, during training, the model not updating the student's model parameters as you can see on fig.1 below.

figure 1. model loss not decreasing
fyi, this is the configuration variable i use for the code above:
tokenizer_type = 'bert-base-multilingual-cased'
mbert_type = 'bert-base-multilingual-cased'
model_teacher_type = 'jalaluddin94/nli_mbert' # this is an already fine-tuned mbert on the indonesian language
model_path = 'd:/training/machine learning/nlp/nli/indo-javanese-nli/researchedmodels/'

student_lrate = 2e-5
max_len = 512
num_epochs = 25
batch_size = 12
batch_norm_epsilon = 1e-5
lambda_l2 = 3e-5","['pytorch', 'nlp', 'artificial-intelligence', 'huggingface-transformers', 'transfer-learning']",76601976,"i have figured it out.
it's because of with torch.no_grad(): on the forward pass of the model.
the bert student parameters wouldn't be updated because it lost its gradient.",https://stackoverflow.com/questions/76470759,pytorch,14-06-2023 06:41,114.0,0.0,1.0,True,03-07-2023 05:13,14-06-2023 06:47
76393971,bert ner model start and end position none after fine-tuning,"i have fine-tuned a bert ner model to my dataset. the base model that i am fine-tuning is ï¿½ï¿½ï¿½dslim/bert-base-nerï¿½ï¿½ï¿½. i have been successfully able to train the model using the following script as refren"" rel=""nofollow noreferrer"">
the code which does the prediction:
from transformers import pipeline, berttokenizer

tokenizer = berttokenizer.from_pretrained('dslim/bert-base-ner', return_offsets_mapping=true, is_split_into_words=true)
model = bertfortokenclassification.from_pretrained('dslim/bert-base-ner')

pipe = pipeline(task=""ner"", model=model.to(""cpu""), tokenizer=tokenizer, grouped_entities=true)
pipe(""this is a abc corp. ltd"")

the prediction form the base model contained the start and end position of the word in the original text like:
{ï¿½ï¿½ï¿½entity_groupï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½orgï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½scoreï¿½ï¿½ï¿½: 0.9992545247077942, ï¿½ï¿½ï¿½wordï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½aï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½startï¿½ï¿½ï¿½: 10, ï¿½ï¿½ï¿½endï¿½ï¿½ï¿½: 11}
{ï¿½ï¿½ï¿½entity_groupï¿½ï¿½ï¿½ýýendýýý: 22}

while the prediction from the re-trained model is:
{ýýýentity_groupýýý: ýýýorgýýý, ýýýscoreýýý: 0.747031033039093, ýýýwordýýý: ýýý##7ýýý, ýýýstartýýý: none, ýýýendýýý: none},
{ýýýentity_groupýýý: ýýýorgýýý, ýýýscoreýýý: 0.9055356582005819, ýýýwordýýý: ýýýgames , incýýý, ýýýstartýýý: none, ýýýendýýý: none}

i am passing the position ids to the model during the training process. i looked at the model training parameters but, could not find a way to pass start and end position of the words to model training process. i have the start and end position of the tokenized words.","['nlp', 'huggingface-transformers', 'bert-language-model', 'named-entity-recognition']",76399667,"the pipeline can not return positions when you pass a ""slow""-tokenizer. use a ""fast""-tokenizer to get the positions as well:
from transformers import pipeline, berttokenizer, berttokenizerfast, bertfortokenclassification

fast_t = berttokenizerfast.from_pretrained('dslim/bert-base-ner')
slow_t = berttokenizer.from_pretrained('dslim/bert-base-ner')
model = bertfortokenclassification.from_pretrained('dslim/bert-base-ner')


text= ""this is a abc corp. ltd""

slow_p = pipeline(task=""ner"", model=model, tokenizer=slow_t, device=""cpu"", aggregation_strategy=""simple"")
print(slow_p(text))
fast_p = pipeline(task=""ner"", model=model, tokenizer=fast_t, device=""cpu"", aggregation_strategy=""simple"")
print(fast_p(text))

output:
[{'entity_group': 'org', 'score': 0.9992956, 'word': 'a', 'start': none, 'end': none}, {'entity_group': 'org', 'score': 0.97245616, 'word': '##bc corp . ltd', 'start': none, 'end': none}]
[{'entity_group': 'org', 'score': 0.9992956, 'word': 'a', 'start': 10, 'end': 11}, {'entity_group': 'org', 'score': 0.97245616, 'word': '##bc corp. ltd', 'start': 11, 'end': 23}]",https://stackoverflow.com/questions/76393971,nlp,02-06-2023 23:23,559.0,1.0,1.0,True,04-06-2023 09:13,03-06-2023 15:34
76571812,multiclass text classification using hugging face models,"i am trying to do sentiment analysis on customer feedback and for that i am using hugging face models (required). the issue is that all the responses i am getting are either positive or negative , i haven't gotten a neutral response.
this is how my dataset looks like
import pandas as pd
from transformers import autotokenizer, automodelforsequenceclassification
import numpy as np


# example dataframe
df = pd.dataframe({'text': ['this movie is great!','neutral','happy this movie!' ,'i feel bored.', 'the weather is nice.',np.nan]})

    # function to predict sentiment
    def predict_sentiment(text):
        # load tokenizer and model
        if pd.isna(text):
            return 'n/a'  # return a default value for nan
        tokenizer = autotokenizer.from_pretrained(""bert-base-uncased"")
        model = automodelforsequenceclassification.from_pretrained(""textattack/bert-base-uncased-imdb"")
        tokens = tokenizer.encode_plus(text, padding=true, truncation=true, return_tensors=""pt"")
        outputs = model(**tokens)
        predicted_class = outputs.logits.argmax().item()
        sentiment_classes = ['negative','positive', 'neutral']
        predicted_sentiment = sentiment_classes[predicted_class]
        return predicted_sentiment
    
    # apply sentiment prediction on dataframe column
    df['predicted_sentiment'] = df['text'].apply(predict_sentiment)

             text                      predicted_sentiment
0   this movie is great!                  positive
1   neutral                               positive
2   happy this movie!                     positive
3   i feel bored.                         negative
4   the weather is nice.                  positive
5   nan                                     n/a

now, if i switch the lables like this ['negative','neutral','positive'] i only get results
             text                      predicted_sentiment
0   this movie is great!                  neutral
1   neutral                               neutral
2   happy this movie!                     neutral
3   i feel bored.                         negative
4   the weather is nice.                  neutral
5   nan                                     n/a

whereas the results should be
         text                      predicted_sentiment
0   this movie is great!                  positive
1   neutral                               neutral
2   happy this movie!                     positive
3   i feel bored.                         negative
4   the weather is nice.                  positive
5   nan                                     n/a","['python', 'nlp', 'huggingface-transformers', 'huggingface']",76580210,"the main reason why you're not getting the neutral label is because the model that you're using (textattack/bert-base-uncased-imdb, huggingface link here) has been trained on the imdb dataset which is binary classification dataset. in other words, the model only has two final layers and will only output a prediction between two outputs, positive or negative. you can see more info about the dataset here. as you can clearly see it's a binary classification dataset so the model trained on it does not allow for 3 different outputs.
if you pass it three or more labels (as you did with ['negative','neutral','positive']) it will only consider the first two and ignore the others. the model is set such that the first label is ""negative"" and the second is ""positive"", irrespective of what you set. so your ""neutral"" is a ""positive"" label as it takes the second position. even if you set the labels to random words such as ['hello','there'] then your ""hello"" label will signify a negative score (first place) and ""there"" a positive score (second place). be careful because if you set your labels to ['positive','negative'] then a ""positive"" review (first item) will effectively be ""negative"" one. this is just how the model you're using is set up. hope this makes sense.",https://stackoverflow.com/questions/76571812,python,28-06-2023 09:28,883.0,1.0,2.0,True,29-06-2023 10:33,28-06-2023 09:36
76474907,how to visulaize or print the output from each layer of pytorch saved model specifically for vision_transformer?,"what i have done?
i have trained a vit using pytorch from torchvision.models import vision_transformer as vits specifically model = vits.vit_b_16(pretrained=false, num_classes=10).to(device) and saved the whole model using torch.save(model, ""vit_mnist_model.pth"") in the separate python file i have loaded the model and also test it on single image from the dataset it is working fine.
what i want to do?
as transformer use the attention mechanism. i want to visualize the patches self attention is focusing most in the prediction of the image. to do that i want to pass the same image to the vit and get the output from the each encoder block. further, my plan is to visualize these patches that are output from from each block using matplot.
code i am using till now
import torch
from torchvision import datasets, transforms
from torch.utils.data import dataloader
import torch.nn as nn
from torchsummary import summary

# set device
device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

# load mnist test dataset and apply transformations
transform = transforms.compose([
    transforms.resize(224),
    transforms.grayscale(num_output_channels=3),
    transforms.totensor(),
    transforms.normalize((0.5,), (0.5,))
])

test_dataset = datasets.mnist(root='./data', train=false, download=true, transform=transform)

# create data loader
batch_size = 1  # set batch size to 1 to process one image at a time
test_loader = dataloader(test_dataset, batch_size=batch_size, shuffle=false)


# load the saved model state dictionary
model =torch.load('vit_mnist_model.pth')

# set the model in evaluation mode
model.eval()

# get a single image from the test data at position 5
index = 5
image, label = test_dataset[index]
image = image.unsqueeze(0).to(device)  # add a batch dimension and move to the device

# print the summary of the model
summary(model, input_size=(3, 224, 224), print_summary=true)

print(image.shape)

for name, module in model.named_modules():
    print(name)

# pass the image through the model up to the 'encoder_layer_11'
output = image
for name, module in model.named_children():
    output = module(output)
    if name == 'encoder':
        for encoder_name, encoder_module in module.named_children():
            output = encoder_module(output)
            if encoder_name == 'layer11':
                encoder_layer_output = output
                break

print(f""output shape of 'encoder_layer_11': {encoder_layer_output.shape}"")
print(encoder_layer_output)

error i am getting
traceback (most recent call last):
  file ""/home/blggpu2/projects/visiontransformer/visualize_vit.py"", line 54, in <module>
    output = module(output)
  file ""/home/blggpu2/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  file ""/home/blggpu2/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/vision_transformer.py"", line 155, in forward
    torch._assert(input.dim() == 3, f""expected (batch_size, seq_length, hidden_dim) got {input.shape}"")
  file ""/home/blggpu2/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/__init__.py"", line 853, in _assert
    assert condition, message
assertionerror: expected (batch_size, seq_length, hidden_dim) got torch.size([1, 768, 14, 14])

i don't understand why it is giving an error when i pass the image using the for loop. i am just passing the image to the input layer and whatever it giving output i am transferring that output to the next layer.
update
i used the below code to get the output for specific layers. however, it is a bit confusing, first of all, according to my knowledge transformer breaks the image into patches but when i return the output of the 'conv_proj' it returns [1, 768, 14, 14] meaning there are 768 patches of size 14 x 14. but when i plot them they show the whole 768 images not just a patch. further 'encoder.layers.encoder_layer_11' returns [1, 197, 768] which doesn't make any sense to me as it cant be plotted.
for name, module in model.named_modules():
    print(name)
return_nodes = {
    
    ""conv_proj"": ""layer1"",
    ""encoder.layers.encoder_layer_11"": ""layer2"",
   

}
model2 = create_feature_extractor(model, return_nodes=return_nodes)
intermediate_outputs = model2(image)


print(intermediate_outputs['layer1'].shape)
print(intermediate_outputs['layer2'].shape)

out1= intermediate_outputs['layer1'][0,0,:,:]
normalized_output = out1.cpu().detach().numpy()

plt.imshow(normalized_output, cmap='gray')
plt.colorbar()
plt.show()

output
torch.size([1, 768, 14, 14])
torch.size([1, 197, 768])","['python', 'pytorch', 'huggingface-transformers']",76475442,"as shown in this other question, the order of the elements in model.named_children() is given by the order of appearance in the __init__ method of the model, which means that without access to this method, the order of appearance of the different layers is not guaranteed.
moreover, even if you are certain that modules will appear in the correct order, your loop will still probably cause an error, because some parts of the model will be used twice. in your loop, when the module named encoder appears, the current value of output is processed and saved to the same variable with output = encoder(output). after, children layers of the encoder are applied to the current value of output, which have already been processed by the encoder. the effect of that will be, in the best case if the shapes and layers are not compatible, to raise an error like you got, and in the worst case if shapes and layers are compatible, to give you an answer as a tensor, but which would not be what you wanted, since the data would have gone through the encoder once and then through some parts of the encoder again, resulting on uninterpretable results.
update
first, about the conv_proj output :
this is the opposite, you have 14*14=196 patches, each of which being summarized by a 768-sized representation. this representation is not really vizualizable, since each feature could be independant from the other. if you want something that can be seen as an image, you could check the kernel of the convolution layer. this will give you a [k, k, n] tensor, with k being the kernel size of the convolution, and n the number of filters (likely to be 768, unless there are other layers after the convolutions). for interpretation, you could visualize each [k, k] array as ""how important is each pixel for this filter"". values can be positive or negative, depending on whether the pixel has a positive or negative effect on this filter.
then about the transformer layer :
the dimension you get is [b, t, n], where b is batch size (1 in your case), t is some kind of ""time"" dimension and n is the size of the representation. since an image does not have a time dimension, one is constructed by defining an order between the patches, so you find again the 196 patches you already had from the convolution layer, with one patch added by the transformer layer, which gives the t=196 dimension. finally n is the size of the embedding, which is kept at 768. as for the previous one, this representation is not really meant to be visualized, something that can be interesting to watch is the attention maps from the transformer layers. it is a [t,t,h]-sized tensor, where t is still the ""time"" dimension (197) and h is the number of heads in this layer. each of these t*t tensors are describing the importance of each patch relatively to the other to construct the output of the layer, and so for each attention head you have. you can find some details in this link from pytorch doc, the output is named attn_output_weights and you might have to tweak a bit your implementation to get these.",https://stackoverflow.com/questions/76474907,python,14-06-2023 14:56,2222.0,1.0,1.0,True,08-03-2024 17:40,08-03-2024 17:40
37793118,load pretrained glove vectors in python,i have downloaded pretrained glove vector file from the internet. it is a .txt file. i am unable to load and access it. it is easy to load and access a word vector binary file using gensim but i don't know how to do it when it is a text file format.,"['python-2.7', 'vector', 'nlp']",38230349,"glove model files are in a word - vector format. you can open the textfile to verify this. here is a small snippet of code you can use to load a pretrained glove file:
import numpy as np

def load_glove_model(file):
    print(""loading glove model"")
    glove_model = {}
    with open(file,'r') as f:
        for line in f:
            split_line = line.split()
            word = split_line[0]
            embedding = np.array(split_line[1:], dtype=np.float64)
            glove_model[word] = embedding
    print(f""{len(glove_model)} words loaded!"")
    return glove_model

you can then access the word vectors by simply using the glovemodel variable.
print(glovemodel['hello'])",https://stackoverflow.com/questions/37793118,python-2.7,13-06-2016 15:01,90957.0,50.0,14.0,True,29-12-2022 02:41,29-12-2022 02:41
69837764,find index of word in sentence with information from phrase,"i need the index of word in sentence. but sometimes there are repetitions of the words. the phrase information would then be helpful. or the previous or next row in the word column.
basically, i just need to identify the word in the utterance, e.g. if the word is 'seaside', i want to know which 'seaside' it is in the sentence. i have extra information from the phrase that can help with this identification. the order of their appearance in the dataframe can also help.
what i have right now is this:




file_id
phrase
word
sentence
word_indices




a
i am
i
i am a happy bird. i sing every day. i eat worms.
[0, 5, 9]


b
the seaside is
the
she is by the seaside. the seaside is  packed.
[3, 5]


b
the seaside is
seaside
she is by the seaside. the seaside is  packed.
[4, 6]


b
the seaside is
is
she is by the seaside. the seaside is  packed.
[1, 7]


c
nobody knows
nobody
nobody knows what is going on. she can find nobody
[0, 9]


c
find nobody
nobody
nobody knows what is going on. she can find nobody
[0, 9]


d
it is such a sunny day
sunny
it is such a sunny day ah i am so happy when it's sunny such a sunny day is the best
[4, 13, 16]




but what i want to get is what is in the target column.




file_id
phrase
word
sentence
word_indices
target




a
i am
i
i am a happy bird. i sing every day. i eat worms.
[0, 5, 9]
[0]


b
the seaside is
the
she is by the seaside. the seaside is  packed.
[3, 5]
[5]


b
the seaside is
seaside
she is by the seaside. the seaside is  packed.
[4, 6]
[6]


b
the seaside is
is
she is by the seaside. the seaside is  packed.
[1, 7]
[7]


c
nobody knows
nobody
nobody knows what is going on. she can find nobody
[0, 9]
[0]


c
find nobody
nobody
nobody knows what is going on. she can find nobody
[0, 9]
[9]


d
it is such a sunny day
sunny
it is such a sunny day ah i am so happy when it's sunny such a sunny day is the best
[4, 13, 16]
[4]




i found a similar question here: find index of words in matched text
but unfortunately, this is in java and i need an answer using python.
thanks a lot!","['python', 'pandas', 'indexing', 'nlp', 'nltk']",69838364,"i would break it down into two steps. find the number of words leading up to the phrase in the sentence and then the word index number of the word in the phrase: something like below:
def get_index_of_word_in_sentence(word, phrase, sentence):
    index1 = sentence.index(phrase)
    word_num1 = len(sentence[:index1].split())
    word_num2 = phrase.split().index(word)
    return word_num1 + word_num2

df[""target""] = df.apply(lambda x: get_index_of_word_in_sentence(x[""word""], x[""phrase""], x[""sentence""]), axis=1)",https://stackoverflow.com/questions/69837764,python,04-11-2021 10:28,353.0,0.0,1.0,True,04-11-2021 11:50,04-11-2021 11:46
18446408,adding custom stopwords in r tm,"i have a corpus in r using the tm package. i am applying the removewords function to remove stopwords
tm_map(abs, removewords, stopwords(""english"")) 

is there a way to add my own custom stop words to this list?","['r', 'text-mining', 'stop-words', 'corpus', 'tm']",18446631,"stopwords just provides you with a vector of words, just combine your own ones to this.
tm_map(abs, removewords, c(stopwords(""english""),""my"",""custom"",""words""))",https://stackoverflow.com/questions/18446408,r,26-08-2013 14:22,42266.0,17.0,6.0,True,11-03-2021 12:44,20-07-2016 02:24
78775926,how can i fix my flask server&#39;s 405 error that includes openai api?,"i'm trying to add an api to my webpage and have never used any flask server before, i have never used javascript too so this is a completely brand new learning experience.
my problem is that i keep receiving a 405 error code saying that the method is not allowed. i keep on using the post method but it isn't working, i am banking that my issue may be with my html code more than my flask server because the code is extremely generic and simple.
import openai
from flask import flask, request, jsonify

app = flask(__name__)

openai.api_key = '**my key is in here**'

@app.route('/', methods=['post'])
def chat():
    data = request.get_json()
    message = data.get('message')
    
    response = openai.completion.create(
        model=""gpt-3.5-turbo"",  
        prompt=message,
        max_tokens=50
    )

    return {'response': response.choices[0].text.strip()}

if __name__ == '__main__':
    app.run(port=5000)

async function sendmessage() {
            const message = document.getelementbyid('message').value;
            document.getelementbyid('chat-box').innerhtml += `<div>you: ${message}</div>`;
            
            const response = await fetch('/', {
                method: ""post"",
                body: json.stringify({ message }),
                headers: {
                    'content-type': 'application/json',
                },
            });

            const data = await response.json();
            document.getelementbyid('chat-box').innerhtml += `<div>bot: ${data.reply}</div>`;
            document.getelementbyid('message').value = '';
        }

i tried changing up the structure of the code, i uninstalled flask and reinstalled it again. i've also extensively used chatgpt to try and come up with better code but it just kept taking me in circles. i'm hoping someone can help with this. i even tried a simple server that just said hello world which worked, but i truly think the issue might be with my javascript. also, i am a beginner and this is supposed to be one of my first coding projects so please take it easy on me if possible. thanks.","['javascript', 'python', 'flask', 'openai-api']",78776107,"you have to add a route for '/' to serve the html file. i also fixed the way you call the openai api because you're using a deprecated one.
import openai
from flask import flask, request, jsonify, render_template

chat_client = openai(api_key='....')


@app.route('/')
def index():
    return render_template('index.html')

@app.route('/', methods=['post'])
def chat():
    data = request.get_json()
    message = data.get('message')
    
    response = chat_client.chat.completions.create(
        model=""gpt-3.5-turbo"",
        messages=[
            {""role"": ""user"", ""content"": message}
        ],
        max_tokens=50
    )

    return jsonify({'reply': response.choices[0].message.content.strip()})

if __name__ == '__main__':
    app.run(port=5000, debug=true)

sample index.html that i tested with
<!doctype html>
<html lang=""en"">
<head>
    <meta charset=""utf-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    <title>chat app</title>
    <script>
        async function sendmessage() {
            const message = document.getelementbyid('message').value;
            document.getelementbyid('chat-box').innerhtml += `<div>you: ${message}</div>`;
            
            const response = await fetch('/', {
                method: ""post"",
                body: json.stringify({ message }),
                headers: {
                    'content-type': 'application/json',
                },
            });

            const data = await response.json();
            document.getelementbyid('chat-box').innerhtml += `<div>bot: ${data.reply}</div>`;
            document.getelementbyid('message').value = '';
        }
    </script>
</head>
<body>
    <div id=""chat-box""></div>
    <input type=""text"" id=""message"" placeholder=""type your message here..."">
    <button onclick=""sendmessage()"">send</button>
</body>
</html>


directory structure
- app.py
- templates/
  - index.html",https://stackoverflow.com/questions/78775926,javascript,21-07-2024 17:40,136.0,0.0,1.0,True,22-07-2024 08:19,22-07-2024 06:36
75935259,"expected file to have jsonl format, where every line is a json dictionary. openai createfile for fine tune","i created file with name mydata.jsonl and i put on it these lines
    {
        ""prompt"": ""aa"",
        ""completion"": ""bb""
    }
    {
        ""prompt"": ""cc"",
        ""completion"": ""dd""
    }

then in index.js i did this function
    const { configuration, openaiapi } = require(""openai"");
    const fs = require(""fs"");
    
    const configuration = new configuration({
        apikey: process.env.openai_api_key,
    });
    async function getairesponse(topic) {
        const openai = new openaiapi(configuration);
        const filename = 'example.txt';
        const filecontents = 'this is some example text.';
        const response = await openai.createfile(
            fs.createreadstream(""mydata.jsonl""),
            ""fine-tune""
        );
        console.log(response)
    }
    getairesponse(""ee"");

when i run my code i got error
    expected file to have jsonl format, where every line is a json dictionary. line 1 is not a dictionary (hint: line starts with: ""{..."").

i can't get where is the exactly the error","['openai-api', 'fine-tuning']",75935581,"the jsonl format is where the each json object is sperated by a newline characater.
so your output should be:
{ ""prompt"": ""aa"", ""completion"": ""bb"" }
{ ""prompt"": ""cc"", ""completion"": ""dd"" }

i.e. one line per json object",https://stackoverflow.com/questions/75935259,openai-api,05-04-2023 02:44,7795.0,4.0,2.0,True,04-05-2023 12:10,05-04-2023 19:48
77465301,trying adding embeddings in azure cognitive search leads to error &quot;the property &#39;content&#39; does not exist on type &#39;search.documentfields&#39;.&quot;,"i am extracting text from pdf documents and load it to azure cognitive search for a rag approach. unfortunately this does not work. i am receiving the error message
 () the request is invalid. details: the property 'content' does not exist on type 'search.documentfields'. make sure to only use property names that are defined by the type.
code: 
message: the request is invalid. details: the property 'content' does not exist on type 'search.documentfields'. make sure to only use property names that are defined by the type.

what i want to do is

extract text from pdf via pymupdf - works
upload it to azure vector search as embeddings with vectors and metdata `filename``
query this through chatgpt model

beside the error i want to add to this document object the metadata information filename but also dont know how to extend this ...
my code:
!pip install cohere tiktoken
!pip install openai==0.28.1
!pip install pymupdf
!pip install azure-storage-blob azure-identity
!pip install azure-search-documents --pre --upgrade
!pip install langchain

import fitz
import time
import uuid
import os
import openai

from pil import image
from io import bytesio
from ipython.display import display

from azure.identity import defaultazurecredential
from azure.storage.blob import blobserviceclient, blobclient, containerclient

from langchain.embeddings import openaiembeddings
from langchain.text_splitter import recursivecharactertextsplitter

from langchain.chat_models import azurechatopenai
from langchain.vectorstores import azuresearch
from langchain.docstore.document import document
from langchain.document_loaders import directoryloader
from langchain.document_loaders import textloader
from langchain.text_splitter import tokentextsplitter
from langchain.chains import conversationalretrievalchain
from langchain.prompts import prompttemplate

from google.colab import drive

openai_api_base = ""
openai_api_key = ""xxx""
openai_api_version = ""2023-05-15""

openai.api_type = ""azure""
openai.api_key = openai_api_key
openai.api_base = openai_api_base
openai.api_version = openai_api_version

azure_cognitive_search_service_name = ""
azure_cognitive_search_api_key = ""xxx""
azure_cognitive_search_index_name = ""test""

llm = azurechatopenai(deployment_name=""gpt35"", openai_api_key=openai_api_key, openai_api_base=openai_api_base, openai_api_version=openai_api_version)
embeddings = openaiembeddings(deployment_id=""ada002"", chunk_size=1, openai_api_key=openai_api_key, openai_api_base=openai_api_base, openai_api_version=openai_api_version)

acs = azuresearch(azure_search_endpoint=azure_cognitive_search_service_name,
                  azure_search_key = azure_cognitive_search_api_key,
                  index_name = azure_cognitive_search_index_name,
                  embedding_function = embeddings.embed_query)
    
def generate_tokens(s, f):
  text_splitter = recursivecharactertextsplitter(chunk_size=1000, chunk_overlap=100)
  splits = text_splitter.split_text(s)
  i = 0

  documents = []
  for split in splits:
    metadata = {}
    metadata[""index""] = i
    metadata[""file_source""] = f
    i = i+1

    new_doc = document(page_content=split, metadata=metadata)
    documents.append(new_doc)
    #documents = text_splitter.create_documents(splits)

  print (documents)

  return documents


drive.mount('/content/drive')
folder = ""/content/drive/.../pdf/""

page_content = ''
doc_content = ''
    
for filename in os.listdir(folder):
    file_path = os.path.join(folder, filename)
    if os.path.isfile(file_path):
        print(f""processing file: {file_path}"")

        doc = fitz.open(file_path)
        for page in doc: # iterate the document pages
          page_content += page.get_text() # get plain text encoded as utf-8    
          d = generate_tokens(doc_content)

          # the following line throws the error
          # how can i add the chunks + filename to 
          # azure cognitive search?

          doc_content += page_content
          d = generate_tokens(doc_content, file_path)

          acs.add_documents(documents=d)
    
        print(metadatas)
        print(""----------"")
        print(doc_content)
        count = len(doc_content.split())
        print(""number of tokens: "", count)


                         traceback (most recent call last)
<ipython-input-11-d9eaff7ee027> in <cell line: 10>()
     31           all_texts.extend(d)
     32 
---> 33           acs.add_documents(documents=d)
     34 
     35           metadatas = [{""source"": f""{i}-pl""} for i in range(len(all_texts))]

7 frames
/usr/local/lib/python3.10/dist-packages/azure/search/documents/_generated/operations/_documents_operations.py in index(self, batch, request_options, **kwargs)
   1249             map_error(status_code=response.status_code, response=response, error_map=error_map)
   1250             error = self._deserialize.failsafe_deserialize(_models.searcherror, pipeline_response)
-> 1251             raise  model=error)
   1252 
   1253         if response.status_code == 200:

 () the request is invalid. details: the property 'content' does not exist on type 'search.documentfields'. make sure to only use property names that are defined by the type.
code: 
message: the request is invalid. details: the property 'content' does not exist on type 'search.documentfields'. make sure to only use property names that are defined by the type.

this is my index in azure cognitive search index:","['python', 'azure', 'azure-cognitive-search', 'langchain', 'azure-openai']",77465478,"i have solved it now. you had to create the necessary fields in azure cognitive search. these are

the field content_vector seems to hold the vectors. the json definition of the field is
{
  ""name"": ""content_vector"",
  ""type"": ""collection(edm.single)"",
  ""searchable"": true,
  ""filterable"": false,
  ""retrievable"": true,
  ""sortable"": false,
  ""facetable"": false,
  ""key"": false,
  ""indexanalyzer"": null,
  ""searchanalyzer"": null,
  ""analyzer"": null,
  ""normalizer"": null,
  ""dimensions"": 1536,
  ""vectorsearchconfiguration"": ""vector-config-1699712748580"",
  ""synonymmaps"": []
}

and",https://stackoverflow.com/questions/77465301,python,11-11-2023 13:44,2524.0,1.0,1.0,True,11-11-2023 14:32,11-11-2023 14:16
76580372,problems with openai authentification,"i've got some problems with the authentification to openai in my python code. it seems like, openai doesn't accept my key. i did a new on and tried it with other ones before. i always get the same issues. i just copied and pasted the key, same for the organization. there isn't a typo. there is another problem, but i don't know, why the authentifitaion failed.
here is my code:
i used it in the same way like the openai website 
import os
import openai
openai.organization = ""org-cxk0suzbnyzzclcasbcfav64""
openai.api_key = os.getenv(""sk-**********************************************9q"")
openai.model.list()

it produces the follwing error:
openai.error.authenticationerror: no api key provided. you can set your api key in code using 'openai.api_key = <api-key>', or you can set the environment variable openai_api_key=<api-key>). if your api key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <path>'. you can generate api keys in the openai web interface. see  for details, or email support@openai.com if you have any questions.


did anyone had the some issues before? are there any solutions?","['python', 'authentication', 'api-key', 'openai-api']",76580533,"your environment variables are like a dictionary, and the code to retrieve should match the sample code:
openai.api_key = os.getenv(""openai_api_key"")

however, you have to set the environment keys first.
from your code, it looks like you just want to hard code the value, in which case your code should look like:
import openai
openai.organization = ""org-cxk0suzbnyzzclcasbcfav64""
openai.api_key = ""sk-**********************************************9q""
openai.model.list()",https://stackoverflow.com/questions/76580372,python,29-06-2023 10:53,1214.0,0.0,1.0,True,29-06-2023 11:17,29-06-2023 10:54
70051086,python sklearn tfidfvectorizer arguments error,"i've been using the sklearn tfidfvectorizer but suddenly it has been throwing an error:
typeerror: __init__() takes 1 positional argument but 2 positional arguments 
(and 4 keyword-only arguments) were given

the argument i'm giving is:
tfidf_vectorizer = tfidfvectorizer(x_train, ngram_range=(1,2), max_df=0.9, min_df=5, token_pattern=r'(\s+)' )

where x_train is a list of strings such as:
 'done earlier siesta',
 'sunday mass us family greatful opportunity',
 'wet wet wet frustrated outside',
 'tired headache headache',
 'friends creative talented inspired friendship love creatives',
 'grateful lucky beaches sunshine hubby family pets awesome sunday',
 'latest artwork',
 'two headache sick tired sore'

im confused as to why it would say i'm giving two positional arguments when im only inputting one which is the x_train list. even when i simplify the statement down to:
tfidfvectorizer(x_train)

it still gives the same error saying i gave two positional arguments.
i'm using sklearn 1.0.1 but i tried reverting it to 1.0.0 and it still have the same error
could the error be in the list that i am passing in?","['python', 'machine-learning', 'scikit-learn', 'nlp', 'tfidfvectorizer']",70051388,"library and their implementation did change. if we look at the version 0.23.1 we get a warning which states that it needs to pass with keyword args.
tfidvect=tfidfvectorizer(x_train)
futurewarning: pass input=['done earlier siesta', 'sunday mass us family greatful opportunity', 'wet wet wet frustrated outside', 'tired headache headache', 'friends creative talented inspired friendship love creatives', 'grateful lucky beaches sunshine hubby family pets awesome sunday', 'latest artwork', 'two headache sick tired sore'] as keyword args. from version 0.25 passing these as positional arguments will result in an error
  warnings.warn(""pass {} as keyword args. from version 0.25 ""

so fast forward to 1.0.1, the same call will be like:
tfidvect1_01=tfidfvectorizer(input=x_train) # input positional argument

@ambrayers added.
an alternative way is, create the object and then fit_transform , refer the example in official documentation
vectorizer = tfidfvectorizer()  
x_train = vectorizer.fit_transform(x_train)",https://stackoverflow.com/questions/70051086,python,21-11-2021 01:06,1526.0,0.0,1.0,True,21-11-2021 22:31,21-11-2021 10:31
77413194,how to increase the code view box size of chatgpt web interface,"it is frustrating how small the code viewing area is in chatgpt and to make the matters worse it has a fixed width that does not change with the size of the browser window.
update: guys use the extension in the answer below. it is a better solution than mine.
a temporary solution is this bookmarklet to maximize all the elements:
create a bookmark and put this in the url. then run by selecting the bookmark on chatgpt ui.
javascript:(function() { 
  document.queryselectorall('div[class*=""group w-full""] > div > div')
    .foreach((element) => { element.classname = ""flex flex-1""; }); 
})();

is there a better solution than this?",['openai-api'],78908485,"there's a chrome extension now -  its pretty clean.
ps: not built by me.",https://stackoverflow.com/questions/77413194,openai-api,02-11-2023 22:21,3042.0,4.0,4.0,True,06-09-2024 20:24,06-09-2024 20:24
73106002,udpipe_annotate() in r labels the same word differently if followed by punctuation,"i'm doing a standard topic modelling task on nouns in newspaper articles using udpipe to annotate the article content. using the function udpipe_annotate() i noticed that words together with the following punctuation mark sometimes were labelled as upos = noun. thus when i run the topic model function - lda() from library topicmodels - the most common words for a topic might include, say, 'product' and 'product.', the latter including the punctuation mark. they should be seen as the same word. how can i remedy this and remove the punctuation?
another issue is when words before a punctuation were labelled as upos = punct. e.g. 'energy' and 'energy,' were labelled differently. thus i have to specify that i want to include punct in the analysis, and even then i run into the same problem as above of the algorithm treating this as two different words.
is this a problem with the udpipe annotation or is there an easy fix to this problem?
edit: adding code example using first two sentences of wikipedia article on norway in norwegian:
text <- c('norge, offisielt kongeriket norge, er et nordisk, europeisk land og en selvstendig stat vest pï¿½ï¿½ den skandinaviske halvï¿½ï¿½y. geografisk sett er landet langt og smalt.', 'pï¿½ï¿½ den langstrakte kysten mot nord-atlanteren befinner norges vidkjente fjorder seg.', 'kongeriket norge omfatter hovedlandet (fastlandet med tilliggende ï¿½ï¿½yer innenfor grunnlinjen), jan mayen og svalbard.')

id <- c(1:3)

df <- data.frame(text, id)

ud_model <- udpipe_download_model(language = ""norwegian-bokmaal"")
ud_model <- udpipe_load_model(ud_model$file_model)
x <- udpipe_annotate(ud_model, x = df$text, doc_id = df$id)
x_df = data.frame(x)

showing example of the problematic outputs (the rest (adj, verb, etc) are fine i think):
head(x_df[x_df$upos=='noun',5:8], 5)
output:""s-table-container"">



token_id
token
lemma
upos




1
norge,
norge,
noun


4
norge,
norge,
noun


9
land
land
noun


13
stat
stat
noun


18
halvï¿½ï¿½y.
halvï¿½ï¿½y.
noun




head(x_df[x_df$upos=='punct',5:8])
the words with token_id 1,4,and 18 are not correct.
output:




token_id
token
lemma
upos




7
nordisk,
$nordisk,
punct


10
grunnlinjen),
$grunnlinjen),
punct




here, udpipe is finding the punctuation but it also includes the preceding word.
edit2: the problem does not occur for me with the french or english language models. nor does it seem to appear on the norwegian-nynorsk version.","['r', 'nlp', 'annotations', 'punctuation', 'udpipe']",73109347,"looks like there is an issue with the norwegian-bokmaal ud 2.5 model. looking at the ud treebank for norwegian bokmal they are already on version 2.10.
if you use either norwegian-nynorks it works correctly or norwegian-bokmaal ud 2.4 model.
# switch to older model
ud_model <- udpipe_download_model(language = ""norwegian-bokmaal"", 
                                  udpipe_model_repo = ""jwijffels/udpipe.models.ud.2.4"")

# nynorsk works as well
ud_model <- udpipe_download_model(language = ""norwegian-nynorsk"")

you can, of course, get version 2.10, but then you have to train your udpipe model yourself. more info about this in the model building vignette.",https://stackoverflow.com/questions/73106002,r,25-07-2022 08:17,156.0,2.0,1.0,True,25-07-2022 12:34,25-07-2022 10:51
73484411,"can i plot roc curve for multiclass text classification problem, without using onevsrestclassifier?","i have a pickle file that when loaded returns a trained randomforest classifier. i want to plot the roc curve for the classes, but from what i read online, the classifier must be wrapped in scikit learn's onevsrestclassifier. the problem is that since i already have the trained model i cannot wrap it in it to fit the model again.
so i would like to know if there is some workaround to plot the roc curve. from my trained model i have y_test, y_proba. i also have x_test values.

the shape of my y_proba examples is: (6715, 5)



the shape of y_test is (6715, 5)


this is the output of the code @dx2-66 suggested:","['python', 'scikit-learn', 'text-classification', 'roc', 'multiclass-classification']",73485714,"i assume your y_test is single column with class id, and your y_proba has as much columns as there are classes (at least that's what you'd usually get from predict_proba().
how about this? it should yield you ovr-style curves:
from sklearn.metrics import roc_curve
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt

classes = range(y_proba.shape[1])

for i in classes:
    fpr, tpr, _ = roc_curve(label_binarize(y_test, classes=classes)[:,i], y_proba[:,i])
    plt.plot(fpr, tpr, alpha=0.7)
    plt.legend(classes)

update: solution for non-monotonic class labels:
classes = sorted(list(y_test['label'].unique()))

plt.plot([0, 1], linestyle='--')

for i in range(len(classes)):
    fpr, tpr, _ = roc_curve(label_binarize(y_test, classes=classes)[:,i], y_proba.values[:,i])
    plt.plot(fpr, tpr, alpha=0.7)
    plt.legend(['baseline']+classes) # fixed the baseline legend",https://stackoverflow.com/questions/73484411,python,25-08-2022 08:38,290.0,0.0,1.0,True,25-08-2022 14:54,25-08-2022 13:06
34351978,nltk regex chunker not capturing defined grammar patterns with wildcards,"i am trying to chunk a sentence using nltk's pos tags as regular expressions. 2 rules are defined to identify phrases, based on the tags of words in the sentence.
mainly, i wanted to capture the chunk of one or more verbs followed by an optional determiner and then one or more nouns at the end. this is the first rule in definition. but it is not getting captured as phrase chunk.
import nltk

## defining the pos tagger 
tagger = nltk.data.load(nltk.tag._pos_tagger)


## a single sentence - input text value
textv=""this has allowed the device to start, and i then see glitches which is not nice.""
tagged_text = tagger.tag(textv.split())

## defining grammar rules for  phrases
actphgrammar = r""""""
     ph: {<vb*>+<dt>?<nn*>+}  # verbal phrase - one or more verbs followed by optional determiner, and one or more nouns at the end
     {<rb*><vb*|jj*|nn*\$>} # adverbial phrase - adverb followed by adjective / noun or verb
     """"""

### parsing the defined grammar for  phrases
actp = nltk.regexpparser(actphgrammar)

actphrases = actp.parse(tagged_text)

the input to the chunker, tagged_text is as below.

tagged_text
out[7]:
[('this', 'dt'),
('has', 'vbz'),
('allowed', 'vbn'),
('the', 'dt'),
('device', 'nn'),
('to', 'to'),
('start,', 'nnp'),
('and', 'cc'),
('i', 'prp'),
('then', 'rb'),
('see', 'vb'),
('glitches', 'nns'),
('which', 'wdt'),
('is', 'vbz'),
('not', 'rb'),
('nice.', 'nnp')]

in the final output, only the adverbial phrase ('then see'), that is matching the second rule is being captured.
i expected the verbal phrase ('allowed the device') to match with the first rule and get captured as well, but its not.

actphrases out[8]: tree('s', [('this', 'dt'), ('has', 'vbz'),
('allowed', 'vbn'), ('the', 'dt'), ('device', 'nn'), ('to', 'to'),
('start,', 'nnp'), ('and', 'cc'), ('i', 'prp'), tree('ph', [('then',
'rb'), ('see', 'vb')]), ('glitches', 'nns'), ('which', 'wdt'), ('is',
'vbz'), ('not', 'rb'), ('nice.', 'nnp')])

nltk version used is 2.0.5 (python 2.7)
any help or suggestion would be greatly appreciated.","['python', 'regex', 'nlp', 'nltk', 'text-chunking']",34361516,"close but minor changes to your regex will get you your desired output. when you want to get a wildcard using regexpparser grammar, you should use .* instead of *, e.g. vb.* instead of vb*:
>>> from nltk import word_tokenize, pos_tag, regexpparser
>>> text = ""this has allowed the device to start, and i then see glitches which is not nice.""
>>> tagged_text = pos_tag(word_tokenize(text))    
>>> g = r""""""
... vp: {<vb.*><dt><nn.*>}
... """"""
>>> p = regexpparser(g); p.parse(tagged_text)
tree('s', [('this', 'dt'), ('has', 'vbz'), tree('vp', [('allowed', 'vbn'), ('the', 'dt'), ('device', 'nn')]), ('to', 'to'), ('start', 'vb'), (',', ','), ('and', 'cc'), ('i', 'prp'), ('then', 'rb'), ('see', 'vbp'), ('glitches', 'nns'), ('which', 'wdt'), ('is', 'vbz'), ('not', 'rb'), ('nice', 'jj'), ('.', '.')])

note that you're catching the tree(advp, [('then', 'rb'), ('see', 'vb')]),  because the tags are exactly rb and vb. so the wildcard in your grammar (i.e. `""""""advp: {}"""""") in this scenario is ignored.
also, if it's two different types of phrases, it's more advisable to use 2 labels not one. and (i think) end of string after wildcard is sort of redundant, so it's better to:
g = r""""""
vp:{<vb.*><dt><nn.*>} 
advp: {<rb.*><vb.*|jj.*|nn.*>}
""""""",https://stackoverflow.com/questions/34351978,python,18-12-2015 09:07,2246.0,5.0,1.0,True,21-06-2024 17:36,21-06-2024 17:36
13892638,extracting the relationship between entities in stanford corenlp,"i want to extract the complete relationship between two entities using stanford corenlp (or maybe other tools).
for example:

windows is more popular than linux.
this tool requires java.
football is the most popular game in the world.

what is the fastest way? and what is the best practice for that?
thanks in advance","['nlp', 'stanford-nlp']",13924721,"you are probably looking for dependency relations between nouns. stanford parser provides such output. have a look here. you can combine what pete said (i.e. the pos graph) with the dependency graph to identify what relationship (for example, direct object or nominal subject, etc.) a pair of nouns (or noun phrases) share.",https://stackoverflow.com/questions/13892638,nlp,15-12-2012 13:30,8493.0,13.0,5.0,True,16-08-2023 12:11,15-12-2012 13:36
76926213,nextjs with &quot;openai api&quot; got 401 error at endpoint createchatcompletion() but 200 ok at createcompletion(),"in short
nextjs api router failed to call openai api endpoint createchatcompletion() while pure node.js code can succeed.
detail
i use nextjs 's api router as coded here to call ""openai api"" endpoints using node.js package openai
i got 401 at calling endpoint createchatcompletion() view code - though succeeded 200 with createcompletion() view code

i double check with pure node.js code, and can succeed calling createchatcompletion(), strange...
what am i missing?
p.s.
my search in stack overflow and google search result with no helpful found","['next.js', 'openai-api', 'nextjs-api-router']",76933499,"as far as i know, nextjs define env in next.config.js
sample
  // ... other nextjs config
  env: {
    apiapp_host: ' 
    // ...
  }",https://stackoverflow.com/questions/76926213,next.js,18-08-2023 03:06,303.0,0.0,1.0,True,21-08-2023 00:06,21-08-2023 00:06
77487437,open ai gpt api: how to get completion in between (fill-in-the-middle)?,"in this announcement:  an insert mode that lets you have completion in the middle of a text is introduced. how can i have it through the open ai gpt models' completion api?
for instance in the case of a code completion (which github copilot does currently):
def say_hello():
    print('hi', name)

should be completed with name argument inside the parentheses:
def say_hello(name):
    print('hi', name)","['openai-api', 'gpt-3']",77487438,"as insertion instruction guide explains, we can use suffix parameter to provide the text which is located after where you want to insert the new completion.
prompt = ""def say_hello(""
suffix = """"""):
  print('hi', name)""""""

response = client.completions.create(
    model=""gpt-3.5-turbo-instruct"",
    prompt=prompt,
    suffix=suffix,
    max_tokens=32
)
print(response.choices[0].text) # should be 'name'",https://stackoverflow.com/questions/77487437,openai-api,15-11-2023 11:45,1204.0,0.0,1.0,True,16-11-2023 09:48,16-11-2023 09:48
78712878,size mismatch for embed_out.weight: copying a param with shape torch.size([0]) from checkpoint - huggingface pytorch,"i want to finetune an llm. i am able to successfully finetune llm. but when reload the model after save, gets error. below is the code
import argparse
import numpy as np
import torch
from datasets import load_dataset
from transformers import autotokenizer, automodelforcausallm

from trl import dpotrainer, dpoconfig
def preprocess_data(item):
    return {
        'prompt': 'instruct: ' + item['prompt'] + '\n',
        'chosen': 'output: ' + item['chosen'],
        'rejected': 'output: ' + item['rejected']
    }        

def main():
    parser = argparse.argumentparser()
    parser.add_argument(""--epochs"", type=int, default=1)
    parser.add_argument(""--beta"", type=float, default=0.1)
    parser.add_argument(""--batch_size"", type=int, default=4)
    parser.add_argument(""--lr"", type=float, default=1e-6)
    parser.add_argument(""--seed"", type=int, default=2003)
    parser.add_argument(""--model_name"", type=str, default=""eleutherai/pythia-14m"")
    parser.add_argument(""--dataset_name"", type=str, default=""jondurbin/truthy-dpo-v0.1"")
    parser.add_argument(""--local_rank"", type=int, default=0)

    args = parser.parse_args()

    # determine device based on local_rank
    device = torch.device(""cuda"", args.local_rank) if torch.cuda.is_available() else torch.device(""cpu"")


    tokenizer = autotokenizer.from_pretrained(args.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    model = automodelforcausallm.from_pretrained(args.model_name).to(device)
    ref_model = automodelforcausallm.from_pretrained(args.model_name).to(device)

    dataset = load_dataset(args.dataset_name, split=""train"")
    dataset = dataset.map(preprocess_data)

    # split the dataset into training and validation sets
    dataset = dataset.train_test_split(test_size=0.1, seed=args.seed)
    train_dataset = dataset['train']
    val_dataset = dataset['test']

    training_args = dpoconfig(
        learning_rate=args.lr,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        logging_steps=10,
        remove_unused_columns=false,
        max_length=1024,
        max_prompt_length=512,
        fp16=true        
    )

    

    # verify and print embedding dimensions before finetuning
    print(""base model embedding dimension:"", model.config.hidden_size)

    model.train()
    ref_model.eval()

    dpo_trainer = dpotrainer(
        model,
        ref_model,
        beta=args.beta,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        args=training_args,
    )

    dpo_trainer.train()
    # evaluate
    evaluation_results = dpo_trainer.evaluate()
    print(""evaluation results:"", evaluation_results)

    save_model_name = 'finetuned_model'
    model.save_pretrained(save_model_name)

if __name__ == ""__main__"":
    main()

error i was getting as below
    return model_class.from_pretrained(
    file ""/.local/lib/python3.10/site-packages/transformers/modeling_utils.py"", line 3838, in from_pretrained
        ) = cls._load_pretrained_model(
    file ""/.local/lib/python3.10/site-packages/transformers/modeling_utils.py"", line 4349, in _load_pretrained_model
        raise runtimeerror(f""error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}"")
        runtimeerror: error(s) in loading state_dict for gptneoxforcausallm:
            size mismatch for gpt_neox.embed_in.weight: copying a param with shape torch.size([0]) from checkpoint, the shape in current model is torch.size([50304, 128]).
            size mismatch for embed_out.weight: copying a param with shape torch.size([0]) from checkpoint, the shape in current model is torch.size([50304, 128]).
            you may consider adding `ignore_mismatched_sizes=true` in the model `from_pretrained` method.

after finetuning,  model works perfectly. but after reloading the saved trained model its not working. any idea why gets this error when reloading the model ?","['pytorch', 'huggingface-transformers', 'large-language-model', 'huggingface']",78722271,"instead of
model.save_pretrained(save_model_name)

try this
dpo_trainer.save_model(save_model_name)",https://stackoverflow.com/questions/78712878,pytorch,05-07-2024 18:29,514.0,0.0,1.0,True,08-07-2024 17:46,08-07-2024 17:44
18707401,natural language processing for smart homes,"i'm writing up a smart home software for my bachelor's degree, that will only simulate the actual house, but i'm stuck at the nlp part of the project. the idea is to have the client listen to voice inputs (already done), transform it into text (done) and send it to the server, which does all the heavy lifting / decision making.
so all my inputs will be fairly short (like ""please turn on the porch light""). based on this, i want to take the decision on which object to act, and how to act. so i came up with a few things to do, in order to write up something somewhat efficient.

get rid of unnecessary words (in the previous example ""please"" and ""the"" are words that don't change the meaning of what needs to be done; but if i say ""turn off my lights"", ""my"" does have a fairly important meaning).
deal with synonyms (""turn on lights"" should do the same as ""enable lights"" -- i know it's a stupid example). i'm guessing the only option is to have some kind of a dictionary (xml maybe), and just have a list of possible words for one particular object in the house.
detecting the verb and subject. ""turn on"" is the verb, and ""lights"" is the subject. i need a good way to detect this.
general implementation. how are these things usually developed in terms of algorithms? i only managed to find one article about nlp in smart homes, which was very vague (and had bad english). any links welcome.","['algorithm', 'nlp']",18707691,"for your project i would suggest you to go through stanford parser

from your problem definition i guess you don't need anything other then verbs and nouns. sp generates pos(part of speech tags) that you can use to prune the words that you don't require.

for this i can't think of any better option then what you have in mind right now.

for this again you can use grammatical dependency structure from sp and i am pretty much sure that it is good enough to tackle this problem.

this is where your research part lies. i guess you can find enough patterns using gd and pos tags to come up with an algorithm for your problem. i hardly doubt that any algorithm would be efficient enough to handle every set of input sentence(structured+unstructured) but something that is more that 85% accurate should be good enough for you.",https://stackoverflow.com/questions/18707401,algorithm,09-09-2013 21:35,1062.0,8.0,3.0,True,19-05-2023 02:41,19-05-2023 02:40
74804449,splitting sentences from a .txt file to .csv using nltk,"i have a corpus of newspaper articles in a .txt file, and i'm trying to split the sentences from it to a .csv in order to annotate each sentence.
i was told to use nltk for this purpose, and i found the following code for sentence splitting:
import nltk

from nltk.tokenize import sent_tokenize

sent_tokenize(""here is my first sentence. and that's a second one."")

however, i'm wondering:

how does one use a .txt file as an input for the tokenizer (so that i don't have to just copy and paste everything), and
how does one output a .csv file instead of just printing the sentences in my terminal.","['csv', 'nlp', 'nltk', 'txt', 'sentence']",74806058,"reading a .txt file & tokenizing its sentences
assuming the .txt file is located in the same folder as your python script, you can read a .txt file and tokenize the sentences using nltk as shown below:
from nltk import sent_tokenize

with open(""myfile.txt"") as file:
    textfile = file.read()

tokentextlist = sent_tokenize(textfile)
print(tokentextlist)
# output: ['here is my first sentence.', ""and that's a second one.""]


writing a list of sentence tokens to .csv file
there are a number of options for writing a .csv file. pick whichever is more convenient (e.g. if you already have pandas loaded, use the pandas option).
to write a .csv file using the pandas module:
import pandas as pd

df = pd.dataframe(tokentextlist)
df.to_csv(""mycsvfile.csv"", index=false, header=false)

to write a .csv file using the numpy module:
import numpy as np

np.savetxt(""mycsvfile.csv"", tokentextlist, delimiter="","", fmt=""%s"")

to write a .csv file using the csv module:
import csv

with open('mycsvfile.csv', 'w', newline='') as file:
    write = csv.writer(file, lineterminator='\n')
    # write.writerows([tokentextlist])
    write.writerows([[token] for token in tokentextlist]) # for pandas style output",https://stackoverflow.com/questions/74804449,csv,14-12-2022 21:40,307.0,1.0,1.0,True,15-12-2022 13:13,15-12-2022 13:13
76994550,no region has availability azure openai,"on the microsoft website it says that only the north usa zone is congested, but i tried it with:
australia,
east us,
east us2
and none of them work.
for each test i created a new resource imagining it to be this problem.
error

some of the tests","['azure', 'openai-api', 'azure-openai']",76994762,"sorry to disappoint you, it seems like its turned off for now, for all regions. fine-tuning is currently unavailable to new customers. this is mentioned in features overview
you can see fine-tuning models in the documentation is set to n/a


doc:",https://stackoverflow.com/questions/76994550,azure,28-08-2023 16:31,999.0,1.0,1.0,True,15-10-2023 16:06,15-10-2023 16:06
72350021,spacy matcher isn&#39;t always matching,"i can't figure out why the matcher isn't working. this works:
test = [""14k""]

nlp = spacy.blank(""en"")
matcher = matcher(nlp.vocab)

matcher.add(""test"", [[{""norm"": ""14k""}]])

docs = []
for doc in nlp.pipe(test):
    matches = matcher(doc)
    print(matches)

but if i change 14k to 14k in both my matcher and text, the matcher finds nothing.  why? i just want to understand the difference and why this doesn't work and how i could go about troubleshooting this myself in the future.  i've looked at the docs:

and can't figure out where i'm going wrong.  i changed ""norm"" to orth and text and it still hasn't found it. thank you for any help.
edit
ok, so i did:
for ent in doc:
   print(ent)

and for the lowercase version, spacy was catorgizing it all as one ent, but when i uppercased the k, spacy says it two different ents. with this knowledge i did, matcher.add(""test"", [[{""orth"": ""14""}, {""orth"":""k""}]]) and it worked.
i still want to know why.  why does spacy think 14k is one ""word"" but 14k is two ""words""?","['python-3.x', 'spacy', 'spacy-3']",72357752,"it looks like you may be running into issues with differences in tokenization for this kind of sequence. in particular note that things that look like temperatures (so number + [fck]) may get special treatment. this may seem odd but it usually results in better compatibility with existing corpora.
you can find out why an input is tokenized a particular way using tokenizer.explain() like so:
import spacy

nlp = spacy.blank(""en"")
print(nlp.tokenizer.explain(""14k""))
print(""..."")
print(nlp.tokenizer.explain(""14k""))

that gives the output:
[('token', '14'), ('suffix', 'k')]
...
[('token', '14k')]

you can read more about this at the tokenizer.explain docs.",https://stackoverflow.com/questions/72350021,python-3.x,23-05-2022 14:11,552.0,1.0,1.0,True,24-05-2022 05:49,23-05-2022 14:41
77622170,how can i execute openai&#39;s api with changed proxy in a better way?,"according to the way which openai officially offered, i can't execute the api because of some geographic reasons. although i think i have changed the proxies, it still cause error as apiconnectionerror and 429.
it is so weird because i believe i only send one request,
and when i do the same options in java before with okhttp (execute openai api),
it is 429 too. so weird!
import requests
import openai

proxies = {
# my proxy host
    ""
    ""
}
requests.session().proxies.update(proxies)

openai.api_key = ""myapikey""

completion = openai.chat.completions.create(
    model=""gpt-3.5"",
    messages=[{""role"": ""user""},
              {""content"": ""tell me about math""}]
)

print(completion)

so after i changed the way, and it succeeds finally.
here is code.
import requests
# openai api url
url = ""
headers = {
    ""authorization"": ""bearer myapikey"",
    ""content-type"": ""application/json""
}
proxies = {
# my proxy host
    "" ""
    "" ""
}
data = {
    ""prompt"": ""tell me about math"",
    ""max_tokens"": 60
}
response = requests.post(url, json=data, headers=headers, verify=false)

in this traditional way, i can use it successfull, but i still want to improve it,and i also have heard if you execute openai's api in this way, your apikey will be banned.
can anybody help me with this, like improve the executing way adding some other codes?
or how can i execute it with openai's officially package? i have stuckwith n it for while, waiting for your answer please. :)
that's the official fail record:

{'id': 'cmpl-8tcp9evm1poau8pbmybpfabjtbkaz', 'object': 'text_completion', 'created': 1701971499, 'model': 'gpt-3.5-turbo-instruct', 'choices': [{'text': '\n\nmath, also known as mathematics, is the study of numbers, quantity, and space. it is a fundamental subject in education and plays a crucial role in various fields such as science, engineering, and finance.\n\nthe study of math involves learning about mathematical concepts, theories, and techniques to solve problems', 'index': 0, 'logprobs': none, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 4, 'completion_tokens': 60, 'total_tokens': 64}}

that's the success record in traditional way.
traceback (most recent call last):
  file ""c:\user\pycharmprojects\pythonproject\main.py"", line 42, in <module>
    completion = openai.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""c:\user\pycharmprojects\pythonproject\venv\lib\site-packages\openai\_utils\_utils.py"", line 301, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  file ""c:\users\pycharmprojects\pythonproject\venv\lib\site-packages\openai\resources\chat\completions.py"", line 598, in create
    return self._post(
           ^^^^^^^^^^^
  file ""c:\users\pycharmprojects\pythonproject\venv\lib\site-packages\openai\_base_client.py"", line 1096, in post
    return cast(responset, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""c:\users\pycharmprojects\pythonproject\venv\lib\site-packages\openai\_base_client.py"", line 856, in request
    return self._request(
           ^^^^^^^^^^^^^^
  file ""c:\users\pycharmprojects\pythonproject\venv\lib\site-packages\openai\_base_client.py"", line 894, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  file ""c:\user\pycharmprojects\pythonproject\venv\lib\site-packages\openai\_base_client.py"", line 966, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  file ""c:\users\pycharmprojects\pythonproject\venv\lib\site-packages\openai\_base_client.py"", line 894, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  file ""c:\users\pycharmprojects\pythonproject\venv\lib\site-packages\openai\_base_client.py"", line 966, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  file ""c:\users\pycharmprojects\pythonproject\venv\lib\site-packages\openai\_base_client.py"", line 908, in _request
    raise self._make_status_error_from_response(err.response) from none
openai.ratelimiterror: error code: 429 - {'error': {'message': 'you exceeded your current quota, please check your plan and billing details.', 'type': 'insufficient_quota', 'param': none, 'code': 'insufficient_quota'}}","['python', 'python-requests', 'proxy', 'openai-api']",77658673,"these are the officially supported openai countries:

the client docs have an example of configuring a proxy:
import 
from openai import openai

client = openai(
    # or use the `openai_base_url` env var
    base_url=""
    
        proxies=""
        transport=
    ),
)",https://stackoverflow.com/questions/77622170,python,07-12-2023 17:57,2725.0,-1.0,1.0,True,14-12-2023 08:19,07-12-2023 19:47
58417374,how to load the saved tokenizer from pretrained model,"i fine-tuned a pretrained bert model in pytorch using huggingface transformer. all the training/validation is done on a gpu in cloud.
at the end of the training, i save the model and tokenizer like below:
best_model.save_pretrained('./saved_model/')
tokenizer.save_pretrained('./saved_model/')

this creates below files in the saved_model directory:
config.json
added_token.json
special_tokens_map.json
tokenizer_config.json
vocab.txt
pytorch_model.bin

now, i download the saved_model directory in my computer and want to load the model and tokenizer. i can load the model like below
model = torch.load('./saved_model/pytorch_model.bin',map_location=torch.device('cpu'))
but how do i load the tokenizer? i am new to pytorch and not sure because there are multiple files. probably i am not saving the model in the right way?","['machine-learning', 'pytorch', 'huggingface-transformers']",58427691,"if you look at the syntax, it is the directory of the pre-trained model that you are supposed to pass. hence, the correct way to load tokenizer must be:
tokenizer = berttokenizer.from_pretrained(<path to the directory containing pretrained model/tokenizer>)

in your case:
tokenizer = berttokenizer.from_pretrained('./saved_model/')

./saved_model here is the directory where you'll be saving your pretrained model and tokenizer.",https://stackoverflow.com/questions/58417374,machine-learning,16-10-2019 15:57,30477.0,12.0,2.0,True,28-11-2023 07:13,11-07-2021 19:00
76411359,openai api: how do i migrate from text-davinci-003 to gpt-3.5-turbo in nodejs?,"how do i migrate from text-davinci-003 to gpt-3.5-turbo?
what i tried to do is the following:
changing this...
model: ""text-davinci-003""

...to this.
model: ""gpt-3.5-turbo""

also, changing this...
const api_url = ""

...to this.
const api_url = ""

the problem is that it does not work. the code i will be giving is the unmodified code, so that anyone can help me what to change.
why i wanted this upgrade?
i was irritated by text-davinci-003's completion. like sending ""hello"" gives me an entire letter not a greeting.
live sample (via github pages):

github repository:","['openai-api', 'gpt-3', 'chatgpt-api', 'text-davinci-003']",76412710,"you're using the gpt-3.5-turbo model.
there are three main differences between the chat completions api (i.e., the gpt-3.5 api) and the completions api (i.e., the gpt-3 api).

api endpoint

completions api: 
chat completions api: 


the prompt parameter (completions api) is replaced by the messages parameter (chat completions api)
response access

completions api: response.choices[0].text.trim()
chat completions api: response.choices[0].message.content.trim()



try this:
const getchatresponse = async (incomingchatdiv) => {
    const api_url = "" /* changed */
    const pelement = document.createelement(""p"");

    // define the properties and data for the api request
    const requestoptions = {
        method: ""post"",
        headers: {
            ""content-type"": ""application/json"",
            ""authorization"": `bearer ${api_key}`
        },
        body: json.stringify({
            model: ""gpt-3.5-turbo"",
            messages: [{role: ""user"", content: `${usertext}`}], /* changed */
            max_tokens: 2048,
            temperature: 0.2,
            n: 1,
            stop: null
        })
    }

    // send post request to api, get response and set the reponse as paragraph element text
    try {
        const response = await (await fetch(api_url, requestoptions)).json();
        pelement.textcontent = response.choices[0].message.content.trim(); /* changed */
    } catch (error) { // add error class to the paragraph element and set error text
        pelement.classlist.add(""error"");
        pelement.textcontent = ""oops! something went wrong while retrieving the response. please try again."";
    }

    // remove the typing animation, append the paragraph element and save the chats to local storage
    incomingchatdiv.queryselector("".typing-animation"").remove();
    incomingchatdiv.queryselector("".chat-details"").appendchild(pelement);
    localstorage.setitem(""all-chats-thedoggybrad"", chatcontainer.innerhtml);
    chatcontainer.scrollto(0, chatcontainer.scrollheight);
}",https://stackoverflow.com/questions/76411359,openai-api,06-06-2023 03:56,1326.0,0.0,1.0,True,14-09-2023 15:02,07-06-2023 10:50
63312859,how to change huggingface transformers default cache directory?,"the default cache directory lacks disk capacity, i need to change the configuration of the default cache directory. how can i do that?",['huggingface-transformers'],63314437,"you can specify the cache directory whenever you load a model with .from_pretrained by setting the parameter cache_dir. you can also set a default location by exporting an environment variable hf_home each time before you use the library (i.e. before importing it!).
python example:
import os
os.environ['hf_home'] = '/blabla/cache/'

bash example:
export hf_home=/blabla/cache/

windows example:
set hf_home=e:\huggingface_cache

google colab example (export via os works fine but not the bash variant. an alternative are the magic commands):
%env hf_home=/blabla/cache/

transformers <v4.0.0
use the variable transformers_cache instead of hf_home. you can also use it in v4.0.0 <= transformers <= v5.0.0 but starting from v4.36.0 you will see the following warning:

futurewarning: using transformers_cache is deprecated and will be
removed in v5 of transformers. use hf_home instead.",https://stackoverflow.com/questions/63312859,huggingface-transformers,08-08-2020 07:28,229148.0,135.0,8.0,True,07-04-2025 13:56,09-02-2025 22:26
76982659,how to get attentions part from the output of a bert model?,"i am using bert-model for  query expansion and i am trying to extract the keywords from the document i have
tokenizer = berttokenizer.from_pretrained(""bert-base-uncased"")
model = bertmodel.from_pretrained(""bert-base-uncased"")
sentence=""this is a sentence""
tokens = tokenizer.tokenize(sentence)
print(tokens,""-tokens"")
input_ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(\[input_ids\])
print(input_ids)
with torch.no_grad():
output = model(input_ids)
attention_scores = output.attentions
print(attention_scores)   #this prints none

this is my code i am using a simple sentence and trying to extract keywords from it in order to do that i need attention of the output part of bert-model
i tried with different tokenizer methods(tokenize,encode,encode_plus) to tokenize and tried with
different bert variants (bert-large-uncased)
i want to extract the attention part form the model output but i am not able to do that
i get none in that place i am not able to get any value in the attention part of  the output","['python', 'nlp', 'huggingface-transformers', 'bert-language-model', 'word-embedding']",76983149,"you can't really extract keywords from the code you provided. your output has no attribute called attentions, that's why output.attentions is returning none, hence the error you're facing.
i benchmarked some transformer models for keyword extraction last year for my university project. i will provide you with a solution from there. this script will return you the extracted keywords with their score as a dictionary.
solution

install the dependencies

pip install nltk
pip install spacy
pip install torch
pip install transformers


necessary imports

from transformers import automodel, autotokenizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.stem.wordnet import wordnetlemmatizer
import re
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import countvectorizer
import spacy

nlp = spacy.load('en_core_web_sm')
nltk.download('stopwords')
nltk.download('wordnet')
stop_words = set(stopwords.words('english'))


utility functions

def pre_process(text):
    # lowercase
    text=text.lower()
    #remove tags
    text=re.sub(""&lt;/?.*?&gt;"","" &lt;&gt; "",text)
    # remove special characters and digits
    text=re.sub(""(\\d|\\w)+"","" "",text)
    ##convert to list from string
    text = text.split()
    # remove stopwords
    text = [word for word in text if word not in stop_words]
    # remove words less than three letters
    text = [word for word in text if len(word) >= 3]
    #lemmatize
    lmtzr = wordnetlemmatizer()
    text = [lmtzr.lemmatize(word) for word in text]
    return ' '.join(text)

def get_candidates(text):
    # group of words
    n_gram_range = (1, 1)

    # extract candidate words/phrases
    count = countvectorizer(ngram_range=n_gram_range, stop_words=""english"").fit([text])
    all_candidates = count.get_feature_names_out()
    
    doc = nlp(text)
    noun_phrases = set(chunk.text.strip() for chunk in doc.noun_chunks)

    # only pass the noun/adjective keywords for extraction
    noun_adj = set()
    for token in doc:
        if token.pos_ == ""noun"":
            noun_adj.add(token.text)
        if token.pos_ == ""adj"":
            noun_adj.add(token.text)

    all_words = noun_adj.union(noun_phrases)
    candidates = list(filter(lambda candidate: candidate in all_words, all_candidates))
    return candidates


keyword extraction method

# the keyword extraction method
def keyword_extract(string, model_name):

    # obtaining candidate keywords and document representations
    model = automodel.from_pretrained(model_name)
    tokenizer = autotokenizer.from_pretrained(model_name)

    text=pre_process(string)
    candidates=get_candidates(text)
    candidate_tokens = tokenizer(candidates, padding=true, return_tensors=""pt"")
    candidate_embeddings = model(**candidate_tokens)[""pooler_output""]

    # determination of keywords:
    text_tokens = tokenizer([text], padding=true, return_tensors=""pt"", truncation=true, max_length=512)
    text_embedding = model(**text_tokens)[""pooler_output""]

    candidate_embeddings = candidate_embeddings.detach().numpy()
    text_embedding = text_embedding.detach().numpy()

    distances = cosine_similarity(text_embedding, candidate_embeddings)
    keywords = {i:j for i,j in zip(candidates, distances[0])}

    # sort based on attention score and return the dictionary
    return dict(sorted(keywords.items(), key=lambda x:x[1], reverse=true))

# put the model name here
model_name = ""bert-base-uncased""

my_sentence = ""hello there! this is an example sentence and this is the code that i'm using to extract keywords from a sentence""

keywords = keyword_extract(my_sentence, model_name)
print(keywords)

output
output returned the keywords sorted according to their score
{'keywords': 0.97554314, 'example': 0.94142365, 'extract': 0.885766, 'code': 0.83722556, 'sentence': 0.76782566}",https://stackoverflow.com/questions/76982659,python,26-08-2023 11:00,560.0,0.0,1.0,True,27-08-2023 17:45,27-08-2023 17:45
61633485,extract noun phrases with stanza and corenlpclient,"i am trying to extract noun phrases from sentences using stanza(with stanford corenlp). this can only be done with the corenlpclient module in stanza. 
# import client module
from stanza.server import corenlpclient
# construct a corenlpclient with some basic annotators, a memory allocation of 4gb, and port number 9001
client = corenlpclient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse'], memory='4g', endpoint='

here is an example of a sentence, and i am using the tregrex function in client to get all the noun phrases. tregex function returns a dict of dicts in python. thus i needed to process the output of the tregrex before passing it to the tree.fromstring function in nltk to correctly extract the noun phrases as strings. 
pattern = 'np'
text = ""albert einstein was a german-born theoretical physicist. he developed the theory of relativity.""
matches = client.tregrex(text, pattern) ``

hence, i came up with the method stanza_phrases which has to loop through the dict of dicts which is the output of tregrex and correctly format for tree.fromstring in nltk.
def stanza_phrases(matches):
  nps = []
  for match in matches:
    for items in matches['sentences']:
      for keys,values in items.items():
        s = '(root\n'+ values['match']+')'
        nps.extend(extract_phrase(s, pattern))
  return set(nps)

generates a tree to be used by nltk 
from nltk.tree import tree
def extract_phrase(tree_str, label):
    phrases = []
    trees = tree.fromstring(tree_str)
    for tree in trees:
        for subtree in tree.subtrees():
            if subtree.label() == label:
                t = subtree
                t = ' '.join(t.leaves())
                phrases.append(t)

    return phrases

here is my output:
{'albert einstein', 'he', 'a german-born theoretical physicist', 'relativity',  'the theory', 'the theory of relativity'}

is there a way i can make this more code efficient with less number of lines (especially, stanza_phrases and extract_phrase methods)","['python', 'nlp', 'stanford-nlp', 'stanford-stanza']",61648173,"from stanza.server import corenlpclient

# get noun phrases with tregex
def noun_phrases(_client, _text, _annotators=none):
    pattern = 'np'
    matches = _client.tregex(_text,pattern,annotators=_annotators)
    print(""\n"".join([""\t""+sentence[match_id]['spanstring'] for sentence in matches['sentences'] for match_id in sentence]))

# english example
with corenlpclient(timeout=30000, memory='16g') as client:
    englishtext = ""albert einstein was a german-born theoretical physicist. he developed the theory of relativity.""
    print('---')
    print(englishtext)
    noun_phrases(client,englishtext,_annotators=""tokenize,ssplit,pos,lemma,parse"")

# french example
with corenlpclient(properties='french', timeout=30000, memory='16g') as client:
    frenchtext = ""je suis john.""
    print('---')
    print(frenchtext)
    noun_phrases(client,frenchtext,_annotators=""tokenize,ssplit,mwt,pos,lemma,parse"")",https://stackoverflow.com/questions/61633485,python,06-05-2020 11:00,2898.0,4.0,2.0,True,16-01-2023 20:58,19-05-2020 08:34
71768061,huggingface transformers classification using num_labels 1 vs 2,"question 1)
the answer to this question suggested that for a binary classification problem i could use num_labels as 1 (positive or not) or 2 (positive and negative). is there any guideline regarding which setting is better? it seems that if we use 1 then probability would be calculated using sigmoid function and if we use 2 then probabilities would be calculated using softmax function.
question 2)
in both cases are my y labels going to be same? each data point will have 0 or 1 and not one hot encoding? for example, if i have 2 data points then y would be 0,1 and not [0,0],[0,1]
i have very unbalanced classification problem where class 1 is present only 2% of times. in my training data i am oversampling
question 3)
my data is in pandas dataframe and i am converting it to a dataset and creating y variable using below. how should i cast my y column - label if i am planning to use num_labels=1?
`train_dataset=dataset.from_pandas(train_df).cast_column(""label"", classlabel(num_classes=2, names=['neg', 'pos'], names_file=none, id=none))`","['python', 'classification', 'huggingface-transformers']",72510500,"well, it probably is kind of late. but i want to point out one thing, according to the hugging face code, if you set num_labels = 1, it will actually trigger the regression modeling, and the loss function will be set to mseloss(). you can find the code here.
also, in their own tutorial, for a binary classification problem (imdb, positive vs. negative), they set num_labels = 2.
from transformers import automodelforsequenceclassification, trainingarguments, trainer
model = automodelforsequenceclassification.from_pretrained(""distilbert-base-uncased"", num_labels=2)

here is the link.",https://stackoverflow.com/questions/71768061,python,06-04-2022 13:53,6986.0,5.0,2.0,True,05-06-2022 19:26,08-04-2022 17:47
71189665,change value in pandas dataframe using iteration,"i have a training data set of the following format:
print(data.head(5))
#output



           0  1
0  a b c d e  1
1  a b c d e  1
2  a b c d e  1
3  a b c d e  1
4  a b c d e  1

it is a text classification task and i am trying to split the text ""a b c d e"" in to a python list. i tried iteration:
data #the dataset
len_data = len(data)
for row_num in range(len_data):
    data.loc[row_num, 0] = data.loc[row_num, 0].split("" "")

however this doesn't work and returned the error must have equal len keys and value when setting with an iterable. could someone help me with this problem? many thanks!","['python', 'pandas', 'dataframe', 'nlp']",71189717,"use str.split:
df[0] = df[0].str.split()
print(df)

# output
                 0  1
0  [a, b, c, d, e]  1
1  [a, b, c, d, e]  1
2  [a, b, c, d, e]  1
3  [a, b, c, d, e]  1
4  [a, b, c, d, e]  1

setup:
data = {0: {0: 'a b c d e', 1: 'a b c d e'}, 1: {0: 1, 1: 1}}
df = pd.dataframe(data)",https://stackoverflow.com/questions/71189665,python,19-02-2022 22:15,79.0,1.0,1.0,True,19-02-2022 23:51,19-02-2022 23:51
70064477,how to handle lemmatizertrainer &#39;utfdataformatexception: encoded string too long&#39;?,"i am using opennlp to train a model for lemmatization of german words. therefore i use the opennlp cli and the training set of ud_german-hdt which can be downloaded here
the training itself works fine (just need a little bit of ram) but the cli fails to write the model because of an utfdataformatexception: encoded string too long exception.
the cli command i am using: opennlp lemmatizertrainerme.conllu -params params.txt -lang de -model de-lemmatizer.bin -data ud_german-hdt/de_hdt-ud-train.conllu -encoding utf-8
stacktrace:
writing lemmatizer model ... failed
error during writing model file 'de-lemmatizer.bin'
encoded string too long: 383769 bytes
java.io.utfdataformatexception: encoded string too long: 383769 bytes
        at java.base/java.io.dataoutputstream.writeutf(dataoutputstream.java:364)
        at java.base/java.io.dataoutputstream.writeutf(dataoutputstream.java:323)
        at opennlp.tools.ml.maxent.io.binarygismodelwriter.writeutf(binarygismodelwriter.java:71)
        at opennlp.tools.ml.maxent.io.gismodelwriter.persist(gismodelwriter.java:97)
        at opennlp.tools.ml.model.genericmodelwriter.persist(genericmodelwriter.java:75)
        at opennlp.tools.util.model.modelutil.writemodel(modelutil.java:71)
        at opennlp.tools.util.model.genericmodelserializer.serialize(genericmodelserializer.java:36)
        at opennlp.tools.util.model.genericmodelserializer.serialize(genericmodelserializer.java:29)
        at opennlp.tools.util.model.basemodel.serialize(basemodel.java:597)
        at opennlp.tools.cmdline.cmdlineutil.writemodel(cmdlineutil.java:182)
        at opennlp.tools.cmdline.lemmatizer.lemmatizertrainertool.run(lemmatizertrainertool.java:77)
        at opennlp.tools.cmdline.cli.main(cli.java:256)

has somebody encountered this problem and has a solution?","['java', 'nlp', 'opennlp', 'lemmatization']",74253692,"recently, i've written a patch to cure opennlp-1366. the related pr  documents the problem and solution in detail.
in this context, the upcoming opennlp version 2.0.1 will bring the cure for the problem reported in the op. updating to the aforementioned version will resolve the crashing during writing trained model files.
note: 
i verified that the patch works with ud_german-hdt, ud_german-gsd, and other treebanks for the german language.",https://stackoverflow.com/questions/70064477,java,22-11-2021 10:41,473.0,1.0,1.0,True,30-10-2022 14:05,30-10-2022 14:05
78525417,&quot;deadline&quot; error when embedding video with google vertex ai multimodal embedding modal,"i am currently using vertex ai's multimodal embedding model (
i was able to get the image and text examples running just fine using the python sdk and post, however, when i try to run the following video example (from the above link), i get the following error
    import vertexai
    
    from vertexai.vision_models import multimodalembeddingmodel, video
    
    project_id = 'project_name'
    location = 'us-central1'
    
    vertexai.init(project=project_id, location=location)
    
    # document metadata
    video_path = 'gs://test-public-bucket-123/dog_jumping_short.mp4' # public small 1 mb 7 second video file
    description = 'dogs jumping'
    
    model = multimodalembeddingmodel.from_pretrained(""multimodalembedding@001"")
    
    video = video.load_from_file(video_path)
    
    embeddings = model.get_embeddings(
        video=video
    )
    
    # video embeddings are segmented based on the video_segment_config.
    print(""video embeddings:"")
    for video_embedding in embeddings.video_embeddings:
        print(
            f""video segment: {video_embedding.start_offset_sec} - {video_embedding.end_offset_sec}""
        )
        print(f""embedding: {video_embedding.embedding}"")
    
    print(f""text embedding: {embeddings.text_embedding}"")

the error i get when running this sample code is related to deadline
traceback (most recent call last):
  file ""/users/me/code/multi-modal-test/multi-modal/lib/python3.12/site-packages/google/api_core/grpc_helpers.py"", line 76, in error_remapped_callable
    return callable_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""/users/me/code/multi-modal-test/multi-modal/lib/python3.12/site-packages/grpc/_channel.py"", line 1181, in __call__
    return _end_unary_response_blocking(state, call, false, none)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""/users/me/code/multi-modal-test/multi-modal/lib/python3.12/site-packages/grpc/_channel.py"", line 1006, in _end_unary_response_blocking
    raise _inactiverpcerror(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._inactiverpcerror: <_inactiverpcerror of rpc that terminated with:
        status = statuscode.unauthenticated
        details = ""video embedding failed with the following error: deadline""
        debug_error_string = ""unknown:error received from peer ipv4:142.250.81.234:443 {grpc_message:""video embedding failed with the following error: deadline"", grpc_status:16, created_time:""2024-05-23t15:53:15.429704-04:00""}""

note that i have been able to embed contextual_text and image just fine with the same authentication, so i am fairly certain this has nothing to do with authentication even though the error says so.
i have also tried to post using the following curl, but i get the same error response deadline
    curl -x post \ 
     -h ""authorization: bearer $(gcloud auth print-access-token)"" \
     -h ""content-type: application/json; charset=utf-8"" \
     -d @request.json \
     ""

i have the following apis enabled (note that according to docs, i only need to have vertex ai api enabled)

these are my iam permissions
- members:
  - user:me@gmail.com
  role: roles/aiplatform.admin
- members:
  - user:me@gmail.com
  role: roles/aiplatform.user
- members:
  - user:me@gmail.com
  role: roles/ml.admin
- members:
  - user:me@gmail.com
  role: roles/owner
- members:
  - user:me@gmail.com
  role: roles/storage.admin
- members:
  - user:me@gmail.com
  role: roles/visionai.admin
etag: bwyajsgtzok=
version: 1

i am not using a service account, but a user account.
anyone know what i can do here?","['google-cloud-platform', 'word-embedding', 'google-cloud-vertex-ai', 'video-embedding']",78618078,"this might come from a discrepancy between the ""current project"" of gcloud, and the ""application defaults credentials"". this can be confusing because the exact order of these commands matter.
to make sure, could you try in this order:
1.
gcloud config set project my_project_id

(doc)
2.
gcloud auth application-default login

(doc)

run your python program (or your curl command)",https://stackoverflow.com/questions/78525417,google-cloud-platform,23-05-2024 20:27,485.0,2.0,4.0,True,15-08-2024 12:29,05-06-2024 17:49
71099545,failedpreconditionerror: table not initialized,"i am trying to create an nlp neural-network using the following code:
imports:
import zipfile
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import textvectorization,embedding,input,globalaveragepooling1d,dense
from sklearn.model_selection import train_test_split

download and unzip dataset:
# download data (same as from kaggle)
!wget ""

# unzip data
zip_ref = zipfile.zipfile(""nlp_getting_started.zip"", ""r"")
zip_ref.extractall()
zip_ref.close()

split data into train and test datasets:
    train_df = pd.read_csv(""train.csv"")
    
    train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df['text'], train_df['target'],train_size=.8)
    average_output_sequence_length = 15

                                           

create the neural-network:
input = input(shape=(1,),dtype='string')
x = textvectorization(max_tokens=10000,
                      ngrams=5,
                      standardize='lower_and_strip_punctuation',
                      output_mode='int',
                      output_sequence_length = average_output_sequence_length)(input)
x = embedding(input_dim=22,embeddings_initializer='uniform',output_dim=128, name= 'embeding_layer')(x)
x = globalaveragepooling1d()(x)
output = dense(1,activation='sigmoid')(x)

model = tf.keras.model(input,output)

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'] )

model.fit(x=train_sentences,y=train_labels,epochs=6,validation_data=(val_sentences,val_labels))

unfortunately, when i run the code, i face with the following error:
epoch 1/6
---------------------------------------------------------------------------
failedpreconditionerror                   traceback (most recent call last)
<ipython-input-52-991547d73612> in <module>()
     13 model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'] )
     14 
---> 15 model.fit(x=train_sentences,y=train_labels,epochs=6,validation_data=(val_sentences,val_labels))

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     57     ctx.ensure_initialized()
     58     tensors = pywrap_tfe.tfe_py_execute(ctx._handle, device_name, op_name,
---> 59                                         inputs, attrs, num_outputs)
     60   except core._notokstatusexception as e:
     61     if name is not none:

failedpreconditionerror:  table not initialized.
     [[node model_6/text_vectorization_7/string_lookup_7/none_lookup/lookuptablefindv2
 (defined at /usr/local/lib/python3.7/dist-packages/keras/layers/preprocessing/index_lookup.py:669)
]] [op:__inference_train_function_5066]

errors may have originated from an input operation.

update:
i got it to work when i change the way that i use the functional code:
create textvectorization function:
text_vectorization_layer =  textvectorization(max_tokens=10000,
                                              ngrams=5,
                                              standardize='lower_and_strip_punctuation',
                                              output_mode='int',
                                              output_sequence_length = average_output_sequence_length
                                              )

and then finally create the neural-network:
input = input(shape=(1,),dtype='string')
x = text_vectorization_layer(input)
x = embedding(input_dim=22,embeddings_initializer='uniform',output_dim=128, name= 'embeding_layer')(x)
x = globalaveragepooling1d()(x)
output = dense(1,activation='sigmoid')(x)

model = tf.keras.model(input,output)

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'] )

model.fit(x=train_sentences,y=train_labels,epochs=6,validation_data=(val_sentences,val_labels))

question:
but, i still do not understand why these change fix the issue ?
in other word, why the:
x = textvectorization(input)

and the
x = textvectorization(max_tokens=10000,
                      ngrams=5,
                      standardize='lower_and_strip_punctuation',
                      output_mode='int',
                      output_sequence_length = average_output_sequence_length)(input)

are not equal ?","['python', 'tensorflow', 'keras', 'deep-learning', 'nlp']",71099954,"the textvectorization layer is a preprocessing layer that needs to be instantiated before being called. also as the docs explain:

the vocabulary for the layer must be either supplied on construction
or learned via adapt().

another important information can be found here:

crucially, these layers are non-trainable. their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by ""adapting"" them on data

furthermore, it is important to note, that the textvectorization layer uses an underlying stringlookup layer that also needs to be initialized beforehand. otherwise, you will get the failedpreconditionerror: table not initialized as you posted.",https://stackoverflow.com/questions/71099545,python,13-02-2022 09:55,2121.0,1.0,1.0,True,13-02-2022 11:58,13-02-2022 11:34
72917714,combinations of words that co-occur most often across strings,"here's the problem...
i have a list of strings:
strings = ['one two three four', 'one two four five', 'four one two', 'three four']

i'm trying to find combinations of words that co-occur in two or more strings.
and here's the output i'm trying to get...

[one, two, four] - 3 times
[three, four] - 2 times
[one, two] - 3 times
[two, four] - 3 times

the combinations could be any length of two or more words.
here's what i've already looked at - though i'm not having much luck finding anything i can bootstrap for my needs : (

text processing to find co-occurences of strings

efficient way of extracting co-occurence values of specific word pairs from python counter() results","['python', 'pandas', 'nlp']",72917799,"you can compute the powersets with minimum 2 combinations and count the combinations:
from itertools import chain, combinations
from collections import counter

# 
def powerset(iterable, min=2):
    ""powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)""
    s = list(iterable)
    return chain.from_iterable(combinations(s, r) for r in range(min, len(s)+1))

c = counter(chain.from_iterable(set(powerset(s.split()))
            for s in strings))

# keep counts of 2 or more
out = {k: v for k, v in c.items() if v >= 2}

output:
{('three', 'four'): 2, 
 ('two', 'four'): 2, 
 ('one', 'two', 'four'): 2, 
 ('one', 'four'): 2, 
 ('one', 'two'): 3}

keep order
use:
c = counter(chain.from_iterable(tuple(powerset(s.split()))
            for s in strings))",https://stackoverflow.com/questions/72917714,python,08-07-2022 22:53,790.0,2.0,1.0,True,08-07-2022 23:42,08-07-2022 23:14
75874218,fails to generate image using openai,"i am trying to take a text from discord user and then generating an image of that. i made a code for that but openai shows 400 error.
error:
error: request failed with status code 400
    at createerror (/users/anshtyagi/documents/backup/s/node_modules/openai/node_modules/axios/lib/core/createerror.js:16:15)
    at settle (/users/anshtyagi/documents/backup/s/node_modules/openai/node_modules/axios/lib/core/settle.js:17:12)
    at incomingmessage.handlestreamend (/users/anshtyagi/documents/backup/s/node_modules/openai/node_modules/axios/lib/adapters/
    at incomingmessage.emit (node:events:538:35)
    at endreadablent (node:internal/streams/readable:1345:12)
    at processticksandrejections (node:internal/process/task_queues:83:21)

code:
const { configuration, openaiapi } = require('openai')
const configuration = new configuration({
    apikey: 'sk-aqe0ero77gonnsbr2n7'
})
const openai = new openaiapi(configuration);

let text = interaction.options.getstring(`text`)
const response = await openai.createimage({
  prompt: text,
  n: 1,
  size: '1024x1024',
});
let imageurl = response.data.data[0].url;
const embed = new embedbuilder()
  .settitle('generated image')
  .setimage(imageurl);
interaction.editreply(embed)
  .catch(error => {
    interaction.editreply('failed to generate image');
  });

i have taking prompt as text from discord slash command options
i don't know what is this error for please help me fix this.","['javascript', 'openai-api']",76564074,that was a issue with openai which is now resolved it will work now.,https://stackoverflow.com/questions/75874218,javascript,29-03-2023 07:16,686.0,0.0,1.0,True,27-06-2023 10:46,29-03-2023 15:59
60290640,embedding layer in keras: vocab size +1,"from a number of examples i have seen, when we use text_tokenizer from keras, when specifying the input size for the input layer, we use vocab size +1.  this naturally yields an embedding space with +1 'rows'.  
for example, i fit a simple model to estimate the embedding vectors for a vocab of size 3 = i like turtles.  the embedding space has length 5 per word in our vocabulary.
the embedding weights are:
0.01209533  0.034303080 -0.04666784 0.02803965  -0.03691160
-0.01302978 -0.030584216    -0.02506201 0.04771456  0.01906699
0.02800793  0.042204402 0.05223191  -0.01184921 0.02000498
0.02692273  -0.008792922    0.01560913  -0.02783649 0.02692282


my question:  i assume that the first ""row"" in our matrix is the 0-based vector, such that rows 2, 3, and 4 would be associated with ""i"", ""like"", and ""turtles"" respectively.   
is this the case?  i want to ensure that i align my vocabulary properly, but i haven't been able to pin down any documentation to confirm this assumption.","['r', 'keras', 'tensorflow2.0', 'word-embedding']",60294856,"i understand that you are wanting to extract the embedding for each word, but i think the real question is: what is the output the tokenizer is producing.
also, that tokenizer is a bit of a mess. you'll see what i mean below.
because the tokenizer will filter words (assuming a non-trivial vocabulary), i don't want to assume that the words are stored in the order in which they are found. so here i programmatically determine the vocabulary using word_index. i then explicitly check what words are tokenized after filtering for the most frequently used words. (word_index remembers all words; i.e. the pre-filtered values.)
import tensorflow as tf
from tensorflow.keras.preprocessing.text import tokenizer
corpus = 'i like turtles'
num_words = len(corpus.split())
oov = 'oov'
tokenizer = tokenizer(num_words=num_words + 2, oov_token=oov)
tokenizer.fit_on_texts(corpus.split())
print(f'word_index: {tokenizer.word_index}')
print(f'vocabulary: {tokenizer.word_index.keys()}')
text = [key for key in tokenizer.word_index.keys()]
print(f'keys: {text}: {tokenizer.texts_to_sequences(text)}')

text = 'i like turtles'.split()
print(f'{text}: {tokenizer.texts_to_sequences(text)}')

text = 'i like marshmallows'.split() 
print(f'{text}: {tokenizer.texts_to_sequences(text)}')

this produces the following output:
word_index: {'oov': 1, 'i': 2, 'like': 3, 'turtles': 4}
vocabulary: dict_keys(['oov', 'i', 'like', 'turtles'])
keys: ['oov', 'i', 'like', 'turtles']: [[1], [2], [3], [4]]
['i', 'like', 'turtles']: [[2], [3], [4]]
['i', 'like', 'marshmallows']: [[2], [3], [1]]

however, if you specify oov_token, the output looks like this:
{'oov': 1, 'i': 2, 'like': 3, 'turtles': 4}

notice how i had to specify num_words=num_words + 2 instead of the expected '+1'.
that's because we're explicitly defining an oov token, which gets added to the vocabulary, which is a bit nuts imo.
if you specify an oov token and you set num_words=num_words + 1 (as documented), then 'i like turtles' gets the same encoding as 'i like marshmallows'. also nuts.
hopefully, you now have to tools to know what the tokenizer is feeding the encoding layer. then hopefully, it'll be trivial to correlate the tokens with their embeddings.
please let us know what you find. :) 
(for more on the madness, check out this stackoverflow post.)",https://stackoverflow.com/questions/60290640,r,18-02-2020 22:57,3515.0,4.0,2.0,True,26-11-2022 21:29,19-02-2020 04:13
66712753,how to use languagedetector() from spacy_langdetect package?,"i'm trying to use the spacy_langdetect package and the only example code i can find is (
import spacy
from spacy_langdetect import languagedetector
nlp = spacy.load(""en_core_web_sm"")
nlp.add_pipe(languagedetector(), name='language_detector', last=true)
text = 'this is an english text.'
doc = nlp(text)
print(doc._.language)

it's throwing error:

nlp.add_pipe now takes the string name of the registered component factory, not a callable component.

so i tried using the below for adding to my nlp pipeline
language_detector = languagedetector()
nlp.add_pipe(""language_detector"")

but this gives error:

can't find factory for 'language_detector' for language english (en). this usually happens when spacy calls nlp.create_pipe with a custom component name that's not registered on the current language class. if you're using a transformer, make sure to install 'spacy-transformers'. if you're using a custom component, make sure you've added the decorator @language.component (for function components) or @language.factory (for class components).
available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer

i don't fully understand how to add it since it's not really a custom component.","['python', 'spacy']",66727355,"with spacy v3.0 for components not built-in such as languagedetector, you will have to wrap it into a function prior to adding it to the nlp pipe. in your example, you can do the following:
import spacy
from spacy.language import language
from spacy_langdetect import languagedetector

def get_lang_detector(nlp, name):
    return languagedetector()

nlp = spacy.load(""en_core_web_sm"")
language.factory(""language_detector"", func=get_lang_detector)
nlp.add_pipe('language_detector', last=true)
text = 'this is an english text.'
doc = nlp(text)
print(doc._.language)

for built-in components (i.e. tagger, parser, ner, etc.), see:",https://stackoverflow.com/questions/66712753,python,19-03-2021 17:17,17146.0,16.0,2.0,True,14-03-2023 19:22,14-03-2023 19:22
53612938,spacy nlp word.pos returns digits instead of pos tags,"i am using spacy library for pos tagging but when i run this code, it returns numbers in the place of the pos tags:
import spacy
from spacy.lang.fr.examples import sentences

nlp = spacy.load('en')
mystring = "" i am missing my lovely family a lot.""
exuu = nlp(mystring)
for word in exuu: 
  print(word.text, word.pos)

here is how the output looks like:
102
i 94
am 99
missing 99
my 83
dear 83
family 91
a 89
lot 91
. 96","['python', 'python-3.x', 'nlp', 'spacy', 'part-of-speech']",54382451,"you are reading the ""wrong"" attribute. word.pos returns the pos tag id, not the pos tag string. to do what you want, just replace word.pos with word.pos_.
the following code will work fine:
import spacy
from spacy.lang.fr.examples import sentences
nlp = spacy.load('en')
mystring = "" i am missing my lovely family a lot.""
exuu = nlp(mystring)
for word in exuu: 
  print(word.text, word.pos_)",https://stackoverflow.com/questions/53612938,python,04-12-2018 12:23,235.0,3.0,1.0,True,08-07-2021 02:35,08-07-2021 02:35
73127139,equivalent to tokenizer() in transformers 2.5.0?,"i am trying to convert the following code to work with transformers 2.5.0. as written, it works in version 4.18.0, but not 2.5.0.
# converting pretrained bert classification model to regression model
# i.e. extracting base model and swapping out heads

from transformers import berttokenizer, bertmodel, bertconfig, bertformaskedlm, bertforsequenceclassification, autoconfig, automodelfortokenclassification
import torch
import numpy as np

old_model = bertforsequenceclassification.from_pretrained(""textattack/bert-base-uncased-yelp-polarity"")
model = bertforsequenceclassification.from_pretrained(""bert-base-uncased"", num_labels=1) 
model.bert = old_model.bert

# ensure that model parameters are equivalent except for classifier head layer
for param_name in model.state_dict():
    if 'classifier' not in param_name:
        sub_param, full_param = model.state_dict()[param_name], old_model.state_dict()[param_name] # type: torch.tensor, torch.tensor
        assert (sub_param.cpu().numpy() == full_param.cpu().numpy()).all(), param_name


tokenizer = berttokenizer.from_pretrained(""textattack/bert-base-uncased-yelp-polarity"")
inputs = tokenizer(""hello, my dog is cute"", return_tensors=""pt"")

with torch.no_grad():
    logits = model(**inputs).logits

output_value = np.array(logits)[0][0]
print(output_value)

tokenizer is not callable with transformers 2.5.0, resulting the following:
typeerror                                 traceback (most recent call last)
<ipython-input-1-d83f0d613f4b> in <module>
     19 
     20 
---> 21 inputs = tokenizer(""hello, my dog is cute"", return_tensors=""pt"")
     22 
     23 with torch.no_grad():

typeerror: 'berttokenizer' object is not callable

however, attempting to replace tokenizer() with tokenizer.tokenize() results in the following:
typeerror                                 traceback (most recent call last)
<ipython-input-2-1d431131eb87> in <module>
     21 
     22 with torch.no_grad():
---> 23     logits = model(**inputs).logits
     24 
     25 output_value = np.array(logits)[0][0]

typeerror: bertforsequenceclassification object argument after ** must be a mapping, not list

any help would be greatly appreciated.

solution
using tokenizer.encode_plus() as suggested by @cronoik:
tokenized = tokenizer.encode_plus(""hello, my dog is cute"", return_tensors=""pt"")

with torch.no_grad():
    logits = model(**tokenized)

output_value = np.array(logits)[0]
print(output_value)","['pytorch', 'tokenize', 'huggingface-transformers', 'bert-language-model', 'huggingface-tokenizers']",73127802,"sadly their documentation for the old versions is broken, but you can use encode_plus as shown in the following (he oldest available documentation of encode_plus is from 2.10.0):
import torch
from transformers import berttokenizer


t = berttokenizer.from_pretrained(""textattack/bert-base-uncased-yelp-polarity"")
tokenized = t.encode_plus(""hello, my dog is cute"", return_tensors='pt')
print(tokenized)

output:
{'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]]), 
'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 
'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}",https://stackoverflow.com/questions/73127139,pytorch,26-07-2022 16:55,395.0,1.0,1.0,True,26-07-2022 20:00,26-07-2022 19:40
77820916,openai api error: &quot;modulenotfounderror: no module named &#39;openai.error&#39;&quot;,"i am having an issue installing the openai.error package in python. this is what i am trying to install:
from flask import flask, render_template, request, jsonify
from flask_cors import cors
import openai
from openai.error import openaierror
import re
import os

after installing the openai package, i thought the openai.error package would install automatically. however, it gives me this error:
traceback (most recent call last):
  file ""c:\users\user\downloads\examiner.production\ielts_examiner_private\app.py"", line 4, in <module>
    from openai.error import openaierror
modulenotfounderror: no module named 'openai.error'","['python', 'flask', 'openai-api']",77820930,"you imported the openaierror class from the openai incorrectly.
change this...
from openai.error import openaierror

...to this.
from openai import openaierror",https://stackoverflow.com/questions/77820916,python,15-01-2024 15:54,6531.0,2.0,1.0,True,16-01-2024 14:58,15-01-2024 18:59
3158132,verbally format a number in python,"how do pythonistas print a number as words, like the equivalent of the common lisp code:
[3]> (format t ""~r"" 1e25)
nine septillion, nine hundred and ninety-nine sextillion, nine hundred and ninety-nine quintillion, seven hundred and seventy-eight quadrillion, one hundred and ninety-six trillion, three hundred and eight billion, three hundred and sixty-one million, two hundred and sixteen thousand","python, formatting, nlp, language-comparisons",3158344,"no in python core, but there is 3rd party library num2words
>>> from num2words import num2words
>>> num2words(1e25)
'ten septillion, one billion, seventy-three million, seven hundred and forty-one thousand, eight hundred and twenty-four'

>>> num2words(10000000000000000000000000)
'ten septillion'

(note that 1e25 is not converted to integer precisely, neither in your example)",https://stackoverflow.com/q/3158132,"python, formatting, nlp, language-comparisons",01-07-2010 13:17,3913.0,9.0,3.0,True,05-11-2023 04:21,02-07-2010 06:52
62360325,determining best fit distributions by sse - python 3.8,"i am trying to come up with a way to determine the ""best fit"" between the following distributions:
gaussian, multinomial, bernoulli. 
i have a large pandas df, where each column can be thought of as a distribution of numbers. what i am trying to do, is for each column, determine the distribution of the above list as the best fit.
i noticed this question which asks something familiar, but these all look like discrete distribution tests, not continuous. i know scipy has metrics for a lot of these, but i can't determine how to to properly place the inputs. my thought would be:

for each column, save the data in a temporary np array
generate gaussian, multinomial, bernoulli distributions, perform a sse test to determine the distribution that gives the ""best fit"", and move on to the next column.

an example dataset (arbitrary, my dataset is 29888 x 73231) could be:
| could | couldnt | coupl | cours | death | develop | dialogu | differ | direct | director | done |
|:-----:|:-------:|:-----:|:-----:|:-----:|:-------:|:-------:|:------:|:------:|:--------:|:----:|
|   0   |    0    |   0   |   1   |   0   |    1    |    1    |    0   |    0   |     0    |   0  |
|   0   |    2    |   1   |   0   |   0   |    1    |    0    |    2   |    0   |     0    |   1  |
|   0   |    0    |   0   |   0   |   0   |    0    |    0    |    0   |    1   |     1    |   2  |
|   1   |    0    |   0   |   0   |   0   |    1    |    0    |    1   |    0   |     0    |   0  |
|   0   |    0    |   0   |   0   |   0   |    1    |    1    |    1   |    1   |     0    |   0  |
|   0   |    0    |   0   |   1   |   0   |    0    |    0    |    0   |    0   |     0    |   1  |
|   0   |    0    |   0   |   0   |   2   |    1    |    0    |    1   |    0   |     0    |   2  |
|   0   |    0    |   0   |   0   |   0   |    1    |    0    |    0   |    2   |     0    |   1  |
|   0   |    0    |   0   |   0   |   0   |    2    |    0    |    0   |    0   |     0    |   0  |
|   0   |    0    |   0   |   1   |   0   |    0    |    5    |    0   |    0   |     0    |   3  |
|   1   |    1    |   0   |   0   |   1   |    2    |    0    |    0   |    1   |     0    |   0  |
|   1   |    1    |   0   |   0   |   0   |    4    |    0    |    0   |    1   |     0    |   1  |
|   0   |    0    |   0   |   0   |   1   |    0    |    0    |    0   |    0   |     0    |   0  |
|   0   |    0    |   0   |   0   |   0   |    0    |    1    |    0   |    0   |     0    |   0  |
|   0   |    0    |   0   |   0   |   0   |    1    |    0    |    3   |    0   |     0    |   1  |
|   2   |    0    |   0   |   0   |   0   |    0    |    0    |    0   |    1   |     0    |   2  |
|   0   |    0    |   1   |   0   |   0   |    0    |    0    |    0   |    0   |     0    |   2  |
|   1   |    1    |   0   |   0   |   1   |    0    |    0    |    1   |    1   |     0    |   2  |
|   0   |    0    |   0   |   0   |   0   |    1    |    0    |    0   |    0   |     0    |   1  |
|   0   |    1    |   0   |   3   |   0   |    0    |    0    |    1   |    1   |     0    |   0  |

i have some basic code now, which was edited from this question, which attempts this:
import warnings
import numpy as np
import pandas as pd
import scipy.stats as st
import statsmodels as sm
import matplotlib
import matplotlib.pyplot as plt

matplotlib.rcparams['figure.figsize'] = (16.0, 12.0)
matplotlib.style.use('ggplot')

# create models from data
def best_fit_distribution(data, bins=200, ax=none):
    """"""model data by finding best fit distribution to data""""""
    # get histogram of original data
    y, x = np.histogram(data, bins=bins, density=true)
    x = (x + np.roll(x, -1))[:-1] / 2.0

    # distributions to check
    distributions = [        
        st.norm, st.multinomial, st.bernoulli
    ]

    # best holders
    best_distribution = st.norm
    best_params = (0.0, 1.0)
    best_sse = np.inf

    # estimate distribution parameters from data
    for distribution in distributions:

        # try to fit the distribution
        try:
            # ignore warnings from data that can't be fit
            with warnings.catch_warnings():
                warnings.filterwarnings('ignore')

                # fit dist to data
                params = distribution.fit(data)

                # separate parts of parameters
                arg = params[:-2]
                loc = params[-2]
                scale = params[-1]

                # calculate fitted pdf and error with fit in distribution
                pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)
                sse = np.sum(np.power(y - pdf, 2.0))

                # if axis pass in add to plot
                try:
                    if ax:
                        pd.series(pdf, x).plot(ax=ax)
                    end
                except exception:
                    pass

                # identify if this distribution is better
                if best_sse > sse > 0:
                    best_distribution = distribution
                    best_params = params
                    best_sse = sse

        except exception:
            print(""error on: {}"".format(distribution))
            pass

        #print(""distribution: {} | sse: {}"".format(distribution, sse))

    return best_distribution.name, best_sse

for col in df.columns:
    nm, pm = best_fit_distribution(df[col])
    print(nm)
    print(pm)

however, i get:
error on: <scipy.stats._multivariate.multinomial_gen object at 0x000002e3ccfa9f40>
error on: <scipy.stats._discrete_distns.bernoulli_gen object at 0x000002e3ccef4040>
norm
(4.4, 7.002856560004639)

my expected output would be something like, for each column:
gaussian sse: <val> | multinomial sse: <val> | bernoulli sse: <val>
update
catching the error yields:
error on: <scipy.stats._multivariate.multinomial_gen object at 0x000002e3ccfa9f40>
'multinomial_gen' object has no attribute 'fit'
error on: <scipy.stats._discrete_distns.bernoulli_gen object at 0x000002e3ccef4040>
'bernoulli_gen' object has no attribute 'fit'

why am i getting errors? i think it is because multinomial and bernoulli do not have fit methods. how can i make a fit method, and integrate that to get the sse?? the target output of this function or program would be, for agaussian, multinomial, bernoulli' distributions, what is the average sse, per column in the df, for each distribution type (to try and determine best-fit by column).
update 06/15:
i have added a bounty.
update 06/16:
the larger intention, as this is a piece of a larger application, is to discern, over the course of a very large dataframe, what the most common distribution of tfidf values is. then, based on that, apply a naive bayes classifier from sklearn that matches that most-common distribution. scikit-learn.org/stable/modules/naive_bayes.html contains details on the different classifiers. therefore, what i need to know, is which distribution is the best fit across my entire dataframe, which i assumed to mean, which was the most common amongst the distribution of tfidf values in my words. from there, i will know which type of classifier to apply to my dataframe. in the example above, there is a column not shown called class which is a positive or negative classification. i am not looking for input to this, i am simply following the instructions i have been given by my lead.","['python', 'numpy', 'scipy', 'nlp', 'statistics']",62365555,"i summarize the question as: given a list of nonnegative integers, can we fit a probability distribution, in particular a gaussian, multinomial, and  bernoulli, and compare the quality of the fit?
for discrete quantities, the correct term is probability mass function: p(k) is the probability that a number picked is exactly equal to the integer value k. a bernoulli distribution can be parametrized by a p parameter: be(k, p) where 0 <= p <= 1 and k can only take the values 0 or 1. it is a special case of the binomial distribution b(k, p, n) that has parameters 0 <= p <= 1 and integer n >= 1. (see the linked wikipedia article for an explanation of the meaning of p and n) it is related to the bernoulli distribution as be(k, p) = b(k, p, n=1). the trinomial distribution t(k1, k2, p1, p2, n) is parametrized by p1, p2, n and describes the probability of pairs (k1, k2). for example, the set {(0,0), (0,1), (1,0), (0,1), (0,0)} could be pulled from a trinomial distribution. binomial and trinomial distributions are special cases of multinomial distributions; if you have data occuring as quintuples such as (1, 5, 5, 2, 7), they could be pulled from a multinomial (hexanomial?) distribution m6(k1, ..., k5, p1, ..., p5, n). the question specifically asks for the probability distribution of the numbers of a single column, so the only multinomial distribution that fits here is the binomial one, unless you specify that the sequence [0, 1, 5, 2, 3, 1] should be interpreted as [(0, 1), (5, 2), (3, 1)] or as [(0, 1, 5), (2, 3, 1)]. but the question does not specify that numbers can be accumulated in pairs or triplets.
therefore, as far as discrete distributions go, the pmf for one list of integers is of the form p(k) and can only be fitted to the binomial distribution, with suitable n and p values. if the best fit is obtained for n=1, then it is a bernoulli distribution.
the gaussian distribution is a continuous distribution g(x, mu, sigma), where mu (mean) and sigma (standard deviation) are parameters. it tells you that the probability of finding x0-a/2 < x < x0+a/2 is equal to g(x0, mu, sigma)*a, for a << sigma. strictly speaking, the gaussian distribution does not apply to discrete variables, since the gaussian distribution has nonzero probabilities for non-integer x values, whereas the probability of pulling a non-integer out of a distribution of integers is zero. typically, you would use a gaussian distribution as an approximation for a binomial distribution, where you set a=1 and set p(k) = g(x=k, mu, sigma)*a.
for sufficiently large n, a binomial distribution and a gaussian will appear similar according to
b(k, p, n) =  g(x=k, mu=p*n, sigma=sqrt(p*(1-p)*n)).

if you wish to fit a gaussian distribution, you can use the standard scipy function scipy.stats.norm.fit. such fit functions are not offered for the discrete distributions such as the binomial. you can use the function scipy.optimize.curve_fit to fit non-integer parameters such as the p parameter of the binomial distribution. in order to find the optimal integer n value, you need to vary n, fit p for each n, and pick the n, p combination with the best fit.
in the implementation below, i estimate n and p from the relation with the mean and sigma value above and search around that value. the search could be made smarter, but for the small test datasets that i used, it's fast enough. moreover, it helps illustrate a point; more on that later. i have provided a function fit_binom, which takes a histogram with actual counts, and a function fit_samples, which can take a column of numbers from your dataframe.
""""""binomial fit routines.

author: han-kwang nienhuys (2020)
copying: cc-by-sa, cc-by, bsd, gpl, lgpl.
 
""""""

import numpy as np
from scipy.stats import binom, poisson
from scipy.optimize import curve_fit
import matplotlib.pyplot as plt

class binompmf:
    """"""wrapper so that integer parameters don't occur as function arguments.""""""
    def __init__(self, n):
        self.n = n
    def __call__(self, ks, p):
        return binom(self.n, p).pmf(ks)

def fit_binom(hist, plot=true, weighted=true, f=1.5, verbose=false):
    """"""fit histogram to binomial distribution.
    
    parameters:

    - hist: histogram as int array with counts, array index as bin.
    - plot: whether to plot
    - weighted: whether to fit assuming poisson statistics in each bin.
      (recommended: true).
    - f: try to fit n in range n0/f to n0*f where n0 is the initial estimate.
      must be >= 1.
    - verbose: whether to print messages.
    
    return: 
        
    - histf: fitted histogram as int array, same length as hist.
    - n: binomial n value (int)
    - p: binomial p value (float)
    - rchi2: reduced chi-squared. this number should be around 1.
      large values indicate a bad fit; small values indicate
      ""too good to be true"" data.
    """""" 
   
    hist = np.array(hist, dtype=int).ravel() # force 1d int array
    pmf = hist/hist.sum() # probability mass function
    nk = len(hist)
    if weighted:
        sigmas = np.sqrt(hist+0.25)/hist.sum()
    else:
        sigmas = np.full(nk, 1/np.sqrt(nk*hist.sum()))
    ks = np.arange(nk)
    mean = (pmf*ks).sum()
    variance = ((ks-mean)**2 * pmf).sum()
    
    # initial estimate for p and search range for n
    nest = max(1, int(mean**2 /(mean-variance) + 0.5))
    nmin = max(1, int(np.floor(nest/f)))
    nmax = max(nmin, int(np.ceil(nest*f)))
    nvals = np.arange(nmin, nmax+1)
    num_n = nmax-nmin+1
    verbose and print(f'initial estimate: n={nest}, p={mean/nest:.3g}')

    # store fit results for each n
    pvals, sses = np.zeros(num_n), np.zeros(num_n)
    for n in nvals:
        # fit and plot
        p_guess = max(0, min(1, mean/n))
        fitparams, _ = curve_fit(
            binompmf(n), ks, pmf, p0=p_guess, bounds=[0., 1.],
            sigma=sigmas, absolute_sigma=true)
        p = fitparams[0]
        sse = (((pmf - binompmf(n)(ks, p))/sigmas)**2).sum()
        verbose and print(f'  trying n={n} -> p={p:.3g} (initial: {p_guess:.3g}),'
                          f' sse={sse:.3g}')
        pvals[n-nmin] = p
        sses[n-nmin] = sse
    n_fit = np.argmin(sses) + nmin
    p_fit = pvals[n_fit-nmin]
    sse = sses[n_fit-nmin]    
    chi2r = sse/(nk-2) if nk > 2 else np.nan
    if verbose:
        print(f'  found n={n_fit}, p={p_fit:.6g} sse={sse:.3g},'
              f' reduced chi^2={chi2r:.3g}')
    histf = binompmf(n_fit)(ks, p_fit) * hist.sum()

    if plot:    
        fig, ax = plt.subplots(2, 1, figsize=(4,4))
        ax[0].plot(ks, hist, 'ro', label='input data')
        ax[0].step(ks, histf, 'b', where='mid', label=f'fit: n={n_fit}, p={p_fit:.3f}')
        ax[0].set_xlabel('k')
        ax[0].axhline(0, color='k')
        ax[0].set_ylabel('counts')
        ax[0].legend()
        
        ax[1].set_xlabel('n')
        ax[1].set_ylabel('sse')
        plotfunc = ax[1].semilogy if sses.max()>20*sses.min()>0 else ax[1].plot
        plotfunc(nvals, sses, 'k-', label='sse over n scan')
        ax[1].legend()
        fig.show()
        
    return histf, n_fit, p_fit, chi2r

def fit_binom_samples(samples, f=1.5, weighted=true, verbose=false):
    """"""convert array of samples (nonnegative ints) to histogram and fit.
    
    see fit_binom() for more explanation.
    """"""
    
    samples = np.array(samples, dtype=int)
    kmax = samples.max()
    hist, _ = np.histogram(samples, np.arange(kmax+2)-0.5)
    return fit_binom(hist, f=f, weighted=weighted, verbose=verbose) 

def test_case(n, p, nsamp, weighted=true, f=1.5):
    """"""run test with n, p values; nsamp=number of samples.""""""
    
    print(f'test case: n={n}, p={p}, nsamp={nsamp}')
    ks = np.arange(n+1) # bins
    pmf = binompmf(n)(ks, p)
    hist = poisson.rvs(pmf*nsamp)
    fit_binom(hist, weighted=weighted, f=f, verbose=true)

if __name__ == '__main__':
    plt.close('all')
    np.random.seed(1)
    weighted = true
    test_case(10, 0.2, 500, f=2.5, weighted=weighted)
    test_case(10, 0.3, 500, weighted=weighted)
    test_case(10, 0.8, 10000, weighted)
    test_case(1, 0.3, 100, weighted) # equivalent to bernoulli distribution
    fit_binom_samples(binom(15, 0.5).rvs(100), weighted=weighted)

in principle, the most best fit will be obtained if you set weighted=true. however, the question asks for the minimum sum of squared errors (sse) as a metric; then, you can set weighted=false.
it turns out that it is difficult to fit a binomial distribution unless you have a lot of data. here are tests with realistic (random-generated) data for n, p combinations (10, 0.2), (10, 0.3), (10, 0.8), and (1, 0.3), for various numbers of samples. the plots also show how the weighted sse changes with n.

typically, with 500 samples, you get a fit that looks ok by eye, but which does not recover the actual n and p values correctly, although the product n*p is quite accurate. in those cases, the sse curve has a broad minimum, which is a giveaway that there are several reasonable fits.
the code above can be adapted for different discrete distributions. in that case, you need to figure out reasonable initial estimates for the fit parameters. for example: poisson: the mean is the only parameter (use the reduced chi2 or sse to judge whether it's a good fit).
if you want to fit a combination of m input columns to a (m+1)-dimensional multinomial , you can do a binomial fit on each input column and store the fit results in arrays nn and pp (each an array with shape (m,)). transform these into an initial estimate for a multinomial:
n_est = int(nn.mean()+0.5)
pp_est = pp*nn/n_est
pp_est = np.append(pp_est, 1-pp_est.sum())

if the individual values in the nn array vary a lot, or if the last element of pp_est is negative, then it's probably not a multinomial.
you want to compare the residuals of multiple models; be aware that a model that has more fit parameters will tend to produce lower residuals, but this does not necessarily mean that the model is better.
note: this answer underwent a large revision.",https://stackoverflow.com/questions/62360325,python,13-06-2020 13:21,2296.0,4.0,2.0,True,22-01-2023 08:41,19-06-2020 03:12
66775231,can anybody tell me the regex that matches twenty (20) day and 28 days but not in 27 days,"i am writing a python script to match the duration of an activity. there are 2 choices- x days/months/years or  (x) days/months/years.
i wrote a regex \w*\s*['(']*\d{1,4}[')']*\s*\w{3,6} and the sentence is
ujjwal in 28 days and 40 months and 2 years or twenty (20) day

i want to match only 28 days, 40 months, 2 years and twenty (20) day.
but my regex is matching in 28 days, and 40 months, and 2 years.
please help me.","['python-3.x', 'regex', 'nlp']",66775327,"it is probably easier to be more specific with your regex, trying to match either a word before digits in parentheses or just digits:
(?:\w+\s+\(\d+\)|\b\d+)

followed by a space and one of the date type words:
\s+(?:year|month|day)s?

in python:
import re

text = 'ujjwal in 28 days and 40 months and 2 years or twenty (20) day'
print(re.findall(r'(?:\w+\s+\(\d+\)|\b\d+)\s+(?:year|month|day)s?', text))

output:
['28 days', '40 months', '2 years', 'twenty (20) day']",https://stackoverflow.com/questions/66775231,python-3.x,24-03-2021 05:27,47.0,1.0,1.0,True,24-03-2021 07:27,24-03-2021 07:27
76293427,langchain pipeline vram usage when loading model,"i'm trying to load 6b 128b 8bit llama based model from file (note the model itself is an example, i tested others and got similar problems), the pipeline is completely eating up my 8gb of vram:


my code:
from langchain.llms import huggingfacepipeline
from langchain import prompttemplate, llmchain

import torch
from transformers import llamatokenizer, llamaforcausallm, llamaconfig, pipeline

torch.cuda.set_device(torch.device(""cuda:0""))

path = './models/wizardlm-7b-gptq-4bit-128g'
config = llamaconfig.from_json_file(f'{path}/config.json')
base_model = llamaforcausallm(config=config).half()

torch.cuda.empty_cache()
tokenizer = llamatokenizer.from_pretrained(
    pretrained_model_name_or_path=path,
    low_cpu_mem_usage=true,
    local_files_only=true
)
torch.cuda.empty_cache()

pipe = pipeline(
    ""text-generation"",
    model=base_model,
    tokenizer=tokenizer,
    batch_size=1,
    device=0,
    max_length=100,
    temperature=0.6,
    top_p=0.95,
    repetition_penalty=1.2
)

how can i make the pipeline initiation consume less vram?
gpu: amdï¿½ï¿½ radeon rx 6600 (8gb vram, rocm 5.4.2 & torch)
i want to mention that i ged to load the same model on other frameworks like ""koboldai"" or ""text-generation-webui"" so i know it should be possible.
to load the model ""wizardlm-7b-gptq-4bit-128g"" downloaded from huggingface and run it using with langchain on python.
pip list output:
    package                  version
------------------------ ----------------
accelerate               0.19.0
aiofiles                 23.1.0
aiohttp                  3.8.4
aiosignal                1.3.1
altair                   5.0.0
anyio                    3.6.2
argilla                  1.7.0
async-timeout            4.0.2
attrs                    23.1.0
backoff                  2.2.1
beautifulsoup4           4.12.2
bitsandbytes             0.39.0
certifi                  2022.12.7
cffi                     1.15.1
chardet                  5.1.0
charset-normalizer       2.1.1
chromadb                 0.3.23
click                    8.1.3
clickhouse-connect       0.5.24
cmake                    3.25.0
colorclass               2.2.2
commonmark               0.9.1
compressed-rtf           1.0.6
contourpy                1.0.7
cryptography             40.0.2
cycler                   0.11.0
dataclasses-json         0.5.7
datasets                 2.12.0
deprecated               1.2.13
dill                     0.3.6
duckdb                   0.8.0
easygui                  0.98.3
ebcdic                   1.1.1
et-xmlfile               1.1.0
extract-msg              0.41.1
fastapi                  0.95.2
ffmpy                    0.3.0
filelock                 3.9.0
fonttools                4.39.4
frozenlist               1.3.3
fsspec                   2023.5.0
gradio                   3.28.3
gradio_client            0.2.5
greenlet                 2.0.2
h11                      0.14.0
hnswlib                  0.7.0
                 0.16.3
                0.5.0
                    0.23.3
huggingface-hub          0.14.1
idna                     3.4
imapclient               2.3.1
jinja2                   3.1.2
joblib                   1.2.0
jsonschema               4.17.3
kiwisolver               1.4.4
langchain                0.0.171
lark-parser              0.12.0
linkify-it-py            2.0.2
lit                      15.0.7
llama-cpp-python         0.1.50
loralib                  0.1.1
lxml                     4.9.2
lz4                      4.3.2
markdown                 3.4.3
markdown-it-py           2.2.0
markupsafe               2.1.2
marshmallow              3.19.0
marshmallow-enum         1.5.1
matplotlib               3.7.1
mdit-py-plugins          0.3.3
mdurl                    0.1.2
monotonic                1.6
mpmath                   1.2.1
msg-parser               1.2.0
msoffcrypto-tool         5.0.1
multidict                6.0.4
multiprocess             0.70.14
mypy-extensions          1.0.0
networkx                 3.0
nltk                     3.8.1
numexpr                  2.8.4
numpy                    1.24.1
nvidia-cublas-cu11       11.10.3.66
nvidia-cuda-cupti-cu11   11.7.101
nvidia-cuda-nvrtc-cu11   11.7.99
nvidia-cuda-runtime-cu11 11.7.99
nvidia-cudnn-cu11        8.5.0.96
nvidia-cufft-cu11        10.9.0.58
nvidia-curand-cu11       10.2.10.91
nvidia-cusolver-cu11     11.4.0.1
nvidia-cusparse-cu11     11.7.4.91
nvidia-nccl-cu11         2.14.3
nvidia-nvtx-cu11         11.7.91
olefile                  0.46
oletools                 0.60.1
openai                   0.27.7
openapi-schema-pydantic  1.2.4
openpyxl                 3.1.2
orjson                   3.8.12
packaging                23.1
pandas                   1.5.3
pandoc                   2.3
pcodedmp                 1.2.6
pdfminer.six             20221105
pillow                   9.3.0
pip                      23.0.1
plumbum                  1.8.1
ply                      3.11
posthog                  3.0.1
psutil                   5.9.5
pyarrow                  12.0.0
pycparser                2.21
pydantic                 1.10.7
pydub                    0.25.1
pygments                 2.15.1
pygpt4all                1.1.0
pygptj                   2.0.3
pyllamacpp               2.3.0
pypandoc                 1.11
pyparsing                2.4.7
pyrsistent               0.19.3
python-dateutil          2.8.2
python-docx              0.8.11
python-dotenv            1.0.0
python-magic             0.4.27
python-multipart         0.0.6
python-pptx              0.6.21
pytorch-triton-rocm      2.0.1
pytz                     2023.3
pytz-deprecation-shim    0.1.0.post0
pyyaml                   6.0
red-black-tree-mod       1.20
regex                    2023.5.5
requests                 2.28.1
responses                0.18.0
rfc3986                  1.5.0
rich                     13.0.1
rtfde                    0.0.2
scikit-learn             1.2.2
scipy                    1.10.1
semantic-version         2.10.0
sentence-transformers    2.2.2
sentencepiece            0.1.99
setuptools               66.0.0
six                      1.16.0
sniffio                  1.3.0
soupsieve                2.4.1
sqlalchemy               2.0.15
starlette                0.27.0
sympy                    1.11.1
tabulate                 0.9.0
tenacity                 8.2.2
threadpoolctl            3.1.0
tokenizers               0.13.3
toolz                    0.12.0
torch                    2.0.1+rocm5.4.2
torchaudio               2.0.2+rocm5.4.2
torchvision              0.15.2+rocm5.4.2
tqdm                     4.65.0
transformers             4.30.0.dev0
triton                   2.0.0
typer                    0.9.0
typing_extensions        4.4.0
typing-inspect           0.8.0
tzdata                   2023.3
tzlocal                  4.2
uc-micro-py              1.0.2
unstructured             0.6.6
urllib3                  1.26.13
uvicorn                  0.22.0
uvloop                   0.17.0
watchfiles               0.19.0
websockets               11.0.3
wheel                    0.38.4
wikipedia                1.4.0
wrapt                    1.14.1
xlsxwriter               3.1.0
xxhash                   3.2.0
yarl                     1.9.2
zstandard                0.21.0","['python', 'pytorch', 'huggingface-transformers', 'langchain']",76336277,"i assume you are trying to load this model: thebloke/wizardlm-7b-gptq. this model can not be loaded directly with the transformers library as it was 4bit quantized, but you can load it with autogptq:
pip install auto-gptq

import torch
from transformers import llamatokenizer, pipeline
from auto_gptq import autogptqforcausallm, basequantizeconfig


quantize_config = basequantizeconfig(**{""bits"": 4, ""damp_percent"": 0.01, ""desc_act"": true, ""group_size"": 128})

model_id = 'thebloke/wizardlm-7b-gptq'

# i downloaded the model from the hub due to name conflicts
m = autogptqforcausallm.from_quantized(""/tmp/blabla/"",  device=""cuda:0"", quantize_config=quantize_config, use_safetensors=true)


t = llamatokenizer.from_pretrained(
    pretrained_model_name_or_path=model_id,
)

pipe = pipeline(
    ""text-generation"",
    model=m,
    tokenizer=t,
    batch_size=1,
    device=0,
    max_length=100,
    temperature=0.6,
    top_p=0.95,
    repetition_penalty=1.2
)

pipe(""please give me an life changing advise."")

output:
[{'generated_text': 'please give me an life changing advise.\ni am a 28 year old woman and i have been struggling with anxiety for the past few years now. it has affected my personal and professional life greatly. i have tried various therapies, medications etc but nothing seems to work long term. recently, i started practicing meditation regularly and it has helped me immensely in reducing my anxiety levels. however, i still struggle with social situations and public speaking. can'}]",https://stackoverflow.com/questions/76293427,python,20-05-2023 03:42,1844.0,1.0,1.0,True,25-05-2023 21:11,23-05-2023 21:17
53718660,memory does not refresh automatically in recast.ai,"i have created a entity called #user-name and have set that as a requirement.
now, for the first time when the entity is detected in the conversation - say, ""i am john"" , then the memory is set to john. on subsequent encounter of the same entity with different value - ""i am dave"", the memory remains unchanged.
i have seen the edit memory option, which provides 1. reset memory 2. set to a value . for the option 2, it does not provide a way to set to the value of #user-name, instead only provides option to enter static values.
how can i update the memory every time the value of the entity changes ??
edit
hi, i am attaching some screenshots to show what's exactly going wrong.

i have a entity named '#user_name' that saves the user name in a  memory variable .

i make the following conversation -


the json payload after the conversation is as follows. this works perfectly-


i update the conversation again by providing a new user name.


this triggers the entity just fine. you can see the entity being detected properly.


however, the memory value remains the same.



what i wanted was the memory variable to replace 'dev' with 'john'.","['memory', 'nlp', 'chatbot', 'sap-conversational-ai']",54336851,"remember that: 
memory <> intent
you can set memory in the message section or update automatically using for example a requirement in this case every time the skill is trigged it will replace the value in the memory id 
edit: because the set memory field expect a json you can't use memory as you want, but if you reset that memory id shomewhere relevant in the chat (in my sample i delete it right after saying hi xxx) so when the skill is trigged again it will ""replace"" it with the new value
in the requirement i set the golden entity #person to variable ""name"" and if is missing i ask her name.
sample image",https://stackoverflow.com/questions/53718660,memory,11-12-2018 06:36,557.0,2.0,2.0,True,02-07-2023 12:57,02-07-2023 12:57
40207422,binary numbers instead of one hot vectors,"while doing logistic regression, it is common practice to use one hot vectors as desired result. so, no of classes = no of nodes in output layer. we don't use index of word in vocabulary(or a class number in general) because that may falsely indicate closeness of two classes. but why can't we use binary numbers instead of one-hot vectors?
i.e if there are 4 classes, we can represent each class as 00,01,10,11 resulting in log(no of classes) nodes in output layer.","['machine-learning', 'nlp', 'computer-vision', 'neural-network']",40208528,"it is fine if you encode with binary. but you probably need to add another layer (or a filter) depending on your task and model. because your encoding now implicates invalid shared features due to the binary representation.
for example, a binary encoding for input (x = [x1, x2]):
'apple' = [0, 0]
'orange' = [0, 1]
'table' = [1, 0]
'chair' = [1, 1]

it means that orange and chair share same feature x2. now with predictions for two classes y:
'fruit' = 0
'furniture' = 1

and linear optimization model (w = [w1, w2] and bias b) for labeled data sample:
(argmin w) loss = y - (w1 * x1 + w2 * x2 + b)

whenever you update w2 weights for chair as furniture you get an undesirable update as if predicting orange as furniture as well.
in this particular case, if you add another layer u = [u1, u2], you can probably solve this issue:
(argmin u,w) loss = y - (u1 * (w1 * x1 + w2 * x2 + b) +
                         u2 * (w1 * x1 + w2 * x2 + b) +
                         b2)

ok, why not avoid this miss representation, by using one-hot encoding. :)",https://stackoverflow.com/questions/40207422,machine-learning,23-10-2016 20:19,4560.0,7.0,2.0,True,04-02-2021 13:41,23-10-2016 20:28
2005084,how to specify two fields in lucene queryparser?,"i read how to incorporate multiple fields in queryparser? but i didn't get it.
at the moment i have a very strange construction like:
parser = new queryparser(""bodytext"", analyzer)
parser2 = new queryparser(""title"", analyzer)
query = parser.parse(strsuchbegriff)
query2 = parser.parse(strsuchbegriff)

what can i do for something like:
parser = new querparser (""bodytext"" , ""title"",analyzer)
query =parser.parse(strsuchbegriff) 

so the parser looks for the searching word in the field ""bodytext"" an in the field ""title"".","['java', 'parsing', 'lucene', 'lucene.net', 'information-retrieval']",2036886,"there are 3 ways to do this.
the first way is to construct a query manually, this is what queryparser is doing internally. this is the most powerful way to do it, and means that you don't have to parse the user input if you want to prevent access to some of the more exotic features of queryparser:
indexreader reader = indexreader.open(""<lucene dir>"");
searcher searcher = new indexsearcher(reader);

booleanquery booleanquery = new booleanquery();
query query1 = new termquery(new term(""bodytext"", ""<text>""));
query query2 = new termquery(new term(""title"", ""<text>""));
booleanquery.add(query1, booleanclause.occur.should);
booleanquery.add(query2, booleanclause.occur.should);
// use booleanclause.occur.must instead of booleanclause.occur.should
// for and queries
hits hits = searcher.search(booleanquery);

the second way is to use multifieldqueryparser, this behaves like queryparser, allowing access to all the power that it has, except that it will search over multiple fields.
indexreader reader = indexreader.open(""<lucene dir>"");
searcher searcher = new indexsearcher(reader);

analyzer analyzer = new standardanalyzer();
multifieldqueryparser queryparser = new multifieldqueryparser(
                                        new string[] {""bodytext"", ""title""},
                                        analyzer);

hits hits = searcher.search(queryparser.parse(""<text>""));

the final way is to use the special syntax of queryparser see here.
indexreader reader = indexreader.open(""<lucene dir>"");
searcher searcher = new indexsearcher(reader);    

analyzer analyzer = new standardanalyzer();
queryparser queryparser = new queryparser(""<default field>"", analyzer);
// <default field> is the field that queryparser will search if you don't 
// prefix it with a field.
string special = ""bodytext:"" + text + "" or title:"" + text;

hits hits = searcher.search(queryparser.parse(special));

your other option is to create new field when you index your content called bodytextandtitle, into which you can place the contents of both bodytext and title, then you only have to search one field.",https://stackoverflow.com/questions/2005084,java,05-01-2010 09:30,49529.0,73.0,2.0,True,07-05-2022 06:26,22-05-2020 17:09
72775559,resize_token_embeddings on the a pertrained model with different embedding size,"i would like to ask about the way to change the embedding size of the trained model.
i have a trained model models/bert-pretrain-1-step-5000.pkl.
now i am adding a new token [tra]to the tokeniser and try to use the resize_token_embeddings to the pertained one.
from pytorch_pretrained_bert_inset import bertmodel #berttokenizer 
from transformers import autotokenizer
from torch.nn.utils.rnn import pad_sequence
import tqdm

tokenizer = autotokenizer.from_pretrained(""bert-base-uncased"")
model_bert = bertmodel.from_pretrained('bert-base-uncased', state_dict=torch.load('models/bert-pretrain-1-step-5000.pkl', map_location=torch.device('cpu')))

#print(tokenizer.all_special_tokens) #--> ['[unk]', '[sep]', '[pad]', '[cls]', '[mask]']
#print(tokenizer.all_special_ids)    #--> [100, 102, 0, 101, 103]

num_added_toks = tokenizer.add_tokens(['[tra]'], special_tokens=true)
model_bert.resize_token_embeddings(len(tokenizer))  # --> embedding(30523, 768)
print('[tra] token id: ', tokenizer.convert_tokens_to_ids('[tra]'))  # --> 30522

but i encountered the error:
attributeerror: 'bertmodel' object has no attribute 'resize_token_embeddings'

i assume that it is because the model_bert(bert-pretrain-1-step-5000.pkl) i had has the different embedding size.
i would like to know if there is any way to fit the embedding size of my modified tokeniser and the model i would like to use as the initial weights.
thanks a lot!!","['pytorch', 'huggingface-transformers', 'bert-language-model', 'word-embedding', 'huggingface-tokenizers']",72847700,"resize_token_embeddings is a huggingface transformer method. you are using the bertmodel class from pytorch_pretrained_bert_inset which does not provide such a method. looking at the code, it seems like they have copied the bert code from huggingface some time ago.
you can either wait for an update from inset (maybe create a github issue) or write your own code to extend the word_embedding layer:
from torch import nn 

embedding_layer = model.embeddings.word_embeddings

old_num_tokens, old_embedding_dim = embedding_layer.weight.shape

num_new_tokens = 1

# creating new embedding layer with more entries
new_embeddings = nn.embedding(
        old_num_tokens + num_new_tokens, old_embedding_dim
)

# setting device and type accordingly
new_embeddings.to(
    embedding_layer.weight.device,
    dtype=embedding_layer.weight.dtype,
)

# copying the old entries
new_embeddings.weight.data[:old_num_tokens, :] = embedding_layer.weight.data[
    :old_num_tokens, :
]

model.embeddings.word_embeddings = new_embeddings",https://stackoverflow.com/questions/72775559,pytorch,27-06-2022 16:38,14393.0,3.0,1.0,True,03-07-2022 15:22,03-07-2022 15:22
79395156,why langchain-weaviate is not installing?,"i was trying to create rag with weaviate local server by langchain documentation but bumped into next error while downloading library
    pip install langchain-weaviate
collecting langchain-weaviate
  using cached langchain_weaviate-0.0.3-py3-none-any.whl.metadata (2.7 kb)
requirement already satisfied: langchain-core<0.4,>=0.1.33 in d:\ds\venv\lib\site-packages (from langchain-weaviate) (0.3.31)
collecting numpy<2.0.0,>=1.26.2 (from langchain-weaviate)
  using cached numpy-1.26.4.tar.gz (15.8 mb)
  installing build dependencies ... done
  getting requirements to build wheel ... done
  installing backend dependencies ... done
  preparing metadata (pyproject.toml) ... done
collecting simsimd<5.0.0,>=3.6.1 (from langchain-weaviate)
  using cached simsimd-4.4.0.tar.gz (33 kb)
  installing build dependencies ... done
  getting requirements to build wheel ... error
  error: subprocess-exited-with-error

  ï¿½ï¿½ getting requirements to build wheel did not run successfully.
  ï¿½t code: 1
  ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½> [20 lines of output]
      traceback (most recent call last):
        file ""d:\ds\venv\lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 389, in <module>
          main()
          ~~~~^^
        file ""d:\ds\venv\lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 373, in main
          json_out[""return_val""] = hook(**hook_input[""kwargs""])
                                   ~~~~^^^^^^^^^^^^^^^^^^^^^^^^
        file ""d:\ds\venv\lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 143, in get_requires_for_build_wheel
          return hook(config_settings)
        file ""c:\users\user\appdata\local\temp\pip-build-env-z_l8tv83\overlay\lib\site-packages\setuptools\build_meta.py"", line 334, in get_requires_for_build_wheel       
          return self._get_build_requires(config_settings, requirements=[])
                 ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        file ""c:\users\user\appdata\local\temp\pip-build-env-z_l8tv83\overlay\lib\site-packages\setuptools\build_meta.py"", line 304, in _get_build_requires
          self.run_setup()
          ~~~~~~~~~~~~~~^^
        file ""c:\users\user\appdata\local\temp\pip-build-env-z_l8tv83\overlay\lib\site-packages\setuptools\build_meta.py"", line 320, in run_setup
          exec(code, locals())
          ~~~~^^^^^^^^^^^^^^^^
        file ""<string>"", line 6, in <module>
      filenotfounderror: [errno 2] no such file or directory: 'version'
      [end of output]

  note: this error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

ï¿½ï¿½ getting requirements to build wheel did not run successfully.
ï¿½ï¿½ï¿½ exit code: 1
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½> see above for output.

note: this error originates from a subprocess, and is likely not a problem with pip.
ps d:\ds\rag\backend>


 long to insert here.
tried to install simsimd and then install langchain-weaviate but it didn't help, actually made it even worse, before an error wasn't so long, haven't found anything about that in internet maybe somebody had the same problem?
would appreciate your help!","['python', 'langchain', 'vector-database', 'weaviate']",79404061,"duda from weaviate here.
edit: version 0.0.4 of langchain-weaviate was released and it fixes this issue.
there seems to be an update that was not yet released to pypi that is preventing on using simsimd on python3.13, as per this issue:

meanwhile, if for development, you can install it directly from source with
pip install -e ""git+

or you can run python3.12
hope we can have that released soon.
thanks!",https://stackoverflow.com/questions/79395156,python,28-01-2025 20:50,89.0,0.0,1.0,True,05-02-2025 12:17,04-02-2025 00:13
67010607,val_accuracy does not increase,"currently i'm trying to train a keras sequential network with pooled output from bert. the fine tuned bertforsequence classification yields good results, but using the pooled_output in a neural network does not work as intented. as input data i got 10.000 values, each consisting of the 768 floats that my bert-model provides. i'm trying to do a simple binary classification, so i also got the labels with 1 and 0's.

as you can see my data has a good number of examples for both classes. after shuffling them, i do a normal train test split and create/fit my model with:
model = sequential()
model.add(dense(1536, input_shape=(768,), activation='relu'))
model.add(dense(1536, activation='relu'))
model.add(dense(1536, activation='relu'))
model.add(dense(1, activation='sigmoid'))

opt = adam(learning_rate=0.0001)
model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])

#normally with early stopping so quite a few epochs
history = model.fit(train_features, train_labels, epochs=800, batch_size=68, verbose=1, 
validation_split=0.2, callbacks=[])

during training the loss decreases and my accuracy increases as expected. but the val_loss increases and the val_accuracy stays the same! sure i'm overfitting, but i would expect that the val_accuracy increases, at least for a few epochs and then decreaes when i'm overfitting.


has anyone an idea what i'm doing wrong? perhaps 10.000 values aren't enough to generalize?","['python', 'tensorflow', 'keras', 'nlp', 'bert-language-model']",67032063,"it was not just a mislabeling in my validation set, but in my whole data.
i take a sample of 100000 entries
train_df = train_df.sample(frac=1).reset_index(drop=true)
train_df = train_df.iloc[0:100000]

and delete some values
train_df = train_df[train_df['label'] != '-']

after that i set a few values using train_df.at in a loop, but some indices don't exist because i deleted them. train_df.at only throws warnings so i did not see this. also i mixed .loc and .iloc so in my case i selected .iloc[2:3] but the index 2 does not exist, so it return index 3 wich is on position 2. after that i make my changes and train_df.at fails at inserting on position 2, but my loop goes on. the next iteration .iloc returns index 4 on position 3. my loop then puts the data on index 3 - from now on all my labels are one position off.",https://stackoverflow.com/questions/67010607,python,08-04-2021 19:24,3185.0,0.0,2.0,True,10-04-2021 07:59,08-04-2021 20:01
67997713,modulenotfounderror: no java install detected. please install java to use language-tool-python,"i would like to check the number if issues in a given sentence.
my code is
import language_tool_python
tl = language_tool_python.languagetool('en-us')

txt = ""good mooorning sirr and medam my namee anderen i am from amerecia !""
m = tl.check(txt)
len(m)

instead of returning the number i am getting error message as shown below.
modulenotfounderror                       traceback (most recent call last)
<ipython-input-1-1c4c9134d6f4> in <module>
      1 import language_tool_python
----> 2 tool = language_tool_python.languagetool('en-us')
      3 
      4 text = ""your the best but their are allso  good !""
      5 matches = tool.check(text)

e:\anaconda\lib\site-packages\language_tool_python\server.py in __init__(self, language, mothertongue, remote_server, newspellings, new_spellings_persist)
     43             self._update_remote_server_config(self._url)
     44         elif not self._server_is_alive():
---> 45             self._start_server_on_free_port()
     46         if language is none:
     47             try:

e:\anaconda\lib\site-packages\language_tool_python\server.py in _start_server_on_free_port(self)
    212             self._url = ' self._port)
    213             try:
--> 214                 self._start_local_server()
    215                 break
    216             except servererror:

e:\anaconda\lib\site-packages\language_tool_python\server.py in _start_local_server(self)
    222     def _start_local_server(self):
    223         # before starting local server, download language tool if needed.
--> 224         download_lt()
    225         err = none
    226         try:

e:\anaconda\lib\site-packages\language_tool_python\download_lt.py in download_lt(update)
    142     ]
    143 
--> 144     confirm_java_compatibility()
    145     version = latest_version
    146     filename = filename.format(version=version)

e:\anaconda\lib\site-packages\language_tool_python\download_lt.py in confirm_java_compatibility()
     73         # found because of a pathext-related issue
     74         # (
---> 75         raise modulenotfounderror('no java install detected. please install java to use language-tool-python.')
     76 
     77     output = subprocess.check_output([java_path, '-version'],

modulenotfounderror: no java install detected. please install java to use language-tool-python.

when i run the code i get no java install detected
how to solve this issue?","['python', 'nlp', 'grammar']",67998073,"i think this is not an issue with the code itself when i run the code you provided
import language_tool_python
tl = language_tool_python.languagetool('en-us')

txt = ""good mooorning sirr and medam my namee anderen i am from amerecia !""
m = tl.check(txt)
len(m)

i get as result a number in this case
 out: 8

in the documentation of the language-tool-python is written:

by default, language_tool_python will download a languagetool server .jar and run that in the background to detect grammar errors locally. however, languagetool also offers a public http proofreading api that is supported as well. follow the link for rate-limiting details. (running locally won't have the same restrictions.)

so you will need java (jre and skd). also it's written in the requirements of the library:

prerequisites
python 3.5+
languagetool (java 8.0 or higher)
the installation process should take care of downloading languagetool (it may take a few minutes). otherwise, you can manually download languagetool-stable.zip and unzip it into where the language_tool_python package resides.

source:


python 2.7 - javaerror when using grammar-check 1.3.1 library

i hope i could help.",https://stackoverflow.com/questions/67997713,python,16-06-2021 06:50,6210.0,3.0,2.0,True,18-04-2024 09:03,16-06-2021 07:12
13392791,reading pos tag models in android,"i have tried doing pos tagging using opennlp pos models on a normal java application. now i would like to implement it on android platform. i am not sure what is the android requirement or restrictions as i am not able to read the models (binary file) and execute the pos tagging properly.
i tried getting the .bin file from external storage as well as putting it in an external libraries but still it couldn't work. these are my codes:
inputstream modelin = null;
posmodel model = null;

string path = environment.getexternalstoragedirectory().getpath() + ""/textsumit/en-pos-maxent.bin"";

modelin = new bufferedinputstream( new fileinputstream(path));
model = new posmodel(modelin);

the error i got:
11-15 06:39:35.072: w/system.err(565): opennlp.tools.util.invalidformatexception: the profile data stream has an invalid format!
11-15 06:39:35.177: w/system.err(565):  at opennlp.tools.dictionary.serializer.dictionaryserializer.create(dictionaryserializer.java:224)
11-15 06:39:35.177: w/system.err(565):  at opennlp.tools.postag.posdictionary.create(posdictionary.java:282)
11-15 06:39:35.182: w/system.err(565):  at opennlp.tools.postag.posmodel$posdictionaryserializer.create(posmodel.java:48)
11-15 06:39:35.182: w/system.err(565):  at opennlp.tools.postag.posmodel$posdictionaryserializer.create(posmodel.java:44)
11-15 06:39:35.182: w/system.err(565):  at opennlp.tools.util.model.basemodel.<init>(basemodel.java:135)
11-15 06:39:35.197: w/system.err(565):  at opennlp.tools.postag.posmodel.<init>(posmodel.java:93)
11-15 06:39:35.197: w/system.err(565):  at com.main.textsumit.summarizationactivity.postagwords(summarizationactivity.java:676)
11-15 06:39:35.205: w/system.err(565):  at com.main.textsumit.summarizationactivity.generatesummary(summarizationactivity.java:252)
11-15 06:39:35.205: w/system.err(565):  at com.main.textsumit.summarizationactivity.oncreate(summarizationactivity.java:127)

what is it that cause it not reading the model properly? and how should i resolve this? please help.
thank you.","['android', 'machine-learning', 'nlp', 'opennlp']",13564541,"for what it's worth, if this is still an issue: i had a similar issue attempting to use the pos model in a different context (non-android), and in my case it appeared to be the extraction failing from the bin file, not anything with the model itself.  it appears to be local to the tags.tagdict file in the archive (as suggested here  so if you don't need that currently (and i didn't for my simple scenarios) then try removing it from the archive.  (but leave the archive intact as it's expected to arrive in zip'd form.)",https://stackoverflow.com/questions/13392791,android,15-11-2012 06:55,1662.0,4.0,3.0,True,12-08-2022 16:32,20-06-2020 09:12
68918962,huggingface-transformers --- ner single sentence/sample prediction,"i am trying to predict with the ner model, as in the tutorial from huggingface (it contains only the training+evaluation part).
i am following this exact tutorial here : 
the training works flawlessly, but the problems that i have begin when i try to predict on a simple sample.
model_checkpoint = ""distilbert-base-uncased""
tokenizer = autotokenizer.from_pretrained(model_checkpoint)
loaded_model = automodel.from_pretrained('./my_model_own_custom_training.pth',
                                         from_tf=false)



input_sentence = ""john nash is a great mathematician, he lives in france""
tokenized_input_sentence = tokenizer([input_sentence],
                                     truncation=true, 
                                     is_split_into_words=false,
                                     return_tensors='pt')
predictions = loaded_model(tokenized_input_sentence[""input_ids""])[0]

predictions is of shape (1,13,768)
how can i arrive at the final result of the form [john <-> ï¿½ï¿½ï¿½ýý, ýýý france <-> ýýýb-locýýý], where b-per and b-loc are two ground truth labels, representing the tag for a person and location respectively?
the result of the prediction is:
torch.size([1, 13, 768])

if i write:
print(predictions.argmax(axis=2))
tensor([613, 705, 244, 620, 206, 206, 206, 620, 620, 620, 477, 693, 308])

i get the tensor above.
however i would have expected to get the tensor representing the ground truth [0ýýý8] labels from the ground truth annotations.
summary when loading the model :

loading configuration file ./my_model_own_custom_training.pth/config.json
model config distilbertconfig {
ýýýname_or_path"": ýýýdistilbert-base-uncasedýýý,
ýýýactivationýýý: ýýýgeluýýý,
ýýýarchitecturesýýý: [
ýýýdistilbertfortokenclassificationýýý
],
ýýýattention_dropoutýýý: 0.1,
ýýýdimýýý: 768,
ýýýdropoutýýý: 0.1,
ýýýhidden_dimýýý: 3072,
ýýýid2labelï¿½ï¿½ï¿½: {
ï¿½ï¿½ï¿½0ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_0ï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½1ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_1ï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½2ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_2ï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½3ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_3ï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½4ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_4ï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½5ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_5ï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½6ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_6ï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½7ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_7ï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½8ï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½label_8ï¿½ï¿½ï¿½
},
ï¿½ï¿½ï¿½initializer_rangeï¿½ï¿½ï¿½: 0.02,
ï¿½ï¿½ï¿½label2idï¿½ï¿½ï¿½: {
ï¿½ï¿½ï¿½label_0ï¿½ï¿½ï¿½: 0,
ï¿½ï¿½ï¿½label_1ï¿½ï¿½ï¿½: 1,
ï¿½ï¿½ï¿½label_2ï¿½ï¿½ï¿½: 2,
ï¿½ï¿½ï¿½label_3ï¿½ï¿½ï¿½: 3,
ï¿½ï¿½ï¿½label_4ï¿½ï¿½ï¿½: 4,
ï¿½ï¿½ï¿½label_5ï¿½ï¿½ï¿½: 5,
ï¿½ï¿½ï¿½label_6ï¿½ï¿½ï¿½: 6,
ï¿½ï¿½ï¿½label_7ï¿½ï¿½ï¿½: 7,
ï¿½ï¿½ï¿½label_8ï¿½ï¿½ï¿","['python-3.x', 'deep-learning', 'pytorch', 'huggingface-transformers', 'huggingface-tokenizers']",68919603,"the answer is a bit trickier than expected[huge credits to niels rogge].
firstly, loading models in huggingface-transformers can be done in (at least) two ways:

automodel.from_pretrained('./my_model_own_custom_training.pth', from_tf=false)
automodelfortokenclassification.from_pretrained('./my_model_own_custom_training.pth', from_tf=false)

it seems that, according to the task at hand, different automodels subclasses need to be used. in this scenario i posted, it is the automodelfortokenclassification() that has to be used.
after that, a solution to obtain the predictions would be to do the following:
# forward pass
outputs = model(**encoding)
logits = outputs.logits

predictions = logits.argmax(-1)",https://stackoverflow.com/questions/68918962,python-3.x,25-08-2021 07:56,1008.0,2.0,1.0,True,09-08-2022 14:02,09-08-2022 14:02
76802096,runtimeerror when trying to extract text features from a bert model then using knn for classification,"i'm trying to use camembert model to just to extract text features. after that, i'm trying to use a knn classifier to classify the feature vectors as inputs.
this is the code i wrote
import torch
from transformers import autotokenizer, camembertmodel
from sklearn.neighbors import kneighborsclassifier

tokenizer = autotokenizer.from_pretrained(""camembert-base"")
model = camembertmodel.from_pretrained(""camembert-base"")

data = df.to_dict(orient='split')
data = dict(zip(data['index'], data['data']))

# collect all the input texts into a list of strings
input_texts = [str(text) for text in data.values()]

# tokenize all the input texts together
inputs = tokenizer(input_texts, return_tensors=""pt"", padding=true, truncation=true)

# get the model outputs for all the input texts
with torch.no_grad():
    outputs = model(**inputs)

# extract the last hidden states and convert them to a numpy array
last_hidden_states = outputs.last_hidden_state
input_features = last_hidden_states[:, 0, :].numpy()

# extract the labels from the data dictionary
input_labels = list(data.keys())

neigh = kneighborsclassifier(n_neighbors=3)
neigh.fit(input_features, input_labels)


however, i get this error
runtimeerror: [enforce fail at ..\c10\core\impl\alloc_cpu.cpp:72] data. defaultcpuallocator: not enough memory: you tried to allocate 19209424896 bytes.

the data i'm using in the dictionary has this form:
{
    'index': [row_index_1, row_index_2, ...],
    'columns': [column_name_1, column_name_2, ...],
    'data': [
        [cell_value_row_1_col_1, cell_value_row_1_col_2, ...],
        [cell_value_row_2_col_1, cell_value_row_2_col_2, ...],
        ...
    ]
}","['python', 'machine-learning', 'nlp', 'bert-language-model', 'knn']",76802422,"it seems that you are feeding all your data to the model at once and you don't have enough memory to do that. instead of doing that, you can invoke the model sentence by sentence or with small sentence batches, so that you keep the needed memory within the available system resources.",https://stackoverflow.com/questions/76802096,python,31-07-2023 09:02,139.0,2.0,1.0,True,01-08-2023 07:36,01-08-2023 07:36
77103010,how to combine conversationalretrievalchain with an agent?,"i have a chain which is defined as:
convr_qa = conversationalretrievalchain(retriever=customretriever, 
memory=memory, question_generator=question_generator_chain, 
combine_docs_chain=qa_chain, return_source_documents=true,  
return_generated_question=true, verbose=true )`

but now, i want to combine my chain with an agent, where agent can decide whether to retrieve or not depends on the user's intention.
i know there is ""conversational retrieval agent"" to handle this problem, but i have no idea how to combine my conversationalretrievalchain with an agent, as both question_generator_chain and qa_chain are important in my case, and i don't want to drop them.
thanks for your attention.
i have tried conversational retrieval agent in langchain document. but it is hard to customize for me.","['python', 'langchain']",77140368,"to handle with ""how to decide to retrieve or not when using conversationalretrievalchain"", i have a another solution rather than using ""conversational retrieval agent"", which is token-consuming and not robust.
a new llmchain called ""intention_detector"" is defined in my conversationalretrievalchain, taking user's question as input.
then it will decide:
intention = self.intention_detector.run(question=question)

return true or false. the prompt template of intention_detector behaves like:
prompt_few_shots_intention_detection = fewshotprompttemplate(
examples=_intention_detection_examples, 
example_prompt=_intention_detection_prompt_template, 
prefix=""please judge does user's query be related to knowledge in specific domain. return true or false."",
suffix=""input: {question}\noutput: "", 
input_variables=[""question""]

)
fewshots examples are given above depending on your demand.
then, another llmchain can handle with:
prompt_chat_history = chatprompttemplate.from_messages([
systemmessage(content=""refer to the chat_history and answer the latest question.""), 
messagesplaceholder(variable_name=""chat_history""),
humanmessageprompttemplate.from_template(""{question}""), 

])
therefore, our conversationalretrievalchain bahaves much more smart, like:
human: who are you?
ai(answer directly): i am an ai assistant. how can i help you?

and if you propose some professional issues:
human: is biyadi worth investing in?
(after retrieving the vectordb)
ai: refer to the infomation provided by gf securities, biyadi launches a lot new ......

and finally, we can end the conversation:
human: thanks for your help!
ai(answer directly): you are welcome!

here our conversationalretrievalchain can distinguish human's intention, choosing to retrieve the vectordb or not.",https://stackoverflow.com/questions/77103010,python,14-09-2023 08:22,1887.0,2.0,1.0,True,20-09-2023 07:51,14-09-2023 09:16
61269954,attribute error using neuralcoref in colab,"i'm trying to use the following spacy module in colab: 

i install the following packages: 
!pip install spacy
import spacy 
!pip show spacy

!git clone 
import neuralcoref

i get the following output after installing: 
name: spacy
version: 2.2.4
summary: industrial-strength natural language processing (nlp) in python
home-page: 
author: explosion
author-email: contact@explosion.ai
license: mit
location: /usr/local/lib/python3.6/dist-packages
requires: thinc, murmurhash, preshed, blis, srsly, cymem, setuptools, plac, requests, tqdm, numpy, wasabi, catalogue
required-by: fastai, en-core-web-sm
cloning into 'neuralcoref'...
remote: enumerating objects: 48, done.
remote: counting objects: 100% (48/48), done.
remote: compressing objects: 100% (44/44), done.
remote: total 739 (delta 14), reused 10 (delta 1), pack-reused 691
receiving objects: 100% (739/739), 67.86 mib | 30.25 mib/s, done.
resolving deltas: 100% (368/368), done.

i then follow the instructions on the website: 
nlp = spacy.load('en')
neuralcoref.add_to_pipe(nlp)

however, i get the following error: 
---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-8-fe99e1a1a10f> in <module>()
      1 nlp = spacy.load('en')
----> 2 neuralcoref.add_to_pipe(nlp)
      3 #coref = neuralcoref.neuralcoref(nlp.vocab)
      4 #nlp.add_pipe(coref, name='neuralcoref')

attributeerror: module 'neuralcoref' has no attribute 'add_to_pipe'

does anybody know how to fix this? 
edit
after (successfully) using the suggestion below, colab crashed on me when i tried to run the example provided (see details below). 
here is the code used: 
from google.colab import drive
drive.mount('/content/gdrive')

!pip install neuralcoref

import spacy
import neuralcoref

nlp = spacy.load('en') #ï¿½ï¿½this is the line where it crashes
neuralcoref.add_to_pipe(nlp)

doc1 = nlp('my sister has a dog. she loves him.')
print(doc1._.coref_clusters)

i've attached a screenshot with the original error message at the bottom left. 

edit 2
i got the code to work on colab when changing the order of installing the modules (not sure why). 
the following has worked for me now: 
from google.colab import drive
drive.mount('/content/gdrive')

!git clone 
!pip install -u spacy
!python -m spacy download en

import spacy
nlp = spacy.load('en')

%cd neuralcoref

!pip install -r requirements.txt
!pip install -e .

import neuralcoref
neuralcoref.add_to_pipe(nlp)

doc1 = nlp('my sister has a dog. she loves him.')
print(doc1._.coref_clusters)","['python-3.x', 'google-colaboratory', 'spacy']",61270808,"update:
since the previous helped solving the first problem but created another problem, i have updated the answer.
according to neuralcoref page, for our version of spacy, we need to manually install it from the source.
also, try each of the following blocks in new cell in colab, and restart runtime after installation.
mkdir temp

cd temp

!git clone 
!pip install -u spacy
!python -m spacy download en

cd neuralcoref

!pip install -r requirements.txt
!pip install -e .


import neuralcoref
import spacy

nlp = spacy.load('en')
neuralcoref.add_to_pipe(nlp)

doc1 = nlp('my sister has a dog. she loves him.')
print(doc1._.coref_clusters)",https://stackoverflow.com/questions/61269954,python-3.x,17-04-2020 10:56,5598.0,6.0,4.0,True,06-04-2023 15:36,17-04-2020 13:27
73183103,clustering based on semantic similarity returning no values,"i have 'key_phrases' as a column in pandas dataframe df. the objective is to cluster them on semantic similarity. i am using sentencetransformer model.
 df['key phrases'] is as follows

                'key_phrases'

0              ['byd' 'daiwa capital markets analyst' 'nio' 'order flows'\n 'consumer preferences' 'cost pressures' 'raw materials'\n 'regulatory pressure' 'sales cannibalization' 'sales volume growth'\n 'vehicle batteries']
1              ['canada' 'canada' 'global carbon pricing challenge'\n 'major economies forum' 'climate finance commitment'\n 'developing countries' 'energy security' 'food security'\n 'international shipping' 'pollution pricing']
2              ['clean power plan' 'epa' 'environmental protection agency'\n 'supreme court' 'supreme court decision' 'virginia' 'west virginia'\n 'renewable energy' 'tax subsidies']
3              ['blueovalsk' 'ford' 'ford motor' 'kathleen valley' 'lg energy' 'liontown'\n 'liontown resources' 'sk innovation' 'sk on' 'tesla' 'battery metals'\n 'joint venture' 'lithium spodumene concentrate'\n 'lithium supply agreement']
4              ['emissions trading system' 'european commission' 'european parliament'\n 'icis' 'carbon border adjustment mechanism' 'carbon leakage']
5              ['digital industries' 'mg motor india' 'mindsphere'\n 'plant simulation software' 'siemens' 'carbon footprints'\n 'digitalisation' 'experience' 'intelligent manufacturing'\n 'production efficiency' 'strategic collaborations']
6              ['malaysia' 'mosti' 'ntis' 'national technology and innovation sandbox'\n 'national urbanisation policy' 'sunway innovation labs'\n 'sunway ilabs super accelerator' 'economic growth'\n 'memorandum of understanding' 'quality of life' 'safe environment'\n 'smart cities' 'smart city sandbox' 'urban management' 'urban population']
7              ['artificial intelligence' 'electricity and water authority'\n 'green mobility' 'grid automation' 'internet of things' 'smart dubai'\n 'smart energy solutions' 'smart grid' 'smart water'\n 'artificial intelligence' 'blockchain' 'connected services'\n 'energy storage' 'integrated systems' 'interoperability' 'smart city'\n 'smart grid' 'sustainability' 'water network']
8              ['artificial intelligence' 'clean energy strategy 2050'\n 'dubai electricity and water authority' 'green mobility'\n 'grid automation' 'internet of things' 'smart dubai'\n 'smart energy solutions' 'smart grid' 'smart water'\n 'zero carbon emissions strategy' 'artificial intelligence' 'blockchain'\n 'clean energy sources' 'connected services' 'energy storage'\n 'integrated systems' 'interoperability' 'smart city' 'smart grid'\n 'sustainability']

key_phrases_list_1 = df['key phrases'].tolist()
from sentence_transformers import sentencetransformer, util
import numpy as np

model = sentencetransformer('distilbert-base-nli-stsb-quora-ranking')    
#encoding is done with one simple step
embeddings = model.encode(key_phrases_list_1, show_progress_bar=true, convert_to_numpy=true)

then the following function is created:
def detect_clusters(embeddings, threshold=0.90, min_community_size=20):
    # compute cosine similarity scores
    cos_scores = util.pytorch_cos_sim(embeddings, embeddings)

    #we filter those scores according to the minimum community size we specified earlier
    # minimum size for a community
    top_k_values, _ = cos_scores.topk(k=min_community_size, largest=true)
    # filter for rows >= min_threshold
    extracted_communities = []
    for i in range(len(top_k_values)):
        if top_k_values[i][-1] >= threshold:
            new_cluster = []

    # only check top k most similar entries
            top_val_large, top_idx_large = cos_scores[i].topk(k=init_max_size, largest=true)
            top_idx_large = top_idx_large.tolist()
            top_val_large = top_val_large.tolist()
            
            if top_val_large[-1] < threshold:
                for idx, val in zip(top_idx_large, top_val_large):
                    if val < threshold:
                        break
                        new_cluster.append(idx)
            else:
                # iterate over all entries (slow)
                for idx, val in enumerate(cos_scores[i].tolist()):
                    if val >= threshold:
                        new_cluster.append(idx)
                        
            extracted_communities.append(new_cluster)

    unique_communities = []
    extracted_ids = set()
        
    for community in extracted_communities:
        add_cluster = true
        for idx in community:
            if idx in extracted_ids:
                add_cluster = false
                break
        if add_cluster:
            unique_communities.append(community)
            for idx in community:
                extracted_ids.add(idx)
    return unique_communities

then the function is called:
clusters = detect_clusters(embeddings, min_community_size=6, threshold=0.75)

i am getting no values in return. am i missing anything in the detect_clusters function.","['python-3.x', 'pandas', 'numpy', 'word-embedding', 'transformer-model']",73218870,"as the op asked for a solution where the number of clusters would be automatic selected it is easier to use something more robust like sklearn:
from sentence_transformers import sentencetransformer, util
import numpy as np
model = sentencetransformer('distilbert-base-nli-stsb-quora-ranking')
from sklearn.cluster import kmeans
from sklearn.metrics import silhouette_score
def choose_classifier(x):
    x1 = x / (x**2).sum(axis=-1, keepdims=true)
    vv = []
    cc = np.arange(2, len(x))
    for nclusters in cc:
        km_model = kmeans(nclusters).fit(x1)
        labels = km_model.labels_
        v = silhouette_score(x1, labels)
        vv.append(v)
    nclusters = cc[np.argmax(vv)]
    return kmeans(nclusters).fit(x1)


use it like this
phrases = [
    'i like ice cream',
    'i like cake',
    'you are so kind',
    'you are very intelligent'
]
embeddings = model.encode(phrases, show_progress_bar=true, convert_to_numpy=true)

classifier = choose_classifier(embeddings)

for i, (v, s) in enumerate(zip(embeddings, phrases)):
    print(classifier.predict(v[np.newaxis]), s)

[1] i like ice cream
[1] i like cake
[0] you are so kind
[0] you are very intelligent

gpu capable solution
at a first sight i couldn't grasp all you are doing in your code, but let me suggest you some simplified method. i use pytorch_kmeans, and i explore the fact that the squared euclidean distance is dot(a-b,a-b) = dot(a,a) + dot(b,b) - 2 * dot(a, b), and that cosine similarity is dot(a, b) / sqrt(dot(a,a) * dot(b,b)). so (1)
multiplying a or b by a scalar does not change cosine similarity, (2) if a and b have the same length, minimizing euclidean maximizes cosine similarity. given the set of vectors you want to cluster you can (1) normalize all of them, making them the same length, (2) compute the clusters that minimize euclidean distance. then you have the clusters that maximize cosine similarity.
pip install kmeans_pytorch

setup
since you didnt' give data i will generate an example myself
import torch;
# 2d example data
# generate some random data in three clusters
npc=10
x = torch.cat([
  (torch.randn((npc, 2)) + c) * (torch.rand((npc,1))**2+1)/2 
      for c in torch.tensor([[5,3], [-7,0], [-0, -7]])])

solution
this is the code
from kmeans_pytorch import kmeans
import torch
def detect_clusters(x, nclusters, tol=1e-6):
  x = torch.as_tensor(x)
  assert x.ndim == 2
  # project the points in a hypersphere
  x1 = x / torch.sqrt(torch.sum(x**2, axis=-1, keepdims=true))

  # run kmeans on the normalized points with euclidean distance
  cluster_id, c = kmeans(x1, nclusters, distance='euclidean', tol=tol)
  return cluster_id, c

example visualization
import matplotlib.pyplot as plt
import numpy as np
import torch;

#### the resutls ####
cluster_id, c = detect_clusters(x, 3)
# avoid distortion of the angles
plt.axes().set_aspect('equal')
# initial points
plt.plot(x[:,0], x[:,1], '.')
# reference circle
theta = torch.linspace(0, 2*np.pi, 1000)
plt.plot(torch.cos(theta), torch.sin(theta), '--k')
plt.plot(x1[:,0], x1[:,1], '.')
xlim = plt.xlim()
ylim = plt.ylim()
plt.xlim(xlim)
plt.ylim(ylim)

# draw lines in the directions given by the centroids
r = 20
for c in c:
    plt.plot([0, c[0]*r], [0, c[1]*r]);

plt.grid();



using with sentence embeddings
some example embeddings
from sentence_transformers import sentencetransformer, util
import numpy as np

model = sentencetransformer('distilbert-base-nli-stsb-quora-ranking')   


phrases = [
    'i like ice cream',
    'i like cake',
    'you are so kind',
    'you are very intelligent'
]
embeddings = model.encode(phrases, show_progress_bar=true, convert_to_numpy=true)

then you can pass the embeddings to the detect_cluster function i provided above
label, center = detect_clusters(torch.as_tensor(embeddings), 2)
for c, s in zip(label, phrases):
    print(f'[{c}] {s}')

that should give give you the sentences with the corresponding cluster
[0] i like ice cream
[0] i like cake
[1] you are so kind
[1] you are very intelligent",https://stackoverflow.com/questions/73183103,python-3.x,31-07-2022 11:56,1002.0,3.0,1.0,True,05-08-2022 17:56,03-08-2022 18:30
70446032,how does text encoding from tensorflow.keras.preprocessing.text.tokenizer differ from the old tfds.deprecated.text.tokentextencoder,"tfds.deprecated.text.tokentextencoder
in the deprecated encoding method with tfds.deprecated.text.tokentextencoder
we first create a vocab set of token
tokenizer = tfds.deprecated.text.tokenizer()
vocabulary_set = set()

#imdb_train --> imdb dataset from tensorflow_datasets
for example, label in imdb_train:
    some_tokens = tokenizer.tokenize(example.numpy())


then load it into the encoder
encoder = tfds.deprecated.text.tokentextencoder(vocabulary_set,
                                                   lowercase=true,
                                                   tokenizer=tokenizer)

afterward when performing encoding i notice the encoder will output a single integer, for example while debugging i found that the word ""the"" got encoded with 112
 token_id = encoder.encode(word)[0]
>> token_id = 112

but then when it comes to
tensorflow.keras.preprocessing.text.tokenizer
tokenizer = tensorflow.keras.preprocessing.text.tokenizer()
tokenizer.fit_on_texts(words)
token_id = tokenizer.texts_to_sequences(word) #word = the
>> token_id = [800,2085,936]

it produces a sequence of 3 integers, so now do i use all 3 numbers or should it be also correct if i take just 1 number in that sequence? i'm trying to use this encoded integer to create embedding matrix using glove embedding. the old deprecated one produces just one integer so it's easier to map, with integer sequence i'm not sure how to proceed","['python', 'tensorflow', 'keras', 'nlp', 'tensorflow2.0']",70447071,"maybe try something like this:
import tensorflow as tf

lines = ['you are a fish', 'this is a fish', 'where are the fishes']
tokenizer = tf.keras.preprocessing.text.tokenizer()
tokenizer.fit_on_texts(lines)
text_sequences = tokenizer.texts_to_sequences(lines)
text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences, padding='post')
vocab_size = len(tokenizer.word_index) + 1
print(tokenizer.word_index)
print(vocab_size)
print(tokenizer.texts_to_sequences(['fish'])[0])

{'are': 1, 'a': 2, 'fish': 3, 'you': 4, 'this': 5, 'is': 6, 'where': 7, 'the': 8, 'fishes': 9}
10
[3]

the index 0 is reserved for the padding token. and then to create the weight matrix with the glove model, try this:
import gensim.downloader as api
import numpy as np

model = api.load(""glove-twitter-25"")
embedding_dim = 25
weight_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in tokenizer.word_index.items():
  try:
    embedding_vector = model[word]
    weight_matrix[i] = embedding_vector
  except keyerror:
    weight_matrix[i] = np.random.uniform(-5, 5, embedding_dim)
print(weight_matrix.shape)
# (10, 25)",https://stackoverflow.com/questions/70446032,python,22-12-2021 08:16,1254.0,2.0,1.0,True,23-12-2021 07:49,23-12-2021 07:49
77455738,finding the nouns in a sentence given the context in python,"how to find the nouns in a sentence regarding the context? i am using the nltk library as follows:
text = 'i bought a vintage car.'
text = nltk.word_tokenize(text)
result = nltk.pos_tag(text)
result = [i for i in result if i[1] == 'nn']

#result = [('vintage', 'nn'), ('car', 'nn')]

the problem with this script is that it considers vintage as a noun, which can be true, but given the context, it is an adjective.
how can we achieve this task?
appendix: using textblob, we get ""vintage car"" as the noun:
!python -m textblob.download_corpora
from textblob import textblob
txt = ""i bought a vintage car.""
blob = textblob(txt)
print(blob.noun_phrases) #['vintage car']","['python', 'nlp', 'nltk', 'textblob']",77455927,"using spacy might solve your task. try this:
import spacy
nlp = spacy.load(""en_core_web_lg"")

def analyze(text):
    doc = nlp(text)
    for token in doc:
        print(token.text, token.pos_)

analyze(""i bought a vintage car."")
print()
analyze(""this old wine is a vintage."")

output
i pron
bought verb
a det
vintage adj <- correctly identified as adjective
car noun
. punct

this det
old adj
wine noun
is aux
a det
vintage noun  <- correctly identified as noun
. punct",https://stackoverflow.com/questions/77455738,python,09-11-2023 18:59,181.0,3.0,2.0,True,09-11-2023 19:43,09-11-2023 19:13
70607224,huggingface - &#39;optimum&#39; modulenotfounderror,"i want to run the 3 code snippets from this webpage.
i've made all 3 one post, as i am assuming it all stems from the same problem of optimum not having been imported correctly?
kernel: conda_pytorch_p36

installations:
pip install optimum

or
! pip install datasets transformers optimum[intel]

both provide same traceback:
requirement already satisfied: optimum in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.1.3)
requirement already satisfied: transformers>=4.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (4.15.0)
requirement already satisfied: coloredlogs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (15.0.1)
requirement already satisfied: torch>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (1.10.1)
requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (1.8)
requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch>=1.9->optimum) (3.10.0.0)
requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch>=1.9->optimum) (0.8)
requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (1.19.5)
requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (21.3)
requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (5.4.1)
requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (0.0.46)
requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (4.62.3)
requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (2021.4.4)
requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (2.25.1)
requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (0.2.1)
requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (0.10.3)
requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (4.5.0)
requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (3.0.12)
requirement already satisfied: humanfriendly>=9.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from coloredlogs->optimum) (10.0)
requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sympy->optimum) (1.2.1)
requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->transformers>=4.12.0->optimum) (2.4.7)
requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers>=4.12.0->optimum) (3.4.1)
requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers>=4.12.0->optimum) (2.10)
requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers>=4.12.0->optimum) (2021.5.30)
requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers>=4.12.0->optimum) (4.0.0)
requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers>=4.12.0->optimum) (1.26.5)
requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers>=4.12.0->optimum) (1.0.1)
requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers>=4.12.0->optimum) (8.0.1)
requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers>=4.12.0->optimum) (1.16.0)
note: you may need to restart the kernel to use updated packages.


from optimum.intel.lpot.quantization import lpotquantizerforsequenceclassification

# create quantizer from config 
quantizer = lpotquantizerforsequenceclassification.from_config(
    ""echarlaix/quantize-dynamic-test"",
    ""quantization.yml"",
    model_name_or_path=""textattack/bert-base-uncased-sst-2"",
)

model = quantizer.fit_dynamic()

traceback:
---------------------------------------------------------------------------
modulenotfounderror                       traceback (most recent call last)
<ipython-input-6-9dcf25f181ea> in <module>
----> 1 from optimum.intel.lpot.quantization import lpotquantizerforsequenceclassification
      2 
      3 # create quantizer from config
      4 quantizer = lpotquantizerforsequenceclassification.from_config(
      5     ""echarlaix/quantize-dynamic-test"",

modulenotfounderror: no module named 'optimum.intel.lpot'

from optimum.intel.lpot.pruning import lpotprunerforsequenceclassification

# create pruner from config 
pruner = lpotprunerforsequenceclassification.from_config(
    ""echarlaix/magnitude-pruning-test"",
    ""prune.yml"",
    model_name_or_path=""textattack/bert-base-uncased-sst-2"",
)

model = pruner.fit()

traceback:
---------------------------------------------------------------------------
modulenotfounderror                       traceback (most recent call last)
<ipython-input-7-e9872c164aee> in <module>
----> 1 from optimum.intel.lpot.pruning import lpotprunerforsequenceclassification
      2 
      3 # create pruner from config
      4 pruner = lpotprunerforsequenceclassification.from_config(
      5     ""echarlaix/magnitude-pruning-test"",

modulenotfounderror: no module named 'optimum.intel.lpot'

from optimum.graphcore import iputrainer
from optimum.graphcore.bert import bertipuconfig
from transformers import bertformaskedlm, berttokenizer
from poptorch.optim import adamw

# allocate model and tokenizer as usual
tokenizer = berttokenizer.from_pretrained(""bert-base-cased"")
model = bertformaskedlm.from_pretrained(""bert-base-cased"")

# trainer + poptorch custom configuration optional 
ipu_config = bertipuconfig()
trainer = iputrainer(model, trainings_args, config=ipu_config)
optimizer = adamw(model.parameters)

# this is hidden from the user, it will be handled by the trainer
with trainer.compile(some_data_loader) as model_f:
    for steps in range(10):  # !
        outputs = trainer.step(optimizer)    

# save the model and/or push to hub
model.save_pretrained(""..."")
model.push_to_hub(""..."")

traceback:
---------------------------------------------------------------------------
modulenotfounderror                       traceback (most recent call last)
<ipython-input-8-921e03245390> in <module>
----> 1 from optimum.graphcore import iputrainer
      2 from optimum.graphcore.bert import bertipuconfig
      3 from transformers import bertformaskedlm, berttokenizer
      4 from poptorch.optim import adamw
      5 

modulenotfounderror: no module named 'optimum.graphcore'

please let me know if there's anything else i can add to post.","['python', 'huggingface-transformers', 'quantization', 'modulenotfounderror', 'pruning']",70667079,"pointed out by a contributor of huggingface, on this git issue,

the library previously named lpot has been renamed to intel neural compressor (inc), which resulted in a change in the name of our subpackage from lpot to neural_compressor.
the correct way to import would now be from optimum.intel.neural_compressor.quantization import incquantizerforsequenceclassification
concerning the graphcore subpackage, you need to install it first with pip install optimum[graphcore]
furthermore you'll need to have access to an ipu in order to use it.


solution
! pip install datasets transformers optimum[graphcore]

instead of:
from optimum.intel.lpot.quantization import lpotquantizerforsequenceclassification
from optimum.intel.lpot.pruning import lpotprunerforsequenceclassification

from optimum.intel.neural_compressor.quantization import incquantizerforsequenceclassification
from optimum.intel.neural_compressor.pruning import incprunerforsequenceclassification",https://stackoverflow.com/questions/70607224,python,06-01-2022 12:39,11186.0,1.0,2.0,True,27-08-2024 18:36,06-01-2022 12:46
195010,how can i split multiple joined words?,"i have an array of 1000 or so entries, with examples below:
wickedweather
liquidweather
driveourtrucks
gocompact
slimprojector

i would like to be able to split these into their respective words, as:
wicked weather
liquid weather
drive our trucks
go compact
slim projector

i was hoping a regular expression my do the trick.  but, since there is no boundary to stop on, nor is there any sort of capitalization that i could possibly key on, i am thinking, that some sort of reference to a dictionary might be necessary?  
i suppose it could be done by hand, but why - when it can be done with code! =)  but this has stumped me.  any ideas?","string, nlp",481773,"the viterbi algorithm is much faster. it computes the same scores as the recursive search in dmitry's answer above, but in o(n) time. (dmitry's search takes exponential time; viterbi does it by dynamic programming.)
import re
from collections import counter

def viterbi_segment(text):
    probs, lasts = [1.0], [0]
    for i in range(1, len(text) + 1):
        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)
                        for j in range(max(0, i - max_word_length), i))
        probs.append(prob_k)
        lasts.append(k)
    words = []
    i = len(text)
    while 0 < i:
        words.append(text[lasts[i]:i])
        i = lasts[i]
    words.reverse()
    return words, probs[-1]

def word_prob(word): return dictionary[word] / total
def words(text): return re.findall('[a-z]+', text.lower()) 
dictionary = counter(words(open('big.txt').read()))
max_word_length = max(map(len, dictionary))
total = float(sum(dictionary.values()))

testing it:
>>> viterbi_segment('wickedweather')
(['wicked', 'weather'], 5.1518198982768158e-10)
>>> ' '.join(viterbi_segment('itseasyformetosplitlongruntogetherblocks')[0])
'its easy for me to split long run together blocks'

to be practical you'll likely want a couple refinements:

add logs of probabilities, don't multiply probabilities. this avoids floating-point underflow.
your inputs will in general use words not in your corpus. these substrings must be assigned a nonzero probability as words, or you end up with no solution or a bad solution. (that's just as true for the above exponential search algorithm.) this probability has to be siphoned off the corpus words' probabilities and distributed plausibly among all other word candidates: the general topic is known as smoothing in statistical language models. (you can get away with some pretty rough hacks, though.) this is where the o(n) viterbi algorithm blows away the search algorithm, because considering non-corpus words blows up the branching factor.",https://stackoverflow.com/q/195010,"string, nlp",12-10-2008 02:37,37898.0,58.0,16.0,True,05-07-2023 05:27,06-05-2012 07:34
75624308,openai gpt-3 api errors: &#39;text&#39; does not exist ts(2339) &amp; &#39;prompt&#39; does not exist on type &#39;createchatcompletion&#39; ts(2345),"import openai from ""./zggpt"";

const query = async (prompt:string,  chatid:string, model:string) => {
    const res= await openai
    .createchatcompletion({
        model,
        prompt,
        temperature: 0.9,
        
        top_p:1,
       
        max_tokens:1000,
        frequency_penalty:0,
        presence_penalty:0,
    })
    .then((res) => res.data.choices[0].text)
    .catch((err)=>
    `zg was unable to find an answer for that!
     (error: ${err.message})`
     );
     return res;
};

export default query;

property 'text' does not exist on type 'createchatcompletionresponsechoicesinner'.ts(2339)
argument of type '{ model: string; prompt: string; temperature: number; top_p: number; max_tokens: number; frequency_penalty: number; presence_penalty: number; }' is not assignable to parameter of type 'createchatcompletionrequest'.
object literal may only specify known properties, and 'prompt' does not exist in type 'createchatcompletionrequest'.ts(2345)
even though i do everything as in the video, i get these errors.
i'm a beginner in coding, so i'm trying to make applications based on videos to learn.
thanks
the application responds without returning an error.
enter image description here","['next.js', 'openai-api', 'gpt-3']",75626340,"problem
you watched a tutorial which used the completions api endpoint where you need to provide the prompt and parameters to get a completion). in this case, this is the function which generates a completion:
openai.createcompletion()

whereas, you used the code from the tutorial, but used the chat completions api endpoint (you need to provide the chat message to get a completion). in this case, this is the function which generates a completion:
openai.createchatcompletion()

note: openai nodejs sdk v4 was released on august 16, 2023, and is a complete rewrite of the sdk. among other things, there are changes in method names. see the v3 to v4 migration guide.




api endpoint
nodejs function (sdk v3)
nodejs function (sdk v4)




chat completions api
openai.createchatcompletion
openai.chat.completions.create


completions api
openai.createcompletion
openai.completions.create




solution
ï¿½ï¿½ï¿½ if you have the openai nodejs sdk v3:
change this...
openai.createchatcompletion()

...to this.
openai.createcompletion()

ï¿½ï¿½ï¿½ if you have the openai nodejs sdk v4:
change this...
openai.chat.completions.create

...to this.
openai.completions.create

both errors will disappear.

my advice
however, you want to achieve a chat-like bot using a gpt-3 model. at the time the tutorial was recorded, thily way to do it. since 1 march 2023, the gpt-3.5-turbo model is available. i strongly suggest you to use it. see the official openai documentation.",https://stackoverflow.com/questions/75624308,next.js,03-03-2023 07:34,2142.0,0.0,2.0,True,14-09-2023 15:24,03-03-2023 11:23
79091722,unable to install `evals` python cli for openai,"failing to install: pip install evals. for complete logs please see this gist file here.
tools version:

python --version: python 3.9.6
pip --version: pip 24.2 from /users/apple/library/python/3.9/lib/python/site-packages/pip (python 3.9)

collecting keras<2.8,>=2.7.0rc0 (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals)
  using cached keras-2.7.0-py2.py3-none-any.whl.metadata (1.3 kb)
collecting tensorboard~=2.6 (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals)
  using cached tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kb)
  using cached tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kb)
  using cached tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kb)
  using cached tensorboard-2.11.2-py3-none-any.whl.metadata (1.9 kb)
collecting protobuf (from google-generativeai->evals)
  using cached protobuf-3.20.3-cp39-cp39-macosx_10_9_x86_64.whl.metadata (679 bytes)
collecting tensorboard~=2.6 (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals)
  using cached tensorboard-2.11.0-py3-none-any.whl.metadata (1.9 kb)
  using cached tensorboard-2.10.1-py3-none-any.whl.metadata (1.9 kb)
  using cached tensorboard-2.10.0-py3-none-any.whl.metadata (1.9 kb)
  using cached tensorboard-2.7.0-py3-none-any.whl.metadata (1.9 kb)
  using cached tensorboard-2.6.0-py3-none-any.whl.metadata (1.9 kb)
collecting tensorflow<3.0.0,>=2.4.0 (from spacy-universal-sentence-encoder->evals)
  using cached tensorflow-2.7.1-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.9 kb)
  using cached tensorflow-2.7.0-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.9 kb)
info: pip is still looking at multiple versions of tf-keras to determine which version is compatible with other requirements. this could take a while.
  using cached tensorflow-2.6.5-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.6.4-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.6.3-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.6.2-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.6.1-cp39-cp39-macosx_10_14_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.6.0-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.5.3-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.5.2-cp39-cp39-macosx_10_14_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.5.1-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kb)
  using cached tensorflow-2.5.0-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kb)
collecting sqlalchemy<3,>=1.4 (from langchain->evals)
  using cached sqlalchemy-2.0.35-cp39-cp39-macosx_10_9_x86_64.whl.metadata (9.6 kb)
info: this is taking longer than usual. you might need to provide the dependency resolver with stricter constraints to reduce runtime. see  for guidance. if you want to abort this run, press ctrl + c.
  using cached sqlalchemy-2.0.34-cp39-cp39-macosx_10_9_x86_64.whl.metadata (9.6 kb)
  using cached sqlalchemy-2.0.33-cp39-cp39-macosx_10_9_x86_64.whl.metadata (9.6 kb)
error: exception:
traceback (most recent call last):
  file ""/users/apple/library/python/3.9/lib/python/site-packages/pip/_internal/cli/base_command.py"", line 105, in _run_wrapper
    status = _inner_run()
  file ""/users/apple/library/python/3.9/lib/python/site-packages/pip/_internal/cli/base_command.py"", line 96, in _inner_run
    return self.run(options, args)
  file ""/users/apple/library/python/3.9/lib/python/site-packages/pip/_internal/cli/req_command.py"", line 67, in wrapper
    return func(self, options, args)
  file ""/users/apple/library/python/3.9/lib/python/site-packages/pip/_internal/commands/install.py"", line 379, in run
    requirement_set = resolver.resolve(
  file ""/users/apple/library/python/3.9/lib/python/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 95, in resolve
    result = self._result = resolver.resolve(
  file ""/users/apple/library/python/3.9/lib/python/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 546, in resolve
    state = resolution.resolve(requirements, max_rounds=max_rounds)
  file ""/users/apple/library/python/3.9/lib/python/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 457, in resolve
    raise resolutiontoodeep(max_rounds)
pip._vendor.resolvelib.resolvers.resolutiontoodeep: 200000

as we can see in the end of below log, we see some error. please help me debug this, i'm new to python and tried to find the answer but couldn't figure it out after waiting to install this for like 1 hours twice. the error is like pip._vendor.resolvelib.resolvers.resolutiontoodeep: 200000.
please help. thanks in advance.
github respository issue by me on this python library: 
community post on openai:","['python', 'python-3.x', 'pip', 'openai-api']",79107808,"i updated my python to latest version from official python website on recommendation of @stark-jarvis, and that helped me. thanks @stark-jarvis.",https://stackoverflow.com/questions/79091722,python,15-10-2024 20:54,133.0,-1.0,2.0,True,20-10-2024 18:43,16-10-2024 10:25
66685754,finding the dominant topic in each sentence in topic modeling,"one question that i can't find the answer for in r is that how i can find the dominant topic in nlp model for each sentence?
imagine i have data frame like this:
comment <- c(""outstanding renovation all improvements are topoftheline and done with energy efficiency in mind low monthly utilities even the interior"",
             ""solidly constructed lovingly maintained sf crest built"",
             ""one year since built new this well designed storey home"",
             ""beautiful street large bdm in the heart of lynn valley over sqft bathrooms"",
             ""rare to find legal beautiful upgr in port moody centre with a mountain view all bedroom units were nicely renovated"",
             ""fantastic opportunity to get value for the money excellent family home in desirable blueridge with legal selfcontained bachelor suite on the main floor great location close to swimming ice skating community"",
             ""original owner tired but rock solid perfect location half a block to norquay elementary school and short quiet blocks to slocan park and sky train station"")

id <- c(1,2,3,4,5,6,7)

data <- data.frame(id, comment)

i do preprocess as shown below:
text_cleaning_tokens <- data %>% 
  tidytext::unnest_tokens(word, comment)
text_cleaning_tokens$word <- gsub('[[:digit:]]+', '', text_cleaning_tokens$word)
text_cleaning_tokens$word <- gsub('[[:punct:]]+', '', text_cleaning_tokens$word)


text_cleaning_tokens <- text_cleaning_tokens %>% filter(!(nchar(word) == 1))%>% 
  anti_join(stop_words)

stemmed_token <- text_cleaning_tokens %>% mutate(word=wordstem(word))


tokens <- stemmed_token %>% filter(!(word==""""))
tokens <- tokens %>% mutate(ind = row_number())
tokens <- tokens %>% group_by(id) %>% mutate(ind = row_number()) %>%
  tidyr::spread(key = ind, value = word)
tokens [is.na(tokens)] <- """"
tokens <- tidyr::unite(tokens, clean_remark,-id,sep ="" "" )
tokens$clean_remark <- trimws(tokens$clean_remark)

the i ran fitldamodel function on this data and finally, found the best topics based on 2 groups:
             t_1            t_2
1         beauti          built
2          block           home
3          renov          legal
4       bathroom          locat
5            bdm       bachelor
6      bdm_heart  bachelor_suit
7  beauti_street  block_norquai
8    beauti_upgr       blueridg
9        bedroom blueridg_legal
10  bedroom_unit   built_design

now based on the result i have, i want to find the most dominant topic in each sentence in topic modelling. for example, i want to know that for comment 1 (""outstanding renovation all improvements are topoftheline and done with energy efficiency in mind low monthly utilities even the interior""), which topic (topic 1 or topic 2) is the most dominant?
can anyone help me with this question? do we have any package that can do this?","['r', 'nlp', 'text-mining', 'topic-modeling']",66687746,"it is pretty easy to work with quanteda and topicmodels. the former is for data management and quantitative analysis of textual data, the latter is for topic modeling inference.
here i take your comment object and transform it to a corpus and then to a dfm. i then convert it to be understandable by topicmodels.
the function lda() gives you all you need to easily extract information. in particular, with get_topics() you get the most probable topic for each document. if you instead want to see the document-topic-weights you can do so with ldamodel@gamma. you will see that get_topics() does exactly what you asked.
please, see if this works for you.
library(quanteda)
#> package version: 2.1.2
#> parallel computing: 2 of 16 threads used.
#> see  for tutorials and examples.
#> 
#> attaching package: 'quanteda'
#> the following object is masked from 'package:utils':
#> 
#>     view
library(topicmodels)


comment <- c(""outstanding renovation all improvements are topoftheline and done with energy efficiency in mind low monthly utilities even the interior"",
             ""solidly constructed lovingly maintained sf crest built"",
             ""one year since built new this well designed storey home"",
             ""beautiful street large bdm in the heart of lynn valley over sqft bathrooms"",
             ""rare to find legal beautiful upgr in port moody centre with a mountain view all bedroom units were nicely renovated"",
             ""fantastic opportunity to get value for the money excellent family home in desirable blueridge with legal selfcontained bachelor suite on the main floor great location close to swimming ice skating community"",
             ""original owner tired but rock solid perfect location half a block to norquay elementary school and short quiet blocks to slocan park and sky train station"")

mycorp <- corpus(comment)
docvars(mycorp, ""id"") <- 1l:7l

mydfm <- dfm(mycorp)

# convert the dfm to a document matrix for topicmodels
fortm <- convert(mydfm, to = ""topicmodels"")

mylda <- lda(fortm, k = 2)

dominant_topics <- get_topics(mylda)
dominant_topics
#> text1 text2 text3 text4 text5 text6 text7 
#>     2     2     2     2     1     1     1

dtw <- mylda@gamma
dtw
#>           [,1]      [,2]
#> [1,] 0.4870600 0.5129400
#> [2,] 0.4994974 0.5005026
#> [3,] 0.4980144 0.5019856
#> [4,] 0.4938985 0.5061015
#> [5,] 0.5037667 0.4962333
#> [6,] 0.5000727 0.4999273
#> [7,] 0.5176960 0.4823040

created on 2021-03-18 by the reprex package (v1.0.0)",https://stackoverflow.com/questions/66685754,r,18-03-2021 06:22,1040.0,0.0,2.0,True,07-10-2022 11:40,18-03-2021 06:56
52286330,inaccurate similarities results by doc2vec using gensim library,"i am working with gensim library to train some data files using doc2vec,  while trying to test the similarity of one of the files using the method model.docvecs.most_similar(""file"") , i always get all the results above 91% with almost no difference between them (which is not logic), because the files do not have similarities between them. so the results are inaccurate.
here is the code for training the model
model = gensim.models.doc2vec(vector_size=300, min_count=0, alpha=0.025, min_alpha=0.00025,dm=1)
model.build_vocab(it)
for epoch in range(100):
    model.train(it,epochs=model.iter, total_examples=model.corpus_count)
    model.alpha -= 0.0002
    model.min_alpha = model.alpha
model.save('doc2vecs.model')
model_d2v = gensim.models.doc2vec.doc2vec.load('doc2vecs.model')
sim = model_d2v.docvecs.most_similar('file1.txt')
print sim

 
**this is the output result**


[('file2.txt', 0.9279470443725586), ('file6.txt', 0.9258157014846802), ('file3.txt', 0.92499840259552), ('file5.txt', 0.9209873676300049), ('file4.txt', 0.9180108308792114), ('file7.txt', 0.9141069650650024)]


what am i doing wrong ? how could i improve the accuracy of results ?","['python', 'nlp', 'gensim', 'doc2vec']",52300508,"what is your it data, and how is it prepared? (for example, what does print(iter(it).next()) do, especially if you call it twice in a row?)
by calling train() 100 times, and also retaining the default model.iter of 5, you're actually making 500 passes over the data. and the first 5 passes will use train()s internal, effective alpha-management to lower the learning rate gradually to your declared min_alpha value. then your next 495 passes will be at your own clumsily-managed alpha rates, first back up near 0.025 and then lower each batch-of-5 until you reach 0.005.
none of that is a good idea. you can just call train() once, passing it your desired number of epochs. a typical number of epochs in published work  is 10-20. (a bit more might help with a small dataset, but if you think you need hundreds, something else is probably wrong with the data or setup.)
if it's a small amount of data, you won't get very interesting word2vec/doc2vec results, as these algorithms depend on lots of varied examples. published results tend to use training sets with tens-of-thousands to millions of documents, and each document at least dozens, but preferably hundreds, of words long. with tinier datasets, sometimes you can squeeze out adequate results by using more training passes, and smaller vectors. also using the simpler pv-dbow mode (dm=0) may help with smaller corpuses/documents. 
the values reported by most_similar() are not similarity ""percentages"". they're cosine-similarity values, from -1.0 to 1.0, and their absolute values are less important than the relative ranks of different results. so it shouldn't matter if there are a lot of results with >0.9 similarities ï¿½ï¿½ï¿½ as long as those documents are more like the query document than those lower in the rankings.
looking at the individual documents suggested as most-similar is thus the real test. if they seem like nonsense, it's likely there are problems with your data  or its preparation, or training parameters. 
for datasets with sufficient, real natural-language text, it's typical for higher min_count values to give better results. real text tends to have lots of low-frequency words that don't imply strong things without many more examples, and thus keeping them during training serves as noise making the model less strong.",https://stackoverflow.com/questions/52286330,python,12-09-2018 01:44,1327.0,2.0,2.0,True,01-10-2022 14:31,01-10-2022 14:31
66473977,document layout analysis for text extraction,"i need to analyze the layout structure of different documents type like: pdf, doc, docx, odt etc.
my task is:
giving a document, group the text in blocks finding the correct boundaries of each.
i did some tests using apache tika, which is a good extractor, it is a very good tool but it often mess up the order of the block, let me explain a bit what i mean with order.
apache tika just extracts the text, so if my document has two columns, tika extracts the entire text of the first column and then the text of the second column, which is ok...but sometimes the text on the first column is related to the text  on the second, like a table that has row relation.
so i must take care of the positions of each block, so the problems are:

define the box boundaries, which is hard... i should understand if a sentence is starting a new block or not.

define the orientation, for example, giving a table the ""sentence"" should be the row, not the column.


so basically here i have to deal with the layout structure to correcly understand the block boundaries.
i give you a visual example:

a classical extractor returns:
2019
2018
2017
2016
2015
2014
oregon arts commission individual artist fellowship...

which is wrong (in my case) because the dates are related to the texts on the right.
this task is preparatory for other nlp analysis, so it is very important, because, for example doing, when i need to recognize the entities(ner) inside the text, and then identify their relations, working with the correct context is very important.
how to extract the text from the document and assembly related pieces of text (understanding the layout structure of the document) under the same block?","['python', 'machine-learning', 'nlp', 'artificial-intelligence', 'document-layout-analysis']",66550563,"this is but a partial solution to your issue, but it may simplify the task at hand.
this tool receives pdf files and converts them to text files. it works pretty fast and can run on bulks of files.
it creates an output text file for each pdf. the advantage of this tool over others is that the output texts are aligned with accordance to their original layout.
for example, this is a resume with complex layout:

the output for it is the following text file:
christopher                         summary
                                    senior web developer specializing in front end development.
morgan                              experienced with all stages of the development cycle for
                                    dynamic web projects. well-versed in numerous programming
                                    languages including html5, php oop, javascript, css, mysql.
                                    strong background in project management and customer
                                    relations.


                                    skill highlights
                                        ï¿½ï¿½ï¿½   project management          ï¿½ï¿½ï¿½   creative design
                                        ï¿½ï¿½ï¿½   strong decision maker       ï¿½ï¿½ï¿½   innovative
                                        ï¿½ï¿½ï¿½   complex problem             ï¿½ï¿½ï¿½   service-focused
               

                                    experience
contact
                                    web developer - 09/2015 to 05/2019
address:                            luna web design, new york
177 great portland street, london      ï¿½ï¿½ï¿½ cooperate with designers to create clean interfaces and
w5w 6pq                                   simple, intuitive interactions and experiences.
                                       ï¿½ï¿½ï¿½ develop project concepts and maintain optimal
phone:                                    workflow.
+44 (0)20 7666 8555
                                       ï¿½ï¿½ï¿½ work with senior developer to manage large, complex
                                          design projects for corporate clients.
email:
                                       ï¿½ï¿½ï¿½ complete detailed programming and development tasks
christoper.m@gmail.com
                                          for front end public and internal websites as well as
                                          challenging back-end                                       ï¿½ï¿½ï¿½ carry out quality assurance tests to discover errors and
linkedin.com/christopher.morgan
                                          optimize usability.

languages                           education
spanish ï¿½ï¿½ï¿½ c2
                                    bachelor of science: computer information systems - 2014
chinese ï¿½ï¿½ï¿½ a1
                                    columbia university, ny
german ï¿½ï¿½ï¿½ a2


hobbies                             certifications
                                    php framework (certificate): zend, codeigniter, symfony.
   ï¿½ï¿½ï¿½   writing
                                    programming languages: javascript, html5, php oop, css,
   ï¿½ï¿½ï¿½   sketching
                                    sql, mysql.
   ï¿½ï¿½ï¿½   photography
   ï¿½ï¿½ï¿½   design
-----------------------page 1 end-----------------------

now your task is reduced to finding the bulks within a text file, and using the spaces between words as alat finds the margin between to columns of text and yields rhs and lhs - the text stream of the right and left columns respectively.
import numpy as np
import matplotlib.pyplot as plt
import re

txt_lines = txt.split('\n')
max_line_index = max([len(line) for line in txt_lines])
padded_txt_lines = [line + "" "" * (max_line_index - len(line)) for line in txt_lines] # pad short lines with spaces
space_idx_counters = np.zeros(max_line_index)

for idx, line in enumerate(padded_txt_lines):
    if line.find(""-----------------------page"") >= 0: # reached end of page
        break
    space_idxs = [pos for pos, char in enumerate(line) if char == "" ""]
    space_idx_counters[space_idxs] += 1

padded_txt_lines = padded_txt_lines[:idx] #remove end page line

# plot histogram of spaces in each character column
plt.bar(list(range(len(space_idx_counters))), space_idx_counters)
plt.title(""number of spaces in each column over all lines"")
plt.show()

# find the separator column idx
separator_idx = np.argmax(space_idx_counters)
print(f""separator index: {separator_idx}"")
left_lines = []
right_lines = []

# separate two columns of text
for line in padded_txt_lines:
    left_lines.append(line[:separator_idx])
    right_lines.append(line[separator_idx:])

# join each bulk into one stream of text, remove redundant spaces
lhs = ' '.join(left_lines)
lhs = re.sub(""\s{4,}"", "" "", lhs)
rhs = ' '.join(right_lines)
rhs = re.sub(""\s{4,}"", "" "", rhs)

print(""************ left hand side ************"")
print(lhs)
print(""************ right hand side ************"")
print(rhs)

plot output:

text output:
separator index: 33
************ left hand side ************
christopher morgan contact address: 177 great portland street, london w5w 6pq phone: +44 (0)20 7666 8555 email: christoper.m@gmail.com linkedin: linkedin.com/christopher.morgan languages spanish ï¿½ï¿½ï¿½ c2 chinese ï¿½ï¿½ï¿½ a1 german ï¿½ï¿½ï¿½ a2 hobbies ï¿½ï¿½ï¿½   writing ï¿½ï¿½ï¿½   sketching ï¿½ï¿½ï¿½   photography ï¿½ï¿½ï¿½   design 
************ right hand side ************
   summary senior web developer specializing in front end development. experienced with all stages of the development cycle for dynamic web projects. well-versed in numerous programming languages including html5, php oop, javascript, css, mysql. strong background in project management and customer relations. skill highlights ï¿½ï¿½ï¿½   project management ï¿½ï¿½ï¿½   creative design ï¿½ï¿½ï¿½   strong decision maker ï¿½ï¿½ï¿½   innovative ï¿½ï¿½ï¿½   complex problem ï¿½ï¿½ï¿½   service-focused solver experience web developer - 09ate clean interfaces and simple, intuitive interactions and experiences. ï¿½ï¿½ï¿½ develop project concepts and maintain optimal workflow. ï¿½ï¿½ï¿½ work with senior developer to manage large, complex design projects for corporate clients. ï¿½ï¿½ï¿½ complete detailed programming and development tasks for front end public and internal websites as well as challenging back-end server code. ï¿½ï¿½ï¿½ carry out quality assurance tests to discover errors and optimize usability. education bachelor of science: computer information systems - 2014 columbia university, ny certifications php framework (certificate): zend, codeigniter, symfony. programming languages: javascript, html5, php oop, css, sql, mysql. 

the next step would be to generalize this script to work on multi-page documents, remove redundant signs, etc",https://stackoverflow.com/questions/66473977,python,04-03-2021 11:20,5819.0,6.0,5.0,True,15-07-2022 12:53,15-07-2022 12:53
77570838,number of tokens exceeded maximum limit,"i am using the llama2 quantized model from huggingface and loading it using ctransformers from langchain. when i run the query, i got the below warning
number of tokens (512) exceeded maximum context length (512)
below is my code:
from langchain.llms import ctransformers
llm = ctransformers(model='models_k/llama-2-7b-chat.ggmlv3.q2_k.bin',
                      model_type='llama',
                      config={'max_new_tokens': 512,
                              'temperature': 0.01}
                      )

b_inst, e_inst = ""[inst]"", ""[/inst]""
b_sys, e_sys = ""<<sys>>\n"", ""\n<</sys>>\n\n""

default_system_prompt=""""""\
you are a helpful, respectful and honest assistant. always answer as helpfully as possible. 
please ensure that your responses are socially unbiased and positive in nature.

if a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. 
if you don't know the answer to a question, please don't share false information.""""""

instruction = db_schema + "" based on the database schema provided to you \n convert the following text from natural language to sql query: \n\n {text} \n only display the sql query""

system_prompt = b_sys + default_system_prompt + e_sys

template = b_inst + system_prompt + instruction + e_inst

prompt = prompttemplate(template=template, input_variables=[""text""])
llm_chain=llmchain(prompt=prompt, llm=llm)
print(llm_chain.run(""list the names and prices of electronic products that cost less than $500.""))

can anyone tell me why am i getting this error? do i have to change the settings?","['langchain', 'large-language-model', 'llama', 'ctransformers']",77576262,"you can fix this by the suggestion: context length.
code like here:
llm = ctransformers(model='models_k/llama-2-7b-chat.ggmlv3.q2_k.bin',
                      model_type='llama',
                      config={'max_new_tokens': 600,
                              'temperature': 0.01,
                              'context_length': 700}
                      )",https://stackoverflow.com/questions/77570838,langchain,29-11-2023 11:39,5109.0,4.0,1.0,True,30-11-2023 18:47,30-11-2023 18:47
73299450,notfittederror: the tf-idf vectorizer is not fitted,"i've trained a sentiment analysis classifier using tripadvisor's textual reviews datasets. it can predict the input textual reviews' rating based on sentiment. everything is ok with the training and testing.
however, when i loaded the classifier in a new .ipynb file and tried to use a review for prediction, i get
 notfittederror: the tf-idf vectorizer is not fitted** arises. 

this is the detailed error:
---------------------------------------------------------------------------
notfittederror                            traceback (most recent call last)
/var/folders/rn/vqtp35xn15zd9d5scq3rxsth0000gn/t/ipykernel_71297/777236349.py in <module>
----> 1 prediction(test_str,hotelmodel1000)

/var/folders/rn/vqtp35xn15zd9d5scq3rxsth0000gn/t/ipykernel_71297/1165328373.py in prediction(text, model)
      4     cw = clean_string(text)
      5     cw = tokenize(cw)
----> 6     cw = tfidf_vectorizer.transform([cw])
      7     result = model.predict(cw)
      8     print(""expected rating:"",int(result))

~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py in transform(self, raw_documents, copy)
   1869             tf-idf-weighted document-term matrix.
   1870         """"""
-> 1871         check_is_fitted(self, msg='the tf-idf vectorizer is not fitted')
   1872 
   1873         # fixme remove copy parameter support in 0.24

~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py in inner_f(*args, **kwargs)
     71                           futurewarning)
     72         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})
---> 73         return f(**kwargs)
     74     return inner_f
     75 

~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py in check_is_fitted(estimator, attributes, msg, all_or_any)
   1018 
   1019     if not attrs:
-> 1020         raise notfittederror(msg % {'name': type(estimator).__name__})
   1021 
   1022 

notfittederror: the tf-idf vectorizer is not fitted


here is my code to predict:
hotelmodel = pickle.load(open('./models/tripadvisorhotels_svm_model1000(2).pickle','rb'))
test_str = input('')
prediction(test_str,hotelmodel)

here is prediction() i called:
tfidf_vectorizer = tfidfvectorizer(max_features=5000,ngram_range=(2,2))

def prediction(text,model):
    cw = clean_string(text)
    cw = tokenize(cw)
    cw = tfidf_vectorizer.transform([cw])
    result = model.predict(cw)
    print(""expected rating:"",int(result)) 
    print(""\nthe confidence of the prediction is:"",model.predict_proba(cw)[0][int(result)-1])","['python', 'machine-learning', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']",73301712,"as mentioned in the comment,
you have correctly loaded the trained model from pickle file.
hotelmodel = pickle.load(open('./models/tripadvisorhotels_svm_model1000(2).pickle','rb'))

it can do the prediction because you saved the fitted version.
similarly, tfidf_vectorizer also need the fitted version.
you have to pickle the tfidf_vectorizer fitted version, then load from pickle to use it.
if you are using the svm based model, keep an eye with vectorizer length for fine tuning.",https://stackoverflow.com/questions/73299450,python,10-08-2022 00:32,2983.0,-1.0,1.0,True,10-08-2022 19:24,10-08-2022 19:24
78690284,oobabooga-textgen-web-ui how to get authorization to view model list from port 5000 via the ooba&#39;s api-key in python,"i wish to extract and print out a list of llm models from the oobabooga-text-gen-web-ui in python.
some context first before i head over to my problem.
for those familiar with what ooba is its essentially a gradio web ui for large language models.
i downloaded and loaded a few llm models onto this web ui. the web ui uses  to display the web user interface on port 7860.
but if i enable the openai and api extensions, and also edit the cmd_flags.txt within the ooba folder into something like:
--listen --api --api-key ""enter-your-fake-api-key-here"" 

the extensions will mimic an open ai api key by connecting to ooba from a network via port 5000
here come the problem...
this is my code to view the model list from ooba :
import requests

url = ""

#model list
headers = {
    ""content-type"": ""application/json""
}

response = requests.get(f'{url}/internal/model/list',
                        headers=headers,
                        verify=false)
print(response.json())

the output should look something like this:
{'model_names': ['l3-8b-stheno-v3.2', 'l3-8b-stheno-v3.2_exl2_8h_8bpw', 'l3-8b-stheno-v3.2_q8_0.gguf', 'mixtao-7bx2-moe-v8.1_q8_0.gguf']}

but instead i got this:
{'detail': 'unauthorized'}

after some fiddling around i found that if i leave cmd_flags.txt blank the code works as intended and i get the model lists but i dont have access to an api key since its not enable on ooba.
if i do enable it with cmd_flags.txt and typing:
--listen --api --api-key ""enter-your-fake-api-key-here"" 

i'll have access to the openai api key but the code to extract the model list will return as :
{'detail': 'unauthorized'}
i need the api key enabled from ooba cuz i plan to use the openai.client feature for model interactions.
how do i keep the configuration that enables the fake open ai api key but also allows me to extract the model list?","['python', 'openai-api', 'large-language-model']",78699532,"1 day later... found the answer.
turns out if i want the api key to be enabled while also being able to view the model list from ooba via the api, i need to add an 'authorization': f'bearer {api_key}' in the header.
the code should look something like this:
import requests

api_key = ""enter-your-fake-api-key-here""
url = ""

#model list
headers = {
    ""content-type"": ""application/json"",
    ""authorization"": f""bearer {api_key}""
}

response = requests.get(f'{url}/internal/model/list',
                        headers=headers,
                        verify=false)
print(response.json())",https://stackoverflow.com/questions/78690284,python,01-07-2024 02:42,957.0,1.0,1.0,True,02-07-2024 23:18,02-07-2024 23:14
77178058,how to set safety parameters for text generation model in google cloud vertex ai?,"i am working on a research project where i need to summarize news articles using the google palm2 text generation model. i have encountered an issue with certain news articles in my dataset where i'm getting empty responses along with safety attributes that block the output. here is the code i'm using:
from vertexai.language_models import textgenerationmodel
parameters = {  # default values
    'max_output_tokens': 256,
    'temperature': 0.0,
    'top_p': 1.0,
    'top_k': 40,
}
prompt = ""...""
model = textgenerationmodel.from_pretrained('text-bison@001')
response = model.predict(
    prompt,
    **parameters,
)

the following is an example prediction:
prediction(predictions=[{'content': '', 'citationmetadata': none, 'safetyattributes': {'blocked': true, 'errors': [253.0]}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=none)

the issue seems to be related to safety parameters preventing the model from generating a summary for certain news articles. i've been trying to find documentation on how to configure these safety parameters using the python api, but i could not locate the relevant information.
could someone please provide guidance on how to set the safety parameters for the textgenerationmodel? any help or pointers to documentation would be greatly appreciated. thank you!","['python', 'machine-learning', 'nlp', 'google-cloud-vertex-ai', 'large-language-model']",77181428,"i'm not sure about vertex ai but you can set the safety_settings of the palm model (from google generative ai) by the following:
import google.generativeai as palm

completion = palm.generate_text(
    model=model,
    prompt=prompt,
    safety_settings=[
        {
            ""category"": safety_types.harmcategory.harm_category_derogatory,
            ""threshold"": safety_types.harmblockthreshold.block_none,
        },
        {
            ""category"": safety_types.harmcategory.harm_category_violence,
            ""threshold"": safety_types.harmblockthreshold.block_none,
        },
    ]
) 

you should checkout this guide to get complete details of the safety catalogue and how to set threshold for each category as there are multiple categories and different threshold levels.
note: to use the palm api from generative ai, you'd need to install it first via:
pip install -q google-generativeai

and then set an api key which you'll get from here:
import google.generativeai as palm
palm.configure(api_key='your_api_key')

and then to access the same text-bison-001 model:
models = [m for m in palm.list_models() if 'generatetext' in m.supported_generation_methods]
model = models[0].name # use this model on the first code snippet
print(model) # prints 'models/text-bison-001'",https://stackoverflow.com/questions/77178058,python,26-09-2023 07:58,2558.0,3.0,1.0,True,27-09-2023 09:03,27-09-2023 09:03
67949858,deal with dictionaries in python,"i am a begginer in nlp and i want to do a preprocessing in a dataset which has the following form
dataset = {'key1': [{ 'x1': '...', 'x2': '...', 'x3': '...' }], 'key2': [...], ....}

first of all i want to create a list which contains all the values of 'x2', such as
dataset_list = [[value of 'x2' of key1], [value of 'x2' of key2], ...]

could you please help you know can i do it?","['python', 'dictionary', 'nlp']",67949879,"each key of your dictionary has a list which contains a dict.

first, you access the list using dataset[key]

then, you access the dict using dataset[key][0], the first element of the list.

lastly, to access the value of x2 in this dictionary, you use dataset[key][0]['x2'].


putting this together into a list comprehension, you would get this:
dataset_list = [dataset[k][0]['x2'] for k in dataset]",https://stackoverflow.com/questions/67949858,python,12-06-2021 14:38,54.0,-1.0,1.0,True,12-06-2021 14:49,12-06-2021 14:49
66467748,spacy how do i make a matcher which is noun-noun without white space within it?,"i tried to make a matcher which could detect words like
'all-purpose'
i was trying to make a pattern like
pattern=[{'pos':'noun'}, {'orth':'-'},{'pos':'noun'}]
however, i realized that it only find the matches like
'all - purpose' with white space between tokens instead of 'all-purpose'.
how could i make a matcher like this?
it has to be a generalized pattern like noun-noun instead of
specific words like 'barak obama' as in the example in spacy documentation
best,","['nlp', 'tokenize', 'spacy']",66468894,"what exactly are you trying to match? using en_core_web_sm, ""all-purpose"" is three tokens and all has the adv pos tag for me. so that might be the issue with your match pattern. if you just want hyphenated words this might be a better match:
pattern = [{'is_alpha': true}, {'orth':'-'}, {'is_alpha': true}]

more generally, you are correct that your pattern will only match three tokens, though that doesn't require white space - it depends on how the tokenizer works. for example, that's has no spaces but is two tokens.
if you are finding hyphenated words that occur as one token and want to match them, you can use regular expressions in matcher rules. here's an example ofhow that would work from the docs:
pattern = [{""text"": {""regex"": ""deff?in[ia]tely""}}]

in your case it could just look like this:
pattern = [{""text"": {""regex"": ""-""}}]",https://stackoverflow.com/questions/66467748,nlp,04-03-2021 02:31,721.0,2.0,1.0,True,06-09-2022 09:31,06-09-2022 09:31
71680285,how to deploy a question answering bert model as a chat bot on ms teams,"i have a text2sql model (editsql:  which i have configured to take a sentence as input and return a sql query as output.
now, i want to deploy this program as a chat bot application in microsoft teams.
i understand there's microsoft bot framework that enables publishing a bot and the 3 options are described here.

however, i am not finding any of them suitable for my use case since i need to deploy a question-answering bot where the questions from users need to be sent to an external server like aws and the response from aws (could be an excel file) needs to be sent back to the user. multiple questions can be the part of a conversation, so the chat client should be able to mark start and end of a conversation.
my problem:

what are the basic steps of exposing a ml model via a server so that it can be queried in production.
what are the tools that will allow me to make a client on teams and a server for this model on aws?

please let me know if i should add more information on this.
thanks","['microsoft-teams', 'chatbot', 'bert-language-model']",71682612,"as you've seen, there are a bunch of tools/approaches to creating bots in the microsoft world, for teams or otherwise. underneath, these all use the bot framework, but you can develop directly (i.e. write code), or use a higher-level tool like bot framework composer - the choice is yours depending on your own internal skills. if you want to work with code directly, here are a bunch of bot samples, in multiple languages:  . for isntance, here is an example of integrating the microsoft qnamaker service into your bot: 
basically, if you go the development approach, your bot is just a web service. once it receives the message, it can call out to any other service behind the scenes. that means it can receive a message, call out to an aws service, receive the response, and send a reply to the user.
for multiple questions as part of a 'set' of chats, bot framework provides an idea called ""dialogs"" that should work for you.",https://stackoverflow.com/questions/71680285,microsoft-teams,30-03-2022 15:45,551.0,1.0,1.0,True,30-03-2022 18:51,30-03-2022 16:29
60833301,train huggingface&#39;s gpt2 from scratch : assert n_state % config.n_head == 0 error,"i am trying to use a gpt2 architecture for musical applications and consequently need to train it from scratch. after a bit of googling i found that the issue #1714 from huggingface's github already had ""solved"" the question. when i try the to run the propose solution :
from transformers import gpt2config, gpt2model

numlayer = 4
numhead = 4
sizereduction = 10 #the factor by which we reduce the size of the velocity argument.
velsize = int(np.floor(127/sizereduction)) + 1 
seqlen=40 #size of data sequences.
embedsize = 5 

config = gpt2config(vocab_size = velsize, n_positions = seqlen, n_embd = embedsize, n_layer = numlayer, n_ctx = seqlen, n_head = numhead)  
model = gpt2model(config)

i get the following error : 
traceback (most recent call last):

  file ""<ipython-input-7-b043a7a2425f>"", line 1, in <module>
    runfile('c:/users/cnelias/desktop/phd/swing project/code/script/gpt2.py', wdir='c:/users/cnelias/desktop/phd/swing project/code/script')

  file ""c:\users\cnelias\anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 786, in runfile
    execfile(filename, namespace)

  file ""c:\users\cnelias\anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  file ""c:/users/cnelias/desktop/phd/swing project/code/script/gpt2.py"", line 191, in <module>
    model = gpt2model(config)

  file ""c:\users\cnelias\anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 355, in __init__
    self.h = nn.modulelist([block(config.n_ctx, config, scale=true) for _ in range(config.n_layer)])

  file ""c:\users\cnelias\anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 355, in <listcomp>
    self.h = nn.modulelist([block(config.n_ctx, config, scale=true) for _ in range(config.n_layer)])

  file ""c:\users\cnelias\anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 223, in __init__
    self.attn = attention(nx, n_ctx, config, scale)

  file ""c:\users\cnelias\anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 109, in __init__
    assert n_state % config.n_head == 0

what does it mean and how can i solve it ?
also more generally, is there a documentation on how to do a forward call with the gpt2 ? can i define my own train() function or do i have to use the model's build-in function ? am i forced to use a dataset to do the training or can i feed it individual tensors ? 
i looked for it but couldn't find answer to these on the doc, but maybe i missed something.
ps : i already read the blogpost fron huggingface.co, but it omits too much informations and details to be usefull for my application.","['python', 'nlp', 'huggingface-transformers', 'transformer-model', 'gpt-2']",60833512,"i think the error message is pretty clear:

assert n_state % config.n_head == 0

tracing it back through the code, we can see

n_state = nx  # in attention: n_state=768

which indicates that n_state represents the embedding dimension (which is generally 768 by default in bert-like models). when we then look at the gpt-2 documentation, it seems the parameter specifying this is n_embd, which you are setting to 5. as the error indicates, the embedding dimension has to be evenly divisible through the number of attention heads, which were specified as 4. so, choosing a different embedding dimension as a multiple of 4 should solve the problem. of course, you can also change the number of heads to begin with, but it seems that odd embedding dimensions are not supported.",https://stackoverflow.com/questions/60833301,python,24-03-2020 14:42,950.0,0.0,1.0,True,29-11-2020 12:10,29-11-2020 12:10
78788751,what is the best practice to calculate global frequency of list of elements with exact orders in python within multiple pandas dataframe?,"let's say i have the following datafarme df1 corresponding to user1:
+-------------------+-------+--------+-------+-------+----------+----------------+
|      models       |  mae  |  mse   | rmse  | mape  | rï¿½ï¿½ score |  runtime [ms]  |
+-------------------+-------+--------+-------+-------+----------+----------------+
| linearregression  | 4.906 | 27.784 | 5.271 | 0.405 |  -6.917  | 0:00:43.387145 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|   random forest   | 2.739 | 10.239 |  3.2  | 0.231 |  -1.917  | 0:28:11.761681 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|      xgboost      | 2.826 | 10.898 | 3.301 | 0.234 |  -2.105  | 0:03:58.883474 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|   mlpregressor    | 5.234 | 30.924 | 5.561 | 0.43  |  -7.812  | 0:01:44.252276 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|        svr        | 5.061 | 29.301 | 5.413 | 0.417 |  -7.349  | 0:04:52.754769 |
+---------------+-------+--------+-------+-------+----------+----------------+
| catboostregressor | 2.454 | 8.823  | 2.97  | 0.201 |  -1.514  | 0:19:36.925169 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|   lgbmregressor   | 2.76  | 10.204 | 3.194 | 0.231 |  -1.907  | 0:04:51.223103 |
+-------------------+-------+--------+-------+-------+----------+----------------+


+-------------------+----------------------------------------------------------------------------------------------------------+
|      rank         |                                                          mae                                             | 
+-------------------+----------------------------------------------------------------------------------------------------------+
| top models(sorted)| [""catboostregressor"",""randomforest"",""lgbmregressor"", ""xgboost"",""linearregression"",""svr"",""mlpregressor""]  |  
+-------------------+----------------------------------------------------------------------------------------------------------+

i have following datafarme df2 corresponding to user2:
+-------------------+-------+--------+-------+-------+----------+----------------+
|      models       |  mae  |  mse   | rmse  | mape  | rï¿½ï¿½ score |  runtime [ms]  |
+-------------------+-------+--------+-------+-------+----------+----------------+
| linearregression  | 4.575 | 24.809 | 4.981 | 0.377 |  -6.079  | 0:00:45.055854 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|   random forest   | 2.345 | 8.065  | 2.84  | 0.199 |  -1.301  | 0:10:55.468473 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|      xgboost      | 2.129 | 7.217  | 2.686 | 0.179 |  -1.059  | 0:01:01.575033 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|   mlpregressor    | 4.414 | 23.477 | 4.845 | 0.363 |  -5.699  | 0:00:31.231719 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|        svr        | 4.353 | 22.826 | 4.778 | 0.357 |  -5.513  | 0:02:12.258870 |
+---------------+-------+--------+-------+-------+----------+----------------+
| catboostregressor | 2.281 | 7.671  | 2.77  | 0.189 |  -1.189  | 0:08:16.526615 |
+-------------------+-------+--------+-------+-------+----------+----------------+
|   lgbmregressor   | 2.511 |  9.18  | 3.03  | 0.212 |  -1.619  | 0:15:25.084937 |
+-------------------+-------+--------+-------+-------+----------+----------------+


+-------------------+----------------------------------------------------------------------------------------------------------+
|      rank         |                                                          mae                                             | 
+-------------------+----------------------------------------------------------------------------------------------------------+
| top models(sorted)| [""xgboost"",""catboostregressor"",""randomforest"",""lgbmregressor"",""linearregression"",""svr"",""mlpregressor""]  |  
+-------------------+----------------------------------------------------------------------------------------------------------+

let's say i have more datafarmes df1000 corresponding to user1000.
problem statement: i want to count how often each ranking order occurs across all users (for a given metric). (and then, sort the ranking orders by their counts, and, additionally, compute the percentage of how often each particular ranking order occurs (based on the counts).)
i want to rank models result (sorted over a specific column (e.g. mae ) iteratively and return the frequency of top models over all dfs (df1 till df1000). so this is not something i can easily reach using the:
df[""category""].value_counts()

we are interested in computing absolute\relative frequencies in final ranked table in expected output.
so definitely i need to transform and add the list of sorted models' names that'd be a list of strings.
possible transformation or aggregation stages from my understanding:

take each df and create the list of sorted model names based desired column or metric:
['model2','model7', 'model6', 'model5', 'model4', 'model3', 'model1' ]
including the name of users in the final transformed dataframe could also be useful (however i did not mention it in the following table in the expected output.)
computing absolute\relative frequencies and return as counts and freq(%) in final table

expected output:
+-------------------+----------------------------------------------------------------------------------------------------------+--------+---------+
|      rank         |                                                    mae                                                  |counts  |freq(%)  |
+-------------------+----------------------------------------------------------------------------------------------------------+--------+---------+
| top models(sorted)| [""catboostregressor"",""randomforest"",""lgbmregressor"", ""xgboost"",""linearregression"",""svr"",""mlpregressor""]   | 70     |   65%   |
| top models(sorted)| [""xgboost"",""catboostregressor"",""randomforest"",""lgbmregressor"",""linearregression"",""svr"",""mlpregressor""]   | 20     |   12%   |
| top models(sorted)|                                             ....                                                  | ....     |   ....  |
....
+-------------------+----------------------------------------------------------------------------------------------------------+--------+---------+

i also was thinking maybe i can use natural language processing (nlp) methods called tf-idf to handle this problem using:
# import required module
from sklearn.feature_extraction.text import tfidfvectorizer


potentially related posts i have checked:

how can i compute a histogram (frequency table) for a single series?
count the frequency that a value occurs in a dataframe column
efficient way to get frequency of elements in a pandas column of lists
calculate frequency of item in list
get the frequency of individual items in a list of each row of a column in a dataframe
count the frequency of elements in list of lists in python
what's the best alternative to using lists as elements in a pandas dataframe?
pandas - create dataframe with counts and frequency of elements
python: calculate pmf for list in pandas dataframe
frequency plot of a pandas dataframe
python & pandas - how to calculate frequency under conditions in columns in dataframe?","['python', 'pandas', 'dataframe', 'frequency', 'tf-idf']",78789737,"does this work?
import pandas as pd
from collections import counter

# i've left out the other metrics because they are irrelevant to your question, but you can add them
data1 = {
    'models': ['linearregression', 'random forest', 'xgboost', 'mlpregressor', 'svr', 'catboostregressor', 'lgbmregressor'],
    'mae': [4.906, 2.739, 2.826, 5.234, 5.061, 2.454, 2.76]
}
data2 = {
    'models': ['linearregression', 'random forest', 'xgboost', 'mlpregressor', 'svr', 'catboostregressor', 'lgbmregressor'],
    'mae': [4.575, 2.345, 2.129, 4.414, 4.353, 2.281, 2.511]
}
data3 = {
    'models': ['linearregression', 'random forest', 'xgboost', 'mlpregressor', 'svr', 'catboostregressor', 'lgbmregressor'],
    'mae': [4.575, 2.345, 2.129, 4.414, 4.353, 2.281, 2.511]
}


df1 = pd.dataframe(data1)
df2 = pd.dataframe(data2)
df3 = pd.dataframe(data3)

# add your 1000 data frames here if you want
dataframes = [df1, df2, df3]

concatenated_models_list = []

for df in dataframes:
    df_sorted = df.sort_values(by='mae', ascending=false)
    # concatenate the sorted-by-mae model names into one string, so that
    # you can group by it as a key later
    concatenated_models = ','.join(df_sorted['models'].tolist())
    concatenated_models_list.append(concatenated_models)

# union everything into a single dataframe
union_df = pd.dataframe(concatenated_models_list, columns=['top models(sorted)'])

grouped_df = union_df['top models(sorted)'].value_counts().reset_index()
grouped_df.columns = ['top models(sorted)', 'count']

grouped_df['freq(%)'] = (grouped_df['count'] / len(dataframes)) * 100

grouped_df


output
top models(sorted)  count   freq(%)
linearregression,mlpregressor,svr,lgbmregresso...   2   66.666667
mlpregressor,svr,linearregression,xgboost,lgbm...   1   33.333333",https://stackoverflow.com/questions/78788751,python,24-07-2024 13:47,159.0,1.0,2.0,True,05-08-2024 10:00,05-08-2024 01:01
79111733,"how to derive attributes/labels from short plain text descriptions? (ner, llm, ?)","how to derive attributes/labels from short plain text descriptions? (ner, llm, ?)
i have short product descriptions that iï¿½ï¿½ï¿½d like to transform into structured attributes.
example:
input:
ï¿½ï¿½ï¿½la lecciaia cabernet sauvignon 2017 ï¿½ï¿½ï¿½ red ï¿½ï¿½ï¿½ 750mlï¿½ï¿½ï¿½

output:
year = 2017

color = red

weight = 750

weight unit = ml

if everything was in this format it would be trivial to write a regular expression and be done with it, but there are many different formats and nuances. it is increasingly cumbersome to hard-code logic for each format. trying to create a generic solution i immediately run into issues with a ï¿½ï¿½ï¿½basicï¿½ï¿½ï¿½ approach:

there are several different data providers, and each has its own format. for the example above, another provider might use ï¿½ï¿½ï¿½(red) 2017 la lecciaia cabernet sauvignon 750 mlï¿½ï¿½ï¿½. even for a given provider, there may be multiple formats atrictly followed.

there are many ways of expressing particular components. as an example, weight might be expressed as any one of these: ï¿½ï¿½ï¿½1.5lï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½1 1/2 litersï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½1500mlï¿½ï¿½ï¿½, etc.

parts of the description may be confused for target components. there may be a white wine from a brand called ï¿½ï¿½ï¿½red head vineyardï¿½ï¿½ï¿½. a weight of ï¿½ï¿½ï¿½2000 mlï¿½ï¿½ï¿½ may be confused for a year, etc. iï¿½ï¿½ï¿½m only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues.

iï¿½ï¿½ï¿½d consider this more of a ï¿½ï¿½ï¿½nice to haveï¿½ï¿½ï¿½ but would be useful to be able to parse out even more detail like the algo would be smart enough to know that ï¿½ï¿½ï¿½la lecciaiaï¿½ï¿½ï¿½ is the brand and ï¿½ï¿½ï¿½cabernet sauvignonï¿½ï¿½ï¿½ is the grape variety. assuming this would take more up front work and harder to get right but if thereï¿½ï¿½ï¿½s a straigpose function that can accept a description from any format. i have little experience with nlp/artificial intelligence but suspect there are useful tools/algos i can leverage. i have 1,000+ example records that i could potentially use to train a model. something that can run locally would be preferred but not absolutely necessary.
iï¿½ï¿½ï¿½m not looking for a specific implementation but for guidance from anyone whoï¿½ï¿½ï¿½s worked on a similar problem. open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies.
appreciate any insight into approaches or suggested learning resources.

i've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical","['nlp', 'artificial-intelligence', 'large-language-model', 'named-entity-recognition']",79113907,"llm would work nicely for this.  i'v done similar tasks before and it worked nicely with minimal training.  just keep in mind that any of the statistical methods nlp / llm / ner will never be 100% accurate,  but for practical purposes i find llms to be more accurate then a custom soup of regular expressions.
for you task i would use a framework like langchain,  and the following prompt (note you might need to work on your prompt a bit this just an example).  when run with a model it will create an xml output which would be trivial to parse.  you can modify the prompt to create different type of outputs. but, personally i find xml working very well for me.
you are an ai language model designed to parse wine bottle descriptions into structured data. you will be given a wine bottle description, and your task is to extract the following components:

- **year**: the vintage year of the wine.
- **color**: the color of the wine (e.g., red, white, rosï¿½ï¿½).
- **weight**: the volume of the wbottle expressed as a number (e.g., 750, 1500).
- **weight unit**: the unit of measurement for the weight (e.g., ml, ml, l, liters).
- **brand**: the brand or producer of the wine.
- **grape variety**: the variety of grape used (e.g., cabernet sauvignon, merlot).

**instructions:**

- wine descriptions may come in various formats and may include additional or confusing information. carefully analyze the description to accurately extract the components.
- be cautious of potential ambiguities. for example:
  - a brand name may include words like ""red"" or ""white"" (e.g., ""red head vineyard"") which should not be confused with the wine color.
  - large numbers may represent weight (e.g., ""1500 ml"") rather than a year.
- **do not assume information not present in the description.** if a component is missing, you may leave the corresponding tag empty or omit it.

**output format:**

provide the extracted information in xml format, using the following structure:

<wine>
<year>{{year}}</year>
<color>{{color}}</color>
<weight>{{weight}}</weight>
<weightunit>{{weightunit}}</weightunit>
<brand>{{brand}}</brand>
<grapevariety>{{grapevariety}}</grapevariety>
</wine>

**examples:**

  1. **input:**

 `la lecciaia cabernet sauvignon 2017 ï¿½ï¿½ï¿½ red ï¿½ï¿½ï¿½ 750ml`

 **output:**



```xml
   <wine>
     <year>2017</year>
     <color>red</color>
     <weight>750</weight>
     <weightunit>ml</weightunit>
     <brand>la lecciaia</brand>
     <grapevariety>cabernet sauvignon</grapevariety>
   </wine>
   ```

   
   `red head vineyard chardonnay 2020 1.5l`

   **output:**

   <wine>
     <year>2020</year>
     <color></color>
     <weight>1.5</weight>
     <weightunit>l</weightunit>
     <brand>red head vineyard</brand>
     <grapevariety>chardonnay</gra;
   </wine>

 

    **task:**
    
    given the following wine description, extract the components and provide the output in xml format as specified.
    
    {win_description}

keep in mind that llms are not cheap to run.  but for this tasks given ambiguousness of the domain it is most likely the best choice.  for this particular task it would be 1/1000 of a penny per label using openai service.  you might find a cheaper model / provider.  however when working with llm it is very important to ensure accuracy first,  then optimize for costs.
the whole thing will probably take 1-2 hours to build for the intermediate llm developer.  if you are learning it may vary.  but this is a perfect project to learn about llms",https://stackoverflow.com/questions/79111733,nlp,21-10-2024 20:54,186.0,0.0,1.0,True,22-10-2024 15:18,22-10-2024 11:29
65246703,"how does max_length, padding and truncation arguments work in huggingface&#39; berttokenizerfast.from_pretrained(&#39;bert-base-uncased&#39;)?","i am working with text classification problem where i want to use the bert model as the base followed by dense layers. i want to know how does the 3 arguments work? for example, if i have 3 sentences as:
'my name is slim shade and i am an aspiring ai engineer',
'i am an aspiring ai engineer',
'my name is slim'

so what will these 3 arguments do? what i think is as follows:

max_length=5 will keep all the sentences as of length 5 strictly
padding=max_length will add a padding of 1 to the third sentence
truncate=true will truncate the first and second sentence so that their length will be strictly 5.

please correct me if i am wrong.
below is my code which i have used.
! pip install transformers==3.5.1

from transformers import berttokenizerfast

tokenizer = berttokenizerfast.from_pretrained('bert-base-uncased')

tokens = tokenizer.batch_encode_plus(text,max_length=5,padding='max_length', truncation=true)
  
text_seq = torch.tensor(tokens['input_ids'])
text_mask = torch.tensor(tokens['attention_mask'])","['python', 'deep-learning', 'pytorch', 'bert-language-model', 'huggingface-tokenizers']",65255500,"what you have assumed is almost correct, however, there are few differences.
max_length=5, the max_length specifies the length of the tokenized text. by default, bert performs word-piece tokenization. for example the word ""playing"" can be split into ""play"" and ""##ing"" (this may not be very precise, but just to help you understand about word-piece tokenization), followed by adding [cls] token at the beginning of the sentence, and [sep] token at the end of sentence. thus, it first tokenizes the sentence, truncates it to max_length-2 (if truncation=true), then prepend [cls] at the beginning and [sep] token at the end.(so a total length of max_length)
padding='max_length', in this example it is not very evident that the 3rd example will be padded, as the length exceeds 5 after appending [cls] and [sep] tokens. however, if you have a max_length of 10. the tokenized text corresponds to [101, 2026, 2171, 2003, 11754, 102, 0, 0, 0, 0], where 101 is id of [cls] and 102 is id of [sep] tokens. thus, padded by zeros to make all the text to the length of max_length
likewise, truncate=true will ensure that the max_length is strictly adhered, i.e, longer sentences are truncated to max_length only if truncate=true",https://stackoverflow.com/questions/65246703,python,11-12-2020 06:26,60899.0,27.0,1.0,True,24-11-2022 22:00,24-11-2022 22:00
73205546,"spacy, how not to remove &quot;not&quot; when cleaning the text with space","i use this spacy code to later apply it on my text, but i need the negative words to stay in the text like ""not"".
nlp = spacy.load(""en_core_web_sm"") 

def my_tokenizer(sentence): 
    return [token.lemma_ for token in tqdm(nlp(sentence.lower()), leave = false) if token.is_stop == false and token.is_alpha == true and  token.lemma_ ] 

whit this when i apply i get this as a result :
[hello, earphone, work]

however the original sentence was
hello,my earphones are still not working.

so, i would like to see the following sentence: [earphone, still, not, work]
thank you","['python', 'spacy', 'stop-words']",73208162,"""not"" is actually a stop word and in your code if a token is removed if it's a stopword. you can see this either by looking at the list of spacy stopwords 
""not"" in spacy.lang.en.stop_words.stop_words

or by looping over the tokens of your doc object
for tok in nlp(text.lower()):
  print(tok.text, tok.is_stop, tok.lemma_)

#hello false hello
#, false ,
#my true my
#earphones false earphone
#are true be
#still true still
#not true not
#working false work
#. false .

solution
to solve this, you should remove the target words such as ""not"" from the list of stop_words. you can do it this way: 
# spacy.lang.en.stop_words.stop_words.remove(""not"")
# or for multiple words use this
to_del_elements = {""not"", ""no""}
nlp.defaults.stop_words = nlp.defaults.stop_words - to_del_elements

then you can rerun your code and you'll get your expected results:
import spacy
#spacy.lang.en.stop_words.stop_words.remove(""not"")
to_del_elements = {""not"", ""no""}
nlp.defaults.stop_words = nlp.defaults.stop_words - to_del_elements
nlp = spacy.load(""en_core_web_sm"") 
def my_tokenizer(sentence): 
    return [token.lemma_ for token in tqdm(nlp(sentence.lower()), leave = false) if token.is_stop == false and token.is_alpha == true and  token.lemma_ ] 

sentence = ""hello,my earphones are still not working. no way they will work""
results = my_tokenizer(sentence)
print(results)

#['hello', 'earphone', 'not', 'work', 'no', 'way', 'work']",https://stackoverflow.com/questions/73205546,python,02-08-2022 09:57,645.0,4.0,1.0,True,03-08-2022 21:26,02-08-2022 09:59
73078231,how to get all stop words from spacy and don&#39;t get any errors? typeerror: argument of type &#39;module&#39; is not iterable,"how to get all stop words from spacy.lang.en and don't get any errors?
from spacy.lang.en import stop_words as stop_words


def tokenize(sentence):
    sentence = nlp(sentence)
    # lemmatizing
    sentence = [ word.lemma_.lower().strip() if word.lemma_ != ""-pron-"" else word.lower_ for word in sentence ]
    # removing stop words
    sentence = [ word for word in sentence if word not in stop_words and word not in punctuations ]        
    return sentence

tokenize(""hallo ik ben leyla en "") and then i get 

then i got the following error and this is the error that i got
typeerror: argument of type 'module' is not iterable","['python', 'nlp', 'data-science', 'spacy']",73078527,"make sure stop_words and punctuations be a list or set and for getting a set of all stop_words from from spacy.lang.en import stop_words you can use stop_words.stop_words or as an alternative solution you can use nlp.defaults.stop_words.
import spacy
from string import punctuation
from spacy.lang.en import stop_words


nlp = spacy.load('en_core_web_sm')

stop_words = stop_words.stop_words
# print(stop_words)
# as an alternative solution
# stop_words = nlp.defaults.stop_words


punctuations = list(punctuation)
print(punctuations)
# ['!', '""', '#', '$', '%', '&', ""'"", '(', ')', '*', '+', '', '', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\', ']', '^', '_', '`', '{', '|', '}', '~']


def tokenize(sentence):
    sentence = nlp(sentence)
    # lemmatizing
    sentence = [ word.lemma_.lower().strip() if word.lemma_ != ""-pron-"" else word.lower_ for word in sentence ]
    # removing stop words
    sentence = [ word for word in sentence if word not in stop_words and word not in punctuations ]        
    return sentence


>>> tokenize(""hallo ik ben leyla en "")
['hallo', 'ik', 'ben', 'leyla', 'en']",https://stackoverflow.com/questions/73078231,python,22-07-2022 09:37,8343.0,4.0,1.0,True,12-01-2023 16:04,22-07-2022 10:28
76490589,valueerror when using model.fit even with the vectors being aligned,"i am attempting to build a naive bayes model for text classification.
here is a sample of the data i'm working with:
df_some_observations = filtered_training.sample(frac=0.0001)
df_some_observations.to_dict()

the output looks like this:
{'intitulï¿½ï¿½ (ce champ doit respecter la nomenclature suivante : code action ï¿½ï¿½ï¿½ libellï¿½ï¿½)_x': {40219: 'aegua00268 format oper scad htbhta fonction avance',
  16820: 'aeedf50490 sort conflit facon construct',
  24771: '4022mps192 prepar a lhabilit electr boho indic v personnel non elec',
  34482: '3095mceg73 affirmezvous relat professionnel bas ref 7114'},
 'nï¿½ï¿½ud parent au niveau n y compris moi-mï¿½ï¿½me.1': {40219: 'distribu electricit rel reseau electricit ecr exploit conduit reseau electricit',
  16820: 'ct competent transvers rhu ressourc humain for pilotag gestion format',
  24771: 'ss sant securit prevent prf prevent risqu professionnel hcp habilit certif perm prevent risqu meti',
  34482: 'nan'},
 'thï¿½ï¿½me de formation (chemin complet)': {40219: 'distribu electricit rel reseau electricit ecr exploit conduit reseau electricit',
  16820: 'ct competent transvers rhu ressourc humain for pilotag gestion format',
  24771: 'ss sant securit prevent prf prevent risqu professionnel hvent risqu meti',
  34482: 'in ingenier esp equip sous pression'},
 'description du champ supplï¿½ï¿½mentaire : objectifs de la formation': {40219: 'nan',
  16820: 'nan',
  24771: 'prepar a lhabilit electr boho indic v autoris special lissu cet format stagiair doit connaitr risqu electr savoir sen proteg doit etre capabl deffectu oper simpl dexploit suiv certain methodolog',
  34482: 'nan'},
 'objectifs': {40219: 'nan', 16820: 'nan', 24771: 'nan', 34482: 'nan'},
 'programme de formation': {40219: 'nan',
  16820: 'nan',
  24771: 'notion elementair delectricit sensibilis risqu electr prevent risqu electr publiqu utec 18 510 definit oper lenviron intervent tbt b appareillag electr bt materiel protect individuel collect manoeuvr mesurag essais verif outillag electr portat a main mis situat coffret didact',
  34482: 'nan'},
 'populations concernï¿½ï¿½es': {40219: 'nan',
  16820: 'nan',
  24771: 'personnel electricien effectu oper dordr electr',
  34482: 'nan'},
 'prï¿½ï¿½requis': {40219: 'nan',
  16820: 'nan',
  2nnel non electricien effectu oper simpl remplac fusibl rearm disjoncteur rel thermiqu',
  34482: 'nan'},
 ""description du champ supplï¿½ï¿½mentaire : commanditaire de l'action"": {40219: 'nan',
  16820: 'nan',
  24771: 'nan',
  34482: 'nan'},
 ""organisme dispensant l'action"": {40219: 'local sei',
  16820: 'intern edf',
  24771: 'intern edf',
  34482: 'intern edf'},
 'durï¿½ï¿½e thï¿½ï¿½orique (h)': {40219: 14.0, 24771: 11.0, 34482: 14.0},
 'coï¿½ï¿½t de la catï¿½ï¿½gorie coï¿½ï¿½t pï¿½ï¿½dagogique': {40219: 0.0,
  16820: 0.0,
  24771: 0.0,
  34482: 0.0},
 'coï¿½ï¿½t de la catï¿½ï¿½gorie coï¿½ï¿½t logistique': {40219: 0.0,
  16820: 0.0,
  24771: 0.0,
  34482: 0.0},

i started by splitting the data after removing some unnecessary columns:
(my target variable i""lang-py prettyprint-override"">df_training = filtered_training.sample(frac=0.8, random_state=42) 
df_test = filtered_training.drop(df_training.index)
x_train = df_training.iloc[:,:14]
y_train = df_training.iloc[:,15]
x_test = df_test.iloc[:,:14]
y_test = df_test.iloc[:,15]

when building the model with:
model = make_pipeline(tfidfvectorizer(), multinomialnb())
model.fit(x_train, y_train)
predicted_categories = model.predict(x_test)

i receive the following error when executing model.fit(x_train, y_train):
valueerror: found input variables with inconsistent numbers of samples: [14, 35478]

additional information that may be helpful:
np.shape(x_train) #(35478, 14)
np.shape(y_train) #(35478,)
np.shape(x_test) #(8870, 14)
np.shape(y_test) #(8870,)","['python', 'machine-learning', 'nlp', 'valueerror', 'naivebayes']",76514426,"i think that the main problem that tfidfvectorizer is able to work with one-dimensional text data only (as i see it from here). that's why when it tries to convert several columns with text data it tries to do it for column names for some reason.
in your case i see 2 ways how to solve this problem:

if you want to apply tfidfvectorizer for each column individually, it would be better to do it like this for example:

column_transformer = columntransformer([(x, tfidfvectorizer(), x) for x in x_train.columns]) # make sure that all columns contains text data
model = make_pipeline(column_transformer, multinomialnb())
model.fit(x_train, y_train)
predicted_categories = model.predict(x_test)


but if you want to apply one vocabulary for your columns, then i would recomment to do it like this:

nex_x_train = x_train.iloc[:,0]
for x in x_train.columns[1:]:
    nex_x_train = nex_x_train + ' ' + x_train[x]

nex_x_test = x_test.iloc[:,0]
for x in x_test.columns[1:]:
    nex_x_test = nex_x_test + ' ' + x_test[x]
    
model = make_pipeline(tfidfvectorizer(), multinomialnb())
model.fit(nex_x_train, y_train)
predicted_categories = model.predict(nex_x_test)",https://stackoverflow.com/questions/76490589,python,16-06-2023 13:17,163.0,4.0,1.0,True,20-06-2023 11:54,19-06-2023 11:29
66513144,pseudo labelling on text classification python,"i'm not good at machine learning. can someone tell me how to doing text classification with pseudo labeling in python? i never know the right implementation, i have searched everywhere in internet, but i give up as found anything :'( i just found the implementation for numeric datasets, but i found no implementation for text classification (vectorized text).. so i wrote this syntax, but i don't know whether my code is correct or not. am i doing wrong? please help me guys, i really need your help.. :'(
this is my datasets if you wanna try. i want to classify 'label' from 'content'
my steps are:

split data 0.75 unlabeled, 0.25 labeled
from 0.25 labeld i split: 0.75 train labeled, and 0.25 test labeled
make vectorizer for train, test and unlabeled datasets
build first model from train labeled, then labelling the unlabeled datasets
concatting train labeled data with prediction of unlabeled that have >0.99 (pseudolabeled), and make the second model
remove pseudolabeled from unabeled datasets
predict the remaining unlabeled from second model, then iterate step 3 until the probability of predicted pseudolabeled <0.99.

this is my code:
performing pseudo labelling on text classification
from sklearn.naive_bayes import multinomialnb

# initiate iteration counter
iterations = 0

# containers to hold f1_scores and # of pseudo-labels
train_f1s = []
test_f1s = []
pseudo_labels = []

# assign value to initiate while loop
high_prob = [1] 

# loop will run until there are no more high-probability pseudo-labels
while len(high_prob) > 0:
    
    # set the vector transformer (from data train)
    columntransformer = columntransformer([
    ('tfidf',tfidfvectorizer(stop_words=none, max_features=100000),
     'content')
    ],remainder='drop')

    def transforms(series):
        before_vect = pd.dataframe({'content':series})
        vector_transformer = columntransformer.fit(pd.dataframe({'content':x_train}))
        return vector_transformer.transform(before_vect)

    x_train_df = transforms(x_train);
    x_test_df = transforms(x_test);
    x_unlabeled_df = transforms(x_unlabeled)
    
    # fit classifier and make train/test predictions
    nb = multinomialnb()
    nb.fit(x_train_df, y_train)
    y_hat_train = nb.predict(x_train_df)
    y_hat_test = nb.predict(x_test_df)

    # calculate and print iteration # and f1 scores, and store f1 scores
    train_f1 = f1_score(y_train, y_hat_train)
    test_f1 = f1_score(y_test, y_hat_test)
    print(f""iteration {iterations}"")
    print(f""train f1: {train_f1}"")
    print(f""test f1: {test_f1}"")
    train_f1s.append(train_f1)
    test_f1s.append(test_f1)
   
    # generate predictions and probabilities for unlabeled data
    print(f""now predicting labels for unlabeled data..."")

    pred_probs = nb.predict_proba(x_unlabeled_df)
    preds = nb.predict(x_unlabeled_df)
    prob_0 = pred_probs[:,0]
    prob_1 = pred_probs[:,1]

    # store predictions and probabilities in dataframe
    df_pred_prob = pd.dataframe([])
    df_pred_prob['preds'] = preds
    df_pred_prob['prob_0'] = prob_0
    df_pred_prob['prob_1'] = prob_1
    df_pred_prob.index = x_unlabeled.index
    
    # separate predictions with > 99% probability
    high_prob = pd.concat([df_pred_prob.loc[df_pred_prob['prob_0'] > 0.99],
                           df_pred_prob.loc[df_pred_prob['prob_1'] > 0.99]],
                          axis=0)
    
    print(f""{len(high_prob)} high-probability predictions added to training data."")
    
    pseudo_labels.append(len(high_prob))

    # add pseudo-labeled data to training data
    x_train = pd.concat([x_train, x_unlabeled.loc[high_prob.index]], axis=0)
    y_train = pd.concat([y_train, high_prob.preds])      
    
    # drop pseudo-labeled instances from unlabeled data
    x_unlabeled = x_unlabeled.drop(index=high_prob.index)
    
    print(f""{len(x_unlabeled)} unlabeled instances remaining.\n"")
    
    # update iteration counter
    iterations += 1

i think i'm doing something wrong.. because when i see the f1 scores it is decreasing. please help me guys :'( i'm stressed.
f1 scores image
=================edit=================
so i've search on journal, then i think that i've got misunderstanding about the concept of data splitting in pseudo-labelling.
i initially thought that, the steps starts from splitting the data into labeled and unlabeled data, then from that labeled data, it was splitted into train and test.
but after surfing and searching, i found in this journal that my steps is incorrect. this journal says that the steps pseudo-labeling should start from splitting the data into train and test sets at first, and then from that train sets, data is splited to labeled and unlabeled datasets.
according to that journal, it reach the best result when splitting data into 90% of train sets and 10% of test sets. then, from that 90% train set, it is splitted into 20% labeled data and 80% unlabeled data sets. this journal trying evidence range from 0.7 till 0.9 as boundary to drop the pseudo labeling, and on that proportion of splitting, the best evidence threshold value is 0.74. so i fix my steps with that new proportion and 0.74 threshold, and i finally got the f1 scores is increasing. here are my steps:

split data 0.9 train, 0.1 test sets (i labeled the test sets, so i can measure the f1 scores)
from 0.9 train, i split: 0.2 labeled, and 0.8 unlabeled data
making vectorizer for x value of labeled train, test and unlabeled training datasets
build first model from labeled train, then labeling the unlabeled training datasets. then measure the f-1 scores according to the test sets (that already labeled).
concatting train labeled data with prediction of unlabeled that have probability > 0.74 (threshold based on journal). we call this new data as pseudo-labelled, likened to the actual label), and make the second model from new train data sets.
remove selected pseudo-labelled from unlabeled datasets
use the second model to predict the remaining of unlabeled data, then iterate step 3 until there are no probability of predicted pseudo-labelled>0.74
so the last model is the final.

my syntax is still the same, i just changing the split proportion and i finally got my f1 scores increasing through 4 iterations: my new f1 scores.
am i doing something right? thank you for all of your attention guys.. so much thank you..","['python', 'text-classification', 'semisupervised-learning']",66532861,"i'm not good at machine learning.

overall i would say that you are quite good at machine learning: semi-supervised learning is an advanced type of problem and i think your solution is quite good. at least the general principle seems correct, but it's difficult to say for sure (i don't have time to analyze the code in detail sorry). a few comments:

one thing which might be improvable is the 0.74 threshold: this value certainly depends on the data, so you could do your own experiment by trying different threshold values and selecting the one which works best with your data.
preferably it would be better to keep a final test set aside and use a separate validation set during the iterations. this would avoid the risk of data leakage.
i'm not sure about the stop condition for the loop. it might be ok but it might be worth trying other options:

simply iterate a fixed number of times (for instance 10 times).
the stop condition could be based on ""no more f1-score improvement"" (i.e. stabilization of the performance), but it's a bit more advanced.



it's pretty good anyway, my comments are just ideas if you want to improve further. note that it's been a long time since i've work with semi-supervised, i'm not sure i remember everything very well ;)",https://stackoverflow.com/questions/66513144,python,07-03-2021 04:17,1447.0,3.0,1.0,True,08-03-2021 15:46,08-03-2021 09:15
68149998,fast filtering of sentences in spacy,"i'm using spacy to divide a text into sentences, match a regex pattern on each sentence, and use some logic based on the results of the match. i started with a naive approach such as:
nlp = spacy.load(""en_core_web_trf"")
regex = re.compile(r'\b(foo|bar)\b')

for text in texts_list:
  doc = nlp(text)
  for sent in doc.sents:
    if re.search(regex, str(s)):
    [...]
    else:
    [...]

and it was very slow. then i used a pipe:
for doc in nlp.pipe(texts_list, disable=['tagger', 'ner', 'attribute_ruler', 'lemmatizer'], n_process=4):
  for sent in doc.sents:
    if re.search(regex, str(s)):
    [...]
    else:
    [...]

but it's still slow. am i missing something?","['python', 'nlp', 'spacy', 'transformer-model', 'sentence']",68153440,"a transformer model is overkill for splitting sentences and will be very slow. instead, a good option is the fast senter from an sm model:
import spacy
nlp = spacy.load(""en_core_web_sm"", disable=[""tok2vec"", ""tagger"", ""parser"", ""attribute_ruler"", ""lemmatizer"", ""ner""])
nlp.enable_pipe(""senter"")
for doc in nlp.pipe(texts, n_process=4):
    ...

the senter should work pretty well if your sentences end with punctuation. if you have a lot of run-on sentences without final punctuation, then the parser might do a better job. to run only the parser, keep the tok2vec and parser components from the original pipeline and don't enable the senter. the parser will be ~5-10x slower than the senter.
if you need this to be even faster, you can use the rule-based sentencizer (start from a blank en model), which is typically a bit worse than the senter because it only splits on the provided punctuation symbols.",https://stackoverflow.com/questions/68149998,python,27-06-2021 10:04,1860.0,1.0,1.0,True,27-06-2021 16:55,27-06-2021 10:26
76676065,cannot get jnius to work with chaquopy/android studio,"i am making a simple python script that utilizes openai and jnius(or as it's now known 'pyjnius) libraries with the intention of integrating with android studio.
i have the mainactivity.xml and everything working perfectly, internet access setup in the manifest etc.
my only issue is when asking chatgpt a question (in the emulator while running the app) i am getting a response in the app ui that the module jnius is not found.
i have pyjnius installed on my dev machine and this shows in visual studio codes interpreter and the like.
i have tried doing a pip install in the build.gradle but android studio wont install jnius that way (however it works perfectly for the openai library).
e.g.
python{
            pip{
                install ""jnius""
            }
        }

in my script.py in src/main/python i have this for my imports:
from jnius import autoclass
import openai

am i simply using jnius(pyjnius) incorrectly? or is it not compatible for use with chaquopy/android studio?
error as seen in emulator","['python', 'android-studio', 'openai-api', 'chaquopy']",76677299,"pyjnius is not compatible with chaquopy. you should use chaquopy's own api instead.
for simple cases, you may be able to simply replace from jnius import autoclass with from java import jclass as autoclass.",https://stackoverflow.com/questions/76676065,python,13-07-2023 05:00,191.0,0.0,1.0,True,13-07-2023 08:18,13-07-2023 06:48
65220447,add new named entity to spacy&#39;s en_core_web_sm model?,"i'm following the example here on training a new entity type:

it works fine when i don't pass an existing model to it, and will correctly create a new model, which recognizes my new named entity e.g.
 python.exe train-new-entity-type.py

it also works fine when i pass an existing model to it (created by running it once before), and will correctly load the model in the dir/my_model dir, which still recognizes my new named entity e.g.
 python.exe train-new-entity-type.py -m dir/my_model

however, i want to train a new entity type, and add that to spacy's existing model, so that spacy will recognize it's own supported named entities as well as my new entity type, so i tried:
 python.exe train-new-entity-type.py -m en_core_web_sm

however, this didn't seem to work. spacy's own supported named entities were recognized but they were not correct (vs just using the en_core_web_sm on its own without adding my new entity type to it), and my new entity type was no longer recognized at all.
am i doing something wrong? is this possible (adding named entities to en_core_web_sm)?","['python', 'spacy']",65231010,"read about the ""catastrophic forgetting"" problem when updating an existing model: 
it can be tricky to update an existing model, so it might be easier to train a separate model for your new entity type and add the ner component to the en_core_web_sm pipeline with a custom name. the main thing to watch out for is that you need to make sure the models are loaded with the same vocab so that you don't run into problems with the string store:
import spacy
nlp = spacy.load(""en_core_web_sm"")
custom_nlp = spacy.load(""my_model"", vocab=nlp.vocab)
nlp.add_pipe(custom_nlp.get_pipe(""ner""), name=""my_ner"", before=""ner"")

where you add it in the pipeline (before/after the existing ner) will determine which entity spans have priority, since the ner component won't modify existing entity spans.",https://stackoverflow.com/questions/65220447,python,09-12-2020 16:05,1846.0,1.0,1.0,True,10-12-2020 08:33,09-12-2020 17:58
78744294,scikit-learn&#39;s feature_names_in method,"a number of scikit-learn's classes have a feature_names_in method, which would be a real time saver if i could understand it better.  specifically, assume your x is a nested list of strings [['a', 'b', 'c'], ['a', 'b', 'd']] and your y is a list of labels ['blue', 'green'].  now, assume you are doing feature selection using, for example, the selectkbest class in scikit.  assume you choose the chi2 univariate approach and ask for the top 2 features (i.e., k=2) and you get your k_best_object.  now, that k_best_object has a method associated with it called feature_names_in which would be really helpful if it returned the ""names"" of the top 2 features.  the problem is that the documentation says that this method is only available when the features are entirely strings.  that would be fine, except for the fact that i haven't been able to get selectkbest (or other scikit classes) to work on strings. instead, i have only been able to get them to work by converting the x values into a numpy array of floats using tfidvectorizer (either count or tf-idf).  so, my question is... how would this method ever be used?  if it's only viable when all x input values are strings, but the only x it will take is floats, then how does this method ever apply?

to illustrate with code, if you try this:
x_t = [['land','building','cat'],['land','building','dog']]
y_t = ['blue', 'green']
chi_select_object_test = selectkbest(chi2, k=100)
chi_select_object_test.fit(x_t,y_t)

it won't work because the data consists of strings, not numbers. you get this error: valueerror: dtype='numeric' is not compatible with arrays of bytes/strings.convert your data to numeric values explicitly instead.
but, if you convert x_t to numbers using, for example, tfid vectorizer(), the class will work:

x_t = ['land building camp','land building dog']
tfidvectorizer_t = tfidfvectorizer(analyzer='word',stop_words= 'english')
x_t = tfidvectorizer_t.fit_transform(x_t)
y_t = ['blue', 'green']
chi_select_object_test = selectkbest(chi2, k=1)
chi_select_object_test.fit(x_t,y_t)

but, then, when you try and access the feature names attribute:
chi_select_object_test.feature_names_in_

you receive the error that: 'selectkbest' object has no attribute 'feature_names_in_'","['numpy', 'scikit-learn', 'tf-idf']",78747386,"i believe what you want to do is pass a pandas.dataframe to selectkbest. the dataframe includes column names that then become the feature names. in the end you can get the best according to the metric you passed by using get_feature_names_out.
in my silly example i generate a dataframe with 3 random columns and tell selectkbest that i want to predict if the 3rd column is bigger than 0.5. obviously we expect it to give us the 3rd column then.
import pandas as pd
import numpy as np
from sklearn.feature_selection import selectkbest, chi2

df = pd.dataframe(np.random.random((100,3)), columns=['a', 'b','c'])

selector = selectkbest(chi2, k=1)
selector.fit(df, df['c'] > 0.5)

selector.get_feature_names_out()

and indeed it returns
array(['c'], dtype=object)

finally feature_names_in is now set to array(['a', 'b', 'c'], dtype=object)  since those are the names of the features we put into the feature selector.",https://stackoverflow.com/questions/78744294,numpy,13-07-2024 15:40,152.0,2.0,1.0,True,14-07-2024 20:16,13-07-2024 21:45
56979376,error: could not find a version that satisfies the requirement preshed==2.0.1,"i am trying to install spacy on windows. i am running python 3.6.  when i run
pip install -u spacy
i get the following error:
looking in indexes: 
collecting spacy
  downloading                               /packages/spacy/2.1.4/spacy-2.1.4.tar.gz (29.8mb)
  installing build dependencies: started
  installing build dependencies: finished with status 'error'
  error: complete output from command 'c:\fast\python\3.6.4\python.exe' 'c:\fast\python\3.6.4\lib\site-packages\pip' install --ignore-installed --no-user --prefix 'c:\users\r419957\appdata\local\temp\pip-build-env-8u72uukr\overlay' --no-warn-script-location --no-binary :none: --only-binary :none: -i  -- setuptools 'wheel>0.32.0.<0.33.0' cython 'cymem>=2.0.2,<2.1.0' 'preshed>=2.0.1,<2.1.0' 'murmurhash>=0.28.0,<1.1.0' thinc==7.0.0.dev6:
  error: looking in indexes: 
  collecting setuptools
    downloading  (575kb)
  collecting wheel>0.32.0.<0.33.0
    downloading 
  collecting cython
    downloading  (1.6mb)
  collecting cymem<2.1.0,>=2.0.2
    downloading 
  collecting preshed<2.1.0,>=2.0.1
    error: could not find a version that satisfies the requirement preshed<2.1.0,>=2.0.1 (from versions: 1.0.0, 1.0.1)
  error: no matching distribution found for preshed<2.1.0,>=2.0.1
  ----------------------------------------
error: command ""'c:\fast\python\3.6.4\python.exe' 'c:\fast\python\3.6.4\lib\site-packages\pip' install --ignore-installed --no-user --prefix 'c:\users\r419957\appdata\local\temp\pip-build-env-8u72uukr\overlay' --no-warn-script-location --no-binary :none: --only-binary :none: -i  -- setuptools 'wheel>0.32.0.<0.33.0' cython 'cymem>=2.0.2,<2.1.0' 'preshed>=2.0.1,<2.1.0' 'murmurhash>=0.28.0,<1.1.0' thinc==7.0.0.dev6"" failed with error code 1 in none

i then tried to download preshed by itself by running
pip install preshed
this however does not install the version i need which is 2.0.1
when i run
pip install preshed==2.0.1
i get the following error
error: could not find a version that satisfies the requirement preshed==2.0.1 (from versions: 1.0.0, 1.0.1)
error: no matching distribution found for preshed==2.0.1

any help figuring out what is going wrong would be appreciated. thanks.","['python', 'python-3.x', 'pip', 'version', 'spacy']",56984255,"you have pip set up to look for packages in your private repository (my-artifact-repo.com) which is missing the package. either upload the preshed package (and its eventual dependencies) to the private repo, or install preshed from pypi:
$ pip install preshed --index-url=",https://stackoverflow.com/questions/56979376,python,10-07-2019 22:21,4286.0,0.0,1.0,True,22-02-2023 16:52,22-02-2023 16:52
76744966,frequency distribution is not returning words but letters,"i am trying to find what words appear the most often.
but each time i run freqdist it does not return the most common words but letters.
freqdist({' ': 496, 'e': 306, 't': 205, 'a': 182, 's': 181, 'n': 160, 'o': 146, 'r': 142, 'i': 118, 'l': 110, ...})
here is my code:
    newdf['tokens1'] = newdf['review'].apply(word_tokenize) newdf['tokens1'] = newdf['tokens1'].apply(str)
for i in range(newdf.shape[1]):
    # add each comment.
    review_comments = review_comments + newdf['tokens1'][i]

from nltk.probability import freqdist
fdist = freqdist(review_comments)
fdist

returns
freqdist({' ': 496, 'e': 306, 't': 205, 'a': 182, 's': 181, 'n': 160, 'o': 146, 'r': 142, 'i': 118, 'l': 110, ...})","['python', 'pandas', 'nltk', 'frequency-distribution']",76744996,"you need first yo use nltk.word_tokenize:
from nltk.tokenize import word_tokenize
tokens = nltk.word_tokenize(review_comments)
fdist = freqdist(tokens)
fdist",https://stackoverflow.com/questions/76744966,python,22-07-2023 17:08,119.0,0.0,1.0,True,22-07-2023 17:16,22-07-2023 17:09
78420651,create bio format to a sentence from a json file - to train ner model,"i have a json file that'll be used as data for a ner model.
it has a sentence and the relevant entities in that specific sentence.
i want to create a function that will generate a bio-labeled string for each sentence according to the entities
for example the following object from the json file
{
      ""request"": ""i want to fly to new york on the 13.3"",
      ""entities"": [
        {""start"": 16, ""end"": 23, ""text"": ""new york"", ""category"": ""destination""},
        {""start"": 32, ""end"": 35, ""text"": ""13.3"", ""category"": ""date""}
      ]
} 

""i want to fly to new york on the 13.3""
the corresponding bio label will be
""o o o o o b-destination i-destination o o b-date""
where b-category is the beginning of that category
i-category stands for inside and o for outside.
i'm looking for a python code to iterate on each object in the json file that will generate a bio-label for it.
change the json format if necessary","['python', 'machine-learning', 'named-entity-recognition']",78421095,"this is just a quick implementation for the above task, and many optimizations are possible, which can be explored later, but at first glace here is the function:
def bio_converter(r, entities):
    to_replace = {} # needed to maintain all the ner to be replaced
    for i in entities:
        sub = r[i['start']+1:i['end']+2].split(' ') # 1 indexed values in entities
        if len(sub) > 1:
            vals = [f""b-{i['category']}""] + ([f""i-{i['category']}""] * (len(sub)-1))
        else:
            vals = [f""b-{i['category']}""]

        to_replace = to_replace | dict(zip(sub,vals))

    r = r.split(' ')
    r = [to_replace[i] if i in to_replace else 'o' for i in r ]
    return ' '.join(r)

js = {
        ""request"": ""i want to fly to new york on the 13.3"",
        ""entities"": [
          {""start"": 16, ""end"": 23, ""text"": ""new york"", ""category"": ""destination""},
          {""start"": 32, ""end"": 35, ""text"": ""13.3"", ""category"": ""date""}
        ]
      }
bio_converter(js['request'], js['entities'])

should output:
o o o o o b-destination i-destination o o b-date",https://stackoverflow.com/questions/78420651,python,02-05-2024 16:56,127.0,0.0,1.0,True,03-05-2024 05:41,03-05-2024 05:41
71702731,"job type(full time , part time) detection with machine learning model in python","i have a dataset of jobs where i have columns ""title"" ,""description"" , ""city"" etc. and ""best jobs"" column. output of the dataset is ""best jobs"" where i have two outputs(yes , no) yes mean jobs are part time and no , mean job is full time. i want to train any machine learning model. firstly i want to train the model x or feature columns will be title , description etc. and label will be ""best jobs"". but i do not know how to train the model on string columns. please help me in this.
import numpy as np
import pandas as pd
import os, sys
from sklearn.preprocessing import minmaxscaler
from xgboost import xgbclassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

df = pd.read_csv(""machinelearning-new-best-gar-jobs.csv"", engine = 'python',encoding='mac_roman')
df.head()\


df['job description'].replace('  ', np.nan, inplace=true) df=df.dropna(subset=['job description']) df.isnull().sum()


then i will convert the label (bestjobs) to integer 1 and 0
df['bestjobs'] = (df['bestjobs']=='yes').astype(int)  # changing yes to 1 and no to 0 print(df['bestjobs'].value_counts())


i want to know which model should i apply to get it done.","['python', 'python-3.x', 'machine-learning', 'nlp']",71785552,"i think you probably can only use two columns ""job description"" and ""best job"" to train the model. then it becomes a text classification problem, like classifying movie reviews as either positive or negative. then you can preprocess the job description text and use a neural network to train your model.
the basic idea is that you may only need a few required features to train your model instead of processing all of the feature data you got. you can refer to this blog  (text preprocessing for nlp).
hope it is helpful for you!",https://stackoverflow.com/questions/71702731,python,01-04-2022 06:35,194.0,-1.0,1.0,True,07-04-2022 16:19,02-04-2022 10:25
74279532,limit the number of repetitive consecutive characters in a string,"i'm preprocessing tweets and need to set the maximum limit of the number of consecutive occurrences of ""@user"" to 3 times. for example, a tweet like this:

this tweet contains hate speech @user@user@user@user@user about a target group @user@user

after processing, should look like this:

this tweet contains hate speech @user@user@user about a target group @user@user

i was able to achieve the desired result with a while loop, however, i'm wondering if someone knows how to do it a simpler way. thanks!
tweets = [""this tweet contains hate speech @user@user@user@user@user about a target group @user@user""]

k = ""@user""
limit = 3
i = 0
for tweet in tweets: 
    tweet = tweet.split(' ')

    while i < len(tweet):
        if tweet[i].count(k) > limit:
            tweet[i] = k*int(limit)
            tweet = "" "".join(str(item) for item in tweet)
        i +=1

print(tweet)
# output: this tweet contains hate speech @user@user@user about a target group @user@user","['python', 'nlp', 'data-preprocessing']",74279686,"you can just use re to replace 4 or more occurrences of @user with three:
tweet = ""this tweet contains hate speech @user@user@user@user@user about a target group @user@user""
re.sub(r'(@user){4,}', r'@user@user@user', tweet)",https://stackoverflow.com/questions/74279532,python,01-11-2022 17:18,111.0,1.0,2.0,True,02-11-2022 06:19,02-11-2022 06:19
70922447,valueerror: the first argument to `layer.call` must always be passed,"i was trying to build a model with the sequential api (it has already worked for me with the functional api). here is the model that i'm trying to built in sequential api:
from tensorflow.keras import layers
model_1 = tf.keras.sequential([
    layers.input(shape=(1,), dtype='string'),
    text_vectorizer(),
    embedding(),
    layer.globalaveragepooling1d(),
    layers.dense(1, activation='sigmoid')
], name=""model_1_dense"")

error:
----> 4     text_vectorizer(),
      5     embedding(),
      6     layer.globalaveragepooling1d(),
valueerror: the first argument to `layer.call` must always be passed.

here is how text_vectorizer layer look like:
max_vocab_length = 10000
max_length = 15

text_vectorizer = textvectorization(max_tokens=max_vocab_length,
                                    output_mode=""int"",
                                    output_sequence_length=max_length)","['python', 'tensorflow', 'keras', 'deep-learning', 'nlp']",70922839,"the text_vectorizer layer should be passed to your model without parentheses. try something like this:
import tensorflow as tf

max_vocab_length = 10000
max_length = 15

text_vectorizer = tf.keras.layers.textvectorization(max_tokens=max_vocab_length,
                                    output_mode=""int"",
                                    output_sequence_length=max_length)

text_dataset = tf.data.dataset.from_tensor_slices([""foo"", ""bar"", ""baz""])
text_vectorizer.adapt(text_dataset.batch(64))
model_1 = tf.keras.sequential([
    tf.keras.layers.input(shape=(1,), dtype='string'),
    text_vectorizer,
    tf.keras.layers.embedding(max_vocab_length, 50),
    tf.keras.layers.globalaveragepooling1d(),
    tf.keras.layers.dense(1, activation='sigmoid')
], name=""model_1_dense"")

print(model_1(tf.constant([['foo']])))

tf.tensor([[0.48518932]], shape=(1, 1), dtype=float32)",https://stackoverflow.com/questions/70922447,python,31-01-2022 07:26,9561.0,2.0,1.0,True,07-05-2022 02:07,31-01-2022 09:24
73244442,huggingface trainer() cannot report to wandb,"i am trying to set trainer with arguments report_to to wandb, refer to this docs
with config:
training_args = trainingarguments(
    output_dir=""test_trainer"",
    evaluation_strategy=""steps"",
    learning_rate=config.learning_rate,
    num_train_epochs=config.epochs,
    weight_decay=config.weight_decay,
    logging_dir=config.logging_dir,
    report_to=""wandb"",
    save_total_limit=1,
    per_device_train_batch_size=config.batch_size,
    per_device_eval_batch_size=config.batch_size,
    fp16=true,
    load_best_model_at_end=true,
    seed=42
)

yet when i set trainer with:
trainer = trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics
)

it shows:
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
<ipython-input-68-b009351ab52d> in <module>
      4     train_dataset=train_dataset,
      5     eval_dataset=eval_dataset,
----> 6     compute_metrics=compute_metrics
      7 )

~/.virtualenvs/transformers_lab/lib/python3.7/site-packages/transformers/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)
    286                 ""you should subclass `trainer` and override the `create_optimizer_and_scheduler` method.""
    287             )
--> 288         default_callbacks = default_callbacks + get_reporting_integration_callbacks(self.args.report_to)
    289         callbacks = default_callbacks if callbacks is none else default_callbacks + callbacks
    290         self.callback_handler = callbackhandler(

~/.virtualenvs/transformers_lab/lib/python3.7/site-packages/transformers/integrations.py in get_reporting_integration_callbacks(report_to)
    794         if integration not in integration_to_callback:
    795             raise valueerror(
--> 796                 f""{integration} is not supported, only {', '.join(integration_to_callback.keys())} are supported.""
    797             )
    798     return [integration_to_callback[integration] for integration in report_to]

valueerror: w is not supported, only azure_ml, comet_ml, mlflow, tensorboard, wandb are supported.

have anyone got same error before?","['python', 'huggingface-transformers', 'wandb']",73258966,"although the documentation states that the report_to parameter can receive both list[str] or str i have always used a list with 1! element for this purpose.
therefore, even if you report only to wandb, the solution to your problem is to replace:
 report_to = 'wandb'

with
report_to = [""wandb""]",https://stackoverflow.com/questions/73244442,python,05-08-2022 03:48,2268.0,1.0,1.0,True,09-08-2022 08:29,09-08-2022 08:29
78145421,how to ensure that the langchain generates the correct output and is not random?,"i am using the below code and for the same question, it return different results, is there any way to fix that?
from langchain.chains import create_sql_query_chain
from langchain_openai import chatopenai
from langchain_community.utilities import sqldatabase
import os

def return_query(question)
   db = sqldatabase.from_uri(os.getenv(""postgres_url""))
   llm = chatopenai(model=""gpt-3.5-turbo"", temperature=0)
   chain = create_sql_query_chain(llm, db)
   response = chain.invoke({""question"": question})
   return response

example my question is ""create table student"" and i get the below responses on re-trying the same code:

response1: this table does not exist in the provided database schema.
response2: select * from information_schema.tables where table_name = 'student' limit 1;
response3: this question cannot be answered directly using the existing tables provided in the database schema. to create a new table named ""student"", you can use the following sql query:

create table student (
    id serial primary key,
    name text not null,
    email text not null,
    age integer,
    major text
);","['langchain', 'large-language-model']",78145784,"firstly, i'd like to point out that large language models (llms) produce non-deterministic outputs, meaning that for every query, you may get different results. this is not a bug, it is feature.
the two most crucial terms for achieving deterministic outcomes with llms are the ""prompt"" and the ""function calling"".
regarding your question, check the source code:


the sqldatabase class provides a get_table_info method that can be
used to get column information as well as sample data from the table.

this functionality does not extend to ""write"" operations. if you want your llm to produce the same result when you query ""create table"", you can use the create_sql_agent method found at  this method enables the creation of a tool (function calling in langchain) with a function that can generate a table in the database with the necessary parameters provided.",https://stackoverflow.com/questions/78145421,langchain,12-03-2024 07:56,686.0,-1.0,1.0,True,12-03-2024 09:01,12-03-2024 08:43
79502752,getting cuda out of memory when importing microsoft/orca-2-13b from hugging faces,"i am using ubuntu 24.04.1 on an aws ec2 instance g5.8xlarge.
i am receiving the following error message:
outofmemoryerror: allocation on device 

code:
import os
os.environ[""pytorch_cuda_alloc_conf""] = ""backend:cudamallocasync""
import torch
torch.cuda.empty_cache()
import transformers
    
if torch.cuda.is_available():
    torch.set_default_device(""cuda"")
    
device = torch.device(""cuda"")
    
model = transformers.automodelforcausallm.from_pretrained(""microsoft/orca-2-13b"", device_map=device)

full error:
/home/ubuntu/anaconda3/envs/ai/lib/python3.12/site-packages/torch/cuda/__init__.py:734: userwarning: can't initialize nvml
  warnings.warn(""can't initialize nvml"")

loadingï¿½ï¿½ï¿½checkpointï¿½ï¿½ï¿½shards:ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½33%
ï¿½ï¿½ï¿½2/6ï¿½ï¿½ï¿½[00:04<00:06,ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½1.72s/it]

/home/ubuntu/anaconda3/envs/ai/lib/p34: userwarning: can't initialize nvml
  warnings.warn(""can't initialize nvml"")

---------------------------------------------------------------------------
outofmemoryerror                          traceback (most recent call last)
cell in[5], line 6
      2     torch.set_default_device(""cuda"")
      4 device = torch.device(""cuda"")
----> 6 model = transformers.automodelforcausallm.from_pretrained(""microsoft/orca-2-13b"", device_map=device)
      8 # 
      9 # please use the slow tokenizer since fast and slow tokenizer produces different tokens
     10 tokenizer = transformers.autotokenizer.from_pretrained(
     11         ""microsoft/orca-2-13b"",
     12         use_fast=true,
     13     )

file ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564, in _baseautomodelclass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    562 elif type(config) in cls._model_mapping.keys():
    563     model_class = _get_model_class(config, cls._model_mapping)
--> 564     return model_class.from_pretrained(
    565         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    566     )
    567 raise valueerror(
    568     f""unrecognized configuration class {config.__class__} for this kind of automodel: {cls.__name__}.\n""
    569     f""model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.""
    570 )

file ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:262, in restore_default_torch_dtype.<locals>._wrapper(*args, **kwargs)
    260 old_dtype = torch.get_default_dtype()
    261 try:
--> 262     return func(*args, **kwargs)
    263 finally:
    264     torch.set_default_dtype(old_dtype)

file ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:4319, in pretrainedmodel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)
   4309     if dtype_orig is not none:
   4310         torch.set_default_dtype(dtype_orig)
   4312     (
   4313         model,
   4314         missing_keys,
   4315         unexpected_keys,
   4316         mismatched_keys,
   4317         offload_index,
   4318         error_msgs,
-> 4319     ) = cls._load_pretrained_model(
   4320         model,
   4321         state_dict,
   4322         loaded_state_dict_keys,  # xxx: rename?
   4323         resolved_archive_file,
   4324         pretrained_model_name_or_path,
   4325         ignore_mismatched_sizes=ignore_mismatched_sizes,
   4326         sharded_metadata=sharded_metadata,
   4327         _fast_init=_fast_init,
   4328         low_cpu_mem_usage=low_cpu_mem_usage,
   4329         device_map=device_map,
   4330         offload_folder=offload_folder,
   4331         offload_state_dict=offload_state_dict,
   4332         dtype=torch_dtype,
   4333         hf_quantizer=hf_quantizer,
   4334         keep_in_fp32_modules=keep_in_fp32_modules,
   4335         gguf_path=gguf_path,
   4336         weights_only=weights_only,
   4337     )
   4339 # make sure token embedding weights are still tied if needed
   4340 model.tie_weights()

file ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:4897, in pretrainedmodel._load_pretrained_model(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)
   4895     else:
   4896         fixed_state_dict = cls._fix_state_dict_keys_on_load(state_dict)
-> 4897         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
   4898             model_to_load,
   4899             fixed_state_dict,
   4900             start_prefix,
   4901             expected_keys,
   4902             device_map=device_map,
   4903             offload_folder=offload_folder,
   4904             offload_index=offload_index,
   4905             state_dict_folder=state_dict_folder,
   4906             state_dict_index=state_dict_index,
   4907             dtype=dtype,
   4908             hf_quantizer=hf_quantizer,
   4909             is_safetensors=is_safetensors,
   4910             keep_in_fp32_modules=keep_in_fp32_modules,
   4911             unexpected_keys=unexpected_keys,
   4912         )
   4913         error_msgs += new_error_msgs
   4914 else:
   4915     # sharded checkpoint or whole but low_cpu_mem_usage==true

file ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:896, in _load_state_dict_into_meta_model(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)
    893         param_device = ""cpu"" if is_local_dist_rank_0() else ""meta""
    895     # for backward compatibility with older versions of `accelerate` and for non-quantized params
--> 896     set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
    897 else:
    898     hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)

file ~/anaconda3/envs/ai/lib/python3.12/site-packages/accelerate/utils/modeling.py:330, in set_module_tensor_to_device(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)
    328             module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)
    329 elif isinstance(value, torch.tensor):
--> 330     new_value = value.to(device)
    331 else:
    332     new_value = torch.tensor(value, device=device)

file ~/anaconda3/envs/ai/lib/python3.12/site-packages/torch/utils/_device.py:104, in devicecontext.__torch_function__(self, func, types, args, kwargs)
    102 if func in _device_constructors() and kwargs.get('device') is none:
    103     kwargs['device'] = self.device
--> 104 return func(*args, **kwargs)

outofmemoryerror: allocation on device","['machine-learning', 'pytorch', 'huggingface-transformers', 'huggingface']",79504165,"you can check out information on the specific model here. but you can see it requires 52.1ï¿½ï¿½ï¿½gb of vram (gpu memory).
based on this table we see that you have 24gb of gpu memory. so it won't be able to fit. if you aren't able to get more gpu memory, you can look into quantized models.
you can check out the models on huggingface that have quantized versions, the gpu memory required, and the best use case.",https://stackoverflow.com/questions/79502752,machine-learning,12-03-2025 06:05,59.0,0.0,1.0,True,12-03-2025 16:00,12-03-2025 11:32
78347434,problem setting up llama-2 in google colab - cell-run fails when loading checkpoint shards,"i'm trying to use llama 2 chat (via hugging face) with 7b parameters in google colab (python 3.10.12). i've already obtain my access token via meta. i simply use the code in hugging face on how to implement the model along with my access token. here is my code:
!pip install transformers
 
from transformers import automodelforcausallm, autotokenizer
import torch

token = ""---token copied from hugging face and pasted here---""

tokenizer = autotokenizer.from_pretrained(""meta-llama/llama-2-7b-chat-hf"", token=token)
model = automodelforcausallm.from_pretrained(""meta-llama/llama-2-7b-chat-hf"", token=token)

it starts downloading the model but when it reaches loading checkpoint shards: it just stops running and there is no error:","['python', 'huggingface-transformers', 'large-language-model', 'llama']",78347722,"the issue is with colab instance running out of ram. based on your comments you are using basic colab instance with 12.7 gb cpu ram.
for llama model you'll need:

for the float32 model about 25 gb (but you'll need both cpu ram and same 25 gb gpu ram);
for the bfloat16 model around 13 gb (and still not enough to fit basic colab cpu instance, given that you'll also need to store intermediate calculations from the model);

check this link for the details on the required resources:
huggingface.co/nousresearch/llama-2-7b-chat-hf/discussions/3
also if you want only to do inference (predictions) on the model i would recommend to use it's quantized 4bit or 8bit versions. both can be ran on cpu and don't need a lot of memory.",https://stackoverflow.com/questions/78347434,python,18-04-2024 12:28,1431.0,3.0,1.0,True,20-10-2024 15:01,18-04-2024 12:34
72936945,pass elements of a list in a function,"i have a function that is able to create triples and relationships from text. however, when i create a list of a column that contains text and pass it through the function, it only processes the first row, or item of the list. therefore, i am wondering how the whole list can be processed within this function. maybe a for loop would work?
the following line contains the list
rez_dictionary = {'decent little reader, poor tablet',
 'ok for what it is',
 'too heavy and poor weld quality,',
 'difficult mount',
 'just got it installed'}

from transformers import pipeline

triplet_extractor = pipeline('text2text-generation', model='babelscape/rebel-large', tokenizer='babelscape/rebel-large')

# we need to use the tokenizer manually since we need special tokens.
extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(rez_dictionary, return_tensors=true, return_text=false)[0][""generated_token_ids""]])

print(extracted_text[0])

if anyone has a suggestion, i am looking forward for it.
would it also be possible to get the output adjusted to the following format:
# function to parse the generated text and extract the triplets
def extract_triplets(text):
    triplets = []
    relation, subject, relation, object_ = '', '', '', ''
    text = text.strip()
    current = 'x'
    for token in text.replace(""<s>"", """").replace(""<pad>"", """").replace(""</s>"", """").split():
        if token == ""<triplet>"":
            current = 't'
            if relation != '':
                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
                relation = ''
            subject = ''
        elif token == ""<subj>"":
            current = 's'
            if relation != '':
                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
            object_ = ''
        elif token == ""<obj>"":
            current = 'o'
            relation = ''
        else:
            if current == 't':
                subject += ' ' + token
            elif current == 's':
                object_ += ' ' + token
            elif current == 'o':
                relation += ' ' + token
    if subject != '' and relation != '' and object_ != '':
        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
    return triplets
extracted_triplets = extract_triplets(extracted_text[0])
print(extracted_triplets)","['python', 'huggingface-transformers']",72972107,"you are removing the other entries of rez_dictionary inside the batch_decode:
triplet_extractor(rez_dictionary, return_tensors=true, return_text=false)[0][""generated_token_ids""]

use a list comprehension instead:
from transformers import pipeline

rez = ['decent little reader, poor tablet',
 'ok for what it is',
 'too heavy and poor weld quality,',
 'difficult mount',
 'just got it installed']

triplet_extractor = pipeline('text2text-generation', model='babelscape/rebel-large', tokenizer='babelscape/rebel-large')

model_output = triplet_extractor(rez, return_tensors=true, return_text=false)

extracted_text = triplet_extractor.tokenizer.batch_decode([x[""generated_token_ids""] for x in model_output])
print(""\n"".join(extracted_text))

output:
<s><triplet> decent little reader <subj> poor tablet <obj> different from <triplet> poor tablet <subj> decent little reader <obj> different from</s>
<s><triplet> ok for what it is <subj> film <obj> instance of</s>
<s><triplet> too heavy and poor <subj> weld quality <obj> subclass of</s>
<s><triplet> difficult mount <subj> mount <obj> subclass of</s>
<s><triplet> 2008 summer olympics <subj> 2008 <obj> point in time</s>

regarding the extension of the op's question, op wanted to know how to run the function extract_triplets. op can simply do that via a for-loop:
for text in extracted_text:
  print(extract_triplets(text))

output:
[{'head': 'decent little reader', 'type': 'different from', 'tail': 'poor tablet'}, {'head': 'poor tablet', 'type': 'different from', 'tail': 'decent little reader'}]
[{'head': 'ok for what it is', 'type': 'instance of', 'tail': 'film'}]
[{'head': 'too heavy and poor', 'type': 'subclass of', 'tail': 'weld quality'}]
[{'head': 'difficult mount', 'type': 'subclass of', 'tail': 'mount'}]
[{'head': '2008 summer olympics', 'type': 'point in time', 'tail': '2008'}]",https://stackoverflow.com/questions/72936945,python,11-07-2022 10:14,154.0,2.0,1.0,True,20-07-2022 09:50,20-07-2022 09:50
77639714,big query - case statements and apostrophe&#39;s,"afternoon, was wondering if anyone is able to assist.
whilst writing case statements to flag particular phrases in netezza, if we came across verbatim which contained an apostrophe i would get this round this with the below example. however im struggling to recreate this on bq and wondered if anyone has a solution?
verbatim example : doesn't have an account
netezza case statement example : when lower(a.case_detail) like lower('%doesn''t have an account%')
thanks in advance for any helpful suggestions :)
currently not found a solution","['sql', 'google-cloud-platform', 'google-bigquery', 'nlp']",77639946,"you have at least two options:

escape the ' using backslash \: '%doesn\'t have an account%'
use double quotes to start and end the string literal: ""%doesn't have an account%""",https://stackoverflow.com/questions/77639714,sql,11-12-2023 12:48,106.0,0.0,1.0,True,11-12-2023 13:30,11-12-2023 12:49
71577525,huggingface sequence classification unfreezing layers,"i am using longformer for sequence classification - binary problem
i have downloaded required files
# load model and tokenizer and define length of the text sequence
model = longformerforsequenceclassification.from_pretrained('allenai/longformer-base-4096',
                                                           gradient_checkpointing=false,
                                                           attention_window = 512)
tokenizer = longformertokenizerfast.from_pretrained('allenai/longformer-base-4096', max_length = 1024)

then as shown here i ran the below code
for name, param in model.named_parameters():
     print(name, param.requires_grad)


longformer.embeddings.word_embeddings.weight true
longformer.embeddings.position_embeddings.weight true
longformer.embeddings.token_type_embeddings.weight true
longformer.embeddings.layernorm.weight true
longformer.embeddings.layernorm.bias true
longformer.encoder.layer.0.attention.self.query.weight true
longformer.encoder.layer.0.attention.self.query.bias true
longformer.encoder.layer.0.attention.self.key.weight true
longformer.encoder.layer.0.attention.self.key.bias true
longformer.encoder.layer.0.attention.self.value.weight true
longformer.encoder.layer.0.attention.self.value.bias true
longformer.encoder.layer.0.attention.self.query_global.weight true
longformer.encoder.layer.0.attention.self.query_global.bias true
longformer.encoder.layer.0.attention.self.key_global.weight true
longformer.encoder.layer.0.attention.self.key_global.bias true
longformer.encoder.layer.0.attention.self.value_global.weight true
longformer.encoder.layer.0.attention.self.value_global.bias true
longformer.encoder.layer.0.attention.output.dense.weight true
longformer.encoder.layer.0.attention.output.dense.bias true
longformer.encoder.layer.0.attention.output.layernorm.weight true
longformer.encoder.layer.0.attention.output.layernorm.bias true
longformer.encoder.layer.0.intermediate.dense.weight true
longformer.encoder.layer.0.intermediate.dense.bias true
longformer.encoder.layer.0.output.dense.weight true
longformer.encoder.layer.0.output.dense.bias true
longformer.encoder.layer.0.output.layernorm.weight true
longformer.encoder.layer.0.output.layernorm.bias true
longformer.encoder.layer.1.attention.self.query.weight true
longformer.encoder.layer.1.attention.self.query.bias true
longformer.encoder.layer.1.attention.self.key.weight true
longformer.encoder.layer.1.attention.self.key.bias true
longformer.encoder.layer.1.attention.self.value.weight true
longformer.encoder.layer.1.attention.self.value.bias true
longformer.encoder.layer.1.attention.self.query_global.weight true
longformer.encoder.layer.1.attention.self.query_global.bias true
longformer.encoder.layer.1.attention.self.key_global.weight true
longformer.encoder.layer.1.attention.self.key_global.bias true
longformer.encoder.layer.1.attention.self.value_global.weight true
longformer.encoder.layer.1.attention.self.value_global.bias true
longformer.encoder.layer.1.attention.output.dense.weight true
longformer.encoder.layer.1.attention.output.dense.bias true
longformer.encoder.layer.1.attention.output.layernorm.weight true
longformer.encoder.layer.1.attention.output.layernorm.bias true
longformer.encoder.layer.1.intermediate.dense.weight true
longformer.encoder.layer.1.intermediate.dense.bias true
longformer.encoder.layer.1.output.dense.weight true
longformer.encoder.layer.1.output.dense.bias true
longformer.encoder.layer.1.output.layernorm.weight true
longformer.encoder.layer.1.output.layernorm.bias true
longformer.encoder.layer.2.attention.self.query.weight true
longformer.encoder.layer.2.attention.self.query.bias true
longformer.encoder.layer.2.attention.self.key.weight true
longformer.encoder.layer.2.attention.self.key.bias true
longformer.encoder.layer.2.attention.self.value.weight true
longformer.encoder.layer.2.attention.self.value.bias true
longformer.encoder.layer.2.attention.self.query_global.weight true
longformer.encoder.layer.2.attention.self.query_global.bias true
longformer.encoder.layer.2.attention.self.key_global.weight true
longformer.encoder.layer.2.attention.self.key_global.bias true
longformer.encoder.layer.2.attention.self.value_global.weight true
longformer.encoder.layer.2.attention.self.value_global.bias true
longformer.encoder.layer.2.attention.output.dense.weight true
longformer.encoder.layer.2.attention.output.dense.bias true
longformer.encoder.layer.2.attention.output.layernorm.weight true
longformer.encoder.layer.2.attention.output.layernorm.bias true
longformer.encoder.layer.2.intermediate.dense.weight true
longformer.encoder.layer.2.intermediate.dense.bias true
longformer.encoder.layer.2.output.dense.weight true
longformer.encoder.layer.2.output.dense.bias true
longformer.encoder.layer.2.output.layernorm.weight true
longformer.encoder.layer.2.output.layernorm.bias true
longformer.encoder.layer.3.attention.self.query.weight true
longformer.encoder.layer.3.attention.self.query.bias true
longformer.encoder.layer.3.attention.self.key.weight true
longformer.encoder.layer.3.attention.self.key.bias true
longformer.encoder.layer.3.attention.self.value.weight true
longformer.encoder.layer.3.attention.self.value.bias true
longformer.encoder.layer.3.attention.self.query_global.weight true
longformer.encoder.layer.3.attention.self.query_global.bias true
longformer.encoder.layer.3.attention.self.key_global.weight true
longformer.encoder.layer.3.attention.self.key_global.bias true
longformer.encoder.layer.3.attention.self.value_global.weight true
longformer.encoder.layer.3.attention.self.value_global.bias true
longformer.encoder.layer.3.attention.output.dense.weight true
longformer.encoder.layer.3.attention.output.dense.bias true
longformer.encoder.layer.3.attention.output.layernorm.weight true
longformer.encoder.layer.3.attention.output.layernorm.bias true
longformer.encoder.layer.3.intermediate.dense.weight true
longformer.encoder.layer.3.intermediate.dense.bias true
longformer.encoder.layer.3.output.dense.weight true
longformer.encoder.layer.3.output.dense.bias true
longformer.encoder.layer.3.output.layernorm.weight true
longformer.encoder.layer.3.output.layernorm.bias true
longformer.encoder.layer.4.attention.self.query.weight true
longformer.encoder.layer.4.attention.self.query.bias true
longformer.encoder.layer.4.attention.self.key.weight true
longformer.encoder.layer.4.attention.self.key.bias true
longformer.encoder.layer.4.attention.self.value.weight true
longformer.encoder.layer.4.attention.self.value.bias true
longformer.encoder.layer.4.attention.self.query_global.weight true
longformer.encoder.layer.4.attention.self.query_global.bias true
longformer.encoder.layer.4.attention.self.key_global.weight true
longformer.encoder.layer.4.attention.self.key_global.bias true
longformer.encoder.layer.4.attention.self.value_global.weight true
longformer.encoder.layer.4.attention.self.value_global.bias true
longformer.encoder.layer.4.attention.output.dense.weight true
longformer.encoder.layer.4.attention.output.dense.bias true
longformer.encoder.layer.4.attention.output.layernorm.weight true
longformer.encoder.layer.4.attention.output.layernorm.bias true
longformer.encoder.layer.4.intermediate.dense.weight true
longformer.encoder.layer.4.intermediate.dense.bias true
longformer.encoder.layer.4.output.dense.weight true
longformer.encoder.layer.4.output.dense.bias true
longformer.encoder.layer.4.output.layernorm.weight true
longformer.encoder.layer.4.output.layernorm.bias true
longformer.encoder.layer.5.attention.self.query.weight true
longformer.encoder.layer.5.attention.self.query.bias true
longformer.encoder.layer.5.attention.self.key.weight true
longformer.encoder.layer.5.attention.self.key.bias true
longformer.encoder.layer.5.attention.self.value.weight true
longformer.encoder.layer.5.attention.self.value.bias true
longformer.encoder.layer.5.attention.self.query_global.weight true
longformer.encoder.layer.5.attention.self.query_global.bias true
longformer.encoder.layer.5.attention.self.key_global.weight true
longformer.encoder.layer.5.attention.self.key_global.bias true
longformer.encoder.layer.5.attention.self.value_global.weight true
longformer.encoder.layer.5.attention.self.value_global.bias true
longformer.encoder.layer.5.attention.output.dense.weight true
longformer.encoder.layer.5.attention.output.dense.bias true
longformer.encoder.layer.5.attention.output.layernorm.weight true
longformer.encoder.layer.5.attention.output.layernorm.bias true
longformer.encoder.layer.5.intermediate.dense.weight true
longformer.encoder.layer.5.intermediate.dense.bias true
longformer.encoder.layer.5.output.dense.weight true
longformer.encoder.layer.5.output.dense.bias true
longformer.encoder.layer.5.output.layernorm.weight true
longformer.encoder.layer.5.output.layernorm.bias true
longformer.encoder.layer.6.attention.self.query.weight true
longformer.encoder.layer.6.attention.self.query.bias true
longformer.encoder.layer.6.attention.self.key.weight true
longformer.encoder.layer.6.attention.self.key.bias true
longformer.encoder.layer.6.attention.self.value.weight true
longformer.encoder.layer.6.attention.self.value.bias true
longformer.encoder.layer.6.attention.self.query_global.weight true
longformer.encoder.layer.6.attention.self.query_global.bias true
longformer.encoder.layer.6.attention.self.key_global.weight true
longformer.encoder.layer.6.attention.self.key_global.bias true
longformer.encoder.layer.6.attention.self.value_global.weight true
longformer.encoder.layer.6.attention.self.value_global.bias true
longformer.encoder.layer.6.attention.output.dense.weight true
longformer.encoder.layer.6.attention.output.dense.bias true
longformer.encoder.layer.6.attention.output.layernorm.weight true
longformer.encoder.layer.6.attention.output.layernorm.bias true
longformer.encoder.layer.6.intermediate.dense.weight true
longformer.encoder.layer.6.intermediate.dense.bias true
longformer.encoder.layer.6.output.dense.weight true
longformer.encoder.layer.6.output.dense.bias true
longformer.encoder.layer.6.output.layernorm.weight true
longformer.encoder.layer.6.output.layernorm.bias true
longformer.encoder.layer.7.attention.self.query.weight true
longformer.encoder.layer.7.attention.self.query.bias true
longformer.encoder.layer.7.attention.self.key.weight true
longformer.encoder.layer.7.attention.self.key.bias true
longformer.encoder.layer.7.attention.self.value.weight true
longformer.encoder.layer.7.attention.self.value.bias true
longformer.encoder.layer.7.attention.self.query_global.weight true
longformer.encoder.layer.7.attention.self.query_global.bias true
longformer.encoder.layer.7.attention.self.key_global.weight true
longformer.encoder.layer.7.attention.self.key_global.bias true
longformer.encoder.layer.7.attention.self.value_global.weight true
longformer.encoder.layer.7.attention.self.value_global.bias true
longformer.encoder.layer.7.attention.output.dense.weight true
longformer.encoder.layer.7.attention.output.dense.bias true
longformer.encoder.layer.7.attention.output.layernorm.weight true
longformer.encoder.layer.7.attention.output.layernorm.bias true
longformer.encoder.layer.7.intermediate.dense.weight true
longformer.encoder.layer.7.intermediate.dense.bias true
longformer.encoder.layer.7.output.dense.weight true
longformer.encoder.layer.7.output.dense.bias true
longformer.encoder.layer.7.output.layernorm.weight true
longformer.encoder.layer.7.output.layernorm.bias true
longformer.encoder.layer.8.attention.self.query.weight true
longformer.encoder.layer.8.attention.self.query.bias true
longformer.encoder.layer.8.attention.self.key.weight true
longformer.encoder.layer.8.attention.self.key.bias true
longformer.encoder.layer.8.attention.self.value.weight true
longformer.encoder.layer.8.attention.self.value.bias true
longformer.encoder.layer.8.attention.self.query_global.weight true
longformer.encoder.layer.8.attention.self.query_global.bias true
longformer.encoder.layer.8.attention.self.key_global.weight true
longformer.encoder.layer.8.attention.self.key_global.bias true
longformer.encoder.layer.8.attention.self.value_global.weight true
longformer.encoder.layer.8.attention.self.value_global.bias true
longformer.encoder.layer.8.attention.output.dense.weight true
longformer.encoder.layer.8.attention.output.dense.bias true
longformer.encoder.layer.8.attention.output.layernorm.weight true
longformer.encoder.layer.8.attention.output.layernorm.bias true
longformer.encoder.layer.8.intermediate.dense.weight true
longformer.encoder.layer.8.intermediate.dense.bias true
longformer.encoder.layer.8.output.dense.weight true
longformer.encoder.layer.8.output.dense.bias true
longformer.encoder.layer.8.output.layernorm.weight true
longformer.encoder.layer.8.output.layernorm.bias true
longformer.encoder.layer.9.attention.self.query.weight true
longformer.encoder.layer.9.attention.self.query.bias true
longformer.encoder.layer.9.attention.self.key.weight true
longformer.encoder.layer.9.attention.self.key.bias true
longformer.encoder.layer.9.attention.self.value.weight true
longformer.encoder.layer.9.attention.self.value.bias true
longformer.encoder.layer.9.attention.self.query_global.weight true
longformer.encoder.layer.9.attention.self.query_global.bias true
longformer.encoder.layer.9.attention.self.key_global.weight true
longformer.encoder.layer.9.attention.self.key_global.bias true
longformer.encoder.layer.9.attention.self.value_global.weight true
longformer.encoder.layer.9.attention.self.value_global.bias true
longformer.encoder.layer.9.attention.output.dense.weight true
longformer.encoder.layer.9.attention.output.dense.bias true
longformer.encoder.layer.9.attention.output.layernorm.weight true
longformer.encoder.layer.9.attention.output.layernorm.bias true
longformer.encoder.layer.9.intermediate.dense.weight true
longformer.encoder.layer.9.intermediate.dense.bias true
longformer.encoder.layer.9.output.dense.weight true
longformer.encoder.layer.9.output.dense.bias true
longformer.encoder.layer.9.output.layernorm.weight true
longformer.encoder.layer.9.output.layernorm.bias true
longformer.encoder.layer.10.attention.self.query.weight true
longformer.encoder.layer.10.attention.self.query.bias true
longformer.encoder.layer.10.attention.self.key.weight true
longformer.encoder.layer.10.attention.self.key.bias true
longformer.encoder.layer.10.attention.self.value.weight true
longformer.encoder.layer.10.attention.self.value.bias true
longformer.encoder.layer.10.attention.self.query_global.weight true
longformer.encoder.layer.10.attention.self.query_global.bias true
longformer.encoder.layer.10.attention.self.key_global.weight true
longformer.encoder.layer.10.attention.self.key_global.bias true
longformer.encoder.layer.10.attention.self.value_global.weight true
longformer.encoder.layer.10.attention.self.value_global.bias true
longformer.encoder.layer.10.attention.output.dense.weight true
longformer.encoder.layer.10.attention.output.dense.bias true
longformer.encoder.layer.10.attention.output.layernorm.weight true
longformer.encoder.layer.10.attention.output.layernorm.bias true
longformer.encoder.layer.10.intermediate.dense.weight true
longformer.encoder.layer.10.intermediate.dense.bias true
longformer.encoder.layer.10.output.dense.weight true
longformer.encoder.layer.10.output.dense.bias true
longformer.encoder.layer.10.output.layernorm.weight true
longformer.encoder.layer.10.output.layernorm.bias true
longformer.encoder.layer.11.attention.self.query.weight true
longformer.encoder.layer.11.attention.self.query.bias true
longformer.encoder.layer.11.attention.self.key.weight true
longformer.encoder.layer.11.attention.self.key.bias true
longformer.encoder.layer.11.attention.self.value.weight true
longformer.encoder.layer.11.attention.self.value.bias true
longformer.encoder.layer.11.attention.self.query_global.weight true
longformer.encoder.layer.11.attention.self.query_global.bias true
longformer.encoder.layer.11.attention.self.key_global.weight true
longformer.encoder.layer.11.attention.self.key_global.bias true
longformer.encoder.layer.11.attention.self.value_global.weight true
longformer.encoder.layer.11.attention.self.value_global.bias true
longformer.encoder.layer.11.attention.output.dense.weight true
longformer.encoder.layer.11.attention.output.dense.bias true
longformer.encoder.layer.11.attention.output.layernorm.weight true
longformer.encoder.layer.11.attention.output.layernorm.bias true
longformer.encoder.layer.11.intermediate.dense.weight true
longformer.encoder.layer.11.intermediate.dense.bias true
longformer.encoder.layer.11.output.dense.weight true
longformer.encoder.layer.11.output.dense.bias true
longformer.encoder.layer.11.output.layernorm.weight true
longformer.encoder.layer.11.output.layernorm.bias true
classifier.dense.weight true
classifier.dense.bias true
classifier.out_proj.weight true
classifier.out_proj.bias true

my questions

why for all layers param.requires_grad is true? shouldnt it be false at least for classifier. layers? aren't we training them?
does param.requires_grad==true mean that particular layer is freezed? i am confused with wording requires_grad. does it mean freezed?
if i want to train some of the previous layers as shown here , should i use below code?

    for name, param in model.named_parameters():
         if name.startswith(""...""): # choose whatever you like here
            param.requires_grad = false

considering it takes a lot of time to train, is there specific recommendation regarding layers that i should train? to begin with i am planning to train -

all layers starting with  longformer.encoder.layer.11. and
`classifier.dense.weight` 
`classifier.dense.bias` 
`classifier.out_proj.weight` 
`classifier.out_proj.bias`


do i need add any additional layers such as dropout or is that already taken care by longformerforsequenceclassification.from_pretrained? i am not seeing any dropout layers in the above output and that's why asking the question

#------------------
update 1
how could i know which layers are frozen by using below code from the answer given by @joe32140 ? my guess is everything except last 4 layers from my output shown in my original question gets frozen. but is there any easier way to check?
for param in model.base_model.parameters():
    param.requires_grad = false","['python', 'pytorch', 'classification', 'huggingface-transformers']",71579242,"requires_grad==true means that we will compute the gradient of this tensor, so the default setting is we will train/finetune all layers.
you can only train the output layer by freezing the encoder with

for param in model.base_model.parameters():
    param.requires_grad = false


yes, dropout is used in huggingface output layer implementation. see here: 

as for update 1, yes, base_model refers to layers excluding the output classification head. however, it's actually two layers instead of four where each layer has a weight and a bias tensors.",https://stackoverflow.com/questions/71577525,python,22-03-2022 18:55,337.0,0.0,1.0,True,22-03-2022 23:48,22-03-2022 23:02
75686316,changing the output of text_tokens function in r,"i have a question redarding text mining with the corpus package and the function text_tokens(). i want to use the function for stemming and deleting stop words. i have a huge amount of data (almost 1.000.000 comments) where i want to use it for. but i've problems with the output, the function text_tokens produces. so here is a basic example of my data and code:
library(tidyverse)
library(corpus)
library(stopwords)

text <- data.frame(comment_id = 1:2,
                   comment_content = c(""hallo mein name ist aaron"",""vielen lieben dank fï¿½ï¿½r das video""))


tmp <- text_tokens(text$comment_content, 
                   text_filter(stemmer = ""de"",drop = stopwords(""german"")))

my problem now is, that i want a data.frame as output with the comment_id in the first column and word_token in the column. so the output i would like to have looks as followed:
df <- data.e(comment_id = c(1,1,1,2,2,2),
                 comment_tokens = c(""hallo"",""nam"",""aaron"",""lieb"",""dank"",""video""))


i tried different do.calls (cbind/rbind), but they don't give me the result i need. so what is the function i'm looking for, is it map() from the tidyverse?
thank you in advance.
cheers,
aaron","['r', 'nlp', 'stemming']",75686412,"here's an option  using imap_dfr from purrr:
library(corpus)
library(dplyr)
library(purrr)

text <- data.frame(comment_id = 1:2,
                   comment_content = c(""hallo mein name ist aaron"",""vielen lieben dank fï¿½ï¿½r das video""))


tmp <- text_tokens(text$comment_content, 
                   text_filter(stemmer = ""de"",drop = corpus::stopwords_de)) %>% 
  purrr::imap_dfr(function(x, y) {
  tibble(
    comment_id = y,
    comment_tokens = x
  )
})

tmp
#> # a tibble: 6 ï¿½ï¿½ 2
#>   comment_id comment_tokens
#>        <int> <chr>         
#> 1          1 hallo         
#> 2          1 nam           
#> 3          1 aaron         
#> 4          2 lieb          
#> 5          2 dank          
#> 6          2 video

or if you prefer using an anonymous function:
tmp <- text_tokens(text$comment_content, text_filter(stemmer = ""de"",drop = corpus::stopwords_de)) %>% 
  purrr::imap_dfr(~ tibble(comment_id = .y, comment_tokens /code>",https://stackoverflow.com/questions/75686316,r,09-03-2023 14:58,69.0,0.0,1.0,True,10-03-2023 12:52,10-03-2023 12:52
76740367,how to resolve the error importerror: cannot import name &#39;generationconfig&#39; from &#39;transformers&#39;,"i am using paperspace for programming. i want to import automodelforcausallmwithvaluehead, ppoconfig, ppotrainer  from trl but i  i encountered an error.  i installed the required libraries such as torch==1.9.0 or 2.0.1,
transformers==4.18.0, and trl. after importing the libraries as shown below:
import pandas as pd
import os
import torch
from transformers import gpt2tokenizer

from trl import automodelforcausallmwithvaluehead, ppoconfig, ppotrainer 

i got the error:
importerror: cannot import name 'generationconfig' from 'transformers' (/usr/local/lib/python3.9/dist-packages/transformers/__init__.py)
please be aware that the code functions correctly when using google colab. however, when running it on paperspace, i encountered an error. it seems that the version of pytorch on paperspace is 1.12.1, which is not suitable for my requirements. i attempted to create a virtual environment to install a different version of pytorch, but unfortunately, the version remains 1.12.1. although i managed to update pytorch to higher versions like 2.0.1, i encountered a conflicting dependency error:
error: pip's dependency resolver does not currently take into account all the packages that are installed. this behavior is the source of the following dependency conflicts.
torchvision 0.13.1+cu116 requires torch==1.12.1, but you have torch 2.0.1 which is incompatible.
torchaudio 0.12.1+cu116 requires torch==1.12.1, but you have torch 2.0.1 which is incompatible.

even with the latest version of pytorch, the error persists, and i am still unable to resolve the issue.","['pytorch', 'huggingface-transformers']",77197021,"it looks like torchvision and torchaudio are not compatible with your current torch version. try to install pytorch's stable version (2.0.1) using this command:
pip3 install torch torchvision torchaudio --index-url 

or generate the desired combination from 
see also my answer to a similar question.",https://stackoverflow.com/questions/76740367,pytorch,21-07-2023 18:22,2938.0,1.0,2.0,True,09-04-2024 02:15,09-04-2024 02:15
78382913,how to know which words are encoded with unknown tokens in huggingface berttokenizer?,"i use the following code to count how many % of words are encoded to unknown tokens.
paragraph_chinese = '...' # it is a long paragraph from a text file.
from transformers import autotokenizer, berttokenizer
tokenizer_bart = berttokenizer.from_pretrained(""fnlp/bart-base-chinese"")
encoded_chinese_bart = tokenizer_bart.encode(paragraph_chinese)
unk_token_id_bart = tokenizer_bart.convert_tokens_to_ids([""[unk]""])
len_paragraph_chinese   = len(paragraph_chinese)

unk_token_cnt_chinese_bart   = encoded_chinese_bart.count(unk_token_id_bart[0])
print(""bart unknown token count in chinese paragraph:"", unk_token_cnt_chinese_bart, ""("" + str(unk_token_cnt_chinese_bart * 100 / len_paragraph_chinese) + ""%)"")
print(type(tokenizer_bart))

which prints:
bart unknown token count in chinese paragraph: 1 (0.015938795027095953%)
<class 'transformers.models.bert.tokenization_bert.berttokenizer'>

my question is: i noticed there is one unknown token. how can i know which word causes this unknown token?
p.s. i tried print(encoded_chinese_bart), but it is a list of token ids.
using transformers 4.28.1","['huggingface-transformers', 'huggingface-tokenizers']",78387343,"when you use the berttokenizerfast instead of the ""slow"" version, you will get a batchencoding object that gives you access to several convenient methods that allow you to map a token back to the original string.
the following code uses the token_to_chars method:
from transformers import berttokenizerfast

# just an example
paragraph_chinese = 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ koï¿½ï¿½ka ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ koï¿½ï¿½ka ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½' 

tokenizer_bart = berttokenizerfast.from_pretrained(""fnlp/bart-base-chinese"")
encoded_chinese_bart = tokenizer_bart(paragraph_chinese)
unk_token_id_bart = tokenizer_bart.unk_token_id
len_paragraph_chinese   = len(paragraph_chinese)

unk_token_cnt_chinese_bart   = encoded_chinese_bart.input_ids.count(unk_token_id_bart)
print(f'bart unknown token count in chinese paragraph: {unk_token_cnt_chinese_bart} ({unk_token_cnt_chinese_bart * 100 / len_paragraph_chinese}%)')

#find all indices
unk_indices = [i for i, x in enumerate(encoded_chinese_bart.input_ids) if x == unk_token_id_bart]
for unk_i in unk_indices:
  start, stop = encoded_chinese_bart.token_to_chars(unk_i)
  print(f""at {start}:{stop}: {paragraph_chinese[start:stop]}"")

original:
<ýka
at 17:22: koýýka",https://stackoverflow.com/questions/78382913,huggingface-transformers,25-04-2024 07:22,702.0,3.0,2.0,True,20-07-2024 02:06,25-04-2024 09:11
46541255,mitie library for nlp,i am trying to understand how mitie is integrated with rasa. i wanted to know what exactly the mitie file total_word_feature_extractor.dat contain? i dont find any good documentation about this.,"['python', 'neural-network', 'nlp', 'rasa-nlu']",46544157,"if you poke around deep enough in the mitie repo's on github you can find your answer. for example here is a bit of information about what goes into that file.

as for what's inside, yes, it's a variant of word2vec based on the two step cca method from this paper:  i also upgraded it to include something that is similar to the cca method but works on out of sample words by analyzing their morphology to produce a word vector. this significantly improved the results on datasets containing lots of words not in the original dictionary.

as far as how mitie integrates into rasa, it is one of a few backend choices for rasa. it provides a few pipeline components that can do both intent classification and ner. both of which use an svm and use the total_word_feature_extractor.dat to provide the individual word vectors.",https://stackoverflow.com/questions/46541255,python,03-10-2017 09:23,1119.0,1.0,1.0,True,07-03-2025 15:32,07-03-2025 15:32
74953426,creating and computing percentage of co-occurrence based on keywords,"i have the following data set:
df <- data.frame (text  = c(""house sky blue"",
                            ""house sky green"",
                            ""house sky red"",
                            ""house sky yellow"",
                            ""house sky green"",
                            ""house sky glue"",
                            ""house sky green""))

i'd like to find the percentage of co-occurrence of some terms of tokens. for example, out of all documents, where can i find the token ""house"" and at the same time how many of them also include the term ""green""?
in out data we have 7 documents that have the term house and 3 out of those 7 p=(100*3/7) also include the term green, it would be so nice to see also what terms or tokens appear within some p threshold along side the token ""house"".
i have used these two functions:
textstat_collocations(tokens)

> textstat_collocations(tokens)
  collocation count count_nested length   lambda        z
1   house sky     7            0      2 5.416100 2.622058
2   sky green     3            0      2 2.456736 1.511653

fun textstat_simil

textstat_simil(dfm(tokens),margin=""features"")

textstat_simil object; method = ""correlation""
       house sky   blue  green    red yellow   glue
house    nan nan    nan    nan    nan    nan    nan
sky      nan nan    nan    nan    nan    nan    nan
blue     nan nan  1.000 -0.354 -0.167 -0.167 -0.167
green    nan nan -0.354  1.000 -0.354 -0.354 -0.354
red      nan nan -0.167 -0.354  1.000 -0.167 -0.167
yellow   nan nan -0.167 -0.354 -0.167  1.000 -0.167
glue     nan nan -0.167 -0.354 -0.167 -0.167  1.000

but they do not seem to give my desired output also i wonder why the correlation btw green and house is nan for the textsats_simil fun
my desired output would show the following info:
feature=""house""
 percentage of co-occurrence 

green = 3/7
blue= 1/7
red = 1/7
yellow = 1/7
glue = 1/7

in the quetda docs i can't seem to find a function that can give me my desired output, although i know there must be a way around since i find this library to be so fast and complete.","['r', 'text-mining', 'quanteda']",74988297,"one way to do this is using the fcm() to get document-level co-occurrences for a target feature.  below, i show how to do this using fcm(), fcm_remove() to remove the target feature, then a loop to get the desired printed output.
library(""quanteda"")
#> package version: 3.2.4
#> unicode version: 14.0
#> icu version: 70.1
#> parallel computing: 10 of 10 threads used.
#> see  for tutorials and examples.

df <- data.frame(text = c(""house sky blue"",
                          ""house sky green"",
                          ""house sky red"",
                          ""house sky yellow"",
                          ""house sky green"",
                          ""house sky glue"",
                          ""house sky green""))
corp <- corpus(df)

coocc_fract <- function(corp, feature) {
   # create a document-level co-occurrence matrix
   fcmat <- fcm(dfm(tokens(corp), tolower = false), context = ""document"")
   # select for the given feature
   fcmat <- fcm_remove(fcmat, feature)
   cat(""feature=\"""", feature, ""\""\n"", sep = """")
   cat("" percentage of co-occurrence\n\n"")
   for (f in featnames(fcmat)) {
       # skip zeroes
       freq <- as.character(fcmat[1, f])
       if (freq != ""0"") {
           cat(f, "" = "", as.character(fcmat[1, f]), ""/"", ndoc(corp), 
               ""\n"", sep = """")
       }
   }
}

this produces this output:
coocc_fract(corp, feature = ""house"")
#> feature=""house""
#>  percentage of co-occurrence
#> 
#> blue = 1/7
#> green = 3/7
#> red = 1/7
#> yellow = 1/7
#> glue = 1/7

created on 2023-01-02 with reprex v2.0.2",https://stackoverflow.com/questions/74953426,r,29-12-2022 16:06,157.0,0.0,2.0,True,19-05-2024 22:53,19-05-2024 22:53
73802610,"is there an api for the azure language studio, especially for custom question answering?","i am currently working with the language studio by azure. for now, i am wondering if there is an api for this, especially for the custom question answering that i can work with on my current project.","['azure', 'azure-cognitive-services', 'nlp-question-answering', 'language-studio']",73803199,"language studio is just a demonstrator. everything is made thanks to apis.
for example for question answering:",https://stackoverflow.com/questions/73802610,azure,21-09-2022 14:34,1036.0,0.0,1.0,True,21-01-2023 20:14,21-01-2023 20:14
61938628,convert from prodigy&#39;s jsonl format for labeled ner to spacy&#39;s training format?,"i am new to prodigy and spacy as well as cli coding. i'd like to use prodigy to label my data for an ner model, and then use spacy in python to create models. 
prodigy outputs in sqlite format. spacy takes in this other kind of format, not sure what to call it: 
train_data = [
    (
        ""horses are too tall and they pretend to care about your feelings"",
        {""entities"": [(0, 6, label)]},
    ),
    (""do they bite?"", {""entities"": []}),
    (
        ""horses are too tall and they pretend to care about your feelings"",
        {""entities"": [(0, 6, label)]},
    ),
    (""horses pretend to care about your feelings"", {""entities"": [(0, 6, label)]}),
    (
        ""they pretend to care about your feelings, those horses"",
        {""entities"": [(48, 54, label)]},
    ),
    (""horses?"", {""entities"": [(0, 6, label)]}),
]

how can i convert from one to the other? it seems like this should be easy, but i cannot find it anywhere. 
i have no problem loading in the dataset, just converting.","['sqlite', 'nlp', 'spacy', 'named-entity-recognition', 'prodigy']",61941227,prodigy should export this training format with data-to-spacy as of version 1.9:,https://stackoverflow.com/questions/61938628,sqlite,21-05-2020 16:00,2456.0,0.0,1.0,True,14-04-2022 18:27,14-04-2022 18:27
70720126,oserror: e053 could not read config.cfg spacy on colab,"i want to use spacytextblob in google colab, when i use the formal installation, i got the below error.
oserror: [e053] could not read config.cfg from /usr/local/lib/python3.7/dist-packages/en_core_web_sm/en_core_web_sm-2.2.5/config.cfg

what i do, first run this block:
import spacy
!pip install spacytextblob

then based on the output i restart the runtime. then run this code:
from spacytextblob.spacytextblob import spacytextblob
nlp = spacy.load('en_core_web_sm')
nlp.add_pipe(""spacytextblob"")

but at the first line, i got the error.
if i don't install spacytextblob the default version of  spacy on colab it's 2.2.4 but after installation the  spacy version it's 3.2.1
there is an answer here for the same error but i can't use it.
i guess the problem it's from the spacy version which changed after installing the spacytextblob but how can i fix it?","['python', 'spacy']",70720293,"i solve the problem by using this  installation guide
!pip install spacytextblob
!python -m textblob.download_corpora
!python -m spacy download en_core_web_sm",https://stackoverflow.com/questions/70720126,python,15-01-2022 09:05,3549.0,2.0,1.0,True,15-01-2022 11:14,15-01-2022 10:56
76451997,langchain: reduce size of tokens being passed to openai,"i am using langchain to create embeddings and then ask a question to those embeddings like so:
embeddings: openaiembeddings = openaiembeddings(disallowed_special=())
db = deeplake(
    dataset_path=deeplake_url,
    read_only=true,
    embedding_function=embeddings,
)
retriever: vectorstoreretriever = db.as_retriever()
model = chatopenai(model_name=""gpt-3.5-turbo"") 
qa = conversationalretrievalchain.from_llm(model, retriever=retriever)
result = qa({""question"": question, ""chat_history"": chat_history})

but i am getting the following error:
file ""/xxxxx/openai/api_requestor.py"", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.invalidrequesterror: this model's maximum context length is 4097 tokens. however, your messages resulted in 13918 tokens. please reduce the length of the messages.

the chat_history is empty and the question is quite small.
how can i reduce the size of tokens being passed to openai?
i'm assuming the response from the embeddings is too large being passed to openai. it might be easy enough to just figure out how to truncate the data being sent to openai.","['openai-api', 'langchain', 'py-langchain', 'deeplake', 'activeloop']",76452214,"summary
when you initiate the conversationalretrievalchain object, pass in a max_tokens_limit amount.
qa = conversationalretrievalchain.from_llm(
        model, retriever=retriever, max_tokens_limit=4000
    )

this will automatically truncate the tokens when asking openai / your llm.
longer explainer
in the base.py of conversationalretrievalchain there is a function that is called when asking your question to deeplake/openai:
    def _get_docs(self, question: str, inputs: dict[str, any]) -> list[document]:
        docs = self.retriever.get_relevant_documents(question)
        return self._reduce_tokens_below_limit(docs)

which reads from the deeplake vector database, and adds that as context to your doc's text that you upload to openai.
the _reduce_tokens_below_limit reads from the class instance variable max_tokens_limit to truncate the size of the input docs.",https://stackoverflow.com/questions/76451997,openai-api,11-06-2023 18:41,20988.0,3.0,2.0,True,15-09-2023 13:47,11-06-2023 19:03
75152760,removing rows that contain above a certain share of upper-case letters in r,"i have a large dataframe that consists of company identifiers and extracted phrases from newspapers. it is very messy, and i want to clean it by conditional row removing.

for this i want to remove rows that have more then 50% upper-case letters.
i have found this code from a post which will remove me rows with all upper-case letters:
data <- data[!grepl(""^[a-z]+(?:[ -][a-z]+)*$"", data$text), ]

how can i express it as a share of the total word or letter count?","['r', 'string', 'nlp', 'grepl']",75153069,"you could do this with regular expressions, but the stringi function stri_count_charclass provide a highly optimized version for detecting categories of characters. the package manual documents the list of unicode general categories, here we use string l for all letters, and lu for uppercase letters.
something like this should accomplish what you need:
library(stringi)

data <- data.frame(text = c(""foo"",
                            ""bar"",
                            ""baz""))

data[which(stri_count_charclass(data[[""text""]],""[\\p{lu}]"") / stri_count_charclass(data[[""text""]],""[\\p{l}]"") < 0.5),]
# [1] ""foo""

one note: i updated my answer here since i failed to point out a powerful feature of stringi in my original response. my instinctive reaction was to use [a-z] and [a-z] to signify lower and upper case characters, respectively. however, using unicode general categories allows the solution to work well for non-ascii characters as well.
x = c(""foo"",
      ""bar"",
      ""baz"",
      ""ï¿½ï¿½ï¿½oo"",
      ""ï¿½ï¿½ï¿½ï¿½ï¿½r"",
      ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"")
stri_count_charclass(x,""[a-z]"")/stri_count_charclass(x,""[[a-z][a-z]]"")
[1] 0.3333333 0.6666667 1.0000000 0.0000000 0.0000000       nan

stri_count_charclass(x,""[\\p{lu}]"")/stri_count_charclass(x,""[\\p{l}]"")
[1] 0.3333333 0.6666667 1.0000000 0.3333333 0.",https://stackoverflow.com/questions/75152760,r,17-01-2023 22:13,43.0,2.0,1.0,True,19-01-2023 14:03,19-01-2023 14:03
70426487,how to find a pattern inside a pattern when start and end is known?,"i have a pattern that has a starting and ending pattern like:
start = '\n\\[\n'
end = '\n\\]\n'

my string is:
'the above mentioned formal formula is\nthat of\n\\[\n\\oplus \\bigoplus_{(5)} \\widehat{c_{(5)}} a_{5} g(2)\n\\]\na. tobacco\nb. tulip\nc. soybean\nd. sunhemp'

i want to find:
\n\\oplus \\bigoplus_{(5)} \\widehat{c_{(5)}} a_{5} g(2)'

if i use:
re.findall(r'\s*\\+\n\\[\n(.*?)\\+\n\\]\n', mystring)

r'\s*\\+\[(.*?)\\+\]' # did not work either

then it gives me an empty result. what am  i doing wrong here?","['python', 'nlp', 'python-re']",70426840,"this works for me:
mystring = 'the above mentioned formal formula is\nthat of\n\\[\n\\oplus \\bigoplus_{(5)} \\widehat{c_{(5)}} a_{5} g(2)\n\\]\na. tobacco\nb. tulip\nc. soybean\nd. sunhemp'

expected_result = '\n\\oplus \\bigoplus_{(5)} \\widehat{c_{(5)}} a_{5} g(2)'

import codecs
import re

matches = re.findall(r'\\n\\\\\[(\\n.*)\\n\\\\\]\\n', repr(mystring))

results = [codecs.decode(match, 'unicode_escape') for match in matches]

results
['\n\\oplus \\bigoplus_{(5)} \\widehat{c_{(5)}} a_{5} g(2)']

results[0] == expected_result
true",https://stackoverflow.com/questions/70426487,python,20-12-2021 18:33,70.0,1.0,2.0,True,21-12-2021 08:13,20-12-2021 18:42
71496028,valueerror: class encoding field is specified without a type,"dim_red = truncatedsvd(n_components=2)
data_red = dim_red.fit_transform(tfidf)

scatter = alt.chart(data_red,title=""dimensionality reduction"",height=400).mark_circle().encode(
    x='principal component 1', y='principal component 2', color=alt.color(
        'class', scale=alt.scale(scheme='blues')),tooltip=[""class""]).interactive()

st.altair_chart(scatter)`","['python', 'nlp', 'altair']",71499667,"this error will generally arise when the encoding names (here 'principal component 1' and/or 'principal component 2') do not match the names of any columns in the dataframe passed to the chart. check the names of the columns in the dataframe, and make sure you're reproducing them correctly.",https://stackoverflow.com/questions/71496028,python,16-03-2022 11:08,1165.0,0.0,1.0,True,24-11-2022 12:40,16-03-2022 13:30
63413414,is there a way to get the location of the substring from which a certain token has been produced in bert?,"i am feeding sentences to a bert model (hugging face library). these sentences get tokenized with a pretrained tokenizer. i know that you can use the decode function to go back from tokens to strings.
string = tokenizer.decode(...)

however, the reconstruction is not perfect. if you use an uncased pretrained model, the uppercase letters get lost. also, if the tokenizer splits a word into 2 tokens, the second token will start with '##'. for example, the word 'coronavirus' gets split into 2 tokens: 'corona' and '##virus'.
so my question is: is there a way to get the indices of the substring from which every token is created?
for example, take the string ""tokyo to report nearly 370 new coronavirus cases, setting new single-day record"". the 9th token is the token corresponding to 'virus'.
['[cls]', 'tokyo', 'to', 'report', 'nearly', '370', 'new', 'corona', '##virus', 'cases', ',', 'setting', 'new', 'single', '-', 'day', 'record', '[sep]']

i want something that tells me that the token '##virus' comes from the 'virus' substring in the original string, which is located between the indices 37 and 41 of the original string.
sentence = ""tokyo to report nearly 370 new coronavirus cases, setting new single-day record""
print(sentence[37:42]) # --> outputs 'virus","['tokenize', 'bert-language-model', 'huggingface-transformers', 'huggingface-tokenizers']",63422347,"as far as i know their is no built-in method for that, but you can create one by yourself:
import re
from transformers import berttokenizer

tokenizer = berttokenizer.from_pretrained('bert-base-uncased')

sentence = ""tokyo to report nearly 370 new coronavirus cases, setting new single-day record""

b = []
b.append(([101],))
for m in re.finditer(r'\s+', sentence):
  w = m.group(0)
  t = (tokenizer.encode(w, add_special_tokens=false), (m.start(), m.end()-1))

  b.append(t)

b.append(([102],))

b

output:
[([101],),
 ([5522], (0, 4)),
 ([2000], (6, 7)),
 ([3189], (9, 14)),
 ([3053], (16, 21)),
 ([16444], (23, 25)),
 ([2047], (27, 29)),
 ([21887, 23350], (31, 41)),
 ([3572, 1010], (43, 48)),
 ([4292], (50, 56)),
 ([2047], (58, 60)),
 ([2309, 1011, 2154], (62, 71)),
 ([2501], (73, 78)),
 ([102],)]",https://stackoverflow.com/questions/63413414,tokenize,14-08-2020 13:09,3211.0,2.0,2.0,True,05-12-2021 14:04,14-08-2020 13:28
71548193,how to use stemming algorithm for a list of words in python,"i have a word list:
'aws', 
'jquery', 
'jquery', 
'sliding', 
'jquery', 
'jquery', 
'manipulating', 
'us!'

i removed common words and need to apply stemming to make the word list more clear.
my code to remove common words:
raw2 = second_headers corpus = common_word_corpus  #my personal word corpus added here

corpus = [w.lower() for w in corpus]  
processed_h2_tag = [w for w in raw2.split(' ') if w.lower() not in corpus] 

print(processed_h2_tag)","['python', 'nlp']",71548298,"how about this?
# download wordnet
import nltk
nltk.download('wordnet')

# import these modules
from nltk.stem import wordnetlemmatizer
from nltk.corpus import wordnet 
nltk.download('wordnet')

lemmatizer = wordnetlemmatizer()

# choose some words to be stemmed
words = ['aws', 
'jquery', 
'jquery', 
'sliding', 
'jquery', 
'jquery', 
'manipulating', 
'manipulateing', 
'manipulate', 
'us!']
 
for w in words:
    print(w, "" : "", lemmatizer.lemmatize(w.lower(), pos=wordnet.verb))


output:
aws  :  aws
jquery  :  jquery
jquery  :  jquery
sliding  :  slide
jquery  :  jquery
jquery  :  jquery
manipulating  :  manipulate
manipulateing  :  manipulate
manipulate  :  manipulate
us!  :  us!",https://stackoverflow.com/questions/71548193,python,20-03-2022 15:41,48.0,0.0,1.0,True,01-04-2022 16:42,21-03-2022 08:23
66193575,why is the vocab size of byte level bpe smaller than unicode&#39;s vocab size?,"i recently read gpt2 and the paper says:

this would result in a base vocabulary of over 130,000 before any multi-symbol tokens are added.  this is prohibitively large compared to the 32,000 to 64,000 token vocabularies often used with bpe. in contrast, a byte-level version of bpe only requires a base vocabulary of size 256.

i really don't understand the words. the number of characters that unicode represents is 130k but how can this be reduced to 256? where's the rest of approximately 129k characters? what am i missing? does byte-level bpe allow duplicating of representation between different characters?
i don't understand the logic. below are my questions:

why the size of vocab is reduced? (from 130k to 256)
what's the logic of the bbpe (byte-level bpe)?


detail question
thank you for your answer but i really don't get it. let's say we have 130k unique characters. what we want (and bbpe do) is to reduce this basic (unique) vocabulary. each unicode character can be converted 1 to 4 bytes by utilizing utf-8 encoding. the original paper of bbpe says (neural machine translation with byte-level subwords):

representing text at the level of bytes and using the 256 bytes set as vocabulary is a potential solution to this issue.

each byte can represent 256 characters (bits, 2^8), we only need 2^17 (131072) bits for representing the unique unicode characters. in this case, where did the 256 bytes in the original paper come from? i don't know both the logic and how to derive this result.
i arrange my questions again, more detail:

how does bbpe work?
why the size of vocab is reduced? (from 130k to 256 bytes)

anyway, we always need 130k space for a vocab. what's the difference between representing unique characters as unicode and bytes?



since i have little knowledge of computer architecture and programming, please let me know if there's something i missed.
sincerely, thank you.","['unicode', 'utf-8', 'nlp']",66193820,"unicode code points are integers in the range 0..1,114,112, of which roughly 130k are in use at the moment. every unicode code point corresponds to a character, like ""a"" or ""ï¿½ï¿½"" or ""ï¿½ï¿½ï¿½"", which is handy to work with in many cases (but there are a lot of complicated details, eg. combining marks).
when you save text data to a file, you use one of the utfs (utf-8, utf-16, utf-32) to convert code points (integers) to bytes. for utf-8 (the most popular file encoding), each character is represented by 1, 2, 3, or 4 bytes (there's some inner logic to discriminate single- and multi-byte characters).
so when the base vocabulary are bytes, this means that rare characters will be encoded with multiple bpe segments.
example
let's consider a short example sentence like ï¿½ï¿½ï¿½thatï¿½ï¿½ï¿½s great ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½.
with a base vocabulary of all unicode characters, the bpe model starts off with something like this:
t      5   2019
s      73
       20
g      67
r      72
e      65
a      61
t      74
       20
ï¿½ï¿½ï¿½ï¿½   1f44d

(the first column is the character, the second its codepoint in hexadecimal notation.)
if you first encode this sentence with utf-8, then this sequence of bytes is fed to bpe instead:
t      54
h      68
a      61
t      74
ï¿½ï¿½ï¿½      e2
ï¿½ï¿½ï¿½      80
ï¿½ï¿½ï¿½      99
s      73
       20
g      67
r      72
e      65
a      61
t      74
       20
ï¿½ï¿½ï¿½      f0
ï¿½ï¿½ï¿½      9f
ï¿½ï¿½ï¿½      91
ï¿½ï¿½ï¿½      8d

the typographic apostrophe ""ï¿½ï¿½ï¿½"" and the thumbs-up emoji are represented by multiple bytes.
with either input, the bpe segmentation (after training) may end with something like this:
th|at|ï¿½ï¿½ï¿½s|great|ï¿½ï¿½ï¿½ï¿½

(this is a hypothetical segmentation, but it's possible that capitalised ï¿½ï¿½ï¿½thatï¿½ï¿½ï¿½ is too rare to be represented as a single segment.)
ýýs, only one merge step is required for code-point input, but three steps for byte input.
with byte input, the bpe segmentation is likely to end up with sub-character segments for rare characters.
the down-stream language model will have to learn to deal with that kind of input.",https://stackoverflow.com/questions/66193575,unicode,14-02-2021 08:17,4747.0,7.0,4.0,True,17-01-2025 22:44,15-02-2021 16:46
69845992,"date pattern for whatsapp chat text file that has 24 hour format, split() error: too many values to unpack","i have a whatsapp chat text file from ios, where it has a 24hour format

[07/04/2018, 14:11:22] mike: ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½

i want to create a dataframe from the text. i've tried different date patterns e.g.
def datetimeios(s):
    pattern = '^\[([0-9]+)(\/)([0-9]+)(\/)([0-9]+), ([0-9]+):([0-9]+):([0-9])?\] ' #(am|pm|am|pm)?
    result = re.match(pattern, s)
    if result:
        return true
    return false 

but they're not working. if i add + [:([0-9]+) ]on the seconds i get split() error: too many values to"" rel=""nofollow noreferrer"">","['python', 'pandas', 'date', 'nlp']",69846118,"try with:
x = re.search(r""^\[([0-9]+)(\/)([0-9]+)(\/)([0-9]+), ([0-9]+):([0-9]+):([0-9]+)]"", s)

print(x.group())

your output:
>>> [07/04/2018, 14:11:22]

have a look at this example.",https://stackoverflow.com/questions/69845992,python,04-11-2021 21:22,584.0,0.0,1.0,True,05-11-2021 08:04,05-11-2021 08:04
76784484,cannot import name amazonkendraretriever from langchain,"i cannot import 'amazonkendraretriever' from langchain due to the following errors

---------------------------------------------------------------------------
importerror                               traceback (most recent call last)
cell in[3], line 1
----> 1 from langchain.retrievers import amazonkendraretriever

importerror: cannot import name 'amazonkendraretriever' from 'langchain.retrievers' (/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/langchain/retrievers/__init__.py)

my python version is 3.10.10. i have tried fixing it by pip installing langchain==0.0.201 based on some discussion on a github thread, but that doesn't fix the issue. kindly assist.","['langchain', 'amazon-kendra']",76839928,i changed the kernel from conda_pytorchp310 to conda_python3 and that fixed the issue. running the code from a sagemaker jupyterlab notebook.,https://stackoverflow.com/questions/76784484,langchain,28-07-2023 02:22,705.0,1.0,1.0,True,03-09-2023 23:48,03-09-2023 23:48
79532570,"use of training, validation and test set in huggingface seq2seqtrainer","i have the following dataset, which has 3 splits (train, validation and test). the data are parallel corpus of 2 languages.
datasetdict({
    train: dataset({
        features: ['translation'],
        num_rows: 109942
    })
    validation: dataset({
        features: ['translation'],
        num_rows: 6545
    })
    test: dataset({
        features: ['translation'],
        num_rows: 13743
    })
})

for my seq2seqtrainer, i supply the dataset as follows:
trainer = seq2seqtrainer(
    model = model,
    args = training_args,
    train_dataset = tokenized_dataset['train'],
    eval_dataset = tokenized_dataset['validation'],
    tokenizer = tokenizer,
    data_collator = data_collator,
    compute_metrics = compute_metrics,
)

is it correct to put the validation split in eval_dataset? in the documentation, it says:

the dataset to use for evaluation. if it is a dataset, columns not accepted by the model.forward() method are automatically removed. if it is a dictionary, it will evaluate on each dataset prepending the dictionary key to the metric name.

or should i put the test split in eval_dataset? in either way, is that true that one of the splits is not used?","['python', 'machine-learning', 'dataset', 'huggingface-transformers']",79533126,"i am going to focus on the code side here. for a deeper theoretical explanation of why we need (or should have) training, validation and test set, see what is the difference between test set and validation set?.
for training, using the validation set is correct, the way you already do:
trainer = seq2seqtrainer(
    model = model,
    args = training_args,
    train_dataset = tokenized_dataset['train'],
    eval_dataset = tokenized_dataset['validation'],
    tokenizer = tokenizer,
    data_collator = data_collator,
    compute_metrics = compute_metrics,
)

after training, you can use .predict() or .evaluate(), with your test set.
if you want only the metrics, and not the outputs, you can use .evaluate():
metrics = trainer.evaluate(tokenized_dataset['test'])

if you want the outputs as well as the metrics (or maybe just the outputs), you can use .predict():
preds = trainer.predict(tokenized_dataset['test'])
print(preds.predictions)
print(preds.metrics)",https://stackoverflow.com/questions/79532570,python,25-03-2025 02:22,42.0,0.0,1.0,True,25-03-2025 15:18,25-03-2025 15:18
36064946,text summarization in r language,"i have long text file using help of r language i want to summarize text in at least 10 to 20 line or in small sentences.
how to summarize text in at least 10 line with r language ?","['r', 'text', 'text-mining', 'summarization']",36065535,"you may try this (from the lsafun package):
genericsummary(d,k=1)

whereby 'd' specifies your text document and 'k' the number of sentences to be used in the summary. (further modifications are shown in the package documentation).
for more information:",https://stackoverflow.com/questions/36064946,r,17-03-2016 15:26,10453.0,4.0,4.0,True,31-10-2023 16:24,17-03-2016 16:23
76041048,"&quot;invalid input image - format must be in [&#39;rgba&#39;, &#39;la&#39;, &#39;l&#39;], got rgb.&quot; ios swift","i am using the openai ""create image edit"" api link. my mask image is maskimage.png.
after calling the api, when the response comes, i have got the error:

invalid input image - format must be in ['rgba', 'la', 'l'], got rgb.""

how can i change the png image format as ['rgba', 'la', 'l'] in ios swift?
i have found a solution in javascript, how to validate an image before sending to dall e api with front-end javascript. but i can not find a solution in ios.","['ios', 'swift', 'openai-api']",76056584,"all you have to do is add opacity/alpha to the uiimage/pngdata.
extension uiimage {
    var hasalpha: bool {
        guard let alphainfo = self.cgimage?.alphainfo else {return false}
        return alphainfo != cgimagealphainfo.none &&
            alphainfo != cgimagealphainfo.noneskipfirst &&
            alphainfo != cgimagealphainfo.noneskiplast
    }
}

is a quick way to check if a uiimage has alpha.
you can add alpha with
extension uiimage {

    func imagewithalpha(alpha: cgfloat) throws -> uiimage {
        uigraphicsbeginimagecontextwithoptions(size, false, scale)
        draw(at: cgpointzero, blendmode: .normal, alpha: alpha)
        guard let newimage = uigraphicsgetimagefromcurrentimagecontext() else{
            uigraphicsendimagecontext()
            throw imageerror.unabletoaddaplhatoimage
        }
        uigraphicsendimagecontext()
        return newimage
    }
    enum imageerror: localizederror{
        case unabletoaddaplhatoimage
    }
}

1 being a dark image and 0 being a transparent image, openai is looking for 0.
the method above will change the entire uiimage, if you use this the entire image will be replaced.
a mask will only have sections of an image that have a value of 0.
the png you are showing doesn't show any pixels as zero.
in short, with the code above if image.hasalpha == false you will get the error you are talking about when you submit.
if if image.hasalpha == true then it is ok with openai.
in javascript you can use jimp to simplify the whole thing, you can do all the checks in just a few lines.
dall e api error: ""invalid input image - format must be in ['rgba', 'la', 'l'], got rgb.""",https://stackoverflow.com/questions/76041048,ios,18-04-2023 04:13,1434.0,-1.0,1.0,True,19-04-2023 16:11,18-04-2023 23:11
75943880,what is the most efficient way to identify text similarity between items in large lists of strings in python?,"the following piece of code achieves the results i'm trying to achieve. there is a list of strings called 'lemmas' that contains the accepted forms of a specific class of words. the other list, called 'forms' contains a lot of spelling variations of words found in a large amount of texts from different periods and different dialects of a specific language. for each one of the words in 'forms', i want to get the string in 'lemmas' that is the closest match.
the script, as i said, seems to work well with some test lists i've constructed. the problem i have, though, is that when i use the real lists, which are rather large, it takes forever to produce the results. in fact, i have had to stop the execution of the program because it was taking already more than two hours and the computer was becoming very slow and i couldn't do anything else.
what could i do to make this more efficient? how would i have to modify the code using other python tools or libraries to make this faster? thanks in advance.
    import textdistance
    from textdistance import hamming
    from textdistance import cosine
    from textdistance import jaro_winkler
    import heapq
    
    # 'lemmas' is a list containing a huge amount of words, basically dictionary entries
    # 'forms' is a huge list of spelling variations of words found in hundreds of texts
    
    distances = {}
    
    processed_pairs = set() # keep track of processed pairs
    for lemma in lemmas:
        if lemma is none:
            continue
        lemma_lower = lemma.lower()
        for form in forms:
            if form is none:
                continue        
            form_lower = form.lower()
            pair = (lemma_lower, form_lower) # create a tuple with the lowercase pair
            if pair not in processed_pairs: # check if the pair has been processed before
                processed_pairs.add(pair)
                if textdistance.hamming.normalized_similarity(lemma_lower, form_lower) > 0.34 and textdistance.jaro_winkler(lemma_lower, form_lower) > 0.7 and textdistance.cosine(lemma_lower, form_lower) > 0.5:             
                    dist = hamming.normalized_similarity(lemma_lower, form_lower)
                    distances.setdefault(form_lower, []).append((dist, lemma_lower))
    
    # find the closest pairs
    closest_pairs = {}
    for form, dist_lemmas in distances.items():
        closest_pairs[form] = heapq.nsmallest(2, dist_lemmas)
    
    with open(root / 'potential_lemmas.txt', 'w') as f:
        for form, pairs in closest_pairs.items():
            for dist, lemma in pairs:
                f.write(f""{form} ï¿½ï¿½ï¿½  {lemma}: {dist}\n"")
             


edit:
in the end, the solution that worked the best was an integration of @kyle f hartzenberg's proposal with @jamie_b suggestion of using joblib to parallelize (see comments after code, though):
from itertools import zip_longest
from bisect import insort
from joblib import parallel, delayed
import line_pr

profile = line_profiler.lineprofiler()

emmas = ['gran', 'vermell', 'groc', 'atens', 'do', 'done', 'purpose', 'can', 'be', 'use', 'for', 'cannon', 'amuse', 'useful', 'user', 'become', 'downtown', 'develop', 'fulminate', 'deduce', 'de', 'bezant']

forms = ['preriarenos', 'marinara', 'grand', 'gran', 'grans', 'grands', 'grandeses', 'grandullons', 'grand', 'grandissisimus', 'gran', 'grans', 'grands', 'grandeses', 'grandullons', 'grandullon', 'grandullones', 'uermell', 'uermells', 'vermell', 'vermells', 'vermella', 'vermelles', 'varmellï¿½ï¿½ssimes', 'uarmellï¿½ï¿½ssimes', 'uermellï¿½ï¿½ssimes', 'uarnellï¿½ï¿½ssimes', 'varmellï¿½ï¿½ssima', 'uermella', 'uarmella', 'uarnella', 'varnella', 'uarnellas', 'varnellas', 'varmella', 'uermelles', 'grog', 'grogues', 'done', 'done', 'doing', 'purposeful', 'canonical', 'becareful', 'being', 'berate', 'best', 'bezant', 'full', 'fulmination', 'predict', 'downgrade', 'down', 'developing', 'deduct', 'deducing']

distances = {}

@delayed
def calc_distances(form, lemmas_low):
   
    for lemma in lemmas_low:
        char_matches = [c1 != c2 for c1, c2 in zip_longest(lemma, form)]
        dist = 1 - (sum(char_matches)/len(char_matches))
        if dist > 0.25:
            insort(form_distances, (dist, lemma))
    return (form, form_distances)

@profile
def profile_distance_calcs():
    lemmas_low = [lemma.lower() for lemma in lemmas]
    forms_low = [form.lower() for form in forms]
    results = parallel(n_jobs=-1, prefer=""threads"")(calc_distances(form, lemmas_low) for form in forms_low)
    for form, form_distances in results:
        distances[form] = form_distances

    with open(""potential_lemmas_hamming-like.txt"", ""w"") as f:
        for form, form_distances in distances.items():
            for dist, lemma in reversed(form_distances[-2:]):
                f.write(f""{form} ï¿½ï¿½ï¿½  {lemma}: {dist}\n"")

if __name__ == ""__main__"":
    profile_distance_calcs()
    profile.print_stats()


this was a huge improvemen everything i had tried before. besides the test with the short lists in the example, i ran it with the actual lists containing around 190,000 strings and the processing time was 118 minutes. while i'm pretty sure this could be improved (one might look for ways to do it using some kind of vectorization - someone suggested using arrays from numpy or ai oriented libraries), for the time being, this is quite manageable. there is still a problem that doesn't have to do with efficiency.
i mention this in my comment to @jqurious below but i will explain it here in more detail. running the script above with the test list, one gets results like the following:
berate ï¿½ï¿½ï¿½  bezant: 0.5
berate ï¿½ï¿½ï¿½  become: 0.5

from a linguistic point of view, any english speaker would know that these pairs of words are not related (ok, unless you know about the history of the language and know that be- used to be a productive prefix). what i'm trying to do with this script is to determin be the appropriate lemma (the dictionary form or representative word) for all the variants of a particular word found in the texts of a corpus.
this is a diachronic corpus containing many texts from many different authors and from many different dialects of a language writen over a period of more than 5 centuries. a 'u' could often be used instead of 'v' or a 'y' instead of an 'i'. an 'h' can als be often be missing from a word that is spelt with 'h' even in the same text by the same author. the variation is huge and yet even a modern speaker of the languate can usually detect whether the words are related quite easily. of course, the speaker of the language is knowledgeable about the word structure and the morphology and so can immediately see that, for instance, 'uermellï¿½ï¿½ssima' is related to 'vermell' despite the fact that a lot of characters are different.
using kyle's suggestion with the actual lists, i got results like the following:
beato ï¿½ï¿½ï¿½  beat: 0.8
beatriï¿½iu: 0.5714285714285714
beatriï¿½ï¿½ ï¿½ï¿½ï¿½  teatral: 0.5714285714285714
beatte ï¿½ï¿½ï¿½  beats: 0.6666666666666667
beatus ï¿½ï¿½ï¿½  certus: 0.6666666666666667
beatï¿½ï¿½ssim ï¿½ï¿½ï¿½  nequï¿½ï¿½ssim: 0.6666666666666667
beatï¿½ï¿½ssim ï¿½ï¿½ï¿½  gravï¿½ï¿½ssim: 0.6666666666666667

even if you don't know the language (medieval catalan in case anybody is interested), you can see how this is very wrong (using other algorithms like the levenshtein or the cosine distance it is just hopeless). the lemmas 'beat' or 'beats' should ideally be the ones selected as being the ""closest"" in all these cases. yet the algorithm does what it does.
perhaps i haven't looked hard enough, but with all the work in nlp, i'm surprised there aren't other algorithms that could do better in this kind of scenario. i know this deviates a little bit from the main point in the original question but if anybody can give me some","['python', 'text', 'nlp', 'cosine-similarity', 'hamming-distance']",75946347,"the following solution is based on your original code (hamming distance) which offers an (almost) order of magnitude speed-up (~89.41%), averaged across five runs of each, as measured by line-profiler. using this solution as a base for parallel processing may get you closer to the total processing times you are after.
to use line-profiler, pip install line-profiler and then run kernprof -l -v test.py after adding @profile and calling the function to be profiled from __main__.
from itertools import zip_longest
from bisect import insort

lemmas = [""do"", ""done"", ""purpose"", ""can"", ""be"", ""use"", ""for"", ""cannon"", ""amuse"", ""useful"", ""user"", ""become"", ""downtown"", ""develop"", ""fulminate"", ""deduce"", ""de"", ""bezant""]
forms = [""done"", ""done"", ""doing"", ""purposeful"", ""canonical"", ""becareful"", ""being"", ""berate"", ""best"", ""bezant"", ""full"", ""fulmination"", ""predict"", ""downgrade"", ""down"", ""developing"", ""deduct"", ""deducing""]
distances = {}

@profile
def profile_distance_calcs():
    lemmas_low = [lemma.lower() for lemma in lemmas]
    forms_low = [form.lower() for form in forms]
    for form in forms_low:
        form_distances = []
        for lemma in lemmas_low:
            char_matches = [c1 != c2 for c1, c2 in zip_longest(lemma, form)]
            dist = 1 - (sum(char_matches)/len(char_matches))
            if dist > 0.25:
                insort(form_distances, (dist, lemma))
        distances[form] = form_distances

    with open(""potential_lemmas_hamming.txt"", ""w"") as f:
        for form, form_distances in distances.items():
            for dist, lemma in reversed(form_distances[-2:]):
                f.write(f""{form} ï¿½ï¿½ï¿½  {lemma}: {dist}\n"")

if __name__ == ""__main__"":
    profile_distance_calcs()

from the time profile breakdown below (total time: 0.00122992 s), you can get an idea of where the slow-downs are coming from.
the main culprit is (obviously) the distance computation which is why i switched the textdistance.hamming.normalized_similarity for a much more efficient (barebones) manual calculation of the same thing based on the textdistancehamming and hamming.normalized_similarity source code. i also believe using bisect.insort and maintaining a sorted list while inserting is faster than inserting all elements and then running heapq.nlargest.
line #      hits         time  per hit   % time  line contents
==============================================================
    10                                           @profile
    11                                           def profile_distance_calcs():
    12         1          7.9      7.9      0.6      lemmas_low = [lemma.lower() for lemma in lemmas]
    13         1          7.0      7.0      0.6      forms_low = [form.lower() for form in forms]
    14        18          1.8      0.1      0.1      for form in forms_low:
    15        18          2.0      0.1      0.2          form_distances = []
    16       324         33.4      0.1      2.7          for lemma in lemmas_low:
    17       324        844.5      2.6     68.7              char_matches = [c1 != c2 for c1, c2 in zip_longest(lemma, form)]
    18       324        155.6      0.5     12.7              dist = 1 - (sum(char_matches)/len(char_matches))
    19       285         44.4      0.2      3.6              if dist > 0.25:
    20        39         12.3      0.3      1.0                  insort(form_distances, (dist, lemma))
    21        18          4.7      0.3      0.4          distances[form] = form_distances
    22
    23         1         52.5     52.5      4.3      with open(""potential_lemmas_hamming.txt"", ""w"") as f:
    24        17          4.2      0.2      0.3          for form, form_distances in distances.items():
    25        26         11.5      0.4      0.9              for dist, lemma in reversed(form_distances[-2:]):
    26        26         48.3      1.9      3.9                  f.write(f""{form} ï¿½ï¿½ï¿½  {lemma}: {dist}\n"")

original code speed profile
here is your original code for comparison. i modified some aspects of it, the main difference is the use of heapq.nlargest as i believe you were after the 2 most similar lemmas for each form and not the 2 least similar which heapq.nsmallest provided.
from textdistance import hamming, cosine, jaro_winkler
import heapq

lemmas = [""do"", ""done"", ""purpose"", ""can"", ""be"", ""use"", ""for"", ""cannon"", ""amuse"", ""useful"", ""user"", ""become"", ""downtown"", ""develop"", ""fulminate"", ""deduce"", ""de"", ""bezant""]
forms = [""done"", ""done"", ""doing"", ""purposeful"", ""canonical"", ""becareful"", ""being"", ""berate"", ""best"", ""bezant"", ""full"", ""fulmination"", ""predict"", ""downgrade"", ""down"", ""developing"", ""deduct"", ""deducing""]
distances = {}
processed_pairs = set() # keep track of processed pairs

@profile
def profile_distance_calcs():
    for lemma in lemmas:
        if lemma is none:
            continue
        lemma_lower = lemma.lower()
        for form in forms:
            if form is none:
                continue        
            form_lower = form.lower()
            pair = (lemma_lower, form_lower)
            if pair not in processed_pairs:
                processed_pairs.add(pair)
                dist = hamming.normalized_similarity(lemma_lower, form_lower)
                if dist > 0.25: 
                    distances.setdefault(form_lower, []).append((dist, lemma_lower))

    # find the closest pairs
    closest_pairs = {}
    for form, dist_lemmas in distances.items():
        closest_pairs[form] = heapq.nlargest(2, dist_lemmas)

    with open(""potential_lemmas_orig.txt"", ""w"") as f:
        for form, pairs in closest_pairs.items():
            for dist, lemma in pairs:
                f.write(f""{form} ï¿½ï¿½ï¿½  {lemma}: {dist}\n"")

if __name__ == ""__main__"":
    profile_distance_calcs()

time profile breakdown for the original code (total time: 0.0114992 s):
line #      hits         time  per hit   % time  line contents
==============================================================
    11                                           @profile
    12                                           def profile_distance_calcs():
    13        18          2.4      0.1      0.0      for lemma in lemmas:
    14        18          1.9      0.1      0.0          if lemma is none:
    15                                                       continue
    16        18          6.4      0.4      0.1          lemma_lower = lemma.lower()
    17       324         38.8      0.1      0.3          for form in forms:
    18       324         32.6      0.1      0.3              if form is none:
    19                                                           continue
    20       324        108.2      0.3      0.9              form_lower = form.lower()
    21       324         46.9      0.1      0.4              pair = (lemma_lower, form_lower)
    22       306         60.2      0.2      0.5              if pair not in processed_pairs:
    23       306         92.0      0.3      0.8                  processed_pairs.add(pair)
    24       306      10828.9     35.4     94.2                  dist = hamming.normalized_similarity(lemma_lower, form_lower)
    25       270         47.5      0.2      0.4                  if dist > 0.25:
    26        36         24.1      0.7      0.2                      distances.setdefault(form_lower, []).append((dist, lemma_lower))
    27
    28                                               # find the closest pairs
    29         1          0.2      0.2      0.0      closest_pairs = {}
    30        16          4.3      0.3      0.0      for form, dist_lemmas in distances.items():
    31        16         72.7      4.5      0.6          closest_pairs[form] = heapq.nlargest(2, dist_lemmas)
    32
    33         1         72.3     72.3      0.6      with open(""potential_lemmas_orig.txt"", ""w"") as f:
    34        16          4.2      0.3      0.0          for form, pairs in closest_pairs.items():
    35        26          6.5      0.3      0.1              for dist, lemma in pairs:
    36        26         49.0      1.9      0.4                  f.write(f""{form} ï¿½ï¿½ï¿½  {lemma}: {dist}\n"")

measuring natural language similarity
measuring the similarity between two pieces of natural language text is a non-trivial task. attempting to gauge spelling/morphological/semantic similarity based purely on rudimentary character-based metrics (e.g. hamming distance, levenshtein distance etc.) won't suffice as these metrics fail to capture complex linguistic patterns (hence why neural network methods are commonly used to pick up these patterns in large bodies of text). with that being said, one can begin to add their own ""rules"" to calculate more ""accurate"" similarity scores. for example, the code below modifies the normalised hamming similarity computation to track how many consecutive characters matd then scales the ""similarity score"" accordingly. there is obviously scope for fine-tuning and/or increasing the complexity/number of rules used, but with more complexity comes slower processing times. this custom function avoids the issue of results like beatte ï¿½ï¿½ï¿½ beats: 0.667 and beatus ï¿½ï¿½ï¿½ certus: 0.667, instead scoring them as beatte ï¿½ï¿½ï¿½ beats 0.79167 and beatus ï¿½ï¿½ï¿½ certu""lang-py prettyprint-override"">def custom_hamming_norm_sim(stra, strb, scale=0.5):
    max_str_len = max(len(stra), len(strb))
    max_score_per_char = 1 / max_str_len
    penalty = 1
    score = 0
    for c1, c2 in zip_longest(stra, strb):
        if c1 != c2:
            penalty = penalty * scale
            score += max_score_per_char * penalty
        else:
            p = penalty / scale
            if p < max_score_per_char:
                penalty = p
            score += max_score_per_char * penalty
    return score


@profile
def profile_distance_calcs():
    lemmas_low = [lemma.lower() for lemma in lemmas]
    forms_low = [form.lower() for form in forms]
    for form in forms_low:
        form_distances = []
        for lemma in lemmas_low:
            dist = custom_hamming_norm_sim(lemma, form)
            if dist > 0.25:
                insort(form_distances, (dist, lemma))
        distances[form] = form_distances

    with open(""potential_lemmas_hamming.txt"", ""w"") as f:
        for form, form_distances in distances.items():
            for dist, lemma in reversed(form_distances[-2:]):
                f.write(f""{form} ï¿½ï¿½ï¿½  {lemma}: {dist}\n"")

if __name__ == ""__main__"":
    profile_distance_calcs()
<",https://stackoverflow.com/questions/75943880,python,05-04-2023 20:54,3214.0,3.0,2.0,True,18-04-2023 12:32,09-04-2023 10:33
79154295,llama-3.1-nemotron-70b-instruct not working with langchain chatnvidia,"userwarning: found nvidia/llama-3.1-nemotron-70b-instruct in
available_models, but type is unknown and inference may fail.
warnings.warn(

python code to call model
## core lc chat interface
from langchain_nvidia_ai_endpoints import chatnvidia
llm = chatnvidia(model=""nvidia/llama-3.1-nemotron-70b-instruct"")
result = llm.invoke(""write a ballad about langchain."")
print(result.content)","['python', 'langchain']",79177231,"you can use chatopenai to invoke:
from langchain_openai import chatopenai
from openai import openai
import os
chatopenai(
    client=openai(
        base_url = ""
        api_key=os.getenv(""nvidia_api_key""),
    ).chat.completions,
).invoke(messages, model=""nvidia/llama-3.1-nemotron-70b-instruct"")

nvidia example",https://stackoverflow.com/questions/79154295,python,04-11-2024 05:35,205.0,0.0,2.0,True,13-11-2024 13:15,13-11-2024 13:15
77082604,error inserting spacy.tokens.span.span into pandas dataframe,"using scispacy, trying to use the hearst patterns feature, which returns a spacy.tokens.span.span object. when trying to get the result into a datafame i get an error,  object is treated as several words and not as a single object.
following the example -
import spacy
from scispacy.hyponym_detector import hyponymdetector

nlp = spacy.load(""en_core_sci_sm"")
nlp.add_pipe(""hyponym_detector"", last=true, config={""extended"": false})

doc = nlp(""keystone plant species such as fig trees are good for the soil."")

print(doc._.hearst_patterns)
>>> [('such_as', keystone plant species, fig trees)]
print(type(doc_hp[0][1]))
>>> <class 'spacy.tokens.span.span'>

doc_hp = doc._.hearst_patterns
dict = {
    ""hp_connector"": doc_hp[0][0],
    ""hp_entity_1"":doc_hp[0][1],
    ""hp_entity_2"":doc_hp[0][2],
}

df = pd.dataframe.from_dict(dict)

throws an error:
traceback (most recent call last):
  file ""extract_hearst_patterns.py"", line 42, in <module>
    df = pd.dataframe.from_dict(dict)
  file ""/venv/lib/python3.9/site-packages/pandas/core/frame.py"", line 1760, in from_dict
    return cls(data, index=index, columns=columns, dtype=dtype)
  file ""/venv/lib/python3.9/site-packages/pandas/core/frame.py"", line 709, in __init__
    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)
  file ""/venv/lib/python3.9/site-packages/pandas/core/internals/construction.py"", line 481, in dict_to_mgr
    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)
  file ""/venv/lib/python3.9/site-packages/pandas/core/internals/construction.py"", line 115, in arrays_to_mgr
    index = _extract_index(arrays)
  file ""/venv/lib/python3.9/site-packages/pandas/core/internals/construction.py"", line 655, in _extract_index
    raise valueerror(""all arrays must be of the same length"")
valueerror: all arrays must be of the same length","['python', 'dataframe', 'nlp', 'spacy', 'spacy-3']",77171874,"this ended up working for me -
doc_hp = doc._.hearst_patterns
for pattern in doc_hp:
    patten_dict = get_pattern_dict(full_sent, pattern)
    patten_dict = {
        ""hp object"": [patten],
        ""hp_connector"": str(patten[0]),
        ""hp_entity_1"": patten[1].text,
        ""hp_entity_2"": patten[2].text,
    }
    list_of_pattern_dicts.append(patten_dict)
df = pd.dataframe.from_dict(list_of_pattern_dicts)",https://stackoverflow.com/questions/77082604,python,11-09-2023 14:41,54.0,-1.0,1.0,True,25-09-2023 10:30,25-09-2023 10:22
78237430,integrating gpt-4 with team foundation server for data insights,"i'm exploring options to integrate gpt-4, the latest version of openai's powerful language model, with team foundation server (tfs). my objective is to leverage gpt-4's capabilities to gain insights from the data stored within our tfs environment.
i'm wondering if there are any apis or existing integrations available that facilitate this process. specifically, i'm interested in extracting data from tfs and feeding it into gpt-4 for analysis and generating insights.
could anyone provide guidance on whether such an api or integration exists? if not, are there any alternative approaches or workarounds that could achieve similar results?
any advice, resources, or experiences related to integrating gpt-4 with tfs for data analysis and insights?
i tried exporting the data from tfs and then feeding the same data to chat gpt, but exported data doe s not contain all the required details and also exported data is very huge, so it crosses the prompt limit. my expectation is to have an api, which can directly be integrated with tfs server. like we can integrate 365 copilot with office and github co - pilot with codebase.","['artificial-intelligence', 'openai-api', 'gpt-3', 'gpt-4']",78238614,"there isn't a direct integration between openai's gpt models and team foundation server, not that i am aware off. but to utilize gpt-4 for gaining insights from tfs data, you would generally need to build a custom integration.
you will use the tfs/azure rest api to programatically retrieve the data you are interested in. then the data will be processed and cleaned so unnecessary data will not be sent to gpt. once it is done you can send the data to gpt api using gpt-4-0125-preview which has context window of 128000 tokens can generate output upto 4096 tokens",https://stackoverflow.com/questions/78237430,artificial-intelligence,28-03-2024 10:19,99.0,-1.0,1.0,True,18-04-2024 11:37,18-04-2024 11:37
64977817,gerund form of a word in python,"i'd like to get the gerund form of a string. i have not found a straightforward way to invoke a library to get the gerund.
i applied the rules for words ending in 'ing`, but because i am getting some errors due to exceptions. then, i am checking against the cmu words to ensure the generated gerund word is correct. the code looks as follows:
import cmudict
import re

ing= 'ing'
vowels = ""aeiou""
consonants = ""bcdfghjklmnpqrstvwxyz""
words=['lead','take','hit','begin','stop','refer','visit']
cmu_words= cmudict.words()
g_w = []

for word in words:
    if word[-1] == 'e':
        if word[:-1] + ing in cmu_words:
            g_w.append(word[:-1] + ing)             
    elif count_syllables(word) == 1 and word[-2] in vowels and word[-1] in consonants:
        if word.__len__()>2 and word[-3] in vowels:
            if word + ing in cmu_words:
                g_w.append(word + ing)                 
        else:
            if word + word[-1] + ing in cmu_words:
                g_w.append(word + word[-1] + ing)
    elif count_syllables(word)>1 and word[-2] in vowels and word[-1] in consonants:
        if word + word[-1]+ ing in cmu_words:
            g_w.append(word + word[-1]+ ing)            
        else:
            if word + ing in cmu_words:
                g_w.append(word + ing) 
    
print(g_w)

the rules are as follow:
when a verb ends in ""e"", drop the ""e"" and add ""-ing"". for example: ""take + ing = taking"".
when a one-syllable verb ends in vowel + consonant, double the final consonant and add ""-ing"". for example: ""hit + ing = hitting"".
when a verb ends in vowel + consonant with stress on the final syllable, double the consonant and add ""-ing"". for example: ""begin + ing = beginning"".
do not double the consonant of words with more than one syllable if the stress is not on the final

is there a more efficient way to get the gerunds of a string if exists?
thanks","['python', 'nlp', 'nltk', 'porter-stemmer']",64978015,"maybe this is what you are looking for. library called pyinflect

a python module for word inflections that works as a spacy extension. to use standalone, import the method getallinflections and/or getinflection and call them directly. the method getinflection takes a lemma and a penn treebank tag and returns a tuple of the specific inflection(s) associated with it.

there is a variety of tags available for getting inflections including the 'vbg' tag (verb, gerund) you are looking for.
pos_type = 'a'
* jj      adjective
* jjr     adjective, comparative
* jjs     adjective, superlative
* rb      adverb
* rbr     adverb, comparative
* rbs     adverb, superlative

pos_type = 'n'
* nn      noun, singular or mass
* nns     noun, plural

pos_type = 'v'
* vb      verb, base form
* vbd     verb, past tense
* vbg     verb, gerund or present participle
* vbn     verb, past participle
* vbp     verb, non-3rd person singular present
* vbz     verb, 3rd person singular present
* md      modal

here is a sample implementation.
#!pip install pyinflect
from pyinflect import getinflection

words = ['lead','take','hit','begin','stop','refer','visit']
[getinflection(i, 'vbg') for i in words]

[('leading',),
 ('taking',),
 ('hitting',),
 ('beginning',),
 ('stopping', 'stoping'),
 ('referring',),
 ('visiting',)]

note: the authors have setup a more sophisticated and benchmarked library which does both lemmatization and inflections called lemminflect. do check this out if you want something more reliable than the above library. the syntax is pretty much the same as above.",https://stackoverflow.com/questions/64977817,python,23-11-2020 22:56,1528.0,3.0,1.0,True,08-01-2023 01:27,08-01-2023 01:27
49917033,built-in function to get the frequency of one word with spacy?,"i'm looking for faster alternatives to nltk to analyze big corpora and do basic things like calculating frequencies, pos tagging etc... spacy seems great and easy to use in many ways, but i can't find any built-in function to count the frequency of a specific word for example. i've looked at the spacy documentation, but i can't find a straightforward way to do it. am i missing something?
what i would like would be the nltk equivalent of:
tokens.count(""word"") #where tokens is the tokenized text in which the word is to be counted

in nltk, the above code would tell me that in my text, the word ""word"" appears x number of times.
note that i've come by the count_by function, but it doesn't seem to do what i'm looking for.","['python', 'nlp', 'spacy']",60714164,"i use spacy for frequency counts in corpora quite often. this is what i usually do:
import spacy
nlp = spacy.load(""en_core_web_sm"")

list_of_words = ['run', 'jump', 'catch']

def word_count(string):
    words_counted = 0
    my_string = nlp(string)

    for token in my_string:
        # actual word
        word = token.text
        # lemma
        lemma_word = token.lemma_
        # part of speech
        word_pos = token.pos_
        if lemma_word in list_of_words:
            words_counted += 1
            print(lemma_word)
    return words_counted


sentence = ""i ran, jumped, and caught the ball.""
words_counted = word_count(sentence)
print(words_counted)",https://stackoverflow.com/questions/49917033,python,19-04-2018 09:07,7349.0,4.0,3.0,True,14-02-2023 23:57,19-04-2018 09:16
8097383,implementing a top down parser in c#,"i am student and i want to implement a top-down parser in my c# language translation project. for example, if i need to construct a parser tree for the sentence ""my name is husni and i am a student"", how can i do it with c#?","['c#', 'artificial-intelligence', 'nlp']",8097485,after the book you can also find interesting to read about a compiler generator as antlr that can help you to write the compiler ( also in c# ) and browsing the ast even visually.,https://stackoverflow.com/questions/8097383,c#,11-11-2011 17:02,4087.0,1.0,4.0,True,28-01-2022 15:58,28-01-2022 15:58
68797316,python: extract non-english words and iterate it over a dataframe,"i have a table of about 30,000 rows and need to extract non-english words from a column named dummy_df  from a dummy_df dataframe. i need to put the non-english words in an adjacent column named non_english.
a dummy data is as thus:
dummy_df = pandas.dataframe({'outcome':    [""i want to go to church"",  ""i love matauranga"", ""take me to  oranga tamariki""]})

my idea is to extract non-english words from a sentence, and then iterate the process over a dataframe. i was able to accurately extract non-english words from a sentence with this code:
import nltk
nltk.download('words')
from nltk.corpus import words

words = set(nltk.corpus.words.words())

sent = ""i love matauranga""
"" "".join(w for w in nltk.wordpunct_tokenize(sent) \
         if not w.lower() in words or not w.isalpha())

the result of the above code is 'matauranga' which is perfectly correct.
but when i try to iterate the code over a dataframe using this code:
import nltk
nltk.download('words')
from nltk.corpus import words

def no_english(text):
  words = set(nltk.corpus.words.words())
  "" "".join(w for w in nltk.wordpunct_tokenize(text['outcome']) \
         if not w.lower() in words or not w.isalpha())

dummy_df['non_english'] = dummy_df.apply(no_english, axis = 1)
print(dummy_df)

i got an undesirable result in that the non_english column has none value instead of the desired non-english words (see below):
                       outcome non_english
0       i want to go to church        none
1            i love matauranga        none
2  take me to  oranga tamariki        none
3                                     none

instead, the desired result should be:
                       outcome        non_english
0       i want to go to church        
1            i love matauranga        matauranga
2  take me to  oranga tamariki        oranga tamariki","['python', 'pandas', 'text', 'nlp', 'nltk']",68797328,"you are missing the return in your function:
import nltk
nltk.download('words')
from nltk.corpus import words

def no_english(text):
    words = set(nltk.corpus.words.words())
    return "" "".join(w for w in nltk.wordpunct_tokenize(text['outcome']) \
           if not w.lower() in words or not w.isalpha())

dummy_df['non_english'] = dummy_df.apply(no_english, axis = 1)
print(dummy_df)

output:
                       outcome      non_english
0       i want to go to church                 
1            i love matauranga       matauranga
2  take me to  oranga tamariki  oranga tamariki",https://stackoverflow.com/questions/68797316,python,16-08-2021 04:08,437.0,1.0,1.0,True,25-05-2022 11:22,25-05-2022 11:22
61085302,can inverted index have multiple words in one entry?,"in information retrieval, the inverted index has entries which are the words of corpus, and each word has a posting list which is the list of documents it appears in.
if stemming is applied, index entry would be a stem, so multiple words may finally map to the same entry if they share the same stem. for example:
without stemming:
(slowing) --> [d1, d5, d9,...]

(slower) --> [d9, d10, d20,...]

(slow) --> [d2,...]

with stemming:
(slow) --> [d1, d2, d5, d9, , d10, d20...]

i want to avoid stemming, and instead would like to make each entry in my inverted index as a bag of words (inflections) such as (slow, slows, slowing, slowed, slower, slowest). for example:
(slow, slows, slowing, slowed, slower, slowest) --> [d1, d2, d5, d9, , d10, d20...]

would that be possible and feasible or not?","['information-retrieval', 'stemming', 'inverted-index']",61098980,"short answer:
simply avoid stemming to suit your need of not considering slow and slows to be a match.
long answer:
question: 
i want to avoid stemming, and instead would like to make each entry in my inverted index as a bag of words (inflections) such as (slow, slows, slowing, slowed, slower, slowest).

let me try to clear some confusion that you have about inverted lists. it is the documents that are stored in the postings for each term (not the terms themselves). 
the words are typically stored in a in-memory dictionary (implemented with a hash-table or a trie) containing pointers to the postings (list of documents which contain that particular term) stored and loaded on the fly from secondary storage.
a simple example (without showing document weights):

(information) --> [d1, d5, d9,...]
(informative) --> [d9, d10, d20,...]
(retrieval) --> [d1, d9, d17,...]
..

so, if you don't want to apply stemming, that's fine... in fact, the above example shows an unstemmed index, where the words information and informative appear in their non-conflated forms. in a conflated term index (with a stemmer or a lemmatizer), you would substitute the different forms with an equivalent representation (say inform). in that case, the index will be:

(inform) --> [d1, d5, d9, d10, d20...]. --- union of the different forms
(retrieval) --> [d1, d9, d17,...]
..

so, this conflated representation matches all possible forms of the word information, e.g. informative, informational etc.
longer answer
now let's say you want to achieve the best of both worlds, i.e. a representation which allows this conflation to be done in a user controlled way, e.g. wrapping a word around quotes to denote requiring an exact match (""slow""vs.slowin the query), or some indicator to include synonyms for a query term for semantic search (e.g.syn(slow)` to include synonyms of the word slow).
for this, you need to maintain separate postings for the non-conflated words and maintain additional equivalence indicating pointers between a set of equivalent (stem relation/synonym relation/ semantic relation etc.) terms.
coming back to our example, you would have something like:
(e1)-->(information) --> [d1, d5, d9,...]
 |---->(informative) --> [d9, d10, d20,...]
 |---->(data) --> [d20, d23, d25,...]


(e2)-->(retrieval) --> [d1, d9, d17,...]
 |---->(search) --> [d20, d30, d31,...]

..

here, i have shown two examples of equivalence classes (concept representations) of two sets of terms information, data... and retrieval, search....
depending on the query syntax, it would then be possible at the retrieval time to facilitate exact search or relaxed search (based on inflections/synonyms etc.)",https://stackoverflow.com/questions/61085302,information-retrieval,07-04-2020 16:56,751.0,0.0,1.0,True,27-04-2022 15:09,27-04-2022 15:09
64685243,getting sentence embedding from huggingface feature extraction pipeline,"how do i get an embedding for the whole sentence from huggingface's feature extraction pipeline?
i understand how to get the features for each token (below) but how do i get the overall features for the sentence as a whole?
feature_extraction = pipeline('feature-extraction', model=""distilroberta-base"", tokenizer=""distilroberta-base"")
features = feature_extraction(""i am sentence"")","['machine-learning', 'nlp', 'huggingface-transformers', 'spacy-transformers']",64714245,"to explain more on the comment that i have put under stackoverflowuser2010's answer, i will use ""barebone"" models, but the behavior is the same with the pipeline component.
bert and derived models (including distilroberta, which is the model you are using in the pipeline) agenerally indicate the start and end of a sentence with special tokens (mostly denoted as [cls] for the first token) that usually are the easiest way of making predictions/generating embeddings over the entire sequence. there is a discussion within the community about which method is superior (see also a more detailed answer by stackoverflowuser2010 here), however, if you simply want a ""quick"" solution, then taking the [cls] token is certainly a valid strategy.
now, while the documentation of the featureextractionpipeline isn't very clear, in your example we can easily compare the outputs, specifically their lengths, with a direct model call:
from transformers import pipeline, autotokenizer

# direct encoding of the sample sentence
tokenizer = autotokenizer.from_pretrained('distilroberta-base')
encoded_seq = tokenizer.encode(""i am sentence"")

# your approach
feature_extraction = pipeline('feature-extraction', model=""distilroberta-base"", tokenizer=""distilroberta-base"")
features = feature_extraction(""i am sentence"")

# compare lengths of outputs
print(len(encoded_seq)) # 5
# note that the output has a weird list output that requires to index with 0.
print(len(features[0])) # 5

when inspecting the content of encoded_seq, you will notice that the first token is indexed with 0, denoting the beginning-of-sequence token (in our case, the embedding token). since the output lengths are the same, you could then simply access a preliminary sentence embedding by doing something like
sentence_embedding = features[0][0]",https://stackoverflow.com/questions/64685243,machine-learning,04-11-2020 17:52,16444.0,4.0,4.0,True,29-12-2023 11:26,06-11-2020 03:53
78609056,openai assistants api: how do i pass the assistant to the api?,"i am trying to pass my assistant to the openai assistants api, but i get the following error when doing so:

message: 'the model my model id does not exist or you do not have
access to it.',
type: 'invalid_request_error',
param: null,
code: 'model_not_found'

code:
try {
    const fetch = await import('node-fetch').then(mod => mod.default);

    const response = await fetch(' {
        method: 'post',
        headers: {
            'content-type': 'application/json',
            'authorization': `bearer ${apikey}`
        },
        body: json.stringify({
            model: 'my model id',
            messages: conversation
        })
    });
}

if i change the model parameter to gpt-3.5-turbo it works, and my app can interface with the standard model.
i know the api key is correct and it has all permissions. the assistant id i'm trying to pass to the model parameter is also correct. for testing purposes, both are defined in the same file.","['javascript', 'openai-api', 'openai-assistants-api']",78612505,"problem
the openai assistants api doesn't use the chat completions api endpoint (i.e.,  using the openai assistants api is fundamentally different (i.e., more complex) than using any other apis, like the completions api or chat completions api.
solution
you don't pass the assistant by using the model parameter. you pass the assistant by using the assistant_id parameter (see step 4 below).
here are the steps you need to follow to get a response from the assistant:
step 1: create an assistant
post 

step 2: create a thread
post 

step 3: add the user's question to the thread
post 

step 4: run the assistant
post 

step 5: periodically retrieve the run to check its status to see if it has moved to completed
get 

step 6: retrieve the assistant's answer
get 

also, i've made a youtube tutorial on how to use the assistants api and posted the code on my github profile.",https://stackoverflow.com/questions/78609056,javascript,11-06-2024 17:44,890.0,2.0,1.0,True,05-08-2024 10:24,12-06-2024 13:15
35596031,gensim word2vec: find number of words in vocabulary,"after training a word2vec model using python gensim, how do you find the number of words in the model's vocabulary?","['python', 'neural-network', 'nlp', 'gensim', 'word2vec']",35641434,"in recent versions, the model.wv property holds the words-and-vectors, and can itself can report a length ï¿½ï¿½ï¿½ the number of words it contains. so if w2v_model is your word2vec (or doc2vec or fasttext) model, it's enough to just do:
vocab_len = len(w2v_model.wv)

if your model is just a raw set of word-vectors, like a keyedvectors instance rather than a full word2vec/etc model, it's just:
vocab_len = len(kv_model)

other useful internals in gensim 4.0+ include model.wv.index_to_key, a plain list of the key (word) in each index position, and model.wv.key_to_index, a plain dict mapping keys (words) to their index positions.
in pre-4.0 versions, the vocabulary was in the vocab field of the word2vec model's wv property, as a dictionary, with the keys being each token (word). so there it was just the usual python for getting a dictionary's length:
len(w2v_model.wv.vocab)

in very-old gensim versions before 0.13 vocab appeared directly on the model. so way back then you would use w2v_model.vocab instead of w2v_model.wv.vocab.
but if you're still using anything from before gensim 4.0, you should definitely upgrade! there are big memory & performance improvements, and the changes required in calling code are relatively small ï¿½ï¿½ï¿½ some rgs & moves, covered in the 4.0 migration notes.",https://stackoverflow.com/questions/35596031,python,24-02-2016 07:39,97358.0,55.0,5.0,True,23-05-2023 06:04,26-02-2019 18:28
70161048,python - how to loop through each index position in a list?,"given a list [[[""source1""], [""target1""], [""alignment1""]], [""source2""], [""target2""], [""alignment2""]], ...] , i want to extract the words in the source that align with the words in the target.
for example, in the english-german sentence pair the hat is on the table . - der hut liegt auf dem tisch ., i want to print the following:
the - der
hat - hut
is - liegt
on - auf
the - dem
table - tisch
. - . 

so i have written the following:
en_de = [
[['the', 'hat', 'is', 'on', 'the', 'table', '.'], ['der', 'hut', 'liegt', 'auf', 'dem', 'tisch', '.'], '0-0 1-1 2-2 3-3 4-4 5-5 6-6'], 
[['the', 'picture', 'is', 'on', 'the', 'wall', '.'], ['das', 'bild', 'hï¿½ï¿½ngt', 'an', 'der', 'wand', '.'], '0-0 1-1 2-2 3-3 4-4 5-5 6-6'], 
[['the', 'bottle', 'is', 'under', 'the', 'sink', '.'], ['die', 'flasche', 'ist', 'under', 'dem', 'waschbecken', '.'], '0-0 1-1 2-2 3-3 4-4 5-5 6-6']
]

for group in en_d   src_sent = group[0]
    tgt_sent = group[1]
    aligns = group[2]

    split_aligns = aligns.split()

    hyphen_split = [align.split(""-"") for align in split_aligns]

    align_index = hyphen_split[0]

    print(src_sent[int(align_index[0])],""-"", tgt_sent[int(align_index[1])])

this prints, as expected, the words in index position 0 of src_sent and tgt_sent:
the - der
the - das
the - die

now, i don't know how i can print the words of all index positions of src_sent and tgt_sent. obviously, i could manually update align_index to a new index position for each position in the sentence pair, but on the full dataset, some sentences will have up to 25 index positions.
is there a way to possibly for-loop through each index position?
when i try:
align_index = hyphen_split[0:]
print(src_sent[int(align_index[0])],""-"", tgt_sent[int(align_index[1])])

i get a typeerror:  int() argument must be a string, a bytes-like object or a number, not 'list'
it's clear that align_index can't be a list, but i'm not sure how to convert it into something that will do what i want it to do.
any advice or help would be greatly appreciated. thank you in advance.","['python', 'for-loop', 'nlp', 'linguistics']",70161318,"you are forgetting to loop over your hyphen_split list:
for group in en_de:
    src_sent = group[0]
    tgt_sent = group[1]
    aligns = group[2]

    split_aligns = aligns.split()

    hyphen_split = [align.split(""-"") for align in split_aligns]

    for align_index in hyphen_split:
        print(src_sent[int(align_index[0])],""-"", tgt_sent[int(align_index[1])])

see the last two lines, updated from your code.",https://stackoverflow.com/questions/70161048,python,29-11-2021 20:39,650.0,1.0,2.0,True,29-11-2021 21:06,29-11-2021 20:58
33374010,does word2vec has a hidden layer?,"when i am reading one of papers of tomas mikolov: 
i have one concern on the continuous bag-of-words model sectionï¿½ï¿½ï¿½

the first proposed architecture is similar to the feedforward nnlm, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged).

i find some people mention that there is a hidden layer in word2vec model, but from my understanding, there is only one projection layer in that model. does this projection layer do the same work as hidden layer?
the another question is that how to project input data into the projection layer?","['machine-learning', 'nlp', 'neural-network', 'word2vec']",33786063,"from the original paper, section 3.1, it is clear that there is no hidden layer:

""the first proposed architecture is similar to the feedforward nnlm
  where the non-linear hidden layer is removed and the projection layer is shared for all words"".

with respect to your second question (what does sharing the projection layer means), it means that you consider only one single vector, which is the centroid of the vectors of all the words in context. thus, instead of having n-1 word vectors as input, you consider only one vector. this is why it is called continuous bag of words (because word order is lost within the context of size n-1).",https://stackoverflow.com/questions/33374010,machine-learning,27-10-2015 16:57,3105.0,11.0,1.0,True,06-03-2025 03:22,06-03-2025 03:22
72516835,word frequency over time : how to count the word frequency by date?,"i have a data frame look like this :




date
text




201901
thank you for helping me


201902
you  are amazing


201902
for helping with this




my aim is to calculate the word frequency in each line, and eventually look like this:




date
thank
you
for
helping
me
are
amazing
with
this
for




201901
1
1
1
1
1
0
0
0
0
0


201902
0
1
1
1
0
1
1
1
1
1




the actual data set is like this frame, but contains millions of text lines. so i was wondering how to automate this process using r, without typing all those texts lines.","['r', 'nlp', 'word-frequency']",72516955,"using r and tidyverse:
df <- data.frame(date = c(201901, 201902, 201902),
                 text = c(""thank you for helping me"", ""you are amazing"", ""for helping with this""))

library(tidyverse)

if you want your data as a table of counts
df %>% 
            separate_rows(text, sep = "" "") %>% 
            mutate(text = tolower(text)) %>% 
            table()

output:
text
date     amazing are for helping me thank this with you
  201901       0   0   1       1  1     1    0    0   1
  201902       1   1   1       1  0     0    1    1   1

if you want your output as a tibble
df %>% 
        separate_rows(text, sep = "" "") %>% 
        mutate(text = tolower(text)) %>% 
        table() %>% 
        as_tibble() %>% 
        pivot_wider(names_from = text, values_from = n)

output:
# a tibble: 2 x 10
  date   amazing   are `for` helping    me thank  this  with   you
  <chr>    <int> <int> <int>   <int> <int> <int> <int> <int> <int>
1 201901       0     0     1       1     1     1     0     0     1
2 201902       1     1     1       1     0     0     1     1     1

edit: to transform everything to lowercase as your desired output and to show you the output
edit2: to show you that you can also get your data as a tibble to further work with it",https://stackoverflow.com/questions/72516835,r,06-06-2022 11:14,243.0,-1.0,2.0,True,06-06-2022 11:57,06-06-2022 11:55
78959794,"how to convert the result from openai call, convert it into json and write to .txt file?","i am very new to python and only know the basics, so basically i am calling openai and getting a response in return and want to write that response in a .txt file.
i want to convert the response in json before writing in the file. my response is already in json format but weird when print it shows json format with json {}  with it, this is my script
def get_json(image_file, category):
    with open(image_file, ""rb"") as image:
        response = openai_client.chat.completions.create(
            model=""gpt-4-vision-preview"",
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": [
                        {""type"": ""text"", ""text"": f""analyze this image and provide the following attributes: color theme, font style, and a short description of about 4-7 words. categorize it as {category}. return the result as a json object.""},
                        {""type"": ""image_url"", ""image_url"": {""url"": f""data:image/jpeg;base64,{base64.b64encode(image.read()).decode()}""}},
                    ],
                }
            ],
            temperature=1,
            max_tokens=4095,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0,
        ) 
        return response.choices[0].message.content
     
with open(file, 'a') as file:
    for filename in os.listdir(images_folder):
        filepath = os.path.join(images_folder, filename)
        result =get_json(filepath, 'hero')
        file.write(result + '\n')
        json_result = json.loads(result)
        print(json_result)

this is the result i am getting
enter image description here
i want to remove the text '''json'''
tried to convert it into json by json.loads(result) but getting the following error:-
raise jsondecodeerror(""expecting value"", s, err.value) from none
json.decoder.jsondecodeerror: expecting value: line 1 column 1 (char 0)","['python', 'json', 'openai-api']",78959861,"you are asking it to return a json object in you prompt, ""return the result as a json object."" that it why! if you give it the same prompt using the website, you will notice that the response is nicely formatted, that is because of those ""```json ...content ```"" markdown formatting.
you can solve it using two methods:

explicitly replace ""```json"" and ""```"" with empty string:

import json

# for example, this is the content in response:
response = '```json{""color_theme"": ""mint green and black"",""font_style"": ""sans-serif"",""short_description"": ""web publishing platform"",""category"": ""hero""}```'

# replace and assign back to original content
response = response.replace(""```json"", """")
response = response.replace(""```"", """")

# don't forget to convert to json as it is a string right now:
json_result = json.loads(response)


using slicing:

import json

# for example, this is the content in response:
response = '```json{""color_theme"": ""mint green and black"",""font_style"": ""sans-serif"",""short_description"": ""web publishing platform"",""category"": ""hero""}```'

# ""```json"" is 7 character long, but slicing count start from 0. ""{"" is at 7th character.
# ""```"" is 3 character long (at the end).
response = response[7:-3]

# don't forget to convert to json as it is a string right now:
json_result = json.loads(response)


and for writing to the .txt file, you can use json.dump() as follows:
import json

response = '```json{""color_theme"": ""mint green and black"",""font_style"": ""sans-serif"",""short_description"": ""web publishing platform"",""category"": ""hero""}```'

response = response[7:-3]
response = json.loads(response)

# write to response.txt file (overwriting it).
with open(""response.txt"", ""w"") as file:
    json.dump(response, file)",https://stackoverflow.com/questions/78959794,python,07-09-2024 09:22,1758.0,0.0,1.0,True,08-09-2024 19:34,08-09-2024 19:34
39109743,adding new text to sklearn tfidif vectorizer (python),"is there a function to add to the existing corpus? i've already generated my matrix, i'm looking to periodically add to the table without re-crunching the whole sha-bang
e.g;
articlelist = ['here is some text blah blah','another text object', 'more foo for your bar right now']
tfidf_vectorizer = tfidfvectorizer(
                        max_df=.8,
                        max_features=2000,
                        min_df=.05,
                        preprocessor=prep_text,
                        use_idf=true,
                        tokenizer=tokenize_text
                    )
tfidf_matrix = tfidf_vectorizer.fit_transform(articlelist)

#### adding a new article to existing set?
bigger_tfidf_matrix = tfidf_vectorizer.fit_transform(['the last article i wanted to add'])","['python', 'scikit-learn', 'tf-idf']",39114555,"you can access the vocabulary_ attribute of your vectoriser directly, and you can access the idf_ vector via _tfidf._idf_diag, so it would be possible to monkey-patch something like this:
import re 
import numpy as np
from scipy.sparse.dia import dia_matrix
from sklearn.feature_extraction.text import tfidfvectorizer

def partial_fit(self, x):
    max_idx = max(self.vocabulary_.values())
    for a in x:
        #update vocabulary_
        if self.lowercase: a = a.lower()
        tokens = re.findall(self.token_pattern, a)
        for w in tokens:
            if w not in self.vocabulary_:
                max_idx += 1
                self.vocabulary_[w] = max_idx

        #update idf_
        df = (self.n_docs + self.smooth_idf)/np.exp(self.idf_ - 1) - self.smooth_idf
        self.n_docs += 1
        df.resize(len(self.vocabulary_))
        for w in tokens:
            df[self.vocabulary_[w]] += 1
        idf = np.log((self.n_docs + self.smooth_idf)/(df + self.smooth_idf)) + 1
        self._tfidf._idf_diag = dia_matrix((idf, 0), shape=(len(idf), len(idf)))

tfidfvectorizer.partial_fit = partial_fit
articlelist = ['here is some text blah blah','another text object', 'more foo for your bar right now']
vec = tfidfvectorizer()
vec.fit(articlelist)
vec.n_docs = len(articlelist)
vec.partial_fit(['the last text i wanted to add'])
vec.transform(['the last text i wanted to add']).toarray()

# array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
#          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
#          0.        ,  0.        ,  0.27448674,  0.        ,  0.43003652,
#          0.43003652,  0.43003652,  0.43003652,  0.43003652]])",https://stackoverflow.com/questions/39109743,python,23-08-2016 20:00,3421.0,15.0,2.0,True,20-07-2023 08:36,24-08-2016 03:41
79160811,python: compare html tags in ro folder with their corresponding tags in en folder and displays in output the unique tags from both files,"in short, i have two files, one in romanian, the other has been translated into english. in the ro file there are some tags that have not been translated into en. so i want to display in an html output all the tags in en that have corresponding tags in ro, but also those tags in ro that do not appear in en.
i have this files:
   ro_file_path = r'd:\3\ro\incotro-vezi-tu-privire.html'
   en_file_path = r'd:\3\en\where-do-you-see-look.html'
   output =  d:\3\output\where-do-you-see-look.html 

task: compare the 3 tags below, in both files.
<p class=""text_obisnuit"">(.*?)</p>
<p class=""text_obisnuit2"">(.*?)</p>
<p class=""text_obisnuit""><span class=""text_obisnuit2"">(.*?)</span>(.*?)</p>

requirements:

all tags are enclosed between: <!-- start article --> and <!-- final article -->
count the tags in ro and count the tags in en, and compare.
then count the words in the tags in ro and compare with the number of words in the tags in en.
compares the html tags in ro with the html tags in en, in order, and displays in output the unique tags from both files

ro       d:\3\ro\incotro-vezi-tu-privire.html
<!-- articol start --> 
<p class=""text_obisnuit""><span class=""text_obisnuit2"">stiu ca este dificil sa conduci la inceput, </span>dar dupa 4-5 luni inveti.</p> 
<p class=""text_obisnuit2"">imi place sa merg la scoala si sa invat, mai ales in timpul saptamanii.</p> 
<p class=""text_obisnuit"">sunt un bun conducator auto, dar am facut si greseli din care am invatat.</p> 
<p class=""text_obisnuit"">ï¿½ï¿½n fond, cele scrise de mine, sunt adevarate.</p> 
<p class=""text_obisnuit""&gtesc sa conduc masina.</p> 
<p class=""text_obisnuit""><span class=""text_obisnuit2"">ma iubesti?</p> 
<p class=""text_obisnuit""><span class=""text_obisnuit2"">stiu ca este dificil sa conduci la inceput, </span>dar dupa 4-5 luni inveti.</p> 
<p class=""text_obisnuit"">totul se repetï¿½ï¿½, chiar ï¿½ï¿½i ochii care nu se vad.</p> 
<p class=""text_obisnuit2"">bee servesc o cafea 2 mai buna</p> 
<!-- articol final -->

   

en    d:\3\en\where-do-you-see-look.html
<!-- articol start -->
<p class=""text_obisnuit2"">i like going to school and learning, especially during the week.</p>
<p class=""text_obisnuit"">i'm a good driver, but i've also made mistakes that i've learned from.</p>
<p class=""text_obisnuit"">basically, what i wrote is true.</p>
<p class=""text_obisnuigt;i love driving.</p>
<p class=""text_obisnuit""><span class=""text_obisinuit2"">i know it's difficult to drive at first, </span> but after 4-5 months you learn.</p>
<p class=""text_obisnuit"">everything is repeated, even the eyes that can't see.</p>
<!-- articol final -->

expected output:  d:\3\output\where-do-you-see-look.html
<!-- articol start -->
<p class=""text_obisnuit""><span class=""text_obisnuit2"">stiu ca este dificil sa conduci la inceput, </span> dar dupa 4-5 luni inveti.</p> 
<p class=""text_obisnuit2"">i like going to school and learning, especially during the week.</p>
<p class=""text_obisnuit"">i'm a good driver, but i've also made mistakes that i've learned from.</p>
<p class=""text_obisnuit"">basically, what i wrote is true.</p>
<p class=""text_obisnuit""><span class=""text_obisnuit2"">ma iubesti?</p> 
<p class=""text_obisnuit"">i love driving.</p>
<p class=""text_obisnuit""><span class=""text_obisinuit2"">i know it's difficult to drive at first, </span> but after 4-5 months you learn.</p>
<p class=""text_obisnuit"">everything is repeated, even the eyes that can't see.</p>
<p class=""text_obisnuit2"">bee servesc o cafea 2 mai buna</p> 
<!-- articol final -->

python code must compares the html tags in ro with the html tags in en and displays in output the unique tags in both files, taking into account that most of the tags in ro have their corresponding translation in the tags in en. but the idea of ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½the code is that the code also finds those html tags in ro that were omitted from being translated into en.
here's how i came up with the solution in python code. i followed a simple calculation.ong>first method:
first, you have to count all the tags in ro, then all the tags in en.
then you have to memorize each type of tag in ro, but then also in en.
then you have to count the words in each tag in ro and the words in each tag in en.
don't forget that there can be 2 identical tags, but on different lines, just like in ro.
then you have to statistically calculate the result. how much are the tags in ro minus the tags in en?
the second method, to verify the output, is to print the screen. compare the entire ro part and the entire en part separately through ocr, then line by line, see which tags in ro are plus compared to the tags in en
python code:
import re
import os

def extract_tags(content):
    start = content.find('<!-- articol start -->')
    end = content.find('<!-- articol final -->')
    if start == -1 or end == -1:
        raise valueerror(""marcajele 'articol start' sau 'articol final' lipsesc."")

    section_content = content[start:end]
    pattern = re.compile(r'<p class=""text_obisnuit(?:2)?"">(?:<span class=""text_obisnuit2"">)?.*?</p>', re.dotall)
    tags = []

    for idx, match in enumerate(pattern.finditer(section_content), 1):
        tag = match.group(0)
        text = re.sub(r'<[^>]+>', '', tag).strip()

        if '<span class=""text_obisnuit2"">' in tag or '<span class=""text_obisinuit2"">' in tag:
            tag_type = 'span'
        elif 'class=""text_obisnuit2""' in tag:
            tag_type = 'text_obisnuit2'
        else:
            tag_type = 'text_obisnuit'

        tags.append({
            'index': idx,
            'tag': tag,
            'text': text,
            'type': tag_type,
            'word_count': len(text.split())
        })
    return tags

def find_matching_pairs(ro_tags, en_tags):
    matched_indices = set()
    used_en = set()

    for i, ro_tag in enumerate(ro_tags):
        for j, en_tag in enumerate(en_tags):
            if j in used_en:
                continue

            if ro_tag['type'] == en_tag['type']:
                word_diff = abs(ro_tag['word_count'] - en_tag['word_count'])
                if word_diff <= 3:
                    matched_indices.add(i)
                    used_en.add(j)
                    break
    return matched_indices

def fix_duplicates(output_content, ro_content):
    """"""corecteazï¿½ï¿½ poziï¿½ï¿½ia tag-urilor duplicate""""""
    ro_tags = extract_tags(ro_content)
    output_tags = extract_tags(output_content)

    # gï¿½ï¿½sim tag-urile care apar ï¿½ï¿½n ro ï¿½ï¿½i output
    for ro_idx, ro_tag in enumerate(ro_tags):
        for out_idx, out_tag in enumerate(output_tags):
            if ro_tag['tag'] == out_tag['tag'] and ro_idx != out_idx:
                # am gï¿½ï¿½sit un tag care apare ï¿½ï¿½n poziï¿½ï¿½ii diferite
                # verificï¿½ï¿½m dacï¿½ï¿½ este cazul de duplicat care trebuie mutat
                          out_lines = output_content.split('\n')

                if ro_tag['tag'] in ro_lines[ro_idx+1] and out_tag['tag'] in out_lines[out_idx+1]:
                    # mutï¿½ï¿½m tag-ul la poziï¿½ï¿½ia corectï¿½ï¿½
                    out_lines.remove(out_tag['tag'])
                    out_lines.insert(ro_idx+1, out_tag['tag'])
                    output_content = '\n'.join(out_lines)
                    break

    return output_content

def generate_output(ro_tags, en_tags, original_content):
    start = original_content.find('<!-- articol start -->')
    end = original_content.find('<!-- articol final -->')
    if start == -1 or end == -1:
        raise valueerror(""marcajele 'articol start' sau 'articol final' lipsesc."")

    output_content = original_content[:start + len('<!-- articol start -->')] + ""\n""
    matched_indices = find_matching_pairs(ro_tags, en_tags)
    en_index = 0

    for i, ro_tag in enumerate(ro_tags):
        if i in matched_indices:
         ntent += en_tags[en_index]['tag'] + ""\n""
            en_index += 1
        else:
            output_content += ro_tag['tag'] + ""\n""

    while en_index < len(en_tags):
        output_content += en_tags[en_index]['tag'] + ""\n""
        en_index += 1

    output_content += original_content[end:]
    return output_content

def main():
    try:
        ro_file_path = r'd:\3\ro\incotro-vezi-tu-privire.html'
        en_file_path = r'd:\3\en\where-do-you-see-look.html'
        output_file_path = r'd:\3\output\where-do-you-see-look.html'

        with open(ro_file_path, 'r', encoding='utf-8') as ro_file:
            ro_content = ro_file.read()
        with open(en_file_path, 'r', encoding='utf-8') as en_file:
            en_content = en_file.read()

        ro_tags = extract_tags(ro_content)
        en_tags = extract_tags(en_content)

        # generï¿½ï¿½m primul output
        initial_output = generate_output(ro_tags, en_tags, en_content)

        # corectï¿½ï¿½m poziï¿½ï¿½iile tag-uate
        final_output = fix_duplicates(initial_output, ro_content)

        with open(output_file_path, 'w', encoding='utf-8') as output_file:
            output_file.write(final_output)

        print(f""output-ul a fost generat la {output_file_path}"")

    except exception as e:
        print(f""eroare: {str(e)}"")

if __name__ == ""__main__"":
    main()

my python code is almost perfect, but not perfect. the problem occurs when i introduce other tags in ro, such as:
<!-- articol start --> 
<p class=""text_obisnuit"">laptopul meu este de culoare neagra.</p>
<p class=""text_obisnuit2"">imi place sa merg la scoala si sa invat, mai ales in timpul saptamanii.</p> 
<p class=""text_obisnuit"">sunt un bun conducator auto, dar am facut si greseli din care am invatat.</p> 
<p class=""text_obisnuit""><span class=""text_obisnuit2"">stiu ca este dificil sa conduci la inceput, </span>dar dupa 4-5 luni inveti.</p>
<p class=""text_obisnuit"">ï¿½ï¿½n fond, cele scrise de mine, sunt adevarate.</p> 
<p class=""text_obisnuit"">iubesc sa conduc masina.</p> 

<p class=""text_obisnuit""><span class=""text_obisnuit2"">stiu ca este dificil sa conduci la inceput, </span>dar dupa 4-5 luni inveti.</p>
<p class=""text_obisnuit"">totul se repetï¿½ï¿½, chiar ï¿½ï¿½i ochii care nu se vad.</p> 

<!-- articol final -->
</","['python', 'python-3.x', 'openai-api', 'claude']",79191744,"second, and the best solution.
finally i solved the problem, but not with chatgpt or claude. no other ai could find the solution, because it didn't know how to think about the solution.
in fact, to find the solution to this problem, you had to assign some identifiers to each tag, and do multiple searches.
chatgpt or claude, or other ais, will have to seriously consider this type of solution for such problems.
here are the specifications, the way i thought about solving the problem. it's a different way of thinking about doing parsings.

python code made by a friend of mine. i think the solution, he made the code:
from bs4 import beautifulsoup
import re

def count_words(text):
    """"""numï¿½ï¿½rï¿½ï¿½ cuvintele dintr-un text.""""""
    return len(text.strip().split())

def get_greek_identifier(word_count):
    """"""determinï¿½ï¿½ identificatorul grecesc bazat de cuvinte.""""""
    if word_count < 7:
        return 'ï¿½ï¿½'
    elif word_count <= 14:
        return 'ï¿½ï¿½'
    else:
        return 'ï¿½ï¿½'

def get_tag_type(tag):
    """"""determinï¿½ï¿½ tipul tagului (a, b, sau c).""""""
    if tag.find('span'):
        return 'a'
    elif 'text_obisnuit2' in tag.get('class', []):
        return 'b'
    return 'c'

def analyze_tags(content):
    """"""analizeazï¿½ï¿½ tagurile ï¿½ï¿½i returneazï¿½ï¿½ informaï¿½ï¿½ii despre fiecare tag.""""""
    soup = beautifulsoup(content, 'html.parser')
    tags_info = []

    article_content = re.search(r'<!-- articol start -->(.*?)<!-- articol final -->',
                              content, re.dotall)

    if article_content:
        content = article_content.group(1)
        soup = beautifulsoup(content, 'html.parser')

    for i, tag in enumerate(soup.find_all('p', recursive=false)):
        text_content = tag.get_text(strip=true)
        tag_ word_count = count_words(text_content)
        greek_id = get_greek_identifier(word_count)

        tags_info.append({
            'number': i + 1,
            'type': tag_type,
            'greek': greek_id,
            'content': str(tag),
            'text': text_content
        })

    return tags_info

def compare_tags(ro_tags, en_tags):
    """"""comparï¿½ï¿½ tagurile ï¿½ï¿½i gï¿½ï¿½seï¿½ï¿½te diferenï¿½ï¿½ele.""""""
    wrong_tags = []
    i = 0
    j = 0

    while i < len(ro_tags):
        ro_tag = ro_tags[i]
        if j >= len(en_tags):
            wrong_tags.append(ro_tag)
            i += 1
            continue

        en_tag = en_tags[j]

        if ro_tag['type'] != en_tag['type']:
            wrong_tags.append(ro_tag)
            i += 1
            continue

        i += 1
        j += 1

    return wrong_tags

def format_results(wrong_tags):
    """"""formateazï¿½ï¿½ rezultatele pentru afiï¿½ï¿½are ï¿½ï¿½i salvare.""""""
    type_counts ype_content = {'a': [], 'b': [], 'c': []}

    for tag in wrong_tags:
        type_counts[tag['type']] += 1
        type_content[tag['type']].append(tag['content'])

    # creï¿½ï¿½m rezultatul formatat
    result = []

    # prima linie cu sumarul
    summary_parts = []
    for tag_type in ['a', 'b', 'c']:
        if type_counts[tag_type] > 0:
            summary_parts.append(f""{type_counts[tag_type]} taguri de tipul ({tag_type})"")
    result.append(""in ro exista in plus fata de en urmatoarele: "" + "" si "".join(summary_parts))

    # detaliile pentru fiecare tip de tag
    for tag_type in ['a', 'b', 'c']:
        if type_counts[tag_type] > 0:
            result.append(f""\n{type_counts[tag_type]}({tag_type}) adica asta {'taguri' if type_counts[tag_type] > 1 else 'tag'}:"")
            for content in type_content[tag_type]:
                result.append(content)
            result.append("""")  # linie goalï¿½ï¿½ pentru separare

    return ""\n"".ult)

def merge_content(ro_tags, en_tags, wrong_tags):
    """"""combinï¿½ï¿½ conï¿½ï¿½inutul ro ï¿½ï¿½i en, inserï¿½ï¿½nd tagurile wrong ï¿½ï¿½n poziï¿½ï¿½iile lor originale.""""""
    merged_tags = []

    # creï¿½ï¿½m un dicï¿½ï¿½ionar pentru tagurile wrong indexat dupï¿½ï¿½ numï¿½ï¿½rul lor original
    wrong_dict = {tag['number']: tag for tag in wrong_tags}

    # parcurgem poziï¿½ï¿½iile ï¿½ï¿½i decidem ce tag sï¿½ï¿½ punem ï¿½ï¿½n fiecare poziï¿½ï¿½ie
    current_en_idx = 0
    for i in range(max(len(ro_tags), len(en_tags))):
        position = i + 1

        # verificï¿½ï¿½m dacï¿½ï¿½ aceastï¿½ï¿½ poziï¿½ï¿½ie este pentru un tag wrong
        if position in wrong_dict:
            merged_tags.append(wrong_dict[position]['content'])
        elif current_en_idx < len(en_tags):
            merged_tags.append(en_tags[current_en_idx]['content'])
            current_en_idx += 1

    return merged_tags

def save_results(merged_content, results, output_path):
    """"""s""""
    final_content = '<!-- rezultate analiza -->\n'
    final_content += '<!-- articol start -->\n'

    # adaugï¿½ï¿½ conï¿½ï¿½inutul combinat
    for tag in merged_content:
        final_content += tag + '\n'

    final_content += '<!-- articol final -->\n'
    final_content += '<!-- final rezultate analiza -->\n'

    # adaugï¿½ï¿½ rezultatele analizei
    final_content += results

    # salveazï¿½ï¿½ ï¿½ï¿½n fiï¿½ï¿½ier
    with open(output_path, 'w', encoding='utf-8') as file:
        file.write(final_content)

# citeï¿½ï¿½te fiï¿½ï¿½ierele
with open(r'd:/3/ro/incotro-vezi-tu-privire.html', 'r', encoding='utf-8') as file:
    ro_content = file.read()

with open(r'd:/3/en/where-do-you-see-look.html', 'r', encoding='utf-8') as file:
    en_content = file.read()

# defineï¿½ï¿½te calea pentru fiï¿½ï¿½ierul de output
output_path = r'd:/3/output/where-do-you-see-look.html'

# analizeazï¿½ï¿½ tagurile
ro_tags = analyze_tags(ro_content)
en_tags = analyze_tags(en_content)

# gags(ro_tags, en_tags)

# formateazï¿½ï¿½ rezultatele
results = format_results(wrong_tags)

# genereazï¿½ï¿½ conï¿½ï¿½inutul combinat
merged_content = merge_content(ro_tags, en_tags, wrong_tags)

# afiï¿½ï¿½eazï¿½ï¿½ rezultatele ï¿½ï¿½n consolï¿½ï¿½
print(results)

# salveazï¿½ï¿½ rezultatele ï¿½ï¿½n fiï¿½ï¿½ierul de output
save_results(merged_conte",https://stackoverflow.com/questions/79160811,python,05-11-2024 22:01,53.0,0.0,2.0,True,15-11-2024 09:51,08-11-2024 07:14
77360174,map bert token indices to spacy token indices,"iï¿½ï¿½ï¿½m trying to make bertï¿½ï¿½ï¿½s (bert-base-uncased) tokenization token indices (not ids, token indices) map to spacyï¿½ï¿½ï¿½s tokenization token indices. in the following example, my approach doesnï¿½ï¿½ï¿½t work becos spacyï¿½ï¿½ï¿½s tokenization behaves a bit more complex than i anticipated. thoughts on solving this?
import spacy
from transformers import berttokenizer
tokenizer = berttokenizer.from_pretrained(""bert-base-uncased"")
nlp = spacy.load(""en_core_web_sm"")

sent = nlp(""britain's railways cost ï¿½ï¿½20.7bn during the 2020-21 financial year, with ï¿½ï¿½2.5bn generated through fares and other income, ï¿½ï¿½1.3bn through other sources and ï¿½ï¿½16.9bn from government, figures released by the regulator the office of rail and road (orr) on november 30 revealed."")
# get spacy word index to bert token indice mapping
wd_to_tok_map = [wd.i for wd in sent for el in tokenizer.encode(wd.text, add_special_tokens=false)]
len(sent) # 55
len(wd__ids = tokenizer.encode(sent.text, add_special_tokens=false)
len(input_ids) # 65

i can print both tokenizations and look for perfect text matches, but the problem i run into is what if a word repeats twice in the tokenization? looking for a word match will return two indices at different sections of the sentence.
[el.text for el in sent]
['britain', ''s', 'railways', 'cost', 'ï¿½ï¿½', '20.7bn', 'during', 'the', '2020', '-', '21', 'financial', 'year', ',', 'with', 'ï¿½ï¿½','2.5bn','generated','through', 'fares', 'and','other', 'income', ',', 'ï¿½ï¿½', '1.3bn', 'through', 'other', 'sources', 'and', 'ï¿½ï¿½', '16.9bn', 'from', 'government', ',', 'figures', 'released', 'by', 'the', 'regulator', 'the', 'office', 'of', 'rail', 'and', 'road', '(', 'orr', ')', 'on', 'november', '30', 'revealed', '.']

[tokenizer.ids_to_tokens[el] for el in input_ids]
['britain',''', 's', 'railways', 'cost', 'ï¿½ï¿½2', '##0', '.', '7', '##bn', 'during', 'the', '2020', '-', '21', 'financial', 'year', ',', '5', '##bn', 'generated', 'through', 'fares', 'and', 'other', 'income', ',', 'ï¿½ï¿½1', '.', '3', '##bn', 'through', 'other', 'sources', 'and', 'ï¿½ï¿½1', '##6', '.', '9', '##bn', 'from', 'government', ',', 'figures', 'released', 'by', 'the', 'regulator', 'the', 'office', 'of', 'rail', 'and', 'road', '(', 'orr', ')', 'on', 'november', '30', 'revealed', '.']

decode() doesnï¿½ï¿½ï¿½t seem to give me what i want, as iï¿½ï¿½ï¿½m aft","['python', 'mapping', 'spacy', 'tokenize', 'bert-language-model']",77361610,"use a fast tokenizer to get the character offsets directly from the transformer tokenizer with return_offsets_mapping=true, and then map those to the spacy tokens however you'd like:
from transformers import autotokenizer
tokenizer = autotokenizer.from_pretrained(""bert-base-uncased"")
text = ""britain's railways cost ï¿½ï¿½20.7bn""
output = tokenizer([text], return_offsets_mapping=true)

print(output[""input_ids""])
# [[101, 3725, 1005, 1055, 7111, 3465, 21853, 2692, 1012, 1021, 24700, 102]]

print(tokenizer.convert_ids_to_tokens(output[""input_ids""][0]))
# ['[cls]', 'britain', ""'"", 's', 'railways', 'cost', 'ï¿½ï¿½2', '##0', '.', '7', '##bn', '[sep]']

print(output[""offset_mapping""])
# [[(0, 0), (0, 7), (7, 8), (8, 9), (10, 18), (19, 23), (24, 26), (26, 27), (27, 28), (28, 29), (29, 31), (0, 0)]]
</code",https://stackoverflow.com/questions/77360174,python,25-10-2023 13:58,203.0,0.0,1.0,True,25-10-2023 17:10,25-10-2023 14:47
77646675,trying to create vectors and chunked data using azure cognitive search/ azure ai search,"i am trying to create a search solution using azure ai search/cognitive search. i need to chunk the data, so that the retrieved text is limited and more relevant. i also need to implement a hybrid search over the data hence trying out the embedding creation as well.
i tried using splittext and azureopenaiembedding skillsets however they are not getting indexed. my intention is to use ocr ,key phrase extraction etc and get them collated using merge skill and then do chunking and embeddings over it. also i would like to add incremental indexing [updating the indexes without redoing the whole indexing] as an additional feature.
i am not sure where i am going wrong. any help will be highly appreciated.
the following is what i tried:
def create_skillset(skillset_name, uri, headers):
    print(""trying out updated 2 set ------------------------->"")
    skillset = {
    ""description"": ""skillset created from the portal. skillsetname: azureblob-skillsetn; contentfield: merged_content; enrichmentgranularity: document; knowledgestorestorageaccount: ;"",
    ""skills"": [
        {
        ""@odata.type"": ""#microsoft.skills.text.v3.entityrecognitionskill"",
        ""name"": ""#1"",
        ""context"": ""/document/merged_content"",
        ""categories"": [
            ""product"",
            ""phonenumber"",
            ""person"",
            ""quantity"",
            ""organization"",
            ""ipaddress"",
            ""url"",
            ""email"",
            ""event"",
            ""skill"",
            ""location"",
            ""persontype"",
            ""address"",
            ""datetime""
        ],
        ""defaultlanguagecode"": ""en"",
        ""inputs"": [
            {
            ""name"": ""text"",
            ""source"": ""/document/merged_content""
            }
        ],
        ""outputs"": [
            {
            ""name"": ""persons"",
            ""targetname"": ""people""
            },
            {
            ""name"": ""organizations"",
            ""targetname"": ""organizations""
            },
            {
            ""name"": ""locations"",
            ""targetname"": ""locations""
            }
        ]
        },
        {
        ""@odata.type"": ""#microsoft.skills.text.keyphraseextractionskill"",
        ""name"": ""#2"",
        ""context"": ""/document/merged_content"",
        ""defaultlanguagecode"": ""en"",
        ""inputs"": [
            {
            ""name"": ""text"",
            ""source"": ""/document/merged_content""
            }
        ],
        ""outputs"": [
            {
            ""name"": ""keyphrases"",
            ""targetname"": ""keyphrases""
            }
        ]
        },
        {
          ""@odata.type"": ""#microsoft.skills.text.splitskill"",
          ""textsplitmode"": ""pages"",
          ""maximumpagelength"": 500,
          ""pageoverlaplength"": 50,
          ""maximumpagestotake"": 1,
          ""defaultlanguagecode"": ""en"",
          ""context"": ""/document"",
          ""inputs"": [
            {
              ""name"": ""text"",
              ""source"": ""/document/merged_text""
            }
          ],
          ""outputs"": [
            {
              ""name"": ""textitems"",
              ""targetname"": ""pages""
            }
          ]
        },
        {
        ""@odata.type"": ""#microsoft.skills.text.translationskill"",
        ""name"": ""#3"",
        ""context"": ""/document/merged_content"",
        ""defaulttolanguagecode"": ""en"",
        ""suggestedfrom"": ""en"",
        ""inputs"": [
            {
            ""name"": ""text"",
            ""source"": ""/document/merged_content""
            }
        ],
        ""outputs"": [
            {
            ""name"": ""translatedtext"",
            ""targetname"": ""translated_text""
            }
        ]
        },
        {
        ""@odata.type"": ""#microsoft.skills.text.mergeskill"",
        ""name"": ""#4"",
        ""context"": ""/document"",
        ""insertpretag"": "" "",
        ""insertposttag"": "" "",
        ""inputs"": [
            {
            ""name"": ""text"",
            ""source"": ""/document/content""
            },
            {
            ""name"": ""itemstoinsert"",
            ""source"": ""/document/normalized_images/*/text""
            },
            {
            ""name"": ""offsets"",
            ""source"": ""/document/normalized_images/*/contentoffset""
            }
        ],
        ""outputs"": [
            {
            ""name"": ""mergedtext"",
            ""targetname"": ""merged_content""
            }
        ]
        },
        {
        ""@odata.type"": ""#microsoft.skills.text.azureopenaiembeddingskill"",
        ""description"": ""connects a deployed embedding model."",
        ""resourceuri"": ""
        ""deploymentid"": ""text-embedding-ada-002"",
        ""inputs"": [
          {
            ""name"": ""text"",
            ""source"": ""/document/merged_content""
          }
        ],
        ""outputs"": [
          {
            ""name"": ""embedding""
          }
          ]
         },
        {
        ""@odata.type"": ""#microsoft.skills.vision.ocrskill"",
        ""name"": ""#5"",
        ""context"": ""/document/normalized_images/*"",
        ""lineending"": ""space"",
        ""defaultlanguagecode"": ""en"",
        ""inputs"": [
            {
            ""name"": ""image"",
            ""source"": ""/document/normalized_images/*""
            }
        ],
        ""outputs"": [
            {
            ""name"": ""text"",
            ""targetname"": ""text""
            },
            {
            ""name"": ""layouttext"",
            ""targetname"": ""layouttext""
            }
        ]
        },
        {
        ""@odata.type"": ""#microsoft.skills.vision.imageanalysisskill"",
        ""name"": ""#6"",
        ""context"": ""/document/normalized_images/*"",
        ""defaultlanguagecode"": ""en"",
        ""visualfeatures"": [
            ""tags"",
            ""description""
        ],
        ""details"": [],
        ""inputs"": [
            {
            ""name"": ""image"",
            ""source"": ""/document/normalized_images/*""
            }
        ],
        ""outputs"": [
            {
            ""name"": ""tags"",
            ""targetname"": ""imagetags""
            },
            {
            ""name"": ""description"",
            ""targetname"": ""imagecaption""
            }
        ]
        }
    ],
    ""cognitiveservices"": {
        ""@odata.type"": ""#microsoft.azure.search.cognitiveservicesbykey"",
        ""description"": """",
        ""key"": """"
    }
    }
    resp = requests.put(uri, headers=headers, data=json.dumps(skillset), verify=false)
    logging.info(f""message: {resp.status_code}\n""
                 f""message: {resp.ok}\n"")

def create_index(index_name, uri, headers):
    index = {
    ""fields"": [
        {
        ""name"": ""id"",
        ""type"": ""edm.string"",
        ""key"": true,
        ""searchable"": true,
        ""filterable"": true,
        ""facetable"": true,
        ""sortable"": true
        },
        {
        ""name"": ""metadata_storage_name"",
        ""type"": ""edm.string"",
        ""searchable"": true,
        ""filterable"": false,
        ""facetable"": false,
        ""sortable"": false
        },
        {
        ""name"": ""imagecaption"",
        ""type"": ""collection(edm.string)"",
        ""searchable"": true,
        ""filterable"": false,
        ""facetable"": false,
        ""sortable"": false
        },
        {
        ""name"": ""imagetags"",
        ""type"": ""collection(edm.string)"",
        ""searchable"": true,
        ""filterable"": false,
        ""facetable"": false,
        ""sortable"": false
        },
        {
        ""name"": ""translated_text"",
        ""type"": ""edm.string"",
        ""searchable"": true,
        ""filterable"": false,
        ""facetable"": false,
        ""sortable"": false
        },
        {
        ""name"": ""keyphrases"",
        ""type"": ""collection(edm.string)"",
        ""searchable"": true,
        ""filterable"": false,
        ""facetable"": false,
        ""sortable"": false
        },
        {
        ""name"": ""merged_content"",
        ""type"": ""edm.string"",
        ""searchable"": true,
        ""filterable"": true,
        ""facetable"": true,
        ""sortable"": true
        },
        {
        ""name"": ""pages"",
        ""type"": ""edm.string"",
        ""searchable"": true,
        ""filterable"": true,
        ""facetable"": true,
        ""sortable"": true
        },

    ]
    }
    resp = requests.put(uri, headers=headers, data=json.dumps(index), verify=false)
    logging.info(f""message: {resp.status_code}\n""
             f""message: {resp.ok}\n"")

def create_indexer(indexer_name, datasource_name, index_name, skillset_name, uri, headers):
    indexer = {
    ""name"": indexer_name,
    ""datasourcename"" : datasource_name,
    ""targetindexname"" : index_name,
    ""skillsetname"" : skillset_name,
    ""fieldmappings"" : [
        {
        ""sourcefieldname"" : ""metadata_storage_path"",
        ""targetfieldname"" : ""id"",
        ""mappingfunction"" : {""name"": ""base64encode""}
        },
        {
        ""sourcefieldname"" : ""metadata_storage_name"",
        ""targetfieldname"" : ""metadata_storage_name"",
        }
    ],
    ""outputfieldmappings"" :
    [
        {
        ""sourcefieldname"" : ""/document/merged_content"",
        ""targetfieldname"" : ""merged_content""
        },
        {
        ""sourcefieldname"" : ""/document/pages"",
        ""targetfieldname"" : ""pages""
        },
        {
        ""sourcefieldname"": ""/document/merged_content/translated_text"",
        ""targetfieldname"": ""translated_text""
        },
        {
        ""sourcefieldname"" : ""/document/merged_content/keyphrases"",
        ""targetfieldname"" : ""keyphrases""
        },
        {
        ""sourcefieldname"": ""/document/normalized_images/*/imagetags/*/name"",
        ""targetfieldname"": ""imagetags""
        },
        {
        ""sourcefieldname"": ""/document/normalized_images/*/imagecaption"",
        ""targetfieldname"": ""imagecaption""
        }
    ],
    ""parameters"":
    {
        ""maxfaileditems"": 4,
        ""maxfaileditemsperbatch"": 4,
        ""configuration"":
        {
        ""datatoextract"": ""contentandmetadata"",
        ""parsingmode"": ""default"",
        ""firstlinecontainsheaders"": true,
        ""delimitedtextdelimiter"": "","",
        ""imageaction"": ""generatenormalizedimages""
        }
    }
    }
    resp = requests.put(uri, headers=headers, data=json.dumps(indexer), verify=false)
    logging.info(f""message: {resp.status_code}\n""
                 f""message: {resp.ok}\n"")


*** i am calling these functions like this ***
# delete already exisitng datasource
    uri = f""
    resp = requests.delete(uri, headers=headers, verify=false)

    uri = f""
    # delete already existing indexer
    resp = requests.delete(uri, headers=headers, verify=false)
    con_str=""hjgjhgkj""
    blob_service_client = blobserviceclient.from_connection_string(con_str)


    container_client = blob_service_client.get_container_client(container_name)
    uri = f""
    create_datasource(datasource_name, uri, headers,container_name)
    time.sleep(15)

    uri = f""
    #delete already existing skillset
    resp = requests.delete(uri, headers=headers, verify=false)
    create_skillset(skillset_name, uri, headers)

    uri = f""
    # delete already existing index
    resp = requests.delete(uri, headers=headers, verify=false)
    create_index(index_name, uri, headers)
    uri = f""
    create_indexer(indexer_name, datasource_name, index_name, skillset_name, uri, headers)
    uri = f""
    resp = requests.get(uri, headers=headers, verify=false)


the error i get is as follows:
{""error"":{""code"":""invalidrequestparameter"",""message"":""the request is invalid. details: skillset : one or more skills are invalid. details: unexpected properties found on skill."",""details"":[{""code"":""invalidskillset"",""message"":""one or more skills are invalid. details: unexpected properties found on skill. parameters: skillset""}]}}

the spittext and openai embeddings are the two new skillsets that i added, rest of it are working, i followed the azure documentation while creating these skillsets. please advice on where am i going wrong.","['azure', 'openai-api', 'azure-cognitive-search', 'azure-openai', 'vertex-ai-search']",77655453,"the issue is that, the following 2 parameters are not available, even though it is thus mentioned in azure documentation. take these out and that will fix the splittext skill issue:
""pageoverlaplength"": 50,

""maximumpagestotake"": 1,
for azureopenaiembeddingskill, it is available only on 2023-10-01-preview api. you just need to change the api parameter while creating the skillset, it should work.",https://stackoverflow.com/questions/77646675,azure,12-12-2023 14:01,1262.0,0.0,1.0,True,13-12-2023 17:18,13-12-2023 02:48
72842851,how to use cosine similarity within triplet loss,"the triplet loss is defined as follows:
l(a, p, n) = max(ï¿½ï¿½ï¿½f(a) - f(p)ï¿½ï¿½ï¿½ï¿½ï¿½ - ï¿½ï¿½ï¿½f(a) - f(n)ï¿½ï¿½ï¿½ï¿½ï¿½ + margin, 0)

where a=anchor, p=positive, and n=negative are the data samples in the loss, and margin is the minimum distance between the anchor and positive/negative samples.
i read somewhere that (1 - cosine_similarity) may be used instead of the l2 distance.
note that i am using tensorflow - and the cosine similarity loss is defined that when it is a negative number between -1 and 0, 0 indicates orthogonality and values closer to -1 indicate greater similarity. the values closer to 1 indicate greater dissimilarity. so, it is the opposite of cosine similarity metric.
any suggestions on how to write my triplet loss with cosine similarity?
edit
all good stuff in the answers (comments and answers). based on ok for me:
 self.margin = 1
 self.loss = tf.keras.losses.cosinesimilarity(axis=1)
 ap_distance = self.loss(anchor, positive)
 an_distance = self.loss(anchor, negative)
 loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)

i would like to eventually use the tensorflow addon loss as @pygeek pointed out but i haven't figured out how to pass the data yet.
note
to use it standalone - one must do something like this:
cosine_similarity = tf.keras.metrics.cosinesimilarity()
cosine_similarity.reset_state()
cosine_similarity.update_state(anch_prediction, other_prediction)
similarity = cosine_similarity.result().numpy() 

resources
pytorch cosine embedding layer
tensorflow cosine similarity implmentation
tensorflow triplet loss hard/soft margin","['machine-learning', 'deep-learning', 'nlp', 'artificial-intelligence', 'loss-function']",72879131,"first of all, cosine_distance = 1 - cosine_similarity. the distance and similarity are different. this is not correctly mentioned in some of the answers!
secondly, you should look at the tensorflow code on how the cosine similarity loss is implemented  which is different from pytorch!!
finally, i suggest you use existing loss: you should replace the || ... ||^2 with tf.losses.cosinedistance(...).",https://stackoverflow.com/questions/72842851,machine-learning,02-07-2022 22:37,5042.0,2.0,2.0,True,13-11-2022 19:28,13-11-2022 19:28
78347352,how to make conversationalretrievalchain to include metadata in the prompt using langchain with chromadb to make the llm aware of metadata?,"im trying to do a bot that answer questions from a chromadb , i have stored multiple pdf files with metadata like the filename and candidate name , my problem is when i use conversational retrieval chain the llm model just receive page_content without the metadata , i want the llm model to be aware of the page_content with its metadata like filename and candidate name
here is my code
conversation_chain=conversationalretrievalchain.from_llm(
        llm=llm,
        retriever=selfqueryretriever.from_llm(llm,vectorstore,document_content_description,metadata_field_info),
       
        memory=memory,
        verbose=true,
       
        
    )

and here is my attribute info
metadata_field_info = [
    attributeinfo(
        name=""filename"",
        description=""the name of the resumee"",
        type=""string"",

        
    ),
    attributeinfo(
        name=""candidatename"",
        description=""the name of the candidate"",
        type=""string""
    )

]","['python', 'openai-api', 'langchain', 'large-language-model']",78369897,"i did fix this issue by including document_prompt
    document_combine_prompt = prompttemplate(
     input_variables=[""candidatename"",""page_content""],
     template= """"""
        page_content: {page_content}
        candidatename:{candidatename}
        """"""
)
    conversation_chain=conversationalretrievalchain.from_llm(
        llm=llm,
        retriever=selfqueryretriever.from_llm(llm,vectorstore,document_content_description,metadata_field_info,verbose=true),
        
        memory=memory,
        verbose=true,
        return_source_documents=true,
        combine_docs_chain_kwargs={""prompt"": custom_prompt,
                                   ""document_prompt"":document_combine_prompt
                                   }
    )",https://stackoverflow.com/questions/78347352,python,18-04-2024 12:12,1233.0,0.0,1.0,True,23-04-2024 04:35,19-04-2024 06:25
77521121,validationerror: 1 validation error for sqldatabasetoolkit llm value is not a valid dict,"trying to conenct postgresql with langchain.llm used - azureopenai
from langchain.llms import azureopenai

llms = azureopenai( temperature=0,deployment_name=""gpt3turbo"".......)

toolkit = sqldatabasetoolkit(db=db,llm=llms)

error:
validationerror: 1 validation error for sqldatabasetoolkit
llm
  value is not a valid dict (type=type_error.dict)

tried different versions of langchain","['python', 'pip', 'openai-api', 'langchain', 'azure-openai']",77521354,"see if all the steps are correct from the beginning.
install packages
pip install langchain 
pip install openai
pip install psycopg2


next create a python file called main.py and import the following:
from langchain.agents import create_sql_agent 
from langchain.agents.agent_toolkits import sqldatabasetoolkit 
from langchain.sql_database import sqldatabase 
from langchain.llms.openai import openai 
from langchain.agents import agentexecutor 
from langchain.agents.agent_types import agenttype
from langchain.chat_models import chatopenai


connect to database:
pg_uri = f""postgresql+psycopg2://{username}:{password}@{host}:{port}/{mydatabase}""

set database:
db = sqldatabase.from_uri(pg_uri)

once you have your openai_api_key:
openai_api_key = ""your openai key""


define your llm model:
gpt = openai(temperature=0, openai_api_key=openai_api_key, model_name='gpt-3.5-turbo')


toolkit should be:
toolkit = sqldatabasetoolkit(db=db, llm=gpt)

information on agent types : 

credit to @dishenwang for the complete article:",https://stackoverflow.com/questions/77521121,python,21-11-2023 07:54,1348.0,2.0,1.0,True,26-05-2024 21:17,26-05-2024 21:17
73318795,build vocab in doc2vec,"i have a list of abstracts and articles approx 500 in csv each paragraph contains approx 800 to 1000 words whenever i build vocab and print with words giving none and how i can improve results?
    lst_doc = doc.translate(str.maketrans('', '', string.punctuation))

    target_data = word_tokenize(lst_doc)

    train_data = list(read_data())

    model = gensim.models.doc2vec.doc2vec(vector_size=50, min_count=2, epochs=40)

    train_vocab = model.build_vocab(train_data)

    print(train_vocab)

   {train = model.train(train_data, total_examples=model.corpus_count, 
   epochs=model.epochs) }

output:
none","['machine-learning', 'nlp', 'word2vec', 'doc2vec']",73324980,"a call to build_vocab() only builds the vocabulary inside the model, for further usage. that function call doesn't return anything, so your train_vocab variable will be python none.
so, the behavior you're seeing is as expected, and you should say more about what your ultimate aims are, and what you'd want to see as steps towards those aims, if you're stuck.
if you want to see reporting of the progress of your calls to build_vocab() or train(), you can set the logging level to info. this is always a usually a good idea working to learn a new library: even if initially the copious info shown is hard to understand, by reviewing it you'll start to see the various internal steps, and internal counts/timings/etc, that hint whehter things are doing well or poorly.
you can also examine the state of the model and its various internal properties after the code has run.
for example, the model.wv property contains, after build_vocab(), a gensim keyedvectors structure holding all the untrained ready-for-training vectors. you can ask for its length (len(model.wv) or examine the discovered active list of words (model.wv.index_to_key).
other comments:

it's not clear your 1st two lines ï¿½ï¿½ï¿½ assigning into lst_doc and target_data ï¿½ï¿½ï¿½ affect anything further, since it's unclear what read_data() might be doing to fill the train_corpus.

often low min_count values worsen results, by including more words that have so few usage examples that they're little more than noise during training.

only 500 documents is rather small compared to most published work showing impressive results with this algorithm, which uses tens-of-thousands of documents (if not millions). so, keep in mind that results on such a smaay be unrepresentative of what's possible with a larger corpus - in terms of quality, optimal parameters, etc.",https://stackoverflow.com/questions/73318795,machine-learning,11-08-2022 10:01,449.0,0.0,1.0,True,11-08-2022 17:55,11-08-2022 11:03
65541788,how to reduce the inference time of helsinki-nlp/opus-mt-es-en (translation model) from transformer,"currently helsinki-nlp/opus-mt-es-en model takes around 1.5sec for inference from transformer. how can that be reduced?
also when trying to convert it to onxx runtime getting this error:

valueerror: unrecognized configuration class <class 'transformers.models.marian.configuration_marian.marianconfig'> for this kind of automodel: automodel.
model type should be one of retribertconfig, mt5config, t5config, distilbertconfig, albertconfig, camembertconfig, xlmrobertaconfig, bartconfig, longformerconfig, robertaconfig, layoutlmconfig, squeezebertconfig, bertconfig, openaigptconfig, gpt2config, mobilebertconfig, transfoxlconfig, xlnetconfig, flaubertconfig, fsmtconfig, xlmconfig, ctrlconfig, electraconfig, reformerconfig, funnelconfig, lxmertconfig, bertgenerationconfig, debertaconfig, dprconfig, xlmprophetnetconfig, prophetnetconfig, mpnetconfig, tapasconfig.

is it possible to convert this to onxx runtime?","['pytorch', 'huggingface-transformers', 'machine-translation']",65699717,"the opus models are originally trained with marian which is a highly optimized toolkit for machine translation written fully in c++. unlike pytorch, it does have the ambition to be a general deep learning toolkit, so it can focus on mt efficiency. the marian configurations and instructions on how to download the models are at 
the opus-mt models for huggingface's transformers are converted from the original marian models are meant more for prototyping and analyzing the models rather than for using them for translation in a production-like setup.
running the models in marian will certainly much faster than in python and it is certainly much easier than hacking transformers to run with onxx runtime. marian also offers further tricks to speed up the translation, e.g., by model quantization, which is however at the expense of the translation quality.
with both marian and tranformers, you can speed things up if you use gpu or if you narrow the beam width during decoding (attribute num_beams in the generate method in transformers).",https://stackoverflow.com/questions/65541788,pytorch,02-01-2021 17:06,3826.0,7.0,2.0,True,10-01-2022 13:56,02-01-2021 17:14
78905614,why doesn&#39;t permuting positional encodings in bert affect the output as expected?,"i am working on a jupyter notebook about transformers. in the section on positional encodings, i want to demonstrate that the transformer relies entirely on positional encoding to understand the order of the sequence. i previously learned from another question i posted that this concept only applies to models that don't use masked attention, like gpt-2. however, when i attempted the same approach with a bert model (which uses cross-attention) to predict a [mask] token, i encountered unexpected results.
what i expected to happen:

no permutation should cause the model to predict a different token, i.e., distribution a should be consistent over the vocabulary.
permuting only the input ids should return distribution b.
permuting only the positional embeddings should return distribution b.
permuting both the input ids and positional embeddings should return distribution a.

what actually happens:
sometimes the results align with my expectations, but other times, permuting one aspect (either the input ids or positional embeddings) leads to different outcomes, even though occasionally, they produce the same result.
my question is: is there something else in hugging face's bert model that might be influenced by position, beyond just the positional encoding?
for completeness, i have included the full code from this part of the notebook below, so it can be tried out directly. the important part happens in masked_prediction.
import torch
import ipywidgets as widgets
from ipython.display import display
from transformers import bertformaskedlm, autotokenizer
import matplotlib.pyplot as plt
import torch.nn.functional as f

# surpress renaming warnings
logging.getlogger(""transformers.modeling_utils"").setlevel(logging.error)
warnings.simplefilter(""ignore"", futurewarning)

tokenizer = autotokenizer.from_pretrained(""bert-base-uncased"")

input_ids = torch.tensor([[]])
tokens = []
permutation = []

output = widgets.output()

def permute_columns(matrix, permutation=none):
    n = len(permutation)
    first_n_columns = matrix[:, :n]
    permuted_columns = first_n_columns[:, permutation]
    remaining_columns = matrix[:, n:]
    new_matrix = torch.hstack((permuted_columns, remaining_columns))
    return new_matrix

def update_permutation(ordered_tags):
    global permutation
    fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]]
    
    permutation = [tokens.index(tag) for tag in fixed_tokens]
    

def tokenize(text):
    global input_ids, tokens
    input_ids = tokenizer(text, return_tensors=""pt"").input_ids
    tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]]
    
    if len(tokens) > 2:
        reorderable_tokens = tokens[1:-1]
    else:
        reorderable_tokens = []
    
    with output:
        output.clear_output(wait=true)
        tags_input.allowed_tags = reorderable_tokens
        tags_input.value = reorderable_tokens
        update_permutation(tags_input.value)

def on_tags_change(change):
    if len(change['new']) != len(tags_input.allowed_tags):
        tags_input.value = tags_input.allowed_tags  # restore original value


def masked_prediction(input_ids, permutation, permute_input, permute_encoding):
    
    with output:
        output.clear_output(wait=true)  # clear previous outputs
        
        if input_ids.numel() == 0:
            print(""you can't use an empty sequence for prediction"")
            return
        
        model = bertformaskedlm.from_pretrained(""bert-base-uncased"")
        
        if permute_encoding:
            model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.t, permutation).t
        if permute_input:
            input_ids = permute_columns(input_ids, permutation)
            
        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=false)
            
        with torch.no_grad():
            outputs = model(input_ids)
            
        logits = outputs.logits

        top_k = 5

        mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1]
        print(decoded_text, mask_token_indices, permutation)
        num_masks = len(mask_token_indices)
        if num_masks == 0:
            print(""you need to include a [mask] token for prediction"")
            return

        fig, axs = plt.subplots(1, num_masks, figsize=(15, 6))
        
        if num_masks == 1:
            axs = [axs]

        for i, idx in enumerate(mask_token_indices):
            mask_token_logits = logits[0, idx, :]

            softmax_probs = f.softmax(mask_token_logits, dim=0)

            top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0)

            predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids]
            predicted_confidences = top_token_probs.tolist()

            axs[i].bar(predicted_tokens, predicted_confidences, color='blue')
            axs[i].set_xlabel('predicted tokens')
            axs[i].set_ylabel('confidence')
            axs[i].set_title(f'masked token at position {idx.item()}')
            axs[i].set_ylim(0, 1)

        plt.show()

def on_predict_button_click(b):
    masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value)

text_input = widgets.text(placeholder='write text here to encode.', description='input:')
text_input.observe(lambda change: tokenize(change['new']), names='value')
tags_input = widgets.tagsinput(value=[], allowed_tags=[], allow_duplicates=false)

# observe changes in tags order to update the permutation and prevent deletion
tags_input.observe(on_tags_change, names='value')
tags_input.observe(lambda change: update_permutation(change['new']), names='value')

# create checkboxes for permute_input and permute_encoding
permute_input_checkbox = widgets.checkbox(value=false, description='permute inputs')
permute_encoding_checkbox = widgets.checkbox(value=false, description='permute encodings')

# create a button to trigger the prediction
predict_button = widgets.button(description=""run prediction"")
predict_button.on_click(on_predict_button_click)

# display the widgets
display(text_input)
display(tags_input)
display(permute_input_checkbox)
display(permute_encoding_checkbox)
display(predict_button)
display(output)","['python', 'pytorch', 'nlp', 'huggingface-transformers']",78906902,"the model inputs have token ids and position ids. there are four scenarios to consider:

baseline. correct order for tokens and positions
permute position ids only
permute token ids only
permute position ids and token ids

you are correct that scenario 1 and 4 should produce the same results. however you are incorrect in assuming that permuting tokens or positions separately should give the same result. consider:
# given:
tokens = [0, 1, 2]
positions = [0, 1, 2]
permutation = [2, 0, 1]

# ex1: permute tokens but not positions
[2, 0, 1] # permuted tokens
[0, 1, 2] # standard positions

# ex2: permute positions but not tokens
[0, 1, 2] # standard tokens
[2, 0, 1] # permuted positions

in ex1, the model is told that token 2 occurs at position 0. in ex2, the model is told that token 2 occurs at position 1. even though we used the same permutation, the mapping of tokens to positions is different. this results in different model outputs.
the reason you sometimes see these results line up is because you can (through random chance) sample a permutation that results in token/position embeddings lining up the same way (or mostly the same way) when permuting just one of them. this is luck - the average case produces different results.
it is simple to test this. huggingface models take a position_ids input parameter. we can use this to test permutations of the input ids without messing with the weight matrices.
to test this, we'll create input data, permute as needed, compute logits and compare logits.
when comparing logits, we will permute or depermute as needed to compare on a token to token basis. for example if token i in scenario 1 is permuted to token j in scenario 3, we want to compare logits i from scenario 1 to logits j in scenario 3.
import torch
from transformers import bertformaskedlm, autotokenizer

def get_logits(inputs):
    with torch.no_grad():
        outputs = model(**inputs)  
        logits = outputs.logits
    return logits

def permute_inputs(inputs, permutation, permute_ids=true, permute_positions=true):
    outputs = {}
    for k,v in inputs.items():
        if k=='position_ids' and permute_positions:
            outputs[k] = v[permutation]
        elif k!='position_ids' and permute_ids:
            outputs[k] = v[:,permutation]
        else:
            outputs[k] = v
            
    return outputs

# load tokenizer/model
tokenizer = autotokenizer.from_pretrained(""bert-base-uncased"")
model = bertformaskedlm.from_pretrained(""bert-base-uncased"")
model.eval() # remember to set model to eval

# create input ids and position ids
inputs = tokenizer('input text test sequence', return_tensors='pt')

inputs['position_ids'] = torch.tensor(list(range(inputs['input_ids'].shape[1])))

# create permutation tensor
permutation = torch.randperm(inputs['input_ids'].shape[1])

# compute scenario data
data = {
    's1' : { # scenario 1 - baseline
        'inputs' : inputs,
        'permuted_ids' : false
    },
    's2' : { # scenario 2 - permute positions only
        'inputs' : permute_inputs(inputs, permutation, permute_ids=false, permute_positions=true),
        'permuted_ids' : false
    },
    's3' : { # scenario 3 - permute token ids only
        'inputs' : permute_inputs(inputs, permutation, permute_ids=true, permute_positions=false),
        'permuted_ids' : true
    },
    's4' : { # scenario 4 - permute tokens and positions
        'inputs' : permute_inputs(inputs, permutation),
        'permuted_ids' : true
    }
}

# compute logits
for k,v in data.items():
    v['logits'] = get_logits(v['inputs'])

comparisons = [
    ['s1', 's2'],
    ['s1', 's3'],
    ['s1', 's4'],
    ['s2', 's3'],
    ['s2', 's4'],
    ['s3', 's4'],
]

# compare scenarios 
for sa, sb in comparisons:
    data_a = data[sa]
    data_b = data[sb]
    
    logits_a = data_a['logits']
    logits_b = data_b['logits']
    
    if data_a['permuted_ids'] == data_b['permuted_ids']:
        # either both logits are permuted or both logits are unpermuted
        # so we can compare directly
        val = (logits_a - logits_b).abs().mean()
    elif data_a['permuted_ids'] and (not data_b['permuted_ids']):
        # if `a` is permuted but `b` is not, we permute `b` to make tokens line up
        val = (logits_a - logits_b[:,permutation]).abs().mean()
    else:
        # otherwise we permute `b` to make tokens line up
        val = (logits_a[:,permutation] - logits_b).abs().mean()
        
    print(f""comparison {sa}, {sb}: {val.item():.6f}"")

the code should produce an output like:
comparison s1, s2: 1.407895
comparison s1, s3: 1.583560
comparison s1, s4: 0.000003
comparison s2, s3: 1.750883
comparison s2, s4: 1.407894
comparison s3, s4: 1.583560

run the code a bunch of times. you will find that the s1, s4 comparison always has a small deviation. this is because permuting tokens and positions together always produces the same result, ignoring small deviations caused by numeric issues.
you will find the s2, s3 comparison generally has a large deviation, but sometimes has a small deviation. as discussed, this is due to getting a lucky permutation where positions and ids mostly line up.",https://stackoverflow.com/questions/78905614,python,23-08-2024 11:12,80.0,1.0,1.0,True,23-08-2024 20:26,23-08-2024 20:26
77812092,how to resolve the error that occurs in the process of integrating the openai model made directly on the flutter app? (error: invalid_request_error),"i am a student who is interested in flutter and gpt api and developing my flutter app named 'fit buddy'.
i want to create a function that connects my own gpt model to the flutter app via api to send and receive exercise routine information using post and modify it to the appropriate exercise routine based on user response via gpt.
however, during the process of configuring the program, the error message 'invalid_request_error' appeared and the problem that the information processing did not proceed normally from the gpt was confirmed. the schema used in the gpt add action is as follows and the prompt in the flutter app is as follows. could you tell me what the problem is and how to solve it?
flutter app prompt example :
 prompt: 'model: gpt-3.5-turbo, exercisetype: leg curl, reps: 15, sets: 3, weight: 40, duration: '15 mins', userinput: 'gain weight'

schema for my gpt program:
{
    ""openapi"": ""3.0.0"",
    ""info"": {
        ""title"": ""fit buddy user routine modulator"",
        ""description"": ""generate and revise exercise routines based on user input."",
        ""version"": ""1.0.0""
    },
    ""servers"": [
        {
            ""url"": ""
        }
    ],
    ""paths"": {
        ""/gpt-3.5-turbo/completions"": {
            ""post"": {
                ""operationid"": ""reviseexerciseroutine"",
                ""requestbody"": {
                    ""required"": true,
                    ""content"": {
                        ""application/json"": {
                            ""schema"": {
                                ""type"": ""object"",
                                ""properties"": {
                                    ""model"": {
                                        ""type"": ""string"",
                                        ""example"": ""gpt-3.5-turbo""
                                    },
                                    ""prompt"": {
                                        ""type"": ""string""
                                    },
                                    `reps...`
                                },
                                ""required"": [
                                    ""exercisetype"",
                                    ""reps"",
                                    ""sets"",
                                    ""weight"",
                                    ""userinput""
                                ]
                            }
                        }
                    }
                },
                `responses`
                `components / security part`
            }
        }
    }
}

debug and flutter app tests under various conditions have been conducted, and the codes developed so far are as described above.","['flutter', 'openai-api', 'large-language-model', 'chatgpt-api']",77812105,"you're trying to use the wrong openai api endpoint. all engines api endpoints were deprecated a very, very long time ago.
change this...


...to this.


then, set the model parameter to gpt-3.5-turbo. also, don't forget to set the messages parameter. both are required for the chat completions api.
see the official openai documentation.",https://stackoverflow.com/questions/77812092,flutter,13-01-2024 15:59,127.0,0.0,1.0,True,14-01-2024 11:32,14-01-2024 11:32
76866751,i don&#39;t understand how the prompts work in llama_index,"i have been trying to query a pdf file in my local directory using llm, i have downloaded the llm model i'm using in my local system (gpt4all-13b-snoozy.ggmlv3.q4_0.bin) and trying to use langchain and hugging face's instructor-large model for embedding purpose, i was able to set the service_context and then building index but i'm not able to query , i keeping getting this error regarding prompt..

valueerror: argument prompt is expected to be a string. instead found <class 'llama_index.prompts.base.prompt'>. if you want to run the llm on multiple prompts, use generate instead.

i'm just starting to learn how to use llm, hope the community helps me....
error message part1
error message part2
from llama_index import vectorstoreindex, simpledirectoryreader
from instructorembedding import instructor
from llama_index import prompthelper, servicecontext
from llama_index import langchainembedding
from langchain.chat_models import chatopenai
from langchain.embeddings import huggingfaceembeddings
from langchain.llms import openllm
# from langchain.chat_models.human import humaninputchatmodel
from langchain import prompttemplate, llmchain
from langchain.llms import gpt4all
from langchain.callbacks.streaming_stdout import streamingstdoutcallbackhandler

documents = simpledirectoryreader(r'c:\users\avish.wagde\documents\work_avish\llm_trials\instructor_large').load_data()

model_id = 'hkunlp/instructor-large'

model_path = ""..\models\gpt4all-13b-snoozy.ggmlv3.q4_0.bin""

callbacks = [streamingstdoutcallbackhandler()]

# verbose is required to pass to the callback manager
llm = gpt4all(model = model_path, callbacks=callbacks, verbose=true)

embed_model = langchainembedding(huggingfaceembeddings(model_name = model_id))

# define prompt helper
# set maximum input size
max_input_size = 4096
# set number of output tokens
num_output = 256
# set maximum chunk overlap
max_chunk_overlap = 0.2

prompt_helper = prompthelper(max_input_size, num_output, max_chunk_overlap)

service_context = servicecontext.from_defaults(chunk_size= 1024, llm_predictor=llm, prompt_helper=prompt_helper, embed_model=embed_model)

index = vectorstoreindex.from_documents(documents, service_context= service_context)

query_engine = index.as_query_engine()

response = query_engine.query(""what is apple's finnacial situation"")
print(response)


i have been going through, the source code of the library as the error message guides but i couldn't find the problemï¿½ï¿½ï¿½","['langchain', 'huggingface', 'large-language-model', 'llama-index', 'vector-database']",76868784,"the code you have written here is a little old/erroneous. but the main error is the service context setup with llm_predictor=llm. you can just pass the llm in directly as a kwarg.
using the latest version (v0.7.22 at the time of writing) i would re-write your service context like so:
service_context = servicecontext.from_defaults(
    chunk_size= 1024, 
    llm=llm, # this is updated
    prompt_helper=prompt_helper, 
    embed_model=embed_model
)

source: 
for reference, if you pass in an llm from langchain like this, the service context detects this and wraps it with our langchain wrapper for you:
from llama_index.llms import langchainllm

llm = langchainllm(langchain_llm)

this is useful to know, since other parts of llama-index (agents, chat engines, etc.) my expect an llm object as the input, and won't wrap it for you.",https://stackoverflow.com/questions/76866751,langchain,09-08-2023 10:13,11231.0,4.0,1.0,True,09-08-2023 14:33,09-08-2023 11:27
69551405,sparknlp&#39;s nercrfapproach with custom labels,"i am trying to train a sparknlp nercrfapproach model with a dataset in conll format that has custom labels for product entities (like i-prod, b-prod etc.). however, when using the trained model to make predictions, i get only ""o"" as the assigned label for all tokens. when using the same model trained on the conll data from the sparknlp workshop example, the classification works fine.
(cf. 
so, the question is: does nercrfapproach rely on the standard tag set for ner labels used by the conll data? or can i use it for any custom labels and, if yes, do i need to specify these somehow? my assumption was that the labels are inferred from the training data.
cheers,
martin
update: the issue might not be related to the labels after all. i tried to replace my custom labels with conll standard labels and i am still not getting the expected classification results.","['named-entity-recognition', 'johnsnowlabs-spark-nlp']",69565969,"as it turns out, this issue was not caused by the labels, but rather by the size of the dataset. i was using a rather small dataset for development purposes. not only was this dataset quite small, but also heavily imbalanced, with a lot more ""o"" labels than the other labels. fixing this by using a dataset of 10x the original size (in terms of sentences), i am able to get meaningful results, even for my custom labels.",https://stackoverflow.com/questions/69551405,named-entity-recognition,13-10-2021 07:33,366.0,0.0,2.0,True,18-11-2022 05:02,14-10-2021 06:01
74885225,cast features to classlabel,"i have a dataset with type dictionary which i converted to dataset:
ds = datasets.dataset.from_dict(bio_dict)
the shape now is:
dataset({
    features: ['id', 'text', 'ner_tags', 'input_ids', 'attention_mask', 'label'],
    num_rows: 8805
})

when i use the train_test_split function of datasets i receive the following error:
train_testvalid = ds.train_test_split(test_size=0.5, shuffle=true, stratify_by_column=""label"")


valueerror: stratifying by column is only supported for classlabel
column, and column label is sequence.

how can i change the type to classlabel so that stratify works?","['python', 'huggingface-transformers', 'huggingface-datasets']",75106464,"you should apply the following class_encode_column function:
ds = ds.class_encode_column(""label"")",https://stackoverflow.com/questions/74885225,python,22-12-2022 07:19,4368.0,8.0,1.0,True,13-01-2023 08:16,22-12-2022 16:05
75013624,early stopping based on bleu in fairseq,"my goal is to use bleu as early stopping metric while training a translation model in fairseq.
following the documentation, i am adding the following arguments to my training script:
--eval-bleu --eval-bleu-args --eval-bleu-detok --eval-bleu-remove-bpe

i am getting the following error:
fairseq-train: error: unrecognized arguments: --eval-bleu --eval-bleu-args --eval-bleu-detok --eval-bleu-remove-bpe

system information:

fairseq version: 0.10.2
torch: 1.10.1+cu113

more details:
when i am trying to finetune m2m100 model, i am getting error as:
keyerror: 'bleu'
when using following:
cuda_visible_devices=0,1,2,3 fairseq-train \
    $path_2_data --ddp-backend=no_c10d \
    --best-checkpoint-metric bleu \
    --maximize-best-checkpoint-metric \
    --max-tokens 2048 --no-epoch-checkpoints \
    --finetune-from-model $pretrained_model \
    --save-dir $checkpoint --task translation_multi_simple_epoch \
    --encoder-normalize-before \
    --langs 'af,am,ar,ast,az,ba,be,bg,bn,br,bs,ca,ceb,cs,cy,da,de,el,en,es,et,fa,ff,fi,fr,fy,ga,gd,gl,gu,ha,he,hi,hr,ht,hu,hy,id,ig,ilo,is,it,ja,jv,ka,kk,km,kn,ko,lb,lg,ln,lo,lt,lv,mg,mk,ml,mn,mr,ms,my,ne,nl,no,ns,oc,or,pa,pl,ps,pt,ro,ru,sd,si,sk,sl,so,sq,sr,ss,su,sv,sw,ta,th,tl,tn,tr,uk,ur,uz,vi,wo,xh,yi,yo,zh,zu' \
    --lang-pairs $lang_pairs \
    --decoder-normalize-before --sampling-method temperature \
    --sampling-temperature 1.5 --encoder-langtok src \
    --decoder-langtok --criterion label_smoothed_cross_entropy \
    --label-smoothing 0.2 --optimizer adam --adam-eps 1e-06
    --adam-betas '(0.9, 0.98)' --lr-scheduler inverse_sqrt \
    --lr 3e-05 --warmup-updates 2500 --max-update 400000 \
    --dropout 0.3 --attention-dropout 0.1 \
    --weight-decay 0.0 --update-freq 2 --save-interval 1 \
    --save-interval-updates 5000 --keep-interval-updates 10 \
    --seed 222 --log-format simple --log-interval 2 --patience 5  \
    --arch transformer_wmt_en_de_big --encoder-layers 24 \
    --decoder-layers 24 --encoder-ffn-embed-dim 8192 \
    --decoder-ffn-embed-dim 8192 --encoder-layerdrop 0.05 \
    --decoder-layerdrop 0.05 --share-decoder-input-output-embed \
    --share-all-embeddings --fixed-dictionary $fix_dict --fp16 \
    --skip-invalid-size-inputs-valid-test","['deep-learning', 'nlp', 'machine-translation', 'bleu', 'fairseq']",75015379,"the task that you are using translation_multi_simple_epoch does not have these arguments; they are specific for translation task.
note that some of the arguments that you are using require values.

--eval-bleu-args expects a path to a configuration json for sacrebleu.  if you want to you the default 4-gram bleu, you should skip this.

--eval-bleu-detok expects a specification of how you want to detokenize the model output. the default value is space which does not do anything.


for more details, see the documentation of the translation task in fairseq.",https://stackoverflow.com/questions/75013624,deep-learning,05-01-2023 03:40,499.0,1.0,1.0,True,05-01-2023 08:12,05-01-2023 08:07
71284177,how to remove words from a sentence that carry no positive or negative sentiment?,"im trying a sentiment analysis based approach on youtube comments, but the comments many times have words like mrbeast, tiger/'s, lion/'s, pewdiepie, james, etc which do not add any feeling in the sentence. i've gone through nltk's average_perception_tagger but it didn't work well as it gave the results as
my input:
""mrbeast james lion tigers bad sad clickbait fight nice good""

words that i need in my sentence:
""bad sad clickbait fight nice good""

what i got using average_perception_tagger:
[('mrbeast', 'nn'),
 ('james', 'nns'),
 ('lion', 'jj'),
 ('tigers', 'nns'),
 ('bad', 'jj'),
 ('sad', 'jj'),
 ('clickbait', 'nn'),
 ('fight', 'nn'),
 ('nice', 'rb'),
 ('good', 'jj')]


so as you can see if i remove mrbeast i.e nn the words like clickbait, fight will also get removed which than ultimately remove expressions from that sentence.","['python', 'machine-learning', 'nlp', 'sentiment-analysis']",73273796,"there are multiple ways of doing this like

you can create a set of positive and negative words and for each word in your grammar you can check if it exists in your set, if it does you should keep the word, else delete it. this however would first require all positive and negative words dataset.

you can use something like textblob which can give you the sentiment score of a word or a sentence. so with a cutoff sentiment score you can filter out the words that you don't need.",https://stackoverflow.com/questions/71284177,python,27-02-2022 10:59,1112.0,-2.0,2.0,True,19-06-2024 11:53,27-02-2022 21:13
75173490,how can i check similarity in meaning and not just having same words between two texts with spacy,"i'm trying to compare two different textsï¿½ï¿½ï¿½one coming from a curriculum vitae (cv) and the other from a job announcement.
after cleaning the texts, i'm trying to compare them to detect if a job announcement is more linked to a specific cv.
i am trying to do this using similarity matching in spacy via the following code:
similarity = pdf_text.similarity(final_text_from_annonce)

this works well, but i'm getting strange results from two different cvs for the same job announcement. specifically, i get the same similarity score (~0.6), however, one should clearly be higher than the other.
i checked on spacy website and i found this very important sentence:

vector averaging means that the vector of multiple tokens is insensitive to the order of the words. two documents expressing the same meaning with dissimilar wording will return a lower similarity score than two documents that happen to contain the same words while expressing different meanings.

so, what do i need to use or code to make spacy compare my two texts based on their meaning instead of the occurrence of words?
i am expecting a parameter for the similarity function of spacy, or another function that will compare my both texts and calculate a similarity score based on the meaning of the texts and not if the same words are used.","['python', 'nlp', 'spacy', 'similarity', 'semantics']",75193330,"the spacy library by default will use the average of the word embeddings of words in a sentence to determine semantic similarity. this can be thought of as a naive sentence embedding approach. such an approach could work, but if you were to use it is recommended that you first filter non-meaningful words (e.g. common words) to prevent them from undesirably influencing the final sentence embeddings.
the alternative (and more reliable) solution is to use a different pipeline within spacy that has been designed to use sentence embeddings created specifically with a dedicated sentence encoder (e.g. the universal sentence encoder (use) [1] by cer et al.). martino mensio created a package called spacy-universal-sentence-encoder that makes use of this model. install it via the following command in your command prompt:
pip install spacy-universal-sentence-encoder

then you can compute the semantic similarity between sentences as follows:
import spacy_universal_sentence_encoder

# load one of the models: ['en_use_md', 'en_use_lg', 'xx_use_md', 'xx_use_lg']
nlp = spacy_universal_sentence_encoder.load_model('en_use_lg')

# create two documents
doc_1 = nlp('hi there, how are you?')
doc_2 = nlp('hello there, how are you doing today?')

# use the similarity method to compare the full documents (i.e. sentences)
print(doc_1.similarity(doc_2))  # output: 0.9356049733134972
# or make the comparison using a predefined span of the second document 
print(doc_1.similarity(doc_2[0:7])) # output: 0.9739387861159459

as a side note, when you run the nlp = spacy_universal_sentence_encoder.load_model('en_use_lg') command for the first time, you may have to do so with administrator rights to allow tensorflow to create the models folder in c:\program files\python310\lib\site-packages\spacy_universal_sentence_encoder and download the appropriate model. if you don't, it is possible that there will be a permissiondeniederror and the code will not run.
references
[1] cer, d., yang, y., kong, s.y., hua, n., limtiaco, n., john, r.s., constant, n., guajardo-cespedes, m., yuan, s., tar, c. and sung, y.h., 2018. universal sentence encoder. arxiv preprint arxiv:1803.11175.",https://stackoverflow.com/questions/75173490,python,19-01-2023 14:06,1656.0,2.0,1.0,True,27-01-2023 09:53,27-01-2023 09:53
76601293,calculating similarity score in contexto.me clone,"i am currently trying to clone the popular browser game contexto.me and i am having trouble with as to how to calculate the similarity score between two words (the target word and the user inputted guess word). i am able to get the cosine similarity between the two words, but as to how to properly quantify the score into a clean integer like in the game, i am confused as to how it is done.
for example, if the target word is 'helicopter' and i guess the word plane, contexto will return something like a similarity score of 13, but if i guess a word like 'king' contexto will return a score of '2000' for instance.
target_word = ""helicopter""
glove = torchtext.vocab.glove(name=""6b"", dim=100)


@app.route('/', methods=[""get"", ""post""])
def getsimscore():
    if request.method == ""post"":
        text = request.form.get(""word"")
        new_text = singularize(text)
        sim_score = ((torch.cosine_similarity(glove[target_word].unsqueeze(0), glove[new_text].unsqueeze(0))).numpy()[0])
        print(sim_score)
    return render_template('homepage.html', messagetext='sample text', gamenum=1, guessnum=1, wordaccuracy=999)

this is my code so far with sim_score printing to be ~0.77 for the input 'truck' and ~0.29 for the input 'king' (closer to 1 the more similar the word is to the target word).","['python', 'python-3.x', 'nlp', 'stanford-nlp', 'torch']",76601396,"for example, if the target word is 'helicopter' and i guess the word plane, contexto will return something like a similarity score of 13, but if i guess a word like 'king' contexto will return a score of '2000' for instance.

this metric is typically called ""rank,"" and you can calculate it with the following algorithm.

compute the similarity score of every word the user can enter.
sort this list.
given a specific score, find what position it appears on the list. if the score appears at index 0, then it is rank 1. if it appears at index 4, then  it is rank 5, and so on.

for speed, steps 1 and 2 can be computed ahead of time, if you want.",https://stackoverflow.com/questions/76601293,python,03-07-2023 00:44,441.0,0.0,1.0,True,03-07-2023 01:32,03-07-2023 00:51
47725035,lemmatization with apache lucene,"i'm developing a text analysis project using apache lucene. i need to lemmatize some text (transform the words to their canonical forms). i've already written the code that makes stemming. using it, i am able to convert the following sentence

the stem is the part of the word that never changes even when morphologically inflected; a lemma is the base form of the word. for example, from ""produced"", the lemma is ""produce"", but the stem is ""produc-"". this is because there are words such as production

into

stem part word never chang even when morpholog inflect lemma base form word exampl from produc lemma produc stem produc becaus word product

however, i need to get the base forms of the words: example instead of exampl, produce instead of produc, and so on.
i am using lucene because it has analyzers for many languages (i need at least english and russian). i know about stanford nlp library, but it has no russian language support.
so is there any way to do lemmatization for several languages like i do stemming using lucene?
the simplified version of my code responsible for stemming:
//using apache tika to identify the language
languageidentifier identifier = new languageidentifier(text);
//getting analyzer according to the language (eg, englishanalyzer for 'en')
analyzer analyzer = getanalyzer(identifier.getlanguage());
tokenstream stream = analyzer.tokenstream(""field"", text);
stream.reset();
while (stream.incrementtoken()) {
    string stem = stream.getattribute(chartermattribute.class).tostring();
    // doing something with the stem
    system.out.print(stem+ "" "");
}
stream.end();
stream.close();

update: i found the library that does almost what i need (for english and russian languages) and uses apache lucene (although in its own way), it's definitely worth exploring.","['java', 'lucene', 'nlp', 'stemming', 'lemmatization']",62033221,"in case someone still needs it, i decided to return to this question and illustrate how to use the russianmorphology library i found earlier to do lemmatization for english and russian languages.
first of all, you will need these dependencies (besides the lucene-core):
<!-- if you need russain -->
<dependency>
    <groupid>org.apache.lucene.morphology</groupid>
    <artifactid>russian</artifactid>
    <version>1.1</version>
</dependency>

<!-- if you need english-->
<dependency>
    <groupid>org.apache.lucene.morphology</groupid>
    <artifactid>english</artifactid>
    <version>1.1</version>
</dependency>

<dependency>
    <groupid>org.apache.lucene.morphology</groupid>
    <artifactid>morph</artifactid>
    <version>1.1</version>
</dependency>

then, make sure you import the right analyzer:
import org.apache.lucene.morphology.english.englishanalyzer;
import org.apache.lucene.morphology.russian.russiananalyzer;

these analyzers, unlike standard lucene analyzers, use morphologyfilter which converts each word into a set of its normal forms.
so if you use the following code
string text = ""the stem is the part of the word that never changes even when morphologically inflected; a lemma is the base form of the word. for example, from \""produced\"", the lemma is \""produce\"", but the stem is \""produc-\"". this is because there are words such as production"";
analyzer analyzer = new englishanalyzer();
tokenstream stream = analyzer.tokenstream(""field"", text);
stream.reset();
while (stream.incrementtoken()) {
    string lemma = stream.getattribute(chartermattribute.class).tostring();
    system.out.print(lemma + "" "");
}
stream.end();
stream.close();

it will print

the stem be the part of the word that never change even when
morphologically inflected inflect a lemma be the base form of the word
for example from produced produce the lemma be produce but the stem be
produc this be because there are be word such as production

and for the russian text
string text = ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½. ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½. ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½reýýýýýýýýýýýýýýýýýýýý

yo may notice that some words have more that one base form, e.g. inflected is converted to [inflected, inflect]. if you don't like this behaviour, you would have to change the implementation of the org.apache.lucene.morphology.analyzer.morhpologyfilter (if you are interested in how exactly to do it, let me know and i'll elaborate on this).
hope it helps, good luck!",https://stackoverflow.com/questions/47725035,java,09-12-2017 03:29,4280.0,10.0,2.0,True,28-01-2022 21:37,31-12-2020 07:15
77446419,callback in llm chain doesn&#39;t get executed,"the following code do not do what it is supposed to do:
from langchain.callbacks.base import basecallbackhandler
from langchain import prompttemplate
from langchain.chains import llmchain
from langchain.llms import vertexai


class mycustomhandler(basecallbackhandler):
    def on_llm_end(self, event, context):
        print(f""prompt: {event.prompt}"")
        print(f""response: {event.response}"")


llm = vertexai(
            model_name='text-bison@001',
            max_output_tokens=1024,
            temperature=0.3,
            verbose=false)
prompt = prompttemplate.from_template(""1 + {number} = "")
handler = mycustomhandler()
chain = llmchain(llm=llm, prompt=prompt, callbacks=[handler])
response = chain.run(number=2)
print(response)

based on this documentation and this tutorial, the code should execute the custom handler callback on_llm_end but in fact it doesn't.
can anyone please tell me why?","['python', 'langchain', 'google-cloud-vertex-ai', 'py-langchain']",77769356,"i did some research and found the solution.
you need to pass callback parameter to llm itself. in your case you need to change the code as below
callback_handler  = mycustomhandler()
llm = vertexai(
               model_name='text-bison@001',
               max_output_tokens=1024,
               temperature=0.3,
               callbacks=[callback_handler]
               verbose=false)

secondly change the implementation of on_llm_end as below
class mycustomhandler(basecallbackhandler):
    def on_llm_end(self, response, **kwargs):
        print(f""response: {response}"")

this should fix the problem.",https://stackoverflow.com/questions/77446419,python,08-11-2023 14:26,2785.0,1.0,4.0,True,08-01-2024 12:17,18-11-2023 01:56
37524799,what is the best way to do natural language processing in rails app?,"i have a rails app. i need to implement automatic text categorization algorithms and possibly more nlp capabilities in app. i believe ruby does not have good nlp tools available as python has. i am using a separate resque server for process background jobs. i believe i have following 

run python scripts using resque jobs
run a flask application on a separate server which can either talk to resque job or can automatically update the app database with processed results. 
use ruby tools mentioned in this thread
any other suggestions welcome

please let me know what is the best way to do it. are there any similar working examples?","['ruby-on-rails', 'nlp', 'nltk']",37526239,"i had the same problem a few months ago. after a bit of research and testing this is the solution i implemented 
run several python processes as many as one machine can hold. and use as many machines as you need.
use zeromq to communicate between the web servers and the machines running python processes
do not use http to communicate because the overhead is significant and it will be very slow compared to zeromq. you will also not need an as complex handler with zeromq as you would with http
take care to expose zeromq sockets to internal networks only, or you would need to set up authentication on each python server
another option is to just use one of the many available nlp apis, if don't need any corpus based algorithms (such as pos tagging, sentiment analysis, etc).",https://stackoverflow.com/questions/37524799,ruby-on-rails,30-05-2016 11:41,580.0,1.0,1.0,True,27-02-2023 15:47,27-02-2023 15:47
70123519,valueerror: unknown url type: &#39;languagetool-3.2.zip&#39;,"i am trying to install pycontractions library which is dependent on language-check library. so when installing language-check i am getting the below error
valueerror: unknown url type: 'languagetool-3.2.zip

i have python-3.10 and java-8 installed. when i am using pip install language-check, i get urllib.error. http error 403: forbidden so, i downloaded language tool manually(as instructed in github link) and run the following command:
pip install git+

i tried all the possible ways available here but still couldnt resolve it. its getting installed on mac with no issues but i am a windows user. so i need to install it in my windows desktop.
error:
collecting git+
  cloning  to c:\users\skamble\appdata\local\temp\pip-req-build-c78lrf2e
  running command git clone --filter=blob:none -q  'c:\users\skamble\appdata\local\temp\pip-req-build-c78lrf2e'
  resolved  to commit 58e419833ef28a9193fcaa21193616a8a14504a9
  preparing metadata (setup.py) ... done
using legacy 'setup.py install' for language-check, since package 'wheel' is not installed.
installing collected packages: language-check
    running setup.py install for language-check ... error
    error: command errored out with exit status 1:
     command: 'c:\users\skamble\appdata\local\programs\python\python310\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'c:\\users\\skamble\\appdata\\local\\temp\\pip-req-build-c78lrf2e\\setup.py'""'""'; __file__='""'""'c:\\users\\skamble\\appdata\\local\\temp\\pip-req-build-c78lrf2e\\setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.stringio('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'c:\users\skamble\appdata\local\temp\pip-record-msctmso8\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\users\skamble\appdata\local\programs\python\python310\include\language-check'
         cwd: c:\users\skamble\appdata\local\temp\pip-req-build-c78lrf2e\
    complete output (23 lines):
    traceback (most recent call last):
      file ""<string>"", line 1, in <module>
      file ""c:\users\skamble\appdata\local\temp\pip-req-build-c78lrf2e\setup.py"", line 595, in <module>
        sys.exit(main())
      file ""c:\users\skamble\appdata\local\temp\pip-req-build-c78lrf2e\setup.py"", line 590, in main
        run_setup_hooks(config)
      file ""c:\users\skamble\appdata\local\temp\pip-req-build-c78lrf2e\setup.py"", line 561, in run_setup_hooks
        language_tool_hook(config)
      file ""c:\users\skamble\appdata\local\temp\pip-req-build-c78lrf2e\setup.py"", line 584, in language_tool_hook
        download_lt()
      file ""c:\users\skamble\appdata\local\temp\pip-req-build-c78lrf2e\download_lt.py"", line 131, in download_lt
        with closing(urlopen(url)) as u:
      file ""c:\users\skamble\appdata\local\programs\python\python310\lib\urllib\request.py"", line 216, in urlopen
        return opener.open(url, data, timeout)
      file ""c:\users\skamble\appdata\local\programs\python\python310\lib\urllib\request.py"", line 503, in open
        req = request(fullurl, data)
      file ""c:\users\skamble\appdata\local\programs\python\python310\lib\urllib\request.py"", line 322, in __init__
        self.full_url = url
      file ""c:\users\skamble\appdata\local\programs\python\python310\lib\urllib\request.py"", line 348, in full_url
        self._parse()
      file ""c:\users\skamble\appdata\local\programs\python\python310\lib\urllib\request.py"", line 377, in _parse
        raise valueerror(""unknown url type: %r"" % self.full_url)
    valueerror: unknown url type: 'languagetool-3.2.zip'
    ----------------------------------------
error: command errored out with exit status 1: 'c:\users\skamble\appdata\local\programs\python\python310\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'c:\\users\\skamble\\appdata\\local\\temp\\pip-req-build-c78lrf2e\\setup.py'""'""'; __file__='""'""'c:\\users\\skamble\\appdata\\local\\temp\\pip-req-build-c78lrf2e\\setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.stringio('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'c:\users\skamble\appdata\local\temp\pip-record-msctmso8\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\users\skamble\appdata\local\programs\python\python310\include\language-check' check the logs for full command output.","['python-3.x', 'url', 'pip', 'nlp', 'urllib']",70161691,i resolved it. used git+ and it worked.,https://stackoverflow.com/questions/70123519,python-3.x,26-11-2021 11:13,289.0,0.0,1.0,True,29-11-2021 21:44,26-11-2021 22:17
78739644,how to specify the langsmith project name for each chain?,"according to the langsmith documentation you need to set the langchain_project environment variable to specify the project name in langsmith.
however, if we want to execute multiple chains within a single service but under different project names, how do we specify the project name at runtime per-chain?
we think we have a solution, but it required looking into the langchain code, so we are concerned this is not the idomatic way to do it and it might cause issues when their api changes.
this is what we think we should do.

instantiate the chain.
instantiate a custom langchain tracer but with a specific project name defined at runtime.
replace the chain's callbacks with the tracer.

        chain = llmchain(llm=self._chat_model, prompt=prompt, verbose=true)
        tracer = langchaintracer(project_name=""whatever"")
        chain.callbacks = callbackmanager(handlers=[tracer])

thank you in advance for any help.","['langchain', 'langsmith']",78813387,"you can specify project name in the @traceable decorator as an argument.
@traceable(
    run_type=""llm"",
    project_name=""my project""
)
def my_llm_call(
    ...
) -> str:

you can also put it as a part of langsmith_extra when calling the traced function. this takes precedence over the traceable project:
my_llm_call(
   ...,
   langsmith_extra={""project_name"": ""my project 2""},
)",https://stackoverflow.com/questions/78739644,langchain,12-07-2024 09:40,514.0,0.0,1.0,True,30-07-2024 19:24,12-07-2024 12:33
79375287,gpu utilization almost always 0 during training hugging face transformer,"i am fine-tuning a donut cord-v2 model with my invoice data which is around 360 gb in size when preprocessed and saved on disk as a dataset. i am following this notebook almost exactly, except i have 6 training epochs instead of 3.
i am training on single nvidia h100 sxm gpu / intel xeonï¿½ï¿½ gold 6448y / 128 gb ram.
whenever i start training, and inspect cpu and gpu utilization using htop and nvidia-smi, i see that cpu is at 10-12% utilization, used by python, gpu memory is almost 90% filled constantly, but gpu utilization is almost always 0. if i keep refreshing the output of nvidia-smi, once every 10-12 seconds the utilization will jump to 100% and then go back to 0 immediately. i cant help but feel ther eis a bottleneck between my cpu and gpu, where cpu attempts to constantly process data and send it to gpu, gpu processes it very fast, and just idles, awaiting for the next batch from cpu. i load already pre-processed dataset from disk like so:
from datasets import load_from_disk
processed_dataset = load_from_disk(r""/dataset/dataset_final"")

my processor configas follows:
from transformers import donutprocessor

new_special_tokens = [] # new tokens which will be added to the tokenizer
task_start_token = ""<s>""  # start of task token
eos_token = ""</s>"" # eos token of tokenizer

processor = donutprocessor.from_pretrained(""naver-clova-ix/donut-base-finetuned-cord-v2"")

# add new special tokens to tokenizer
processor.tokenizer.add_special_tokens({""additional_special_tokens"": new_special_tokens + [task_start_token] + [eos_token]})

# we update some settings which differ from pretraining; namely the size of the images + no rotation required
processor.feature_extractor.size = [1200,1553] # should be (width, height)
processor.feature_extractor.do_align_long_axis = false

my model config is:
import torch
from transformers import visionencoderdecodermodel, visionencoderdecoderconfig

#print(torch.cuda.is_available())

# load model from huggingface.co
model = visionencoderdecodermodel.from_pretrained(""naver-clova-ix/donut-base-finetuned-cord-v2"")

# resize embedding layer to match vocabulary size
new_emb = model.decoder.resize_token_embeddings(len(processor.tokenizer))
print(f""new embedding size: {new_emb}"")
# adjust our image size and output sequence lengths
model.config.encoder.image_size = processor.feature_extractor.size[::-1] # (height, width)
model.config.decoder.max_length = len(max(processed_dataset[""train""][""labels""], key=len))

# add task token for decoder to start
model.config.pad_token_id = processor.tokenizer.pad_token_id
model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['<s>'])[0]

and my training code is:
import gc
gc.collect()

torch.cuda.empty_cache()


from transformers import seq2seqtrainingarguments, seq2seqtrainer

import logging
logging.basicconfig(level=logging.info)

# arguments for training
training_args = seq2seqtrainingarguments(
    output_dir=r""/trained"",  # specify a local directory to save the model
    num_train_epochs=6,
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    weight_decay=0.01,
    fp16=true,
    logging_steps=50,
    save_total_limit=2,
    evaluation_strategy=""no"",
    save_strategy=""epoch"",
    predict_with_generate=true,
    report_to=""none"",
    # disable push to hub
    push_to_hub=false
   
)

# create trainer
trainer = seq2seqtrainer(
    model=model,
    args=training_args,
    train_dataset=processed_dataset[""train""],
)


# start training
trainer.train()

the estimated time to complete the training with 6 epochs, with 360 gb dataset, is 54 hours. when i run the same exact code on my pc that has intel i9 11900kf / rtx 3050, i see gpu utilization constantly at 100%. is there a bottleneck in my code? why does cpu keep processing so much on already preprocessed dataset? cuda 12.6
edit:
does it make sense to change the dataloader_num_workers parameter of seq2seqtrainer to >0 value, since my ram and cpu core count allows it? (and since cpu utilization is at 10-12% max.)","['python', 'machine-learning', 'huggingface-transformers']",79378532,"you seem to have an io bottleneck. it means the data cannot be transfered fast enough and your gpu ends up waiting for the data most of the time.
you can verify that claim by checking the status of the python workers in htop.
you do not seem to have a cpu bottleneck because your cpu isn't fully used.
this often happens on vms when the data is being transfered using old protocols like nfs.
if the vm you're using has a local disk, you can try copying the data there before the training, and point your huggingface dataset to that local path.
this could also be due to a suboptimal configuration of the data loading process. you might want to give this a read.
you might not be seeing this issue on your pc because:

your gpu is slower than an h100 hence takes more time to process a single batch. as a result, your system has more time to load the next batch.
your data is stored in your local disk and therefore the time to load the data is much smaller.

and yes, please increase your number of workers, it can drastically improve the performance.",https://stackoverflow.com/questions/79375287,python,21-01-2025 17:09,236.0,1.0,1.0,True,22-01-2025 16:34,21-01-2025 20:11
47687797,when to remove stop words when using bigram_measures like pmi?,"i need to verify an overall approach to dealing with bigram stop words that are returned from bigram_measures such as pmi. why deal with these stop words? well, they're noise and donï¿½ï¿½ï¿½t add any additional value past a certain point.
i've seen several specific examples of how to use bigram_measures. however, i'm wondering when it's best to remove stop word in the overall process of cleaning data, expansion, lemmatizing/stemming, etc.
and yes, i am using a corpus that is sufficiently large. i remember the size of your corpus will also affect the quality of the bigram_measures result.
based on the accepted answer in this post (nltk - counting frequency of bigram) it seems that stop words could be removed after pmi or other bigram_measures are used on the corpus.

""imagine that if filtering collocations was simply deleting them, then there were many probability measures such as liklihood ratio or the pmi itself (that compute probability of a word relative to other words in a corpus) which would not function properly after deleting words from random positions in the given corpus. by deleting some collocations from the given list of words, many potential functionalities and computations would be disabled...""

therefore, i believe the best process is:

clean the text and remove garbage chars like html tags, etc.
expand contractions (e.g.: they're -> they are)
lemmatize or stem to normalize the words
calculate bigrams using bigram_measures like pmi. you can calculate bigrams using other methods, but this is what i'm using.
apply a frequency filter like ""apply_freq_filter(n)"" to get the bigrams that occur above your threshold. note this will still return some bigrams with stop words mixed in with valuable bigrams.
check to see if both words are stop words. if yes, then don't include that bigram in the final results but leave them in the corpus for the reasons quoted above.

is this a correct overall approach to dealing with bigram stop words mixed in with valuable bigrams?","['python', 'nlp', 'nltk']",48491681,"one approach is to:

clean the text
expand contractions
lemmatize
remove stop words
run pmi or other measure to score n-grams.

source: text analytics with python, pg 224.
my purpose in providing the source above is to show where i received this answer from rather than providing some ungrounded answer.",https://stackoverflow.com/questions/47687797,python,07-12-2017 04:49,3459.0,4.0,1.0,True,29-10-2023 18:33,29-10-2023 18:32
78213463,running chatgpt programmatically - how to continue conversation without re-submitting all past messages?,"one can obtain a chatgpt response to a prompt using the following example:
from openai import openai

client = openai()  # requires key in open_ai_key environment variable

completion = client.chat.completions.create(
  model=""gpt-3.5-turbo"",
  messages=[
    {""role"": ""system"", ""content"": ""you are a poetic assistant, skilled in explaining complex programming concepts with creative flair.""},
    {""role"": ""user"", ""content"": ""compose a poem that explains the concept of recursion in programming.""}
  ]
)

print(completion.choices[0].message.content)

how can one continue the conversation? i've seen examples saying you just add a new message to the list of messages and re-submit:
# continue the conversation by including the initial messages and adding a new one
continued_completion = client.chat.completions.create(
    model=""gpt-3.5-turbo"",
    messages=[
        {""role"": ""system"", ""content"": ""you are a poetic assistant, skilled in explaining complex programming concepts with creative flair.""},
        {""role"": ""user"", ""content"": ""compose a poem that explains the concept of recursion in programming.""},
        {""role"": ""assistant"", ""content"": initial_completion.choices[0].message.content},  # include the initial response
        {""role"": ""user"", ""content"": ""can you elaborate more on how recursion can lead to infinite loops if not properly handled?""}  # new follow-up prompt
    ]
)

but i would imagine this means processing the previous messages all over again at every new prompt, which seems quite wasteful. is that really the only way? isn't there a way to keep a ""session"" of some sort that keeps chatgpt's internal state and just processes a newly given prompt?","['python', 'openai-api', 'langchain', 'chatgpt-api']",78214798,"from here

conversation summary
now let's take a look at using a slightly more complex type of memory

conversationsummarymemory. this type of memory creates a summary of the conversation over time. this can be useful for condensing
information from the conversation over time. conversation summary
memory summarizes the conversation as it happens and stores the
current summary in memory. this memory can then be used to inject the
summary of the conversation so far into a prompt/chain. this memory is
most useful for longer conversations, where keeping the past message
history in the prompt verbatim would take up too many tokens.


in a conversational context where you want to maintain continuity and provide context to openai's model, you need to send some form of conversation history. this history helps the model understand the ongoing dialogue and generate responses that are contextually relevant.",https://stackoverflow.com/questions/78213463,python,24-03-2024 05:11,1460.0,2.0,1.0,True,25-03-2024 03:00,24-03-2024 22:13
76768113,permissionerror while executing langchain with azureopenai,"i'm working with azureopenai and langchain, constantly getting hit by permissionerror. this mostly could be due to the proxy, but can someone please check the code --
from langchain.llms import openai, azureopenai
from langchain.prompts import prompttemplate
from langchain.chains import llmchain

llm = azureopenai(openai_api_type="""", openai_api_base="""", deployment_name="""", model_name="""", openai_api_key="""", openai_api_version="""")

template = """"""""
translate the following text from {source_lang} to {dest_lang}: {source_text}
""""""

prompt_name = prompttemplate(input_variables=[""source_lang"", ""dest_lang"", ""source_text""], template=template)
chain = llmchain(llm=llm, prompt=prompt_name)

chain.predict(source_lang=""english"", dest_lang=""spanish"", source_text=""how are you?"")

chain(inputs={""source_lang"": ""english"", ""dest_lang"": ""spanish"", ""source_text"": ""how are you""})

i also tried the additional openai_proxy parameter without much luck.","['azure', 'openai-api', 'langchain', 'azure-openai']",76815315,"your code runs smoothly! below my llm config. other variables are set with .env
llm=azurechatopenai(
    deployment_name=azure_openai_chatgpt_deployment,
    temperature=0.0,
)",https://stackoverflow.com/questions/76768113,azure,26-07-2023 04:50,627.0,1.0,1.0,True,31-08-2023 17:06,26-07-2023 06:30
75650840,openai chat completions api error 400: &quot;bad request&quot; (migrating from gpt-3 api to gpt-3.5 api),"i'm trying to call the chat completions api that was just released, but i'm getting a bad request error.

    var body = new
                    {
                        model = ""gpt-3.5-turbo"",
                        messages = data
                    };

                    string jsonmessage = jsonconvert.serializeobject(body);

  using ( client = new 
                    {
                        servicepointmanager.securityprotocol = securityprotocoltype.tls12;

                         requestmessage = new
                         ""
                        {
                            content = new stringcontent(jsonmessage, encoding.utf8, ""application/json"")
                        };

                        string api_key = pageextension_currentuser.community.caichatgptapikey.length > 30 ? pageextension_currentuser.community.caichatgptapikey : genesis.generic.readappsettingsvalue(""chatgptapikey"");
                        requestmessage.headers.add(""authorization"", $""bearer {api_key}"");

                         response = client.sendasync(requestmessage).result;
                        if (response.statuscode == 
                        {
                            string responsedata = response.content.readasstringasync().result;
                            dynamic responseobj = jsonconvert.deserializeobject(responsedata);
                            string choices = responseobj.choices[0].text;
                           
                    }


there is the code from their api documentation:
curl  \
  -h 'content-type: application/json' \
  -h 'authorization: bearer your_api_key' \
  -d '{
  ""model"": ""gpt-3.5-turbo"",
  ""messages"": [{""role"": ""user"", ""content"": ""hello!""}]
}'

here is another example:
openai.chatcompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
        {""role"": ""system"", ""content"": ""you are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""who won the world series in 2020?""},
        {""role"": ""assistant"", ""content"": ""the los angeles dodgers won the world series in 2020.""},
        {""role"": ""user"", ""content"": ""where was it played?""}
    ]
)

can anyone see why i'm getting the following error?
{statuscode: 400, reasonphrase: 'bad request', version: 1.1, content: system.net. headers:
{
  connection: keep-alive
  access-control-allow-origin: *
  openai-organization: user-lmjzqj7ba2bggaekkhr68aqn
  openai-processing-ms: 141
  openai-version: 2020-10-01
  strict-transport-security: max-age=15724800; includesubdomains
  x-request-id: 9eddf8bb8dcc106ca11d44ad7f8bbecc
  date: mon, 06 mar 2023 12:49:46 gmt
  content-length: 201
  content-type: application/json
}}



{method: post, requesturi: ' version: 1.1, content: system.net. headers:
{
  authorization: bearer sk-ihuxxxxxxxxxxxxxxxxxx[just removed my api key]xxxxxxxxxxxxxxx
  content-type: application/json; charset=utf-8
  content-length: 79
}}","['post', 'openai-api', 'chatgpt-api']",75650860,"you're using the gpt-3.5-turbo model.
there are three main differences between the chat completions api (i.e., the gpt-3.5 api) and the completions api (i.e., the gpt-3 api):

api endpoint

completions api: 
chat completions api: 


the prompt parameter (completions api) is replaced by the messages parameter (chat completions api)
response access

completions api: response.getjsonarray(""choices"").getjsonobject(0).getstring(""text"")
chat completions api: response.getjsonarray(""choices"").getjsonobject(0).getstring(""message"")




problem 1: you're using the wrong api endpoint
change this (completions api)...


...to this (chat completions api).



problem 2: make sure the json for the messages parameter is valid

curl: ""messages"": [{""role"": ""user"", ""content"": ""hello!""}]

python: messages = [{""role"": ""user"", ""content"": ""hello!""}]

nodejs: messages: [{role: ""user"", content: ""hello world""}]



problem 3: you're accessing the response incorrectly
note: openai nodejs sdk v4 was released on august 16, 2023, and is a complete rewrite of the sdk. among other things, there are changes in extracting the message content. see the v3 to v4 migration guide.
change this...
string choices = responseobj.choices[0].text;

...to this.
string choices = responseobj.choices[0].message.content;


problem 4: you didn't set the content-type header
add this:
requestmessage.headers.add(""content-type"", ""application/json"");

note: ""application/json, utf-8"" won't work, as @srishti mentioned in the comment below.",https://stackoverflow.com/questions/75650840,post,06-03-2023 12:26,24081.0,6.0,4.0,True,12-06-2024 17:20,12-06-2024 17:20
76434311,how to get the logits of the model with a text classification pipeline from huggingface?,"i need to use pipeline in order to get the tokenization and inference from the distilbert-base-uncased-finetuned-sst-2-english model over my dataset.
my data is a list of sentences, for recreation purposes we can assume it is:
texts = [""this is the first sentence"", ""of my data."", ""in fact, thats not true,"", ""but we are going to assume it"", ""is""]
before using pipeline, i was getting the logits from the model outputs like this:
with torch.no_grad():
     logits = model(**tokenized_test).logits

now i have to use pipeline, so this is the way i'm getting the model's output:
 selected_model = ""distilbert-base-uncased-finetuned-sst-2-english""
 tokenizer = autotokenizer.from_pretrained(selected_model)
 model = automodelforsequenceclassification.from_pretrained(selected_model, num_labels=2)
 classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
 print(classifier(text))

which gives me:
[{'label': 'positive', 'score': 0.9746173024177551}, {'label': 'negative', 'score': 0.5020197629928589}, {'label': 'negative', 'score': 0.9995120763778687}, {'label': 'negative', 'score': 0.9802979826927185}, {'label': 'positive', 'score': 0.9274746775627136}]
and i cant get the 'logits' field anymore.
is there a way to get the logits instead of the label and score? would a custom pipeline be the best and/or easiest way to do it?","['python', 'huggingface-transformers', 'sentiment-analysis', 'huggingface', 'large-language-model']",76435401,"when you use the default pipeline, the postprocess function will usually take the softmax, e.g.
from transformers import autotokenizer, automodelforsequenceclassification

tokenizer = autotokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')
model = automodelforsequenceclassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')


text = ['hello this is a test',
 'that transforms a list of sentences',
 'into a list of list of sentences',
 'in order to emulate, in this case, two batches of the same lenght',
 'to be tokenized by the hf tokenizer for the defined model']

classifier(text, batch_size=2, truncation=""only_first"")

[out]:
[{'label': 'negative', 'score': 0.9379090666770935},
 {'label': 'positive', 'score': 0.9990271329879761},
 {'label': 'negative', 'score': 0.9726701378822327},
 {'label': 'negative', 'score': 0.9965035915374756},
 {'label': 'negative', 'score': 0.9913086891174316}]

so what you want is to overload the postprocess logic by inheriting from the pipeline.
to check which pipeline the classifier inherits do this:
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
type(classifier)

[out]:
transformers.pipelines.text_classification.textclassificationpipeline

now that you know the parent class of the task pipeline you want to use, now you can do this and still enjoy the perks of the precoded batching from textclassificationpipeline:
from transformers import textclassificationpipeline

class mariotheplumber(textclassificationpipeline):
    def postprocess(self, model_outputs):
        best_class = model_outputs[""logits""]
        return best_class

pipe = mariotheplumber(model=model, tokenizer=tokenizer)

pipe(text, batch_size=2, truncation=""only_first"")

[out]:
[tensor([[ 1.5094, -1.2056]]),
 tensor([[-3.4114,  3.5229]]),
 tensor([[ 1.8835, -1.6886]]),
 tensor([[ 3.0780, -2.5745]]),
 tensor([[ 2.5383, -2.1984]])]",https://stackoverflow.com/questions/76434311,python,08-06-2023 17:26,4458.0,8.0,1.0,True,12-06-2023 00:41,08-06-2023 20:15
76633913,"if an fst transition is based on a given context, how can it be called as &#39;non deterministic&#39;?","i am going through the paper 'speech recognition with weighted finite-state transducers' (hbka.pdf - 
at page 9, and figure 6
i do not understand how 6 a) figure be considered for 'non-determinism'.
[here ae/k t represents the triphonic model for ae with left context k and right context t. this transition is considered as non-deterministic. why?]
the differences between transitions that are termed 'non-deterministic' and 'deterministic' are

nondeterministic transitions allow (epsilon transitions) a transition to happen between states without any input symbol consumption and/or any output symbol generation.

this is especially useful where the transducer is supposed to output english word morphological description from the input english word, like in cities -> city -pl. here there are transitions between states where you input and output the same alphabet. but when you input i or e you need not output anything and in the final transition for input s you would output y -pl
so, i understand the need for epsilon transitions. i also understand that non-deterministic transitions are when multiple transitions for same input label can exist. this could lead to ambiguity and hence supports the name 'non-deterministic'.

deterministic transitions are the very opposite of the above. no multiple transitions for same input label; hence every transition is unique. no epsilon transitions.

with this limited knowing, i could not decipher why a transition could ever be called 'non-deterministic' when you have provided the context. here ae/k t represents the triphonic model for ae with left context k and right context t. this transition is considered as non-deterministic. why?
the main idea of providing context is to remove the ambiguity.","['nlp', 'speech-recognition', 'speech-to-text', 'state-machine', 'kaldi']",76633959,"the text to the right of the colon : is the output of the transition, not its input; so it doesn't help determine which transition to take, but rather, you need to take the right transition in order to get the right output.",https://stackoverflow.com/questions/76633913,nlp,07-07-2023 04:16,83.0,0.0,1.0,True,07-07-2023 04:30,07-07-2023 04:19
78921595,does the answer quality of openai gpt4o (from api) change over time?,"i'm creating an application that uses gpt-4 (via the openai api) for visual question answering.
the problem is that tests using this module, which previously passed, have started to fail constantly due to a decline in the quality of the answers. is this expected behavior for gpt-4 (or the openai api)?","['openai-api', 'chatgpt-api', 'gpt-4', 'chat-gpt-4']",78936797,"if you use the gpt-4o as a model identifier in your application, then yes, the underlying model might change.
let's take gpt-4o as an example:

as i write this, it points to gpt-4o-2024-05-13
but soon it will point to the newer gpt-4o-2024-08-06

it's a good question whether the quality of the answer can decrease in newer models. companies use various metrics to ensure it doesn't happen. and the blind tests rating of ai models generally show that people rank newer models higher.
at the same time, it's easy to find anecdotal opinions that some previous models were better. at first, my impression too was that gpt-4 was better than the newer gpt-4o, but when i did a blind test, i preferred answers from gpt-4o.
if you prefer to stick to tested legacy model version, you can still use more specific id like gpt-4o-2024-05-13.",https://stackoverflow.com/questions/78921595,openai-api,28-08-2024 05:01,361.0,1.0,1.0,True,01-09-2024 07:31,28-08-2024 07:54
70449122,change last layer on pretrained huggingface model,"i want to re-finetuned a transformer model but i get an unknown error when i tried to train the model.
i can't change the ""num_labels"" on loading the model.
so, i tried to change it manually
model_name = ""mrm8488/flaubert-small-finetuned-movie-review-sentiment-analysis""
tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforsequenceclassification.from_pretrained(model_name).to('cuda')

num_labels = 3
model.sequence_summary.summary = torch.nn.linear(in_features=model.sequence_summary.summary.in_features, out_features=num_labels, bias=true)




trainer = trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train['train'],
    eval_dataset=tokenized_test['train'],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    #data_collator=data_collator,
)

trainer.train()

the error
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
<ipython-input-93-8139f38c5ec6> in <module>()
     20 )
     21 
---> 22 trainer.train()

7 frames
/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)
   2844     if size_average is not none or reduce is not none:
   2845         reduction = _reduction.legacy_get_string(size_average, reduce)
-> 2846     return torch._c._nn.cross_entropy_loss(input, target, weight, _reduction.get_enum(reduction), ignore_index, label_smoothing)
   2847 
   2848 

valueerror: expected input batch_size (24) to match target batch_size (16).","['python', 'pytorch', 'torch', 'huggingface-transformers']",70450157,"so, there is a solution for this
just add ignore_mismatched_sizes=true when loading the model as:
model = automodelforsequenceclassification.from_pretrained(model_name,num_labels=3, ignore_mismatched_sizes=true).to('cuda')",https://stackoverflow.com/questions/70449122,python,22-12-2021 12:33,3401.0,2.0,2.0,True,25-12-2021 18:14,25-12-2021 18:14
66895997,pattern extract using regex in python,"i am trying to use regex for word extraction in python, since i am beginner and not experienced in regex i want you to help me, i have this string :
deadline for nsf-bsf programs in elementary particle physics ï¿½ï¿½ï¿½ theory; particle astrophysics and cosmology ï¿½ï¿½ï¿½ theory; quantum information science (nsf deadline is dec. 14)

and i want the output to be a list of area or research in this word, so the output should be:
[elementary particle physics, particle astrophysics and cosmology, quantum information science]

could any one give regular expression to identify this pattern using re.findall().
thanks in a","['python', 'regex', 'nlp']",66896188,"assuming that (1) ' in ' is the indicator that the words you're interested in are are starting, and that (2) all areas are separated by ';' and that (3) all areas end with  - theory or something in parenthesis, we can get the list you're looking for. note: these are the assumptions that must be consistent across all input sources if the below code is to work as expected.
import re

src = ""deadline for nsf-bsf programs in elementary particle physics - theory; "" \
      ""particle astrophysics and cosmology - theory; "" \
      ""quantum information science (nsf deadline is dec. 14)""

_, out = src.split(' in ')
out = [re.split(r'( - theory)|\(.*\)', o)[0].strip() for o in out.split(';')]

print(out)

out:
['elementary particle physics',
 'particle astrophysics and cosmology',
 'quantum information science']",https://stackoverflow.com/questions/66895997,python,31-03-2021 22:13,87.0,-1.0,1.0,True,24-11-2021 05:58,31-03-2021 22:29
71512301,"error: could not build wheels for spacy, which is required to install pyproject.toml-based projects","hi guys, i am trying to install spacy model == 2.3.5 but i am getting this error, please help me!","['python', 'python-3.x', 'pip', 'nlp', 'spacy']",74540622,"i had the similar error while executing pip install -r requirements.txt but for aiohttp module:
socket.c -o build/temp.linux-armv8l-cpython-311/aio
aio fatal error: 'longintrepr.h' file not found
#include ""longintrepr.h""                                   
          ^~~~~~~                        1 error generated.
error: command '/data/data/com.termux/files/usr/bin/arm-linux-androideabi-clang' 
failed with exit code 1
[end of output]
note: this error originates from a subprocess, and is likely not a problem with pip.
error: failed building wheel for aiohttp
failed to build aiohttp
error: could not build wheels for aio which is required to install
pyproject.toml-based projects

just in case i will leave here solution to my error. this error is specific to python 3.11 version. on python with 3.10.6 version installation went fine.
to solve it i needed to update requirements.txt.
not working versions of modules with python 3.11:
aio
yarl==1.4.2
frozenlist==1.3.0

working versions:
aio
yarl==1.8.1
frozenlist==1.3.1

links to the corresponding issues with fixes:",https://stackoverflow.com/questions/71512301,python,17-03-2022 12:29,47328.0,5.0,4.0,True,27-05-2024 06:20,27-05-2024 06:20
73234626,how to keep structure of text after feeding it to a pipeline for ner,"i've build an ner (named entity recognition) model, based on a huggingface existing model and that i fine-tuned to recognize my custom entities. the text i want to run my model on is in a txt file.
the code of how i use the model:
from transformers import pipeline

# loading the fine-tuned model
ner_pipeline =  pipeline('token-classification', model=""./my-model.model/"", tokenizer=""./my-model.model/"", ignore_labels=[])

with open(my_file, 'r', encoding=""utf8"") as f:
  lines = f.readlines()
  joined_lines = ' '.join(lines)

  result = ner_pipeline(joined_lines, aggregation_strategy='first')
  text = """"
      
  for group in result:
     if group[""entity_group""] != 'o':
        #ï¿½ï¿½substitute the entity with its tag
        text += group[""entity_group""]+ "" ""
     else:
        text += group[""word""] + "" ""

basically what i do is substituting the entities recognized with the entity tag, and leave the rest of the text as is.
with my code, the final text is filled with the content exactly as i want it, but the structure is lost. while doing ' '.join(lines) i'm basically throwing away the \nde>s inside the text, that however i would like to keep in my reconstructed text.
i've tried feeding the pipeline with single sentences (each of the f.readlines()) end not the full joined text, but the results are far worse. the model works a lot better predicting on the whole text.
does anyone knows a way how i could keep or retrieve the structure of the original text? thanks.","['python', 'nlp', 'tokenize', 'huggingface-transformers', 'named-entity-recognition']",73259456,"the groups have a start and end index that tell you which part of the input string each label corresponds to. i.e., you can pass the text as a whole, with the newlines intact (ner_pipeline(f.read(), ...)) and subsequently replace substrings.
here's a working, minimal reproducible example. the only thing to note here is that we replace from right to left (result[::-1]) so we don't mess up the indices of subsequent labels by changing the length of the string when replacing.
from nltk.corpus import brown # for example data
from transformers import pipeline

ner_pipeline =  pipeline('token-classification')

# equivalent to f.read()
text = '\n'.join(' '.join(sent) for sent in brown.sents()[:100])

result = ner_pipeline(lines_joined, aggregation_strategy='first')

def replace_at(label, start, end, txt):
    """"""replace substring of txt from start to end with label""""""
    return ''.join((txt[:start], label, txt[end:]))

# substitution
for group in result[::-1]:
    ent = group[""entity_group""]
    if ent != 'org': # for testing since there's no 'o' in the default model
        text = replace_at(ent, group['start'], group['end'], text)

sentences = text.split('\n')

example input/output (first line):
""the fulton county grand jury said friday an investigation of atlanta's recent primary election produced `` no evidence '' that any irregularities took place .""

after processing:
""the fulton county grand jury said friday an investigation of loc's recent primary election produced `` no evidence '' that any irregularities took place .""
                                                              ^^^",https://stackoverflow.com/questions/73234626,python,04-08-2022 10:51,646.0,1.0,1.0,True,06-08-2022 11:46,04-08-2022 13:06
76160057,openai chat completions api: how do i customize answers from gpt-3.5 or gpt-4 models if i can&#39;t fine-tune them?,"we have seen some companies use gpt-3.5 or gpt-4 models to train their own data and provide customized answers. but gpt-3.5 and gpt-4 models are not available for fine-tuning.
i've seen the document from openai about this issue, but i had seen openai only allow fine-tuning davinci, for example.
how do i customize answers from a gpt-3.5 or gpt-4 model if i can't fine-tune them?","['openai-api', 'chatgpt-api', 'gpt-4']",76161653,"they don't fine-tune the gpt-3.5 or gpt-4 models.
you have two options.

option 1: using llamaindex or langchain
what they do is use llamaindex (formerly gpt-index) or langchain. both of them enable you to connect openai models with your existing data sources.

option 2: using the openai embeddings api endpoint
see my past answer. also, as @peter_the_oak mentioned, you can use pinecone to store embedding vectors. pinecone is designed specifically for handling this type of data.",https://stackoverflow.com/questions/76160057,openai-api,03-05-2023 02:27,2401.0,3.0,2.0,True,20-02-2024 15:00,20-02-2024 15:00
58900022,get the topics or the keywords of a sentence,"good morning all
does anyone of you know a tool or an api or something that takes a sentence as input and as output, it gives the topics or keywords of this sentence?
i tried textrazor in the online demo it works well like you can see in the screenshot 

but when i used as a library in my python code it always gives me a blank list even for the sentence used in the demo
this is my code in python:
import textrazor
import ssl
textrazor.api_key =""bdd69bdc3f91045cdb6d4261d39df34d887278602cb8f60401b7eb0b""
client = textrazor.textrazor(extractors=[""entities"", ""topics""])
client.set_cleanup_mode(""cleanhtml"")
client.set_classifiers([""textrazor_newscodes""])
sentence = ""adam hill,b it's super bowl sunday  pastors. get your jesus jukes ready! guilt is an awesome motivator! #sarcasm""
response = client.analyze(sentence)
print(sentence)
print(len(response.topics()))
entities = list(response.entities())
print(len(entities))
for topic in response.topics():
    if topic.score > 0.3:
        print (topic.label)

it gives me zero for entities and topics length
someone proposed for me to use opennlp but i didn't get how to extract topics and keywords if anyone of you has any tutorial or clarification please help me  
and thank you in advance","['python', 'nlp', 'data-analysis']",58900447,"you have to remove the line client.set_cleanup_mode(""cleanhtml""). then it should work just fine.
as i understand the cleanup_mode, it treads your text as an html. as your example text is not html it won't find any raw text in between html tags.",https://stackoverflow.com/questions/58900022,python,17-11-2019 11:33,842.0,0.0,1.0,True,29-12-2023 15:01,29-12-2023 15:01
72814534,error when passing argument through function for converting pandas dataframe of tweets into corpus files,"i want to prepare my text data that is in a pandas dataframe for sentiment analysis with nltk. for that, i'm using code for a function that converts each row of a pandas dataframe into a corpus.
import nltk
# convert each row of the pandas dataframe of tweets into corpus files
def createcorpusfromdataframe(corpusfolder,df):
    for index, r in df.iterrows():
        date=r['date']
        tweet=r['text']
        place=r['place']
        fname=str(date)+'_'+'.txt'
        corpusfile=open(corpusfolder+'/'+fname,'a')
        corpusfile.write(str(tweet) +"" "" +str(date))
        corpusfile.close()
createcorpusfromdataframe(myfolder,mydf)

the problem is i keep getting the message that
nameerror: name 'myfolder' is not defined

even though i have a folder called 'myfolder' in the same path directory of jupyter notebook that my code is in?
update:
i can see now that the issue was simply that i needed to pass the folder name as a string. now that i've done that and amended my code. the problem i have now is that the contents of the text file created with the function are not being written into a corpus and the type of variable being created is a 'nonetype'.
import nltk
# convert each row of the pandas dataframe of tweets into corpus files
def createcorpusfromdataframe(corpusfolder,df):
    for index, r in df.iterrows():
        id=r['date']
        tweet=r['text']
        #place=r['place']
        #fname=str(date)+'_'+'.txt'
        fname='tweets'+'.txt'
        corpusfile=open(corpusfolder+'/'+fname,'a')
        corpusfile.write(str(tweet) +"" "")
        corpusfile.close()
corpus df = createcorpusfromdataframe('myfolder',mydf)
type(corpusdf)
nonetype","['python', 'pandas', 'function', 'nltk']",72814664,"problem
you are passing myfolder as a variable to your function which you have not defined in your code and hence it raises a nameerror.
solution
just replace it with 'myfolder' [pass it as a string].
createcorpusfromdataframe('myfolder',mydf)",https://stackoverflow.com/questions/72814534,python,30-06-2022 10:52,47.0,0.0,1.0,True,30-06-2022 16:07,30-06-2022 16:07
75503124,bitwise operation on a dynamic data structure,"i am implementing a simple document indexer for information retrieval. now i need to implement an incidence matrix, that should be able to be extended dynamically (not satisfied with just a static array or smth).
and to make boolean search possible i have to be able to perform bitwise operations on the rows of the matrix. however, i have not come up with a fast solution. the question is data structure for each row of the matrix.
if it were just a std::vector<bool>, is it possible to do fast bitwise operations on it? or is there any other data structure, like bitarray from c#, applicable in the situation?","['c++', 'bitwise-operators', 'information-retrieval', 'adjacency-matrix', 'boolean-search']",75503280,"if fast is your goal, look into using largest int available on your system (likely - uint64_t) and do a simple bitwise operations on that. if your matrix is wider that 64 - use an std::array of those. then check if your compiler generates simd instructions from your code. if not - consider using intrinsics",https://stackoverflow.com/questions/75503124,c++,19-02-2023 20:10,109.0,1.0,1.0,True,19-02-2023 20:36,19-02-2023 20:24
73124816,split data frame of comments into multiple rows,"i have a data frame with long comments and i want to split them into indiviual sentences using spacy sentencizer.
comments = pd.read_excel('comments.xlsx', sheet_name = 'sheet1')  
comments
>>>
         reviews
    0    one of the rare films where every discussion leaving the theater is about how much you 
         just had, instead of an analysis of its quotients.
    1    gorgeous cinematography, insane flying action sequences, thrilling, emotionally moving, 
         and a sequel that absolutely surpasses its predecessor. well-paced, executed & has that 
         re-watchability factor.


i loaded the model like this
import spacy
nlp = spacy.load(""en_core_news_sm"")

and using sentencizer
from spacy.lang.en import english
nlp = english()
nlp.add_pipe('sentencizer')
data = comments.reviews.apply(lambda x : list( nlp(x).sents))

but when i check the sentence is in just one row like this
[one of the rare films where every discussion leaving the theater is about how much you just had.,
 instead of an analysis of its quotients.]

thanks a lot for any help. i'm new using nlp tools in data frame.","['python', 'dataframe', 'lambda', 'nlp', 'spacy']",73175632,"currently, data is a series whose rows are lists of sentences, or actually, lists of spacy's span objects. you probably want to obtain the text of these sentences and to put each sentence on a different row.
comments = [{'reviews': 'this is the first sentence of the first review. and this is the second.'},
            {'reviews': 'this is the first sentence of the second review. and this is the second.'}]

comments = pd.dataframe(comments) # building your input dataframe

+----+--------------------------------------------------------------------------+
|    | reviews                                                                  |
|----+--------------------------------------------------------------------------|
|  0 | this is the first sentence of the first review. and this is the second.  |
|  1 | this is the first sentence of the second review. and this is the second. |
+----+--------------------------------------------------------------------------+

now let's define a function which, given a string, returns the list of its sentences as texts (strings).
def obtain_sentences(s):
    doc = nlp(s)
    sents = [sent.text for sent in doc.sents]
    return sents

the function can be applied to the comments dataframe to produce a new dataframe containing sentences.
data = comments.copy()
data['reviews'] = comments.apply(lambda x: obtain_sentences(x['reviews']), axis=1)
data = data.explode('reviews').reset_index(drop=true)
data

i used explode to transform the elements of the lists of sentences into rows.
and this is the obtained output!
+----+--------------------------------------------------+
|    | reviews                                          |
|----+--------------------------------------------------|
|  0 | this is the first sentence of the first review.  |
|  1 | and this is the second.                          |
|  2 | this is the first sentence of the second review. |
|  3 | and this is the second.                          |
+----+--------------------------------------------------+",https://stackoverflow.com/questions/73124816,python,26-07-2022 14:02,169.0,1.0,1.0,True,30-07-2022 12:31,26-07-2022 14:09
76435173,categorize rows per their similarity in python,"i am here to look for input for a data manipulation problem related to natural language processing.
to make life easier, i am using a mock dataset posted several years ago from how to group text data based on document similarity?.
import pandas as pd
from difflib import sequencematcher

df = pd.dataframe({'questions': ['what are you doing?','what are you doing tonight?','what are you doing now?','what is your name?','what is your nick name?','what is your full name?','shall we meet?',
                             'how are you doing?' ]})

def similarity_score(s1, s2):
    return sequencematcher(none, s1, s2).ratio()

def similarity(x,df):
    sim_score = []
    for i in df['questions']:
        sim_score.append(similarity_score(x,i))
    return sim_score

df['similarity'] = df['questions'].apply(lambda x : similarity(x, df)).astype(str)
print(df)

the output is as following
questions  \
0          what are you doing?   
1  what are you doing tonight?   
2      what are you doing now?   
3           what is your name?   
4      what is your nick name?   
5      what is your full name?   
6               shall we meet?   
7           how are you doing?   

                                          similarity  
0  [1.0, 0.8260869565217391, 0.9047619047619048, ...  
1  [0.8260869565217391, 1.0, 0.84, 0.533333333333...  
2  [0.9047619047619048, 0.84, 1.0, 0.585365853658...  
3  [0.6486486486486487, 0.5333333333333333, 0.585...  
4  [0.5714285714285714, 0.52, 0.5217391304347826,...  
5  [0.5714285714285714, 0.52, 0.5652173913043478,...  
6  [0.36363636363636365, 0.34146341463414637, 0.3...  
7  [0.8108108108108109, 0.6666666666666666, 0.731...  

the logic is that i go through each row in the data frame to compare it to all over rows (including itself) in order to compute their similarity. i then store the similarity score as a list in another column called ""similarity"".
next, i want to categorize the questions in the first column. if the similarity score > 0.9, then those rows should be assigned to the same group. how can i achieve this?","['python', 'pandas', 'nlp', 'aggregate', 'grouping']",76436371,"a solution is to iterate row-wise over your similarity scores, create a binary mask based on some threshold, and then use the binary mask to only extract those questions who meet the threshold.
note that this solution presumes that the ""groups"" you desire are the questions themselves (i.e. for each question, you want a list of similar questions associated with it). i made up similarity scores for the rest of the array to create this minimal example.
solution
import pandas as pd

orig_data = {
    ""questions"": [
        ""what are you doing?"",
        ""what are you doing tonight?"",
        ""what are you doing now?"",
        ""what is your name?"",
        ""what is your nick name?"",
        ""what is your full name?"",
        ""shall we meet?"",
        ""how are you doing?"",
    ],
    ""similarity"": [
        [1.0, 0.826, 0.905, 0.234, 0.544, 0.673, 0.411, 0.45],
        [0.826, 1.0, 0.84, 0.533, 0.444, 0.525, 0.641, 0.62],
        [0.905, 0.84, 1.0, 0.585, 0.861, 0.685, 0.455, 0.65],
        [0.649, 0.533, 0.585, 1.0, 0.901, 0.902, 0.642, 0.234],
        [0.571, 0.52, 0.522, 0.901, 1.0, 0.905, 0.753, 0.786],
        [0.571, 0.52, 0.565, 0.902, 0.903, 1.0, 0.123, 0.586],
        [0.364, 0.341, 0.3, 0.674, 0.584, 0.421, 1.0, 0.544],
        [0.811, 0.667, 0.731, 0.345, 0.764, 0.242, 0.55, 1.0],
    ],
}

df = pd.dataframe(orig_data)

results = []
for idx, sim_row in enumerate(df[""similarity""]):
    bin_mask = [true if score > 0.9 else false for score in sim_row]
    curr_q = df[""questions""][idx]
    sim_quests = [q for q, b in zip(df[""questions""], bin_mask) if b and q != curr_q]
    results.append(sim_quests)

df[""similar-questions""] = results
print(df)

output
                     questions  ...                                  similar-questions
0          what are you doing?  ...                          [what are you doing now?]
1  what are you doing tonight?  ...                                                 []
2      what are you doing now?  ...                              [what are you doing?]
3           what is your name?  ...  [what is your nick name?, what is your full na...
4      what is your nick name?  ...      [what is your name?, what is your full name?]
5      what is your full name?  ...      [what is your name?, what is your nick name?]
6               shall we meet?  ...                                                 []
7           how are you doing?  ...                                                 []",https://stackoverflow.com/questions/76435173,python,08-06-2023 19:35,226.0,1.0,1.0,True,08-06-2023 23:56,08-06-2023 23:53
74922924,how to add threshold limit to tf-idf values in a sparse matrix,"i am using sklearn.feature_extraction.text, tfidftransformer to get the tf_idf values for my corpus.
this is how my code looks like
    x = dataset[:,0]
    y = dataset[:,1]

    for index, item in enumerate(x):
        reqjson = json.loads(item, object_pairs_hook=ordereddict)
        x[index] = json.dumps(reqjson, separators=(',', ':'))
    count_vect = countvectorizer()
    x_train_counts = count_vect.fit_transform(x)


    tfidf_transformer = tfidftransformer()
    x_train_tfidf = (tfidf_transformer.fit_transform(x_train_counts))

    #(58720, 167216) is the size of my sparse matrix


    for i in range (0,58720):
        for j in range (0,167216):
            print(i,j)
            if x_train_tfidf[i,j]>0.35:
                x_train_tfidf[i,j]=0

as you can see that i want to filter out tf-idf values which more than 0.35 so that i can reduce my feature set and make my model more time efficient but using a for loop just makes worse. i have looked into the documentation of tfidftransformer but cannot find a way to make it any better. any ideas or tips? thank you.","['python', 'scikit-learn', 'nlp', 'tf-idf']",74923600,"it sounds like this question is trying to ignore frequent words.
the tfidfvectorizer (not tfidftransformer) implementation includes a max_df parameter for:

when building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).

in the following example, word1 and word3 occur in >50% of documents, so setting max_df=0.5 means the resulting array only includes word2:
from sklearn.feature_extraction.text import tfidfvectorizer

raw_data = [
    ""word1 word2 word3"",
    ""word1 word1 word1"",
    ""word2 word2 word3"",
    ""word1 word1 word3"",
]

vect = tfidfvectorizer(max_df=0.5)
x = vect.fit_transform(raw_data)

print(vect.get_feature_names_out())
print(x.todense())

['word2']
[[1.]
 [0.]
 [1.]
 [0.]]",https://stackoverflow.com/questions/74922924,python,26-12-2022 18:14,702.0,2.0,1.0,True,26-12-2022 19:57,26-12-2022 18:20
79330283,can&#39;t compile marian nmt,"i'm using endeavouros. i'm trying to compile marian with these instructions:  but it fails.
the error message seemingly indicates a conflict between the code and c++20. but in all the cmakelists.txt files of the repo, there is the line set (cmake_cxx_standard 11).
these are the steps that i followed:
git clone 
mkdir marian/build
cd marian/build
cmake ..
make -j4

this is the result i had:
ï¿½ï¿½ï¿½ make -j4
[  1%] built target 3rd_party_installs
[  1%] built target marian_version
[  6%] built target sentencepiece_train-static
[ 19%] built target libyaml-cpp
[ 25%] built target sqlitecpp
[ 25%] built target pathie-cpp
[ 32%] built target zlib
[ 35%] built target intgemm
[ 35%] built target faiss
[ 53%] built target sentencepiece-static
[ 55%] built target spm_decode
[ 55%] built target spm_normalize
[ 55%] built target spm_encode
[ 55%] building cxx object src/cmakefiles/marian.dir/common/aliases.cpp.o
[ 55%] building cxx object src/cmakefiles/marian.dir/common/fastopt.cpp.o
[ 56%] built target spm_train
[ 57%] built target spm_export_vocab
[ 57%] building cxx object src/cmakefiles/marian.dir/common/utils.cpp.o
[ 58%] building cxx object src/cmakefiles/marian.dir/common/logging.cpp.o
in file included from /data/tools/marian/src/3rd_spdlog/details/spdlog_impl.h:12,
                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,
                 from /data/tools/marian/src/common/logging.h:5,
                 from /data/tools/marian/src/common/definitions.h:3,
                 from /data/tools/marian/src/common/fastopt.h:3,
                 from /data/tools/marian/src/common/fastopt.cpp:1:
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  138 |     registry_t<mutex>() {}
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ï¿½ï¿½ï¿½< >ï¿½ï¿½ï¿½
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  139 |     registry_t<mutex>(const registry_t<mutex>&) = delete;
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/re:22: note: remove the ï¿½ï¿½ï¿½< >ï¿½ï¿½ï¿½
in file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,
                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,
                 from /data/tools/marian/src/common/logging.h:5,
                 from /data/tools/marian/src/common/utils.cpp:2:
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  138 |     registry_t<mutex>() {}
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ï¿½ï¿½ï¿½< >ï¿½ï¿½ï¿½
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  139 |     registry_t<mutex>(const registry_t<mutex>&) = delete;
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove tle included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,
                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,
                 from /data/tools/marian/src/common/logging.h:5,
                 from /data/tools/marian/src/common/logging.cpp:1:
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  138 |     registry_t<mutex>() {}
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ï¿½ï¿½ï¿½< >ï¿½ï¿½ï¿½
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  139 |     registry_t<mutex>(const registry_t<mutex>&) = delete;
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ï¿½ï¿½ï¿½< >ï¿½ï¿½ï¿½
in file included from rd_party/spdlog/details/spdlog_impl.h:12,
                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,
                 from /data/tools/marian/src/common/logging.h:5,
                 from /data/tools/marian/src/common/definitions.h:3,
                 from /data/tools/marian/src/common/cli_wrapper.h:6,
                 from /data/tools/marian/src/common/config_parser.h:4,
                 from /data/tools/marian/src/common/aliases.cpp:1:
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  138 |     registry_t<mutex>() {}
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ï¿½ï¿½ï¿½< >ï¿½ï¿½ï¿½
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  139 |     registry_t<mutex>(const registry_t<mutex>&) = delet                    ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ï¿½ï¿½ï¿½< >ï¿½ï¿½ï¿½
cc1plus: all warnings being treated as errors
make[2]: *** [src/cmakefiles/marian.dir/build.make:93: src/cmakefiles/marian.dir/common/fastopt.cpp.o] error 1
make[2]: *** waiting for unfinished jobs....
cc1plus: all warnings being treated as errors
make[2]: *** [src/cmakefiles/marian.dir/build.make:121: src/cmakefiles/marian.dir/common/utils.cpp.o] error 1
cc1plus: all warnings being treated as errors
make[2]: *** [src/cmakefiles/marian.dir/build.make:79: src/cmakefiles/marian.dir/common/aliases.cpp.o] error 1
cc1plus: all warnings being treated as errors
make[2]: *** [src/cmakefiles/marian.dir/build.make:135: src/cmakefiles/marian.dir/common/logging.cpp.o] error 1
make[1]: *** [cmakefiles/makefile2:374: src/cmakefiles/marian.dir/all] error 2
make: *** [makefile:156: all] error 2

pleas","['gcc', 'cmake', 'nlp', 'g++']",79332711,"the diagnostic that your build is tripping, wtemplate-id-cdtor, was introduced
with gcc 14.1. it is a warning, not an error, but your build promotes all warnings to
errors, so it breaks your build.
although your build specifies -std=c++11 in src/3rd_party/spdlog/cmakelists.txt, which
generates the failure, g++-14 emits wtemplate-id-cdtor to warn you that the code would be
illegal under the more recent standard c++20 (and later). then the warning is made an error.
the warning is made an error by the compile option -werror. this option is included in the list
of compile options all_warnings, which is created in the top-level marian/cmakelists.txt
at line 227 et seq:
# these are used in src/cmakelists.txt on a per-target basis
list(append all_warnings -wall; -werror; -wextra; -wno-unused-result; -wno-deprecated;
-wno-pragmas; -wno-unused-parameter; -wno-unused-function;
-wno-unused-value; -wno-unknown-pragmas; -wno-sign-compare;
-wno-missing-field-initializers;)

and then applied as compile options for the marian library target in src/cmakelists.txt
at line 133:
target_compile_options(marian private ${all_warnings})

whence the options are operative for the failing compilation of src/cmakefiles/marian.dir/common/logging.cpp.
this failure is a bug in the marian repo which you should report to the maintainers, as
it does not seem to have been reported already. the head revision v1.12.0 is more than a year older than gcc 14.
pending a fix, you seem to have three interim options to get your build done. either:

make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence:
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in c++20 [-werror=template-id-cdtor]
  139 |     registry_t<mutex>(const registry_t<mutex>&) = delete;
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ï¿½ï¿½ï¿½< >ï¿½ï¿½ï¿½



e.g. make it registry_t(const registry_t<mutex>&) = delete; in this occurrence.
or:

locally disable -wtemplate-id-cdtor at each occurrence, e.g:
#pragma gcc diagnostica gcc diagnostic ignored ""-wtemplate-id-cdtor""
registry_t<mutex>(const registry_t<mutex>&) = delete;
#pragma gcc diagnostic pop



or:

remove -werror from the all_warnings list in marian/cmakelists.txt so that wtemplate-id-cdtor remains just a warning. this may result in other diagnostics being demoted from errors to warnings (their default status).

i haven't tested any of these options as i'd need to go to the trouble of installing cuda.",https://stackoverflow.com/questions/79330283,gcc,05-01-2025 06:04,77.0,4.0,1.0,True,07-01-2025 09:00,05-01-2025 06:12
78005051,modulenotfounderror: no module named &#39;llama_index.graph_stores&#39;,"i am trying to use the nebulagraphstore class from llama_index via from llama_index.graph_stores.nebula import nebulagraphstore as suggested by the llama_index documentation, but the following error occurred:
modulenotfounderror                       traceback (most recent call last)
cell in[2], line 1
----> 1 from llama_index.graph_stores.nebula import nebulagraphstore

modulenotfounderror: no module named 'llama_index.graph_stores'

i tried updating llama_index (version 0.10.5) with pip install -u llama-index but it doesn't work. how can i resolve this?","['python', 'langchain', 'large-language-model', 'llama-index', 'nebula-graph']",78031871,"according to latest doc of llama-index, all graph-store module are not included in llama-index core packages and needs to install it by pip:
%pip install llama-index-llms-openai
%pip install llama-index-embeddings-openai
%pip install llama-index-graph-stores-nebula
%pip install llama-index-llms-azure-openai",https://stackoverflow.com/questions/78005051,python,16-02-2024 03:32,16969.0,2.0,2.0,True,21-04-2024 04:53,16-02-2024 19:08
74921920,i can this specific error while download nltk,"import nltk
nltk.download('punkt')

showing error:
[nltk_data] error loading punkt: <urlopen error [winerror 10060] a
[nltk_data]     connection attempt failed because the connected party
[nltk_data]     did not properly respond after a period of time, or
[nltk_data]     established connection failed because connected host
[nltk_data]     has failed to respond>

i have pip installed nltk and tried downloading the full nltk version for my learning.
# i used this key to download full version
import nltk
nltk.download()

showing the error: winerror 10060","['python', 'python-3.x', 'windows', 'nltk']",75072632,changing the network to another wifi connection worked for me. the previous connection had some sort of blockers for me. hope this helps.,https://stackoverflow.com/questions/74921920,python,26-12-2022 15:53,634.0,1.0,1.0,True,10-01-2023 16:00,26-12-2022 20:55
73729113,extracting n-grams from txt only returns the first lines,"i'm a total newbie in ml and everything in it.
i have a ~15k log and my goal is to extract 3 to 8-grams from it. the code i'm using is partially adopted from this question.

    df = pd.read_fwf(r'c:\path\to\my\log.txt')
    vect = sklearn.feature_extraction.text.countvectorizer(ngram_range=(3,8))
    vect.fit(df)
    for w in vect.get_feature_names_out():
    print(w)


the code actually works, but i'm not able to ""iterate"" over the txt. the result of the execution only returns the first x n-grams extracted from the first 2-3 lines of the log. how can i read and extract all the n-grams from the document?
extra question: since the final goal is to extract the n-grams and build a tf-idf model on them, does the fact that my log is a txt instead of csv represent a problem? i have variable-lenght lines so csv is not feasible i guess.","['python', 'scikit-learn', 'tf-idf']",73729200,"use a for loop on a file object to read it line-by-line. use with open(...) to let a context manager ensure that the file is closed after reading:
with open(""log.txt"") as infile:
    for line in infile:
        print(line)",https://stackoverflow.com/questions/73729113,python,15-09-2022 09:59,38.0,0.0,1.0,True,15-09-2022 10:04,15-09-2022 10:02
72772448,spacy - adding multiple patterns to a single ner using entity ruler,"so this is my problem in spacy rule based matching.
i have a txt group say
text = ('wan, flex, havelock st, wan, premium, fibre, 15a,  uk, fletcher inc, fletcher, princeton street, fendalton road, bealey avenue)
doc = nlp3(text)
for ent in doc.ents:
print(ent, '|', ent.label_)
#this provides me a result where : wan, wan are classified as persons and fibre as an org
#now when i build my custom pattern using entity ruler
**nlp3 = spacy.load(""en_core_web_sm"")
ruler = nlp3.add_pipe(""entity_ruler"", before=""ner"")**
#list of entities and patterns
patterns = [{""label"": ""product"", ""pattern"": [{""lower"": ""wan""}, {""lower"": ""fibre""}, {""lower"": ""flex""},{""lower"": ""premium""},{""lower"": ""standard""},{""lower"": ""service""}]}]
ruler.add_patterns(patterns)
nlp3.pipe_names
even after this when i run i get wan classified as person (while i wish to see wan, wan, fibre classified as product). what am i doing wrong in adding patterns here. and is there a way i can add multiple patterns in a single dictionary to a label. any help in this regard is appreciated.","['pattern-matching', 'spacy', 'named-entity-recognition']",72781191,"each pattern you add to the ruler is one sequence of tokens. so you aren't matching each of those terms individually, you're matching all of them in a row, without punctuation. you should add them as separate patterns, something like this:
words = (""wan"", ""fibre"", ...)
patterns = []
for word in words:
    patterns.append({""label"":""product"", ""pattern"":[{""lower"":word}]})

couple of other things:

you may need to set overwrite_ents = true to get the results you want, see here.
if your actual input looks like ""wan, flex, havelock st, wan, premium, ..."", that's not the normal prose the spacy models were trained on, and they may not work very well.",https://stackoverflow.com/questions/72772448,pattern-matching,27-06-2022 12:54,1523.0,1.0,1.0,True,28-06-2022 05:34,27-06-2022 12:59
5479333,what are the available tools to summarize or simplify text?,"is there any library, preferably in python but at least open source, that can summarize and or simplify natural-language text?","python, nlp, text-processing",5479497,"i'm not sure if there is currently any libraries that do this, as text summarization, or at least understandable text summarization isn't something that will be easily accomplished by a simple plug & play library.
here are a few links that i managed to find regarding projects / resources that are related to text summarization to get you started:

the lemur project
python natural language toolkit
o'reilly's book on natural language processing in python
google resource on natural language processing
tutorial : how to create a keyword summary of text in python

hope that helps :)",https://stackoverflow.com/q/5479333,"python, nlp, text-processing",29-03-2011 21:46,31599.0,36.0,7.0,True,26-12-2022 21:07,26-12-2022 21:07
76952054,the requested module &#39;openai&#39; does not provide an export named &#39;configuration&#39; error,"i'm trying to build an ai image generating website using mern and i got this error:

the requested module 'openai' does not provide an export named
'configuration'.
file:///c:/users/rashmika%20satish/ai_website/server/routes/dalleroutes.js:3
import {configuration, openaiapi} from 'openai';
^^^^^^^^^^^^^ syntaxerror: the requested module 'openai' does not provide an export named 'configuration'
at modulejob._instantiate (node:internal/modules/esm/module_job:124:21)
at async modulejob.run (node:internal/modules/esm/module_job:190:5)
node.js v18.15.0 [nodemon] app crashed - waiting for file changes
before starting...

this is the dalleroutes.js page:
import express from 'express';
import * as dotenv from 'dotenv';
import {configuration, openaiapi} from 'openai';



dotenv.config();

const router = express.router();

this is the index.js page:
import express from 'express'
import *  as dotenv from 'dotenv';
import cors from 'cors';

import connectdb from './mongodb/connect.js';

import postroutes from './routes/postroutes.js';
import dalleroutes from './routes/dalleroutes.js';

dotenv.config();

const app = express();
app.use(cors());
app.use(express.json({limit: '50mb'}));

app.use('/api/v1/post', postroutes);
app.use('/api/v1/dalle', dalleroutes);

app.get('/', async(req, res)=>{
    res.send('hello from createai');
})

const startserver = async () =>{


    try{
        connectdb(process.env.mongodb_url);
        app.listen(8080, () => console.log('server has started on port 
    }catch(error){
         console.log(error);
    }
    

}
startserver();

this is the postroutes.js page
import express from 'express';
import * as dotenv from 'dotenv';
import {v2 as cloudinary} from 'cloudinary';

import post from '../mongodb/models/post.js';

dotenv.config();

const router = express.router();","['javascript', 'reactjs', 'express', 'mern', 'openai-api']",76962380,"i've got this same error. i'm assuming that you're following the jsm tutorial to create the app. after a lot of searching, i finally found a similar discussion on the openai forum 5 days back, and it seems like it's a version change - configuring the api key has been simplified in v4.
follow the forum here, if you're interested: 
here's the github guide on migrating from v3 to v4: 
in short, just run npm exec openai migrate and it should automatically migrate and change the code in your codebase to the latest version and should fix this version issue.",https://stackoverflow.com/questions/76952054,javascript,22-08-2023 09:47,14343.0,4.0,8.0,True,27-03-2024 05:46,22-08-2023 10:04
65370140,unable to import process_tweets from utils,"thanks for looking into this, i have a python program for which i need to have process_tweet and build_freqs for some nlp task, nltk is installed already and utils wasn't so i installed it via pip install utils but the above mentioned two modules apparently weren't installed, the error i got is standard one here,
importerror: cannot import name 'process_tweet' from
'utils' (c:\python\lib\site-packages\utils\__init__.py)

what have i done wrong or is there anything missing?
also i referred this stackoverflow answer but it didn't help.","['python', 'nlp', 'nltk', 'sentiment-analysis']",66375357,"you can easily access any source code with ??, for example in this case: process_tweet?? (the code above from deeplearning.ai nlp course custome utils library):
def process_tweet(tweet):
""""""process tweet function.
input:
    tweet: a string containing a tweet
output:
    tweets_clean: a list of words containing the processed tweet

""""""
stemmer = porterstemmer()
stopwords_english = stopwords.words('english')
# remove stock market tickers like $ge
tweet = re.sub(r'\$\w*', '', tweet)
# remove old style retweet text ""rt""
tweet = re.sub(r'^rt[\s]+', '', tweet)
# remove hyperlinks
tweet = re.sub(r' '', tweet)
# remove hashtags
# only removing the hash # sign from the word
tweet = re.sub(r'#', '', tweet)
# tokenize tweets
tokenizer = tweettokenizer(preserve_case=false, strip_handles=true,
                           reduce_len=true)
tweet_tokens = tokenizer.tokenize(tweet)

tweets_clean = []
for word in tweet_tokens:
    if (word not in stopwords_english and  # remove stopwords
            word not in string.punctuation):  # remove punctuation
        # tweets_clean.append(word)
        stem_word = stemmer.stem(word)  # stemming word
        tweets_clean.append(stem_word)",https://stackoverflow.com/questions/65370140,python,19-12-2020 13:06,6394.0,1.0,5.0,True,18-06-2023 12:28,19-12-2020 13:12
73777328,splitting string made out of dataframe row wise,"i'm trying to tokenize the words within dataframe which looks like
  a            b       c          d            e           f
0 orange     robot   x eyes   discomfort   striped tee    nan
1 orange     robot  blue beams   grin      vietnam jacket nan
2 aquamarine robot   3d          bored        cigarette   nan   
     

after removing all the special characters the dataframe became a string like this
df_str = df.to_string(header=false)
    
import re

normalised_text = bayc_features_str.lower()
text = re.sub(r""[^\a-za-z0-9 ]"","""", normalised_text)

print(text)

    1    orange   robot   x eyes   discomfort   striped tee   nan
    2    orange   robot   blue beams   grin   vietnam jacket  nan
    3    aquamarine  robot   3d       bored       cigarette    nan   

so when i tokenize this string, with below code
def tokenize(obj):
    if obj is none:
        return none
    elif isinstance(obj, str): 
        return word_tokenize(obj)
    elif isinstance(obj, list):
        return [tokenize(i) for i in obj
    else:
        return obj

tokenized_text = (tokenize(text))

i get the output
['orange', 'robot', 'x', 'eyes', 'discomfort', 'striped', 'tee', nan,'orange', 'robot', 'blue', 'beams', 'grin', 'vietnam', 'jacket', nan,'aquamarine', 'robot', '3d', 'bored', 'cigarette', nan, 'sea', 'captains', 'hat']

which is quite different from the output i expected
[['orange'], ['robot'], ['x', 'eyes'], ['discomfort'], ['striped', 'tee'], nan]
[['orange'], ['robot'], ['blue', 'beams'], ['grin'], ['vietnam', 'jacket'], nan]
[['aquamarine'], ['robot'], ['3d'], ['bored', 'cigarette'], nan, ['sea', 'captains', 'hat']]

any ideas on how can i get the output i expected?
any help would be greatly appreciated!","['python', 'string', 'list', 'nlp', 'tokenize']",73778975,"don't convert dataframe to string but work with every text in dataframe separatelly.
use.applymap(function) to execute function on every text (on every cell in dataframe).
new_df = df.applymap(tokenize)

result = new_df.values.tolist()


minimal working example:
import pandas as pd
from nltk.tokenize import word_tokenize

data = {
    'background': ['orange', 'orange', 'aqua'], 
    'fur': ['robot', 'robot', 'robot'], 
    'eyes': ['x eyes', 'blue beams', '3d'],
    'mouth': ['discomfort', 'grin', 'bored cigarette'],
    'clothes': ['striped tee', 'vietman jacket', none],
    'hat': [none, none, ""sea captain's hat""],
}

df = pd.dataframe(data)

print(df.to_string())  # `to_string()` to display full dataframe without `...`

# ----------------------------------------

def tokenize(obj):
    if obj is none:
        return none
    elif isinstance(obj, str): 
        return word_tokenize(obj)
    elif isinstance(obj, list):
        return [tokenize(i) for i in obj]
    else:
        return obj

new_df = df.applymap(tokenize)

result = new_df.values.tolist()

print(result)

result:
  background    fur        eyes            mouth         clothes                hat
0     orange  robot      x eyes       discomfort     striped tee               none
1     orange  robot  blue beams             grin  vietman jacket               none
2       aqua  robot          3d  bored cigarette            none  sea captain's hat

[
  [['orange'], ['robot'], ['x', 'eyes'], ['discomfort'], ['striped', 'tee'], none], 
  [['orange'], ['robot'], ['blue', 'beams'], ['grin'], ['vietman', 'jacket'], none], 
  [['aqua'], ['robot'], ['3d'], ['bored', 'cigarette'], none, ['sea', 'captain', ""'s"", 'hat']]
]",https://stackoverflow.com/questions/73777328,python,19-09-2022 17:41,88.0,-1.0,1.0,True,20-09-2022 18:01,20-09-2022 18:01
76982260,huggingface transformer evaluation process is too slow,"i used the huggingface transformers library to train a bert model for sequence classification.
the training process is good on gpu, but the evaluation process(which is running gpu) is too slow. for example, when i just have a sanity check for just 20 short text inputs, the evaluation runtime is about 160 seconds per step.
here's the snippet code:
def compute_metrics(eval_pred):
    accuracy_metric = evaluate.load(""accuracy"")
    f1_metric = evaluate.load(""f1"", average=""macro"")

    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)
    f1_score = f1_metric.compute(predictions=predictions, references=labels, average=""macro"")

    return {**accuracy, **f1_score}

model = automodelforsequenceclassification.from_pretrained(
        base_model_path,
        num_labels=num_labels,
        id2label=id2label,
        label2id=label2id
    )

    training_args = trainingarguments(
        output_dir=""."",
        learning_rate=lr,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=n_epoch,
        weight_decay=weight_decay,
        evaluation_strategy=""steps"",
        eval_steps=eval_steps,
        logging_strategy=""steps"",
        logging_steps=logging_steps,
        save_strategy=""steps"",
        save_steps=saving_steps,
        load_best_model_at_end=true,
        report_to=[""tensorboard""],
    )

    trainer = trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train_ds,
        eval_dataset=tokenized_valid_ds,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    trainer.train()

the properties of the environment:
transformers              4.29.2
python                    3.10.9

and the configuration of training is like the following:
len(train_data) ~= 36k
len(valid_data) ~= 2k
len(test_data) ~= 2k

model_name = 'bert-base-uncased'

per_device_train_batch_size=16
per_device_eval_batch_size=16
num_train_epochs=30


p.s.: the length of all data is small(less than ten tokens).
can anyone suggest a solution to reduce the time overhead of the evaluation process?","['machine-learning', 'huggingface-transformers', 'bert-language-model', 'training-data', 'huggingface']",77027142,"so i finally got the problem. it's related to evaluate.load() calls inside the compute_metrics function. it seems this method has a significant overhead in time, so it shouldn't be inside some functions e.g. compute_metrics which are called many times. i moved out two load() methods of compute_metrics function and it works quickly now.",https://stackoverflow.com/questions/76982260,machine-learning,26-08-2023 09:06,3150.0,1.0,1.0,True,30-09-2023 18:48,26-08-2023 12:59
28339622,is there a corpus of english words in nltk?,"is there any way to get the list of english words in python nltk library?
i tried to find it but the only thing i have found is wordnet from nltk.corpus. but based on documentation, it does not have what i need (it finds synonyms for a word).
i know how to find the list of this words by myself (this answer covers it in details), so i am interested whether i can do this by only using nltk library.",['nltk'],28339791,"yes, from nltk.corpus import words
and check using:
>>> ""fine"" in words.words()
true

reference: section 4.1 (wordlist corpora), chapter 2 of natural language processing with python.",https://stackoverflow.com/questions/28339622,nltk,05-02-2015 08:48,67056.0,39.0,2.0,True,09-01-2023 15:59,09-01-2023 15:59
73946165,how to avoid double-extraction of patterns in spacy?,"i'm using an incident database to identify the causes of accidents. i have defined a pattern and a function to extract the matching patterns. however, sometimes this function creates overlapping results. i saw in a previous post that we can use for span in spacy.util.filter_spans(spans):
to avoid repetition of answers. but i don't know how to rewrite the function with this. i will be grateful for any help you can provide.
pattern111 = [{'dep':'compound','op':'?'},{'dep':'nsubj'}]
def get_relation111(x):
    doc = nlp(x)
    matcher = matcher(nlp.vocab)
    relation= []

    matcher.add(""matching_111"", [pattern111], on_match=none)

    matches = matcher(doc)
  
    for match_id, start, end in matches:
        matched_span = doc[start: end]
        relation.append(matched_span.text)
    return relation","['dataframe', 'function', 'spacy', 'repeat', 'matcher']",73947115,"filter_spans can be used on any list of spans. this is a little weird because you want a list of strings, but you can work around it by saving a list of spans first and only converting to strings after you've filtered.
def get_relation111(x):
    doc = nlp(x)
    matcher = matcher(nlp.vocab)
    relation= []

    matcher.add(""matching_111"", [pattern111], on_match=none)

    matches = matcher(doc)
  
    for match_id, start, end in matches:
        matched_span = doc[start: end]
        relation.append(matched_span)
    # xxx just add this line
    relation = [ss.text for ss in filter_spans(relation)]
    return relation",https://stackoverflow.com/questions/73946165,dataframe,04-10-2022 09:57,101.0,1.0,1.0,True,04-10-2022 11:21,04-10-2022 10:16
73875337,nn.lstm doesn&#39;t seem to learn anything or not updating properly,"i was trying out a simple lstm use case form pytorch, with the following model.
class simplelstm(nn.module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super().__init__()
        
        self.embedding = nn.embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.lstm(batch_first=true, input_size=embedding_dim, num_layers=1, hidden_size=hidden_dim, bidirectional=true)
        self.linear = nn.linear(hidden_dim*2, 1)
        self.sigmoid = nn.sigmoid()
        
    def forward(self, x):   # nxd, padded to same length with 0s in n-sized batch
        x = self.embedding(x)
        output, (final_hidden_state, final_cell_state) = self.lstm(x)
        x = self.linear(output[:,-1,:])
        x=self.sigmoid(x)
        return x

it is a binary classification, with bceloss (combined with the sigmoid output layer). unfortunately, loss is stuck at 0.6969 (i.e. it is not learning anything).
i've tried using final_hidden_state, output[:,0,:] feeding into the linear layer, but so far no dice.
everything else (optimizer, loss criterion, train loop, val loop) already works because i tried the exact same setup with a basic nn using nn.embedding, nn.linear, and nn.sigmoid only, and could get to good loss decrease and high accuracy. in the simplelstm, the only thing i added is the nn.lstm.","['python', 'machine-learning', 'pytorch', 'nlp', 'lstm']",73888500,"typically final_hidden_state is passed to linear, not output. use it.
add 1-2 more linear layers after the lstm.
try lower lr, especially when embeddings are not pre-trained.
better yet, try loading pre-trained embeddings.",https://stackoverflow.com/questions/73875337,python,28-09-2022 01:38,545.0,1.0,1.0,True,28-09-2022 22:36,28-09-2022 22:34
74173869,bert transformer model gives an error for multiclass classification,"i am trying to train a sentiment analysis model with 5 classes (1-very negative, 2-negative, 3-neutral, 4-positive, 5-very positive) with the bert model.
from transformers import berttokenizer, tfbertforsequenceclassification
from transformers import inputexample, inputfeatures
        
model = tfbertforsequenceclassification.from_pretrained(""bert-base-cased"")
tokenizer = berttokenizer.from_pretrained(""bert-base-cased"")
        
model.compile(optimizer=tf.keras.optimizers.adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), 
              loss=tf.keras.losses.sparsecategoricalcrossentropy(from_logits=true), 
              metrics=[tf.keras.metrics.sparsecategoricalaccuracy('accuracy')])
    
model.fit(train_data, epochs=2, validation_data=validation_data)

but i get the following error (just the last part of the error message)
node: 'sparse_categorical_crossentropy/sparsesoftmaxcrossentropywithlogits/sparsesoftmaxcrossentropywithlogits'
received a label value of 5 which is outside the valid range of [0, 2).  label values: 3 4 5 2 2 4 4 3 4 5 5 4 5 5 4 4 4 3 4 4 5 5 5 4 4 5 3 5 4 4 3 5
         [[{{node sparse_categorical_crossentropy/sparsesoftmaxcrossentropywithlogits/sparsesoftmaxcrossentropywithlogits}}]] [op:__inference_train_function_31614]

can somebody tell me what i am doing wrong here?","['bert-language-model', 'softmax', 'cross-entropy']",74178380,"the tfbertforsequenceclassification object needs to create a so-called classification head. the classification head is a cool name for a single nn layer that projects the [cls] token representation into a vector with one item for each possible target class.
when you initialize the model by calling from_pretrained, you can specify num_labels, which is a number of target labels (see an example in transformers documentation). if you do not specify it, the number of target classes will be inferred from the first training batch by taking the maximum class id in the batch. if you are not lucky and the first batch only contains lower label ids, it initializes a smaller classification head and fails when a batch with higher ids comes.
note also, that the class numbers start from zero. if you use labels 1-5, the model will have an additional 0th class that will not be used. if you want to keep the numbers 1-5, your num_labels will be 6.",https://stackoverflow.com/questions/74173869,bert-language-model,23-10-2022 18:47,169.0,0.0,1.0,True,24-10-2022 08:26,24-10-2022 08:17
44827930,evaluation in a spacy ner model,"i am trying to evaluate a trained ner model created using spacy lib.
 normally for these kind of problems you can use f1 score (a ratio between precision and recall). i could not find in the documentation an accuracy function for a trained ner model. 
i am not sure if it's correct but i am trying to do it with the following way(example) and using f1_score from sklearn:
from sklearn.metrics import f1_score
import spacy
from spacy.gold import goldparse


nlp = spacy.load(""en"") #load ner model
test_text = ""my name is john"" # text to test accuracy
doc_to_test = nlp(test_text) # transform the text to spacy doc format

# we create a golden doc where we know the tagged entity for the text to be tested
doc_gold_text= nlp.make_doc(test_text)
entity_offsets_of_gold_text = [(11, 15,""person"")]
gold = goldparse(doc_gold_text, entities=entity_offsets_of_gold_text)

# bring the data in a format acceptable for sklearn f1 function
y_true = [""person"" if ""person"" in x else 'o' for x in gold.ner]
y_predicted = [x.ent_type_ if x.ent_type_ !='' else 'o' for x in doc_to_test]
f1_score(y_true, y_predicted, average='macro')`[1]
> 1.0

any thoughts are or insights are useful.","['python', 'spacy']",44841535,"you can find different metrics including f-score, recall and precision in spacy/scorer.py.
this example shows how you can use it:
import spacy
from spacy.gold import goldparse
from spacy.scorer import scorer

def evaluate(ner_model, examples):
    scorer = scorer()
    for input_, annot in examples:
        doc_gold_text = ner_model.make_doc(input_)
        gold = goldparse(doc_gold_text, entities=annot)
        pred_value = ner_model(input_)
        scorer.score(pred_value, gold)
    return scorer.scores

# example run

examples = [
    ('who is shaka khan?',
     [(7, 17, 'person')]),
    ('i like london and berlin.',
     [(7, 13, 'loc'), (18, 24, 'loc')])
]

ner_model = spacy.load(ner_model_path) # for spacy's pretrained use 'en_core_web_sm'
results = evaluate(ner_model, examples)

the scorer.scores returns multiple scores. when running the example, the result looks like this: (note the low scores occuring because the examples classify london and berlin as 'loc' while the model classifies them as 'gpe'. you can figure this out by looking at the ents_per_type.)
{'uas': 0.0, 'las': 0.0, 'las_per_type': {'attr': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'root': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'compound': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'nsubj': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'dobj': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'cc': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'conj': {'p': 0.0, 'r': 0.0, 'f': 0.0}}, 'ents_p': 33.33333333333333, 'ents_r': 33.33333333333333, 'ents_f': 33.33333333333333, 'ents_per_type': {'person': {'p': 100.0, 'r': 100.0, 'f': 100.0}, 'loc': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'gpe': {'p': 0.0, 'r': 0.0, 'f': 0.0}}, 'tags_acc': 0.0, 'token_acc': 100.0, 'textcat_score': 0.0, 'textcats_per_cat': {}}

the example is taken from a spacy example on github (link does not work anymore). it was last tested with spacy 2.2.4.",https://stackoverflow.com/questions/44827930,python,29-06-2017 14:27,26383.0,29.0,5.0,True,20-01-2023 00:49,19-03-2018 15:20
58384286,calculate tf-idf using sklearn for variable-n-grams in python,"problem:
using scikit-learn to find the number of hits of variable n-grams of a particular vocabulary.
explanation.
i got examples from here.
imagine i have a corpus and i want to find how many hits (counting) has a vocabulary like the following one:
myvocabulary = [(window=4, words=['tin', 'tan']),
                (window=3, words=['electrical', 'car'])
                (window=3, words=['elephant','banana'])

what i call here window is the length of the span of words in which the words can appear. as follows:
'tin tan' is hit (within 4 words)
'tin dog tan' is hit (within 4 words)
'tin dog cat tan is hit (within 4 words)
'tin car sun eclipse tan' is not hit. tin and tan appear more than 4 words away from each other.
i just want to count how many times (window=4, words=['tin', 'tan']) appears in a text and the same for all the other ones and then add the result to a pandas in order to calculate a tf-idf algorithm.
i could only find something like this:
from sklearn.feature_extraction.text import tfidfvectorizer
tfidf = tfidfvectorizer(vocabulary = myvocabulary, stop_words = 'english')
tfs = tfidf.fit_transform(corpus.values())

where vocabulary is a simple list of strings, being single words or several words.
besides from scikitlearn:
class sklearn.feature_extraction.text.countvectorizer
ngram_range : tuple (min_n, max_n)

the lower and upper boundary of the range of n-values for different n-grams to be extracted. all values of n such that min_n <= n <= max_n will be used.
does not help neither.
any ideas?","['python', 'text', 'scikit-learn', 'tf-idf', 'n-gram']",58439141,"i am not sure if this can be done using countvectorizer or tfidfvectorizer. i have written my own function for doing this as follows:
import pandas as pd
import numpy as np
import string 

def contained_within_window(token, word1, word2, threshold):
  word1 = word1.lower()
  word2 = word2.lower()
  token = token.translate(str.maketrans('', '', string.punctuation)).lower()
  if (word1 in token) and word2 in (token):
      word_list = token.split("" "")
      word1_index = [i for i, x in enumerate(word_list) if x == word1]
      word2_index = [i for i, x in enumerate(word_list) if x == word2]
      count = 0
      for i in word1_index:
        for j in word2_index:
          if np.abs(i-j) <= threshold:
            count=count+1
      return count
  return 0


sample:

corpus = [
    'this is the first document. and this is what i want',
    'this document is the second document.',
    'and this is the third one.',
    'is this the first document?',
    'i like coding in sklearn',
    'this is a very good question'
]

df = pd.dataframe(corpus, columns=[""test""])

your df will look like this:
    test
0   this is the first document. and this is what i...
1   this document is the second document.
2   and this is the third one.
3   is this the first document?
4   i like coding in sklearn
5   this is a very good question

now you can apply contained_within_window as follows:
sum(df.test.apply(lambda x: contained_within_window(x,word1=""this"", word2=""document"",threshold=2)))

and you get:
2

you can just run a for loop for checking different instances.
and you this to construct your pandas df  and apply tfidf on it, which is straight forward.",https://stackoverflow.com/questions/58384286,python,14-10-2019 21:29,667.0,2.0,1.0,True,23-11-2022 13:06,23-11-2022 13:06
76045605,using a custom trained huggingface tokenizer,"iï¿½ï¿½ï¿½ve trained a custom tokenizer using a custom dataset using this code thatï¿½ï¿½ï¿½s on the documentation. is there a method for me to add this tokenizer to the hub and to use it as the other tokenizers by calling the autotokenizer.from_pretrained() function? if i canï¿½ï¿½ï¿½t do that how can i use the tokenizer to train a custom model from scratch?
thanks for your help!!!
here's the code below:
from tokenizers import tokenizer
from tokenizers.models import bpe
tokenizer = tokenizer(bpe(unk_token=""[unk]""))

from tokenizers.trainers import bpetrainer
trainer = bpetrainer(special_tokens=[""[unk]"", ""[cls]"", ""[sep]"", ""[pad]"", ""[mask]""])

from tokenizers.pre_tokenizers import whitespace
tokenizer.pre_tokenizer = whitespace()

folder = 'dataset_unicode'
files = [f""/content/drive/mydrive/{folder}/{split}.txt"" for split in [""test"", ""train"", ""valid""]]
tokenizer.train(files, trainer)

from tokenizers.processors import templateprocessing
tokenizer.post_procesteprocessing(
    single=""[cls] $a [sep]"",
    pair=""[cls] $a [sep] $b:1 [sep]:1"",
    special_tokens=[
        (""[cls]"", tokenizer.token_to_id(""[cls]"")),
        (""[sep]"", tokenizer.token_to_id(""[sep]"")),
    ],
)

# i've tried saving it like this but it doesn't work as i expect it:
tokenizer.save(""data/tokenizer-custom.json"")","['python', 'huggingface-transformers', 'huggingface-tokenizers', 'huggingface', 'huggingface-hub']",76058017,"the autotokenizer expects a few files in the directory:
awesometokenizer/
    tokenizer_config.json
    special_tokens_map.json
    tokenizer.json

but the default tokenizer.tokenizer.save() function only saves the vocab file in awesometokenizer/tokenizer.json, open up the json file and compare the ['model']['vocab'] keys to your json from data/tokenizer-custom.json.
the simplest way to let autotokenizer load .from_pretrained is to follow the answer that @cronoik posted in the comment, using pretrainedtokenizerfast, i.e. adding a few lines to your existing code:
from tokenizers import tokenizer
from tokenizers.models import bpe
from tokenizers.trainers import bpetrainer
from tokenizers.pre_tokenizers import whitespace
from tokenizers.processors import templateprocessing

from transformers import pretrainedtokenizerfast  # <---- add this line.



trainer = bpetrainer(special_tokens=[""[unk]"", ""[cls]"", ""[sep]"", ""[pad]"", ""[mask]""])

tokenizer = tokenizer(bpe(unk_token=""[unk]""))
tokenizer.pre_tokenizer = whitespace()

files = [""big.txt""]  # e.g. training with 
tokenizer.train(files, trainer)

tokenizer.post_processor = templateprocessing(
    single=""[cls] $a [sep]"",
    pair=""[cls] $a [sep] $b:1 [sep]:1"",
    special_tokens=[
        (""[cls]"", tokenizer.token_to_id(""[cls]"")),
        (""[sep]"", tokenizer.token_to_id(""[sep]"")),
    ],
)

# add these lines:
#     |
#     |
#     v
awesome_tokenizer = pretrainedtokenizerfast(tokenizer_object=tokenizer)
awesome_tokenizer.save_pretrained(""awesome_tokenizer"")

then you can load the trained tokenizer:
from transformers import autotokenizer

auto_loaded_tokenizer = autotokenizer.from_pretrained(
    ""awesome_tokenizer"", 
    local_files_only=true
)


note: tokenizers though can be pip installed, is a library in rust with python bindings",https://stackoverflow.com/questions/76045605,python,18-04-2023 14:05,2358.0,5.0,2.0,True,29-02-2024 08:33,19-04-2023 19:03
71726244,is possible to get dependency/pos information for entities in spacy?,"i am working on extracting entities from scientific text (i am using scispacy) and later i will want to extract relations using hand-written rules. i have extracted entities and their character span successfully, and i can also get the pos and dependency tags for tokens and noun chunks. so i am comfortable with the two tasks separately, but i want to bring the two together and i have been stuck for a while.
the idea is that i want to be able to write rules such as: (just an example) if in a sentence/clause there are two entities where the first one is a 'drug/chemical' + is the subject, and the second one is a 'disease' + is an object --> (then) infer 'treatment' relation between the two.
if anyone has any hints on how to approach this task, i would really appreciate it. thank you!
s.
what i am doing to extract entities:
doc = nlp(text-with-more-than-one-sent)
for ent in doc.ents:
`... (get information about the ent e.g. its character span)`

getting dependency information (for noun chunks and for tokens):
for chunk in doc.noun_chunks:
    print(f""text: {chunk.text}, root text: {chunk.root.text}, root dep: {chunk.root.dep_}, root head text: {chunk.root.head.text}, pos: {chunk.root.head.pos_}"")
_
for token in doc:
    print(f""text: {token.text}, dep label: {token.dep_}, head text: {token.head.text}, head pos: {token.head.pos_}, children: {[child for child in token.children]}"")","['nlp', 'spacy', 'named-entity-recognition', 'dependency-parsing']",71732367,"you can use the merge_entities mini-component to convert entities to single tokens, which would simplify what you're trying to do. there's also a component to merge noun chunks similarly.",https://stackoverflow.com/questions/71726244,nlp,03-04-2022 13:11,771.0,1.0,1.0,True,04-04-2022 05:21,03-04-2022 13:24
61133531,how does spacy generate vectors for phrases?,"medium and large vocabularies of spacy can generate vectors for words and phrases.  let's consider the following example:
import spacy
    
nlp = spacy.load(""en_core_web_md"")
tokens = nlp(""apple cat sky"")
    
print(tokens.text, tokens.vector[:3], tokens.vector_norm) # only the first three components of the vector 
    
for token in tokens:
    print(token.text, token.vector[:3], token.vector_norm)

output:
apple cat sky [-0.06734333  0.03672066 -0.13952099] 4.845729844425328
apple [-0.36391  0.43771 -0.20447] 7.1346846
cat [-0.15067  -0.024468 -0.23368 ] 6.6808186
sky [ 0.31255  -0.30308   0.019587] 6.617719

it is clear that the vocabulary contains vectors for each word, but how are the vectors for the entire phase generated? as one can see it is not just simple sum of vectors.","['nlp', 'spacy', 'word2vec']",61135858,"by default, the vector of a doc is the average of the vectors of the tokens, cf 

models that come with built-in word vectors make them available as the token.vector attribute. doc.vector and span.vector will default to an average of their token vectors.",https://stackoverflow.com/questions/61133531,nlp,10-04-2020 03:19,6917.0,5.0,2.0,True,24-04-2023 06:37,24-04-2023 06:37
45495190,initializing out of vocabulary (oov) tokens,"i am building tensorflow model for nlp task, and i am using pretrained glove 300d word-vector/embedding dataset.
obviously some tokens can't be resolved as embeddings, because were not included into training dataset for word vector embedding model, e.g. rare names.
i can replace those tokens with vectors of 0s, but rather than dropping this information on the floor, i prefer to encode it somehow and include to my training data.
say, i have 'raijin' word, which can't be resolved as embedding vector, what would be the best way to encode it consistently with glove embedding dataset? what is the best approach to convert it to 300d vector?
thank you.","['tensorflow', 'embedding', 'word-embedding', 'oov']",45496869,"instead of assigning all the out of vocabulary tokens to a common unk vector (zeros), it is better to assign them a unique random vector. at-least this way when you find the similarity between them with any other word, each of them will be unique and the model can learn something out of it. in the unk case, they will all be same and so all the unk words will be treated as having the same context.
i tried this approach and got a 3% accuracy improvement on the quora duplicate question pair detection dataset using an lstm model.",https://stackoverflow.com/questions/45495190,tensorflow,03-08-2017 21:58,8789.0,3.0,2.0,True,05-03-2024 09:33,05-03-2024 09:33
74123446,tenserflow issue when tokenizing sentences,"i followed a tutorial about tokenizing sentences using tensorflow, here's the code i'm trying:
from tensorflow.keras.preprocessing.text import tokenizer #api for tokenization

t = tokenizer(num_words=4) #meant to catch most imp _
listofsentences=['apples are fruits', 'an orange is a tasty fruit', 'fruits are tasty!']
t.fit_on_texts(listofsentences) #processes words

print(t.word_index)
print(t.texts_to_sequences(listofsentences)) #arranges tokens, returns nested list

the first print statement shows a dictionary as expected:
{'are': 1, 'fruits': 2, 'tasty': 3, 'apples': 4, 'an': 5, 'orange': 6, 'is': 7, 'a': 8, 'fruit': 9}

but the last line outputs a list that misses many words:
[[1, 2], [3], [2, 1, 3]]

please let me know what i'm doing wrong and how to get the expected list:
[[4,1,2],[5,6,7,8,3,9],[2,1,3]]","['python', 'tensorflow', 'nlp', 'tokenize']",74125318,"to specify an unlimited amount of tokens use:
t = tokenizer(num_words=none)

output:
{'are': 1, 'fruits': 2, 'tasty': 3, 'apples': 4, 'an': 5, 'orange': 6, 'is': 7, 'a': 8, 'fruit': 9}
[[4, 1, 2], [5, 6, 7, 8, 3, 9], [2, 1, 3]]",https://stackoverflow.com/questions/74123446,python,19-10-2022 10:00,42.0,1.0,1.0,True,19-10-2022 12:27,19-10-2022 10:14
77068488,how to efficiently convert a markdown table to a dataframe in python?,"i need to convert a markdown table into a pandas dataframe. i've managed to do this using the pd.read_csv function with '|' as the separator, but it seems like there's some additional cleanup required. specifically, i need to remove the row containing '-----', which is used for table separation, and i also want to get rid of the last column.
here's a simplified example of what i'm doing:
import pandas as pd
from io import stringio

# the text containing the table
text = """"""
| some title | some description             | some number |
|------------|------------------------------|-------------|
| dark souls | this is a fun game           | 5           |
| bloodborne | this one is even better      | 2           |
| sekiro     | this one is also pretty good | 110101      |
""""""

# use stringio to create a file-like object from the text
text_file = stringio(text)

# read the table using pandas read_csv with '|' as the separator
df = pd.read_csv(text_file, sep='|', skipinitialspace=true)

# remove leading/trailing whitespace from column names
df.columns = df.columns.str.strip()

# remove the index column
df = df.iloc[:, 1:]

is there a more elegant and efficient way to convert a markdown table into a dataframe without needing to perform these additional cleanup steps? i'd appreciate any suggestions or insights on improving this process.","['python', 'nlp', 'markdown']",77068592,"like this
import re
import pandas as pd

text = """"""
| some title | some description             | some number |
|------------|------------------------------|-------------|  
| dark souls | this is a fun game           | 5           |
| bloodborne | this one is even better      | 2           |
| sekiro     | this one is also pretty good | 110101      |
""""""

pattern = r""\| ([\w\s]+) \| ([\w\s]+) \| ([\w\s]+) \|""

# use the findall function to extract all rows that match the pattern
matches = re.findall(pattern, text)

# extract the header and data rows
header = matches[0]
data = matches[1:]

# create a pandas dataframe using the extracted header and data rows
df = pd.dataframe(data, columns=header)

# optionally, convert numerical columns to appropriate types
df['some number'] = df['some number'].astype(int)

print(df)",https://stackoverflow.com/questions/77068488,python,08-09-2023 16:21,6123.0,9.0,2.0,True,09-09-2023 08:08,09-09-2023 08:08
73571053,how to know the topic from trained data (or predict the topic of new data) using trained topic modelling using octis?,"i've trained an lda for topic modelling using octis. but i don't know how to see the predicted topic for each data input or how to apply/predict my trained model to new data.
this is the code and the output of the trained model:
input:
# custom dataset
from octis.dataset.dataset import dataset
dataset = dataset()
dataset.load_custom_dataset_from_folder(""/content/new"")

# create model
model = lda(num_topics=5, alpha=0.1)

# train the model 
output = model.train_model(dataset)

output:
>> output
{'topic-word-matrix': array([[0.00030817, 0.00646953, 0.00338882, ..., 0.00030812, 0.00030813,
         0.00030812],
        [0.00041419, 0.00248425, 0.0004141 , ..., 0.0004141 , 0.0004141 ,
         0.0004141 ],
        [0.0002584 , 0.00025837, 0.00025836, ..., 0.00025836, 0.00025836,
         0.00025836],
        [0.00044957, 0.0004495 , 0.00044949, ..., 0.00269659, 0.00269643,
         0.00269655],
        [0.00238244, 0.0003972 , 0.00039719, ..., 0.00039719, 0.00039719,
         0.00039719]], dtype=float32),
 'topics': [['vaksin',
   'sertifikat',
   'aplikasi',
   'pertama',
   'alhamdulillah',
   'pedulilindungi',
   'ada',
   'padahal',
   'terimakasih',
   'nik'],
  ['aplikasi',
   'vaksin',
   'sertifikat',
   'guna',
   'data',
   'udh',
   'bikin',
   'pake',
   'login',
   'nik'],
  ['aplikasi',
   'vaksin',
   'sertifikat',
   'ada',
   'tanggal',
   'tgl',
   'di',
   'belum',
   'coba',
   'sangat'],
  ['covid',
   'vaksin',
   'jadi',
   'hp',
   'ada',
   'aplikasi',
   'mau',
   'kalau',
   'sakit',
   'salah'],
  ['aplikasi',
   'bahasa',
   'lahir',
   'bisa',
   'nx',
   'sertifikat',
   'pakai',
   'di',
   'vaksin',
   'indonesia']],
 'topic-document-matrix': array([[0.06667246, 0.98769081, 0.00190489, 0.00165357, 0.98805857,
         0.00392223, 0.00210558, 0.00219824, 0.0029861 , 0.00170955,
         0.00215115, 0.00160036, 0.00210585, 0.00210572, 0.0030779 ,
         0.00289892, 0.00289916, 0.00307764, 0.00317504, 0.00307748,
         0.00183547, 0.10921329, 0.00160071, 0.98933005, 0.00219851,
         0.49730667, 0.98768848, 0.00194217, 0.00194207, 0.99120653,
         0.00160038, 0.00363727, 0.23678468, 0.98545253, 0.00168113,
         0.0016811 , 0.99349433, 0.00229977, 0.00339057, 0.98769081,
         0.00190489, 0.00165355, 0.98805857, 0.00392223, 0.00210558,
         0.00219824, 0.00298613, 0.00170955, 0.00215115, 0.00160037],
        [0.06667251, 0.00307732, 0.00190493, 0.99338686, 0.00298535,
         0.00392244, 0.00210581, 0.00219835, 0.00298549, 0.00170954,
         0.00215096, 0.00160031, 0.00210574, 0.00210569, 0.00307775,
         0.00289909, 0.0028996 , 0.00307763, 0.0031751 , 0.98769003,
         0.00183529, 0.88495934, 0.00160057, 0.00266775, 0.0021984 ,
         0.00224775, 0.00307769, 0.99223095, 0.99223143, 0.00219834,
         0.64113957, 0.98545176, 0.00219818, 0.00363676, 0.00168091,
         0.00168108, 0.00162644, 0.00229943, 0.00339064, 0.00307732,
         0.00190493, 0.99338692, 0.00298535, 0.00392238, 0.00210581,
         0.00219834, 0.00298549, 0.00170954, 0.00215096, 0.00160031],
        [0.06667244, 0.00307741, 0.00190496, 0.0016533 , 0.00298539,
         0.98431122, 0.99157733, 0.99120724, 0.9880569 , 0.00170964,
         0.00215104, 0.99359852, 0.002106  , 0.99157727, 0.00307766,
         0.00289921, 0.9884032 , 0.0681117 , 0.98729986, 0.00307752,
         0.00183526, 0.0019427 , 0.0016006 , 0.00266733, 0.06295873,
         0.49595022, 0.00307819, 0.00194241, 0.00194218, 0.00219866,
         0.35405946, 0.00363706, 0.75662065, 0.00363694, 0.00168095,
         0.99327528, 0.00162655, 0.00229944, 0.00339072, 0.00307741,
         0.00190496, 0.0016533 , 0.00298541, 0.98431122, 0.99157733,
         0.99120724, 0.9880569 , 0.00170964, 0.00215105, 0.99359864],
        [0.06667253, 0.00307718, 0.00190488, 0.00165313, 0.00298533,
         0.00392208, 0.00210552, 0.00219805, 0.00298561, 0.99316174,
         0.00215088, 0.00160023, 0.00210566, 0.00210558, 0.98768842,
         0.9884038 , 0.00289898, 0.00307778, 0.00317501, 0.00307729,
         0.0018353 , 0.00194239, 0.99359751, 0.00266747, 0.93044597,
         0.00224771, 0.0030776 , 0.00194221, 0.00194201, 0.00219822,
         0.00160026, 0.0036369 , 0.00219826, 0.00363692, 0.99327594,
         0.00168124, 0.00162628, 0.00229925, 0.98643756, 0.00307718,
         0.00190488, 0.00165313, 0.00298533, 0.00392208, 0.00210552,
         0.00219805, 0.00298561, 0.99316174, 0.00215088, 0.00160023],
        [0.73331004, 0.00307727, 0.99238032, 0.00165311, 0.00298531,
         0.00392205, 0.00210574, 0.00219817, 0.00298594, 0.00170954,
         0.99139601, 0.00160052, 0.99157673, 0.00210574, 0.00307827,
         0.00289894, 0.00289911, 0.92265522, 0.00317498, 0.00307761,
         0.99265867, 0.00194224, 0.00160059, 0.00266738, 0.00219837,
         0.00224767, 0.00307806, 0.00194227, 0.0019423 , 0.00219831,
         0.00160033, 0.00363697, 0.00219824, 0.00363684, 0.00168108,
         0.00168125, 0.00162644, 0.99080211, 0.00339047, 0.00307727,
         0.99238032, 0.00165311, 0.00298531, 0.00392205, 0.00210574,
         0.00219817, 0.00298594, 0.00170954, 0.99139601, 0.00160043]]),
 'test-topic-document-matrix': array([], dtype=float64)}

my goal is to at least know the topic for each input data (it'll be great if i can predict new data by the trained model too!)

i used trial data, so the result is still not that great, but my main objective is to understand how to do topic modelling with octis
num topic = 5 (if this helps)
octis framework is new, it was published last year! (2021)ï¿½ï¿½
</","['python', 'machine-learning', 'nlp', 'lda', 'topic-modeling']",76714445,"topics in the training data
the topics that the model has found are represented by the top 10 words in that topic.
these can be found in output['topics'].
so your first topic would be represented by the words: ['vaksin','sertifikat','aplikasi','pertama','alhamdulillah','pedulilindungi','ada','padahal','terimakasih','nik'].
to know which topics are found in which document, you should look at output['topic-document-matrix'].
the first list in this list represents the distribution of topics in the first document of your training data. example: the first document mostly consists of topic 2 (because of the value 0.98769081)
prediction on new documents
unfortunately, this is not possible using octis. octis is exclusively a package for optimizing and comparing topic models. it is possible to define a test set, to see how models perform on unseen data. however, octis is not suitable for developing production topic models. if that is your goal, take a look at gensim. (this is the package that octis uses behind the scenes.)",https://stackoverflow.com/questions/73571053,python,01-09-2022 14:27,314.0,0.0,1.0,True,18-07-2023 15:35,02-09-2022 13:16
73569160,valueerror(&#39;[e999] unable to merge the doc objects because they do not all share the same `vocab`.&#39;),"i want to combine the results of two different spacy.languages but will receive the following error:

valueerror('[e999] unable to merge the doc objects because they do not
all share the same vocab.')

example code:
import spacy
from spacy.tokens import doc

nlp_1 = spacy.blank(""en"")
ruler = nlp_1.add_pipe(""entity_ruler"")
ruler.add_patterns([{""label"": ""org"", ""pattern"": ""apple""}, ])
doc_1 = nlp_1('apple')

nlp_2 = spacy.blank(""en"")
ruler = nlp_2.add_pipe(""entity_ruler"")
ruler.add_patterns([{""label"": ""per"", ""pattern"": ""peter""}, ])
doc_2 = nlp_2('peter')

print(doc.from_docs([doc_1, doc_2]))
# valueerror: [e999] unable to merge the doc objects because they do not all share the same `vocab`.

question:
how do i fix this, e.g. share the vocabs between both nlp objects?
why would i want that?
lets say i want to analyse a mail. it is one document, but likelyhood that a number in the adress field is postal code is much higher than in the footer where it is probably a phone number. therefore depending on the field i want to apply different ""languages"" but which share the same vocab and then combine them into one doc for the mail.","['python', 'spacy', 'spacy-3']",73569856,"provide the existing vocab from the first model for any subsequent models when they're loaded:
nlp_2 = spacy.blank(""en"", vocab=nlp_1.vocab)",https://stackoverflow.com/questions/73569160,python,01-09-2022 12:12,87.0,1.0,1.0,True,02-09-2022 14:13,02-09-2022 14:13
72979157,how can i test my openai fine-tuned model against question answering benchmarks?,"i think the documentation only explains how to use the model through an api but that does not allow much flexibility nor automation. for example, i do not know how to test my model against some popular benchmarks from huggingface.",['openai-api'],73083718,"the general flow of fine tuning open ai models consists of creating an account, having a valid api key and then uploading the data for fine tuning using the cli tool, as described here: 
then to test against question answering benchmarks, like squad you simply dowload the dataset, create a script that takes the questions (see below json snippet) and feeds to your model by calling the api as described here (using curl): 
""question"": ""what century did the normans first gain their separate identity?"",
""id"": ""56ddde6b9a695914005b962c"",
""answers"": [
    {
        ""text"": ""10th century"",
        ""answer_start"": 671
    },
    {
        ""text"": ""the first half of the 10th century"",
        ""answer_start"": 649
    },
    {
        ""text"": ""10th"",
        ""answer_start"": 671
    },
    {
        ""text"": ""10th"",
        ""answer_start"": 671
    }
],
""is_impossible"": false",https://stackoverflow.com/questions/72979157,openai-api,14-07-2022 10:38,742.0,1.0,1.0,True,15-02-2023 12:23,15-02-2023 12:23
29788047,keep tfidf result for predicting new content,"i am using sklearn on python to do some clustering. i've trained 200,000 data, and code below works well.
corpus = open(""token_from_xml.txt"")
vectorizer = countvectorizer(decode_error=""replace"")
transformer = tfidftransformer()
tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))
km = kmeans(30)
kmresult = km.fit(tfidf).predict(tfidf)

but when i have new testing content, i'd like to cluster it to existed clusters i'd trained. so i'm wondering how to save idf result, so that i can do tfidf for the new testing content and make sure the result for new testing content have same array length.
thanks in advance.
update
i may need to save ""transformer"" or ""tfidf"" variable to file(txt or others), if one of them contains the trained idf result.
update
for example. i have the training data:
[""a"", ""b"", ""c""]
[""a"", ""b"", ""d""]

and do tfidf, the result will contains 4 features(a,b,c,d)
when i test:
[""a"", ""c"", ""d""]

to see which cluster(already made by k-means) it belongs to. tfidf will only give the result with 3 features(a,c,d), so the clustering in k-means will fall. (if i test [""a"", ""b"", ""e""], there may have other problems.)
so how to store the features list for testing data (even more, store it in file)?","['python', 'machine-learning', 'scikit-learn', 'tf-idf']",29793628,"i successfully saved the feature list by saving vectorizer.vocabulary_, and reuse by countvectorizer(decode_error=""replace"",vocabulary=vectorizer.vocabulary_)
codes below:
corpus = np.array([""aaa bbb ccc"", ""aaa bbb ddd""])
vectorizer = countvectorizer(decode_error=""replace"")
vec_train = vectorizer.fit_transform(corpus)
#save vectorizer.vocabulary_
pickle.dump(vectorizer.vocabulary_,open(""feature.pkl"",""wb""))

#load it later
transformer = tfidftransformer()
loaded_vec = countvectorizer(decode_error=""replace"",vocabulary=pickle.load(open(""feature.pkl"", ""rb"")))
tfidf = transformer.fit_transform(loaded_vec.fit_transform(np.array([""aaa ccc eee""])))

that works. tfidf will have same feature length as trained data.",https://stackoverflow.com/questions/29788047,python,22-04-2015 04:55,41058.0,27.0,5.0,True,15-10-2024 09:35,15-10-2024 09:35
78128206,return sentences from list of sentences using user specified keyword,"i got a list of sentences (roughly 20000) stored in excel file named list.xlsx and  sheet named sentence under column name named sentence.
my intention is to get words from user and return those sentences where in those exact words matches.
i am currently able to do so with the code i developed using spacy. but it takes lot of time to check and return output.
is there any other time saving way of achieving this by any other means.
i see in geany notepad or libre calc wherein its search function return sentences in a jiffy.
how?
kindly help.
import pandas as pd
import spacy

# load the english language model in spacy
nlp = spacy.load(""en_core_web_sm"")

# function to extract sentences containing the keyword
def extract_sentences_with_keyword(text, keyword):
    doc = nlp(text)
    sentences = [sent.text for sent in doc.sents if keyword in sent.text.lower()]
    return sentences

i = input(""enter keyword(s):"")

# read the excel file
file_path = ""list.xlsx""
sheet_name = ""sentence""  # update with your sheet name
column_name = ""sentence""   # update with the column containing text data

data = pd.read_excel(file_path, sheet_name=sheet_name)



# iterate over the rows and extract sentences with the keyword
keyword = i  # update with the keyword you want to search for
for index, row in data.iterrows():
    text = row[column_name]
    sentences = extract_sentences_with_keyword(text, keyword)
    
    if sentences:
        for sentence in sentences:
            print(sentence)
        print(""\n"")","['python', 'spacy', 'text-processing']",78141386,"you can use sqlite with a full text index. i tried the following proof of concept code with a 6 mb text file and it is very fast. you of course need to adjust the code for your needs, using spacy for sentence splitting as you did above might be a decent option:
import sqlite3
import re

conn = sqlite3.connect(':memory:')
cursor = conn.cursor()

cursor.execute('create virtual table fts_sentences using fts5(content)')

def load_and_split_file(file_path):
    sentence_endings = r'[.!?]\s+|\s*$'
    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()
        sentences = re.split(sentence_endings, text)
        return sentences

def insert_sentences(sentences):
    for sentence in sentences:
        cursor.execute('insert into fts_sentences (content) values (?)', (sentence,))
    conn.commit()

def search_word(word):
    cursor.execute('select content from fts_sentences where fts_sentences match ?', (word,))
    return cursor.fetchall()

file_path = 'big.txt' 
sentences = load_and_split_file(file_path)
insert_sentences(sentences)

while true:
    word_to_search = input('enter a word to search for: ')
    matching_sentences = search_word(word_to_search)

    for sentence in matching_sentences:
        print(sentence[0])

your code using spacy is also very slow because you do not disable any pipelines, so it also performs stuff like part of speech detection, which you do not need for your use case. for details you can look here: 
quoting from the docs (you might need to disable more or less pipelines):
import spacy

texts = [
    ""net income was $9.4 million compared to the prior year of $2.7 million."",
    ""revenue exceeded twelve billion dollars, with a loss of $1b."",
]

nlp = spacy.load(""en_core_web_sm"")
for doc in nlp.pipe(texts, disable=[""tok2vec"", ""tagger"", ""parser"", ""attribute_ruler"", ""lemmatizer""]):
    # do something with the doc here
    print([(ent.text, ent.label_) for ent in doc.ents])",https://stackoverflow.com/questions/78128206,python,08-03-2024 14:02,117.0,0.0,1.0,True,11-03-2024 14:29,08-03-2024 15:45
78892056,how to disable the warning message for g4f version deprecation?,"i am using this code to get my response out of the model :
from g4f.client import client

client = client()

response = client.chat.completions.create(
    model=""gpt-3.5-turbo"",
    messages=[{""role"": ""user"", ""content"": ""hello""}],
)

this code is run through subprocess.popen() call like so:
p = subprocess.popen(['c:\\python38\\python.exe','-wignore', 'c:\\users\\user\\proj\\projname\\chatgpt.py'],
                             stdin=subprocess.pipe, stdout=subprocess.pipe, shell=true, env=env)

but the call to client.chat.completions.create() generates this warning message before actually returning the model response:
new g4f version: 0.3.2.4 (current: 0.3.2.2) | pip install -u g4f

my question is how to suppress that warning message from being generated by the mentioned call?","['python', 'warnings', 'openai-api']",78892118,"you can suppress it using the warnings module:
import warnings
warnings.filterwarnings(""ignore"")

or, you can resolve the issue by upgrading the g4f package to the latest version via pip install -u g4f

you can also use sys.stdout to redirect standard output temporarily.
for example:
# save the current stdout
old_stdout = sys.stdout

# redirect all output that would normally go to stdout (e.g., the console) into a buffer instead
sys.stdout = io.stringio()

# call the g4f function that generates the warning here
...
...

# restore stdout to its original state
sys.stdout = old_stdout",https://stackoverflow.com/questions/78892056,python,20-08-2024 11:01,304.0,0.0,1.0,True,23-08-2024 13:00,20-08-2024 13:54
53118666,spacy - convert token type into list,"i have few elements which i got after performing operation in spacy having type
input -
li = ['india', 'australia', 'brazil']
for i in li:
    print(type(i))

output:

<class 'spacy.tokens.token.token'>
<class 'spacy.tokens.token.token'>
<class 'spacy.tokens.token.token'>

i want to make all elements in list with str type for iteration.
expected output -
li = ['india', 'australia', 'brazil']
for i in li:
    print(type(i))

output

<class 'str'>
<class 'str'>
<class 'str'>

please suggest some optimized way..","['python-3.x', 'list', 'token', 'spacy']",53119749,"spacy token has a attribute called text.
here's a complete example:
import spacy
nlp = spacy.load('en_core_web_sm')
t = (u""india australia brazil"")
li = nlp(t)
for i in li:
    print(i.text)

or if you want the list of tokens as list of strings:
list_of_strings  = [i.text for i in li]",https://stackoverflow.com/questions/53118666,python-3.x,02-11-2018 12:30,13206.0,5.0,2.0,True,20-06-2022 05:47,20-06-2020 09:12
58735715,getting a spacy error: no module named &#39;spacy.pipeline.pipes&#39;; &#39;spacy.pipeline&#39; is not a package,"i'm trying to test a model that is working in another machine, but when i try to import it to my notebook, i get this error: 
modulenotfounderror: no module named 'spacy.pipeline.pipes'; 'spacy.pipeline' is not a package
we have installed:
spacy 2.0.18 (frozen version, not updatable whatsoever) 
and i'm importing:
import spacy
import thinc
import unidecode
import nltk
from spacy.vocab    import vocab
from spacy.language import language
from spacy.lang.pt  import portuguese
from spacy.lang.en  import english
from spacy.pipeline import entityrecognizer
ner = entityrecognizer(nlp.vocab)
nlp = language(vocab())
nlp = portuguese()
# load ner model
ner_model = pickle.load( open(""/ner_model_v022_epoch=706_loss=09o76364626.pkl"", ""rb"" ) )

and i get the following error:
---------------------------------------------------------------------------
modulenotfounderror                       traceback (most recent call last)
<ipython-input-12-83d4770d3e3e> in <module>

---> 40 ner_model = pickle.load( open(""/ner_model_v022_epoch=706_loss=09o76364626.pkl"", ""rb"" ) )

modulenotfounderror: no module named 'spacy.pipeline.pipes'; 'spacy.pipeline' is not a package

any ideas why this might be happening? already installed everything again from 0 but keeps giving me the same error.
any help will be greatly appreciated.","['nlp', 'spacy', 'named-entity-recognition']",60910286,i had this problem come up and found that switching my spacy version from spacy==2.0.18 to spacy==2.1.4 worked! went back through their releases and spacy.pipeline.pipes isn't present until v2.1.0a8,https://stackoverflow.com/questions/58735715,nlp,06-11-2019 17:37,5713.0,3.0,3.0,True,01-01-2022 07:12,07-11-2019 15:05
52021855,nltk linguistic tree traversal and extract noun phrase (np),"i created a custom classifier based chunker: digdug_classifier, which chunks the following sentence:
sentence = ""there is high signal intensity evident within the disc at t1.""

to create these chunks:
(s
  (np there/ex)
  (vp is/vbz)
  (np high/jj signal/jj intensity/nn evident/nn)
  (pp within/in)
  (np the/dt disc/nn)
  (pp at/in)
  (np t1/nnp)
  ./.)

i need to create a list of just the np from the above, like this:
np = ['there', 'high signal intensity evident', 'the disc', 't1']

i wrote the following code:
output = []
for subtree in digdug_classifier.parse(pos_tags): 
    try:
        if subtree.label() == 'np': output.append(subtree)
    except attributeerror:
        output.append(subtree)
print(output)

but that gives me this answer instead:
[tree('np', [('there', 'ex')]), tree('np', [('high', 'jj'), ('signal', 'jj'), ('intensity', 'nn'), ('evident', 'nn')]), tree('np', [('the', 'dt'), ('disc', 'nn')]), tree('np', [('t1', 'nnp')]), ('.', '.')]

what can i do to get the desired answer?","['python', 'tree', 'nltk', 'chunking']",52023337,"first, see how to traverse an nltk tree object?
specific to your question of extraction np:
>>> from nltk import tree
>>> parse_tree = tree.fromstring(""""""(s
...   (np there/ex)
...   (vp is/vbz)
...   (np high/jj signal/jj intensity/nn evident/nn)
...   (pp within/in)
...   (np the/dt disc/nn)
...   (pp at/in)
...   (np t1/nnp)
...   ./.)"""""")

# iterating through the parse tree and 
# 1. check that the subtree is a tree type and 
# 2. make sure the subtree label is np
>>> [subtree for subtree in parse_tree if type(subtree) == tree and subtree.label() == ""np""]
[tree('np', ['there/ex']), tree('np', ['high/jj', 'signal/jj', 'intensity/nn', 'evident/nn']), tree('np', ['the/dt', 'disc/nn']), tree('np', ['t1/nnp'])]

# to access the item inside the tree object, 
# use the .leaves() function
>>> [subtree.leaves() for subtree in parse_tree if type(subtree) == tree and subtree.label() == ""np""]
[['there/ex'], ['high/jj', 'signal/jj', 'intensity/nn', 'evident/nn'], ['the/dt', 'disc/nn'], ['t1/nnp']]

# to get the string representation of the leaves
# use "" "".join()
>>> [' '.join(subtree.leaves()) for subtree in parse_tree if type(subtree) == tree and subtree.label() == ""np""]
['there/ex', 'high/jj signal/jj intensity/nn evident/nn', 'the/dt disc/nn', 't1/nnp']


# to just get the leaves' string, 
# iterate through the leaves and split the string and
# keep the first part of the ""/""
>>> ["" "".join([leaf.split('/')[0] for leaf in subtree.leaves()]) for subtree in parse_tree if type(subtree) == tree and subtree.label() == ""np""]
['there', 'high signal intensity evident', 'the disc', 't1']",https://stackoverflow.com/questions/52021855,python,25-08-2018 22:55,1528.0,1.0,2.0,True,30-10-2022 12:16,26-08-2018 05:03
77940314,cannot change training arguments when resuming from a checkpoint,"i noticed when resuming the training of a model from a checkpoint changing properties like save_steps and per_device_train_batch_size has no affect. i'm wondering if there's something syntactically wrong here or technically the config of the model checkpoint overrides everything?
import transformers
from datetime import datetime

tokenizer.pad_token = tokenizer.eos_token

learning_rate = 5e-5  
warmup_steps = 100

gradient_accumulation_steps = 2  

trainer = transformers.trainer(
    model=model,
    callbacks=[upload_checkpoint_callback],
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_val_dataset,
    args=transformers.trainingarguments(
        output_dir=output_dir,
        warmup_steps=warmup_steps,
        per_device_train_batch_size=8,
        gradient_checkpointing=true,
        gradient_accumulation_steps=gradient_accumulation_steps,
        max_steps=5000,
        learning_rate=learning_rate,
        logging_steps=10,
        fp16=true,
        optim=""paged_adamw_8bit"",
        logging_dir=""/content/logs"",       
        save_strategy=""steps"",      
        save_steps=10,              
        evaluation_strategy=""steps"", 
        eval_steps=10,               
        load_best_model_at_end=true,
        report_to=""wandb"",           
        run_name=f""{run_name}-{datetime.now().strftime('%y-%m-%d-%h-%m')}""          # name of the w&b run (optional)
    ),
    data_collator=transformers.datacollatorforlanguagemodeling(tokenizer, mlm=false),
)

model.config.use_cache = false
trainer.train(resume_from_checkpoint=""/content/latest_checkpoint/"")","['huggingface-transformers', 'huggingface-trainer']",77949365,the transformers library does not have the ability to change training arguments when resuming from a checkpoint.,https://stackoverflow.com/questions/77940314,huggingface-transformers,05-02-2024 10:45,1108.0,0.0,1.0,True,06-02-2024 16:32,05-02-2024 10:54
75130116,getting 400 bad request from open ai api using python flask,"i want to get response using flask from openai api. whether i am getting status 400 bad request from browser through 
bad request
the browser (or proxy) sent a request that this server could not understand.
also i am checking this from postman
from flask import flask, request, render_template
import requests

app = flask(__name__)

@app.route('/')
def index():
    return 'welcome to chatgpt app!'

@app.route('/chat', methods=['get', 'post'])
def chat():
    user_input = request.form['text']
    # use openai's api to generate a response from chatgpt
    response = generate_response_from_chatgpt(user_input)
    return response

def generate_response_from_chatgpt(user_input):
    api_key = ""your_api_key""
    url = ""
    headers = {
        ""content-type"": ""application/json"",
        ""authorization"": f""bearer {api_key}""
    }
    data = {
        ""prompt"": user_input,
        ""engine"": ""davinci""
    }
    response = requests.post(url, headers=headers, json=data)
    return response.json()[""choices""][0][""text""]


if __name__ == '__main__':
    app.run()","['flask', 'flask-restful', 'openai-api', 'gpt-3']",75130180,"it would be best if you check the openai documentation to make sure you are using the correct endpoint and data format in your request.
also, you should check your api key, if it is correct and if you have reached the limit of requests.
also, it's worth noting that the code you provided is missing the import statement for flask. you will need to add the following line at the top of your file:
from flask import flask, request
also, i see that you're using request.form['text'] but you should check if the request is a get or post request.
if request.method == 'post':
    user_input = request.form['text']
else:
    user_input = request.args.get('text')

this is to avoid a keyerror being raised when the request is a get request.",https://stackoverflow.com/questions/75130116,flask,16-01-2023 03:56,1430.0,-2.0,1.0,True,24-01-2023 18:21,24-01-2023 18:21
77640823,how to make azure openai service use default system message in api calls?,"i have created an azure openai service and set a default system message. i want to create a chatbot by sending api calls to the endpoint and integrating it into an existing react app as a component.
my aim is to allow other team members to modify and fine tune the system message and examples. however, whenever i call the openai service through api calls, i must always send the system message and examples with the request. it seems that the api responses disregard the system message unless i send it with the request.
i assumed that the default system message gets saved to the deployment and i would not have to send it on every request. is there a way to configure the azure openai service to always use the default system message that is configured in the azure openai studio?","['openai-api', 'azure-openai']",77641567,"the current 'state of the art' does not support that, as it works on the basis of stateless interaction. what you can do is place a system prompt in a variable and include it in the openai sdk call, or you can build a facade class around it. this makes it part of a class's default prompt, especially useful if you're leveraging it from multiple places, ensuring it's not 'forgotten'.",https://stackoverflow.com/questions/77640823,openai-api,11-12-2023 16:01,941.0,0.0,1.0,True,11-12-2023 18:14,11-12-2023 16:08
76950609,what is the difference between openai and chatopenai in langchain?,"i read the langchain quickstart.
there is a demo inside:
from langchain.llms import openai
from langchain.chat_models import chatopenai

llm = openai()
chat_model = chatopenai()

llm.predict(""hi!"")
>>> ""hi""

chat_model.predict(""hi!"")
>>> ""hi""

i searched the rest of the document and also online, but didn't find any info for the difference between openai and chatopenai.
based on from langchain.llms import openai, openai is a large language model (llm) which is also chat related.
so is openai more general-purpose, while chatopenai more chat focused?
what is the difference between openai class and chatopenai class in langchain? could someone clarify?","['openai-api', 'langchain']",76955946,"tl;dr
based on my research,

openai class includes more generic machine learning task attributes such as frequency_penalty, presence_penalty, logit_bias, allowed_special, disallowed_special, best_of.

chatopenai class provides more chat-related methods, such as completion_with_retry, get_num_tokens_from_messages to make it more user-friendly when build chatbot related applications.



class inheritance
upon reviewing the source code, here's what i've discovered.
listed below are the class inheritances for both the openai and chatopenai classes, along with their respective class attributes and methods.
openai
openai ï¿½ï¿½ï¿½ baseopenai ï¿½ï¿½ï¿½ basellm ï¿½ï¿½ï¿½ baselanguagemodel

chatopenai
chatopenai ï¿½ï¿½ï¿½ basechatmodel ï¿½ï¿½ï¿½ baselanguagemodel

comparison
let's begin our comparison, moving from the fourth column to the first column.
fourth column
both classes ultimately inherit from the base class baselanguagemodel.
third column
basellm and basechatmodel are very similar with slightly difference:

for openai's basellm, it includes additional methods:

batch(self, inputs, config=none, max_concurrency=none, **kwargs)
abatch (self, inputs, config=none, max_concurrency=none,**kwargs)


for chatopenai's basechatmodel, it includes an extra method:

_combine_llm_outputs(self, llm_outputs)



second column
the second column contains the baseopenai class, which primarily exists due to the presence of higher-level classes openai and azureopenai. however, they all share the same class attributes and methods.
first column
at the top-level class (first column):

openai class includes more generic machine learning task attributes such as frequency_penalty, presence_penalty, logit_bias, allowed_special, disallowed_special, best_of.

chatopenai class provides more chat-related methods, such as completion_with_retry, get_num_tokens_from_messages to make it more user-friendly when build chatbot related applications.",https://stackoverflow.com/questions/76950609,openai-api,22-08-2023 06:30,31111.0,30.0,2.0,True,29-09-2024 18:56,23-08-2023 19:37
1833252,java stanford nlp: part of speech labels?,"the stanford nlp, demo'd here, gives an output like this:
colorless/jj green/jj ideas/nns sleep/vbp furiously/rb ./.

what do the part of speech tags mean? i am unable to find an official list. is it stanford's own system, or are they using universal tags? (what is jj, for instance?)
also, when i am iterating through the sentences, looking for nouns, for instance, i end up doing something like checking to see if the tag .contains('n'). this feels pretty weak. is there a better way to programmatically search for a certain part of speech?","java, nlp, stanford-nlp, part-of-speech",1833718,"the penn treebank project. look at the part-of-speech tagging ps.
jj is adjective. nns is noun, plural. vbp is verb present tense. rb is adverb.
that's for english. for chinese, it's the penn chinese treebank. and for german it's the negra corpus.


cc coordinating conjunction 
cd cardinal number 
dt determiner 
ex existential there 
fw foreign word 
in preposition or subordinating conjunction 
jj adjective 
jjr adjective, comparative 
jjs adjective, superlative 
ls list item marker 
md modal 
nn noun, singular or mass 
nns noun, plural 
nnp proper noun, singular 
nnps proper noun, plural 
pdt predeterminer 
pos possessive ending 
prp personal pronoun 
prp$ possessive pronoun 
rb adverb 
rbr adverb, comparative 
rbs adverb, superlative 
rp particle 
sym symbol 
to to 
uh interjection 
vb verb, base form 
vbd verb, past tense 
vbg verb, gerund or present participle 
vbn verb, past participle 
vbp verb, nonï¿½ï¿½3rd person singular present 
vbz verb, 3rd person singular present 
wdt whï¿½ï¿½determiner 
wp whï¿½ï¿½pronoun 
wp$ possessive whï¿½ï¿½pronoun 
wrb whï¿½ï¿½adverb",https://stackoverflow.com/q/1833252,"java, nlp, stanford-nlp, part-of-speech",02-12-2009 14:30,105441.0,186.0,9.0,True,14-12-2023 12:33,11-08-2015 03:27
3926891,trying to use hpsg pet parser,"i'm trying to use the pet parser, but the given documentation for usage is insufficient. can anyone point me to a good article or tutorial on using pet? does it support utf-8?","['parsing', 'utf-8', 'nlp', 'pos-tagger']",5129891,"to use the pet parser, first you have to load a grammar for the language of interest. the grammar must be authored in the tdl language, as used in the delph-in consortium (wiki here). large, compatible grammars are available for several languages, including english, japanese, and german. there are also smaller grammars available, and you can write your own.
for this--and for working with these grammars--your best bet is ann copestake's book, ""implementing typed feature structure grammars"" (csli 2002). the book provides a thorough introduction to tdl and grammars such as these which function via the unification of typed feature structures. the grammars support bidirectional mapping between syntax (surface strings) and semantics (""meaning,"" represented according to copestake's mrs--minimal recursion semantics). note that these are precision grammars, which means that they are generally less tolerant of ungrammatical inputs than statistical systems.
the english resource grammar (erg) is a large grammar of english which has broad, general-domain coverage. it's open source and you can download it from the website. an online demo, powered by the pet parser, can be found here.
the pet parser runs in two steps. the first, called flop produces a ""compiled"" version of the grammar. the second step is the actual parsing, which uses the cheap program. you will need to obtain these two pet binaries for your linux machine, or build them yourself. this step may not be easy if you're not familiar with building software on linux. pet does not run on windows (or mac, to my knowledge).
running flop is easy. just go to your /erg directory, and type:
$ flop english.tdl

this will produce the english.grm file. now you can parse sentences by running cheap:
$ echo the child has the flu. | cheap --mrs english.grm

this example produces a single semantic representation of the sentence in mrs (minimal recursion semantics) format:
 [ ltop: h1
   index: e2 [ e sf: prop tense: pres mood: indicative prog: - perf: - ]
   rels: <
          [ _the_q_rel<-1:-1>
            lbl: h3
            arg0: x6 [ x pers: 3 num: sg ind: + ]
            rstr: h5
            body: h4 ]
          [ ""_child_n_1_rel""<-1:-1>
            lbl: h7
            arg0: x6 ]
          [ ""_have_v_1_rel""<-1:-1>
            lbl: h8
            arg0: e2
            arg1: x6
            arg2: x9 [ x pers: 3 num: sg ] ]
          [ _the_q_rel<-1:-1>
            lbl: h10
            arg0: x9
            rstr: h12
            body: h11 ]
          [ ""_flu_n_1_rel""<-1:-1>
            lbl: h13
            arg0: x9 ] >
   hcons: < h5 qeq h7 h12 qeq h13 > ]

copestake's book explains the specific syntax and linguistic formalism used in grammars that are compatible with pet. it also serves as a user's manual for the open-source lkb system, which is a more interactive system that can also parse with these grammars. in addition to parsing, the lkb can do the reverse: generate sentences from mrs semantic representations. the lkb is currently only supported on linux/unix. there are actually a total of four delph-in compliant grammar processing engines, including lkb and pet.
for windows, there is agree, a multi-threaded parser/generator (and here) that i've developed for .net; it also supports both generation and parsing. if you need to work with the grammars interactively, you might want to consider using the lkb or agree in addition to--or instead of--pet. the interactive client front-ends for agree are mostly wpf-based, but the engine and a simple console client can run on any mono platform.
ace is another open-source delph-in compatible parsing and generation system which is designed for high performance, and it is available for linux and macos.
the lkb is written in lisp, whereas pet and ace are c/c++, so the latter are the faster parsers for production use. agree is also much faster than the lkb, but only becomes faster than pet when parsing complex sentences, where overheads from agree's lock-free concurrency become amortized.
[11/25/2011 edit: agree now supports generation as well as parsing]",https://stackoverflow.com/questions/3926891,parsing,13-10-2010 18:40,1406.0,8.0,2.0,True,07-11-2023 05:21,07-11-2023 05:21
77152662,error [err_fr_max_body_length_exceeded]: request body larger than maxbodylength limit when sending a request to openai apu,"i'm using whisper.js code to get transcription of a wav file. however when running the code below this is the error i'm getting.
error [err_fr_max_body_length_exceeded]: request body larger than maxbodylength limit.
when i asked chatgpt for the error it said, 'indicates that the size of the audio file you're trying to send to openai's api is exceeding the allowed request body size' so i added the the following part in the code as per said by chatgpt:
const configuration = new configuration({
    apikey: ""your_api_key"",
    maxbodylength: infinity,
});


this is my code:
require('dotenv').config();
const { configuration, openaiapi } = require(""openai"");
const fs = require(""fs"");



const configuration = new configuration({
    apikey: ""your_api_key"",
    maxbodylength: infinity,
  });


const openai = new openaiapi(configuration);

async function createtranscription(audiofilename) {
    const resp = await openai.createtranscription(
      fs.createreadstream(audiofilename),
      ""whisper-1""
    );

    return resp.data.text;
}

async function main() {
    try {
        const audiofilename = '/users/rahulsharma/desktop/whisper/output_dir/stream.wav';
        const transcription = await createtranscription(audiofilename);
        console.log(transcription);
    } catch (e) {
        console.error(e);
    }
}

main();

does anyone how to get around this error?","['javascript', 'node.js', 'openai-api']",77156756,"you can pass some request options when creating the transcription. with the openai v3 package you need to specify all options you can pass according to the openai api reference, then you can pass on an object with maxbodylength.
await openai.createtranscription(
          fs.createreadstream(audiofilename),
          model, // ""whisper-1""
          prompt,
          responseformat,
          temperature,
          language,
          {
            maxbodylength: infinity,
          },
        );

if you don't want to specify prompt, responseformat, temperature, or language, you can just set them to undefined.
edit:
note that you only need to set maxbodylength when you use the openai library < version 4. before version 4 the library uses axios, starting with v4 the library uses fetch (well, node-fetch). maxbodylength is an option specific to axios, fetch doesn't have it and you don't need to do anything to be able to send large files to the whisper api.",https://stackoverflow.com/questions/77152662,javascript,21-09-2023 18:11,1105.0,1.0,1.0,True,22-09-2023 10:54,21-09-2023 18:17
75091786,openai unity - post request not working properly (400 status),"i'm connecting gpt3 openai but i just cant manage to make a proper post request to it (i'm following some guides but for them it works...).
private ienumerator upload ( )
{
     form = new 
    form.addfield ( ""prompt"", prompt );
    form.addfield ( ""max_tokens"", maxtokens );
    form.addfield ( ""model"", model );
    form.addfield ( ""temperature"", temperature );

    using ( unitywebrequest wr = unitywebrequest.post ( "" form ) )
    {
        wr.setrequestheader ( ""authorization"", ""bearer "" + apikey );
        wr.setrequestheader ( ""content-type"", ""json"" );
        yield return wr.sendwebrequest ( );
        if ( wr.result != unitywebrequest.result.success )
        {
            debug.log ( ""error:\n"" + wr.error );
        }
        else
        {
            debug.log ( ""success:\n"" + wr.result + ""\nupload completed!);
        }
    }
}

my code is always returning me a bad request (a.k.a 400 bad request).","['unity-game-engine', 'httprequest', 'openai-api', 'gpt-3']",75092193,"remove the ""content-type"" from the headers. the content is not json, it's form data.
i.e.
using ( unitywebrequest wr = unitywebrequest.post ( "" form ) )
{
    wr.setrequestheader ( ""authorization"", ""bearer "" + apikey );
    //wr.setrequestheader ( ""content-type"", ""json"" );
    yield return wr.sendwebrequest ( );
    if ( wr.result != unitywebrequest.result.success )
    {
        debug.log ( ""error:\n"" + wr.error );
    }
    else
    {
        debug.log ( ""success:\n"" + wr.result + ""\nupload completed!);
    }
}",https://stackoverflow.com/questions/75091786,unity-game-engine,12-01-2023 04:58,546.0,4.0,1.0,True,30-01-2023 07:17,12-01-2023 06:45
76772986,how do i decode the output of a pytorch openaigptmodel?,"i am trying to decode the outputs of a pytorch openaigptmodel, but i can't see how to go about it, and i can't find any complete examples online.
i've found only this much:
from transformers import openaigpttokenizer, openaigptmodel
tokenizer = openaigpttokenizer.from_pretrained('openai-gpt')
model = openaigptmodel.from_pretrained('openai-gpt')

inputs = tokenizer(""how does a kite fly?"", return_tensors=""pt"")
outputs = model(**inputs)

outputs has an attribute last_hidden_state which is a torch.floattensor of shape (batch_size, sequence_length, hidden_size). i've tried grabbing the first vector of length hidden_size and calling tokenizer.decode(vector.tolist()), but i get:
'<unk><unk><unk><unk>'

i've also tried interpreting my last_hidden_state as a series of probabilities for each token in the lexicon with tokenizer.decode(torch.argmax(last_hidden_states, 2)[0].tolist()), but that also outputs nonsense:
'ï¿½ï¿½ ï¿½ï¿½ore ï¿½ï¿½have ï¿½ï¿½ï","['pytorch', 'torch', 'openai-api']",76774526,"what the model returns in this case isn't actually a representation of tokens.
there are many ways to return results from a large language model: logits predicting particular token ids, softmaxed percent probabilities of each possible token, a vector representing the embedding of a single token, etc.
given that you get batch x seq_len x hidden_size it appears that the vector of size ""hidden size"" is probably a vector representing the embedding of a single token. it's the ""lm_head"" that's used to convert a token embedding to a token id.
checking the doc for that model i see

the bare openai gpt transformer model outputting raw hidden-states without any specific head on top.

without any head, you're not getting tokens, just embeddings that can be used to predict tokens. you probably want something like openaigptlmheadmodel that has an lm head. the given example for that model appears to output logits, so maybe from that you might need to use argmax to select specific tokens. then once you have token ids you'd use something like tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(tokens))
you may be wondering why a user would want a model that returns logits instead of tokens. users don't always want to select the ""most likely"" predicted next token. by returning logits (or a probability distribution) the caller can sample from likely next tokens.",https://stackoverflow.com/questions/76772986,pytorch,26-07-2023 15:43,545.0,0.0,1.0,True,07-08-2023 21:07,07-08-2023 21:07
71622637,elastic: treat symbol and html encoded symbol the same during search,"my goal is to return the same results when searching by the symbol or html encoded version.  example queries:
# searching with symbol
get my-test-index/_search
{
  ""query"": {
    ""bool"": {
      ""must"": {
        ""simple_query_string"": {
          ""query"": ""helloï¿½ï¿½"",
          ""analyzer"": ""english_syn"",
          ""fields"": [
            ""allcontent""
          ]
        }
      }
    }
  }
}

# html symbol
get my-test-index/_search
{
  ""query"": {
    ""bool"": {
      ""must"": {
        ""simple_query_string"": {
          ""query"": ""hello&reg;"",
          ""analyzer"": ""english_syn"",
          ""fields"": [
            ""allcontent""
          ]
        }
      }
    }
  }
}

i've tried a couple different things.
adding synonyms but they still produced different resu
#######################################
# synonyms
# symbols
#######################################
ï¿½ï¿½ï¿½, &trade;
ï¿½ï¿½, &reg;

created a char_filter to replace special characters so they would at least be searching for ""hello"". but that comes with its own set of issues that is out of scope of what i am trying to achieve.
char_filter"": {
    ""specialcharactersfilter"": {
    ""type"": ""pattern_replace"",
    ""pattern"": ""[^a-za-z0-9]"",
    ""replacement"": "" ""
}

i appreciate any feedback for any new alternatives to achieve this goal. ideally a solution that covers more than ï¿½ï¿½ and","['elasticsearch', 'filter', 'analyzer', 'elasticsearch-query', 'stemming']",71625917,"what you are looking for is the html strip char filter, which works not only for two symbols but for a broad html characters.
working example
index mapping with html strip char filter
{
    ""settings"": {
        ""analysis"": {
            ""analyzer"": {
                ""my_analyzer"": {
                    ""tokenizer"": ""standard"",
                    ""char_filter"": [
                        ""html_strip""
                    ]
                }
            }
        }
    },
    ""mappings"": {
        ""properties"": {
            ""title"": {
                ""type"": ""text"",
                ""analyzer"": ""my_analyzer""
            }
        }
    }
}

index sample doc with just (ï¿½ï¿½ï¿½) in that document.
put 71622637/_doc/1

{
   ""title"" : ""ï¿½ï¿½ï¿½""
}


search encoded version
{
    ""query"" :{
        ""match"" : {
            ""title"" : ""&trade""
        }
    }
}

and search result

""hits"": [
            {
                ""_index"": ""71622637"",
                ""_id"": ""1"",
                ""_score"": 0.89701396,
                ""_source"": {
                    ""title"": ""ï¿½ï¿½ï¿½""
                }
            }
        ]

similar to this, search on trademark symbol
{
    ""query"" :{
        ""match"" : {
            ""title"" : ""ï¿½ï¿½ï¿½""
        }
    }
}

and search result

""hits"": [
            {
                ""_index"": ""71622637"",
                ""_id"": ""1"",
                ""_score"": 0.89701396,
                ""_source"": {
                    ""title"": ""ï¿½ï¿½        }
            }
        ]",https://stackoverflow.com/questions/71622637,elasticsearch,25-03-2022 20:16,409.0,1.0,1.0,True,26-03-2022 10:28,26-03-2022 10:28
74116535,"python/pandas/nltk: iterating through a dataframe, get value, transform it and add the new value to a new column","i scraped some data from google news into a dataframe:
dataframe:
df

title   link    pubdate     description     source  source_url
0   australian research finds cost-effective way t...      sat, 15 oct 2022 23:51:00 gmt   australian research finds cost-effective way t...   the guardian    
1   something new under the sun: floating solar pa...      tue, 18 oct 2022 11:49:11 gmt   something new under the sun: floating solar pa...   voice of america - voa news     
2   adapt solar panels for sub-saharan africa - na...      tue, 18 oct 2022 09:06:41 gmt   adapt solar panels for sub-saharan africanatur...   nature.com  
3   cost of living: the people using solar panels ...      wed, 05 oct 2022 07:00:00 gmt   cost of living: the people using solar panels ...   bbc     
4   business matters: solar panels on commercial p...      mon, 17 oct 2022 09:13:35 gmt   business matters: solar panels on commercial p...   insider media   
...     ...     ...     ...     ...     ...     ...

what i want to do now is basically to iterate through the ""link"" column and summarize every article with nltk and add the summary to a new column. here is an example:
article = article(df.iloc[4, 1]) #get the url from the link column
article.download()
article.parse()
article.nlp()
article = article.summary
print(article)

output:
north westgemma cornwall, head of sustainability of anderton gables, looks into the benefit of solar panels.
and, with the cost of solar panels continually dropping, it is becoming increasingly affordable for commercial property owners.
reduce your energy spendmost people are familiar with solar energy, but many are unaware of the significant financial savings that can be gained by installing solar panels in commercial buildings.
as with all things, there are pros and cons to weigh up when considering solar panels.
if youï¿½ï¿½ï¿½re considering solar panels for your property, contact one of the anderton gables team, who can advise you on the best course of action.

i tried a little bit, but i couldn't make it work...
thanks for your help","['python', 'pandas', 'dataframe', 'loops', 'nltk']",74118072,"this will be a very slow solution with a for loop, but it might work for a small dataset. iterating through all the links and then applying the transformations needed, and ultimately create a new column in the dataframe
summaries = []
for l in df['source_url'].values:
    article = article(l)
    article.download()
    article.parse()
    article.nlp()
    summaries.append(article.summary)
df['summaries'] = summaries

or you could define a custom function and the use pd.apply:
def get_description(x):
    art = article(x)
    art.download()
    art.parse()
    art.nlp()
    return art.summary

df['summary'] = df['source_url'].apply(get_description)",https://stackoverflow.com/questions/74116535,python,18-10-2022 19:33,41.0,0.0,1.0,True,19-10-2022 10:13,18-10-2022 19:39
78301939,having trouble using openai api,"i am having trouble with this code. i want to implement ai using openai api in my react.js project but i cannot seem to get what the issue is. i ask it a question in the search bar in my project and it says ""no response from ai"". there is more to it but this is just what i think is having trouble.
//landingpage.js
import react, { usestate, useeffect } from 'react';
import { fasearch } from 'react-icons/fa';
import './app.css';
import { entryform } from './entryform';

function landingpage() {
  // states related to the healthy innovations features
  const [search, setsearch] = usestate('');
  const [showsuggestions, setshowsuggestions] = usestate(true);
  const [isloading, setisloading] = usestate(false);
  const [recipedetails, setrecipedetails] = usestate(null);
  const [showworkoutquestion, setshowworkoutquestion] = usestate(false);
  const [selectedsuggestion, setselectedsuggestion] = usestate(null);
  const [showworkoutplan, setshowworkoutplan] = usestate(false);
  const [showcaloriecalculator, setshowcaloriecalculator] = usestate(false);
  const [workoutsplit, setworkoutsplit] = usestate('');
  const [showcaloriequestion, setshowcaloriequestion] = usestate(false);
  const [chatinput, setchatinput] = usestate('');
  const [chathistory, setchathistory] = usestate([]);
  const [currenttitle, setcurrenttitle]= usestate(null)
  
  console.log(chathistory); // debugging: check the structure before rendering
  

  const createnewchat = () => {
    // clears the chat to start a new conversation
    setchatinput('');
    setcurrenttitle(null)
    // no need for setcurrenttitle in this context
  };

  const renderchathistory = () =>
  chathistory.map((chat, index) => (
      <div key={index} classname=""chat-history"">
          <p>role: {chat.role}</p>
          {/* check if chat.content is a string; if not, handle it appropriately */}
          <p>message: {chat.content}</p>
      </div>
  ));

  const handlesearchchange = (e) => {
    const inputvalue = e.target.value;
    setchatinput(inputvalue); // update chatinput instead of search state.
    setshowsuggestions(inputvalue.length > 0); // show suggestions if there's input
  };

  const renderdynamicrecommendations = () => {
    // filter suggestions based on search input
    const filteredsuggestions = staticsuggestions.filter(suggestion =>
      suggestion.tolowercase().includes(search.tolowercase())
    ); 

    return (
      <ul>
        {filteredsuggestions.map((suggestion, index) => (
          <li key={index} onclick={() => handleselectsuggestion(suggestion)} style={{ cursor: 'pointer' }}>
            {suggestion}
          </li>
        ))}
      </ul>
    );
  };

  const server_url = ""
  // get messages function and other logic remain the same, ensure you're using chatinput for input value management
  // adjusting the getmessages function to handle server response correctly
  const getmessages = async () => {
    if (!chatinput.trim()) return; // avoid sending empty messages
    setisloading(true);
  
    try {
      const response = await fetch(' {
        method: 'post',
        headers: { 'content-type': 'application/json' },
        body: json.stringify({ message: chatinput })
      });
  
      if (!response.ok) {
        throw new error(`http error! status: ${response.status}`);
      }
  
      const data = await response.json();
      const airesponse = data.choices && data.choices.length > 0
        ? data.choices[0].message
        : ""no response from ai.""; 
      // update chat history
      setchathistory(prev => [...prev, { role: 'user', content: chatinput }, { role: 'ai', content: airesponse }]);
      setchatinput(''); // clear the input field
    } catch (error) {
      console.error('fetch error:', error);
      setchathistory(prev => [...prev, { role: 'user', content: chatinput }, { role: 'ai', content: ""error communicating with ai."" }]);
    } finally {
      setisloading(false);
    }
  };


//server.js 

const port = 8000
const express = require('express')
const cors = require('cors')
require('dotenv').config()
const app = express()
app.use(express.json())
app.use(cors())

const api_key = process.env.api_key

app.post('/completions', async (req, res) => {
    const options = {
        method: ""post"",
        headers: {
            ""authorization"": `bearer ${api_key}`, 
            ""content-type"": ""application/json"" 
        },
        body: json.stringify({
            model: ""gpt-3.5-turbo"",
            messages: [{role: ""user"", content: req.body.message}],
            max_tokens: 100,
        })
    };
    try {
        const response = await fetch(' options);
        const data = await response.json();

        if (data.choices && data.choices.length > 0 && data.choices[0].message) {
            // adjust this path according to the actual structure of openai's response
            res.json({ message: data.choices[0].message.content });
        } else {
            throw new error(""invalid response structure from openai api."");
        }
    } catch (error) {
        console.error(""server error:"", error);
        res.status(500).json({ message: ""failed to get response from ai."" });
    }
});

app.listen(port, () => console.log('your server is running on port'+ port))

.env file: api_key = ""api key""
i have tried changing varablies and also seing if i have everything downloaded which i do.","['reactjs', 'node.js', 'openai-api']",78303163,"the backend returns a response in a format different from what the frontend expects.
from server.js
  if (data.choices && data.choices.length > 0 && data.choices[0].message) {
    res.json({ message: data.choices[0].message.content });
  } else {
    throw new error(""invalid response structure from openai api."");
  }

this will produce json response { message: ""response from openai"" }
however on the frontend act as if backend return raw response straight from the openai api
   const data = await response.json();
   const airesponse = data.choices && data.choices.length > 0
     ? data.choices[0].message
     : ""no response from ai.""; 

here is a fix of the frontend code to match response shape from the backend:
   const { message } = await response.json();
   const airesponse = message || ""no response from ai."";",https://stackoverflow.com/questions/78301939,reactjs,10-04-2024 02:59,561.0,1.0,1.0,True,10-04-2024 08:48,10-04-2024 04:31
76435499,extracting text and tables in semi-structured .txt,"i have a .txt file that serves as the codebook for a large dataset that looks similar to this
==============================                                                
var v960922                                                                    
              numeric                                                         
                                                                              
         admin.48                                                             
                                                                              
         summary - post mode assignment and administration                    
         -----------------------------------------------------------          
                                                                              
              post mode in this variable refers to beginning mode             
              (question admin.47).                                            
                                                                              
        749      1.   assigned to personal, administered as                   
                      personal iw                                             
          7      2.   assigned to personal, administered as                   
                      telephone iw                                            
         28      3.   assigned to telephone, administered as                  
                      personal iw                                             
        750      4.   assigned to telephone, administered as                  
                      telephone iw                                            
                                                                              
                 0.   inap, no post iw                                        
                                                                              
============================== 

i would like to be able to convert this structure into a data frame to help with cleaning and labeling the dataset for use later. my ideal end result would be a table like this

| var name | freqeuncies | value labels
| -------- | --------    | ---------------------------------------------------
| v960922  |        749  | 1. assigned to personal, administered as personal iw
| v960922  |          7  | 2. assigned to personal, administered as telephone iw
| v960922  |         28  | 3. assigned to telephone, administered as personal iw
| v960922  |        750  | 4. assigned to telephone, administered as telephone iw
| v960922  |         na  | 0. inap, no post iw
     

repeating for each of the variables included in the txt file. each variable in the file follows a similar structure but has variations in the number of values or length of the summary for instance.
my main strategy so far has been to read in the txt file with readlines and then use str_subset to break off lines of the text that meet the criteria i need with the goal of then appending these together to create a data frame.
nes <- readlines(""nes1996var.txt"")
 
vars <- str_subset(nes, ""^var"", )
vars


numbers <- str_subset(nes,""\\d?\\."")
numbers

the first instance of just grabbing variable names worked okay since i ended up with a vector of all the variables like i wanted.
however, trying to pull the tables has been trickier. i've seen other threads on stackoverflow suggest to filter off of the rows that start with numbers, but in the text file there's a lot of deadspace before the numbers so i can't pull just the rows that begin with numbers because technically there aren't any.
so instead i've pulled all the rows that have any numbers at all that are then followed by a period, hoping to catch on the value labels formatting. this was better but not perfect, both because it captured a lot of rows from summaries that included years or other numbers and the fact that some of the rows in the tables actually go over and fill in the second row, meaning sometimes the necessary text got cut off.
even after that i couldn't find a way to separate the frequency number from the value label strings since they were placed on the same row.
is there a more efficient/effective method of achieving what i want? i'm somewhat experienced with r but i am also still learning a lot if that helps also.
edit: the solution provided by dave did what i needed once i made a few tweaks. here is the code that worked for me in case anyone happens to be in a similar situation.
rl <- readlines(.txt file path)


## trim the white space from the front and back of each string 
## this will put the frequencies as the first characters in their lines. 
rl <- trimws(rl)

## find the variable delimiters
delims <- grep(""=============================="", rl)

## initialize the output as a list
out <- vector(mode=""list"", length=length(delims)-1)
    ## loop over the delimiters
for (i in 1:(length(delims) - 1)) {
  ## find the text between adjacent delimiters and call that vbl
  vbl <- rl[(delims[i] + 1):(delims[(i + 1)] - 1)]
  ## capture the varname as the stuff after ""var "" in the first row of vbl
  varname <- gsub(""var (.*)"", ""\\1"", vbl[1])
  ## identify the lines that start with a number
  resps <- grep(""^\\d"", vbl)
  
  if (length(resps) > 0) {
    ## identify the closest blank line to the last last response value and treat 
    ## that as the delimiter for the end of the last response category
    blanks <- which(vbl == """")
    resps <- c(resps, blanks[min(which(blanks > max(resps)))])
    ## grab the frequencies and remove the last one because the last one should be blank
    freqs <- gsub(""^(\\d+).*"", ""\\1"", vbl[resps])
    ## thanks to use padding out resps with the blank line after the last response category
    freqs <- freqs[-length(freqs)]
    ## for each identified response, paste together the text between the identified response row 
    ## and everything that comes before the next identifies response row.
    vlabs <- sapply(1:(length(resps) - 1), function(j) {
      paste(vbl[resps[j]:(resps[(j + 1)] - 1)], collapse = "" "")
    })
    ## remove the frequencies and white space from the start of the variable labels
    ## trim the white space around variable labels as well
    vlabs <- trimws(gsub(""^\\d+\\s+(.*)"", ""\\1"", vlabs))
    ## collect all the information in one place
    out[[i]] <- data.frame(`var name` = varname, frequencies = freqs, `value labels` = vlabs)
  } else {
    out[[i]] <- data.frame(`var name` = character(0), frequencies = character(0), `value labels` = character(0))
  }
}","['r', 'text-mining', 'txt']",76435627,"here's an example.  comments through identify what each piece of code does.  my assumption is that the delisting rows of equals signs separate each variable.
rl <- readlines(textconnection(""==============================                                                
var v960922                                                                    
              numeric                                                         
                                                                              
         admin.48                                                             
                                                                              
         summary - post mode assignment and administration                    
         -----------------------------------------------------------          
                                                                              
              post mode in this variable refers to beginning mode             
              (question admin.47).                                            
                                                                              
        749      1.   assigned to personal, administered as                   
                      personal iw                                             
          7      2.   assigned to personal, administered as                   
                      telephone iw                                            
         28      3.   assigned to telephone, administered as                  
                      personal iw                                             
        750      4.   assigned to telephone, administered as                  
                      telephone iw                                            
                                                                              
                 0.   inap, no post iw                                        
                                                                              
============================== ""))

## trim the white space from the front and back of each string 
## this will put the frequencies as the first characters in their lines. 
rl <- trimws(rl)

## find the variable delimiters
delims <- grep(""=============================="", rl)

## initialize the output as a list
out <- vector(mode=""list"", length=length(delims)-1)

## loop over the delimiters
for(i in 1:(length(delims)-1)){
  ## find the text between adjacent delimiters and call that vbl
  vbl <- rl[(delims[i]+1):(delims[(i+1)]-1)]
  ## capture the varname as the stuff after ""var "" in the first row of vbl
  varname <- gsub(""var (.*)"", ""\\1"", vbl[1])
  ## identify the lines that start with a number 
  resps <- grep(""^\\d"", vbl)
  ## identify the closest blank line to the last last response value and treat 
  ## that as the delimiter for the end of the last response category
  blanks <- which(vbl == """")
  resps <- c(resps, blanks[min(which(blanks > max(resps)))])
  ## grab the frequencies and remove the last one because the last one should be blank 
  freqs <- gsub(""^(\\d+).*"", ""\\1"", vbl[resps])
  ## thanks to use padding out resps with the blank line after the last response category
  freqs <- freqs[-length(freqs)]
  ## for each identified response, paste together the text between the identified response row 
  ## and everything that comes before the next identifies response row. 
  vlabs <- sapply(1:(length(resps)-1), function(i){
    paste(vbl[resps[i]:(resps[(i+1)]-1)], collapse="" "")
  })
  ## remove the frequencies and white space from the start of the variable labels
  ## trim the white space around variable labels as well
  vlabs <- trimws(gsub(""^\\d+\\s+(.*)"", ""\\1"", vlabs))
  ## collect all the information in one place
  out[[i]] <- data.frame(`var name` = varname, 
                    frequencies = freqs, 
                    `value labels` = vlabs)  
  
}
## make all the variables into a markdown table
lapply(out, knitr::kable)
#> [[1]]
#> 
#> 
#> |var.name |frequencies |value.labels                                             |
#> |:--------|:-----------|:--------------------------------------------------------|
#> |v960922  |749         |1.   assigned to personal, administered as personal iw   |
#> |v960922  |7           |2.   assigned to personal, administered as telephone iw  |
#> |v960922  |28          |3.   assigned to telephone, administered as personal iw  |
#> |v960922  |750         |4.   assigned to telephone, administered as telephone iw |
#> |v960922  |0           |0.   inap, no post iw                                    |

created on 2023-06-08 with reprex v2.0.2",https://stackoverflow.com/questions/76435499,r,08-06-2023 20:31,157.0,0.0,2.0,True,15-06-2023 16:34,15-06-2023 16:34
73021827,bi-gram probability,"trying to find the probability of a phrase using bi-gram
filename.txt
# how many times bigram occurs
bg_count = bigrams.count(('word1', 'word2'))

# probabilty of bigram in text p(word1 word2)
bg_count/number_of_bigrams","['python', 'nlp', 'artificial-intelligence', 'probability', 'n-gram']",73026566,"in a bigram langauge model:
p(w1,w2,w3,...wn) = p(w1)*p(w2|w1)*p(w3|w2).....*p(wn-1|wn)
so p(life, might) = p(life)*p(might|life) where

p(life) = count(life)/number of unigrams
p(might|life) = count(life, might)/count(life)

code to calculate p(life might) using bigram model:
p_life = s.count(""life"")/len(s)
p_might_given_life = bigrams.count(('life', 'might'))/s.count('life')
p_life_might = p_life * p_might_given_life
print (p_life_might)

output:
0.0024752475247524753

log probabilities
since probabilities <=1 and it is not safe to multiply many small numbers, we usually use the log probabilities since it converts multiplication to additions. and since the log is a monotone increasing function, the comparison between different log probabilities will be the same as comparing the actual probabilities.
log(p_life_might) = log(p_life * p_might_given_life)
 = log(p_life) + log(p_might_given_life)
code:
print (math.log(p_life)+math.log(p_might_given_life))

output:
-6.0014148779611505",https://stackoverflow.com/questions/73021827,python,18-07-2022 11:46,342.0,-1.0,1.0,True,18-07-2022 18:44,18-07-2022 18:44
72929507,extracting mean number of words in verb phrases,"so i have a bit of a silly question, but being fairly new to python, i can't seem to find the answer to it myself. i extracted verb phrases using spacy's matcher. now, i'm hoping to get the mean number of words in the extracted verb phrases for each person's text and stored them in a new dataframe column. in order to do this, i'm trying to create a function that i'll then apply to said dataframe column.
i created this function:
def get_length_phrases(column):
    for phrase in column:
        phrase_length = len(phrase)
        mean_length = np.mean(phrase_length)
    return mean_length

the problem is, when i apply it to the column in which the verb phrases are stored, i get an output that looks like this :
0      1.0
1      1.0
2      1.0
3      1.0
4      1.0
      ... 
235    1.0
236    1.0
237    1.0
238    1.0
239    1.0
name: verb_phrases_length, length: 240, dtype: float64

the problem is, there is way more than one word per phrase, so clearly, i'm doing something wrong, but can't seem to figure out what... statistics.mean doesn't work either...","['python', 'spacy']",72929586,"np.mean() takes an array (or similar) as an argument. as far as i can tell (correct me if i'm wrong) you are getting the mean of the length of each phase, which is just one number, and the mean of one number will be that number.
from numpy docs:

parameters:
a:array_like -
array containing numbers whose mean is desired. if a is not an array, a conversion is attempted.

you are going to want to save each length to a list, and then give that to np.mean()
def get_length_phrases(column):
    phrase_lengths = []
    for phrase in column:
        phrase_lengths.append(len(phrase))
    mean_length = np.mean(phrase_lengths)
    return mean_length

if, at this point, you are still getting 1.0, it is likely an issue with getting the phrases, not this function.",https://stackoverflow.com/questions/72929507,python,10-07-2022 15:06,68.0,3.0,1.0,True,10-07-2022 15:53,10-07-2022 15:53
70546666,search for particular parts of speech (e.g. nouns) and print them along with a preceding word,"i have a text which is made up of a list of basic sentences, such as ""she is a doctor"", ""he is a good person"", and so forth. i'm trying to write a program which will return only the nouns and the preceding pronoun (e.g. she, he, it). i need them to print as a pair, for example (she, doctor) or (he, person). i'm using spacy as this will allow me to work with similar texts in french and german as well.
this is the closest thing i've found elsewhere on this site as to what i need. what i've been trying so far is to produce a list of nouns in the text and then search the text for nouns in the list, and print the noun and the word 3 places before it (since this is the pattern for most of the sentences, and most is good enough for my purposes). this is what i've got for creating the list:


def spacy_tag(text):
  text_open = codecs.open(text, encoding='latin1').read()
  parsed_text = nlp_en(text_open)
  tokens = list([(token, token.tag_) for token in parsed_text])
  list1 = []
  for token, token.tag_ in tokens:
    if token.tag_ == 'nn':
      list1.append(token)
  return(list1)



however, when i try to do anything with it, i get an error message. i've tried using enumerate but i couldn't get that to work either. this is the current code i have for searching the text for the words in the list (i haven't gotten around to adding the part which should print the word several places beforehand as i'm still stuck on the searching part):


def spacy_search(text, list):
  text_open = codecs.open(text, encoding='latin1').read()
  for word in text_open:
   if word in list:
     print(word)



the error i get is at line 4, ""if word in list:"", and it says ""typeerror: argument 'other' has incorrect type (expected spacy.tokens.token.token, got str)""
is there a more efficient way of printing a prp, nn pair using spacy? and alternatively, how can i amend my code to work so it searches the text for the nouns in the list? (it doesn't need to be a particularly elegant solution, it just needs to produce a result).","['python', 'spacy', 'pos-tagger']",70562037,"here is a clean way to implement your intended approach.
# put your nouns of interest here
noun_list = [""doctor"", ...]

def find_stuff(text):
    doc = nlp(text)
    if len(doc) < 4: return none # too short
    
    for tok in doc[3:]:
        if tok.pos_ == ""noun"" and tok.text in noun_list and doc[tok.i-3].pos_ == ""pron"":
            return (doc[tok.i-3].text, tok.text)

as the other answer mentioned, your approach here is wrong though. you want the subject and object (or predicate nominative) of the sentence. you should use the dependencymatcher for that. here's an example:
from spacy.matcher import dependencymatcher
import spacy

nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""she is a good person"")

pattern = [
  # anchor token: verb, usually ""is""
  {
    ""right_id"": ""verb"",
    ""right_attrs"": {""pos"": ""aux""}
  },
  # verb -> pronoun
  {
    ""left_id"": ""verb"",
    ""rel_op"": "">"",
    ""right_id"": ""pronoun"",
    ""right_attrs"": {""dep"": ""nsubj"", ""pos"": ""pron""}
  },
  # predicate nominatives have ""attr"" relation
  {
    ""left_id"": ""verb"",
    ""rel_op"": "">"",
    ""right_id"": ""target"",
    ""right_attrs"": {""dep"": ""attr"", ""pos"": ""noun""}
  }
]

matcher = dependencymatcher(nlp.vocab)
matcher.add(""prednom"", [pattern])
matches = matcher(doc)

for match_id, (verb, pron, target) in matches:
    print(doc[pron], doc[verb], doc[target])

you can check dependency relations using displacy. you can learn more about what they are in the jurafsky and martin book.",https://stackoverflow.com/questions/70546666,python,01-01-2022 02:17,602.0,1.0,2.0,True,03-01-2022 05:37,01-01-2022 22:54
70563462,is there a way to use pre-trained embedding with tf-idf in tensorflow?,"i am using the default and basic implementation of text classification as:

 

  tokenizer = tokenizer(num_words=vocab_size, filters = filters)
  tokenizer.fit_on_texts(list(train_x))
  train_x = tokenizer.texts_to_sequences(train_x)
  val_x = tokenizer.texts_to_sequences(val_x)
  train_x = pad_sequences(train_x, maxlen=maxlen)
  val_x = pad_sequences(val_x, maxlen=maxlen)

 def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32') # for loading embedding

  embeddings_index = dict(get_coefs(*o.split("" "")) for o in open(embedding_file))
  all_embs = np.stack(embeddings_index.values())
  emb_mean,emb_std = all_embs.mean(), all_embs.std()
  embed_dim = all_embs.shape[1]

  word_index = tokenizer.word_index
  vocab_size = min(vocab_size, len(word_index))

  embedding_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, embed_dim)) # vocab_size was nb_words
  for word, i in word_index.items():
      if i >= vocab_size: continue
      embedding_vector = embeddings_index.get(word)
      if embedding_vector is not none: embedding_matrix[i] = embedding_vector


it works fine but is there a way to texts_to_matrix, where there are options like binart, tfidf, count etc. is it possible that i can use them with existing embeddings?
one possible way could be to use a multiple input model and then concatenate two inputs at one place. apart from that, is there any?","['python', 'tensorflow', 'keras', 'deep-learning', 'nlp']",70565057,"the most common approach is to multiply each word vector by its corresponding tf_idf score. one often sees this approach in academic papers. you could do something like this:
create tfidf scores:
import tensorflow as tf
import numpy as np
import gensim.downloader as api
from sklearn.feature_extraction.text import tfidfvectorizer
import collections

def td_idf_word2weight(text):
    print(""creating tfidfvectorizer..."")
    tfidf = tfidfvectorizer(preprocessor=' '.join)
    tfidf.fit(text)

    # if a word was never seen - it is considered to be at least as infrequent as any of the known words
    max_idf = max(tfidf.idf_)
    return collections.defaultdict(
        lambda: max_idf,
        [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])

text = [['she let the balloon float up into the air with her hopes and dreams'],
        ['the old rusted farm equipment surrounded the house predicting its demise'],
        ['he was so preoccupied with whether or not he could that he failed to stop to consider if he should']]

td_idf = td_idf_word2weight(text)

text = np.concatenate(text)
tokenizer = tf.keras.preprocessing.text.tokenizer()
tokenizer.fit_on_texts(text)
text_sequences = tokenizer.texts_to_sequences(text)
text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences, padding='post')
vocab_size = len(tokenizer.word_index) + 1
print(td_idf.items())
print(vocab_size)

creating tfidfvectorizer...
dict_items([('she', 1.6931471805599454), ('let', 1.6931471805599454), ('the', 1.2876820724517808), ('balloon', 1.6931471805599454), ('float', 1.6931471805599454), ('up', 1.6931471805599454), ('into', 1.6931471805599454), ('air', 1.6931471805599454), ('with', 1.2876820724517808), ('her', 1.6931471805599454), ('hopes', 1.6931471805599454), ('and', 1.6931471805599454), ('dreams', 1.6931471805599454), ('old', 1.6931471805599454), ('rusted', 1.6931471805599454), ('farm', 1.6931471805599454), ('equipment', 1.6931471805599454), ('surrounded', 1.6931471805599454), ('house', 1.6931471805599454), ('predicting', 1.6931471805599454), ('its', 1.6931471805599454), ('demise', 1.6931471805599454), ('he', 1.6931471805599454), ('was', 1.6931471805599454), ('so', 1.6931471805599454), ('preoccupied', 1.6931471805599454), ('whether', 1.6931471805599454), ('or', 1.6931471805599454), ('not', 1.6931471805599454), ('could', 1.6931471805599454), ('that', 1.6931471805599454), ('failed', 1.6931471805599454), ('to', 1.6931471805599454), ('stop', 1.6931471805599454), ('consider', 1.6931471805599454), ('if', 1.6931471805599454), ('should', 1.6931471805599454)])
38

create tf_idf-weighted embeddings matrix:
model = api.load(""glove-twitter-25"")
embedding_dim = 25
weight_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in tokenizer.word_index.items():
  try:
    embedding_vector = model[word] * td_idf[word]
    weight_matrix[i] = embedding_vector 
  except keyerror:
    weight_matrix[i] = np.random.uniform(-5, 5, embedding_dim)
print(weight_matrix.shape)

(38, 25)",https://stackoverflow.com/questions/70563462,python,03-01-2022 08:59,1355.0,0.0,1.0,True,03-01-2022 13:52,03-01-2022 13:52
78840242,how to return used context to answer using langchain in python,"i have built a rag system like this:
loader = pypdfloader(pdf_file_name)
raw_documents = loader.load()

text_splitter = recursivecharactertextsplitter(chunk_size=1000, chunk_overlap=200)
documents = text_splitter.split_documents(raw_documents)
print(documents[-1])

document(
   metadata={'source': '/appraisal.pdf', 'page': 37},
   page_content='file no.\nproperty address\ncity county state zip code\nclient10828\nborrower or owner john smith & kitty smith\n29 dream st\ndreamtown sc 99999\nsouthern first bank\nbb appraisals, llc'
)

compressor = coherererank(
    top_n=top_n,
    model=""rerank-english-v3.0"",
    cohere_api_key=""""
)

retriever = vectorstore.as_retriever(
    search_type=""similarity"", 
    search_kwargs={""k"": top_n}
)

compression_retriever = contextualcompressionretriever(
    base_compressor=compressor, base_retriever=retriever
)

def format_docs(docs):
    return ""\n\n"".join(doc.page_content for doc in docs)

response_schemas = [
    responseschema(name=""price"", description=""price"", type=""float""),
    responseschema(name=""unit"", description=""unit"", type=""int""),
]
output_parser = structuredoutputparser.from_response_schemas(response_schemas)

rag_prompt = prompttemplate(
    input_variables=[""context"",""question""],
    template=template,
    partial_variables={""format_instructions"": output_parser.get_format_instructions()},
)

rag_chain = (
    {""context"": compression_retriever | format_docs, ""question"": runnablepassthrough()}
    | rag_prompt
    | llm
    | output_parser
)

query = ""what is the price? how many units?""

response = rag_chain.invoke(query, config={""configurable"": {""session_id"": ""abc123""}},)

but then my response is a json with my price and unit as keys only. and i would like to be able to have a ""context"" variable that stores the paragraphs used in my document that the algo relied upon to answer the questions.
any idea how i could do that please?","['python', 'langchain', 'large-language-model']",78840830,"there are two ways to do this

for pictorial rep. of the information, as to which document the llm used,  you would need to visit langchain_smith, you must also understand some methodologies in rag - like rag-fusion, this will help you create a rag-fusion used by the llm to get the documents it used to retrieve the information.

i am not sure about this, but you can try this function, pipe it with your compression_retriever chain. the idea of this function is for you to pass it along with the llm or the retriever as the case maybe, so as to make retrieving the docs easier


def unique_union_of_documents(docs: list) -> list[any]:
    """"""
    get the unique union of the documents
    args:
    docs: the documents to be processed

    returns:
    list: the unique union of the documents""""""

    doc_news = [json.dumps(doc.page_content) for _ in docs]
    # find the unique union of the documents
    unique_union = list(set(doc_news))

    return [json.loads(doc) for doc in unique_union]

you can now call this function after your variable response",https://stackoverflow.com/questions/78840242,python,06-08-2024 16:57,162.0,1.0,1.0,True,07-08-2024 08:43,07-08-2024 07:53
75119911,lstm named entity recognition model - shape are incompatible or logits/labels have different dimensions - tensorflow 2.9,"i am working on nlp lstm named entity extraction model but running into different errors below are more details about error. i am running this code in jupiter notebook
tensorflow version 2.9
both input and output are of length 50
input sentence : [123 88 170 221 132 52 105 32 211 91 126 211 24 221 134 154 221 162
215 80 144 101 61 136 68 133 40 200 133 40 218 131 139 199 124 74
184 92 213 185 221 221 221 221 221 221 221 221 221 221]
output sentece label: [ 7 7 7 7 0 7 6 2 7 5 1 7 7 7 7 7 7 7 7 10 7 7 7 7
3 8 7 3 8 7 7 7 7 7 7 7 7 6 2 7 7 7 7 7 7 7 7 7
7 7]
added upto 5 layers to train the model
here is the model:
model = tf.keras.sequential([

tf.keras.layers.embedding(num_words, 50, input_length=50),

tf.keras.layers.bidirectional(tf.keras.layers.lstm(64, return_sequences=true)),

tf.keras.layers.bidirectional(tf.keras.layers.lstm(32)),

tf.keras.layers.dropout(0.5),

tf.keras.layers.dense(64, activation=ï¿½ï¿½ï¿½reluï¿½ï¿½ï¿½),

tf.keras.layers.dense(num_tags, activation=ï¿½ï¿½ï¿½softmaxï¿½ï¿½ï¿½)
])

if i use loss function as ï¿½ï¿½ï¿½categorical_crossentropyï¿½ï¿½ï¿½ , i get this error:
valueerror: shapes (none, 50) and (none, 11) are incompatible
if i use loss function as ï¿½ï¿½ï¿½sparse_categorical_crossentropyï¿½ï¿½ï¿½ , i get this error:
logits and labels must have the same first dimension, got logits shape [13,11] and labels shape [650]
[[{{node sparse_categorical_crossentropy/sparsesoftmaxcrossentropywithlogits/sparsesoftmaxcrossentropywithlogits}}]]
i tried adding input shape as first layer but still no luck
tf.keras.layers.input(shape=(max_len,))
can anyone help , how to solve this. tried  different approaches but no luck
here is model summary
layer (type)                output shape              param #   
=============================================================            11100     
                                                                 
 bidirectional_35 (bidirecti  (none, 50, 128)          58880     
 onal)                                                           
                                                                 
 bidirectional_36 (bidirecti  (none, 64)               41216     
 onal)                                                           
                                                                 
 dropout_17 (dropout)        (none, 64)                0         
                                                                 
 dense_35 (dense)            (none, 64)                4160      
                                                                 
 dense_36 (dense)            (none, 11)                715       
                                                                 
=================================================================
total params: 116,071
trainable params: 116,071
non-trainable params: 0
_________________________________________________________________","['keras', 'nlp', 'lstm', 'tensorflow2.0', 'named-entity-recognition']",75127524,"i think you have a problem in 2 last dense layers. when run on a sequence of 50 numbers, you will get 'num_tags' numbers as output (11).
but you want to get 'num_tags' outputs at each step of the sequence, not at the end. to achieve this, you can use timedistributed layer:
tf.keras.layers.timedistributed(tf.keras.layers.dense(64, activation=ï¿½ï¿½ï¿½reluï¿½ï¿½ï¿½)),
tf.keras.layers.timedistributed(tf.keras.layers.dense(num_tags, activation=ï¿½ï¿½ï¿½softmaxï¿½ï¿½ï¿½))

then you can use ï¿½ï¿½ï¿½sparse_categorical_crossentropyï¿½ï¿½ï¿½ loss function since your labels are ints.",https://stackoverflow.com/questions/75119911,keras,14-01-2023 17:25,91.0,2.0,1.0,True,15-01-2023 18:40,14-01-2023 17:31
78697835,deepspeed : attributeerror: &#39;dummyoptim&#39; object has no attribute &#39;step&#39;,"i want to use deepspeed for training llms along with huggingface trainer. but when i use deepspeed  along with trainer i get error ""attributeerror: 'dummyoptim' object has no attribute 'step'"". below is my code
import argparse
import numpy as np
import torch
from datasets import load_dataset
from transformers import autotokenizer, automodelforcausallm

from trl import dpotrainer, dpoconfig
def preprocess_data(item):
    return {
        'prompt': 'instruct: ' + item['prompt'] + '\n',
        'chosen': 'output: ' + item['chosen'],
        'rejected': 'output: ' + item['rejected']
    }        

def main():
    parser = argparse.argumentparser()
    parser.add_argument(""--epochs"", type=int, default=1)
    parser.add_argument(""--beta"", type=float, default=0.1)
    parser.add_argument(""--batch_size"", type=int, default=4)
    parser.add_argument(""--lr"", type=float, default=1e-6)
    parser.add_argument(""--seed"", type=int, default=2003)
    parser.add_argument(""--model_name"", type=str, default=""eleutherai/pythia-14m"")
    parser.add_argument(""--dataset_name"", type=str, default=""jondurbin/truthy-dpo-v0.1"")
    parser.add_argument(""--local_rank"", type=int, default=0)

    args = parser.parse_args()

    # determine device based on local_rank
    device = torch.device(""cuda"", args.local_rank) if torch.cuda.is_available() else torch.device(""cpu"")


    tokenizer = autotokenizer.from_pretrained(args.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    model = automodelforcausallm.from_pretrained(args.model_name).to(device)
    ref_model = automodelforcausallm.from_pretrained(args.model_name).to(device)

    dataset = load_dataset(args.dataset_name, split=""train"")
    dataset = dataset.map(preprocess_data)

    # split the dataset into training and validation sets
    dataset = dataset.train_test_split(test_size=0.1, seed=args.seed)
    train_dataset = dataset['train']
    val_dataset = dataset['test']

    training_args = dpoconfig(
        learning_rate=args.lr,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        logging_steps=10,
        remove_unused_columns=false,
        max_length=1024,
        max_prompt_length=512,
        deepspeed=""ds_config.json""       
    )

    

    # verify and print embedding dimensions before finetuning
    print(""base model embedding dimension:"", model.config.hidden_size)

    model.train()
    ref_model.eval()

    dpo_trainer = dpotrainer(
        model,
        ref_model,
        beta=args.beta,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        args=training_args,
    )

    dpo_trainer.train()
    # evaluate
    evaluation_results = dpo_trainer.evaluate()
    print(""evaluation results:"", evaluation_results)

    save_model_name = 'finetuned_model'
    model.save_pretrained(save_model_name)

if __name__ == ""__main__"":
    main()

the config file used is the below one
{
""zero_optimization"": {
        ""stage"": 3,
        ""offload_optimizer"": {
            ""device"": ""cpu"",
            ""pin_memory"": true
        },
        ""offload_param"": {
            ""device"": ""cpu"",
            ""pin_memory"": true
        },
        ""overlap_comm"": true,
        ""contiguous_gradients"": true,
        ""sub_group_size"": 1e9,
        ""reduce_bucket_size"": ""auto"",
        ""stage3_prefetch_bucket_size"": ""auto"",
        ""stage3_param_persistence_threshold"": ""auto"",
        ""stage3_max_live_parameters"": 1e9,
        ""stage3_max_reuse_distance"": 1e9,
        ""stage3_gather_16bit_weights_on_model_save"": true
    },
""bf16"": {
    ""enabled"": ""auto""
},
""fp16"": {
    ""enabled"": ""auto"",
    ""loss_scale"": 0,
    ""initial_scale_power"": 32,
    ""loss_scale_window"": 1000,
    ""hysteresis"": 2,
    ""min_loss_scale"": 1
},

""gradient_accumulation_steps"": ""auto"",
""gradient_clipping"": ""auto"",
""train_batch_size"": ""auto"",
""train_micro_batch_size_per_gpu"": ""auto"",
""wall_clock_breakdown"": false,
""flops_profiler"": {
    ""enabled"": false,
    ""detailed"": false
},
""optimizer"": {
    ""type"": ""lamb"",
    ""params"": {
    ""lr"": ""auto"",
    ""betas"": [0.9, 0.999],
    ""eps"": ""auto"",
    ""weight_decay"": ""auto""
    }
},
""zero_allow_untested_optimizer"": true
}

the code works with out deepspeed. i have torch=2.3.1, deepspeed                 =0.14.5, trl=0.9.4 and cuda version: 12.5.
appreciate any hint on this !","['python', 'huggingface-transformers', 'large-language-model', 'huggingface-trainer', 'deepspeed']",78713256,"from accelerate.utils import distributedtype

training_args.distributed_state.distributed_type = distributedtype.deepspeed

adding this solves the issue",https://stackoverflow.com/questions/78697835,python,02-07-2024 14:53,635.0,0.0,1.0,True,08-07-2024 17:50,08-07-2024 17:50
74961297,failed to connect to tensorflow master: tpu worker may not be ready or tensorflow master address is incorrect,"i signed up for the tensor research cloud (trc) program for the third time in two years. now i barely created a preemptible v3-8 tpu. before that, i could efficiently allocate five non-preemptible v3-8 tpus. even with this allocation (preemptible and non-preemptible), the tpu is listed as ready and healthy. however, when i try to access it from the pretraining script, i run into this error that i have never encountered before:
failed to connect to the tensorflow master. the tpu worker may not be ready (still scheduling), or the tensorflow master address is incorrect.

i know that the tensorflow master address is correct, and i have checked that the tpu is healthy and ready. i have also double-checked that my code is correctly creating the tensorflow session and specifying the tpu address.
what could be causing this error message, and how can i troubleshoot and fix it?
i also tried this code from  note that i'm not using colab but using google cloud platform.
resolver = tf.distribute.cluster_resolver.tpuclusterresolver(tpu='pretrain-1')
tf.config.experimental_connect_to_cluster(resolver)
# this is the tpu initialization code that has to be at the beginning.
tf.tpu.experimental.initialize_tpu_system(resolver)
print(""all devices: "", tf.config.list_logical_devices('tpu'))

any i'm stuck at:
info:tensorflow:initializing the tpu system: pretrain-1

however, i expected something like this:
info:tensorflow:deallocate tpu buffers before initializing tpu system.
2022-12-20 13:08:56.187870: e tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuinit: cuda_error_no_device: no cuda-capable device is detected
info:tensorflow:deallocate tpu buffers before initializing tpu system.
info:tensorflow:initializing the tpu system: grpc://10.99.59.162:8470
info:tensorflow:initializing the tpu system: grpc://10.99.59.162:8470
info:tensorflow:finished initializing tpu system.
info:tensorflow:finished initializing tpu system.
all devices:  [logicaldevice(name='/job:worker/replica:0/task:0/device:tpu:0', device_type='tpu'), logicaldevice(name='/job:worker/replica:0/task:0/device:tpu:1', device_type='tpu'), logicaldevice(name='/job:worker/replica:0/task:0/device:tpu:2', device_type='tpu'), logicaldevice(name='/job:worker/replica:0/task:0/device:tpu:3', device_type='tpu'), logicaldevice(name='/job:worker/replica:0/task:0/device:tpu:4', device_type='tpu'), logicaldevice(name='/job:worker/replica:0/task:0/device:tpu:5', device_type='tpu'), logicaldevice(name='/job:worker/replica:0/task:0/device:tpu:6', device_type='tpu'), logicaldevice(name='/job:worker/replica:0/task:0/device:tpu:7', device_type='tpu')]

edit: i successfully accessed the tpu with the same configurations from a new tensor research cloud (trc) account. however, the problem is still ongoing with the previous trc account. i suspect it might be a problem with the google cloud platform (gcp) configuration.","['google-cloud-platform', 'google-compute-engine', 'bert-language-model', 'pre-trained-model', 'tpu']",74974689,"i solved the problem by deleting all tpus and vm instances and then disabling and reenabling all apis.
the issue might be related to the vpn connection to a gpu cluster during enabling services.",https://stackoverflow.com/questions/74961297,google-cloud-platform,30-12-2022 12:01,551.0,0.0,2.0,True,01-01-2023 12:48,01-01-2023 12:48
59858705,is it possible to add date picker in rasa chatbot?,"i am trying to make a chatbot using rasa for a website where the user will be asked their birth date like the photo i have attached.  
is it possible to add date picker in rasa? i have already read the documentation but confused about how to customize it for website.","['nlp', 'chatbot', 'rasa-nlu', 'rasa-core', 'rasa']",60169173,"date picker or other accessories are possible to add to rasa and you can include them to your assistant by specifying custom output payloads. however, for that to work, you will need the specific accessory to be implemented in your client first.
for example, slack has an block datepicker which can then be incorporated to rasa as follows:
responses:
  utter_take_bet:
  - custom:
      blocks:
      - type: section
        text:
          text: ""make a bet on when the world will end:""
          type: mrkdwn
        accessory:
          type: datepicker
          initial_date: '2019-05-21'
          placeholder:
            type: plain_text
            text: select a date

which chat widget are you using for integrating rasa on your website?",https://stackoverflow.com/questions/59858705,nlp,22-01-2020 11:27,1940.0,0.0,1.0,True,09-02-2023 06:19,06-02-2020 10:11
75879812,using a single api key or generating temporary keys for each user?,"i'm building a flutter app that requires the openai gpt3 api, and i'm not sure how to implement it. currently, i'm using a single api key that's stored in a .env file and accessed via the flutter_dotenv package. i'm wondering whether it's best to use this one api key for all users of the app, or whether i should implement an api gateway and generate temporary api keys for each user.
while i don't anticipate reaching the request limit for my single api key after i release the app, i'm uncertain about the best approach. what are the potential downsides to using a single api key for all users, and what are the benefits of generating temporary keys for each user? would an api gateway be necessary for my use case?","['flutter', 'openai-api', 'api-key']",75884936,"are you concerned about security or people abusing your tool and hurting your openai api limits?
security
if you are concerned about security, keep your api key secret, and make sure it does not leak to any frontend or public repo.
you can even use secret manager solutions like doppler, aws secret manager, or 1password for developers.
cost/api limits
you may want reduce the risk of someone harming your system and potentially costing you thousands of dollars by making a lot of api requests.
one solution is to track on your side how many calls are made over a period of time per user.
ex: one user can generate 15 completions per period of 24 hours.
if you offer a paying plan for your service, this is an incentive for people to upgrade.
to fight abuse, openai has also implemented end-user ids.
you can add a user parameter to your api requests. it will identify which user is responsible for which api call, and eventually, you can shut it down.
here you can read more in the doc.",https://stackoverflow.com/questions/75879812,flutter,29-03-2023 16:23,1442.0,-2.0,2.0,True,31-10-2024 11:30,31-10-2024 11:30
78104294,azureopenai upload a file from memory,"i am building an assistant and i would like to give it a dataset to analyze. i understand that i can upload a file that an assistant can use with the following code:
from openai import azureopenai
import pandas as pd

client = azureopenai(**credentials_here)

pd.dataframe({
    ""a"": [1, 2, 3, 4, 5],
    ""b"": [6, 7, 8, 9, 10],
    ""c"": [11, 12, 13, 14, 15],
}).to_csv('data.csv', index=false)

file = client.files.create(
    file=open(
        ""data.csv"",
        ""rb"",
    ),
    purpose=""assistants"",
)

i would prefer to upload the file from a data structure in memory. how can i upload a data from memory using the azureopenai client?
i read that openai allows users to provide bytes-like objects so i hoped i could do this with pickle.dumps
import pickle
df = pd.dataframe({
    ""a"": [1, 2, 3, 4, 5],
    ""b"": [6, 7, 8, 9, 10],
    ""c"": [11, 12, 13, 14, 15],
})

file = client.files.create(
    file=pickle.dumps(df),
    purpose=""assistants""
)

this snippet does not throw an error using the openai client. i get the below through the azureopenai client.
openai.badrequesterror: error code: 400 - {'error': {'message': ""invalid file format. supported formats: ['c', 'cpp', 'csv', 'docx', 'html', 'java', 'json', 'md', 'pdf', 'php', 'pptx', 'py', 'rb', 'tex', 'txt', 'css', 'jpeg', 'jpg', 'js', 'gif', 'png', 'tar', 'ts', 'xlsx', 'xml', 'zip']"", 'type': 'invalid_request_error', 'param': none, 'code': none}}","['python', 'openai-api', 'azure-openai', 'openai-assistants-api']",78110840,"it looks like azureopenai does accept bytes encoded objects from io.bytesio. so one easy way to do this for a dataframe is to use io.bytesio on the string representation of a dataframe.
import io
df = pd.dataframe({
    ""a"": [1, 2, 3, 4, 5],
    ""b"": [6, 7, 8, 9, 10],
    ""c"": [11, 12, 13, 14, 15],
})

in_memory_df = io.bytesio(df.to_csv().encode())

file = client.files.create(
    file=in_memory_df,
    purpose=""assistants""
)

tuples of (file_name, bytes_contents, file_type) are also accepted so this code snippet is also valid and more explicit.
file = client.files.create(
    file=('name_dataset_here.csv', in_memory_df, 'text/csv'),
    purpose=""assistants""
)",https://stackoverflow.com/questions/78104294,python,04-03-2024 22:06,650.0,0.0,1.0,True,05-03-2024 22:18,05-03-2024 13:29
69195950,problem with inputs when building a model with tfbertmodel and autotokenizer from huggingface&#39;s transformers,"i'm trying to build the model illustrated in this picture:

i obtained a pre-trained bert and respective tokenizer from huggingface's transformers in the following way:
from transformers import autotokenizer, tfbertmodel
model_name = ""dbmdz/bert-base-italian-xxl-cased""
tokenizer = autotokenizer.from_pretrained(model_name)
bert = tfbertmodel.from_pretrained(model_name)

the model will be fed a sequence of italian tweets and will need to determine if they are ironic or not.
i'm having problems building the initial part of the model, which takes the inputs and feeds them to the tokenizer in order to get a representation i can feed to bert.
i can do it outside of the model-building context:
my_phrase = ""ciao, come va?""
# an equivalent version is tokenizer(my_phrase, other parameters)
bert_input = tokenizer.encode(my_phrase, add_special_tokens=true, return_tensors='tf', max_length=110, padding='max_length', truncation=true) 
attention_mask = bert_input > 0
outputs = bert(bert_input, attention_mask)['pooler_output']

but i'm having troubles building a model that does this. here is the code for building such a model (the problem is in the first 4 lines ):
def build_classifier_model():
  text_input = tf.keras.layers.input(shape=(), dtype=tf.string, name='text')
  encoder_inputs = tokenizer(text_input, return_tensors='tf', add_special_tokens=true, max_length=110, padding='max_length', truncation=true)
  outputs = bert(encoder_inputs)
  net = outputs['pooler_output']
  
  x = tf.keras.layers.bidirectional(tf.keras.layers.lstm(64, return_sequences=true, dropout=0.1, recurrent_dropout=0.1))(net)
  x = tf.keras.layers.concatenate(axis=-1)([x, input_layer])
  x = tf.keras.layers.maxpooling1d(20)(x)
  x = tf.keras.layers.spatialdropout1d(0.4)(x)
  x = tf.keras.layers.flatten()(x)
  x = tf.keras.layers.dense(128, activation=""relu"")(x)
  x = tf.keras.layers.dropout(0.25)(x)
  x = tf.keras.layers.dense(2, activation='softmax')(x)

  model = tf.keras.model(inputs=text_input, outputs = x) 
  
  return model

and when i call the function for creating this model i get this error:

text input must of type str (single example), list[str] (batch or single pretokenized example) or list[list[str]] (batch of pretokenized examples).

one thing i thought was that maybe i had to use the tokenizer.batch_encode_plus function which works with lists of strings:
class bertpreprocessinglayer(tf.keras.layers.layer):
  def __init__(self, tokenizer, maxlength):
    super().__init__()
    self._tokenizer = tokenizer
    self._maxlength = maxlength
  
  def call(self, inputs):
    print(type(inputs))
    print(inputs)
    tokenized = tokenizer.batch_encode_plus(inputs, add_special_tokens=true, return_tensors='tf', max_length=self._maxlength, padding='max_length', truncation=true)
    return tokenized

def build_classifier_model():
  text_input = tf.keras.layers.input(shape=(), dtype=tf.string, name='text')
  encoder_inputs = bertpreprocessinglayer(tokenizer, 100)(text_input)
  outputs = bert(encoder_inputs)
  net = outputs['pooler_output']
  # ... same as above

but i get this error:

batch_text_or_text_pairs has to be a list (got <class 'keras.engine.keras_tensor.kerastensor'>)

and beside the fact i haven't found a way to convert that tensor to a list with a quick google search, it seems weird that i have to go in and out of tensorflow in this way.
i've also looked up on the huggingface's documentation but there is only a single usage example, with a single phrase, and what they do is analogous at my ""out of model-building context"" example.
edit:
i also tried with lambdas in this way:
tf.executing_eagerly()

def tokenize_tensor(tensor):
  t = tensor.numpy()
  t = np.array([str(s, 'utf-8') for s in t])
  return tokenizer(t.tolist(), return_tensors='tf', add_special_tokens=true, max_length=110, padding='max_length', truncation=true)

def build_classifier_model():
  text_input = tf.keras.layers.input(shape=(1,), dtype=tf.string, name='text')
  
  encoder_inputs = tf.keras.layers.lambda(tokenize_tensor, name='tokenize')(text_input)
  ...
  
  outputs = bert(encoder_inputs)

but i get the following error:

'tensor' object has no attribute 'numpy'

edit 2:
i also tried the approach suggested by @mdaoust of wrapping everything in a tf.py_function and got this error.
def py_func_tokenize_tensor(tensor):
  return tf.py_function(tokenize_tensor, [tensor], tout=[tf.int32, tf.int32, tf.int32])


eager_py_func() missing 1 required positional argument: 'tout'

then i defined tout as the type of the value returned by the tokenizer:
transformers.tokenization_utils_base.batchencoding
and got the following error:

expected datatype for argument 'tout' not <class
'transformers.tokenization_utils_base.batchencoding'>

finally i unpacked the value in the batchencoding in the following way:
def tokenize_tensor(tensor):
  t = tensor.numpy()
  t = np.array([str(s, 'utf-8') for s in t])
  dictionary = tokenizer(t.tolist(), return_tensors='tf', add_special_tokens=true, max_length=110, padding='max_length', truncation=true)
  #unpacking
  input_ids = dictionary['input_ids']
  tok_type = dictionary['token_type_ids']
  attention_mask = dictionary['attention_mask']
  return input_ids, tok_type, attention_mask

and get an error in the line below:
...
outputs = bert(encoder_inputs)


valueerror: cannot take the length of shape with unknown rank.","['tensorflow', 'keras', 'huggingface-transformers', 'bert-language-model', 'huggingface-tokenizers']",69336070,"for now i solved by taking the tokenization step out of the model:
def tokenize(sentences, tokenizer):
    input_ids, input_masks, input_segments = [],[],[]
    for sentence in sentences:
        inputs = tokenizer.encode_plus(sentence, add_special_tokens=true, max_length=128, pad_to_max_length=true, return_attention_mask=true, return_token_type_ids=true)
        input_ids.append(inputs['input_ids'])
        input_masks.append(inputs['attention_mask'])
        input_segments.append(inputs['token_type_ids'])        
        
    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')

the model takes two inputs which are the first two values returned by the tokenize funciton.
def build_classifier_model():
   input_ids_in = tf.keras.layers.input(shape=(128,), name='input_token', dtype='int32')
   input_masks_in = tf.keras.layers.input(shape=(128,), name='masked_token', dtype='int32') 

   embedding_layer = bert(input_ids_in, attention_mask=input_masks_in)[0]
...
   model = tf.keras.model(inputs=[input_ids_in, input_masks_in], outputs = x)

   for layer in model.layers[:3]:
     layer.trainable = false
   return model

i'd still like to know if someone has a solution which integrates the tokenization step inside the model-building context so that an user of the model can simply feed phrases to it to get a prediction or to train the model.",https://stackoverflow.com/questions/69195950,tensorflow,15-09-2021 15:28,6885.0,8.0,6.0,True,08-09-2022 11:11,26-09-2021 11:26
73975817,how do a put a different classifier on top of bertforsequenceclassification?,"i have a huggingface model:
model_name = 'bert-base-uncased'
model = bertforsequenceclassification.from_pretrained(model_name, num_labels=1).to(device)

how can i change the default classifier head? since it's only a single linearclassifier. i found this issue in the huggingface github which said:

you can also replace self.classifier with your own model.
model = bertforsequenceclassification.from_pretrained(""bert-base-multilingual-cased"")
model.classifier = new_classifier

where new_classifier is any pytorch model that you want.

however, i can't figure out how the structure of the new_classifier should look like (in particular the inputs and outputs so it can handle batches).","['machine-learning', 'pytorch', 'huggingface-transformers', 'huggingface']",73976116,"by looking at the source code of bertforsequenceclassification here, you can see that the classifier is simply a linear layer that project the bert output from hidden_size dimension to num_labels dimension. suppose you want to change the linear classifier to a two layer mlp with relu activation, you can do the following:
new_classifier = nn.sequential(
      nn.linear(config.hidden_size, config.hidden_size *2),
      nn.relu(),
      nn.linear(config.hidden_size * 2, config.num_labels)
    )
model.classifier = new_classifier

the requirement of the structure of your new classifier is its input dimension and output dimension need to be config.hidden_size dimension and config.num_labels accordingly. the structure of the classifier doesn't rely on the batch size, and module like nn.linear takes (*, h_dimension) dimension as input so you don't need to specify the batch size when creating the new classifier.",https://stackoverflow.com/questions/73975817,machine-learning,06-10-2022 14:47,384.0,0.0,1.0,True,06-10-2022 17:22,06-10-2022 17:22
76863889,how does one fix an interleaved data set from only sampling one data set?,"the following
from datasets import load_dataset
from datasets import interleave_datasets

# preprocess each dataset
c4 = load_dataset(""c4"", ""en"", split=""train"", streaming=true) 
wikitext = load_dataset(""wikitext"", ""wikitext-103-v1"", split=""train"", streaming=true)

# interleave the preprocessed datasets  
datasets = [c4, wikitext]
for dataset in datasets:
  print(dataset.description)
interleaved = interleave_datasets(datasets, probabilities=[0.5, 0.5])
print(interleaved)

only samples from one data set, why?
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
counts=100

colab: 

cross:

hf discord: 
hf discuss:","['python', 'huggingface-transformers', 'huggingface', 'huggingface-datasets']",76868861,"the interleave_datasets function works correctly here, it's your conclusion that is incorrect. what happens is that when two datasets are interleaved, their features are combined.
these are the features of c4 and wikitext:
print(c4.column_names)

>>> ['text', 'timestamp', 'url']

print(wikitext.column_names)

>>> ['text']

when you combine the datasets, all examples in the new dataset will have features ['text', 'timestamp', 'url'], even if they come from wikitext dataset. since wikitext dataset does not have features timestamp and url, these will be none.
dummy example:
from datasets import dataset, interleave_datasets
d1 = dataset.from_dict({
  'feature_1': ['a', 'b', 'c']
})
d2 = dataset.from_dict({
  'feature_2': [1, 2, 3]
})

dataset = interleave_datasets([d1, d2], probabilities=[0.5, 0.5], seed=42)
print('features:', dataset.column_names)

for e in dataset:
  print(e)

output:
features: ['feature_1', 'feature_2']
{'feature_1': none, 'feature_2': 1}
{'feature_1': 'a', 'feature_2': none}
{'feature_1': none, 'feature_2': 2}
{'feature_1': none, 'feature_2': 3}",https://stackoverflow.com/questions/76863889,python,09-08-2023 00:37,105.0,0.0,1.0,True,09-08-2023 14:40,09-08-2023 00:43
56653159,why is the value of tf-idf different from idf_?,"why is the value of the vectorized corpus different from the value obtained through the idf_ attribute? should not the idf_ attribute just return the inverse document frequency (idf) in the same way it appears in the corpus vectorized? 
from sklearn.feature_extraction.text import tfidfvectorizer
corpus = [""this is very strange"",
          ""this is very nice""]
vectorizer = tfidfvectorizer()
corpus = vectorizer.fit_transform(corpus)

print(corpus)

corpus vectorized:
  (0, 2)    0.6300993445179441
  (0, 4)    0.44832087319911734
  (0, 0)    0.44832087319911734
  (0, 3)    0.44832087319911734
  (1, 1)    0.6300993445179441
  (1, 4)    0.44832087319911734
  (1, 0)    0.44832087319911734
  (1, 3)    0.44832087319911734

vocabulary and idf_ values:
print(dict(zip(vectorizer.vocabulary_, vectorizer.idf_)))

output:
{'this': 1.0, 
 'is': 1.4054651081081644, 
 'very': 1.4054651081081644, 
 'strange': 1.0, 
 'nice': 1.0}

vocabulary index:
print(vectorizer.vocabulary_)

output:
{'this': 3, 
 'is': 0, 
 'very': 4, 
 'strange': 2, 
 'nice': 1}

why is the idf value of the word this is 0.44 in the corpus and 1.0 when obtained by idf_?","['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']",56653559,"this is because of l2 normalization, which is applied by default in tfidfvectorizer().
if you set the norm param as none, you will get the same values as idf_.

>>> vectorizer = tfidfvectorizer(norm=none)

#output

  (0, 2)    1.4054651081081644
  (0, 4)    1.0
  (0, 0)    1.0
  (0, 3)    1.0
  (1, 1)    1.4054651081081644
  (1, 4)    1.0
  (1, 0)    1.0
  (1, 3)    1.0

also, your way of computing the feature's corresponding idf values is wrong because dict does not preserve the order.
you could use the following method:
 >>>> print(dict(zip(vectorizer.get_feature_names(), vectorizer.idf_)))
      
     {'is': 1.0,
      'nice': 1.4054651081081644, 
      'strange': 1.4054651081081644, 
      'this': 1.0, 
      'very': 1.0}",https://stackoverflow.com/questions/56653159,python,18-06-2019 16:08,547.0,3.0,1.0,True,16-03-2023 06:32,18-06-2019 17:40
48573174,how to combine tfidf features with other features,"i have a classic nlp problem, i have to classify a news as fake or real.
i have created two sets of features:
a) bigram term frequency-inverse document frequency
b) approximately 20 features associated to each document obtained using pattern.en ( as subjectivity of the text, polarity, #stopwords, #verbs, #subject, relations grammaticals etc ...
which is the best way to combine the tfidf features with the other features for a single prediction?
thanks a lot to everyone.","['machine-learning', 'nlp', 'text-analysis']",48573688,"not sure if your asking technically how to combine two objects in code or what to do theoretically after so i will try and answer both.
technically your tfidf is just a matrix where the rows are records and the columns are features. as such to combine you can append your new features as columns to the end of the matrix. probably your matrix is a sparse matrix (from scipy) if you did this with sklearn so you will have to make sure your new features are a sparse matrix as well (or make the other dense).
that gives you your training data, in terms of what to do with it then it is a little more tricky. your features from a bigram frequency matrix will be sparse (im not talking data structures here i just mean that you will have a lot of 0s), and it will be binary. whilst your other data is dense and continuous. this will run in most machine learning algorithms as is although the prediction will probably be dominated by the dense variables. however with a bit of feature engineering i have built several classifiers in the past using tree ensambles that take a combination of term-frequency variables enriched with some other more dense variables and give boosted results (for example a classifier that looks at twitter profiles and classifies them as companies or people). usually i found better results when i could at least bin the dense variables into binary (or categorical and then hot encoded into binary) so that they didn't dominate.",https://stackoverflow.com/questions/48573174,machine-learning,01-02-2018 23:02,9771.0,13.0,2.0,True,11-07-2022 17:57,02-02-2018 01:55
73764895,can&#39;t run the spacy spancat (spancategorizer) model?,"i am trying to train the spancat model without luck.
i am getting:
valueerror: [e143] labels for component 'spancat' not initialized. this can be fixed by calling add_label, or by providing a representative batch of examples to the component's 'initialize' method.
i did convert my ner ents to spans:
def main(loc: path, lang: str, span_key: str):
    """"""
    set the ner data into the doc.spans, under a given key.
    the spancategorizer component uses the doc.spans, so that it can work with
    overlapping or nested annotations, which can't be represented on the
    per-token level.
    """"""
    nlp = spacy.blank(lang)
    docbin = docbin().from_disk(loc)
    docs = list(docbin.get_docs(nlp.vocab))
    for doc in docs:
        doc.spans[span_key] = list(doc.ents)
    docbin(docs=docs).to_disk(loc)

here is my config file:
[paths]
train = null
dev = null
vectors = null
init_tok2vec = null

[system]
gpu_allocator = null
seed = 444

[nlp]
lang = ""en""
pipeline = [""tok2vec"",""spancat""]
batch_size = 1000
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {""@tokenizers"":""spacy.tokenizer.v1""}

[components]

[components.spancat]
factory = ""spancat""
max_positive = null
scorer = {""@scorers"":""spacy.spancat_scorer.v1""}
spans_key = ""sc""
threshold = 0.5

[components.spancat.model]
@architectures = ""spacy.spancategorizer.v1""

[components.spancat.model.reducer]
@layers = ""spacy.mean_max_reducer.v1""
hidden_size = 128

[components.spancat.model.scorer]
@layers = ""spacy.linearlogistic.v1""
no = null
ni = null

[components.spancat.model.tok2vec]
@architectures = ""spacy.tok2veclistener.v1""
width = ${components.tok2vec.model.encode.width}
upstream = ""*""

[components.spancat.suggester]
@misc = ""spacy.ngram_suggester.v1""
sizes = [1,2,3]

[components.tok2vec]
factory = ""tok2vec""

[components.tok2vec.model]
@architectures = ""spacy.tok2vec.v2""

[components.tok2vec.model.embed]
@architectures = ""spacy.multihashembed.v2""
width = ${components.tok2vec.model.encode.width}
attrs = [""norm"",""prefix"",""suffix"",""shape""]
rows = [5000,1000,2500,2500]
include_static_vectors = true

[components.tok2vec.model.encode]
@architectures = ""spacy.maxoutwindowencoder.v2""
width = 256
depth = 8
window_size = 1
maxout_pieces = 3

[corpora]

[corpora.dev]
@readers = ""spacy.corpus.v1""
path = ${paths.dev}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[corpora.train]
@readers = ""spacy.corpus.v1""
path = ${paths.train}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[training]
dev_corpus = ""corpora.dev""
train_corpus = ""corpora.train""
max_epochs = 70
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_steps = 20000
eval_frequency = 200
frozen_components = []
annotating_components = []
before_to_disk = null

[training.batcher]
@batchers = ""spacy.batch_by_words.v1""
discard_oversize = false
tolerance = 0.2
get_length = null

[training.batcher.size]
@schedules = ""compounding.v1""
start = 100
stop = 1000
compound = 1.001
t = 0.0

[training.logger]
@loggers = ""spacy.consolelogger.v1""
progress_bar = false

[training.optimizer]
@optimizers = ""adam.v1""
beta1 = 0.9
beta2 = 0.999
l2_is_weight_decay = true
l2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001
learn_rate = 0.001

[training.score_weights]
spans_sc_f = 1.0
spans_sc_p = 0.0
spans_sc_r = 0.0

[pretraining]

[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}
vocab_data = null
lookups = null
before_init = null
after_init = null

[initialize.components]

[initialize.tokenizer]

i am using the ""sc"" key. please advise how to solve it.","['python-3.x', 'spacy', 'named-entity-recognition', 'spacy-3']",73784715,"i have solved it using the following function, but one should address the spans span(doc, start, end, label) according to the project/text for their task. it worked for me because all the text (a few words in my case) are labeled with a label and this is my need.
def convert_to_docbin(input, output_path=""./train.spacy"", lang='en'):
    """""" convert a pair of text annotations into docbin then save """"""
    # load a new spacy model:
    nlp = spacy.blank(lang)
    # create a docbin object:
    db = docbin()
    for text, annotations in input: # data in previous format
        doc = nlp(text)
        ents = []
        spans = []
        for start, end, label in annotations: # add character indexes
            spans.append(span(doc, 0, len(doc), label=label))
            span = doc.char_span(start, end, label=label)
            ents.append(span)
        doc.ents = ents # label the text with the ents
        group = spangroup(doc, name=""sc"", spans=spans)
        doc.spans[""sc""] = group
        db.add(doc)
    db.to_disk(output_path)",https://stackoverflow.com/questions/73764895,python-3.x,18-09-2022 17:16,663.0,1.0,1.0,True,20-09-2022 09:47,18-09-2022 17:50
56545363,how to find matching patterns in strings irrespective of order?,"i am trying to match patterns between two strings. for example, i have
pattern_search = ['education four year'] 
string1 = 'it is mandatory to have at least of four years of professional education'
string2 = 'need to have education four years with professional degree'

i am trying a way to say true when i try to find match between pattern_search and string1 & string2.
when i am using regex library match/search/findall doesn't help me. in string i have the all the words required but not in order, in string2 i have one extra word with added plural.
currently i am splitting the strings checking with each word in pattern_search with each word in string1 & 2 after preprocessing, is there any way to find match between the sentences?","['regex', 'python-3.x', 'string', 'nlp']",56546751,"you should take a nice look at the difflib library, specifically the get_close_matches function which returns words that are ""close enough"" to fill that requirement of words that may not exactly match. be sure to adjust your threshold (cutoff=) accordingly.
from difflib import get_close_matches
from re import sub

pattern_search = 'education four year'
string1 = 'it is mandatory to have at least of four years of professional education'
string2 = 'need to have education four years with professional degree'
string3 = 'we have four years of military experience'

def match(string, pattern):
  pattern = pattern.lower().split()
  words = set(sub(r""[^a-z0-9 ]"", """", string.lower()).split())  # sanitize input
  return all(get_close_matches(word, words, cutoff=0.8) for word in pattern)

print(match(string1, pattern_search))  # true
print(match(string2, pattern_search))  # true
print(match(string3, pattern_search))  # false

if you want to make pattern_search a list of patterns, then you should probably loop through the match function.",https://stackoverflow.com/questions/56545363,regex,11-06-2019 13:53,1147.0,3.0,3.0,True,19-05-2022 16:53,13-06-2019 13:01
77482126,"openai api error: &quot;you tried to access openai.model, but this is no longer supported in openai\&gt;=1.0.0&quot;","using visual studio code and pycharm, after install openai (pip install openai) a strange error is bugging me - please help.
if for example i write:
import openai

openai.api_key = ""sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""

lista_de_modelos = openai.model.list()
print(lista_de_modelos)

it fails and i get an this error:
ps c:\\proyectovs_python\> & ""c:/users/kitkatuser/appdata/local/programs/python/python312/python.exe"" ""c:/proyectovs_python/import os.py""
traceback (most recent call last):
file ""c:\\proyectovs_python\\import os.py"", line 5, in \<module\>
lista_de_modelos = openai.model.list()
^^^^^^^^^^^^^^^^^
file ""c:\\users\\kitkatuser\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\openai_utils_proxy.py"", line 22, in __getattr__
return getattr(self.__get_proxied__(), attr)
^^^^^^^^^^^^^^^^^^^^^^
file ""c:\\users\\kitkatuser\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\openai_utils_proxy.py"", line 43, in __get_proxied__  
return self.__load__()
^^^^^^^^^^^^^^^
file ""c:\\users\\kitkatuser\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\openai\\lib_old_api.py"", line 33, in __load__
raise apiremovedinv1(symbol=self.\_symbol)
openai.lib.\_old_api.apiremovedinv1:

you tried to access openai.model, but this is no longer supported in openai\>=1.0.0 - see the readme at  for the api.

you can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface.

alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

a detailed migration guide is available here: 

ps c:\\proyectovs_python\>

what iï¿½ï¿½m doing wrong? why i can't access openai, i've try several keys, the same program and process to install works well with other friends. using pycharm showmilar. i'm used several programs to try but always similar response! i don't find a solution or similar problems! i'm really confused!please help","['python', 'visual-studio-code', 'error-handling', 'configuration', 'openai-api']",77482219,"problem
the method name you're trying to use doesn't work with the openai python sdk version 1.0.0 or newer.
the old sdk (i.e., version 0.28) works with the following method name:
client.model.list

the new sdk (i.e., version 1.0.0 or newer) works with the following method name:
client.models.list

note: be careful because the api is case-sensitive (i.e., client.models.list will not work with the new sdk version).
solution
try this:
import os
from openai import openai
client = openai()
openai.api_key = os.getenv('openai_api_key')

client.models.list()",https://stackoverflow.com/questions/77482126,python,14-11-2023 16:06,3099.0,0.0,2.0,True,22-06-2024 00:40,22-11-2023 16:56
74432598,typeerror: openai__webpack_imported_module_1__.openaiapi is not a constructor,"i'm trying to recreate an idea using openai api with react.
i'm using the official openai documentation here but got stuck and i'm hoping someone is able to help.
the idea is to have an simple text-input and a button on a page. reciving the user's prompt and sending the information to the openai api.
onclick to the button i'm handling everything within the handlesubmit function which looks like this:
const config = new configuration({
    apikey: ""api_key_is_here"",
})

const handlesubmit = async (e) => {
    e.preventdefault()
    setstate('loading...')
    
    const openai = new openaiapi(config)
    const res = await openai.createimage({
      prompt: prompt,
      n: 1,
      size: ""256ï¿½ï¿½256"",
    })

    const url = res.data.data[0].url
    console.log(url)
    console.log('clicked: ' + prompt)
}

which clicking the button and calling the function handlesubmit now i'm getting an error
uncaught typeerror: openai__webpack_imported_module_1__.openaiapi is not a constructor

besides putting api key directly into the source code instead of env variables (testing and playing around locally) nothing seems out of the ordernary to me.
thank you for any input!
edit:
imports:
import { useeffect, usestate } from ""react""
import { configuration, openaiapi } from ""openai""

versions:
""openai"": ""^3.1.0""
""react"": ""^18.2.0""","['reactjs', 'openai-api']",74432688,"it could be webpack complaining about how openai package is imported.
could you show us the lines of code that import.
also it seems from a glance at the documentation that it is supposed to be:
openaiapi and not openaiapi

case matters :d",https://stackoverflow.com/questions/74432598,reactjs,14-11-2022 13:42,3120.0,0.0,1.0,True,25-11-2023 03:53,14-11-2022 13:50
78712265,problems retrieving the embeddings data form openai api batch embedding job,"i have to embed over 300,000 products description for a multi-classification project. i split the descriptions onto chunks of 34,337 descriptions to be under the batch embeddings limit size.
a sample of my jsonl file for batch processing:
{""custom_id"": ""request-0"", ""method"": ""post"", ""url"": ""/v1/embeddings"", ""body"": {""model"": ""text-embedding-ada-002"", ""input"": ""base l\u00edquida maybelline superstay 24 horas full coverage cor 220 natural beige 30ml"", ""encoding_format"": ""float""}}
{""custom_id"": ""request-1"", ""method"": ""post"", ""url"": ""/v1/embeddings"", ""body"": {""model"": ""text-embedding-ada-002"", ""input"": ""sand\u00e1lia havaianas top animals cinza/gelo 39/40"", ""encoding_format"": ""float""}}

my jsonl file has 34,337 lines.
i've susscesfully uploaded the file:
file 'batch_emb_file_1.jsonl' uploaded succesfully:
 fileobject(id='redacted for work compliance', bytes=6663946, created_at=1720128016, filename='batch_emb_file_1.jsonl', object='file', purpose='batch', status='processed', status_details=none)

and ran the embedding job:
batch job created successfully:
 batch(id='redacted for work compliance', completion_window='24h', created_at=1720129886, endpoint='/v1/embeddings', input_file_id='redacted for work compliance', object='batch', status='validating', cancelled_at=none, cancelling_at=none, completed_at=none, error_file_id=none, errors=none, expired_at=none, expires_at=1720216286, failed_at=none, finalizing_at=none, in_progress_at=none, metadata={'description': 'batch job for embedding large quantity of product descriptions', 'initiated_by': 'marcio', 'project': 'product classification', 'date': '2024-07-04 21:51', 'comments': 'this is the 1 batch job of embeddings'}, output_file_id=none, request_counts=batchrequestcounts(completed=0, failed=0, total=0))

the work was completed:
client.batches.retrieve(batch_job_1.id).status
'completed'

client.batches.retrieve('redacted for work compliance'), returns:
batch(id='redacted for work compliance', completion_window='24h', created_at=1720129886, endpoint='/v1/embeddings', input_file_id='redacted for work compliance', object='batch', status='completed', cancelled_at=none, cancelling_at=none, completed_at=1720135956, error_file_id=none, errors=none, expired_at=none, expires_at=1720216286, failed_at=none, finalizing_at=1720133521, in_progress_at=1720129903, metadata={'description': 'batch job for embedding large quantity of product descriptions', 'initiated_by': 'marcio', 'project': 'product classification', 'date': '2024-07-04 21:51', 'comments': 'this is the 1 batch job of embeddings'}, output_file_id='redacted for work compliance', request_counts=batchrequestcounts(completed=34337, failed=0, total=34337))

but when i try to get the content using output_file_id string
client.files.content(value of output_file_id), returns:
<openai._legacy_response. at 0x79ae81ec7d90>

i have tried:
client.files.content(value of output_file_id).content but this kills my kernel
what am i doing wrong? also i believe i am under utilizing batch embeddings. the 90,000 limits conflicts with batch queue limit of 'text-embedding-ada-002' model which is: 3,000,000
could someone help?","['batch-processing', 'openai-api', 'openaiembeddings']",78752532,"retrieving the embedding data from batch file is a bit trick, this tutorial breaks it down set by set link
after getting the output_file_id, you need to:
output_file =client.files.content(output_files_id).text

embedding_results = []
for line in output_file.split('\n')[:-1]:
            data =json.loads(line)
            custom_id = data.get('custom_id')
            embedding = data['response']['body']['data'][0]['embedding']
            embedding_results.append([custom_id, embedding])


embedding_results = pd.dataframe(embedding_results, columns=['custom_id', 'embedding'])

in my case, this retrieves the embedding data from the batch job file",https://stackoverflow.com/questions/78712265,batch-processing,05-07-2024 15:35,530.0,0.0,1.0,True,16-07-2024 02:57,05-07-2024 15:43
74192948,attributeerror: &#39;list&#39; object has no attribute &#39;ents&#39; in building ner using bert,"i'm trying to build a ner model using bert-base-ner for a tweets dataset and ending up getting this error . please help
this is what i have done
from transformers import autotokenizer, automodelfortokenclassification
from transformers import pipeline

tokenizer = autotokenizer.from_pretrained(""dslim/bert-base-ner"")
model = automodelfortokenclassification.from_pretrained(""dslim/bert-base-ner"")

nlp = pipeline(""ner"", model=model, tokenizer=tokenizer)

# ---------

def all_ents(v):
        return [(ent.text, ent.label_) for ent in nlp(v).ents]

df1['entities'] = df['text'].apply(lambda v: all_ents(v))

df1.head()

attributeerror: 'list' object has no attribute 'ents'

thank you for the help","['python', 'pandas', 'bert-language-model', 'named-entity-recognition']",74196722,"it seems you mix code from different modules.
.ents exists in module spacy but not in transformers
#import spacy
import en_core_web_sm

nlp = en_core_web_sm.load()

doc = nlp('hello world of python. have a nice day')

print([(x.text, x.label_) for x in doc.ents])

in transformers you should use directly nlp(v) but it gives directory with ent[""entity""], ent[""score""], ent[""index""], ent[""word""], ent[""start""], ent[""end""]
from transformers import autotokenizer, automodelfortokenclassification
from transformers import pipeline

tokenizer = autotokenizer.from_pretrained(""dslim/bert-base-ner"")
model = automodelfortokenclassification.from_pretrained(""dslim/bert-base-ner"")

nlp = pipeline(""ner"", model=model, tokenizer=tokenizer)

# ---------

import pandas as pd

df = pd.dataframe({
    'text': ['hello world of python. have a nice day']
})

# ---------

def all_ents(v):
    #print(nlp(v))
    return [(ent['word'], ent['entity']) for ent in nlp(v)]

df['entities'] = df['text'].apply(all_ents)

#df1['entities'] = df['text'].apply(lambda v: [(ent['word'], ent['entity']) for ent in nlp(v)])

print(df['entities'].head())",https://stackoverflow.com/questions/74192948,python,25-10-2022 10:53,301.0,0.0,1.0,True,25-10-2022 16:18,25-10-2022 16:18
69609401,suppress huggingface logging warning: &quot;setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.&quot;,"in huggingface, every time i call a pipeline() object, i get a warning:
`""setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.""

how do i suppress this warning without suppressing all logging warnings? i want other warnings, but i don't want this one.","['huggingface-transformers', 'huggingface-tokenizers']",71397707,"the warning comes for any text generation task done by huggingface. this is explained here, and you can see the code here.
avoid that warning by manually setting the pad_token_id (e.g., to match the tokenizer or the eos_token_id).
set the pad_token_id in the generation_config with:
model.generation_config.pad_token_id = tokenizer.pad_token_id

alternatively, if you only need to make a single call to generate:

when you call
model.generate(**encoded_input)

just change it to
model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id)",https://stackoverflow.com/questions/69609401,huggingface-transformers,17-10-2021 23:40,96819.0,60.0,5.0,True,16-12-2024 00:11,18-10-2021 08:05
78439451,create_sql_agent with azureopenai?,"i have put together a script that works just fine using openai api. i am now trying to switch it over to azureopenai yet it seems i am running into an issue with the create_sql_agent(). can you use create_sql_agent with azureopenai model gpt-35-turbo-1106? could it be an issue with my api_version within azureopenai()? the error i receive is ""typeerror: completions. create() got an unexpected keyword argument 'tools'"" which i think could also be the option using 'openai-tools' as my agent_type?
code
import os
from langchain_openai import azureopenai
from langchain.agents import create_sql_agent
from langchain.agents.agent_toolkits import sqldatabasetoolkit
from langchain.sql_database import sqldatabase
from dotenv import load_dotenv
from langchain.agents import agentexecutor

from langchain_core.prompts.chat import (
    chatprompttemplate,
    humanmessageprompttemplate,
    systemmessageprompttemplate,
    aimessageprompttemplate,
    messagesplaceholder,
)

path = (os.getcwd()+'\creds.env')

load_dotenv(path)  

db = sqldatabase.from_uri(
    f""postgresql://{os.environ.get('user')}:{os.environ.get('password')}@{os.environ.get('host')}:{os.environ.get('port')}/{os.environ.get('database')}"")

llm = azureopenai(azure_endpoint=my_endpoint,
                  deployment_name=my_deployment_name,
                  model_name='gpt-35-turbo', # should it be 'gpt-35-turbo-1106'?
                 temperature = 0,
                 api_key = my_key,
                 api_version = '2023-07-01-preview') #my api_version correct? uncertain which one

toolkit = sqldatabasetoolkit(db=db, llm=llm)

prefix = """"""
you are an agent designed to interact with a sql database.
given an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.
unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.
you can order the results by a relevant column to return the most interesting examples in the database.
never query for all the columns from a specific table, only ask for the relevant columns given the question.
you have access to tools for interacting with the database.
only use the below tools. only use the information returned by the below tools to construct your final answer.
you must double-check your query before executing it. if you get an error while executing a query, rewrite the query and try again.

do not make any dml statements (insert, update, delete, drop, cascade, etc.) to the database.

if the question does not seem related to the database, just return ""i don't know"" as the answer.

if asked about a person do not return an 'id' but return a first name and last name.

""""""

suffix = """""" i should look at the tables in the database to see what i can query.  then i should query the schema of the most relevant tables.
""""""

messages = [
                systemmessageprompttemplate.from_template(prefix),
                humanmessageprompttemplate.from_template(""{input}""),
                aimessageprompttemplate.from_template(suffix),
                messagesplaceholder(variable_name=""agent_scratchpad""),
            ]


agent_executor = create_sql_agent(llm,
                                  toolkit=toolkit,
                                  agent_type='openai-tools', #does this work with azure?
                                  prompt=prompt,
                                  verbose=false)


print(agent_executor.invoke(""what are the names of the tables""))

error
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
cell in[69], line 1
----> 1 print(agent_executor.invoke(""what are the names of the tables""))

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain\chains\base.py:163, in chain.invoke(self, input, config, **kwargs)
    161 except baseexception as e:
    162     run_manager.on_chain_error(e)
--> 163     raise e
    164 run_manager.on_chain_end(outputs)
    166 if include_run_info:

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain\chains\base.py:153, in chain.invoke(self, input, config, **kwargs)
    150 try:
    151     self._validate_inputs(inputs)
    152     outputs = (
--> 153         self._call(inputs, run_manager=run_manager)
    154         if new_arg_supported
    155         else self._call(inputs)
    156     )
    158     final_outputs: dict[str, any] = self.prep_outputs(
    159         inputs, outputs, return_only_outputs
    160     )
    161 except baseexception as e:

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain\agents\agent.py:1432, in agentexecutor._call(self, inputs, run_manager)
   1430 # we now enter the agent loop (until it returns something).
   1431 while self._should_continue(iterations, time_elapsed):
-> 1432     next_step_output = self._take_next_step(
   1433         name_to_tool_map,
   1434         color_mapping,
   1435         inputs,
   1436         intermediate_steps,
   1437         run_manager=run_manager,
   1438     )
   1439     if isinstance(next_step_output, agentfinish):
   1440         return self._return(
   1441             next_step_output, intermediate_steps, run_manager=run_manager
   1442         )

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain\agents\agent.py:1138, in agentexecutor._take_next_step(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)
   1129 def _take_next_step(
   1130     self,
   1131     name_to_tool_map: dict[str, basetool],
   (...)
   1135     run_manager: optional[callbackmanagerforchainrun] = none,
   1136 ) -> union[agentfinish, list[tuple[agentaction, str]]]:
   1137     return self._consume_next_step(
-> 1138         [
   1139             a
   1140             for a in self._iter_next_step(
   1141                 name_to_tool_map,
   1142                 color_mapping,
   1143                 inputs,
   1144                 intermediate_steps,
   1145                 run_manager,
   1146             )
   1147         ]
   1148     )

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain\agents\agent.py:1138, in <listcomp>(.0)
   1129 def _take_next_step(
   1130     self,
   1131     name_to_tool_map: dict[str, basetool],
   (...)
   1135     run_manager: optional[callbackmanagerforchainrun] = none,
   1136 ) -> union[agentfinish, list[tuple[agentaction, str]]]:
   1137     return self._consume_next_step(
-> 1138         [
   1139             a
   1140             for a in self._iter_next_step(
   1141                 name_to_tool_map,
   1142                 color_mapping,
   1143                 inputs,
   1144                 intermediate_steps,
   1145                 run_manager,
   1146             )
   1147         ]
   1148     )

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain\agents\agent.py:1166, in agentexecutor._iter_next_step(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)
   1163     intermediate_steps = self._prepare_intermediate_steps(intermediate_steps)
   1165     # call the llm to see what to do.
-> 1166     output = self.agent.plan(
   1167         intermediate_steps,
   1168         callbacks=run_manager.get_child() if run_manager else none,
   1169         **inputs,
   1170     )
   1171 except outputparserexception as e:
   1172     if isinstance(self.handle_parsing_errors, bool):

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain\agents\agent.py:514, in runnablemultiactionagent.plan(self, intermediate_steps, callbacks, **kwargs)
    506 final_output: any = none
    507 if self.stream_runnable:
    508     # use streaming to make sure that the underlying llm is invoked in a
    509     # streaming
   (...)
    512     # because the response from the plan is not a generator, we need to
    513     # accumulate the output into final output and return that.
--> 514     for chunk in self.runnable.stream(inputs, config={""callbacks"": callbacks}):
    515         if final_output is none:
    516             final_output = chunk

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\runnables\base.py:2875, in runnablesequence.stream(self, input, config, **kwargs)
   2869 def stream(
   2870     self,
   2871     input: input,
   2872     config: optional[runnableconfig] = none,
   2873     **kwargs: optional[any],
   2874 ) -> iterator[output]:
-> 2875     yield from self.transform(iter([input]), config, **kwargs)

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\runnables\base.py:2862, in runnablesequence.transform(self, input, config, **kwargs)
   2856 def transform(
   2857     self,
   2858     input: iterator[input],
   2859     config: optional[runnableconfig] = none,
   2860     **kwargs: optional[any],
   2861 ) -> iterator[output]:
-> 2862     yield from self._transform_stream_with_config(
   2863         input,
   2864         self._transform,
   2865         patch_config(config, run_name=(config or {}).get(""run_name"") or self.name),
   2866         **kwargs,
   2867     )

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\runnables\base.py:1880, in runnable._transform_stream_with_config(self, input, transformer, config, run_type, **kwargs)
   1878 try:
   1879     while true:
-> 1880         chunk: output = context.run(next, iterator)  # type: ignore
   1881         yield chunk
   1882         if final_output_supported:

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\runnables\base.py:2826, in runnablesequence._transform(self, input, run_manager, config)
   2817 for step in steps:
   2818     final_pipeline = step.transform(
   2819         final_pipeline,
   2820         patch_config(
   (...)
   2823         ),
   2824     )
-> 2826 for output in final_pipeline:
   2827     yield output

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\runnables\base.py:1283, in runnable.transform(self, input, config, **kwargs)
   1280 final: input
   1281 got_first_val = false
-> 1283 for chunk in input:
   1284     if not got_first_val:
   1285         final = adapt_first_streaming_chunk(chunk)  # type: ignore

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\runnables\base.py:4728, in runnablebindingbase.transform(self, input, config, **kwargs)
   4722 def transform(
   4723     self,
   4724     input: iterator[input],
   4725     config: optional[runnableconfig] = none,
   4726     **kwargs: any,
   4727 ) -> iterator[output]:
-> 4728     yield from self.bound.transform(
   4729         input,
   4730         self._merge_configs(config),
   4731         **{**self.kwargs, **kwargs},
   4732     )

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\runnables\base.py:1300, in runnable.transform(self, input, config, **kwargs)
   1293             raise typeerror(
   1294                 f""failed while trying to add together ""
   1295                 f""type {type(final)} and {type(chunk)}.""
   1296                 f""these types should be addable for transform to work.""
   1297             )
   1299 if got_first_val:
-> 1300     yield from self.stream(final, config, **kwargs)

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\language_models\llms.py:458, in basellm.stream(self, input, config, stop, **kwargs)
    451 except baseexception as e:
    452     run_manager.on_llm_error(
    453         e,
    454         response=llmresult(
    455             generations=[[generation]] if generation else []
    456         ),
    457     )
--> 458     raise e
    459 else:
    460     run_manager.on_llm_end(llmresult(generations=[[generation]]))

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_core\language_models\llms.py:442, in basellm.stream(self, input, config, stop, **kwargs)
    440 generation: optional[generationchunk] = none
    441 try:
--> 442     for chunk in self._stream(
    443         prompt, stop=stop, run_manager=run_manager, **kwargs
    444     ):
    445         yield chunk.text
    446         if generation is none:

file ~\appdata\local\programs\python\python311\lib\site-packages\langchain_openai\llms\base.py:262, in baseopenai._stream(self, prompt, stop, run_manager, **kwargs)
    260 params = {**self._invocation_params, **kwargs, ""stream"": true}
    261 self.get_sub_prompts(params, [prompt], stop)  # this mutates params
--> 262 for stream_resp in self.client.create(prompt=prompt, **params):
    263     if not isinstance(stream_resp, dict):
    264         stream_resp = stream_resp.model_dump()

file ~\appdata\local\programs\python\python311\lib\site-packages\openai\_utils\_utils.py:277, in required_args.<locals>.inner.<locals>.wrapper(*args, **kwargs)
    275             msg = f""missing required argument: {quote(missing[0])}""
    276     raise typeerror(msg)
--> 277 return func(*args, **kwargs)

typeerror: completions.create() got an unexpected keyword argument 'tools'","['python', 'azure', 'openai-api', 'langchain', 'azure-openai']",78440581,"your model name and api version should be fine. however, you should be using the chat model type. the azurechatopenai class is used for this.
update your code:
from langchain.chat_models import azurechatopenai

# ...

llm = azurechatopenai(azure_endpoint=my_endpoint,
                  deployment_name=my_deployment_name,
                  model_name='gpt-35-turbo',
                  temperature = 0,
                  api_key = my_key,
                  api_version = '2023-07-01-preview')

when you create the sql agent, use the agenttype enumerator, and zero shot to tell the agent not to use memory.
from langchain.agents import agenttype, create_sql_agent

# ...

agent_executor = create_sql_agent(llm=llm,
                                  toolkit=toolkit,
                                  agent_type=agenttype.zero_shot_react_description,
                                  prompt=prompt,
                                  verbose=false)",https://stackoverflow.com/questions/78439451,python,06-05-2024 23:03,952.0,1.0,1.0,True,07-05-2024 06:45,07-05-2024 02:58
74914461,"spacy add_alias, typeerror","mwe
from spacy.kb import knowledgebase
import spacy 


#kb.add_entity already called.
nlp = spacy.blank(""en"")
kb = knowledgebase(vocab=nlp.vocab, entity_vector_length=96)
name = ""test""
qid = 1 # type(qid) => int
kb.add_alias(alias=name.lower(), entities=[qid], probabilities=[1])

produces the error at the last line: typeerror: an integer is required
a previous so post suggested that the same error arose in another context (importing spacy) because the version of srsly was greater than 2. using their solution of downgrading to v1.0.1 of srsly merely switched the error to module srsly has no attribute read_yaml.
i am using spacy 3.4.4 and srsly 2.4.5.
update
a fuller stack trace points to line 228 in spacy/kb.pyx:
  for entity, prob in zip(entities, probabilities):
            entity_hash = self.vocab.strings[entity] #this gives the error
            if not entity_hash in self._entry_index:
                raise valueerror(errors.e134.format(entity=entity))

            entry_index = <int64_t>self._entry_index.get(entity_hash)
            entry_indices.push_back(int(entry_index))
            probs.push_back(float(prob))","['python', 'cython', 'spacy', 'spacy-3']",74960152,"that looks like a bug. in the api docs knowledgebase.add_alias has type iterable[union[str, int]] for entities but the code above (the actual error is actually one line below) only works for str and not int values. (the marked line should have self.vocab.strings.as_int(entity).)
that said, the value 1 is probably not going to be the right value here no matter what and the simplest solution is to use strings instead like ""1"" or ""q1"", which should currently work as expected. you also need to add the entity before adding aliases (this snippet is not going to work even with a string value).",https://stackoverflow.com/questions/74914461,python,25-12-2022 16:14,133.0,0.0,1.0,True,31-12-2022 11:33,31-12-2022 11:33
77340227,add log entry on model export actions,"i have enabled log entries in the django admin
class customlogentryadmin(admin.modeladmin):
    list_display = [
        ""action_time"",
        ""user"",
        ""content_type"",
        ""object_id"",
        ""object_repr"",
        ""action_flag"",
        ""change_message"",
    ]
    # list filter
    list_filter = [""action_time"", ""user"", ""content_type""]

    # search
    search_fields = [""user__username"", ""object_repr""]


admin.site.register(logentry, customlogentryadmin)

and i have another model whose admin.py code is like this
class regadmin(exportactionmixin, admin.modeladmin):
    resource_class = regadminresource

    def has_view_permission(self, request, obj=none):
        return true

    def has_module_permission(self, request):
        return true


by default all change, addition and deletion entries are logged but i also want to log an entry when any export action is performed on it. chatgpt suggests that i should something like this
    # in admin class 
    def export_action(self, request, *args, **kwargs):
        # log the export action
        logentry.objects.create(
            user_id=request.user.id,
            content_type_id=contenttype.objects.get_for_model(self.model).id,
            object_id=none,
            object_repr=str(self.model),
            action_flag=1,  # assuming 1 stands for the action flag of 'change'
            change_message=""export action triggered."",
        )
        return super().export_action(request, *args, **kwargs)

but this function is not triggered when an export action is performed. i confirmed by adding a print statement.
how should i do this?","['python', 'django', 'import', 'export-to-csv', 'openai-api']",77413332,"you are quite close, the only problem is that the actions have reference to the one defined in the exportactionmixin, so you can override whatever you want, it is not going to refer to your method. you can however override that to to get the right action, so:
from django.utils.translation import gettext as _


class regadmin(exportactionmixin, admin.modeladmin):
    # …
    
    def get_actions(self, request):
        actions = super().get_actions(request)
        actions.update(
            export_admin_action=(
                regadmin.export_admin_action,
                'export_admin_action',
                _('export selected %(verbose_name_plural)s'),
            )
        )
        return actions
update: i've opened a pull requestï¿½ï¿½[github] to fix this behavior.<",https://stackoverflow.com/questions/77340227,python,22-10-2023 13:54,117.0,1.0,1.0,True,26-12-2023 14:30,26-12-2023 14:30
75849546,problem tokenizing with huggingface&#39;s library when fine tuning bloom,"i have a problem with my tokenizer function. to be honest i am quiet lost, since i do not really understand whats happening inside the transformer library. here is what i wanted to do:
i would like to fine tune the bloom model to a conversation bot. now when tokenizing i dont really understand whats happening and therefore how the data is supposed to look. all examples i find online are with plain text but none of them touch the topic of conversation training with a dataset.
in huggingface's example they simply put ['text']at the end of their tokenizer function. since i dont have the feature text, but ['dialog'] i thought replacing it here would work. but apparently, it does not.
i would really appreciate if someone could say a few words of what exactly went wrong in my code and how to fix it. since i want to train varies models over the next months, explaining the mistake would help a lot in future.
here is my code and below the exact error as well as my notebook:
import torch
import random
import numpy as np
from transformers import autotokenizer, automodelforcausallm, trainer, trainingarguments
import datasets

# laden des modells und des tokenizers
model_name = ""bigscience/bloom-560m""
tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforcausallm.from_pretrained(model_name)

# laden des datasets
dataset = datasets.load_dataset('conv_ai_2')

# tokenisieren des datasets
def tokenize_function(examples):
    return tokenizer(examples[""dialog""])

tokenized_dataset = dataset.map(tokenize_function, batched=true, num_proc=4)

# aufteilen in trainings- und validierungsset
train_dataset = tokenized_dataset['train']
val_dataset = tokenized_dataset['valid']

# trainingsargumente
training_args = trainingarguments(
    output_dir='./results',
    evaluation_strategy = ""epoch"",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    logging_steps=500,
    save_steps=500,
    seed=42,
    learning_rate=5e-5,
    report_to=""none""
)

# trainer-objekt
trainer = trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# finetuning des modells
trainer.train()

# generieren einer antwort
def generate_response(input_text, model, tokenizer):
    input_ids = tokenizer.encode(input_text, return_tensors='pt')
    chat_history_ids = model.generate(
        input_ids=input_ids,
        max_length=1000,
        do_sample=true,
        top_p=0.9,
        top_k=50
    )
    return tokenizer.decode(chat_history_ids[0], skip_special_tokens=true)

# testen des conversational bots
while true:
    user_input = input(""you: "")
    response = generate_response(user_input, model, tokenizer)
    print(""bot: "" + response)


error:
---------------------------------------------------------------------------
remotetraceback                           traceback (most recent call last)
remotetraceback: 
""""""
traceback (most recent call last):
  file ""/usr/local/lib/python3.9/dist-packages/multiprocess/pool.py"", line 125, in worker
    result = (true, func(*args, **kwds))
  file ""/usr/local/lib/python3.9/dist-packages/datasets/utils/py_utils.py"", line 1349, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  file ""/usr/local/lib/python3.9/dist-packages/datasets/arrow_dataset.py"", line 3329, in _map_single
    batch = apply_function_on_filtered_inputs(
  file ""/usr/local/lib/python3.9/dist-packages/datasets/arrow_dataset.py"", line 3210, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  file ""<ipython-input-18-25d239b4d59f>"", line 17, in tokenize_function
    return tokenizer(examples[""dialog""])
  file ""/usr/local/lib/python3.9/dist-packages/datasets/formatting/formatting.py"", line 280, in __getitem__
    value = self.data[key]
keyerror: 'dialog'
""""""

the above exception was the direct cause of the following exception:

keyerror                                  traceback (most recent call last)
<ipython-input-18-25d239b4d59f> in <module>
     17     return tokenizer(examples[""dialog""])
     18 
---> 19 tokenized_dataset = dataset.map(tokenize_function, batched=true, num_proc=4)
     20 
     21 # aufteilen in trainings- und validierungsset

13 frames
/usr/local/lib/python3.9/dist-packages/datasets/formatting/formatting.py in __getitem__()
    278 
    279     def __getitem__(self, key):
--> 280         value = self.data[key]
    281         if key in self.keys_to_format:
    282             value = self.format(key)

keyerror: 'dialog'","['python', 'nlp', 'artificial-intelligence', 'huggingface-transformers']",75855859,"in the original tokenize_function, you were directly tokenizing the ""dialog"" key from the examples. however, this didn't ensure that the dimensions of the input and label tensors were consistent. this mismatch in dimensions was causing the error you encountered during training i converted each dialog entry into a single string by joining the ""text"" key values in each dialog. then i tokenize the dialog strings with proper truncation, padding, and a specified maximum length. this creates tokenized input tensors with consistent dimensions. then i shift the input_ids by one position. this means that the model will learn to predict the next token in the sequence. i also clone the shifted input_ids to avoid modifying the original tensor in place.
def tokenize_function(examples):
    dialog_texts = [' '.join([entry[""text""] for entry in dialog]) for dialog in examples[""dialog""]]
    tokenized = tokenizer(dialog_texts, truncation=true, padding='max_length', max_length=128, return_tensors=""pt"")  
    tokenized[""labels""] = tokenized.input_ids[:, 1:].clone()
    tokenized.input_ids = tokenized.input_ids[:, :-1]
    tokenized[""labels""] = torch.cat([tokenized.labels, torch.full((tokenized.labels.size(0), 1), tokenizer.pad_token_id, dtype=torch.long)], dim=1)

    return tokenized",https://stackoverflow.com/questions/75849546,python,26-03-2023 17:54,987.0,2.0,1.0,True,28-03-2023 13:59,27-03-2023 12:25
77560044,spacy displacy output using anvil.works server,"i am attempting to display entities using spacy's displacy feature.
the output of my render is being shown in my jupyter notebook code cell with my anvil.server.wait_forever() code.
here is an example of the code cell output i'm getting.
i would rather have the output appear here.
i have already tried using displacy.server instead of displacy.render and tried auto_port.
here is my code:
def visualize_entities_in_sentences(self, doc_id):
    """"""visualize entities in the sentences of a document.

    :param doc_id: the id of the document to visualize
    :type doc_id: str
    """"""
    doc = self.get_document(doc_id)
    sentences = list(doc.sents)
    labels = displacy.render(sentences, style=""ent"", page=false, minify=true)
    return labels

and my callable for the anvil implementation:
@anvil.server.callable
def get_visualize_entities_in_sentences(doc_id):
    """"""""get the document markdown for a document in my_corpus with entity labels visualized.

    :param doc_id: a document id
    :type doc_id: str
    :returns: markdown
    :rtype: str
    """"""    
    return my_corpus.visualize_entities_in_sentences(doc_id)","['python', 'jupyter-notebook', 'nlp', 'spacy', 'displacy']",77562648,try adding jupyter=false to displacy.render to skip the jupyter auto-detection.,https://stackoverflow.com/questions/77560044,python,27-11-2023 21:29,90.0,0.0,2.0,True,29-11-2023 01:52,28-11-2023 13:25
70456489,convert list to string with conditions,"i have a dataframe that looks like:
x <- tibble(
  experiment_id = rep(c('1a','1b'),each=5),
  keystroke = rep(c('a','shift','b','space','e'),2)
)

i know i can concatenate a list into a string using str_c or str_flatten and only keep certain values like below:
> y <- c('b','a','space','d')
> y[y %in% letters]
[1] ""b"" ""a"" ""d""

but when i try the same thing in a grouped pipe:
x_out <- x %>%
  group_by(experiment_id) %>%
  mutate(
    grp = cumsum(lag(keystroke=='space',default=0))) %>% 
    group_by(grp, .add=true) %>%
      mutate(within_keystrokes = list(keystroke),
             within_word = within_keystrokes[within_keystrokes %in% letters]
             ) %>% 
  ungroup()

i get the error:
error: problem with `mutate()` input `within_word`.
x input `within_word` can't be recycled to size 2.
ï¿½ï¿½ï¿½ input `within is `within_keystrokes[within_keystrokes %in% letters]`.
ï¿½ï¿½ï¿½ input `within_word` must be size 2 or 1, not 0.
ï¿½ï¿½ï¿½ the error occurred in group 1: experiment_id = ""1a"", grp = 0.

i r"" answer and tried using ifelse but still ran into errors.
any insight into what i'm doing wrong?
edit: expected output sorry for not including this. i would expect the final df to look like:
    x <- tibble(
      experiment_id = rep(c('1a','1b'),each=5),
      keystroke = rep(c('a','shift','b','space','e'),2),
      within_keystrokes = list(list('a','shift','b','space'), 
                          list('a','shift','b','space'), 
                          list('a','shift','b','space'), 
                          list('a','shift','b','space'),
                          'e',
                          list('a','shift','b','space'), 
                          list('a','shift','b','space'), 
                          list('a','shift','b','space'), 
                          list('a','shift','b','space'),
                          'e'),
      within_word = rep(list('ab','ab','ab','ab','e'),2)
)","['r', 'nlp', 'tidyverse']",70456568,"you almost solved your issue. you could use
library(dplyr)
library(stringr)

x %>%
  group_by(experiment_id, grp = cumsum(lag(keystroke == ""space"", default = 0))) %>% 
  mutate(
    within_keystrokes = list(keystroke),
    within_word = list(str_c(keystroke[keystroke %in% letters], collapse = """"))
    )

to get
# a tibble: 10 x 4
   experiment_id keystroke within_keystrokes within_word
   <chr>         <chr>     <list>            <list>     
 1 1a            a         <list [4]>        <chr [1]>  
 2 1a            shift     <list [4]>        <chr [1]>  
 3 1a            b         <list [4]>        <chr [1]>  
 4 1a            space     <list [4]>        <chr [1]>  
 5 1a            e         <chr [1]>         <chr [1]>  
 6 1b            a         <list [4]>        <chr [1]>  
 7 1b            shift     <list [4]>        <chr [1]>  
 8 1b            b         <list [4]>        <chr [1]>  
 9 1b            space     <list [4]>        <chr [1]>  
10 1b            e         <chr [1]>         <chr [1]> 

if you don't want within_word to be a list, just remove the list() function.",https://stackoverflow.com/questions/70456489,r,23-12-2021 00:28,58.0,0.0,1.0,True,23-12-2021 19:50,23-12-2021 02:24
62785916,spacy replace token,"i am trying to replace a word without destroying the space structure in the sentence. suppose i have the sentence text = ""hi this is my dog."". and i wish to replace dog with simba. following the answer from  i did:
import spacy
nlp = spacy.load(""en_core_web_lg"")
from spacy.tokens import doc

doc1 = nlp(""hi this is my dog."")
new_words = [token.text if token.text!=""dog"" else ""simba"" for token in doc1]
doc(doc1.vocab, words=new_words)
# hi this is my simba . 

notice how there was an extra space at the end before the full stop (it ought to be hi this is my simba.). is there a way to remove this behaviour. happy for a general python string processing answer too.","['python', 'spacy']",62787267,"thanks to @lora-johns i found this answer. so without going down the matcher route, i think this might be a simpler answer:
new_words = [(token.idx, len(""dog"")) for token in doc1 if token.text.lower()==""dog""]
# reverse order of replacement words from end to start
new_words = sorted(new_words, key=lambda x:-x[0])
for i, l in new_words: 
    text = text[:i] +  ""simba"" + text[i+l:]",https://stackoverflow.com/questions/62785916,python,08-07-2020 00:47,8803.0,4.0,9.0,True,17-05-2022 14:43,08-07-2020 03:27
75744401,can someone explain how to create a ptb dataset and/or train my own model using stanfordnlp?,"i'm learning about sentiment analysis and i can't seem to find anything online that outlines how to create a ptb dataset. i'm using stanfordnlp with java. i've downloaded the test, dev and validate data that they used and i can't get my head around how these have been outlined:
test.txt:
(3 (2 (2 the) (2 rock)) (4 (3 (2 is) (4 (2 destined) (2 (2 (2 (2 (2 to) (2 (2 be) (2 (2 the) (2 (2 21st) (2 (2 (2 century) (2 's)) (2 (3 new) (2 (2 ``) (2 conan)))))))) (2 '')) (2 and)) (3 (2 that) (3 (2 he) (3 (2 's) (3 (2 going) (3 (2 to) (4 (3 (2 make) (3 (3 (2 a) (3 splash)) (2 (2 even) (3 greater)))) (2 (2 than) (2 (2 (2 (2 (1 (2 arnold) (2 schwarzenegger)) (2 ,)) (2 (2 jean-claud) (2 (2 van) (2 damme)))) (2 or)) (2 (2 steven) (2 segal))))))))))))) (2 .)))

i figure that numbers are aligned to sentiment value but i'm still not sure how it works.
tldr; i'm trying to develop my own model for news analysis and have seen that the stanfordnlp model has been trained on movie reviews which is leading to poor sentiment analysis so, i thought to attempt to develop my own but i can't find anything online that teaches what each element is or how to even do this.
at best; outlined on this page: 
is the dataset available and the code to train.
models can be retrained using the following command using the ptb format dataset:

java -mx8g edu.stanford.nlp.sentiment.sentimenttraining -numhid 25 -trainpath train.txt -devpath dev.txt -train -model model.ser.gz

i have the data that i need to parse ready.","['java', 'machine-learning', 'stanford-nlp']",75745771,"okay.. so i've done some more digging and have started to finally understand (some what) as how to create a dataset tree and will try to break it down for anyone who stumbles upon this post with the same troubles as i've been having.
step 1.

find your data. (in my case it's news articles about the uk housing
market)

uk renters: are you living with someone youï¿½ï¿½ï¿½ve fallen out with?
uk property asking prices stagnating, lifting hopes of softer landing for housing market

step 2.

annotate your data

2 uk renters: are you living with someone youï¿½ï¿½ï¿½ve fallen out with?
1 fallen out with
1 fallen out
2 uk renters
2 living with someone
3 fallen
2 :
2 ?
2 living with
2 someone

3 uk property asking prices stagnating, lifting hopes of softer landing for housing market
2 uk property
3 asking prices stagnating
2 asking prices
4 lifting hopes
2 hopes
4 lifting hopes of softer landing
3 softer landing for housing mark market
2 lifting
2 landing
2 , 

annotation meanings
very positive= 4
positive = 3
neutral = 2
negative = 1
very negative = 0

structure
2 uk renters: are you living with someone youï¿½ï¿½ï¿½ve fallen out with?
   //overall sentiment

1 fallen out with
   // negative

1 fallen out
   // negative

2 uk renters
   // neutral

...etc..


save the annotated data to a .txt (sample.txt)

step 3:

locate your stanford-corenlp-4.5.2.jar

example  ~/.m2/repository/edu/stanford/nlp/stanford-corenlp/4.5.2



step 4:

open bash and run

java -cp ""*"" -mx5g edu.stanford.nlp.sentiment.buildbinarizeddataset -input /c/users/rusku/desktop/stanfordnpl/russample/sample.txt
replace the above data location



step 5:

result

(2 (2 (2 (2 uk) (2 renters)) (2 :)) (2 (2 (2 (2 are) (2 you)) (2 (2 living) (2 (2 with) (2 (2 someone) (2 (2 you) (2 (2 ï¿½ï¿½ï¿½ve) (1 (1 (3 fallen) (2 out)) (2 with)))))))) (2 ?)))
(3 (3 (2 (3 uk) (3 property)) (2 (3 asking) (3 prices))) (3 (3 (3 stagnating) (3 (2 ,) (4 (2 lifting) (2 hopes)))) (3 (3 of) (3 (3 (3 softer) (2 landing)) (3 (3 for) (2 (3 housing) (3 market)))))))

resource: train stanford corenlp about the sentiment of domain-specific phrases
this is as far as i've currently gotten.
hope this helps.",https://stackoverflow.com/questions/75744401,java,15-03-2023 11:58,165.0,0.0,1.0,True,15-03-2023 17:45,15-03-2023 15:10
79118378,how to save and load spacy encodings in a polars dataframe,"i want to use spacy to generate embeddings of text stored in a polars dataframe and store the results in the same dataframe. next, i want to save this dataframe to the disk and be able to load again as a polars dataframe. the backtransformation from pandas to polars results in an error.
this is the error message:
arrowinvalid: could not convert hello with type spacy.tokens.doc.doc: did not recognize python value type when inferring an arrow data type
here is my code:
from io import stringio
import polars as pl
import pandas as pd
import spacy


nlp = spacy.load(""de_core_news_sm"")
json_str = '[{""foo"":""hello"",""bar"":6},{""foo"":""what a lovely day"",""bar"":7},{""foo"":""nice to meet you"",""bar"":8}]'


#initalize and store dataframe
df = pl.read_json(stringio(json_str))
df = df.with_columns(pl.col(""foo"").map_elements(lambda x: nlp(x)).alias(""encoding""))
df.to_pandas().to_pickle('pickled_df.pkl')

#load dataframe
df_loaded_pd = pd.read_pickle('pickled_df.pkl')
df_loaded_pl = pl.from_pandas(df_loaded_pd)


these are the package versions i used:
# name                    version                   build  channel
pandas                    2.2.3           py312hf9745cd_1    conda-forge
polars                    1.9.0           py312hfe7c9be_0    conda-forge
spacy                     3.7.2           py312h6db74b5_0  
spacy-curated-transformers 0.2.2                    pypi_0    pypi
spacy-legacy              3.0.12             pyhd8ed1ab_0    conda-forge
spacy-loggers             1.0.5              pyhd8ed1ab_0    conda-forge

thank you for your help!","['python', 'dataframe', 'spacy', 'python-polars']",79124658,"serializing and deserializing
spacy objects within a polars dataframe can be stored by using spacys native docbin class. the following code generates doc objects, saves them locally, and successfully loads them afterwards.
from io import stringio
from spacy.tokens import docbin
import polars as pl
import spacy

nlp = spacy.load(""de_core_news_md"")
json_str = '[{""foo"":""hello"",""bar"":6},{""foo"":""what a lovely day"",""bar"":7},{""foo"":""nice to meet you"",""bar"":8}]'
doc = nlp(""some text"")

#serialize polars dataframe
df = pl.read_json(stringio(json_str))
df = df.with_columns(pl.col(""foo"").map_elements(lambda x: docbin(docs=[nlp(x)]).to_bytes()).alias('binary_embbeding'))
df.write_parquet('saved.pq')

#deserialize polars dataframe
df_loaded = pl.read_parquet('saved.pq')
df_loaded = df_loaded.with_columns(pl.col('binary_embbeding').map_elements(lambda x: list(docbin().from_bytes(x).get_docs(nlp.vocab))[0]).alias(""spacy_embedding""))

#calculate similarity
df_loaded.with_columns(pl.col(""spacy_embedding"").map_elements(lambda x: doc.similarity(x), return_dtype=pl.float64).alias('score'))


applying functions to deserialized spacy objects
serializing and deserializing spacys objects with native polars functions (such as df.write_parquet()) heavily depends on the used model. in the above case the similarity calculation only works when utilizing spacys language model that contain wordvectors.
nlp = spacy.load(""de_core_news_sm"") # line 20 does not works
nlp = spacy.load(""de_core_news_md"") # line 20 works
nlp = spacy.load(""de_core_news_lg"") # line 20 works
nlp = spacy.load(""de_dep_news_trf"") # line 20 does not works",https://stackoverflow.com/questions/79118378,python,23-10-2024 14:31,174.0,4.0,1.0,True,25-10-2024 07:26,23-10-2024 14:50
36610179,how to get the dependency tree with spacy?,"i have been trying to find how to get the dependency tree with spacy but i can't find anything on how to get the tree, only on how to navigate the tree.","['python', 'spacy']",36612605,"it turns out, the tree is available through the tokens in a document.
would you want to find the root of the tree, you can just go though the document:
def find_root(docu):
    for token in docu:
        if token.head is token:
            return token

to then navigate the tree, the tokens have api to get through the children",https://stackoverflow.com/questions/36610179,python,13-04-2016 21:47,57621.0,73.0,9.0,True,23-12-2022 15:27,14-05-2018 15:16
77011579,error &quot;&#39;tuple&#39; object has no attribute &#39;page_content&#39;&quot; when using weaviate.add_documents,"i have the following piece of code:
if file.filename.lower().endswith('.pdf'):
                        pdf = ep.pdfload(file_path)  # this is the loader from langchain
                        doc = pdf.load()
                        archivo = crear_archivo(doc, file)

inside crear_archivo function i am splitting the document and sending it to the weaviate.add_documents:
   cliente = db.newvect()  # this one creates the weaviate.client
    text_splitter = charactertextsplitter(chunk_size=1000, chunk_overlap=0)
    docs = text_splitter.split_documents(document)

    embeddings = openaiembeddings()
    return weaviate.add_documents(docs, embeddings, client=client, weaviate_url=envvect.host, by_text=false,      index_name=""langchain"") 
# using this instead of from_documents since i don't want to initialize a new vectorstore         

# some more logic to save the doc to another database

whenever i try to run the code it breaks during the weaviate.add_documents() function prompting the following error:
'tuple' object has no attribute 'page_content'.
i tried to check the type of docs, but that doesn't seem wrong since it returns a list[document] which is the same type the function accepts.
how can i make it work? i kind of followed this approach but the difference is i am loading files such as pdf, txt etc.","['python', 'langchain', 'py-langchain', 'weaviate']",77031161,"(tambiï¿½ï¿½n soy nuevo acï¿½ï¿½)
the error you're getting is likely because docs isn't a document object.
afaik, a ""document"" in langchain is a list of document objects. if you run type(docs[0]) you should get langchain.schema.document.document. this document object is a dictionary with two keys: one is page_content: which accepts string values, and the second key is metadata: which only accepts dictionaries. {page_content: str, metadata: dict}. not very well explained in langchain's documentation.
my suggestions to tackle your problem:

make sure that the document you're splitting here docs = text_splitter.split_documents(document), is effectively a langchain document object. use print(document), and you should see this in the first line: [document(page_content='your text etc... and at the end of the output, you ee ...end of your text', metadata={'...
if document isn't a langchain document, you'll need to check how you created it.
if document is a langchain document, try weaviate.from_documents() instead.

hope this helps, y un abrazo!",https://stackoverflow.com/questions/77011579,python,30-08-2023 21:20,5320.0,2.0,1.0,True,03-09-2023 06:31,30-08-2023 22:13
71223907,how to change allennlp bert based semantic role labeling to roberta in allennlp,"currently i'm able to train a semantic role labeling model using the config file below. this config file is based on the one provided by allennlp and works for the default bert-base-uncased model and also gronlp/bert-base-dutch-cased.
{
  ""dataset_reader"": {
    ""type"": ""srl_custom"",
    ""bert_model_name"": ""gronlp/bert-base-dutch-cased""
  },
  ""data_loader"": {
    ""batch_sampler"": {
      ""type"": ""bucket"",
      ""batch_size"": 32
    }
  },
  ""train_data_path"": ""./data/srl/sonar_1_srl/manual500/"",
  ""validation_data_path"": ""./data/srl/sonar_1_srl/manual500/"",
  ""model"": {
    ""type"": ""srl_bert"",
    ""embedding_dropout"": 0.1,
    ""bert_model"": ""gronlp/bert-base-dutch-cased""
  },
  ""trainer"": {
    ""optimizer"": {
      ""type"": ""huggingface_adamw"",
      ""lr"": 5e-5,
      ""correct_bias"": false,
      ""weight_decay"": 0.01,
      ""parameter_groups"": [
        [
          [
            ""bias"",
            ""layernorm.bias"",
            ""layernorm.weight"",
            ""layer_norm.weight""
          ],
          {
            ""weight_decay"": 0.0
          }
        ]
      ]
    },
    ""learning_rate_scheduler"": {
      ""type"": ""slanted_triangular""
    },
    ""checkpointer"": {
      ""keep_most_recent_by_count"": 2
    },
    ""grad_norm"": 1.0,
    ""num_epochs"": 3,
    ""validation_metric"": ""+f1-measure-overall""
  }
}

swapping the values of bert_model_name and bert_model parameters from gronlp/bert-base-dutch-cased to roberta-base won't work out of the box since the srl datareader only supports the berttokenizer and not the robertatokenizer. so i changed the config file to the following:
{
  ""dataset_reader"": {
    ""type"": ""srl_custom"",
    ""token_indexers"": {
      ""tokens"": {
        ""type"": ""pretrained_transformer"",
        ""model_name"": ""roberta-base""
      }
    }
  },
  ""data_loader"": {
    ""batch_sampler"": {
      ""type"": ""bucket"",
      ""batch_size"": 32
    }
  },
  ""train_data_path"": ""./data/srl/sonar_1_srl/manual500/"",
  ""validation_data_path"": ""./data/srl/sonar_1_srl/manual500/"",
  ""model"": {
    ""type"": ""srl_bert"",
    ""embedding_dropout"": 0.1,
    ""bert_model"": ""roberta-base""
  },
  ""trainer"": {
    ""optimizer"": {
      ""type"": ""huggingface_adamw"",
      ""lr"": 5e-5,
      ""correct_bias"": false,
      ""weight_decay"": 0.01,
      ""parameter_groups"": [
        [
          [
            ""bias"",
            ""layernorm.bias"",
            ""layernorm.weight"",
            ""layer_norm.weight""
          ],
          {
            ""weight_decay"": 0.0
          }
        ]
      ]
    },
    ""learning_rate_scheduler"": {
      ""type"": ""slanted_triangular""
    },
    ""checkpointer"": {
      ""keep_most_recent_by_count"": 2
    },
    ""grad_norm"": 1.0,
    ""num_epochs"": 15,
    ""validation_metric"": ""+f1-measure-overall""
  }
}

however, this is still not working. i'm receiving the following error:
2022-02-22 16:19:34,122 - info - allennlp.training.gradient_descent_trainer - training
  0%|          | 0/1546 [00:00<?, ?it/s]2022-02-22 16:19:34,142 - info - allennlp.data.samplers.bucket_batch_sampler - no sorting keys given; trying to guess a good one
2022-02-22 16:19:34,142 - info - allennlp.data.samplers.bucket_batch_sampler - using ['tokens'] as the sorting keys
  0%|          | 0/1546 [00:00<?, ?it/s]
2022-02-22 16:19:34,526 - critical - root - uncaught exception
traceback (most recent call last):
  file ""c:\program files\python39\lib\runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, none,
  file ""c:\program files\python39\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  file ""c:\users\denbe\appdata\roaming\python\python39\scripts\allennlp.exe\__main__.py"", line 7, in <module>
    sys.exit(run())
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\__main__.py"", line 39, in run
    main(prog=""allennlp"")
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\commands\__init__.py"", line 119, in main
    args.func(args)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\commands\train.py"", line 111, in train_model_from_args
    train_model_from_file(
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\commands\train.py"", line 177, in train_model_from_file
    return train_model(
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\commands\train.py"", line 258, in train_model
    model = _train_worker(
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\commands\train.py"", line 508, in _train_worker
    metrics = train_loop.run()
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\commands\train.py"", line 581, in run
    return self.trainer.train()
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\training\gradient_descent_trainer.py"", line 771, in train
    metrics, epoch = self._try_train()
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\training\gradient_descent_trainer.py"", line 793, in _try_train
    train_metrics = self._train_epoch(epoch)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\training\gradient_descent_trainer.py"", line 510, in _train_epoch
    batch_outputs = self.batch_outputs(batch, for_training=true)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp\training\gradient_descent_trainer.py"", line 403, in batch_outputs
    output_dict = self._pytorch_model(**batch)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\torch\nn\modules\module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp_models\structured_prediction\models\srl_bert.py"", line 141, in forward
    bert_embeddings, _ = self.bert_model(
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\torch\nn\modules\module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\transformers\models\bert\modeling_bert.py"", line 989, in forward
    embedding_output = self.embeddings(
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\torch\nn\modules\module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\transformers\models\bert\modeling_bert.py"", line 215, in forward
    token_type_embeddings = self.token_type_embeddings(token_type_ids)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\torch\nn\modules\module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\torch\nn\modules\sparse.py"", line 156, in forward
    return f.embedding(
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\torch\nn\functional.py"", line 1916, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
indexerror: index out of range in self

i don't fully understand whats going wrong and couldn't find any documentation on how to change the config file to load in a 'custom' bert/roberta model (one thats not mentioned here). i'm running the default allennlp train config.jsonnet command to start training. allennlp train config.jsonnet --dry-run produces no errors however.
thanks in advance!
thijs
edit:
i've now swapped out and inherited the ""srl_bert"" for a custom ""srl_roberta"" class to make use of the robertamodel. this however still produces the same error.
edit2: i'm now using the autotokenizer as suggested by dirk groeneveld. it looks like changing the srlreader class to support roberta based models involves way more changes like swapping berts wordpiece tokenizer to roberta's bpe tokenizer. is there an easy way to adapt the srlreader class or is it better to write a new robertasrlreader from scratch?
i've inherited the srlreader class and changed this line to the following:
self.bert_tokenizer = autotokenizer.from_pretrained(bert_model_name)

it produces the following error since roberta tokenization differs from bert:
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp_models\structured_prediction\dataset_readers\srl.py"", line 255, in text_to_instance
    wordpieces, offsets, start_offsets = self._wordpiece_tokenize_input(
  file ""c:\users\denbe\appdata\roaming\python\python39\site-packages\allennlp_models\structured_prediction\dataset_readers\srl.py"", line 196, in _wordpiece_tokenize_input
    word_pieces = self.bert_tokenizer.wordpiece_tokenizer.tokenize(token)
attributeerror: 'robertatokenizerfast' object has no attribute 'wordpiece_tokenizer'","['bert-language-model', 'allennlp', 'roberta-language-model', 'srl']",71246269,"the easiest way to resolve this is to patch srlreader so that it uses pretrainedtransformertokenizer (from allennlp) or autotokenizer (from huggingface) instead of berttokenizer. srlreader is an old class, and was written against an old version of the huggingface tokenizer api, so it's not so easy to upgrade.
if you want to submit a pull request in the allennlp project, i'd be happy to help you get it merged into allennlp!",https://stackoverflow.com/questions/71223907,bert-language-model,22-02-2022 15:24,447.0,2.0,1.0,True,24-02-2022 12:34,24-02-2022 12:34
75158430,error &#39;img&#39; when applying increment with keras and transformers for image classification,"i would like to apply vit for image classification. but i have one problem and i don't know as resolve it. my error is this ""keyerror: 'img'"". the error is shown when i apply the last comand, and i don't know where is my error. the image within dataset are in .png, but i don't think that this was mistake.
below there is the script:
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_addons as tfa
import os
import cv2
import matplotlib.pyplot as plt
from transformers import vitfeatureextractor, vitforimageclassification
from transformers import trainingarguments, trainer
from tensorflow import keras 
from tensorflow.keras import layers
from datasets import load_metric
from pil import image as img
from ipython.display import image, display
from datasets import load_dataset 
import torch
dataset = load_dataset(""imagefolder"", data_dir=""datasets"")

dataset
example = dataset[""train""][10]
example
dataset[""train""].features
example['image']
example['image'].resize((200, 200))
example['label']
dataset[""train""].features[""label""]


img_class_labels = dataset[""train""].features[""label""].names

from transformers import vitfeatureextractor
from tensorflow import keras
from tensorflow.keras import layers


model_id = ""google/vit-base-patch16-224-in21k""
feature_extractor = vitfeatureextractor.from_pretrained(model_id)

# learn more about data augmentation here: 
data_augmentation = keras.sequential(
    [
        layers.resizing(feature_extractor.size, feature_extractor.size),
        layers.rescaling(1./255),
        layers.randomflip(""horizontal""),
        layers.randomrotation(factor=0.02),
        layers.randomzoom(
            height_factor=0.2, width_factor=0.2
        ),
    ],
    name=""data_augmentation"",
)
# use keras image data augementation processing
def augmentation(examples):
    # print(examples[""img""])
    examples[""pixel_values""] = [data_augmentation(image) for image in examples[""img""]]
    return examples


# basic processing (only resizing)
def process(examples):
    examples.update(feature_extractor(examples['img'], ))
    return examples

# we are also renaming our label col to labels to use `.to_tf_dataset` later
dataset_ds = dataset[""train""].rename_column(""label"", ""labels"")

processed_dataset = dataset_ds.map(augmentation, batched=true)
processed_dataset","['python', 'image', 'keras', 'classification', 'huggingface-transformers']",75158976,"i guess the error is here:
def augmentation(examples):
    # print(examples[""img""])
    examples[""pixel_values""] = [data_augmentation(image) for image in examples[""img""]]
    return examples

you are trying to access 'examples' dictionary using 'img' key. from some code above it looks like the key should be 'image':
    examples[""pixel_values""] = [data_augmentation(image) for image in examples[""image""]]",https://stackoverflow.com/questions/75158430,python,18-01-2023 11:18,109.0,1.0,1.0,True,18-01-2023 15:57,18-01-2023 15:57
65083581,how to compute mean/max of huggingface transformers bert token embeddings with attention mask?,"i'm using the huggingface transformers bert model, and i want to compute a summary vector (a.k.a. embedding) over the tokens in a sentence, using either the mean or max function. the complication is that some tokens are [pad], so i want to ignore the vectors for those tokens when computing the average or max.
here's an example. i initially instantiate a berttokenizer and a bertmodel:
import torch
import transformers
from transformers import autotokenizer, automodel

transformer_name = 'bert-base-uncased'

tokenizer = autotokenizer.from_pretrained(transformer_name, use_fast=true)

model = automodel.from_pretrained(transformer_name)

i then input some sentences into the tokenizer and get out input_ids and attention_mask. notably, an attention_mask value of 0 means that the token was a [pad] that i can ignore.
sentences = ['deep learning is difficult yet very rewarding.',
             'deep learning is not easy.',
             'but is rewarding if done right.']
tokenizer_result = tokenizer(sentences, max_length=32, padding=true, return_attention_mask=true, return_tensors='pt')

input_ids = tokenizer_result.input_ids
attention_mask = tokenizer_result.attention_mask

print(input_ids.shape) # torch.size([3, 11])

print(input_ids)
# tensor([[  101,  2784,  4083,  2003,  3697,  2664,  2200, 10377,  2075,  1012,  102],
#         [  101,  2784,  4083,  2003,  2025,  3733,  1012,   102,     0,     0,    0],
#         [  101,  2021,  2003, 10377,  2075,  2065,  2589,  2157,  1012,   102,   0]])

print(attention_mask.shape) # torch.size([3, 11])

print(attention_mask)
# tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
#         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],
#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])

now, i call the bert model to get the 768-d token embeddings (the top-layer hidden states).
model_result = model(input_ids, attention_mask=attention_mask, return_dict=true)

token_embeddings = model_result.last_hidden_state
print(token_embeddings.shape) # torch.size([3, 11, 768])

so at this point, i have:

token embeddings in a [3, 11, 768] matrix: 3 sentences, 11 tokens, 768-d vector for each token.
attention mask in a [3, 11] matrix: 3 sentences, 11 tokens. a 1 value indicates non-[pad].

how do i compute the mean / max over the vectors for the valid, non-[pad] tokens?
i tried using the attention mask as a mask and then called torch.max(), but i don't get the right dimensions:
masked_token_embeddings = token_embeddings[attention_mask==1]
print(masked_token_embeddings.shape) # torch.size([29, 768] <-- wrong. should be [3, 11, 768]

pooled = torch.max(masked_token_embeddings, 1)
print(pooled.values.shape) # torch.size([29]) <-- wrong. should be [3, 768]

what i really want is a tensor of shape [3, 768]. that is, a 768-d vector for each of the 3 sentences.","['machine-learning', 'pytorch', 'bert-language-model', 'huggingface-transformers']",65084300,"for max, you can multiply with attention_mask:
pooled = torch.max((token_embeddings * attention_mask.unsqueeze(-1)), axis=1)

for mean, you can sum along the axis and divide by attention_mask along that axis:
mean_pooled = token_embeddings.sum(axis=1) / attention_mask.sum(axis=-1).unsqueeze(-1)",https://stackoverflow.com/questions/65083581,machine-learning,01-12-2020 01:38,6251.0,8.0,3.0,True,07-09-2022 17:39,01-12-2020 01:49
77433096,notimplementederror: loading a dataset cached in a localfilesystem is not supported,"i try to load a dataset using the datasets python module in my local python notebook. i am running a python 3.10.13 kernel as i do for my virtual environment.
i cannot load the datasets i am following from a tutorial. here's the error:
---------------------------------------------------------------------------
notimplementederror                       traceback (most recent call last)
/users/ari/downloads/00-fine-tuning.ipynb celda 2 line 3
      1 from datasets import load_dataset
----> 3 data = load_dataset(
      4     ""jamescalam/agent-conversations-retrieval-tool"",
      5     split=""train""
      6 )
      7 data

file ~/documents/fastapi_language_tutor/env/lib/python3.10/site-packages/datasets/load.py:2149, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)
   2145 # build dataset for splits
   2146 keep_in_memory = (
   2147     keep_in_memory if keep_in_memory is not none else is_small_dataset(builder_instance.info.dataset_size)
   2148 )
-> 2149 ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory)
   2150 # rename and cast features to match task schema
   2151 if task is not none:
   2152     # to avoid issuing the same warning twice

file ~/documents/fastapi_language_tutor/env/lib/python3.10/site-packages/datasets/builder.py:1173, in datasetbuilder.as_dataset(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)
   1171 is_local = not is_remote_filesystem(self._fs)
   1172 if not is_local:
-> 1173     raise notimplementederror(f""loading a dataset cached in a {type(self._fs).__name__} is not supported."")
   1174 if not os.path.exists(self._output_dir):
   1175     raise filenotfounderror(
   1176         f""dataset {self.dataset_name}: could not find data in {self._output_dir}. please make sure to call ""
   1177         ""builder.download_and_prepare(), or use ""
   1178         ""datasets.load_dataset() before trying to access the dataset object.""
   1179     )

notimplementederror: loading a dataset cached in a localfilesystem is not supported.

how do i resolve this? i don't understand how this error is applicable, given that the dataset is something i am fetching and thus cannot be cached in my localfilesystem in the first place.","['python', 'python-3.x', 'pip', 'openai-api', 'huggingface-datasets']",77433141,"try doing:
pip install -u datasets


this error stems from a breaking change in fsspec. it has been fixed in the latest datasets release (2.14.6). updating the installation with pip install -u datasets should fix the issue.

git link : 

if you are using fsspec, then do:
pip install fsspec==2023.9.2

there is a problem with fsspec==2023.10.0
git link : 


edit: looks like it broken again in 2.17 and 2.18 downgrading to 2.16 should work.",https://stackoverflow.com/questions/77433096,python,06-11-2023 17:29,34028.0,24.0,2.0,True,09-05-2024 20:10,09-05-2024 20:10
78501938,how to stop generating response in openai library for python?,"i am using completions in openai library for python. something like this:
self.__response = self.client.chat.completions.create(
    model='gpt-4',
    messages=messages,
    stream=true
)

after this i just loop through chunks:
    for chunk in self.__response:
        text = chunk.choices[0].delta.content
        # processing text here

is it enough to just do break inside the loop to prevent server generating response and wasting tokens if i see that the response is not meeting my expectations? or probably there is correct way to achieve this?","['python', 'openai-api']",78501966,"you are charged for all the tokens (words or parts of words) the api generates, even if you don't process them. so, breaking the loop early stops you from processing more tokens but doesn't stop you from being charged for them.
you can limit that using 'max_tokens', that'll save you from the cost, but in that case, you'll be forever stuck with the lower max_tokens response, even if that particular response is the desired one.",https://stackoverflow.com/questions/78501938,python,19-05-2024 07:58,498.0,0.0,1.0,True,19-05-2024 08:21,19-05-2024 08:06
72442701,how can i find repeated string segments in python?,"so i have some medium-length string - somewhere between a few words and a few sentences. sometimes, a substring in the text is repeated twice in a row. i need to write automatic code to identify the repeated part. or at least flag it with a high probability.
what i know:

the repeated substring is a series of a few whole words (and punctuation marks). a repeat will not happen in the middle of a word.
the repeat is of a variable length. it can be a few words to a few sentences itself. but it's always at least a few words long. i would like to avoid flagging single word repetitions if possible.
when a repeat happens, it's always repeated exactly once, and right after the previous appearence. right after the previous appearence. (<- example)
i need to run this check on about a million different strings, so the code has to be somewhat efficient at least (not the brute force check-every-option approach).

i've been struggling with this for a while now. would really appreciate your help.","['python', 'string', 'nlp']",72443424,"since the repetition of one word is a subclass of a multiple-word repetition, it's already helpful to match single words or word-like sequences. here is the regular expression i tried on your question in an editor with regex search:
(\<\w.{3,16}\w\>).{2,}\1

this is the first repetition found

the repeat is of a variable length. it can be a few words to a few sentences itself. but it's always at least a few words long. i would like to avoid flagging single word repetitions if possible.

but it next finds repeat in repeating. so we have to tune the limits.
the part (\<\w.{3,16}\w\>) means

from word start (including a character)
3 to 16 arbitrary characters
before word end (including a character)

in other words, one or more word with a total character count of 5 to 18.
the part .{2,}\1 means

at least two characters
no upper limit
captured match

here, the lower limit can be higher. an upper limit should be tried, especially on longer text.
i'd think that starting with finding short character sequences which repeat, then refine by looking for longer sequences that repeat in the result of the first step (plus additional characters at the end).
it's also a matter of preprocessing. i'd guess that repeating multiple-word sequences should be missed if line breaks (instead of space occur) on different places.
to automate this further, you may switch to python's re module.",https://stackoverflow.com/questions/72442701,python,31-05-2022 05:37,503.0,2.0,1.0,True,31-05-2022 07:10,31-05-2022 05:49
68870383,type errors with bert example,"i'm new to bert qa model & was trying to follow the example found in this article. the problem is when i run the code attached to the example it produces a type error as follows typeerror: argmax(): argument 'input' (position 1) must be tensor, not str.
here is the code that i've tried running :
import torch
from transformers import bertforquestionanswering
from transformers import berttokenizer

#model
model = bertforquestionanswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

#tokenizer
tokenizer = berttokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

question = '''sample question""'''

paragraph = '''sample paragraph'''
            
encoding = tokenizer.encode_plus(text=question,text_pair=paragraph, add_special=true)

inputs = encoding['input_ids']  #token embeddings
sentence_embedding = encoding['token_type_ids']  #segment embeddings
tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens

start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))

start_index = torch.argmax(start_scores)

end_index = torch.argmax(end_scores)

answer = ' '.join(tokens[start_index:end_index+1])

the issue appears at line 13 of this code where i'm trying to get the maximum element in start_scores saying that this is not a tensor. when i tried printing this variable it showed ""start_logits"" as a string. does anyone know a solution to this issue?","['python', 'bert-language-model']",68871099,"so after referring to the bert documentation we identified that the model output object contains multiple properties not only start & end scores. thus, we applied the following changes to the code.

outputs = model(input_ids=torch.tensor([inputs]),token_type_ids=torch.tensor([sentence_embedding]))

start_index = torch.argmax(outputs.start_logits)

end_index = torch.argmax(outputs.end_logits)

answer = ' '.join(tokens[start_index:end_index+1])

always refer to the documentation first :""d",https://stackoverflow.com/questions/68870383,python,21-08-2021 05:28,426.0,0.0,1.0,True,21-08-2021 07:45,21-08-2021 05:38
77427999,custom spacy tagger to tag all words that are in a dictionary,"i'm trying spacy to extract specific information from a text.
so i need to configure a custom tokenizer to identify them and a custom tagger to label all the words that are in an external dictionary in json format.
the tokenizer worked on several attempts, but the labeler has been having problems when processing simple text.
i hope that the label i will add to the words is a custom pos-tag ""unm"" and that i can attribute it to token.pos_ like all other labels ""noun"", ""verb"", etc.
import requests

#keywords dictionary
dictionary = requests.get(
    ""

    
#creating the custom tagger
doc.set_extension('pos_tag', default=none, force=true)

@language.factory(""keyword_pos_tagger"")
class keywordpostagger:
   def __init__(self, name, nlp, keywords, pos_tag):
       self.keywords = keywords
       self.pos_tag = pos_tag
       #doc.set_extension('pos_tag', default=none, force=true)

   def __call__(self, doc):
       for token in doc:
           if token.text in self.keywords:
               token._.pos_tag = self.pos_tag
       return doc

nlp = spacy.load('pt_core_news_md')


keywords = ('mï¿½ï¿½', 'm2', '(w/k)', 'ï¿½ï¿½c')
pos_tag = 'unm' # substitua por seu rï¿½ï¿½tulo pos

keyword_pos_tagger = keywordpostagger(nlp, 'keyword_pos_tagger', keywords, pos_tag)

config = {""nlp"": nlp, ""keywords"": keywords, ""pos_tag"": pos_tag}

nlp.add_pipe('keyword_pos_tagger', config = config)

<main.keywordpostagger at 0x78d568e4cee0>
and when i use the custom tagger:
doc = nlp('a temperatura tem 159ï¿½ï¿½c ou 20 ï¿½ï¿½c. tambï¿½ï¿½m precisa ter 20m de largura e 14 mï¿½ï¿½ de ï¿½ï¿½rea, caso contrï¿½ï¿½rio terï¿½ï¿½ 1 kelvin (w/k)')
for token in doc:
   print(token.text, token._.pos_tag)

it returns this error
-------------------------------------------------------------                 traceback (most recent call last)
<ipython-input-5-3c241e1c89fd> in <cell line: 1>()
----> 1 doc = nlp('a temperatura tem 159ï¿½ï¿½c ou 20 ï¿½ï¿½c. tambï¿½ï¿½m precisa ter 20m de largura e 14 mï¿½ï¿½ de ï¿½ï¿½rea, caso contrï¿½ï¿½rio terï¿½ï¿½ 1 kelvin (w/k)')
      2 for token in doc:
      3    print(token.text, token._.pos_tag)

4 frames
/usr/local/lib/python3.10/dist-packages/spacy/tokens/underscore.py in __setattr__(self, name, value)
     74     def __setattr__(self, name: str, value: any):
     75         if name not in self._extensions:
---> 76             raise attributeerror(errors.e047.format(name=name))
     77         default, method, getter, setter = self._extensions[name]
     78         if setter is not none:

attributeerror: [e047] can't assign a value to unregistered extension attribute 'pos_tag'. did you forget to call the `set_exten","['python', 'nlp', 'spacy', 'pos-tagger']",77429721,"you need to provide the config settings in the add_pipe method through a config dict. in your code, the keyword_pos_tagger variable is a stranded component that's not actually added to the nlp pipeline. it shares the same vocab and you could use it for unit testing, but otherwise you can't add it to a pipeline when it's created like this.
nlp.add_pipe(""keyword_pos_tagger"", config={""keywords"": keywords, ""pos_tag"": pos_tag})


edited to expand answer:
# tested with spacy==3.7.2
import spacy
from spacy.language import language
from spacy.tokens import token

# creating the custom tagger
token.set_extension(""pos_tag"", default=none, force=true)


@language.factory(""keyword_pos_tagger"")
class keywordpostagger:
    def __init__(self, name, nlp, keywords, pos_tag):
        self.keywords = keywords
        self.pos_tag = pos_tag

    def __call__(self, doc):
        for token in doc:
            if token.text in self.keywords:
                token._.pos_tag = self.pos_tag
        return doc


nlp = spacy.load(""pt_core_news_md"")

keywords = (""mï¿½ï¿½"", ""m2"", ""(w/k)"", ""ï¿½ï¿½c"")
pos_tag = ""unm""  # substitua por seu rï¿½ï¿½tulo pos

config = {""keywords"": keywords, ""pos_tag"": pos_tag}

nlp.add_pipe(""keyword_pos_tagger"", config=config)

doc = nlp(
    ""a temperatura tem 159ï¿½ï¿½c ou 20 ï¿½ï¿½c. tambï¿½ï¿½m precisa ter 20m de largura e 14 mï¿½ï¿½ rï¿½ï¿½ 1 kelvin (w/k)""
)
assert doc[16]._.pos_tag == ""unm""
</p",https://stackoverflow.com/questions/77427999,python,05-11-2023 23:07,184.0,1.0,1.0,True,06-11-2023 14:17,06-11-2023 14:02
69028332,spacy 3 - lemma_ returned will be empty string,"i normalize ten thousands of docs using spacy 3.

to speed up the process, i try this way,

nlp = spacy.load('en_core_web_sm')
docs = nlp.tokenizer.pipe(doc_list)
return [[word.lemma_ for word in doc if word.is_punct == false and word.is_stop == false] for doc, _ in doc_list]

but all lemma_ returned would be empty string.

so i directly use nlp(doc) like the following, but it's too slow.

a = [[word.lemma_ for word in nlp(doc) if word.is_punct == false and word.is_stop == false] for doc in doc_list]

how can i do this properly?","['python', 'spacy']",69052065,"the difference is in how you are creating the docs.

in the first example you use nlp.tokenizer.pipe() - this will only run the tokenizer on all your docs but not the lemmatizer. so, all you get is your docs split into tokens but the lemma_ attribute is not set.
in the second example you use nlp(doc) this will run all the default components (which are ['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']. since the lemmatizer is part of the pipeline the lemma_ attribute is set. but, it slower because you are running all the components, even the ones you don't need.

what you should be doing:
import spacy

# exclude components not required when loading the spacy model.
nlp = spacy.load(""en_core_web_sm"", exclude=[""tok2vec"", ""parser"", ""ner"", ""attrbute_ruler""]) 

# extract lemmas as required.
a = [[word.lemma_ for word in nlp(doc) if word.is_punct == false and word.is_stop == false] for doc in doc_list]",https://stackoverflow.com/questions/69028332,python,02-09-2021 10:17,945.0,2.0,1.0,True,11-10-2021 13:39,11-10-2021 13:39
76080243,what is the correct way to send a long string by an http stream in expressjs/nestjs?,"i'm using nestjs to write a forwarding service for the openai chat completion api. i want to do a transformation for the original stream and then forward stream to the client side.
the code is like below, and it's inside a nestjs controller
const completion = await openai.createchatcompletion(
  {
    model: 'gpt-3.5-turbo',
    messages: messages,
    n: 1,
    stream: true,
    max_tokens: 4000,
  },
  { responsetype: 'stream' },
);

class transformerstream extends transform {
  _transform(chunk, encoding, callback) {
    // if i directly forward the chunk like this, the client can receive chunk by chunk
    this.push(chunk)
    // however, if i use string, the client can't receive chunk by chunk.
    // my original code is to transform the chunk to string and do some transformation, to simplify the question, just use 'data: ping\n' here
    this.push('data: ping\n', 'utf8')
    callback()
  }
}

const transformer = new transformerstream()
completion.data.pipe(transformer).pipe(res)

and i'm using axios to request the api from the client side, and i'm trying to receive it chunk by chunk using ondownloadprogress
axios.post('/api/chat', body, {
  responsetype: 'stream',
  ondownloadprogress: progress => {
    console.log(progress)
  }
} )

in summary, when i directly send the buffer chunk from the openai api, the progress can be logged several times.
but when i send the string, it can only be logged once.","['node.js', 'express', 'http', 'openai-api', 'http-streaming']",76089098,"it might be due to the difference between the length of the original chunk and the length of the string you are trying to write to the stream.
you can consider setting the following headers in your nestjs controller:

transfer-encoding: chunked
x-content-type-options: nosniff

sample code:
res.setheader('transfer-encoding', 'chunked');
res.setheader('x-content-type-options', 'nosniff');

transfer-encoding tells the browser to start processing the data instead of waiting for all the content to be loaded first
x-content-type-options tells the browser to respect the content-type specified by your header instead of trying to guess based on the head of the content returned. based on my test with latest chrome browser, it seems like the initial 1024 bytes are ""blocked"" before browser correctly identified the content-type.
you can read more about the behaviour here: what is ""x-content-type-options=nosniff""?
reference:",https://stackoverflow.com/questions/76080243,node.js,22-04-2023 14:48,2188.0,0.0,1.0,True,24-04-2023 15:51,24-04-2023 15:51
76674599,can i make use charset-normalizer 3.0.0 instead of 3.2.0 in build.gradle so that i do not get aiohttp related library error in android studio?,"i am using chaquopy to integrate python into android. i am making a fairly simple app the uses the openai and webvtt libraries; hence my src/main/script.py contains ""import open ai"" and ""import webvtt"" at the beginning of the script. i also have in my build.gradle the block:
android {

    namespace 'com.example.python_integration'
    compilesdk 33

    defaultconfig {


        python{
            pip{
                install ""numpy""
            }
        }
        python{
             pip{
                install ""openai""
            }
        }
        python{
            pip{
                install ""tk""
            }
        }
        python{
            pip{
                install ""webvtt_py""
            }
        }

when i run the app in an emulator i get a warning ""aiohttp 3.8.1 has requirement charset-normalizer<3.0,>=2.0, but you'll have charset-normalizer 3.2.0 which is incompatible.""
i tried adding an implementation to the gradle:
implementation 'com.github.ousret:charset_normalizer:3.0.0'
as well as trying
implementation 'com.github.mizosoft.charset-normalizer:charset-normalizer:3.0.0'
but i still get the error/warning. is this warning significant? is there another block of code i need to add so that the gradle uses the previous charset-normalizer version?
the app still runs but i am still editing my script.py so i have no way of knowing whether this is a critical error or not at this stage, i am currently just making sure i can get all of the libraries i need to work from my original python script.
sometimes when i build the app in the emulator i don't even get this error in the build log so it is a little frustrating that it randomly keeps popping up.
maybe i am wildly misunderstanding the situation?","['python', 'android', 'openai-api', 'chaquopy', 'webvtt']",76677277,"the warning may or may not be critical, depending on what your app does. but you can probably work around it by adding a direct requirement on the indicated version:
install ""charset-normalizer<3.0,>=2.0""

the reason you don't see the warning every time you build your app is that it only re-runs pip when you change the pip block, or other relevant settings like the python version.
additional notes:

you can include more than one install line in a python { pip } block, so you don't need to copy the entire block multiple times.
the gradle dependencies block is completely independent from your python requirements, so don't bother editing that.",https://stackoverflow.com/questions/76674599,python,12-07-2023 21:36,965.0,1.0,1.0,True,13-07-2023 08:16,13-07-2023 00:50
60867353,using lime for bert transformer visualization results in memory error,"situation:  i am currently working on visualizing the results of a huggingface transformers machine learning model i have been building using the lime package following this tutorial. 
complication: my code is set up and runs well until i create the lime explainer object. at this point i get a memory error.
question: what am i doing wrong? why am i running into a memory error?
code: here is my code (you should be able to just copy-paste this into google colab and run the whole thing)
########################## load packages ######################
# install new packages in our environment
!pip install lime
!pip install wget
!pip install transformers

# import general libraries
import sklearn
import sklearn.ensemble
import sklearn.metrics
import numpy as np
import pandas as pd

# import libraries specific to this notebook
import lime
import wget
import os
from __future__ import print_function
from transformers import featureextractionpipeline, bertmodel, berttokenizer, bertconfig
from lime.lime_text import limetextexplainer

# let the notebook know to plot inline
%matplotlib inline

########################## load data ##########################
# get url
url = '

# download the file (if we haven't already)
if not os.path.exists('./cola_public_1.1.zip'):
    wget.download(url, './cola_public_1.1.zip')

# unzip the dataset (if we haven't already)
if not os.path.exists('./cola_public/'):
    !unzip cola_public_1.1.zip

# load the dataset into a pandas dataframe.
df_cola = pd.read_csv(""./cola_public/raw/in_domain_train.tsv"", delimiter='\t', 
                      header=none, names=['sentence_source', 'label', 
                                          'label_notes', 'sentence'])

# only look at the first 50 observations for debugging
df_cola = df_cola.head(50)

###################### train test split ######################
# apply the train test split
x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(
    df_cola.sentence, df_cola.label, test_size=0.2, random_state=42
)

###################### create lime classifier ######################
# create a function to extract vectors from a single sentence
def vector_extractor(sentence):

    # create a basic bert model, config and tokenizer for the pipeline
    configuration = bertconfig()
    configuration.max_len = 64
    tokenizer = berttokenizer.from_pretrained('bert-base-uncased',
                                              do_lower_case=true, 
                                              max_length=64,
                                              pad_to_max_length=true)
    model = bertmodel.from_pretrained('bert-base-uncased',config=configuration)

    # create the pipeline
    vector_extractor = featureextractionpipeline(model=model,
                                                 tokenizer=tokenizer,
                                                 device=0)

    # the pipeline gives us all tokens in the final layer - we want the cls token
    vector = vector_extractor(sentence)
    vector = vector[0][0]

    # return the vector
    return vector

# adjust the format of our sentences (from pandas series to python list)
x_train = x_train.values.tolist()
x_test = x_test.values.tolist()

# first we vectorize our train features for the classifier
x_train_vectorized = [vector_extractor(x) for x in x_train]

# create and fit the random forest classifier
rf = sklearn.ensemble.randomforestclassifier(n_estimators=100)
rf.fit(x_train_vectorized, y_train)

# define the lime_classifier function
def lime_classifier(sentences): 

    # turn all the sentences into vectors
    vectors = [vector_extractor(x) for x in sentences]

    # get predictions for all 
    predictions = rf.predict_proba(vectors)

    # return the probabilies as a 2d-array
    return predictions  

########################### apply lime ##########################
# create the general explainer object
explainer = limetextexplainer()

# ""fit"" the explainer object to a specific observation
exp = explainer.explain_instance(x_test[1], 
                                 lime_classifier, 
                                 num_features=6)","['python', 'machine-learning', 'huggingface-transformers', 'lime']",60989317,"ended up solving this by re-implementing along the lines of this github post: 

my code is now very different from the above - probably makes sense if you look to the github post for guidance if you're running into similar issues.",https://stackoverflow.com/questions/60867353,python,26-03-2020 12:32,4150.0,2.0,1.0,True,18-12-2022 20:45,18-12-2022 20:45
73251309,how to feed big data into pipeline of huggingface for inference,"model = ""bert-base-uncased""

# load the model
model_name = model + '-text-classification'

from transformers import automodelforsequenceclassification, autotokenizer

load_model = automodelforsequenceclassification.from_pretrained(model_name)
load_tokenizer = autotokenizer.from_pretrained(model_name)
from transformers import pipeline
my_pipeline  = pipeline(""text-classification"", model=load_model, 
                                                tokenizer=load_tokenizer)
a = list(df_0.limit(10000).topandas()[""lines""])
my_pipeline(a)

error message:
token indices sequence length is longer than the specified maximum sequence length for this model (1081 > 512). running this sequence through the model will result in indexing errors
--------------------------------------------------------------------------- runtimeerror                              traceback (most recent call last) input in [26], in <cell line: 1>()
----> 1 b = my_pipeline(a)

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:138, in textclassificationpipeline.__call__(self, *args, **kwargs)
    104 def __call__(self, *args, **kwargs):
    105     """"""
    106     classify the text(s) given as inputs.
    107     (...)
    136         if `top_k` is used, one such dictionary is returned per label.
    137     """"""
--> 138     result = super().__call__(*args, **kwargs)
    139     if isinstance(args[0], str) and isinstance(result, dict):
    140         # this pipeline is odd, and return a list when single item is run
    141         return [result]

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/pipelines/base.py:1032, in pipeline.__call__(self, inputs, num_workers, batch_size, *args,
**kwargs)    1028 if can_use_iterator:    1029     final_iterator = self.get_iterator(    1030         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params    1031     )
-> 1032     outputs = [output for output in final_iterator]    1033     return outputs    1034 else:

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/pipelines/base.py:1032, in <listcomp>(.0)    1028 if can_use_iterator:    1029     final_iterator = self.get_iterator(    1030         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params    1031     )
-> 1032     outputs = [output for output in final_iterator]    1033     return outputs    1034 else:

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:111, in pipelineiterator.__next__(self)
    108     return self.loader_batch_item()
    110 # we're out of items within a batch
--> 111 item = next(self.iterator)
    112 processed = self.infer(item, **self.params)
    113 # we now have a batch of ""inferred things"".

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:112, in pipelineiterator.__next__(self)
    110 # we're out of items within a batch
    111 item = next(self.iterator)
--> 112 processed = self.infer(item, **self.params)
    113 # we now have a batch of ""inferred things"".
    114 if self.loader_batch_size is not none:
    115     # try to infer the size of the batch

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/pipelines/base.py:959, in pipeline.forward(self, model_inputs, **forward_params)
    957     with inference_context():
    958         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)
--> 959         model_outputs = self._forward(model_inputs, **forward_params)
    960         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(""cpu""))
    961 else:

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:163, in textclassificationpipeline._forward(self, model_inputs)
    162 def _forward(self, model_inputs):
--> 163     return self.model(**model_inputs)

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in module._call_impl(self, *input, **kwargs)    1126 # if we don't have any hooks, we want to skip the rest of the logic in    1127 # this function, and just call forward.    1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks    1129         or
_global_forward_hooks or _global_forward_pre_hooks):
-> 1130     return forward_call(*input, **kwargs)    1131 # do not call functions when jit is used    1132 full_backward_hooks, non_full_backward_hooks = [], []

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1556, in bertforsequenceclassification.forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)    1548 r""""""    1549 labels (`torch.longtensor` of shape `(batch_size,)`, *optional*):    1550     labels for computing the sequence classification/regression loss. indices should be in `[0, ...,    1551     config.num_labels - 1]`. if `config.num_labels == 1` a regression loss is computed (mean-square loss), if    1552     `config.num_labels > 1` a classification loss is computed (cross-entropy).    1553 """"""    1554 return_dict = return_dict if return_dict is not none else self.config.use_return_dict
-> 1556 outputs = self.bert(    1557     input_ids,    1558     attention_mask=attention_mask,    1559     token_type_ids=token_type_ids,    1560     position_ids=position_ids,  1561     head_mask=head_mask,    1562     inputs_embeds=inputs_embeds, 1563     output_attentions=output_attentions,    1564     output_hidden_states=output_hidden_states,    1565     return_dict=return_dict,    1566 )    1568 pooled_output = outputs[1]  1570 pooled_output = self.dropout(pooled_output)

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in module._call_impl(self, *input, **kwargs)    1126 # if we don't have any hooks, we want to skip the rest of the logic in    1127 # this function, and just call forward.    1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks    1129         or
_global_forward_hooks or _global_forward_pre_hooks):
-> 1130     return forward_call(*input, **kwargs)    1131 # do not call functions when jit is used    1132 full_backward_hooks, non_full_backward_hooks = [], []

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1011, in bertmodel.forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)    1004 # prepare head mask if needed    1005 # 1.0 in head_mask indicate we keep the head    1006 # attention_probs has shape bsz x n_heads x n x n    1007 # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]    1008 # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]    1009 head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)
-> 1011 embedding_output = self.embeddings(    1012     input_ids=input_ids,    1013     position_ids=position_ids,    1014    token_type_ids=token_type_ids,    1015     inputs_embeds=inputs_embeds,    1016     past_key_values_length=past_key_values_length,    1017 )    1018 encoder_outputs = self.encoder(    1019     embedding_output,    1020  attention_mask=extended_attention_mask,    (...)    1028     return_dict=return_dict,    1029 )    1030 sequence_output = encoder_outputs[0]

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in module._call_impl(self, *input, **kwargs)    1126 # if we don't have any hooks, we want to skip the rest of the logic in    1127 # this function, and just call forward.    1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks    1129         or
_global_forward_hooks or _global_forward_pre_hooks):
-> 1130     return forward_call(*input, **kwargs)    1131 # do not call functions when jit is used    1132 full_backward_hooks, non_full_backward_hooks = [], []

file /nfs/workspaces/virtualenvs/nlpspark/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:241, in bertembeddings.forward(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)
    239 if self.position_embedding_type == ""absolute"":
    240     position_embeddings = self.position_embeddings(position_ids)
--> 241     embeddings += position_embeddings
    242 embeddings = self.layernorm(embeddings)
    243 embeddings = self.dropout(embeddings)

df_0 is spark dataframe, which include huge data. my question is how to feed this dataframe into the pipeline with entire data, or with batch size.",['huggingface-transformers'],73255205,"the error you get (please always post the full error stacktrace in the future), is not caused by the size of a, it is caused by one of the texts exceeding the length your model can handle. your model can handle up to 512 tokens and you need to truncate your input otherwise:
from transformers import pipeline
my_pipeline  = pipeline(""text-classification"", model=""distilbert-base-uncased-finetuned-sst-2-english"")

te = ""this is a long text ""*1024
print(te)
print(len(my_pipeline.tokenizer.tokenize(te)))
my_pipeline(te, truncation=true)

output:
this is a long text this is a long text this is a long text this is a long text this is a long text ...
5120
[{'label': 'negative', 'score': 0.9979830980300903}]

the pipeline object will process a list with one sample at a time. you can try to speed up the classification by specifying a batch_size, however, note that it is not necessarily faster and depends on the model and hardware:
te_list = [te]*10
my_pipeline(te_list, batch_size=5, truncation=true,)",https://stackoverflow.com/questions/73251309,huggingface-transformers,05-08-2022 14:26,4433.0,2.0,1.0,True,04-04-2025 23:03,04-04-2025 23:03
53309192,similarity between two lists of documents,"i need to find the similarity between two lists of the short texts in python.
texts can be 1-4 word long. the length of the lists can be 10k each. so, i need to effectively calculate 10k*10k=100m similarity scores.
i didn't find how to do this effectively in spacy. maybe other packages can do this?
i assume the words are represented by a vector (300d), but any other options are also ok.
this task can be done in a cycle, but there should be a more effective way for sure. this task fits the tensorflow, pytorch, and similar packages, but i'm not familiar with details of these packages.","['tensorflow', 'nlp', 'similarity', 'spacy', 'sentence-similarity']",71697660,the solution was to use something like spotify annoy which uses approximate nearest neighbours method. there are some other libraries to do the nearest neighbour search.,https://stackoverflow.com/questions/53309192,tensorflow,14-11-2018 21:45,1774.0,0.0,2.0,True,06-12-2023 04:48,31-03-2022 18:40
72628556,&#39;maskedlmoutput&#39; object has no attribute &#39;view&#39;,"i wrote this:
def forward(self, x):
    x = self.bert(x)
    
    x = x.view(x.shape[0], -1)
    x = self.fc(self.dropout(self.bn(x)))
    return x

but it doesn't work well, and the error is 'maskedlmoutput' object has no attribute 'view'.
i'm considering the input might not be 'tensor' type, so i change it as below:
def forward(self, x):
        x = torch.tensor(x)     # this part
        x = self.bert(x)
        
        x = x.view(x.shape[0], -1)
        x = self.fc(self.dropout(self.bn(x)))
        return x

but it still gets wrong, same error 'maskedlmoutput' object has no attribute 'view'.
could someone tell me how to fix this?  much thanks.
whole error information here:
 ------------------------------------------------------------------------
    attributeerror                            traceback (most recent call last)
    input in [5], in <cell line: 8>()
          6 optimizer = optim.adam(bert_punc.parameters(), lr=learning_rate_top)
          7 criterion = nn.crossentropyloss()
    ----> 8 bert_punc, optimizer, best_val_loss = train(bert_punc, optimizer, criterion, epochs_top, 
          9     data_loader_train, data_loader_valid, save_path, punctuation_enc, iterations_top, best_val_loss=1e9)
    
    input in [3], in train(model, optimizer, criterion, epochs, data_loader_train, data_loader_valid, save_path, punctuation_enc, iterations, best_val_loss)
         17 inputs.requires_grad = false
         18 labels.requires_grad = false
    ---> 19 output = model(inputs)
         20 loss = criterion(output, labels)
         21 loss.backward()
    
    file ~\anaconda3\lib\site-packages\torch\nn\modules\module.py:1110, in module._call_impl(self, *input, **kwargs)
       1106 # if we don't have any hooks, we want to skip the rest of the logic in
       1107 # this function, and just call forward.
       1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
       1109         or _global_forward_hooks or _global_forward_pre_hooks):
    -> 1110     return forward_call(*input, **kwargs)
       1111 # do not call functions when jit is used
       1112 full_backward_hooks, non_full_backward_hooks = [], []
    
    file ~\anaconda3\lib\site-packages\torch\nn\parallel\data_parallel.py:166, in dataparallel.forward(self, *inputs, **kwargs)
        163     kwargs = ({},)
        165 if len(self.device_ids) == 1:
    --> 166     return self.module(*inputs[0], **kwargs[0])
        167 replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
        168 outputs = self.parallel_apply(replicas, inputs, kwargs)
    
    file ~\anaconda3\lib\site-packages\torch\nn\modules\module.py:1110, in module._call_impl(self, *input, **kwargs)
       1106 # if we don't have any hooks, we want to skip the rest of the logic in
       1107 # this function, and just call forward.
       1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
       1109         or _global_forward_hooks or _global_forward_pre_hooks):
    -> 1110     return forward_call(*input, **kwargs)
       1111 # do not call functions when jit is used
       1112 full_backward_hooks, non_full_backward_hooks = [], []
    
    file d:\bertpunc-original\model.py:21, in bertpunc.forward(self, x)
         18 x = torch.tensor(x)
         19 x = self.bert(x)
    ---> 21 x = x.view(x.shape[0], -1)
         22 x = self.fc(self.dropout(self.bn(x)))
         23 return x
    
    attributeerror: 'maskedlmoutput' object has no attribute 'view'","['python', 'tensorflow', 'pytorch', 'huggingface-transformers', 'bert-language-model']",72628798,"i think so this should help you solve the error. 
the output after self.bert(x) is an object of the class maskedlmoutput.",https://stackoverflow.com/questions/72628556,python,15-06-2022 08:59,1886.0,0.0,1.0,True,15-06-2022 09:16,15-06-2022 09:10
18902608,generating the plural form of a noun,"given a word, which may or may not be a singular-form noun, how would you generate its plural form?
based on this nltk tutorial and this informal list on pluralization rules, i wrote this simple function:
def plural(word):
    """"""
    converts a word to its plural form.
    """"""
    if word in c.plurale_tantums:
        # defective nouns, fish, deer, etc
        return word
    elif word in c.irregular_nouns:
        # foot->feet, person->people, etc
        return c.irregular_nouns[word]
    elif word.endswith('fe'):
        # wolf -> wolves
        return word[:-2] + 'ves'
    elif word.endswith('f'):
        # knife -> knives
        return word[:-1] + 'ves'
    elif word.endswith('o'):
        # potato -> potatoes
        return word + 'es'
    elif word.endswith('us'):
        # cactus -> cacti
        return word[:-2] + 'i'
    elif word.endswith('on'):
        # criterion -> criteria
        return word[:-2] + 'a'
    elif word.endswith('y'):
        # community -> communities
        return word[:-1] + 'ies'
    elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:
        return word + 'es'
    elif word.endswith('an'):
        return word[:-2] + 'en'
    else:
        return word + 's'

but i think this is incomplete. is there a better way to do this?","['python', 'nlp', 'wordnet', 'linguistics']",19018986,"the pattern-en package offers pluralization
>>> import pattern.text.en
>>> pattern.text.en.pluralize(""dog"")
'dogs'

note also that in order to run the import above successfully, you may have to first execute the following (at least the first time):
>>> import nltk
>>> nltk.download('omw-1.4')",https://stackoverflow.com/questions/18902608,python,19-09-2013 18:44,29817.0,14.0,5.0,True,03-06-2024 02:39,20-09-2013 13:35
3541371,php: how do i detect if an input string is arabic,is there a way to detect the language of the data being entered via the input field?,"['php', 'language-detection']",3542914,"hmm i may offer an improved version of dimakrasun's function:
functoin is_arabic($string) {
    if($string === 'arabic') {
         return true;
    }
    return false;
}

okay, enough joking!
pekkas suggestion to use the google translate api is a good one! but you are relying on an external service which is always more complicated etc.
i think rushyos approch is good! its just not that easy.
i wrote the following function for you but its not tested, but it should work...
    <?
function uniord($u) {
    // i just copied this function fron the php.net comments, but it should work fine!
    $k = mb_convert_encoding($u, 'ucs-2le', 'utf-8');
    $k1 = ord(substr($k, 0, 1));
    $k2 = ord(substr($k, 1, 1));
    return $k2 * 256 + $k1;
}
function is_arabic($str) {
    if(mb_detect_encoding($str) !== 'utf-8') {
        $str = mb_convert_encoding($str,mb_detect_encoding($str),'utf-8');
    }

    /*
    $str = str_split($str); <- this function is not mb safe, it splits by bytes, not characters. we cannot use it
    $str = preg_split('//u',$str); <- this function woulrd probably work fine but there was a bug reported in some php version so it pslits by bytes and not chars as well
    */
    preg_match_all('/.|\n/u', $str, $matches);
    $chars = $matches[0];
    $arabic_count = 0;
    $latin_count = 0;
    $total_count = 0;
    foreach($chars as $char) {
        //$pos = ord($char); we cant use that, its not binary safe 
        $pos = uniord($char);
        echo $char ."" --> "".$pos.php_eol;

        if($pos >= 1536 && $pos <= 1791) {
            $arabic_count++;
        } else if($pos > 123 && $pos < 123) {
            $latin_count++;
        }
        $total_count++;
    }
    if(($arabic_count/$total_count) > 0.6) {
        // 60% arabic chars, its probably arabic
        return true;
    }
    return false;
}
$arabic = is_arabic('ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½. ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½'); 
var_dump($arabic);
?>

final thoughts:
as you see i added for example a latin counter, the range is just a dummy number b ut this way you could detect charsets (hebrew, latin, arabic, hindi, chinese, etc...) 
you may also want to all the character sets and see which one of course the most... 
and finally you should consider chopping your string off after 200 chars or something. this should be enough to tell what character set is used.
and you have to do some error handling! like division by zero, empty string etc etc! don't forget that please... any questions? comment!
if you want to detect the language of a string, you should split into words and check for the words in some pre-defined tables. you don't need a complete dictionary, just the most common words and it should work fine. tokenization/normalization is a must as well! there are libraries for that anyway and this is not what you asked for :) just wanted to mention it",https://stackoverflow.com/questions/3541371,php,22-08-2010 11:53,27408.0,19.0,10.0,True,21-01-2022 09:07,03-11-2010 00:09
69364068,"loading tf.keras model, valueerror: the two structures don&#39;t have the same nested structure","i created a tf.keras model that has bert and i want to train and save it for further use.
loading this model is a big issue cause i keep getting error: valueerror: the two structures don't have the same nested structure.
i simplified the model a lot, to see where is the problem exactly. the code is pretty simple:
bert = tfbertmodel.from_pretrained(""bert-base-german-cased"")

model_name = ""model""
txt12_input_ids = tf.keras.layers.input(shape=(max_length,),  name='txt12_input_ids', dtype='int32')
txt12_mask      = tf.keras.layers.input(shape=(max_length,),  name='txt12_mask', dtype='int32')
txt12_outputs = bert(txt12_input_ids, txt12_mask).pooler_output

model_k = tf.keras.model(inputs=(txt12_input_ids,  txt12_mask), outputs=txt12_outputs, name=model_name)
model_k.compile(optimizer=adam(1e-5), loss=""binary_crossentropy"", metrics=""accuracy"")


model_k.save(dir_path+'prob')
model_2 = tf.keras.models.load_model(dir_path+'prob')

some notes before you start replying:

i did specified dtype.

no, i don't want to save just weights.

i tried to use tf.keras.models.save_model(model_k, dir_path+'prob') instead and it gives the same error.


and the last thing, i work with tf version: 2.6.0. does anyone knows how to solve it?
full error message:
valueerror: the two structures don't have the same nested structure.

first structure: type=tuple str=(({'input_ids': tensorspec(shape=(none, 5), dtype=tf.int32, name='input_ids/input_ids')}, none, none, none, none, none, none, none, none, false), {})

second structure: type=tuple str=((tensorspec(shape=(none, 120), dtype=tf.int32, name='input_ids'), tensorspec(shape=(none, 120), dtype=tf.int32, name='attention_mask'), none, none, none, none, none, none, none, false), {})

more specifically: substructure ""type=dict str={'input_ids': tensorspec(shape=(none, 5), dtype=tf.int32, name='input_ids/input_ids')}"" is a sequence, while substructure ""type=tensorspec str=tensorspec(shape=(none, 120), dtype=tf.int32, name='input_ids')"" is not
entire first structure:
(({'input_ids': .}, ., ., ., ., ., ., ., ., .), {})
entire second structure:
((., ., ., ., ., ., ., ., ., .), {})","['python', 'tensorflow', 'keras', 'huggingface-transformers', 'bert-language-model']",69451206,check out the issue here on github. it should help you to solve the problem with the shape.,https://stackoverflow.com/questions/69364068,python,28-09-2021 14:57,3893.0,4.0,3.0,True,21-01-2025 07:29,29-09-2021 14:37
76070777,powerbi custom visual with chatgpt,"i am developing a custom visual into power bi using typescript. i have an input of type text for user input prompt and an input of type text for chatgpt answer. the idea is that the user can ask anything about report's data or any report's visual and get an answer. the visual at the current stage looks like this:

behind the scenes the user prompt is sent to azure-openai service and is being processed by chatgpt deployment to get the response. the only part which is missing is to be able to pass also the report's data. i have seen a similar video doing this with powerautomate visual, here is the video: 

in this video we are able to pass though report's data though power automate visual into user prompt in order to be analyzed together with the question on our data:

i managed to do the same by passing visual's data in a structured json format along with prompt and seems to working, but the question is if it is possible to get report's data though typescript into the custom visual without having the dataset on the visual it self?
i tried already a library called powerbi client inside my custom visual but with any use of this library the visual stop working (i think this can be used only with powerbi embedded):




based on this article is not possible to use a custom visual and access data on page or report scope level: 
any ideas?","['typescript', 'powerbi', 'openai-api', 'powerbi-custom-visuals', 'chatgpt-api']",78388995,"after a lot of research, at the moment, this is not support and cannot be done though powerbi, as we cannot access the whole reports data, so i changed the direction by creating a langchain server inside a kubernetes cluster accessing directly the respective database which exists also inside the cluster. this service is capable of converting questions to sql then take the sql query, execute it inside infrastructure and push again the result to chatgpt to generate a html visual.",https://stackoverflow.com/questions/76070777,typescript,21-04-2023 07:13,649.0,1.0,1.0,True,26-04-2024 07:54,02-05-2023 09:39
72380308,split player and chat from chat log (text-mining),"i have a chat log which includes 4 players (a, b, c, d) and their chats in one row in my data frame (across many groups). i want to split each phrase into its own row and identify the speaker of that phrase in a separate column.
i have attempted many things using the following packages but haven't been able to succeed.
psych
dplyr
splitstackshape
tidytext
stringr
tidyr
the data frame is not a txt.document, but i'm thinking it needs to be?
for example this is what the chat log looks like. this is all in one row in my dataset.
[1] ""ï¿½ï¿½ï¿½*** d has joined the chat ***""                                                                                                                                         
  [2] ""ï¿½ï¿½ï¿½*** b has joined the chat ***""                                                                                                                                         
  [3] ""ï¿½ï¿½ï¿½*** a has joined the chat ***""                                                                                                                       
  [4] ""d: hi""                                                                                                                                                                  
  [5] ""b: hello!""                                                                                                                                                              
  [6] ""a: hi!""                                                                                                                                                                 
  [7] ""d: i think oxygen is most important""                                                                                                                                    
  [8] ""a: i do too""                                                                                                                                                            
  [9] ""ï¿½ï¿½ï¿½*** c has joined the chat ***""                                                                                                                                         
 [10] ""b: agreed, that was my #1""                                                                                                                                              
 [11] ""a: i didnt at first but then on second guess""                                                                                                                           
 [12] ""a: oxygen then water""                                                                                                                                                   
 [13] ""c: hi hi""                                                              

i want the following (to have these columns where each row is a new phrase)




player id
phrase




a
hi!


b
hello!




i want to eventually use this to count # of words/characters per player","['r', 'text-mining']",72382010,"library(dplyr)
library(tidyr)

d %>%
  t() %>%
  as.data.frame(""v1"") %>%
  filter(!grepl(""***"", v1, fixed = true)) %>%
  separate(v1, into = c(""playerid"", ""phrase""), sep = "": "") %>%
  mutate(count = nchar(phrase))

result:
#>   playerid                                    phrase count
#> 1        d                                        hi     2
#> 2        b                                    hello!     6
#> 3        a                                       hi!     3
#> 4        d          i think oxygen is most important    32
#> 5        a                                  i do too     8
#> 6        b                    agreed, that was my #1    22
#> 7        a i didnt at first but then on second guess    41
#> 8        a                         oxygen then water    17
#> 9        c                                     hi hi     5

you could use add this to the dplyr chain to count the number of characters per player:
group_by(playerid) %>%
summarize(total = sum(count))

#>   playerid total
#>   <chr>    <int>
#> 1 a           69
#> 2 b           28
#> 3 c            5
#> 4 d           34

data:
d <- structure(c("" *** d has joined the chat ***"", "" *** b has joined the chat ***"", 
                 "" *** a has joined the chat ***"", ""d: hi"", ""b: hello!"", ""a: hi!"", 
                 ""d: i think oxygen is most important"", ""a: i do too"", "" *** c has joined the chat ***"", 
                 ""b: agreed, that was my #1"", ""a: i didnt at first but then on second guess"", 
                 ""a: oxygen then water"", ""c: hi hi""), .dim = c(1l, 13l))

created on 2022-05-25 by the reprex package (v2.0.1)",https://stackoverflow.com/questions/72380308,r,25-05-2022 15:19,76.0,0.0,1.0,True,25-05-2022 17:57,25-05-2022 17:38
70496137,can we calculate feature importance in huggingface bert?,"we can fit a linearregression model on the regression dataset and retrieve the coeff_ property that contains the coefficients found for each input variable. these coefficients can provide the basis for a crude feature importance score. this assumes that the input variables have the same scale or have been scaled prior to fitting a model.

what about bert? can we get coef_ variable from the model and use it to calculate feature importance like linearregression model in text classification task?","['nlp', 'huggingface-transformers', 'bert-language-model']",70500609,captum is a prominent tool (from  pytorch/facebook) for interpreting transformers and you can get a score for the attention the model pays to specific tokens at specific layers. see a tutorial here:  or here,https://stackoverflow.com/questions/70496137,nlp,27-12-2021 13:29,2649.0,3.0,1.0,True,27-12-2021 21:30,27-12-2021 18:27
65527427,extract named entities using spacy and python lambda,"i am using following code to extract named entities using lambda.
df['place'] = df['text'].apply(lambda x: [entity.text for entity in nlp(x).ents if entity.label_ == 'gpe'])

and
df['text'].apply(lambda x: ([entity.text for entity in nlp(x).ents if entity.label_ == 'gpe'] or [''])[0])

for a few hundred records it can extract results. but when it comes to thousands of records. it takes pretty much forever. can someone help me to optimize this line of code?","['python', 'nlp', 'spacy', 'named-entity-extraction']",65529502,"you may improve by:

calling nlp.pipe on the whole list of documents
disabling unnecessary pipes.

try:
import spacy
nlp = spacy.load(""en_core_web_md"", disable = [""tagger"",""parser""])

df = pd.dataframe({""text"":[""this is a text about germany"",""this is another about trump""]})

texts = df[""text""].to_list()
ents = []
for doc in nlp.pipe(texts):
    for ent in doc.ents:
        if ent.label_ == ""gpe"":
            ents.append(ent)
            
print(ents)


[germany]",https://stackoverflow.com/questions/65527427,python,01-01-2021 05:21,835.0,0.0,1.0,True,01-01-2021 17:19,01-01-2021 17:19
76000137,openai embeddings api: how embeddings work?,"there are quite a few tutorials on embeddings in openai. i can't understand how they work.
referring to  , an embedding is a vector or list. a string is passed to an embedding model and the model returns a number (in simplest terms). i can use this number(s).
if i use a simple string to get its embeddings, i get a massive list
result = get_embedding(""i live in space"", engine = ""textsearchcuriedoc001mc"")

result when printed
[5.4967957112239674e-05,
 -0.01301578339189291,
 -0.002223075833171606,
 0.013594076968729496,
 -0.027540158480405807,
 0.008867159485816956,
 0.009403547272086143,
 -0.010987567715346813,
 0.01919262297451496,
 0.022209804505109787,
 -0.01397960539907217,
 -0.012806257233023643,
 -0.027908924967050552,
 0.013074451126158237,
 0.024942029267549515,
 0.0200139675289392 , ..... -> truncated this much, much, much longer list 

question 1 - how is this massive list correlated with my 4-word text?
question 2 -
i create embeddings of the text i want to use in query. note that it is exactly the same as the text of original content i live in space
queryembedding = get_embedding(
        'i live in space',
        engine=""textsearchcuriequery001mc""
    )
queryembedding

when i run cosine similarity , the value is 0.42056650555103214.
similarity = cosine_similarity(embeddings_of_i_live,queryembedding)
similarity

i get value 0.42056650555103214
shouldn't the value be 1 to indicate identical value?","['azure', 'openai-api', 'azure-openai']",76003658,"q1:

how is this massive list correlated with my 4-word text?

a1: let's say you want to use the openai text-embedding-ada-002 model. no matter what your input is, you will always get a 1536-dimensional embedding vector (i.e., there are 1536 numbers inside). you are probably familiar with 3-dimensional space (i.e., x, y, z). well, this is a 1536-dimensional space, which is very hard to imagine. why are there exactly 1536 numbers inside the embedding vector? because the text-embedding-ada-002 model has an output dimension of 1536. it's pre-defined.

q2:

i create embeddings of the text i want to use in the query. note that it
is exactly the same as the text of the original content: i live in space.
when i run cosine similarity, the value is 0.42056650555103214.
should the value be 1 to indicate an identical value?

a2: yes, the value should be 1 if you calculate cosine similarity between two identical texts. see an example here.
for an example of semantic search based on embeddings, see this answer.",https://stackoverflow.com/questions/76000137,azure,12-04-2023 21:56,6159.0,4.0,1.0,True,04-07-2023 07:10,21-04-2023 15:50
39850826,how can i convert pcfg in cnf for this grammar?,"given the following probabilistic context-free grammar - 
1.np -> adj n [0.6]
2.np -> n     [0.4] 
3.n  -> cat   [0.2] 
4.n  -> dog   [0.8]

what will be the cnf??","['nlp', 'stanford-nlp', 'cnf']",42428525,"given pcfg in cnf is given below.
1.np -> adj n [0.6]
2.np -> cat   [0.08] 
3.np -> dog   [0.32] 

because you need to get the same probability for the result by applying both the original and the converted set of rules (in cnf).",https://stackoverflow.com/questions/39850826,nlp,04-10-2016 11:13,1024.0,1.0,2.0,True,16-01-2021 02:13,23-02-2017 23:55
1787110,what is the difference between lemmatization vs stemming?,"when do i use each ?
also...is the nltk lemmatization dependent upon parts of speech?
wouldn't it be more accurate if it was?","['nlp', 'nltk', 'lemmatization']",1787121,"short and dense: 

the goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.
however, the two words differ in their flavor. stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .

from the nltk docs:

lemmatization and stemming are special cases of normalization. they identify a canonical representative for a set of related word forms.",https://stackoverflow.com/questions/1787110,nlp,24-11-2009 00:48,169254.0,207.0,15.0,True,26-02-2024 04:36,16-12-2021 09:03
65674941,how do i leverage spark&#39;s pipelines to find phrases in strings then add feature category?,"i would like to search my text column in a pyspark data frame for phrases. here is an example to show you what i mean.
sentencedata = spark.createdataframe([
(0, ""hi i heard about spark""),
(4, ""i wish java could use case classes""),
(11, ""logistic regression models are neat"")], 
[""id"", ""sentence""])

if the sentence contains ""heard about spark"" then categoryspark=1 and categoryheard=1.
if the sentence contains ""java or regression"" then categorycool=1.
i have about 28 booleans (or maybe better if i use regex) to check for.
sentencedata.withcolumn('categorycool',sentencedata['sentence'].rlike('java | regression')).show()

returns:
+---+--------------------+------------+
| id|            sentence|categorycool|
+---+--------------------+------------+
|  0|hi i heard about ...|       false|
|  4|i wish java could...|        true|
| 11|logistic regressi...|        true|
+---+--------------------+------------+

this is what i want, but i'd like to add it to a pipeline as a transformation step.","['apache-spark', 'pyspark', 'nlp', 'feature-extraction']",65675709,"i found this nice medium article and this s.o. answer which i combined to answer my own question! i hope someone finds this helpful someday.
    from pyspark.ml.pipeline import transformer
    from pyspark.ml import pipeline
    from pyspark.sql.types import *
    from pyspark.ml.util import identifiable
    
    sentencedata = spark.createdataframe([
        (0, ""hi i heard about spark""),
        (4, ""i wish java could use case classes""),
        (11, ""logistic regression models are neat"")
    ], [""id"", ""sentence""])
    
    class onesearchmultilabelextractor(transformer):
        def __init__(self, rlikesearch, outputcols, inputcol = 'fulltext'):
            self.inputcol = inputcol
            self.outputcols = outputcols
            self.rlikesearch = rlikesearch
            self.uid = str(identifiable())
        def copy(extra):
            defaultcopy(extra)
        def check_input_type(self, schema):
            field = schema[self.inputcol]
            if (field.datatype != stringtype()):
                raise exception('onesearchmultilabelextractor input type %s did not match input type stringtype' % field.datatype)
        def check_output_type(self):
            if not (isinstance(self.outputcols,list)):
                raise exception('onesearchmultilabelextractor output columns must be a list')
        def _transform(self, df):
            self.check_input_type(df.schema)
            self.check_output_type()
            df = df.withcolumn(""searchresult"", df[self.inputcol].rlike(self.rlikesearch)).cache()
            for outputcol in self.outputcols:
                df = df.withcolumn(outputcol, df[""searchresult""])
            return df.drop(""searchresult"")
            
    dex = coolextractor(inputcol='sentence',rlikesearch='java | regression',outputcols=['coolcategory'])
    featurespipeline =  pipeline(stages=[dex])
    featpip = featurespipeline.fit(sentencedata)
    featpip.transform(sentencedata).show()",https://stackoverflow.com/questions/65674941,apache-spark,11-01-2021 21:42,162.0,0.0,1.0,True,13-01-2021 17:56,12-01-2021 17:09
74875388,how do i extract the text of a single page with pypdf2?,"i have a document library which consists of several hundred pdf documents. i am attempting to export the first page of each pdf document. below is my script which extracts the page. it saves each page as an individual pdf. however, the files which are exported seem to be exporting in unreadable or damaged format.
is there something missing from my script?
import os
from pypdf2 import pdfreader, pdfwriter

# get the file names in the directory
input_directory = ""fund_docs_sample""
entries = os.listdir(input_directory)
output_directory = ""first pages""
outputs = os.listdir(output_directory)

for output_file_name in entries:
    reader = pdfreader(input_directory + ""/"" + output_file_name)
    page = reader.pages[0]
    first_page = ""\n"" + page.extract_text() + ""\n""

    with open(output_file_name, ""wb"") as outputstream:
        pdf_writer = pdfwriter(output_file_name + first_page)","['python', 'data-science', 'text-mining', 'pypdf']",74882928,"you're missing pdf_writer.write(outputstream)
do you want to write a text file (containing the extracted text) or a pdf file (containing the first page of the pdf)?
you seem to overwrite the files of the input
output_directory is not used at all

after reading the comments, you likely want this:
from pathlib import path
from pypdf2 import pdfreader

# get the file names in the directory
input_directory = path(""fund_docs_sample"")
output_directory = path(""first pages"")

for input_file_path in input_directory.glob(""*.pdf""):
    print(input_file_path)
    reader = pdfreader(input_file_path)
    page = reader.pages[0]
    first_page_text = ""\n"" + page.extract_text() + ""\n""
    
    # create the output text file path
    output_file_path = output_directory / f""{input_file_path.name}.txt""
    
    # write the text to the output file
    with open(output_file_path, ""w"") as output_file:
        output_file.write(first_page_text)",https://stackoverflow.com/questions/74875388,python,21-12-2022 11:29,1441.0,0.0,2.0,True,23-12-2022 11:20,23-12-2022 11:20
69564293,pyspark - counting particular words in sentences,"i have a pyspark dataframe with a column that contains textual content.
i am trying to count the number of sentences that contain an exclamation mark '!' along with the word ""like"" and ""want"".
for example: the column with a row that contains the following sentences:
i don't like to sing!
i like to go shopping!
i want to go home!
i like fast food. 
you don't want to!
what does he want?

the desired output i'm hoping to achieve would like something like this (only counts the sentences that contain ""like"" or ""want"" and ""!""):
+----+-----+
|word|count|
+----+-----+
|like|   2 |
|want|   2 |
+----+-----+

can someone help me with writing a udf that can do this? this is what i have written so far, but i can't seem to get it to work.
nltk.tokenize import sent_tokenize

def convert_a_sentence(a_string):
    sentence = lower(nltk.sent_tokenize(a_string))
    return sentence

df = df.withcolumn('a_sentence', convert_a_sentence(df['text']))

df.select(explode('a_sentence').alias('found')).filter(df['a_sentence'].isin('like', 'want', '!').groupby('found').count().collect()","['python', 'apache-spark', 'pyspark', 'data-science', 'nltk']",69564772,"if all you want is uni-gram (i.e 1 token), you can just split the sentence by space, then explode, group by, count then filter what you wants
(df
    .withcolumn('words', f.split('sentence', ' '))
    .withcolumn('word', f.explode('words'))
    .groupby('word')
    .agg(
        f.count('*').alias('word_cnt')
    )
    .where(f.col('word').isin(['like', 'want']))
    .show()
)

# output
# +----+--------+
# |word|word_cnt|
# +----+--------+
# |want|       2|
# |like|       3|
# +----+--------+

note #1: you can apply filter before groupby, with contains function
note #2: if you ever want to do n-gram instead of ""hacking"" like above, you can consider using sparkml package with tokenizer
from pyspark.ml.feature import tokenizer

tokenizer = tokenizer(inputcol='sentence', outputcol=""words"")
tokenized = tokenizer.transform(df)

# output
# +----------------------+----------------------------+
# |sentence              |words                       |
# +----------------------+----------------------------+
# |i don't like to sing! |[i, don't, like, to, sing!] |
# |i like to go shopping!|[i, like, to, go, shopping!]|
# |i want to go home!    |[i, want, to, go, home!]    |
# |i like fast food.     |[i, like, fast, food.]      |
# |you don't want to!    |[you, don't, want, to!]     |
# |what does he want?    |[what, does, he, want?]     |
# +----------------------+----------------------------+

or ngram
from pyspark.ml.feature import ngram

ngram = ngram(n=2, inputcol=""words"", outputcol=""ngrams"")
ngramed = ngram.transform(tokenized)

# output
# +----------------------+----------------------------+----------------------------------------+
# |col                   |words                       |ngrams                                  |
# +----------------------+----------------------------+----------------------------------------+
# |i don't like to sing! |[i, don't, like, to, sing!] |[i don't, don't like, like to, to sing!]|
# |i like to go shopping!|[i, like, to, go, shopping!]|[i like, like to, to go, go shopping!]  |
# |i want to go home!    |[i, want, to, go, home!]    |[i want, want to, to go, go home!]      |
# |i like fast food.     |[i, like, fast, food.]      |[i like, like fast, fast food.]         |
# |you don't want to!    |[you, don't, want, to!]     |[you don't, don't want, want to!]       |
# |what does he want?    |[what, does, he, want?]     |[what does, does he, he want?]          |
# +----------------------+----------------------------+----------------------------------------+",https://stackoverflow.com/questions/69564293,python,14-10-2021 02:07,434.0,0.0,2.0,True,14-10-2021 03:44,14-10-2021 03:01
76056193,tokenclassificationchunkpipeline is throwing error: &#39;batchencoding&#39; object is not an iterator,"following this huggingface anonymisation tutorial.
using pytorch 2.0.0 and transformers-4.28.1
running the code as it is, i get an error over the custom pipeline:
def anonymize(text):
    ents = pipe(text) # this errors out
    ...
typeerror: 'batchencoding' object is not an iterator

i realise it's a tokenizer issue,
class tokenclassificationchunkpipeline(tokenclassificationpipeline):
def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)

def preprocess(self, sentence, offset_mapping=none):
    model_inputs = self.tokenizer(
        sentence,
        return_tensors=""pt"",
        truncation=true,
        return_special_tokens_mask=true,
        return_offsets_mapping=true,
        return_overflowing_tokens=true,  # return multiple chunks
        max_length=self.tokenizer.model_max_length,
        padding=true
    )
    if offset_mapping:
        model_inputs[""offset_mapping""] = offset_mapping

    model_inputs[""sentence""] = sentence

    return model_inputs

this model_inputs is a

<class 'transformers.tokenization_utils_base.batchencoding'>

how can i make an iterator batchencoding object?
else, is there another way?
for full code, please visit the tutorial link above.","['pytorch', 'nlp', 'huggingface-transformers', 'torch', 'named-entity-recognition']",76058924,"not sure why the pipeline was coded that way in the blogpost, but here's a working version:
import torch
from transformers import autotokenizer, automodelfortokenclassification
from transformers.pipelines.token_classification import tokenclassificationpipeline

model_checkpoint = ""davlan/bert-base-multilingual-cased-ner-hrl""

tokenizer = autotokenizer.from_pretrained(model_checkpoint)
model = automodelfortokenclassification.from_pretrained(model_checkpoint)


class tokenclassificationchunkpipeline(tokenclassificationpipeline):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def preprocess(self, sentence, offset_mapping=none, **preprocess_params):
        tokenizer_params = preprocess_params.pop(""tokenizer_params"", {})
        truncation = true if self.tokenizer.model_max_length and self.tokenizer.model_max_length > 0 else false
        inputs = self.tokenizer(
            sentence,
            return_tensors=""pt"",
            truncation=true,
            return_special_tokens_mask=true,
            return_offsets_mapping=true,
            return_overflowing_tokens=true,  # return multiple chunks
            max_length=self.tokenizer.model_max_length,
            padding=true
        )
        #inputs.pop(""overflow_to_sample_mapping"", none)
        num_chunks = len(inputs[""input_ids""])

        for i in range(num_chunks):
            if self.framework == ""tf"":
                model_inputs = {k: tf.expand_dims(v[i], 0) for k, v in inputs.items()}
            else:
                model_inputs = {k: v[i].unsqueeze(0) for k, v in inputs.items()}
            if offset_mapping is not none:
                model_inputs[""offset_mapping""] = offset_mapping
            model_inputs[""sentence""] = sentence if i == 0 else none
            model_inputs[""is_last""] = i == num_chunks - 1
            yield model_inputs

    def _forward(self, model_inputs):
        # forward
        special_tokens_mask = model_inputs.pop(""special_tokens_mask"")
        offset_mapping = model_inputs.pop(""offset_mapping"", none)
        sentence = model_inputs.pop(""sentence"")
        is_last = model_inputs.pop(""is_last"")

        overflow_to_sample_mapping = model_inputs.pop(""overflow_to_sample_mapping"")

        output = self.model(**model_inputs)
        logits = output[""logits""] if isinstance(output, dict) else output[0]


        model_outputs = {
            ""logits"": logits,
            ""special_tokens_mask"": special_tokens_mask,
            ""offset_mapping"": offset_mapping,
            ""sentence"": sentence,
            ""overflow_to_sample_mapping"": overflow_to_sample_mapping,
            ""is_last"": is_last,
            **model_inputs,
        }

        # we reshape outputs to fit with the postprocess inputs
        model_outputs[""input_ids""] = torch.reshape(model_outputs[""input_ids""], (1, -1))
        model_outputs[""token_type_ids""] = torch.reshape(model_outputs[""token_type_ids""], (1, -1))
        model_outputs[""attention_mask""] = torch.reshape(model_outputs[""attention_mask""], (1, -1))
        model_outputs[""special_tokens_mask""] = torch.reshape(model_outputs[""special_tokens_mask""], (1, -1))
        model_outputs[""offset_mapping""] = torch.reshape(model_outputs[""offset_mapping""], (1, -1, 2))

        return model_outputs


pipe = tokenclassificationchunkpipeline(model=model, tokenizer=tokenizer, aggregation_strategy=""simple"")

pipe(""bernard works at bnp paribas in paris."")


[out]:
[{'entity_group': 'per',
  'score': 0.9994497,
  'word': 'bernard',
  'start': 0,
  'end': 7},
 {'entity_group': 'org',
  'score': 0.9997708,
  'word': 'bnp paribas',
  'start': 17,
  'end': 28},
 {'entity_group': 'loc',
  'score': 0.99906,
  'word': 'paris',
  'start': 32,
  'end': 37}]

for reference, take a look at how the preproces() and the _forward() functions are coded in the tokenclassificationpipeline class, 
the preprocess should return a generator, that's why the _forward is expecting a generator and complains typeerror: 'batchencoding' object is not an iterator.",https://stackoverflow.com/questions/76056193,pytorch,19-04-2023 15:17,1134.0,1.0,1.0,True,19-04-2023 21:26,19-04-2023 21:26
8897593,how to compute the similarity between two text documents?,i want to take two documents and determine how similar they are. any programming language if fine but i prefer python.,"['python', 'nlp']",8897648,"the common way of doing this is to transform the documents into tf-idf vectors and then compute the cosine similarity between them. any textbook on information retrieval (ir) covers this. see esp. introduction to information retrieval, which is free and available online.
computing pairwise similarities
tf-idf (and similar text transformations) are implemented in the python packages gensim and scikit-learn. in the latter package, computing cosine similarities is as easy as
from sklearn.feature_extraction.text import tfidfvectorizer

documents = [open(f).read() for f in text_files]
tfidf = tfidfvectorizer().fit_transform(documents)
# no need to normalize, since vectorizer will return normalized tf-idf
pairwise_similarity = tfidf * tfidf.t

or, if the documents are plain strings,
>>> corpus = [""i'd like an apple"", 
...           ""an apple a day keeps the doctor away"", 
...           ""never compare an apple to an orange"", 
...           ""i prefer scikit-learn to orange"", 
...           ""the scikit-learn docs are orange and blue""]                                                                                                                                                                                                   
>>> vect = tfidfvectorizer(min_df=1, stop_words=""english"")                                                                                                                                                                                                   
>>> tfidf = vect.fit_transform(corpus)                                                                                                                                                                                                                       
>>> pairwise_similarity = tfidf * tfidf.t 

though gensim may have more options for this kind of task.
see also this question.
[disclaimer: i was involved in the scikit-learn tf-idf implementation.]
interpreting the results
from above, pairwise_similarity is a scipy sparse matrix that is square in shape, with the number of rows and columns equal to the number of documents in the corpus.
>>> pairwise_similarity                                                                                                                                                                                                                                      
<5x5 sparse matrix of type '<class 'numpy.float64'>'
    with 17 stored elements in compressed sparse row format>

you can convert the sparse array to a numpy array via .toarray() or .a:
>>> pairwise_similarity.toarray()                                                                                                                                                                                                                            
array([[1.        , 0.17668795, 0.27056873, 0.        , 0.        ],
       [0.17668795, 1.        , 0.15439436, 0.        , 0.        ],
       [0.27056873, 0.15439436, 1.        , 0.19635649, 0.16815247],
       [0.        , 0.        , 0.19635649, 1.        , 0.54499756],
       [0.        , 0.        , 0.16815247, 0.54499756, 1.        ]])

let's say we want to find the document most similar to the final document, ""the scikit-learn docs are orange and blue"".  this document has index 4 in corpus.  you can find the index of the most similar document by taking the argmax of that row, but first you'll need to mask the 1's, which represent the similarity of each document to itself.  you can do the latter through np.fill_diagonal(), and the former through np.nanargmax():
>>> import numpy as np     
                                                                                                                                                                                                                                  
>>> arr = pairwise_similarity.toarray()     
>>> np.fill_diagonal(arr, np.nan)                                                                                                                                                                                                                            
                                                                                                                                                                                                                 
>>> input_doc = ""the scikit-learn docs are orange and blue""                                                                                                                                                                                                  
>>> input_idx = corpus.index(input_doc)                                                                                                                                                                                                                      
>>> input_idx                                                                                                                                                                                                                                                
4

>>> result_idx = np.nanargmax(arr[input_idx])                                                                                                                                                                                                                
>>> corpus[result_idx]                                                                                                                                                                                                                                       
'i prefer scikit-learn to orange'

note: the purpose of using a sparse matrix is to save (a substantial amount of space) for a large corpus & vocabulary.  instead of converting to a numpy array, you could do:
>>> n, _ = pairwise_similarity.shape                                                                                                                                                                                                                         
>>> pairwise_similarity[np.arange(n), np.arange(n)] = -1.0
>>> pairwise_similarity[input_idx].argmax()                                                                                                                                                                                                                  
3",https://stackoverflow.com/questions/8897593,python,17-01-2012 15:51,316592.0,287.0,14.0,True,14-03-2025 00:25,14-03-2025 00:25
72912929,huggingface - finetuning in tensorflow with custom datasets,"i have been battling with my own implementation on my dataset with a different transformer model than the tutorial, and i have been getting this error attributeerror: 'nonetype' object has no attribute 'dtype', when i was starting to train my model. i have been trying to debug for hours, and then i have tried the tutorial from hugging face as it can be found here  running this exact code, so i could identify my mistake, also leads to the same error.
!wget 
!tar -xf aclimdb_v1.tar.gz

from pathlib import path
def read_imdb_split(split_dir):
    split_dir = path(split_dir)
    texts = []
    labels = []
    for label_dir in [""pos"", ""neg""]:
        for text_file in (split_dir/label_dir).iterdir():
            texts.append(text_file.read_text())
            labels.append(0 if label_dir is ""neg"" else 1)

    return texts, labels

train_texts, train_labels = read_imdb_split('aclimdb/train')
test_texts, test_labels = read_imdb_split('aclimdb/test')

from sklearn.model_selection import train_test_split
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)

from transformers import distilberttokenizerfast
tokenizer = distilberttokenizerfast.from_pretrained('distilbert-base-uncased')

train_encodings = tokenizer(train_texts, truncation=true, padding=true)
val_encodings = tokenizer(val_texts, truncation=true, padding=true)
test_encodings = tokenizer(test_texts, truncation=true, padding=true)

import tensorflow as tf

train_dataset = tf.data.dataset.from_tensor_slices((
    dict(train_encodings),
    train_labels
))
val_dataset = tf.data.dataset.from_tensor_slices((
    dict(val_encodings),
    val_labels
))
test_dataset = tf.data.dataset.from_tensor_slices((
    dict(test_encodings),
    test_labels
))

from transformers import tfdistilbertforsequenceclassification

model = tfdistilbertforsequenceclassification.from_pretrained('distilbert-base-uncased')

optimizer = tf.keras.optimizers.adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn
model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)

my goal will be to perform multi-label text classification on my own custom dataset, which unfortunately i cannot share for privacy reasons. if anyone could point out what is wrong with this implementation, will be highly appreciated.","['tensorflow', 'huggingface-transformers', 'transfer-learning', 'huggingface-tokenizers', 'fine-tuning']",72918008,"there seems to be an error, when you are passing the loss parameter.
model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn

you don't need to pass the loss parameter, if you want to use the model's built-in loss function.
i was able to train the model with your provided source code by changing mentioned line to:
model.compile(optimizer=optimizer)

or by passing a loss function
loss_fn = tf.keras.losses.sparsecategoricalcrossentropy(from_logits=true)

model.compile(optimizer=optimizer, loss=loss_fn)

transformers version: 4.20.1
hope it helps.",https://stackoverflow.com/questions/72912929,tensorflow,08-07-2022 14:28,854.0,0.0,1.0,True,26-06-2024 17:34,09-07-2022 05:20
70783834,how can i categorize tweets with google cloud natural language api - if possible?,"i am trying to use google cloud natural language api to classify/categorize tweets in order to filter out tweets that are not relevant to my audience (weather related). i can understand it must be tricky for an ai solution to make a classification on a short amount of text but i would imagine it would at least have a guess on text like this:

wind chills of zero to -5 degrees are expected in northwestern
arkansas into north-central arkansas extending into portions of
northern oklahoma during the 6-9am window . #arwx #okwx

i have tested several tweets but only very few get a categorization, the rest gets no result (or ""no categories found. try a longer text input."" if i try it through the gui).
is it pointless to hope for this to work? or, is it possible to decrease the threshold for the categorization? an ""educated guess"" from the nlp-solution would be better than no filter at all. is there an alternate solution (outside training my own nlp-model)?
edit: in order to clarify:
i am, in the end, using the google cloud platform natural language api in order to classify tweets. in order to test it i am using the gui (linked above). i can see that quite few of the tweets i test (in the gui) gets a categorization from gcp nlp, i.e. the category is empty.
the desired state i want is for gcp nlp to provide a category guess of a tweet text, rather than providing an empty result. i assume the nlp model removes any results with a confidence less than x%. it would be interesting to know if that threshold could be configured.
i assume the categorization of tweets must have been done before, and if there is any other way to solve this?
edit 2: classifytweet-code:
async function classifytweet(tweettext) {
   const language = require('@google-cloud/language');
   const client = new language.languageserviceclient({projectid, keyfilename});
   //const tweettext = ""some light snow dusted the ground this morning, adding to the intense snow fall of yesterday. here at my warwick station the numbers are in, new snow 19.5cm and total depth 26.6cm. a very good snow event. photos to be posted. #onstorm #canwarnon4464 #cocorahson525""
   const document = {
      content: tweettext,
      type: 'plain_text',
   };   
   const [classification] = await client.classifytext({document});
   
   console.log('categories:');
   classification.categories.foreach(category => {
     console.log(`name: ${category.name}, confidence: ${category.confidence}`);
   });
   
   return classification.categories
}","['google-cloud-platform', 'nlp', 'google-natural-language']",70897988,"i have dig on the current state of cloud natural language and my answer to your principal question will be that at the current state of the natural language classify text is not possible. although, a workaround would be if you base your categories on the output you get from analyzing the text from your inputs.
consider that we are not using a custom model for this and just using the options that cloud natural language offers, one tentative approach on this matter will be as follows:
to start, i have updated the code from the official samples to our needs to explain a bit further on this:
from google.cloud import language_v1 
from google.cloud.language_v1 import enums 


def sample_cloud_natural_language_text(text_content):
    """""" 
    args:
      text_content the text content to analyze. must include at least 20 words.
    """"""

    client = language_v1.languageserviceclient()
    type_ = enums.document.type.plain_text

    language = ""en""
    document = {""content"": text_content, ""type"": type_, ""language"": language}


    print(""=====classify text====="")
    response = client.classify_text(document)
    for category in response.categories:
        print(u""category name: {}"".format(category.name))
        print(u""confidence: {}"".format(category.confidence))


    print(""=====analyze text====="")
    response = client.analyze_entities(document)
    for entity in response.entities:
        print(f"">>>>> entity {entity.name}"")  
        print(u""entity type: {}"".format(enums.entity.type(entity.type).name))
        print(u""salience score: {}"".format(entity.salience))

        for metadata_name, metadata_value in entity.metadata.items():
            print(u""{}: {}"".format(metadata_name, metadata_value))

        for mention in entity.mentions:
            print(u""mention text: {}"".format(mention.text.content))
            print(u""mention type: {}"".format(enums.entitymention.type(mention.type).name))


if __name__ == ""__main__"":
    #text_content = ""that actor on tv makes movies in hollywood and also stars in a variety of popular new tv shows.""
    text_content=""wind chills of zero to -5 degrees are expected in northwestern arkansas into north-central arkansas extending into portions of northern oklahoma during the 6-9am window""
    
    sample_cloud_natural_language_text(text_content)

output
=====classify text=====
=====analyze text=====
>>>>> entity wind chills
entity type: other
salience score: 0.46825599670410156
mention text: wind chills
mention type: common
>>>>> entity degrees
entity type: other
salience score: 0.16041776537895203
mention text: degrees
mention type: common
>>>>> entity northwestern arkansas
entity type: organization
salience score: 0.07702474296092987
mid: /m/02vvkn4
wikipedia_url: 
mention text: northwestern arkansas
mention type: proper
>>>>> entity north
entity type: location
salience score: 0.07702474296092987
mention text: north
mention type: proper
>>>>> entity arkansas
entity type: location
salience score: 0.07088913768529892
mid: /m/0vbk
wikipedia_url: 
mention text: arkansas
mention type: proper
>>>>> entity window
entity type: other
salience score: 0.06348973512649536
mention text: window
mention type: common
>>>>> entity oklahoma
entity type: location
salience score: 0.04747137427330017
wikipedia_url: 
mid: /m/05mph
mention text: oklahoma
mention type: proper
>>>>> entity portions
entity type: other
salience score: 0.03542650490999222
mention text: portions
mention type: common
>>>>> entity 6
entity type: number
salience score: 0.0
value: 6
mention text: 6
mention type: type_unknown
>>>>> entity 9
entity type: number
salience score: 0.0
value: 9
mention text: 9
mention type: type_unknown
>>>>> entity -5
entity type: number
salience score: 0.0
value: -5
mention text: -5
mention type: type_unknown
>>>>> entity zero
entity type: number
salience score: 0.0
value: 0
mention text: zero
mention type: type_unknown

as you can see, classify text do not helps a lot (the result its empty). its when we start to analyze text that we can get some values. we can use that to build or own categories. the trick (and hard-work too) will be to make the pool of key words that will fit each category (a category built by us) that we can use to set the data that we are analyzing. about categorization, we can check the current list of available categories made by google to have an idea of what categories should look like.
i don't think there is a feature to lower the bar yet implemented with current builds but its something than can be requested to google as a feature.",https://stackoverflow.com/questions/70783834,google-cloud-platform,20-01-2022 09:37,611.0,2.0,1.0,True,31-01-2022 12:11,31-01-2022 12:11
77546477,how to build a model and train it with tensorflow keras sub classing,"i have written a custom encoder and decoder layers that implements the architecture described in the attention is all you need paper. everything works fine until i trying compiling it, i get one error. if i run it with a sample data it compiles but then when i call the fit method to train the model it throws another error. i'm going to provide the blocks that i might be implementing incorrectly and let me know if more code is needed to debug.
tf version: 2.14.0
multi-head sub layer and positional encoding layer:
class mhasublayer(layer):
  def __init__(self, units, **kwargs):
    super().__init__()
    self.mha = multiheadattention(key_dim=units, **kwargs)
    self.inner_dense = timedistributed(dense(2048, activation='relu'))
    self.outer_dense = timedistributed(dense(units))
    self.layernorm_mha = layernormalization()
    self.layernorm_ff = layernormalization()
    self.add = add()

  def call(self, x, context, **kwargs):
    ### calculate attention output
    attn_out, attn_scores = self.mha(query=x, value=context, return_attention_scores=true, **kwargs)

    attn_resid_cnxt = self.add([x, attn_out])  ## residual connection
    attn_layer_norm = self.layernorm_mha(attn_resid_cnxt) 

    attn_scores = tf.reduce_mean(attn_scores, axis=1)
    self.last_attention_weights = attn_scores

    ### pass the attention output to the dense layer
    dense_out = self.outer_dense(self.inner_dense(attn_layer_norm))
    dense_resid_cnxt = self.add([attn_layer_norm, dense_out])  ### feed forward residual connection

    dense_layer_norm = self.layernorm_ff(dense_resid_cnxt)
    return dense_layer_norm

class positionalencodinglayer(layer):
  def __init__(self, **kwargs):
    super().__init__()
    self.add = add()

  def get_positional_encodings(self, x):
    seq_len = x.shape[0]
    d = x.shape[1]
    
    p = np.zeros((seq_len, d))

    for k in range(seq_len):
        for i in np.arange(int(d/2)):
            denominator = np.power(10000, 2*i/d)
            p[k, 2*i] = np.sin(k/denominator)
            p[k, 2*i+1] = np.cos(k/denominator)
    return p
  
  def call(self, x):

    # pos_enc = []
    pos_enc = tf.map_fn(fn=self.get_positional_encodings, elems=x)

    # for n, elm in enumerate(x):
    #   p = self.get_positional_encodings(elm)
    #   pos_enc.append(p)
    
    # pos_enc = tf.convert_to_tensor(pos_enc)

    pos_embeddings = self.add([x, pos_enc])
    return pos_embeddings

encoder-decoder block:
class encoder(layer):
  def __init__(self, units, embed_input_dim, name='encoder', **kwargs):
    super().__init__()

    ### encoder input embedding and layer
    self.embedding = embedding(input_dim=embed_input_dim, output_dim=units, name='en_embed_layer')
    self.pos_embedding = positionalencodinglayer(name='en_positional_embed_layer')

    ### encoder multi-head self attention sub layer
    self.mha_sub_layer1 = mhasublayer(units, num_heads=8, name='en_mha_layer_1')
    self.mha_sub_layer2 = mhasublayer(units, num_heads=8, name='en_mha_layer_2')
    self.mha_sub_layer3 = mhasublayer(units, num_heads=8, name='en_mha_layer_3')
    self.mha_sub_layer4 = mhasublayer(units, num_heads=8, name='en_mha_layer_4')
    self.mha_sub_layer5 = mhasublayer(units, num_heads=8, name='en_mha_layer_5')
    self.mha_sub_layer6 = mhasublayer(units, num_heads=8, name='en_mha_layer_6')

    ### encoder mha dropout layer
    self.dropout =  dropout(rate=0.1, name='en_dropout_pos_enc')
    self.dropout1 = dropout(rate=0.1, name='en_dropout_layer1')
    self.dropout2 = dropout(rate=0.1, name='en_dropout_layer2')
    self.dropout3 = dropout(rate=0.1, name='en_dropout_layer3')
    self.dropout4 = dropout(rate=0.1, name='en_dropout_layer4')
    self.dropout5 = dropout(rate=0.1, name='en_dropout_layer5')
    self.dropout6 = dropout(rate=0.1, name='en_dropout_layer6')

  def call(self, x):
    embedding_output = self.embedding(x)

    positional_embedding = self.pos_embedding(embedding_output)
    postitional_embedding = self.dropout(positional_embedding)

    ### first mha sub-layer
    sub_layer1_out = self.mha_sub_layer1(positional_embedding, positional_embedding)
    sub_layer1_out = self.dropout1(sub_layer1_out)

    ### second mha sub-layer
    sub_layer2_out = self.mha_sub_layer2(sub_layer1_out, sub_layer1_out)
    sub_layer2_out = self.dropout2(sub_layer2_out)

    ### third mha sub-layer
    sub_layer3_out = self.mha_sub_layer3(sub_layer2_out, sub_layer2_out)
    sub_layer3_out = self.dropout3(sub_layer3_out)

    ### fourth mha sub-layer
    sub_layer4_out = self.mha_sub_layer4(sub_layer3_out, sub_layer3_out)
    sub_layer4_out = self.dropout4(sub_layer4_out)

    ### fifth mha sub-layer
    sub_layer5_out = self.mha_sub_layer5(sub_layer4_out, sub_layer4_out)
    sub_layer5_out = self.dropout5(sub_layer5_out)

    ### sixth mha sub-layer
    sub_layer6_out = self.mha_sub_layer6(sub_layer5_out, sub_layer5_out)
    sub_layer6_out = self.dropout6(sub_layer6_out)

    return sub_layer6_out

class decoder(layer):
  def __init__(self, units, embed_input_dim, name='decoder', **kwargs):
    super().__init__()
    ### decoder input embedding layer
    self.embedding = embedding(input_dim=embed_input_dim, output_dim=units, name='de_embed_layer')
    self.pos_embedding = positionalencodinglayer(name='de_positional_embed_layer')

    ### decoder multi-head attention sub layer
    self.mha_sub_layer1 = mhasublayer(units, num_heads=8, name='de_mha_layer_1')
    self.mha_sub_layer2 = mhasublayer(units, num_heads=8, name='de_mha_layer_2')
    self.mha_sub_layer3 = mhasublayer(units, num_heads=8, name='de_mha_layer_3')
    self.mha_sub_layer4 = mhasublayer(units, num_heads=8, name='de_mha_layer_4')
    self.mha_sub_layer5 = mhasublayer(units, num_heads=8, name='de_mha_layer_5')
    self.mha_sub_layer6 = mhasublayer(units, num_heads=8, name='de_mha_layer_6')

    ### decoder mha droput layer
    self.dropout =  dropout(rate=0.1, name='de_dropout_pos_enc')
    self.dropout1 = dropout(rate=0.1, name='de_dropout_layer1')
    self.dropout2 = dropout(rate=0.1, name='de_dropout_layer2')
    self.dropout3 = dropout(rate=0.1, name='de_dropout_layer3')
    self.dropout4 = dropout(rate=0.1, name='de_dropout_layer4')
    self.dropout5 = dropout(rate=0.1, name='de_dropout_layer5')
    self.dropout6 = dropout(rate=0.1, name='de_dropout_layer6')

    ### dense output layer
    self.output_dense_layer = timedistributed(dense(1), name=""output_layer"")

  def call(self, x, en_context):
    embedding_output = self.embedding(x)
    positional_embedding = self.pos_embedding(embedding_output)
    postitional_embedding = self.dropout(positional_embedding)

    ### first mha sub-layer
    sub_layer1_out = self.mha_sub_layer1(positional_embedding, positional_embedding)
    sub_layer1_out = self.dropout1(sub_layer1_out)

    ### second mha sub-layer
    sub_layer2_out = self.mha_sub_layer2(sub_layer1_out, en_context)
    sub_layer2_out = self.dropout2(sub_layer2_out)

    ### third mha sub-layer
    sub_layer3_out = self.mha_sub_layer3(sub_layer2_out, en_context)
    sub_layer3_out = self.dropout3(sub_layer3_out)

    ### fourth mha sub-layer
    sub_layer4_out = self.mha_sub_layer4(sub_layer3_out, en_context)
    sub_layer4_out = self.dropout4(sub_layer4_out)

    ### fifth mha sub-layer
    sub_layer5_out = self.mha_sub_layer5(sub_layer4_out, en_context)
    sub_layer5_out = self.dropout5(sub_layer5_out)

    ### sixth mha sub-layer
    sub_layer6_out = self.mha_sub_layer6(sub_layer5_out, en_context)
    sub_layer6_out = self.dropout6(sub_layer6_out)

    ### output dense layer
    output = self.output_dense_layer(sub_layer6_out)
    output = tf.round(tf.abs(output))
    return output

sample data:
np.random.seed(42)
trainx = np.random.randint(0, high=250, size=(5,12))
trainxt_in = np.random.randint(0, high=250, size=(5,3))
trainy = np.random.randint(0, high=250, size=(5,3,1))

modelling block:
training shape: ((1616304, 12), (1616304, 3), (1616304, 3, 1))

## the model sub-class

class trxster(model):
  def __init__(self, units, en_embed_dim, de_embed_dim, name='trxster', **kwargs):
    super().__init__()
    self.encoder = encoder(units, en_embed_dim)
    self.decoder = decoder(units, de_embed_dim)

  def call(self, inputs):
    context_vec, target_in = inputs
    context = self.encoder(context_vec)
    preds = self.decoder(target_in, context)
    return preds

forecastor = trxster(hsize, embed_dim, embed_dim)
forecastor.build(((12, 1),(3, 1)))
forecastor.summary()

error-1:

typeerror: error converting shape to a tensorshape: dimension value
must be integer or none or have an index method, got value '(12,
1)' with type '<class 'tuple'>'.

if run the model with an example:
hsize = 512
embed_dim = 268

forecastor = trxster(hsize, embed_dim, embed_dim)
forecastor((trainx, trainxt_in))

model: ""trxster_11""
_________________________________________________________________
 layer (type)                output shape              param #   
=================================================================
 encoder_13 (encoder)        multiple                  63156224  
                                                                 
 decoder_13 (decoder)        multiple                  63156737  
                                                                 
=================================================================
total params: 126312961 (481.85 mb)
trainable params: 126312961 (481.85 mb)
non-trainable params: 0 (0.00 byte)
_________________________________________________________________

### fit the model
batch_size = 64
epochs = 100
steps = trainx.shape[0]//batch_size
warmup_steps = steps//25

class mylrschedule(tf.keras.optimizers.schedules.learningrateschedule):
  def __init__(self, d_model, warmup_steps):
    self.d_model = d_model
    self.warmup_steps = warmup_steps

  def __call__(self, step):
    step_num = step.numpy()
    self.lr = []

    denom = self.d_model**(-0.5)
    numer = min(step_num**(-0.5), step_num*(self.warmup_steps**(-1.5)))
    lrate = np.divide(numer, denom)
    self.lr.append(lrate)

    return lrate

opt = tf.keras.optimizers.adam(learning_rate=mylrschedule(hsize, warmup_steps), beta_1=0.9, beta_2=0.98, epsilon=1e-8)

### configure trxster
checkpoint_filepath = './training_ckpt'

cb = [tf.keras.callbacks.earlystopping(patience=10, 
                                        monitor='val_loss',
                                        restore_best_weights=true),
      tf.keras.callbacks.modelcheckpoint(
                                        filepath=checkpoint_filepath,
                                        save_weights_only=true,
                                        monitor='val_loss',
                                        mode='min',
                                        verbose=1,
                                        save_best_only=true)]

loss = tf.keras.losses.meansquarederror()
metrics = [tf.keras.metrics.accuracy(), tf.keras.losses.meanabsoluteerror()]

forecastor.compile(optimizer=opt,
                   loss='mean_squared_error',
                   metrics=['acc','mean_absolute_error'])

history = forecastor.fit((trainx, trainxt_in), trainy,
                          batch_size=batch_size,
                          steps_per_epoch=steps,
                          epochs=1,
                          validation_data=((valx, valxt_in), valy),
                          callbacks=cb)

error-2: providing few lines of error trace:

valueerror: no gradients provided for any variable:
(['trxster_11/encoder_13/en_embed_layer/embeddings:0',
'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/query/kernel:0',
'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/query/bias:0',
'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/key/kernel:0',
'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/key/bias:0',
'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/value/kernel:0',
'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/value/bias:0',
'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/attention_output/kernel:0',
'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/attention_output/bias:0',
'trxster_11/encoder_13/mha_sub_layer_157/time_distributed_314/kernel:0'

every examples i see tells me that it should work just the way i have written but it isn't so.","['python-3.x', 'tensorflow', 'keras', 'deep-learning', 'nlp']",77554281,"i figured it out! gradient calculations fail when there is a tensorflow function in the graph which is the case in my network where i have applied tf.round and tf.abs in the output layer of the decoder. that was failing the gradient calculations. i removed them and it the model trains as expected. here is the link to the issue 
decoder:
class decoder(layer):
  def __init__(self, units, embed_input_dim, name='decoder', **kwargs):
    super().__init__()
    ### decoder input embedding layer
    self.embedding = embedding(input_dim=embed_input_dim, output_dim=units, name='de_embed_layer')
    self.pos_embedding = positionalencodinglayer(name='de_positional_embed_layer')

    ### decoder multi-head attention sub layer
    self.mha_sub_layer1 = mhasublayer(units, num_heads=8, name='de_mha_layer_1')
    self.mha_sub_layer2 = mhasublayer(units, num_heads=8, name='de_mha_layer_2')
    self.mha_sub_layer3 = mhasublayer(units, num_heads=8, name='de_mha_layer_3')
    self.mha_sub_layer4 = mhasublayer(units, num_heads=8, name='de_mha_layer_4')
    self.mha_sub_layer5 = mhasublayer(units, num_heads=8, name='de_mha_layer_5')
    self.mha_sub_layer6 = mhasublayer(units, num_heads=8, name='de_mha_layer_6')

    ### decoder mha droput layer
    self.dropout =  dropout(rate=0.1, name='de_dropout_pos_enc')
    self.dropout1 = dropout(rate=0.1, name='de_dropout_layer1')
    self.dropout2 = dropout(rate=0.1, name='de_dropout_layer2')
    self.dropout3 = dropout(rate=0.1, name='de_dropout_layer3')
    self.dropout4 = dropout(rate=0.1, name='de_dropout_layer4')
    self.dropout5 = dropout(rate=0.1, name='de_dropout_layer5')
    self.dropout6 = dropout(rate=0.1, name='de_dropout_layer6')

    ### dense output layer
    self.output_dense_layer = timedistributed(dense(1), name=""output_layer"")

  def call(self, x, en_context):
    embedding_output = self.embedding(x)
    positional_embedding = self.pos_embedding(embedding_output)
    postitional_embedding = self.dropout(positional_embedding)

    ### first mha sub-layer
    sub_layer1_out = self.mha_sub_layer1(positional_embedding, positional_embedding)
    sub_layer1_out = self.dropout1(sub_layer1_out)

    ### second mha sub-layer
    sub_layer2_out = self.mha_sub_layer2(sub_layer1_out, en_context)
    sub_layer2_out = self.dropout2(sub_layer2_out)

    ### third mha sub-layer
    sub_layer3_out = self.mha_sub_layer3(sub_layer2_out, en_context)
    sub_layer3_out = self.dropout3(sub_layer3_out)

    ### fourth mha sub-layer
    sub_layer4_out = self.mha_sub_layer4(sub_layer3_out, en_context)
    sub_layer4_out = self.dropout4(sub_layer4_out)

    ### fifth mha sub-layer
    sub_layer5_out = self.mha_sub_layer5(sub_layer4_out, en_context)
    sub_layer5_out = self.dropout5(sub_layer5_out)

    ### sixth mha sub-layer
    sub_layer6_out = self.mha_sub_layer6(sub_layer5_out, en_context)
    sub_layer6_out = self.dropout6(sub_layer6_out)

    ### output dense layer
    output = self.output_dense_layer(sub_layer6_out)
    return output",https://stackoverflow.com/questions/77546477,python-3.x,25-11-2023 02:49,63.0,1.0,1.0,True,27-11-2023 02:46,25-11-2023 16:44
72611335,what are the differences between fine tuning and few shot learning?,"i am trying to understand the concept of fine-tuning and few-shot learning.
i understand the need for fine-tuning. it is essentially tuning a pre-trained model to a specific downstream task. however, recently i have seen a plethora of blog posts stating zero-shot learning, one-shot learning and few-shot learning.

how are they different from fine-tuning? it appears to me that few-shot learning is a specialization of fine-tuning. what am i missing here?

can anyone please help me?","['machine-learning', 'deep-learning', 'artificial-intelligence', 'fine-tuning', 'few-shot-learning']",72709377,"fine tuning - when you already have a model trained to perform the task you want but on a different dataset, you initialise using the pre-trained weights and train it on target (usually smaller) dataset (usually with a smaller learning rate).
few shot learning - when you want to train a model on any task using very few samples. e.g., you have a model trained on different but related task and you (optionally) modify it and train for target task using small number of examples.
for example:
fine tuning - training a model for intent classification and then fine tuning it on a different dataset.
few shot learning - training a language model on large text dataset and modifying it (usually last (few) layer) to classify intents by training on small labelled dataset.
there could be many more ways to do few shot learning. for 1 more example, training a model to classify images where some classes have very small (or 0 for zero shot and 1 for one shot) number of training samples. here in inference, classifying these rare classes (rare in training) correctly becomes the aim of few shot learning.",https://stackoverflow.com/questions/72611335,machine-learning,14-06-2022 03:54,10467.0,8.0,4.0,True,08-02-2025 00:54,22-06-2022 02:55
76420235,laravel openai client doesn&#39;t work with list result,"when i try request throw openai laravel library to model gpt-3.5-turbo and result must be list, library throw exception
warning  undefined array key ""choices"" in 

vendor/openaiphp/client/src/responses/completions/createresponse.php on line 45.

typeerror  array_map(): argument #2 ($array) must be of type array, null given.


i tried this request for test
use openai\laravel\facades\openai;

openai::completions()->create([
                'model' => 'gpt-3.5-turbo',
                'prompt' => 'top 3 reachest peaople',
            ]);","['php', 'laravel', 'openai-api', 'chatgpt-api']",76635758,the problem was related to the openai model. i changed the model to text-davinci-003 and it worked.,https://stackoverflow.com/questions/76420235,php,07-06-2023 05:53,798.0,-1.0,1.0,True,07-07-2023 09:39,08-06-2023 06:33
90580,word frequency algorithm for natural language processing,"without getting a degree in information retrieval, i'd like to know if there exists any algorithms for counting the frequency that words occur in a given body of text.  the goal is to get a ""general feel"" of what people are saying over a set of textual comments.  along the lines of wordle.
what i'd like:

ignore articles, pronouns, etc ('a', 'an', 'the', 'him', 'them' etc)
preserve proper nouns
ignore hyphenation, except for soft kind

reaching for the stars, these would be peachy:

handling stemming & plurals (e.g. like, likes, liked, liking match the same result)
grouping of adjectives (adverbs, etc) with their subjects (""great service"" as opposed to ""great"", ""service"")

i've attempted some basic stuff using wordnet but i'm just tweaking things blindly and hoping it works for my specific data.  something more generic would be great.","algorithm, nlp, word-frequency",90846,"you'll need not one, but several nice algorithms, along the lines of the following.

ignoring pronouns is done via a stoplist.
preserving proper nouns? you mean, detecting named entities, like hoover dam and saying ""it's one word"" or compound nouns, like programming language? i'll give you a hint: that's tough one, but there exist libraries for both. look for ner (named entitiy recognition) and lexical chunking. opennlp is a java-toolkit that does both.
ignoring hyphenation? you mean, like at line breaks? use regular expressions and verify the resulting word via dictionary lookup.
handling plurals/stemming: you can look into the snowball stemmer. it does the trick nicely.
""grouping"" adjectives with their nouns is generally a task of shallow parsing. but if you are looking specifically for qualitative adjectives (good, bad, shitty, amazing...) you may be interested in sentiment analysis. lingpipe does this, and a lot more.

i'm sorry, i know you said you wanted to kiss, but unfortunately, your demands aren't that easy to meet. nevertheless, there exist tools for all of this, and you should be able to just tie them together and not have to perform any task yourself, if you don't want to. if you want to perform a task yourself, i suggest you look at stemming, it's the easiest of all.
if you go with java, combine lucene with the opennlp toolkit. you will get very good results, as lucene already has a stemmer built in and a lot of tutorial. the opennlp toolkit on the other hand is poorly documented, but you won't need too much out of it. you might also be interested in nltk, written in python.
i would say you drop your last requirement, as it involves shallow parsing and will definetly not impove your results.
ah, btw. the exact term of that document-term-frequency-thing you were looking for is called tf-idf. it's pretty much the best way to look for document frequency for terms. in order to do it properly, you won't get around using multidimenional vector matrices.
... yes, i know. after taking a seminar on ir, my respect for google was even greater. after doing some stuff in ir, my respect for them fell just as quick, though.",https://stackoverflow.com/q/90580,"algorithm, nlp, word-frequency",18-09-2008 06:49,22656.0,33.0,8.0,True,11-03-2025 10:08,22-06-2015 13:56
74969153,"how to display response with proper spacing, line-break and formatting in reactjs?","i am taking response from openai and store it using react usestate
const [response, setresponse] = usestate();

try {
  const res = await axios.post(' body, config)
  const completions = res.data.choices[0].text
  console.log(completions)
  setresponse(completions)
  setloading(false)
} catch (err) {
  console.error(err)
  setloading(false)
}

when i console the response it appears in this way

and when display using this code..
{loading ? (
        <div classname='block p-2.5 h-full w-full text-sm text-gray-900 bg-gray-50 rounded-lg border border-gray-300  dark:bg-gray-700 dark:border-gray-600  dark:text-white '>loading...</div>
      ) : (
        <div classname=""block p-3 h-full w-full text-sm text-gray-900 bg-gray-50 rounded-lg border border-gray-300  dark:bg-gray-700 dark:border-gray-600  dark:text-white "" >
          {response}
        </div>
      )}

it will appears like this:","['reactjs', 'react-hooks', 'openai-api']",74969447,an div element does not parse spaces or carriage return. as a solution for your problem i recommend to use <pre> tag or to use css property white-space (white-space: pre;),https://stackoverflow.com/questions/74969153,reactjs,31-12-2022 11:16,1258.0,-1.0,1.0,True,31-12-2022 12:17,31-12-2022 11:37
72983957,regex: searching for words that starts with @ or @,"i want to create a regex in python that find words that start with @ or @.
i have created the following regex, but the output contains one extra space in each string as you can see
regex = r'\s@\/?[\w\.\-]{2,}'
exp = 'george want@to play @.hdgska football @dddada'
re.findall(regex, exp)
output: [' @.hdgska', ' @dddada']

however, the output that i want to have is the following
output: ['@.hdgska', '@dddada']

i would be grateful if you could help me!
edit:
@the fourth bird, thank you so much for your help. there is one more thing that i don't know how to deal with. in case that we have this string
s = ""george want@to play @.hdgska football @dddada@snhfbjskjs""

the output is
['@.hdgska', '@dddada']

however, the output that i want should be one this
'@.hdgska'","['python', 'regex', 'nlp']",72983995,"in your pattern you are actually matching the leading \s and after the @ there can be an optional / with \/? but it should optionally start with a dot.
you could match for example an optional dot, and then 2 or more times the allowed characters in the character class.
at the left of the @ sign, either assert a non word boundary or assert a whitespace boundary.
note that you don't have to escape the dot and the hyphen in the character class.
\b@\.?[\w.-]{2,}

regex demo
another option:
(?<!\s)@\.?[\w.-]{2,}

regex demo
example
import re
 
pattern = r""(?<!\s)@\.?\w+[\w.-]{2,}""
s = ""george want@to play @.hdgska football @dddada""
print(re.findall(pattern, s))

output
['@.hdgska', '@dddada']",https://stackoverflow.com/questions/72983957,python,14-07-2022 16:41,53.0,0.0,1.0,True,15-07-2022 11:07,15-07-2022 11:07
40941761,i am having trouble downloading nltk&#39;s punkt tokenizer,"i'm trying to download punkt, but i'm getting the following error...
>>> import nltk
>>> nltk.download('punkt')
>>> [nltk_data] error loading punkt: <urlopen error [ssl] unknown error
>>> [nltk_data]     (_ssl.c:590)>
>>> false
>>> 

can someone please help i've been trying for days...","['python', 'nltk']",40944901,"i guess the downloader script is broken. as a temporal workaround can manually download the punkt tokenizer from here and then place the unzipped folder in the corresponding location. the default folders for each os are:

windows: c:\nltk_data\tokenizers
osx: /usr/local/share/nltk_data/tokenizers
unix: /usr/share/nltk_data/tokenizers

i am not sure but you may find this post helpful.",https://stackoverflow.com/questions/40941761,python,02-12-2016 22:22,20115.0,3.0,5.0,True,29-12-2024 19:44,03-12-2016 06:21
76387002,python inheritance - interfaces/classes,"from langchain.schema import basememory

class chatmemory(basememory):
   def __init__(self, user_id: uuid, type: str):
    self.user_id = user_id
    self.type = type

   # implemented abstract methods

class anothermem(chatmemory):
    def __init__(self, user_id: uuid, type: str):
        super().__init__(user_id, type)

this seems simple enough - but i get an error: valueerror: ""anothermem"" object has no field ""user_id"". what am i doing wrong?
note that basememory is an interface.","['python', 'inheritance', 'langchain']",76387031,"it looks like basememory from langchain is defined as a pydantic model, which has strict rules for defining instance attributes. instead of using a constructor method, use the standard pydantic syntax of declaring expected instance attributes on the child class.
import uuid
from langchain.schema import basememory


class chatmemory(basememory):
    user_id: uuid.uuid
    type: str

    ...

class anothermem(chatmemory):
    pass


print(anothermem(user_id=uuid.uuid4(), type=""foo"").user_id)

d952d62e-79e0-4cf4-a786-d23d880f96a2",https://stackoverflow.com/questions/76387002,python,02-06-2023 03:50,103.0,2.0,1.0,True,02-06-2023 03:58,02-06-2023 03:57
68959472,no vector when using spacy.load(&#39;en_core_web_trf&#39;)?,"after running this
nlp = spacy.load('en_core_web_lg')
has_vector = nlp('test text').has_vector
# ...
...has_vector == true

but after running this
nlp = spacy.load('en_core_web_trf')
has_vector = nlp('test text').has_vector
# ...
...has_vector == false

what am i missing?","['nlp', 'spacy', 'spacy-3']",72681901,"as per the docs here ( though, you can get the vector using
nlp('test text')._.trf_data.tensors[-1]",https://stackoverflow.com/questions/68959472,nlp,27-08-2021 21:19,2078.0,3.0,2.0,True,20-06-2022 03:21,29-08-2021 00:07
76720413,how can i install chatgpt seo plugin in chatgpt?,"i learn about chatgpt plugin in internet and below link.

but i don't know how can i download and install this plugin?

seo plugin
core seo ai plugin
aiprm
seo assistant
bramework

also please let me know, if it's safe to use this plugin in our seo.
i don't get any download link of core seo ai plugin.","['plugins', 'openai-api']",76817442,"if you go into the aiprm website  there is a link to install chrome extension.
chrome extension link

this only provides you prompt and help you to add those prompts to the chatgpt page so it's fully safe.
if any website or plugin asks for your api key to use chatgpt functionality then stay away from those applications.",https://stackoverflow.com/questions/76720413,plugins,19-07-2023 10:38,259.0,0.0,1.0,True,01-03-2024 07:22,01-03-2024 07:22
78122648,openai api: how do i get a list of all available openai models?,"can anyone find the curl command to get a list of all available openai models? i've been looking for like 10 minutes and can't find it.

edit: i got the answer, and i see the problem. it is one of those docs where everything is on one page.",['openai-api'],78122662,"python
from openai import openai
import os
client = openai(
    api_key = os.getenv('openai_api_key')
)
models = client.models.list()
for model in models:
    print(model.id)

node.js
const openai = require(""openai"");

const client = new openai({
  apikey: process.env.openai_api_key,
});

async function main() {
  const list = await client.models.list();

  for await (const model of list) {
    console.log(model);
  }
}

main();

curl
curl  \
  -h ""authorization: bearer $openai_api_key""

see the official openai documentation.",https://stackoverflow.com/questions/78122648,openai-api,07-03-2024 15:44,18888.0,4.0,1.0,True,23-03-2025 15:18,22-03-2024 12:20
73574217,re:sub expected string or byte-like object error,"class rereplacer(object):
   def __init__(self, pattern = r_patterns):
      self.pattern = [(re.compile(regex), repl) for (regex, repl) in patterns]

   def replace(self, text):
      s = text
      for (pattern, repl) in self.pattern:
         s = re.sub(pattern, repl, s)
      return s

i have got this code which replaces certain words with their replacements. when i call the method replace of class replacer,
rep=replacer()
rep.replace(""i like oranges"") 

it works perfectly fine with strings but gives an error with list or nested lists.
error- (re.sub) expected string or bytes-like object.
is there a way (except for converting the list to string) in order to make the function work of list of sentences? thanks in advance.
seems like predefined re.sub takes string as argument. should i split the words of the list?","['python', 'regex', 'nlp', 'nltk', 'python-re']",73574390,"if text is a list (or some other iterable type), loop over it and perform the replacements, and return a list of results.
from collections.abc import iterable

def replace(self, text):
    if isinstance(text, string):
        for (pattern, repl) in self.pattern:
            text = re.sub(pattern, repl, text)
        return text
    elif isinstance(text, iterable):
        return [self.replace(i) for i in text]
    else:
        raise valueerror('text must be a string or iterable collection')",https://stackoverflow.com/questions/73574217,python,01-09-2022 18:53,97.0,-3.0,1.0,True,04-09-2022 17:40,04-09-2022 17:40
74857383,unnecessary words included in the word cloud created using r programming,"i am trying to create some word cloud in r, which i am managing well so far with the exception of one little problem. i don't know where these words/symbols are coming from, but the following words are also getting displayed in my word cloud:

""language""
""en""
""=""

and i can't seem to remove them.these words/symbols are not part of the original text and don't know why and how they are getting displayed in my word cloud and really need help in understanding how i can remove such unwanted words and why they are there. below, i am attaching a screen shot of my word cloud for clarity, and added blue arrows to show where those words/symbols are located in the word cloud. i am also attaching my lines of code and the text i used for creating the word cloud. any help is much appreciated and many thanks.

    the_txt <-  ""
  - the wealthiest country\n
  - the highest proportion of wealthy population (population aged 40-49)\n
  - the highest numbers of ""rich business men and women"" and ""rich soil and land""\n
  - the country with the highest ""employed populstion"" and ""self employed"" numbers

  ""

    mydata <- corpus(vectorsource(the_txt))


mydata <- mydata %>%
    tm_map(removenumbers) %>%
    tm_map(removepunctuation) %>%
    tm_map(stripwhitespace)

mydata <- tm_map(mydata, content_transformer(tolower))

mydata <- tm_map(mydata, removenumbers)

mydata <- tm_map(mydata, removewords, stopwords(""english""))

mydata <- tm_map(mydata, stemdocument)


as.character(mydata[[1]])

minfreq_trigram<-1

token_delim <- "" \\t\\r\\n.!?,;\""()""

tritoken <- ngramtokenizer(my data, weka_control(min=1,max=3, delimiters = token_delim))

three_word <- data.frame(table(tritoken))

sort_three <- three_word[order(three_word$freq, decreasing=true),]

set.seed(1234)

wordcloud(sort_three$tritoken, sort_three$freq, 
              random.order=false, scale = c(3,0.4),
              min.freq = minfreq_trigram,
              colors = brewer.pal(8,""dark2""),
              max.words=200)","['r', 'nlp', 'word-cloud']",74861996,"> as.character(mydata)
[1] ""wealthiest countri highest proport wealthi popul popul age highest number rich busi men women rich soil land countri highest employ populst self employ number""
[2] ""list(language = \""en\"")""                                                                                                                                       
[3] ""list()"" 

you checked mydata[[1]] , explicitly looking at a part of mydata, but the rest has content, that you fed into ngramtokenizer and ultimitaly the wordcloud.
if you want to pass mydata[[1]]] instead of mydata i would think that would work out for you, and is a straightforward approach. i think the recommended approach is to use content()
i.e.
mycontent <- content(mydata)

to get the character vector out",https://stackoverflow.com/questions/74857383,r,20-12-2022 00:24,108.0,0.0,1.0,True,20-12-2022 16:06,20-12-2022 16:06
76759792,how do i do function calling in azure openai using the javascript sdk,"i want to be able to call functions based on the user input, i could do this with the openai library but can't find a way to do so in the azure openai library
below is the code from azure openai in python that is able to do what i want to accomplish
as such i want to replicate this code using the javascript sdk link
import openai 
openai.api_type = ""azure"" 
openai.api_base = "" 
openai.api_version = ""2023-07-01-preview"" 
openai.api_key = os.getenv(""openai_api_key"") 
response = openai.chatcompletion.create(             
    engine=""gpt-35-turbo-xxx"",             
    model=""gpt-35-turbo-0613-xxxx""             
    messages=messages,             
    functions=functions,             
    function_call=""auto"",         
)

i tried this
const response = await openai.getchatcompletions(
deploymentid,       
messages,    
{  function_call: functions }

and
const response = await openai.getchatcompletions(
deploymentid,       
messages,    
functions )

i couldn't find anything on the documentation for this as well","['azure', 'openai-api', 'azure-openai']",77094357,"the updated azure openai javascript documentation now incorporates support for functions and function calls.
you can find the relevant documentation at the following link, 
it is recommended that you ensure your azure openai library is updated to the most recent version.(",https://stackoverflow.com/questions/76759792,azure,25-07-2023 05:55,1783.0,2.0,1.0,True,17-01-2024 22:19,26-07-2023 08:48
36966019,how aretf-idf calculated by the scikit-learn tfidfvectorizer,"i run the following code to convert the text matrix to tf-idf matrix.
text = ['this is a string','this is another string','tfidf computation calculation','tfidf is the product of tf and idf']

from sklearn.feature_extraction.text import tfidfvectorizer
vectorizer = tfidfvectorizer(max_df=1.0, min_df=1, stop_words='english',norm = none)
                    
x = vectorizer.fit_transform(text)
x_vocab = vectorizer.get_feature_names_out()
x_mat = x.todense()
x_idf = vectorizer.idf_

i get the following output
x_vocab =
[u'calculation',
 u'computation',
 u'idf',
 u'product',
 u'string',
 u'tf',
 u'tfidf']

and x_mat =
  ([[ 0.        ,  0.        ,  0.        ,  0.        ,  1.51082562,
      0.        ,  0.        ],
    [ 0.        ,  0.        ,  0.        ,  0.        ,  1.51082562,
      0.        ,  0.        ],
    [ 1.91629073,  1.91629073,  0.        ,  0.        ,  0.        ,
      0.        ,  1.51082562],
    [ 0.        ,  0.        ,  1.91629073,  1.91629073,  0.        ,
      1.91629073,  1.51082562]])

now i dont understand how these scores are computed. my idea is that for the text[0], score for only 'string' is computed and there is a score in the 5th coloumn. but as tf_idf is the product of term frequency which is 2 and idf which is log(4/2) is 1.39 and not 1.51 as shown in the matrix. how is the tf-idf score calculated in scikit-learn.","['nlp', 'scikit-learn', 'tf-idf']",36972265,"tf-idf is done in multiple steps by scikit learn's tfidfvectorizer, which in fact uses tfidftransformer and inherits countvectorizer.
let me summarize the steps it does to make it more straightforward:

tfs are calculated by countvectorizer's fit_transform()
idfs are calculated by tfidftransformer's fit()
tfidfs are calculated by tfidftransformer's transform()

you can check the source code here.
back to your example. here is the calculation that is done for the tfidf weight for the 5th term of the vocabulary, 1st document (x_mat[0,4]):
first, the tf for 'string', in the 1st document:
tf = 1

second, the idf for 'string', with smoothing enabled (default behavior):
df = 2
n = 4
idf = ln(n + 1 / df + 1) + 1 = ln (5 / 3) + 1 = 1.5108256238

and finally, the tfidf weight for (document 0, feature 4):
tfidf(0,4) = tf * idf = 1 * 1.5108256238 = 1.5108256238

i noticed you choose not to normalize the tfidf matrix. keep in mind normalizing the tfidf matrix is a common and usually recommended approach, since most models will require the feature matrix (or design matrix) to be normalized.
tfidfvectorizer will l-2 normalize the output matrix by default, as a final step of the calculation. having it normalized means it will have only weights between 0 and 1.",https://stackoverflow.com/questions/36966019,nlp,01-05-2016 11:16,14203.0,21.0,3.0,True,10-06-2024 23:20,10-06-2024 23:20
68471586,training epochs interpretation during spacy ner training,"i was training my ner model with transformers, and am not really sure why the training stopped at some point, or why did it even go with so many batches. this is how my configuration file looks like (relevant part):
[training]
train_corpus = ""corpora.train""
dev_corpus = ""corpora.dev""
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_epochs = 2
max_steps = 0
eval_frequency = 200
frozen_components = []
before_to_disk = null

[training.batcher]
@batchers = ""spacy.batch_by_words.v1""
discard_oversize = false
tolerance = 0.2
get_length = null

[training.batcher.size]
@schedules = ""compounding.v1""
start = 100
stop = 1000
compound = 1.001
t = 0.0

[training.optimizer]
@optimizers = ""adam.v1""
beta1 = 0.9
beta2 = 0.999
l2_is_weight_decay = true
l2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001
learn_rate = 0.00005

and this is the training log:
============================= training pipeline =============================
ï¿½ï¿½ï¿½ pipeline: ['transformer', 'ner']
ï¿½ï¿½ï¿½ initial learn rate: 5e-05
e    #       loss trans...  loss ner  ents_f  ents_p  ents_r  score 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0         398.75     40.97    2.84    3.36    2.46    0.03
  0     200         906.30   1861.38   94.51   94.00   95.03    0.95
  0     400         230.06   1028.51   98.10   97.32   98.89    0.98
  0     600          90.22   1013.38   98.99   98.40   99.58    0.99
  0     800          80.64   1131.73   99.02   98.25   99.81    0.99
  0    1000          98.50   1260.47   99.50   99.16   99.85    1.00
  0    1200          73.32   1414.91   99.49   99.25   99.73    0.99
  0    1400          84.94   1529.75   99.70   99.56   99.85    1.00
  0    1600          55.61   1697.55   99.75   99.63   99.87    1.00
  0    1800          80.41   1936.64   99.75   99.63   99.87    1.00
  0    2000         115.39   2125  99.69   99.87    1.00
  0    2200          63.06   2395.48   99.80   99.75   99.85    1.00
  0    2400         104.14   2574.36   99.87   99.79   99.96    1.00
  0    2600          86.07   2308.35   99.88   99.79   99.97    1.00
  0    2800          81.05   1853.15   99.90   99.87   99.93    1.00
  0    3000          52.67   1462.61   99.96   99.93   99.99    1.00
  0    3200          57.99   1154.62   99.94   99.91   99.97    1.00
  0    3400         110.74    847.50   99.90   99.85   99.96    1.00
  0    3600          90.49    621.99   99.90   99.91   99.90    1.00
  0    3800          51.03    378.93   99.87   99.78   99.97    1.00
  0    4000          93.40    274.80   99.95   99.93   99.97    1.00
  0    4200         138.98    203.28   99.91   99.87   99.96    1.00
  0    4400         106.16    127.60   99.70   99.75   99.64    1.00
  0    4600          70.28     87.25   99.95   99.94   99.96    1.00
ï¿½ï¿½ï¿½ saved pipeline to output directory
training/model-last


i was trying to traiodel for 2 epochs (max_epochs=2), and my train file has around 123591 examples, and dev file has 2522 examples.
my question is:

since my minimum batch size is 100, i expect my training to end before the 2400th eval batch, right? because 2400th batch evaluated implies i have a minimum of 2400*100 = 240000, and it would actually be even more than that, since my batch size is increasing. so why did it go all the way to # 4600?

the training ended automatically, but the e still reads the 0th epoch. why is that?


edit: in continuation to my 2nd bullet point, i'm curious to know why did the training went all the way upto 4600 batches, because 4600 batches at minimum means 4600*100 = 460000 examples, and i gave 123591  examples for train, so i'm clearly well above and over the 1st epoch, but e still reads as 0.","['nlp', 'spacy', 'named-entity-recognition', 'spacy-3']",68479425,"there's an entry for this in the faq, but to summarize:

max_steps is the maximum iterations. (not ""evaluation iterations"", but batches.)
max_epochs is the maximum number of epochs.
if training goes for patience batches without improvement it will stop. that is what stopped your training.

it seems like your model has already gotten a perfect score so i'm not sure why early stopping is a problem in this case, but that's what's happening.",https://stackoverflow.com/questions/68471586,nlp,21-07-2021 14:42,5789.0,4.0,3.0,True,21-07-2022 14:54,21-07-2021 21:43
76734099,openai chat completions api: how do i use a function to store conversation memory?,"i am trying to make a chatbot using openai function calling. i have taken the basic example of getting the current weather condition, which was given in the documentation.
what i want to implement is to have a memory with it.
i tried to append into the message, but what i want is when i have a new message, so instead of calling the function, how can it get the response from memory if it's already asked?
my code is like this:
def get_current_weather(location, unit=""fahrenheit""):
    print(""it ran>>>>>>>>>>"")
    weather_info = {
        ""location"": location,
        ""temperature"": ""72"",
        ""unit"": unit,
        ""forecast"": [""sunny"", ""windy""],
    }
    return json.dumps(weather_info)


messages = []


def run_conversation(input_message):
    messages.append({""role"": ""user"", ""content"": f""{input_message}""})
    functions = [
        {
            ""name"": ""get_current_weather"",
            ""description"": ""get the details about a drug/medication"",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""definition"": {
                        ""type"": ""string"",
                        ""description"": ""the city and state, e.g. san francisco, ca"",
                    },
                    ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
                },
                ""required"": [""location""],
            },
        }
    ]
    print(""message 1"", messages)
    response = openai.chatcompletion.create(
        model=""gpt-3.5-turbo-0613"",
        messages=messages,
        functions=functions,
        # function_call=""auto"",
    )
    response_message = response[""choices""][0][""message""]
    print(""response msg"", response_message)

    if response_message.get(""function_call""):
        available_functions = {""get_current_weather"": get_current_weather}
        function_name = response_message[""function_call""][""name""]
        function_to_call = available_functions[function_name]
        function_args = json.loads(response_message[""function_call""][""arguments""])
        function_response = function_to_call(
            location=function_args.get(""location""),
            unit=function_args.get(""unit""),
        )

        # messages.append(response_message)
        messages.append(
            {""role"": ""function"", ""name"": function_name, ""content"": function_response}
        )
        print(""message 2"", messages)
        second_response = openai.chatcompletion.create(
            model=""gpt-3.5-turbo-0613"",
            messages=messages,
        )
        print(""second response"", second_response['choices'][0]['message'].to_dict())
        messages.append(second_response['choices'][0]['message'].to_dict())
        print(""message 3"", messages)
        return second_response

it always runs the function even if i ask the same question
output:
response msg {
  ""role"": ""assistant"",
  ""content"": null,
  ""function_call"": {
    ""name"": ""get_current_weather"",
    ""arguments"": ""{\n  \""definition\"": \""boston, ma\"",\n  \""unit\"": \""celsius\""\n}""
  }
}
it ran>>>>>>>>>>
message 2 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}]
second response {'role': 'assistant', 'content': 'the temperature in boston is 72 degrees celsius.'}
message 3 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degree
s celsius.'}]
message 1 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degree
s celsius.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}]
response msg {
  ""role"": ""assistant"",
  ""content"": null,
  ""function_call"": {
    ""name"": ""get_current_weather"",
    ""arguments"": ""{\""location\"": \""new york\""}""
  }
}
it ran>>>>>>>>>>
message 2 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degree
s celsius.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""locati
on"": ""new york"", ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}]
second response {'role': 'assistant', 'content': 'the temperature in new york is 72 degrees. please note that i did not specify the temperature unit, as i
t is missing in the response.'}
message 3 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degree
s celsius.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""locati
on"": ""new york"", ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in new york is 72
 degrees. please note that i did not specify the temperature unit, as it is missing in the response.'}]
message 1 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degree
s celsius.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""locati
on"": ""new york"", ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in new york is 72
 degrees. please note that i did not specify the temperature unit, as it is missing in the response.'}, {'role': 'user', 'content': 'what is the temperatu
re in boston?'}]
response msg {
  ""role"": ""assistant"",
  ""content"": null,
  ""function_call"": {
    ""name"": ""get_current_weather"",
    ""arguments"": ""{\""definition\"": \""boston\""}""
  }
}
it ran>>>>>>>>>>
message 2 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degree
s celsius.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""locati
message 3 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degree
s celsius.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""locati
on"": ""new york"", ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in new york is 72
 degrees. please note that i did not specify the temperature unit, as it is missing in the response.'}, {'role': 'user', 'content': 'what is the temperatu
re in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"": null, ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny
"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degrees. please note that the unit of temperature is missing in the resp
onse.'}]
message 1 [{'role': 'user', 'content': 'what is the temperature in boston?'}]
response msg {        
  ""role"": ""assistant"",
  ""content"": null,    
  ""function_call"": {
    ""name"": ""get_current_weather"",
    ""arguments"": ""{\n  \""definition\"": \""boston, ma\""\n}""
  }
}
it ran>>>>>>>>>>
message 2 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}]
second response {'role': 'assistant', 'content': 'the temperature in boston is 72 degrees fahrenheit.'}
message 3 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degrees fah
renheit.'}]
message 1 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degrees fah
renheit.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}]
response msg {
  ""role"": ""assistant"",
  ""content"": null,
  ""function_call"": {
    ""name"": ""get_current_weather"",
    ""arguments"": ""{\""location\"": \""new york\"", \""unit\"": \""celsius\""}""
  }
}
it ran>>>>>>>>>>
message 2 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degrees fah
renheit.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location
"": ""new york"", ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}]
second response {'role': 'assistant', 'content': 'the temperature in new york is 72 degrees celsius.'}
message 3 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degrees fah
renheit.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location
"": ""new york"", ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in new york is
 72 degrees celsius.'}]
message 1 [{'role': 'user', 'content': 'what is the temperature in boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location"":
 null, ""temperature"": ""72"", ""unit"": null, ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in boston is 72 degrees fah
renheit.'}, {'role': 'user', 'content': 'what is the temperature in newyork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{""location
"": ""new york"", ""temperature"": ""72"", ""unit"": ""celsius"", ""forecast"": [""sunny"", ""windy""]}'}, {'role': 'assistant', 'content': 'the temperature in new york is
 72 degrees celsius.'}, {'role': 'user', 'content': 'what is the temperature in boston?'}]
response msg {
  ""role"": ""assistant"",
  ""content"": null,
  ""function_call"": {
    ""name"": ""get_current_weather"",
    ""arguments"": ""{\""location\"": \""boston\""}""
  }
is 72 degrees.'}]","['python', 'openai-api', 'chatgpt-api', 'gpt-4']",76778206,"using the openai api function to store conversation memory is not the right approach because of the possibility that the model may generate invalid json or hallucinate parameters.
people are already having problems with that.
if you take a look at the official openai documentation, you can see that openai transparently states:

the basic sequence of steps for function calling is as follows:

call the model with the user query and a set of functions defined in the functions parameter.
the model can choose to call a function; if so, the content will be a stringified json object adhering to your custom schema (note: the
model may generate invalid json or hallucinate parameters).
parse the string into json in your code, and call your function with the provided arguments if they exist.
call the model again by appending the function response as a new message, and let the model summarize the results back to the user.",https://stackoverflow.com/questions/76734099,python,20-07-2023 22:33,2361.0,-1.0,1.0,True,12-08-2023 15:37,12-08-2023 15:37
77491173,is it possible use &quot;gpt-4-vision-preview&quot; with batching?,"i am trying to use the ""gpt-4-vision-preview"" model with the batching option (since the limits are very low at the moment).
this is my messages object (not sure if it's correct but i tried to follow the docs).
            let messages = batch.map(doc => {
            const imageurl =`someurl`;
            const question = doc.questao;
            const answers = doc.respostas;
            let options = object.keys(answers).map(key => `${key}: ${answers[key]}`).join('\n');

            return {
                role: ""user"",
                content: [
                    {
                        type: ""text"",
                        text: `${question} \n ${options} \n ${questionexplanation}`
                    },

                    {
                        type: ""image_url"",
                        image_url: {
                            url: imageurl
                        }
                    }
                ]
            };
        });

and this is how i make the request.
const response = await openai.chat.completions.create({
            model: ""gpt-4-vision-preview"",
            max_tokens: 4000, // adjust if needed
            messages: messages
        });

i did not see anywhere in the docs saying if it was possible or not.","['node.js', 'openai-api']",77514874,"in terms of batching, it's only possible to pass multiple images with one text message at the moment.
where your messages would look like this
 const prompt_messages = [
  {
    role: ""user"",
    content: [
      {
        type: ""text"",
        text: ""<prompt message>"",
      },
      ...batch.map((doc) => ({
        type: ""image_url"",
        image_url: {
          url: ""<image_url>""
          detail: ""low"",
        },
      })),
    ],
  },

optional: by passing detail: ""low"", you specify the low-res 512px x 512px version of the image to the model which will represent the image with a budget of 65 tokens.",https://stackoverflow.com/questions/77491173,node.js,15-11-2023 22:13,534.0,0.0,1.0,True,20-11-2023 09:52,16-11-2023 07:58
74074350,tensorflow expected 2 inputs but received 1 input tensor,"hey guys so i'm building a model based on the roberta-base and at the end when i try to fit the model i get a error saying: valueerror: layer model_39 expects 2 input(s), but it received 1 input tensors. inputs received: [<tf.tensor 'iteratorgetnext:0' shape=(16, 128) dtype=float64>]
i'm using tf.data.dataset to make the dataset:
def map_dataset(ids, masks, labels):
    return {'input_ids': ids, 'input_mask': masks}, labels

# create dataset
dataset = tf.data.dataset.from_tensor_slices((ids, mask, labels))
dataset.map(map_dataset)
dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=true)

supposedly dataset is generating 2 inputs properly but for some reason fit is refusing to work and i'm not sure why.
full code:
len_seq = 128
batch_size = 16
test_train_split = 0.9
transformer = 'roberta-base'

# load roberta model
base_model = tfautomodel.from_pretrained('roberta-base')
for layer in base_model.layers:
    layer.trainable = false

# define input layers
input_ids = tf.keras.layers.input(shape=(len_seq,), name='input_ids', dtype='int32')
input_mask = tf.keras.layers.input(shape=(len_seq,), name='input_mask', dtype='int32')

# define hidden layers
embedding = base_model([input_ids, input_mask])[1]
layer = tf.keras.layers.dense(len_seq * 2, activation='relu')(embedding)
layer = tf.keras.layers.dense(len_seq, activation='relu')(layer)

# define output
output = tf.keras.layers.dense(1, activation='softmax', name='output')(layer)

model = tf.keras.model(inputs=[input_ids, input_mask], outputs=[output])

model.compile(
    optimizer = adam(learning_rate=1e-3, decay=1e-4),
    loss = categoricalcrossentropy(),
    metrics = [
        categoricalaccuracy('accuracy')
    ]
)

# load data
df = pd.read_csv('train-processed.csv')
df = df.head(100)
samples_count = len(df)

# tokenize data
tokenizer = autotokenizer.from_pretrained(transformer)
tokens = tokenizer(
    df['first_phrase'].tolist(),
    max_length=len_seq,
    truncation=true,
    padding='max_length',
    add_special_tokens=true,
    return_tensors='tf'
)
ids = tokens['input_ids']
mask = tokens['attention_mask']

def map_dataset(ids, masks, labels):
    return {'input_ids': ids, 'input_mask': masks}, labels

# create dataset
dataset = tf.data.dataset.from_tensor_slices((ids, mask, labels))
dataset.map(map_dataset)
dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=true)

# split data intro train and test
train_size = int((samples_count / batch_size) * test_train_split)
train = dataset.take(train_size)
test = dataset.skip(train_size)

# train model
history = model.fit(
    train,
    validation_data=test,
    epochs=2
)

inside dataset -> <batchdataset shapes: ((16, 128), (16, 128), (16, 5)), types: (tf.float64, tf.float64, tf.float64)>
inside train -> <takedataset shapes: ((16, 128), (16, 128), (16, 5)), types: (tf.float64, tf.float64, tf.float64)>
data example:

any help appreciated. i'm new to transformers so please feel free to point any extra considerations.","['python', 'tensorflow', 'bert-language-model']",74079275,"so i managed to fix this as far as i know with the help of @djinn.
i did remove dataset api and instead built my own datasets manually using the following code:
# split into training and validation sets
train_size = int(samples_count * test_train_split)
train = [ids[:train_size], mask[:train_size]]
train_labels = labels[:train_size]
test = [ids[train_size:], mask[train_size:]], labels[train_size:]
# train model
history = model.fit(
    train, train_labels,
    validation_data=test,
    epochs=10
)

this seems to be working and fit() accepted this data, but feel free to point out if this is wrong or could be made differently.",https://stackoverflow.com/questions/74074350,python,14-10-2022 20:03,973.0,0.0,1.0,True,15-10-2022 12:03,14-10-2022 20:31
77615264,why does opennlp cli output &quot;slf4j: failed to load class &quot;org.slf4j.impl.staticloggerbinder&quot; on windows?,"based on apache opennlp documentation, i downloaded binary version of opennlp, then set java_home and opennlp_home. when i run opennlp command it faced to below exception:
slf4j: failed to load class ""org.slf4j.impl.staticloggerbinder"".
slf4j: defaulting to no-operation (nop) logger implementation
slf4j: see  for further details.

i used windows operating system and tried this command on different systems with different versions of java, but always this exception is shown. it is too odd, because i've searched and no one faced this issued before.","['java', 'nlp', 'slf4j', 'opennlp']",77630535,"you found a bug. congratulation! i created opennlp-1527 for it.
meanwhile, you can override the content of the opennlp.bat file with
@echo off

rem #   licensed to the apache software foundation (asf) under one
rem #   or more contributor license agreements.  see the notice file
rem #   distributed with this work for additional information
rem #   regarding copyright ownership.  the asf licenses this file
rem #   to you under the apache license, version 2.0 (the
rem #   ""license""); you may not use this file except in compliance
rem #   with the license.  you may obtain a copy of the license at
rem #
rem #    
rem #
rem #   unless required by applicable law or agreed to in writing,
rem #   software distributed under the license is distributed on an
rem #   #  ""as is"" basis, without warranties or conditions of any
rem #   kind, either express or implied.  see the license for the
rem #   specific language governing permissions and limitations
rem #   under the license.

rem # note:  do not output anything in this script file, any output
rem #        may be inadvertantly placed in any output files if
rem #        output redirection is used.
setlocal

if ""%java_cmd%"" == """" (
    if ""%java_home%"" == """" (
        set java_cmd=java 
    ) else (
        rem # keep java_home to short-name without spaces
        for %%a in (""%java_home%"") do set java_cmd=%%~sfa\bin\java
    )
)

rem remove heap variable
set heap=
if not ""%java_heap%"" == """" (
    set heap=""-xmx%java_heap%""
)

rem #  should work with windows xp and greater.  if not, specify the path to where it is installed.
if ""%opennlp_home%"" == """" (
    set opennlp_home=%~sp0..
) else (
    rem # keep opennlp_home to short-name without spaces
    for %%a in (""%opennlp_home%"") do set opennlp_home=%%~sfa
)

echo environment
echo java_home=%java_home%
echo opennlp_home=%opennlp_home% 

rem add lib directory to the classpath
set classpath=""%opennlp_home%\lib\*""

echo classpath=%classpath%

%java_cmd% %heap% ""-dlog4j.configurationfile=%opennlp_home%\conf\log4j2.xml"" -cp %classpath% opennlp.tools.cmdline.cli %*

endlocal

this will correctly append the classes contained in the lib folder to the classpath and the cli will work as expected.",https://stackoverflow.com/questions/77615264,java,06-12-2023 17:47,174.0,1.0,1.0,True,09-12-2023 07:47,09-12-2023 07:15
78030052,delete files downloaded by outlines python library,"import outlines
model = outlines.models.transformers('mistralai/mistal-7b-v0.1')

i had run the above code on my m1 macbook to try mistral llm, because of that 16 gb size of weights were downloaded on my system. now i cant find where they were downloaded. i need to delete those files from system to free up my space. please help me locate those files. uninstalling the library also didnt help free up my space.","['python', 'langchain', 'large-language-model', 'mistral-7b']",78030509,"it appears to be downloaded to the directory ~/.cache/huggingface/hub.
you can run cd ~/.cache/huggingface/hub in the terminal to change to this directory, or run open ~/.cache/huggingface/hub to open finder in this directory.
versions used: osx 13.6.1, python 3.10.10, outlines 0.0.32, and transformers 4.37.2.",https://stackoverflow.com/questions/78030052,python,20-02-2024 20:09,135.0,0.0,2.0,True,20-02-2024 22:17,20-02-2024 21:23
78824299,openai api returns null when i retrieve the fine-tuned model,"i'm fine-tuning a model and generating actions from text. i create a train.jsonl file, upload it, and fine-tune the model. however, when i try to get the name of the model i just created, it returns null.
i fine-tune a model like this:
const model = await openai.finetuning.jobs.create({
  training_file: process.env.file_id,
  model: 'babbage-002',
})

then i try to retrieve the fine-tuned model like this:
const response = await openai.finetuning.jobs.retrieve(
    process.env.fine_tune_id,
)

but this is the response i get from the openai api:
data:  {
  object: 'fine_tuning.job',
  id: 'ftjob-nsfvxzjttsfr5jcqqtfedtco',
  model: 'babbage-002',
  created_at: 1722546992,
  fine_tuned_model: null,
  organization_id: 'org-gljhkxwkbqrlovhk0762ucml',
  result_files: [],
  status: 'running',
  validation_file: null,
  training_file: 'file-lh1c4vv1hdiv7lxxugh9mil9',
  hyperparameters: { n_epochs: 9, batch_size: 1, learning_rate_multiplier: 16 },
  trained_tokens: null,
  error: {},
  user_provided_suffix: null,
  seed: 1564492262,
  estimated_finish: null,
  integrations: []
}","['node.js', 'openai-api', 'fine-tuning']",78825035,"the fine-tuning job hasn't finished yet.
the fine-tuning flow is the following:

create a fine-tuning job.
fine-tuning is in progress.
retrieve the fine-tuning job.

try to run the following code to see if the fine-tuning is still in progress.
import openai from ""openai"";

const client = new openai();

async function main() {
  const list = await client.finetuning.jobs.list();

  for await (const finetune of list) {
    console.log(finetune);
  }
}

main();",https://stackoverflow.com/questions/78824299,node.js,02-08-2024 07:40,76.0,0.0,1.0,True,21-09-2024 15:52,21-09-2024 15:51
77469097,how can i process a pdf using openai&#39;s apis (gpts)?,"the web interface for chatgpt has an easy pdf upload. is there an api from openai that can receive pdfs?
i know there are 3rd party libraries that can read pdf but given there are images and other important information in a pdf, it might be better if a model like gpt 4 turbo was fed the actual pdf directly.
i'll state my use case to add more context. i intent to do rag. in the code below i handle the pdf and a prompt. normally i'd append the text at the end of the prompt. i could still do that with a pdf if i extract its contents manually.
the following code is taken from here  is this how i'm supposed to do it?
# upload a file with an ""assistants"" purpose
file = client.files.create(
  file=open(""example.pdf"", ""rb""),
  purpose='assistants'
)

# create an assistant using the file id
assistant = client.beta.assistants.create(
  instructions=""you are a personal math tutor. when asked a math question, write and run code to answer the question."",
  model=""gpt-4-1106-preview"",
  tools=[{""type"": ""code_interpreter""}],
  file_ids=[file.id]
)

there is an upload endpoint as well, but it seems the intent of those endpoints are for fine-tuning and assistants. i think the rag use case is a normal one and not necessarily related to assistants.","['python', 'machine-learning', 'pdf', 'openai-api', 'chat-gpt-4']",78924474,"as of today (openai.__version__==1.42.0) using openai assistants + gpt-4o allows to extract content of (or answer questions on) an input pdf file foobar.pdf stored locally, with a solution along the lines of
from openai import openai
from openai.types.beta.threads.message_create_params import (
    attachment,
    attachmenttoolfilesearch,
)
import os

filename = ""foobar.pdf""
prompt = ""extract the content from the file provided without altering it. just output its exact content and nothing else.""

client = openai(api_key=os.environ.get(""my_openai_key""))

pdf_assistant = client.beta.assistants.create(
    model=""gpt-4o"",
    description=""an assistant to extract the contents of pdf files."",
    tools=[{""type"": ""file_search""}],
    name=""pdf assistant"",
)

# create thread
thread = client.beta.threads.create()

file = client.files.create(file=open(filename, ""rb""), purpose=""assistants"")

# create assistant
client.beta.threads.messages.create(
    thread_id=thread.id,
    role=""user"",
    attachments=[
        attachment(
            file_id=file.id, tools=[attachmenttoolfilesearch(type=""file_search"")]
        )
    ],
    content=prompt,
)

# run thread
run = client.beta.threads.runs.create_and_poll(
    thread_id=thread.id, assistant_id=pdf_assistant.id, timeout=1000
)

if run.status != ""completed"":
    raise exception(""run failed:"", run.status)

messages_cursor = client.beta.threads.messages.list(thread_id=thread.id)
messages = [message for message in messages_cursor]

# output text
res_txt = messages[0].content[0].text.value
print(res_txt)

the prompt can of course be replaced with the desired user request and i assume that the openai key is stored in a env var named my_openai_key.
limitations:

it's not (yet) possible to enforce json structure (other than with instructions in the prompt). this solution is inspired by 

this relies on text content in the pdf (i.e. searchable text content), and the queries won't be able to access e.g. image content in the pdf.",https://stackoverflow.com/questions/77469097,python,12-11-2023 13:25,50796.0,32.0,5.0,True,27-03-2025 11:51,27-03-2025 11:51
79007155,does prompt_token usage affect my billing when using azure openai models with your own data?,"i have set azure openai on my data, chat with azure openai models using your own data. my goal was to reduce token usage in each request.
however, i have noticed additional prompt_token usage, even when i am sending user content without any prompt. for example, if i only send the text hello there, it results in a total of 2628 tokens, whereas it should only be 24. if a longer text (7 words) is provided without any prompt, it results in a total of approximately 3.4k tokens.
example:
[{'role': 'system', 'content': ''}, {'role': 'user', 'content': 'hello there'}]
total_tokens: {'completion_tokens': 24, 'prompt_tokens': 2604, 'total_tokens': 2628}

----------------------------------------------------

[{'role': 'system', 'content': ''},
{'role': 'user', 'content': 'i worked overtime what should i do?'}]
total_tokens: {'completion_tokens': 52, 'prompt_tokens': 3334, 'total_tokens': 3386}

as you can see, under total token i am seeing prompt_token usage close the ~3.5k.
where does prompt_token usage come from since i do not provide any prompt or system message? isn't the whole purpose of using azure openai models with your own data to reduce token usage? for each request, additional 3.5k token is very expensive. will it affect my billing, where would prompt_token be considered as part of input tokens?

pricing/details/cognitive-services/openai-service/, states that input (per 1,000 tokens) is $0.0025, than as i understand for 4,000 tokens, the cost should be $0.01.","['openai-api', 'chatgpt-api', 'azure-openai', 'azure-ai']",79016386,"there is a bit of magic going on when they ""use your data""; it is essentially retrieval augmented generation or rag. basically there's a few more steps than simply your prompt text.
they explain it fairly well here:

in total, there are two calls made to the model:


for processing the intent: the token estimate for the intent prompt includes those for the user question, conversation history, and the instructions sent to the model for intent generation.


for generating the response: the token estimate for the generation prompt includes those for the user question, conversation history, the retrieved list of document chunks, role information, and the instructions sent to it for generation.",https://stackoverflow.com/questions/79007155,openai-api,20-09-2024 14:18,429.0,1.0,1.0,True,20-10-2024 17:36,20-09-2024 15:03
74790060,view takes 1 positional argument but 2 were given,"trying to make a post request to openai with the input:
{""write hello world""}

but getting the error:
typeerror: view.__init__() takes 1 positional argument but 2 were given

here is my view:
def get_help(user_input):
    response = openai.completion.create(
        engine=""text-davinci-002"",
        prompt=""user_input"",
        temperature=0.5,
        max_tokens=1024,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0
    )
    return response[""choices""][0][""text""]

@api_view(['post'])
class receive_response(view):
    def post(self, request):
        user_input = request.post[""user_input""]
        response = get_help(user_input)
        return 

and my urls.py:
urlpatterns = [
    path(""get"", get_help, name=""get_help""),
    path(""post"", receive_response, name=""post""),
]","['python', 'django', 'openai-api']",74790220,"your problem is on this line: class receive_response(view):
(why does receive_response inherit from view?)
essentially what is happening is:

post request is received
request_received object is initialized (with the args of the post - of which there must be two)
since it inherits from view (and no __init__() is specified, the parent class' __init__() is passed the same inputs
since view.__init__() accepts a single input value, but it received two, you get your error: typeerror: view.__init__() takes 1 positional argument but 2 were given

add a def __init__(self, v1, v2): declaration to your class, and debug it to see what v1/v2 are (and decide which/what to pass to super().__init__() (view's constructor)",https://stackoverflow.com/questions/74790060,python,13-12-2022 19:34,3369.0,0.0,1.0,True,14-12-2022 05:17,14-12-2022 05:17
74100762,iterate over vector of vectors of strings without using for loops in julia,"given a vector of vectors of strings, like:
sentences = [ [""julia"", ""is"", ""1000x"", ""faster"", ""than"", ""python!""], 
              [""julia"", ""reads"", ""beautiful!""], 
              [""python"", ""has"", ""600"", ""times"", ""more"", ""libraries""] 
]

i'm trying to filter out some tokens in each of them, without losing the outer vector structure (i.e., without flattening the vector down to a single list of tokens).
so far i've achieved this using a classic for loop:
number_of_alphabetical_tokens = []
number_of_long_tokens = []
total_tokens = []

for sent in sentences
    append!(number_of_alphabetical_tokens, length([token for token in sent if all(isletter, token)]))
    append!(number_of_long_words, length([token for token in sent if length(token) > 2]))
    append!(total_tokens, length(sent))
end

collect(zip(number_of_alphabetical_tokens, number_of_long_words, total_tokens))

output: (edited as per @shayan observation)
3-element vector{tuple{any, any, any}}:
 (4, 5, 6)
 (2, 3, 3)
 (5, 6, 6)

this gets the job done, but it takes more time than i'd like (i have 6000+ documents, with thousands of sentences each...), and it looks a bit like an antipattern.
is there a way of doing this with comprehensions or broadcasting (or any more performant method)?","['loops', 'vector', 'nlp', 'julia', 'array-broadcasting']",74101904,"there's no reason to avoid loops for performance reasons in julia. loops are fast, and vectorized code is just loops in disguise.
here's an example of doing this with loops, and some reductions, like all and count:
function wordstats(sentences)
    out = vector{ntuple{3, int}}(undef, length(sentences))
    for (i, sent) in pairs(sentences)
        a = count(all(isletter, word) for word in sent)
        b = count(length(word)>2 for word in sent)
        c = length(sent)
        out[i] = (a, b, c)
    end
    return out
end

the above code is not optimized, for example, counting words longer than 2 can be improved, but it runs in approximately 700ns on my laptop, which is much faster than the vectorized solution.
edit: here's basically the same code, but using the map do syntax (so you don't have to figure out the return type):
function wordstats2(sentences)
    map(sentences) do sent
        a = count(all(isletter, word) for word in sent)
        b = count(length(word)>2 for word in sent)
        c = length(sent)
        return (a, b, c)
    end
end",https://stackoverflow.com/questions/74100762,loops,17-10-2022 17:03,557.0,1.0,2.0,True,20-10-2022 12:17,17-10-2022 23:31
79293919,determining most popular words in the english dictionary within a dictionary of words,"forgive me if my wording is awful, but i'm trying to figure out how to determine the most used words in the english language from a set of words in a dictionary i've made. i've done some research on nltk but can't seem to find a function within it (or any other library for that matter) that will help me do what i need to do.
for example:
a sentence ""i enjoy a cold glass of water on a hot day"" would return ""water"" because it's the most used word in day to day conversation from the sentence. essentially i need a returned value of the most frequently used word in conversations.
i figure i'll likely have to involve ai, but any time i've tried to use ai i wind up copy and pasting code because i just don't understand it, so i'm trying to avoid going that route
any and all help is welcome and appreciated.
for context, i decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.","['python', 'nlp', 'nltk', 'detection']",79294074,"you need a external dataset for this task. you can try dataset such as google n gram dataset.
here is the breakdown of the problem statement:

input: ""i enjoy a cold glass of water on a hot day"". output: ""water"".
split the sentences into words list.


example: [""i"", ""enjoy"", ""a"", ""cold"", ""glass"", ""of"", ""water"", ""on"",
""a"", ""hot"", ""day""]


first loop in through all the word of the sentences. so let say you are at first word ""i"".
now you will look the same word ""i"" in external dataset and will look for the frequency of that word.
let say the word ""i"" in external dataset is repeated 5000000 times
repeat this task for all the word.
now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data.
frequency in the below example is random value not exact value.


{
    ""i"": 5000000,
    ""enjoy"": 50000,
    ""a"": 10000000,
    ""cold"": 30000,
    ""glass"": 100000,
    ""of"": 8000000,
    ""water"": 1200000,
    ""on"": 6000000,
    ""hot"": 700000,
    ""day"": 400000
}



pick the word with highest frequency.

note: you can try any big corpus as external data. using big corpus will have most of the english word which is used in conversation. and even if the frequency is not mentioned then you can create that yourself",https://stackoverflow.com/questions/79293919,python,19-12-2024 10:24,73.0,0.0,2.0,True,19-12-2024 11:17,19-12-2024 10:41
77836174,how can i add a progress bar/status when creating a vector store with langchain?,"creating a vector store with the python library langchain may take a while. how can i add a progress bar?

example of code where a vector store is created with langchain:
import pprint
from langchain_community.vectorstores import faiss
from langchain_community.embeddings import huggingfaceembeddings
from langchain.docstore.document import document

model = ""sentence-transformers/multi-qa-minilm-l6-cos-v1""
embeddings = huggingfaceembeddings(model_name = model)

def main():
    doc1 = document(page_content=""the sky is blue."",    metadata={""document_id"": ""10""})
    doc2 = document(page_content=""the forest is green"", metadata={""document_id"": ""62""})
    docs = []
    docs.append(doc1)
    docs.append(doc2)

    for doc in docs:
        doc.metadata['summary'] = 'hello'

    pprint.pprint(docs)
    db = faiss.from_documents(docs, embeddings)
    db.save_local(""faiss_index"")
    new_db = faiss.load_local(""faiss_index"", embeddings)

    query = ""which color is the sky?""
    docs = new_db.similarity_search_with_score(query)
    print('retrieved docs:', docs)
    print('metadata of the most relevant document:', docs[0][0].metadata)

if __name__ == '__main__':
    main()

tested with python 3.11 with:
pip install langchain==0.1.1 langchain_openai==0.0.2.post1 sentence-transformers==2.2.2 langchain_community==0.0.13 faiss-cpu==1.7.4

the vector store is created with db = faiss.from_documents(docs, embeddings).","['python', 'progress-bar', 'langchain', 'faiss']",77839038,"langchain does not natively support any progress bar for this at the moment with release of 1.0.0
i also had similar case, so instead of sending all the documents, i send independent document for ingestion and tracked progress at my end. this was helpful for me.
you can do the ingestion in the following way
    with tqdm(total=len(docs), desc=""ingesting documents"") as pbar:
        for d in docs:
            if db:
                db.add_documents([d])
            else:
                db = faiss.from_documents([d], embeddings)
            pbar.update(1)  


from what i checked from langchain code  they are making call to add_texts as well, so no major operation is being performed here other than parsing.
i had simple documents, and i didn't observe much difference. probably others who has tried on huge documents can add if it adds latency in their usecase.
below is your updated code
import pprint
from tqdm import tqdm
from langchain_community.vectorstores import faiss
from langchain_community.embeddings import huggingfaceembeddings
from langchain.docstore.document import document

model = ""sentence-transformers/multi-qa-minilm-l6-cos-v1""
embeddings = huggingfaceembeddings(model_name = model)

def main():
    doc1 = document(page_content=""the sky is blue."",    metadata={""document_id"": ""10""})
    doc2 = document(page_content=""the forest is green"", metadata={""document_id"": ""62""})
    docs = []
    docs.append(doc1)
    docs.append(doc2)

    for doc in docs:
        doc.metadata['summary'] = 'hello'

    db = none
    with tqdm(total=len(docs), desc=""ingesting documents"") as pbar:
        for d in docs:
            if db:
                db.add_documents([d])
            else:
                db = faiss.from_documents([d], embeddings)
            pbar.update(1)  

    # pprint.pprint(docs)
    # db = faiss.from_documents(docs, embeddings)
    db.save_local(""faiss_index"")
    new_db = faiss.load_local(""faiss_index"", embeddings)

    query = ""which color is the sky?""
    docs = new_db.similarity_search_with_score(query)
    print('retrieved docs:', docs)
    print('metadata of the most relevant document:', docs[0][0].metadata)

if __name__ == '__main__':
    main()",https://stackoverflow.com/questions/77836174,python,17-01-2024 23:59,5710.0,6.0,2.0,True,14-08-2024 06:12,18-01-2024 05:43
27750608,error installing nltk supporting packages : nltk.download(),"i have installed the nltk package. following that i am trying to download the supporting packages using nltk.download() and am getting error:
[errno 11001] getaddrinfo 

my machine / software details are:
os: windows 8.1
python: 3.3.4
nltk package: 3.0
below are the commands run in python:
python 3.3.4 (v3.3.4:7ff62415e426, feb 10 2014, 18:13:51) [msc v.1600 64 bit (amd64)] on win32
type ""copyright"", ""credits"" or ""license()"" for more information.

import nltk

nltk.download()
showing info 
true

nltk.download(""all"")
[nltk_data] error loading all: <urlopen error [errno 11001]
[nltk_data]     getaddrinfo failed>
false


it looks like it is going to   whereas it should ideally try to get the data from 
on another machine when we type  in the browser, it redirects to  i am not understanding why the redirection is not happening on my laptop.
i feel that this might be the issue.
i have added the command prompt screenshot.","['python', 'python-3.x', 'nltk']",27764910,"got the solution. the issue in my case was that when the nltk downloader started it had the server index as - 
this needs to be changed to - 
you can change this by going into the nltk downloader window and the file->change server index.
regards,
bonson",https://stackoverflow.com/questions/27750608,python,03-01-2015 00:51,67274.0,9.0,11.0,True,05-01-2024 10:31,15-09-2023 15:16
29151329,arabic lemmatization and stanford nlp,"i try to make lemmatization, ie identifying the lemma and possibly the arabic root of a verb, for example:
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ==> lemma (infinitive of the verb) ==> ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ==> root (triliteral root / jidr thoulathi)
==> ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½
do you think stanford","['nlp', 'stanford-nlp', 'lexical-analysis', 'stemming', 'lemmatization']",29219800,"the stanford arabic segmenter can't do true lemmatization. however, it is possible to train a new model to do something like stemming:

ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½+ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ +ï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½+ ï¿½ï¿""ýýýýýý"" is not a true lemma), you might be better off with a tool like madamira (
elaboration: the stanford arabic segmenter produces its output character-by-character using only these operations (implemented in edu.stanford.nlp.international.arabic.process.iobutils):

split a word between two characters
transform lil- (ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½) into li+ al- (ï¿½ï¿½+ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½)
transform ta (ï¿½ï¿½) or ha (ï¿½ï¿½) into ta marbuta (ï¿½ï¿½)
transform ya (ï¿½ï¿½) or alif (ï¿½ï¿½) into alif maqsura (ï¿½ï¿½)
transform alif maqsura (ï¿½ï¿½) into ya (ï¿½ï¿½)

so lemmatizing ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ to ï¿½ï¿½+ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ would require implementing an extra rule, i.e., to insert an alif after ya or ta. lemmatization of certain irregular forms would be completely impossible (for example, ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½).
the version of the stanford segmenter available for download also only breaks off proebank or a similarly rich source of arabic text with morphological segmentation annotated, it is possible to train your own model to remove all morphological affixes, which is closer to lemmatization:
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½""ýýýýýýýý"" is not a real arabic word, but the segmenter should at least consistently produce ""ýýýýýýýý"" for ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ,ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ,ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½, etc. if this is acceptable, you would need to change the atb preprocessing script to instead use the morphological segmentation annotations. you could do this by replacing the script called 
then follow the instructions for ""training the segmenter"" in the readme.",https://stackoverflow.com/questions/29151329,nlp,19-03-2015 17:33,5327.0,5.0,2.0,True,06-01-2025 09:17,19-03-2015 21:13
78925963,unexpected value passed to langchain tool argument,"i'm trying to create a simple example tool that creates new user accounts in a hypothetical application when instructed to do so via a user prompt. the llm being used is llama3.1:8b via ollama.
so far what i've written works, but it's very unreliable.
the reason why it's unreliable is because when langchain calls on my tool, it provides unexpected/inconsistent values to the user creation tool's single username argument.
sometime the argument will be a proper username and other times it will be a username with the value ""username="" prefixed to the username (eg: ""username=jdoe"" rather than simply ""jdoe"").
also, if i ask for multiple users to be created, sometimes langchain will correctly invoke the tool multiple times while other times, it will invoke the tool once with a string in the format of an array (eg: ""['jdoe','jsmith']"")
my questions are:

is the issue i'm encountering due to the limitations of langchain or the llama3.1:8b model that i'm using? or is the issue something else?
is there a way to get langchain to more reliably call my user creation tool with a correctly formatted username?
are there are other useful tips/recommendations that you can provide for a beginner like me?

below is my code:
from dotenv import load_dotenv
from langchain.agents import agentexecutor, create_react_agent
from langchain.tools import tool
from langchain_core.prompts import prompttemplate
from langchain_ollama.chat_models import chatollama

load_dotenv()


# define the tool to create a user account
mock_user_db = [""jdoe"", ""jrogers"", ""jsmith""]


def create_user_tool(username: str):
    print(""username provided for creation: "" + username)
    if username in mock_user_db:
        return f""user {username} already exists.""
    mock_user_db.append(username)
    return f""user {username} created successfully.""


# define the tool to delete a user account
def delete_user_tool(username: str):
    print(""username provided for deletion: "" + username)
    if username not in mock_user_db:
        return f""user {username} does not exist.""
    
    mock_user_db.remove(username)
    return f""user {username} deleted successfully.""


def list_users_tool(ignore) -> list:
    return mock_user_db


# wrap these functions as langchain tools
create_user = tool(
    name=""create user"",
    func=create_user_tool,
    description=""creates a new user account in the company hr system.""
)

delete_user = tool(
    name=""delete user"",
    func=delete_user_tool,
    description=""deletes an existing user account in company hr system.""
)

list_users = tool(
    name=""list users"",
    func=list_users_tool,
    description=""lists all user accounts in company hr system.""
)

# initialize the language model
llm = chatollama(model=""llama3.1:latest"", temperature=0)

# create the agent using the tools
tools = [create_user, delete_user, list_users]

# get the prompt to use
#prompt = hub.pull(""hwchase17/react"") # does not work with ollama/llama3:8b
prompt = hub.pull(""hwchase17/react-chat"") # kinda works with ollama/llama3:8b

agent = create_react_agent(llm, tools, prompt)

# create an agent executor by passing in the agent and tools
agent_executor = agentexecutor(agent=agent, tools=tools, handle_parsing_errors=true)

print(agent_executor.invoke({""input"": ""please introduce yourself.""})['output'])

while true:
    user_prompt = input(""prompt: "")
    agent_response = agent_executor.invoke({""input"": user_prompt})
    print(agent_response['output'])","['python', 'langchain', 'large-language-model']",79022969,"prompt engineering (what you are attempting here), is far from an exact science.  however, there are ways you can clarify the schema of the tool.
one example (from their docs) is getting it to parse your docstrings:
@tool(parse_docstring=true)
def create_user(username: str):
    """"""creates a user

        args:
            username: username of the user to be created. the exact string of the username, no longer than 20 characters long
    """"""
    ... # rest of your code here

see docs here
but even more reliable would be to create your schema with pydantic (great tool in general), again, from their docs:

class create_user(basemodel):
    """"""creates a user""""""

    username: str = field(..., description=""username of the user to be created. the exact string of the username, no longer than 20 characters long""

in general, the more detail you provide, regarding the shape and nature of the tools and the data, the better results you can expect.
you may also want to consider setting your temperature to 0, so you get repeatable responses for any given prompt, which should help with debugging, but you need to test with a higher range of prompts to ensure reliable behaviour",https://stackoverflow.com/questions/78925963,python,29-08-2024 02:57,794.0,4.0,1.0,True,23-12-2024 12:57,23-12-2024 12:57
70880940,combine two regexp grammars in nltk,"i'm defining a noun phrase using grammar in nltk. the example provided by nltk is:
grammar = ""np: {<dt>?<nnp>*<nn>}""

then if i have a sentence like: show me the paris hospitals, the library can detect the noun phrase:
>>> s
'show me the paris hospitals'
>>> grammar = ""np: {<dt>?<nnp>*<nns>}""
>>> nltk.regexpparser(grammar).parse(nltk.pos_tag(nltk.word_tokenize(s)))
tree('s', [('show', 'vb'), ('me', 'prp'), tree('np', [('the', 'dt'), ('paris', 'nnp'), ('hospitals', 'nns')])])

now, the sentence can be written in another way: show me the hospitals of paris, and hence i need to change the grammar to:
>>> grammar = ""np: {<dt>?<nns><in><nnp>}""
>>> s = ""show me the hospitals in paris""
>>> nltk.regexpparser(grammar).parse(nltk.pos_tag(nltk.word_tokenize(s)))
tree('s', [('show', 'vb'), ('me', 'prp'), tree('np', [('the', 'dt'), ('hospitals', 'nns'), ('in', 'in'), ('paris', 'nnp')])])

how do i combine the two grammars in a unique one? i couldn't figure out the or condition for the two grammars.","['python', 'nlp', 'nltk', 'grammar']",70885691,"you can just define two np rules in one grammar:
grammar = """"""
np: {<dt>?<nnp>*<nns>}
np: {<dt>?<nns><in><nnp>}
""""""

or using | as the wanted or condition:
grammar = ""np: {<dt>?<nnp>*<nns>|<dt>?<nns><in><nnp>}""

full example:
import nltk

sentence_1 = 'show me the paris hospitals'
sentence_2 = ""show me the hospitals in paris""

grammar_1 = """"""
np: {<dt>?<nnp>*<nns>}
np: {<dt>?<nns><in><nnp>}
""""""
parser_1 = nltk.regexpparser(grammar_1)

grammar_2 = ""np: {<dt>?<nnp>*<nns>|<dt>?<nns><in><nnp>}""
parser_2 = nltk.regexpparser(grammar_2)

for s in sentence_1, sentence_2:
    tokens = nltk.word_tokenize(s)
    pos_tags = nltk.pos_tag(tokens)
    print(parser_1.parse(pos_tags))
    print(parser_2.parse(pos_tags))

# outputs the same for both parsers:
# (s show/vb me/prp (np the/dt paris/nnp hospitals/nns))
# (s show/vb me/prp (np the/dt paris/nnp hospitals/nns))
# (s show/vb me/prp (np the/dt hospitals/nns) in/in paris/nnp)
# (s show/vb me/prp (np the/dt hospitals/nns) in/in paris/nnp)

(link to the documentation)",https://stackoverflow.com/questions/70880940,python,27-01-2022 15:10,211.0,1.0,1.0,True,27-01-2022 21:28,27-01-2022 21:26
77282461,huggingface bettertransformer in `with` context - cannot disable after context,"i am writing a custom with context manager to temporarily make the model a bettertransformer model while calling trainer.evaluate().
i evaluated before, in, and after the with context. i noticed that the evaluation after the with context still uses bettertransformer. this is a problem because the trainer.train() call afterwards will also use bettertransformer, resulting in poor training due to padding.
how do i create a custom with context that only uses bettertransformer inside the context, not afterwards?
please find the mwe gist here.
i created a custom context manager:
class bettertransformercontext:
    """"""temporarily replace a model with a bettertransformer model.""""""

    def __init__(self, model):
        self.model = model
        self.original_model = none

    def __enter__(self):
        self.original_model = self.model
        self.model = bettertransformer.transform(self.model)
        return self.model

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.model = self.original_model
        # self.model = bettertransformer.reverse(self.model)  # note: same result

the output is as follows. evaluating without bettertransformer handles approximately 100 it/s, with bettertransformer handles approximately 115 it/s. as you can see, evaluating after the context still results in 115 it/s.
========== without optimum (-> should be slow) ==========
bt before context:  false
100%|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý| 204/204 [00:01<00:00, 103.09it/s]
0.3161764705882353
========== with optimum (-> should be fast) ==========
bt in context:  true
100%|ýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý| 204/204 [00:01<00:00, 116.68it/s]
0.3161764705882353
========== without optimum (-> should be slow) ==========
bt after context:  true
100%|ýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý| 204/204 [00:01<00:00, 116.53it/s]
0.3161764705882353","['python', 'huggingface-transformers', 'with-statement', 'contextmanager', 'huggingface-trainer']",77302404,"i found a solution by using a custom context manager on the trainer object, as opposed to applying it on a model object.
the custom context manager is as follows:
class bettertransformertrainercontext:
    """"""context manager to wrap trainer.model with bettertransformer.""""""
    def __init__(self, trainer):
        self.trainer = trainer

    def __enter__(self):
        self.trainer.model = bettertransformer.transform(
            self.trainer.model, keep_original_model=true
        )
        return self.trainer

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.trainer.model = bettertransformer.reverse(self.trainer.model)

it can be used as follows:
print(""="" * 10, ""with optimum (-> should be fast)"", ""="" * 10)
with bettertransformertrainercontext(trainer) as _optimum_trainer:
    eval_accuracy = _optimum_trainer.evaluate()[""eval_accuracy""]
    print(eval_accuracy)

i hope this might be helpful to someone else.",https://stackoverflow.com/questions/77282461,python,12-10-2023 16:48,112.0,0.0,1.0,True,16-10-2023 13:31,12-10-2023 16:51
74836900,valueerror: [e966] `nlp.add_pipe` when changing the sentence segmentaion rule of spacy model,"i am using python 3.9.7 and the spacy library and want to change the way the model segments a given sentence. here is a sentence and the segmentation rule i created as an example:
import spacy
nlp=spacy.load('en_core_web_sm')

doc2=nlp(u'""management is doing the  right things; leadership is doing the right things."" -peter drucker')

def set_custom_boundaries(doc):
    for token in doc[:-1]:
        if token.text=="";"":
            doc[token.i +1].is_sent_start=true
    return doc

nlp.add_pipe(set_custom_boundaries, before='parser')

however, this produces the error message below:
valueerror                                traceback (most recent call last)
c:\users\seydou~1\appdata\local\temp/ipykernel_21000/1705623728.py in <module>
----> 1 nlp.add_pipe(set_custom_boundaries, before='parser')

~\anaconda3\lib\site-packages\spacy\language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)
    777             bad_val = repr(factory_name)
    778             err = errors.e966.format(component=bad_val, name=name)
--> 779             raise valueerror(err)
    780         name = name if name is not none else factory_name
    781         if name in self.component_names:

valueerror: [e966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. expected string, but got <function set_custom_boundaries at 0x000002520a59cca0> (name: 'none').

- if you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.
    
- if you passed in a component like `textcategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.
    
- if you're using a custom component: add the decorator `@language.component` (for function components) or `@language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@language.component('your_name')`. you can then run `nlp.add_pipe('your_name')` to add it to the pipeline.

i looked at some solutions online, however, i could not solve the problem as a beginner in python. how does one use their own custom segmentation rule in the spacy pipeline?","['python', 'nlp', 'spacy']",74837826,"the syntax of
nlp.add_pipe with a custom function is given here.  you must (1) declare the component function with a 'decorator' and (2) pass the name of the component/function as a string. so it should be something like this:
@language.component(""set_custom_boundaries"")
def set_custom_boundaries(doc):
    for token in doc[:-1]:
        if token.text=="";"":
            doc[token.i +1].is_sent_start=true
    return doc



nlp.add_pipe(""set_custom_boundaries"", before='parser')

note: your function is doing a strange sentence segmentation, it won't work in general. for example it won't work if a sentence ends with '.', '...', or '!', etc.",https://stackoverflow.com/questions/74836900,python,17-12-2022 19:27,429.0,2.0,1.0,True,18-12-2022 23:56,18-12-2022 23:56
76757241,i&#39;m trying to turn this python script into a web app with flask,"i am trying to turn my script into a web app, it uses wordpress and chatgpt to generate articles based on keywords and what you type in the input boxes and allows the user to instantly upload to drafts on their wordpress site..
i am running into an issue:
updated
# app.py
from flask import flask, render_template, request, flash
from flask_wtf import flaskform
from wtforms import stringfield, textareafield, submitfield
from wtforms.validators import inputrequired
import openai
import requests
import random

app = flask(__name__)
app.config['secret_key'] = 'your_secret_key'

# set your openai api key
openai.api_key = """"


class inputform(flaskform):
    api_key = stringfield('openai api key', validators=[inputrequired()])
    input_text = textareafield('input text', validators=[inputrequired()])
    keywords = textareafield('keywords (comma-separated)', validators=[inputrequired()])
    wp_base_url = stringfield('wordpress base url', validators=[inputrequired()])
    wp_username = stringfield('wordpress username', validators=[inputrequired()])
    wp_password = stringfield('wordpress password', validators=[inputrequired()])
    submit = submitfield('submit')


# array holding wordpress author ids
authorlist = [""1"", ""2""]

# you can call the ""gpt-3.5-turbo"" model
modelengine = ""gpt-3.5-turbo""

# wordpress information
wp_base_url = """"
wp_username = """"
wp_password = """"


# post creator function
def create_post(inputtitlesent, outputtext):
    randomauthor = random.choice(authorlist)

    post_status = ""draft""
    headers = {
        ""content-type"": ""application/x-
    }
    post = {
        ""title"": inputtitlesent,
        ""content"": outputtext,
        ""status"": post_status,
        ""author"": randomauthor,
        ""categories:"": ""6""
    }
    url = wp_base_url + ""/wp-json/wp/v2/posts""
    response = requests.post(url, data=post, headers=headers, auth=(wp_username, wp_password))
    return response


# post title creator function
def create_title(outputtext):
    response = openai.chatcompletion.create(
        model=modelengine,
        messages=[{""role"": ""user"", ""content"": f""write a title for this article:\n\n {outputtext}""}],
        n=1
    )
    create_title = response.choices[0].message.content
    create_title = create_title.replace('""', '')
    return create_title


@app.route('/', methods=['get', 'post'])
def index():
    global wp_base_url, wp_username, wp_password  # move the global declaration here

    form = inputform()

    if form.validate_on_submit():
        # get user input from the form
        data = request.get_json()
        api_key = data.get('api_key')
        input_text = data.get('input_text')
        keywords = data.get('keywords')
        wp_base_url = data.get('wp_base_url')
        wp_username = data.get('wp_username')
        wp_password = data.get('wp_password')

        try:
            response = openai.chatcompletion.create(
                model=modelengine,
                messages=[{""role"": ""user"", ""content"": ""testing api key""}],
                n=1
            )

            if 'choices' not in response or not response['choices']:
                flash(""invalid api key. please check and try again."", ""error"")
                return render_template('index.html', form=form, data=none)

            # ... (rest of the code remains the same)
        except requests.exceptions.requestexception as e:
            flash(""error occurred while connecting to the openai api. please check your internet connection and try again."", ""error"")
            return render_template('index.html', form=form, data=none)

        except exception as e:
            flash(""an unexpected error occurred. please try again later."", ""error"")
            return render_template('index.html', form=form, data=none)

    return render_template('index.html', form=form, data=none)


if __name__ == '__main__':
    app.run(debug=true)

the requests  are getting sent to the server on debug but it doesn't run the script when pressing submit and the inputs doesn't render, how can i fix this?
index.html (updated):
<!doctype html>
<html>
<head>
    <title>openai wordpress web app</title>
</head>
<body>
    <h1>input your information:</h1>
    <form id=""inputform"" method=""post"">
        {{ form.csrf_token }}
        {{ form.api_key.label }} {{ form.api_key(id='api_key', size=60) }}<br>
        {{ form.input_text.label }} {{ form.input_text(id='input_text', rows=10, cols=60) }}<br>
        {{ form.keywords.label }} {{ form.keywords(id='keywords', rows=3, cols=60) }}<br>
        {{ form.wp_base_url.label }} {{ form.wp_base_url(id='wp_base_url', size=60) }}<br>
        {{ form.wp_username.label }} {{ form.wp_username(id='wp_username', size=60) }}<br>
        {{ form.wp_password.label }} {{ form.wp_password(id='wp_password', size=60) }}<br>
        {{ form.submit() }}
    </form>

    {% if data %}
        <h1>generated posts:</h1>
        {% for post in data %}
            <h2>{{ post.title }}</h2>
            <p>{{ post.content }}</p>
            <hr>
        {% endfor %}
    {% endif %}

    <script>
        document.getelementbyid('inputform').addeventlistener('submit', function (event) {
            event.preventdefault(); // prevent the form from submitting normally
            var api_key = document.getelementbyid('api_key').value;
            var input_text = document.getelementbyid('input_text').value;
            var keywords = document.getelementbyid('keywords').value;
            var wp_base_url = document.getelementbyid('wp_base_url').value;
            var wp_username = document.getelementbyid('wp_username').value;
            var wp_password = document.getelementbyid('wp_password').value;

            var xhr = new xml
            xhr.open('post', '/');
            xhr.setrequestheader('content-type', 'application/json');
            xhr.onreadystatechange = function () {
                if (xhr.readystate === xml {
                    if (xhr.status === 200) {
                        // request successful, update the page if necessary
                        var response = json.parse(xhr.responsetext);
                        if (response.error) {
                            alert('error: ' + response.error);
                        } else {
                            alert('data processed successfully!');
                            // optionally, you can update the page with the generated posts here
                        }
                    } else {
                        // handle error here
                        alert('error occurred. please try again later.');
                    }
                }
            };

            var data = json.stringify({
                api_key: api_key,
                input_text: input_text,
                keywords: keywords,
                wp_base_url: wp_base_url,
                wp_username: wp_username,
                wp_password: wp_password
            });

            xhr.send(data);
        });
    </script>
</body>
</html>","['python', 'flask', 'openai-api']",76759240,"edir your index.html :
    <!doctype html>
<html lang=""en"">
<head>
    <title>openai wordpress web app</title>
</head>
<body>
    <h1>input your information:</h1>
    <form id=""inputform"" method=""post"" action=""{{ url_for('index') }}"">
        {{ form.csrf_token }}
        {{ form.api_key.label }} {{ form.api_key(id='api_key', size=60) }}<br>
        {{ form.input_text.label }} {{ form.input_text(id='input_text', rows=10, cols=60) }}<br>
        {{ form.keywords.label }} {{ form.keywords(id='keywords', rows=3, cols=60) }}<br>
        {{ form.wp_base_url.label }} {{ form.wp_base_url(id='wp_base_url', size=60) }}<br>
        {{ form.wp_username.label }} {{ form.wp_username(id='wp_username', size=60) }}<br>
        {{ form.wp_password.label }} {{ form.wp_password(id='wp_password', size=60) }}<br>
        {{ form.submit() }}
    </form>

    {% if data %}
        <h1>generated posts:</h1>
        {% for post in data %}
            <h2>{{ post.title }}</h2>
            <p>{{ post.content }}</p>
            <hr>
        {% endfor %}
    {% endif %}


</body>
</html>

and it will be you flask wtf form :
from flask_wtf import flaskform
from wtforms import stringfield, textareafield, submitfield
from wtforms.validators import  datarequired

class inputform(flaskform):
    api_key = stringfield('openai api key', validators=[datarequired()])
    input_text = textareafield('input text', validators=[datarequired()])
    keywords = textareafield('keywords (comma-separated)', validators=[datarequired()])
    wp_base_url = stringfield('wordpress base url', validators=[datarequired()])
    wp_username = stringfield('wordpress username', validators=[datarequired()])
    wp_password = stringfield('wordpress password', validators=[datarequired()])
    submit = submitfield('submit')

and tour index route will be you can just add you rest of your code problem was getting information right ?
@app.route('/', methods=['get', 'post'])
def index():
    print(request.method) # testing request method coming from your index.html form
    form = inputform()
    if request.method == 'post' and form.validate_on_submit():
    # get user input from the for
        api_key = form.api_key.data
        input_text = form.input_text.data
        keywords = form.keywords.data
        wp_base_url = form.wp_base_url.data
        wp_username = form.wp_username.data
        wp_password = form.wp_password.data
        for item in form: # for testing your results
            print(item.data)
        # rest of your code 

return render_template('index.html', form=form, data=none)",https://stackoverflow.com/questions/76757241,python,24-07-2023 18:43,118.0,1.0,1.0,True,26-07-2023 06:38,25-07-2023 19:36
78001556,error while installing sentence-transformers,"get the following error while installing sentence-transformers on windows 11 (using the latest version of python and pip). can someone please help with this? checked many other similar posts, but none of those solutions work.
c:\users\abc\ai\llama\jupyterproj\stlit>py -m pip install sentence-transformers
collecting sentence-transformers
  using cached sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kb)
collecting transformers<5.0.0,>=4.32.0 (from sentence-transformers)
  using cached transformers-4.37.2-py3-none-any.whl.metadata (129 kb)
requirement already satisfied: tqdm in c:\users\abc\appdata\local\programs\python\python312\lib\site-packages (from sentence-transformers) (4.66.2)
requirement already satisfied: torch>=1.11.0 in c:\users\abc\appdata\local\programs\python\python312\lib\site-packages (from sentence-transformers) (2.2.0)
requirement already satisfied: numpy in c:\users\abc\appdata\local\programs\python\python312\lib\site-packages (from sentence-transformers) (1.26.4)
collecting scikit-learn (from sentence-transformers)
  using cached scikit_learn-1.4.0-1-cp312-cp312-win_amd64.whl.metadata (11 kb)
collecting scipy (from sentence-transformers)
  using cached scipy-1.12.0-cp312-cp312-win_amd64.whl.metadata (60 kb)
collecting nltk (from sentence-transformers)
  using cached nltk-3.8.1-py3-none-any.whl (1.5 mb)
collecting sentencepiece (from sentence-transformers)
  using cached sentencepiece-0.1.99.tar.gz (2.6 mb)
  preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  ï¿½ï¿½ python setup.py egg_info did not run successfully.
  ï¿½ï¿½ï¿½ exit code: 1
  ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½> [17 lines of output]
      traceback (most recent call last):
        file ""<string>"", line 2, in <module>
        file ""<pip-setuptools-caller>"", line 34, in <module>
        file ""c:\users\abc\appdata\local\temp\pip-install-3n9shirh\sentencepiece_fc383392079e43b6a8c226f0484c0928\setup.py"", line 126, in <moduocess.check_call([
        file ""c:\users\abc\appdata\local\programs\python\python312\lib\subprocess.py"", line 408, in check_call
          retcode = call(*popenargs, **kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
        file ""c:\users\abc\appdata\local\programs\python\python312\lib\subprocess.py"", line 389, in call
          with popen(*popenargs, **kwargs) as p:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
        file ""c:\users\abc\appdata\local\programs\python\python312\lib\subprocess.py"", line 1026, in __init__
          self._execute_child(args, executable, preexec_fn, close_fds,
        file ""c:\users\abc\appdata\local\programs\python\python312\lib\subprocess.py"", line 1538, in _execute_child
          hp, ht, pid, tid = _winapi.createprocess(executable, args,
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      filenotfounderror: [winerror 2] the system cannot find the file specified
      [end of output]

note: this error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed
ï¿½ï¿½ encountered error while generating package metadata.
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½> see above for output.
note: this is an issue with the package mentioned above, not pip.
hint: see above f","['python', 'python-3.x', 'pip', 'sentence-transformers']",78001637,"you need to use python 3.11 to install sentence-transformers
it is dependent on pytorch


currently, pytorch on windows only supports python 3.8-3.11; python 2.x is not supported.



we recommend python 3.8 or higher, pytorch 1.11.0 or higher and transformers v4.32.0 or higher. the code does not work with python 2.7",https://stackoverflow.com/questions/78001556,python,15-02-2024 14:26,1299.0,1.0,1.0,True,27-05-2024 06:21,27-05-2024 06:21
53316174,using pre-trained word embeddings - how to create vector for unknown / oov token?,"i wan't to add pre-trained embeddings to a model. but as it seems there is no out-of-vocabulary (oov) token resp. no vector for unseen words existent. 
so what can i do to handle oov-tokens i come across? i have some ideas, but none of them seem to be very good:

i could just create a random vector for this token, but ideally i'd like the vector to within the logic of the existing model. if i just create it randomly i'm afraid the vector accidentally could be very similar to a very frequent word like 'the', 'for', 'that' etc. which is not my intention.
or should i just initialize the vector with plain zeros instead?
another idea would be averaging the token over other existing vectors. but averaging on what vectors then? on all? this doesn't seem to be very conclusive either.
i also thought about trying to train this vector. however this doesn't come very handy if i want to freeze the rest of the embedding during training. 

(a general solution is appreciated, but i wanted to add that i'm using pytorch - just in case pytorch already comes with a handy solution to this problem.)
so what would be a good and easy strategy to create such a vector?","['neural-network', 'deep-learning', 'nlp', 'pytorch', 'word-embedding']",53329859,"there are multiple ways you can deal with it. i don't think i can cite references about which works better.
non-trainable option:

random vector as embedding
you can use an all-zero vector for oov.
you can use the mean of all the embedding vectors, that way you avoid the risk of being away from the actual distribution.
also embeddings generally come with ""unk"" vectors learned during the training you can use that.

trainable option:
you can declare a separate embedding vector for oov and make it trainable keeping other embedding fixed. you might have to over-write the forward method of embedding lookup for this. you can declare a new trainable variable and in the forward pass use this vector as embedding for oov instead of doing a look-up.

addressing the comments of op:
i am not sure which of the three non-trainable methods may work better and i am not sure if there is some work about this. but method 4) should be working better.
for trainable option, you can create a new embedding layer as below.
class embeddings_new(torch.nn.module): 
    def __init__(self, dim, vocab): 
        super().__init__() 
        self.embedding = torch.nn.embedding(vocab, dim) 
        self.embedding.weight.requires_grad = false
        # vector for oov 
        self.oov = torch.nn.parameter(data=torch.rand(1,dim)) 
        self.oov_index = -1 
        self.dim = dim 

    def forward(self, arr): 
        n = arr.shape[0] 
        mask =  (arr==self.oov_index).long() 
        mask_ = mask.unsqueeze(dim=1).float() 
        embed =(1-mask_)*self.embedding((1-mask)*arr) + mask_*(self.oov.expand((n,self.dim))) 
        return embed 

usage:
model = embeddings_new(10,20000)    
out = model.forward(torch.tensor([-1,-1, 100, 1, 0]))
# dummy loss
loss = torch.sum(a**2)
loss.backward()",https://stackoverflow.com/questions/53316174,neural-network,15-11-2018 09:26,3856.0,4.0,1.0,True,11-04-2022 17:56,15-11-2018 11:06
78837277,resourcenotfound error when connecting to azure openai using latest javascript sdk (version 2.0.0-beta.1),"i am trying to connect to azure openai using the latest version of sdk (version 2.0.0-beta.1) but no matter what i do, i am getting resource not found error). when i use the previous version (1.0.0-beta.12) of the sdk, everything works well.
these are the settings in my environment file:
azure_openai_endpoint=""
azure_openai_api_key=""xxxxyyyyxxxxyyyyxxxxyyyyxxxxyyyy""
azure_openai_chat_completion_model_deployment_id=""gpt-4o""
azure_openai_api_version=""2024-04-01-preview""

and this is how i am creating my client:
azureopenaiclient = new azureopenai({
    baseurl: process.env.azure_openai_endpoint,
    // apikey: process.env.azure_openai_api_key, (tried without commenting it as well)
    deployment: process.env.azure_openai_chat_completion_model_deployment_id,
    apiversion: process.env.azure_openai_api_version,
});

and this is my code:
const messages: openai.chatcompletionmessageparam[] = [
    { role: 'system', content: systemmessage },
    { role: 'user', content: usermessage },
    ];
const result = await azureopenaiclient.chat.completions.create(
    {
        messages,
        model: '',
        response_format: { type: jsonoutput ? 'json_object' : 'text' },
    },
    {},
    );
let response = '';
for await (const choice of result.choices) {
    response += choice.message?.content;
}

i even tried changing the model to my deployment id above but the result is the same.
i am pretty sure i am missing something really trivial but i am not able to figure it out. can anyone tell me what i am doing wrong here? thanks.
update
here's the stack trace:
notfounderror: 404 resource not found
    at apierror.generate (webpack-internal:///(action-browser)/../../node_modules/openai/error.mjs:67:20)
    at azureopenai.makestatuserror (webpack-internal:///(action-browser)/../../node_modules/openai/core.mjs:304:65)
    at azureopenai.makerequest (webpack-internal:///(action-browser)/../../node_modules/openai/core.mjs:347:30)
    at process.processticksandrejections (node:internal/process/task_queues:95:5)
....rest is the list of my files","['typescript', 'azure', 'openai-api', 'azure-openai']",78841338,"the issue was that i was trying to set baseurl property when i was instantiating azureopenai. the correct property is endpoint.
here's the github issue that solved my problem:",https://stackoverflow.com/questions/78837277,typescript,06-08-2024 04:40,529.0,2.0,1.0,True,06-08-2024 23:25,06-08-2024 04:58
70996277,why flair does&#39;t recognize the entire location name of simple sentence?,"i'm tying to to detect simple location with ner algorithm, and i'm getting semi-correct results:
from flair.data   import sentence
from flair.models import sequencetagger

tagger   = sequencetagger.load('ner')
text     = 'jackson leaves at north carolina'
sentence = sentence(text)

tagger.predict(sentence)
for entity in sentence.get_spans('ner'):
    print(entity)

output:
span [1]: ""jackson""   [ï¿½ï¿½ï¿½ labels: per (0.9996)]
span [5]: ""carolina""   [ï¿½ï¿½ï¿½ labels: loc (0.7363)]

i was expecting to receive ""north carolina"".

can flair detect full location description? what do we need for it?
is there any ner algorithm that cat detect full location description?","['deep-learning', 'nlp', 'named-entity-recognition', 'flair']",72038013,"flair can detect full location description. the reason for your issue is that the 'north' is not capitalized.
if you run
from flair.data   import sentence
from flair.models import sequencetagger

tagger   = sequencetagger.load('ner')
text     = 'jackson leaves at north carolina'
sentence = sentence(text)

tagger.predict(sentence)
for entity in sentence.get_spans('ner'):
    print(entity)

you'll get
span[0:1]: ""jackson"" ï¿½ï¿½ï¿½ per (0.9997)
span[3:5]: ""north carolina"" ï¿½ï¿½ï¿½ loc (0.9246)
</",https://stackoverflow.com/questions/70996277,deep-learning,05-02-2022 08:09,501.0,0.0,1.0,True,28-04-2022 04:21,05-02-2022 21:14
78084538,openai assistants api: how do i upload a file and use it as a knowledge base?,"my goal is to create a chatbot that i can provide a file to that holds a bunch of text, and then use the openai assistants api to actually use the file when querying my chatbot. i will use the gpt-3.5-turbo model to answer the questions.
the code i have is the following:
file_response = client.files.create(
   file=open(""website_content.txt"", ""rb""),
   purpose=""assistants""
)

query_response = client.assistants.query(
   assistant_id=""my_assistant_id"", 
   input=""tell me about xxx?"",
   files=[file_response['id']] 
)

however, this is not working, for what i think could be a few things. for one, i don't fully understand the way it is supposed to work, so i was looking for some guidance. i have already created an assistant via the dashboard, but now i want to just upload a file and then query it. do i have to use something else, like ""threads"" via the api, or no?
how do i do this?","['python', 'artificial-intelligence', 'openai-api', 'chatgpt-api', 'openai-assistants-api']",78103896,"note: the code below works with the openai assistants api v1. in april 2024, the openai assistants api v2 was released. see the migration guide.

i created a customer support chatbot and made a youtube tutorial about it.
the process is as follows:
step 1: upload a file with an ""assistants"" purpose
my_file = client.files.create(
  file=open(""knowledge.txt"", ""rb""),
  purpose='assistants'
)

step 2: create an assistant
my_assistant = client.beta.assistants.create(
    model=""gpt-3.5-turbo-1106"",
    instructions=""you are a customer support chatbot. use your knowledge base to best respond to customer queries."",
    name=""customer support chatbot"",
    tools=[{""type"": ""retrieval""}]
)

step 3: create a thread
my_thread = client.beta.threads.create()

step 4: add a message to a thread
my_thread_message = client.beta.threads.messages.create(
  thread_id=my_thread.id,
  role=""user"",
  content=""what can i buy in your online store?"",
  file_ids=[my_file.id]
)

step 5: run the assistant
my_run = client.beta.threads.runs.create(
  thread_id=my_thread.id,
  assistant_id=my_assistant.id,
)

step 6: periodically retrieve the run to check on its status to see if it has moved to completed
keep_retrieving_run = client.beta.threads.runs.retrieve(
    thread_id=my_thread.id,
    run_id=my_run.id
)

step 7: retrieve the messages added by the assistant to the thread once the run status is ""completed""
all_messages = client.beta.threads.messages.list(
    thread_id=my_thread.id
)

print(f""user: {my_thread_message.content[0].text.value}"")
print(f""assistant: {all_messages.data[0].content[0].text.value}"")

see the full code.
important note
the assistant might sometimes behave strangely. the assistants api is still in beta, and it seems that openai has trouble keeping it realiable, as discussed on the official openai forum.
the assistant might sometimes answer that it cannot access the files you uploaded. you might think you did something wrong, but if you run identical code later or the next day, the assistant will successfully access all files and give you an answer.
the weird responses i got were the following:

assistant: i currently do not have access to the file you uploaded.
could you provide some details about what you're selling or any
specific questions you have in mind?
assistant: i currently don't have the ability to directly access the
contents of the file you uploaded. however, if you can provide some
details or specific questions about the than happy to assist you in
finding the information you need.
assistant: i currently don't have visibility into the specific
contents of the file you've uploaded. could you provide more details
about the file or its contents so that i can assist you further?
assistant: i see you've uploaded a file. how can i assist you with
it?",https://stackoverflow.com/questions/78084538,python,29-02-2024 22:06,18398.0,2.0,1.0,True,09-07-2024 17:56,22-03-2024 12:26
77990896,"importerror: dependencies for instructorembedding not found, while it is installed","i already installed instructorembedding, but it keeps giving me the error, in jupyter notebook environment using python 3.12 (i also tried in 3.11). kernel restarting didn't help.
import torch
from langchain.embeddings import huggingfaceinstructembeddings

device = ""cuda:0"" if torch.cuda.is_available() else ""cpu""


embedding = huggingfaceinstructembeddings(model_name=""sentence-transformers/all-minilm-l6-v2"", model_kwargs={""device"": device})

error:
---------------------------------------------------------------------------
modulenotfounderror                       traceback (most recent call last)
file /opt/conda/lib/python3.11/site-packages/langchain_community/embeddings/huggingface.py:151, in huggingfaceinstructembeddings.__init__(self, **kwargs)
    150 try:
--> 151     from instructorembedding import instructor
    153     self.client = instructor(
    154         self.model_name, cache_folder=self.cache_folder, **self.model_kwargs
    155     )

file /opt/conda/lib/python3.11/site-packages/instructorembedding/__init__.py:1
----> 1 from .instructor import *

file /opt/conda/lib/python3.11/site-packages/instructorembedding/instructor.py:9
      8 from torch import tensor, device
----> 9 from sentence_transformers import sentencetransformer
     10 from sentence_transformers.models import transformer

modulenotfounderror: no module named 'sentence_transformers'

the above exception was the direct cause of the following exception:

importerror                               traceback (most recent call last)
cell in[2], line 10
      4 device = ""cuda:0"" if torch.cuda.is_available() else ""cpu""
      6 #loader = pypdfdirectoryloader(""aircraft_pdfs"")
      7 #docs = loader.load()
      8 #print(len(docs))  # length of all pages together
---> 10 embedding = huggingfaceinstructembeddings(model_name=""sentence-transformers/all-minilm-l6-v2"", model_kwargs={""device"": device})

file /opt/conda/lib/python3.11/site-packages/langchain_community/embeddings/huggingface.py:157, in huggingfaceinstructembeddings.__init__(self, **kwargs)
    153     self.client = instructor(
    154         self.model_name, cache_folder=self.cache_folder, **self.model_kwargs
    155     )
    156 except importerror as e:
--> 157     raise importerror(""dependencies for instructorembedding not found."") from e

importerror: dependencies for instructorembedding not found.

here is the output of pip freeze
transformers==4.37.2
torch==2.2.0
langchain==0.1.6
instructorembedding==1.0.1
...","['python', 'langchain', 'sentence-transformers']",77990923,"i think you would also need to install sentence-transformers. try installing it via:
pip install -u sentence-transformers==2.2.2

and then run your code. please make sure you install the version 2.2.2 otherwise you'll end up with this error:
typeerror: instructor._load_sbert_model() got an unexpected keyword argument 'token'

it seems the latest version of sentence-transformers has some compatibility issues.",https://stackoverflow.com/questions/77990896,python,13-02-2024 21:13,10112.0,2.0,2.0,True,09-03-2024 21:03,09-03-2024 21:03
76883181,running sentence transformers at pythonanywhere,"i am trying to run a huggingface model for computing vector embeddings as explained here at pythonanywhere (it worked just fine locally on my laptop under ubuntu under wsl2).
the installation went fine:
pip install -u sentence-transformers

however, when i run the following code:
from sentence_transformers import sentencetransformer
import time

def ms_now():
    return int(time.time_ns() / 1000000)

class timer():
    def __init__(self):
        self.start = ms_now()
    
    def stop(self):
        return ms_now() - self.start

sentences = [""this is an example sentence each sentence is converted""] * 10

timer = timer()
model = sentencetransformer('sentence-transformers/all-minilm-l6-v2')
print(""model initialized"", timer.stop())
for _ in range(10):
    timer = timer()
    embeddings = model.encode(sentences)
    print(timer.stop())

i get the error:
traceback (most recent call last):
  file ""/home/drmeir/test/test.py"", line 17, in <module>
    model = sentencetransformer('sentence-transformers/all-minilm-l6-v2')
  file ""/home/drmeir/.local/lib/python3.9/site-packages/sentence_transformers/sentencetransformer.py"", line 95, in __init__
    modules = self._load_sbert_model(model_path)
  file ""/home/drmeir/.local/lib/python3.9/site-packages/sentence_transformers/sentencetransformer.py"", line 840, in _load_sbert_model
    module = module_class.load(os.path.join(model_path, module_config['path']))
  file ""/home/drmeir/.local/lib/python3.9/site-packages/sentence_transformers/models/transformer.py"", line 137, in load
    return transformer(model_name_or_path=input_path, **config)
  file ""/home/drmeir/.local/lib/python3.9/site-packages/sentence_transformers/models/transformer.py"", line 29, in __init__
    self._load_model(model_name_or_path, config, cache_dir)
  file ""/home/drmeir/.local/lib/python3.9/site-packages/sentence_transformers/models/transformer.py"", line 49, in _load_model
    self.auto_model = automodel.from_pretrained(model_name_or_path, config=config, cache_dir=cache_dir)
  file ""/home/drmeir/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py"", line 493, in from_pretrained
    return model_class.from_pretrained(
  file ""/home/drmeir/.local/lib/python3.9/site-packages/transformers/modeling_utils.py"", line 2903, in from_pretrained
    ) = cls._load_pretrained_model(
  file ""/home/drmeir/.local/lib/python3.9/site-packages/transformers/modeling_utils.py"", line 3061, in _load_pretrained_model
    id_tensor = id_tensor_storage(tensor) if tensor.device != torch.device(""meta"") else id(tensor)
runtimeerror: expected one of cpu, cuda, xpu, mkldnn, opengl, opencl, ideep, hip, msnpu, xla, vulkan device type at start of device string: meta

they have torch 1.8.1+cpu at pythonanywhere. on my laptop, it's 2.0.1.
what is the reason for the error and how can i get this to work?","['python', 'pytorch', 'huggingface-transformers', 'pythonanywhere', 'sentence-transformers']",76883714,"as mentioned in the comments, the meta device was added in the pytorch version 1.9. and the pythonanywhere comes with pytorch version 1.8.1.
downgrading transformers library to 4.6.0 which was released in may 12, 2021 (before torch 1.9 was released) solved this issue.",https://stackoverflow.com/questions/76883181,python,11-08-2023 11:34,792.0,2.0,1.0,True,11-08-2023 12:56,11-08-2023 12:56
71666450,how to get average pairwise cosine similarity per group in pandas,"i have a sample dataframe as below
df=pd.dataframe(np.array([['facebook', ""women tennis""], ['facebook', ""men basketball""], ['facebook', 'club'],['apple', ""vice president""], ['apple', 'swimming contest']]),columns=['firm','text'])


now i'd like to calculate the degree of text similarity within each firm using word embedding. for example, the average cosine similarity for facebook would be the cosine similarity between row 0, 1, and 2. the final dataframe should have a column ['mean_cos_between_items'] next to each row for each firm. the value will be the same for each company, since it is a within-firm pairwise comparison.
i wrote below code:
import gensim
from gensim import utils
from gensim.models import word2vec
from gensim.models import keyedvectors
from gensim.scripts.glove2word2vec import glove2word2vec
from sklearn.metrics.pairwise import cosine_similarity

 # map each word to vector space
    def represent(sentence):
        vectors = []
        for word in sentence:
            try:
                vector = model.wv[word]
                vectors.append(vector)
            except keyerror:
                pass
        return np.array(vectors).mean(axis=0)
    
    # get average if more than 1 word is included in the ""text"" column
    def document_vector(items):
        # remove out-of-vocabulary words
        doc = [word for word in items if word in model_glove.vocab]
        if doc:
            doc_vector = model_glove[doc]
            mean_vec=np.mean(doc_vector, axis=0)
        else:
            mean_vec = none
        return mean_vec
    
# get average pairwise cosine distance score 
def mean_cos_sim(grp):
   output = []
   for i,j in combinations(grp.index.tolist(),2 ): 
       doc_vec=document_vector(grp.iloc[i]['text'])
       if doc_vec is not none and len(doc_vec) > 0:      
           sim = cosine_similarity(document_vector(grp.iloc[i]['text']).reshape(1,-1),document_vector(grp.iloc[j]['text']).reshape(1,-1))
           output.append([i, j, sim])
       return np.mean(np.array(output), axis=0)

# save the result to a new column    
df['mean_cos_between_items']=df.groupby(['firm']).apply(mean_cos_sim)

however, i got below error:

could you kindly help? thanks!","['python', 'pandas', 'nlp']",71667247,"remove the .vocab here in model_glove.vocab, this is not supported in the current version of gensim any more: edit: also needs split() to iterate over words and not characters here.
# get average if more than 1 word is included in the ""text"" column
def document_vector(items):
    # remove out-of-vocabulary words
    doc = [word for word in items.split() if word in model_glove]
    if doc:
        doc_vector = model_glove[doc]
        mean_vec = np.mean(doc_vector, axis=0)
    else:
        mean_vec = none
    return mean_vec

here you iterate over tuples of indices when you want to iterate over the values, so drop the .index. also you put all values in output including the words (/indices) i and j, so if you want to get their average you would have to specify what exactly you want the average over. since you seem to not need i and j you can just put only the resulting sims in a list and then take the lists average:
# get pairwise cosine similarity score
def mean_cos_sim(grp):
    output = []
    for i, j in combinations(grp.tolist(), 2):
        if document_vector(i) is not none and len(document_vector(i)) > 0:
            sim = cosine_similarity(document_vector(i).reshape(1, -1), document_vector(j).reshape(1, -1))
            output.append(sim)
    return np.mean(output, axis=0)

here you try to add the results as a column but the number of rows is going to be different as the result dataframe only has one row per firm while the original dataframe has one per text. so you have to create a new dataframe (which you can optionally then merge/join with the original dataframe based on the firm column):
df = pd.dataframe(np.array(
    [['facebook', ""women tennis""], ['facebook', ""men basketball""], ['facebook', 'club'],
     ['apple', ""vice president""], ['apple', 'swimming contest']]), columns=['firm', 'text'])
df_grpd = df.groupby(['firm'])[""text""].apply(mean_cos_sim)

which overall will give you (edit: updated):
print(df_grpd)
> firm
  apple       [[0.53190523]]
  facebook    [[0.83989316]]
  name: text, dtype: object

edit:
i just noticed that the reason for the super high score is that this is missing a tokenization, see the changed part. without the split() this just compares character similarities which tend to be super high.",https://stackoverflow.com/questions/71666450,python,29-03-2022 17:33,1992.0,2.0,2.0,True,29-03-2022 20:51,29-03-2022 18:23
70976353,"after installing scrubadub_spacy package, spacy.load(&quot;en_core_web_sm&quot;) not working oserror: [e053] could not read config.cfg","i am getting the below error when i'm trying to run the following line of code to load en_core_web_sm in the azure machine learning instance.
i debugged the issue and found out that once i install scrubadub_spacy, that seems is the issue causing the error.
spacy.load(""en_core_web_sm"")

oserror                                   traceback (most recent call last)
<ipython-input-2-c6e652d70518> in <module>
     1 # load english tokenizer, tagger, parser and ner
----> 2 nlp = spacy.load(""en_core_web_sm"")

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/spacy/__init__.py in load(name, vocab, disable, exclude, config)
    50     """"""
    51     return util.load_model(
---> 52         name, vocab=vocab, disable=disable, exclude=exclude, config=config
    53     )
    54 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/spacy/util.py in load_model(name, vocab, disable, exclude, config)
   418             return get_lang_class(name.replace(""blank:"", """"))()
   419         if is_package(name):  # installed as package
--> 420             return load_model_from_package(name, **kwargs)  # type: ignore[arg-type]
   421         if path(name).exists():  # path to model data directory
   422             return load_model_from_path(path(name), **kwargs)  # type: ignore[arg-type]

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/spacy/util.py in load_model_from_package(name, vocab, disable, exclude, config)
   451     """"""
   452     cls = importlib.import_module(name)
--> 453     return cls.load(vocab=vocab, disable=disable, exclude=exclude, config=config)  # type: ignore[attr-defined]
   454 
   455 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/en_core_web_sm/__init__.py in load(**overrides)
    10 
    11 def load(**overrides):
---> 12     return load_model_from_init_py(__file__, **overrides)

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config)
   619         disable=disable,
   620         exclude=exclude,
--> 621         config=config,
   622     )
   623 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config)
   485     config_path = model_path / ""config.cfg""
   486     overrides = dict_to_dot(config)
--> 487     config = load_config(config_path, overrides=overrides)
   488     nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude)
   489     return nlp.from_disk(model_path, exclude=exclude, overrides=overrides)

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/spacy/util.py in load_config(path, overrides, interpolate)
   644     else:
   645         if not config_path or not config_path.exists() or not config_path.is_file():
--> 646             raise ioerror(errors.e053.format(path=config_path, name=""config.cfg""))
   647         return config.from_disk(
   648             config_path, overrides=overrides, interpolate=interpolate

oserror: [e053] could not read config.cfg from /anaconda/envs/azureml_py36/lib/python3.6/site-packages/en_core_web_sm/en_core_web_sm-2.3.1/config.cfg

i installed the packages using the below three lines codes from spacy
pip install -u pip setuptools wheel
pip install -u spacy
python -m spacy download en_core_web_sm

how should i fix this issue? thanks in advance.","['python', 'python-3.6', 'spacy', 'azure-machine-learning-service', 'oserror']",71004266,"taking the path from your error message:
en_core_web_sm-2.3.1/config.cfg

you have a model for v2.3, but it's looking for a config.cfg, which is only a thing in v3 of spacy. it looks like you upgraded spacy without realizing it.
there are two ways to fix this. one is to reinstall the model with spacy download, which will get a version that matches your current spacy version. if you are just starting something that is probably the best idea. based on the release date of scrubadub, it seems to be intended for use with spacy v3.
however, note that v2 and v3 are pretty different - if you have a project with v2 of spacy you might want to downgrade instead.",https://stackoverflow.com/questions/70976353,python,03-02-2022 18:19,508.0,2.0,1.0,True,06-02-2022 04:46,04-02-2022 18:12
78912227,"typeerror: expected a runnable, callable or dict.instead got an unsupported type: &lt;class &#39;list&#39;&gt;","i am experimenting with llm development.
here is my code:
import langchain, pydantic, transformers
from langchain import huggingfacepipeline
from langchain_core.prompts import prompttemplate
from langchain_core.runnables.base import runnablesequence, runnablemap, runnablelambda

from langchain.callbacks import get_openai_callback
from pydantic import basemodel, field
from langchain.output_parsers import pydanticoutputparser
from transformers import pipeline

class medicalspecialty(basemodel):
    medical_specialty: str = field(description=""medical specialty the patient should go to"")
    urgent: bool = field(description=""the patient should go to the hospital immediately"")

parser = pydanticoutputparser(pydantic_object=medicalspecialty)

queries = [""i have ache in my chest and in my left arm. which medical specialty should i go to?""]

template = """"""
question: {question}
""""""
prompt = prompttemplate(template=template, input_variables=[""question""])


llm = huggingfacepipeline.from_model_id(
    model_id=""bigscience/bloom-1b7"",
    task=""text-generation"",
    model_kwargs={""max_length"": 1024},
    device=-1  # ensure it runs on cpu for macos m1
)

# wrap the prompt in a runnablelambda to make it a runnable
prompt_runnable = runnablelambda(lambda x: prompt.format(**x))

# define the sequence that includes the prompt and llm
sequence = runnablesequence([
    prompt_runnable,
    llm
])

with get_openai_callback() as cb:
    for query in queries:
        result = sequence.invoke({""question"": query})
        print(query)
        print(result)
        print(""===================================="")
    
    # print the costs of the requests
    print(cb)

unfortunately, after several iterations, i keep getting this error:
typeerror                                 traceback (most recent call last)
cell in[6], line 19
     16 prompt_runnable = runnablelambda(lambda x: prompt.format(**x))
     18 # define the sequence that includes the prompt and llm
---> 19 sequence = runnablesequence([
     20     prompt_runnable,
     21     llm
     22 ])
     24 with get_openai_callback() as cb:
     25     for query in queries:

file /opt/anaconda3/envs/llm/lib/python3.11/site-    packages/langchain_core/runnables/base.py:2632, in runnablesequence.__init__(self,     name, first, middle, last, *steps)
   2630         steps_flat.extend(step.steps)
   2631     else:
-> 2632         steps_flat.append(coerce_to_runnable(step))
   2633 if len(steps_flat) < 2:
   2634     raise valueerror(
   2635         f""runnablesequence must have at least 2 steps, got     {len(steps_flat)}""
   2636     )

file /opt/anaconda3/envs/llm/lib/python3.11/site-    packages/langchain_core/runnables/base.py:5554, in coerce_to_runnable(thing)
   5552     return cast(runnable[input, output], runnableparallel(thing))
   5553 else:
-> 5554     raise typeerror(
   5555         f""expected a runnable, callable or dict.""
   5556         f""instead got an unsupported type: {type(thing)}""
   5557     )

typeerror: expected a runnable, callable or dict.instead got an unsupported type:     <class 'list'>

please, someone help!","['python', 'langchain', 'runnable', 'large-language-model', 'huggingface']",78913307,"no need to use lambda for this simple prompt with one input.
try to use below and report back here on what happens
prompt_runnable = promptrunnable(prompt)",https://stackoverflow.com/questions/78912227,python,25-08-2024 20:40,934.0,0.0,1.0,True,26-08-2024 07:42,25-08-2024 20:51
77042911,how to use multiple ner pipes with the same spacy nlp object?,"i trained a custom ner model for specific entity types (say, drugs) that are different from those that come out of the box in the standard spacy models (org, person, etc.). is it possible to add the ner pipe from this custom model to another model that already contains the standard spacy ner pipe? i tried the following:
import spacy

custom_nlp = spacy.load('my_trained_model/model-best/') #trained with the gpu option from the spacy quickstart page
doc = custom_nlp('chantix is a drug')
print(doc.ents) # prints chantix as a drug, as expected

main_nlp = spacy.load('en_core_web_trf')
doc = custom_nlp('chantix is a drug')
print(doc.ents) # prints chantix as a product, as expected

main_nlp.add_pipe('ner', source=custom_nlp, name='custom_ner', before='ner')
print(main_nlp.pipe_names) # both custom_ner and ner are there in the expected order
doc = main_nlp('chantix is a drug')
print(doc.ents) # here hell breaks loose, basically any token becomes an entity","['spacy', 'named-entity-recognition']",77081284,"the problem is that your added custom_ner is listening to the transformer component from en_core_web_trf rather than the one from the custom_nlp pipeline, so it's not getting the right input and is producing nonsense.
you need to ""replace the listeners"" before you add the component to en_core_web_trf:
custom_nlp.replace_listeners(""transformer"", ""ner"", [""model.tok2vec""])
main_nlp.add_pipe('ner', source=custom_nlp, name='custom_ner', before='ner')

docs:",https://stackoverflow.com/questions/77042911,spacy,05-09-2023 08:28,240.0,0.0,1.0,True,11-09-2023 11:36,06-09-2023 09:44
77759685,how to return source documents when using langchain expression language (lcel)?,"most samples of using langchain's expression language (lcel) look like this:
chain = setup_and_retrieval | prompt | model | output_parser

how can i access the source_documents in a rag application when using this expression language?","['artificial-intelligence', 'langchain', 'py-langchain', 'retrieval-augmented-generation']",77759686,"this works well for me:
rag_chain = (
    runnablepassthrough.assign(source_documents=condense_question | retriever)
    | runnablepassthrough.assign(context=lambda inputs: format_docs(inputs[""source_documents""]) if inputs[""source_documents""] else """")
    | runnablepassthrough.assign(prompt=qa_prompt)
    | runnablepassthrough.assign(response=lambda inputs: llm(inputs[""prompt""].messages))
)

it's called like this:
response_dict = rag_chain.invoke({""question"": question, ""chat_history"": chat_history})
ai_msg = response_dict[""response""]
source_documents = response_dict[""source_documents""]

the way that helped me understand how to do it was this:

you initially pass a dictionary into the chain (in my case with the keys question and chat_history).
every time you use runnablepassthrough.assign, you can add stuff to that dictionary and then pass that on to the next step.
runnablepassthrough.assign always returns a dictionary.

this is what happens in my code example:

we use runnablepassthrough.assign to add a new source_documents key to the dictionary. its value is the result of calling the condense_question function (defined elsewhere) that builds and returns a condenser chain. its condensed result is passed into our retriever (also defined elsewhere).
we use runnablepassthrough.assign to add a new context key to the dictionary. its value is the result of calling a format_docs method (defined elsewhere) that combines the source_documents into a single context string.
we use runnablepassthrough.assign to add a new prompt key to the dictionary. its value is the result of calling qa_prompt, which is defined as qa_prompt = chatprompttemplate.from_messages(...).
we use runnablepassthrough.assign one more time to add a new response key to the dictionary. its value is the result of actually calling the llm with the messages from our prompt.
the chain returns a dictionary with all the keys we've added along the way. the response key contains the llm's response as an aimessage, and the source_documents key contains the source documents.

i'm sure this can be done in a more concise way, but this worked for me and i can understand it :)",https://stackoverflow.com/questions/77759685,artificial-intelligence,04-01-2024 16:12,2654.0,3.0,2.0,True,03-06-2024 13:50,04-01-2024 20:33
69091576,string comparison with bert seems to ignore &quot;not&quot; in sentence,"i implemented a string comparison method using sentencetransformers and bert like following
from sentence_transformers import sentencetransformer
from sklearn.metrics.pairwise import cosine_similarity

model = sentencetransformer('sentence-transformers/all-distilroberta-v1')

sentences = [
    ""i'm a good person"",
    ""i'm not a good person""
]

sentence_embeddings = model.encode(sentences)

cosine_similarity(
    [sentence_embeddings[0]],
    sentence_embeddings[1:]
)

notice how my sentence examples are very similar but with the opposite meaning. the problem is the cosine similarity returns 0.9, indicating that these two strings are very similar in context when i expected it to return something closer to zero, as they have the opposite meanings.
how can i adapt my code to return a more accurate result?","['nlp', 'bert-language-model', 'transformer-model', 'sentence-similarity', 'sentence-transformers']",69260955,"tl;dr: nli is all you need
first, the cosine similarity is reasonably high, because the sentences are similar in the following sense:

they are about the same topic (evaluation of a person)
they are about the same subject (""i"") and the same property (""being a good person"")
they have similar syntactic structure
they have almost the same vocabulary

so, from the formal point of view, they should be considered similar. moreover, from the practical point of view, they should often be considered similar. for example, if you google ""gmo are causing cancer"", you might find that the text with label ""gmo are not causing cancer"" is relevant.
second, if you want to measure logical connection between sentences, cosine similarity of embeddings is just not expressive enough. this is because embeddings contain lots of semantic stylistic, lexical and syntactic information, but they are fixed-size (768-dimensional, in your case), so they cannot contain complete information about the meaning of both sentences. so you need another model with the following properties:

it encodes both texts simultaneously, so it compares the texts themselves, not just their fixed-size embeddings
it is explicitly trained to evaluate logical connection between sentences

the task of assesing logical connection between texts is called natural language inference (nli), and its most common formulation is recognizing textual entailment (rte): it is the problem of predicting whether the first sentence entails the second one.
there are lots of models trained for this task in the huggingface repo, with roberta-large-mnli being a good one. you can use it to evaluate equivalence of two texts. if each text entails another, they are equivalent, so you can estimate the degree of equivalence as the product of the entailment scores in both directions.
import torch
from transformers import autotokenizer, automodelforsequenceclassification

tokenizer = autotokenizer.from_pretrained(""roberta-large-mnli"")
model = automodelforsequenceclassification.from_pretrained(""roberta-large-mnli"")

def test_entailment(text1, text2):
    batch = tokenizer(text1, text2, return_tensors='pt').to(model.device)
    with torch.no_grad():
        proba = torch.softmax(model(**batch).logits, -1)
    return proba.cpu().numpy()[0, model.config.label2id['entailment']]

def test_equivalence(text1, text2):
    return test_entailment(text1, text2) * test_entailment(text2, text1)

print(test_equivalence(""i'm a good person"", ""i'm not a good person""))  # 2.0751484e-07
print(test_equivalence(""i'm a good person"", ""you are a good person""))  # 0.49342492
print(test_equivalence(""i'm a good person"", ""i'm not a bad person""))   # 0.94236994",https://stackoverflow.com/questions/69091576,nlp,07-09-2021 16:18,1421.0,4.0,3.0,True,10-04-2023 17:25,18-09-2021 18:10
77592601,why am i getting a rate limit error when i use the openai api for my first time?,"import openai

openai.api_key = ""myapikey""

completion = openai.chatcompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""give me 3 ideas for apps i could build with openai apis ""}])
print(completion.choices[0].message.content)

this is my code that i am trying to run. but whenever i run it, i keep getting this error message:
traceback (most recent call last):
  file ""/users/easwar/downloads/chatgpt api/01 chatgpt simple"", line 5, in <module>
    completion = openai.chatcompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""give me 3 ideas for apps i could build with openai apis ""}])
  file ""/users/easwar/library/python/3.9/lib/python/site-packages/openai/api_resources/chat_completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  file ""/users/easwar/library/python/3.9/lib/python/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  file ""/users/easwar/library/python/3.9/lib/python/site-packages/openai/api_requestor.py"", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  file ""/users/easwar/library/python/3.9/lib/python/site-packages/openai/api_requestor.py"", line 700, in _interpret_response
    self._interpret_response_line(
  file ""/users/easwar/library/python/3.9/lib/python/site-packages/openai/api_requestor.py"", line 765, in _interpret_response_line
    raise self.handle_error_response(
openai.error.ratelimiterror: you exceeded your current quota, please check your plan and billing details.

the main error is at the bottom.
usage limit
this is what my usage limit looks like on the openai website. i don't understand why most of it is expired because this is my first time using the api.
i would be really thankful to anyone who can help me fix this.","['openai-api', 'chatgpt-api']",77592657,"i had this same issue during my first time.
the problem is not from you code.
you have just used up you api key quota.
and the only way that you can fix it is by creating a very new openai account, and then requesting for the key on that one.
it will definitely work.
be careful not to use same details as this other accounts.",https://stackoverflow.com/questions/77592601,openai-api,02-12-2023 23:51,2749.0,1.0,2.0,True,25-04-2024 21:10,02-12-2023 23:51
70404372,attributeerror: &#39;field&#39; object has no attribute &#39;vocab&#39; preventing me to run the code,"i have found this code and i wanna see what is the object that im printing in the last line. im new in field of nlp so please help me fix this code, because it gives attributeerror: 'field' object has no attribute 'vocab'error. by the way i have found out that torchtext has been changed and the error is probably related to these changes, and the code probably was working before.
import spacy
from torchtext.legacy.data import field
spacy_eng = spacy.load(""en"")
def tokenize_eng(text):
    return [tok.text for tok in spacy_eng.tokenizer(text)]

english = field(
    tokenize=tokenize_eng, lower=true, init_token=""<sos>"", eos_token=""<eos>""
)
print([english.vocab.stoi[""<sos>""]])","['nlp', 'torchtext']",70412909,"you have to build the vocabulary for the english field before you try to access it. you will need a dataset to build the vocabulary, which will be the dataset you are looking to build a model for. you can use english.build_vocab(...). here are the docs for build_vocab.
also, if you would like to learn how to migrate what you are doing to the new version of torchtext, here is a good resource.",https://stackoverflow.com/questions/70404372,nlp,18-12-2021 14:47,1410.0,0.0,1.0,True,19-12-2021 15:43,18-12-2021 14:57
74107063,openai fine tuning each of the classes must start with a different token error,"i am trying to run a fine tune similar to the one in openai cookbook example for a multiclass classification problem. after preparing the train and valid jsonl files with fine_tunes.prepare_data, when i try to run the recommended fine_tunes.create command, i'm getting the following error:

if compute_classification_metrics is true, each of the classes must start with a different token. you can view your class tokenizations at  fine-tune failed. for help, please contact openai and include your fine-tune id.",['openai-api'],74107064,"looks like this error comes when the completion value is of more than a single token. after changing the completion values to numerical ids to ensure that they are of single tokens, the fine tune ran fine.
i'm not sure why the prepare_data step itself didn't say any error regarding this given that i used openai command line tool to prepare that.",https://stackoverflow.com/questions/74107063,openai-api,18-10-2022 07:14,1680.0,1.0,1.0,True,09-12-2022 23:54,09-12-2022 23:54
76448287,how can i solve importerror: using the `trainer` with `pytorch` requires `accelerate&gt;=0.20.1` when using huggingface&#39;s trainarguments?,"i'm using the transformers library in google colab, and
when i am using trainingarguments from transformers library i'm getting import error with this  code:
from transformers import trainingarguments

training_args = trainingarguments(
    output_dir = ""/content/our-model"",
    learning_rate=2e-5,
    per_device_train_batch_size= 64,
    per_device_eval_batch_size = 16,
    num_train_epochs = 2,
    weight_decay = 0.01,
    evaluation_strategy = ""epoch"",
    save_strategy = ""epoch"",
    load_best_model_at_end = true,
    push_to_hub = false
)

this is the error i'm getting:
<ipython-input-28-0518ea5ff407> in <cell line: 2>()
      1 from transformers import trainingarguments
----> 2 training_args = trainingarguments(
      3     output_dir = ""/content/our-model"",
      4     learning_rate=2e-5,
      5     per_device_train_batch_size= 64,

4 frames
/usr/local/lib/python3.10/dist-packages/transformers/training_args.py in _setup_devices(self)
   1670         if not is_sagemaker_mp_enabled():
   1671             if not is_accelerate_available(min_version=""0.20.1""):
-> 1672                 raise importerror(
   1673                     ""using the `trainer` with `pytorch` requires `accelerate>=0.20.1`: please run `pip install transformers[torch]` or `pip install accelerate -u`""
   1674                 )

importerror: using the `trainer` with `pytorch` requires `accelerate>=0.20.1`: please run `pip install transformers[torch]` or `pip install accelerate -u 

i already tried pip install for 0.20.1 version of accelerate and pip install transformers[torch]
and both didn't worked.","['python', 'nlp', 'importerror', 'huggingface-transformers', 'huggingface']",76452964,"if you're not particular about which transformers and accelerate version to tie to, then do this to use the most up-to-date version in google colab:
! pip install -u accelerate
! pip install -u transformers

then the issue you are having with accelerate should auto-resolve itself.
note:

underspecifying pip install -u transformers instead of pip install transformers[pytorch] might be easier since that's what most of the users do and the developers of the library will make sure that the basic pip works with the common functions and class like trainingarguments

instead of specifying accelerate to the pip install accelerate>=0.20.1, if you have no particular need to fixed the version, automatically upgrading to the latest version might get you more stability when using the library, esp. with ""hot""/""trending"" libraries that are constantly changing (almost) daily.



if further debugging is necessary, i.e. if the above didn't work. to check your transformers and accelerate version, do this:
import accelerate

accelerate.__version__

most probably you might have an importerror at the first line if accelerate is not already installed when you installed transformers.
and then if the first line works and the 2nd line is not outputting a version >=0.20.1, then that is the cause of your issue.
the current versions to-date (july 2023) are:
import accelerate
import transformers

transformers.__version__, accelerate.__version__

[out]:
('4.30.1', '0.21.0')

here's an example notebook with the model that you wish to use as per the comments in your question, 

if the error persist after the pip install ..., try restarting the runtime.
if you can't find the buttons to press to restart, try this in the cell restart kernel in google colab then re-run the cells for import ...
import os
os._exit(00)",https://stackoverflow.com/questions/76448287,python,10-06-2023 21:51,42296.0,27.0,4.0,True,12-06-2024 05:57,12-06-2023 00:43
77679383,validationerror: 1 validation error for structuredtool,"i was getting an error when trying to use a pydantic schema as an args_schema parameter value on a @tool decorator, following the deeplearning.ai course.
my code was:
from pydantic import basemodel, field

class searchinput(basemodel):
    query: str = field(description=""thing to search for"")

@tool(args_schema=searchinput)
def search(query: str) -> str:
    """"""searches for weather online""""""
    return ""21c""

and was getting this error:
validationerror: 1 validation error for structuredtool
args_schema subclass of basemodel expected (type=type_error.subclass; expected_class=basemodel)","['python', 'openai-api', 'pydantic', 'langchain']",77679407,"downgrading to pydantic 1.10.10 worked for me.
add pydantic==1.10.10 to your requirements.txt and install it with pip install -r requirements.txt
or with the command pip install pydantic==1.10.10",https://stackoverflow.com/questions/77679383,python,18-12-2023 13:23,3616.0,3.0,2.0,True,21-12-2023 06:03,18-12-2023 13:28
71113891,spacy tokenization add extra white space for dates with hyphen separator when i manually build the doc,"i've been trying to solve a problem with the spacy tokenizer for a while, without any success. also, i'm not sure if it's a problem with the tokenizer or some other part of the pipeline.
description
i have an application that for reasons besides the point, creates a spacy doc from the spacy vocab and the list of tokens from a string (see code below). note that while this is not the simplest and most common way to do this, according to spacy doc this can be done.
however, when i create a doc for a text that contains compound words or dates with hyphen as a separator, the behavior i am getting is not what i expected.
import spacy
from spacy.language import doc

# my current way
doc = doc(nlp.vocab, words=tokens)  # tokens is a well defined list of tokens for a certein string

# standard way
doc = nlp(""my text..."")

for example, with the following text, if i create the doc using the standard procedure, the spacy tokenizer recognizes the ""-"" as tokens but the doc text is the same as the input text, in addition the spacy ner model correctly recognizes the date entity.
import spacy

doc = nlp(""what time will sunset be on 2022-12-24?"")
print(doc.text)

tokens = [str(token) for token in doc]
print(tokens)

# show entities
print(doc.ents[0].label_)
print(doc.ents[0].text)

output:
what time will sunset be on 2022-12-24?
['what', 'time', 'will', 'sunset', 'be', 'on', '2022', '-', '12', '-', '24', '?']

date
2022-12-24

on the other hand, if i create the doc from the model's vocab and the previously calculated tokens, the result obtained is different. note that for the sake of simplicity i am using the tokens from doc, so i'm sure there are no differences in tokens. also note that i am manually running each pipeline model in the correct order with the doc, so at the end of this process i would theoretically get the same results.
however, as you can see in the output below, while the doc's tokens are the same, the doc's text is different, there were blank spaces between the digits and the date separators.
doc2 = doc(nlp.vocab, words=tokens)

# run each model in pipeline
for model_name in nlp.pipe_names:
    pipe = nlp.get_pipe(model_name)
    doc2 = pipe(doc2)

# print text and tokens
print(doc2.text)
tokens = [str(token) for token in doc2]
print(tokens)

# show entities
print(doc.ents[0].label_)
print(doc.ents[0].text)

output:
what time will sunset be on 2022 - 12 - 24 ? 
['what', 'time', 'will', 'sunset', 'be', 'on', '2022', '-', '12', '-', '24', '?']

date
2022 - 12 - 24

i know it must be something silly that i'm missing but i don't realize it.
could someone please explain to me what i'm doing wrong and point me in the right direction?
thanks a lot in advance!
edit
following the talha tayyab suggestion, i have to create an array of booleans with the same length that my list of tokens to indicate for each one, if the token is followed by an empty space. then pass this array in doc construction as follows: doc = doc(nlp.vocab, words=words, spaces=spaces).
to compute this list of boolean values ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½based on my original text string and list of tokens, i imp following vanilla function:
def get_spaces(self, text: str, tokens: list[str]) -> list[bool]:
     
    # spaces
    spaces = []
    # copy text to easy operate
    t = text.lower()

    # iterate over tokens
    for token in tokens:

        if t.startswith(token.lower()):

            t = t[len(token):]  # remove token

            # if after removing token we have an empty space
            if len(t) > 0 and t[0] == "" "":
                spaces.append(true)
                t = t[1:]  # remove space
            else:
                spaces.append(false)

    return spaces


with these two improvements in my code, the result obtained is as expected. however, now i have the following question:
is there a more spacy-like way to compute whitespace, instead of using my vanilla implementation?","['python', 'python-3.x', 'nlp', 'tokenize', 'spacy-3']",71118272,"please try this:
from spacy.language import doc
doc2 = doc(nlp.vocab, words=tokens,spaces=[1,1,1,1,1,1,0,0,0,0,0,0])
# run each model in pipeline
for model_name in nlp.pipe_names:
    pipe = nlp.get_pipe(model_name)
    doc2 = pipe(doc2)

# print text and tokens
print(doc2.text)
tokens = [str(token) for token in doc2]
print(tokens)

# show entities
print(doc.ents[0].label_)
print(doc.ents[0].text)

# you can also replace 0 with false and 1 with true

this is the complete syntax:
doc = doc(nlp.vocab, words=words, spaces=spaces)

spaces are a list of boolean values indicating whether each word has a subsequent space. must have the same length as words, if specified. defaults to a sequence of true.
so you can choose which ones you gonna have space and which ones you do not need.
reference:",https://stackoverflow.com/questions/71113891,python,14-02-2022 15:00,1558.0,3.0,2.0,True,18-01-2023 11:25,18-01-2023 11:25
34721984,stopword removing when using the word2vec,"i have been trying word2vec for a while now using the gensim's word2vec library. my question is do i have to remove stopwords from my input text?  because, based on my initial experimental results, i could see words like 'of', 'when'.. (stopwords) popping up when i do a model.most_similar('someword')..?
but i didn't see anywhere referring that stop word removal is necessary with word2vec? does the word2vec is supposed to handle stop words even if you don't remove them?
what are the must do pre processing things (like for topic modeling, it's almost a must that you should do stopword removal)?","['nlp', 'gensim', 'word2vec']",34737150,"personaly i think, removal of stop word will give better results, check link
also for topic modeling, you shlould perform preprocessing on the text, following things you must do,

remove of stop words.
tokenization.
stemming and lemmatization.",https://stackoverflow.com/questions/34721984,nlp,11-01-2016 12:49,21661.0,27.0,3.0,True,03-09-2022 11:51,12-01-2016 06:25
73698110,keras model fit throws shape mismatch error,"i am building a siamese network using keras(tensorflow) where the target is a binary column, i.e., match or mismatch(1 or 0). but the model fit method throws an error saying that the y_pred is not compatible with the y_true shape. i am using the binary_crossentropy loss function.
here is the error i see:

here is the code i am using:
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[tf.keras.metrics.recall()])
history = model.fit([x_train_entity_1.todense(),x_train_entity_2.todense()],np.array(y_train),
                    epochs=2, 
                    batch_size=32,
                    verbose=2,
                    shuffle=true)

my input data shapes are as follows:
inputs:
x_train_entity_1.shape is (700,2822)
x_train_entity_2.shape is (700,2822)

target:
y_train.shape is (700,1)

in the error it throws, y_pred is the variable which was created internally. what is y_pred dimension is 2822 when i am having a binary target. and 2822 dimension actually matches the input size, but how do i understand this?
here is the model i created:
in_layers = []
out_layers = []
for i in range(2):
  input_layer = input(shape=(1,))
  embedding_layer = embedding(embed_input_size+1, embed_output_size)(input_layer)
  lstm_layer_1 = bidirectional(lstm(1024, return_sequences=true,recurrent_dropout=0.2, dropout=0.2))(embedding_layer)
  lstm_layer_2 = bidirectional(lstm(512, return_sequences=true,recurrent_dropout=0.2, dropout=0.2))(lstm_layer_1)

  in_layers.append(input_layer)
  out_layers.append(lstm_layer_2)

merge = concatenate(out_layers)
dense1 = dense(256, activation='relu', kernel_initializer='he_normal', name='data_embed')(merge)
drp1 = dropout(0.4)(dense1)
btch_norm1 = batchnormalization()(drp1)
dense2 = dense(32, activation='relu', kernel_initializer='he_normal')(btch_norm1)
drp2 = dropout(0.4)(dense2)
btch_norm2 = batchnormalization()(drp2)
output = dense(1, activation='sigmoid')(btch_norm2)
model = model(inputs=in_layers, outputs=output)
model.summary()

since my data is very sparse, i used todense. and there the type is as follows:
type(x_train_entity_1) is scipy.sparse.csr.csr_matrix
type(x_train_entity_1.todense()) is numpy.matrix
type(x_train_entity_2) is scipy.sparse.csr.csr_matrix
type(x_train_entity_2.todense()) is numpy.matrix

summary of last few layers as follows:","['tensorflow', 'keras', 'deep-learning', 'neural-network', 'nlp']",73698693,"mismatched shape in the input layer. the input shape needs to match the shape of a single element passed as x, or dataset.shape[1:]. so since your dataset size is (700,2822), that is 700 samples of size 2822. so your input shape should be 2822.
change:
input_layer = input(shape=(1,))

to:
input_layer = input(shape=(2822,))",https://stackoverflow.com/questions/73698110,tensorflow,13-09-2022 05:59,908.0,0.0,2.0,True,13-09-2022 07:30,13-09-2022 07:24
67811619,"this method is deprecated, __call__ should be used instead, how to solve this problem in bert?","i am trying to use the batch_encode_plus() function from bert. the problem comes when it says that this function does not exist. i go to the documentation in this page and it says the following: ""this method is deprecated, __call__ should be used instead."". i tried to use __call__ as a function but it does not work.
then i get into this page and it seems that the function batch_encode_plus() was replaced by __call__. but when i use the function is not working. i tried to use encoded_plus() but it does not give the the expected results.
i do not know how to use this __call__ function, any suggestions?","['python', 'huggingface-transformers', 'huggingface-tokenizers']",76509315,"i've been working with this recently. here is a good look at some documentation for the call function
call( inputs*args**kwargs ) ï¿½ï¿½ï¿½ a list or a list of list of dict
parameters:

args (str or list[str]) ï¿½ï¿½ï¿½ one or several texts (or one list of prompts) with masked tokens.
targets (str or list[str], optional) ï¿½ï¿½ï¿½ when passed, the model will limit the scores to the passed targets instead of looking up in the whole vocab. if the provided targets are not in the model vocab, they will be tokenized and the first resulting token will be used (with a warning, and that might be slower).
top_k (int, optional) ï¿½ï¿½ï¿½ when passed, overrides the number of predictions to return.

returns: a list or a list of list of dict
here's an example of how i used it. (keep in mind that i'm using the fill-mask mode of bert which is a one word prediction model and the python language)
    from transformers imter = pipeline('fill-mask', model='bert-base-uncased')
    print(predicter.__call__(""example string. hello [mask]"",top_k=1)

the model will predict the word that is tokenized by [mask]
the example output for this is:
    {'score': 0.9415972232818604, 'token': 1037, 'token_str': 'kitty', 'sequence': 'example string. hello kitty'}",https://stackoverflow.com/questions/67811619,python,02-06-2021 19:47,688.0,1.0,1.0,True,22-06-2023 16:29,03-06-2021 21:42
78270035,pytorch cuda allocated memory is going into 100&#39;s of gb,"i am trying to get inference from huggingface transformer model running using pytorch framework. i have a gpu instance running and when i am checking the cuda memory summary, i find that allocated memory (total allocation) is increasing by 100's of gb's with each inference e.g. after 2nd inference allocated memory (total allocation)  was 19gb, with 3rd inference allocated memory (total allocation)  was 205gb. this total allocation is freed up. the memory maps don't show any anomalous pattern. current usage and peak usage from nearly constant. my inference sagemaker instance has 128gb of cpu memory only and 24 gb of gpu memory.

so, i have three queries/concerns:

how is it possible that total allocation memory is more than sagemaker instance on which inference is running.
how do i control this anomalous behaviour?
is this a concern that i need to rectify, as the inference seems to be running fine.","['memory-management', 'pytorch', 'huggingface-transformers', 'large-language-model']",78270162,"tot alloc and tot freed show the total amount of memory allocated or freed over the memory snapshot. they are accumulated stats.
cur usage shows how much memory is currently being used by your process. peak usage shows the highest amount of memory used at a single time.
peak usage is the main value you should be concerned about - you will get a cuda memory error if this value tries to exceed your gpu's memory.
additionally, the cuda memory profiler profiles cuda memory, not system memory. the values shown have nothing to do with system/cpu memory.",https://stackoverflow.com/questions/78270035,memory-management,03-04-2024 19:22,121.0,0.0,1.0,True,03-04-2024 21:37,03-04-2024 21:37
74160976,an error in implementing regex function on a list,"i was trying to implement a regex on a list of grammar tags in python, for finding the tense form of the list of grammar. and i wrote the following code to implement it.
data preprocessing:
from nltk import word_tokenize, pos_tag
import nltk

text = ""he will have been doing his homework."" 

tokenized = word_tokenize(text)
tagged = pos_tag(tokenized)
tags = []
for i in range(len(tagged)):
    t = tagged[i]
    tags.append(t[1])
print(tags)

regex formula i.e. to be implemented
grammar = r""""""
future_perfect_continuous: {<md><vb><vbn><vbg>}
future_continuous:         {<md><vb><vbg>}
future_perfect:            {<md><vb><vbn>}
past_perfect_continuous:   {<vbd><vbn><vbg>}
present_perfect_continuous:{<vbp|vbz><vbn><vbg>}
future_indefinite:         {<md><vb>}
past_continuous:           {<vbd><vbg>}
past_perfect:              {<vbd><vbn>}
present_continuous:        {<vbz|vbp><vbg>}
present_perfect:           {<vbz|vbp><vbn>}
past_indefinite:           {<vbd>}
present_indefinite:        {<vbz>|<vbp>}

function to implement the regex on the list tags
def check_grammar(grammar, tags):
    cp = nltk.regexpparser(grammar)
    result = cp.parse(tags)
    print(result)
    result.draw()
 
check_grammar(grammar, tags)

but it returned an error as:
traceback (most recent call last):
  file ""/home/samar/desktop/twitter_tense/main.py"", line 35, in <module>
    check_grammar(grammar, tags)
  file ""/home/samar/desktop/twitter_tense/main.py"", line 31, in check_grammar
    result = cp.parse(tags)
  file ""/home/samar/.local/lib/python3.8/site-packages/nltk/chunk/regexp.py"", line 1276, in parse
    chunk_struct = parser.parse(chunk_struct, trace=trace)
  file ""/home/samar/.local/lib/python3.8/site-packages/nltk/chunk/regexp.py"", line 1083, in parse
    chunkstr = chunkstring(chunk_struct)
  file ""/home/samar/.local/lib/python3.8/site-packages/nltk/chunk/regexp.py"", line 95, in __init__
    tags = [self._tag(tok) for tok in self._pieces]
  file ""/home/samar/.local/lib/python3.8/site-packages/nltk/chunk/regexp.py"", line 95, in <listcomp>
    tags = [self._tag(tok) for tok in self._pieces]
  file ""/home/samar/.local/lib/python3.8/site-packages/nltk/chunk/regexp.py"", line 105, in _tag
    raise valueerror(""chunk structures must contain tagged "" ""tokens or trees"")
valueerror: chunk structures must contain tagged tokens or trees","['python', 'nlp', 'nltk', 'nlp-question-answering']",74161157,"your call to the cp.parse() function expects each of the tokens in your sentence to be tagged, however, the tags list you created only contains the tags but not the tokens as well, hence your valueerror. the solution is to instead pass the output from the pos_tag() call (i.e. tagged) to your check_grammar call (see below).
solution
from nltk import word_tokenize, pos_tag
import nltk

text = ""he will have been doing his homework."" 
tokenized = word_tokenize(text)
tagged = pos_tag(tokenized)
print(tagged)
# output
>>> [('he', 'prp'), ('will', 'md'), ('have', 'vb'), ('been', 'vbn'), ('doing', 'vbg'), ('his', 'prp$'), ('homework', 'nn'), ('.', '.')]

my_grammar = r""""""
future_perfect_continuous: {<md><vb><vbn><vbg>}
future_continuous:         {<md><vb><vbg>}
future_perfect:            {<md><vb><vbn>}
past_perfect_continuous:   {<vbd><vbn><vbg>}
present_perfect_continuous:{<vbp|vbz><vbn><vbg>}
future_indefinite:         {<md><vb>}
past_continuous:           {<vbd><vbg>}
past_perfect:              {<vbd><vbn>}
present_continuous:        {<vbz|vbp><vbg>}
present_perfect:           {<vbz|vbp><vbn>}
past_indefinite:           {<vbd>}
present_indefinite:        {<vbz>|<vbp>}""""""


def check_grammar(grammar, tags):
    cp = nltk.regexpparser(grammar)
    result = cp.parse(tags)
    print(result)
    result.draw()


check_grammar(my_grammar, tagged)

output
>>> (s
>>>   he/prp
>>>   (future_perfect_continuous will/md have/vb been/vbn doing/vbg)
>>>   his/prp$
>>>   homework/nn
>>>   ./.)",https://stackoverflow.com/questions/74160976,python,22-10-2022 04:00,58.0,1.0,1.0,True,22-10-2022 05:01,22-10-2022 05:01
76873337,text mining in r: delete first sentence of each document,"i have several documents and do not need the first sentence of each document.
i could not find a solution so far.
here is an example. the structure of the data looks like this




case_number
text




1
today is a good day. it is sunny.


2
today is a bad day. it is rainy.




so the results should look like this




case_number
text




1
it is sunny.


2
it is rainy.




here is the example dataset:
case_number <- c(1, 2)

text <- c(""today is a good day. it is sunny."",
          ""today is a bad day. it is rainy."")

data <- data.frame(case_number, text)","['r', 'text-mining']",76874280,"if there's a chance that sentences might include some punctuation (e.g. abbreviations or numerics), and you are using some text mining library anyway, it makes perfect sense to let it handle tokenization.
with {tidytext} :
library(dplyr)
library(tidytext)

# exmple with punctuation in 1st sentence
data <- data.frame(case_number = c(1, 2),
                   text = c(""today is a good day, above avg. for sure, by 5.1 points. it is sunny."",
                            ""today is a bad day. it is rainy.""))
# tokenize to sentences, converting tokens to lowercase is optional
data %>% 
  unnest_sentences(s, text)
#>   case_number                                                        s
#> 1           1 today is a good day, above avg. for sure, by 5.1 points.
#> 2           1                                             it is sunny.
#> 3           2                                      today is a bad day.
#> 4           2                                             it is rainy.

# drop 1st record of every case_number group
data %>% 
  unnest_sentences(s, text) %>% 
  filter(row_number() > 1, .by = case_number)
#>   case_number            s
#> 1           1 it is sunny.
#> 2           2 it is rainy.

created on 2023-08-10 with reprex v2.0.2",https://stackoverflow.com/questions/76873337,r,10-08-2023 07:05,54.0,0.0,1.0,True,10-08-2023 09:14,10-08-2023 08:14
76711533,how to use the python openai client with both azure and openai at the same time?,"openai offers a python client, currently in version 0.27.8, which supports both azure and openai.
here are examples of how to use it to call the chatcompletion for each provider:
# openai_chatcompletion.py

""""""test openai's chatcompletion endpoint""""""
import os
import openai
import dotenv
dotenv.load_dotenv()
openai.api_key = os.environ.get('openai_api_key')

# hello, world.
api_response = openai.chatcompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
    {""role"": ""user"", ""content"": ""hello!""}
  ],
  max_tokens=16,
  temperature=0,
  top_p=1,
  frequency_penalty=0,
  presence_penalty=0,
)

print('api_response:', type(api_response), api_response)
print('api_response.choices[0].message:', type(api_response.choices[0].message), api_response.choices[0].message)

and:
# azure_openai_35turbo.py

""""""test microsoft azure's chatcompletion endpoint""""""
import os
import openai
import dotenv
dotenv.load_dotenv()

openai.api_type = ""azure""
openai.api_base = os.getenv(""azure_openai_endpoint"") 
openai.api_version = ""2023-05-15""
openai.api_key = os.getenv(""azure_openai_key"")


# hello, world.
# in addition to the `api_*` properties above, mind the difference in arguments
# as well between openai and azure:
# - openai from openai uses `model=""gpt-3.5-turbo""`!
# - openai from azure uses `engine=""ï¿½ï¿½ï¿½deployment nameï¿½ï¿½ï¿½""`! ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
#   > you need to set the engine variable to the deployment name you chose when
#   > you deployed the gpt-35-turbo or gpt-4 models.
#  this is the name of the deployment i created in the azure portal on the resource.
api_response = openai.chatcompletion.create(
  engine=""gpt-35-turbo"", # engine = ""deployment_name"".
  me"": ""user"", ""content"": ""hello!""}
  ],
  max_tokens=16,
  temperature=0,
  top_p=1,
  frequency_penalty=0,
  presence_penalty=0,
)

print('api_response:', type(api_response), api_response)
print('api_response.choices[0].message:', type(api_response.choices[0].message), api_response.choices[0].message)

i.e. api_type and other settings are globals of the python library.
here is a third example to transcribe audio (it uses whisper, which is available on openai but not on azure):
# openai_transcribe.py

""""""
test the transcription endpoint

""""""
import os
import openai
import dotenv
dotenv.load_dotenv()


openai.api_key = os.getenv(""openai_api_key"")
audio_file = open(""minitests/minitests_data/bilingual-english-bosnian.wav"", ""rb"")
transcript = openai.audio.transcribe(
    model=""whisper-1"",
    file=audio_file,
    prompt=""part of a bosnian language class."",
    response_format=""verbose_json"",
)
print(transcript)

these are minimal examples but i use similar code as part of my webapp (a flask app).
now my challenge is that i'd like to:

use the chatcompletion endpoint from azure; but:
use the transcribe endpoint from openai (since it's not available on azure)

is there any way to do so?
i have a few options in mind:

changing the globals before every call. but i'm worried that this might cause side-effects i did not expect.
duplicating/forking the library to have two versions run concurrently, one for each provider, but this also feels very messy.
use an alternative client for openai's whisper, if any.

i'm not too comfortable with these and feel i may have missed a more obvious solution.
or of courseï¿½ï¿½ï¿½ alternatively, i could just use whisper with a different provider (e.g. replicate) or an alternative to whisper altogether.
see also

someone reported the issue (but without a solution) on github (openai/openai-python): using azure and openai at the same time #411","['python', 'azure', 'python-module', 'openai-api', 'openai-whisper']",76740496,"each api in the library accepts per-method overrides for the configuration options. if you want to access the azure api for chat completions, you can explicitly pass in your azure config. for the transcribe endpoint, you can explicitly pass the openai config. for example:
import os
import openai

api_response = openai.chatcompletion.create(
    api_base=os.getenv(""azure_openai_endpoint""),
    api_key=os.getenv(""azure_openai_key""),
    api_type=""azure"",
    api_version=""2023-05-15"",
    engine=""gpt-35-turbo"",
    messages=[
    {""role"": ""user"", ""content"": ""hello!""}
    ],
    max_tokens=16,
    temperature=0,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0,
)
print(api_response)



audio_file = open(""minitests/minitests_data/bilingual-english-bosnian.wav"", ""rb"")
transcript = openai.audio.transcribe(
    api_key=os.getenv(""openai_api_key""),
    model=""whisper-1"",
    file=audio_file,
    prompt=""part of a bosnian language class."",
    response_format=""verbose_json"",
)
print(transcript)",https://stackoverflow.com/questions/76711533,python,18-07-2023 09:52,10485.0,3.0,2.0,True,30-11-2023 13:55,18-07-2023 15:50
73586255,r text analysis: counting occurences of any combinations of words from two different keyword lists with a given distance of each other,"thanks for reading. for a reserach project, i'm doing some text analysis. we are analyzing large texts (company reports) and i'm looking to count keyword frequencies within that text.
however, i have two lists of keywords, and i dont want to count the number of occurances of these words, but the number of times any two words from these lists appear within a certain distance from each other in the main text.
text <- c(""the house is blue. the car is very big and red."")
words1 <- c(""car"", ""house"") 
words2 <- c(""blue"", ""red"") 

the desired functionality should, for example, return 1 for distance 3. (number of any combinations in given distance.)
i know about the text_count function from the stringb package and kwic from quantea. however, thats not really what im looking for.
thanks, any help is appreciated.","['r', 'nlp', 'corpus', 'quanteda']",73597973,"the quanteda package has the function fcm() that counts frequency of their co-occurrences.
require(quanteda)
txt <- c(""the house is blue. the car is very big and red."")
toks <- tokens(txt) %>% tokens_tolower()
fcm(toks, window = 3, tri = false)
#> feature co-occurrence matrix of: 10 by 10 features.
#>         features
#> features the house is blue . car very big and red
#>    the     1     2  4    2 4   2    2   2   2   2
#>    house   2     0  2    1 2   1    1   1   1   1
#>    is      4     2  1    2 4   2    2   2   2   2
#>    blue    2     1  2    0 2   1    1   1   1   1
#>    .       4     2  4    2 1   2    2   2   2   2
#>    car     2     1  2    1 2   0    1   1   1   1
#>    very    2     1  2    1 2   1    0   1   1   1
#>    big     2     1  2    1 2   1    1   0   1   1
#>    and     2     1  2    1 2   1    1   1   0   1
#>    red     2     1  2    1 2   1    1   1   1   0",https://stackoverflow.com/questions/73586255,r,02-09-2022 17:40,289.0,1.0,2.0,True,17-09-2022 09:38,17-09-2022 09:38
41162876,get weight matrices from gensim word2vec,"i am using gensim word2vec package in python.
i would like to retrieve the w and w' weight matrices that have been learn during the skip-gram learning.
it seems to me that model.syn0 gives me the first one but i am not sure how i can get the other one. any idea?
i would actually love to find any exhaustive documentation on models accessible attributes because the official one does not seem to be precise (for instance syn0 is not described as an attribute)","['python', 'machine-learning', 'nlp', 'word2vec', 'gensim']",47925136,"the model.wv.syn0 contains the input embedding matrix. output embedding is stored in model.syn1 when it's trained with hierarchical softmax (hs=1) or in model.syn1neg when it uses negative sampling (negative>0). that's it! when both hierarchical softmax and negative sampling are not enabled, word2vec uses a single weight matrix model.wv.syn0 for training.
see also a related discussion here.",https://stackoverflow.com/questions/41162876,python,15-12-2016 11:19,9105.0,16.0,2.0,True,27-10-2023 03:15,02-02-2021 07:18
75744031,why do we need to write a function to &quot;compute metrics&quot; with huggingface question answering trainer when evaluating squad?,"currently, i'm trying to build a extractive qa pipeline, following the huggingface course on the matter. there, they show how to create a compute_metrics() function to evaluate the model after training. however, i was wondering if there's a way to obtain those metrics on training, and pass the compute_metrics() function directly to the trainer. they are training using only the training loss, and i would like to have the evaluation f1 score on training.
but, as i see it, it might be a little bit tricky, because they need the original spans to calculate the squad metrics, but you don't get those original spans passed on your tokenized training dataset.
predicted_answer = {'id': '56be4db0acb8001400a502ec', 'prediction_text': 'denver broncos'}
theoretical_answer = {'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['denver broncos', 'denver broncos', 'denver broncos'], 'answer_start': [177, 177, 177]}}

metric.compute(predictions=predicted_answers, references=theoretical_answers)

that's why they make the whole compute_metrics() function, taking a few extra parameters than the prediction outputted in the evaluation loop, as they need to rebuild those spans.
q: how do i make the squad metric outputs f1 and accuracy scores from evaluate? how do i use the squad metric with the trainer object?","['python', 'machine-learning', 'nlp', 'huggingface-transformers', 'squad']",75751929,"the compute_metrics function can be passed into the trainer so that it validating on the metrics you need, e.g.
from transformers import trainer

trainer = trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
trainer.train()

i'm not sure if it works out of the box with the code to process the train_dataset and validation_dataset in the course code 
but this ones shows how the trainer + compute_metrics work 

before proceeding to read the rest of the answer, here's some disclaimers:

try to get through the full course chapter 1-9 and the compute_metrics and usage of evaluate.metric would make a sense why you can't plug in evaluate.metric directly to the trainer object. 

alternatively, walking through this book would help too: 



and now, here goes...
firstly, lets take a look at what the evaluate library is/does
from 
from evaluate import load

squad_metric = load(""squad"")

predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]
references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]

results = squad_metric.compute(predictions=predictions, references=references)

print(results)

[out]:
{'exact_match': 100.0, 'f1': 100.0}

next, we take a look at what the compute_metrics argument in the trainer expects
from line 600 
    metric = evaluate.load(""squad_v2"" if data_args.version_2_with_negative else ""squad"")

    def compute_metrics(p: evalprediction):
        return metric.compute(predictions=p.predictions, references=p.label_ids)

    # initialize our trainer
    trainer = questionansweringtrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset if training_args.do_train else none,
        eval_dataset=eval_dataset if training_args.do_eval else none,
        eval_examples=eval_examples if training_args.do_eval else none,
        tokenizer=tokenizer,
        data_collator=data_collator,
        post_process_function=post_processing_function,
        compute_metrics=compute_metrics,
    )


the compute_metrics argument in the questionansweringtrainer is expecting a function that:

[in]: takes in an evalprediction object as input
[out]: returns a dict of keys-value pairs where the key is the name of the output metric in string type and the value is expected to a floating point

un momento! (wait a minute!) what are these questionansweringtrainer and evalprediction objects?
q: why are you not using the normal trainer object?
a: the questionansweringtrainer is a specific sub-class of the trainer object that is used for the qa task. if you're going to train a model to evaluate on the squad dataset, then the questionansweringtrainer is the most appropriate trainer object to use.
[suggestion]: most probably huggingface devs and dev-advocate should add some notes on the object in questionansweringtrainer 
q: what is this evalprediction object then?
a: officially, i guess it's this: 
if we look at the doc:  and the code, it looks like the object is a custom container class that holds the (i) predictions, (ii) label_ids and (iii) inputs np.ndarray. these are what the model's  inference function need to return in order for the compute_metrics to work as expected.
class evalprediction:
    """"""
    evaluation output (always contains labels), to be used to compute metrics.
    parameters:
        predictions (`np.ndarray`): predictions of the model.
        label_ids (`np.ndarray`): targets to be matched.
        inputs (`np.ndarray`, *optional*)
    """"""

    def __init__(
        self,
        predictions: union[np.ndarray, tuple[np.ndarray]],
        label_ids: union[np.ndarray, tuple[np.ndarray]],
        inputs: optional[union[np.ndarray, tuple[np.ndarray]]] = none,
    ):
        self.predictions = predictions
        self.label_ids = label_ids
        self.inputs = inputs

    def __iter__(self):
        if self.inputs is not none:
            return iter((self.predictions, self.label_ids, self.inputs))
        else:
            return iter((self.predictions, self.label_ids))

    def __getitem__(self, idx):
        if idx == 0:
            return self.predictions
        elif idx == 1:
            return self.label_ids
        elif idx == 2:
            return self.inputs


hey, you still haven't answer the question of how i can use the evaluate.metrics('squad') directly to the the compute_metrics args!
yes, for now, you can't directly use it but it's a simple wrapper.
step 1. make sure the model you want to use outputs the required evalprediction object that contains, predictions and label_ids
if you're using most the models supported for qa in huggingface's transformers library, they should already output the expected evalprediction.
otherwise, take a look at models supported by 
step 2: since the model inference outputs evalprediction but the compute_metrics expects a dictionary outputs, _you have to wrap the evaluate.metrics function
e.g.
    metric = evaluate.load(""squad_v2"" if data_args.version_2_with_negative else ""squad"")

    def compute_metrics(p: evalprediction):
        return metric.compute(predictions=p.predictions, references=p.label_ids)

q: do we really always need to write that wrapper function?
a: for now, yes, it is by design not directly integrated with the outputs of the evaluate.metrics to give the different metrics' developers freedom to define how they want their inputs/outputs to look like.
but there might be hope to make compute_metrics more integrated with evaluate.metric if someone picks this feature request up!",https://stackoverflow.com/questions/75744031,python,15-03-2023 11:22,15598.0,2.0,1.0,True,18-03-2023 23:11,18-03-2023 22:21
78631323,is there a way to filter and exclude documents when doing similarity search in a vector db using langchain?,"so far my research only shows me how to filter to a specific a specific document or page but it doesn't show how to exclude some documents from the search.
results_with_scores = db.similarity_search_with_score(""foo"", filter=dict(page=1))","['langchain', 'vector-database', 'faiss', 'retrieval-augmented-generation', 'similarity-search']",78656891,"this depends on the underlying vector database being used. the arguments to filter will typically be passed to the vector database, and behavior will be implementation specific.
a common choice of vector database is chromadb, the filter arguments are passed to where, you can consult the filter documentation in the chromadb guide. see also inequality in the unofficial chromadb cookbook. you will want to use a $ne expression.
if you are using a different vector database you may need to consult to documentation for that database and even check the langchain code to see how things get passed through.",https://stackoverflow.com/questions/78631323,langchain,17-06-2024 07:24,1093.0,1.0,1.0,True,22-06-2024 18:36,17-06-2024 15:10
74500309,python regex to match a colon either side (left and right) of a word,"at a complete loss here - trying to match a a colon either side of any given word in a passage of text.
for example:
:wave: hello guys! :partyface: another huge win for us all to celebrate!

an appropriate regex that would match:
:wave:
:partyface:

really appreciate your help!
\w*:\b","['python', 'regex', 'nlp']",74500391,"to catch all the content
:[^:]*:

to catch the content between
(?<=:)[^:]*(?=:)",https://stackoverflow.com/questions/74500309,python,19-11-2022 13:11,393.0,1.0,1.0,True,19-11-2022 13:21,19-11-2022 13:13
78827482,can&#39;t suppress warning from transformers/src/transformers/modeling_utils.py,"my implementation for the automodel autotokenizer classes are fairly simple:
from transformers import automodel, autotokenizer
import numpy as np
from rank_bm25 import bm25okapi
from sklearn.neighbors import nearestneighbors

class embeddingmodels:

    def bert(self, model_name, text):
        tokenizer = autotokenizer.from_pretrained(model_name)
        model = automodel.from_pretrained(model_name)
        inputs = tokenizer(text, return_tensors=""pt"", truncation=true, padding=true)
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()
        return embeddings
    
    def create_chunks(self, text, chunk_size):
        return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

but i can't get this warning to go away:
a parameter name that contains 'beta' will be renamed internally to 'bias'. 
please use a different name to suppress this warning.
a parameter name that contains 'gamma' will be renamed internally to 'weight'. 
please use a different name to suppress this warning.

there is no reference to the word beta or gamma anywhere in my repo.
updating the package, suppressing the warnings with import warnings","['python', 'machine-learning', 'pytorch', 'huggingface-transformers', 'tokenize']",78844884,"before loading from pretrained model set transformers logger level to error as shown below. it sure is really frustrating not being able to leverage the warnings library filter
    loggers = [logging.getlogger(name) for name in logging.root.manager.loggerdict]
    for logger in loggers:
        if ""transformers"" in logger.name.lower():
            logger.setlevel(logging.error)

    # now you can load state dict from pretrained
    model = transformers.bertmodel.from_pretrained(
        ""bert-base-uncased"",
        use_safetensors=true,
        return_dict=false,
        attn_implementation=""sdpa"",
    )",https://stackoverflow.com/questions/78827482,python,02-08-2024 23:04,2052.0,1.0,1.0,True,07-08-2024 17:01,02-08-2024 23:25
77461542,entity extraction in rasa,"i want to create a chatbot that extract words from the user message as entity and send it to dictionary and in return get the meaning of that word.
but the problem is entity values are not getting extracted, and i am getting empty brackets [ ]. i am trying to solve this issue for weeks now. now, i am exhausted and desperate. please help me to figure this out.
here is all the files:

installation versions on my system are:
rasa version      :         3.6.13
minimum compatible version: 3.5.0
rasa sdk version  :         3.6.2
python version    :         3.10.0
operating system  :         windows-10-10.0.19045-sp0","['machine-learning', 'nlp', 'entity', 'rasa-nlu', 'rasa-sdk']",77463236,"you need to add the regexentityextractor to the pipeline in your config.yml.
config.yml:
pipeline:
- name: regexentityextractor
  case_sensitive: false
  use_lookup_tables: true
  use_regexes: true
  use_word_boundaries: true

additionally you will need to modify your nlu training data to match the correct format for extracting entities.
before:
clarify the term [sympathy] for me. (term)

after:
clarify the term [sympathy](term) for me. 

the rasa documentation goes into more detail about how to format nlu training data.",https://stackoverflow.com/questions/77461542,machine-learning,10-11-2023 16:35,423.0,0.0,1.0,True,11-11-2023 08:51,11-11-2023 08:51
79257787,how to get intermediary chain step outputs in final output?,"for simplicity sake i have the following chain:

extract names from a list
validate these names against a second list of names

what i want is to receive a json with all intermediary steps at the end as well as the input:
{""first_value"": ""dave, john, carrot"", ""first_prompt_output"" ""dave, john"", ""possible_values"": ""john""...}

but i am confused by the lc docs and i seem to be able to get all of the inputs using the runnable passthrough, but in a hard to read format. i fiddled with it for half an hour (tried runnableparallel, runnablepassthrough.assign(), ...) but i don't seem to be able to get it, there must be some key feature i'm missing.
{'result': {'third_prompt_output': 'johny'},
 'first_prompt_output': {'first_prompt_output': {'first_prompt_output': ['dave',
    'john']},
  'possible_values': {'first_value': 'dave, john',
   'possible_values': ['john']}}}

first_prompt = prompttemplate.from_template(""""""find all names in the following text and extract it as json with a field `first_prompt_output`: {first_value}""
                                                    first_prompt_output:"""""")

second_prompt = prompttemplate.from_template(""""""here is a list of possible values: {possible_values} and a list of found value {first_prompt_output}. find values that are in both lists. return a json with the fields `first_prompt_output` and `second_prompt_output` and `possible_values`."""""")

first_value = ""dave, john""
possible_values = [""john""]

first_chain = (
    first_prompt
    | llm
    | simplejsonoutputparser()
)

second_chain = (
    second_prompt
    | llm
    | simplejsonoutputparser()
)

chain = (
    {""first_prompt_output"": first_chain, ""possible_values"": runnablepassthrough(), ""first_value"": runnablepassthrough()} 
    | runnableparallel(result={""second_prompt_output"": second_chain, ""first_value"": itemgetter(""first_value"")})
)

chain.invoke({""first_value"": first_value, ""possible_values"": possible_values})

i tried using the runnableparallel or runnablepassthrough.assign, but neither does what i expect it to do. what i basically need is a dict.update() but in the pipeline.","['langchain', 'large-language-model', 'chain']",79264564,"i have not found a way to do this with langchain, but i found a function that allows me to flatten the output and results in what i want, although it seems a bit clunky and i believe there must be a better solution.
the key is to add the following function to the chain:
def flatten_dict(*vars) -> dict:
    '''
    flatten a dictionary by removing unnecessary mid-level keys.
    returns a runnable (chainable) function.
    '''
    flat = {}
    for var in vars:
        keys = [k for k in var]
        for key in keys:
            if isinstance(var[key], dict):
                flat.update(var[key])
            else:
                flat[key] = var[key]
    return flat

chain = (
    {""first_prompt_output"": first_chain, ""possible_values"": runnablepassthrough(), ""first_value"": runnablepassthrough()} 
    | runnableparallel(result={""second_prompt_output"": second_chain, ""first_value"": itemgetter(""first_value"")})
)
| flatten_dict",https://stackoverflow.com/questions/79257787,langchain,06-12-2024 11:15,79.0,1.0,1.0,True,09-12-2024 10:07,06-12-2024 11:18
75951190,sentence transformer use of evaluator,"i came across this script which is second link on this page  and this explanation
i am using all-mpnet-base-v2 (link) and i am using my custom data
i am having hard time understanding use of
evaluator = embeddingsimilarityevaluator.from_input_examples(
    dev_samples, name='sts-dev')

the documentation says:

evaluator ï¿½ï¿½ï¿½ an evaluator (sentence_transformers.evaluation) evaluates the model performance during training on held-out dev data. it is used to determine the best model that is saved to disc.

but in this case, as we are fine tuning on our own examples, train_dataloaderhas train_samples which has our model sentences and scores.
q1. how is train_samples different than dev_samples?
q2a: if the model is going to print performance against dev_samples then how is it going to help ""to determine the best model that is saved to disc""?
q2b: are we required to run dev_samples a the model saved on the disc and then compare scores?
q3. if my goal is to take a single model and then fine tune it, is it okay to skip parameters evaluator and evaluation_steps?
q4. how to determine total steps in the model? do i need to set evaluation_steps?

updated
i followed the answer provided by kyle and have below follow up questions
in the fit method i used the evaluator and below data was written to a file

q5. which metric is used to select the best epoch? is it cosine_pearson?
q6: why steps are -1 in the above output?
q7a: how to find steps based upon size of my data, batch size etc.
currently i have kept them to 1000. but not sure if that it is too much. i am running for 10 epochs, i have 2509 examples in the training data and batch size is 64.
q7b: are my steps going to be 2509/64? if yes then 1000 seems to be too high number","['python', 'nlp', 'sentence-transformers']",75981408,"question 1

how is train_samples different from dev_samples in the context of the embeddingsimilarityevaluator?

one needs to have a ""held-out"" split of data to be used for evaluation during training to avoid over-fitting. this ""held-out"" set is commonly referred to as the ""development set"" as it is the set of data that is used during development of the model/system. a pedagogical analogy can be drawn between a traditional education curriculum and that of training deep learning models: if one were to give students all the questions for a given topic, and then use the same subset of questions for evaluation, then eventually (most) students will learn to memorise the set of answers they repeatedly see while practicing, instead of learning the procedures to solve the questions in general. so if you are using your own custom data, make sure that a subset of that data is allocated to dev_samples in addition to train_samples and test_samples. alternatively, if your own data is scarce, you can use the original training data to supplement your own training, development and test sets. the ""test set"" is the one that is only used after training has completed to determine the final performance of the model (i.e. all samples in the test set (ideally) haven't been seen before).
question 2

how is the model going to determine the best model that is saved to disc? are we required to run dev_samples against the model saved on the disc and then compare scores?

the previous answer alludes to how this will work, but in brief, once the evaluator has been instantiated, it will measure the correlation against the gold labels and then return the similarity score (depending on what main_similarity was initially set). if the produced embeddings (based on the development set) offer a higher correlation with their gold labels, and therefore, a higher score overall, then this ""better"" model is saved to disk. hence, there is no need for you to ""run dev_samples against the model saved on the disc and then compare scores"", this process happens automatically provided everything has been set up appropriately.
question 3

if my goal is to take a single model and then fine tune it, is it okay to skip parameters evaluator and evaluation_steps?

based on the above answers, you can understand why you cannot ""skip the evaluator and evaluation_steps"". the evaluator is an integral part of ""fine-tuning"" (i.e. training) the model.
question 4

how to determine the total number of steps for the model? i need to set evaluation_steps.

the evaluation_steps parameter sets the number of training steps that must occur before the model is evaluated using the evaluator. if the authors have set this to 1000, then leave it as is unless you notice problems with training. alternatively, experiment with either increasing of decreasing it and select a value that works best for training.
follow-up questions
question 5

which metric is used to select the best epoch? is it cosine_pearson?

by default, the maximum of the cosine spearman, manhattan spearman, euclidean spearman and dot product spearman is used.
question 6

why are steps -1 in the output?

the -1 lets the user know that the evaluator was called after all training steps occurred for a particular epoch.
if the steps_per_epoch was not set when calling the model.fit(), it defaults to none which sets the number of steps_per_epoch to the size of the train_dataloader which is passed to train_objectives when model.fit() is initially called, i.e.:
model.fit(train_objectives=[(train_dataloader, train_loss)],
          ...)

in your case, train_samples is 2,509 and train_batch_size is 64, so the size of train_dataloader, and therefore steps_per_epoch, will be 39.
if the steps_per_epoch, is less than the evaluation_steps, then the number of training steps won't reach or exceed evaluation_steps and so additional calls to _eval_during_training on line 737 won't occur. this isn't a problem as the evaluation is forced to call at the end of each epoch anyway based on line 747.
question 7

how do i find the number of evaluation_steps based on the size of my training data (2,509 samples) and batch size (64)? is 1000 too high?

the evaluation_steps is available to tell the model during the training process whether it should prematurely run an evaluation using the evaluator part-way through an epoch. otherwise, the evaluation is forced to run at the end of the epoch after steps_per_epoch have completed.
based on the numbers you provided, you could, for example, set evaluation_steps to 20 to get an evaluation to run approx. half-way through an epoch (assuming an epoch is 39 training_steps). see this answer and its question for more info. on batch size vs. epochs vs. steps per epoch.",https://stackoverflow.com/questions/75951190,python,06-04-2023 15:22,2768.0,4.0,1.0,True,14-04-2023 14:56,14-04-2023 14:56
73315383,in spacy: add a span (doc[a:b]) as entity in a spacy doc (python),"i am using regex over a whole document to catch the spans in which such regex occurs:
import spacy
import re

nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""the united states of america (usa) are commonly known as the united states (u.s. or us) or america."")

expression = r""[uu](nited|\.?) ?[ss](tates|\.?)""
for match in re.finditer(expression, doc.text):
    start, end = match.span()
    span = doc.char_span(start, end)
    # this is a span object or none 
    # if match doesn't map to valid token sequence
    if span is not none:
        print(""found match:"", span.text)

there is a way to get the span (list of tokens) corresponding to the regex match on the doc even if the boundaries of the regex match do not correspond to token boundaries.
see:
how can i expand the match to a valid token sequence? in 
so far so good.
now that i have a collectuon of spans how do i convert them into entities?
i am aware of the entity ruler:
the entityruler is a pipeline component (see also the link above) but that entityruler takes patterns as inputs to search in the doc and not spans.
if i want to use regex over the whole document to get the collection os spans i want to convert into ents what is the next step here? entityruler? how? or something else?
put simpler:
nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""the aplicable law is article 102 section b sentence 6 that deals with robery"")

i would like to generate an spacy ent (entity) out of doc[5,10] with label ""law"" in order to be able to:
a) loop over all the law entities in the texts
b) use the visualizer to display the different entities contained in the doc","['python', 'nlp', 'spacy', 'named-entity-recognition']",73331138,"the most flexible way to add spans as entities to a doc is to use doc.set_ents:
from spacy.tokens import span

span = doc.char_span(start, end, label=""ent"")
doc.set_ents(entities=[span], default=""unmodified"")

use the default option to specify how to set all the other tokens in the doc. by default the other tokens are set to o, but you can use default=""unmodified"" to leave them untouched, e.g. if you're adding entities incrementally.",https://stackoverflow.com/questions/73315383,python,11-08-2022 04:45,1963.0,4.0,1.0,True,15-08-2022 06:22,15-08-2022 06:22
68738363,building own classifier based pos tagger using nltk&#39;s sklearnclassifier and classifierbasedpostagger,"i'm trying to build my own classifier based pos tagger using sklearnclassifier and classifierbasedpostagger. the code that i've tried is given below.
from nltk.corpus import treebank
nltk.download('treebank')

data = treebank.tagged_sents()
train_data = data[:3500]
test_data = data[3500:]

from nltk.classify import sklearnclassifier
from sklearn.naive_bayes import bernoullinb
from nltk.tag.sequential import classifierbasedpostagger

bnb = sklearnclassifier(bernoullinb())
bnb_tagger = classifierbasedpostagger(train=train_data,
                                      classifier_builder=bnb.train)

# evaluate tagger on test data and sample sentence
print(bnb_tagger.evaluate(test_data))

# see results on our previously defined sentence
print(bnb_tagger.tag(nltk.word_tokenize(sentence)))

this code is yielding the following error:
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
c:\users\abdull~1.imr\appdata\local\temp/ipykernel_6580/266992580.py in <module>
      4 
      5 bnb = sklearnclassifier(bernoullinb())
----> 6 bnb_tagger = classifierbasedpostagger(train=train_data,
      7                                       classifier_builder=bnb.train)
      8 

~\miniconda3\envs\nlp_course\lib\site-packages\nltk\tag\sequential.py in __init__(self, feature_detector, train, classifier_builder, classifier, backoff, cutoff_prob, verbose)
    637 
    638         if train:
--> 639             self._train(train, classifier_builder, verbose)
    640 
    641     def choose_tag(self, tokens, index, history):

~\miniconda3\envs\nlp_course\lib\site-packages\nltk\tag\sequential.py in _train(self, tagged_corpus, classifier_builder, verbose)
    673         if verbose:
    674             print(""training classifier ({} instances)"".format(len(classifier_corpus)))
--> 675         self._classifier = classifier_builder(classifier_corpus)
    676 
    677     def __repr__(self):

~\miniconda3\envs\nlp_course\lib\site-packages\nltk\classify\scikitlearn.py in train(self, labeled_featuresets)
    110 
    111         x, y = list(zip(*labeled_featuresets))
--> 112         x = self._vectorizer.fit_transform(x)
    113         y = self._encoder.fit_transform(y)
    114         self._clf.fit(x, y)

~\miniconda3\envs\nlp_course\lib\site-packages\sklearn\feature_extraction\_dict_vectorizer.py in fit_transform(self, x, y)
    288             feature vectors; always 2-d.
    289         
--> 290         return self._transform(x, fitting=true)
    291 
    292     def inverse_transform(self, x, dict_type=dict):

~\miniconda3\envs\nlp_course\lib\site-packages\sklearn\feature_extraction\_dict_vectorizer.py in _transform(self, x, fitting)
    233                     if feature_name in vocab:
    234                         indices.append(vocab[feature_name])
--> 235                         values.append(self.dtype(v))
    236 
    237             indptr.append(len(indices))

typeerror: float() argument must be a string or a number, not 'nonetype'

how to do it right?","['python', 'scikit-learn', 'nlp', 'nltk', 'pos-tagger']",68894824,"according to the comment from this issue, this is a consequence of a bug in scikit-learn. scikit-learn's _transform method of dictvectorizer in sklearn/feature_extraction/_dict_vectorizer.py fails when the input argument x contains mappings to none. according to tom aarsen, we can now use the following example to make the work done:
import nltk
from nltk.corpus import treebank

from nltk.classify import sklearnclassifier
from sklearn.naive_bayes import bernoullinb
from nltk.tag.sequential import classifierbasedpostagger

nltk.download('treebank')

data = treebank.tagged_sents()
train_data = data[:3]
test_data = data[3:]

class customclassifierbasedpostagger(classifierbasedpostagger):

    def feature_detector(self, tokens, index, history):
        return {
            key: str(value) # ensure that the feature value is a string. converts none to 'none'
            for key, value in super().feature_detector(tokens, index, history).items()
        }

bnb = sklearnclassifier(bernoullinb())
bnb_tagger = customclassifierbasedpostagger(train=train_data,
                                            classifier_builder=bnb.train,
                                            verbose=true)

sentence = ""this is a sample sentence which i just made for fun.""
# evaluate tagger on test data and sample sentence
print(bnb_tagger.evaluate(test_data))

# see results on our previously defined sentence
print(bnb_tagger.tag(nltk.word_tokenize(sentence)))

the output will be like:
[nltk_data] downloading package treebank to c:\users\tom/nltk_data...
[nltk_data]   package treebank is already up-to-date!
constructing training corpus for classifier.
training classifier (58 instances)
0.09338289371682999
[('this', 'nnp'), ('is', 'nnp'), ('a', 'nnp'), ('sample', 'nnp'), ('sentence', 'nnp'), ('which', 'nnp'), ('i', 'nnp'), ('just', 'nnp'), ('made', 'nnp'), ('for', 'nnp'), ('fun', 'nnp'), ('.', 'nnp')]",https://stackoverflow.com/questions/68738363,python,11-08-2021 08:17,263.0,0.0,1.0,True,23-08-2021 14:55,11-08-2021 08:25
66637485,spacy 3.0.1 accuracy prediction,"how to test accuracy of a spacy pretrained model in version 3.0.1. i want to see my output how accurate my tested model is predicted.this the code below for spacy version 2 but it doesn't work in spacy version 3.can somone tell me the code on spacy version 3.
 from spacy.gold import goldparse
 from spacy.scorer import scorer

def evaluate(nlp, examples, ent='person'):
scorer = scorer()
for input_, annot in examples:
    text_entities = []
    for entity in annot.get('entities'):
        if ent in entity:
            text_entities.append(entity)
    doc_gold_text = nlp.make_doc(input_)
    gold = goldparse(doc_gold_text, entities=text_entities)
    pred_value = nlp(input_)
    scorer.score(pred_value, gold)
return scorer.scores

examples = [
(""trump says he's answered mueller's russia inquiry questions \u2013 live"",{""entities"":[[0,5,""person""],[25,32,""person""],[35,41,""gpe""]]}),
(""alexander zverev reaches atp finals semis then reminds lendl who is boss"",{""entities"":[[0,16,""person""],[55,60,""person""]]}),
(""britain's worst landlord to take nine years to pay off string of fines"",{""entities"":[[0,7,""gpe""]]}),
(""tom watson: people's vote more likely given weakness of may's position"",{""entities"":[[0,10,""person""],[56,59,""person""]]}),
]

nlp = spacy.load('en_core_web_sm')
results = evaluate(nlp, examples)
print(results)","['python', 'nlp', 'spacy']",68151189,"personnally i had used this method, and i wish it will help you in your work:
in your case, i think:
from spacy.training import example

#get test data

test_data = [
    (""trump says he's answered mueller's russia inquiry questions \u2013 
    live"",{""entities"":[[0,5,""person""],[25,32,""person""],[35,41,""gpe""]]}),
    (""alexander zverev reaches atp finals semis then reminds lendl who is 
    boss"",{""entities"":[[0,16,""person""],[55,60,""person""]]}),
    (""britain's worst landlord to take nine years to pay off string of fines"", 
    {""entities"":[[0,7,""gpe""]]}),
    (""tom watson: people's vote more likely given weakness of may's position"", 
    {""entities"":[[0,10,""person""],[56,59,""person""]]}),
]

#formatted test data in order to adapt with the new version 3 of spacy

#get nlp object
nlp = spacy.load('en_core_web_sm')

new_test_data = []
for text, annots in test_data:
    new_test_data.append(example.from_dict(nlp.make_doc(text), annots))

#end formatted test data

#begin evaluation
#using , the evaluate() methos

scores_model = nlp.evaluate(new_test_data)

#print scores that you want
#precision_model = scores_model[""ents_p""]
#recall_model = scores_model[""ents_r""]
#f_score_model = scores_model[""ents_f""]
#scores_entities = scores_model[""ents_per_type""]",https://stackoverflow.com/questions/66637485,python,15-03-2021 11:53,1250.0,1.0,1.0,True,27-06-2021 22:46,17-03-2021 04:14
71865451,open_model_zoo demos stuck at reading model,"i execute some open_model_zoo demos, it works successfully when i choose the cpu device.
but when i change the device to myriad or gpu, it will stuck and do nothing.
(i've used hello_query_device.py to checked, my pc can detected the neural compute stick 2 )
version : openvino_2022.1.0.643 at windows10
picture of error message","['python', 'bert-language-model', 'openvino']",71868694,"the bert-large-uncased-whole-word-masking-squad-0001 model is supported on cpu and gpu, while wav2vec2-base model is supported on cpu only.
refer to intelï¿½ï¿½ï¿½s pre-trained models device support and public pre-trained models device support for open model zoo models' compatibility with cpu, gpu and myriad devices.",https://stackoverflow.com/questions/71865451,python,14-04-2022 01:25,153.0,-1.0,1.0,True,14-04-2022 08:31,14-04-2022 05:09
75845842,is the default `trainer` class in huggingface transformers using pytorch or tensorflow under the hood?,"question
according to the official documentation, the trainer class ""provides an api for feature-complete training in pytorch for most standard use cases"".
however, when i try to actually use trainer in practice, i get the following error message that seems to suggest that tensorflow is currently being used under the hood.
tensorflow/core/platform/cpu_feature_guard.cc:193] this tensorflow binary is optimized with oneapi deep neural network library (onednn) to use the following cpu instructions in performance-critical operations:  avx2 fma
to enable them in other operations, rebuild tensorflow with the appropriate compiler flags.

so which one is it? does the huggingface transformers library use pytorch or tensorflow for their internal implementation of trainer? and is it possible to switch to only using pytorch? i can't seem to find a relevant parameter in trainingarguments.
why does my script keep printing out tensorflow related errors? shouldn't trainer be using pytorch only?
source code
from transformers import gpt2tokenizer
from transformers import gpt2lmheadmodel
from transformers import textdataset
from transformers import datacollatorforlanguagemodeling
from transformers import trainer
from transformers import trainingarguments

import torch

# load the gpt-2 tokenizer and lm head model
tokenizer    = gpt2tokenizer.from_pretrained('gpt2')
lmhead_model = gpt2lmheadmodel.from_pretrained('gpt2')

# load the training dataset and divide blocksize
train_dataset = textdataset(
    tokenizer=tokenizer,
    file_path='./datasets/tinyshakespeare.txt',
    block_size=64
)

# create a data collator for preprocessing batches
data_collator = datacollatorforlanguagemodeling(
    tokenizer=tokenizer,
    mlm=false
)

# defining the training arguments
training_args = trainingarguments(
    output_dir='./models/tinyshakespeare', # output directory for checkpoints
    overwrite_output_dir=true,             # overwrite any existing content

    per_device_train_batch_size=4,         # sample batch size for training
    dataloader_num_workers=1,              # number of workers for dataloader
    max_steps=100,                         # maximum number of training steps
    save_steps=50,                         # after # steps checkpoints are saved
    save_total_limit=5,                    # maximum number of checkpoints to save

    prediction_loss_only=true,             # only compute loss during prediction
    learning_rate=3e-4,                    # learning rate
    fp16=false,                            # use 16-bit (mixed) precision

    optim='adamw_torch',                   # define the optimizer for training
    lr_scheduler_type='linear',            # define the learning rate scheduler

    logging_steps=5,                       # after # steps logs are printed
    report_to='none',                      # report to wandb, tensorboard, etc.
)

if __name__ == '__main__':
    torch.multiprocessing.freeze_support()

    trainer = trainer(
        model=lmhead_model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
    )

    trainer.train()","['python', 'tensorflow', 'pytorch', 'huggingface-transformers']",75873210,"it depends on how the model is trained and how you load the model. most popular models on transformers supports both pytorch and tensorflow (and sometimes also jax).
from transformers import autotokenizer, automodelforseq2seqlm
from transformers import tfautomodelforseq2seqlm

model_name = ""google/flan-t5-large""

model = automodelforseq2seqlm.from_pretrained(model_name)

# this would work if the model's backend is pytorch.
print(type(next(model.parameters())))


tf_model = tfautomodelforseq2seqlm.from_pretrained(model_name)

# the `model.parameters()` would not work for tensorflow,
# instead you can try `.summary()`
tf_model.summary()

[out]:
<class 'torch.nn.parameter.parameter'>

model: ""tft5_for_conditional_generation""
_________________________________________________________________
 layer (type)                output shape              param #   
=================================================================
 shared (embedding)          multiple                  32899072  
                                                                 
 encoder (tft5mainlayer)     multiple                  341231104 
                                                                 
 decoder (tft5mainlayer)     multiple                  441918976 
                                                                 
 lm_head (dense)             multiple                  32899072  
                                                                 
=================================================================
total params: 783,150,080
trainable params: 783,150,080
non-trainable params: 0
_________________________________________________________________



maybe something like:

def which_backend(model):
  try:
    model.parameters()
    return 'torch'
  except:
    try:
      model.summary()
      return 'tensorflow'
    except:
      return 'i have no idea... maybe jax?'



q: so if i use trainer, it's pytorch?
a: yes, most probably the model has pytorch backend, and the training loop (optimizer, loss, etc.) uses pytorch. but the trainer() isn't the model, it's the wrapper object.
q: and if i want to use trainer for tensorflow backend models, i should use tftrainer?
not really. in the latest version of transformers, the tftrainer object is deprecated, see 
it is recommended that you use keras' sklearn-style .fit() training if you are using a model with tensorflow backend.




q: why does my script keep printing out tensorflow related errors? shouldn't trainer be using pytorch only?
try checking your transformers version, most probably you are using an outdated version that uses some deprecated objects, e.g. textdataset (see how to resolve ""only integer tensors of a single element can be converted to an index"" error when creating a dataset to fine-tune gpt2 model?)
in the later versions, most probably pip install transformers>=4.26.1, the trainer shouldn't be activating tf warnings and using tftrainer would have raised warnings to suggest users to use keras instead.",https://stackoverflow.com/questions/75845842,python,26-03-2023 04:19,2916.0,4.0,2.0,True,29-03-2023 05:09,26-03-2023 04:45
73543698,&#39;lazycorpusloader&#39; is not iterable,"i was doing some cleaning in the text with a func
def spacy_tokenizer(sentence):
    # create token object from spacy
    docs = nlp(sentence)

    # correct spelling
    tokens = docs._.outcome_spellcheck
    tokens = nlp(tokens)

    # lemmatize each token and convert each token into lowercase
    tokens = [word.lemma_.lower().strip() if word.lemma_ != ""propn"" else word.lower_ for word in tokens]
    
    # remove stopwords
    tokens = [word for word in tokens if word not in stopwords and word not in punctuations]
    
    # remove links
    tokens = [remove_urls(word) for word in tokens]
    
    # return preprocessed list of tokens
    return tokens


but it raise an error in this part tokens = [word for word in tokens if word not in stopwords and word not in punctuations]
in specific:
<ipython-input-13-b662eacf73f8> in <listcomp>(.0)
     12 
     13     # remove stopwords
---> 14     tokens = [word for word in tokens if word not in stopwords and word not in punctuations]
     15 
     16     # remove links

typeerror: argument of type 'lazycorpusloader' is not iterable","['python', 'python-3.x', 'nlp', 'spacy']",73544303,"it seems that lazycorpusloader from nltk (not spacy) is being iterated by you and it probably should not. stopwords is of nltk.corpus.util.lazycorpusloader type. you might want to use: stopwords.words('english') instead.
please also check this answer from data science stack exchange.",https://stackoverflow.com/questions/73543698,python,30-08-2022 13:58,386.0,0.0,1.0,True,31-08-2022 06:49,31-08-2022 06:49
71011333,"runtimeerror: stack expects each tensor to be equal size, but got [7, 768] at entry 0 and [8, 768] at entry 1","when running this code:
embedding_matrix = torch.stack(embeddings)

i got this error:

runtimeerror: stack expects each tensor to be equal size, but got [7, 768] at entry 0 and [8, 768] at entry 1

i'm trying to get embedding using bert via:
split_sent = sent.split()
tokens_embedding = []
j = 0
for full_token in split_sent:
    curr_token = ''
    x = 0
    for i,_ in enumerate(tokenized_sent[1:]): 
        token = tokenized_sent[i+j]
        piece_embedding = bert_embedding[i+j]
        if token == full_token and curr_token == '' :
            tokens_embedding.append(piece_embedding)
            j += 1
            break                                     
sent_embedding = torch.stack(tokens_embedding)
embeddings.append(sent_embedding)
embedding_matrix = torch.stack(embeddings)

does anyone know how i can fix this?","['python', 'pytorch', 'runtime-error', 'tensor', 'bert-language-model']",71014842,"as per pytorch docs about torch.stack() function, it needs the input tensors in the same shape to stack. i don't know how will you be using the embedding_matrix but either you can add padding to your tensors (which will be a list of zeros at the end till a certain user-defined length and is recommended if you will train with this stacked tensor, refer this tutorial) to make them equidimensional or you can simply use something like torch.cat(data,dim=0).",https://stackoverflow.com/questions/71011333,python,06-02-2022 20:23,40516.0,6.0,1.0,True,21-08-2024 18:06,21-08-2024 18:06
73862111,trouble retrieving value from a list inside a python dictionary,"i am developing a discord bot in python with py-cord and i have implemented the openai api to allow users to query the ai and modify the different weights and biases.
i am trying to make the weights specific to each guild, so people in one server can have different settings to someone in a different server. to achieve this, i am trying to use a dictionary where the guild id is the key and the weights are in a list as the values, but i keep getting keyerror exceptions.
import os
import discord
import openai
import re
import asyncio
from discord.ext import commands
from gtts import gtts
from discord import ffmpegpcmaudio
from mutagen.mp3 import mp3

bot = discord.bot(intents=discord.intents.default())

guilds = {}

@bot.event
async def on_ready():
    bot.temp = 1
    bot.topp = 0.5
    bot.freqpen = 0.3
    bot.prespen = 0.3
    x = datetime.datetime.now()
    print('logged in as {0.user} at'.format(bot), x, ""\n"")

class mymodal(discord.ui.modal):
    def __init__(self, *args, **kwargs) -> none:
        super().__init__(*args, **kwargs)

        self.add_item(discord.ui.inputtext(label=f""temperature. current: {bot.temp}""))
        self.add_item(discord.ui.inputtext(label=f""frequency penalty. current: {bot.freqpen}""))
        self.add_item(discord.ui.inputtext(label=f""presence penalty. current: {bot.prespen}""))
        self.add_item(discord.ui.inputtext(label=f""top p. current: {bot.topp}""))

    async def callback(self, interaction: discord.interaction):
        guilds[f'{bot.id}'] = [self.children[0].value, self.children[1].value, self.children[2].value, self.children[3].value]
        embed = discord.embed(title=""new gpt-3 weights and biases"", color=0xff5733)
        embed.add_field(name=f""temperature: {bot.temp}"", value=""controls randomness: lowering results in less random completions. i recommend not going higher than 1."", inline=false)
        embed.add_field(name=f""frequency penalty: {bot.freqpen}"", value=""how much to penalize new tokens based on their existing frequency in the text so far. "", inline=false)
        embed.add_field(name=f""presence penalty: {bot.prespen}"", value=""how much to penalize new tokens based on whether they appear in the text so far. increases the model's likelihood to talk about new topics. will not function above 2."", inline=false)
        embed.add_field(name=f""top p: {bot.topp}"", value=""controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. will not function above 1."", inline=false)
        await interaction.response.send_message(embeds=[embed])

@bot.slash_command(description=""change the gpt-3 weights and biases"")
async def setvalues(ctx: discord.applicationcontext):
    bot.id = ctx.guild.id
    modal = mymodal(title=""modify gpt-3 weights"")
    await ctx.send_modal(modal)

bot.run('token')

this creates a modal for users to input the values they want and then sends those values in a list to the dictionary called guilds with the key being bot.id. however, when i run a command i created to test pulling a value from the list, i get a keyerror exception. the command i run to check is:
@bot.slash_command()
async def dictionarytest(ctx):
    await ctx.respond(f'{guilds[bot.id][1]}')

the error i get is

ignoring exception in command dictionarytest: traceback (most recent
call last):   file
""/home/liam/.local/lib/python3.10/site-packages/discord/commands/core.py"",
line 127, in wrapped
ret = await coro(arg)   file ""/home/liam/.local/lib/python3.10/site-packages/discord/commands/core.py"",
line 911, in _invoke
await self.callback(ctx, **kwargs)   file ""/home/liam/pycharmprojects/discordbot/maintest.py"", line 76, in
dictionarytest
await ctx.respond(f'{str(guilds[bot.id][1])}') keyerror: 545151014702022656
the above exception was the direct cause of the following exception:
traceback (most recent call last):   file
""/home/liam/.local/lib/python3.10/site-packages/discord/bot.py"", line
1009, in invoke_application_command
await ctx.command.invoke(ctx)   file ""/home/liam/.local/lib/python3.10/site-packages/discord/commands/core.py"",
line 359, in invoke
await injected(ctx)   file ""/home/liam/.local/lib/python3.10/site-packages/discord/commands/core.py"",
line 135, in wrapped
raise applicationcommandinvokeerror(exc) from exc discord.errors.applicationcommandinvokeerror: application command
raised an exception: keyerror: 545151014702022656","['python', 'python-3.x', 'discord', 'pycord', 'openai-api']",73862209,"since we can't see the data associated with your app, it's hard to know for sure, but i see something suspicious that i think may be your problem. the error you are getting is pretty self-explanatory.  when executing this line:
await ctx.respond(f'{guilds[bot.id][1]}')

the error message is telling you that there isn't any key bot.id in the guilds dict, and bot.id has the value 545151014702022656. so in this case, the key is an integer.
the only place you add values to the guild dict is this line:
guilds[f'{bot.id}'] = [self.children[0].value, self.children[1].value, self.children[2].value, self.children[3].value]

here, you are adding a value to the guilds dict with a key that is a string. i bet that's your problem. the integer 545151014702022656 and the string ""545151014702022656"" aren't the same thing. you fail to match the keys you have added to guilds, because you're adding string keys, but looking for integer keys.  so i bet all you have to do to fix your code is change the above line to:
guilds[bot.id] = [self.children[0].value, self.children[1].value, self.children[2].value, self.children[3].value]",https://stackoverflow.com/questions/73862111,python,27-09-2022 03:43,99.0,0.0,1.0,True,16-04-2023 14:52,16-04-2023 14:50
79548202,"gpt-2 and other models from huggingface -100 label index for training, instead of pad token","i understand the -100 label id is used so that the predictions for these are not included when calculating the loss.
however on huggingface, they state
""complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not"", when replacing pad tokens. in their implementation, they use nn.crossentropyloss(), which has an argument ""ignore_index"".
is there any benefit to changing the id to -100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id? or are the results the same?
the way it is written makes me think there is some benefit, but the description of ""ignore_index"" appears to achieve what is wanted.","['nlp', 'huggingface-transformers', 'pre-trained-model']",79551169,the author of the tutorial you mentioned sets it to -100 and uses ignore_index to save a few lines of code. you don't see the line where the author pass something to ignore_index because it has a default value. the default value of ignore_index for nn.crossentropyloss is -100. using this value instead of the respective pad token id allows you to write some model indepent training code and you don't have to pass the pad token id from tokenizer down to the loss function.,https://stackoverflow.com/questions/79548202,nlp,01-04-2025 09:21,57.0,0.0,1.0,True,04-04-2025 22:41,04-04-2025 22:41
78356841,getting 404 on openai azure endpoint,"using the azureopenai client, and getting a 404
asyncazureopenai(
                api_key=os.getenv(""azure_openai_api_key""),  
                api_version=""2024-01-25-preview"",
                azure_deployment=""xxx-staging"",
                azure_endpoint=os.getenv(""azure_openai_endpoint"", """"),
            )

as you can see in the image below, i'm using the 0125 model version. interestingly, i don't even see my model version listed directly here:

i've tried this on openai-1.23.2 and openai-1.14.3","['python', 'azure', 'openai-api']",78360252,"found the answer.
found the answer. the problem was actually in the request params.normally, openai expects a model name to be passed in for model like
gpt-4-1106-preview
but in the case of azure, it expects the given model name in your deployment, so whatever name you wrote down for the deployment should be passed in here.
params = {
    ""model"": model.name, // name in azure
    ""messages"": chat_messages,
    ""max_tokens"": model_max_tokens,
    ""temperature"": temperature,
    ""stream"": stream,
}",https://stackoverflow.com/questions/78356841,python,20-04-2024 02:38,503.0,-1.0,1.0,True,21-04-2024 02:49,20-04-2024 22:24
71048521,how to freeze parts of t5 transformer model,"i know that t5 has k, q and v vectors in each layer. it also has a feedforward network. i would like to freeze k, q and v vectors and only train the feedforward layers on each layer of t5. i use pytorch library. the model could be a wrapper for huggingface t5 model or a modified version of it. i know how to freeze all parameters using the following code:
tokenizer = autotokenizer.from_pretrained(underlying_model_name)
model = t5forconditionalgeneration.from_pretrained(underlying_model_name)

for p in model.parameters():
    p.requires_grad = false # freezing

could you please guide me how can i do this?
this github project probably could be helpful but it's for roberta and gpt, could i adapt it for t5?","['huggingface-transformers', 't5-transformer']",71068151,"i've adapted a solution based on this discussion from the huggingface forums.
basically, you have to specify the names of the modules/pytorch layers that you want to freeze.
in your particular case of t5, i started by looking at the model summary:
from transformers import t5modelforconditionalgeneration

model = t5modelforconditionalgeneration.from_pretrained(""t5-small"")
print(model)

this gives the following (abbreviated output):
t5forconditionalgeneration(
  (shared): embedding(32128, 512)
  (encoder): t5stack(
    (embed_tokens): embedding(32128, 512)
    (block): modulelist(
      (0): t5block(
        (layer): modulelist(
          (0): t5layerselfattention(
            (selfattention): t5attention(
              (q): linear(in_features=512, out_features=512, bias=false)
              (k): linear(in_features=512, out_features=512, bias=false)
              (v): linear(in_features=512, out_features=512, bias=false)
              (o): linear(in_features=512, out_features=512, bias=false)
              (relative_attention_bias): embedding(32, 8)
            )
            (layer_norm): t5layernorm()
            (dropout): dropout(p=0.1, inplace=false)
          )
          (1): t5layerff(
            (densereludense): t5densereludense(
              (wi): linear(in_features=512, out_features=2048, bias=false)
              (wo): linear(in_features=2048, out_features=512, bias=false)
              (dropout): dropout(p=0.1, inplace=false)
            )
            (layer_norm): t5layernorm()
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
      )
[...]  # abbreviated output

with this, we can then generate a list of modules that we want to freeze. in particular, i decided to freeze the entire t5layerselfattention block for the encoder (and, additionally, the t5layercrossattention for the decoder):
# all modules in the 
modules_to_freeze = [model.encoder.block[i].layer[0] for i in range(len(model.encoder.block))]
# and the decoder modules, which has both a selfattention (layer[0]) 
modules_to_freeze.extend([model.decoder.block[i].layer[0] for i in range(len(model.decoder.block))])
# and crossattention (layer[1]) block
modules_to_freeze.extend([model.decoder.block[i].layer[1] for i in range(len(model.decoder.block))])

and then simply freeze all the parameters in the respective modules:
for module in modules_to_freeze:
    for param in module.parameters():
        param.requires_grad = false  # actual freezing operation

you can verify that these are actually frozen in your model by running the following:
for param in model.parameters():
    print(param.requires_grad)

which should print quite a few false as well. if you really only want to freeze k, q and v, you can adapt the above process to just sub-select the modules you want.",https://stackoverflow.com/questions/71048521,huggingface-transformers,09-02-2022 11:10,3363.0,2.0,2.0,True,30-05-2023 10:36,10-02-2022 15:25
77010524,"will using arguments - max_length, truncate, and padding in tranformers pipeline affect the output?","hello so i was checking sentiment of a text using transformers pretrained model ,but doing so gave me error

runtimeerror: the size of tensor a (1954) must match the size of tensor b (512) at non-singleton dimension 1

i went through few post which suggested that setting max_length as 512 will sort the error.
it did resolve the error, but i want to know how it affects the quality of output. does it truncate my text? for example, if the length of my text is 1195 will it process till 512, something like text[:512]?","['python', 'nlp', 'huggingface-transformers', 'sentiment-analysis']",77017166,"yes. it means the sentiment will be based on the first 512 tokens, and any tokens after that will not influence the result.
note that this is tokens, not characters. if text was your raw string, and if we assume that on average each token is 2.5 characters, then truncating at 512 tokens would be the same as text[:1280].
(the characters per token can vary a lot based on the model, the tokenizer, the language, the domain, but mainly how unusual the string is compared to the text used to train the tokenizer.)
by the way, according to  if you don't specify truncation then no truncation is applied; and if you do, but don't specify max_length then it will default to the maximum supported by the model. so setting max_length and not changing anything else shouldn't have fixed it. (i've not tested anything, or read the code, that is just based on my understanding of the documentation.)",https://stackoverflow.com/questions/77010524,python,30-08-2023 18:08,873.0,0.0,1.0,True,01-09-2023 19:07,01-09-2023 19:07
72480729,error when taking fft2d in tensorflow on gpu,"the code below runs on cpu without any problems, but when i change to gpu in colab it fails to calculate the fft2d.
import tensorflow as tf

sample_fft_input = tf.random.uniform((2, 10, 20))
sfi = tf.cast(sample_fft_input , tf.complex64)
sfi = tf.math.real(tf.signal.fft2d(sfi))
print(sfi.shape) -> tensorshape([2, 10, 20])

but on gpu:
---------------------------------------------------------------------------
internalerror                             traceback (most recent call last)
<ipython-input-41-094e0f7d5037> in <module>()
      3 sample_fft_input = tf.random.uniform((2, 10, 20))
      4 sfi = tf.cast(sample_fft_input, tf.complex64)
----> 5 sfi = tf.math.real(tf.signal.fft2d(sfi))
      6 print(sfi.shape)

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   7162 def raise_from_not_ok_status(e, name):
   7163   e.message += ("" name: "" + name if name is not none else """")
-> 7164   raise core._status_to_exception(e) from none  # pylint: disable=protected-access
   7165 
   7166 

internalerror: fft failed : type=1 in.shape=[2,10,20] [op:fft2d]","['tensorflow', 'machine-learning', 'nlp', 'tensorflow2.0', 'fft']",72482363,i downgraded to tensorflow 2.8.2 and the problem is gone.,https://stackoverflow.com/questions/72480729,tensorflow,02-06-2022 18:20,157.0,0.0,1.0,True,02-06-2022 21:08,02-06-2022 18:27
62669261,how to encode multiple sentences using transformers.berttokenizer?,"i would like to create a minibatch by encoding multiple sentences using transform.berttokenizer. it seems working for a single sentence. how to make it work for several sentences?
from transformers import berttokenizer

tokenizer = berttokenizer.from_pretrained('bert-base-uncased')

# tokenize a single sentence seems working
tokenizer.encode('this is the first sentence')
>>> [2023, 2003, 1996, 2034, 6251]

# tokenize two sentences
tokenizer.encode(['this is the first sentence', 'another sentence'])
>>> [100, 100] # expecting 7 tokens","['word-embedding', 'huggingface-transformers', 'huggingface-tokenizers']",62688252,"transformers >= 4.0.0:
use __call__ method of the tokenizer. it will generate a dictionary which contains the input_ids, token_type_ids and the attention_mask as list for each input sentence:
tokenizer(['this is the first sentence', 'another setence'])

output:
{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 102], [101, 2178, 2275, 10127, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}

transformers < 4.0.0:
use tokenizer.batch_encode_plus (documentation). it will generate a dictionary which contains the input_ids, token_type_ids and the attention_mask as list for each input sentence:
tokenizer.batch_encode_plus(['this is the first sentence', 'another setence'])

output:
{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 102], [101, 2178, 2275, 10127, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}

applies to call and batch_encode_plus:
in case you only want to generate the input_ids, you have to set return_token_type_ids and return_attention_mask to false:
tokenizer.batch_encode_plus(['this is the first sentence', 'another setence'], return_token_type_ids=false, return_attention_mask=false)

output:
{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 102], [101, 2178, 2275, 10127, 102]]}",https://stackoverflow.com/questions/62669261,word-embedding,01-07-2020 03:32,24158.0,15.0,2.0,True,08-03-2023 22:10,07-10-2021 05:55
72845812,"bert-base-uncased: typeerror: tuple indices must be integers or slices, not tuple","i want to see embeddings for the input text i give to the model, and then feed it to the rest of the bert. to do so, i partitioned the model into two sequential models, but i must have done it wrong because rest_of_bert model raises typeerror. original model does not raise any error with the input_ids as input processed with text_to_input function.
input[0]:
import torch
from transformers import berttokenizer, bertmodel

tokenizer = berttokenizer.from_pretrained('bert-base-uncased')
cls_token_id = tokenizer.cls_token_id
sep_token_id = tokenizer.sep_token_id
pad_token_id = tokenizer.pad_token_id

model = bertmodel.from_pretrained('bert-base-uncased', output_hidden_states=true)
model.eval()

output[0]:
some weights of the model checkpoint at bert-base-uncased were not used when initializing bertmodel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.layernorm.bias', 'cls.predictions.transform.layernorm.weight', 'cls.seq_relationship.bias']
- this is expected if you are initializing bertmodel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a bertforsequenceclassification model from a bertforpretraining model).
- this is not expected if you are initializing bertmodel from the checkpoint of a model that you expect to be exactly identical (initializing a bertforsequenceclassification model from a bertforsequenceclassification model).
bertmodel(
  (embeddings): bertembeddings(
    (word_embeddings): embedding(30522, 768, padding_idx=0)
    (position_embeddings): embedding(512, 768)
    (token_type_embeddings): embedding(2, 768)
    (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
    (dropout): dropout(p=0.1, inplace=false)
  )
  (encoder): bertencoder(
    (layer): modulelist(
      (0): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (1): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (2): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (3): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (4): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (5): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (6): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (7): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (8): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (9): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (10): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
      (11): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
          (intermediate_act_fn): geluactivation()
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
    )
  )
  (pooler): bertpooler(
    (dense): linear(in_features=768, out_features=768, bias=true)
    (activation): tanh()
  )
)

input[1]:
def text_to_input(text):
  x = tokenizer.encode(text, add_special_tokens=false) # returns python list
  x = [cls_token_id] + x + [sep_token_id]
  token_count = len(x)
  pad_count = 512 - token_count
  x = x + [pad_token_id for i in range(pad_count)]
  return torch.tensor([x])

extract_embeddings = torch.nn.sequential(list(model.children())[0])
rest_of_bert = torch.nn.sequential(*list(model.children())[1:])

input_ids = text_to_input('a sentence.')
x_embedding = extract_embeddings(input_ids)
output = rest_of_bert(x_embedding)

output[1]:
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
<ipython-input-5-d371d8a2fb3c> in <module>()
     12 input_ids = text_to_input('a sentence.')
     13 x_embedding = extract_embeddings(input_ids)
---> 14 output = rest_of_bert(x_embedding)

4 frames
/usr/local/lib/python3.7/dist-packages/transformers/utils/generic.py in __getitem__(self, k)
    220             return inner_dict[k]
    221         else:
--> 222             return self.to_tuple()[k]
    223 
    224     def __setattr__(self, name, value):

typeerror: tuple indices must be integers or slices, not tuple","['python', 'machine-learning', 'pytorch', 'huggingface-transformers', 'bert-language-model']",72847541,"each bertlayer returns a tuple that contains at least one tensor (depending on what output you requested). the first element of the tuple is the tensor you want to feed to the next bertlayer.
a more huggingface-like approach would be calling the model with output_hidden_states:
o = model(input_ids, output_hidden_states=true)
print(len(o.hidden_states))

output:
13

the first tensor of the hidden_states tuple is the output of your extract_embeddings object (token embeddings). the other 12 tensors are the contextualized embeddings that are the output of each bertlayer.
you should, by the way, provide an attention mask, because otherwise, your padding tokens will affect your output. the tokenizer is able to do that for you and you can replace your whole text_to_input method with:
tokenizer('a sentence.', return_tensors='pt', padding='max_length', max_length=512)",https://stackoverflow.com/questions/72845812,python,03-07-2022 10:43,1448.0,0.0,1.0,True,03-07-2022 15:02,03-07-2022 14:33
77868284,combining falcon 40b instruct with langchain,"i want to create a local llm using falcon 40b instruct model and combine it with lanchain so i can give it a pdf or some resource to learn from so i can query it ask it questions, learn from it and ultimately be able to derive insights from the pdf report from an excel sheet.
for now, i just want to load a pdf using langchain and have the falcon-40b-instruct model as the agent.
i want to build an llm where i can make it interact with my own data using langchain.
here is my attempt so far:
from langchain_community.llms import huggingfacehub

llm = huggingfacehub(
repo_id=model_name,
task=""text-generation"",
model_kwargs={
""max_new_tokens"": 512,
""top_k"": 30,
""temperature"": 0.1,
""repetition_penalty"": 1.03
},
huggingfacehub_api_token=""hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""
)

i reached the following stage:
from langchain_community.chat_models.huggingface import chathuggingface
llm = chathuggingface(llm=llm)

yet i get this error:

hfhub 401 client error: unauthorized for url

i am doing do this to be able to run the following:
qa_chain = retrievalqa.from_chain_type(
llm=llm,
retriever=vector_db.as_retriever()
)

what am i missing and is there a way to be able to do this fully local like doing the falcon model and pass it to chathuggingface?","['nlp', 'chatbot', 'langchain', 'large-language-model', 'falcon']",77870500,"the message of hfhub 401 client error: unauthorized for url indicates that you don't have access to endpoint service from huggingface > 
as you want to run everything locally, here's an example, using huggingfacepipeline function
from transformers import autotokenizer, automodelforcausallm, pipeline

# model_id = ""tiiuae/falcon-7b-instruct""  # this model is too large to run on nvidia 4090 with 16g ram
model_id = ""gpt2""
tokenizer = autotokenizer.from_pretrained(model_id)
model = automodelforcausallm.from_pretrained(model_id)
pipeline = pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=200,
)


from langchain_community.llms.huggingface_pipeline import huggingfacepipeline
llm = huggingfacepipeline(pipeline=pipeline)


from langchain.prompts import prompttemplate
template = """"""question: {question}

answer: let's think step by step.""""""
prompt = prompttemplate.from_template(template)

chain = prompt | llm
question = ""tell me about italy""

print(chain.invoke({""question"": question}))",https://stackoverflow.com/questions/77868284,nlp,23-01-2024 17:43,328.0,0.0,1.0,True,24-01-2024 03:54,24-01-2024 02:17
21492480,python isristemmer for arabic text,"i am running the following code on idle(python) and i want to enter arabic string and get the stemming for it but actually it doesn't work
>>> from nltk.stem.isri import isristemmer
>>> st = isristemmer()
>>> w= 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½'
>>> join = w.decode('windows-1256')
>>> print st.stem(join).encode('windows-1256').decode('utf-8')

the result of running it is the same text in w which is 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½' which is not the stem
but when do the following:
>>> print st.stem(u'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½')

the result succeeded and returns the stem which is 'ï¿½ï¿½ï¿½ï¿½ï¿","['python', 'utf-8', 'arabic', 'stemming']",21506361,"ok, i solved the problem by myself using the following:
w = 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½' 
st.stem(w.decode('utf-8'))

and it gives the root correctly which is ""ï¿½ï",https://stackoverflow.com/questions/21492480,python,01-02-2014 00:17,10727.0,6.0,7.0,True,18-12-2023 03:08,19-07-2020 07:30
66110901,fine-tuned albert question and answering with huggingface,"i'm trying to create a question and answering ai, i would like it to be as accurate as possible without having to train the model myself.
i can create a simple ai using the existing base models like so via their documentation:
from transformers import alberttokenizer, albertforquestionanswering
import torch
tokenizer = alberttokenizer.from_pretrained('albert-base-v2')
model = albertforquestionanswering.from_pretrained('albert-base-v2')
question, text = ""what does he like?"", ""he likes bears""
inputs = tokenizer(question, text, return_tensors='pt')
start_positions = torch.tensor([1])
end_positions = torch.tensor([3])
outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
loss = outputs.loss
start_scores = outputs.start_logits
end_scores = outputs.end_logits

answer_start = torch.argmax(start_scores)  # get the most likely beginning of answer with the argmax of the score
answer_end = torch.argmax(end_scores) + 1
tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[""input_ids""][0][answer_start:answer_end]))

however this model doesn't answer questions as accurate as others.  on the huggingface site i've found an example that i'd like to use of a fine-tuned model
however the instructions show how to train a model like so.  the example works on the page so clearly a pretrained model of the exists.
does anyone know how i can reuse the existing models so i don't have to train one from scratch?","['python', 'nlp', 'artificial-intelligence', 'torch', 'bert-language-model']",66111128,"turns out i just needed to grab an additional identifier when trying to request the model:
from transformers import alberttokenizer, albertforquestionanswering
import torch

model_path = 'ktrapeznikov/albert-xlarge-v2-squad-v2';

tokenizer = alberttokenizer.from_pretrained(model_path)
model = albertforquestionanswering.from_pretrained(model_path)

for future reference this information can be grabbed from the transformers use button.  seem in the image below.",https://stackoverflow.com/questions/66110901,python,08-02-2021 23:32,996.0,0.0,1.0,True,09-02-2021 00:02,08-02-2021 23:41
74519464,attributeerror: &#39;tuple&#39; object has no attribute &#39;rank&#39; when calling model.fit() in nlp task,"i'm following this tutorial

however, while implementing the ann based on the tf-idf features, i'm getting this error
attributeerror: 'tuple' object has no attribute 'rank'
this is the snippet-
from sklearn.feature_extraction.text import tfidfvectorizer
tvec1 = tfidfvectorizer(max_features=100000,ngram_range=(1, 3))
tvec1.fit(x_train)


x_train_tfidf = tvec1.transform(x_train)
x_validation_tfidf = tvec1.transform(x_validation).toarray()

seed = 7
np.random.seed(seed)
import tensorflow as tf
from tensorflow.keras.models import sequential
from tensorflow.keras.layers import dense, dropout
from tensorflow.keras.layers import flatten
#from tensorflow.keras.layers.embeddings import embedding
from tensorflow.keras.layers import embedding
from tensorflow.keras.preprocessing import sequence

def batch_generator(x_data, y_data, batch_size):
    samples_per_epoch = x_data.shape[0]
    number_of_batches = samples_per_epoch/batch_size
    counter=0
    index = np.arange(np.shape(y_data)[0])
    while 1:
        index_batch = index[batch_size*counter:batch_size*(counter+1)]
        x_batch = x_data[index_batch,:].toarray()
        y_batch = y_data[y_data.index[index_batch]]
        counter += 1
        yield x_batch,y_batch
        if (counter > number_of_batches):
            counter=0

model = sequential()
model.add(dense(64, activation='relu', input_dim=100000))
model.add(dense(1, activation='sigmoid'))
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(batch_generator(x_train_tfidf, y_train, 32), epochs=5, validation_data=(x_validation_tfidf, y_validation),steps_per_epoch=x_train_tfidf.shape[0]/32)

this is the error-
---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
~\appdata\local\temp\ipykernel_13000\1276649087.py in <module>
      1 model.fit(batch_generator(x_train_tfidf, y_train, 32),
      2                     epochs=5, validation_data=(x_validation_tfidf, y_validation),
----> 3                     steps_per_epoch=x_train_tfidf.shape[0]/32)

~\appdata\roaming\python\python37\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1145           use_multiprocessing=use_multiprocessing,
   1146           model=self,
-> 1147           steps_per_execution=self._steps_per_execution)
   1148 
   1149       # container that configures and calls `tf.keras.callback`s.

~\appdata\roaming\python\python37\site-packages\tensorflow\python\keras\engine\data_adapter.py in get_data_handler(*args, **kwargs)
   1362   if getattr(kwargs[""model""], ""_cluster_coordinator"", none):
   1363     return _clustercoordinatordatahandler(*args, **kwargs)
-> 1364   return datahandler(*args, **kwargs)
   1365 
   1366 

~\appdata\roaming\python\python37\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)
   1164         use_multiprocessing=use_multiprocessing,
   1165         distribution_strategy=ds_context.get_strategy(),
-> 1166         model=model)
   1167 
   1168     strategy = ds_context.get_strategy()

~\appdata\roaming\python\python37\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)
    826       return tensor_shape.tensorshape([none for _ in shape.as_list()])
    827 
--> 828     output_shapes = nest.map_structure(_get_dynamic_shape, peek)
    829     output_types = nest.map_structure(lambda t: t.dtype, peek)
    830 

~\appdata\roaming\python\python37\site-packages\tensorflow\python\util\nest.py in map_structure(func, *structure, **kwargs)
    865 
    866   return pack_sequence_as(
--> 867       structure[0], [func(*x) for x in entries],
    868       expand_composites=expand_composites)
    869 

~\appdata\roaming\python\python37\site-packages\tensorflow\python\util\nest.py in <listcomp>(.0)
    865 
    866   return pack_sequence_as(
--> 867       structure[0], [func(*x) for x in entries],
    868       expand_composites=expand_composites)
    869 

~\appdata\roaming\python\python37\site-packages\tensorflow\python\keras\engine\data_adapter.py in _get_dynamic_shape(t)
    822       shape = t.shape
    823       # unknown number of dimensions, `as_list` cannot be called.
--> 824       if shape.rank is none:
    825         return shape
    826       return tensor_shape.tensorshape([none for _ in shape.as_list()])

attributeerror: 'tuple' object has no attribute 'rank'","['python', 'keras', 'neural-network', 'nlp', 'tf-idf']",78104970,"this happens when function expects a tensor, but other type of data is passed instead.
for instance, numpy array, like the one below:
y.shape


out: (2000,)
y.shape.rank

out: attributeerror: 'tuple' object has no attribute 'rank'
to correct this, data should be converted to tensor.
tf.constant(y).shape.rank

out: 1",https://stackoverflow.com/questions/74519464,python,21-11-2022 13:08,926.0,1.0,1.0,True,11-09-2024 15:42,21-11-2022 13:12
66087475,chatterbot error- oserror: [e941] can&#39;t find model &#39;en&#39;,"i tried running my first chatterbot program (its from the pypi page of chatterbot), and when i run it, i get an error. the error is related to spacy, but i am unable to find a solution.
here is the code:
from chatterbot import chatbot
from chatterbot.trainers import chatterbotcorpustrainer

chatbot = chatbot('ron obvious')

trainer = chatterbotcorpustrainer(chatbot)

trainer.train(""chatterbot.corpus.english"")

chatbot.get_response(""hello, how are you today?"")

and here is the error:
traceback (most recent call last):
  file ""c:/users/user/desktop/bot.py"", line 77, in <module>
    chatbot = chatbot('ron obvious')
  file ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\chatterbot\chatterbot.py"", line 28, in __init__
    self.storage = utils.initialize_class(storage_adapter, **kwargs)
  file ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\chatterbot\utils.py"", line 33, in initialize_class
    return class(*args, **kwargs)
  file ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\chatterbot\storage\sql_storage.py"", line 20, in __init__
    super().__init__(**kwargs)
  file ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\chatterbot\storage\storage_adapter.py"", line 21, in __init__
    'tagger_language', languages.eng
  file ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\chatterbot\tagging.py"", line 13, in __init__
    self.nlp = spacy.load(self.language.iso_639_1.lower())
  file ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\spacy\__init__.py"", line 47, in load
    return util.load_model(name, disable=disable, exclude=exclude, config=config)
  file ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\spacy\util.py"", line 328, in load_model
    raise ioerror(errors.e941.format(name=name, full=old_model_shortcuts[name]))
oserror: [e941] can't find model 'en'. it looks like you're trying to load a model from a shortcut, which is deprecated as of spacy v3.0. to load the model, use its full name instead:

nlp = spacy.load(""en_core_web_sm"")

for more details on the available models, see the models directory:  if you want to create a blank model, use spacy.blank: nlp = spacy.blank(""en"")

it would be helpful if someone finds a solution for this.","['python', 'windows', 'spacy', 'chatterbot']",66087946,"make sure you actually have the right spacy model installed. for example, install en_core_web_sm with the python -m spacy download en_core_web_sm command in the terminal.
next, fix this error:
file ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\chatterbot\tagging.py"", line 13, in __init__
    self.nlp = spacy.load(self.language.iso_639_1.lower())

that is,

open the c:\users\user\appdata\local\programs\python\python37\lib\site-packages\chatterbot\tagging.py file
go to line 13
replace self.nlp = spacy.load(self.language.iso_639_1.lower()) with

if self.language.iso_639_1.lower() == 'en':
    self.nlp = spacy.load('en_core_web_sm')
else:
    self.nlp = spacy.load(self.language.iso_639_1.lower())

you will need to add more conditions for other languages you need to support.",https://stackoverflow.com/questions/66087475,python,07-02-2021 11:41,34041.0,10.0,9.0,True,23-04-2024 23:35,23-04-2024 23:35
71205484,using torchtext vocab with torchscript,"i'm trying to use the torchtext vocab layer along with torchscript but i'm getting some errors and i was wondering if someone here has made it work.
my current model is
class vocabtext(torch.nn.module):

    def __init__(self):

        super(vocabtext, self).__init__()

        self.embedding = torch.nn.embedding(10,128)

        vocab = ['this', 'is', 'a', 'test']

        counter = counter(vocab)

        self.lookup = text.vocab.vocab(counter)

        self.tensor = torch.tensor

    def forward(self, x: str):

        x_mapped = self.lookup(x)

        x_mapped = self.tensor(x_mapped).int()

        x_mapped = self.embedding(x_mapped)

        

        return x

that works when i do a pass of the model like this:
example_str = [""is""]

model(example_str)

but when i try to compile it with torchscript it fails:
model_scripted = torch.jit.script(model)   

model_scripted.save('model_scripted.pt')

with the following error:
runtimeerror: 
unknown builtin op: aten::tensor.
here are some suggestions: 

for when i map the result of the lookup layer during the forward function
i think is due to typing as the vocab layer expects strings as input but the embedding layer will use tensors. im doing a cast in the middle of the forward.
i have a working notebook in colab to reproduce this issue if anybody wants:","['nlp', 'pytorch', 'torchtext', 'torchscript']",71205542,"turns out i had to change the function to built the tensor, found at",https://stackoverflow.com/questions/71205484,nlp,21-02-2022 11:22,295.0,0.0,1.0,True,23-02-2022 08:37,23-02-2022 08:37
70489853,count words in texts that are not in a given dictionary,"how can i find and count words that are not in a given dictionary?
the example below counts every time specific dictionary words (clouds and storms) appear in the text.
library(""quanteda"")
txt <- ""forty-four americans have now taken the presidential oath. the words have been spoken during rising tides of prosperity and the still waters of peace. yet, every so often the oath is taken amidst gathering clouds and raging storms. at these moments, america has carried on not simply because of the skill or vision of those in high office, but because we the people have remained faithful to the ideals of our forbearers, and true to our founding documents.""   
mydict <- dictionary(list(all_terms = c(""clouds"", ""storms"")))
dfmat <- tokens(txt) %>%
  tokens_select(mydict) %>%
  dfm()
dfmat

the output:
docs    clouds storms
  text1      1      1

how can i instead generate a count of all words that are not in the dictionary (clouds/storms)? ideally with stopwords excluded.
e.g., desired output:
docs    forty-four americans ...
  text1      1      1","['r', 'nlp', 'word-count', 'quanteda']",70501318,"when you check the help file for tokens_select (run ?tokens_select) you can see that the third argument is selection. the default value is ""keep"", yet what you want is ""remove"". since this is a common thing to do, there is also a dedicated tokens_remove command, which i use below to to remove stopwords.
dfmat <- tokens(txt) %>%
  tokens_select(mydict, selection = ""remove"") %>%
  tokens_remove(stopwords::stopwords(language = ""en"")) %>% 
  dfm()
dfmat
#> document-feature matrix of: 1 document, 38 features (0.00% sparse) and 0 docvars.
#>        features
#> docs    forty-four americans now taken presidential oath . words spoken rising
#>   text1          1         1   1     2            1    2 4     1      1      1
#> [ reached max_nfeat ... 28 more features ]

i think this is what you are trying to do.
created on 2021-12-28 by the reprex package (v2.0.1)",https://stackoverflow.com/questions/70489853,r,26-12-2021 21:02,84.0,0.0,2.0,True,28-12-2021 17:14,27-12-2021 15:26
46983843,is it possible to filter on a non-indexed field with doc_values=true in elasticsearch,"in elasticsearch 5.6 using the following mapping:
""category"" => [
    ""type""=>""keyword"",
    ""doc_values""=>true,
    ""index""=>false
    ""store""=>true
]

i was given advice that it was possible to write a query that filters on this field because of its doc_values setting, even though the index attribute was set to false, but it seems like doc_values fields are only useful for aggregations and sorting. 
is it possible to create a query which filters on this field?","['elasticsearch', 'search', 'lucene', 'information-retrieval', 'elasticsearch-5']",47041799,"an field that is not indexed is, by definition, not searchable. elasticsearch won't put it in the inverted index (which is used for searching). if you try to run a search query, you will get an error like cannot search on field [category] since it is not indexed.",https://stackoverflow.com/questions/46983843,elasticsearch,27-10-2017 21:47,11515.0,4.0,3.0,True,20-07-2023 09:15,30-10-2017 20:25
65699672,how to force a certain tag in spacy?,"i'm using spacy '3.0.0rc2' with a custom model. unfortunately my training data is low in hyphens (-), therefore the hyphen often gets tagged as noun.
is there some way to force a certain tag or pos, to make sure that all the  - tokens get tagged with punct?
basically i am looking for a solution like proposed in the answer to this question here:
how to force a pos tag in spacy before/after tagger?
unfortunately this does not seem to work anymore (at least for spacy 3) and raises an error:
valueerror: [e1005] unable to set attribute 'pos' in tokenizer exception for '{g}'. tokenizer exceptions are only allowed to specify orth and norm.

(same when trying to assign the tag attribute)
i know that it would be possible to create a custom component with a matcher that looks just for the hyphen and assigns the right tag. however this seems to be overkill when considering that i currently just want to handle one token.
is there some way to force tags in spacy 3, without re-tagging during processing using a custom component?
ideally i would want to modify the tag attribute and let the pos attribute get assigned automatically by spacy based on that tag attribute.
as in the spacy-annotations tag=hyph should be mapped to pos=punct.","['python', 'nlp', 'spacy']",65701412,"in spacy v3, exceptions like this can be implemented in the attribute_ruler component:
ruler = nlp.add_pipe(""attribute_ruler"")
patterns = [[{""orth"": ""-""}]]
attrs = {""tag"": ""hyph"", ""pos"": ""punct""}
ruler.add(patterns=patterns, attrs=attrs)

be aware that the attribute ruler runs the pattern matching once based on the initial doc state, so you can't use the output attrs of one rule as the input pattern for another. this comes up in pipelines like en_core_web_sm, where the included attribute ruler does the tag->pos mapping. so if you have another rule that should match on a pos pattern, you'd have to add a second attribute ruler component to handle those cases.
see:",https://stackoverflow.com/questions/65699672,python,13-01-2021 10:07,1533.0,2.0,2.0,True,26-04-2021 09:51,13-01-2021 10:52
62031910,textplot::plot.btm(): avoid thick edges and weird colors in cluster visualization,"goal and tools
i currently try to familiarize with the r packages btm and textplot, that is how to create readable and meaningful visualizations of biterm topic models (btm models) created with btm via textplot. textplot::plot.btm() is a method that creates a cluster visualization of models created with btm::btm(). both the documentation of btm::btm() and the documentation of textplot::plot.btm() contain code examples dedicated to plotting. according to the documentation, textplot::plot.btm() returns an object of class ggplot.
previous attempts and observations
after installing the packages mentioned in these examples, namely concaveman, ggraph and igraph, i could successfully replicate these example plots. also, my own plots looked like the examples.
though, when i start with a fresh r session and run my script (see minimal code below), my plots suddenly look different. the colors of the cluster shapes and especially the edges are no longer light pastel shades like in the demo plots, but rather dark and bright shades, e.g. a dark brown that yields a low contrast regarding the black tokens (see this screenshot). besides that, the edges have become extremely thick, so they cover and exceed the cluster shapes underneath. that way, the plot is unreadable and definitely looks broken. very weird.
i have noticed that r outputs something like*
load required namespace: ggraph
load required namespace: concaveman

when i run textplot::plot.btm(), although sometimes only concaveman is mentioned for an unknown reason. calling class() indicates the return value is of class ggraph which inherits from ggplot. it seems to me that these packages are properly installed and used by the function if needed. all involved packages are updated, that is btm 0.3.1 and textplot 0.1.2 are installed to explicitly mention the core package versions.
*i get these in german and translated literally.
my questions

how can i ensure that my plots always look as intended, that is have light pastel shades and adequately sized edges?
why do my plots look differently, that is have bright dark shades and extremly thick edges?
bonus question in terms of readability ;) : how can i ensure that all tokens have a readable font size? i noticed in both my and the example plots, that tokens with a low frequency are very tiny and, thus, hard to read.

many thanks for your help!
this is my first post on stackoverflow ever, so please let me know if i missed best practices of asking questions.
minimal code
library(btm)
library(textplot)
library(udpipe)
    
data(""brussels_reviews_anno"", package = ""udpipe"")
brussel_reviews <- subset(brussels_reviews_anno, language == ""nl"")
brussel_reviews <- subset(brussel_reviews, xpos %in% c(""nn"", ""nnp"", ""nns""))
brussel_reviews <- brussel_reviews[, c(""doc_id"", ""lemma"")]
    
btm_model <- btm(brussel_reviews, k = 5)
    
plot(btm_model, top_n = 15,
     title = ""topic clusters of top 15 biterms"",
     labels = c(""1 - too dark color masking terms"",
                ""2 - looks okish"",
                ""3 - too thick edges"",
                ""4 - too thick edges"",
                ""5 - too thick edges""))","['r', 'plot', 'cluster-computing', 'visualization', 'topic-modeling']",62220854,"i have actually found the answer to question 2 shortly after posting my question, but also got in touch with the developer of the btm package to clarify question 1 and would like to share with you in my own words. though, i might not explain every detail completely accurately, so corrections are welcome.
summary

how correct? attach the ggraph package in every session (see corrected minimal code below).
why wrong? with ggraph attached, the plot is drawn with another method added by ggraph.
how ensure readable tokens? i see no option, because we cannot change that some tokens very rarely cooccur with their neighbor terms. still, a readable minimal display size could be introduced. i will consider a smaller sliding window to avoid rare biterms with e.g. btm::btm(window = 5).

corrected minimal code
library(btm)
library(ggraph)
library(textplot)
library(udpipe)

[...]

details

how? for btm plots to look as expected, you not only have to install the packages
concaveman, ggraph and igraph, but to attach the ggraph package in every session. (of course, also btm is needed to produce btm models to visualize.)
looking at the documentation of btmand textplot, it is striking that ggraph is always attached in any example code. at first, though, i understood the lists as which packages need to be installed, not necessarily to be attached, especially after seeing the package lists vary from example to example:
btm::btm() mentions:

library(igraph)
library(btm)
library(ggraph)

textplot::plot.btm() mentions:
library(textplot)
library(ggraph)
library(concaveman)

textplot::textplot_bitermclusters(), the underlying function, mentions:
library(igraph)
library(ggraph)
library(concaveman)
library(btm)


why? plots look differently with my minimal code, because the package ggraph is not attached. attaching ggraph is not necessary for plot objects to be produced properly by plot.btm(), but for the plot objects to be printed correctly. more precisely, ggraph plots via ggplot2::print.ggplot(), but extends the internal generic function ggplot2::ggplot_build() by a method for ggraph objects, namely ggplot_build.ggraph(). there, the algorithm of how to plot is temporarily changed before calling ggplot_build.ggplot(), yielding the different, more pleasant look.
ensure readable tokens? in textplot::plot.btm(), it seems that the token size depends on how often the token cooccurs with its neighbor tokens within a topic cluster. i would like the option to set a meaningful minimal display size, because i see no point in including tokens i cannot read, especially given that i cannot infer the frequency from the token size that precisely anyway. one thing to try is to narrow the sliding window which biterms are build in via e.g. btm::btm(window = 5). that way, i might avoid too many rare combinations of tokens and, thus, tiny tokens in the plot. i am not sure, though.",https://stackoverflow.com/questions/62031910,r,26-05-2020 21:43,291.0,0.0,1.0,True,03-11-2022 00:47,03-11-2022 00:47
79471648,"when using &#39;interrupt&#39; followed by &#39;new command({ resume: ...})&#39;, get an undefined message error from langchain + langgraph","when i invoke a graph that includes interrupts in one of its nodes, it seems to get into an invalid/ unrecoverable state, with the following error:
        return chatgeneration.message;
                              ^

typeerror: cannot read properties of undefined (reading 'message')
    at chatopenai.invoke (file:///users/user/code/lgdemo//node_modules/@langchain/core/dist/language_models/chat_models.js:64:31)

the first encounter of the interrupt appears to go well, but after the second encounter this occurs.
(i include the minimal code to reproduce this in full below the question text.)
the main logic is in approvenode, which contains the interrupt.

if the user responds with y, it proceeds to the toolsnode which the agentnode requested.
if the user responds with anything else (e.g. n), it proceeds to end

the issue is that once it proceeds to end,
the subsequent call to graph.invoke results in this error.
another thing that i have tried is to change the logic in approvenode such that:

if the user responds with y, it proceeds to the toolsnode which the agentnode requested. (same as before)
if the user responds with anything else (e.g. n), it proceeds to back to agentnode. (this has changed)

(and i change the main graph accordingly to reflect this updated flow)
however, this results in the same error as above, just that it happens after the first interrupt instead of the second interrupt.
questions:

is the workflow that i have defined valid? is there a better way to structure it?
what's the underlying cause of this particular error message? (unfortunately being able to reproduce it hasn't given me clues as to why it occurs)
otherwise, how can i implement this such that i get a simple approve/ deny for tool calls going on?


references used:





main graph:
const workflow = new stategraph(messagesannotation)
  .addnode('agent', agentnode)
  .addnode('tools', toolsnode)
  .addnode('approve', approvenode, {
    ends: ['tools', end],
  })
  .addedge(start, 'agent')
  .addedge('tools', 'agent')
  .addconditionaledges('agent', agentrouter, ['approve', end]);
const checkpointer = new memorysaver();
const graph = workflow.compile({
  checkpointer,
});
const graphconfig = {
  configurable: { thread_id: '0x0004' },
};

tools, nodes, and routers:
const cmdfootool = tool(async function(inputs) {
  console.log('===tool cmd_foo===');
  return inputs.name;
}, {
  name: 'cmd_foo',
  description: 'invoke when you want to do a foo.',
  schema: z.object({
    name: z.string('any string'),
  }),
});
const cmdbartool = tool(async function(inputs) {
  console.log('===tool qry_bar===');
  return inputs.name;
}, {
  name: 'qry_bar',
  description: 'invoke when you want to query a bar.',
  schema: z.object({
    name: z.string('any string'),
  }),
});
const tools = [cmdfootool, cmdbartool];
const llmwithtools = llm.bindtools(tools);

const toolsnode = new toolnode(tools);

async function agentnode(state) {
  console.log('===agent node===');
  const response = await llmwithtools.invoke(state.messages);
  console.log('=response=',
    '\ncontent:', response.content,
    '\ntool_calls:', response.tool_calls.map((toolcall) => (toolcall.name)));
  return { messages: [response] };
}

async function approvenode (state) {
  console.log('===approve node===');
  const lastmsg = state.messages.at(-1);
  const toolcall = lastmsg.tool_calls.at(-1);

  const interruptmessage = `please review the following tool invocation:
${toolcall.name} with inputs ${json.stringify(toolcall.args, undefined, 2)}
do you approve (y/n)`;

  console.log('=interrupt pre=');
  const interruptresponse = interrupt(interruptmessage);
  console.log('=interrupt post=');

  const isapproved = (interruptresponse.trim().charat(0).tolowercase() === 'y');
  const goto = (isapproved) ? 'tools' : end;
  console.log('=result=\n', { isapproved, goto });
  return new command({ goto });
}

function hastoolcalls(message) {
  return message?.tool_calls?.length > 0;
}

async function agentrouter (state) {
  const lastmsg = state.messages.at(-1);
  if (hastoolcalls(lastmsg)) {
    return 'approve';
  }
  return end;
}

simulate a run:
let state;
let agentresult;
let inputtext;
let invokewith;

// step 1: prompt
inputtext = 'pls perform a foo with name ""asdf"".';
console.log('===human prompt===\n', inputtext);
invokewith = { messages: [new humanmessage(inputtext)] };
agentresult = await graph.invoke(invokewith, graphconfig);

state = await graph.getstate(graphconfig);
console.log('===state next===\n', state.next);
console.log('=last msg=\n', agentresult.messages.at(-1).content);
console.log('=last tool calls=\n', agentresult.messages.at(-1).tool_calls);

// step 2: interrupted in the 'approve' node, human in the loop authorises
inputtext = 'yes'
console.log('===human interrupt response===\n', inputtext);
invokewith = new command({ resume: inputtext });
agentresult = await graph.invoke(invokewith, graphconfig);

state = await graph.getstate(graphconfig);
console.log('===state next===\n', state.next);
console.log('=last msg=\n', agentresult.messages.at(-1).content);
console.log('=last tool calls=\n', agentresult.messages.at(-1).tool_calls);

// step 3: prompt
inputtext = 'pls perform a foo with name ""zxcv"".';
console.log('===human prompt===\n', inputtext);
invokewith = { messages: [new humanmessage(inputtext)] };
agentresult = await graph.invoke(invokewith, graphconfig);

state = await graph.getstate(graphconfig);
console.log('===state next===\n', state.next);
console.log('=last msg=\n', agentresult.messages.at(-1).content);
console.log('=last tool calls=\n', agentresult.messages.at(-1).tool_calls);

// step 4: interrupted in the 'approve' node, human in the loop does not authorise
inputtext = 'no';
console.log('===human interrupt response===\n', inputtext);
invokewith = new command({ resume: inputtext });
agentresult = await graph.invoke(invokewith, graphconfig);

state = await graph.getstate(graphconfig);
console.log('===state next===\n', state.next);
console.log('=last msg=\n', agentresult.messages.at(-1).content);
console.log('=last tool calls=\n', agentresult.messages.at(-1).tool_calls);

// step 5: prompt
inputtext = 'pls perform a foo with name ""ghjk"".';
console.log('===human prompt===\n', inputtext);
invokewith = { messages: [new humanmessage(inputtext)] };
agentresult = await graph.invoke(invokewith, graphconfig);

state = await graph.getstate(graphconfig);
console.log('===state next===\n', state.next);
console.log('=last msg=\n', agentresult.messages.at(-1).content);
console.log('=last tool calls=\n', agentresult.messages.at(-1).tool_calls);


full output:
===human prompt===
 pls perform a foo with name ""asdf"".
===agent node===
(node:58990) [dep0040] deprecationwarning: the `punycode` module is deprecated. please use a userland alternative instead.
(use `node --trace-deprecation ...` to show where the warning was created)
=response= 
content:  
tool_calls: [ 'cmd_foo' ]
===approve node===
=interrupt pre=
===state next===
 [ 'approve' ]
=last msg=
 
=last tool calls=
 [
  {
    name: 'cmd_foo',
    args: { name: 'asdf' },
    type: 'tool_call',
    id: 'call_u7ciywdtesfatz5bgg2uavuz'
  }
]
===human interrupt response===
 yes
===approve node===
=interrupt pre=
=interrupt post=
=result=
 { isapproved: true, goto: 'tools' }
===tool cmd_foo===
===agent node===
=response= 
content: the foo operation has been performed with the name ""asdf"". 
tool_calls: []
===state next===
 []
=last msg=
 the foo operation has been performed with the name ""asdf"".
=last tool calls=
 []
===human prompt===
 pls perform a foo with name ""zxcv"".
===agent node===
=response= 
content:  
tool_calls: [ 'cmd_foo' ]
===approve node===
=interrupt pre=
===state next===
 [ 'approve' ]
=last msg=
 
=last tool calls=
 [
  {
    name: 'cmd_foo',
    args: { name: 'zxcv' },
    type: 'tool_call',
    id: 'call_kkf91c8g6enwwlrlfon8tylj'
  }
]
===human interrupt response===
 no
===approve node===
=interrupt pre=
=interrupt post=
=result=
 { isapproved: false, goto: '__end__' }
===state next===
 []
=last msg=
 
=last tool calls=
 [
  {
    name: 'cmd_foo',
    args: { name: 'zxcv' },
    type: 'tool_call',
    id: 'call_kkf91c8g6enwwlrlfon8tylj'
  }
]
===human prompt===
 pls perform a foo with name ""ghjk"".
===agent node===
file:///users/user/code/lgdemo/node_modules/@langchain/core/dist/language_models/chat_models.js:64
        return chatgeneration.message;
                              ^

typeerror: cannot read properties of undefined (reading 'message')
    at chatopenai.invoke (file:///users/user/code/lgdemo//node_modules/@langchain/core/dist/language_models/chat_models.js:64:31)
    at process.processticksandrejections (node:internal/process/task_queues:105:5)
    at async runnablecallable.agentnode [as func] (file:///users/user/code/lgdemo//test.js:51:20)
    at async runnablecallable.invoke (file:///users/user/code/lgdemo//node_modules/@langchain/langgraph/dist/utils.js:79:27)
    at async runnablesequence.invoke (file:///users/user/code/lgdemo//node_modules/@langchain/core/dist/runnables/base.js:1274:33)
    at async _runwithretry (file:///users/user/code/lgdemo//node_modules/@langchain/langgraph/dist/pregel/retry.js:67:22)
    at async pregelrunner._executetaskswithretry (file:///users/user/code/lgdemo//node_modules/@langchain/langgraph/dist/pregel/runner.js:217:33)
    at async pregelrunner.tick (file:///users/user/code/lgdemo//node_modules/@langchain/langgraph/dist/pregel/runner.js:45:40)
    at async compiledstategraph._runloop (file:///users/user/code/lgdemo//node_modules/@langchain/langgraph/dist/pregel/index.js:1296:17)
    at async createandrunloop (file:///users/user/code/lgdemo//node_modules/@langchain/langgraph/dist/pregel/index.js:1195:17) {
  pregeltaskid: '7bd60c12-4beb-54b7-85a7-9bc1461600f5'
}

node.js v23.3.0","['javascript', 'langchain', 'langgraph']",79507858,"i'm an engineer on the langchain team, and what follows is a copy & paste of my response to the same question posed as a github issue on the langgraphjs repository.

i haven't executed your code, but i think that the issue could be that on refusal you're not inserting a toolmessage into the messages state. there are some docs on this here
you can handle this on refusal by returning a command with an update:  field that has a tool message. for example:
async function approvenode (state) {
  console.log('===approve node===');
  const lastmsg = state.messages.at(-1);
  const toolcall = lastmsg.tool_calls.at(-1);

  const interruptmessage = `please review the following tool invocation:
${toolcall.name} with inputs ${json.stringify(toolcall.args, undefined, 2)}
do you approve (y/n)`;

  console.log('=interrupt pre=');
  const interruptresponse = interrupt(interruptmessage);
  console.log('=interrupt post=');

  const isapproved = (interruptresponse.trim().charat(0).tolowercase() === 'y');
  if (isapproved) {
      return new command({ goto: 'tools' });
  }

  // rejection case
  return new command({
    goto: end,
    update: {
      messages: [
        new toolmessage({
          status: ""error""
          content: `the user declined your request to execute the ${toolcall.name} tool, with arguments ${json.stringify(toolcall.args)}`,
          tool_call_id: toolcall.id
        }]
    });
}

also bear in mind that this implementation is not handling parallel tool calls. to handle parallel tool calls you have a few options.

decline to process all tool calls if the user disallows any tool call

you'll need to add one  rejection toolmessage per tool call, as shown above
in this model you might as well also only call interrupt once for the whole batch of calls


allow approved calls to proceed without running denied calls:

two ways to do this:
option 1: process all of the interrupts/approvals in a loop and return a command that routes to tools if any calls are approved (or end if no calls are approved)

to prevent the declined calls from processing, you'll want to use a send object in the goto field and send a copy of the aimessage with the tool calls filtered down to just the approved list.
you'll still need the array of toolmessage in the update field of the command as above - one for each declined call.


option 2: use an array of send in your conditional edge to fan out the tool calls to the tools node (by sending a filtered copy of the aimessage, as mentioned above) and do the interrupt in the tool handler.

without send here you would wind up processing all tool messages in the same node, which would cause the approved tool calls to be reprocessed every time the graph is interrupted after that particular tool call is approved.





here's a hastily-written example of how you could write a wrapper that requires approval for individual tool handlers for use with the ""option 2"" approach mentioned in the last bullet above:
function requiresapproval<toolhandlert extends (...args: unknown[]) => unknown>(toolhandler: toolhandlert) {
  return (...args: unknown[]) => {
    const interruptmessage = `please review the following tool invocation: ${toolhandler.name}(${args.map(json.stringify).join"", ""})`;
    const interruptresponse = interrupt(interruptmessage);
    const isapproved = (interruptresponse.trim().charat(0).tolowercase() === 'y');
    if (isapproved) {
      return toolhandler(..args);
    }
    throw new error(`the user declined your request to execute the ${toolhandler.name} tool, with arguments ${json.stringify(args)}`);
  }
}",https://stackoverflow.com/questions/79471648,javascript,27-02-2025 05:09,383.0,2.0,1.0,True,17-03-2025 08:46,27-02-2025 08:09
78917743,how to process data on gpu instead of ram for this python code?,"i'm currently using the following code to process audio data, but it runs on the ram. i want to offload the processing to the gpu to improve performance.
my code :
def prepare_dataset(batch):
    audio = batch[""audio""]
    batch[""input_features""] = feature_extractor(
        audio[""array""], 
        sampling_rate=audio[""sampling_rate""]
    ).input_features[0]
    batch[""labels""] = tokenizer(batch[""sentence""]).input_ids
    return batch

common_voice = common_voice.map(
    prepare_dataset, 
    remove_columns=common_voice.column_names[""train""], 
    num_proc=1
)

how can i modify this code to utilize the gpu for processing instead of the ram? any guidance or specific changes are much appreciated!","['nlp', 'gpu', 'torch', 'openai-whisper']",78918851,"you can using the following code to process audio data on gpu
import torch
device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
print(device)

def prepare_dataset(batch):
    audio = batch[""audio""]

    input_features = feature_extractor(audio[""array""], sampling_rate=audio[""sampling_rate""]).input_features[0]
    batch[""input_features""] = torch.tensor(input_features).to(device)

    labels = tokenizer(batch[""sentence""]).input_ids
    batch[""labels""] = torch.tensor(labels).to(device)
    return batch

common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[""train""])",https://stackoverflow.com/questions/78917743,nlp,27-08-2024 08:03,64.0,1.0,1.0,True,28-08-2024 04:42,27-08-2024 10:54
77839828,openai assistants api: why does a single question i ask my assistant spend so many tokens?,"i have a nodejs program that connects to openai's assistant api to create messages. i have followed this documentation from openai to create the steps below:

i have created an assistant (gpt-4-1106-preview) and a thread in that assistant that i'm accessing to interact with.
add a message to the thread. the message contains around 1000 tokens, checked via 

    openai.beta.threads.messages.create(threadid, {
        role: ""user"",
        content: createmessage(),
    });


run the assistant

    await openai.beta.threads.runs.create(threadid, {
        assistant_id: assistantid,
        instructions:
             ""please address the user as mahesh. the user is an administrator."",
    });


check the status. i'm running this every 5 seconds until the status is ""completed""

    await openai.beta.threads.runs.retrieve(threadid, runid);


get the last response from the assistant

   const messages = await openai.beta.threads.messages.list(threadid, {
      limit: 1,
   });

this code takes around 250,000 tokens to complete. the image shows today's token usage for three requests.","['openai-api', 'gpt-4', 'openai-assistants-api']",77841022,"there could be multiple reasons why your cost of running an assistant is very high.
what openai model do you use?
if you take a look at the official openai documentation, you'll see that they use the gpt-4-1106-preview model. they state:

we recommend using openaiï¿½ï¿½ï¿½s latest models with the assistants api for
best results and maximum compatibility with tools.

but older models might be good enough. it depends on what your assistant is used for. you can lower the cost of running the assistant just by changing the model. of course, if you see that the performance of the assistant is considerably worse, then you need to use the latest models. just take a look at the table below to see what a difference a model decision can make:



model
input
output




gpt-4-1106-preview
$0.01 / 1k tokens
$0.03 / 1k tokens


gpt-3.5-turbo-1106
$0.001 / 1k tokens
$0.002 / 1k tokens



how long have you been using the same thread?
as stated in the official openai documentation:

assistants can access persistent threads. threads simplify ai
application development by storing message history and truncating it
when the conversation gets too long for the modelï¿½ï¿½ï¿½s context length.
you create a thread once, and simply append messages to it as your
users reply.
/ ... /
threads and messages represent a conversation session between an
assistant and a user. there is no limit to the number of messages you
can store in a thread. once the size of the messages exceeds the
context window of the model, the thread will attempt to include as
many messages as possible that fit in the context window and drop the
oldest messages.

the tread is storing the message history! the gpt-4-1106-preview has a context window of 128,000 tokens. so, if you chat with your assistant using the same thread long enough, you will fill up the thread up to the context window of your chosen model.
if you chhe gpt-4-1106-preview this means that after some time chatting with your assistant using the same thread, a single question you ask your assistant means that you used 128,000 tokens. your recent question might contain 1,000 tokens, but you also need to keep in mind that hundreds of messages that were either asked by you or answered by the assistant in the past were also sent to the assistants api.
in your case, you can see that today you spent 760,564 context tokens. you have probably been using the same thread for quite some time.
how often do you check the run status?
you said that you check the run status to see if it has been moved to completed every 5 seconds. try to increase this number, let's say 10 seconds, to make fewer api calls. you pay for every api call you make.",https://stackoverflow.com/questions/77839828,openai-api,18-01-2024 13:39,2072.0,0.0,1.0,True,10-04-2024 10:18,22-01-2024 16:00
78919818,embedding using the langchain_aws is giving none value,"i am trying to embed a text using the langchain_aws bedrockembeddings, but when i invoke the function, i get a list with the none values.
here's the code:
from langchain_community.llms.bedrock import bedrock 
from langchain_aws import bedrockembeddings
import boto3

# initialize the bedrock client
bedrock_client = boto3.client(service_name='bedrock-runtime')

# initialize bedrock embeddings
bedrock_embeddings = bedrockembeddings(
    model_id=""amazon.titan-text-express-v1"",
    credentials_profile_name=""default"",
    client=bedrock_client,
    region_name=""ap-south-1""
)


embed_data=bedrock_embeddings.embed_documents([""this is a content of the document"", ""this is another document""])

print(embed_data)


output:
[none, none]","['python', 'langchain', 'large-language-model', 'embedding', 'amazon-bedrock']",78924664,"amazon.titan-text-express-v1 is not embedding model. its a text generation model.
use amazon.titan-embed-text-v2:0",https://stackoverflow.com/questions/78919818,python,27-08-2024 16:05,266.0,0.0,1.0,True,29-08-2024 09:58,29-08-2024 09:58
29989464,how do i remove 1 instance of x characters in a string and find the word it makes in python3?,"this is what i have so far, but i'm stuck.  i'm using nltk for the word list and trying to find all the words with the letters in ""sand"".  from this list i want to find all the words i can make from the remaining letters.
import nltk.corpus.words.words()
pwordlist = []

for w in wordlist:
    if 's' in w:
        if 'a' in w:
            if 'n' in w:
                if 'd' in w:
                    pwordlist.append(w)

in this case i have to use all the letters to find the words possible.
i think this will work for finding the possible words with the remaining letters, but i can't figure out how to remove only 1 instance of the letters in 'sand'.
puzzle_letters = nltk.freqdist(x)

[w for w in pwordlist if len(w) = len(pwordlist) and nltk.freqdist(w) = puzzle_letters]","['python', 'python-3.x', 'nltk']",29990395,"i would separate the logic into four sections:

a function contains(word, letters), which we'll use to detect whether a word contains ""sand""
a function subtract(word, letters), which we'll use to remove ""sand"" from the word.
a function get_anagrams(word), which finds all of the anagrams of a word.
the main algorithm that combines all of the above to find words that are anagrams of other words once you remove ""sand"".

ï¿½ï¿½
from collections import counter

words = ??? #todo: somehow get a list of every english word.

def contains(word, letters):
    return not counter(letters) - counter(word)

def subtract(word, letters):
    remaining = counter(word) - counter(letters)
    return """".join(remaining.elements())

anagrams = {}
for word in words:
    base = """".join(sorted(word))
    anagrams.setdefault(base, []).append(word)
def get_anagrams(word):
    retanagrams.get("""".join(sorted(word)), [])

for word in words:
    if contains(word, ""sand""):
        reduced_word = subtract(word, ""sand"")
        matches = get_anagrams(reduced_word)
        if matches:
            print word, matches

running the above code on the words with friends dictionary, i get a lot of results, including:
...
cowhands ['chow']
credentials ['reticle', 'tiercel']
cyanids ['icy']
daftness ['efts', 'fest', 'fets']
dahoons ['oho', 'ooh']
daikons ['koi']
daintiness ['seniti']
daintinesses ['sienites']
dalapons ['opal']
dalesman ['alme', 'lame', 'male', 'meal']
...",https://stackoverflow.com/questions/29989464,python,01-05-2015 15:07,156.0,1.0,3.0,True,26-03-2022 17:48,01-05-2015 16:16
79300067,reproducing huggingface clip&#39;s output for the text encoder,"i am trying to de-compose clip's text_model from huggingface but i'm running into some issues i don't understand.
in particular, as far as i understand calling clip.text_model should be the same as:

calculating the text embeddings with clip.text_model.embeddings
feeding the embeddings to clip.text_model.encoder
using clip.text_model.final_layer_norm

but when i try to compare the outputs i get different values for the two approaches.
here is my code so far:
device = ""cuda"" if torch.cuda.is_available() else ""cpu""

model = clipmodel.from_pretrained(""openai/clip-vit-base-patch32"")
model = model.to(device)
processor = clipprocessor.from_pretrained(""openai/clip-vit-base-patch32"")

def decomposed_text_model(text, processor, model, device):
    inputs = processor(text=text, return_tensors=""pt"", padding=true)
    attn_mask = inputs[""attention_mask""].clone().detach().to(torch.bool).to(device)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    embeddings = model.text_model.embeddings(inputs[""input_ids""])
    position_embeddings=model.text_model.embeddings.position_embedding.weight[:inputs['input_ids'].shape[1]]
    embeddings = embeddings + position_embeddings.unsqueeze(0)
    encoder_output = model.text_model.encoder(
        inputs_embeds=embeddings, 
        attention_mask=attn_mask).last_hidden_state

    embeddings = model.text_model.final_layer_norm(encoder_output)

    return embeddings

def text_model(text, processor, model):
  inputs = processor(text=""a photo of a cat"", return_tensors=""pt"")
  inputs = {k: v.to(device) for k, v in inputs.items()}
  return model.text_model(**inputs)

# two step text approach
out1 = decomposed_text_model(""a photo of a cat"", processor, model)

out1 = out1.last_hidden_state[0, -1, :] # get eos token
out1 = out1.squeeze()

# one step text approach
out2 = text_model(""a photo of a cat"", processor, model)
out2 = out2.last_hidden_state[0, -1, :] # get eos token
out2 = out2.squeeze()

# compare
out1 = out1 / out1.norm(p = 2, dim=-1, keepdim=true)
out2 = out2 / out2.norm(p = 2, dim=-1, keepdim=true)

diff = torch.max(torch.abs(out1 - out2))
print(diff)

with diff being a somewhat high number (more fine-grained logging also revealed significant differences between the two eos tensors).
what am i missing? please understand that this approach is necessary to implement something, so i cannot just call text_model.","['python-3.x', 'huggingface-transformers', 'huggingface', 'clip']",79422353,"there are a few errors in your code, but the most important step you forgot to reproduce the logic of the clip text model is the use of the 4d-causualmask. you can find the relevant code here and your code should look as follows:
import torch
from transformers import clipmodel, cliptokenizerfast
from transformers.modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask 

device = ""cuda"" if torch.cuda.is_available() else ""cpu""

m = clipmodel.from_pretrained(""openai/clip-vit-base-patch32"")
m = m.to(device)
t = cliptokenizerfast.from_pretrained(""openai/clip-vit-base-patch32"")

text = ""a photo of a cat""
inputs = t(text, return_tensors=""pt"")
inputs.to(device)

@torch.no_grad()
def decomposed_text_model(inputs, model):    
    input_ids = inputs['input_ids']
    attention_mask = inputs['attention_mask']
    input_shape = input_ids.size()
    hidden_states = model.text_model.embeddings(input_ids=input_ids)
    
    causal_attention_mask = _create_4d_causal_attention_mask(
        input_shape, hidden_states.dtype, device=hidden_states.device
    )
    attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)
    
    encoder_output = model.text_model.encoder(
        inputs_embeds=hidden_states, 
        attention_mask=attention_mask,
        causal_attention_mask=causal_attention_mask
        ).last_hidden_state

    embeddings = model.text_model.final_layer_norm(encoder_output)

    return embeddings

@torch.no_grad()
def text_model(inputs, model):
  return model.text_model(**inputs)

# two step text approach
out1 = decomposed_text_model(inputs, m)

out1 = out1[0, -1, :] # get eos token
out1 = out1.squeeze()

# one step text approach
out2 = text_model(inputs, m)
out2 = out2.last_hidden_state[0, -1, :] # get eos token
out2 = out2.squeeze()

# compare
print(torch.allclose(out1, out2))

output:
true",https://stackoverflow.com/questions/79300067,python-3.x,21-12-2024 20:36,89.0,2.0,1.0,True,07-02-2025 23:15,07-02-2025 23:10
77811059,openai api error: &quot;nameerror: name &#39;client&#39; is not defined&quot;,"this is my code:
import telebot
import openai
 
bot = telebot.telebot(""0"")
openai.api_key = ""0""
 
@bot.message_handler(content_types=['text'])
def lalala(message):
    print(message.chat.title, message.chat.username)
    if message.chat.id == -1002097745017:
    #print(message.text)
        if ""@0"" in message.text:
            message.text = (message.text).replace(""@0 "", """")
            #print(message.text)
            response = client.completions.create(model=""gpt-3.5-turbo-0613"", prompt=message.text, max_tokens=1000)
            full_response = response['choices'][0]['text']  # use the text property of the first element of the choices list to access the full response
            lines = full_response.splitlines()  # split the response into individual lines
            for line in lines:  # iterate over the lines
                try:
                    #print(line)
                    bot.send_message(message.chat.id, line)  # send each line back to the user as a separate message
                except exception as e:
                    print(e)
    else:
        bot.send_message(message.chat.id, ""work only - tg.com/123123"")
 
bot.polling(none_stop=true, interval=0)

i'm getting the following error:

nameerror: name 'client' is not defined

if i change this...
response = client.completions.create(model=""gpt-3.5-turbo-0613"", prompt=message.text, max_tokens=1000)

...to this.
response = openai.completion.create(model=""text-davinci-003"", prompt=message.text, max_tokens=1000)

then i'm getting the following error:

this is a chat model and not supported in the v1/completions endpoint. did you mean to use v1/chat/completions?","['python', 'telegram', 'openai-api', 'gpt-3']",77811071,"you tried multiple combinations, but not the right one.

use openai not client, because you didn't initialize openai with client, but with openai.
use completions.create method name, not completion.create if you're using openai python sdk version >=1.0.0.
use the completions model, not the chat completions model. while the text-davinci-003 is a completions model, it will not work because it was deprecated not long ago. use gpt-3.5-turbo-instruct instead, which is the recommended replacement for the text-davinci-003.

the following should work:
response = openai.completions.create(model=""gpt-3.5-turbo-instruct"", prompt=message.text, max_tokens=1000)",https://stackoverflow.com/questions/77811059,python,13-01-2024 10:24,3655.0,0.0,1.0,True,14-01-2024 11:32,14-01-2024 11:32
65779837,importerror caused by file with the same name in working dir and file from imported package,"i've bumped into an issue when trying to run a python script and for simplicity let's call it my_tokenizer.py and its content is just importing hugging face's transformers. unfortunately, trying to run it from the working directory leads to importerror and it seems it is caused due to the name of the file that is in the working directory and has the same name as the file that transformer package uses somewhere in its internals.
having 2 files in the working directory:

/project/my_tokenizer.py (contains only line with import ""import transformers"")
/project/tokenizers.py (empty file)

and running python my_tokenizer.py leads to following importerror:
traceback (most recent call last):
  file ""project/my_tokenizer.py"", line 1, in <module>
    import transformers
  file ""/users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/__init__.py"", line 54, in <module>
    from .data import (
  file ""/users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/data/__init__.py"", line 6, in <module>
    from .processors import (
  file ""/users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/data/processors/__init__.py"", line 5, in <module>
    from .glue import glue_convert_examples_to_features, glue_output_modes, glue_processors, glue_tasks_num_labels
  file ""/users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/data/processors/glue.py"", line 24, in <module>
    from ...tokenization_utils import pretrainedtokenizer
  file ""/users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/tokenization_utils.py"", line 26, in <module>
    from .tokenization_utils_base import (
  file ""/users/radoslawslowinski/opt/anaconda3/envs/aa_ee/lib/python3.8/site-packages/transformers/tokenization_utils_base.py"", line 31, in <module>
    from tokenizers import addedtoken
importerror: cannot import name 'addedtoken' from 'tokenizers' (/users/radoslawslowinski/project/tokenizers.py)


although i could just rename my file from project/tokenizers.py to something else, i'd like to know why it occurs.","['python', 'python-3.x', 'huggingface-transformers', 'huggingface-tokenizers']",65787421,"i think i've understood what causes the issue - it is shadowing file with the same name in package transformer (that internally import another package called tokenizers) with my local file called tokenizers.py.
it is so because my working directory is first on list of paths that will be searched to find imports. it can be checked with:
import sys
print(sys.path)
from transformers import basictokenizer

and to proof that search for imports starts from directory in which you call script, you can move first sys.path to the end of the list and the following code will work:
import sys
sys.path = sys.path[1:] + sys.path[:1]
import transformers",https://stackoverflow.com/questions/65779837,python,18-01-2021 18:05,593.0,1.0,2.0,True,04-01-2025 12:02,19-01-2021 08:21
76357844,how to extract subtitles from youtube videos in varied languages,"i have used the code below to extract subtitles from youtube videos, but it only works for videos in english. i have some videos in spanish, so i would like to know how i can modify the code to extract spanish subtitles too?
from pytube import youtube
from youtube_transcript_api import youtubetranscriptapi

# define the video url or id of the youtube video you want to extract text from
video_url = '

# download the video using pytube
youtube = youtube(video_url)
video = youtube.streams.get_highest_resolution()
video.download()

# get the downloaded video file path
video_path = video.default_filename

# get the video id from the url
video_id = video_url.split('v=')[-1]

# get the transcript for the specified video id
transcript = youtubetranscriptapi.get_transcript(video_id)

# extract the text from the transcript
captions_text = ''
for segment in transcript:
    caption = segment['text']
    captions_text += caption + ' '

# print the extracted text
print(captions_text)","['python', 'web-scraping', 'nlp', 'youtube', 'video-streaming']",76358528,"use - list_transcripts - for get the list of available languages:
example:
video_id = 'xygoniso-ky'
transcript_list = youtubetranscriptapi.list_transcripts(video_id)

then, loop the transcript_list variable to see the available languages obtained:
example:
for x, tr in enumerate(transcript_list):
  print(tr.language_code)

in this case, the result is:

es

modify your code for loop the languages available on the video and download the generated captions:
example:
# variables for store the downloaded captions:
all_captions = []
caption = none
captions_text = ''

# loop all languages available for this video and download the generated captions:
for x, tr in enumerate(transcript_list):
  print(""downloading captions in "" + tr.language + ""..."")
  transcript_obtained_in_language = transcript_list.find_transcript([tr.language_code]).fetch()
  for segment in transcript_obtained_in_language:
    caption = segment['text']
    captions_text += caption + ' '
  all_captions.append({""language "" : tr.language_code + "" - "" + tr.language, ""captions"" : captions_text})
  caption = none
  captions_text = ''
  print(""=""*20)
print(""done"")

in the all_captions variable, will be stored the captions and the language obtained from the given video_id.",https://stackoverflow.com/questions/76357844,python,29-05-2023 13:35,1188.0,2.0,2.0,True,01-09-2024 14:48,30-05-2023 05:47
71923159,how to get output_attentions of a pretrained distilbert model?,"i am using a pretrained distilbert model:
from transformers import tfdistilbertmodel,distilbertconfig

dbert = 'distilbert-base-uncased'

config = distilbertconfig(max_position_embeddings=256 , dropout=0.2, 
                          attention_dropout=0.2, 
                          output_hidden_states=true,
                          output_attentions=true) #or true

dbert_model = tfdistilbertmodel.from_pretrained(dbert, config)

input_ids_in = tf.keras.layers.input(shape=(256,), name='input_id', dtype='int32')
input_masks_in = tf.keras.layers.input(shape=(256,), name='attn_mask', dtype='int32') 

outputs = dbert_model([input_ids_in, input_masks_in], output_attentions = 1)

i am trying to get the output_attentions. but the output is of length 1 and is given as:

tfbasemodeloutput([('last_hidden_state',
<kerastensor: shape=(none, 256, 768) dtype=float32 (created by layer 'tf_distil_bert_model_6')>)])

i have given ""output_attentions = true"" in config and in forward pass ""output_attentions = 1"" is specified. can anyone let me know what i am doing wrong?
edit:
i have changed the default configuration value of max_positional_embeddings  of 512 to 256. if i change my model instantiation to
dbert_model = tfdistilbertmodel.from_pretrained('distilbert-base-uncased',config=config)

it gives me the following error.
valueerror: cannot reshape array of size 393216 into shape (256,768)

768*512 being 393216. so it might be related to config code.
any ideas?","['python', 'tensorflow', 'tf.keras', 'huggingface-transformers', 'distilbert']",71949300,"i am posting the answer as @cronoik suggested: i modified the code as  dbert_model = tfdistilbertmodel.from_pretrained('distilbert-base-uncased',config, output_attentions=true) this gave both hidden states and attention in output.",https://stackoverflow.com/questions/71923159,python,19-04-2022 10:06,1127.0,2.0,1.0,True,21-04-2022 05:20,20-04-2022 07:05
62386631,cannot import bertmodel from transformers,"i am trying to import bertmodel from transformers, but it fails. this is code i am using
from transformers import bertmodel, bertformaskedlm

this is the error i get
importerror: cannot import name 'bertmodel' from 'transformers'

can anyone help me fix this?","['python', 'nlp', 'pytorch', 'huggingface-transformers', 'bert-language-model']",62386694,"fixed the error. this is the code
from transformers.modeling_bert import bertmodel, bertformaskedlm",https://stackoverflow.com/questions/62386631,python,15-06-2020 10:47,22809.0,4.0,4.0,True,26-04-2023 13:48,15-06-2020 12:44
77540512,how to extract calculations using tf-idf,"i used tfidfvectorizer to extract tf-idf but don't know how it calculates the results. when i calculate it manually, i get a different answer, so i want to extract the values ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½that the function calculates in order to learn how it works.
data = ['souvenir shop|architecture and art|culture and history', 'souvenir shop|resort|diverse cuisine|fishing|folk games|beautiful scenery', 'diverse cuisine|resort|beautiful scenery']

vectorizer = tfidfvectorizer()
tfidf_matrix = vectorizer.fit_transform(data)
</","['python', 'tf-idf', 'tfidfvectorizer']",77541207,"have a look in the scikit documentation at the attributes section.
try this:
print(vectorizer.vocabulary_)

output
{'souvenir': 14,
 'shop': 13,
 'architecture': 1,
 'and': 0,
 'art': 2,
 'culture': 5,
 'history': 10,
 'resort': 11,
 'diverse': 6,
 'cuisine': 4,
 'fishing': 7,
 'folk': 8,
 'games': 9,
 'beautiful': 3,
 'scenery': 12}

you get the idf calculations with print(vectorizer.idf_)
output
array([1.69314718, 1.69314718, 1.69314718, 1.28768207, 1.28768207,
       1.69314718, 1.28768207, 1.69314718, 1.69314718, 1.69314718,
       1.69314718, 1.28768207, 1.28768207, 1.28768207, 1.28768207])

for your case you can do this (with pandas):
df_idf = pd.dataframe(
    vectorizer.idf_, index=vectorizer.get_feature_names_out(), columns=[""idf_weights""]
)

display(df_idf)

output
             idf_weights
and          1.693147
architecture 1.693147
art          1.693147
beautiful    1.287682
cuisine      1.287682
culture      1.693147
diverse      1.287682
fishing      1.693147
folk         1.693147
games        1.693147
history      1.693147
resort       1.287682
scenery      1.287682
shop         1.287682
souvenir     1.287682",https://stackoverflow.com/questions/77540512,python,24-11-2023 03:02,97.0,0.0,1.0,True,24-11-2023 19:37,24-11-2023 19:37
75888473,equivalent of apache lucene &quot;proximity searches&quot; in r,"i'm working on a corpus of documents (clinical narratives from hospital stays), mainly using the quanteda package.
the objective is to be able to classify documents based on the presence/absence of a feature, let's say ""spastic cough"".
i would like to be able to reproduce the behaviour of an apache lucene ""proximity search"" ( using r.
let's take an example:
""spastic and productive cough in a 91-year-old patient following femoral neck surgery""
i would begin tokenizing the phrase as follows:
toks = 
tokens(
c(text1 = ""spastic and productive cough in a 91-year-old patient following femoral neck surgery""), 
remove_punct = t, remove_symbols = t, remove_numbers = t, padding = t
) %>% 
tokens_remove(pattern = stopwords(""en"",source = ""nltk""))

which yields the following output:
tokens consisting of 1 document.
text1 :
[1] ""spastic""     ""productive""  ""cough""       ""91-year-old"" ""patient""     ""following""   ""femoral""    
[8] ""neck""        ""surgery"" 

i can then proceed to generate n-grams and skip-grams:
toks = tokens_ngrams(toks,n=4,skip = 0:3)

toks
[1] ""spastic_productive_cough_91-year-old""     ""spastic_productive_cough_patient""        
  [3] ""spastic_productive_cough_following""       ""spastic_productive_cough_femoral""        
  [5] ""spastic_productive_91-year-old_patient""   ""spastic_productive_91-year-old_following""
  [7] ""spastic_productive_91-year-old_femoral""   ""spastic_productive_91-year-old_neck""     
  [9] ""spastic_productive_patient_following""     ""spastic_productive_patient_femoral""      
 [11] ""spastic_productive_patient_neck""          ""spastic_productive_patient_surgery""      
 [13] ""spastic_productive_following_femoral""     ""spastic_productive_following_neck""       
 [15] ""spastic_productive_following_surgery""     ""spastic_cough_91-year-old_patient""       
 [17] ""spastic_cough_91-year-old_following""      ""spastic_cough_91-year-old_femoral""       
 [19] ""spastic_cough_91-year-old_neck""           ""spastic_cough_patient_following""         
 [21] ""spastic_cough_patient_femoral""            ""spastic_cough_patient_neck""              
 [23] ""spastic_cough_patient_surgery""            ""spastic_cough_following_femoral""         
 [25] ""spastic_cough_following_neck""             ""spastic_cough_following_surgery""         
 [27] ""spastic_cough_femoral_neck""               ""spastic_cough_femoral_surgery""           
 [29] ""spastic_91-year-old_patient_following""    ""spastic_91-year-old_patient_femoral""     
 [31] ""spastic_91-year-old_patient_neck""         ""spastic_91-year-old_patient_surgery""     
.........

at this point i guess i could simply:
any(str_detect(as.character(toks),""spastic_cough""))
[1] true

but i'm not sure i'm using the correct approach as it feels clunky compared to how a lucene query would work. if i were trying to identify patients with ""spastic cough"" using apache lucene to query the corpus i may use something like ""spastic cough""~3 where ""~3"" means that any skip-gram 0:3 would match.
any input about how and where i could improve my method?
edit:
this may do the trick: 
but, at the moment, i can't figure out how to include it in the workflow.
edit 2:
it seems like i can query the corpus using subset_query using a lucene like syntax. the big problem i'm facing now is that ""corpustools"" isn't accepting as input tokens object and the function tokens_to_corpus() isn't working for me. this prevents me from being able to control the tokenization process","['r', 'nlp', 'lucene', 'bioinformatics', 'quanteda']",75919543,"actually, after delving deeper into the documentation, the ""corpustools"" package offers all i need for an apache lucene like experience in r =)",https://stackoverflow.com/questions/75888473,r,30-03-2023 12:46,80.0,0.0,1.0,True,03-04-2023 12:34,30-03-2023 14:31
60492839,how to compare sentence similarities using embeddings from bert,"i am using the huggingface transformers package to access pretrained models. as my use case needs functionality for both english and arabic, i am using the bert-base-multilingual-cased pretrained model. i need to be able to compare the similarity of sentences using something such as cosine similarity. to use  this, i first need to get an embedding vector for each sentence, and can then compute the cosine similarity.
firstly, what is the best way to extratc the semantic embedding from the bert model? would taking the last hidden state of the model after being fed the sentence suffice?
import torch
from transformers import bertmodel, berttokenizer

model_class = bertmodel
tokenizer_class = berttokenizer
pretrained_weights = 'bert-base-multilingual-cased'

tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)

sentence = 'this is a test sentence'

input_ids = torch.tensor([tokenizer.encode(sentence, add_special_tokens=true)])
with torch.no_grad():
    output_tuple = model(input_ids)
    last_hidden_states = output_tuple[0]

print(last_hidden_states.size(), last_hidden_states)

secondly, if this is a sufficient way to get embeddings from my sentence, i now have another problem where the embedding vectors have different lengths depending on the length of the original sentence. the shapes output are [1, n, vocab_size], where n can have any value. 
in order to compute two vectors' cosine similarity, they need to be the same  length. how can i do this here? could something as naive as first summing across axis=1 still work? what other options do i have?","['python', 'vector', 'nlp', 'cosine-similarity', 'huggingface-transformers']",60493083,"you can use the [cls] token as a representation for the entire sequence. this token is typically prepended to your sentence during the preprocessing step. this token that is typically used for classification tasks (see figure 2 and paragraph 3.2 in the bert paper).
it is the very first token of the embedding.
alternatively you can take the average vector of the sequence (like you say over the first(?) axis), which can yield better results according to the huggingface documentation (3rd tip).
note that bert was not designed for sentence similarity using the cosine distance, though in my experience it does yield decent results.",https://stackoverflow.com/questions/60492839,python,02-03-2020 16:20,30439.0,30.0,5.0,True,25-01-2023 07:38,02-03-2020 16:25
15388831,what are all possible pos tags of nltk?,how do i find a list with all possible pos tags used by the natural language toolkit (nltk)?,"['python', 'nltk']",15389153,"the book has a note how to find help on tag sets, e.g.:
nltk.help.upenn_tagset()

others are probably similar. (note: maybe you first have to download tagsets from the download helper's models section for this)",https://stackoverflow.com/questions/15388831,python,13-03-2013 14:59,163448.0,202.0,9.0,True,24-05-2023 12:32,24-05-2023 12:30
41879957,stanford corenlp and emoji?,"so far when i tried to use emoji and using the pos tagger, it appeared as unknown symbols, small boxes. is there a way to get the pos tagger to work with emoji? emoji (eg ï¿½ï¿½ï¿½ï¿½) the unicode versio","['java', 'nlp', 'stanford-nlp']",41885643,"provided the character encoding is correct throughout your code, system and the stanford corenlp code, emoji should be represented correctly. however, you'll have two more fundamental problems:
first, emoji are one character long and they are unlikely to be tagged as anything other than an indefinite article. 'a' in english. a smart tokenizer might make better sense of emoji, but i doubt it.
secondly, and more importantly, pos taggers annotate parts of speech. emoji are not a part of speech. in the very least, they are an independent, new class of tokens, but certainly not grammatical.
all that said ... you know their character codes ... they're already tagged.",https://stackoverflow.com/questions/41879957,java,26-01-2017 17:59,693.0,1.0,2.0,True,18-09-2023 21:16,18-09-2023 21:16
68417246,"affinity propagation did not converge, this model will not have any cluster centers","when i try to cluster using affinity propagation, the below error occurs and the number of clusters is one.
""...\anaconda\lib\site-packages\sklearn\cluster\_affinity_propagation.py:246: convergencewarning: affinity propagation did not converge, this model will not have any cluster centers.
  warnings.warn(""affinity propagation did not converge, this model """"

below is the code i tried.
def build_feature_matrix(documents, feature_type='frequency',
                         ngram_range=(1, 1), min_df=0.0, max_df=1.0):

    feature_type = feature_type.lower().strip()  
    
    if feature_type == 'binary':
        vectorizer = countvectorizer(binary=true, min_df=min_df,
                                     max_df=max_df, ngram_range=ngram_range)
    elif feature_type == 'frequency':
        vectorizer = countvectorizer(binary=false, min_df=min_df,
                                     max_df=max_df, ngram_range=ngram_range)
    elif feature_type == 'tfidf':
        vectorizer = tfidfvectorizer(min_df=min_df, max_df=max_df, 
                                     ngram_range=ngram_range)
    else:
        raise exception(""wrong feature type entered. possible values: 'binary', 'frequency', 'tfidf'"")

    feature_matrix = vectorizer.fit_transform(documents).astype(float)
    
    return vectorizer, feature_matrix

vectorizer, feature_matrix = build_feature_matrix(filtered_list_6,
                                                  feature_type='tfidf',
                                                  min_df=0.15, max_df=0.85,
                                                  ngram_range=(1, 2))

def affinity_propagation(feature_matrix):
    
    sim = feature_matrix * feature_matrix.t
    sim = sim.todense()
    ap = affinitypropagation()
    ap.fit(sim)
    clusters = ap.labels_          
    return ap, clusters

ap_obj, clusters = affinity_propagation(feature_matrix=feature_matrix)
df[len(df.columns)] = clusters

c = counter(clusters)   
print(c.items())

total_clusters = len(c)
print('total clusters:', total_clusters)

could someone point what i am doing wrong here?
thanks in advance!","['python', 'text', 'nlp', 'data-science']",70085670,"i could change the damping value, max_iter and preference values to eliminate the issue. initially you can start with damping = 0.9, max_iter = 1000.
you can change the preference value as needed and this will change the number of clusters generated by the model",https://stackoverflow.com/questions/68417246,python,17-07-2021 03:50,3471.0,0.0,3.0,True,15-02-2023 16:29,18-07-2021 07:39
66441952,tokenize list of strings without comma separation,"i'm still new to python and want to know how i can tokenize a list of strings without every word being separated by a comma.
for example, starting from a list like ['i have to get groceries.','i need some bananas.','anything else?'], i want to obtain a list like this: ['i have to get groceries .', 'i need some bananas .', 'anything else ?']. the point is thus not to create a list with separate tokens necessarily, but to create a list with sentences in which all words and punctuation marks are separated from each other.
any ideas? i only managed to create a list of comma-separated tokens, using this code:
import nltk
nltk.download('punkt')
from nltk import word_tokenize 
tokenized = []
for line in unique:
      tokenized.append(word_tokenize(line))","['python', 'nlp', 'tokenize']",66442075,"you can join the tokenized lines with a space, just use
from nltk import word_tokenize
unique = ['i have to get groceries.','i need some bananas.','anything else?']
tokenized = ["" "".join(word_tokenize(line)) for line in unique]
print(tokenized)
# => ['i have to get groceries .', 'i need some bananas .', 'anything else ?']",https://stackoverflow.com/questions/66441952,python,02-03-2021 15:09,363.0,1.0,1.0,True,02-05-2023 16:50,02-05-2023 16:50
76213873,how to finetune a zero-shot model for text classification,"i need a model that is able to classify text for an unknown number of classes (i.e. the number might grow over time). the entailment approach for zero-shot text classification seems to be the solution to my problem, the model i tried facebook/bart-large-mnli doesn't perform well on my annotated data. is there a way to fine-tune it without losing the robustness of the model?
my dataset looks like this:
# 
world, ""afghan army dispatched to calm violence kabul, afghanistan - government troops intervened in afghanistan's latest outbreak of deadly fighting between warlords, flying from the capital to the far west on u.s. and nato airplanes to retake an air base contested in the violence, officials said sunday...""
sports, ""johnson helps d-backs end nine-game slide (ap) ap - randy johnson took a four-hitter into the ninth inning to help the arizona diamondbacks end a nine-game losing streak sunday, beating steve trachsel and the new york mets 2-0."" 
business, ""retailers vie for back-to-school buyers (reuters) reuters - apparel retailers are hoping their\back-to-school fashions will make the grade among\style-conscious teens and young adults this fall, but it could\be a tough sell, with students and parents keeping a tighter\hold on their wallets.""

p.s.: this is an artificial question that was created because this topic came up in the comment section of this post which is related to this post.","['python', 'nlp', 'huggingface-transformers']",76213874,"concept explanation
before i answer your question, it is crucial to understand how the entailment approach for zero-shot text classification works. this approach requires a model that was trained for nli, which means, that it is able to determine if the hypothesis is:

supported,
not supported,
undetermined

by a given premise [1]. you can verify that for the model you mentioned with the following code:
from transformers import automodelforsequenceclassification, autotokenizer
nli_model = automodelforsequenceclassification.from_pretrained('facebook/bart-large-mnli')
# it will output three logits
print(nli_model.classification_head.out_proj)
# each vector corresponds to the following labels
print(nli_model.config.id2label)

output:
linear(in_features=1024, out_features=3, bias=true)
{0: 'contradiction', 1: 'neutral', 2: 'entailment'}

the entailment approach, proposed by yin et. al, utilizes these nli capabilities by using the text as premise and formulating a hypothesis for each possible class with the template:
""the text is about {}ï¿½ï¿½ï¿½

that means when you have a text and three potential classes, you will pass three sequences to the nli model and compare the entailment logits to classify the text.
finetuning
to fine-tune an nli model on your annotated data, you, therefore, need to formulate your text classification task as an nli task! that means, you need to generate premises and the labels need to be either contradiction or entailment. the contradiction label is included to avoid the model only seeing hypotheses that are entailed by their respective premise (i.e. the model needs to learn contraction to predict a low score for entailment for the zero-shot text classification task).
the following code shows you an example of how to prepare your dataset:
import random
from datasets import load_dataset
from transformers import  autotokenizer

your_dataset = load_dataset(""ag_news"", split=""test"")
id2labels = [""world"", ""sports"", ""business"", ""sci/tech""]
your_dataset = your_dataset.map(lambda x: {""class"": id2labels[x[""label""]]}, remove_columns=[""label""])

print(your_dataset[0])

# the relevant code
t = autotokenizer.from_pretrained('facebook/bart-large-mnli')
template = ""this example is {}.""

def create_input_sequence(sample):
  text = sample[""text""]
  label = sample[""class""][0]
  contradiction_label = random.choice([x for x in id2labels if x!=label])

  encoded_sequence = t(text*2, [template.format(label), template.format(contradiction_label)])
  encoded_sequence[""labels""] = [2,0]
  encoded_sequence[""input_sentence""] = t.batch_decode(encoded_sequence.input_ids)

  return encoded_sequence

train_dataset = your_dataset.map(create_input_sequence, batched=true, batch_size=1, remove_columns=[""class"", ""text""])
print(train_dataset[0])

output:
{'text': ""fears for t n pension after talks unions representing workers at turner   newall say they are 'disappointed' after talks with stricken parent firm federal mogul."", 
'class': 'business'}

{'input_ids': [0, 597, 12541, 13, 255, 234, 4931, 71, 1431, 1890, 2485, 4561, 1138, 23, 6980, 1437, 1437, 188, 1250, 224, 51, 32, 128, 7779, 19051, 108, 71, 1431, 19, 35876, 4095, 933, 1853, 18059, 922, 4, 2, 2, 713, 1246, 16, 2090, 4, 2], 
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
'labels': 2, 
'input_sentence': ""<s>fears for t n pension after talks unions representing workers at turner   newall say they are 'disappointed' after talks with stricken parent firm federal mogul.</s></s>this example is business.</s>""}

robustness
finetuning will obviously reduce the robustness (i.e. the ability to provide decent results for classes that weren't part of your fine-tuning dataset) of your model. to avoid that you could try:

to stop training before conversion and check if the performance is still sufficient for your needs.
wise-ft proposed by wortsmann et. al. pseudocode is shown in appendix a.",https://stackoverflow.com/questions/76213873,python,09-05-2023 23:01,6294.0,2.0,1.0,True,09-05-2023 23:06,09-05-2023 23:06
45170093,latent dirichlet allocation with prior topic words,"context
i'm trying to extract topics from a set of texts using latent dirichlet allocation from scikit-learn's decomposition module.
this works really well, except for the quality of topic words found/selected.
in a article by li et al (2017), the authors describe using prior topic words as input for the lda. they manually choose 4 topics and the main words associated/belonging to these topics. for these words they set the default value to a high number for the associated topic and 0 for the other topics. all other words (not manually selected for a topic) are given equal values for all topics (1). this matrix of values is used as input for the lda.
my question
how can i create a similar analysis with the latentdirichletallocation module from scikit-learn using a customized default values matrix (prior topics words) as input?
(i know there's a topic_word_prior parameter, but it only takes one float instead of a matrix with different 'default values'.)","['python', 'scikit-learn', 'nlp', 'topic-modeling']",74941735,"using anis' help, i created a subclass of the original module, and edited the function that sets the starting values matrix. for all prior topic words you wish to give as input, it transforms the components_ matrix by multiplying the values with the topic values of that (prior) word.
this is the code:
# list with prior topic words as tuples
# (word index, [topic values])
prior_topic_words = []

# example (word at index 3000 belongs to topic with index 0)
prior_topic_words.append(
    (3000, [(np.finfo(np.float64).max/4),0.,0.,0.,0.])
)

# custom subclass for ptw-guided lda
from sklearn.utils import check_random_state
from sklearn.decomposition._online_lda import _dirichlet_expectation_2d
class ptwguidedlatentdirichletallocation(latentdirichletallocation):

    def __init__(self, n_components=10, doc_topic_prior=none, topic_word_prior=none, learning_method=ï¿½ï¿½ï¿½batchï¿½ï¿½ï¿½, learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1, total_s00.0, perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=none, verbose=0, random_state=none, n_topics=none, ptws=none):
        super(ptwguidedlatentdirichletallocation, self).__init__(n_components, doc_topic_prior, topic_word_prior, learning_method, learning_decay, learning_offset, max_iter, batch_size, evaluate_every, total_samples, perp_tol, mean_change_tol, max_doc_update_iter, n_jobs, verbose, random_state, n_topics)
        self.ptws = ptws

    def _init_latent_vars(self, n_features):
        """"""initialize latent variables.""""""

        self.random_state_ = check_random_state(self.random_state)
        self.n_batch_iter_ = 1
        self.n_iter_ = 0

        if self.doc_topic_prior is none:
            self.doc_topic_prior_ = 1. / self.n_topics
        else:
            self.doc_topic_prior_ = self.doc_topic_prior

        if self.topic_word_prior is none:
            self.topic_word_prior_ = 1. / self.n_topics
        else:
            self.topic_word_prior_ = self.topic_word_prior

        init_gamma = 100.
        init_var = 1. / init_gamma
        # in the literature, this is called `lambda`
        self.components_ = self.random_state_.gamma(
            init_gamma, init_var, (self.n_topics, n_features))

        # transform topic values in matrix for prior topic words
        if self.ptws is not none:
            for ptw in self.ptws:
                word_index = ptw[0]
                word_topic_values = ptw[1]
                self.components_[:, word_index] *= word_topic_values

        # in the literature, this is `exp(e[log(beta)])`
        self.exp_dirichlet_component_ = np.exp(
            _dirichlet_expectation_2d(self.components_))

initiation is the same as the original latentdirichletallocation class, but now you can provide prior topic words using the ptws parameter.",https://stackoverflow.com/questions/45170093,python,18-07-2017 14:46,2897.0,4.0,2.0,True,28-12-2022 14:55,27-12-2022 12:45
79032225,aggregating output from langchain lcel elements,"i have two chains, one that generates a document and one that creates a short document resume. i want to chain them, using the output from the first on inside the other one. but i want to get both outputs in the result.
before lcel, i could do it using llmchain's output_key parameter. with lcel, there seems to be a runnablepassthrough class, but i don't seem to get how to use it to aggregate the output.
code example:
generate_document_chain = generate_document_prompt | llm | stroutputparser()
resume_document_chain = resume_document_prompt | llm | stroutputparser()

aggregated_chain = generate_document_chain | resume_document_chain 
content = aggregated_chain.invoke({""topic"": topic})","['python', 'langchain', 'py-langchain']",79033375,"perhaps the following is what you want. it feeds the output of the first chain into second chain as input.
from langchain_core.runnables import runnablepassthrough

aggregated_chain = generate_document_chain | {
    ""first_chain_output"": runnablepassthrough(),
    ""second_chain_output"": resume_document_chain
}
content = aggregated_chain.invoke({""topic"": topic})

then the output will be a dictionary with keys: ""first_chain_output"" and ""second_chain_output"".
you can also use runnablepassthrough.assign. unlike the case above, the key of generate_document_chain output should match the input variable name of the second chain. below, the input variable of the second chain is  assumed to be ""input"" (btw, the input variable of the first chain is ""topic"").
aggregated_chain = (
    {""input"": generate_document_chain} 
    | runnablepassthrough.assign(second_chain_output=resume_document_chain)
)

the output of this chain will be a dict with keys: ""input"" and ""second_chain_output"".",https://stackoverflow.com/questions/79032225,python,27-09-2024 16:30,124.0,2.0,1.0,True,28-09-2024 15:49,28-09-2024 15:49
78216871,integrating llama index vectorstoreindex with langchain agents for rag applications,"i have been reading the documentation all day and can't seem to wrap my head around how i can create a vectorstoreindex with llama_index and use the created embeddings as supplemental information for a rag application/chatbot that can communicate with a user. i want to use llama_index because they have some cool ways to perform more advanced retrieval techniques like sentence window retrieval and auto-merging retrieval (to be fair i have not investigated if langchain also supports these types of vector retrieval methods). i want to use langchain because of its functionality for developing more complex prompt templates (similarly i have not really investigated if llama_index supports this).
my goal is to ultimately evaluate how these different retrieval methods perform within the context of the application/chatbot. i know how to evaluate them with a separate evaluation questions file, but i would like to do things like compare the speed and humanness of responses, token usage, etc.
the code for a minimal reproducible example would be as follows

langchain chatbot initiation

from langchain_core.prompts import chatprompttemplate, messagesplaceholder

from langchain.memory import chatmessagehistory


prompt = chatprompttemplate.from_messages(
    [
        (
            ""system"",
            """"""you are the world's greatest... \
            use this document base to help you provide the best support possible to everyone you engage with. 
            """""",
        ),
        messagesplaceholder(variable_name=""messages""),
    ]
)

chat = chatopenai(model=llm_model, temperature=0.7)



chain = prompt | chat


chat_history = chatmessagehistory()

while true:
    user_input = input(""you: "")
    chat_history.add_user_message(user_input)
    
    response = chain.invoke({""messages"": chat_history.messages})
    
    if user_input.lower() == 'exit':
        break
    
    print(""ai:"", response)
    chat_history.add_ai_message(response)



llama index sentence window retrieval

from llama_index.core.node_parser import sentencewindownodeparser
        from llama_index.core.indices.postprocessor import metadatareplacementpostprocessor
        from llama_index.core.postprocessor import llmrerank
    
class sentencewindowutils:
    def __init__(self, documents, llm, embed_model, sentence_window_size):
        self.documents = documents
        self.llm = llm
        self.embed_model = embed_model
        self.sentence_window_size = sentence_window_size
        # self.save_dir = save_dir

        self.node_parser = sentencewindownodeparser.from_defaults(
            window_size=self.sentence_window_size,
            window_metadata_key=""window"",
            original_text_metadata_key=""original_text"",
        )

        self.sentence_context = servicecontext.from_defaults(
            llm=self.llm,
            embed_model=self.embed_model,
            node_parser=self.node_parser,
        )

    def build_sentence_window_index(self, save_dir):
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
            sentence_index = vectorstoreindex.from_documents(
                self.documents, service_context=self.sentence_context
            )
            sentence_index.storage_context.persist(persist_dir=save_dir)
        else:
            sentence_index = load_index_from_storage(
                storagecontext.from_defaults(persist_dir=save_dir),
                service_context=self.sentence_context,
            )

        return sentence_index

    def get_sentence_window_query_engine(self, sentence_index, similarity_top_k=6, rerank_top_n=3):
        postproc = metadatareplacementpostprocessor(target_metadata_key=""window"")
        rerank = llmrerank(top_n=rerank_top_n, service_context=self.sentence_context)

        sentence_window_engine = sentence_index.as_query_engine(
            similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]
        )

        return sentence_window_engine


sentence_window = sentencewindowutils(documents=documents, llm = llm, embed_model=embed_model, sentence_window_size=1)
sentence_window_1 = sentence_window.build_sentence_window_index(save_dir='./indexes/sentence_window_index_1')
sentence_window_engine_1 = sentence_window.get_sentence_window_query_engine(sentence_window_1)

both blocks of code independently will run. but the goal is that when a query is performed that warrants a retrieval to the existing document base, i can use the sentence_window_engine that was built. i suppose i could retrieve relevant information based on the query and then pass that information into a subsequent prompt for the chatbot, but i would like to try and avoid including the document data in a prompt.
any suggestions?","['python', 'langchain', 'embedding', 'large-language-model', 'llama-index']",78249192,"i never found an exact way to retrieve the information via llama_index like i had hoped but i basically found a workaround by doing what i initially wanted to avoid by querying my document base and adding that as context information to my chatbot as such
#### conversation prompt chain #####
prompt = chatprompttemplate.from_messages(
    [
        (
            ""system"",
            """"""you are the world's greatest...
            you have access to an extensive document base of information.
            relevant information to the user query is provided below. use the information at your own discretion if it improves the quality of the response.
            a summary of the previous conversation is also provided to contextualize you on previous conversation.

            <<relevant information>>
            {relevant_information}


            << previous conversation summary>>
            {previous_conversation}


            << current prompt >>
            {user_input}
            """""",
        ),
        messagesplaceholder(variable_name=""messages""),
    ]
)

chat = chatopenai(model=llm_model, temperature=0.0)



chain = prompt | chat


### application start ###


while true:
    # some code....
    if route['destination'] == ""data querying"":
                formatted_response = query_and_format_sql(username, password, host, port, mydatabase, query_prompt, model = 'gpt-4', client_name = client_name, user_input=user_input)
                print(formatted_response)
                chat_history.add_ai_message(aimessage(f'the previous query triggered a sql agent response that was {formatted_response}'))
        else:
            # search document base
            rag_context = sentence_window_engine_1.query(user_input)
    
            # inject the retrieved information into the chatbot's context
            context_with_relevant_info = {
                ""user_input"": user_input,
                ""messages"": chat_history.messages,
                ""previous_conversation"": memory.load_memory_variables({}),
                ""relevant_information"": rag_context # ==> inject relevant information from llama_index here
            }
            
            response = chain.invoke(context_with_relevant_info)


i haven't ran into a token issue yet but i can imagine if my application grows and scales it may run into problem trying to inject relevant information, the message history, and the prompt. i limit my memory with a conversationbuffermemoryhistory and that seems to work ok for now.",https://stackoverflow.com/questions/78216871,python,25-03-2024 02:50,1175.0,0.0,2.0,True,13-03-2025 22:11,13-03-2025 22:11
76067091,gpu out of memory fine tune flan-ul2,"outofmemoryerror: cuda out of memory. tried to allocate 256.00 mib
(gpu 0; 15.78 gib total capacity; 14.99 gib already allocated; 3.50
mib free; 14.99 gib reserved in total by pytorch) if reserved memory
is >> allocated memory try setting max_split_size_mb to avoid
fragmentation.  see documentation for memory management and
pytorch_cuda_alloc_conf

i have standard_nc24s_v3 single node gpu with 448gb memory and 4 gpus. however the error message says the total capacity is 15.78gib. is the fine tune not using 4 gpus? how to get all the 4 gpus used in the fine tune of flan-ul2 using huggingface transformers?","['gpu', 'huggingface-transformers', 'huggingface-tokenizers', 'gpt-3', 'fine-tuning']",76320287,"i solve the issue by using the following package versions.
!pip install transformers==4.28.1
!pip install sentencepiece==0.1.97
!pip install accelerate==0.18.0
!pip install bitsandbytes==0.37.2
!pip install torch==1.13.1",https://stackoverflow.com/questions/76067091,gpu,20-04-2023 18:13,521.0,1.0,1.0,True,24-05-2023 05:23,20-04-2023 18:33
71492980,huggingface sagemaker,"i am trying to use the text2text (translation) model facebook/m2m100_418m to run on sagemaker.
so if you click on deploy and then sagemaker there is some boilerplate code that works well but i can't seem to find how to pass it the arguments src_lang=""en"", tgt_lang=""fr"" just like when using the pipeline or transformers.
so right now it translates into random languages.
i'm guessing i should add it in here somehow but it's not documented.
predictor.predict({
    'inputs': ""the answer to the universe is""
})

does anybody have an idea of how to pass arguments to the predict method?
edit
this is the code that was wrong where you will need to change the hf_task:
import sagemaker

role = sagemaker.get_execution_role()
# hub model configuration. 
hub = {
    'hf_model_id':'facebook/m2m100_418m',
    'hf_task':'text2text-generation'
}

# create hugging face model class
huggingface_model = huggingfacemodel(
    transformers_version='4.6.1',
    pytorch_version='1.7.1',
    py_version='py36',
    env=hub,
    role=role, 
)

# deploy model to sagemaker inference
predictor = huggingface_model.deploy(
    initial_instance_count=1, # number of instances
    instance_type='ml.m5.xlarge' # ec2 instance type
)```","['python', 'artificial-intelligence', 'amazon-sagemaker', 'huggingface-transformers', 'huggingface-tokenizers']",71509545,"ok i figured it out.
the task was wrong, the source and target language need to be in the task (hf_task)
so for example:
'hf_task': 'translation_en_to_fr'",https://stackoverflow.com/questions/71492980,python,16-03-2022 07:09,478.0,1.0,3.0,True,17-03-2022 09:04,17-03-2022 09:04
77159136,efficiently using hugging face transformers pipelines on gpu with large datasets,"i'm relatively new to python and facing some performance issues while using hugging face transformers for sentiment analysis on a relatively large dataset. i've created a dataframe with 6000 rows of text data in spanish, and i'm applying a sentiment analysis pipeline to each row of text. here's a simplified version of my code:
import pandas as pd
import torch
from tqdm import tqdm
from transformers import pipeline


data = {
    'td': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'text': [
        # ... (your text data here)
    ]
}

df_model = pd.dataframe(data)

device = 0 if torch.cuda.is_available() else -1
py_sentimiento = pipeline(""sentiment-analysis"", model=""finiteautomata/beto-sentiment-analysis"", tokenizer=""finiteautomata/beto-sentiment-analysis"", device=device, truncation=true)

tqdm.pandas()
df_model['py_sentimiento'] = df_model['text'].progress_apply(py_sentimiento)
df_model['py_sentimiento'] = df_model['py_sentimiento'].apply(lambda x: x[0]['label'])

however, i've encountered a warning message that suggests i should use a dataset for more efficient processing. the warning message is as follows:
""you seem to be using the pipelines sequentially on gpu. in order to maximize efficiency please use a dataset.""

i have a two questions:
what does this warning mean, and why should i use a dataset for efficiency?
how can i modify my code to batch my data and use parallel computing to make better use of my gpu resources, what code or function or library should be used with hugging face transformers?
i'm eager to learn and optimize my code.","['python', 'gpu', 'huggingface-transformers', 'huggingface-datasets']",77452808,"i think you can ignore this message. i found it being reported on different websites this year, but if i get it correctly, this github issue on the huggingface transformers ( shows that the warning can be safely ignored. in addition, batching or using datasets might not remove the warning or automatically use the resources in the best way. you can do call_count = 0 in here ( to ignore the warning, as explained by martin weyssow above.
how can i modify my code to batch my data and use parallel computing to make better use of my gpu resources:
you can add batching like this:
py_sentimiento = pipeline(""sentiment-analysis"", model=""finiteautomata/beto-sentiment-analysis"", tokenizer=""finiteautomata/beto-sentiment-analysis"", batch_size=8, device=device, truncation=true)

and most importantly, you can experiment with the batch size that will result to the highest gpu usage possible on your device and particular task.
huggingface provides here some rules to help users figure out how to batch:  making the best resource/gpu usage possible might take some experimentation and it depends on the use case you work on every time.
what does this warning mean, and why should i use a dataset for efficiency?
this means the gpu utilization is not optimal, because the data is not grouped together and it is thus not processed efficiently. using a dataset from the huggingface library datasets will utilize your resources more efficiently.
however, it is not so easy to tell what exactly is going on, especially considering that we donï¿½ï¿½ï¿½t know exactly how the data looks like, what the device is and how the model deals with the data internally. the warning might go away by using the datasets library, but that does not necessarily mean that the resources are optimally used.
what code or function or library should be used with hugging face transformers?
here is a code example with pipelines and the datasets library:  it mentions that using iterables will fill your gpu as fast as possible and batching might also help with computational time improvements.
in your case it seems you are doing a relatively small poc (doing inference for under 10,000 documents with a medium size model), so i donï¿½ï¿½ï¿½t think you need to use pipelines. i assume the sentiment analysis model is a classifier and you want to keep using pandas as shown in the post, so here is how you can combine both. this is usually fast enough for my experiments and prints no warnings about the resources.
from transformers import autotokenizer, automodelforsequenceclassification
import torch as t
import pandas as pd
        
model = automodelforsequenceclassification.from_pretrained(""finiteautomata/beto-sentiment-analysis"")
tokenizer = autotokenizer.from_pretrained(""finiteautomata/beto-sentiment-analysis"")
      
def classify_dataframe_row(
    example: pd.series,
):
    output = model(**tokenizer(example[""text""], return_tensors=""pt""))
    prediction = t.argmax(output[0]).detach().numpy()
    return prediction

dataset = pd.read_csv(""file"")
dataset = dataset.assign(
    prediction=dataset.progress_apply(classify_dataframe_row, axis=1)
)

as soon as your inference starts, either with this snippet or with the datasets library code, you can run nvidia-smi in a terminal and check what the gpu usage is and play around with the parameters to optimize it. beware that running the code on your local machine with a gpu vs running it on a larger machine, e.g., a linux server with perhaps a more powerful gpu might lead to different performance and might need different tuning. if you wish to run the code for larger document collections, you can split the data in order to avoid gpu memory errors locally, or in order to speed up the inference with concurrent runs in a server.",https://stackoverflow.com/questions/77159136,python,22-09-2023 15:57,37222.0,18.0,2.0,True,23-01-2024 15:01,08-10-2023 20:35
75779489,reading files in a directory and saving each dynamically,"text_open = open(""inputfiles/(22).txt"", ""r"")
text = text_open.read()

doc = nlp(text)
db.add(doc)
db.to_disk(""./outputfiles/22.spacy"")

i am trying to loop over each of the 500+ documents in the inputfiles folder and output them through db.to_disk. instead of changing the hard coded numbers every-time, how would i dynamically rename each new output file to match the input file?
if i were to open the directory with glob/os, how do i then add this to the output directory without overwriting every single file with the hardcoded '22' or creating new 22(1).spacy, 22(2).spacy etc files?
thanks!","['python', 'spacy']",75779504,"if you're using python 3.6+, then use f-strings.
for i in range(1, 501):
    text_open = open(f""inputfiles/({i}).txt"", ""r"")
    text = text_open.read()

    doc = nlp(text)
    db.add(doc)
    db.to_disk(f""./outputfiles/{i}.spacy"")

would it help?",https://stackoverflow.com/questions/75779489,python,19-03-2023 00:39,37.0,0.0,1.0,True,19-03-2023 00:44,19-03-2023 00:39
66171956,number of learnable parameters of multiheadattention,"while testing (using pytorch's multiheadattention), i noticed that increasing or decreasing the number of heads of the multi-head attention does not change the total number of learnable parameters of my model.
is this behavior correct? and if so, why?
shouldn't the number of heads affect the number of parameters the model can learn?","['python', 'python-3.x', 'nlp', 'pytorch', 'attention-model']",66176914,"the standard implementation of multi-headed attention divides the model's dimensionality by the number of attention heads.
a model of dimensionality d with a single attention head would project embeddings to a single triplet of d-dimensional query, key and value tensors (each projection counting d2 parameters, excluding biases, for a total of 3d2).
a model of the same dimensionality with k attention heads would project embeddings to k triplets of d/k-dimensional query, key and value tensors (each projection counting dï¿½ï¿½d/k=d2/k parameters, excluding biases, for a total of 3kd2/k=3d2).

references:
from the original paper:

the pytorch implementation you cited:",https://stackoverflow.com/questions/66171956,python,12-02-2021 12:31,2741.0,5.0,1.0,True,04-01-2023 10:48,16-03-2021 09:59
71661321,"valueerror: dimensions must be equal, but are 2 and 64 for &#39;{{node binary_crossentropy/mul}} with input shapes[?,2], [?,64]","i'm trying binary classification of text with bi-lstm model but getting this error: valueerror: dimensions must be equal, but are 2 and 64 for '{{node binary_crossentropy/mul}} = mul[t=dt_float](binary_crossentropy/cast, binary_crossentropy/log)' with input shapes: [?,2], [?,64].
i am a beginner please provide some valuable solutions.
text=df['text']
label=df['label']

x = pad_sequences(x, maxlen=max_len,padding=pad_type,truncating=trunc_type)
y = pd.get_dummies(label).values    
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.20)
print(x_train.shape,y_train.shape)
print(x_test.shape,y_test.shape)

#model creation
model=tf.keras.sequential([
 # add an embedding layer
 tf.keras.layers.embedding(word_count, 16, input_length=max_len),
 tf.keras.layers.dropout(0.2),
 # add another bi-lstm layer
 tf.keras.layers.bidirectional(tf.keras.layers.lstm(2,return_sequences=true)),
 # add a dense layer
 tf.keras.layers.dense(32, activation=tf.keras.activations.relu),
 tf.keras.layers.dense(32, activation=tf.keras.activations.relu),
 tf.keras.layers.dense(32, activation=tf.keras.activations.relu),
 tf.keras.layers.dense(32, activation=tf.keras.activations.softmax),
 # add the prediction layer
 tf.keras.layers.dense(1, activation=tf.keras.activations.sigmoid),
])
model.compile(loss=tf.keras.losses.binarycrossentropy(), optimizer=tf.keras.optimizers.adam(), metrics=['accuracy'])
model.summary()
history = model.fit(x_train,  y_train, validation_data=(x_test,  y_test), epochs = 10, batch_size=batch_size, callbacks = [callback_func], verbose=1)","['tensorflow', 'keras', 'nlp', 'tensorflow2.0']",71662595,"the output dimension of the prediction layer of the binary classification should be 2:
# add the prediction layer
tf.keras.layers.dense(2, activation=tf.keras.activations.sigmoid)

flatten:
#model creation
model=tf.keras.sequential([
 # add an embedding layer
 tf.keras.layers.embedding(word_count, 16, input_length=max_len),
 tf.keras.layers.dropout(0.2),
 # add another bi-lstm layer
 tf.keras.layers.bidirectional(tf.keras.layers.lstm(2,return_sequences=true)),
 # add flatten
 tf.keras.layers.flatten(),  #<========================
 # add a dense layer
 tf.keras.layers.dense(32, activation=tf.keras.activations.relu),
 tf.keras.layers.dense(32, activation=tf.keras.activations.relu),
 tf.keras.layers.dense(32, activation=tf.keras.activations.relu),
 tf.keras.layers.dense(32, activation=tf.keras.activations.softmax),
 # add the prediction layer
 tf.keras.layers.dense(2, activation=tf.keras.activations.sigmoid),
])",https://stackoverflow.com/questions/71661321,tensorflow,29-03-2022 11:37,2440.0,1.0,1.0,True,29-03-2022 13:42,29-03-2022 12:02
72875686,nlp timer *how do i get the order of certain numbers in a string to use setting a timer?,"import time
import datetime
from nltk_utils import bag_of_words, tokenize
from nltk.tokenize.treebank import treebankworddetokenizer
import multiprocessing
from playsound import playsound


def is_number(x):
    if type(x) == str:
        x = x.replace(',', '')
    try:
        float(x)
    except:
        return false
    return true

def text2int (textnum, numwords={}):
    units = [
        'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight',
        'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen',
        'sixteen', 'seventeen', 'eighteen', 'nineteen',
    ]
    tens = ['', '', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety']
    scales = ['hundred', 'thousand', 'million', 'billion', 'trillion']
    ordinal_words = {'first':1, 'second':2, 'third':3, 'fifth':5, 'eighth':8, 'ninth':9, 'twelfth':12}
    ordinal_endings = [('ieth', 'y'), ('th', '')]

    if not numwords:
        numwords['and'] = (1, 0)
        for idx, word in enumerate(units): numwords[word] = (1, idx)
        for idx, word in enumerate(tens): numwords[word] = (1, idx * 10)
        for idx, word in enumerate(scales): numwords[word] = (10 ** (idx * 3 or 2), 0)

    textnum = textnum.replace('-', ' ')

    current = result = 0
    curstring = ''
    onnumber = false
    lastunit = false
    lastscale = false

    def is_numword(x):
        if is_number(x):
            return true
        if word in numwords:
            return true
        return false

    def from_numword(x):
        if is_number(x):
            scale = 0
            increment = int(x.replace(',', ''))
            return scale, increment
        return numwords[x]

    for word in textnum.split():
        if word in ordinal_words:
            scale, increment = (1, ordinal_words[word])
            current = current * scale + increment
            if scale > 100:
                result += current
                current = 0
            onnumber = true
            lastunit = false
            lastscale = false
        else:
            for ending, replacement in ordinal_endings:
                if word.endswith(ending):
                    word = ""%s%s"" % (word[:-len(ending)], replacement)

            if (not is_numword(word)) or (word == 'and' and not lastscale):
                if onnumber:
                    # flush the current number we are building
                    curstring += repr(result + current) + "" ""
                curstring += word + "" ""
                result = current = 0
                onnumber = false
                lastunit = false
                lastscale = false
            else:
                scale, increment = from_numword(word)
                onnumber = true

                if lastunit and (word not in scales):
                    # assume this is part of a string of individual numbers to
                    # be flushed, such as a zipcode ""one two three four five""
                    curstring += repr(result + current)
                    result = current = 0

                if scale > 1:
                    current = max(1, current)

                current = current * scale + increment
                if scale > 100:
                    result += current
                    current = 0

                lastscale = false
                lastunit = false
                if word in scales:
                    lastscale = true
                elif word in units:
                    lastunit = true

    if onnumber:
        curstring += repr(result + current)

    return curstring


input = ""please will you set a ten seconds timer""
sentence1 = text2int(input)

sentence = tokenize(sentence1)
print(sentence)


if ""hour"" in sentence or 'hours' in sentence:
    remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours', 'please',
              'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"", 'a', 'for',
              'and', 'if', 'privacy', 'time', 'but', 'end', 'put', 'me', 'my', 'will',
              'you', 'now', 'right', 'privacy', 'rite', 'wright', 'write', 'your', 'go', 'ahead', 't']
    remove = set(remove)
    search = set(sentence) - set(remove)
    hours = treebankworddetokenizer().detokenize(search)
    hours1 = (f""{hours}"")
    print(hours1 + ' hours starting now')
else:
    hours1 = '0'

if ""minutes"" in sentence or 'minute' in sentence:
    remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours', 'please',
              'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"", 'a', 'for',
              'and', 'if', 'privacy', 'time', 'but', 'end', 'put', 'me', 'my', 'will',
              'you', 'now', 'right', 'privacy', 'rite', 'wright', 'write', 'your', 'go', 'ahead', 't']
    remove = set(remove)
    search = set(sentence) - set(remove)
    minutes = treebankworddetokenizer().detokenize(search)
    minutes1 = (f""{minutes}"")
    print(minutes + ' minutes starting now')
else:
    minutes1 = '0'

if ""seconds"" in sentence or 'second' in sentence:
    remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours', 'please',
              'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"", 'a', 'for',
              'and', 'if', 'privacy', 'time', 'but', 'end', 'put', 'me', 'my', 'will',
              'you', 'now', 'right', 'privacy', 'rite', 'wright', 'write', 'your', 'go', 'ahead', 't']
    remove = set(remove)
    search = set(sentence) - set(remove)
    seconds = treebankworddetokenizer().detokenize(search)
    seconds1 = (f""{seconds}"")
    print(seconds + ' seconds starting now')
else:
    seconds1 = '0'


def countdown(h, m, s):
    total_seconds = h * 3600 + m * 60 + s
    while total_seconds > 0:
        timer = datetime.timedelta(seconds = total_seconds)

        print(timer, end=""\r"")

        time.sleep(1)
        print(total_seconds)
        total_seconds -= 1

    print('timer ended')



h = hours1
m = minutes1
s = seconds1
countdown(int(h), int(m), int(s))

in this code, you can only ask for a timer of x hours, x minutes, or x seconds but not a combination of them. i want to be able to say set a timer for 10 minutes and 45 seconds and have the system know that the number 10 is associated with minutes and 45 is associated with seconds.
also i am brand new to coding so i know this code probably isn't pretty.","['python', 'nlp', 'stringtokenizer', 'nlu']",73029415,"i figured it out. if you are looking for a way to set a timer with natural language then look no further. i really hope that people improve and critique this but as it sits right now i have a working piece of code. again i am sure there are flaws and things to improve on so if you know any i would be all ears. anyway here is the code.
from nltk.tokenize.treebank import treebankworddetokenizer
from nltk_utils import bag_of_words, tokenize
import time
from datetime import datetime
import datetime
import pyttsx3

try:
    engine = pyttsx3.init()
except importerror:
    print('requested driver not found')
except runtimeerror:
    print('driver fails to initialize')

voices = engine.getproperty('voices')
for voice in voices:
    print(voice.id)
engine.setproperty('voice',
                   'hkey_local_machine\software\microsoft\speech\voices\tokens\tts_ms_en-us_zira_11.0')  # diffrent voices = us_david, gb_hazel, us_zira
rate = engine.getproperty('rate')
engine.setproperty('rate', rate)


def speak_text_cmd(cmd):
    engine.say(cmd)
    engine.runandwait()


def is_number(x):
    if type(x) == str:
        x = x.replace(',', '')
    try:
        float(x)
    except:
        return false
    return true

def text2int (textnum, numwords={}):
    units = [
        'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight',
        'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen',
        'sixteen', 'seventeen', 'eighteen', 'nineteen',
    ]
    tens = ['', '', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety']
    scales = ['hundred', 'thousand', 'million', 'billion', 'trillion']
    ordinal_words = {'first':1, 'third':3, 'fifth':5, 'eighth':8, 'ninth':9, 'twelfth':12}
    ordinal_endings = [('ieth', 'y'), ('th', '')]

    if not numwords:
        numwords['and'] = (1, 0)
        for idx, word in enumerate(units): numwords[word] = (1, idx)
        for idx, word in enumerate(tens): numwords[word] = (1, idx * 10)
        for idx, word in enumerate(scales): numwords[word] = (10 ** (idx * 3 or 2), 0)

    textnum = textnum.replace('-', ' ')

    current = result = 0
    curstring = ''
    onnumber = false
    lastunit = false
    lastscale = false

    def is_numword(x):
        if is_number(x):
            return true
        if word in numwords:
            return true
        return false

    def from_numword(x):
        if is_number(x):
            scale = 0
            increment = int(x.replace(',', ''))
            return scale, increment
        return numwords[x]

    for word in textnum.split():
        if word in ordinal_words:
            scale, increment = (1, ordinal_words[word])
            current = current * scale + increment
            if scale > 100:
                result += current
                current = 0
            onnumber = true
            lastunit = false
            lastscale = false
        else:
            for ending, replacement in ordinal_endings:
                if word.endswith(ending):
                    word = ""%s%s"" % (word[:-len(ending)], replacement)

            if (not is_numword(word)) or (word == 'and' and not lastscale):
                if onnumber:
                    # flush the current number we are building
                    curstring += repr(result + current) + "" ""
                curstring += word + "" ""
                result = current = 0
                onnumber = false
                lastunit = false
                lastscale = false
            else:
                scale, increment = from_numword(word)
                onnumber = true

                if lastunit and (word not in scales):
                    # assume this is part of a string of individual numbers to
                    # be flushed, such as a zipcode ""one two three four five""
                    curstring += repr(result + current)
                    result = current = 0

                if scale > 1:
                    current = max(1, current)

                current = current * scale + increment
                if scale > 100:
                    result += current
                    current = 0

                lastscale = false
                lastunit = false
                if word in scales:
                    lastscale = true
                elif word in units:
                    lastunit = true

    if onnumber:
        curstring += repr(result + current)

    return curstring

numbers = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight',
        'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen',
        'sixteen', 'seventeen', 'eighteen', 'nineteen']




input = search8
sentence = tokenize(input)


if ""timer"" in sentence:
    input = treebankworddetokenizer().detokenize(sentence)
    timerinput1 = text2int(input)

    timerinput = tokenize(timerinput1)
    try:

        if ""hour"" in timerinput or 'hours' in timerinput:
            if ""hour"" in timerinput:
                stringinput1 = input.split('hour', 1)[0]
                stringinput = tokenize(stringinput1)
                remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                          'please',
                          'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                          'a', 'for',
                          'and', 'if', 'time', 'but', 'end', 'put', 'me',
                          'my', 'will',
                          'you', 'now', 'right', 'rite', 'wright',
                          'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time', 'start']
                remove = set(remove)
                search = set(stringinput) - set(remove)
                minutes = treebankworddetokenizer().detokenize(search)
                hours1 = (f""{minutes}"")
            if 'hours' in timerinput:
                stringinput1 = input.split('hours', 1)[0]
                stringinput = tokenize(stringinput1)
                remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                          'please',
                          'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                          'a', 'for',
                          'and', 'if', 'time', 'but', 'end', 'put', 'me',
                          'my', 'will',
                          'you', 'now', 'right', 'rite', 'wright',
                          'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time', 'start']
                remove = set(remove)
                search = set(stringinput) - set(remove)
                minutes = treebankworddetokenizer().detokenize(search)
                hours1 = (f""{minutes}"")
        else:
            hours1 = '0'

        if ""minutes"" in timerinput or 'minute' in timerinput:
            if ""hour"" in timerinput:
                stringinput1 = input.split('hour', 1)[1]
                stringinput = tokenize(stringinput1)
                remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                          'please',
                          'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                          'a', 'for',
                          'and', 'if', 'time', 'but', 'end', 'put', 'me',
                          'my', 'will',
                          'you', 'now', 'right', 'rite', 'wright',
                          'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time', 'start']
                remove = set(remove)
                search = set(stringinput) - set(remove)
                minutes = treebankworddetokenizer().detokenize(search)
                minutes1 = (f""{minutes}"")
            if 'hours' in timerinput:
                stringinput1 = input.split('hours', 1)[1]
                stringinput = tokenize(stringinput1)
                remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                          'please',
                          'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                          'a', 'for',
                          'and', 'if', 'time', 'but', 'end', 'put', 'me',
                          'my', 'will',
                          'you', 'now', 'right', 'rite', 'wright',
                          'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time', 'start']
                remove = set(remove)
                search = set(stringinput) - set(remove)
                minutes = treebankworddetokenizer().detokenize(search)
                minutes1 = (f""{minutes}"")
            else:
                if ""minute"" in timerinput:
                    stringinput1 = input.split('minute', 1)[0]
                    stringinput = tokenize(stringinput1)
                    remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                              'please',
                              'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                              'a', 'for',
                              'and', 'if', 'time', 'but', 'end', 'put', 'me',
                              'my', 'will',
                              'you', 'now', 'right', 'rite', 'wright',
                              'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time',
                              'start']
                    remove = set(remove)
                    search = set(stringinput) - set(remove)
                    minutes = treebankworddetokenizer().detokenize(search)
                    minutes1 = (f""{minutes}"")
                if 'minutes' in timerinput:
                    stringinput1 = input.split('minutes', 1)[0]
                    stringinput = tokenize(stringinput1)
                    remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                              'please',
                              'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                              'a', 'for',
                              'and', 'if', 'time', 'but', 'end', 'put', 'me',
                              'my', 'will',
                              'you', 'now', 'right', 'rite', 'wright',
                              'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time',
                              'start']
                    remove = set(remove)
                    search = set(stringinput) - set(remove)
                    minutes = treebankworddetokenizer().detokenize(search)
                    minutes1 = (f""{minutes}"")

        else:
            minutes1 = '0'

        if ""seconds"" in timerinput or 'second' in timerinput:
            if ""minute"" in timerinput:
                stringinput1 = input.split('minute', 1)[1]
                stringinput = tokenize(stringinput1)
                remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                          'please',
                          'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                          'a', 'for',
                          'and', 'if', 'time', 'but', 'end', 'put', 'me',
                          'my', 'will',
                          'you', 'now', 'right', 'rite', 'wright',
                          'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time',
                          'start']
                remove = set(remove)
                search = set(stringinput) - set(remove)
                seconds = treebankworddetokenizer().detokenize(search)
                seconds1 = (f""{seconds}"")
            if 'minutes' in timerinput:
                stringinput1 = input.split('minutes', 1)[1]
                stringinput = tokenize(stringinput1)
                remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                          'please',
                          'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                          'a', 'for',
                          'and', 'if', 'time', 'but', 'end', 'put', 'me',
                          'my', 'will',
                          'you', 'now', 'right', 'rite', 'wright',
                          'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time',
                          'start']
                remove = set(remove)


                search = set(stringinput) - set(remove)
                seconds = treebankworddetokenizer().detokenize(search)
                seconds1 = (f""{seconds}"")
            else:
                if ""second"" in timerinput or 'seconds' in timerinput:
                    remove = ['second', 'seconds', 'minute', 'minutes', 'hour', 'hours',
                              'please',
                              'set', 's', 'timer', '``', '{', '}', 'text', ':', ""''"",
                              'a', 'for',
                              'and', 'if', 'time', 'but', 'end', 'put', 'me',
                              'my', 'will',
                              'you', 'now', 'right', 'rite', 'wright',
                              'write', 'your', 'go', 'ahead', 't', 'create', 'said', 'make', 'her', 'no', 'time',
                              'start']
                    remove = set(remove)
                    search = set(timerinput) - set(remove)
                    seconds = treebankworddetokenizer().detokenize(search)
                    seconds1 = (f""{seconds}"")

        else:
            seconds1 = '0'

        if ""minutes"" in timerinput or 'minute' in timerinput:

            if ""seconds"" in timerinput or 'second' in timerinput:
                 speak_text_cmd('starting timer for ' + minutes1 + ' minutes and ' + seconds1 + ' seconds')

            if ""hours"" in timerinput or 'hour' in timerinput:
                speak_text_cmd(hours1 + ' hour and ' + minutes1 + ' minute timer starting now')

            else:
                speak_text_cmd('starting timer for ' + minutes1 + ' minutes')
        else:
            if ""seconds"" in timerinput or 'second' in timerinput:
                speak_text_cmd(seconds1 + "" timer starting now"")

            if ""hours"" in timerinput or 'hour' in timerinput:
                speak_text_cmd(hours1 + "" timer starting now"")

    except valueerror:
            speak_text_cmd('i didnt quite get that, can you say that again')


def countdown(h, m, s):
    total_seconds = h * 3600 + m * 60 + s
    while total_seconds > 0:
        timer = datetime.timedelta(seconds=total_seconds)

        speak_text_cmd(timer, end=""\r"")

        time.sleep(1)
        speak_text_cmd(total_seconds)
        total_seconds -= 1

    speak_text_cmd('timer has ended')
#data = str.split('from',1)[0]

h = hours1
m = minutes1
s = seconds1
countdown(int(h), int(m), int(s))

you can have inputs with numbers as words like ""set a timer for thirty minutes and fifteen seconds"" or not like ""set a timer for 30 minutes and 15 seconds"". just replace the ""search8"" with what you want to say.
enjoy!",https://stackoverflow.com/questions/72875686,python,05-07-2022 21:23,71.0,1.0,2.0,True,18-07-2022 22:53,05-07-2022 22:11
77454475,typeerror when using openai-api,"using the code below and openai version 0.28.0 i get an error which i can't resolve:
file """", line 11, in 
typeerror: string indices must be integers, not 'str'
which indice is it complaining about. seems i'm a little blind today...
import requests
from bs4 import beautifulsoup
from docx import document
import openai

# set your openai api key
openai.api_key = ""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""


# url of the website you want to scrape
website_url = ""


# send a get request to the website
response = requests.get(website_url)

# parse the html content of the website using beautifulsoup
soup = beautifulsoup(response.content, ""html.parser"")

# extract text blocks larger than 100 characters
text_blocks = []
for paragraph in soup.find_all(""p""):
    text = paragraph.get_text().strip()
    if len(text) >= 100:
        text_blocks.append(text)

# translate text blocks from english to german using openai's chat api
translated_text_blocks = []
for text_block in text_blocks:
    chat_input = f""translate the following english text to german: '{text_block}'""
    response = openai.chatcompletion.create(
        model=""gpt-3.5-turbo"",  # use the language model
        messages=[
            {""role"": ""system"", ""content"": ""you are a helpful assistant.""},
            {""role"": ""user"", ""content"": chat_input},
        ],
    )

    # extract translated text from the api response
    translated_text = response.choices[0].message[""content""][""body""]
    translated_text_blocks.append(translated_text)

# create a new word document
document = document()

# add translated text blocks to the word document
for translated_text in translated_text_blocks:
    document.add_paragraph(translated_text)

# save the word document
document.save(""translated_content.docx"")

the full console output is:
>>> # send a get request to the website
>>> 
>>> response = requests.get(website_url)
>>> # parse the html content of the website using beautifulsoup
>>> 
>>> soup = beautifulsoup(response.content, ""html.parser"")      
>>> # extract text blocks larger than 100 characters
>>> 
>>> text_blocks = []
>>> for paragraph in soup.find_all(""p""):
...     text = paragraph.get_text().strip()
...     if len(text) >= 100:
...         text_blocks.append(text)
... # translate text blocks from english to german using openai's chat api
...
>>> translated_text_blocks = []
>>> for text_block in text_blocks:
...     chat_input = f""translate the following english text to german: '{text_block}'""
...     response = openai.chatcompletion.create(
...         model=""gpt-3.5-turbo"",  # use the language model
...         messages=[
...             {""role"": ""system"", ""content"": ""you are a helpful assistant.""},
...             {""role"": ""user"", ""content"": chat_input},
...         ],
...     )
...     # extract translated text from the api response
...     translated_text = response.choices[0].message[""content""][""body""]
...     translated_text_blocks.append(translated_text)
... # create a new word document
...
traceback (most recent call last):
  file ""<stdin>"", line 11, in <module>
typeerror: string indices must be integers, not 'str'
>>> document = document()
>>> # add translated text blocks to the word document
>>>
>>> for translated_text in translated_text_blocks:
...     document.add_paragraph(translated_text)
... # save the word document
...
>>> document.save(""translated_content.docx"")
>>> print(""translated text blocks have been saved to 'translated_content.docx'."")
translated text blocks have been saved to 'translated_content.docx'.","['python', 'openai-api']",77455938,"your problem is caused by this line of code:
translated_text = response.choices[0].message[""content""][""body""]
response.choices[0].message[""content""] is already your response from openai api in str type
and so you are getting this error because you are trying to get item from str by key what is wrong.
so just replace this line on this line:
translated_text = response.choices[0].message[""content""]",https://stackoverflow.com/questions/77454475,python,09-11-2023 15:38,253.0,0.0,1.0,True,09-11-2023 19:41,09-11-2023 15:41
75396481,openai gpt-3 api error: &quot;this model&#39;s maximum context length is 4097 tokens&quot;,"i am making a request to the completions endpoint. my prompt is 1360 tokens, as verified by the playground and the tokenizer. i won't show the prompt as it's a little too long for this question.
here is my request to openai in nodejs using the openai npm package.
const response = await openai.createcompletion({
  model: 'text-davinci-003',
  prompt,
  max_tokens: 4000,
  temperature: 0.2
})

when testing in the playground my total tokens after response are 1374.
when submitting my prompt via the completions api i am getting the following error:
error: {
  message: ""this model's maximum context length is 4097 tokens, however you requested 5360 tokens (1360 in your prompt; 4000 for the completion). please reduce your prompt; or completion length."",
  type: 'invalid_request_error',
  param: null,
  code: null
}

if you have been able to solve this one, i'd love to hear how you did it.","['openai-api', 'gpt-3']",75397187,"the max_tokens parameter is shared between the prompt and the completion. tokens from the prompt and the completion all together should not exceed the token limit of a particular openai model.
as stated in the official openai article:

depending on the model used, requests can use up to 4097 tokens shared
between prompt and completion. if your prompt is 4000 tokens, your
completion can be 97 tokens at most.
the limit is currently a technical limitation, but there are often
creative ways to solve problems within the limit, e.g. condensing your
prompt, breaking the text into smaller pieces, etc.

note: for counting tokens before(!) sending an api request, see this answer.
gpt-4 and gpt-4 turbo models:




latest model
description
max tokens
training data




gpt-4-1106-preview
gpt-4 turbo  the latest gpt-4 model with improved instruction following, json mode, reproducible outputs, parallel function calling, and more. returns a maximum of 4,096 output tokens. this preview model is not yet suited for production traffic. learn more.
128,000 tokens
up to apr 2023


gpt-4-vision-preview
gpt-4 turbo with vision  ability to understand images, in addition to all other gpt-4 turbo capabilties. returns a maximum of 4,096 output tokens. this is a preview model version and not suited yet for production traffic. learn more.
128,000 tokens
up to apr 2023


gpt-4
currently points to gpt-4-0613. see continuous model upgrades.
8,192 tokens
up to sep 2021


gpt-4-0613
snapshot of gpt-4 from june 13th 2023 with improved function calling support.
8,192 tokens
up to sep 2021


gpt-4-32k
currently points to gpt-4-32k-0613. see continuous model upgrades.
32,768 tokens
up to sep 2021


gpt-4-32k-0613
snapshot of gpt-4-32k from june 13th 2023 with improved function calling support.
32,768 tokens
up to sep 2021


gpt-4-0314 (legacy)
snapshot of gpt-4 from march 14th 2023 with function calling support. this model version will be deprecated on june 13th 2024.
8,192 tokens
up to sep 2021


gpt-4-32k-0314 (legacy)
snapshot of gpt-4-32k from march 14th 2023 with function calling support. this model version will be deprecated on june 13th 2024.
32,768 tokens
up to sep 2021




gpt-3.5 models:




latest model
description
max tokens
training data




gpt-3.5-turbo-1106
updated gpt 3.5 turbo  the latest gpt-3.5 turbo model with improved instruction following, json mode, reproducible outputs, parallel function calling, and more. returns a maximum of 4,096 output tokens. learn more.
16,385 tokens
up to sep 2021


gpt-3.5-turbo
currently points to gpt-3.5-turbo-0613. will point to gpt-3.5-turbo-1106 starting dec 11, 2023. see continuous model upgrades.
4,096 tokens
up to sep 2021


gpt-3.5-turbo-16k
currently points to gpt-3.5-turbo-0613. will point to gpt-3.5-turbo-1106 starting dec 11, 2023. see continuous model upgrades.
16,385 tokens
up to sep 2021


gpt-3.5-turbo-instruct
similar capabilities as text-davinci-003 but compatible with legacy completions endpoint and not chat completions.
4,096 tokens
up to sep 2021


gpt-3.5-turbo-0613 (legacy)
snapshot of gpt-3.5-turbo from june 13th 2023. will be deprecated on june 13, 2024.
4,096 tokens
up to sep 2021


gpt-3.5-turbo-16k-0613 (legacy)
snapshot of gpt-3.5-16k-turbo from june 13th 2023. will be deprecated on june 13, 2024.
16,385 tokens
up to sep 2021


gpt-3.5-turbo-0301 (legacy)
snapshot of gpt-3.5-turbo from march 1st 2023. will be deprecated on june 13th 2024.
4,096 tokens
up to sep 2021




gpt-3 models (legacy):




latest model
description
max tokens
training data




text-curie-001
very capable, faster and lower cost than davinci.
2,049 tokens
up to oct 2019


text-babbage-001
capable of straightforward tasks, very fast, and lower cost.
2,049 tokens
up to oct 2019


text-ada-001
capable of very simple tasks, usually the fastest model in the gpt-3 series, and lowest cost.
2,049 tokens
up to oct 2019


davinci
most capable gpt-3 model. can do any task the other models can do, often with higher quality.
2,049 tokens
up to oct 2019


curie
very capable, but faster and lower cost than davinci.
2,049 tokens
up to oct 2019


babbage
capable of straightforward tasks, very fast, and lower cost.
2,049 tokens
up to oct 2019


ada
capable of very simple tasks, usually the fastest model in the gpt-3 series, and lowest cost.
2,049 tokens
up to oct 2019




gpt base models:




latest model
description
max tokens
training data




babbage-002
replacement for the gpt-3 ada and babbage base models.
16,384 tokens
up to sep 2021


davinci-002
replacement for the gpt-3 curie and davinci base models.
16,384 tokens
up to sep 2021",https://stackoverflow.com/questions/75396481,openai-api,09-02-2023 09:18,110339.0,45.0,3.0,True,28-11-2023 16:46,13-03-2023 14:20
78438846,read a stream of a word document (.doc) in python,"i'm trying to read a word document (.doc) to create a customwordloader for langchain.  i'm currently able to read .docx files using the python-docx package.
the stream is created by reading a word document from a sharepoint site.
here is code for docs:
class customwordloader(baseloader):
    """"""
    this class is a custom loader for word documents. it extends the baseloader class and overrides its methods.
    it uses the python-docx library to parse word documents and optionally splits the text into manageable documents.
    
    attributes:
    stream (io.bytesio): a binary stream of the word document.
    filename (str): the name of the word document.
    """"""
    def __init__(self, stream, filename: str):
        # initialize with a binary stream and filename
        self.stream = stream
        self.filename = filename

    def load_and_split(self, text_splitter=none):
        # use python-docx to parse the word document from the binary stream
        doc = docxdocument(self.stream)
        # extract and concatenate all paragraph texts into a single string
        text = ""\n"".join([p.text for p in doc.paragraphs])

        # check if a text splitter utility is provided
        if text_splitter is not none:
            # use the provided splitter to divide the text into manageable documents
            split_text = text_splitter.create_documents([text])
        else:
            # without a splitter, treat the entire text as one document
            split_text = [{'text': text, 'metadata': {'source': self.filename}}]

        # add source metadata to each resulting document
        for doc in split_text:
            if isinstance(doc, dict):
                doc['metadata'] = {**doc.get('metadata', {}), 'source': self.filename}
            else:
                doc.metadata = {**doc.metadata, 'source': self.filename}

        return split_text

my solution will be deployed on a docker using ""3.11.8-alpine3.18"" (a slim version of unix).
for security reasons, i can't download the file locally, so i really need to able to read the stream like my example: doc = docxdocument(self.stream)
i tried to find the equivalent package to python-docx that is able to read a .docx but not a .doc.","['python', 'langchain', 'doc']",78449665,"i was able to do it using textract. i have to save the stream in a file locally, but that's the only way i found.
here is my code:
class customwordloader(baseloader):
""""""
a custom loader for word documents, extending baseloader. it reads word documents from a binary stream,
writes them temporarily to disk, and uses textract to extract text. if textract fails, an exception is raised.
""""""
def __init__(self, stream, filename: str):
    self.stream = stream
    self.filename = filename

def load_and_split(self, text_splitter=none):
    # generate a unique filename
    temp_filename = str(uuid.uuid4()) + '.doc'
    
    # create a temporary directory
    temp_dir = os.path.join(os.getcwd(), 'temp')
    os.makedirs(temp_dir, exist_ok=true)
    
    # full path to the temporary file
    temp_file_path = os.path.join(temp_dir, temp_filename)
    
    # write the content of the stream into the temporary file
    with open(temp_file_path, 'wb') as f:
        f.write(self.stream.read())
    
    # use textract to extract the text from the file
    text = textract.process(temp_file_path).decode('utf-8')
    
    if text_splitter is not none:
        split_text = text_splitter.create_documents([text])
    else:
        split_text = [{'text': text, 'metadata': {'source': self.filename}}]

    for doc in split_text:
        if isinstance(doc, dict):
            doc['metadata'] = {**doc.get('metadata', {}), 'source': self.filename}
        else:
            doc.metadata = {**doc.metadata, 'source': self.filename}

    # remove the temporary file
    os.remove(temp_file_path)

    return split_text

i hope this can help someone!",https://stackoverflow.com/questions/78438846,python,06-05-2024 19:53,384.0,0.0,1.0,True,08-05-2024 15:36,06-05-2024 20:03
74522493,creating and visualizing spacy spans,"i have a problem visualizing manually created spans in spacy:
given the simple code:
from spacy.tokens import span
text = ""welcome to the bank of china. ""
nlp = spacy.blank(""en"")
doc = nlp(text)

doc.spans[""xx""] = [span(doc, 0, 1, ""org"")]
doc.spans[""sc""] = [
    span(doc, 3, 6, ""org""), 
    span(doc, 5, 6, ""gpe""),
    span(doc, 2, 4, ""welcome"")
]

the following visualizer works:
displacy.render(doc, style=""span"")


but if the spans do not contain the key ""sc"" it does not work

the error is key error ""sc""
what is the problem there? why is the rendering not showing me all the spans?
the code giving the error is:
doc.spans[""xx""] = [
    span(doc, 3, 6, ""org""), 
    span(doc, 5, 6, ""gpe""),
    span(doc, 2, 4, ""welcome"")
]
displacy.render(doc, style=""span"", options ={""spans_key"":""xx""})","['python', 'spacy']",74527341,"as explained in the displacy documentation, by default the spans in the key ""sc"" are used. you can change it with the spans_key parameter.
render doesn't take spans_key correctly, you have to include it in options.
from the docs, modified to use render instead of serve:
doc.spans[""custom""] = [span(doc, 3, 6, ""bank"")]
options = {""spans_key"": ""custom""}
displacy.render(doc, style=""span"", options=options)",https://stackoverflow.com/questions/74522493,python,21-11-2022 17:05,424.0,0.0,1.0,True,22-11-2022 13:44,22-11-2022 13:44
77563897,bert topic clasiffying over a quarter of documents in outlier topic -1,"i am running bert topic with default options
import pandas as pd
from sentence_transformers import sentencetransformer
import time
import pickle
from bertopic import bertopic

llm_mod =  ""all-minilm-l6-v2""
model = sentencetransformer(llm_mod)

embeddings = model.encode(skills_augmented, show_progress_bar=true)
bertopic_model = bertopic(verbose=true)

i have a dataset of 40,000 documents that are only one short sentence. 13,573 of the documents get placed in the -1 topic (below distribution across top 5 topics).
-1      13573
 0       1593
 1       1043
 2        628
 3        627

from the documentation: the -1 refers to all outliers and should typically be ignored. is there a parameter i can use to get less documents in -1? perhaps get a more even distribution across topics? would running kmeans be better?","['python', 'nlp', 'bert-language-model', 'topic-modeling']",77565599,"from the documentation :

the main way to reduce your outliers in bertopic is by using the .reduce_outliers function. to make it work without too much tweaking, you will only need to pass the docs and their corresponding topics. you can pass outlier and non-outlier documents together since it will only try to reduce outlier documents and label them to a non-outlier topic.

the following is a minimal example:
from bertopic import bertopic

# train your bertopic model
topic_model = bertopic()
topics, probs = topic_model.fit_transform(docs)

# reduce outliers
new_topics = topic_model.reduce_outliers(docs, topics)

you can find all the strategies for reducing outliers in this page outlier reduction",https://stackoverflow.com/questions/77563897,python,28-11-2023 12:32,1108.0,-1.0,1.0,True,28-11-2023 16:32,28-11-2023 16:13
69585176,using sentence-bert with other features in scikit-learn,"i have a dataset, one feature is text and 4 more features. sentence-bert vectorizer transforms text data into tensors. i can use these sparse matrices directly with a machine learning classifier. can i replace the text column with tensors? and, how can i train the model. the code below is how i transform the text into vectors.
model = sentencetransformer('sentence-transformers/labse')
sentence_embeddings = model.encode(x_train['tweet'], convert_to_tensor=true, show_progress_bar=true)
sentence_embeddings1 = model.encode(x_test['tweet'], convert_to_tensor=true, show_progress_bar=true)","['python', 'machine-learning', 'nlp', 'embedding', 'bert-language-model']",69595712,"let's assume this is your data
x_train = pd.dataframe({
    'tweet':['foo', 'foo', 'bar'],
    'feature1':[1, 1, 0],
    'feature2':[1, 0, 1],
})
y_train = [1, 1, 0]

and you are willing to use it with sklearn api (cross-validation, pipeline, grid-search, and so on). there is a utility named columntransformer which can map pandas data frames to the desired data using user-defined arbitrary functions! what you have to do is define a function and create an official sklearn.transformer from it.
model = sentencetransformer('mrm8488/bert-tiny-finetuned-squadv2') # model named is changed for time and computation gians :)
embedder = functiontransformer(lambda item:model.encode(item, convert_to_tensor=true, show_progress_bar=false).detach().cpu().numpy())

after that, you would be able to use the transformer like any other transformer and map your text column into semantic space, like:
preprocessor = columntransformer(
    transformers=[('embedder', embedder, 'tweet')],
    remainder='passthrough'
    )
x_train = preprocessor.fit_transform(x_train) # x_train.shape => (len(df), your_transformer_model_hidden_dim + your_features_count)

x_train would be the data you wanted. it's proper to use with sklearn ecosystem.
gnb = gaussiannb()
gnb.fit(x_train, y_train) 

output:
gaussiannb(priors=none, var_smoothing=1e-09)
caveat: numerical features and the tweets embeddings should belong to the same scale otherwise some would dominate others and degrade the performance",https://stackoverflow.com/questions/69585176,python,15-10-2021 13:03,2079.0,3.0,1.0,True,16-10-2021 12:47,15-10-2021 15:26
64896922,how do i truncate long document for bert?,"i am trying some bert tutorial in my language, document(korean, non-latin)
however, document is very long. so i have to truncate it. and i dont know how.
if there is a text(ex: 5, ""i have a brown cat"") longer than max_length(for ex:3), then which one is the right truncation do i have to make? (dont think about start/end word/mask)
a: [(""i have a""), (""brown cat [pad]"")] or  b:[(""i have a""), (""have a brown""), (""a brown cat"")]
which one should be better?
or is there any better solution?",['nlp'],64897138,"well, the notion is, you have to do the truncation before prepending and appending the [cls] and [sep] tokens. and you can indeed choose the number max tokens of 512, for bert-base model alone. so, the idea is, first you choose the max tokens less than 512 (if you are using bert-base). then, split the sentence to its list of word-pieces, then truncate the sentence to max_tokens - 2. with this, when you add [cls] and [sep] tokens, it would have a number of tokens equal to max_tokens. for any sentence having the number of tokens (including [cls] and [sep]) less than max_tokens, you can always append with zeros.",https://stackoverflow.com/questions/64896922,nlp,18-11-2020 16:13,2059.0,0.0,2.0,True,30-01-2021 15:34,18-11-2020 16:34
74379471,spacy matcher pattern in + regex tag,"my goal is to match with spacy the sentences that contain one of the following words:
['studium','abschluss','ausbildung']
i can solve the problem with this line:
pattern = [{""lower"": {'in':['studium','abschluss', 'ausbildung']}}]

my problem is that in german there is a vast use of composed words like hochschulstudium, masterstudium, studiengang etc.
how can use the regex inside the in sentence to match all words containing the word studium?","['python', 'regex', 'nlp', 'nltk', 'spacy']",74379663,"you can use the regex operator:
import re
l = ['abschluss', 'ausbildung']
pattern = [{'lower': {'regex':fr'^(?:{""|"".join(map(re.escape, l))}|[^\w\d_]*studium)$'}}]

note:

map(re.escape, l) - escapes the items in the l list
""|"".join(...) - joins the words as alternatives (word1|word2|wordn)
^(?:...|[^\w\d_]*studium)$ - a regex that matches

^ - start of string (here, token)
(?:...|[^\w\d_]*studium) - a non-capturing group matching any of the l items or any zero or more letters ([^\w\d_]*) followed with studium
$ - end of string (token here).",https://stackoverflow.com/questions/74379471,python,09-11-2022 18:05,394.0,1.0,1.0,True,09-11-2022 18:19,09-11-2022 18:09
75682331,does chatgpt-3.5-turbo api recounts the tokens in billing when sending the conversation history in api,"when creating a chat app using chatgpt-3.5-turbo model. does the api consider the whole tokens (including the assistant messages and old set of messages) in billing or just the last message from the user is counted in billing whenever i resend the api request with a new message appended to the conversation?
for eg:
messages = [
    {""role"": ""system"", ""content"": ""you are a kind helpful assistant.""},
]
     

while true:
    message = input(""user : "")
    if message:
        messages.append(
            {""role"": ""user"", ""content"": message},
        )
        chat = openai.chatcompletion.create(
            model=""gpt-3.5-turbo"", messages=messages
        )
    
    reply = chat.choices[0].message.content
    print(f""chatgpt: {reply}"")
    messages.append({""role"": ""assistant"", ""content"": reply})","['openai-api', 'chatgpt-api']",75729602,"as mentioned in openai document:

the total number of tokens in an api call affects how much your api call costs, as you pay per token 
both input and output tokens count toward these quantities. for example, if your api call used 10 tokens in the message input and you received 20 tokens in the message output, you would be billed for 30 tokens.

to see how many tokens are used by an api call, check the usage field in the api response
response['usage']['total_tokens']

each time you append previous chats to messages, the number of total_token will increases. so all tokens of previous messages will be considered in the bill.",https://stackoverflow.com/questions/75682331,openai-api,09-03-2023 08:57,2939.0,4.0,1.0,True,27-03-2023 06:41,10-03-2023 07:25
72235951,issue installing spacyr on r 4.2,"i am having trouble installing spacyr on r version 4.2. what is strange is that i've used this package in the past just fine. the installation is pretty basic. per their site you do the following:
install.packages(""spacyr"")
library(""spacyr"")
spacy_install()

however, when i run spacy_install() i get the error: error: one or more python packages failed to install [error code 1] with the traceback:
5.
stop(sprintf(fmt, ...), call. = call.)
4.
stopf(fmt, result)
3.
reticulate::conda_install(envname, packages, pip = pip, conda = conda)
2.
process_spacy_installation_conda(conda, version, lang_models, 
python_version, prompt, envname = envname, pip = pip)
1.
spacy_install()


i've tried uninstalling and reinstalling spacyr.","['r', 'spacy', 'failed-installation']",72905993,"i solved my problem:
i tried more things like install_miniconda() (see the following code block)

install.packages('spacyr')
spacyr:::install_miniconda()
library(spacyr);spacy_install(prompt = false)


but i was still unable to use spacy.
i deleted my miniconda folder in my home directory and then ran the above code and i was able to use spacy.",https://stackoverflow.com/questions/72235951,r,13-05-2022 22:30,714.0,0.0,1.0,True,08-07-2022 02:11,14-05-2022 15:11
66888668,name entity recognition (ner) for multiple languages,"i am writing some code to perform named entity recognition (ner), which is coming along quite nicely for english texts. however, i would like to be able to apply ner to any language. to do this, i would like to 1) identify the language of a text, and then 2) apply the ner for the identified language. for step 2, i'm doubting to a) translate the text to english, and then apply the ner (in english), or b) apply the ner in the language identified.
below is the code i have so far. what i would like is for the ner to work for text2, or in any other language, after this language is first recognized:
import spacy
from spacy_langdetect import languagedetector
from langdetect import detectorfactory

text = 'in 1793, alexander hamilton recruited webster to move to new york city and become an editor for a federalist party newspaper.'
text2 = 'em 1793, alexander hamilton recrutou webster para se mudar para a cidade de nova york e se tornar editor de um jornal do partido federalista.'

# step 1: identify the language of a text
detectorfactory.seed = 0
nlp = spacy.load('en_core_web_sm')
nlp.add_pipe(languagedetector(), name='language_detector', last=true)
doc = nlp(text)
print(doc._.language)

# step 2: ner
entities = [(str(x), x.label_) for x in nlp(str(text)).ents]
print(entities)

does anyone have any experience with this? much appreciated!","['python', 'nlp', 'spacy', 'named-entity-recognition']",66890744,"spacy needs to load the correct model for the right language.
see  for available models.
import spacy
from langdetect import detect
nlp={}    
for lang in [""en"", ""es"", ""pt"", ""ru""]: # fill in the languages you want, hopefully they are supported by spacy.
    if lang == ""en"":
        nlp[lang]=spacy.load(lang + '_core_web_lg')
    else: 
        nlp[lang]=spacy.load(lang + '_core_news_lg')

def entites(text):
     lang = detect(text)
     try:
         nlp2 =nlp[lang]
     except keyerror:
         return exception(lang + "" model is not loaded"")
     return [(str(x), x.label_) for x in nlp2(str(text)).ents]

then, you could run the two steps together
ents = entites(text)
print(ents)",https://stackoverflow.com/questions/66888668,python,31-03-2021 13:16,2163.0,3.0,1.0,True,01-04-2021 18:38,31-03-2021 14:07
67295568,no attribute &quot;str&quot; on dataframe when creating a plot,"i filtered largest 5 tweets with max polarity after sentimental analysis.
maxx = df.nlargest(5,['polarity']).astype(str)
maxx

output:
unnamed: 0  clean_tweet tweet_tokenized polarity    subjectivity    sentiment_type  scores  compound    sentiment_type  pca
315 315 best ofï¿½ï¿½ï¿½ luck bidenï¿½ï¿½ï¿½ï¿½   best / luck biden   1.0 0.3 positive    {'neg': 0.0, 'neu': 0.122, 'pos': 0.878, 'comp...   0.802   positive    [-0.06151099614792966, -0.030998756958434074]

and now i'd like to create some wordcloud but i'm getting error:
hero.wordcloud(maxx, max_words=100)

attributeerror: 'dataframe' object has no attribute 'str'","['python', 'pandas', 'string', 'dataframe', 'nlp']",67295663,"sorting based on polarity
df['polarity'] = df['polarity'].astype('float')   
maxx = df.nlargest(5, 'polarity')

if you are using wordcloud package try this
from wordcloud import wordcloud

text_data = ' '.join(maxx['clen_tweet']) 

wordcloud = wordcloud().generate(text_data)
plt.imshow(wordcloud2)
plt.axis(""off"")
plt.show()

using texthero
import texthero as hero

hero.wordcloud(maxx['clean_tweet'], max_words=100)",https://stackoverflow.com/questions/67295568,python,28-04-2021 07:20,87.0,0.0,1.0,True,28-04-2021 07:35,28-04-2021 07:31
78698002,how can i get the lastmodifieddate (or any date) in azure open ai service maps?,"i created an azure resource for testing that i am populating with a custom blob i created in azure only for testing, i created the skillset, the indexer, the index, everything for the retrieval of the date and up until the index all the data is being sent properly:
example 1
example 2
my main interest is to do a hybrid search (so i can use a scoring profile i have created to use freshness... and then when i go to open azure ai the date is not part of the mapping and the files are not getting the date, and i really do not know why:
example 3
example 4
i have gone back and forth between index (up until where all the data is being sent properly) and open azure ai tool, but i have not found how to make it take the date into consideration, does anybody know how ?","['azure', 'openai-api', 'langchain', 'azure-openai', 'azure-ai-search']",78720869,"the field of type datetimeoffset is not supported while doing custom field mapping in azure open ai.
all of the fields content data, file name, title,url and vector fields asked is of type string.
whatever the fields you do map for content data will be used by model for search query and the references are given by the field file name.
if you want your date as a result in the reference you need to convert the type to string.

then you will get the date field for mapping.
if you map it to file name then instead of file name you will get the date as reference but no filename.

next, if you want to search in the query like
""give me content before 20th apr 2024""
kind of query then make the date field as string type and searchable as true then map it to content data.",https://stackoverflow.com/questions/78698002,azure,02-07-2024 15:27,211.0,0.0,1.0,True,08-07-2024 12:30,03-07-2024 07:57
67003269,&#39;word not in the vocabulary&#39; when evaluating similarity using gensim word2vec.most_similar method,"through the method
gensim.models.word2vec.most_similar
i get the top-n most similar words.
i trained a model with a list of sentences like
list_of_list = [[""i like going to the beach""],
                [""the war is over""], 
                [""we are all made of stars""],  
                         ...
                [""i don't know what to do""]] 
model = gensim.models.word2vec(list_of_list, size=100, window=longest_list, min_count=2)

suggestions = model.most_similar(""i don't know what to do"", topn=10)       

and i wanted to evaluate phrases similarity.
if for example i run
suggestions = model.most_similar(""i don't know what to do"", topn=10)       

it works correctly.
but if i give a subquery like ""to the beach"" or ""what to do"", it returns an error message because the sub-phrase is not in the vocabulary.
 ""word 'to the beach' not in vocabulary""

how can i solve this issue without training again the model?
how can the model identify the most similar phrases based on a new phrase, not necessary a subphrase?","['python', 'nlp', 'gensim', 'similarity']",67004013,"it seems that you are not training the word2vec model correctly. sentences should be lists of words not list of single strings. so, if you change it to:
list_of_list = [[""i like going to the beach""],
                [""the war is over""], 
                [""we are all made of stars""],  
                         ...
                [""i don't know what to do""]]

list_for_training = [sent[0].split() for sent in list_of_list]

and use list_for_training as the first parameter of the constructor of word2vec.
similarly, when calling most_similar method, provide a list of strings instead of a string:
suggestions = model.most_similar(""i don't know what to do"".split(), topn=10)  

or
suggestions = model.most_similar(""to the beach"".split(), topn=10)",https://stackoverflow.com/questions/67003269,python,08-04-2021 11:41,419.0,1.0,1.0,True,08-04-2021 12:50,08-04-2021 12:50
1783653,computing precision and recall in named entity recognition,"now i am about to report the results from named entity recognition. one thing that i find a bit confusing is that my understanding of precision and recall was that one simply sums up true positives, true negatives, false positives and false negatives over all classes.
but this seems implausible now that i think of it as each misclassification would give simultaneously rise to one false positive and one false negative (e.g. a token that should have been labelled as ""a"" but was labelled as ""b"" is a false negative for ""a"" and false positive for ""b""). thus the number of the false positives and the false negatives over all classes would be the same which means that precision is (always!) equal to recall. this simply can't be true so there is an error in my reasoning and i wonder where it is. it is certainly something quite obvious and straight-forward but it escapes me right now.","nlp, precision-recall",1814764,"the way precision and recall is typically computed (this is what i use in my papers) is to measure entities against each other. supposing the ground truth has the following (without any differentiaton as to what type of entities they are)
[microsoft corp.] ceo [steve ballmer] announced the release of [windows 7] today
this has 3 entities.
supposing your actual extraction has the following
[microsoft corp.] [ceo] [steve] ballmer announced the release of windows 7 [today]
you have an exact match for microsoft corp, false positives for ceo and today, a false negative for windows 7 and a substring match for steve
we compute precision and recall by first defining matching criteria. for example, do they have to be an exact match? is it a match if they overlap at all? do entity types matter? typically we want to provide precision and recall for several of these criteria.
exact match: true positives = 1 (microsoft corp., the only exact match), false positives =3 (ceo, today, and steve, which isn't an exact match), false negatives = 2 (steve ballmer and windows 7)
precision = true positives / (true positives + false positives) = 1/(1+3) = 0.25
recall = true positives / (true positives + false negatives) = 1/(1+2) = 0.33

any overlap ok: true positives = 2 (microsoft corp., and steve which overlaps steve ballmer), false positives =2 (ceo, and today), false negatives = 1 (windows 7)
precision = true positives / (true positives + false positives) = 2/(2+2) = 0.55
recall = true positives / (true positives + false negatives) = 2/(2+1) = 0.66

the reader is then left to infer that the ""real performance"" (the precision and recall that an unbiased human checker would give when allowed to use human judgement to decide which overlap discrepancies are significant, and which are not) is somewhere between the two.
it's also often useful to report the f1 measure, which is the harmonic mean of precision and recall, and which gives some idea of ""performance"" when you have to trade off precision against recall.",https://stackoverflow.com/q/1783653,"nlp, precision-recall",23-11-2009 15:00,21634.0,38.0,7.0,True,28-05-2023 20:22,12-03-2016 13:30
13858983,increasing relevancy of search results,"i have a problem with making search output more practically usefull for the end users. the problem is rather related to the algorithm and approach then to exact technology or framework to use.
at the moment we have a database of products, that can be described with following schema:
 
from the search perspective we've done pretty standard things, 3-rd party text search with token analyzer, handling mistypes and synonyms (it is not the full list, but as i said, it is rather out of scope). but stil we need to perform extra work to make the search result closer to real life user needs, probably, in somewhat similar way how google ranks indexed pages by relevancy. ideas, that we`ve already considered as potentially applicable in solving the problem:

analyze most popular search requests in widespread search engines (it is still a question how to get them) and increase rank for those entries in the index, which correspond (could be found with) to the popular requests;
increase rank for newest (hot) entries;
increase rank for the biggest group of entries, which correspond to the popular request and have something in common (that`s why it is a group);

appreciate for any help or advising a direction, where to dig.","['algorithm', 'search', 'nlp', 'full-text-search', 'search-engine']",13862135,"you may try plsa; there are many references on the web, and there should be libraries and source code.
edit:
well, i took a closer look at lucene recently, and it seems to give a much better answer to what the question actually asked (it does not use plsa). as for the integration with db, you may use hibernate search (although it does not seem to be as powerful as using lucene directy is).",https://stackoverflow.com/questions/13858983,algorithm,13-12-2012 11:36,121.0,1.0,1.0,True,09-10-2024 21:20,09-10-2024 21:20
65145665,how to remove rows that have english (or a specific languages) sentences in pandas,"i have a pandas data frame which has 2 columns, first contains arabic sentences and the second one contain labels (1,0)
i want to remove all rows that contain english sentences.
any suggestions?
here is an example, i want to remove the second row

ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ [0]
border patrol agents recover 44 migrants from stash house [0]
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½","['python', 'pandas', 'nlp']",65145720,"you can create an array of common english letters and remove a line which contain either of these letters, like this:
engletters = ['a', 'o', 'i']
df = df[~df['text_col'].str.contains('|'.join(engletters))]",https://stackoverflow.com/questions/65145665,python,04-12-2020 14:47,601.0,0.0,3.0,True,06-01-2023 08:53,06-01-2023 08:53
76072098,removing html code from text data in spark,"i'm working with stack exchange data dumps and i want to clean body of a question from code which occurs really often.
i tried using beautifulsoup but it leaves the text which occurs inside the code and i want to remove everything between  < pre >< code>  and < /code > < /pre >. maybe it can be done with regular expressions? preferably with spark.","['pyspark', 'nlp', 'text-processing']",76076955,"something like this might work
# re example
import re
s1 = ""< pre >< code> some sample code < /code > < /pre >.""
p = re.compile(r""< pre >< code>(.*?)< /code > < /pre >"")
print(p.search(s1).group(1))

# convert to udf
from pyspark.sql.functions import udf, col
from pyspark.sql.types import stringtype
codefinder = udf(lambda x: p.search(x).group(1), returntype=stringtype())
df = df.withcolumn('codetext', codefinder(col('htmlcol')))

if i misunderstood your question and you wanted to strip out the code blocks then something like this might work
# re example
import re
s2 = ""a text <pre><code> some sample code </code></pre> b text""
s3 = ""a text <pre><code> some sample code </code></pre> b text <pre><code> some sample code </code></pre> c text""
s4 = """"""here is the sample of the post data: but if i open it using mne it the first list is eeg data, and the second list (inside the list of eeg data) are two list of eeg data.</p>\n\n<p>this is how i open the data using mne</p>\n\n<pre><code>raw_file=read_raw_edf(""e:\eegdata\\256\s02_reduced.bdf"",preload=true,verbose=true)\n</code></pre>\n\n<p>am i missing something here?</p>\n""""""
pattern = r""(?s)<pre><code>(.*?)</code></pre>""
print(re.sub(pattern=pattern, repl='', string=s2))  # a text  b text
print(re.sub(pattern=pattern, repl='', string=s3))  # a text  b text  c text
print(re.sub(pattern=pattern, repl='', string=s4))  


# udf example
from pyspark.sql.functions import udf, col
from pyspark.sql.types import structtype,structfield, stringtype, integertype, timestamptype, floattype
codestripper = udf(lambda x: re.sub(pattern=pattern, repl='', string=x), returntype=stringtype())
testschema = structtype([
  structfield(""html"",stringtype(),false),
])
df = spark.createdataframe([{'html':s3}, {'html':s4}], testschema)
df = df.withcolumn('codelesstext', codestripper(col('html')))
df.topandas()",https://stackoverflow.com/questions/76072098,pyspark,21-04-2023 10:03,491.0,-2.0,1.0,True,22-04-2023 19:23,21-04-2023 10:11
55692700,how to apply a sentence-level lda model using gensim?,"is it possible to apply a sentence-level lda model using gensim as proposed in bao and datta(2014)?  the paper is here.  
the distinct feature is that it makes the ""one topic per sentence assumption"" (p.1376). this is different from other sentence-level methods, which typically allow each sentence to include multiple topics. ""the most straightforward method is to treat each sentence as a document and apply the lda model on the collection of sentences rather than documents."" (p.1376). but, i think it is more reasonable to assume that one sentence deals with one topic. 
thank you!","['python', 'nlp', 'gensim', 'lda']",55695487,"you can run what brody & elhadad (2010) call local-lda - just feeding your text data to lda sentence by sentence - easily, if you split your documents into sentences. however, lda will still give you more than one topic per sentence (by definition, you get values for all topics, although gensim has the minimum_probabiliy default of 0.01), which of course is not the same as the approach proposed by bao & datta.
however, the supplemental material to the article by bao & datta (2014) contains a c or c++ (i assume, it doesn't say in the readme) .exe plus usage instructions in the materials. you could just run that from the command line, or write a wrapper for python (to make the output in gensim format would be icing on the cake) - if you do, please share your code, it might be helpful to others.",https://stackoverflow.com/questions/55692700,python,15-04-2019 15:36,1367.0,0.0,1.0,True,23-07-2021 06:07,15-04-2019 19:36
69743520,putting in pieces of information in a nested dictionary (python),"i'm trying to create a nested dictionary that tells me what document each word appears in and in which position it appears in: for example:
dictionary ={}
textfile_list = ['file1.txt', 'file2.txt', 'file3.txt']
file_contents = ['mario luigi friend mushroom', 'rick mario morty portal summer mario', 'peter griffin shop'] 
#first element corresponds to the contents of file1.txt and etc.

words = [['mario', 'luigi', 'friend', 'mushroom'],
        ['rick', 'mario', 'morty', 'portal', 'summer', 'mario'],
        ['peter', 'griffin', 'shop']] #tokenising the text

i'd want print(dictionary['mario']) to give [{'file1.txt': [0]}, {'file2.txt': [1,5]} ]
my code so far is:
dict = {}
for i in range(len(textfile_list)):
    check = file_contents
    for item in words:  #a list of every word from every file ['word1','wordn','word3',...]
  
        if item in check:
            if item not in dict:
                dict[item] = []
  
            if item in dict:
                dict[item].append(textfile_list[i])

dict = {k: list(set(v)) for k, v in dict.items()}


i don't know how to implement the postion of the word in a nested dictionary which i don't have at the moment! could anyone help?","['python', 'list', 'dictionary', 'nlp']",69745771,"you have one layer of nesting too many. your first description corresponds to a dictionary whose keys are words, and whose values are dictionaries of (filename, position_list) pairs (e.g. dictionary['mario'] = {'file1.txt': [0], 'file2.txt': [1, 5]} ) rather than a dictionary whose keys are words, and whose values are a list of dictionaries with one filename per dictionary, as you had.
textfile_list = ['file1.txt', 'file2.txt', 'file3.txt']
file_contents = ['mario luigi friend mushroom', 'rick mario morty portal summer mario',
                 'peter griffin shop']
# first element corresponds to the contents of file1.txt and etc.

# words = [string_list.split() for string_list in file_contents]

words = [['mario', 'luigi', 'friend', 'mushroom'],
         ['rick', 'mario', 'morty', 'portal', 'summer', 'mario'],
         ['peter', 'griffin', 'shop']]  # tokenising the text

dictionary = {}

for textfile_name, file_strings in zip(textfile_list, words):
    for position, word in enumerate(file_strings):
        if word not in dictionary:
            dictionary[word] = {}
        if textfile_name not in dictionary[word]:
            dictionary[word][textfile_name] = []

        dictionary[word][textfile_name].append(position)

print(dictionary['mario'])
>>> {'file1.txt': [0], 'file2.txt': [1, 5]}

i'm not sure what the final line is for, since there are no duplicates currently; in any case, don't use dict as a variable name in python, since it's a builtin.",https://stackoverflow.com/questions/69743520,python,27-10-2021 18:07,57.0,0.0,1.0,True,27-10-2021 21:37,27-10-2021 19:55
68585678,"pytorch summary fails with huggingface model ii: expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu","i want a summary of a pytorch model downloaded from huggingface:
from torchinfo import summary
from transformers import automodelforsequenceclassification

model = automodelforsequenceclassification.from_pretrained('bert-base-uncased', num_labels=2)
summary(model, input_size=(16, 512), dtypes=['torch.inttensor'])

(see so for why the dtypes is needed.)
however, i am getting the error expected all tensors to be on the same device, ... even though i have not provided any tensors. see the output below.
how can i fix this?


---------------------------------------------------------------------------

runtimeerror                              traceback (most recent call last)

/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py in forward_pass(model, x, batch_dim, cache_forward_pass, device, **kwargs)
    257             if isinstance(x, (list, tuple)):
--> 258                 _ = model.to(device)(*x, **kwargs)
    259             elif isinstance(x, dict):

11 frames

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # do not call functions when jit is used

/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)
   1530             output_hidden_states=output_hidden_states,
-> 1531             return_dict=return_dict,
   1532         )

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1070 
-> 1071         result = forward_call(*input, **kwargs)
   1072         if _global_forward_hooks or self._forward_hooks:

/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)
    988             inputs_embeds=inputs_embeds,
--> 989             past_key_values_length=past_key_values_length,
    990         )

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1070 
-> 1071         result = forward_call(*input, **kwargs)
   1072         if _global_forward_hooks or self._forward_hooks:

/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)
    214         if inputs_embeds is none:
--> 215             inputs_embeds = self.word_embeddings(input_ids)
    216         token_type_embeddings = self.token_type_embeddings(token_type_ids)

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1070 
-> 1071         result = forward_call(*input, **kwargs)
   1072         if _global_forward_hooks or self._forward_hooks:

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py in forward(self, input)
    159             input, self.weight, self.padding_idx, self.max_norm,
--> 160             self.norm_type, self.scale_grad_by_freq, self.sparse)
    161 

/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2042         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-> 2043     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   2044 

runtimeerror: expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument index in method wrapper_index_select)


the above exception was the direct cause of the following exception:

runtimeerror                              traceback (most recent call last)

<ipython-input-13-d6f4e53beef7> in <module>()
      3 else:
      4     # can't get this working. see 
----> 5     summary(model, input_size=(16, 512), dtypes=['torch.inttensor'])
      6     print(model)

/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py in summary(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, row_settings, verbose, **kwargs)
    190     )
    191     summary_list = forward_pass(
--> 192         model, x, batch_dim, cache_forward_pass, device, **kwargs
    193     )
    194     formatting = formattingoptions(depth, verbose, col_names, col_width, row_settings)

/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py in forward_pass(model, x, batch_dim, cache_forward_pass, device, **kwargs)
    268             ""failed to run torchinfo. see above stack traces for more details. ""
    269             f""executed layers up to: {executed_layers}""
--> 270         ) from e
    271     finally:
    272         if hooks is not none:

runtimeerror: failed to run torchinfo. see above stack traces for more details. executed layers up to: []


output from transformers-cli:
- `transformers` version: 4.9.1
- platform: linux-5.4.104+-x86_64-with-ubuntu-18.04-bionic
- python version: 3.7.11
- pytorch version (gpu?): 1.9.0+cu102 (true)
- tensorflow version (gpu?): 2.5.0 (true)
- flax version (cpu?/gpu?/tpu?): not installed (na)
- jax version: not installed
- jaxlib version: not installed
- using gpu in script?: <fill in>
- using distributed or parallel set-up in script?: <fill in>","['python', 'pytorch', 'huggingface-transformers']",68769749,"a working solution (or workaround?) is kind of obvious:
summary(model, input_size=(16, 512), dtypes=['torch.inttensor'], device='cpu')",https://stackoverflow.com/questions/68585678,python,30-07-2021 05:07,1978.0,4.0,1.0,True,13-08-2021 09:17,31-07-2021 07:53
75209070,txtai.database.sql.base.sqlerror: no such function: json_extract,"using txtai python module with sql query select id, text, score, solution_id, column_name from txtai where similar('{query}') and score >= 0.5, i am seeing this error txtai.database.sql.base.sqlerror: no such function: json_extract
i am trying to use dynamic search using txtai module, which uses a sql based context manager but it is not working as expected.
this are working perfectly on my widows machine, but the same is not happening on the centos server. i have tried to maintain all the modules and packages versions same using requirement.txt files.","['python', 'huggingface-transformers', 'sentence-transformers']",75210108,"it looks like the version of sqlite packaged with centos 7.9 doesn't have the json1 extension enabled. there may be a 3rd party repository with a newer version.
the link below shows how sqlite could be recompiled with json1 enabled.

alternatively, you can try with another linux distro. almost all distros released within the last 5 years have json1 support enabled.",https://stackoverflow.com/questions/75209070,python,23-01-2023 11:47,300.0,0.0,1.0,True,26-01-2023 14:09,23-01-2023 12:41
78656632,"the completion operation does not work with the specified model, gpt-4","i have this langchain code for answering questions by getting similar docs from the vector store and using llm to get the answer of the query:
llm_4 = azureopenai(
        # temperature=0,
        api_version= os.environ['openai_api_version_4'], 
        openai_api_key= os.environ['azure_openai_api_key_4'], 

        deployment_name=""gpt4-deploy"",
        # model_name=""gpt4-o"",
        azure_endpoint=os.environ['azure_openai_endpoint_4']
    )

    llm_3 = azureopenai(
        # temperature=0,
        api_version= os.environ['openai_api_version_3'], 
        openai_api_key= os.environ['azure_openai_api_key_3'], 

        deployment_name=""test-deployment"", 
        # deployment_name=""gpt-16k-deployment"",
        # model_name=""gpt-3.5-turbo-16k"",

        azure_endpoint=os.environ['azure_openai_endpoint_3']
    )

    response=get_answer(relavant_docs, user_input, llm_4)

...
#create embeddings instance
def create_embeddings():
    #embeddings = openaiembeddings()
    embeddings = sentencetransformerembeddings(model_name=""all-minilm-l6-v2"")
    # embeddings = sentencetransformerembeddings(model_name=""text-davinci-003"") 
    return embeddings


def get_answer(docs, user_input, llm=none):
    if llm:
        chain = load_qa_chain(llm, chain_type=""stuff"")
    else:
        chain = load_qa_chain(openai(), chain_type=""stuff"")

    with get_openai_callback() as cb:
        response = chain.run(input_documents=docs, question=user_input)
    return response

it's working with gpt3, but with gpt4 getting:

badrequesterror: error code: 400 - {'error': {'code': 'operationnotsupported', 'message': 'the completion operation does not work with the specified model, gpt-4. please choose different model and try again. you can learn more about which models can be used with each operation here: 

i tried what was suggested by these similar issues:
how to use the new gpt-3.5-16k model with langchain?
i am trying to make a docs question answering program with azureopenai and langchain
but i still didn't figure out how to solve it!","['azure', 'langchain', 'azure-ai']",78660975,"just giving it has an answer. so, that it helps community to find better solution.
internally, the chain invokes the completion api to the model which it is not supported and gives you the error.
in such case you can either use supported model or azure openai chat client azurechatopenai.
to see which model are supported for completions operation,
in the model tab you check mark completion and save.

as you can see the instruct model are supported.",https://stackoverflow.com/questions/78656632,azure,22-06-2024 16:32,5476.0,2.0,1.0,True,24-06-2024 07:07,23-06-2024 07:13
60794145,python glove missing module &#39;glove&#39; &#39;glove&#39;,"here is what i performed:
installed pip3 install glove_py ok.
in jupyter python, import glove works ok.
from glove import *

problem:
when i try a basic setup code to ensure all the modules are loaded and working. i have this code, which errors on message: ""nameerror: name 'glove' is not defined"". now since module glove import works ok, i have tried function 'glove' and 'glove', both with nameerror not defined. 
i did find libraries like 'git clone  and did download and build the code with make. this code ran ok in the console for a sample. 
pip3 install glove_py

pip install for glove_py installed ok. 
pip3 install glove_python

but pip install for glove_python failed to install with ""error command errored out with exit status 1:"". 
glove && make
mkdir -p build

glove 'git clone  download ok and build with make ok. but even with this make'd version, i was not able to get the python import glove to find this c code make realized inside the jupyter python environment.
i suspect that i am missing something simple, i would appreciate any insight.
python code, test run. here is my python code test run which failed on module not found.
model = glove(df, vocab_size=3, d=50, alpha=0.75, x_max=100.0)
model.train(df)
model.to_txt()
words = model.most_similary(""one"", 10)

nameerror                                 traceback (most recent call last)
<ipython-input-11-517b339bba36> in <module>
----> 1 model = glove(df, vocab_size=3, d=50, alpha=0.75, x_max=100.0)
      2 model.train(df)
      3 model.to_txt()
      4 words = model.most_similary(""one"", 10)
      5 print(words)

nameerror: name 'glove' is not defined

directory function to see the functions inside 'gl' module, imported from glove package, no module function names showed. so this clearly shows that the import of glove as gl, had some issues.
dir(gl)

['__doc__',
 '__file__',
 '__loader__',
 '__name__',
 '__package__',
 '__path__',
 '__spec__']","['python-3.x', 'nlp', 'text-mining', 'glove']",60854302,"what you want is the glove class inside the module; note the capital letter.
i think this line
glove(df, vocab_size=3, d=50, alpha=0.75, x_max=100.0)
should be
glove(df, vocab_size=3, d=50, alpha=0.75, x_max=100.0)",https://stackoverflow.com/questions/60794145,python-3.x,21-03-2020 22:51,2946.0,0.0,1.0,True,19-01-2022 16:37,25-03-2020 23:21
70195392,difference between model-best and model-last in spacy,"after model training, spacy has generated model\model-best and model\model-last folders. what's the difference between the two models and which one should be used for predictions?","['python', 'machine-learning', 'nlp', 'spacy', 'spacy-3']",70209537,"model-best is the model that got the highest score on the dev set. it is usually the model you would want to use.
model-last is the model trained in the last iteration. you might want to use it if you resume training.",https://stackoverflow.com/questions/70195392,python,02-12-2021 07:13,1679.0,3.0,1.0,True,03-12-2021 04:27,02-12-2021 07:34
77511368,llm slow inference even on a100 gpu,"i am planning to deploy a fine-tuned version of open-orca-platypus-2. it takes around 13.5gb on the gpu. i tried using g4dn.12xlarge in aws which has 4 gpus, but the inference still takes around 40 seconds. i also tried it on a100 gpu provided by colab, but still the same.
what am i doing wrong? do i still need more computational power or is anything wrong with my code?

    from transformers import autotokenizer, automodelforcausallm
    import torch
    import os

    os.environ[""cuda_visible_devices""] = ""0,1,2,3""
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    # load model
    model = automodelforcausallm.from_pretrained(
        model_path,
        device_map=""auto""
    )

    # set the model to evaluation mode
    model.eval()

    # load tokenizer
    tokenizer = autotokenizer.from_pretrained(""open-orca/openorca-platypus2-13b"", trust_remote_code=true)

    def ask_bot(question):
        with torch.no_grad():
            # tokenize input question
            input_ids = tokenizer.encode(question, return_tensors=""pt"").cuda()

            # generate output
            output = model.module.generate(
                input_ids,
                max_length=200,
                num_return_sequences=1,
                do_sample=true,
                top_k=50
            )

        # decode and extract the response
        generated_text = tokenizer.decode(output[0], skip_special_tokens=true)
        response = generated_text.split(""->:"")[-1]
        return response","['nlp', 'gpu', 'huggingface-transformers', 'large-language-model']",77592933,"it does take a long time to generate an output even on powerful gpus. my use-case was a chatbot, so i figured it would be ideal to stream the output token by token as generated by the model. this reduced the perceived time although the actual output would remain the same.
from transformers import autotokenizer, automodelforcausallm, textiteratorstreamer

    with torch.no_grad():
    # tokenize input question
    input_ids = tokenizer.encode(question, return_tensors=""pt"", truncation=true).cuda()
    streamer = textiteratorstreamer(
        tokenizer=tokenizer, timeout=60.0, skip_prompt=true, skip_special_tokens=true
    )
    def generate_and_signal_complete():
        output = model.generate(
            input_ids,
            max_length=1500,
            num_return_sequences=1,
            do_sample=true,
            top_k=50,
            streamer=streamer
        )
t1 = thread(target=generate_and_signal_complete)
t1.start()
# decode and extract the response
for new_text in streamer:
    yield new_text",https://stackoverflow.com/questions/77511368,nlp,19-11-2023 15:31,1405.0,0.0,2.0,True,03-12-2023 03:16,19-11-2023 22:23
73186315,openai command not found (mac),"i'm trying to follow the fine tuning guide for openai here.
i ran:
pip install --upgrade openai

which install without any errors.
but even after restarting my terminal, i still get
zsh: command not found: openai

here is the output of echo $path:
/bin:/usr/bin:/usr/local/bin:/users/nickrose/downloads/google-cloud-sdk/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin

here is the output of which python:
/usr/bin/python

any tips for how to fix this? i'm on macos big sur 11.6.","['python', 'machine-learning', 'openai-api']",73186415,"basically pip installs the packages under its related python directory, in a directory called site-packages (most likely, i'm not a python expert tbh). this is not included in the path you provided. first, ask pip to show the location to the package:
pip show openai

the output would be something like this:
name: openai
version: 0.22.0
summary: python client library for the openai api
home-page: 
author: openai
author-email: support@openai.com
license: 
location: /users/<user>/dir/to/some/python/site-packages
requires: numpy, openpyxl, pandas, pandas-stubs, requests, tqdm
required-by:

so your package will be available in
/users/<user>/dir/to/some/python/site-packages/openai

either add /users/<user>/dir/to/some/python/site-packages/ to your path, or use the complete address to your package, or try to access it using your python:
python -m openai # -m stands for module

to get more information about the -m flag, run python --help.
update
so as you mentioned in the comments, you get permission denied after you add the directory to your package. this actually means that the package exists, but it's not permitted by your os to execute. this is the thing you have to do, locate your package, and then:
sudo chmod +x /path/to/script

and the reason you're getting command not found after you use sudo directly with the package, is that you update your path variable in zsh, but when you use sudo, superuser uses sh instead of zsh.",https://stackoverflow.com/questions/73186315,python,31-07-2022 19:28,15493.0,6.0,9.0,True,04-08-2023 09:42,31-07-2022 23:18
76906469,langchain zero shot react agent uses memory or not?,"i'm experimenting with langchain's agenttype.chat_zero_shot_react agent. by its name i'd assume this is an agent intended for chat use and i've given it memory but it doesn't seem able to access its memory. what else do i need to do so that this will access its memory? or have i incorrectly assumed that this agent can handle chats?
here is my code and sample output:
llm = chatopenai(model_name=""gpt-4"",
                 temperature=0)

tools = load_tools([""llm-math"", ""wolfram-alpha"", ""wikipedia""], llm=llm)
memory = conversationbuffermemory(memory_key=""chat_history"")

agent_test = initialize_agent(
    tools=tools, 
    llm=llm, 
    agent=agenttype.chat_zero_shot_react_description, 
    handle_parsing_errors=true,
    memory=memory, 
    verbose=true
)

>>> agent_test.run(""what is the height of the empire state building?"")
'the empire state building stands a total of 1,454 feet tall, including its antenna.'
>>> agent_test.run(""what was the last question i asked?"")
""i'm sorry, but i can't provide the information you're looking for.""","['python', 'langchain', 'large-language-model', 'py-langchain']",76930307,"it does not. that's indicated by zero-shot which means just look at the current prompt. from here

zero-shot means the agent functions on the current action only ï¿½ï¿½ï¿½ it
has no memory. it uses the react framework to decide which tool to
use, based solely on the toolï¿½ï¿½ï¿½s description.

i think when you work with this agent type, you should add description to the tool. so that based on the description, llm will infer which tool used. that is the description part of """"zero-shot-react-description"". example from same link above:
math_tool = tool(
    name='calculator',
    func=llm_math.run,
    description='useful for when you need to answer questions about math.'
)

when the llm sees the prompt, if it infers that prompt is related to math, it will use the math_tool
if you want to use memory, you should be us"" rel=""noreferrer"">chat-conversational-react-description 
from langchain.memory import conversationbuffermemory
from langchain.chat_models import chatopenai

memory = conversationbuffermemory(memory_key=""chat_history"", return_messages=true)
llm = chatopenai(openai_api_key=openai_api_key, temperature=0)
agent_chain = initialize_agent(tools, llm, agent=agenttype.chat_conversational_react_description, verbose=true, memory=memory)",https://stackoverflow.com/questions/76906469,python,15-08-2023 13:47,8090.0,2.0,3.0,True,10-01-2024 15:14,10-01-2024 15:14
67888938,how can i scrape the id tags and their content(text) from a website?,"at the top of this site are 17 id tags:
1.boxed warning
2.indications
3.dosage/administration
4.dosage forms
5.contraindications
6.warnings/precautions
7.adverse reactions
8.drug interactions
9.specific populations
10.overdosage
11.description
12.clinical pharmacology
13.nonclinical toxicology
14.clinical studies
15.how supplied
16.patient counseling
17.medication guide

i want to scrape the page and make a dictionary with those tags as the keys. how can i do this? here's what i've tried so far:
urls = ""
response = requests.get(urls)
soup = beautifulsoup(response.text, 'html.parser')
data3 = soup.findall('h2')
out = {}
y1 = []
y2 = []
for header in data3:
   x0 = header.get('id')
   y1.append(x0)
   nextnode = header
   while true:
      nextnode = nextnode.nextsibling
      if nextnode is none:
          break
      if isinstance(nextnode, navigablestring):
          x1 = nextnode.strip()
      if isinstance(nextnode, tag):
          if nextnode.name == ""h2"":
              break

      x2 = nextnode.get_text(strip=true).strip()
      x3 = x1 + "" "" + x2
      y2.append(x3)
 print(y1,y2)

i'm getting
output i'm getting: [none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none] [content]

desired output: ['boxed warning', 'indications', 'dosage/administration', 'dosage forms', 'contraindications', 'warnings/precautions', 'adverse reactions', 'drug interactions', 'specific populations', 'overdosage', 'description', 'clinical pharmacology', 'nonclinical toxicology', 'clinical studies', 'how supplied', 'patient counseling', 'medication guide'] ['content present under boxed warning', 'content present under indications']

how can i get a dictionary or list that replaces all the nones with the list of tags? i'm struggling to work with the structure of the webpage. thank you!","['python', 'beautifulsoup', 'nlp']",67910219,"i'm not 100% sure what you need, but based on the comments i think this is what you are looking for.  you can easily add the output to a list or a dictionary.
import requests
from bs4 import beautifulsoup
urls = ""
response = requests.get(urls)
soup = beautifulsoup(response.text, 'html.parser')
tags = soup.find('div', {'class': 'ddc-anchor-links'})

available_information = []

for tag in tags.find_all('a'):
    available_information.append(tag.text)
    

print(available_information)
# output
['boxed warning', 'indications and usage', 'dosage and administration', 'dosage forms and strengths', 'contraindications', 'warnings and precautions', 'adverse reactions/side effects', 'drug interactions', 'use in specific populations', 'overdosage', 'description', 'clinical pharmacology', 'nonclinical toxicology', 'clinical studies', 'how supplied/storage and handling', 'patient counseling information', 'medication guide']



you can obtain the content for each toc using this code:
anchor_tags = []
soup = beautifulsoup(response.text, 'html.parser')
tags = soup.find('div', {'class': 'ddc-toc-content'})
for tag in tags.find_all('a'):
    anchor_tag = str(tag['href']).replace('#', '')
    anchor_tags.append(anchor_tag)

for tag in anchor_tags:
    anchor_tag = soup.find(""a"", {""id"": tag})
    header_tag = anchor_tag.find_next_sibling('h2')
    # now you need to figure out how you want to store this information that is being extracted. 

based on our chat conversation you can query multiple pages that have different structures this way.  you will have to modified the search_terms and known_tags as you scrape more pages with different structures.
import requests
from bs4 import beautifulsoup

def get_soup(target_url):
    response = requests.get(target_url)
    soup = beautifulsoup(response.text, 'html.parser')
    return soup

def obtain_toc_content(soup):
    available_information = []
    anchor_tags = []
    known_tags = ['div', 'ul']
    search_terms = ['ddc-toc-content', 'ddc-anchor-links']
    for tag, search_string in zip(known_tags, search_terms):
        tag_found = bool(soup.find(tag, {'class': search_string}))
        if tag_found:
            toc = soup.find(tag, {'class': search_string})
            for toc_tag in toc.find_all('a'):
                available_information.append(toc_tag.text)
                anchor_tag = str(toc_tag['href'])
                anchor_tags.append(anchor_tag)

    return available_information, anchor_tags


urls = ['
        '
for url in urls:
    make_soup = get_soup(url)
    results = obtain_toc_content(make_soup)
    table_of_content = results[0]
    toc_tags = results[1]",https://stackoverflow.com/questions/67888938,python,08-06-2021 14:22,224.0,3.0,2.0,True,10-06-2021 17:02,09-06-2021 15:22
70710646,tokenize sentence into words python,"i want to extract information from different sentences so i'm using nltk to divide each sentence to words, i'm using this code:
words=[]
for i in range(len(sentences)):
    words.append(nltk.word_tokenize(sentences[i]))
    words

it works pretty good but i want something little bit different .. for example i have this sentence :
'[\'jan 31 19:28:14 nginx: 10.0.0.0 - - [31/jan/2019:19:28:14 +0100] ""post /test/itf/  404 146 ""-"" ""mozilla/5.2 [en] (x11, u; openvas-xx 9.2.7)""\']'
i want ""mozilla/5.2 [en] (x11, u; openvas-xx 9.2.7)"" to be one word and not divided to several single words .
update:
i want something like that:
[
 'jan',
 '31',
 '19:28:14',
 'nginx',
 '10.0.0.0',
 '31/jan/2019:19:28:14',
 '+0100',
 'post',
 '/test/itf/',
 '
 '404',
 '146',
 'mozilla/5.2 [en] (x11, u; openvas-xx 9.2.7)']

any idea to make it possible !?
thank you in advance","['python', 'token', 'nltk', 'tokenize']",70712148,"you can import re and parse the log line (which is not a natural language sentence) with a regex:
import re

sentences = ['[\'jan 31 19:28:14 nginx: 10.0.0.0 - - [31/jan/2019:19:28:14 +0100] ""post /test/itf/  404 146 ""-"" ""mozilla/5.2 [en] (x11, u; openvas-xx 9.2.7)""\']']

rx = re.compile(r'\b(\w{3})\s+(\d{1,2})\s+(\d{1,2}:\d{1,2}:\d{2})\s+(\w+)\w+(\d{1,3}(?:\.\d{1,3}){3})(?:\s+\s+){2}\s+\[([^][\s]+)\s+([+\d]+)]\s+""([a-z]+)\s+(\s+)\s+(\s+)""\s+(\d+)\s+(\d+)\s+\s+\s+""([^""]*)""')

words=[]
for sent in sentences:
    m = rx.search(sent)
    if m:
        words.append(list(m.groups()))
    else:
        words.append(nltk.word_tokenize(sent))

print(words)

see the python demo.
the output will look like
[['jan', '31', '19:28:14', 'nginx', '10.0.0.0', '31/jan/2019:19:28:14', '+0100', 'post', '/test/itf/', ' '404', '146', 'mozilla/5.2 [en] (x11, u; openvas-xx 9.2.7)']]",https://stackoverflow.com/questions/70710646,python,14-01-2022 12:34,908.0,3.0,3.0,True,14-04-2025 11:37,14-04-2025 11:37
69473082,importerror: cannot import name &#39;save_state_warning&#39; from &#39;torch.optim.lr_scheduler&#39;,"i am attempting to issue this statement in a jupyter notebook.
from transformers import bertforquestionanswering

i get the error:

importerror: cannot import name 'save_state_warning' from 'torch.optim.lr_scheduler' (c:\users\sbing.conda\envs\tensorflow\lib\site-packages\torch\optim\lr_scheduler.py)

here is the complete stack:

importerror                               traceback (most recent call last)
 in 
----> 1 from transformers import bertforquestionanswering
~.conda\envs\tensorflow\lib\site-packages\transformers_init_.py in 
624
625     # trainer
--> 626     from .trainer import trainer
627     from .trainer_pt_utils import torch_distributed_zero_first
628 else:
~.conda\envs\tensorflow\lib\site-packages\transformers\trainer.py in 
67     trainerstate,
68 )
---> 69 from .trainer_pt_utils import (
70     distributedtensorgatherer,
71     sequentialdistributedsampler,
~.conda\envs\tensorflow\lib\site-packages\transformers\trainer_pt_utils.py in 
38     save_state_warning = """"
39 else:
---> 40     from torch.optim.lr_scheduler import save_state_warning
41
42 logger = logging.get_logger(name)
importerror: cannot import name 'save_state_warning' from 'torch.optim.lr_scheduler' (c:\users\sbing.conda\envs\tensorflow\lib\site-packages\torch\optim\lr_scheduler.py)","['import', 'huggingface-transformers', 'bert-language-model', 'nlp-question-answering']",69489963,"you need to update the transformer package to the latest version. you can achieve it by running this code:
!pip install transformers==4.11.3.

for me, there is no error after updating. refer to these links official resource and this",https://stackoverflow.com/questions/69473082,import,06-10-2021 21:52,3624.0,0.0,1.0,True,08-10-2021 07:29,07-10-2021 07:56
73433717,split text rows in dataframe into paragraphs and keep document id - python,"i have a pd dataframe with rows containing document, their respective ids. i would like to split each document into paragraphs while keeping their respective ids:
original dataframe

data={'id':['1','2','3'], 'doc':['paragraph 1.\nparagraph 2.\nparagraph 3.', 'paragraph 1.\nparagraph 2.', 'paragraph 1.\nparagraph 2.\nparagraph 3.\nparagraph 4.']}

df=pd.dataframe(data)

desired output (one paragraph per row with respective id)

id    doc                                             
           
1     paragraph 1.                             
1     paragraph 2.                              
1     paragraph 3.                              
2     paragraph 1.                              
2     paragraph 2.                        
3     paragraph 1.                              
3     paragraph 2.                              
3     paragraph 3.                              
3     paragraph 4.                            

i have the following function:
def split_into_paragraphs(dataframe): 
    
    docs = dataframe.to_json(orient=""records"")
    paragraphs: list[str] = []
    doc_indices: list[str] = []

    for doc in docs:         
        for para in range(len(doc)):
            paragraphs.append(str(doc[para].split(""\n"")))
            doc_indices = doc_indices + [doc[""id""]]

    return (doc_indices, paragraphs)

i am getting this error:
typeerror                                 traceback (most recent call last)
c:path.ipynb cellule 32 in <cell line: 1>()
----> 1 df2 = split_into_paragraphs(df)

c:path.ipynb in split_into_paragraphs(dataframe)
      8     for para in range(len(doc)):
      9         paragraphs.append(str(doc[para].split(""\n"")))
---> 10         doc_indices = doc_indices + [doc[""id""]]
     12 return (doc_indices, paragraphs)

typeerror: string indices must be integers 

since i am new to python, i am having problems figuring out what to do. i am not sure if it is a problem with the function in itself or with how i am calling it:
df2 = df.apply(split_into_paragraphs)

a final note that might be important: in my real dataframe, ids can also be a combination of numbers and words (instead of numbers only).
i would appreciate your help figuring out this problem!","['python', 'pandas', 'nlp']",73434048,"thank you for providing a code to reproduce your df. you could use the apply function to call a lambda function that does what you need, for instance:
df = df.apply(lambda x: x.str.split('\n').explode())

this lambda function will split the strings wherever it finds \n and it will explode it to new rows.",https://stackoverflow.com/questions/73433717,python,21-08-2022 10:51,589.0,1.0,1.0,True,21-08-2022 11:36,21-08-2022 11:28
76050901,haystack: save inmemorydocumentstore and load it in retriever later to save embedding generation time,"i am using inmemory document store and an embedding retriever for the q/a pipeline.
from haystack.document_stores import inmemorydocumentstore
document_store = inmemorydocumentstore(embedding_dim =768,use_bm25=true) 
document_store.write_documents(docs_processed)
     
from haystack.nodes import embeddingretriever
retriever_model_path ='downloaded_models\local\my_local_multi-qa-mpnet-base-dot-v1'
retriever = embeddingretriever(document_store=document_store,
                              embedding_model=retriever_model_path,
                              use_gpu=true)

document_store.update_embeddings(retriever=retriever)

as the embedding takes a while, i want to load the embeddings and later use them again in the retriever. (in rest api side). i don't want to use elasticsearch or faiss. how can i achieve this using in memory store? i tried to use pickle, but there is no way to store the embeddings. again, in the embedding retriever, there is no load function.
i tried to do the following:
with open(""document_store_res.pkl"", ""wb"") as f:
    pickle.dump(document_store.get_all_documents(), f)

and in the rest api, i am trying to load the document store :
def reader_retriever():
# load the pickled model        
        with open(os.path.join(settings.base_dir,'\downloaded_models\document_store_res.pkl'), 'rb') as f:
            document_store_new = pickle.load(f)

            retriever_model_path = os.path.join(settings.base_dir, '\downloaded_models\my_local_multi-qa-mpnet-base-dot-v1')

            retriever = embeddingretriever(document_store=document_store_new,
                               embedding_model=retriever_model_path,
                               use_gpu=true)

            document_store_new.update_embeddings(retriever=retriever,
                                batch_size=100)
            farm_reader_path = os.path.join(settings.base_dir, '\downloaded_models\my_local_bert-large-uncased-whole-word-masking-squad2')

            reader = farmreader(model_name_or_path=farm_reader_path,
                                    use_gpu=true)
            

            return reader, retriever","['python', 'nlp', 'haystack']",76051972,"inmemorydocumentstore: features and limitations
from haystack docs:

use the inmemorydocumentstore, if you are just giving haystack a
quick try on a small sample and are working in a restricted
environment that complicates running elasticsearch or other databases.


slow retrieval on larger datasets.
no approximate nearest neighbours (ann).
not recommended for production.

possible lightweight alternatives
to overcome the limitations of inmemorydocumentstore, if you don't want to use faiss or elasticsearch, you could also consider adopting qdrant which can run smoothly and lightly on haystack.
pickling inmemorydocumentstore
as you can see, i do not recommend this solution.
in any case, i would pickle the document store (which also contains the embeddings):
with open(""document_store_res.pkl"", ""wb"") as f:
    pickle.dump(document_store, f)

in the rest api, you can change your method as follows:
def reader_retriever():
# load the pickled model        
    with open(os.path.join(settings.base_dir,'\downloaded_models\document_store_res.pkl'), 'rb') as f:
        document_store_new = pickle.load(f)

    retriever_model_path = os.path.join(settings.base_dir, '\downloaded_models\my_local_multi-qa-mpnet-base-dot-v1')
    retriever = embeddingretriever(document_store=document_store_new,
                       embedding_model=retriever_model_path,
                       use_gpu=true)

    ### do not update the embeddings, as they have already been calculated
    
    farm_reader_path = os.path.join(settings.base_dir, '\downloaded_models\my_local_bert-large-uncased-whole-word-masking-squad2')
    reader = farmreader(model_name_or_path=farm_reader_path,
                            use_gpu=true)
    
    return reader, retriever",https://stackoverflow.com/questions/76050901,python,19-04-2023 05:04,2016.0,4.0,2.0,True,19-11-2024 10:59,19-04-2023 08:13
78393709,performance of textsimilarity() from r&#39;s text library,"i have a large data.frame with about 4 million rows and 2 columns.
the two columns contain long character strings, texts representing recipes.
for each row, i am comparing the similarity of the recipes in column a and column b, using textsimilarity() from the text library in r.
i like the textsimilarity() function, because it uses text embeddings that ""comprises values that represent the latent meaning of a word"".
yet, performance is very slow. are there ways of speeding this up? or am i coding this wrong? can/should i set this up in parallel? can i use my gpu?
example data - with way shorter texts:
df <- data.frame(
columna= c(""tomato sauce is very tasty to use"", ""without garlic, this dish is not chinese"", ""british food is as tasteless as it can get""), 
columnb= c(""pizza is the source of life"", ""a nice xiaolongbao is steamed until it is soft"", ""braised pork can be very healthy if prepared well"")
)

> df
                                     columna                                           columnb
1          tomato sauce is very tasty to use                       pizza is the source of life
2   without garlic, this dish is not chinese    a nice xiaolongbao is steamed until it is soft
3 british food is as tasteless as it can get braised pork can be very healthy if prepared will

to get the similarity, i use:
df$sim <- textsimilarity(textembed(df$columna)$texts$texts , textembed(df$columnb)$texts$texts)

in the current set-up, this process takes days rather than hours. how to speed this up? parallelization? gpu? or are there alternatives?","['r', 'performance', 'nlp', 'huggingface-transformers']",78407238,"contextual embedding models are computationally expensive
the default model used by the r text package is bert-base-uncased, which has 110m parameters, including a 12 layer feed-forward neural network.
to compare similarity of sentences, each sentence is split into tokens (which are words or parts of words) and each token is represented as a 768-dimensional vector. then the token vectors from each sentence are aggregated to create a single, 768-dimensional vector to represent that sentence in vector space. the idea is that with decent token (or word) embeddings and a sensible way of aggregating them this representation captures meaning, so similar sentences appear closer together in vector space. you can then calculate the distance between sentence vectors as a measure of their semantic similarity.
the expensive step is creating the token vectors. unlike older models such as word2vec, with bert the vector representation of each token depends on the context. a canonical example of this is that the representation of the first word in apple inc. was founded by steve jobs will not be the same as the first word in apple is my favourite fruit. this is a strength of this family of models. these sentences should not appear close together in vector space. but calculating the representation of each token based on the others around it, and working out which tokens affect the meaning of other tokens, is computationally expensive.
use a more lightweight context-dependent model
a good way to speed up things a lot without sacrificing too much accuracy is by using a more lightweight model, e.g. distilbert which has 66m parameters and 6 rather than 12 feed-forward neural network layers.
this is still a transformer model. you still get context-specific embeddings with an attention mechanism to appropriately weight each token based on the surrounding ones. with distilled models trained on larger models like bert, the results tend to be fairly close to the model from which they're derived, and they're much faster:

distilbert is a small, fast, cheap and light transformer model trained by distilling bert base. it has 40% less parameters than google-bert/bert-base-uncased, runs 60% faster while preserving over 95% of bertï¿½ï¿½ï¿½s performances as measured on the glue language understanding benchmark.

benchmarking the default model
we need a little more text to measure the performance. i'll use the biomedical semantic similarity estimation system (biosses) dataset of 100 sentence pairs.
# read in the data
biosses  <- jsonlite::read_json(""
df2  <- data.frame(
    sentence1 = sapply(biosses$rows, \(row) row$row$sentence1),
    sentence2 = sapply(biosses$rows, \(row) row$row$sentence2)
)

# define function to calculate similarity
get_similarity <- function(model, dat = df2, col1 = ""sentence1"", col2 = ""sentence2"") {
    embeds_a <- text::textembed(dat[[col1]], model = model)
    embeds_b <- text::textembed(dat[[col2]], model = model)

    text::textsimilarity(
        embeds_a$texts$texts,
        embeds_b$texts$texts
    )
}

no need to microbenchmark here as it takes so long (about 16 minutes):
system.time(
    base_result <- get_similarity(model = ""bert-base-uncased"")
) # 956.136 seconds

speed of smaller bert models
let's try three other bert derivatives:

distilbert.
bert tiny intel cpu optimized.
distilbert distilled.

models <- c(
    ""distilbert"" = ""distilbert-base-uncased"",
    ""tiny_distilbert"" = ""muhtasham/bert-tiny-finetuned-finer-139-full-intel-cpu"",
    ""distilled_distilbert"" = ""distilbert/distilbert-base-uncased-distilled-squad""
)

l <- lapply(models, \(model)
    list(
        time = system.time(x <- get_similarity(model))[""elapsed""],
        result = x,
        diff_from_base = abs(base_result - x)
    )
)

to compare the results:
sapply(l, \(x) x$time)
# distilbert.elapsed      tiny_distilbert.elapsed distilled_distilbert.elapsed 
#             622.071                       61.956                      569.000 

so the tiny model is by far the fastest at just over a minute to run, with the other two much slower at around 9-10 mins. all are substantially faster than the base model.
accuracy of smaller bert models
of course, speed is not everything. let's compare the distribution of similarity scores to the bert-base-uncased scores:
res  <- cbind(sentence = seq(nrow(df2)), data.frame(l)) |>
    tidyr::pivot_longer(
        cols = !sentence,
        names_pattern = ""(.+)\\.(.+)"",
        names_to = c(""model"", ""var"")
    ) |>
    tidyr::pivot_wider(
        names_from = var
    )

library(ggplot2)
ggplot(res) +
    geom_density(aes(x = diff_from_base, color = model, fill = model), alpha = 0.2) +
    theme(legend.position = ""bottom"") +
    labs(x = ""absolute difference"", title = ""distribution of differences from bert-base-uncased similarity score"")


interestingly, the fastest model also appears to have the closest results to the base model, at least on this task. you may want to do more robust checks with a subset of your actual data to find the optimal trade-off between speed and similarity to bert (or whatever your gold standard of similarity is).
other approaches to speeding up this task
parallel processing
you suggested using parallel processing. this should not speed things up, as under the hood, text::textembed() wraps the python hugging face transformers library, which in turn calls the python torch library, which already multi-threaded by default. when i run this code, cpu utilisation spikes for all cores. you might want to check the same happens on your machine but i suspect it will.
gpu
you suggested using a gpu. if you have one you should use it. this may speed things up significantly, though how much depends on the model. it looks like the way to do this is to call text::textembed(..., device = ""gpu""). i would not feel completely confident this will work out of the box though. getting torch to recognise a gpu generally requires specifying the cuda version during installation. this cannot be done for you from r when you run text::textrpp_initialize(), as it will not know which hardware or drivers you have installed. you might want to enter the python conda virtual environment created by the r text package and check whether torch.cuda.is_available() == true. if not, see this question for how to find out whether your gpu is compatible with cuda and if so how to find the compatible torch version.
a note on using r
there is a cost to using r here. i replicated these results in python and it was about twice as fast again. each sentence is represented as a 768 dimensional vector. there's some work going on to take the python list of torch.tensors representing each sentence, coerce to a numpy array, then pass this to r where ultimately it's stored as a row of a data frame with 768 columns. i suspect that as the size of the data increases, the time spent on translating the vectors from python to r reduces as a proportion of the total time, but it would certainly be somewhat faster still to do all this in python.
nevertheless, if muhtasham/bert-tiny-finetuned-finer-139-full-intel-cpu will do the job and there are benefits to keeping everything in r, hopefully a sixteen-fold speed up is enough to be able to do this.",https://stackoverflow.com/questions/78393709,r,27-04-2024 03:15,249.0,2.0,2.0,True,02-05-2024 17:30,27-04-2024 04:15
76456764,how can i integrate openai whisper model into a kotlin app?,"i require guidance on incorporating whisper openai into my android application developed with kotlin in android studio. unfortunately, i haven't come across any relevant instructions or details regarding its installation and utilization in this particular environment. i would like to use whisper for transcribing audio to text.
i am attempting to establish a connection between whisper and my android application developed using kotlin.","['android', 'kotlin', 'openai-api', 'openai-whisper']",76669512,"i added python code using chaquopy in my app, and it has proven to be a satisfactory solution for me. but if you want still use kotlin code, you can use curl. here it is:",https://stackoverflow.com/questions/76456764,android,12-06-2023 12:40,1774.0,1.0,1.0,True,31-01-2024 02:18,12-06-2023 15:04
66935911,understanding the output of lstm predictions,"it's a 15-class classification model, output_dim = 15. i'm trying to input a frequency vector like this one 'hi my name is' => [1,43,2,56].

when i call predictions = model(x_train[0]) i get an array of size torch.size([100, 15]), instead of just a 1d array with 15 classes like this: torch.size([15]). what's happening? why is this the output? how can i fix it? thank you in advance, more info below.


the model (from main docs) is the following:
import torch.nn as nn

class rnn(nn.module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        
        super().__init__()
        
        self.word_embeddings = nn.embedding(vocab_size, embedding_dim)
        self.lstm = nn.lstm(embedding_dim, hidden_dim)
        self.hidden2tag = nn.linear(hidden_dim, output_dim)
        
    def forward(self, text):
                
        embeds = self.word_embeddings(text)
        lstm_out, _ = self.lstm(embeds.view(len(text), 1, -1))
        tag_space = self.hidden2tag(lstm_out.view(len(text), -1))
        tag_scores = f.log_softmax(tag_space, dim=1)   

        return tag_scores

parameters:
input_dim = 62288
embedding_dim = 64
hidden_dim = 128
output_dim = 15","['machine-learning', 'nlp', 'pytorch', 'lstm', 'text-processing']",66938961,"the lstm function in pytorch returns not just the output of the last timestep but all outputs instead (this is useful in some cases). so in your example you seem to have exactly 100 timesteps (the amount of timesteps is just your sequence length).
but since you are doing classification you just care about the last output. you can normally get it like this:
outputs, _ = self.lstm(embeddings)
# shape: batch_size x 100 x 15
output = outputs[:, -1]    
# shape: batch_size x 1 x 15",https://stackoverflow.com/questions/66935911,machine-learning,03-04-2021 21:10,178.0,0.0,1.0,True,04-04-2021 07:29,03-04-2021 21:18
62731497,summarization-text rank algorithm,"what are the advantages of using text rank algorithm for summarization over bert summarization?
even though both can be used as extractive summarization method, is there any particular advantage for text rank?","['python', 'machine-learning', 'nlp', 'bert-language-model']",62743728,"textrank implementations tend to be lightweight and can run fast even with limited memory resources, while the transformer models such as bert tend to be rather large and require lots of memory. while the tinyml community has outstanding work on techniques to make dl models run within limited resources, there may be a resource advantage for some use cases.
some of the textrank implementations can be ""directed"" by adding semantic relations, which one can consider as a priori structure to enrich the graph used -- or in some cases means of incorporating human-in-the-loop approaches. those can provide advantages over supervised learning models which have been trained purely on data. even so, there are similar efforts for dl in general (e.g., variations on the theme of transfer learning) from which transformers may benefit.
another potential benefit is that textrank approaches tend to be more transparent, while transformer models can be challenging in terms of explainability. there are tools that help greatly, but this concern becomes important in the context of model bias and fairness, data ethics, regulatory compliance, and so on.
based on personal experience, while i'm the lead committer for one of the popular textrank open source implementations, i only use its extractive summarization features for use cases where a ""cheap and fast"" solution is needed. otherwise i'd recommend considering more sophisticated approaches to summarization. for example, i recommend keeping watch on the ongoing research by the author of textrank, rada mihalcea, and her graduate students at u michigan.
in terms of comparing ""which text summarization methods work better?"" i'd point toward work on abstractive summarization, particularly recent work by john bohannon, et al., at primer. for excellent examples, check the ""daily briefings"" of cv19 research which their team generates using natural language understanding, knowledge graph, abstractive summarization, etc. amy heineike discusses their approach in ""machines for unlocking the deluge of covid-19 papers, articles, and conversations"".",https://stackoverflow.com/questions/62731497,python,04-07-2020 16:15,1144.0,2.0,1.0,True,27-11-2021 00:28,27-11-2021 00:28
68468195,does spacy support multiple gpus?,"i was wondering if spacy supports multi-gpu via mpi4py?
i am currently using spacy's nlp.pipe for named entity recognition on a high-performance-computing cluster that supports the mpi protocol and has many gpus. it says here that i would need to specify the gpu to use with cupy, but with pympi, i am not sure if the following will work (should i import spacy after calling cupy device?):

from mpi4py import mpi
import cupy

comm = mpi.comm_world
rank = comm.get_rank()

if rank == 0:
    data = [""his friend nicolas j. smith is here with bart simpon and fred.""*100]
else:
    data = none

unit = comm.scatter(data, root=0)

with cupy.cuda.device(rank):
    import spacy
    from thinc.api import set_gpu_allocator, require_gpu
    set_gpu_allocator(""pytorch"")
    require_gpu(rank)
    nlp = spacy.load('en_core_web_lg')
    nlp.add_pipe(""merge_entities"")
    tmp_list = []
    for doc in nlp.pipe(unit):
        res = "" "".join([t.text if not t.ent_type_ else t.ent_type_ for t in doc])
        tmp_list.append(res)

result = comm.gather(tmp_list, root=0)

if comm.rank == 0:
    print (result)
else:
    result = none


or if i have 4 gpus on the same machine and i do not want to use mpi, can i do the following:
from joblib import parallel, delayed
import cupy

rank = 0

def chunker(iterable, total_length, chunksize):
    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))

def flatten(list_of_lists):
    ""flatten a list of lists to a combined list""
    return [item for sublist in list_of_lists for item in sublist]

def process_chunk(texts):
    with cupy.cuda.device(rank):
        import spacy
        from thinc.api import set_gpu_allocator, require_gpu
        set_gpu_allocator(""pytorch"")
        require_gpu(rank)
        preproc_pipe = []
        for doc in nlp.pipe(texts, batch_size=20):
            preproc_pipe.append(lemmatize_pipe(doc))
        rank+=1
        return preproc_pipe

def preprocess_parallel(texts, chunksize=100):
    executor = parallel(n_jobs=4, backend='multiprocessing', prefer=""processes"")
    do = delayed(process_chunk)
    tasks = (do(chunk) for chunk in chunker(texts, len(texts), chunksize=chunksize))
    result = executor(tasks)
    return flatten(result)

preprocess_parallel(texts = [""his friend nicolas j. smith is here with bart simpon and fred.""*100], chunksize=1000)","['python-3.x', 'nlp', 'mpi', 'spacy', 'gensim']",68487834,"i think i have figured out how to do this:
the key is to instruct cupy to use a new gpu.
import multiprocessing as mp
mp.set_start_method('spawn', force=true)
from joblib import parallel, delayed
from itertools import cycle
import cupy
import spacy
from thinc.api import set_gpu_allocator, require_gpu


def chunker(iterable, total_length, chunksize):
    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))

def flatten(list_of_lists):
    ""flatten a list of lists to a combined list""
    return [item for sublist in list_of_lists for item in sublist]

def process_entity(doc):
    super_word_ls = []
    for s in doc.sents:
        word_ls = []
        for t in s:
            if not t.ent_type_:
                if (t.text.strip()!=""""):
                    word_ls.append(t.text)
            else:
                word_ls.append(t.ent_type_)
        if len(word_ls)>0:
            super_word_ls.append("" "".join(word_ls))
    return "" "".join(super_word_ls)

def process_chunk(texts, rank):
    print(rank)
    with cupy.cuda.device(rank):
        set_gpu_allocator(""pytorch"")
        require_gpu(rank)
        nlp = spacy.load(""en_core_web_trf"")
        preproc_pipe = []
        for doc in nlp.pipe(texts, batch_size=20):
            preproc_pipe.append(process_entity(doc))
        rank+=1
        return preproc_pipe


def preprocess_parallel(texts, chunksize=100):
    executor = parallel(n_jobs=2, backend='multiprocessing', prefer=""processes"")
    do = delayed(process_chunk)
    tasks = []
    gpus = list(range(0, cupy.cuda.runtime.getdevicecount()))
    rank = 0
    for chunk in chunker(texts, len(texts), chunksize=chunksize):
        tasks.append(do(chunk, rank))
        rank = (rank+1)%len(gpus)
    result = executor(tasks)
    return flatten(result)

if __name__ == '__main__':
    print(preprocess_parallel(texts = [""his friend nicolas j. smith is here with bart simpon and fred.""]*100, chunksize=50))",https://stackoverflow.com/questions/68468195,python-3.x,21-07-2021 10:50,1043.0,2.0,1.0,True,22-07-2021 15:45,21-07-2021 11:33
75206732,sparsetermsimilaritymatrix().inner_product() throws &quot;cannot unpack non-iterable bool object&quot;,"while working with cosine similarity, i am facing issue calculating the inner product of two vectors.
code:
from gensim.similarities import (
    wordembeddingsimilarityindex,
    sparsetermsimilaritymatrix
)

w2v_model         = api.load(""glove-wiki-gigaword-50"")
similarity_index  = wordembeddingsimilarityindex(w2v_model)
similarity_matrix = sparsetermsimilaritymatrix(similarity_index, dictionary)

score = similarity_matrix.inner_product(
    x = [
        (0, 1), (1, 1), (2, 1), (3, 2), (4, 1), 
        (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), 
        (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), 
        (15, 1), (16, 3)
    ], 
    y = [(221, 1), (648, 1), (8238, 1)], 
    normalized = true
)

error:
typeerror                                 traceback (most recent call last)
input in [77], in <cell line: 1>()
----> 1 similarity_matrix.inner_product(
      2     [(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), 
      3      (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 3)], 
      4     [(221, 1), (648, 1), (8238, 1)], normalized=true)

file ~\anaconda3\lib\site-packages\gensim\similarities\termsim.py:558, in sparsetermsimilaritymatrix.inner_product(self, x, y, normalized)
    555 if not x or not y:
    556     return self.matrix.dtype.type(0.0)
--> 558 normalized_x, normalized_y = normalized
    559 valid_normalized_values = (true, false, 'maintain')
    561 if normalized_x not in valid_normalized_values:

typeerror: cannot unpack non-iterable bool object

i am not sure why it says bool objects when both x and y are list.","['python', 'nlp', 'nltk', 'gensim', 'cosine-similarity']",75206950,"the normalized parameter should be a 2-tuple which declares for both x and y separately (as in the docs).
therefore, the call should look like this:
score = similarity_matrix.inner_product(
    x = [
        (0, 1), (1, 1), (2, 1), (3, 2), (4, 1), 
        (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), 
        (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), 
        (15, 1), (16, 3)
    ], 
    y = [(221, 1), (648, 1), (8238, 1)], 
    normalized = (true, true)
)",https://stackoverflow.com/questions/75206732,python,23-01-2023 07:39,132.0,1.0,1.0,True,23-01-2023 08:10,23-01-2023 08:06
63934708,how do i prepare to use entire wikipedia for natural language processing?,"i am a bit new here. i have a project where i have to download and use wikipedia for nlp. the questions i am facing are as follows:
i have ram of only 12 gb, but the english wiki dump is over 15 gb compressed. does this limit my processing of wiki? i do not need any picture from the wiki. do i need to uncompress the dump before processing? can someone just tell me the steps required or point to me related content for it?
thanks in advance.","['nlp', 'wikipedia']",63940266,"the easiest to process wikipedia dump is to rely on kiwix.org dump that you can find at: 
then using python you can do the following
% wget 
...
% pip install --user libzim
% ipython
in [2]: from libzim.reader import file

in [3]: total = 0
   ...:
   ...: with file(""wiktionary_eo_all_nopic.zim"") as reader:
   ...:     for uid in range(0, reader.article_count):
   ...:         page = reader.get_article_by_id(uid)
   ...:         total += len(page.content)
   ...: print(total)

this is an simplistic processing, you should get the point to get started. in particular, as of 2020, the raw wikipedia dump using wikimarkup are very difficult to process in the sense you can not convert wikimarkup to html including infoboxes without a full wikimedia setup. there is also the rest api but why struggle when the work is already done :)
regarding where to store the data after processing, i think the industry standard is postgresql or elasticsearch (which also requires lots of memory) but i really like hoply, and more generally okvs.",https://stackoverflow.com/questions/63934708,nlp,17-09-2020 09:02,1529.0,3.0,3.0,True,31-07-2023 10:28,20-09-2020 08:52
75740652,fastapi streamingresponse not streaming with generator function,"i have a relatively simple fastapi app that accepts a query and streams back the response from chatgpt's api. chatgpt is streaming back the result and i can see this being printed to console as it comes in.
what's not working is the streamingresponse back via fastapi. the response gets sent all together instead. i'm really at a loss as to why this isn't working.
here is the fastapi app code:
import os
import time

import openai

import fastapi
from fastapi import depends,  status, request
from fastapi.security import  
from fastapi.responses import streamingresponse

auth_scheme = 
app = fastapi.fastapi()

openai.api_key = os.environ[""openai_api_key""]


def ask_statesman(query: str):
    #prompt = router(query)
    
    completion_reason = none
    response = """"
    while not completion_reason or completion_reason == ""length"":
        openai_stream = openai.chatcompletion.create(
            model=""gpt-3.5-turbo"",
            messages=[{""role"": ""user"", ""content"": query}],
            temperature=0.0,
            stream=true,
        )
        for line in openai_stream:
            completion_reason = line[""choices""][0][""finish_reason""]
            if ""content"" in line[""choices""][0].delta:
                current_response = line[""choices""][0].delta.content
                print(current_response)
                yield current_response
                time.sleep(0.25)


@app.post(""/"")
async def request_handler(auth_key: str, query: str):
    if auth_key != ""123"":
        raise 
            status_code=status.
            detail=""invalid authentication credentials"",
            headers={"" auth_scheme.scheme_name},
        )
    else:
        stream_response = ask_statesman(query)
        return streamingresponse(stream_response, media_type=""text/plain"")


if __name__ == ""__main__"":
    import uvicorn
    uvicorn.run(app, host=""0.0.0.0"", port=8000, debug=true, log_level=""debug"")

and here is the very simple test.py file to test this:
import requests

query = ""how tall is the eiffel tower?""
url = ""
params = {""auth_key"": ""123"", ""query"": query}

response = requests.post(url, params=params, stream=true)

for chunk in response.iter_lines():
    if chunk:
        print(chunk.decode(""utf-8""))","['python', 'python-requests', 'streaming', 'fastapi', 'openai-api']",75760884,"first, it wouldn't be good practice to use a post request for requesting data from the server. instead, a get request would be more suitable to your case scenario. in addition to that, you shouldn't be sending credentials, such as auth_key as part of the url (i.e., using the query string), but you should rather use headers and/or cookies (using  please have a look at this answer for more details and examples on the concepts of headers and cookies, as well as the risks involved when using query parameters instead. helpful posts around this topic can also be found here and here, as well as here, here and here.
second, if you are executing a blocking operation (i.e., blocking i/o-bound or cpu-bound tasks) inside the streamingresponse's generator function, you should define the generator function with def instead of async def, as, otherwise, the blocking operation, as well as the time.sleep() function that is used inside your generator, would blcok the event loop. as explained here, if the function for streaming the response body is a normal def generator and not an async def one, fastapi will use iterate_in_threadpool() to run the iterator/generator in a separate thread that is then awaitedï¿½ï¿½ï¿½see streamingresponse's relevant source code. if you prefer using an async def generator, then make sure to execute any blocking operations in an external threadpool (or processpool) and await it, as well as use await asyncio.sleep() instead of time.sleep(), in cased you need to add delay in the execution of an operation. have a look at this detailed answer for more details and examples.
third, you are using requests' iter_lines() function, which iterates over the response data, one line at a time. if, however, the response data did not include any line break character (i.e., \n), you wouldn't see the data on client's console getting printed as they arrive, until the entire response is received by the client and printed as a whole. in that case, you should instead use iter_content() and specify the chunk_size as desired (both cases are demonstrated in the example below).
finally, if you would like the streamingresponse to work in every web browser (including chrome as well)ï¿½ï¿½ï¿½in the sense of being able to see the data as they stream inï¿½ï¿½ï¿½you should specify the media_type to a different type than text/plain (e.g., application/json or text/event-stream, "" or disable mime sniffing. as explained here, browsers will start buffering text/plain responses for a certain amount (around 1445 bytes, as documented here), in order to check whether or not the content received is actually plain text. to avoid that, you can either set the media_type to text/event-stream (used for server-sent events), or keep using text/plain, but set the x-content-type-options response header to nosniff, which would disable mime sniffing (both options are demonstrated in the example below).
working example
app.py
from fastapi import fastapi
from fastapi.responses import streamingresponse
import asyncio


app = fastapi()


async def fake_data_streamer():
    for i in range(10):
        yield b'some fake data\n\n'
        await asyncio.sleep(0.5)


# if your generator contains blocking operations such as time.sleep(), then define the
# generator function with normal `def`. alternatively, use `async def` and run any 
# blocking operations in an external threadpool/processpool. (see 2nd paragraph of this answer)
'''
import time

def fake_data_streamer():
    for i in range(10):
        yield b'some fake data\n\n'
        time.sleep(0.5)
'''        

    
@app.get('/')
async def main():
    return streamingresponse(fake_data_streamer(), media_type='text/event-stream')
    # or, use:
    '''
    headers = {'x-content-type-options': 'nosniff'}
    return streamingresponse(fake_data_streamer(), headers=headers, media_type='text/plain')
    '''

test.py (using python requests)
import requests

url = ""

with requests.get(url, stream=true) as r:
    for chunk in r.iter_content(1024):  # or, for line in r.iter_lines():
        print(chunk)

test.py (using  this, as well as this and this for the benefits of using  over requests)
import 

url = '

with  url) as r:
    for chunk in r.iter_raw():  # or, for line in r.iter_lines():
        print(chunk)",https://stackoverflow.com/questions/75740652,python,15-03-2023 04:48,75613.0,40.0,6.0,True,14-04-2025 11:33,14-04-2025 11:33
72323825,how to map and update python dictionary with different key value pair?,"i want to transform a dictionary in python, from dictionary 1 into dictionary 2 as follows.
transaction = {
""trans_time"": ""14/07/2015 10:03:20"",
""trans_type"": ""debit"",
""description"": ""239.95 usd dhl.com texas usa"",
}

i want to transform the above dictionary to the following
transaction =  {
    ""trans_time"": ""14/07/2015 10:03:20"",
    ""trans_type"": ""debit"",
    ""description"": ""dhl.com texas usa"",
    ""amount"": 239.95,
    ""currency"": ""usd"",
    ""location"": ""texas usa"",
    ""merchant_name"": ""dhl""
    
    }

i tried the following but it did not work
dic1 = {
""trans_time"": ""14/07/2015 10:03:20"",
""trans_type"": ""debit"",
""description"": ""239.95 usd dhl.com texas usa""
}
print(type(dic1))
copieddic = dic1.copy()
print(""copieddic = "",copieddic)


updatekeys = ['amount', 'currency', 'merchant_name', 'location', 'trans_category']


for key in dic1:
    if key == 'description':
        list_words = dic1[key].split("" "")
newdict =  {updatekeys[i]: x for i, x in enumerate(list_words)}

copieddic.update(newdict)

print(copieddic)

i got the following result
{
'trans_time': '14/07/2015 10:03:20',
 'trans_type': 'debit', 
'description': '239.95 usd dhl.com texas usa',
 'amount': '239.95',
 'currency': 'usd',
 'merchant_name': 'dhl.com',
 'location': 'texas',
 'trans_category': 'usa'
}

my intended output should look like this:
transaction =  {
    ""trans_time"": ""14/07/2015 10:03:20"",
    ""trans_type"": ""debit"",
    ""description"": ""dhl.com texas usa"",
    ""amount"": 239.95,
    ""currency"": ""usd"",
    ""location"": ""texas usa"",
    ""merchant_name"": ""dhl""

    }","['python', 'dataframe', 'dictionary', 'machine-learning', 'nlp']",72329869,"i think it would be easier to turn the value into an array of words and parse it. here, an array of words 'aaa ' is created from the dictionary string 'transaction['description']'. where there are more than one word(array element) 'join' is used to turn the array back into a string. the currency value itself is converted to fractional format from the string. in 'merchant_name', the segment up to the point is taken.
transaction = {
""trans_time"": ""14/07/2015 10:03:20"",
""trans_type"": ""debit"",
""description"": ""239.95 usd dhl.com texas usa"",
}
aaa = transaction['description'].split()
transaction['description'] = ' '.join(aaa[2:])
transaction['amount'] = float(aaa[0])
transaction['currency'] = aaa[1]
transaction['location'] = ' '.join(aaa[3:])
transaction['merchant_name'] = aaa[2].partition('.')[0]

print(transaction)

output
{
 'trans_time': '14/07/2015 10:03:20',
 'trans_type': 'debit',
 'description': 'dhl.com texas usa',
 'amount': 239.95,
 'currency': 'usd',
 'location': 'texas usa',
 'merchant_name': 'dhl'}",https://stackoverflow.com/questions/72323825,python,20-05-2022 19:12,130.0,1.0,2.0,True,21-05-2022 20:28,21-05-2022 20:28
66931415,swift metal compiler failed with xpc_error_connection_interrupted bert model,"i've recently been working on an app where a user can ask questions about a text and the app will print out the answer. for that, i used the apple finding answers to questions template, which is powered by bert. this template worked just fine, until yesterday when it always printed out the error message:
compiler failed with xpc_error_connection_interrupted

when executing the bert inference code:
let answer = self.bert.findanswer(for: searchtext, in: detail.body)

i haven't changed anything of the code, so this seems like a bug, does anyone know how to solve it or did anyone have similar problems?","['swift', 'machine-learning', 'bert-language-model']",66953698,"i was able to resolve the issue by cleaning my build folder, therefore this issue seemed to be caused by corrupted cache files.",https://stackoverflow.com/questions/66931415,swift,03-04-2021 12:59,213.0,0.0,1.0,True,05-04-2021 13:17,03-04-2021 13:49
68438711,how can you ensure a viable endpoint for a stanza corenlpclient?,"i would like to use the stanza corenlpclient to extract noun phrases, similar to this method.
however, i cannot seem to find a good port to start the server on. the default is 9000, but this is often occupied, as indicated by the error message:

permanentlyfailedexception: error: unable to start the corenlp server
on port 9000 (possibly something is already running there)

edit: port 9000 is in use by python.exe, which is why i can't just shut the process down to make space for the corenlpclient.
then, when i select other ports such as 7999, 8000, or 8080, the server keeps listening indefinetely, not executing the consecutive code lines, showing only the following:

2021-07-19 12:05:55 info: starting server with command: java -xmx8g -cp c:\users\timjo\stanza_corenlp* edu.stanford.nlp.pipeline.stanfordcorenlpserver -port 7998 -timeout 60000 -threads 5 -maxcharlength 100000 -quiet true -serverproperties corenlp_server-2e15724b8064491b.props -preload -outputformat serialized

i have the latest version of stanza installed, and am running the following code from an .ipynb file in vs code:
# sample sentence
sentence = ""albert einstein was a german-born theoretical physicist."" 

# start the client as indicated in the docs
with corenlpclient(properties='corenlp_server-2e15724b8064491b.props', endpoint=' memory='8g', be_quiet=true) as client:
     matches = client.tregex(text=sentence, pattern = 'np')

# extract the noun phrases and their indices
noun_phrases = [[text, begin, end] for text, begin, end in
     zip([sentence[match_id]['spanstring'] for sentence in matches['sentences'] for match_id in sentence],
         [sentence[match_id]['characteroffsetbegin'] for sentence in matches['sentences'] for match_id in sentence],
         [sentence[match_id]['characteroffsetend'] for sentence in matches['sentences'] for match_id in sentence])]

main question: how can i ensure that the server starts on an open port, and closes afterwards?   i would prefer having a semi-automatic way to finding open / shutting down occupied ports for the client to run on.","['nlp', 'python-3.7', 'stanford-nlp', 'stanza']",68534328,in general it is sufficient to choose another number that nothing else is using ï¿½ï¿½ï¿½ maybe 9017? there are lots of numbers to choose from! but the more careful choice would be to create the corenlpclient in a while loop with a try/catch and to increment the port number till you found one that was open,https://stackoverflow.com/questions/68438711,nlp,19-07-2021 10:10,195.0,1.0,2.0,True,26-07-2021 17:42,19-07-2021 13:05
42711144,how can i install torchtext?,"i have pytorch installed in my machine but whenever i try to do the following-
from torchtext import data
from torchtext import datasets

i get the following error.
importerror: no module named 'torchtext'

how can i install torchtext?","['python', 'deep-learning', 'pytorch', 'nlp']",42735403,"the package was released with setuptools support. you can clone the repository and run python setup.py install. unfortunately, i don't think that they have released it on pip.",https://stackoverflow.com/questions/42711144,python,10-03-2017 05:47,59695.0,18.0,8.0,True,21-09-2021 13:03,22-08-2018 12:01
78224391,obtain prediction score,"i'm following this notebook for inference with layoutlm. i would like to modify the code to access the prediction score, in % format for each prediction. the same way i access true_predictions and true_boxes.
so far, i tried with         probabilities = torch.nn.functional.softmax(true_predictions, dim=-1) like this:
#until here, just following the notebook
logits = outputs.logits

predictions = logits.argmax(-1).squeeze().tolist()
token_boxes = encoding.bbox.squeeze().tolist()
probabilities = torch.nn.functional.softmax(logits, dim=-1).squeeze().tolist()

if (len(token_boxes) == 512):
    predictions = [predictions]
    token_boxes = [token_boxes]
    probabilities = [probabilities]

predictions = list(itertools.chain(*predictions))
token_boxes = list(itertools.chain(*token_boxes))
probabilities = list(itertools.chain(*probabilities))
      
is_subword = np.array(offset_mapping.squeeze().tolist())[:,0] != 0
true_predictions = [self.id2label[pred] for idx, pred in enumerate(predictions) if not is_subword[idx]]
true_boxes = [box for idx, box in enumerate(token_boxes) if not is_subword[idx]]
true_probabilities = [probability for idx, probability in enumerate(probabilities) if not is_subword[idx]]
  
for prediction, box, probability in zip(true_predictions, true_boxes, true_probabilities):
    print(probability )

output:
[0.00010619303793646395, 3.339954128023237e-05, 2.2820451704319566e-05, 2.2919863113202155e-05, 0.0005767009570263326, 5.0725124310702085e-05, 3.0033241273486055e-05, 0.006056534126400948, 4.6057226427365094e-05, 1.2512471585068852e-05, 0.0002005402639042586, 2.0308254534029402e-05, 0.992790937423706, 3.023005228897091e-05]
which means that there are 14 labels, with the most likely one being label #13 with 0.992 (99,2%). but that is not quite what i was aming for. i'm not looking for the probability of each label for that prediction. i'm looking for the prediction score itself. something like this prediction has a confidence of 75%",['huggingface-transformers'],78224759,"probabilities are in range [0, 1], so, if you need percentages, scale the output of the softmax activation by 100.
softmax_output = [
0.00010619303793646395, 3.339954128023237e-05,
2.2820451704319566e-05, 2.2919863113202155e-05, 0.0005767009570263326,
5.0725124310702085e-05, 3.0033241273486055e-05, 0.006056534126400948,
4.6057226427365094e-05, 1.2512471585068852e-05, 0.0002005402639042586,
2.0308254534029402e-05, 0.992790937423706, 3.023005228897091e-05
]

probabilities_percentage = [round(prob * 100, 2) for prob in softmax_output]

print(probabilities_percentage)

# [0.01, 0.0, 0.0, 0.0, 0.06, 0.01, 0.0, 0.61, 0.0, 0.0, 0.02, 0.0, 99.28, 0.0]

edit 1:
inc case you need a confdidence score for the predictions, you should look at the accuracy on unseen data (test set). if you get an accuracy of 90% in test, you could (roughly) assume that your model will hit the target 90% of the times.",https://stackoverflow.com/questions/78224391,huggingface-transformers,26-03-2024 09:59,404.0,1.0,1.0,True,26-03-2024 11:16,26-03-2024 11:03
71971099,"remove numbers, punctuations, white spaces before tokenization","i have the following data frame
report <- data.frame(text = c(""unit 1 crosses the street"", 
       ""driver 2 was speeding and saw driver# 1"", 
        ""year 2019 was the year before the pandemic"",
        ""hey saw       hei hei in        the    wood"",
        ""hello: my kityy! you are the best""), id = 1:5)
report 
                                         text id
1                   unit 1 crosses the street  1
2     driver 2 was speeding and saw driver# 1  2
3  year 2019 was the year before the pandemic  3
4 hey saw       hei hei in        the    wood  4
5           hello: my kityy! you are the best  5

from a previous coding help, we can remove stop words using the following code.
report$text <- gsub(paste0('\\b',tm::stopwords(""english""), '\\b', 
                          collapse = '|'), '', report$text)
report
                                    text id
1                 unit 1 crosses  street  1
2      driver 2  speeding  saw driver# 1  2
3            year 2019   year   pandemic  3
4 hey saw       hei hei             wood  4
5                 hello:  kityy!    best  5

the above data still has noises (numbers, punctuations, and white space). need to get the data in the following format by removing these noises before tokenization. additionally, i want to remove selected stop words (for example, saw and kitty).
                                    text id
1                   unit crosses  street  1
2                driver speeding  driver  2
3                     year year pandemic  3
4                       hey hei hei wood  4
5                             hello best  5","['r', 'text-mining', 'tm', 'stop-words', 'tidytext']",71971183,"we may get the union of tm::stopwords and the new entries, paste them with collapse = ""|"", remove those with replacement as """" in gsub, along with removing the punctuations and digits and extra spaces (\\s+ - one or more spaces)
trimws(gsub(""\\s+"", "" "", 
 gsub(paste0(""\\b("", paste(union(c(""saw"", ""kityy""), 
   tm::stopwords(""english"")), collapse=""|""), "")\\b""), """", 
     gsub(""[[:punct:]0-9]+"", """", report$text))
))

-output
[1] ""unit crosses street"" 
[2  ""driver speeding driver"" 
[3] ""year year pandemic""   
[4] ""hey hei hei wood""   
[5] ""hello best""",https://stackoverflow.com/questions/71971099,r,22-04-2022 15:20,246.0,2.0,1.0,True,22-04-2022 15:36,22-04-2022 15:26
77959997,how do you send files to the openai api?,"for fun i wanted to try to make a tool to ask chatgpt to document rust files. i found an issue, in that the maximum message length the api allows seems to be 2048 characters.
it seems that the openai api allows you to send files, so i was hoping that by sending  the files to the server the model would have the context it needs to generate the comments.
however, i don't seem to be able to do so, i tried this:
use std::fmt::write;
use std::fs;
use std::io;
use std::io::read;

use chatgpt::prelude::*;
use clap::parser;
use syn;
use syn::item;
use reqwest;

#[derive(parser, debug)]
#[command(author, version, about, long_about = none)]
struct args
{
    /// path to the source code to document
    #[arg(short, long)]
    file_path: string,
}

fn main()
{
    let args = args::parse();

    let mut file = std::fs::file::open(args.file_path).unwrap();
    let mut content = string::new();
    file.read_to_string(&mut content).unwrap();

    let ast = syn::parse_file(&content).unwrap();
    let mut input_buffer = string::new();

    // getting the api key here
    let key = ""my key that i have replaced for obvious reasons"";

    {
        // create a reqwest client
        let client = reqwest::blocking::client::new();

        // make a post request to the openai api
        let response = client
            .post(""
            .header(""authorization"", ""bearer my key that i have replaced for obvious reasons"")
            .header(""content-type"", ""application/json"")
            .body(content.clone())
            .send().unwrap();

        // check if the request was successful
        if response.status().is_success() {
            println!(""file uploaded successfully!"");
        } else {
            println!(""failed to upload file. status code: {}"", response.status());
        }

        std::mem::forget(client);
        std::mem::forget(response);
    }
}

the response i get is:
failed to upload file. status code: 415 unsupported media type
i am not sure what i am doing wrong. i have also tried changign the content type to text/plain, i get the same error.","['rust', 'documentation', 'openai-api']",77962025,"the async-openai crate has files, which allows you to upload files to openai:
let client = client::new();
let request = createfilerequest {
    input: ""path/to/my/file.bin"".into(),
    purpose: ""test"".into(),
};
let response = client.files().create (request).await?;",https://stackoverflow.com/questions/77959997,rust,08-02-2024 07:45,2438.0,0.0,1.0,True,12-02-2024 07:58,08-02-2024 08:01
70159512,finding a list of words in a corpus using nltk? cannot find the frequency of words,"i have downloaded a corpus and tokenised the words. i have a list of the main characters and i want to find out how many times each name appears in the corpus. i have tried using a frequency function with a dictionary but i don't know how to get the name count..
target_url0 = '
book_raw = urlopen(target_url0).read().decode('utf-8')
word_tokens = word_tokenize(book_raw)

character_list = ['myriel','bishop','baptistine','magloire','cravatte','valjean','gervais','fantine','tholomyï¿½ï¿½s'
                  ,'blachevelle','dahlia','fameuil','favourite','listolier','zï¿½ï¿½phine','cosette','thï¿½ï¿½nardier',
                  'ï¿½ï¿½ponine','azelma','javert','fauchelevent','bamatabois','champmathieu',
                  'brevet','simplice','chenildieu','cochepaille','innocente','reverend','ascension','crucifixion',
                  'gavroche','magnon',
                  'gillenormand','marius','colonel','mabeuf','enjolras','combeferre','prouvaire',
   euilly','courfeyrac','bahorel','lesgle','joly','grantaire','patron-minette','brujon',
                 'toussaint'] 


fdist_mis = freqdist(word_tokens)

filtered_word_freq = dict((character_list, freq) for character_list, freq in fdist_mis.items())


when i explore filtered_word_freqt, it just returns all of the word tokens instead of a dictionary of the unique characters and their occurrences. any help? thanks a lot.","['python', 'nltk']",70161863,"how would you like to see the frequency? you can get a count of # times each word was seen or a ratio of how often within the total text or even a fancy formatted table. relevant functions copied from here:
n()[source]
return the total number of sample outcomes that have been recorded by this freqdist. for the number of unique sample values (or bins) with counts greater than zero, use freqdist.b().
return type:    int

freq(sample)[source]
return the frequency of a given sample. the frequency of a sample is defined as the count of that sample divided by the total number of sample outcomes that have been recorded by this freqdist. the count of a sample is defined as the number of times that sample outcome was recorded by this freqdist. frequencies are always real numbers in the range [0, 1].

tabulate(*args, **kwargs)[source]
tabulate the given samples from the frequency distribution (cumulative), displaying the most frequent sample first. if an integer parameter is supplied, stop after this many samples have been plotted.
parameters: samples (list) ï¿½ï¿½ï¿½ the samples to plot (default is all samples)

here's my version of your code (including the using statements and fualified calls for future readers):
import urllib.request
import nltk
nltk.download(""punkt"")

target_url0 = '
book_raw = urllib.request.urlopen(target_url0).read().decode('utf-8')
word_tokens = nltk.word_tokenize(book_raw)

character_list = ['myriel','bishop','baptistine','magloire','cravatte','valjean','gervais','fantine','tholomyï¿½ï¿½s','blachevelle','dahlia','fameuil','favourite','listolier','zï¿½ï¿½phine','cosette','thï¿½ï¿½nardier','ï¿½ï¿½ponine','azelma','javert','fauchelevent','bamatabois','champmathieu','brevet','simplice','chenildieu','cochepaille','innocente','reverend','ascension','crucifixion','gavroche','magnon','gillenormand','marius','colonel','mabeuf','enjolras','combeferre','prouvaire','feuilly','courfeyrac','bahorel','lesgle','joly','grantaire','patron-minette','brujon','toussaint'] 
fdist_mis = nltk.freqdist(word_tokens)

then i can use that dictionary to get the frequency for any word. e.g.",https://stackoverflow.com/questions/70159512,python,29-11-2021 18:21,938.0,1.0,1.0,True,01-12-2021 15:26,30-11-2021 11:41
75252902,openai embeddings cosine similarity search &#39;input vector should be 1-d&#39; error,"i am getting following error in jupyter notebook
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
cell in[2], line 39
     37 query = input(""enter your query: "")
     38 print(""recommended contacts:"")
---> 39 for contact in search_contacts(query):
     40     print(contact)

cell in[2], line 33, in search_contacts(query)
     31 scores = {}
     32 for contact, embedding in embeddings.items():
---> 33     scores[contact] = 1 - cosine(query_embedding, embedding)
     34 return sorted(scores, key=scores.get, reverse=true)[:5]

file ~\appdata\local\programs\python\python311\lib\site-packages\scipy\spatial\distance.py:668, in cosine(u, v, w)
    626 """"""
    627 compute the cosine distance between 1-d arrays.
    628 
   (...)
    663 
    664 """"""
    665 # cosine distance is also referred to as 'uncentered correlation',
    666 #   or 'reflective correlation'
    667 # clamp the result to 0-2
--> 668 return max(0, min(correlation(u, v, w=w, centered=false), 2.0))

file ~\appdata\local\programs\python\python311\lib\site-packages\scipy\spatial\distance.py:608, in correlation(u, v, w, centered)
    575 def correlation(u, v, w=none, centered=true):
    576     """"""
    577     compute the correlation distance between two 1-d arrays.
    578 
   (...)
    606 
    607     """"""
--> 608     u = _validate_vector(u)
    609     v = _validate_vector(v)
    610     if w is not none:

file ~\appdata\local\programs\python\python311\lib\site-packages\scipy\spatial\distance.py:301, in _validate_vector(u, dtype)
    299 if u.ndim == 1:
    300     return u
--> 301 raise valueerror(""input vector should be 1-d."")

valueerror: input vector should be 1-d.

here is my code
import pandas as pd
import openai
import numpy as np
from scipy.spatial.distance import cosine

# authenticate to openai
openai.api_key = ""api_key""

# load the csv file
contacts = pd.read_csv(""c:/tmp/connect.csv"")

# generate embeddings for each contact using gpt-3
embeddings = {}
for index, row in contacts.iterrows():
    combined = row[""combined""]
    response = openai.completion.create(
        model=""text-davinci-002"",
        prompt=f""generate embeddings for {combined}"",
        temperature=0.5,
    )
    embedding = response[""choices""][0][""text""]
    embeddings[combined] = embedding

# search function to return recommended contacts based on a user's query
def search_contacts(query):
    query_embedding = openai.completion.create(
        model=""text-davinci-002"",
        prompt=f""generate embeddings for {query}"",
        temperature=0.5,
    )[""choices""][0][""text""]
    scores = {}
    for contact, embedding in embeddings.items():
        scores[contact] = 1 - cosine(query_embedding, embedding)
    return sorted(scores, key=scores.get, reverse=true)[:5]

# example usage
query = input(""enter your query: "")
print(""recommended contacts:"")
for contact in search_contacts(query):
    print(contact)

my connect.csv file looks like this:




combined




fullname: alex goodwill; company: hypercap; position: business consultant


fullname: amy power; company: hollywood; position: strategy & operations - ceo's office




need help with figuring out how to fix this error. i did google search but was not able to find anything which can help me understand how i am passing non-1d array to the cosine similarity search.","['python', 'openai-api', 'azure-openai', 'text-embedding-ada-002']",75292750,"you are trying to calculate cosine similarity of text instead of vectors. embedding is a vector representation of text that has a semantic meaning. you do not create embeddings by giving a prompt to completions endpoint. you need to use embeddings endpoint.
response = openai.embedding.create(
    input=[
        ""sample text goes here"",
        ""there can be one or several phrases in each batch""
    ], engine=""text-embedding-ada-002""
)

response will contain embeddings of every phrase. for example:
""data"": [
    {
      ""embedding"": [0, 0, 0,....],
      ""index"": 0,
      ""object"": ""embedding""
    },
    {
      ""embedding"": [0, 0, 0,....],
      ""index"": 1,
      ""object"": ""embedding""
    }
  ],
  ""model"": ""text-embedding-ada-002-v2"",
  ""object"": ""list"",
  ""usage"": {
    ""prompt_tokens"": ,
    ""total_tokens"": 
  }
}

so you can take the embeddings from the response and calculate the cosine similarity.
response['data'][0]['embedding']",https://stackoverflow.com/questions/75252902,python,27-01-2023 00:11,2889.0,1.0,2.0,True,17-05-2023 16:45,11-04-2023 09:39
72091006,tokenization of compound words not working in quanteda,"i'm trying to create a dataframe containing specific keywords-in-context using the kwic() function, but unfortunately, i'm running into some error when attempting to tokenize the underlying dataset.
this is the subset of the dataset i'm using as a reproducible example:
test_cluster <- speeches_subset %>%
  filter(grepl('schwester agnes',
                speechcontent,
                ignore.case = true))

test_corpus <- corpus(test_cluster,
                      docid_field = ""id"",
                      text_field = ""speechcontent"")

here, test_cluster contains six observations of 12 variables, that is, six rows in which the column speechcontent contains the compound word ""schwester agnes"". test_corpus transforms the underlying data into a quanteda corpus object.
when i then run the following code, i would expect, first, the content of the speechcontent variables to be tokenized, and due to tokens_compound, the compound word ""schwester agnes"" to be tokenized as such. in a second step, i would expect the kwic() function to return a dataframe consisting of six rows, with the keyword variable including the compound word ""schwester agnes"". instead, however, kwic() returns an empty dataframe containing 0 observations of 7 variables. i think this is because of some mistake i'm making with tokens_compound(), but i'm not sure... any help would be greatly appreciated!
test_tokens <- tokens(test_corpus, 
                      remove_punct = true,
                      remove_numbers = true) %>%
  tokens_compound(pattern = phrase(""schwester agnes""))

test_kwic <- kwic(test_tokens,
                  pattern = ""schwester agnes"",
                  window = 5)

edit: i realize that the examples above are not easily reproducible, so please refer to the reprex below:
speech = c(""this is the first speech. many words are in this speech, but only few are relevant for my research question. one relevant word, for example, is the word stack overflow. however there are so many more words that i am not interested in assessing the sentiment of"", ""this is a second speech, much shorter than the first one. it still includes the word of interest, but at the very end. stack overflow."", ""this is the third speech, and this speech does not include the word of interest so i'm not interested in assessing this speech."")

data <- data.frame(id=1:3, 
                   speechcontent = speech)

test_corpus <- corpus(data,
                      docid_field = ""id"",
                      text_field = ""speechcontent"")

test_tokens <- tokens(test_corpus, 
                      remove_punct = true,
                      remove_numbers = true) %>%
  tokens_compound(pattern = c(""stack"", ""overflow""))

test_kwic <- kwic(test_tokens,
                  pattern = ""stack overflow"",
                  window = 5)","['r', 'nlp', 'token', 'quanteda']",72141529,"you need to apply phrase(""stack overflow"") and set concatenator = "" "" in tokens_compound().
require(quanteda)
#> package version: 3.2.1
#> unicode version: 13.0
#> icu version: 69.1

speech <- c(""this is the first speech. many words are in this speech, but only few are relevant for my research question. one relevant word, for example, is the word stack overflow. however there are so many more words that i am not interested in assessing the sentiment of"", 
           ""this is a second speech, much shorter than the first one. it still includes the word of interest, but at the very end. stack overflow."", 
           ""this is the third speech, and this speech does not include the word of interest so i'm not interested in assessing this speech."")

data <- data.frame(id = 1:3, 
                   speechcontent = speech)

test_corpus <- corpus(data,
                      docid_field = ""id"",
                      text_field = ""speechcontent"")

test_tokens <- tokens(test_corpus, 
                      remove_punct = true,
                      remove_numbers = true) %>%
  tokens_compound(pattern = phrase(""stack overflow""), concatenator = "" "")

test_kwic <- kwic(test_tokens,
                  pattern = ""stack overflow"",
                  window = 5)
test_kwic
#> keyword-in-context with 2 matches.                                                                             
#>  [1, 29] for example is the word | stack overflow | however there are so many
#>  [2, 24]     but at the very end | stack overflow |

created on 2022-05-06 by the reprex package (v2.0.1)",https://stackoverflow.com/questions/72091006,r,02-05-2022 18:39,241.0,2.0,1.0,True,07-05-2022 10:56,02-05-2022 21:16
78574282,openai api key gives ratelimit error even though api key not used,"i am trying to make a chatbot using the langchain-openai.i have never done this before. i created a brand new api key, which was never used before. i copied code from the official langchain-openai docs, and the following code:
from langchain_core.prompts import prompttemplate
from langchain_openai import openai

openai_api_key = 'sk-proj-...'

template = """"""question: {question}

answer: let's think step by step.""""""

prompt = prompttemplate.from_template(template)

llm = openai(openai_api_key=""sk-proj-..."")

llm_chain = prompt | llm

question = ""what nfl team won the super bowl in the year justin beiber was born?""

llm_chain.invoke(question)

it is giving this very long error:
traceback (most recent call last):


file ""c:\users\acer\onedrive\documents\vs_code\python\ai\langchain-openai.py"", line 25, in <module>
    llm_chain.invoke(question)
  file ""c:\users\acer\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langchain_core\runnables\base.py"", line 2399, in invoke
    input = step.invoke(
            ^^^^^^^^^^^^
  file ""c:\users\acer\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langchain_core\language_models\llms.py"", line 276, in invoke
    self.generate_prompt(
  file ""c:\users\acer\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langchain_core\language_models\llms.py"", line 633, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""c:\users\acer\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langchain_core\language_models\llms.py"", line 803, in generate
    output = self._generate_helper(
             ^^^^^^^^^^^^^^^^^^^^^^
  file ""c:\users\acer\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langchain_core\language_models\llms.py"", line 670, in _generate_helper
    raise e
  file ""c:\users\acer\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langchain_core\language_models\llms.py"", line 657, in _generate_helper
    self._generate(
  file ""c:\users\acer\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langchain_openai\llms\base.py"", line 350, in _generate
    response = self.client.create(prompt=_prompts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""c:\users\acer\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\openai\_utils\_utils.py"", line 277, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  file ""c:\users\acer\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\openai\resources\completions.py"", line 528, in create
    return self._post(
           ^^^^^^^^^^^
  file ""c:\users\acer\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\openai\_base_client.py"", line 1240, in post
    return cast(responset, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""c:\users\acer\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\openai\_base_client.py"", line 921, in request
    return self._request(
           ^^^^^^^^^^^^^^
  file ""c:\users\acer\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\openai\_base_client.py"", line 1005, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  file ""c:\users\acer\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\openai\_base_client.py"", line 1053, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  file ""c:\users\acer\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\openai\_base_client.py"", line 1005, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  file ""c:\users\acer\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\openai\_base_client.py"", line 1053, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  file ""c:\users\acer\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\openai\_base_client.py"", line 1020, in _request
    raise self._make_status_error_from_response(err.response) from none
openai.ratelimiterror: error code: 429 - {'error': {'message': 'you exceeded your current quota, please check your plan and billing details. for more information on this error, read the docs:  'type': 'insufficient_quota', 'param': none, 'code': 'insufficient_quota'}}

i even checked the openai api key usage website and it is not showing anything.
all of the code is from the langchain-openai docs.
am i doing something wrong?
edit:
as @trazom pointed out, the code works just fine but apparently i just needed to make a new key and link a credit card. thanks @trazom!","['python', 'chatbot', 'openai-api', 'langchain', 'rate-limiting']",78574672,"as @trazom pointed out, the code works just fine but apparently i just needed to make a new key and link a credit card. thanks @trazom! writing this to make this question appeared as answered.",https://stackoverflow.com/questions/78574282,python,04-06-2024 08:44,413.0,0.0,1.0,True,04-06-2024 10:03,04-06-2024 10:03
62520221,list index out of range to extract text lines from a df column,"i finally find as a need your guidance and support as i don't detect what is my error in the next piece of code.
it supposes that ""list index out of range"" rises when you initialize counter improperly to the length of df, but what i am attempting is to return the first then lines of the column descripciï¿½ï¿½n as sample (doc) to apply the nltk stopwords analysis.
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')
import numpy as np
pd.set_option('display.max_columns', none)

import nltk
from nltk.corpus import stopwords 
stop_words = stopwords.words('spanish')
from nltk.stem import wordnetlemmatizer
import string 


import base64
import re
from collections import counter 


from sklearn.feature_extraction.text import countvectorizer
from sklearn.base import transformermixin
from sklearn.pipeline import pipeline
from sklearn.svm import linearsvc
import sklearn.feature_extraction.stop_wordom sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix
import spacy
spacy.load('es_core_news_sm')
from spacy.lang.es import spanish
parser = spanish()


df = pd.read_csv('geografia_empleos_mx.csv')
df.head(2)

del df['unnamed: 0']
df.head(1)


df.isnull().sum()

df1 = df.copy()

df1['fraudulento'].value_counts()



import spacy

nlp = spacy.load('es_core_news_lg')
stopwords = stopwords.words('spanish')


punctuations = string.punctuation


def limpia_texto(docs, logging = false):
    texts = []
    counter = 1
    for doc in docs:
       if counter % 100 == 0 and logging:
           print('procesados: {} de {} documentos'.format(counter, len(docs)))
           counter += 1
           doc = nlp(doc, disable = ['parser', 'ner'])
           tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-pron-']
           tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]
           tokens = ' '.join(tokens)
           texts.append(tokens)
   return pd.series(texts)   

falso_1 = [text for text in df1[df1['fraudulento'] == 1]['descripciï¿½ï¿½n']]

falso_1[10]    # here is when index error raises :(","['python', 'pandas', 'nlp', 'nltk', 'stop-words']",62521129,"falso_1 does not contain 10 indexes, that's why it raising an error. this line is collecting the columns from your dataframe.
falso_1 = [text for text in df1[df1['fraudulento'] == 1]['descripciï¿½ï¿½n']]

you should replace it by the more pandas-like:
falso_1 = df1.loc[df1['fraudulento'] == 1, 'descripciï¿½ï¿½n'].to_numpy()
falso_1 .shape

falso_1.shape will give you the number of indexes you have in it<",https://stackoverflow.com/questions/62520221,python,22-06-2020 17:31,295.0,0.0,1.0,True,12-11-2021 00:27,12-11-2021 00:27
63415216,list of 2d tensors to one 3d tensor,"i have a list of sentences as of word embeddings. so every sentence is a matrix in 16*300, so it is a 2d tensor. i want to connect them to a 3d tensor and use this 3d tensor as input for a cnn model. unfortunately, i cannot get it into this 3d tensor.
in my opinion, at least connecting two of these 2d tensors to a smaller 3d tensor via tf.concat should work. unfortunately, i get the following error message
tf.concat(0, [tweets_final.text_m[0], tweets_final.text_m[1]])

valueerror: shape (3, 16, 300) must have rank 0

if it works with two 2d tensors i would probably work with one loop
one of these 2d tensors in the list looks like this one:
<tf.tensor: shape=(16, 300), dtype=float32, numpy= array([[-0.03571776,  0.07699937, -0.02208528, ...,  0.00873246,
    -0.05967658, -0.03735098],
   [-0.03044251,  0.050944  , -0.02236165, ..., -0.01745957,
     0.01311598,  0.01744673],
   [ 0.        ,  0.        ,  0.        , ...,  0.        ,
     0.        ,  0.        ],
   ...,
   [ 0.        ,  0.        ,  0.        , ...,  0.        ,
     0.        ,  0.        ],
   [ 0.        ,  0.        ,  0.        , ...,  0.        ,
     0.        ,  0.        ],
   [ 0.        ,  0.        ,  0.        , ...,  0.        ,
     0.        ,  0.        ]], dtype=float32)>","['python', 'tensorflow', 'tensor', 'conv-neural-network', 'word-embedding']",63415309,"you can found the solution in the documentation:

tf.stack: stacks a list of rank-r tensors into one rank-(r+1) tensor.
>>> x = tf.constant([1, 4])
>>> y = tf.constant([2, 5])
>>> z = tf.constant([3, 6])
>>> tf.stack([x, y, z])
<tf.tensor: shape=(3, 2), dtype=int32, numpy=
array([[1, 4],
       [2, 5],
       [3, 6]], dtype=int32)>
>>> tf.stack([x, y, z], axis=1)
<tf.tensor: shape=(2, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)>",https://stackoverflow.com/questions/63415216,python,14-08-2020 14:57,1999.0,2.0,1.0,True,17-05-2021 07:29,14-08-2020 15:09
49100615,nltk. detecting whether a sentence is interrogative or not?,i want to create a python script using nltk or whatever library is best to correctly identify given sentence is interrogative (a question) or not. i tried using regex but there are deeper scenarios where regex fails. so wanted to use natural language processing can anybody help!,"['python', 'machine-learning', 'nlp', 'artificial-intelligence', 'nltk']",50583762,"this will probably solve your question. 
here is the code:
import nltk
nltk.download('nps_chat')
posts = nltk.corpus.nps_chat.xml_posts()[:10000]


def dialogue_act_features(post):
    features = {}
    for word in nltk.word_tokenize(post):
        features['contains({})'.format(word.lower())] = true
    return features

featuresets = [(dialogue_act_features(post.text), post.get('class')) for post in posts]
size = int(len(featuresets) * 0.1)
train_set, test_set = featuresets[size:], featuresets[:size]
classifier = nltk.naivebayesclassifier.train(train_set)
print(nltk.classify.accuracy(classifier, test_set))

and that should print something like 0.67, which is decent accuracy.
if you want to process a string of text through this classifier, try:
print(classifier.classify(dialogue_act_features(line)))

and you can categorise strings into whether they are ynquestion, statement, etc, and extract what you desire. 
this approach was using naivebayes which in my opinion is the easiest, however surely there are many ways to process this. hope this helps!",https://stackoverflow.com/questions/49100615,python,04-03-2018 21:19,14984.0,13.0,4.0,True,09-02-2023 01:22,09-04-2022 21:34
54745482,what is the difference between tfidf vectorizer and tfidf transformer,"i know that the formula for tfidf vectorizer is 
count of word/total count * log(number of documents / no.of documents where word is present)

i saw there's tfidf transformer in the scikit learn and i just wanted to difference between them. i could't find anything that's helpful.","['python', 'scikit-learn', 'nltk', 'tf-idf', 'tfidfvectorizer']",54748136,"tfidfvectorizer is used on raw documents, while
tfidftransformer is used on an existing count matrix, such as one returned by countvectorizer",https://stackoverflow.com/questions/54745482,python,18-02-2019 10:45,12733.0,7.0,4.0,True,09-05-2024 02:30,18-02-2019 12:10
68835360,pyodide filesystem for nltk resources : missing files,"i am trying to use nltk in browser, thanks to pyodide.
pyodide starts well, manages to load nltk, print its version.
nevertheless, while the package downloading seems fine, when invoking nltk.sent_tokenize(str), nltk raises the error that it can't find the package ""punkt"".
i would say the downloaded resource is lost somewhere, but i didn't understand well how pyodide / webassembly manage files. any insights ?

simple version:
import nltk
nltk.download(pkg)
for sent in nltk.sent_tokenize(""test string""):
    print(sent)

version with more details, specifying download directory and server url.
import nltk
pkg = ""punkt""
downloader = nltk.downloader.downloader(server_index_url="" 
downloader.download(pkg, download_dir='/nltk_data')
downloader.status(pkg)
for sent in nltk.sent_tokenize(""test string""):
    print(sent)

full sample code:
<!doctype html>
<html>
  <body>
    <script type=""text/javascript"" src=""
    <script type=""text/javascript"">
      // init pyodide
      async function pyodide_loader() {
        let pyodide_premise = loadpyodide({
          indexurl: ""
        });
        let pyodide = await pyodide_premise;
        await pyodide.loadpackage(""micropip"");
        await pyodide.loadpackage(""nltk"");
        return pyodide_premise;
      }
      let pyodidereadypromise = pyodide_loader();

      
      // run python code and load nltk
      async function load_packages() {
        let pyodide = await pyodidereadypromise;
        let output = pyodide.runpython(`
print(f""*** import nltk"")
import nltk
print(f""*** nltk version {nltk.__version__=} imported, downloading resources now"")

pkg = ""punkt""
nltk.download(pkg)

str = ""just for testing""
for sent in nltk.sent_tokenize(str):
    print(sent)
      `);
      }
      load_packages()
    </script>
  </body>
</html>","['filesystems', 'nltk', 'webassembly', 'pyodide']",69032625,"short answer is that downloading files with python currently won't work in pyodide because  requests etc require posix sockets which are not supported in the browser vm.
it's curious that nltk.download doesn't error though -- it should have.
the workaround is to manually download the needed resources, for instance, using the javascript fetch api as illustrated in this comment;
from js import fetch

response = await fetch(""<url>"")
js_buffer = await response.arraybuffer()
py_buffer = js_buffer.to_py()  # this is a memoryview
stream = py_buffer.tobytes()  # now we have a bytes object

# that we can finally write under the appropriate path
with open(""<file_path>"", ""wb"") as fh:
    fh.write(stream)


i didn't understand well how pyodide / webassembly manage files.

by default it's virtual file-system (memfs) that gets reset at each page load. you can access it with standard python tools (open, 'os', etc). if necessary you can also mount a persistent filesystem.",https://stackoverflow.com/questions/68835360,filesystems,18-08-2021 15:34,663.0,5.0,2.0,True,14-11-2021 22:03,29-09-2021 13:25
69249665,bert text clasisification using pytorch,"i am trying to build a bert model for text classification with the help of this code [ my dataset contains two columns(label, text).
the labels can have three values of (0,1,2). the code works without any error but all values of confusion matrix are 0. is there something wrong with my code?
import matplotlib.pyplot as plt
import pandas as pd
import torch
from torchtext.data import field, tabulardataset, bucketiterator, iterator
import torch.nn as nn
from transformers import berttokenizer, bertforsequenceclassification
import torch.optim as optim
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import seaborn as sns

torch.manual_seed(42)
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

tokenizer = berttokenizer.from_pretrained('bert-base-uncased')

max_seq_len = 128
pad_index = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)
unk_index = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)


label_field = field(sequential=false, use_vocab=false, batch_first=true, dtype=torch.float)
text_field = field(use_vocab=false, tokenize=tokenizer.encode, lower=false, include_lengths=false, batch_first=true, fix_length=max_seq_len, pad_token=pad_index, unk_t>
fields = [('label', label_field), ('text', text_field)]
classification_report = ""classification_report.jsonl""


train, valid, test = tabulardataset.splits(path='', train='train.csv', validation='validate.csv', test='test.csv', format='csv', fields=fields, skip_header=true)

train_iter = bucketiterator(train, batch_size=16, sort_key=lambda x: len(x.text), device=device, train=true, sort=true, sort_within_batch=true)
valid_iter = bucketiterator(valid, batch_size=16, sort_key=lambda x: len(x.text), device=device, train=true, sort=true, sort_within_batch=true)
test_iter = iterator(test, batch_size=16, device=device, train=false, shuffle=false, sort=false)

class bert(nn.module):
        def __init__(self):
                super(bert, self).__init__()
                options_name = ""bert-base-uncased""
                self.encoder = bertforsequenceclassification.from_pretrained(options_name, num_labels = 3)

        def forward(self, text, label):
                loss, text_fea = self.encoder(text, labels=label)[:2]
                return loss, text_fea

def train(model, optimizer, criterion = nn.bceloss(), train_loader = train_iter, valid_loader = valid_iter, num_epochs = 5, eval_every = len(train_iter) // 2, file_pat>        running_loss = 0.0
        valid_running_loss = 0.0
        global_step = 0
        train_loss_list = []
        valid_loss_list = []
        global_steps_list = []

        model.train()

        for epoch in range(num_epochs):
                for (label, text), _ in train_loader:
                        label = label.type(torch.longtensor)
                        label = label.to(device)
                        text = text.type(torch.longtensor)
                        text = text.to(device)
                        output = model(text, label)
                        loss, _ = output
                        optimizer.zero_grad()
                        loss.backward()
                        optimizer.step()
                        running_loss += loss.item()
                        global_step += 1
                        if global_step % eval_every == 0:
                                model.eval()
                                with torch.no_grad():
                                        for (label, text), _ in valid_loader:
                                                label = label.type(torch.longtensor)
                                                label = label.to(device)
                                                text = text.type(torch.longtensor)
                                                text = text.to(device)
                                                output = model(text, label)
                                                loss, _ = output
                                                valid_running_loss += loss.item()

                                average_train_loss = running_loss / eval_every
                                average_valid_loss = valid_running_loss / len(valid_loader)
                                train_loss_list.append(average_train_loss)
                                valid_loss_list.append(average_valid_loss)
                                global_steps_list.append(global_step)


                                # resetting running values
                                running_loss = 0.0
                                valid_running_loss = 0.0
                                model.train()

                                # print progress
                                print('epoch [{}/{}], step [{}/{}], train loss: {:.4f}, valid loss: {:.4f}'.format(epoch+1, num_epochs, global_step, num_epochs*len(tra>
                                if best_valid_loss > average_valid_loss:
                                        best_valid_loss = average_valid_loss
        print('finished training!')

model = bert().to(device)
optimizer = optim.adam(model.parameters(), lr=2e-5)

train(model=model, optimizer=optimizer)


def evaluate(model, test_loader):
        y_pred = []
        y_true = []
        model.eval()
        with torch.no_grad():
                for (label, text), _ in test_loader:
                        label = label.type(torch.longtensor)
                        label = label.to(device)
                        text = text.type(torch.longtensor)
                        text = text.to(device)
                        output = model(text, label)

                        _, output = output
                        y_pred.extend(torch.argmax(output, 2).tolist())
                        y_true.extend(label.tolist())
        print('classification report:')
        print(classification_report(y_true, y_pred, labels=[0,1,2], digits=4))
best_model = bert().to(device)
evaluate(best_model, test_iter)","['python', 'nlp', 'pytorch', 'classification', 'bert-language-model']",69644158,"you are using criterion = nn.bceloss(), binary cross entropy for a multi class classification problem, ""the labels can have three values of (0,1,2)"". use suitable loss function for multiclass classification.",https://stackoverflow.com/questions/69249665,python,20-09-2021 05:57,828.0,0.0,1.0,True,20-10-2021 10:17,20-09-2021 19:40
77788451,"i have rectangular image dataset in vision transformers. i set image_size= (128, 256) but what could be the patch size,?","any help will be greatly appreciated. i am very much confused about when i use image_size=(128, 256 ), what will be the  patch_size, if i use patch_size= 16 only i can run code upto  model training phase. here is how i set dimensions
--------------------------
#import torch.nn.functional as nnf
# create image size
img_size = 128, 256

# create transform pipeline manually
manual_transforms = transforms.compose([
    transforms.resize((img_size)),
    transforms.totensor(),
])           
print (f""manually created transforms: {manual_transforms}"") 

output: manually created transforms: compose (
resize (size= (128, 256), interpolation=bilinear, max_size=none, 
antialias=warn)
totensor()
)

i am trying to use vision transformers for image classification
on custom dataset. but my dataset contains all rectangular shapes. when i use image size = 224 then accuracy is not so good, because i guess rectangular shape is reshaped into a square of 224 x 224, due to which
during training phase image features are not fully extracted. i want to input image to transformers encoder as 128 x 256 but when i set patch_siz= 16 then got an error when go for model training phase. **the runtime error is--> *****runtimeerror: sizes of tensors must
except in dimension 1.expected size 1024 but got size 64 for
tensor number 1 in the list. *******
  i go errorless results when i use image size as 224x224. the issue is with rectangular shaped dimensions. i am using batch size = 16, image_size=(128, 256), patch size= 16. but is patch_size 16 is ok or it could be patch_size=(16x32) but when try to set patch_size=(16, 32) like this


import torch.nn.functional as nnf
# create image size
img_size =  128, 256

# create transform pipeline manually
manual_transforms = transforms.compose([
    transforms.resize((img_size)),
    transforms.totensor(),
])           
print(f""manually created transforms: {manual_transforms}"")
--------
output=manually created transforms: compose(
resize(size=(128, 256), interpolation=bilinear, max_size=none, 
antialias=warn)
totensor()
)
--------
# set the batch size
batch_size = 16

# create data loaders
train_dataloader, test_dataloader, class_names = 
create_dataloaders(
    train_dir=train_dir,
    test_dir=test_dir,
    transform=manual_transforms, 
    batch_size=batch_size
)

train_dataloader, test_dataloader, class_names

----------------------------------
output:(<torch.utils.data.dataloader.dataloader at 
0x2148d4b3700>,
 <torch.utils.data.dataloader.dataloader at 0x2148d4b3a00>,
 ['tempered', 'genuine'])
______________________________________________

# 1. create a class which subclasses nn.module
class patchembedding(nn.module):
    """"""turns a 2d input image into a 1d sequence learnable 
embedding vector.

    args:
        in_channels (int): number of color channels for the input 
images. defaults to 3.
        patch_size (int): size of patches to convert input image 
into. defaults to 16.
        embedding_dim (int): size of embedding to turn image 
into. defaults to 768.
"""""" 
# 2. initialize the class with appropriate variables
def __init__(self, 
                 in_channels:int=3,
                 patch_size:int=(16, 32),
                 embedding_dim:int=768):
        super().__init__()
    
# 3. create a layer to turn an image into patches
self.patcher = nn.conv2d(in_channels=in_channels,
                             out_channels=embedding_dim,
                             kernel_size=patch_size,
                             stride=patch_size,
                             padding=0)

# 4. create a layer to flatten the patch feature maps into a 
single dimension
self.flatten = nn.flatten(start_dim=2, # only flatten the 
feature map dimensions into a
single vector
                              end_dim=3)

# 5. define the forward method 
def forward(self, x):
# create assertion to check that inputs are the correct shape
image_resolution = x.shape[-1]
assert image_resolution % patch_size == 0, f""input image size 
must be divisble by patch size,
image shape: {image_resolution}, patch size: {patch_size}""
    
# perform the forward pass
        x_patched = self.patcher(x)
        x_flattened = self.flatten(x_patched) 
    
# 6. make sure the output shape has the right order 
        return x_flattened.permute(0, 2, 1)
-------------------------------------------------

# let's test it on single image
patch_size = (16, 32)

# set seeds
def set_seeds(seed: int=42):
""""""sets random sets for torch operations.

args:
    seed (int, optional): random seed to set. defaults to 42.
""""""
# set the seed for general torch operations
torch.manual_seed(seed)
# set the seed for cuda torch operations (ones that happen on the 
gpu)
torch.cuda.manual_seed(seed)

set_seeds()

 # create an instance of patch embedding layer
patchify = patchembedding(in_channels=3,
                      patch_size=(16, 32), 
                      embedding_dim=768)

 # pass a single image through
print(f""input image shape: {image.unsqueeze(0).shape}"")
patch_embedded_image = patchify(image.unsqueeze(0)) # add an 
extra batch dimension on the 0th
index, otherwise will error
print(f""output patch embedding shape: 
{patch_embedded_image.shape}"")

**here is full code blocks where error traceback is 
mentionining**

________________________________________________________________
typeerror                                 traceback (most recent 
call last)
cell in[111], line 27
25 # pass a single image through
26 print(f""input image shape: {image.unsqueeze(0).shape}"")
---> 27 patch_embedded_image = patchify(image.unsqueeze(0)) # add 
an extra batch dimension on
the 0th index, otherwise will error
28 print(f""output patch embedding shape: 
{patch_embedded_image.shape}"")

file ~\appdata\local\programs\python\python39\lib\site- 
packages\torch\nn\modules\module.py:1518,
in module._wrapped_call_impl(self, *args, **kwargs)
1516     return self._compiled_call_impl(*args, **kwargs)  # 
type: ignore[misc]
1517 else:
-> 1518     return self._call_impl(*args, **kwargs)

file ~\appdata\local\programs\python\python39\lib\site- 
packages\torch\nn\modules\module.py:1527,
in module._call_impl(self, *args, **kwargs)
1522 # if we don't have any hooks, we want to skip the rest of 
the logic in
1523 # this function, and just call forward.
1524 if not (self._backward_hooks or self._backward_pre_hooks or 
self._forward_hooks or 
self._forward_pre_hooks
1525         or _global_backward_pre_hooks or 
_global_backward_hooks
1526         or _global_forward_hooks or 
_global_forward_pre_hooks):
-> 1527     return forward_call(*args, **kwargs)
1529 try:
1530     result = none

cell in[107], line 32, in patchembedding.forward(self, x)
29 def forward(self, x):
30     # create assertion to check that inputs are the correct 
shape
31     image_resolution = x.shape[-1]
---> 32     assert image_resolution % patch_size == 0, f""input 
image size must be divisble by
patch size, image shape: {image_resolution}, patch size: 
{patch_size}""
34     # perform the forward pass
35     x_patched = self.patcher(x)

typeerror: unsupported operand type(s) for %: 'int' and 'tuple'

----------------------------------------------------------

but if i set patch_size=16 throghly then after this code got 
error 
typeerror: cannot unpack non-iterable int object
_________________________________________________________
if i use patch_size=(16,32) then 

---------------------
import torch
from vit_pytorch import vit

class vit(nn.module):
""""""creates a vision transformer architecture with vit-base 
hyperparameters by default.""""""

def __init__(self, img_size=(128, 256), in_channels=3, 
patch_size=(16, 32), num_transformer_layers=12, 
embedding_dim=768, 
mlp_size=3072, num_classes=1000, dim=1024, depth=6, num_heads=8, 
mlp_dim=2048, mlp_dropout=0.1, embedding_dropout=0.1):
super().__init__()
self.img_size = img_size
self.in_channels=in_channels
self.patch_size = patch_size
self.num_transformer_layers = num_transformer_layers
self.embedding_dim = embedding_dim
self.mlp_size=mlp_size
self.num_classes = num_classes
self.dim =dim
self.depth = depth
self.num_heads = num_heads
self.mlp_dim = mlp_dim
self.mlp_dropout = mlp_dropout
self.embedding_dropout = embedding_dropout
    
    # calculate number of patches
height, width = img_size
patch_height, patch_width = patch_size
self.num_patches = (height // patch_height) * (width // 
patch_width)
    
    # calculate patch embedding
self.patch_embedding = nn.conv2d(in_channels=in_channels, 
embedding_dim=enbedding_dim, patch_size=patch_size, 
stride=patch_size, bias=false)
self.patch_embedding = nn.conv2d(in_channels=in_channels,
                                          kernel_size=patch_size,
                                          
 embedding_dim=enbedding_dim,
                                            bias=false)
    # calculate class token
    self.class_token = nn.parameter(torch.randn(1, 1, dim))
    
    # calculate positional embeddings
    self.row_embeddings = nn.parameter(torch.randn(height // 
patch_height, 1, dim))
self.col_embeddings = nn.parameter(torch.randn(width // 
patch_width, 1, dim))
    
    # calculate transformer blocks
    self.transformer_encoder = 









nn.modulelist([transformerencoderblock(
                                     embedding_dim=embedding_dim,
                                                                        
                                      num_heads=num_heads,
                                                                        
                                       mlp_size=mlp_size,
                                                                    
                                       mlp_dropout=mlp_dropout) 
                           for _ in range(num_transformer_layers)
])
    
    # calculate layer normalization
    self.layer_norm = nn.layernorm(dim)
    
    # calculate classification head
    self.classification_head = nn.linear(dim, num_classes)
    
def forward(self, x):
    # calculate patch embeddings
    x = self.patch_embedding(x)
    x = x.flatten(2).transpose(1, 2)
    
    # calculate class token
    class_token = self.class_token.expand(x.shape[0], -1, -1)
    x = torch.cat((class_token, x), dim=1)
    
    # calculate positional embeddings
    row_embeddings = self.row_embeddings.repeat(1, x.shape[0], 1)
    col_embeddings = self.col_embeddings.repeat(1, x.shape[0], 1)
    x = x + row_embeddings + col_embeddings
    
    # calculate transformer blocks
    for transformer_block in self.transformer_encoder:
        x = transformer_block(x)
    
    # calculate layer normalization
    x = self.layer_norm(x)
    
    # calculate classification head
    class_logits = self.classification_head(x[:, 0])
    
return class_logits
_______________________________________

# train our model

# create an instance of vit with the number of classes we're 
working with (-,-)
vit = vit(num_classes=len(class_names))

____________________________________________________________

from going_modular.going_modular import engine

# setup the optimizer to optimize our vit model parameters using 
hyperparameters from the vit
paper 
optimizer = torch.optim.adam(params=vit.parameters(), 
                         lr=3e-3, # base lr from table 3 for vit- 
* imagenet-1k
                         betas=(0.9, 0.999), 
                         weight_decay=0.3) # from the vit paper 
section 4.1 (training & fine-
                         tuning) and table 3 for vit-* imagenet- 
1k

# setup the loss function for multi-class classification
loss_fn = torch.nn.crossentropyloss()

# set the seeds
set_seeds()

# train the model and save the training results to a dictionary
results = engine.train(model=vit,
                   train_dataloader=train_dataloader,
                   test_dataloader=test_dataloader,
                   optimizer=optimizer,
                   loss_fn=loss_fn,
                   epochs=10,
                   device=device)

---------------------------------
out-
0%|                                                                                                                                               
----------------------------------------------------------------- 

-----------------------------------------------------------------
runtimeerror                              traceback (most recent 
call 
last)
cell in[132], line 16
13 set_seeds()
15 # train the model and save the training results to a 
dictionary
---> 16 results = engine.train(model=vit,
17                        train_dataloader=train_dataloader,
18                        test_dataloader=test_dataloader,
 
19                        optimizer=optimizer,
20                        loss_fn=loss_fn,
21                        epochs=10,
22                        device=device)

file ~\appdata\local\programs\python\python39\scripts\image- 
classification-using-vision-transformer- 
main\going_modular\going_modular\engine.py:169, in train(model, 
train_dataloader, test_dataloader, optimizer, loss_fn, epochs, 
device)
167 # loop through training and testing steps for a number of 
epochs
168 for epoch in tqdm(range(epochs)):
--> 169     train_loss, train_acc = train_step(model=model,
170                                       
dataloader=train_dataloader,
171                                       loss_fn=loss_fn,
172                                       optimizer=optimizer,
173                                       device=device)
174     test_loss, test_acc = test_step(model=model,
175       dataloader=test_dataloader,
176       loss_fn=loss_fn,
177       device=device)
179     # print out what's happening

file ~\appdata\local\programs\python\python39\scripts\image- 
classification-using-vision-transformer- 
main\going_modular\going_modular\engine.py:45, in 
 train_step(model, 
 dataloader, loss_fn, optimizer, device)
 42 x, y = x.to(device), y.to(device)
 44 # 1. forward pass
---> 45 y_pred = model(x)
 47 # 2. calculate  and accumulate loss
 48 loss = loss_fn(y_pred, y)

file ~\appdata\local\programs\python\python39\lib\site- 
 packages\torch\nn\modules\module.py:1518, in 
 module._wrapped_call_impl(self, *args, **kwargs)
 1516     return self._compiled_call_impl(*args, **kwargs)  # 
 type: 
 ignore[misc]
 1517 else:
-> 1518     return self._call_impl(*args, **kwargs)

file ~\appdata\local\programs\python\python39\lib\site- 
packages\torch\nn\modules\module.py:1527, in 
module._call_impl(self, *args, **kwargs)
1522 # if we don't have any hooks, we want to skip the rest of  
logic in
1523 # this function, and just call forward.
1524 if not (self._backward_hooks or self._backward_pre_hooks or 
self._forward_hooks or self._forward_pre_hooks
1525         or _global_backward_pre_hooks or 
_global_backward_hooks
1526         or _global_forward_hooks or 
_global_forward_pre_hooks):
-> 1527     return forward_call(*args, **kwargs)
1529 try:
1530     result = none

cell in[94], line 63, in vit.forward(self, x)
 61 # calculate class token
 62 class_token = self.class_token.expand(x.shape[0], -1, -1)
 ---> 63 x = torch.cat((class_token, x), dim=1)
 65 # calculate positional embeddings
 66 row_embeddings = self.row_embeddings.repeat(1, x.shape[0], 1)

runtimeerror: sizes of tensors must match except in dimension 1. 
expected size 1024 but got size 64 for tensor number 1 in the 
list.","['python', 'machine-learning', 'pytorch', 'typeerror', 'huggingface-transformers']",77792920,"you are using vit from module vit_pytorch
this is the vit class definition taken from github
class vit(nn.module):
    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):
        super().__init__()
        image_height, image_width = pair(image_size)
        patch_height, patch_width = pair(patch_size)

        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'image dimensions must be divisible by the patch size.'

        num_patches = (image_height // patch_height) * (image_width // patch_width)

        # more code follows that we dont care about ...

and this is the definition of pair()
def pair(t):
    return t if isinstance(t, tuple) else (t, t)

and this is taken from the docs

image_size: if you have rectangular images, make sure your image size is the maximum of the width and height.


patch_size: size of patches. image_size must be divisible by patch_size.


the number of patches is:  n = (image_size // patch_size) ** 2 and n must be greater than 16. in your case, choose 16 (larger model) or 32 (smaller model)

as you can see, vit can handle both, image_shape as tuples (first height, then width) or as a single number. the pair function will handle the latter case and duplicates the value into a tuple.
there is only on line of code where the image_size is ever used, and that is for calculating the number of tokens. this calculation is rather easy, as it only requires to divide the image_height by patch_height and image_width by patch_width and multiplying these values.
so, coming back at your problem: the docs tell a different story than the code. reading the code, which is clear about the situation, i'd recommend to pass a tuple of (image_height, image_width) to the image_size argument of vit().",https://stackoverflow.com/questions/77788451,python,09-01-2024 16:55,2176.0,0.0,2.0,True,11-01-2024 09:08,09-01-2024 18:36
70287849,how can i optimize an sql query for calculating word frequency?,"i am trying to populate two tables:
token:
 word  | df(the number of documents containing a word) 
==========
""dog""  | 5    
""cat""  | 2    
""horse""| 1    

token_count:
tokenid | docid| tf(the number of times a word occurs in a document)
====================
   1    |  1   | 6
   2    |  2   | 2
   3    |  2   | 1

using the data from documents:
id   |  title  |     body
=============================
1    |  ""dog""  | ""about dogs"" 
2    |  ""cats"" | ""about cats""

to do that i use  ts_stat( 'select to_tsvector(''english'', body) from documents' ) which returns a table with the document frequency for the word and also the number of times that words appears in the entire column. while the second column is exactly what i need for the token table the third column shows the document frequency for the entire column.
word | ndoc | nentry
====================
dog  | 5    | 6
cat  | 2    | 2
horse| 1    | 1

this code populates the token table and does it in 3sec for a hundred documents.
insert into token (word, document_frequency)
select
    word,
    ndoc
from 
    ts_stat( 'select to_tsvector(''english'', body) from documents' );

i tried running the following code on a smaller dataset of 15 documents and it worked but when i'm trying to run this on the current dataset(100 docs) it never stops running.
with temp_data as (
    select id , 
           (ts_stat('select to_tsvector(''english'', body) from documents where id='||id)).*
    from documents 
)
insert into token_count (docid, tokenid, tf)
select
    id,
    (select id from token where word = temp_data.word limit 1),
    nentry
from temp_data;

how can i optimize this query?
explain analyze for the dataset of 15 documents:
""insert on token_count  (cost=1023803.22..1938766428.23 rows=9100000 width=28) (actual time=59875.204..59875.206 rows=0 loops=1)""
""  cte temp_data""
""    ->  result  (cost=0.00..1023803.22 rows=9100000 width=44) (actual time=0.144..853.320 rows=42449 loops=1)""
""          ->  projectset  (cost=0.00..45553.23 rows=9100000 width=36) (actual time=0.142..809.366 rows=42449 loops=1)""
""                ->  seq scan on wikitable  (cost=0.00..19.10 rows=910 width=4) (actual time=0.010..0.029 rows=16 loops=1)""
""  ->  cte scan on temp_data  (cost=0.00..1937742625.00 rows=9100000 width=28) (actual time=0.509..59652.279 rows=42449 loops=1)""
""        subplan 2""
""          ->  limit  (cost=0.00..212.92 rows=1 width=4) (actual time=1.381..1.381 rows=1 loops=42449)""
""                ->  seq scan on token  (cost=0.00..425.84 rows=2 width=4) (actual time=1.372..1.372 rows=1 loops=42449)""
""                      filter: ((word)::text = temp_data.word)""
""                      rows removed by filter: 10384""
""planning time: 0.202 ms""
""execution time: 59876.350 ms""

explain analyze for the dataset of 30 documents:
""insert on token_count  (cost=1023803.22..6625550803.23 rows=9100000 width=28) (actual time=189910.438..189910.439 rows=0 loops=1)""
""  cte temp_data""
""    ->  result  (cost=0.00..1023803.22 rows=9100000 width=44) (actual time=0.191..2018.758 rows=92168 loops=1)""
""          ->  projectset  (cost=0.00..45553.23 rows=9100000 width=36) (actual time=0.189..1919.726 rows=92168 loops=1)""
""                ->  seq scan on wikitable  (cost=0.00..19.10 rows=910 width=4) (actual time=0.013..0.053 rows=31 loops=1)""
""  ->  cte scan on temp_data  (cost=0.00..6624527000.00 rows=9100000 width=28) (actual time=1.009..189412.022 rows=92168 loops=1)""
""        subplan 2""
""          ->  limit  (cost=0.00..727.95 rows=1 width=4) (actual time=2.029..2.029 rows=1 loops=92168)""
""                ->  seq scan on token  (cost=0.00..727.95 rows=1 width=4) (actual time=2.020..2.020 rows=1 loops=92168)""
""                      filter: ((word)::text = temp_data.word)""
""                      rows removed by filter: 16463""
""planning time: 0.234 ms""
""execution time: 189913.688 ms""","['sql', 'postgresql', 'tf-idf', 'tsvector']",70290671,"here's a demo that doesn't use ts_stat to get the word counts.
instead it uses a lateral join to an unnesting of the ts_vector.

create table documents (
 document_id serial primary key, 
 title varchar(30) not null, 
 body text not null
);

insert into documents (title, body) values
  ('dogs', 'the dog barked at the cat, but the cat ignored her.')
, ('cats', 'cats kill more birds than dogs kill cats')

create table tokens (
 token_id serial primary key, 
 word varchar(30),
 df int
)

insert into tokens (word, df)
select word, ndoc
from ts_stat('select to_tsvector(''english'', body) from documents');



select * from tokens order by df desc


token_id | word  | df
-------: | :---- | -:
       3 | dog   |  2
       4 | cat   |  2
       1 | kill  |  1
       2 | ignor |  1
       5 | bird  |  1
       6 | bark  |  1



create table token_counts (
 document_id int, 
 token_id int,
 tf int, 
 primary key (document_id, token_id), 
 foreign key (document_id) references documents(document_id), 
 foreign key (token_id) references tokens(token_id)
);



insert into token_counts (
 document_id, 
 token_id, 
 tf
)
select 
 doc.document_id, 
 tok.token_id, 
 lex.total
from documents as doc
cross join lateral (
  select lexeme, cardinality(positions) as total
  from unnest(to_tsvector('english', doc.body)) as tsvector
) as lex
inner join tokens as tok
  on tok.word = lex.lexeme;



select title, word, tf
from token_counts cnt
join documents doc using(document_id)
join tokens tok using(token_id)
order by document_id, token_id






title
word
tf




dogs
ignor
1


dogs
dog
1


dogs
cat
2


dogs
bark
1


cats
kill
2


cats
dog
1


cats
cat
2


cats
bird
1




demo on db<>fiddle here",https://stackoverflow.com/questions/70287849,sql,09-12-2021 09:55,318.0,2.0,1.0,True,09-12-2021 13:21,09-12-2021 12:04
76220715,type &quot;vector&quot; does not exist on postgresql - langchain,"i was trying to embed some documents on postgresql with the help of pgvector extension and langchain. unfortunately i'm having trouble with the following error:
(psycopg2.errors.undefinedobject) type ""vector"" does not exist
line 4:  embedding vector(1536), 
                   ^

[sql: 
create table langchain_pg_embedding (
    collection_id uuid, 
    embedding vector(1536), 
    document varchar, 
    cmetadata json, 
    custom_id varchar, 
    uuid uuid not null, 
    primary key (uuid), 
    foreign key(collection_id) references langchain_pg_collection (uuid) on delete cascade
)
]


my environment info:

pgvector docker image ankane/pgvector:v0.4.1
python 3.10.6, psycopg2 2.9.6, pgvector 0.1.6


list of installed extensions on postgres
  name   | version |   schema   |                description                 
---------+---------+------------+--------------------------------------------
 plpgsql | 1.0     | pg_catalog | pl/pgsql procedural language
 vector  | 0.4.1   | public     | vector data type and ivfflat access method

i've tried the following ways to resolve:

fresh installing the postgres docker image with pgvector extension enabled.
manually install the extension with the official instruction.
manually install the extension on postgres like the following:

create extension if not exists vector
    schema public
    version ""0.4.1"";

but no luck.","['python', 'postgresql', 'vectorization', 'langchain']",76221780,"update 17th july 2023
as previously i mentioned my issue was somewhere else in my configuration, here is the other reason that may be responsible for the error,

the pgvector extension isn't enabled in the database you are using. make sure you run create extension vector; in each database you are using for storing vectors.
the vector schema is not in the search_path. run show search_path; to see the available schemas in the search path and \dx to see the list of installed extensions with schemas.


unfortunately, the issue was somewhere else. my extension installation and search_path schema were totally okay for the defined database i was supposed to use. but my environment variable which was responsible for which database to use, got messed up and was using the default database postgres instead of my defined database, which didn't have the extension enabled.",https://stackoverflow.com/questions/76220715,python,10-05-2023 16:28,38889.0,18.0,6.0,True,02-12-2024 10:18,10-05-2023 16:53
69603849,retokenize email address,"is there a way to retokenize email addresses so that they are email addresses again? in the code i am using now (see below), if i give as input mobydick123@gmail.com i get as output mobydick123 @ gmail.com.
from nltk.tokenize.treebank import treebankworddetokenizer
from nltk.tokenize import word_tokenize
from itertools import groupby 
import spacy

input_sent = 'herman melvilles email is mobydick123@gmail.com'

tokens = word_tokenize(input_sent)
print(tokens)

mylist = list(filter((']').__ne__, tokens))
mylist = list(filter(('[').__ne__, mylist))

res = [i[0] for i in groupby(mylist)]
my_list = list(map(lambda item: item.replace(""w_nlp_person"", ""[w_nlp_person]"").replace('w_nlp_date', '[w_nlp_date]').replace('w_nlp_imei_hardware_id', '[w_nlp_imei_hardware_id]').replace('w_nlp_ip_address', '[w_nlp_ip_address]'), res))
my_list = treebankworddetokenizer().detokenize(my_list)

print(my_list)","['python', 'nltk', 'tokenize']",69604019,"assuming that the last 3 elements of your list will always be name + @ + domain, i.e:
>>> my_list.split()[-3:]
['mobydick123', '@', 'gmail.com']

you could split your list based on spaces and join two joins.
elements = my_list.split()
>>> print(' '.join(elements[:-3]),''.join(elements[-3:]))

herman melvilles email is mobydick123@gmail.com

the first part joins with a delimiter space, and the second part joins with no delimiter which puts the email together.  hinging on the assumption that the last 3 elements will be the email, which is a solid assumption as this is the case usually, this should work.",https://stackoverflow.com/questions/69603849,python,17-10-2021 11:26,150.0,0.0,1.0,True,17-10-2021 13:19,17-10-2021 13:19
75935538,openai gpt-3 api error: &quot;attributeerror: &#39;builtin_function_or_method&#39; object has no attribute &#39;text&#39;&quot;,"i'm looking for some help in extracting the ""text"" from chatgpt's ""openai.completion.create"" function.
this the function i'm using to generate the ""response"":
#have chatgpt generate keywords from article
def generate_keywords(article):
    response = openai.completion.create(
        model=""text-davinci-003"",
        prompt=article,
        temperature=0.7,
        max_tokens=60,
        top_p=1.0,
        frequency_penalty=0.0,
        presence_penalty=1
    )
    return response
#---

""article"" in this case is the text i am feeding chatgpt.
""response"" when printed, provides me with the following output:
{
  ""choices"": [
    {
      ""finish_reason"": ""length"",
      ""index"": 0,
      ""logprobs"": null,
      **""text"": "", iraq 2008. image by flickr user vabusdrivernow, i can\u2019t speak for all of us, but i know that after the war we were still running. running from our pasts, guilt, shame, fear, and the unexplainable anger that comes with being a""**
    }
  ],
  ""created"": 1680666103,
  ""id"": ""cmpl-71ojjqfwthltbcvsyfi7zzjrktzvt"",
  ""model"": ""text-davinci-003"",
  ""object"": ""text_completion"",
  ""usage"": {
    ""completion_tokens"": 60,
    ""prompt_tokens"": 1090,
    ""total_tokens"": 1150
  }
}

i want to extract the ""text"" from this data structure.
when i run this:
keywords = generate_keywords(article)
print(keywords.values.text)

but i get this:
file ""/users/wolf/development/openai/generate_medium_story_image/generate_ai_image.py"", line 63, in <module>
    print(keywords.values.text)
          ^^^^^^^^^^^^^^^^^^^^
attributeerror: 'builtin_function_or_method' object has no attribute 'text'","['python', 'openai-api', 'gpt-3']",75937188,"return just the text from the completion like this:
def generate_keywords(article):
    response = openai.completion.create(
        model = 'text-davinci-003',
        prompt = article,
        temperature = 0.7,
        max_tokens = 60,
        top_p = 1.0,
        frequency_penalty = 0.0,
        presence_penalty = 1
    )
    return response['choices'][0]['text'] # change this

then just print keywords like this:
keywords = generate_keywords(article)
print(keywords)",https://stackoverflow.com/questions/75935538,python,05-04-2023 04:01,828.0,0.0,1.0,True,06-04-2023 16:10,06-04-2023 16:10
68536546,using pipelines with a local model,"i am trying to use a simple pipeline offline. i am only allowed to download files directly from the web.
i went to  and downloaded all the files in a local folder c:\\users\\me\\mymodel
however, when i tried to load the model i get a strange error
from transformers import pipeline

classifier = pipeline(task= 'sentiment-analysis', 
                      model= ""c:\\users\\me\\mymodel"",
                      tokenizer = ""c:\\users\\me\\mymodel"")

valueerror: unable to parse c:\users\me\mymodel\modelcard.json as a url or as a local path

what is the issue here?","['python', 'tensorflow2.0', 'huggingface-transformers']",68550539,"the solution was slightly indirect:

load the model on a computer with internet access
save the model with save_pretrained()
transfer the folder obtained above to the offline machine and point its path in the pipeline call

the folder will contain all the expected files.",https://stackoverflow.com/questions/68536546,python,26-07-2021 21:32,9564.0,5.0,2.0,True,28-09-2024 21:31,28-09-2024 21:30
62781349,how to use neuralcoref in spacy,"i have been trying to use the library neuralcoref: state-of-the-art coreference resolution based on neural nets and spacy. i am using ubuntu 16.04, python 3.7.3 in conda 1.9.7 and spacy 2.2.4.
my code (from the 
import spacy
import neuralcoref
    

nlp = spacy.load('en_core_web_sm')
neuralcoref.add_to_pipe(nlp)
doc1 = nlp('my sister has a dog. she loves him.')
print(doc1._.coref_clusters)

doc2 = nlp('angela lives in boston. she is quite happy in that city.')
for ent in doc2.ents:
    print(ent._.coref_cluster)

i have got this error
/home/daniel/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: runtimewarning: spacy.morphology.morphology size changed, may indicate binary incompatibility. expected 104 from c header, got 112 from pyobject
  return f(*args, **kwds)
/home/daniel/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: runtimewarning: spacy.vocab.vocab size changed, may indicate binary incompatibility. expected 96 from c header, got 104 from pyobject
  return f(*args, **kwds)
/home/daniel/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: runtimewarning: spacy.tokens.span.span size changed, may indicate binary incompatibility. expected 72 from c header, got 80 from pyobject
  return f(*args, **kwds)

i have tried to downgrade the version of spacy to 2.1.0 as suggested by this link:
conda config --append channels conda-forge
conda install spacy=2.1.0

however, i am not able
packagesnotfounderror: the following packages are not available from current channels:

  - spacy=2.1.0

current channels:

  - 
  - 
  - 
  - 
  - 
  - 

to search for alternate channels that may provide the conda package you're
looking for, navigate to

    

and use the search bar at the top of the page.

how can i solve this issue without downgrade? is there any new updated version of neuralcoref?","['python-3.x', 'ubuntu', 'spacy', 'coreference-resolution']",62844213,"for neuralcoref to work, you need to use spacy version 2.1.0 and python version 3.7. that is the only combination that neuralcored works for on  ubuntu 16.04 and on mac.

install python 3.7 on your machine, see here
make sure the selected version of python is 3.7
create your project folder
create a python virtual environment in your given project folder like so, python -m venv ./venv,
install spacy 2.1.0 like so python -m pip install spacy==2.1.0.
install neuralcoref python -m pip install neuralcoref

hope this helps.

after running your code above, i get the following output:
[my sister: [my sister, she], a dog: [a dog, him]]
angela: [angela, she]
boston: [boston, that city]",https://stackoverflow.com/questions/62781349,python-3.x,07-07-2020 18:05,9609.0,6.0,5.0,True,10-07-2023 19:58,10-07-2020 10:45
71005700,access n:th element of every list in list of lists,"i have a problem in python and one step i need to do is to access the second element of every list in a list of lists. the list is:
[(0, 'gallery', 'propn', 'nsubj'), (1, 'unveils', 'verb', 'root'), (2, 'interactive', 'adj', 'amod')]

[(0, 'a', 'det' 'det'), (1 'christmas' , 'propn', 'compound'), (2, 'tree' ,'noun', 'nsubjpass')]

the list goes on like that with different sentences for each list.
what i am trying to do is access the third (index 2) element of every list of every tuple. so from the first list, i want to get:

propn (from 'gallery')
verb (from 'unveils')
adj (from 'interactive')

and from the second list i want to get:

det (from 'a')
propn (from 'christmas')
noun (from 'nsubjpass)

what i've tried to do is this:
for word in list:
      print(list[word[0]][2])

but that only outputs the second element from the first list. i tried creating another foor-loop but that didn't work, i suspect i did it wrong but i'm not sure how to solve it
help would be appreciated
thanks in advance!","['python', 'list', 'nlp', 'tuples']",71005762,"when you are enumerating over the list in the for loop:
for word in words:
    pass // do something

you have already accessed the element in the list and stored it in word.
as such, word[0] in your loop would be accessing the first element in your word tuple which is not what you'd like to do.
instead, you'd like to access word[2] in your tuple, so something like this should work:
first_list = [(0, 'gallery', 'propn', 'nsubj'), (1, 'unveils', 'verb', 'root'), (2, 'interactive', 'adj', 'amod')]

second_list = [(0, 'a', 'det' 'det'), (1, 'christmas' , 'propn', 'compound'), (2, 'tree' ,'noun', 'nsubjpass')]

def print_word_pos(words):
    for word in words:
        print(word[2])

print_word_pos(first_list)
print_word_pos(second_list)

another thing is that you should not be naming your lists as list since list is a reserved python keyword and might (will) cause conflict later down the line.
now if the first two lists were combined, you'd want to loop over each list and then for each word in that list, print out the part of speech.
def print_word_pos(list_of_words):
    for words in list_of_words:
        for word in words:
            print(word[2])",https://stackoverflow.com/questions/71005700,python,06-02-2022 09:14,2445.0,0.0,4.0,True,30-05-2023 13:00,06-02-2022 12:09
65099376,segmentation fault error in importing sentence_transformers in azure machine learning service nvidia compute,"i would like to use sentence_transformers in aml to run xlm-roberta model for sentence embedding. i have a script in which i import sentence_transformers:
from sentence_transformers import sentencetransformer

once i run my aml pipeline, the run fails on this script with the following error:
azuremlcompute job failed.
userprocesskilledbysystemsignal: job failed since the user script received system termination signal usually due to out-of-memory or segfault.
    cause: segmentation fault
    taskindex: 
    nodeip: #####
    nodeid: #####

i'm pretty sure that this import is causing this error, because if i comment out this import, the rest of the script will run.
this is weird because the installation of the sentence_transformers succeed.
this is the details of my compute:
virtual machine size
standard_nv24 (24 cores, 224 gb ram, 1440 gb disk)
processing unit
gpu - 4 x nvidia tesla m60

agent pool:
azure pipelines

agent specification:
ubuntu-16.04

requirements.txt file:
torch==1.4.0
sentence-transformers

does anyone have a solution for this error?","['azure', 'nvidia', 'azure-machine-learning-service', 'roberta-language-model', 'sentence-transformers']",65100257,"i fixed the issue by changing the pytorch version from 1.4.0 to 1.6.0.
so the requirements.txt looks like this:
torch==1.6.0
sentence-transformers

at first i tried one of the older versions of sentence-transformers which was compatible with pytorch 1.4.0. but the older version doesn't support ""xml-roberta-base"" model, so i tried to upgrade the pytorch version.",https://stackoverflow.com/questions/65099376,azure,01-12-2020 22:17,1499.0,1.0,2.0,True,26-02-2023 06:50,01-12-2020 23:52
8949517,is there any treebank for free?,is any place i can download treebank of english phrases for free or less than $100? i need training data containing bunch of syntactic parsed sentences (>1000) in english in any format. basically all i need is just words in this sentences being recognized by part of speech.,"['nlp', 'dataset', 'tagging', 'corpus']",8949630,nltk (for python) offers several treebanks for free.,https://stackoverflow.com/questions/8949517,nlp,21-01-2012 00:09,20506.0,25.0,3.0,True,06-08-2022 15:42,06-08-2022 15:42
78219076,how to remove layers in huggingface&#39;s transformers gpt2 pre-trained models?,"my code:
from transformers import gpt2config, gpt2model
from transformers import autotokenizer, automodelformaskedlm, automodelforcausallm
model = automodelforcausallm.from_pretrained(""openai-community/gpt2"")
print(decoder)

here is the output of the console, listing the model architecture:
gpt2lmheadmodel(
  (transformer): gpt2model(
    (wte): embedding(50257, 768)
    (wpe): embedding(1024, 768)
    (drop): dropout(p=0.1, inplace=false)
    (h): modulelist(
      (0-11): 12 x gpt2block(
        (ln_1): layernorm((768,), eps=1e-05, elementwise_affine=true)
        (attn): gpt2attention(
          (c_attn): conv1d()
          (c_proj): conv1d()
          (attn_dropout): dropout(p=0.1, inplace=false)
          (resid_dropout): dropout(p=0.1, inplace=false)
        )
        (ln_2): layernorm((768,), eps=1e-05, elementwise_affine=true)
        (mlp): gpt2mlp(
          (c_fc): conv1d()
          (c_proj): conv1d()
          (act): newgeluactivation()
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
    )
    (ln_f): layernorm((768,), eps=1e-05, elementwise_affine=true)
  )
  (lm_head): linear(in_features=768, out_features=50257, bias=false)
)

i want to remove the first layer:
(wte): embedding(50257, 768)

i've tried the following way:
def deleteencodinglayers(model, num_layers_to_keep):  # must pass in the full bert model
    oldmodulelist = model.bert.encoder.layer
    newmodulelist = nn.modulelist()

    # now iterate over all layers, only keepign only the relevant layers.
    for i in range(0, len(num_layers_to_keep)):
        newmodulelist.append(oldmodulelist[i])

    # create a copy of the model, modify it with the new list, and return
    copyofmodel = copy.deepcopy(model)
    copyofmodel.bert.encoder.layer = newmodulelist

    return copyofmodel

but it didn't work. who knows how to fix it?","['python', 'machine-learning', 'deep-learning', 'nlp', 'transformer-model']",78219403,"try these parameters to bypass the embedding layer :
class gpt2withoutwte(gpt2model):
def __init__(self, config):
    super().__init__(config)
    # remove the word token embedding layer
    del self.wte

def forward(
    self,
    inputs_embeds,
    attention_mask=none,
    token_type_ids=none,
    position_ids=none,
    head_mask=none,
    inputs=none,
    encoder_hidden_states=none,
    encoder_attention_mask=none,
    past_key_values=none,
    use_cache=none,
    output_attentions=none,
    output_hidden_states=none,
    return_dict=none,
):
    # here you will bypass the embedding layer and use inputs_embeds directly
    return super().forward(
        inputs_embeds=inputs_embeds,
        attention_mask=attention_mask,
        token_type_ids=token_type_ids,
        position_ids=position_ids,
        head_mask=head_mask,
        encoder_hidden_states=encoder_hidden_states,
        encoder_attention_mask=encoder_attention_mask,
        past_key_values=past_key_values,
        use_cache=use_cache,
        output_attentions=output_attentions,
        output_hidden_states=output_hidden_states,
        return_dict=return_dict,
    )

for the input embeddings you can use the following
inputs_embeds = torch.rand(1, 10, config.n_embd)

load the config of gpt2, send it to the class and then use the inputs_embeds for the new model.",https://stackoverflow.com/questions/78219076,python,25-03-2024 12:28,346.0,0.0,1.0,True,26-03-2024 10:36,26-03-2024 10:36
37043598,use brain.js neural network to do text analysis,"i'm trying to do some text analysis to determine if a given string is... talking about politics. i'm thinking i could create a neural network where the input is either a string or a list of words (ordering might matter?) and the output is whether the string is about politics.
however the brain.js library only takes inputs of a number between 0 and 1 or an array of numbers between 0 and 1. how can i coerce my data in such a way that i can achieve the task?","['neural-network', 'text-analysis', 'brain.js']",49340951,"new brain.recurrent.lstm(); 

this does the trick for you.
example,
var brain = require('brain.js')
var net = new brain.recurrent.lstm();
net.train([
  {input: ""my unit-tests failed."", output: ""software""},
  {input: ""tried the program, but it was buggy."", output: ""software""},
  {input: ""i need a new power supply."", output: ""hardware""},
  {input: ""the drive has a 2tb capacity."", output: ""hardware""},
  {input: ""unit-tests"", output: ""software""},
  {input: ""program"", output: ""software""},
  {input: ""power supply"", output: ""hardware""},
  {input: ""drive"", output: ""hardware""},
]);

console.log(""output = ""+net.run(""drive""));


output = hardware

refer to this link=>  
this has clear explanation and usage of brain.recurrent.lstm()",https://stackoverflow.com/questions/37043598,neural-network,05-05-2016 06:10,11883.0,15.0,3.0,True,15-11-2022 21:32,05-05-2016 06:53
71280204,group numpy array elements without for-loop,"after doing some text processing, i've got a list of tokens and a list of sentence indices, one for each token. now i'd like to reassemble the tokens into sentences. i've used numpy, but i feel like there's a better/faster/more-numpy-ish way to do this...without a for loop. there could be a lot more than two sentences in the future.
import numpy as np

all_tokens = np.array(['i', 'spent', 'a', 'lot', 'of', 'time', ',', 'money', ',', 'and', 'effort', 'childproofing', 'my', 'house', '.', 'however', ',', 'the', 'kids', 'still', 'get', 'in', '.'])
sent_ids = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])

new_sents = []
for unique_sent_id in np.unique(sent_ids):
    sent_tokens = all_tokens[sent_ids == unique_sent_id].tolist()
    new_sents.append(' '.join(sent_tokens))

result: [""i spent a lot of time , money , and effort childproofing my house ."", ""however , the kids still get in .""]","['python', 'numpy', 'performance', 'nlp']",71280315,"assuming sent_ids is ordered, you can find out the position where sent_id has changed  and then split tokens based on that:
list(map("" "".join, np.split(all_tokens, np.flatnonzero(np.diff(sent_ids) != 0)+1)))
# ['i spent a lot of time , money , and effort childproofing my house .', 'however , the kids still get in .']",https://stackoverflow.com/questions/71280204,python,26-02-2022 20:54,79.0,0.0,2.0,True,26-02-2022 21:14,26-02-2022 21:00
77066419,not receiving the local notification when app is in background,"i have integrated interactive local notifications. i have created a chatbot using openai. i want to interact with chatbot through interactive local notifications when the app is in the background.
when the user responds to an interactive local notification i receive the user response with the below function.
func usernotificationcenter(_ center: unusernotificationcenter, didreceive response: unnotificationresponse, withcompletionhandler completionhandler: @escaping () -> void) {
    if response.actionidentifier == ""replyaction"" {
        if let textresponse = response as? untextinputnotificationresponse {
            userquerytext = textresponse.usertext
            fetchgptchatforresponse(prompt: userquerytext ?? """")
        }
    }
    completionhandler()
}

after receiving the user response from the interactive notification, i then call the below function:
func fetchgptchatforresponse(prompt: string) {
    task {
        do {
            let gpttext = try await apiservice().sendprompttogpt(prompt: prompt)
            await mainactor.run {
                sendinteractivenotification(message: gpttext)
            }
        } catch {
            print(""errrror"")
        }
    }
}

since my app is in the background, i get following error:

nw_write_request_report [c2] send failed with error ""socket is not connected""

and

nw_read_request_report [c1] receive failed with error ""software caused connection abort"" sendprompttogpt

and no local notification is triggred. i have also implemented the background modes feature like this.
func application(_ application: uiapplication, didfinishlaunchingwithoptions launchoptions: [uiapplication.launchoptionskey: any]?) -> bool {
                
          // request notification permission
          unusernotificationcenter.current().requestauthorization(options: [.alert, .sound, .badge]) { (granted, error) in
                    if granted {
                        print(""notification permission granted"")
                    } else if let error = error {
                        print(""notification permission denied: \(error.localizeddescription)"")
                    }
                }
                
                unusernotificationcenter.current().delegate = self
                let customcategory = createnotificationcategory()
    unusernotificationcenter.current().setnotificationcategories([customcategory])
                iqkeyboardmanager.shared.enable = true
                registerbackgroundmodes()
                return true
            }

    func registerbackgroundmodes() {
            print(""registerbackgroundmodes"")
            let apprefreshtaskid = ""com.xyz.iosapp.apprefresh""
            let appprocessingtaskid = ""com.xyz.iosapp.fetch""
    
            bgtaskscheduler.shared.register(fortaskwithidentifier: apprefreshtaskid, using: nil) { task in
                    self.fetchgptchatforresponse(prompt: self.userquerytext ?? """")
                    task.settaskcompleted(success: true)
                    self.scheduleapprefresh()
                }
    
            bgtaskscheduler.shared.register(fortaskwithidentifier: appprocessingtaskid, using: nil) { task in
                //logger.shared.info(""[bgtask] perform bg processing \(appprocessingtaskid)"")
                self.fetchgptchatforresponse(prompt: self.userquerytext ?? """")
                task.settaskcompleted(success: true)
                self.schedulebackgroundprocessing()
    
            }
        }

func scheduleapprefresh() {
    print(""scheduleapprefresh"")
    let request = bgapprefreshtaskrequest(identifier: ""com.quilr.iosapp.apprefresh"")
    request.earliestbegindate = date(timeintervalsincenow: 2 * 60)
    do {
        try bgtaskscheduler.shared.submit(request)
    } catch {
        print(""could not schedule app refresh task \(error.localizeddescription)"")
    }
}

func schedulebackgroundprocessing() {
    print(""schedulebackgroundprocessing"")
    let request = bgprocessingtaskrequest(identifier: ""com.quilr.iosapp.fetch"")
    request.requiresnetworkconnectivity = true // need to true if your task need to network process. defaults to false.
    request.requiresexternalpower = true // need to true if your task requires a device connected to power source. defaults to false.
    request.earliestbegindate = date(timeintervalsincenow: 2 * 60) // process after 5 minutes.
    do {
        try bgtaskscheduler.shared.submit(request)
    } catch {
        print(""could not schedule image fetch: (error)"")
    }
}

and in scenedelegate.swift i call both the background refresh and background processing functions like this:
func scenedidenterbackground(_ scene: uiscene) {
    // called as the scene transitions from the foreground to the background.
    // use this method to save data, release shared resources, and store enough scene-specific state information
    // to restore the scene back to its current state.
    (uiapplication.shared.delegate as? appdelegate)?.scheduleapprefresh()
    (uiapplication.shared.delegate as? appdelegate)?.schedulebackgroundprocessing()
    // save changes in the application's managed object context when the application transitions to the background.
    (uiapplication.shared.delegate as? appdelegate)?.savecontext()
}

i also enabled background mode by adding the capabilities. but i'm still not getting the local notification when the app is in the background.","['swift', 'notifications', 'background-process', 'openai-api', 'ios16']",77100470,"you are calling the completionhandler that is supplied to the delegate method before you have finished your work.  this results in ios suspending your app, which will tear down the network connections.
you need to call the completion handler once fetchgptchatforresponse has done its work.  you can re-structure your code by moving the task into the delegate method and using defer to ensure that completionhandler is called once everything is done.
func usernotificationcenter(_ center: unusernotificationcenter, didreceive response: unnotificationresponse, withcompletionhandler completionhandler: @escaping () -> void) {
    task {
        defer {
             completionhandler()
        }
        if response.actionidentifier == ""replyaction"",
           let textresponse = response as? untextinputnotificationresponse {
            userquerytext = textresponse.usertext
            do {
                try await fetchgptchatforresponse(prompt:userquerytext)
            }
            catch {
               print(error)
            }
        }
    }
}

func fetchgptchatforresponse(prompt: string) async throws {
    let gpttext = try await apiservice().sendprompttogpt(prompt: prompt)
    await mainactor.run {
        sendinteractivenotification(message: gpttext)
    }
}",https://stackoverflow.com/questions/77066419,swift,08-09-2023 11:16,321.0,0.0,1.0,True,13-09-2023 21:07,08-09-2023 17:36
61731279,how to find abstractness of a word using hyper-/hyponyms in wordnet?,"i have 2 words, let's say computer and tool.
computer is a concrete noun whereas tool is relatively abstract.
i want to get level of abstractness of each word that will reflect this.
i thought the best way to do it is by counting number of hyper/hypo nyms for each word.

is it possible?
is there a better way to do it?","['python', 'nlp', 'nltk', 'wordnet']",61734477,"the first problem is which meaning of computer would you refer to?
in wordnet, a word has different ""concepts"", aka synsets:
>>> from nltk.corpus import wordnet as wn

>>> wn.synsets('computer')
[synset('computer.n.01'), synset('calculator.n.01')]

>>> wn.synsets('computer')[0].definition()
'a machine for performing calculations automatically'
>>> wn.synsets('computer')[1].definition()
'an expert at calculation (or at operating calculating machines)'

and hyper/hyponyms are not connected to the word computer
the hyper/hyponyms are concepts, i.e. synsets too, so it's not connected to the form/word but to the possible synsets that might be represented by the word computer, i.e. 
>>> type(wn.synsets('computer')[0])
<class 'nltk.corpus.reader.wordnet.synset'>

>>> wn.synsets('computer')[0].hypernyms()
[synset('machine.n.01')]

>>> wn.synsets('computer')[0].hyponyms()
[synset('analog_computer.n.01'), synset('digital_computer.n.01'), synset('home_computer.n.01'), synset('node.n.08'), synset('number_cruncher.n.02'), synset('pari-mutuel_machine.n.01'), synset('predictor.n.03'), synset('server.n.03'), synset('turing_machine.n.01'), synset('web_site.n.01')]

yes that's a lot of information but how do i get hyper/hyponyms for words?
according to the definition, should words have hyper/hyponyms? or should concept have hypo/hypernyms?
fine, you're bringing me in circles... just tell me how to use hyper-/hyponyms to see if a word is more abstract than another word!!!
okay, then we have to make some assumption. 

lets consider all synsets of a word accessed through the wordnet as a ""holistic"" concept of any word form
we consider the sum of all direct hyper-/hyponyms of all synsets of a given word
based on the number of hyper-/hyponyms of all synsets that can be represented by a certain word form, we deduce that word x is more/less abstract than word y

but how to do (1), (2) and (3) in the code?
>>> hypernym_count = lambda word: sum(len(ss.hypernyms()) for ss in wn.synsets(word)) 
>>> hyponym_count = lambda word: sum(len(ss.hyponyms()) for ss in wn.synsets(word)) 

>>> hyponym_count('computer')
14
>>> hypernym_count('computer')
2


>>> hypernym_count('tool')
8
>>> hyponym_count('tool')
32

since (3) is your hypothesis that you want to test, you should be the one deciding what heuristics to deduce if a word is more/less abstract based on the hyponym_count and hypernym_count results
wait a minute, what's direct hyper-/hyponyms?
we're only accessing the hyper-/hyponyms one level above/below the synset. that's what ""direct"" means here. 
then how to get all the hyponyms below a synset, see 
so should i use direct or all hyponyms below or all all hypernyms above?
that's for you to find out and tell us =) have fun!",https://stackoverflow.com/questions/61731279,python,11-05-2020 13:49,743.0,3.0,1.0,True,23-11-2022 12:47,23-11-2022 12:47
75415183,"how can i optimize knn, gnb nd svc sklearn algorithms to reduce exec time?","i'm currently evaluating which classifier have the best performance for movie reviews sentiment analysis task. so far i have evaluate logistic regression, linear regression, random forest and decision tree but i also want to consider knn, gnb and svc models as well. the problem is that each execution of those algorithms (particulary knn) has a large exec time. even using randomizedsearch in knn i have to wait about 1 hour with 10 iterations. here are some snippets:
knn classifier
 #knearestneighbors x -> large execution time
    knn=kneighborsclassifier()
    k_range=list(range(1,50))
    options=['uniform', 'distance']
    param_grid = dict(n_neighbors=k_range, weights=options)
    rand_knn = randomizedsearchcv(knn, param_grid, cv=10, scoring='accuracy', n_iter=10, random_state=0)
    rand_knn.fit(x_train_bow, y_train)
    print(rand_knn.best_score_)
    print(rand_knn.best_params_)
    confm_knn = confusion_matrix(y_test, y_pred_knn)
    print_confm(confm_knn)
    print(""=============k nearest neighbors============"")
    print_metrics(y_test,y_pred_knn)
    print(""============================================"")

i waited for the execution of the code above for about 85 minutes but it never finished and i had to cut the execution. in order to get any result (at least anything) i try to choose the best k manually with a for loop but still each iteration takes over 12 - 17 minutes.
def testing_k_neighbors(x_train_bow,y_train,x_test_bow,y_test):
    accuracy_hist = []
    for i in range (1,21):
        knn=kneighborsclassifier(n_neighbors=i)
        knn.fit(x_train_bow, y_train)
        yi_pred_knn = knn.predict(x_test_bow)
        acc_i = accuracy_score(y_test, yi_pred_knn)
        accuracy_hist.append(acc_i)
        print(f""k: {i}, accuracy: {acc_i}"")
    print(accuracy_hist)

output:
k: 1, accuracy: 0.7384384634613782
k: 2, accuracy: 0.7435213732188984
k: 3, accuracy: 0.7574368802599784
k: 4, accuracy: 0.7678526789434214
k: 5, accuracy: 0.7681859845012916
k: 6, accuracy: 0.7745187901008249
k: 7, accuracy: 0.7729355887009416
k: 8, accuracy: 0.7774352137321889
k: 9, accuracy: 0.7742688109324223
k: 10, accuracy: 0.7810182484792934
k: 11, accuracy: 0.7776851929005916
k: 12, accuracy: 0.7854345471210732
k: 13, accuracy: 0.783101408215982
k: 14, accuracy: 0.7866844429630864
k: 15, accuracy: 0.784934588784268
k: 16, accuracy: 0.78860094992084
k: 17, accuracy: 0.7873510540788268
k: 18, accuracy: 0.7893508874260479
k: 19, accuracy: 0.7856011999000083
k: 20, accuracy: 0.7916006999416715

also svc and gnb takes similar time to get any result:
    #support vector macine  x -> large execution time
    #svc=svc(c = 100, kernel = 'linear', random_state=123)
    #svc.fit(x_train_bow,y_train)
    #y_pred_svc = svc.predict(x_test_bow)
    #print(""=============support vector machine============"")
    #print_metrics(y_test,y_pred_svc)
    #print(""============================================"")   

    #gaussian naive bayes
    gnbc=gaussiannb()
    gnbc.fit(x_train_bow.toarray(),y_train)
    y_pred_gnbc = gnbc.predict(x_test_bow)
    print(""=============gaussian naive bayes============"")
    print_metrics(y_test,y_pred_gnbc)
    print(""============================================"")   

is there any way to tune my code reduce execution time and mantain or improve models performance?
im expecting to tune my code prioritzing both efficiency and performance","['python', 'machine-learning', 'nlp', 'sentiment-analysis', 'sklearn-pandas']",75422976,"i try your code:
then i print ""x_train_bow"":
<28000x122447 sparse matrix of type '<class 'numpy.float64'>'
    with 2796291 stored elements in compressed sparse row format>

you have 122447 columns then used tfidfvectorizer,
this is a problem of dimension, which is why it takes a lot of time.
there is no solution(knn, svc, trees). you need to reduce the dimension. you need to extract the corresponding words and then use tfidfvectorizer.",https://stackoverflow.com/questions/75415183,python,10-02-2023 19:06,118.0,0.0,1.0,True,16-02-2023 15:41,16-02-2023 15:41
15173225,calculate cosine similarity given 2 sentence strings,"from python: tf-idf-cosine: to find document similarity , it is possible to calculate document similarity using tf-idf cosine. without importing external libraries, are that any ways to calculate cosine similarity between 2 strings?
s1 = ""this is a foo bar sentence .""
s2 = ""this sentence is similar to a foo bar sentence .""
s3 = ""what is this string ? totally not related to the other two lines .""

cosine_sim(s1, s2) # should give high cosine similarity
cosine_sim(s1, s3) # shouldn't give high cosine similarity value
cosine_sim(s2, s3) # shouldn't give high cosine similarity value","['python', 'string', 'nlp', 'similarity', 'cosine-similarity']",15174569,"a simple pure-python implementation would be:
import math
import re
from collections import counter

word = re.compile(r""\w+"")


def get_cosine(vec1, vec2):
    intersection = set(vec1.keys()) & set(vec2.keys())
    numerator = sum([vec1[x] * vec2[x] for x in intersection])

    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])
    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])
    denominator = math.sqrt(sum1) * math.sqrt(sum2)

    if not denominator:
        return 0.0
    else:
        return float(numerator) / denominator


def text_to_vector(text):
    words = word.findall(text)
    return counter(words)


text1 = ""this is a foo bar sentence .""
text2 = ""this sentence is similar to a foo bar sentence .""

vector1 = text_to_vector(text1)
vector2 = text_to_vector(text2)

cosine = get_cosine(vector1, vector2)

print(""cosine:"", cosine)

prints:
cosine: 0.861640436855

the cosine formula used here is described here.
this does not include weighting of the words by tf-idf, but in order to use tf-idf, you need to have a reasonably large corpus from which to estimate tfidf weights.
you can also develop it further, by using a more sophisticated way to extract words from a piece of text, stem or lemmatise it, etc.",https://stackoverflow.com/questions/15173225,python,02-03-2013 10:06,156936.0,90.0,8.0,True,28-03-2022 04:32,12-12-2017 14:59
76812516,low f1 score and also low loss function score,"i am trying to build a multi label text classification model to classify toxic comments.
i followed a medium article from this link: multi-label text classification with bert using pytorch
i also used this data set from kaggle: jigsaw-toxic-comment-classification-challenge
im using google colab to run my model with a v100 gpu runtime settings.
unfortunately after several hours of training (4 epochs), i suffer from a f1 score of only 0.04214842148421484. my final loss score is 0.00354736
i know that the loss function and the f1 score are 2 different things but for my understanding a low cost function score should affect the f1 score. where did i go wrong?
here is the code:
import torch
import numpy as np
import pandas as pd
import shutil, sys
import transformers
from sklearn import metrics
from sklearn.model_selection import train_test_split
from torch.utils.data import dataset, dataloader, randomsampler, sequentialsampler
from transformers import berttokenizer, bertmodel, bertconfig

val_targets=[]
val_outputs=[]

class customdataset(dataset):

    def __init__(self, dataframe, tokenizer, max_len,):
        self.tokenizer = tokenizer
        self.data = dataframe
        self.title = dataframe['comment_text']
        self.targets = self.data.target_list
        self.max_len = max_len

    def __len__(self):
        return len(self.title)

    def __getitem__(self, index):
        title = str(self.title[index])
        title = "" "".join(title.split("" ""))

        inputs = self.tokenizer.encode_plus(
            title,
            none,
            add_special_tokens=true,
            max_length=self.max_len,
            padding='max_length',
            return_token_type_ids=true,
            truncation=true
        )
        ids = inputs['input_ids']
        mask = inputs['attention_mask']
        token_type_ids = inputs[""token_type_ids""]


        return {
            'ids': torch.tensor(ids, dtype=torch.long),
            'mask': torch.tensor(mask, dtype=torch.long),
            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
            'targets': torch.tensor(self.targets[index], dtype=torch.float)
        }
    

class bertclass(torch.nn.module):
    def __init__(self):
        super(bertclass, self).__init__()
        self.l1 = transformers.bertmodel.from_pretrained('bert-base-uncased', return_dict=false)
        self.l2 = torch.nn.dropout(0.3)
        self.l3 = torch.nn.linear(768, 6)

    def forward(self, ids, mask, token_type_ids):
        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)
        output_2 = self.l2(output_1)
        output = self.l3(output_2)
        return output
    
def loss_fn(outputs, targets):
    return torch.nn.bcewithlogitsloss()(outputs, targets)


def save_ckp(state, is_best, checkpoint_path, best_model_path):
    """"""
    state: checkpoint we want to save
    is_best: is this the best checkpoint; min validation loss
    checkpoint_path: path to save checkpoint
    best_model_path: path to save best model
    """"""
    f_path = checkpoint_path
    # save checkpoint data to the path given, checkpoint_path
    torch.save(state, f_path)
    # if it is a best model, min validation loss
    if is_best:
        best_fpath = best_model_path
        # copy that checkpoint file to best path given, best_model_path
        shutil.copyfile(f_path, best_fpath)

def load_ckp(checkpoint_fpath, model, optimizer):
    """"""
    checkpoint_path: path to save checkpoint
    model: model that we want to load checkpoint parameters into
    optimizer: optimizer we defined in previous training
    """"""
    # load checkpoint
    checkpoint = torch.load(checkpoint_fpath)

    # initialize state_dict from checkpoint to model
    model.load_state_dict(checkpoint['state_dict'])

    # initialize optimizer from checkpoint to optimizer
    optimizer.load_state_dict(checkpoint['optimizer'])

    # handle valid_loss_min based on its type
    valid_loss_min = checkpoint['valid_loss_min']
    if isinstance(valid_loss_min, torch.tensor):
        valid_loss_min = valid_loss_min.item()

    # return model, optimizer, epoch value, min validation loss
    return model, optimizer, checkpoint['epoch'], valid_loss_min


def train_model(start_epochs,  n_epochs, valid_loss_min_input,
                training_loader, validation_loader, model,
                optimizer, checkpoint_path, best_model_path):

  # initialize tracker for minimum validation loss
  valid_loss_min = valid_loss_min_input


  for epoch in range(start_epochs, n_epochs+1):
    train_loss = 0
    valid_loss = 0

    model.train()
    print('############# epoch {}: training start   #############'.format(epoch))
    for batch_idx, data in enumerate(training_loader):
        #print('yyy epoch', batch_idx)
        ids = data['ids'].to(device, dtype = torch.long)
        mask = data['mask'].to(device, dtype = torch.long)
        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
        targets = data['targets'].to(device, dtype = torch.float)

        optimizer.zero_grad()
        outputs = model(ids, mask, token_type_ids)
        print(outputs.shape)

        loss = loss_fn(outputs, targets)
        if batch_idx%100==0:
           print(f'epoch: {epoch}, training loss:  {loss.item()}')

        loss.backward()
        optimizer.step()
        #print('before loss data in training', loss.item(), train_loss)
        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))
        #print('after loss data in training', loss.item(), train_loss)

    print('############# epoch {}: training end     #############'.format(epoch))

    print('############# epoch {}: validation start   #############'.format(epoch))
    ######################
    # validate the model #
    ######################

    model.eval()


    outputs, targets = do_validation(validation_loader)
    val_preds = (np.array(outputs) > 0.5).astype(int)
    val_targets = (np.array(targets) > 0.5).astype(int)
    accuracy = metrics.accuracy_score(val_targets, val_preds)
    f1_score_micro = metrics.f1_score(val_targets, val_preds, average='micro')
    f1_score_macro = metrics.f1_score(val_targets, val_preds, average='macro')
    print(f""accuracy score = {accuracy}"")
    print(f""f1 score (micro) = {f1_score_micro}"")
    print(f""f1 score (macro) = {f1_score_macro}"")

          
    print('############# epoch {}: validation end     #############'.format(epoch))
    # calculate average losses
    #print('before cal avg train loss', train_loss)
    train_loss = train_loss/len(training_loader)
    valid_loss = valid_loss/len(validation_loader)
    # print training/validation statistics
    print('epoch: {} \tavgerage training loss: {:.6f} \taverage validation loss: {:.6f}'.format(
          epoch,
          train_loss,
          valid_loss
          ))

    # create checkpoint variable and add important data
    checkpoint = {
          'epoch': epoch + 1,
          'valid_loss_min': valid_loss,
          'state_dict': model.state_dict(),
          'optimizer': optimizer.state_dict()
    }

      # save checkpoint
    save_ckp(checkpoint, false, checkpoint_path, best_model_path)

    ## todo: save the model if validation loss has decreased
    if valid_loss <= valid_loss_min:
      print('validation loss decreased ({:.6f} --> {:.6f}).  saving model ...'.format(valid_loss_min,valid_loss))
      # save checkpoint as best model
      save_ckp(checkpoint, true, checkpoint_path, best_model_path)
      valid_loss_min = valid_loss

    print('############# epoch {}  done   #############\n'.format(epoch))


  return model


def do_validation(dataloader):
    model.eval()
    fin_targets=[]
    fin_outputs=[]
    with torch.no_grad():
        for _, data in enumerate(dataloader, 0):
            ids = data['ids'].to(device, dtype = torch.long)
            mask = data['mask'].to(device, dtype = torch.long)
            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
            targets = data['targets'].to(device, dtype = torch.float)
            outputs = model(ids, mask, token_type_ids)
            fin_targets.extend(targets.cpu().detach().numpy().tolist())
            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())
    return fin_outputs, fin_targets


if __name__ == '__main__':

    # if there's a gpu available...
    if torch.cuda.is_available():

        # tell pytorch to use the gpu.
        device = torch.device(""cuda"")

        print('there are %d gpu(s) available.' % torch.cuda.device_count())

        print('we will use the gpu:', torch.cuda.get_device_name(0))

    # if not...
    else:
        print('no gpu available, using the cpu instead.')
        device = torch.device(""cpu"")

    train_df =  pd.read_csv(train_data_location,on_bad_lines='skip')
    test_df = pd.read_csv(test_data_location,on_bad_lines='skip')
    select_labels = train_df.columns.values.tolist()[2:]
    train_df['target_list'] = train_df[select_labels].values.tolist()
    test_df['target_list'] = test_df[select_labels].values.tolist()
    max_len = 64
    train_batch_size = 8
    valid_batch_size = 8
    epochs = 10
    learning_rate = 1e-05
    tokenizer = berttokenizer.from_pretrained('bert-base-uncased')
    training_set = customdataset(train_df, tokenizer, max_len)
    validation_set = customdataset(test_df, tokenizer, max_len)
    train_params = {'batch_size': train_batch_size,
                    'shuffle': true,
                    'num_workers': 0
                    }

    test_params = {'batch_size': valid_batch_size,
                    'shuffle': false,
                    'num_workers': 0
                    }

    training_loader = dataloader(training_set, **train_params)
    validation_loader = dataloader(validation_set, **test_params)
    model = bertclass()
    model.to(device)
    optimizer = torch.optim.adam(params =  model.parameters(), lr=learning_rate)
    checkpoint_path = '/content/checkpoints/current_checkpoint.pt'
    best_model = '/content/checkpoints/best_model.pt'
    trained_model = train_model(1, epochs, np.inf, training_loader, validation_loader, model,
                        optimizer,checkpoint_path,best_model)","['machine-learning', 'pytorch', 'bert-language-model', 'text-classification']",76815007,"the f1 score is the harmonic mean of precision and recall. it allows the programmer to see precision and recall in a single number. loss scores are not directly correlated to other performance metrics.
for multi-label text classification, accuracy, precision, and recall are the important metrics. specifically, examine your overall accuracy score, and then the precision and recall scores per class. outside of research and commercial purposes, an f1 score is not particularly useful.
as for why you're getting a low f1 score in the first place, did you split your  dataset? i see you imported sklearn's train_test_split library but you never call it in your code. it seems like you're just passing the entire raw dataset to your training function.",https://stackoverflow.com/questions/76812516,machine-learning,01-08-2023 14:10,843.0,-1.0,2.0,True,08-08-2024 23:28,01-08-2023 20:18
76813435,how to remember messages from different chatcompletion instances,"i'm trying to run some commands with the chatgpt api and its not running everything so i made another chatcompletion instance. how can i get chatgpt to remember the prompts i sent in a previous instance? here is my code:
data = openai.chatcompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
        {""role"": ""user"", ""content"": ""generate a title""},
        {""role"": ""user"", ""content"": ""generate 30 chapters""},
    ]
)

data2 = openai.chatcompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
          {""role"": ""user"", ""content"": ""generate chapter 1""},
])","['openai-api', 'chatgpt-api']",76821607,"to be able to keep the model in context, you must send previous conversations along with the current prompt.
[note: conversation history must be less than token limit in order to expect proper behaviour from the model, to avoid this you can summarize the conversation after nth response or use text-embedding ]
here is a example code:
import openai
import time

# set your openai api key
openai.api_key = ""sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxx""

# define the engine and the query
engine = ""gpt-3.5-turbo""
query = ""generate a title""

# start a new conversation with an empty list of messages
messages = []

# call the chat completion api to generate a title
data = openai.chatcompletion.create(
  engine=engine,
  query=query,
  messages=messages
)

# print the generated title
title = data[""message""]
print(title)

# update the messages with the title
messages.append({""role"": ""assistant"", ""content"": title})

# define a function to generate chapters
def generate_chapters(n):
  # loop for n times
  for i in range(n):
    # generate a query for each chapter
    query = f""generate chapter {i+1}""
    # call the chat completion api to generate a chapter
    data = openai.chatcompletion.create(
      engine=engine,
      query=query,
      messages=messages
    )
    # print the generated chapter
    chapter = data[""message""]
    print(chapter)
    # update the messages with the chapter
    messages.append({""role"": ""assistant"", ""content"": chapter})
    # wait for 10 seconds before generating the next chapter
    time.sleep(10)

# generate 30 chapters
generate_chapters(30)",https://stackoverflow.com/questions/76813435,openai-api,01-08-2023 16:05,695.0,0.0,1.0,True,02-08-2023 15:54,01-08-2023 22:22
68907090,get the table by passing table header in pdf using python,"i have a pdf with multiple tables in it. i need to pass table header and get the respected table
for example:

i will pass the table name as ""daily historical stock prices & volumes"", then it must give above table.","['python', 'pdf', 'nlp', 'pypdf', 'pdfplumber']",73305585,i have use pypdf2 and its attribute to get the results,https://stackoverflow.com/questions/68907090,python,24-08-2021 11:52,488.0,0.0,1.0,True,10-08-2022 11:34,25-08-2021 03:22
72842842,how does spacy use the thinc parserstepmodel object in the pipeline,"i'm trying to modify the softmax output from spacy, but i'm not understanding how spacy uses the thinc predict function.
i had assumed that each time a thinc model predict function is called as part of the spacy pipeline it would return data in the same format. however, when i place a break point at 'preds' in the code below i can see that the data returned from self._func returns data in two formats:

a list of numpy arrays - an array
i believe contains the softmax score for each models classification
prediction.
a space.ml.parser_model.parserstepmodel object. i'm not
sure how or why the model is returning data in this format.

i was hoping someone could explain why the thinc model is returning a parserstepmodel object and how its used as part of the spacy pipeline. also if anyone knows how i can detect what the 'preds' data type is (i've unsuccessfully tried isinstance).
import spacy
from thinc.model import model, int, outt
import numpy as np

def predict(self, x:int) -> outt:

    preds = self._func(self, x, is_train=false)[0]

    return preds

model.predict = predict

nlp = spacy.load('en_core_web_sm')

def show_ents(doc):
    if doc.ents:
        for ent in doc.ents:
            print(ent.text + ' - ' + str(ent.start_char) + ' - ' + str(ent.end) + ' - ' +
                  ent.label_ + ' - ' + str(spacy.explain(ent.label_)))
    else:
        print('no named entities found.')

doc = nlp('apple is looking at buying u.k. startup for $1 billion')

show_ents(doc)","['python', 'spacy', 'spacy-3']",72843825,"this is happening because there are two models in the spacy pipeline you have. first the tok2vec runs and creates embeddings of each token, then those are used as features for the parser. see the pipeline docs.
if you have trouble finding the type of anything it's probably a cython type, and you'd need to check the cython source in spacy or thinc. i'm not sure what a ""press"" is, how are you getting it? (make a new question for that)",https://stackoverflow.com/questions/72842842,python,02-07-2022 22:34,78.0,1.0,1.0,True,03-07-2022 18:30,03-07-2022 18:30
79236682,do those `[0]` make sense in making the variable,"the guide for fine-tuning gemma with huggingface toolset is at: 
link to the line: 
the data entry formatter func is:
def formatting_func(example):
    text = f""quote: {example['quote'][0]}\nauthor: {example['author'][0]}<eos>""
    return [text]

do those [0] make sense? they look wrong coz when printing out text variable i can see they are just characters instead of strings.","['machine-learning', 'huggingface-transformers', 'dataformat', 'mlmodel', 'gemma']",79246911,"yes, variable example is a slice of dataset (a batch), not a one item from the dataset. in your particular case you have batch_size set to 1:
per_device_train_batch_size=1
it means that entire dataset is splitted into batches of size 1, i.e. arrays of size 1. so one batch is of the following representation:
[{""quote"": [""quote 1""], ""author"": [""author 1""]}]

so in order to get the values you use 0 as index.
normally if you use batch size larger than or equal to 2 2, the representation would like:
[{""quote"": [""quote 1"", ""quote 2"", ...], ""author"": [""author 1"", ""author 2"", ...]}]

so in line with documentation you would like to use other formatting_func, where you iterate over both arrays like in this case:
def formatting_prompts_func(example):
    output_texts = []
    for i in range(len(example['instruction'])):
        text = f""### question: {example['instruction'][i]}\n ### answer: {example['output'][i]}""
        output_texts.append(text)
    return output_texts

source:",https://stackoverflow.com/questions/79236682,machine-learning,29-11-2024 10:14,37.0,0.0,1.0,True,03-12-2024 09:04,29-11-2024 10:27
66396100,google colab glove_python pip install not working,"i am using
! pip install glove_python

i'm getting this error message:
collecting glove_python
  downloading  (263kb)
     |ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½| 266kb 16.9mb/s 
requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python) (1.19.5)
requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python) (1.4.1)
building wheels for collected packages: glove-python
  building wheel for glove-python (setup.py) ... error
  **error: failed building wheel for glove-python**
  running setup.py clean for glove-pytrror: command errored out with exit status 1**: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-nypxp28t/glove-python/setup.py'""'""'; __file__='""'""'/tmp/pip-install-nypxp28t/glove-python/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-cnn32mbr/install-record.txt --single-version-externally-managed --compile check the logs for full command output.

as suggested below i tried
! python -m pip install glove_python --verbose

which outputs the following recurring error with different members:
has no member named ï¿½ï¿½ï¿½exc_{member}ï¿½ï¿½ï¿½; did you mean ï¿½ï¿½ï¿½curexc_valueï¿½ï¿½ï¿½?

and ends with:
inux-gnu-gcc' failed with exit status 1
    running setup.py install for glove-python ... error
cleaning up...
  removing source in /tmp/pip-install-ru3hxbde/glove-python
removed build tracker '/tmp/pip-req-tracker-ps3qzi71'
error: command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-ru3hxbde/glove-python/setup.py'""'""'; __file__='""'""'/tmp/pip-install-ru3hxbde/glove-python/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-ywzvlm5m/install-record.txt --single-version-externally-managed --compile check the logs for full command output.
exception information:
traceback (most recent call last):
  file ""/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py"", line 153, in _main
    status = self.run(options, args)
  file ""/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py"", line 455, in run
    use_user_site=options.use_user_site,
  file ""/usr/local/lib/python3.7/dist-packages/pip/_internal/req/__init__.py"", line 62, in install_given_reqs
    **kwargs
  file ""/usr/local/lib/python3.7/dist-packages/pip/_internal/req/req_install.py"", line 888, in install
    cwd=self.unpacked_source_directory,
  file ""/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/subprocess.py"", line 275, in runner
    spinner=spinner,
  file ""/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/subprocess.py"", line 242, in call_subprocess
    raise installationerror(exc_msg)
pip._internal.exceptions.installationerror: command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-ru3hxbde/glove-python/setup.py'""'""'; __file__='""'""'/tmp/pip-install-ru3hxbde/glove-python/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-ywzvlm5m/install-record.txt --single-version-externally-managed --compile check the logs for full command output.

trying pip install glove-python-binary is successful but when i import it i get the following error:
    import glove-python-binary
                ^
syntaxerror: invalid syntax","['python', 'pip', 'google-colaboratory', 'stanford-nlp']",66397661,"seems like glove_python package is very old, last relese of it on pip was in 2016. and it has only sources there, so should be compiled by c/c++ compiler, which is usually problematic everywhere (needs manual installing correct compiler and all dependencies).
looks like updated version is glove-python-binary it dates to 2020. try installing it through ! pip install glove-python-binary.
according to pypi site glove-python-binary needs python of versions 3.6 or 3.7 or 3.8, smaller or bigger version of python will not probably work.
when you have any problem or error in pip try adding --verbose to pip command line, this will print you a lot of details about reasons of pip failure.
also as @gamedev said sometimes you have to also try running command ! python -m pip install package instead of ! pip install package, to install any python pip package, because first command uses exactly python that you use to run the script, while second may use pip from other python installation, not from python installation that is used to run actual script later.",https://stackoverflow.com/questions/66396100,python,27-02-2021 06:11,9140.0,9.0,3.0,True,06-04-2024 00:21,22-07-2021 21:00
73505361,how to enlarge custom question answering editor window in language studio,"the existing qna maker editor will be always better than the current custom question answering in the azure language studio. we have added 600+ qna question pairs in custom question answering but the editor is very small and we can able to see only one question pair at a time, remeaning we need to scroll down.
this will be a hectic problem for us to manage all the content in the custom question answering and is there any way to enlarge/maximize this editor in full size window?","['azure', 'azure-language-understanding', 'azure-qna-maker', 'nlp-question-answering', 'language-studio']",75891629,the new editor window is pretty good and was updated a couple of days back.,https://stackoverflow.com/questions/73505361,azure,26-08-2022 18:49,175.0,0.0,1.0,True,30-03-2023 18:12,21-01-2023 20:13
71420789,label any text with multiple topics in sequence of their occurrence,"i have a dataframe with an id and text like below:
df1




id
text




1
i have completed my order


2
i have made the payment. when can i expect the order to be delivered?


3
i am unable to make the payment.


4
i am done with registration and payment. i need the order number?


5
i am unable to complete registration. how will i even order?




i have certain topics to classify these texts:
class = [""order"", ""payment"", ""registration""]
i am doing the following which gets me the results:
classes = [""order"", ""payment"", ""registration""]
for c in classes:
    word_counter = counter()
    list_df = []
    field = ""text""
    df2 = pd.dataframe()
    df2 = df2[df2[field].str.contains(c)] 
    print(c)
    list_df.append(df2)
    final_df = pd.concat(list_df)
    final_df.to_csv(""./"" + c + "".csv"")    

this will generate me 3 csv files which i will later join again:
file_list = []
os.chdir('<file path>')

for file in os.listdir():
    if file.endswith('.csv'):
        df = pd.read_csv(file, sep="","", encoding='iso-8859-1')
        df['filename'] = file
        file_list.append(df)

df_topic = pd.concat(file_list, ignore_index=true)
df_topic['topic'] = df_topic['filename'].str.split('.').str[0]
df_topic= df_topic.drop('filename', 1)

the resultant dataframe looks like this:




id
text
topic




1
i have completed my order
order


2
i have made the payment. when can i expect the order to be delivered?
order


4
i am done with registration and payment. i need the order number?
order


2
i have made the payment. when can i expect the order to be delivered?
payment


3
i am unable to make the payment.
payment


4
i am done with registration and payment. i need the order number?
payment


4
i am done with registration and payment. i need the order number?
registration


5
i am unable to complete registration. how will i even order?
registration




but, the problem you see here is that same text may have the keywords for the other classes too and can be tagged as either (like text for id=2 has both order and payment). i can only have one record label for each id and thus would prefer to have it as primary or secondary topic based on the sequence of their occurrence from the beginning of the text. if a text has more than 2 then first 2 gets preference but just to ensure we may need the third topic (or nth topic) for a future instance i would like to store it as a list in the final field. (example for id = 4 is illustrated)




id
text
primary topic
secondary topic
identified topics
topics list




1
i have completed my order
order
null
1
[order]


2
i have made the payment. when can i expect the order to be delivered?
payment
order
2
[payment,order]


3
i am unable to make the payment.
payment
null
1
[payment]


4
i am done with registration and payment. i need the order number?
registration
payment
3
[registration,payment,order]


5
i am unable to complete registeration. how will i even order?
registration
order
2
[registration,order]




is it possible to do it this way. if not, what is a good way to approach such labelling issues?","['python', 'pandas', 'dataframe', 'data-manipulation', 'text-classification']",71420918,"iiuc, you could use str.extractall combined with groupby.agg:
lst = [""order"", ""payment"", ""registration""]
regex = f'({""|"".join(lst)})'  # if lst contains special chars, wrap in re.escape
df2 = df.join(df['text']
              .str.extractall(regex)[0]
              .groupby(level=0).agg(**{'primary topic': 'first',
                                       'secondary topic': lambda x: x.iloc[1] if len(x)>1 else 'null',
                                       'identified topics': 'nunique',
                                       'topics list': list})
               )

output:
   id                                                                   text primary topic secondary topic  identified topics                     topics list
0   1                                              i have completed my order         order            null                  1                         [order]
1   2  i have made the payment. when can i expect the order to be delivered?       payment           order                  2                [payment, order]
2   3                                       i am unable to make the payment.       payment            null                  1                       [payment]
3   4      i am done with registration and payment. i need the order number?  registration         payment                  3  [registration, payment, order]
4   5           i am unable to complete registration. how will i even order\  registration           order                  2           [registration, order]",https://stackoverflow.com/questions/71420789,python,10-03-2022 08:16,200.0,1.0,1.0,True,10-03-2022 08:46,10-03-2022 08:46
73963008,problem completing bert model for sentiment classification,"i am trying to figure out sentiment classification on movie reviews using bert, transformers and tensorflow. this is the code i currently have:
def read_dataset(filename, model_name=""bert-base-uncased""):
    """"""reads a dataset from the specified path and returns sentences and labels""""""

    tokenizer = berttokenizer.from_pretrained(model_name)
    with open(filename, ""r"", encoding=""utf-8"") as f:
        lines = f.readlines()
        # preallocate memory for the data
        sents, labels = list(), np.empty((len(lines), 1), dtype=int)

        for i, line in enumerate(lines):
            text, str_label, _ = line.split(""\t"")
            labels[i] = int(str_label.split(""="")[1] == ""pos"")
            sents.append(text)
    return dict(tokenizer(sents, padding=true, truncation=true, return_tensors=""tf"")), labels


class bertmlp(tf.keras.model):
    def __init__(self, embed_batch_size=100, model_name=""bert-base-cased""):
        super(bertmlp, self).__init__()
        self.bs = embed_batch_size
        self.model = tfbertmodel.from_pretrained(model_name)
        self.classification_head = tf.keras.models.sequential(
            layers = [
                tf.keras.input(shape=(self.model.config.hidden_size,)),
                tf.keras.layers.dense(350, activation=""tanh""),
                tf.keras.layers.dense(200, activation=""tanh""),
                tf.keras.layers.dense(50, activation=""tanh""),
                tf.keras.layers.dense(1, activation=""sigmoid"", use_bias=false)
            ]
        )

    def call(self, inputs):
        outputs = self.model(inputs)
        return outputs

def evaluate(model, inputs, labels, loss_func):
    mean_loss = tf.keras.metrics.mean(name=""train_loss"")
    accuracy = tf.keras.metrics.binaryaccuracy(name=""train_accuracy"")

    predictions = model(inputs)
    mean_loss(loss_func(labels, predictions))
    accuracy(labels, predictions)

    return mean_loss.result(), accuracy.result() * 100


if __name__ == ""__main__"":
    train = read_dataset(""datasets/rt-polarity.train.vecs"")
    dev = read_dataset(""datasets/rt-polarity.dev.vecs"")
    test = read_dataset(""datasets/rt-polarity.test.vecs"")

    mlp = bertmlp()
    mlp.compile(tf.keras.optimizers.sgd(learning_rate=0.01), loss='mse')
    dev_loss, dev_acc = evaluate(mlp, *dev, tf.keras.losses.meansquarederror())
    print(""before training:"", f""dev loss: {dev_loss}, dev acc: {dev_acc}"")
    mlp.fit(*train, epochs=10, batch_size=10)
    dev_loss, dev_acc = evaluate(mlp, *dev, tf.keras.losses.meansquarederror())
    print(""after training:"", f""dev loss: {dev_loss}, dev acc: {dev_acc}"")

however, when i run this code, i get an error:
traceback (most recent call last):

  file ""c:\users\home\anaconda3\lib\site-packages\spyder_kernels\py3compat.py"", line 356, in compat_exec
    exec(code, globals, locals)

  file ""c:\users\home\downloads\mlp.py"", line 60, in <module>
    dev_loss, dev_acc = evaluate(mlp, *dev, tf.keras.losses.meansquarederror())

  file ""c:\users\home\downloads\mlp.py"", line 46, in evaluate
    predictions = model(inputs)

  file ""c:\users\home\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from none

  file ""c:\users\home\downloads\mlp.py"", line 39, in call
    outputs = self.model(inputs)

  file ""c:\users\home\anaconda3\lib\site-packages\transformers\modeling_tf_utils.py"", line 409, in run_call_with_unpacked_inputs
    return func(self, **unpacked_inputs)

  file ""c:\users\home\anaconda3\lib\site-packages\transformers\models\bert\modeling_tf_bert.py"", line 1108, in call
    outputs = self.bert(

  file ""c:\users\home\anaconda3\lib\site-packages\transformers\modeling_tf_utils.py"", line 409, in run_call_with_unpacked_inputs
    return func(self, **unpacked_inputs)

  file ""c:\users\home\anaconda3\lib\site-packages\transformers\models\bert\modeling_tf_bert.py"", line 781, in call
    embedding_output = self.embeddings(

  file ""c:\users\home\anaconda3\lib\site-packages\transformers\models\bert\modeling_tf_bert.py"", line 203, in call
    inputs_embeds = tf.gather(params=self.weight, indices=input_ids)

invalidargumenterror: exception encountered when calling layer ""embeddings"" (type tfbertembeddings).

indices[1174,8] = 29550 is not in [0, 28996) [op:resourcegather]

call arguments received:
  ï¿½ï¿½ï¿½ input_ids=tf.tensor(shape=(1599, 73), dtype=int32)
  ï¿½ï¿½ï¿½ position_ids=none
  ï¿½ï¿½ï¿½ token_type_ids=tf.tensor(shape=(1599, 73), dtype=int32)
  ï¿½ï¿½ï¿½ inputs_embeds=none
  ï¿½ï¿½ï¿½ past_key_values_length=0
  ï¿½ï¿½ï¿½ training=false

i googled for a while, and i can't find anything conclusive. i am pretty sure it has something to do wil(self, inputs):
        outputs = self.model(inputs)
        return outputs

but again, i have tried a lot of different things, including limiting dataset size and installing different versions of transformers and tensorflow, but to no avail. please let me know what i'm doing wrong. thank you!","['python', 'tensorflow', 'keras', 'sentiment-analysis', 'bert-language-model']",73963784,"op was using bert-base-cased for their model, and bert-base-uncased for their tokenizer, causing issues during training when the vocab size of the model and the tokenized data differed.",https://stackoverflow.com/questions/73963008,python,05-10-2022 15:46,567.0,0.0,1.0,True,05-10-2022 16:53,05-10-2022 16:26
2325588,text similarity algorithm,"i have two subtitles files.
i need a function that tells whether they represent the same text, or the similar text
sometimes there are comments like ""the wind is blowing... the music is playing"" in one file only.
but 80% percent of the contents will be the same. the function must return true (files represent the same text).
and sometimes there are misspellings like 1 instead of l (one - l ) as here:
she 1eft the baggage.
of course, it means function must return true.
my comments: 
the function should return percentage of the similarity of texts  - agree
""all the people were happy"" and ""all the people were not happy"" - here that'd be considered as a misspelling, so that'd be considered the same text. to be exact, the percentage the function returns will be lower, but high enough to say the phrases are similar
do consider whether you want to apply levenshtein on a whole file or just a search string - not sure about levenshtein, but the algorithm must be applied to the file as a whole. it'll be a very long string, though.","java, text, nlp, levenshtein-distance, similarity",2325629,"levenshtein algorithm: 
anything other than a result of zero means the text are not ""identical"". ""similar"" is a measure of how far/near they are. result is an integer.",https://stackoverflow.com/q/2325588,"java, text, nlp, levenshtein-distance, similarity",24-02-2010 11:34,17973.0,14.0,6.0,True,23-03-2022 02:04,06-11-2011 15:15
79234590,why is there a big performance difference between those 2 simple python multithreading codes?,"let's consider this python code:
def process_payload(payload, url, headers):
    response = requests.post(url, headers=headers, json=payload)
    return response

def parallel_group2(payloads, url, headers):
    with threadpoolexecutor() as executor:
        results = executor.map(process_payload,payloads, [url]*len(payloads), [headers]*len(payloads))
    return list(results)

def parallel_group(payloads, url, headers):
    with threadpoolexecutor() as executor:
        results = executor.map(requests.post, [url]*len(payloads), [headers]*len(payloads), payloads)
    return list(results)

times = []
# payloads grouped by 15
payloads_grouped = [payloads[i:i+15] for i in range(0, len(payloads), 15)]
print( ""shape of payloads_grouped"", len(payloads_grouped), "" x "", len(payloads_grouped[0]))
for i in range(3):
    start = time.time()
    with threadpoolexecutor() as executor:
        # results = executor.map(parallel_group2, payloads_grouped, [url]*len(payloads_grouped), [headers]*len(payloads_grouped))
        results = executor.map(parallel_group, payloads_grouped, [url]*len(payloads_grouped), [headers]*len(payloads_grouped))
    end = time.time()
    times.append(end-start)
    print( ""durations of iterations:"", times)
print( ""durations of iterations:"", times)
print( ""average time for 150 requests:"", sum(times)/len(times))

when i run the script with parallel_group, i have those results very consistently:
durations of iterations: [5.246389389038086, 5.195073127746582, 5.278628587722778]
average time for 150 requests: 5.2400303681691485

when i run it with parallel_group2 i have results looking more like this:
durations of iterations: [10.99542498588562, 9.43007493019104, 23.003321170806885]
average time for 150 requests: 10.142940362294516

does someone have good knowledge in python multithreading and could explain why there is such a difference between multithreading calls to request.post and calls to a function that just do requests.call? i don't understand at all.
i ran the previous code several times and results were consistent.
edit :
the url is the chat completion api of openai =""api.openai.com/v1/chat/completions""","['python', 'multithreading', 'asynchronous', 'openai-api']",79234682,"your function parallel_group isn't doing what you would hope. the reason is that of the 3 parameters you're passing to requests.post, only the first one is correct (the url). the payload will be assigned as data and the headers will be assigned to json the api is most likely to return an error but you're ignoring that possibility",https://stackoverflow.com/questions/79234590,python,28-11-2024 15:59,35.0,0.0,1.0,True,28-11-2024 16:28,28-11-2024 16:17
66279882,how does masking work in the scaled_dot_product_attention of language understanding transformers?,"i have been following tensorflow's tutorial on transformers for language understanding. (here). however i'm a bit confused about masks used in the function scaled_dot_product_attention. i know what are masks used for but i do know understand how they work in this function for example.
when i followed the tutorial i understood that the mask will have a matrix indicating which elements are padding elements ( value 1 in the masking matrix) and which are not ( value 0 in the masking matrix).
for example :
[0 , 0 , 1 
 1 , 0 , 0 
 0 , 1 , 0 ]

however i can see that the function  scaled_dot_product_attention tries to update the padded elements with a very large ( or small ) number which is -1e9 ( negative 1 billion ). this can be seen in the below line of the mentioned function :
      if mask is not none:
    scaled_attention_logits += (mask * -1e9)

why is this done ? and how does this mathematically leads to ignoring these values ? . below is the implementation shown in the tutorial :
   def scaled_dot_product_attention(q, k, v, mask):
  """"""calculate the attention weights.
  q, k, v must have matching leading dimensions.
  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.
  the mask has different shapes depending on its type(padding or look ahead) 
  but it must be broadcastable for addition.

  args:
    q: query shape == (..., seq_len_q, depth)
    k: key shape == (..., seq_len_k, depth)
    v: value shape == (..., seq_len_v, depth_v)
    mask: float tensor with shape broadcastable 
          to (..., seq_len_q, seq_len_k). defaults to none.

  returns:
    output, attention_weights
  """"""

  matmul_qk = tf.matmul(q, k, transpose_b=true)  # (..., seq_len_q, seq_len_k)

  # scale matmul_qk
  dk = tf.cast(tf.shape(k)[-1], tf.float32)
  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)

  # add the mask to the scaled tensor.
  if mask is not none:
    scaled_attention_logits += (mask * -1e9)  

  # softmax is normalized on the last axis (seq_len_k) so that the scores
  # add up to 1.
  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)

  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)

  return output, attention_weights","['tensorflow', 'deep-learning', 'neural-network', 'nlp']",67154510,"ok , so the value -1e9 resembles negative infinity. therefor the softmax function will produce a probability of 0 to such elements and will be ignored when calculating the attention values.",https://stackoverflow.com/questions/66279882,tensorflow,19-02-2021 14:57,1172.0,1.0,1.0,True,11-12-2023 17:56,20-02-2021 13:27
23506732,compare two phrases using wordnet?,"i am trying to compare the semantic of two phrases.
in python i am using nltk and difflib.
first i am removing the stop words from the phrases, then i am using wordnetlemmatizer and porterstemmer to normalise the words then i am comparing the rest with the sequencematcher of difflib.
i still think that there is a much better way than using difflib.
any suggestion or propostion?
is there any library that use wordnet in the comparision between phrases?
is the steps i am making are correct?","['nlp', 'nltk', 'wordnet', 'difflib', 'semantic-analysis']",23527690,"in short, no, you cannot do this sort of semantics with nltk. and using wordnet will simply not work because most sentences contain words that are not in the database. the current way to approximate sentential semantics involves distributional techniques (word space models).
if you are a python programmer, scikit-learn and gensim give you the functionality you want by means of latent semantic analysis (lsa, lsi) and latent dirichlet allocation (lda). see the answers to this previous question. in java, i would suggest you to try the excellent s-space package.
however, most models will give you a strictly word-based representation. combining the semantics of words into larger structures is much more difficult, unless you assume that phrases and sentences are bags-of-words (and thus, missing the difference between e.g. mary loves kate and kate loves mary.",https://stackoverflow.com/questions/23506732,nlp,07-05-2014 00:27,1493.0,0.0,1.0,True,07-03-2025 13:35,07-03-2025 13:35
76645989,how are llms assigned tasks in python code?,"i'm following nicholas' renotte's tutorials on vscode, langchain, and openai using python.
these are the codeblocks i've seen from the aforementioned tutorials, and i don't see any other lines of code that tell the ai what to do.
title_template = prompttemplate(
    input_variables = ['topic'],
    template='write me a youtube video title about {topic}'
)

so is ai given a task using this kind of conversational language? is that really all that's needed, and wouldn't this place a lot of pressure on precise wording of the assignment?","['machine-learning', 'nlp', 'artificial-intelligence', 'large-language-model']",76646320,"you're right. the template (write me a youtube video title about {topic}) is called a prompt template, and the only difference between such a template and an actual prompt (like the ones you give to chatgpt for instance) is that prompt templates are formatted strings: {topic} gets substituted by something else. it is true that the wording of the assignment is important - writing good prompts has become something of a domain in itself, called 'prompt engineering' ( if you want to learn more about generative ai and large language models, i would recommend taking a look at cohere's free llm university:",https://stackoverflow.com/questions/76645989,machine-learning,09-07-2023 05:22,128.0,0.0,1.0,True,09-07-2023 19:39,09-07-2023 19:39
43163959,text classification. tfidf and naive bayes?,"i'm attempting a text classification task, where i have training data of around 500 restaurant reviews that are labelled across 12 categories. i spent longer than i should have implementing tf.idf and cosine similarity for the classification of test data, only to get some very poor results (0.4 f-measure). with time not on my side now, i need to implement something significantly more effective that doesn't have a steep learning curve. i am considering using the tf.idf values in conjunction with naive bayes. does this sound sensible? i know if i can get my data in the right format, i can do this with scikit learn. is there anything else you recommend i consider?","['python', 'machine-learning', 'scikit-learn', 'nlp']",43164197,"you should try to use fasttext:  . it can be used to classify text like this:
(don't forget to download a pretrained model here  by changing the language if it's not english)
import fasttext

model = fasttext.load_model('wiki.en.bin')  # the name of the pretrained model

classifier = fasttext.supervised('train.txt', 'model', label_prefix='__label__')

result = classifier.test('test.txt')
print ('p@1:', result.precision)
print ('r@1:', result.recall)
print ('number of examples:', result.nexamples)

every line in your training and test sets should be like this:

__label__classname your restaurant review blah blah blah",https://stackoverflow.com/questions/43163959,python,02-04-2017 02:21,1432.0,-1.0,1.0,True,14-05-2024 14:49,14-05-2024 14:49
76882664,why do we add |v| in the denominator in the add-one smoothing for n-gram language models?,"in nlp when we use laplace(add-one) smoothing technique we assume that the every word is seen one more time than the actual count and the formula is like this

where v is the size of the vocabulary. my question is why do we add v when we are only considering the count of the previous word.
i only have a rough idea that every word is incremented by one so we have to normalize it by v time but i still don't understand it properly. as i said we are only considering the count of previous word right so why don't just add 1 to it.
i also saw that if we add v then the addition of all bigrams will be 1 which is what it should be. but is there any other explanation of why v?","['nlp', 'smoothing', 'language-model']",76889108,"the |v| variable that we see in the determiner of additive smoothing function is not actually a direct definition of the probabilisitic estimation of the n-gram. it is derived from:

first, we start with the naive assumption that if we add 1 to the numerator, we also add 1 to denominator to avoid math division error.
but instead of adding +1 to all terms in the vocabulary, we could simply add the size of the vocab, thus you see the sum(c(wi-1)) + |v| in the denominator, instead of sum(c(wi-1) + 1), note the scope of the ""sum"" function.

more details
sometimes i find it easier to see the math in code, consider this ngram without laplace:
# from 

try:
    from itertools import pairwise
except:
    from itertools import tee
    def pairwise(iterable):
        # pairwise('abcdefg') --> ab bc cd de ef fg
        a, b = tee(iterable)
        next(b, none)
        return zip(a, b)

    
def count_numerator(ngrams, sentences):
    return sum([list(pairwise(s)).count(ngrams) for s in sentences])

def count_denominator(word, sentences):
    return sum([sum([int(ng[0] == word) for ng in pairwise(s)]) for s in sentences])


s1 = [""<s>"", ""this"", ""is"", ""foo"", ""bar"", '</s>']
s2 = [""<s>"", ""foo"", ""bar"", ""is"", ""good"", '</s>']
s3 = [""<s>"", ""fruit"", ""bar"", ""is"", ""better"", '</s>']
s4 = [""<s>"", ""this"", ""is"", ""good"", ""</s>""]
s5 = [""<s>"", ""i"", ""like"", ""this"", ""thing"", '</s>']

sentences = [s1, s2, s3, s4, s5]


prob_bos_this = count_numerator(('<s>', 'this'), sentences) / count_denominator(""<s>"", sentences) 

prob_this_is = count_numerator(('this', 'is'), sentences) / count_denominator(""this"", sentences)

prob_is_good = count_numerator(('is', 'good'), sentences) / count_denominator(""is"", sentences)

prob_good_eos = count_numerator(('good', '</s>'), sentences) / count_denominator(""good"", sentences)

print_this_is_good = prob_bos_this * prob_this_is * prob_is_good * prob_good_eos

print_this_is_good # outputs: 0.1333

now consider the laplace smoothing with the incremental +1 on the numerator and denominator:
def count_numerator_laplace(ngrams, sentences):
    return sum([list(pairwise(s)).count(ngrams) + 1  for s in sentences])

def count_denominator_laplace(word, sentences):
    return sum([sum([int(ng[0] == word) + 1 for ng in pairwise(s)])  for s in sentences])


prob_bos_this = count_numerator_laplace(('<s>', 'this'), sentences) / count_denominator_laplace(""<s>"", sentences) 

prob_this_is = count_numerator_laplace(('this', 'is'), sentences) / count_denominator_laplace(""this"", sentences)

prob_is_good = count_numerator_laplace(('is', 'good'), sentences) / count_denominator_laplace(""is"", sentences)

prob_good_eos = count_numerator_laplace(('good', '</s>'), sentences) / count_denominator_laplace(""good"", sentences)

print_this_is_good = prob_bos_this * prob_this_is * prob_is_good * prob_good_eos
print_this_is_good  # 0.004212103350034384

note that the +1 on the numerator is adding simple +1 to each wi-1, wi count. the denominator is adding +1 for each ngram that exists in the corpus containing the wi-1, *.

now, we see that we are summing +1 for every ngrams that occurs in a similar fashion, we can just add the no. of ngrams that exists, e.g.
def count_numerator_laplace(ngrams, sentences):
    return sum([list(pairwise(s)).count(ngrams) + 1  for s in sentences])

def count_denominator_laplace(word, sentences):
    return sum([sum([int(ng[0] == word) for ng in pairwise(s)]) + len(list(pairwise(s))) for s in sentences])


prob_bos_this = count_numerator_laplace(('<s>', 'this'), sentences) / count_denominator_laplace(""<s>"", sentences) 

prob_this_is = count_numerator_laplace(('this', 'is'), sentences) / count_denominator_laplace(""this"", sentences)

prob_is_good = count_numerator_laplace(('is', 'good'), sentences) / count_denominator_laplace(""is"", sentences)

prob_good_eos = count_numerator_laplace(('good', '</s>'), sentences) / count_denominator_laplace(""good"", sentences)

print_this_is_good = prob_bos_this * prob_this_is * prob_is_good * prob_good_eos
print_this_is_good  # 0.004212103350034384


this is a good prove of concept for how the +1 in the inner sum becomes a + |v| in when you carry the +1 out of the summation, e.g.
my_list = [1,2,3]
sum(i+1 for i in my_list) == sum(i for i in my_list) + len(my_list)",https://stackoverflow.com/questions/76882664,nlp,11-08-2023 10:21,731.0,1.0,1.0,True,14-08-2023 02:15,13-08-2023 04:18
75738343,how can i use &quot;pip install&quot; when using pyscript?,"i am trying to code an html file that is able to contact with chat gpt.
this is the start of the code:
<link rel=""stylesheet"" href="" />
<script defer src=""

<py-script>
import openai
</py-script>

but when i run it it says:
modulenotfounderror: no module named 'openai'

any idea how i can install it just on the html file?
i tried to find out how to use:
pip install openai ,
but i dont know how to run bash commands from the python.
i also tried to use <py-env>, but it had no effect on the code whatsoever.
+ it could be blocked by my proxy, + i'm a beginner to python, so please explain it in simple terms.","['python', 'html', 'module', 'openai-api', 'pyscript']",75738539,"according to the getting started documentation, you import external modules using a <py-config> tag:
<py-config>
    packages = [""openai""]
</py-config>

however, even with that, this code:
<html>
<head>
    <link rel=""stylesheet"" href="" />
    <script defer src=""
</head>
<body>
    <py-config>
        packages = [""openai""]
    </py-config>
    <py-script>
        import openai
    </py-script>
</body>
</html>

gives the following error message:

(py1001): unable to install package(s) 'openai'. reason: can't find a pure python 3 wheel for package(s) 'openai'. see:  for more information.

at the pyodide link above is the following:

why canï¿½ï¿½ï¿½t i just use urllib or requests?


we currently canï¿½ï¿½ï¿½t use such packages since sockets are not available in pyodide. "" rel=""nofollow noreferrer"">write  in terms of web apis for more information.

openai requires requests, so until sockets are implemented in pyodide (which is used by pyscript), you won't be able to use the openai module, or many others, in pyscript.
however, it is possible to make an async http request using pyscript/pyodide: see this part of the docs.",https://stackoverflow.com/questions/75738343,python,14-03-2023 20:59,2777.0,3.0,1.0,True,15-03-2023 14:01,14-03-2023 21:03
71966350,word2vec + cnn overfitting,"currently i'am training my word2vec + cnn for twitter sentiment analysis about covid-19 vaccine domain. i used the pre-trained googlenewsvectornegative300 word embedding. the problem is why i heavily overfit on training proses. the reason i used the pre-trained googlenewsvectornegative300 because the performance much worse when i trained my own word2vec using own dataset. here several processes that i have done before fitting the model:
text pre processing:

lower casing
remove hashtag, mentions, urls, numbers, change words to numbers, non-ascii characters, retweets ""rt""
expand contractions
replace negations with antonyms
remove puncutations
remove stopwords
lemmatization

i split my dataset into 90:10 for train:test as follows:
def split_data(x, y):
    x_train, x_test, y_train, y_test = train_test_split(x, 
                                                        y,
                                                        train_size=0.9, 
                                                        test_size=0.1, 
                                                        stratify=y,
                                                        random_state=0)
    return x_train, x_test, y_train, y_test

the split data resulting in training has 2060 samples with 708 positive sentiment class, 837 negative sentiment class, and 515 sentiment neutral class
training:

testing:

then, i implemented the text augmentation that is eda (easy data augmentation) on all the training data as follows:
class textaugmentation:
    def __init__(self):
        self.augmenter = eda()

    def replace_synonym(self, text):
        augmented_text_portion = int(len(text)*0.1) 
        synonym_replaced = self.augmenter.synonym_replacement(text, n=augmented_text_portion)
        return synonym_replaced

    def random_insert(self, text):
        augmented_text_portion = int(len(text)*0.1) 
        random_inserted = self.augmenter.random_insertion(text, n=augmented_text_portion)
        return random_inserted

    def random_swap(self, text):
        augmented_text_portion = int(len(text)*0.1)
        random_swaped = self.augmenter.random_swap(text, n=augmented_text_portion)
        return random_swaped

    def random_delete(self, text):
        random_deleted = self.augmenter.random_deletion(text, p=0.5)
        return random_deleted

text_augmentation = textaugmentation()

the data augmentation resulting in training has 10300 samples with 3540 positive sentiment class, 4185 negative sentiment class, and 2575 sentiment neutral class
then, i tokenized the sequence as follows:
# tokenize the sequence
pfizer_tokenizer = tokenizer(oov_token='oov')
pfizer_tokenizer.fit_on_texts(df_pfizer_train['text'].values)

x_pfizer_train_tokenized = pfizer_tokenizer.texts_to_sequences(df_pfizer_train['text'].values)
x_pfizer_test_tokenized = pfizer_tokenizer.texts_to_sequences(df_pfizer_test['text'].values)

# pad the sequence
x_pfizer_train_padded = pad_sequences(x_pfizer_train_tokenized, maxlen=100)
x_pfizer_test_padded = pad_sequences(x_pfizer_test_tokenized, maxlen=100)

pfizer_max_length = 100
pfizer_num_words = len(pfizer_tokenizer.word_index) + 1

# encode label
y_pfizer_train_encoded = df_pfizer_train['sentiment'].factorize()[0]
y_pfizer_test_encoded = df_pfizer_test['sentiment'].factorize()[0]

y_pfizer_train_category = to_categorical(y_pfizer_train_encoded)
y_pfizer_test_category = to_categorical(y_pfizer_test_encoded)

resulting in 8869 unique words and 100 maximum sequence length
finally, i fit the into my model using pre trained googlenewsvectornegative300 word embedding and cnn, and i split my training data again with 10% for validation as follows:
# build single cnn model
def build_cnn_model(embedding_matrix, max_sequence_length):
    # input layer
    input_layer = input(shape=(max_sequence_length,))

    # word embedding layer
    embedding_layer = embedding(input_dim=embedding_matrix.shape[0],
                                output_dim=embedding_matrix.shape[1],
                                weights=[embedding_matrix],
                                input_length=max_sequence_length,
                                trainable=true)(input_layer)

    # cnn model layer
    cnn_layer = conv1d(filters=256,
                        kernel_size=2,
                        strides=1,
                        padding='valid',
                        activation='relu')(embedding_layer)
    cnn_layer = maxpooling1d(pool_size=2)(cnn_layer)
    cnn_layer = dropout(rate=0.5)(cnn_layer)
    batch_norm_layer = batchnormalization()(cnn_layer)

    
    cnn_layer = conv1d(filters=256,
                        kernel_size=2,
                        strides=1,
                        padding='valid',
                        activation='relu')(batch_norm_layer)
    cnn_layer = maxpooling1d(pool_size=2)(cnn_layer)
    cnn_layer = dropout(rate=0.5)(cnn_layer)
    batch_norm_layer = batchnormalization()(cnn_layer)

    
    cnn_layer = conv1d(filters=256,
                        kernel_size=2,
                        strides=1,
                        padding='valid',
                        activation='relu')(batch_norm_layer)
    cnn_layer = maxpooling1d(pool_size=2)(cnn_layer)
    cnn_layer = dropout(rate=0.5)(cnn_layer)
    batch_norm_layer = batchnormalization()(cnn_layer)


    flatten = flatten()(batch_norm_layer)
    
    # dense model layer
    dense_layer = dense(units=10, activation='relu')(flatten)
    batch_norm_layer = batchnormalization()(dense_layer)
    output_layer = dense(units=3, activation='softmax')(batch_norm_layer)
  
    cnn_model = model(inputs=input_layer, outputs=output_layer)
  
    return cnn_model

    return lstm_model

sinovac_cnn_history = sinovac_cnn_model.fit(x=x_sinovac_train,
                                                  y=y_sinovac_train,
                                                  batch_size=128,
                                                  epochs=100,
                                                  validation_split=0.1,
                                                  verbose=1)


the training result:

i really need some suggestions or insights because i have been doing this without any performance progress to my model","['python', 'keras', 'nlp', 'conv-neural-network', 'word2vec']",71966893,"that's quite a complex problem. it sure looks like overfitting as you said yourself. meaning the model can't generalize well from your training set to new data.
intuitively, i would suggest for you to cycle hyperparameters (epochs, batch size, learning rate, dropout layers), if you didn't already, to seek a better combination. also, i would suggest to use cross-validation to get a better idea of the performance of your classifier. this would also shuffle the training data and avoid that the model learns the data by heart.
have you tried classifying the original data without the data augmentation? it's not a lot of data, but it could be enough to see if the performance on the test set is better than the final version, and thus see whether the data augmentation might be screwing something up in your data.
did you try another embedding? i don't really think this is the problem, but in the search for the error i would probably switch it to see what happens.
last but not least, do you know for a fact that this model structure can handle this task? meaning did you find a working example somewhere? it sure sounds like it could do it, but there is the chance that the cnn model for example just doesn't generalize well over the embeddings. have you considered using a model specified on text classification, like a transformer or an lstm?",https://stackoverflow.com/questions/71966350,python,22-04-2022 09:19,375.0,0.0,1.0,True,22-04-2022 11:52,22-04-2022 11:52
77846486,bertopic: &quot;make sure that the iterable only contains strings&quot;,"i'm still fairly new to python so this might be easier than it appears to me, but i'm stuck. i'm trying to use bertopic and visualize the results with pyldavis. i want to compare the results with the ones i got using lda.
this is my code, where ""data_words"" is the same object that i previously used with lda topic modeling:
import pyldavis
import numpy as np
from bertopic import bertopic

# train model
bert_model = bertopic(verbose=true, calculate_probabilities=true)
topics, probs = bert_model.fit_transform(data_words)

# prepare data for pyldavis
top_n = 5

topic_term_dists = bert_model.c_tf_idf.toarray()[:top_n+1, ]
new_probs = probs[:, :top_n]
outlier = np.array(1 - new_probs.sum(axis=1)).reshape(-1, 1)
doc_topic_dists = np.hstack((new_probs, outlier))
doc_lengths = [len(doc) for doc in docs]
vocab = [word for word in bert_model.vectorizer_model.vocabulary_.keys()]
term_frequency = [bert_model.vectorizer_model.vocabulary_[word] for word in vocab]

data = {'topic_term_dists': topic_term_dists,
        'doc_topic_dists': doc_topic_dists,
        'doc_lengths': doc_lengths,
        'vocab': vocab,
        'term_frequency': term_frequency}

# visualize using pyldavis
vis_data= pyldavis.prepare(**data, mds='mmds')
pyldavis.display(vis_data)

i keep getting the following error and i don't understand how to fix the problem:
/library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: tqdmwarning: iprogress not found. please update jupyter and ipywidgets. see 
  from .autonotebook import tqdm as notebook_tqdm
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
cell in[9], line 4
      1 from bertopic import bertopic
      3 bert_model = bertopic()
----> 4 topics, probs = bert_model.fit_transform(data_words)
      6 bert_model.get_topic_freq()

file /library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/bertopic/_bertopic.py:373, in bertopic.fit_transform(self, documents, embeddings, images, y)
    325 """""" fit the models on a collection of documents, generate topics,
    326 and return the probabilities and topic per document.
    327 
   (...)
    370 ```
    371 """"""
    372 if documents is not none:
--> 373     check_documents_type(documents)
    374     check_embeddings_shape(embeddings, documents)
    376 doc_ids = range(len(documents)) if documents is not none else range(len(images))

file /library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/bertopic/_utils.py:43, in check_documents_type(documents)
     41 elif isinstance(documents, iterable) and not isinstance(documents, str):
     42     if not any([isinstance(doc, str) for doc in documents]):
---> 43         raise typeerror(""make sure that the iterable only contains strings."")
     44 else:
     45     raise typeerror(""make sure that the documents variable is an iterable containing strings only."")

typeerror: make sure that the iterable only contains strings.

edit:
so, i'm assuming that the data that i'm to trying to analyze aren't formatted the way in which bertopic expects them to be. my dataset is structured like this:
{
    ""tfu_1881_00102"": {
        ""magazine"": ""edited out"",
        ""country"": ""united kingdom"",
        ""year"": ""1881"",
        ""tokens"": [
            ""word1"",
            ""word2""
        ],
        ""bigramfreqs"": {
            ""word1 word2"": 1
        },
        ""tokenfreqs"": {
            ""word1"": 1,
            ""word2"": 1
        }
    },
    ""tfu_1881_00103"": {
        ""magazine"": ""edited out"",
        ""country"": ""united kingdom"",
        ""year"": ""1881"",
        ""tokens"": [
            ""word3"",
            ""word4""
        ],
        ""bigramfreqs"": {
            ""word3 word4"": 1
        },
        ""tokenfreqs"": {
            ""word3"": 1,
            ""word4"": 1
        }
    }
}

i then create the ""data_words"" object with this code:
with open(""data/5_json/output_final.json"", ""r"") as file:
    data = json.load(file)

data_words = []
counter = 0
for key in data:
    counter += 1
    sub_list = data[key][""tokens""]
    data_words.append(sub_list)
print(counter)

edit2: what worked
so, after goku suggested to flatten my list of lists, i initially tried this solution:
flat_data_words = []

for list in data_words:
    for lists in list:
        flat_data_words.append(lists)

it apparently worked, but the code resulted in a new error. i tried to search a bit more and i found a similar topic that made me understand that bertopic is expecting each string in the list to be a document. that wasn't my case, because the code i used to flatten my list of lists simply results in a list of single tokens. i think that that's why i was getting the new error. then i tried this and now it seemingly works:
flat_data_words = []

for list_of_strings in data_words:
    sentence = ' '.join(list_of_strings)
    flat_data_words.append(sentence)","['python', 'python-3.x', 'nlp', 'topic-modeling']",77847778,"data_words is a nested list.
it contains lists and strings.

bert_model.fit_transform(data_words)

.fit() is expecting an iterable with only strings.
you can try flattening data_words so that it only contains strings and then use :
bert_model.fit_transform(data_words)

a related issue:",https://stackoverflow.com/questions/77846486,python,19-01-2024 13:36,940.0,2.0,1.0,True,22-01-2024 15:35,22-01-2024 15:35
78270250,modulenotfounderror: no module named &#39;llama_index.embeddings.langchain&#39;,"i am trying to use langchain embeddings, using the following code in google colab:
these are the installations:
pip install pypdf
pip install -q transformers einops accelerate langchain bitsandbytes
pip install install sentence_transformers
pip3 install llama-index --upgrade
pip install llama-index-llms-huggingface
huggingface-cli login
pip install -u llama-index-core llama-index-llms-openai llama-index-embeddings-openai



then i ran this code in the google colab:
from llama_index.core import vectorstoreindex,simpledirectoryreader,servicecontext
from llama_index.llms.huggingface import huggingfacellm
from llama_index.core.prompts.prompts import simpleinputprompt


# reading pdf
documents=simpledirectoryreader(""/content/sample_data/data"").load_data()

#prompt
query_wrapper_prompt=simpleinputprompt(""<|user|>{query_str}<|assistant|>"")

import torch
llm = huggingfacellm(
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={""temperature"": 0.0, ""do_sample"": false},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name=""meta-llama/llama-2-7b-chat-hf"",
    model_name=""meta-llama/llama-2-7b-chat-hf"",
    device_map=""auto"",
    # uncomment this if using cuda to reduce memory usage
    model_kwargs={""torch_dtype"": torch.float16 , ""load_in_8bit"":true}
)

# embeddings
from langchain.embeddings.huggingface import huggingfaceembeddings
from llama_index.core import servicecontext
from llama_index.embeddings.langchain import langchainembedding

embed_model=langchainembedding(
    huggingfaceembeddings(model_name=""sentence-transformers/all-mpnet-base-v2""))



then i got this error:
modulenotfounderror: no module named 'llama_index.embeddings.langchain'
i am using latest version of llama-index
version: 0.10.26
can someone suggest, how to resolve this error.","['python', 'langchain', 'word-embedding', 'llama-index']",78277147,"you have pip install llama-index-embeddings-openai
and official documentation has pip install llama-index-embeddings-huggingface
and similar way you need
pip install llama-index-embeddings-langchain",https://stackoverflow.com/questions/78270250,python,03-04-2024 20:05,9405.0,1.0,1.0,True,04-04-2024 23:53,04-04-2024 14:43
79459888,opennlp postaggerme and chunkerme synergy,"i'm trying to use the opennlp chunking api to chunk a portuguese sentence. so, first i tokenized a sentence using tokenizerme, then i tagged it with postaggerme. for both i used the ready-made models provided by the project here.
for the sentence ï¿½ï¿½ï¿½ivo viu a uvaï¿½ï¿½ï¿½, postaggerme returns the tags [propn, verb, det, noun]. the model seems to be using "" rel=""nofollow noreferrer"">ud pos tags.
as there is no ready-made model for chunkerme in portuguese, i followed the instructions and did the training first using the chunkerconverter tool (to convert from ""arvore deitada"" to conll2000) and then generating the model with chunkertrainerme tool. everything worked well. for the sentence above, the chunker produced correct tags ([b-np, b-vp, b-np, i-np]).
but, for more complex sentences, it hasn't produced such good results.
i was trying to identify what i could improve in chunker training, and one of the things i noticed is that there is a difference between the types of tags. the portuguese corpus (bosque 8.0) seems to be using portuguese tags. for example, instead of propn, the corpus uses prop and instead of det, it uses art.
it seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with ud tags, but it has been trained with another type of tag...
but before writing code creating a routine to convert from a portuguese notation to ud (or penn) i wanted to ask, if

this does indeed have an impact,
there is a tool that already does this translation and
there are any other suggestions for improving the chunker precision/recall.","['nlp', 'opennlp']",79475445,"q1
yes, the chosen tag set (ud, penn, custom) has an impact. conversion is not possible in a bi-directional manner:

penn -> ud should work well.
ud -> penn is not a good idea as it a lossy conversion. ud tag set are less detailed when compared to the ""classic' penn tag set.

using a custom, language specific tag-set can work, but it is a matter of ""mapping"" from/to ud correctly. this might work for some tag sets and languages, for others it might be too complicated / lossy.
q2
no, there isn't. the opennlp project takes code donations for upcoming releases, if you want to provide such a mapping/translation for pt lang.
q3
this needs details/discussion on the apache opennlp user and/or dev mailing lists. alternatively, feel free to open a jira issue if you can drill the topic down to a clear idea or proposed code addition.",https://stackoverflow.com/questions/79459888,nlp,22-02-2025 16:06,44.0,-1.0,1.0,True,28-02-2025 11:55,26-02-2025 16:12
78904911,pydantic is not compatible with langchain documents,"i am using langchain 0.2.34 together with python 3.12.5 to build a rag architecture and pydantic 2.8.2 for validation. it appears that some langchain classes are not compatible with pydantic although i explicitly allow arbitrary types. or am i missing something?
here is a code sample and the respective error.
from typing import list

from langchain_core.documents.base import document
from pydantic import basemodel, configdict


class responsebody(basemodel):
    message: list[document]
    model_config = configdict(arbitrary_types_allowed=true)

docs = [document(page_content=""this is a document"")]
res = responsebody(message=docs)

error:
typeerror: basemodel.validate() takes 2 positional arguments but 3 were given","['python', 'pydantic', 'langchain', 'pydantic-v2']",78929028,"langchain is using the functionality in pydantic v1. define your model with v1 syntax:
from pydantic.v1 import basemodel, configdict

...

you can read about their migration plan + pydantic compatibility here: how to use langchain with different pydantic versions",https://stackoverflow.com/questions/78904911,python,23-08-2024 08:08,3096.0,2.0,1.0,True,29-08-2024 17:30,26-08-2024 06:35
77078119,openai api: how to catch all 5xx errors in python?,"i want to catch all 5xx errors (e.g., 500) that openai api sends so that i can retry before giving up and reporting an exception.
right now i'm basically doing the following:
try:
    response = openai.chatcompletion.create(req)
except invalidrequesterror as e:
    reporterror
except serviceunavailableerror as e:
    retry
except exception as e:
    response = f""exception: {e}""
    raise exception(response)

some 5xx errors are getting caught as unknown errors (last case) which i want to catch so that i can retry them as i do in the case of the serviceunavailableerror. but i don't know how to go about catching all the 5xx errors for retry. the docs just talk about how to catch the specifically named errors.","['python', 'error-handling', 'openai-api']",77088097,"all 5xx errors belong to the serviceunavailableerror. take a look at the official openai documentation:




type
overview




apierror
cause: issue on our side. solution: retry your request after a brief wait and contact us if the issue persists.


timeout
cause: request timed out. solution: retry your request after a brief wait and contact us if the issue persists.


ratelimiterror
cause: you have hit your assigned rate limit. solution: pace your requests. read more in our rate limit guide.


apiconnectionerror
cause: issue connecting to our services. solution: check your network settings, proxy configuration, ssl certificates, or firewall rules.


invalidrequesterror
cause: your request was malformed or missing some required parameters, such as a token or an input. solution: the error message should advise you on the specific error made. check the documentation for the specific api method you are calling and make sure you are sending valid and complete parameters. you may also need to check the encoding, format, or size of your request data.


authenticationerror
cause: your api key or token was invalid, expired, or revoked. solution: check your api key or token and make sure it is correct and active. you may need to generate a new one from your account dashboard.


serviceunavailableerror
cause: issue on our servers. solution: retry your request after a brief wait and contact us if the issue persists. check the status page.




handle the serviceunavailableerror as follows:
try:
  # make your openai api request here
  response = openai.completion.create(prompt=""hello world"",
                                      model=""text-davinci-003"")

except openai.error.serviceunavailableerror as e:
  # handle 5xx errors here
  print(f""openai api request error: {e}"")
  pass",https://stackoverflow.com/questions/77078119,python,10-09-2023 22:37,831.0,-1.0,2.0,True,13-09-2023 18:37,13-09-2023 18:37
70364824,pytorch multi-class: valueerror: expected input batch_size (416) to match target batch_size (32),"i have created a mutli-class classification neural network. training, and validation iterators where created with bigbucketiterator method with fields {'text_normalized_tweet':text, 'label': label}
text = a tweet
label = a float number (with 3 values: 0,1,2)
below i execute a dummy example of my neural network:
import torch.nn as nn

class multiclassclassifer(nn.module):
  #define all the layers used in model
  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
    
    #constructor
    super(multiclassclassifer, self).__init__()

    #embedding layer
    self.embedding = nn.embedding(vocab_size, embedding_dim)

    #dense layer
    self.hiddenlayer = nn.linear(embedding_dim, hidden_dim)

    #batch normalization layer
    self.batchnorm = nn.batchnorm1d(hidden_dim)

    #output layer
    self.output = nn.linear(hidden_dim, output_dim)

    #activation layer
    self.act = nn.softmax(dim=1) #2d-tensor

    #initialize weights of embedding layer
    self.init_weights()

  def init_weights(self):

    initrange = 1.0
    
    self.embedding.weight.data.uniform_(-initrange, initrange)
  
  def forward(self, text, text_lengths):

    embedded = self.embedding(text)

    #packed sequence
    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=true)

    tensor, batch_size = packed_embedded[0], packed_embedded[1]

    hidden_1 = self.batchnorm(self.hiddenlayer(tensor))

    return self.act(self.output(hidden_1))

instantiate the model
input_dim = len(text.vocab)
embedding_dim = 100
hidden_dim = 64
output_dim = 3

model = multiclassclassifer(input_dim, embedding_dim, hidden_dim, output_dim)

when i call
text, text_lengths = batch.text_normalized_tweet
                
predictions = model(text, text_lengths).squeeze()

loss = criterion(predictions, batch.label)

it returns,

valueerror: expected input batch_size (416) to match target batch_size (32).

model(text, text_lengths).squeeze() = torch.size([416, 3])
batch.label = torch.size([32])

i can see that the two objects have different sizes, but i have no clue how to fix this?
you may find the google colab notebook here
shapes of each in, out tensor of my forward() method:
torch.size([32, 10, 100]) #self.embedding(text)
torch.size([320, 100]) #nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=true)
torch.size([320, 64]) #self.batchnorm(self.hiddenlayer(tensor))
torch.size([320, 3]) #self.act(self.output(hidden_1))","['nlp', 'pytorch', 'multiclass-classification']",70366994,"you shouldn't be using the squeeze function after the forward pass, that doesn't make sense.
after removing the squeeze function, as you see, the shape of your final output is [320,3] whereas it is expecting [32,3]. one way to fix this is to average out the embeddings you obtain for each word after the self.embedding function like shown below:
def forward(self, text, text_lengths):

    embedded = self.embedding(text)
    embedded = torch.mean(embedded, dim=1, keepdim=true)

    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=true)
    tensor, batch_size = packed_embedded[0], packed_embedded[1]

    hidden_1 = self.batchnorm(self.hiddenlayer(tensor))
    return self.act(self.output(hidden_1))",https://stackoverflow.com/questions/70364824,nlp,15-12-2021 13:50,742.0,0.0,1.0,True,15-12-2021 16:39,15-12-2021 15:58
76643819,attributeerror: &#39;tuple&#39; object has no attribute &#39;run&#39; when using langchain llmchain,"i'm building my first langchain app by following 
here's my code:
#bring in deps
import os
from apikey import apikey

import streamlit as st
from langchain import llmchain, openai, prompttemplate

os.environ['openai_api_key'] = apikey

#app framework
st.title('youtube gpt creator')
prompt = st.text_input('plug in your prompt here')

# prompt templates
title_template = prompttemplate(
    input_variables = [""topic""],
    template = 'write me a youtube video title about {topic}'
)


#llms
llm = openai(model=""text-davinci-003"",
             temperature=0.9)
title_chain = llmchain(llm=llm, prompt=title_template, verbose=true),


#show stuff to the screen if it has prompt
if prompt:
    # response = llm(prompt)
    response = title_chain.run(topic=prompt)
    st.write(response)

when i try to run it, i get this error:
attributeerror: 'tuple' object has no attribute 'run'
file ""d:\coding\langchain\app.py"", line 30, in <module>
    response = title_chain.run(topic=prompt)
               ^^^^^^^^^^^^^^^

i'm not familiar with langchain yet and i wonder if anyone can lend a hand to a potentialy real simple problem. thank you for your time","['python', 'langchain', 'py-langchain']",76643855,"thank you @ewyyntomato

you have an extra comma in the end of this line title_chain = llmchain(llm=llm, prompt=title_template, verbose=true), remove that comma and things should work.

silly me. problem solved.",https://stackoverflow.com/questions/76643819,python,08-07-2023 16:03,2341.0,1.0,2.0,True,08-07-2023 16:15,08-07-2023 16:11
70453161,openai semantic search not working with the file parameter,"from what i understand, you can use the documents parameter or the file parameter to tell openai on what labels you want to perform a search.  i'm getting expected results using the documents parameter.  i am getting unsatisfactory results using the file parameter.  i would expect them to be the same.
when performing a search using the documents parameter..
response = dict(openai.engine('davinci').search(
    query='sitcom',
    #file=file_id,
    max_rerank=5,
    documents=[""white house"", ""school"", ""seinfeld""],
    return_metadata=false))

..i get expected results.. ""sitcom"" wins the search with a score of 771.
{'object': 'list', 'data': [<openaiobject search_result at 0xb5e8ef48> json: {
  ""document"": 0,
  ""object"": ""search_result"",
  ""score"": 147.98
}, <openaiobject search_result at 0xb5ebd148> json: {
  ""document"": 1,
  ""object"": ""search_result"",
  ""score"": 211.021
}, <openaiobject search_result at 0xb5ebd030> json: {
  ""document"": 2,
  ""object"": ""search_result"",
  ""score"": 771.348
}], 'model': 'davinci:2020-05-03'}

now trying with the file parameter i create a temp.jsonl file with contents..
{""text"": ""white house"", ""metadata"": ""metadata here""}
{""text"": ""school"", ""metadata"": ""metadata here""}
{""text"": ""seinfeld"", ""metadata"": ""metadata here""}

i then upload the file to openai server with..
res = openai.file.create(file=open('temp.jsonl'), purpose=""search"")

where..
file_id = res['id']

i wait until the file is processed by the server then..
response = dict(openai.engine('davinci').search(
    query='sitcom',
    file=file_id,
    max_rerank=5,
    #documents=[""white house"", ""school"", ""seinfeld""],
    return_metadata=false))

but i get the following message when i perform search..
no similar documents were found in file with id 'file-lzhkasuxbdjtawbhhxhpiof4'.please upload more documents or adjust your query.

i only get results when my query exactly matches a label..
response = dict(openai.engine('davinci').search(
    query='seinfeld',
    file=file_id,
    max_rerank=5,
    #documents=[""white house"", ""school"", ""seinfeld""],
    return_metadata=false))

{'object': 'list', 'data': [<openaiobject search_result at 0xb5e74f48> json: {
  ""document"": 0,
  ""object"": ""search_result"",
  ""score"": 668.846,
  ""text"": ""seinfeld""
}], 'model': 'davinci:2020-05-03'}

what am i doing wrong?  shouldn't the results be the same using the documents parameter or the file parameter?","['search', 'openai-api']",70456946,"rereading the docs, it seems, when using file parameter instead of documents parameter, the server first performs a basic ""keyword"" search with the provided query to narrow down the results before finally reranking those results with a semantic search using the same query.
this is disappointing.
just to provide a working example..
{""text"": ""stairway to the basement"", ""metadata"": ""metadata here""}
{""text"": ""school"", ""metadata"": ""metadata here""}
{""text"": ""stairway to heaven"", ""metadata"": ""metadata here""}

now using the query ""led zeppelin's most famous song stairway"" the server will narrow down the results to document 0 and document 2 finding matches for the ""stairway"" token.  it will then perform a semantic search and score both of them.  document 2 (""stairway to heaven"") will have the highest relevancy score.
using the query ""stairway to the underground floor"" will give document 0 (""stairway to the basement"") the highest relevancy score.
this is disappointing because the query has to be useful for both a keyword search and the semantic search.
in my original post, the keyword search was not providing any results because the query was only designed for a semantic search.  when using the documents parameter, only a semantic search is performed, that is why it worked in that case.",https://stackoverflow.com/questions/70453161,search,22-12-2021 17:49,641.0,2.0,1.0,True,23-12-2021 03:06,23-12-2021 00:22
78952799,the chatcompletion operation does not work with the specified model gpt-4o-mini,"context
i have below the python code.
client = azureopenai(
    api_key = os.getenv(""azure_openai_api_key""),  
    api_version = os.getenv('azure_openai_api_version'),
    azure_endpoint = os.getenv('azure_openai_endpoint')
)  
messages = [
    {""role"": ""user"", ""content"": prompt}
]
response = client.chat.completions.create(
    model=""gpt-4o-mini"",        
    messages=messages,
    temperature=0,  
)

issue
i faced the below error.
openai.badrequesterror: error code: 400 - 
{
   ""error"":{
      ""code"":""operationnotsupported"",
      ""message"":""the chatcompletion operation does not work with the specified model, gpt-4o-mini. please choose different model and try again. you can learn more about which models can be used with each operation here: 
   }
}

the above code works fine when i change the model to ""gpt-4o""","['python', 'openai-api', 'azure-openai', 'gpt-4o-mini']",78968329,"the azure region you selected does not support this model. you need to check this list to select the appropriate region, and select the deployment type as ""standard""",https://stackoverflow.com/questions/78952799,python,05-09-2024 11:31,1370.0,1.0,1.0,True,10-09-2024 07:23,05-09-2024 11:42
77374908,error installing openai in pycharm ce 2023.2.3 (python 3.12). (aiohttp whl error),"i am hoping someone can help me figure out the best course of action for an issue i am having.  i am trying to play with the chatgpt api.  as suggested, i am doing an install, which seems to through an error.  please see the log below.   i then tried to install alternative versions of aiohttp (3.8.1 through to 3.8.6) but they all throw the same error.   based on some reading, i could potentially need to role back to an older version of python. i am not super experienced though, and also see some info on deprecation of something in cython, so figured i would check with the group here to see if that is my only course of action.
log
 pip install --upgrade openai           
collecting openai
  downloading openai-0.28.1-py3-none-any.whl.metadata (11 kb)
collecting requests>=2.20 (from openai)
  downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kb)
collecting tqdm (from openai)
  downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kb)
     ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½oýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý 57.6/57.6 kb 65.9 kb/s eta 0:00:00
collecting aiohttp (from openai)
  downloading aio (7.4 mb)
     ýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý 7.4/7.4 mb 1.6 mb/s eta 0:00:00
  installing build dependencies ... done
  getting requirements to build wheel ... done
  installing backend dependencies ... done
  preparing metadata (pyproject.toml) ... done
collecting charset-normalizer<4,>=2 (from requests>=2.20->openai)
  downloading charset_normalizer-3.3.1-cp312-cp312-macosx_10_9_x86_64.whl.metadata (33 kb)
collecting idna<4,>=2.5 (from requests>=2.20->openai)
  downloading idna-3.4-py3-none-any.whl (61 kb)
     ýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý 61.5/61.5 kb 646.2 kb/s eta 0:00:00
collecting urllib3<3,>=1.21.1 (from requests>=2.20->openai)
  downloading urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kb)
collecting certifi>=2017.4.17 (from requests>=2.20->openai)
  downloading certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kb)
collecting attrs>=17.3.0 (from aio
  downloading attrs-23.1.0-py3-none-any.whl (61 kb)
     ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 61.2/61.2 kb 587.7 kb/s eta 0:00:00
collecting multidict<7.0,>=4.5 (from aio
  downloading multidict-6.0.4.tar.gz (51 kb)
     ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½uesync_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kb)
collecting yarl<2.0,>=1.0 (from aio
  downloading yarl-1.9.2.tar.gz (184 kb)
     ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 184.7/184.7 kb 553.5 kb/s eta 0:00:00
  installing build dependencies ... done
  getting requirements to build wheel ... done
  preparing metadata (pyproject.toml) ... done
collecting frozenlist>=1.1.1 (from aio
  downloading frozenlist-1.4.0.tar.gz (90 kb)
     ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½eýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý 77.0/77.0 kb 1.8 mb/s eta 0:00:00
downloading requests-2.31.0-py3-none-any.whl (62 kb)
   ýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý 62.6/62.6 kb 640.6 kb/s eta 0:00:00
downloading tqdm-4.66.1-py3-none-any.whl (78 kb)
   ýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý 78.3/78.3 kb 753.2 kb/s eta 0:00:00
downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kb)
downloading certifi-2023.7.22-py3-none-any.whl (158 kb)
   ýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý 158.3/158.3 kb 691.2 kb/s eta 0:00:00
downloading charset_normalizer-3.3.1-cp312-cp312-macosx_10_9_x86_64.whl (119 kb)
   ýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý 119.9/119.9 kb 196.1 kb/s eta 0:00:00
downloading urllib3-2.0.7-py3-none-any.whl (124 kb)
   ýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý 124.2/124.2 kb 811.8 kb/s eta 0:00:00
building wheels for collected packages: aio frozenlist, multidict, yarl
  building wheel for aiohttp (pyproject.toml) ... error
  error: subprocess-exited-with-error
  
  ýý building wheel for aiohttp (pyproject.toml) did not run successfully.
  ýýý exit code: 1
  ýýýýýý> [188 lines of output]
      *********************
      * accelerated build *
      *********************
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build/lib.macosx-10.9-universal2-cpython-312
      creating build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      running egg_info
      writing aio
      writing dependency_links to aio
      writing requirements to aio
      writing top-level names to aio
      reading manifest file 'aio
      reading manifest template 'manifest.in'
      warning: no files found matching 'aio anywhere in distribution
      warning: no previously-included files matching '*.pyc' found anywhere in distribution
      warning: no previously-included files matching '*.pyd' found anywhere in distribution
      warning: no previously-included files matching '*.so' found anywhere in distribution
      warning: no previously-included files matching '*.lib' found anywhere in distribution
      warning: no previously-included files matching '*.dll' found anywhere in distribution
      warning: no previously-included files matching '*.a' found anywhere in distribution
      warning: no previously-included files matching '*.obj' found anywhere in distribution
      warning: no previously-included files found matching 'aio
      no previously-included directories found matching 'docs/_build'
      adding license file 'license.txt'
      writing manifest file 'aio
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      creating build/lib.macosx-10.9-universal2-cpython-312/aio
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aio
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aio
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aio
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aio
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aio
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aio
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aio
      copying aio -> build/lib.macosx-10.9-universal2-cpython-312/aio
      running build_ext
      building 'aio extension
      creating build/temp.macosx-10.9-universal2-cpython-312
      creating build/temp.macosx-10.9-universal2-cpython-312/aiohttp
      clang -fno-strict-overflow -wsign-compare -wunreachable-code -fno-common -dynamic -dndebug -g -o3 -wall -arch arm64 -arch x86_64 -g -i/users/travislueth/pycharmprojects/chatgpt/chatgpt/include -i/library/frameworks/python.framework/versions/3.12/include/python3.12 -c aio -o build/temp.macosx-10.9-universal2-cpython-312/aio
      aio warning: 'py_optimizeflag' is deprecated [-wdeprecated-declarations]
        if (unlikely(!py_optimizeflag)) {
                      ^
      /library/frameworks/python.framework/versions/3.12/include/python3.12/cpython/pydebug.h:13:1: note: 'py_optimizeflag' has been explicitly marked deprecated here
      py_deprecated(3.12) pyapi_data(int) py_optimizeflag;
      ^
      /library/frameworks/python.framework/versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'py_deprecated'
      #define py_deprecated(version_unused) __attribute__((__deprecated__))
                                                           ^
      aio warning: 'ma_version_tag' is deprecated [-wdeprecated-declarations]
          return likely(dict) ? __pyx_get_dict_version(dict) : 0;
                                ^
      aio note: expanded from macro '__pyx_get_dict_version'
      #define __pyx_get_dict_version(dict)  (((pydictobject*)(dict))->ma_version_tag)
                                                                      ^
      /library/frameworks/python.framework/versions/3.12/include/python3.12/cpython/dictobject.h:22:5: note: 'ma_version_tag' has been explicitly marked deprecated here
          py_deprecated(3.12) uint64_t ma_version_tag;
          ^
      /library/frameworks/python.framework/versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'py_deprecated'
      #define py_deprecated(version_unused) __attribute__((__deprecated__))
                                                           ^
      aio warning: 'ma_version_tag' is deprecated [-wdeprecated-declarations]
          return (dictptr && *dictptr) ? __pyx_get_dict_version(*dictptr) : 0;
                                         ^
      aio note: expanded from macro '__pyx_get_dict_version'
      #define __pyx_get_dict_version(dict)  (((pydictobject*)(dict))->ma_version_tag)
                                                                      ^
      /library/frameworks/python.framework/versions/3.12/include/python3.12/cpython/dictobject.h:22:5: note: 'ma_version_tag' has been explicitly marked deprecated here
          py_deprecated(3.12) uint64_t ma_version_tag;
          ^
      /library/frameworks/python.framework/versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'py_deprecated'
      #define py_deprecated(version_unused) __attribute__((__deprecated__))
                                                           ^
      aio warning: 'ma_version_tag' is deprecated [-wdeprecated-declarations]
          if (unlikely(!dict) || unlikely(tp_dict_version != __pyx_get_dict_version(dict)))
                                                             ^
      aio note: expanded from macro '__pyx_get_dict_version'
      #define __pyx_get_dict_version(dict)  (((pydictobject*)(dict))->ma_version_tag)
                                                                      ^
      /library/frameworks/python.framework/versions/3.12/include/python3.12/cpython/dictobject.h:22:5: note: 'ma_version_tag' has been explicitly marked deprecated here
          py_deprecated(3.12) uint64_t ma_version_tag;
          ^
      /library/frameworks/python.framework/versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'py_deprecated'
      #define py_deprecated(version_unused) __attribute__((__deprecated__))
                                                           ^
      aio warning: 'ma_version_tag' is deprecated [-wdeprecated-declarations]
              __pyx_py_dict_lookup_if_modified(
              ^
      aio note: expanded from macro '__pyx_py_dict_lookup_if_modified'
          if (likely(__pyx_get_dict_version(dict) == __pyx_dict_version)) {\
                     ^
      aio note: expanded from macro '__pyx_get_dict_version'
      #define __pyx_get_dict_version(dict)  (((pydictobject*)(dict))->ma_version_tag)
                                                                      ^
      /library/frameworks/python.framework/versions/3.12/include/python3.12/cpython/dictobject.h:22:5: note: 'ma_version_tag' has been explicitly marked deprecated here
          py_deprecated(3.12) uint64_t ma_version_tag;
          ^
      /library/frameworks/python.framework/versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'py_deprecated'
      #define py_deprecated(version_unused) __attribute__((__deprecated__))
                                                           ^
      aio warning: 'ma_version_tag' is deprecated [-wdeprecated-declarations]
              __pyx_py_dict_lookup_if_modified(
              ^
      aio note: expanded from macro '__pyx_py_dict_lookup_if_modified'
              __pyx_dict_version = __pyx_get_dict_version(dict);\
                                   ^
      aio note: expanded from macro '__pyx_get_dict_version'
      #define __pyx_get_dict_version(dict)  (((pydictobject*)(dict))->ma_version_tag)
                                                                      ^
      /library/frameworks/python.framework/versions/3.12/include/python3.12/cpython/dictobject.h:22:5: note: 'ma_version_tag' has been explicitly marked deprecated here
          py_deprecated(3.12) uint64_t ma_version_tag;
          ^
      /library/frameworks/python.framework/versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'py_deprecated'
      #define py_deprecated(version_unused) __attribute__((__deprecated__))
                                                           ^
      aio error: no member named 'ob_digit' in 'struct _longobject'
                  const digit* digits = ((pylongobject*)x)->ob_digit;
                                        ~~~~~~~~~~~~~~~~~~  ^
      aio error: no member named 'ob_digit' in 'struct _longobject'
                  const digit* digits = ((pylongobject*)x)->ob_digit;
                                        ~~~~~~~~~~~~~~~~~~  ^
      aio error: no member named 'ob_digit' in 'struct _longobject'
                  const digit* digits = ((pylongobject*)x)->ob_digit;
                                        ~~~~~~~~~~~~~~~~~~  ^
      aio error: no member named 'ob_digit' in 'struct _longobject'
                  const digit* digits = ((pylongobject*)x)->ob_digit;
                                        ~~~~~~~~~~~~~~~~~~  ^
      aio error: no member named 'ob_digit' in 'struct _longobject'
          const digit* digits = ((pylongobject*)b)->ob_digit;
                                ~~~~~~~~~~~~~~~~~~  ^
      6 warnings and 5 errors generated.
      error: command '/usr/bin/clang' failed with exit code 1
      [end of output]
  
  note: this error originates from a subprocess, and is likely not a problem with pip.
  error: failed building wheel for aiohttp
  building wheel for frozenlist (pyproject.toml) ... done
  created wheel for frozenlist: filename=frozenlist-1.4.0-cp312-cp312-macosx_10_9_universal2.whl size=84986 sha256=b708c60a74ddeeb0888d2b0ddc1504fd879d7
  stored in directory: /users/trav/library/caches/pip/wheels/f1/9c/94/9386cb0ea511a93226456388d41d35f1c24ba15a62ffd7b1ef
  building wheel for multidict (pyproject.toml) ... done
  created wheel for multidict: filename=multidict-6.0.4-cp312-cp312-macosx_10_9_universal2.whl size=48165 sha256=afc7344decbaa44b06dad964ac
  stored in directory: /users/trav/library/caches/pip/wheels/f6/d8/ff/3c14a64b8f2ab1aa94ba2888f5a988be6ab446ec5c8d1a82da
  building wheel for yarl (pyproject.toml) ... done
  created wheel for yarl: filename=yarl-1.9.2-cp312-cp312-macosx_10_9_universal2.whl size=98522 sha256=66feda1a1cbdef60e24215208
  stored in directory: /users/trav/library/caches/pip/wheels/84/e3/6a/7d0fa1abee8e4aa3
successfully built frozenlist multidict yarl
failed to build aiohttp
error: could not build wheels for aio which is required to install pyproject.toml-based projects


i tried installing multiple versions of aio
each time i did a cache purge to ensure it installed the right version and got similar errors each time.   i tried 3.8.1 through 3.8.6","['python', 'python-3.x', 'pip', 'openai-api', 'aiohttp']",77503384,"try using for python 3.12
pip install aio",https://stackoverflow.com/questions/77374908,python,27-10-2023 14:19,411.0,-1.0,1.0,True,26-05-2024 21:50,26-05-2024 21:50
79406597,openai_api_key in docker image gives error as &quot;openai.authenticationerror: error code: 401 - &#39;invalid_api_key&#39;,"i have a simple test code to test the openai_api_key locally and in docker image.
the application runs fine locally but for docker image run, it gives error as:
""openai.authenticationerror: error code: 401 - {'error': {'message': 'incorrect api key provided: ""sk-proj****""
here is the testcode.py
from openai import client
from dotenv import load_dotenv
import os

load_dotenv()
openai_api_key = os.getenv(""openai_api_key"")

print(f""key is : {openai_api_key}"")

openai_client = client()

response = openai_client.chat.completions.create(
  model=""gpt-3.5-turbo"",
  messages=[
        {""role"": ""system"", ""content"": ""you are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""hello!""}
    ]
)

here is the dockerfile
from python:3.11-slim

workdir /app

copy . /app

run pip install --no-cache-dir -r requirements.txt

expose 8000

cmd [""python"", ""testcode.py""]

the .env file in my project
openai_api_key=""<my_key>""

this is the run command i use to run the docker image:
docker run --env-file .env -it  -p 8000:8000 testapp_1:v1

if i run this app locally:
it prints the key value and also gives the response from the llm object.
but if i run the docker image, it prints the api key correctly, but throws an authentication error.
on openai site i checked the access permission for the api key. it is set to ""all""
what can be the issue when i use the api key from my docker image?
thanks in adavance!","['docker', 'openai-api']",79433266,"the issue was that double quotes were used to specify opeanai_api_key in the .env file.
if the key is written in double quotes, e.g.
openai_api_key=""sk-proj-....................""

it works in the desktop app, but it results in an invalid_api_key error in a docker image.
i removed the double quotes from the .env file and specified the key as follows:
openai_api_key=sk-proj-....................

now it works both in desktop application as well as in docker image.
thank you!",https://stackoverflow.com/questions/79406597,docker,02-02-2025 12:42,201.0,1.0,1.0,True,17-02-2025 12:40,02-02-2025 14:05
73645084,create hugging face transformers tokenizer using amazon sagemaker in a distributed way,"i am using the sagemaker huggingface processor to create a custom tokenizer on a large volume of text data.
is there a way to make this job data distributed - meaning read partitions of data across nodes and train the tokenizer leveraging multiple cpus/gpus.
at the moment, providing more nodes to the processing cluster merely replicates the tokenization process (basically duplicates the process of creation), which is redundant. you can primarily only scale vertically.
any insights into this?","['amazon-sagemaker', 'huggingface-transformers', 'huggingface-tokenizers', 'amz-sagemaker-distributed-training']",73652132,"considering the following example code forï¿½ï¿½huggingfaceprocessor:
if you have 100 large files in s3 and use a processinginput withï¿½ï¿½s3_data_distribution_type=""shardedbys3key"" (instead of fullyreplicated), the objects in your s3 prefix will be sharded and distributed to your instances.
for example, if you have 100 large files and want to filter records from them using huggingface on 5 instances, the s3_data_distribution_type=""shardedbys3key"" will put 20 objects on each instance, and each instance can read the files from its own path, filter out records, and write (uniquely named) files to the output paths, and sagemaker processing will put the filtered files in s3.
however, if your filtering criteria is stateful or depends on doing a full pass over the dataset first (such as: filtering outliers based on mean and standard deviation on a feature - in case of using sklean processor for example): you'll need to pass that information in to the job so each instance can know how to filter. to send information to the instances launched, you have to use theï¿½ï¿½/opt/ml/config/resourceconfig.json.href="" rel=""nofollow noreferrer"">file:
{ ""current_host"": ""algo-1"", ""hosts"": [""algo-1"",""algo-2"",""algo-3""] }",https://stackoverflow.com/questions/73645084,amazon-sagemaker,08-09-2022 07:17,233.0,0.0,1.0,True,07-10-2022 08:23,07-10-2022 08:23
76817686,text comparison to ignore some newline characters in python,"i have a python script that compares two texts and highlights the differences between them. however, the comparison is being affected by newline characters, causing mismatches for texts with different newline representations. for instance, ""arti\ncle"" and ""article"" are being treated as different.
i'm currently using the difflib
here's a simplified version of my current code:
import difflib

def compare_texts(old_text, new_text):
    old_lines = old_text.splitlines()
    new_lines = new_text.splitlines()
    
    d = difflib.differ()
    diff = d.compare(old_lines, new_lines)
    
    added_lines = []
    deleted_lines = []
    
    for line in diff:
        if line.startswith('+ '):
            added_lines.append(line[2:])
        elif line.startswith('- '):
            deleted_lines.append(line[2:])
    
    return added_lines, deleted_lines

if __name__ == ""__main__"":
    old_text = ""arti\ncle\nthis is some old text.""
    new_text = ""article\nthis is some new text.""
    
    added_lines, deleted_lines = compare_texts(old_text, new_text)
    
    print(""added lines:"")
    print('\n'.join(added_lines))
    
    print(""\ndeleted lines:"")
    print('\n'.join(deleted_lines))


can someone suggest an effective way to compare texts that will handle newline characters appropriately, ensuring that ""arti\ncle"" and ""article"" are treated as the same during the comparison process?
edit1:
in fact, lots of ""\n"" are introduced due to a pdf reading function.
the idea maybe the following: if there is a ""\n"", we can try to delete it. if, after deleting it, we have a match, then we can consider that they are the same.
so ""article"" and ""arti\ncle"" are the same. ""article"" and ""arti\nficial"" are not.
i can't remove all ""\n"" because many of them are still useful.
edit2:
knowing the origins of the bugs, we also may try this approach. some random ""\n"" have been added due to a pdf reading function, so, we can try to delete some meaningless ""\n"" first.","['python', 'nlp', 'difflib']",76818062,"here's  a suggested solution:

i think you need to do a wordwise diff, not linewise. so replace spaces with linebreaks. (or use a different diffing method)
then check if two consecutively deleted or inserted lines joined together match with a neighboring inserted/deleted line

import difflib

def compare_texts(old_text, new_text):
    old_lines = old_text.splitlines()
    new_lines = new_text.splitlines()
    
    d = difflib.differ()
    diff = d.compare(old_lines, new_lines)
    
    added_lines = []
    deleted_lines = []
    
    prev = none
    prev_prev = none
    for line in diff:
        if line.startswith('+ '):
            added_lines.append(line[2:])
        elif line.startswith('- '):
            deleted_lines.append(line[2:])
        if prev is not none and prev_prev is not none:
            # handle + - - 
            if prev_prev.startswith('+ ') and prev.startswith('- ') and line.startswith('- '):
                joined = prev[2:]  + line[2:]
                if joined == prev_prev[2:]:
                    # can remove diffs as they make up the same word
                    del added_lines[-1]
                    del deleted_lines[-1]
                    del deleted_lines[-1]
            # also handle   - - +    + + -    - + + 
        prev_prev = prev
        prev = line
    
    return added_lines, deleted_lines

if __name__ == ""__main__"":
    old_text = ""arti\ncle\nthis is some old text.""
    new_text = ""article\nthis is some new text.""
    
    added_lines, deleted_lines = compare_texts(old_text.replace("" "", ""\n""), new_text.replace("" "", ""\n""))
    
    print(""added lines:"")
    print('\n'.join(added_lines))
    
    print(""\ndeleted lines:"")
    print('\n'.join(deleted_lines))

you need to handle the other cases, i only implemented + - -.
this solution assumes only one linebreak can be in a word. and all 'good' linebreaks are lost.",https://stackoverflow.com/questions/76817686,python,02-08-2023 07:50,78.0,0.0,1.0,True,02-08-2023 08:42,02-08-2023 08:09
72269735,spacy v3 - reading docbin to a json or pandas,"is it possible to be able to read .spacy file as a regular format - e.g. pandas or dict so that we may use it in e.g. sklearn?
suppose we have a spacy docbin object:
nlp = spacy.blank(""en"")
doc_bin = docbin()

df= pd.read_json(""../data/data.jsonl"", lines = true)
df.head()

doc_bin = docbin()

for text, label in zip(df['text'], df['label']):
    doc = nlp(text)
    doc.cats[label] = true
    doc_bin.add(doc)

doc_bin.to_disk('train.spacy')

how would we read train.spacy into dict?","['python', 'spacy']",72283091,"no, it's not possible to read a .spacy file directy into a dict or something in a meaningful way. it's a serialization format specifically for spacy doc objects.
you can always read the docs in and convert them to whatever you want to put in a dict afterwards.",https://stackoverflow.com/questions/72269735,python,17-05-2022 07:21,958.0,1.0,1.0,True,18-05-2022 04:07,17-05-2022 07:23
69876688,loading a huggingface model into allennlp gives different predictions,"i have a custom classification model trained using transformers library based on a bert model. the model classifies text into 7 different categories. it is persisted in a directory using:
trainer.save_model(model_name)
tokenizer.save_pretrained(model_name)

i'm trying to load such persisted model using the allennlp library for further analysis. i managed to do so after a lot of work. however, when running the model inside the allennlp framework, the model tends to predict very different from the predictions i get when i run it using transformers, which lead me think some part of the loading was not done correctly. there are no errors during the inference, it is just that the predictions don't match.
there is little documentation about how to load an existing model, so i'm wondering if someone faced the same situation before. there is just one example of how to do qa classification with roberta, but couldn't extrapolate to what i'm looking for. anyone have an idea if the steps are following are correct?
this is how i'm loading the trained model:
transformer_vocab = vocabulary.from_pretrained_transformer(model_name)
transformer_tokenizer = pretrainedtransformertokenizer(model_name)
transformer_encoder = bertpooler(model_name)

params = params(
    {
     ""token_embedders"": {
        ""tokens"": {
          ""type"": ""pretrained_transformer"",
          ""model_name"": model_name,
        }
      }
    }
)
token_embedder = basictextfieldembedder.from_params(vocab=vocab, params=params)
token_indexer = pretrainedtransformerindexer(model_name)

transformer_model = basicclassifier(vocab=transformer_vocab,
                                    text_field_embedder=token_embedder, 
                                    seq2vec_encoder=transformer_encoder, 
                                    dropout=0.1, 
                                    num_labels=7)

i also had to implement my own datasetreader as follows:
class classificationtransformerreader(datasetreader):
    def __init__(
        self,
        tokenizer: tokenizer,
        token_indexer: tokenindexer,
        max_tokens: int,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.tokenizer = tokenizer
        self.token_indexers: dict[str, tokenindexer] = { ""tokens"": token_indexer }
        self.max_tokens = max_tokens
        self.vocab = vocab

    def text_to_instance(self, text: str, label: str = none) -> instance:
        tokens = self.tokenizer.tokenize(text)
        if self.max_tokens:
            tokens = tokens[: self.max_tokens]
        
        inputs = textfield(tokens, self.token_indexers)
        fields: dict[str, field] = { ""tokens"": inputs }
            
        if label:
            fields[""label""] = labelfield(label)
            
        return instance(fields)

it is instantiated as follows:
dataset_reader = classificationtransformerreader(tokenizer=transformer_tokenizer,
                                                 token_indexer=token_indexer,
                                                 max_tokens=400)

to run the model and test out if it works i'm doing the following:
instance = dataset_reader.text_to_instance(""some sample text here"")
dataset = batch([instance])
dataset.index_instances(transformer_vocab)
model_input = util.move_to_device(dataset.as_tensor_dict(), 
                                  transformer_model._get_prediction_device())

outputs = transformer_model.make_output_human_readable(transformer_model(**model_input))

this works and returns the probabilities correctly, but there don't match what i would get running the model using transformers directly. any idea what's going on?","['python', 'pytorch', 'huggingface-transformers', 'allennlp']",71457943,"answering the original question, the code above loaded most of the components from the original transformer model, but the classifier layer. as dirk mentioned, it is randomly initialized.
the solution is to load the weights of the classifier from transformers into the allennlp one. the following code does the trick.
from transformers import bertforsequenceclassification

model = basicclassifier(vocab=transformer_vocab, 
                        text_field_embedder=token_embedder,
                        seq2vec_encoder=transformer_encoder, 
                        dropout=0.1, 
                        num_labels=7)

# original model loaded using transformers library
classifier = bertforsequenceclassification.from_pretrained(model_name)

transformer_model._classification_layer.weight = classifier.classifier.weight
transformer_model._classification_layer.bias = classifier.classifier.bias",https://stackoverflow.com/questions/69876688,python,07-11-2021 21:34,552.0,0.0,2.0,True,13-03-2022 14:56,07-11-2021 22:56
77800331,how to find positional embeddings from barttokenizer?,"the objective is to add token embeddings (customized- obtained using different model) and the positional embeddings.
is there a way i can find out positonal embedding along with the token embeddings for an article(length 500-1000 words) using bart model.
tokenized_sequence = tokenizer(sentence, padding='max_length', truncation=true, max_length=512, return_tensors=""pt"")

the output is input_ids and attention_mask but not parameter to return position_ids like in bert model.
bert.embeddings.position_embeddings('your_positions_ids')

or the only way to obtain positional embedding is using sinusoidal positional encoding?","['pytorch', 'nlp', 'huggingface-transformers', 'summarization', 'bart']",77800561,"the tokenizer is not responsible for the embeddings. it only generates the ids to be fed into the embedding layer.
barts embeddings are learned, i.e. the embedding come from their own embedding layer.
you can retrieve both types of embeddings like this. here bart is a bartmodel. the encoding is (roughly) done like this:
embed_pos = bart.encoder.embed_positions(input_ids)
inputs_embeds = bart.encoder.embed_tokens(input_ids)
hidden_states = inputs_embeds + embed_pos

full working code:
from transformers import bartforconditionalgeneration, barttokenizer

bart = bartforconditionalgeneration.from_pretrained(""facebook/bart-base"", forced_bos_token_id=0)
tok = barttokenizer.from_pretrained(""facebook/bart-base"")
example_english_phrase = ""un chief says there is no <mask> in syria""
input_ids = tok(example_english_phrase, return_tensors=""pt"").input_ids

embed_pos = bart.model.encoder.embed_positions(input_ids) * bart.model.encoder.embed_scale # by default the scale is 1.0
inputs_embeds = bart.model.encoder.embed_tokens(input_ids)
hidden_states = inputs_embeds + embed_pos


note that embed_pos is invariant to the actual token ids. only their position matters. ""new"" embeddings are added if the input grows larger without changing the embeddings of the earlier positions:
these cases yield the same embeddings:
embed_positions([0, 1]) == embed_positions([123, 241]) == embed_positions([444, 3453, 9344, 3453])[:2]",https://stackoverflow.com/questions/77800331,pytorch,11-01-2024 13:13,549.0,1.0,1.0,True,15-01-2024 15:00,15-01-2024 15:00
9653815,how to get inflections for a word using wordnet,"i want to get inflectional forms for a word using wordnet.
e.g. if the word is make, then its inflections are 
made, makes, making

i tried all the options of the wn command but i did not get the inflections for a word.
any idea how to get these?","['nlp', 'wordnet']",11633692,"i am not sure wordnet was intended to inflect words. just found this little writeup about how wordnet(r) makes use of the morphy algorithm to make a morphological determination of the head term associated with an inflected form  i needed some inflection for a project of mine (python) a little ago and i used  and  (required some hacking, also take a look at the original",https://stackoverflow.com/questions/9653815,nlp,11-03-2012 09:30,3129.0,12.0,2.0,True,17-10-2022 22:55,27-05-2022 15:59
73027195,merging text and image keras layers not working,"please judge me tender. i'm trying to concatenate two inputs, one for images and one for text.
i'm not an expert and i'm new with the functional api, so it's hard for me to identify the problem here.
in the code below, i confirmed that i can train both text_features and image_features models, but when i try to train the end to end model it retrieves the error:
valueerror: failed to find data adapter that can handle input: (<class 'dict'> containing {""<class 'str'>""} keys and {""<class 'tensorflow.python.data.ops.dataset_ops.prefetchdataset'>""} values), <class 'nonetype'>

i can imagine that i'm facing a rather basic problem, but the thing is that i couldn't find a simple example where both images and text are used, so i can't see where it is.
i will copy the entire code i use and will try to comment on each step so this doesn't becomes a general debugging issue.
import matplotlib.pyplot as plt
import numpy as np
import os
import pil
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import sequential
import re
import string

first, i define a common batch size and seed for both the text and images datasets. both my images and text files are saved in a single set of 25 folders.
lets say folder one has a file called sample_1.png. it also has a file called sample_1.txt, which correspond to the text associated with the said image, stored as a single string (using json).
batch_size = 32
seed = 42

then, i load the text data. here, i try to follow this example:  basic text classification without recurrent layers. the only difference is that my output is not binary.
raw_text_train_ds =tf.keras.utils.text_dataset_from_directory(
    'neural', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='training', 
    seed=seed)
raw_text_val_ds = tf.keras.utils.text_dataset_from_directory(
    'neural', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='validation', 
    seed=seed)

i follow the processing steps of the referenced example, except that i've previously treated my text for punctuation and similar.
max_features = 7000
sequence_length = 250    
vectorize_layer = layers.textvectorization(
    max_tokens=max_features,
    output_mode='int',
    output_sequence_length=sequence_length)
train_text = raw_text_train_ds.map(lambda x, y: x)
vectorize_layer.adapt(train_text)
def vectorize_text(text, label):
  text = tf.expand_dims(text, -1)
  return vectorize_layer(text), label
text_train_ds = raw_text_train_ds.map(vectorize_text)
text_val_ds = raw_text_val_ds.map(vectorize_text)

before applying the autotune part of the mentioned example, i upload the image dataset, trying to follow this example: image classification with augmentation layer
img_height = 180
img_width = 180
img_train_ds = tf.keras.utils.image_dataset_from_directory(
  'neural',
  validation_split=0.2,
  subset=""training"",
  seed=seed,
  image_size=(img_height, img_width),
  batch_size=batch_size)
img_val_ds = tf.keras.utils.image_dataset_from_directory(
  'neural',
  validation_split=0.2,
  subset=""validation"",
  seed=seed,
  image_size=(img_height, img_width),
  batch_size=batch_size)

i wonder if applyoing the following data augmantation layer is causing some sort of missmatch, but i don't think so. again, i'm pretty sure my mistake is more basic than anything else.
data_augmentation = keras.sequential(
  [
    layers.randomrotation(0.04,
                         input_shape=(img_height,
                                      img_width,
                                      3)),
    layers.randomzoom(0.1),
  ]
)

as both referenced examples recommend to apply the folowing autotuning, i do it for both data sets at once.
autotune = tf.data.autotune

text_train_ds = text_train_ds.cache().prefetch(buffer_size=autotune)
text_val_ds = text_val_ds.cache().prefetch(buffer_size=autotune)
# test_ds = test_ds.cache().prefetch(buffer_size=autotune)
img_train_ds = img_train_ds.cache().shuffle(1000).prefetch(buffer_size=autotune)
img_val_ds = img_val_ds.cache().prefetch(buffer_size=autotune)

here i define two models, as they are defined, again, in the examples i tried to follow, but trying to adapt them to the api approach.
num_classes = 25 


text_input = keras.input(shape=(none,), name=""text"")  
text_features = layers.embedding(max_features+1, 16)(text_input)
text_features = layers.dropout(0.2)(text_features)
text_features = layers.globalaveragepooling1d()(text_features)
text_features = layers.dropout(0.2)(text_features)
text_features = layers.dense(32)(text_features)
text_features = keras.model(text_input,text_features)

image_input = keras.input(shape=(180, 180, 3),name=""image"")
image_features=data_augmentation(image_input)
image_features=layers.rescaling(1./255)(image_features)
image_features=layers.conv2d(16, 3, padding='same', activation='relu')(image_features)
image_features=layers.maxpooling2d()(image_features)
image_features=layers.conv2d(32, 3, padding='same', activation='relu')(image_features)
image_features= layers.maxpooling2d()(image_features)
image_features=layers.conv2d(64, 3, padding='same', activation='relu')(image_features)
image_features=layers.maxpooling2d()(image_features)
image_features=layers.dropout(0.2)(image_features)
image_features=layers.flatten()(image_features)
image_features=layers.dense(128, activation='relu')(image_features)
image_features=layers.dense(32, activation='relu')(image_features)
image_features=keras.model(image_input,image_features)

x = layers.concatenate([text_features.output, image_features.output])
category_pred = layers.dense(num_classes, name=""classes"")(x)


model = keras.model(
    inputs=[text_input, image_input],
    outputs=[category_pred],)

i tried with different loss, metrics and optimizers, just to try my way out of the problem.
i feel like it's maybe a semantic problem, as the error suggest (remember, not an expert here) that the model doesn't understands what i'm trying to introduce as an input. but this is how inputs are introduced in the examples i strudied, so i'm lost.
model.compile(optimizer='adam',
              loss=tf.keras.losses.sparsecategoricalcrossentropy(from_logits=true),
              metrics=['accuracy'])
epochs = 1
checkpoint_path = ""training_3_test_ojo/cp.ckpt""
checkpoint_dir = os.path.dirname(checkpoint_path)
model.fit(
    {'image':img_train_ds,
     'text':text_train_ds,
     },
    epochs=epochs,
    batch_size=32,)

the problem could be that i naively thought i can just load my two data sets independently and expect my model would find the way to concatenate it.
i'm not specifying the expected output for trainning, again, assuming that the model will extract it from the inputs. but i tried to specify it and it didn't make any difference. i would also vote for this as the problem of my code. not specifying the expected output works for the 'image classification' example i used, but i do realize it doesn't have to work for a model with multiple inputs.
i will appreciate any solution, guidance or reference.","['python', 'tensorflow', 'concatenation', 'text-classification', 'image-classification']",73029535,"as per the documentation on fit(), if you're passing a dictionary, the keys need to point to an array or tensor. you're using tensorflow.python.data.ops.dataset_ops.prefetchdataset, which won't work with dict.",https://stackoverflow.com/questions/73027195,python,18-07-2022 18:47,450.0,0.0,1.0,True,18-07-2022 23:13,18-07-2022 22:23
68920570,correlating between prediction result to label,"i've a keras model that predicted the following results:
(it's a multi-class problem with 6 possible classes)
[[0.44599777 0.00667355 0.10674711 0.02558559 0.29180232 0.12319366]]
so given the above results the model predicts the 1st class - but i know it's wrong.
i was able to achieve a ~92% accuracy:
epoch 1/10
1128/1128 [==============================] - 18s 15ms/step - loss: 1.3685 - accuracy: 0.4596 - val_loss: 0.6238 - val_accuracy: 0.7785
epoch 2/10
1128/1128 [==============================] - 17s 15ms/step - loss: 0.7200 - accuracy: 0.7373 - val_loss: 0.4055 - val_accuracy: 0.8467
epoch 3/10
1128/1128 [==============================] - 17s 15ms/step - loss: 0.4994 - accuracy: 0.8200 - val_loss: 0.3284 - val_accuracy: 0.8772
epoch 4/10
1128/1128 [==============================] - 17s 15ms/step - loss: 0.3966 - accuracy: 0.8568 - val_loss: 0.3100 - val_accuracy: 0.9043
epoch 5/10
1128/1128 [==============================] - 18s 16ms/step - loss: 0.3428 - accuracy: 0.8810 - val_loss: 0.3044 - val_accuracy: 0.9102
epoch 6/10
1128/1128 [==============================] - 39s 34ms/step - loss: 0.3075 - accuracy: 0.8915 - val_loss: 0.2970 - val_accuracy: 0.9184
epoch 7/10
1128/1128 [==============================] - 25s 22ms/step - loss: 0.2779 - accuracy: 0.9035 - val_loss: 0.3051 - val_accuracy: 0.9226
epoch 8/10
1128/1128 [==============================] - 19s 17ms/step - loss: 0.2663 - accuracy: 0.9069 - val_loss: 0.3207 - val_accuracy: 0.9261
epoch 9/10
1128/1128 [==============================] - 19s 17ms/step - loss: 0.2514 - accuracy: 0.9138 - val_loss: 0.2855 - val_accuracy: 0.9311
epoch 10/10
1128/1128 [==============================] - 20s 18ms/step - loss: 0.2331 - accuracy: 0.9196 - val_loss: 0.3352 - val_accuracy: 0.9263
test loss: 0.33516398072242737
test accuracy: 0.9262799024581909

below is how i'm doing the prediction:
bug_name = '51859'
issue = conn.issue(bug_name, expand='changelog')
candidate_bug = bug(issue, connections_dict)
candidate_bug.extract_all_info()
data = candidate_bug.get_data_as_df()
data = data.drop('group_name', axis='columns')

free_text_tokenized, _ = prepare_free_text_inputs(data, data)

model_inputs = [free_text_tokenized]

res = model.predict(model_inputs)
print(f'expected: {get_group_by_bug_owner(candidate_bug.get_owner())}')
# generate arg maxes for predictions
print(res)
classes = np.argmax(res, axis=1)
print(classes)
print(np.unique(y_train))
class_index = classes[0]
print(np.unique(y_train)[class_index])

and here's the output:
expected: d
[[0.44599777 0.00667355 0.10674711 0.02558559 0.29180232 0.12319366]]
[0]
['a' 'b' 'c' 'd' 'e' 'f']
a

... so i'm afraid my problem is i don't know to ""assign"" those results to the labels.
i've tried multiple attempts (where i know what the prediction should be) and it always misses the expected result.
also - i'm using labelencoder as follows:
    
# prepare target
def prepare_targets(y_train, y_test):
    le = labelencoder()
    le.fit(y_train)
    y_train_enc = le.transform(y_train)
    y_test_enc = le.transform(y_test)
    return y_train_enc, y_test_enc

y_train_enc, y_test_enc = prepare_targets(y_train, y_test)

what am i missing? am i using the wrong the wrong list (y_train)?","['python', 'numpy', 'keras', 'scikit-learn', 'text-classification']",69008665,"answering my own question (for whoever's gonna be introduced by it).
2 issues i've found:

i've (very mistakenly) triggered the transformers on the predicted data (fit_on_text) and it's a big no no! - one's must use the same transformer that was already fitted via the trained data.

the labels are encoded in the labelencoder that was originally used before training the model, so i've created a dict to map each label as follows:


# prepare target
print('preparing lables')
le = labelencoder()
le_name_mapping = {}

le.fit(y_train)
le_name_mapping.update(dict(zip(le.transform(le.classes_), le.classes_)))
print(le_name_mapping)
y_train_enc = le.transform(y_train)
y_test_enc = le.transform(y_test)

later on i've used it on the prediction results:
res = model.predict(model_inputs)
selected_class_index = np.argmax(res, axis=1)[0]
print(selected_class_index)
print(f'actual: {le_name_mapping[selected_class_index]}')",https://stackoverflow.com/questions/68920570,python,25-08-2021 09:47,305.0,0.0,1.0,True,01-09-2021 06:23,25-08-2021 12:42
72287712,what does tfidfvectorizer.transform() actually produce?,"i am new to using tf-idf vectorizer. while running the code i came up with this output but was not able to interpret what it actually means.
code
x=[""access modes govern the type of operations possible in the opened file. it refers to how the file will be used once its opened. these modes also define the location of the file handle in the file."",""file handle is like a cursor, which defines from where the data has to be read or written in the file. there are 6 access modes in python.""]

x = np.array(x)

ans = tfidfvectorizer.transform(x)

print(ans)

**output**

  (0, 247682)   0.34757472043242427

  (0, 235525)   0.11981132543319443

  (0, 232967)   0.27278177118815816

  (0, 165607)   0.6769351735727495

  (1, 247953)   0.2657562514567408

  (1, 232967)   0.2589999033874122

  (1, 230813)   0.28434013277955594

  (1, 202607)   0.22380408029504645

can anyone tell what (0,247682) and (1,247953) mean?","['python', 'scikit-learn', 'tf-idf', 'tfidfvectorizer']",72288948,"firstly there are two sentences in your data set. each word found in these sentences will be assigned a word id.
in (0,247682):
0 is the document id or first sentence,  247682 is the word id, and 0.34757472043242427 is its tf-idf score",https://stackoverflow.com/questions/72287712,python,18-05-2022 10:44,137.0,0.0,1.0,True,18-05-2022 17:43,18-05-2022 17:43
76367425,how to split text into sentences and create a new dataframe with one sentence per row?,"i have a dataframe df that has 3 columns containing speechdata: filename, president, text.
i split the text data into sentences using:
# split 'text' column into sentences and create a new 'sentences' column
df['sentences'] = df['text'].apply(lambda x: nltk.sent_tokenize(x))

however, this code tokenizes the text in such a way, that the split text is still in one row in the 'sentences' column.
i want to create a new dataframe named 'data_sentence' that contains the split sentences and their respective filename, president but with each row containing one sentence.
data_sentence = pd.dataframe(columns=['filename', 'president', 'sentnew'])

# iterate over each row in the original dataframe 'df'
for index, row in df.iterrows():
    filename = row['filename']
    president = row['president']
    sentences = row['sentences']
    
    # create a temporary dataframe for the sentences of the current row
    temp_df = pd.dataframe({'filename': [filename] * len(sentences),
                            'president': [president] * len(sentences),
                            'sentnew': sentences})
    
    # concatenate the temporary dataframe with 'data_sentence'
    data_sentence = pd.concat([data_sentence, temp_df], ignore_index=true)

# print the resulting dataframe 'data_sentence'
print(data_sentence)

this code works but does not assign one sentence to one row.
can someone help out?","['python', 'pandas', 'nltk', 'text-processing']",76367700,"looks like you just need to explode the sentences :
df['sentences'] = df.pop('text').apply(lambda x: nltk.sent_tokenize(x)) # use `df.pop`

data_sentence = df.explode('sentences') # <-- add this line

output :




filename
president
sentences




file1.txt
a
how to split text into sentences and create a new dataframe with one sentence per row using nltk and pandas?


file1.txt
a
i have a dataframe df that has 3 columns containing speechdata: 'filename', 'president', 'text'.




input used :
import nltk
nltk.download(""punkt"")

df = pd.dataframe({
    ""filename"": [""file1.txt""],
    ""president"": [""a""],
    ""text"": [
        ""how to split text into sentences and create a new dataframe ""
        ""with one sentence per row using nltk and pandas? ""
        ""i have a dataframe df that has 3 columns containing ""
        ""speechdata: 'filename', 'president', 'text'."",
    ]
})",https://stackoverflow.com/questions/76367425,python,30-05-2023 17:53,411.0,2.0,1.0,True,31-05-2023 11:05,31-05-2023 11:05
71535153,nlp spacy add special case to recognize currency other than usd (ie. cad),"i am doing a very simple task where i need to extract dollar amounts in a string.
i tried spacy but it only recognize usd 1000 not cad1000.
i tried adding a special case
special_case = [{
        'orth': 'cad', 
        'tag': '$', 
        'is_currency': true}]
nlp.tokenizer.add_special_case('cad', special_case)

with no luck.
example:
doc = nlp('i will pay you 1000 cad tomorrow')
extracted_money = [ent.text for ent in doc.ents if ent.label_ == 'money']

looking for a solution to return me extracted_money =['1000 cad']
any help is appreciated.","['nlp', 'spacy']",71540577,"you can use spacy.matcher.matcher:
import spacy
from spacy.matcher import matcher

nlp = spacy.load(""en_core_web_sm"")
doc = nlp('i will pay you 1000 cad tomorrow')
matcher = matcher(nlp.vocab)  
pattern = [{'is_digit': true}, {'text':'cad'}] # number + cad
matcher.add('cad', [pattern])

matches = matcher(doc)
for match_id, start, end in matches:
    string_id = nlp.vocab.strings[match_id]  # get string representation
    span = doc[start:end]  # the matched span
    print( match_id, string_id, start, end, '->', span.text, '<-')

output:
5189151114763691552 cad 4 6 -> 1000 cad <-

the pattern here is [{'is_digit': true}, {'text':'cad'}]: a number token followed with a cad token.",https://stackoverflow.com/questions/71535153,nlp,19-03-2022 03:31,715.0,1.0,1.0,True,19-03-2022 17:44,19-03-2022 11:51
75854700,how to fine tune a huggingface seq2seq model with a dataset from the hub?,"i want to train the ""flax-community/t5-large-wikisplit"" model with the ""dxiao/requirements-ner-id"" dataset. (just for some experiments)
i think my general procedure is not correct, but i don't know how to go further.
my code:
load tokenizer and model:
from transformers import autotokenizer, automodelforseq2seqlm, automodel
checkpoint = ""flax-community/t5-large-wikisplit""
tokenizer = autotokenizer.from_pretrained(checkpoint)
model = automodelforseq2seqlm.from_pretrained(checkpoint).cuda()

load dataset that i want to train:
from datasets import load_dataset
raw_dataset = load_dataset(""dxiao/requirements-ner-id"")

the raw_dataset looks like this ['id', 'tokens', 'tags', 'ner_tags']
i want to get the sentences as sentence and not as tokens.
def tokentostring(tokenarray):
  string = tokenarray[0]
  for x in tokenarray[1:]:
    string += "" "" + x
  return string

def sentence_function(example):
  return {""sentence"" :  tokentostring(example[""tokens""]),
          ""simplefiedsentence"" : tokentostring(example[""tokens""]).replace(""the"", ""xxxxxxxxxxx"")}

wikisplit_req_set = raw_dataset.map(sentence_function)
wikisplit_req_set

i tried to restructure the dataset such that it looks like the wikisplit dataset:
simple1dataset = wikisplit_req_set.remove_columns(['id', 'tags', 'ner_tags', 'tokens']);
complexdataset = wikisplit_req_set.remove_columns(['id', 'tags', 'ner_tags', 'tokens']);
complexdataset[""train""] = complexdataset[""train""].add_column(""simple_sentence_1"",simple1dataset[""train""][""sentence""]).add_column(""simple_sentence_2"",simple1dataset[""train""][""simplefiedsentence""])
complexdataset[""test""] = complexdataset[""test""].add_column(""simple_sentence_1"",simple1dataset[""test""][""sentence""]).add_column(""simple_sentence_2"",simple1dataset[""test""][""simplefiedsentence""])
complexdataset[""validation""] = complexdataset[""validation""].add_column(""simple_sentence_1"",simple1dataset[""validation""][""sentence""]).add_column(""simple_sentence_2"",simple1dataset[""validation""][""simplefiedsentence""])
trainingdataset = complexdataset.rename_column(""sentence"", ""complex_sentence"")
trainingdataset

tokenize it:
def tokenize_function(example):
    model_inputs = tokenizer(example[""complex_sentence""],truncation=true, padding=true)
    targets1 = tokenizer(example[""simple_sentence_1""],truncation=true, padding=true)
    targets2 = tokenizer(example[""simple_sentence_2""],truncation=true, padding=true)
    model_inputs['simple_sentence_1'] = targets1['input_ids']
    model_inputs['simple_sentence_2'] = targets2['input_ids']
    model_inputs['decoder_input_ids'] = targets2['input_ids']
    return model_inputs

tokenized_datasets = trainingdataset.map(tokenize_function, batched=true)
tokenized_datasets=tokenized_datasets.remove_columns(""complex_sentence"")
tokenized_datasets=tokenized_datasets.remove_columns(""simple_sentence_1"")
tokenized_datasets=tokenized_datasets.remove_columns(""simple_sentence_2"")
tokenized_datasets=tokenized_datasets.remove_columns(""simplefiedsentence"")
tokenized_datasets

dataloader:
from transformers import datacollatorforlanguagemodeling
data_collator = datacollatorforlanguagemodeling(tokenizer, mlm=false)
data_collator

training:
from transformers import seq2seqtrainer, seq2seqtrainingarguments, trainingarguments, evalprediction, datacollatorwithpadding, trainer

bleu = evaluate.load(""bleu"")

training_args = seq2seqtrainingarguments(
  output_dir = ""/"",
  log_level = ""error"",
  num_train_epochs = 0.25,
  learning_rate = 5e-4,
  lr_scheduler_type = ""linear"",
  warmup_steps = 50,
  optim = ""adafactor"",
  weight_decay = 0.01,
  per_device_train_batch_size = 1,
  per_device_eval_batch_size = 1,
  gradient_accumulation_steps = 16,
  evaluation_strategy = ""steps"",
  eval_steps = 50,
  predict_with_generate=true,
  generation_max_length = 128,
  save_steps = 500,
  logging_steps = 10,
  push_to_hub = false,
  auto_find_batch_size=true
)

trainer = seq2seqtrainer(
    model,
    training_args,
    train_dataset=tokenized_datasets[""train""],
    eval_dataset=tokenized_datasets[""validation""],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=bleu,

)
trainer.train()

the problem is, that i do not understand how the model know the expected value and how it calculate its loss. can someone give me some ideas what happens where?
i hope some one can help me understand my own code, because the documentation by hugging face does not help me enough. maybe someone have some codeexamples or something else. i do not completely understand how i fine tune the model and how i get the parameters the model expects to train it. i also do not understand how the training works and what the parameters do.","['python', 'nlp', 'huggingface-transformers', 'huggingface-tokenizers', 'huggingface']",75862077,"tl;dr
take some time to go through  or read the 
after that, you would have answered most of the questions you're having.
show me the code: scroll down the bottom of the answer =)

what is a datasets.dataset and datasets.datasetdict?
tl;dr, basically we want to look through it and give us a dictionary of keys of name of the tensors that the model will consume, and the values are actual tensors so that the models can uses in its .forward() function.
in code, you want the processed dataset to be able to do this:
from datasets import load_dataset

ds = load_dataset(...)
ds.map(func_to_preprocess)

for data in ds:
    model(data)  # does a forward propagation pass.

why can't i just feed the dataset into the model directly?
it's because the individual datasets creators/maintainers are not necessary the ones that create the models.
and keeping them independent makes sense since a dataset can be used by different model and each model requires different datasets to be preprocessed/""munge""/""manipulated"" to the format that it expects (kind of like the extract, transform, load (etl) process in transformers-based models).
unless explicitly preprocessed, most datasets are in raw text (str) and annotation/label format, which usually are of these types:

single token decoder output (single token label),

e.g. language id task [in]: hallo welt and [out]: de
normally uses automodelforsequenceclassification


regression float output

e.g. textual similarity [in]: hello world <sep> foo bar and [out]: 32.12
normally uses automodelforsequenceclassification


free-form autoregressive decoder output (a natural text sentence, i.e. a list of tokens)

e.g. machine translation [in]: hallo welt and [out]: hello world
normally uses automodelforseq2seqlm


fixed tokens decoder output (a list of labels)

e.g. bio anntoations [in]: obama is the president and [out]: ['b-per', 'o', 'o', 'o']
normally uses automodelfortokenclassification



for the dataset you're interested in:
from datasets import load_dataset

raw_dataset = load_dataset(""dxiao/requirements-ner-id"")
raw_dataset['train'][0]

[out]:
{'id': 0,
 'tokens': ['the',
  'operating',
  'humidity',
  'shall',
  'be',
  'between',
  '0.4',
  'and',
  '0.6'],
 'tags': ['o',
  'b-attr',
  'i-attr',
  'o',
  'b-act',
  'b-relop',
  'b-quant',
  'o',
  'b-quant'],
 'ner_tags': [0, 3, 4, 0, 1, 5, 7, 0, 7]}

but the model doesn't understand inputs and outputs, it only understand torch.tensor objects, hence you need to do some processing.
so tokenizers usually expects raw strings, not list of tokens.
normally, a model's tokenizer converts raw strings into a list of token ids,
from transformers import autotokenizer, automodelforseq2seqlm, automodel

model_name = ""flax-community/t5-large-wikisplit""

tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforseq2seqlm.from_pretrained(model_name)

tokenizer([""hello world"", ""foo bar is a sentence"", ""fizz buzz""])

[out]:
{'input_ids': [[21820, 296, 1], [5575, 32, 1207, 19, 3, 9, 7142, 1], [361, 5271, 15886, 1]], 'attention_mask': [[1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1]]}

but my dataset comes pre-tokenized? so what do i do?
sentences = [
['the', 'operating','humidity','shall','be','between','0.4','and','0.6'],
['the', 'cis', 'cnet', 'shall', 'accommodate', 'a', 'bandwidth', 'of', 'at', 'least', '24.0575', 'gbps', 'to', 'the', 'computer', 'room', '.']
]

[tokenizer.convert_tokens_to_ids(sent) for sent in sentences]

[out]:
[[634, 2, 2, 2, 346, 24829, 22776, 232, 22787],
 [634, 21134, 2, 2, 2, 9, 2, 858, 144, 2, 2, 2, 235, 532, 2, 2, 5]]

why are there so many tokens with index 2?
because they are unknowns. if we take a look at the vocab,
>>> tokenizer.convert_tokens_to_ids(tokenizer.unk_token)
2

then how do i encode the tags or new tokens?
here's an example:
from itertools import chain

from transformers import autotokenizer, automodelforseq2seqlm, automodel
from datasets import load_dataset


model_name = ""flax-community/t5-large-wikisplit""

tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforseq2seqlm.from_pretrained(model_name)

raw_dataset = load_dataset(""dxiao/requirements-ner-id"")



# get the ner tags.
tag_set = list(map(str, set(chain(*raw_dataset['train']['tags']))))

# put them into the tokenizer.
tokenizer.add_special_tokens({'additional_special_tokens': tag_set})

train_datset = raw_dataset['train'].map(lambda x: 
    {'input_ids': tokenizer.convert_tokens_to_ids(x['tokens']),
     'labels': tokenizer.convert_tokens_to_ids(x['tags'])}
)


valid_datset = raw_dataset['validation'].map(lambda x: 
    {'input_ids': tokenizer.convert_tokens_to_ids(x['tokens']), 
     'labels': tokenizer.convert_tokens_to_ids(x['tags'])}
)


how to train a seq2seq using the text inputs and the ner labels as the outputs?
tl;dr:
from itertools import chain

from transformers import autotokenizer, automodelforseq2seqlm, automodel
from transformers import seq2seqtrainingarguments, seq2seqtrainer
from datasets import load_dataset
import evaluate

model_name = ""flax-community/t5-large-wikisplit""

tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforseq2seqlm.from_pretrained(model_name)



raw_dataset = load_dataset(""dxiao/requirements-ner-id"")


# get the ner tags.
tag_set = list(map(str, set(chain(*raw_dataset['train']['tags']))))

# put them into the tokenizer.
tokenizer.add_special_tokens({'additional_special_tokens': tag_set})

train_data = raw_dataset['train'].map(lambda x: 
    {'input_ids': tokenizer.convert_tokens_to_ids(x['tokens']),
     'labels': tokenizer.convert_tokens_to_ids(x['tags'])}
)


valid_data = raw_dataset['validation'].map(lambda x: 
    {'input_ids': tokenizer.convert_tokens_to_ids(x['tokens']), 
     'labels': tokenizer.convert_tokens_to_ids(x['tags'])}
)

# set special tokens, not sure if it's needed but adding them for sanity...
model.config.eos_token_id = tokenizer.eos_token_id
model.config.pad_token_id = tokenizer.pad_token_id


mt_metrics = evaluate.combine(
    [""bleu"", ""chrf""], force_prefix=true
)

def compute_metrics(pred):
    labels_ids = pred.label_ids
    pred_ids = pred.predictions
    
    predictions = tokenizer.batch_decode(pred_ids, skip_special_tokens=true)

    labels_ids[labels_ids == -100] = tokenizer.pad_token_id
    references = tokenizer.batch_decode(labels_ids, skip_special_tokens=true)

    outputs = mt_metrics.compute(predictions=predictions,
                             references=references)

    return outputs

training_args = seq2seqtrainingarguments(
    output_dir='./',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    logging_steps=1,
    save_steps=5,
    eval_steps=1,
    max_steps=10,
    evaluation_strategy=""steps"",
    predict_with_generate=true,
    report_to=none,
    metric_for_best_model=""chr_f_score"",
    load_best_model_at_end=true
)

trainer = seq2seqtrainer(
    model=model,
    args=training_args,
    train_dataset=train_data.with_format(""torch""),
    eval_dataset=valid_data.with_format(""torch""),
    compute_metrics=compute_metrics
)

trainer.train()


hey, something seems fishy, when we train an ner, shouldn't we be using automodelfortokenclassification not automodelforseq2seqlm?
yeah, but like many things in life, there's many means to get to the same end. so in this case, you can take the liberty and be creative to do, e.g.




ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½! (wait a minute!) that's not what i want to do!
i guess you don't really want to do ner but the lessons learnt from munging the corpus with additional tokens and the .map functions should help what you need.
why don't you just tell me how to manipulate the datasetdict so that it fits what i need?!
alright, alright. here goes...
first, i guess you would need to clarify in your question what task are you tackling on top of what model and dataset you're using.
from your code, i am guessing you are trying to build a model for

task: text simplification

[in]: this is super long sentence that has lots of no meaning words.
[out]: this is a long-winded sentence.


model: seq2seq

wikisplit"")


dataset: texts from dxiao/requirements-ner-id

[in]: ['the', 'operating','humidity','shall','be',...,]
[out]: 'the humidity is high'
only the input tokens from dxiao/requirements-ner-id are use as input texts, everything else in the dataset is not needed


preprocessing: convert the input into a simplified version

[in]: ['the', 'operating','humidity','shall','be',...,]
[out]: ['the', 'xxxxx', 'humidity', ...]
convert the simplified output and original inputs to input_ids and labels (that the model expects)
lets create a random_xxx function for this purpose.



def random_xxx(tokens):
    # pick out 3 tokens to xxx.
    to_xxx = set(random.sample(range(len(tokens)), 3))
    tokens = []
    for i, tok in enumerate(tokens):
        if i in to_xxx:
            tokens.append('<xxx>')
        else:
            tokens.append(tok)
    return tokens

so how do i do what you listed above? stop stalling, just give me the code...
from itertools import chain
import random

import os
os.environ[""wandb_disabled""] = ""true""

from transformers import autotokenizer, automodelforseq2seqlm, automodel
from transformers import seq2seqtrainingarguments, seq2seqtrainer
from datasets import load_dataset
import evaluate

model_name = ""flax-community/t5-large-wikisplit""

tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforseq2seqlm.from_pretrained(model_name)


def random_xxx(tokens):
    # pick out 3 tokens to xxx.
    to_xxx = set(random.sample(range(len(tokens)), 3))
    tokens = []
    for i, tok in enumerate(tokens):
        if i in to_xxx:
            tokens.append('<xxx>')
        else:
            tokens.append(tok)
    return tokens

raw_dataset = load_dataset(""dxiao/requirements-ner-id"")

# put '<xxx>' into the tokenizer.
tokenizer.add_special_tokens({'additional_special_tokens': ['<xxx>']})

# assuming `input_ids` is ""complex"" original sentence.
# and `labels` is ""simplified"" sentence with xxx

train_data = raw_dataset['train'].map(lambda x: 
    {'input_ids': tokenizer("" "".join(x['tokens']),
                            max_length=40, truncation=true, padding=""max_length"")[""input_ids""],
     'labels': tokenizer("" "".join(random_xxx(x['tokens'])), 
                         max_length=40, truncation=true, padding=""max_length"")[""input_ids""]}
)


valid_data = raw_dataset['validation'].map(lambda x: 
    {'input_ids': tokenizer("" "".join(x['tokens']),
                            max_length=40, truncation=true, padding=""max_length"")[""input_ids""],
     'labels': tokenizer("" "".join(random_xxx(x['tokens'])), 
                         max_length=40, truncation=true, padding=""max_length"")[""input_ids""]}
)
    
# set special tokens, not sure if it's needed but adding them for sanity...
model.config.eos_token_id = tokenizer.eos_token_id
model.config.pad_token_id = tokenizer.pad_token_id


mt_metrics = evaluate.combine(
    [""bleu"", ""chrf""], force_prefix=true
)

def compute_metrics(pred):
    labels_ids = pred.label_ids
    pred_ids = pred.predictions
    
    predictions = tokenizer.batch_decode(pred_ids, skip_special_tokens=true)

    labels_ids[labels_ids == -100] = tokenizer.pad_token_id
    references = tokenizer.batch_decode(labels_ids, skip_special_tokens=true)

    outputs = mt_metrics.compute(predictions=predictions,
                             references=references)

    return outputs

training_args = seq2seqtrainingarguments(
    output_dir='./',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    logging_steps=1,
    save_steps=5,
    eval_steps=1,
    max_steps=10,
    evaluation_strategy=""steps"",
    predict_with_generate=true,
    report_to=none,
    metric_for_best_model=""chr_f_score"",
    load_best_model_at_end=true
)

trainer = seq2seqtrainer(
    model=model,
    args=training_args,
    train_dataset=train_data.with_format(""torch""),
    eval_dataset=valid_data.with_format(""torch""),
    compute_metrics=compute_metrics
)

trainer.train()


here's a few other tutorials that will i find helpful:

take some time to go through  or read the 

it would really help answer most of the questions you'll have and be more confident that you understand what you're typing in code.",https://stackoverflow.com/questions/75854700,python,27-03-2023 10:33,7936.0,5.0,1.0,True,28-03-2023 03:59,28-03-2023 03:59
73117890,textclassification/ extraction from image how to get single text frame and string using core ml from a image,need to mark the rec boxes around string and then to get that string after tapping,"['ios', 'swift', 'text-classification', 'coreml', 'apple-vision']",73117903,"import uikit
import vision


class viewcontroller: uiviewcontroller, imageget {

//mark: outlets

@iboutlet weak var selectbutton: uibutton!

//mark: variables
var obju = utilityclass()
var image:uiimage?
var str:string?
var uibutton : uibutton?
var arraystring = [string]()
var imageview : uiimageview = uiimageview()

//mark: delegate function

func img(image: uiimage) {
    self.image = image
    imageview.image = image
    setup()
}

override func viewdidload() {
    super.viewdidload()
    imageview.isuserinteractionenabled = true
    // do any additional setup after loading the view.
}

//mark: setupui
func setup() {
    let realimg =  resizeimage(image: (imageview.image!) , targetsize:cgsize(width: view.frame.width, height: view.frame.height) )
    self.image = realimg
    self.imageview .image = self.image
    imageview.isuserinteractionenabled = true
    self.imageview.frame = cgrect(x: 0, y: 0, width: realimg.size.width, height: realimg.size.height)
    view.addsubview(imageview)
    guard let cgimg = realimg.cgimage else {return}
    let requesthandler = vnimagerequesthandler(cgimage: cgimg)
    let req = vnrecognizetextrequest(completionhandler: recognizetexthandler)
    req.recognitionlevel = .accurate
    do {
        try requesthandler.perform([req])
    } catch {
        print(""unable to perform the request: \(error)"")
    }
}

//mark: select the image
@ibaction func selectbuttontapped(_ sender: any) {
    obju.delegate = self
    obju.obj = self
    obju.imageget()
}

  func recognizetexthandler(request : vnrequest , error:error?) {
    guard let observation = request.results as? [vnrecognizedtextobservation], error == nil else {
        return
    }
    _ = observation.compactmap({
        $0.topcandidates(1).first?.string
    }).joined(separator: ""/n"")
    
    for subview in imageview.subviews {
        subview.removefromsuperview()
    }
    
    let boundingrect :[cgrect]  = observation.compactmap{
        observation in
        guard let candidate = observation.topcandidates(1).first else {return .zero}
        //find the bounding box observation
        let stringrange = candidate.string.startindex..<candidate.string.endindex
        let boxobservation = try? candidate.boundingbox(for: stringrange)
        let boundingbox = boxobservation?.boundingbox ?? .zero
        str = candidate.string
        self.arraystring.append(str!)
        let rectinimg = vnimagerectfornormalizedrect(boundingbox, int((imageview.frame.size.width)), int((imageview.frame.size.height)))
        let convertedrect = self.getconvertedrect(boundingbox: observation.boundingbox, inimage:image!.size , containedin: (imageview.bounds.size))
        drawboundbox(rect: convertedrect)
        return rectinimg
    }
    print(arraystring)
    print(boundingrect)
    
}
func drawboundbox(rect: cgrect) {
    uibutton = uibutton(type: .custom)
    uibutton?.frame = rect
    uibutton?.layer.bordercolor = uicolor.systempink.cgcolor
    uibutton?.settitle("""", for: .normal)
    uibutton?.layer.borderwidth = 2
    uibutton?.tag = arraystring.count
    imageview.addsubview(uibutton ?? uibutton())
    uibutton?.addtarget(self, action: #selector(pressed(_:)), for: .touchupinside)
}

@objc func pressed(_ sender : uibutton) {
    alert(key: arraystring[sender.tag - 1])
    
}

//mark: convert the normalised bounding rect

func getconvertedrect(boundingbox: cgrect, inimage imagesize: cgsize, containedin containersize: cgsize) -> cgrect {
    
    let rectofimage: cgrect
    
    let imageaspect = imagesize.width / imagesize.height
    let containeraspect = containersize.width / containersize.height
    
    if imageaspect > containeraspect { /// image extends left and right
        let newimagewidth = containersize.height * imageaspect /// the width of the overflowing image
        let newx = -(newimagewidth - containersize.width) / 2
        rectofimage = cgrect(x: newx, y: 0, width: newimagewidth, height: containersize.height)
        
    } else { /// image extends top and bottom
        let newimageheight = containersize.width * (1 / imageaspect) /// the width of the overflowing image
        let newy = -(newimageheight - containersize.height) / 2
        rectofimage = cgrect(x: 0, y: newy, width: containersize.width, height: newimageheight)
    }
    
    let neworiginboundingbox = cgrect(
        x: boundingbox.origin.x,
        y: 1 - boundingbox.origin.y - boundingbox.height,
        width: boundingbox.width,
        height: boundingbox.height
    )
    
    var convertedrect = vnimagerectfornormalizedrect(neworiginboundingbox, int(rectofimage.width), int(rectofimage.height))
    
    /// add the margins
    convertedrect.origin.x += rectofimage.origin.x
    convertedrect.origin.y += rectofimage.origin.y
    
    return convertedrect
    
}

//mark: resize the image accord to device
func resizeimage(image: uiimage, targetsize: cgsize) -> uiimage {
    let size = image.size
    
    let widthratio  = targetsize.width  / image.size.width
    let heightratio = targetsize.height / image.size.height
    
    // figure out what our orientation is, and use that to form the rectangle
    var newsize: cgsize
    if(widthratio > heightratio) {
        newsize = cgsize(width: size.width * heightratio, height: size.height * heightratio)
    } else {
        newsize = cgsize(width: size.width * widthratio,  height: size.height * widthratio)
    }
    
    // this is the rect that we've calculated out and this is what is actually used below
    let rect = cgrect(x: 0, y: 0, width: newsize.width, height: newsize.height)
    
    // actually do the resizing to the rect using the imagecontext stuff
    uigraphicsbeginimagecontextwithoptions(newsize, false, 1.0)
    image.draw(in: rect)
    let newimage = uigraphicsgetimagefromcurrentimagecontext()
    uigraphicsendimagecontext()
    
    return newimage!
}

//mark: popping alert with string
func alert(key:string){
    let alertcontroller = uialertcontroller(title: ""string"", message: key, preferredstyle: .alert)
    let okaction = uialertaction(title: ""ok"", style: .default) {
        (action: uialertaction!) in
        // code in this block will trigger when ok button tapped.
       
    }
    let copyaction = uialertaction(title: ""copy"", style: .default) {
        (action: uialertaction!) in
        uipasteboard.general.string = key
       
    }
    alertcontroller.addaction(copyaction)
    alertcontroller.addaction(okaction)
    self.present(alertcontroller, animated: true, completion: nil)
}

}",https://stackoverflow.com/questions/73117890,ios,26-07-2022 04:53,179.0,0.0,1.0,True,29-07-2024 23:37,29-07-2024 23:37
78685093,alternative to receptive field in transformers and what factors impact it,"i have two transformer networks. one with 3 heads per attention and 15 layers in total and second one with 5 heads per layer and 30 layers in total. given an arbitrary set of documents (2048 tokens per each), how to find out, which network is going to be better to use and is less prone to overfitting?
in computer vision we have concept called: ""receptive field"", that allows us to understand how big or small network we need to use. for instance, if we have cnn with 120 layers and cnn with 70 layers, we can calculate their receptive fields and understand which one is going to perform better on a particular dataset of images.
do you guys have something similar in nlp? how do you understand whether one architecture is more optimal to use versus anotherï¿½ï¿½ï¿½having a set of text documents with unique properties","['nlp', 'huggingface-transformers', 'receptive-field']",78687012,"how do you understand whether one architecture is more optimal to use versus another, having a set of text documents with unique properties?

for modern transformer-based language models (lms), there are some empirical ""scaling laws,"" such as the chinchilla scaling laws (wikipedia), that essentially say that larger (deeper) models with more layers, i.e., with more parameters tend to perform better. so far, most lms seem to roughly follow chinchilla scaling. there is another kind of scaling, which is closer to a ""receptive field"", that i talk about below.

do you guys have something similar in nlp?

kind of. transformer-based lms can be thought to have a ""receptive field"" similar to cnn layers, as the attention mechanism in the transformer operates on a pre-defined ""context window"" or ""context length"", which is the maximum number of tokens the layer can look at (""attend to"") at any given time, similar to a cnn kernel. however, with the introduction of new positional encoding (pe) approaches, such as rotary positional encoding (rope), and modified attention architectures, like sliding window attention (swa), this is not strictly accurate.
scaling in terms of ""context length"" is of much interest, but usually, it is very difficult to scale transformers this way, because of attention being a ($\mathcal{o}(n^2)$) (o(n^2)) operation. so, usually, researchers go towards deeper architectures with more parameters (""over-parameterization"") that can allow the model to ""memorize"" as much of the large training corpus as it can (""overfitting""), so that it can perform reasonably well, when fine-tuned for most down-stream tasks (that have at least some representative examples in the training corpus).",https://stackoverflow.com/questions/78685093,nlp,29-06-2024 04:58,147.0,1.0,1.0,True,29-06-2024 20:38,29-06-2024 16:34
76308600,create an unknown label for spacy when returning list of text and label,"i'm trying to create a condition statement for a function that will return the text and label for a passed list.
here's the code:
def get_label(text: list):
    doc = nlp('. '.join(text) + '.')
    keywords = []
    for ent in doc.ents:
        keywords.append((ent.text, ent.label_))
    return keywords

the input is:
['kaggle', 'google', 'san francisco', 'this week', 'as early as tomorrow', 'kag-ingle', 'about half a million', 'ben hamner', '2010', 'earlier this month', 'youtube', 'google cloud platform', 'crunchbase', '$12.5 to $13 million', 'index ventures', 'sv angel', 'hal varian', 'khosla ventures', 'yuri milner']

the output is:
[('google', 'org'), ('san francisco', 'gpe'), ('this week', 'date'), ('as early as tomorrow', 'date'), ('kag-ingle', 'person'), ('about half a million', 'cardinal'), ('ben hamner', 'person'), ('2010', 'date'), ('earlier this month', 'date'), ('google cloud platform', 'org'), ('crunchbase', 'org'), ('$12.5 to $13 million', 'money'), ('index ventures', 'org'), ('hal varian', 'person'), ('khosla ventures', 'org'), ('yuri milner', 'person')]

however, the output should include the entities that were not labelled, assigning them the ""unknown"" label like this:
[('kaggle', 'unknown'), ('google', 'org'), ('san francisco', 'gpe'), ('this week', 'date'), ('as early as tomorrow', 'date'), ('kag-ingle', 'person'), ('about half a million', 'cardinal'), ('ben hamner', 'person'), ('2010', 'date'), ('earlier this month', 'date'), ('youtube', 'unknown'), ('google cloud platform', 'org'), ('crunchbase', 'org'), ('$12.5 to $13 million', 'money'), ('index ventures', 'org'), ('hal varian', 'person'), ('khosla ventures', 'org'), ('yuri milner', 'person')]

i've tried using:
for token in doc.sents:
       keywords.append((token.text, token.label_))

which returns:
[('kaggle.', ''), ('google.', ''), ('san francisco.', ''), ('this week.', ''), ('as early as tomorrow.', ''), ('kag-ingle.', ''), ('about half a million.', ''), ('ben hamner. 2010.', ''), ('earlier this month.', ''), ('youtube.', ''), ('google cloud platform.', ''), ('crunchbase.', ''), ('$12.5 to $13 million.', ''), ('index ventures.', ''), ('sv angel.', ''), ('hal varian.', ''), ('khosla ventures.', ''), ('yuri milner.', '')]

this is (assuming) because there is a period at the end of each token preventing any label from returning.
if anyone has an idea of how i can fix this, i'd really appreciate the help.","['python', 'nlp', 'spacy', 'named-entity-recognition']",76311784,"iterate over the items passed in and check whether they match one of the returned entities after spacy has performed the labelling (see solution below).
notes:

the output labels vary depending on the spacy version and pipeline/pipeline version being used. i used spacy 3.5.3 and the en_core_web_trf==3.5.0 pipeline to produce the following results.
spacy returned ""bill hamner"" as ""bill hamner."" as the labelled entity, hence the extra condition in the if statement to check for these edge cases.

solution
import spacy

txt = ['kaggle', 'google', 'san francisco', 'this week', 'as early as tomorrow', 'kag-ingle', 'about half a million', 'ben hamner', '2010', 'earlier this month', 'youtube', 'google cloud platform', 'crunchbase', '$12.5 to $13 million', 'index ventures', 'sv angel', 'hal varian', 'khosla ventures', 'yuri milner']

nlp = spacy.load(""en_core_web_trf"")


def get_label(text: list):
    doc = nlp("". "".join(text) + ""."")
    keywords = []
    for item in text:
        found_label = false
        for ent in doc.ents:
            if item == ent.text or (ent.text[-1] == ""."" and item == ent.text[:-1]):
                found_label = true
                keywords.append((item, ent.label_))
                break
        if not found_label:
            keywords.append((item, ""unknown""))
    return keywords


for kw in get_label(txt):
    print(kw)

output:
('kaggle', 'unknown')
('google', 'org')
('san francisco', 'gpe')
('this week', 'date')
('as early as tomorrow', 'date')
('kag-ingle', 'unknown')
('about half a million', 'cardinal')
('ben hamner', 'person')
('2010', 'date')
('earlier this month', 'date')
('youtube', 'org')
('google cloud platform', 'unknown')
('crunchbase', 'org')
('$12.5 to $13 million', 'money')
('index ventures', 'org')
('sv angel', 'unknown')
('hal varian', 'person')
('khosla ventures', 'org')
('yuri milner', 'person')

some premature optimization for the get_label function which may be faster if dealing with very large documents returned by the spacy pipline (i.e. a very large tuple of labelled entities for doc.ents). i'll leave it up to you to time the difference to see if its worth using this variation in your end-application:
def get_label(text: list):
    doc = nlp("". "".join(text) + ""."")
    ents = list(doc.ents)
    keywords = []
    for item in text:
        found_label = false
        for idx, ent in enumerate(ents):
            if item == ent.text or (ent.text[-1] == ""."" and item == ent.text[:-1]):
                found_label = true
                keywords.append((item, ent.label_))
                ents.pop(idx)  # reduce size of list to make subsequent searches faster
                break
        if not found_label:
            keywords.append((item, ""unknown""))
    return keywords",https://stackoverflow.com/questions/76308600,python,22-05-2023 17:23,237.0,1.0,1.0,True,23-05-2023 06:29,23-05-2023 06:29
75718913,"openai gpt-3 api: why do i get different, non-related random responses to the same question every time?","i am using the ï¿½ï¿½ï¿½text-davinci-003ï¿½ï¿½ï¿½ model and i copied the code form the openai playground, but the bot keeps giving me random response to a simple ï¿½ï¿½ï¿½helloï¿½ï¿½ï¿½ everytime.
this is the code i am using :
response: dict = openai.completion.create(model=""text-davinci-003"",
                                                    prompt=prompt,
                                                    temperature=0.9,
                                                    max_tokens=150,
                                                    top_p=1,
                                                    frequency_penalty=0,
                                                    presence_penalty=0.6,
                                                    stop=["" human:"", "" ai:""])
        choices: dict = response.get('choices')[0]
        text = choices.get('text')
        print(text)

the response to simple ï¿½ï¿½ï¿½helloï¿½ï¿½ï¿½ chat 3 differe time it gave me a hello world program for java

second time it answered correctly - ï¿½ï¿½ï¿½hi there! how can i help you today?ï¿½ï¿½ï¿½

third time:
  def my_method
        puts ""hello""
     end
   end
 end

# to invoke this method we would call:
mymodule::myclass.my_method



i just dont get it, as using the same simple ï¿½ï¿½ï¿½helloï¿½ï¿½ï¿½ prompt in the openai's playground gives me accurate response eveytime - 'hi there! how can","['python', 'chatbot', 'openai-api', 'gpt-3']",75719777,"as stated in the official openai documentation:

the temperature and top_p settings control how deterministic the model
is in generating a response. if you're asking it for a response where
there's only one right answer, then you'd want to set these lower. if
you're looking for more diverse responses, then you might want to set
them higher. the number one mistake people use with these settings is
assuming that they're ""cleverness"" or ""creativity"" controls.

change this...
temperature = 0.9

...to this.
temperature = 0",https://stackoverflow.com/questions/75718913,python,13-03-2023 07:12,2197.0,0.0,1.0,True,13-03-2023 14:55,13-03-2023 14:55
74533266,how to merge common strings with different values between parenthesis in python,"i am processing some strings within lists that look like these:
['color includes (40)', 'long_description contains (""black"")', 'color includes (38)']
['color includes (30,31,32,33,56,74,84,85,93,99,184,800,823,830,833,838,839)', 'color includes (30,31,32,33,56,74,84,85,93,99,184,409,800,823,830,833,838,839)', 'color includes (800)']
thing is, i want to merge similar strings with their values into one, for each list. expecting something like this:
['color includes (40,38)', 'long_description contains (""black"")']
['color includes (30,31,32,33,56,74,84,85,93,99,184,409,800,823,830,833,838,839)']
and some strings may have values without ():
['family equals 1145']
what could be the more pythonic and fastest (lazy :p) way of doing this?
i have tried using regex to match strings until a ""("" appears, but some strings don't have values between (), and can't find a fitting solution.
i have also tried stree function from suffix_trees lib, which finds the lcs (longest common subsequence) from a list of strings, but then ran out of ideas about handling the values and the closing parenthesis:
from suffix_trees import stree
st = stree.stree(['color includes(30,31,32,33,56,74,84,85,93,99,184,800,823,830,833,838,839)', 
'color includes(30,31,32,33,56,74,84,85,93,99,184,409,800,823,830,833,838,839)', 'color includes (800)'])
st.lcs()

out: 'color includes ('

edit: solved
as @stef in the answer said, i broke the problem in smaller pieces and i solved it with his help. let me paste here the class rule_process and the result:

class rule_process:

    def __init__(self):
        self.rules = '(color includes (40)) or (long_description contains (""black"")):1|||color includes (30,31,32,33,56,74,84,85,93,99,184,800,823,830,833,838,839):0|||color includes (30,31,32,33,56,74,84,85,93,99,184,409,800,823,830,833,838,839):0|||color includes (40):1|||color includes (800):0'
        
        self.rules_dict = {
            0:none,
            1:none,
            2:none,
            4:none,
        }
     
    def append_rules(self):

        rules = self.rules.split(""|||"")

        values_0 = []
        values_1 = []
        values_2 = []
        values_4 = []

        for rule in range(len(rules)):

            if rules[rule][-1]=='0':
                rules[rule] = rules[rule][:-2]
                # self.rules_dict[0].append(rules[rule])
                values_0.append(rules[rule])

            elif rules[rule][-1]=='1':
                rules[rule] = rules[rule][:-2]
                # self.rules_dict[1].append(rules[rule])
                values_1.append(rules[rule])

            elif rules[rule][-1]=='2':
                rules[rule] = rules[rule][:-2]
                # self.rules_dict[2].append(rules[rule])
                values_2.append(rules[rule])

            elif rules[rule][-1]=='4':
                rules[rule] = rules[rule][:-2]
                # self.rules_dict[4].append(rules[rule])
                values_4.append(rules[rule])

        if values_0!=[]:
            self.rules_dict[0] = values_0
        if values_1!=[]:
            self.rules_dict[1] = values_1
        if values_2!=[]:
            self.rules_dict[2] = values_2
        if values_4!=[]:
            self.rules_dict[4] = values_4
        
        regex = r'^\('

        # for rules in self.rules_dict.values():
        for key in self.rules_dict.keys():

            if self.rules_dict[key] is not none:
                for rule in range(len(self.rules_dict[key])):
                    
                    new_rule = self.rules_dict[key][rule].split(' or ')

                    if len(new_rule)>1:
                        joined_rule = []
                        for r in new_rule:
                            r = r.replace(""))"","")"")
                            r = re.sub(regex, """", r)
                            joined_rule.append(r)
                        
                        self.rules_dict[key].remove(self.rules_dict[key][rule])
                        self.rules_dict[key].extend(joined_rule)
                        self.rules_dict[key] = list(set(self.rules_dict[key]))

                    else:
                        new_rule = [r.replace(""))"","")"") for r in new_rule]
                        new_rule = [re.sub(regex, """", r) for r in new_rule]
                    
                        new_rule = "", "".join(new_rule)

                        self.rules_dict[key][rule] = new_rule
                        self.rules_dict[key] = list(set(self.rules_dict[key]))

        return self.rules_dict

    
    def split_rule(self):   # color includes (30,31,32,33) -> name = 'color includes', values = [30,31,32,33]
                            # long_description contains (""black"") -> name = long_description, values ='""black""'
        
        new_dict = {
            0:none,
            1:none,
            2:none,
            4:none,
        }
        
        for key in self.rules_dict.keys():

            pql_dict = {}

            if self.rules_dict[key] is not none:
                for rule in range(len(self.rules_dict[key])): #self.rules_dict[key][rule] -> color includes (30,31,32,33,56,74,84,85,93,99,184,800,823,830,833,838,839)

                    rule = self.rules_dict[key][rule]

                    name = rule.rsplit(maxsplit=1)[0]  #------------------------------->color includes
                    values_as_str = rule.rsplit(maxsplit=1)[1].replace(""("","""")
                    values_as_str = values_as_str.replace("")"","""") #-------------------------------> 30,31,32,33,56,74,84,85,93,99,184,800,823,830,833,838,839
                    
                    try:
                        values = list(map(int, values_as_str.split("",""))) # [30,31,32,33,56,74,84,85,93,99,184,800,823,830,833,838,839]
                    except:
                        values = values_as_str # '""black""'
            
                    if name in pql_dict.keys():
                        pql_dict[name] = pql_dict[name] + (values)
                        pql_dict[name] = list(set(pql_dict[name])) 
                        
                    else:
                        pql_dict.setdefault(name, values)

            # pql_dict = {'color includes': [32, 33, 800, 99, 833, 838, 839, 74, 84, 85, 30, 823, 184, 409, 56, 93, 830, 31]}
                for name in pql_dict.keys():
                    
                    values = pql_dict[name]
                    joined_rule = name + "" "" + str(values)
                    
                    if new_dict[key] is not none:
                        new_dict[key] = new_dict[key] + [joined_rule]
                    else:
                        new_dict[key] = [joined_rule]
                   
        self.rules_dict = new_dict

and the result:

process = rule_process()
process.append_rules()
process.split_rule()
process.rules_dict

out:

{0: ['color includes [32, 33, 800, 99, 833, 838, 839, 74, 84, 85, 30, 823, 184, 409, 56, 93, 830, 31]'],
 1: ['color includes [40]', 'long_description contains ""black""'],
 2: none,
 4: none}","['python', 'nlp', 'substring', 'lcs']",74533988,"split this task into smaller, simpler tasks.
first task:
write a function that takes a string and returns a pair (name, list_of_values) where name is the first part of the string and list_of_values is a python list of integers.
hint: you can use '(' in s to test whether string s contains an opening parenthesis; you can use s.split() to split on whitespace or s.rsplit(maxsplit=1) to only split on the last whitespace; s.split('(') to split on opening parenthesis; and s.split(',') to split on comma.
second task:
write a function that takes a list of pairs (name, list_of_values) and merges the lists when the names are equal.
hint: this is extremely easy in python using a dict with name as key and list_of_values as value. you can use if name in d: ... else: to test whether a name is already in the dict or not; or you can use d.get(name, []) or d.setdefault(name, []) to automatically add a name: [] entry in the dict when name is not already in the dict.
third task:
write a function to convert back, from the pairs (name, list_of_values) to the strings ""name (value1, value2, ...)"". this task is easier than the first task, so i suggest doing it first.
hint: ' '.join(...) and ','.join(...) can both be useful.",https://stackoverflow.com/questions/74533266,python,22-11-2022 13:17,61.0,1.0,1.0,True,22-11-2022 16:05,22-11-2022 16:05
14489309,convert words between verb/noun/adjective forms,"i would like a python library function that translates/converts across different parts of speech. sometimes it should output multiple words (e.g. ""coder"" and ""code"" are both nouns from the verb ""to code"", one's the subject the other's the object)
# :: string => list of string
print verbify('writer') # => ['write']
print nounize('written') # => ['writer']
print adjectivate('write') # => ['written']

i mostly care about verbs <=> nouns, for a note taking program i want to write. i.e. i can write ""caffeine antagonizes a1"" or ""caffeine is an a1 antagonist"" and with some nlp it can figure out they mean the same thing. (i know that's not easy, and that it will take nlp that parses and doesn't just tag, but i want to hack up a prototype).
similar questions ...
converting adjectives and adverbs to their noun forms
(this answer only stems down to the root pos. i want to go between pos.)
ps called conversion in linguistics","['python', 'nlp', 'nltk', 'wordnet']",16752477,"this is more a heuristic approach. i have just coded it so appologies for the style. it uses the derivationally_related_forms() from wordnet. i have implemented nounify. i guess verbify works analogous. from what i've tested works pretty well:
from nltk.corpus import wordnet as wn

def nounify(verb_word):
    """""" transform a verb to the closest noun: die -> death """"""
    verb_synsets = wn.synsets(verb_word, pos=""v"")

    # word not found
    if not verb_synsets:
        return []

    # get all verb lemmas of the word
    verb_lemmas = [l for s in verb_synsets \
                   for l in s.lemmas if s.name.split('.')[1] == 'v']

    # get related forms
    derivationally_related_forms = [(l, l.derivationally_related_forms()) \
                                    for l in    verb_lemmas]

    # filter only the nouns
    related_noun_lemmas = [l for drf in derivationally_related_forms \
                           for l in drf[1] if l.synset.name.split('.')[1] == 'n']

    # extract the words from the lemmas
    words = [l.name for l in related_noun_lemmas]
    len_words = len(words)

    # build the result in the form of a list containing tuples (word, probability)
    result = [(w, float(words.count(w))/len_words) for w in set(words)]
    result.sort(key=lambda w: -w[1])

    # return all the possibilities sorted by probability
    return result",https://stackoverflow.com/questions/14489309,python,23-01-2013 21:01,29451.0,36.0,5.0,True,09-11-2022 16:22,23-05-2017 12:17
71984818,what does `i` in `token.i+1` mean when using a token returned by spacy&#39;s language?,"from spacy.language import language

@language.component(""customb"")
def set_custom_boundaries(doc):
    for token in doc[:-1]:
        if token.text == ';':
            doc[token.i+1].is_sent_start = true
    return doc
nlp.add_pipe(""customb"",before=""parser"")

all i need to know is what does i+1 do in this code:
doc[token.i+1]

knowing that i is not defined in the function, neither as an index nor as a simple variable.","['python', 'spacy']",71984847,"i is not a seperate variable. it is an attribute of token. and notice that it is not i+1 but instead it is token.i + 1. i is from the token object. first python gets the value of i from token then it increases it by one.
consider the example below:
class x:
    i = 10

token = x()

print(token.i+1) # it is in fact (token.i) + 1 so result is: 11

if you have any question please ask.",https://stackoverflow.com/questions/71984818,python,24-04-2022 01:27,448.0,-1.0,2.0,True,24-04-2022 01:39,24-04-2022 01:39
69097388,plot more than 3 words per topic for stm?,"is it possible to plot more than 3 words for each topic using the stm package?
topic.count <- 10
model.stm <- stm(stm_1$documents, stm_1$vocab, k = topic.count, data = stm_1$meta, 
init.type = ""spectral"") 

with ""plot, type = ""summary"", i get 3 words per topic in a plot
plot(model.stm, type = ""summary"", text.cex = 1)

with ""labeltopics"", i get 7 words per topic under ""highest prob"" as a list
labeltopics(model.stm)

is there a way to plot 4 or 5 words instead of having the default 3 like the below code?
plot(model.stm, type = ""summary"", text.cex = 1)","['r', 'plot', 'nlp']",69101346,"yes - by setting n.  example:
library(""stm"")
## stm v1.3.6 successfully loaded. see ?stm for help. 
##  papers, resources, and other materials at structuraltopicmodel.com

plot(gadarianfit, type = ""summary"", text.cex = 0.9, n = 5)",https://stackoverflow.com/questions/69097388,r,08-09-2021 05:23,303.0,1.0,1.0,True,10-09-2021 09:12,10-09-2021 09:12
21063206,information gain calculation for a text file?,"i'm working on ""text categorization using information gain,pca and genetic algorithm"" but after performing preprocessing(stemming, stopword removal, tfidf) on the document m confused how to move ahead for information gain part.
my out file contain word and there tfidf value.
like
word - tfidf value
together(word) - 0.235(tfidf value)
come(word) - 0.2548(tfidf value)
when using weka for information gain (""infogainattributeeval.java"") it require .arff file format as input.
is there any to convert text file into .arff format.
or any other way to preform information gain other than weka?
is there any other open source for calculating information gain for document ?","['java', 'data-mining', 'information-retrieval', 'text-mining']",21453103,"i found my answer.
in this we have to generate arff file.
in .arff file
@relation section will contain all words present in your whole document after preprocessing .each word will be of type real because tfidf value is a real value.
@data section will contain their tfidf value calculated during preprocessing.
for example first will contain tfidf value all words present in first document an at last  colunm the document categary.
@relation filename
@attribute word1 real
@attribute word2 real
@attribute word3 real
.
.
.
.so on
@attribute class {cacm,cisi,cran,med}

@data
0.5545479562,0.27,0.554544479562,0.4479562,cacm
0.5545479562,0.27,0.554544479562,0.4479562,cacm
0.55454479562,0.1619617,0.579562,0.5542,cisi
0.5545479562,0.27,0.554544479562,0.4479562,cisi
0.0,0.2396113617,0.44479562,0.2,cran
0.5545479562,0.27,0.554544479562,0.4479562,carn
0.5545177444479562,0.26196113617,0.0,0.0,med
0.5545479562,0.27,0.554544479562,0.4479562,med

after you generate this file you can give this file as input to infogainattributeeval.java. and this working for me.",https://stackoverflow.com/questions/21063206,java,11-01-2014 14:05,2209.0,2.0,2.0,True,08-01-2022 13:20,30-01-2014 10:00
73232595,huggingface trainer load_best_model f1 score vs. loss and overfitting,"i have trained a roberta-large and specified load_best_model_at_end=true and metric_for_best_model=f1. during training, i can see overfitting after the 6th epoch, which is the sweetspot. in epoch 8, which is the next one to evaluate due to gradient accumulation, we can see that train loss decreases and eval_loss increases. thus, overfitting starts. the transformers trainer in the end loads the model from epoch 8, checkpoint -14928, as the f1 score is a bit highea. i was wondering, in theory, wouldn't be the model from epoch 6 be better suited, as it did not overfit? or does one really go for the f1 metric here even though the model did overfit? (the eval loss decreased in epochs <6 constantly).
the test_loss from the second checkpoint, which is then loaded as the ""best"", is 0.128. is it possible to lower that using the first checkpoint which should be the better model anyway?
checkpoint-11196:
{'loss': 0.0638, 'learning_rate': 8.666799323450404e-06, 'epoch': 6.0}

{'eval_loss': 0.09599845856428146, 'eval_accuracy': 0.9749235986101227, 'eval_precision': 0.9648319293367138, 'eval_recall': 0.9858766505097777, 'eval_f1': 0.9752407721241682, 'eval_runtime': 282.2294, 'eval_samples_per_second': 84.637, 'eval_steps_per_second': 2.647, 'epoch': 6.0}

vs.

checkpoint-14928:
{'loss': 0.0312, 'learning_rate': 7.4291115311909265e-06, 'epoch': 8.0}

{'eval_loss': 0.12377820163965225, 'eval_accuracy': 0.976305103194206, 'eval_precision': 0.9719324391455539, 'eval_recall': 0.9810295838208257, 'eval_f1': 0.9764598236566295, 'eval_runtime': 276.7619, 'eval_samples_per_second': 86.309, 'eval_steps_per_second': 2.699, 'epoch': 8.0}","['machine-learning', 'optimization', 'nlp', 'pytorch', 'huggingface-transformers']",73234634,"you could just comment the metric_for_best_model='f1' part out and see for yourself, loss is the default setting. or, utilize from_pretrained('path/to/checkpoint') to compare two checkpoints back to back. f-score is threshold sensitive, so it's entirely possible for a lower loss checkpoint to be better in the end (assuming you do optimize the threshold).",https://stackoverflow.com/questions/73232595,machine-learning,04-08-2022 08:20,5461.0,2.0,1.0,True,04-08-2022 10:51,04-08-2022 09:08
73279102,custom name entity regognition,"i have the following sentence:
text=""the weather is extremely severe in england""

i want to perform a custom name entity recognition (ner) procedure
first a normal ner procedure will output england with a gpe label
pip install spacy

!python -m spacy download en_core_web_lg

import spacy
nlp = spacy.load('en_core_web_lg')

doc = nlp(text)

for ent in doc.ents:
    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))

result: england - gpe - countries, cities, states

however, i want the whole sentence to take the tag high-severity.
so i am doing the following procedure:
from spacy.strings import stringstore

new_hash = stringstore([u'high_severity']) # <-- match id
nlp.vocab.strings.add('high_severity')

from spacy.tokens import span

# get the hash value of the org entity label
high_severity = doc.vocab.strings[u'high_severity']  

# create a span for the new entity
new_ent = span(doc, 0, 7, label=high_severity)

# add the entity to the existing doc object
doc.ents = list(doc.ents) + [new_ent]

i am taking the following error:
valueerror: [e1010] unable to set entity information for token 6 which is included in more than one span in entities, blocked, missing or outside.

from my understanding, this is happening because ner has already recognised england as gre and cannot add a label over the existing label.
i tried to execute the custom ner code (i.e, without first running the normal ner code) but this did not solve my problem.
any ideas on how to solve this problem?","['python', 'python-3.x', 'nlp', 'spacy', 'named-entity-recognition']",73324933,"indeed it looks like ner do not allow overlapping, and that is your problem, your second part of the code tries to create a ner containing another ner, hence, it fails.
see in:

and therefore spacy has spans categorization.
i did not find yet the way to characterized a predefined span (not coming from a trained model)",https://stackoverflow.com/questions/73279102,python,08-08-2022 13:59,240.0,3.0,1.0,True,24-04-2024 23:14,08-08-2022 14:58
79286365,langchain chatollama always produces invalid format: expected &quot;json&quot; error,"just trying to follow a basic langchain tutorial: 
super simple code:
from langchain_ollama import chatollama
import logging

logging.basicconfig(
    level=logging.debug,
    format='%(asctime)s.%(msecs)03d [%(levelname)s]: %(message)s',
    datefmt='%h:%m:%s'
)

logging.info(""### starting up"")

llm = chatollama(
    model=""llama3.1"",
)

response_message = llm.invoke(
    ""what are you?""
)

print(response_message.content)

no matter what i try i always get this error:
exception has occurred: responseerror
invalid format: expected ""json"" or a json schema
  file ""c:\xxxx\local_rag\main.py"", line 16, in <module>
    response_message = llm.invoke(
                       ^^^^^^^^^^^
ollama._types.responseerror: invalid format: expected ""json"" or a json schema

tried a few different approaches including messages[], prompttemplate, streaming etc. from  but always getting the same error.
no issues going via the rest api i.e.
curl  -d '{
  ""model"": ""llama3.1"",
  ""messages"": [
    { ""role"": ""user"", ""content"": ""why is the sky blue?"" }
  ]
}'

any help would be appreciated. really hope i am just doing something daft here.
edit:
the version of ollama makes a difference but also setting llm.format = none works as suggested in the comment.","['python', 'langchain', 'ollama']",79286471,"i am seeing the same thing after upgrading ollama. i just opened an issue here  for now it should work if you downgrade your version of ollama.
edit:
this was fixed as of",https://stackoverflow.com/questions/79286365,python,16-12-2024 23:47,818.0,1.0,1.0,True,17-12-2024 11:09,17-12-2024 08:30
67361527,what versions of spacy suport en_vectors_web_lg?,"i am trying to download en_vectors_web_lg, but keep getting the below error:
error: could not install requirement en-vectors-web-lg==3.0.0 from  because of http error 404 client error: not found for url:  for url 


is spacy still supporting en_vectors_web_lg?
i also just updated my spacy to the latest version","['python', 'spacy']",67364487,"the naming conventions changed in v3 and the equivalent model is en_core_web_lg. it includes vectors and you can install it like this:
spacy download en_core_web_lg

i would not recommend downgrading to use the old vectors model unless you need to run old code.
if you are concerned about accuracy and have a decent gpu the transformers model, en_core_web_trf, is also worth considering, though it doesn't include word vectors.",https://stackoverflow.com/questions/67361527,python,02-05-2021 22:07,2705.0,1.0,2.0,True,13-07-2021 06:35,13-07-2021 06:35
680907,is there a free library for morphological analysis of the german language?,"i'm looking for a library which can perform a morphological analysis on german words, i.e. it converts any word into its root form and providing meta information about the analysed word.
for example:
gegessen -> essen
wurde [...] gefasst -> fassen
hï¿½ï¿½user -> haus
hunde -> hund

my wishlist:

it has to work with both nouns and verbs.
i'm aware that this is a very hard task given the complexity of the german language, so i'm also looking for libaries which provide only approximations or may only be 80% accurate.
i'd prefer libraries which don't work with dictionaries, but again i'm open to compromise given the cirumstances.
i'd also prefer c/c++/delphi windows libraries, because that would make them easier to integrate but .net, java, ... will also do.
it has to be a free library. (l)gpl, mpl, ...

edit: i'm aware that there is no way to perform a morphological analysis wit any dictionary at all, because of the irregular words. 
when i say, i prefer a library without a dictionary i mean those full blown dictionaries which map each and every word:
arbeite -> arbeiten
arbeitest -> arbeiten
arbeitet -> arbeiten
arbeitete -> arbeiten
arbeitetest -> arbeiten
arbeiteten -> arbeiten
arbeitetet -> arbeiten
gearbeitet -> arbeiten
arbeite -> arbeiten
... 

those dictionaries have several drawbacks, including the huge size and the inability to process unknown words.
of course all exceptions can only be handled with a dictionary:
esse -> essen
isst -> essen
eï¿½ï¿½t -> essen
aï¿½ï¿½ -> essen
aï¿½ï¿½t -> essen
aï¿½ï¿½en -> essen
...

(my mind is spinning rig","nlp, languagetool, morphological-analysis",681122,"i think you are looking for a ""stemming algorithm"".
martin porter's approach is well known among linguists. the porter stemmer is basically an affix stripping algorithm, combined with a few substitution rules for those special cases.
most stemmers deliver stems that are linguistically ""incorrect"". for example: both ""beautiful"" and ""beauty"" can result in the stem ""beauti"", which, of course, is not a real word. this doesn't matter, though, if you're using those stems to improve search results in information retrieval systems. lucene comes with support for the porter stemmer, for instance.
porter also devised a simple programming language for developing stemmers, called snowball.
there are also stemmers for german available in snowball. a c version, generated from the snowball source, is also available on the website, along with a plain text explanation of the algorithm.
here's the german stemmer in snowball: 
if you're looking for the corresponding stem of a word as you would find it in a dictionary, along with information on the part of speech, you should google for ""lemmatization"".",https://stackoverflow.com/q/680907,"nlp, languagetool, morphological-analysis",25-03-2009 09:51,4148.0,9.0,8.0,True,09-10-2024 21:08,09-10-2024 21:08
60964785,how to expose spacy as a rest api?,"i am interested in using the spacy python library for my own open source project. what i am searching for is a rest-based api. what is necessary or what is the recommended way to expose the spacy api via a common rest interface? i already took a look into the spacy services and the spacy-api-docker project form jgontrum. but it seems there is no offical rest api available and everyone have to do it by himself. if so, what is the best way to wrap a python spacy method/script into a rest api? there seems to be frameworks like falcon, hug and flask to help me in doing this.
but is it the recommended approach to write my own rest api server with one of these frameworks or is there something i have overseen and spacy is already available via a rest api interface?","['python', 'rest', 'spacy']",60974229,"spacy is not deeply tied to any framework, so you can choose your favorite and use it.

another option you might consider is fastapi. for example, here's a simple spacy entity recognition api:
from fastapi import fastapi
from pydantic import basemodel
import spacy

nlp_en = spacy.load(""en_core_web_sm"")
app = fastapi()


class data(basemodel):
    text: str


@app.post(""/text/"")
def extract_entities(data: data, lang: str):
    doc_en = nlp_en(data.text)
    ents = []
    for ent in doc_en.ents:
        ents.append({""text"": ent.text, ""label_"": ent.label_})
    return {""message"": data.text, ""lang"": lang, ""ents"": ents}

and the automatic docs ui looks like this:

disclaimers: i created fastapi, and that's what we currently use at explosion (the creators of spacy).",https://stackoverflow.com/questions/60964785,python,01-04-2020 06:22,1970.0,3.0,1.0,True,18-09-2023 20:23,30-07-2020 23:32
69057982,why i am not getting &#39;person&#39; nad &#39;gpe&#39; as label after chunking using `nltk.ne_chunk`?,"i am using nltk.ne_chunk() like this:
sent=""azhar is asking what is weather in chicago today? ""
chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent)), binary=true)
print(list(chunks))

and getting oitput like this:
[tree('ne', [('azhar', 'nnp')]), ('is', 'vbz'), ('asking', 'vbg'), ('what', 'wp'), ('is', 
'vbz'), ('weather', 'nn'), ('in', 'in'), tree('ne', [('chicago', 'nnp')]), ('today', 'nn'), 
('?', '.')]

but i am expecting an output like this:
[tree('person', [('azhar', 'nnp')]), ('is', 'vbz'), ('asking', 'vbg'), ('what', 'wp'), ('is', 
'vbz'), ('weather', 'nn'), ('in', 'in'), tree('gpe', [('chicago', 'nnp')]), ('today', 'nn'), 
('?', '.')]

can some one tell me what i am doing wrong here?","['python', 'nlp', 'nltk', 'tagging', 'chunking']",69060747,"after installing the spacy library and download the relevant model (en_core_web_sm) which is explained here, you can simply extract named-entities!
import spacy
ner = spacy.load(""en_core_web_sm"")
sent=""azhar is asking what is weather in chicago today? ""
text1= ner(sent)
for word in text1.ents:
    print(word.text,word.label_)

output:
azhar person
chicago gpe
today date

update
nltk.ne_chunk returns a nested nltk.tree.tree object so you would have to traverse the tree object to get to the nes. tree2conlltags from nltk.chunk would do something like that!
from nltk import word_tokenize, pos_tag, ne_chunk
from nltk.chunk import tree2conlltags

sentence = ""azhar is asking what is weather in chicago today?""
print(tree2conlltags(ne_chunk(pos_tag(word_tokenize(sentence)))))

output in iob format:
[('azhar', 'nnp', 'b-gpe'), ('is', 'vbz', 'o'), ('asking', 'vbg', 'o'), ('what', 'wp', 'o'), ('is', 'vbz', 'o'), ('weather', 'nn', 'o'), ('in', 'in', 'o'), ('chicago', 'nnp', 'b-gpe'), ('today', 'nn', 'o'), ('?', '.', 'o')]

more on this here!",https://stackoverflow.com/questions/69057982,python,04-09-2021 18:29,772.0,0.0,1.0,True,08-09-2021 20:37,08-09-2021 20:37
68618759,adding tagger to blank english spacy pipeline,"i am having a hard time figuring out how to assemble spacy pipelines bit by bit from built in models in spacy v3. i have downloaded the en_core_web_sm model and can load it with nlp = spacy.load(""en_core_web_sm""). processing of sample text works just fine like this.
now what i want though is to build an english pipeline from blank and add components bit by bit. i do not want to load the entire en_core_web_sm pipeline and exclude components. for the sake of concreteness let's say i only want the spacy default tagger in the pipeline. the documentation suggests to me that
import spacy

from spacy.pipeline.tagger import default_tagger_model
config = {""model"": default_tagger_model}

nlp = spacy.blank(""en"")
nlp.add_pipe(""tagger"", config=config)
nlp(""this is some sample text."")

should work. however i am getting this error related to hashembed:
traceback (most recent call last):
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/spacy/language.py"", line 1000, in __call__
    doc = proc(doc, **component_cfg.get(name, {}))
  file ""spacy/pipeline/trainable_pipe.pyx"", line 56, in spacy.pipeline.trainable_pipe.trainablepipe.__call__
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/spacy/util.py"", line 1507, in raise_error
    raise e
  file ""spacy/pipeline/trainable_pipe.pyx"", line 52, in spacy.pipeline.trainable_pipe.trainablepipe.__call__
  file ""spacy/pipeline/tagger.pyx"", line 111, in spacy.pipeline.tagger.tagger.predict
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/model.py"", line 315, in predict
    return self._func(self, x, is_train=false)[0]
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/layers/chain.py"", line 54, in forward
    y, inc_layer_grad = layer(x, is_train=is_train)
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/model.py"", line 291, in __call__
    return self._func(self, x, is_train=is_train)
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/layers/chain.py"", line 54, in forward
    y, inc_layer_grad = layer(x, is_train=is_train)
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/model.py"", line 291, in __call__
    return self._func(self, x, is_train=is_train)
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/layers/chain.py"", line 54, in forward
    y, inc_layer_grad = layer(x, is_train=is_train)
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/model.py"", line 291, in __call__
    return self._func(self, x, is_train=is_train)
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/layers/with_array.py"", line 30, in forward
    return _ragged_forward(
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/layers/with_array.py"", line 90, in _ragged_forward
    y, get_dx = layer(xr.dataxd, is_train)
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/model.py"", line 291, in __call__
    return self._func(self, x, is_train=is_train)
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/layers/concatenate.py"", line 44, in forward
    ys, callbacks = zip(*[layer(x, is_train=is_train) for layer in model.layers])
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/layers/concatenate.py"", line 44, in <listcomp>
    ys, callbacks = zip(*[layer(x, is_train=is_train) for layer in model.layers])
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/model.py"", line 291, in __call__
    return self._func(self, x, is_train=is_train)
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/layers/chain.py"", line 54, in forward
    y, inc_layer_grad = layer(x, is_train=is_train)
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/model.py"", line 291, in __call__
    return self._func(self, x, is_train=is_train)
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/layers/hashembed.py"", line 61, in forward
    vectors = cast(floats2d, model.get_param(""e""))
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/model.py"", line 216, in get_param
    raise keyerror(
keyerror: ""parameter 'e' for model 'hashembed' has not been allocated yet.""


the above exception was the direct cause of the following exception:
traceback (most recent call last):
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/ipython/core/interactiveshell.py"", line 3437, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  file ""<ipython-input-2-8e2b4cf9fd33>"", line 8, in <module>
    nlp(""this is some sample text."")
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/spacy/language.py"", line 1003, in __call__
    raise valueerror(errors.e109.format(name=name)) from e
valueerror: [e109] component 'tagger' could not be run. did you forget to call `initialize()`?

hinting i should run initialize(). ok. if i then run nlp.initialize() i finally get this error
traceback (most recent call last):
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/ipython/core/interactiveshell.py"", line 3437, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  file ""<ipython-input-3-eeec225a68df>"", line 1, in <module>
    nlp.initialize()
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/spacy/language.py"", line 1273, in initialize
    proc.initialize(get_examples, nlp=self, **p_settings)
  file ""spacy/pipeline/tagger.pyx"", line 271, in spacy.pipeline.tagger.tagger.initialize
  file ""spacy/pipeline/pipe.pyx"", line 104, in spacy.pipeline.pipe.pipe._require_labels
valueerror: [e143] labels for component 'tagger' not initialized. this can be fixed by calling add_label, or by providing a representative batch of examples to the component's `initialize` method.

now i am a bit at a loss. which label examples? where do i take them from? why doesn't the default model config take care of that? do i have to tell spacy to use en_core_web_sm somehow? if so, how can i do so without using spacy.load(""en_core_web_sm"") and excluding a whole bunch of stuff? thanks for your hints!
edit: ideally, i would like to be able to load only parts of the pipeline from a modified config file, like nlp = english.from_config(config). i cannot even use the config file shipped with en_core_web_sm as the resulting pipeline needs to be initialized as well, and upon nlp.initialize() i now receive
traceback (most recent call last):
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/ipython/core/interactiveshell.py"", line 3437, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  file ""<ipython-input-67-eeec225a68df>"", line 1, in <module>
    nlp.initialize()
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/spacy/language.py"", line 1246, in initialize
    i = registry.resolve(config[""initialize""], schema=configschemainit)
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/config.py"", line 727, in resolve
    resolved, _ = cls._make(
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/config.py"", line 776, in _make
    filled, _, resolved = cls._fill(
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/thinc/config.py"", line 848, in _fill
    getter_result = getter(*args, **kwargs)
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/spacy/language.py"", line 98, in load_lookups_data
    lookups = load_lookups(lang=lang, tables=tables)
  file ""/home/valentin/miniconda3/envs/eval/lib/python3.8/site-packages/spacy/lookups.py"", line 30, in load_lookups
    raise valueerror(errors.e955.format(table="", "".join(tables), lang=lang))
valueerror: [e955] can't find table(s) lexeme_norm for language 'en' in spacy-lookups-data. make sure you have the package installed or provide your own lookup tables if no default lookups are available for your language.

hinting towards the fact that it doesn't find required lookup tables.","['python', 'python-3.x', 'spacy', 'spacy-3']",68621299,"nlp.add_pipe(""tagger"") adds a new blank/uninitialized tagger, not the tagger from en_core_web_sm or any other pretrained pipeline. if you add the tagger this way, you need to initialize and train it before you can use it.
you can add a component from an existing pipeline using the source option:
nlp = spacy.add_pipe(""tagger"", source=spacy.load(""en_core_web_sm""))

that said, it's possible that the tokenization from spacy.blank(""en"") is different from what the tagger in the source pipeline was trained on. in general (and especially once you move away from spacy's pretrained pipelines), you should also make sure the tokenizer settings are the same,
and loading while excluding components is an easy way to do this.
alternatively, you can copy the tokenizer settings in addition to using nlp.add_pipe(source=) for models like scispacy's en_core_sci_sm, which is a good example of a pipeline the tokenization is not the same as spacy.blank(""en""):
nlp = spacy.blank(""en"")
source_nlp = spacy.load(""en_core_sci_sm"")
nlp.tokenizer.from_bytes(source_nlp.tokenizer.to_bytes())
nlp.add_pipe(""tagger"", source=source_nlp)",https://stackoverflow.com/questions/68618759,python,02-08-2021 08:47,3297.0,2.0,2.0,True,21-02-2024 19:34,02-08-2021 15:18
26622370,attributeerror: &#39;parentedtree&#39; object has no attribute &#39;label&#39;,"i am basically working on parsed tree and trying to annotate tree nodes dominating empty categories(empty node annotation).
i have defined a recurvsive function as below but the error that i am getting is ""attributeerror: 'parentedtree' object has no attribute 'label'"".  
def annotatetracenodes(node):
numchildren = len(node);
numnone=0;

    for child in node:
        if isinstance(child,tree):
            annotatetracenodes(child);
            if(numchildren==0 or child.label().endswith(""-none-"")):
            numnone+=1;            
    if(numchildren==numnone):
        print ""setting the label"";
        node.set_label(node.label()+""-none-"");","['python', 'tree', 'nlp', 'nltk', 'corpus']",26622717,"google suggests you're using nltk, which i'm going to assume is the case.
a parentedtree does not have a method called .label().  so when you write things like this:
child.label().endswith(""-none-"")

python doesn't know what to do.
a tree, on the other hand, does have a .label() method.  did you perhaps use a parentedtree instead of a tree somewhere?",https://stackoverflow.com/questions/26622370,python,29-10-2014 03:34,1974.0,2.0,2.0,True,11-01-2022 01:48,29-10-2014 04:46
77047800,transformers.js in react.js,"i'm building a component in react and i want to use a model from huggingface. i found the package @xenova/transformers that allows to use of these models in javascript, but when i try to create my pipeline instance like this:
class myextractorpipeline {
    static task = ""feature-extraction"";
    static model = ""xenova/all-minilm-l6-v2"";
    static instance = null;

    static async getinstance(model, progress_callback = null) {
        if (this.instance === null) {
            try {
                this.instance = await pipeline(this.task, model, { progress_callback }); //<= this is failing
            } catch (error) {
                throw new error(error)
            }
        }
        return this.instance;
    }
}

i get this error:
worker.js:17 uncaught (in promise) error: syntaxerror: unexpected token '<', ""<!doctype ""... is not valid json
at myextractorpipeline.getinstance (worker.js:17:1)
at async worker.js:77:1
i'm sure it comes from the 'throw new error(error)' that you can see above, but i don't know how to solve it, i need help.
i tried to do console.log() of almost all the variables, and i tried to use other hugginface models...
anything has been useful.","['javascript', 'reactjs', 'huggingface-transformers', 'huggingface']",77099248,"is your intention to load the model locally? have you used create-react-app to initiate your project?
if so (and this might not be unique to create-react-app) i've noticed that on build, if you reference a file in your 'src' folder, it does not properly find the files (causing 'index.html' to be returned).
try putting your local models under your project's 'public' folder. then set the environment variable like this:
env.localmodelpath = process.env.public_url + '/models/'

also, apply env setting for local loading of the model:
env.allowremotemodels = false;
env.allowlocalmodels = true;

further reference here:",https://stackoverflow.com/questions/77047800,javascript,05-09-2023 21:17,2890.0,3.0,3.0,True,25-08-2024 17:05,06-09-2023 08:05
77859773,"hugging face, whisper model large v2, outputs weird character after training","i have been fine-tuning whisper on jasmin cgn which is a dutch dataset. i have preprocessed it locally. the original encoding for the sentences there is latin_1, but i made sure to convert each one of them to utf_8.v then i followed the steps in that hf tutorial for whisper fine-tuning, and this is the output i get (i use jiwer to get side-by-side sentence comparisons):
sentence 80

ref: \*\*\*\* maar voor de idianen was het niet zo voor hun moest een jongend een goede jager en krijger worden end een meisje moet goed kunnen koken en dat leerden ze niet op school de vader de grootvaders en de ooms vertelden de kleine jongens alles ik kan \*\*\* niet de oorlog ze toonden hem hoe je een spoor van de dier volgt end hoe je een boog opspant

hyp: ï¿½ï¿½ï¿½ien maar voor de idianen was het niet zo voor hen moest een jongent een goede jager en krijger worden en een meisje moet goed kunnen koken en dat leerden ze niet op school de vader de grootvaders en de ooms vertelden de kleine jongens alles ik kaniet de oorlog ze toonden hem hoe je het spoor van de dier volgt en toe je een boog opspant 


sentence 76

ref: \* pakketten zijn uh in verschillende soorten en maten je kunt zo allerlei dingen gebruiken je kunt ze voor allerlei dingen gebruiken een een gemeente is uh een stad of een dorp of een paar klein dorpen samen ambtenaren van de gemeente cuntroleren of alle kinderen naar school gaan de gemeente houdt de stad schoon en zorgt v voor goede uhm straten en fietspaden ook zorgt de gemeente voor de bomen en de parken het is ook belangrijk dat de v

hyp: ï¿½ï¿½ï¿½ pakketten zijn ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½n \*\* verschillende soorten en maten je kunt ze allerlei dingen gebruiken je kunt ze voor allerlei dingen gebruiken een een gemeente is uh een stad of een dorp of een paar klein dorpen samen ambtenaren van de gemeente controleren of alle kinderen naar school gaan de gemeente houdt de stad schoon en zorgt v voor goede uhm straten en fietspaden ook zorgt de gemeente voor de bomen en de parken het is ook belangr/pre>
does anyone know what the ""?"" is? i checked my test file with the whisper model without training it and i don't get any question marks there so i think there is something up with the training.
thanks for your help. :)","['speech-recognition', 'huggingface-transformers', 'openai-whisper', 'huggingface-trainer']",77940116,"i have finally figured out when the replacement character is there. during the evaluation pipeline we can specify the language of the model. when you specify that ""language"":""<|nl|>"", there is no longer the replacement character! that way the whisper processor knows the language and doesnï¿½ï¿½ï¿½t get confused by the utf8 codes that it doesnï¿½ï¿½ï¿½t have in english. (at least thatï¿½ï¿½ï¿½s",https://stackoverflow.com/questions/77859773,speech-recognition,22-01-2024 12:14,658.0,0.0,2.0,True,05-02-2024 10:13,22-01-2024 12:18
36060492,nltk wordnet verb hierarchy,"i spotted some problems with wordnet's hierarchy for verbs.
for example,
a.lowest_common_hypernyms(wn.synset('love.v.02')) returns [].
isn't there a common ancestor like entity for verbs as well ? 
are verbs even connected to nouns in the same hierarchy ?","['python', 'nlp', 'nltk', 'wordnet', 'linguistics']",36061618,"to find the top hypernym of any synset, use the synset.root_hypernyms() function, e.g.:
>>> from nltk.corpus import wordnet as wn
>>> wn.synsets('car')[0].root_hypernyms()
[synset('entity.n.01')]
>>> wn.synsets('love')[0].root_hypernyms()
[synset('entity.n.01')]
>>> wn.synsets('love', 'v')[0].root_hypernyms()
[synset('love.v.01')]

it seems that there's no overarching/umbrella hypernym that covers all verbs, unlike nouns that covered by entity.n.01:
>>> root_hypernyms_of_nouns = counter(chain(*[ss.root_hypernyms() for ss in wn.all_synsets(pos='n')]))
>>> len(root_hypernyms_of_nouns)
1
>>> root_hypernyms_of_nouns.items()
[(synset('entity.n.01'), 82115)]

but you can try to iterate through all verbs, e.g.:
wn.all_synsets(pos='v')

and try to find the top most hypernyms for verbs (it will be a rather large list):
>>> from collections import counter
>>> from itertools import chain
>>> root_hypernyms_of_verbs = counter(chain(*[ss.root_hypernyms() for ss in wn.all_synsets(pos='v')]))
>>> root_hypernyms_of_verbs.most_common(10)
[(synset('change.v.01'), 1704), (synset('change.v.02'), 1295), (synset('act.v.01'), 1083), (synset('move.v.02'), 1027), (synset('make.v.03'), 659), (synset('travel.v.01'), 526), (synset('think.v.03'), 451), (synset('transfer.v.05'), 420), (synset('move.v.03'), 329), (synset('connect.v.01'), 262)]
>>> root_hypernyms_of_verbs.keys() # this will return all root_hypernyms.

visuwords has a very pretty interactive graph that you can use to look through the wordnet hierarchy manually,",https://stackoverflow.com/questions/36060492,python,17-03-2016 12:23,2764.0,3.0,1.0,True,15-10-2021 20:28,17-03-2016 13:15
68889843,valueerror: index length mismatch: 4064 vs. 1,"i am working on a nlp problem  i want to perform vectorization after train_test_split but when i do that, the resulting sparse matrix has size = 1 which cannot be right.
my train_x set size is (4064, 1) and after tfidf.fit_transform i get
size = 1. how can that be??! below is my code:
def clean_text(text):
    tokens = nltk.word_tokenize(text)    #tokenizing the words
    lower = [word.lower() for word in tokens]  #converting words to lowercase
    remove_stopwords = [word for word in lower if word not in set(stopwords.words('english'))]  
    remove_char = [word for word in remove_stopwords if word.isalpha()]
    lemm_text = [ps.stem(word) for word in remove_char]     #lemmatizing the words
    cleaned_data = "" "".join([str(word) for word in lemm_text])
    return cleaned_data

x['clean_text']= x[""text""].map(clean_text)

x.drop(['text'], axis = 1, inplace = true)

from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2, random_state = 69, 
stratify = y)

from sklearn.feature_extraction.text import countvectorizer, tfidfvectorizer
tfidf = tfidfvectorizer()
train_x_vect = tfidf.fit_transform(train_x)
test_x1 = tfidf.transform(test_x)

pd.dataframe.sparse.from_spmatrix(train_x_vect,
                              index=train_x.index,
                              columns=tfidf.get_feature_names())    

when i try to convert the sparse matrix (with size = 1) into a dataframe, it gives me error.
the dataframe x has size = 4064 and my sparse matrix has size = 1 which is why it is giving me error. any help will be aprreciated!","['python', 'nlp', 'vectorization', 'tfidfvectorizer', 'data-preprocessing']",70487195,"the reason you are getting the error is because tfidfvectorizer only accepts lists as the input. you can check this from the documentation itself.
here you are passing a dataframe as the input. hence the weird output.
first convert your dataframe to lists using:
train_x = train_x['column_name'].to_list()

and then pass it to the vectorizer. also there are many ways to convert dataframe to list but the output of of all of them might be different formats of list. for example if you try to convert dataframe to list using:
train_x = train_x.values.tolist()

it will convert the dataframe to list but then the format of this list won't work with tidfvectorizer and will give you the same output as you were getting before in your question. i found the above way of converting to list to work with the vectorizer.
another thing to keep in mind is that you can only have one column/variable in your list/dataframe. if you have more than one columns in your dataframe and you convert it to list and pass it to the vectorizer, it will throw an error! i don't know why this is but just throwing it out there in case someone faces this problem.",https://stackoverflow.com/questions/68889843,python,23-08-2021 08:56,100.0,1.0,1.0,True,26-12-2021 14:28,05-11-2021 17:23
57902256,how to export &quot;document with entities from spacy&quot; for use in doccano,"i want to train my model with doccano or an other ""open source text annotation tool"" and continuously improve my model.
for that my understanding is, that i can import annotated data to doccano in a format described here:

so for a first step i have loaded a model and created a doc:
text = ""test text that should be annotated for michael schumacher"" 
nlp = spacy.load('en_core_news_sm')
doc = nlp(text)

i know i can export the jsonl format (with text and annotated labels) from doccano and train a model with it but i want to know how to export that data from a spacy doc in python so that i can import it to doccano.
thanks in advance.","['python', 'json', 'spacy', 'doccano']",68615623,"doccano and/or spacy seem to have changed things and there are now some flaws in the accepted answer. this revised version should be more correct with spacy 3.1 and doccano as of 8/1/2021...
def text_to_doccano(text):
    """"""
    :text (str): source text
    returns (list (dict)): deccano format json
    """"""
    djson = list()
    doc = nlp(text)
    for sent in doc.sents:
        labels = list()
        for e in sent.ents:
            labels.append([e.start_char - sent.start_char, e.end_char - sent.start_char, e.label_])
        djson.append({'text': sent.text, ""label"": labels})
    return djson

the differences:

labels becomes singular label in the json (?!?)
e.start_char and e.end_char are actually (now?) the start and end within the document, not within the sentence...so you have to offset them by the position of the sentence within the document.",https://stackoverflow.com/questions/57902256,python,12-09-2019 08:00,3167.0,1.0,4.0,True,31-05-2022 15:11,31-05-2022 15:11
72519729,"how to compare strings of 1 column with strings of another within the same dataframe, calculate the percentage of strings matching in result columns","how to compare strings of 1 column with strings of another within the same dataframe, calculate the percentage of strings matching in result columns, as well as whether they are full matches, partial matches, or don't match at all?","['python', 'pandas', 'nlp', 'fuzzywuzzy', 'difflib']",72520343,"try this:
import pandas as pd
table = {
    'column_01':['apple', 'mango', 'banana','coconut','pineaple','guava'],
    'column_02':['apple','man','fruits','cocon','pin','guava']
           }
tf1 = pd.dataframe(table)


print(tf1)

print(f'\n\n-------------break-----------\n\n')


def func(x):
    len_col1 = len(x[0])
    cont = 0
    for y in range (0, len_col1):
        try:
            if x[0][y] == x[1][y]:
                cont += 1
            else:
                pass
        except:
            break
    return round(((cont* 100)/len_col1),2)

def func2(x):
    if x == 0:
        return 'not match at all'
    elif x < 100:
        return 'partial match'
    else:
        return 'full match'

tf1['% of matching stings'] = tf1.apply(func, axis = 1) 

tf1['status'] = tf1['% of matching stings'].apply(func2)

print(tf1)

output
  column_01 column_02
0     apple     apple
1     mango       man
2    banana    fruits
3   coconut     cocon
4  pineaple       pin
5     guava     guava


-------------break-----------


  column_01 column_02  % of matching stings            status
0     apple     apple                100.00        full match
1     mango       man                 60.00     partial match
2    banana    fruits                  0.00  not match at all
3   coconut     cocon                 71.43     partial match
4  pineaple       pin                 37.50     partial match
5     guava     guava                100.00        full match",https://stackoverflow.com/questions/72519729,python,06-06-2022 15:12,565.0,1.0,1.0,True,06-06-2022 16:03,06-06-2022 15:13
71058732,how to load transformers pipeline from folder?,"according to here pipeline provides an interface to save a pretrained pipeline locally with a save_pretrained method. when i use it, i see a folder created with a bunch of json and bin files presumably for the tokenizer and the model.
but the documentation does not specify a load method. how does one initialize a pipeline using a locally saved pipeline?","['python', 'huggingface-transformers']",71069924,"apparently the default initialization works with local folders as well. so one can download a model like this:
pipe = pipeline(""text-classification"")
pipe.save_pretrained(""my_local_path"")

and later load it like
pipe = pipeline(""text-classification"", model = ""my_local_path"")",https://stackoverflow.com/questions/71058732,python,10-02-2022 01:24,21160.0,13.0,2.0,True,26-07-2023 00:14,26-07-2023 00:14
73830225,__init__() got an unexpected keyword argument &#39;cachedir&#39; when importing top2vec,"i keep getting this error when importing top2vec.
typeerror                                 traceback (most recent call last)
cell in [1], line 1
----> 1 from top2vec import top2vec

file ~\appdata\roaming\python\python39\site-packages\top2vec\__init__.py:1
----> 1 from top2vec.top2vec import top2vec
      3 __version__ = '1.0.27'

file ~\appdata\roaming\python\python39\site-packages\top2vec\top2vec.py:12
     10 from gensim.models.phrases import phrases
     11 import umap
---> 12 import hdbscan
     13 from wordcloud import wordcloud
     14 import matplotlib.pyplot as plt

file ~\appdata\roaming\python\python39\site-packages\hdbscan\__init__.py:1
----> 1 from .hdbscan_ import hdbscan, hdbscan
      2 from .robust_single_linkage_ import robustsinglelinkage, robust_single_linkage
      3 from .validity import validity_index

file ~\appdata\roaming\python\python39\site-packages\hdbscan\hdbscan_.py:509
    494         row_indices = np.where(np.isfinite(matrix).sum(axis=1) == matrix.shape[1])[0]
    495     return row_indices
    498 def hdbscan(
    499     x,
    500     min_cluster_size=5,
    501     min_samples=none,
    502     alpha=1.0,
    503     cluster_selection_epsilon=0.0,
    504     max_cluster_size=0,
    505     metric=""minkowski"",
    506     p=2,
    507     leaf_size=40,
    508     algorithm=""best"",
--> 509     memory=memory(cachedir=none, verbose=0),
    510     approx_min_span_tree=true,
    511     gen_min_span_tree=false,
    512     core_dist_n_jobs=4,
    513     cluster_selection_method=""eom"",
    514     allow_single_cluster=false,
    515     match_reference_implementation=false,
    516     **kwargs
    517 ):
    518     """"""perform hdbscan clustering from a vector array or distance matrix.
    519 
    520    parameters
   (...)
    672        density-based cluster selection. arxiv preprint 1911.02282.
    673    """"""
    674     if min_samples is none:

typeerror: __init__() got an unexpected keyword argument 'cachedir'

python version: 3.9.7 (64-bit)
have installed msbuild
no errors when pip installing this package
does anyone know a solution to this problem or experienced a similar problem?","['python', 'machine-learning', 'topic-modeling']",73830525,"update 12 november 2022:

there is new release (ver. 0.8.29) of hdbscan from 31 oct. 2022 that fix the issue. see my original answer for more details.
original answer:
it looks like you are using latest (as of 23 sept 2022) versions of hdbscan and joblib packages available on pypi.
cachedir was removed from joblib.memory in commit on 2 feb 2022 as depreciated. the latest version on pypi is ver. 1.2.0 released on sep 16, 2022, i.e. it incorporate this change
the relevant part of hdbscan source code on github was updated on 16 sept 2022. unfortunately the latest (as of 23 sept 2022) hdbscan release on pypi is ver. 0.8.28 released on feb 8, 2022 and still not updated. it still use memory=memory(cachedir=none, verbose=0)
one possible solution is to force using joblib version before cachedir was removed - ver. 1.1.0 as of oct 7, 2021. however note my edits below.

update 29 sept 2022:

there are open issues on hdbscan repo (#563) and (#565).
note there is vulnerability cve-2022-21797 when using joblib < 1.2.0

update 12 november 2022:

there is new release (ver. 0.8.29) of hdbscan from 31 oct. 2022.",https://stackoverflow.com/questions/73830225,python,23-09-2022 15:50,13195.0,12.0,2.0,True,12-11-2022 07:44,23-09-2022 18:01
30195287,how to save python nltk alignment models for later use?,"in python, i'm using nltk's alignment module to create word alignments between parallel texts. aligning bitexts can be a time-consuming process, especially when done over considerable corpora. it would be nice to do alignments in batch one day and use those alignments later on.
from nltk import ibmmodel1 as ibm
biverses = [list of alignedsent objects]
model = ibm(biverses, 20)

with open(path + ""eng-taq_model.txt"", 'w') as f:
    f.write(model.train(biverses, 20))  // makes empty file

once i create a model, how can i (1) save it to disk and (2) reuse it later?","['python', 'io', 'nlp', 'nltk', 'machine-translation']",30214539,"the immediate answer is to pickle it, see 
but because ibmmodel1 returns a lambda function, it's not possible to pickle it with the default pickle / cpickle (see  and  
so we'll use dill. firstly, install dill, see can python pickle lambda functions?
$ pip install dill
$ python
>>> import dill as pickle

then:
>>> import dill
>>> import dill as pickle
>>> from nltk.corpus import comtrans
>>> from nltk.align import ibmmodel1
>>> bitexts = comtrans.aligned_sents()[:100]
>>> ibm = ibmmodel1(bitexts, 20)
>>> with open('model1.pk', 'wb') as fout:
...     pickle.dump(ibm, fout)
...
>>> exit()

to use pickled model:
>>> import dill as pickle
>>> from nltk.corpus import comtrans
>>> bitexts = comtrans.aligned_sents()[:100]
>>> with open('model1.pk', 'rb') as fin:
...     ibm = pickle.load(fin)
... 
>>> aligned_sent = ibm.align(bitexts[0])
>>> aligned_sent.words
['wiederaufnahme', 'der', 'sitzungsperiode']


if you try to pickle the ibmmodel1 object, which is a lambda function, you'll end up with this:
>>> import cpickle as pickle
>>> from nltk.corpus import comtrans
>>> from nltk.align import ibmmodel1
>>> bitexts = comtrans.aligned_sents()[:100]
>>> ibm = ibmmodel1(bitexts, 20)
>>> with open('model1.pk', 'wb') as fout:
...     pickle.dump(ibm, fout)
... 
traceback (most recent call last):
  file ""<stdin>"", line 2, in <module>
  file ""/usr/lib/python2.7/copy_reg.py"", line 70, in _reduce_ex
    raise typeerror, ""can't pickle %s objects"" % base.__name__
typeerror: can't pickle function objects

(note: the above code snippet comes from nltk version 3.0.0)
in python3 with nltk 3.0.0, you will also face the same problem because ibmmodel1 returns a lambda function:
alvas@ubi:~$ python3
python 3.4.0 (default, apr 11 2014, 13:05:11) 
[gcc 4.8.2] on linux
type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pickle
>>> from nltk.corpus import comtrans
>>> from nltk.align import ibmmodel1
>>> bitexts = comtrans.aligned_sents()[:100]
>>> ibm = ibmmodel1(bitexts, 20)
>>> with open('mode1.pk', 'wb') as fout:
...     pickle.dump(ibm, fout)
... 
traceback (most recent call last):
  file ""<stdin>"", line 2, in <module>
_pickle.picklingerror: can't pickle <function ibmmodel1.train.<locals>.<lambda> at 0x7fa37cf9d620>: attribute lookup <lambda> on nltk.align.ibm1 failed'

>>> import dill
>>> with open('model1.pk', 'wb') as fout:
...     dill.dump(ibm, fout)
... 
>>> exit()

alvas@ubi:~$ python3
python 3.4.0 (default, apr 11 2014, 13:05:11) 
[gcc 4.8.2] on linux
type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import dill
>>> from nltk.corpus import comtrans
>>> with open('model1.pk', 'rb') as fin:
...     ibm = dill.load(fin)
... 
>>> bitexts = comtrans.aligned_sents()[:100]
>>> aligned_sent = ibm.aligned(bitexts[0])
traceback (most recent call last):
  file ""<stdin>"", line 1, in <module>
attributeerror: 'ibmmodel1' object has no attribute 'aligned'
>>> aligned_sent = ibm.align(bitexts[0])
>>> aligned_sent.words
['wiederaufnahme', 'der', 'sitzungsperiode']

(note: in python3, pickle is cpickle, see",https://stackoverflow.com/questions/30195287,python,12-05-2015 15:25,2671.0,14.0,4.0,True,21-02-2022 08:56,24-05-2015 23:46
46105180,typeerror: expected string or bytes-like object ï¿½ï¿½ï¿½ with python/nltk word_to,"i have a dataset with ~40 columns, and am using .apply(word_tokenize) on 5 of them like so: df['token_column'] = df.column.apply(word_tokenize).  
i'm getting a typeerror for only one of the columns, we'll call this problem_column
typeerror: expected string or bytes-like object

here's the full error (stripped df and column names, and pii), i'm new to python and am still trying to figure out which parts of the error messages are relevant:
typeerror                                 traceback (most recent call last)
<ipython-input-51-22429aec3622> in <module>()
----> 1 df['token_column'] = df.problem_column.apply(word_tokenize)

c:\users\egagne\appdata\local\continuum\anaconda3\lib\site-packages\pandas\core\series.py in apply(self, func, convert_dtype, args, **kwds)
   2353             else:
   2354                 values = self.asobject
-> 2355                 mapped = lib.map_infer(values, f, convert=convert_dtype)
   2356 
   2357         if len(mapped) and isinstance(mapped[0], series):

pandas\_libs\src\inference.pyx in pandas._libs.lib.map_infer (pandas\_libs\lib.c:66440)()

c:\users\egagne\appdata\local\continuum\anaconda3\lib\site-packages\nltk\tokenize\__init__.py in word_tokenize(text, language, preserve_line)
    128     :type preserver_line: bool
    129     """"""
--> 130     sentences = [text] if preserve_line else sent_tokenize(text, language)
    131     return [token for sent in sentences
    132             for token in _treebank_word_tokenizer.tokenize(sent)]

c:\users\egagne\appdata\local\continuum\anaconda3\lib\site-packages\nltk\tokenize\__init__.py in sent_tokenize(text, language)
     95     """"""
     96     tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))
---> 97     return tokenizer.tokenize(text)
     98 
     99 # standard word tokenizer.

c:\users\egagne\appdata\local\continuum\anaconda3\lib\site-packages\nltk\tokenize\punkt.py in tokenize(self, text, realign_boundaries)
   1233         given a text, returns a list of the sentences in that text.
   1234         """"""
-> 1235         return list(self.sentences_from_text(text, realign_boundaries))
   1236 
   1237     def debug_decisions(self, text):

c:\users\egagne\appdata\local\continuum\anaconda3\lib\site-packages\nltk\tokenize\punkt.py in sentences_from_text(self, text, realign_boundaries)
   1281         follows the period.
   1282         """"""
-> 1283         return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
   1284 
   1285     def _slices_from_text(self, text):

c:\users\egagne\appdata\local\continuum\anaconda3\lib\site-packages\nltk\tokenize\punkt.py in span_tokenize(self, text, realign_boundaries)
   1272         if realign_boundaries:
   1273             slices = self._realign_boundaries(text, slices)
-> 1274         return [(sl.start, sl.stop) for sl in slices]
   1275 
   1276     def sentences_from_text(self, text, realign_boundaries=true):

c:\users\egagne\appdata\local\continuum\anaconda3\lib\site-packages\nltk\tokenize\punkt.py in <listcomp>(.0)
   1272         if realign_boundaries:
   1273             slices = self._realign_boundaries(text, slices)
-> 1274         return [(sl.start, sl.stop) for sl in slices]
   1275 
   1276     def sentences_from_text(self, text, realign_boundaries=true):

c:\users\egagne\appdata\local\continuum\anaconda3\lib\site-packages\nltk\tokenize\punkt.py in _realign_boundaries(self, text, slices)
   1312         """"""
   1313         realign = 0
-> 1314         for sl1, sl2 in _pair_iter(slices):
   1315             sl1 = slice(sl1.start + realign, sl1.stop)
   1316             if not sl2:

c:\users\egagne\appdata\local\continuum\anaconda3\lib\site-packages\nltk\tokenize\punkt.py in _pair_iter(it)
    310     """"""
    311     it = iter(it)
--> 312     prev = next(it)
    313     for el in it:
    314         yield (prev, el)

c:\users\egagne\appdata\local\continuum\anaconda3\lib\site-packages\nltk\tokenize\punkt.py in _slices_from_text(self, text)
   1285     def _slices_from_text(self, text):
   1286         last_break = 0
-> 1287         for match in self._lang_vars.period_context_re().finditer(text):
   1288             context = match.group() + match.group('after_tok')
   1289             if self.text_contains_sentbreak(context):

typeerror: expected string or bytes-like object

the 5 columns are all character/string (as verified in sql server, sas, and using .select_dtypes(include=[object])).
for good measure i used .to_string() to make sure problem_column is really and truly not anything besides a string, but i continue to get the error.  if i process the columns separately good_column1-good_column4 continue to work and problem_column will still generate the error.
i've googled around and aside from stripping any numbers from the set (which i can't do, because those are meaningful) i haven't found any additional fixes.","['python', 'python-3.x', 'pandas', 'dataframe', 'nltk']",46183042,"this is what got me the desired result.
def custom_tokenize(text):
    if not text:
        print('the text to be tokenized is a none type. defaulting to blank string.')
        text = ''
    return word_tokenize(text)
df['tokenized_column'] = df.column.apply(custom_tokenize)",https://stackoverflow.com/questions/46105180,python,07-09-2017 21:13,50107.0,12.0,6.0,True,01-09-2022 20:22,10-09-2017 02:30
42236677,extract total frequency of words from vector in r,"this is the vector i have:
 posts = c(""originally by: cearainmy only concern with csm is they seem a bit insulated from players.  they have private message boards where it appears most of their work goes on.  i would bet they are posting more there than in jita speakers corner.  i think that is unfortunate because its hard to know who to vote for if you never really see what positions they hold.  its sort of like ccp used to post here on the forums then they stopped.  so they got a csm to represent players and use jita park forum to interact.  now the csm no longer posts there as they have their internal forums where they hash things out.  perhaps we need a csm to the csm to find out what they are up to.i don't think you need to worry too much. the csm has had an internal forum for over 2 years, although it is getting used a lot more now than it was. a lot of what goes on in there is nda stuff that we couldn't discuss anyway.i am quite happy to give my opinion on any topic, to the extent that the nda allows, and i"" , ""fot those of you bleating about imagined nda scandals as you attempt to cast yourselves as the julian assange of eve, here's a quote from the winter summit thread:originally by: sokrateszday 3post dominion 0.0 (3hrs!)if i had to fly to iceland only for this session i would have done it. we had gathered a list of items and prepared it a bit. important things we went over were supercaps, force projection, empire building, profitability of 0.0, objectives for small gangs and of course sovereingty.the csm spent 3 hours talking to ccp about how dominion had changed 0.0, and the first thing on sokratesz's list is supercaps. its not hard to figure out the nature of the discussion.on the other hand, maybe you're right, and the csm's priority for this discussion was to talk about how underpowered and useless supercarriers are and how they needed triple the ehp and dps from their current levels?(it wasn't)"")

i want a data frame as a result, that would contain words and the frequecy of times they occur.
so result should look something like:
word   count
a        300
and      260
be       200
...      ...
...      ...

what i tried to do, was use tm
corpus <- vcorpus(vectorsource(posts))
corpus <-tm_map(corpus, removenumbers)
corpus <-tm_map(corpus, removepunctuation)
m <- documenttermmatrix(corpus)

running findfreqterms(m, lowfreq =0, highfreq =inf ) just gives me the words, so i understand its a sparse matrix, how do i extract the words and their frequency?
is there a easier way to do this, maybe by not using tm at all?","['r', 'text-mining']",42236953,"posts = c(""originally by: cearainmy only concern with csm is they seem a bit insulated from players.  they have private message boards where it appears most of their work goes on.  i would bet they are posting more there than in jita speakers corner.  i think that is unfortunate because its hard to know who to vote for if you never really see what positions they hold.  its sort of like ccp used to post here on the forums then they stopped.  so they got a csm to represent players and use jita park forum to interact.  now the csm no longer posts there as they have their internal forums where they hash things out.  perhaps we need a csm to the csm to find out what they are up to.i don't think you need to worry too much. the csm has had an internal forum for over 2 years, although it is getting used a lot more now than it was. a lot of what goes on in there is nda stuff that we couldn't discuss anyway.i am quite happy to give my opinion on any topic, to the extent that the nda allows, and i"" , ""fot those of you bleating about imagined nda scandals as you attempt to cast yourselves as the julian assange of eve, here's a quote from the winter summit thread:originally by: sokrateszday 3post dominion 0.0 (3hrs!)if i had to fly to iceland only for this session i would have done it. we had gathered a list of items and prepared it a bit. important things we went over were supercaps, force projection, empire building, profitability of 0.0, objectives for small gangs and of course sovereingty.the csm spent 3 hours talking to ccp about how dominion had changed 0.0, and the first thing on sokratesz's list is supercaps. its not hard to figure out the nature of the discussion.on the other hand, maybe you're right, and the csm's priority for this discussion was to talk about how underpowered and useless supercarriers are and how they needed triple the ehp and dps from their current levels?(it wasn't)"")
posts <- gsub(""[[:punct:]]"", '', posts)  # remove punctuations
posts <- gsub(""[[:digit:]]"", '', posts)  # remove numbers
word_counts <- as.data.frame(table(unlist( strsplit(posts, ""\ "") )))  # split vector by space
word_counts <- with(word_counts, word_counts[ var1 != """", ] )  # remove empty characters
head(word_counts)
#       var1 freq
# 2        a    8
# 3    about    3
# 4   allows    1
# 5 although    1
# 6       am    1
# 7       an    1",https://stackoverflow.com/questions/42236677,r,14-02-2017 21:34,5505.0,4.0,5.0,True,06-11-2023 20:11,06-11-2023 20:11
75549632,difference between automodelforseq2seqlm and automodelforcausallm,"as per the title, how are these two auto classes on huggingface different from each other? i tried reading the documentation but did not find differentiating information","['machine-learning', 'nlp', 'huggingface-transformers']",75717311,"intuitively, automodelforseq2seqlm is used for language models with encoder-decoder architecture, like t5 and bart, while automodelforcausallm is used for auto-regressive language models like all the gpt models.
these two classes are conceptual apis to automatically infer a specific model class for the two types of models, e.g., gpt2lmheadmodel using automodelforcausallm.from_pretrained('gpt2'). for example, you can see the source code for all the inference models. (model_for_causal_lm_mapping  and model_for_sequence_classification_mapping)",https://stackoverflow.com/questions/75549632,machine-learning,23-02-2023 19:45,24438.0,16.0,1.0,True,21-01-2024 20:53,21-01-2024 20:52
75674773,creating huggingface dataset to train an bio tagger,"i have a list of dictionaries:
sentences = [ 
{'text': ['i live in madrid'], 'labels':[o, o, o, b-loc]},
{'text': ['peter lives in spain'], 'labels':[b-per, o, o, b-loc]},
{'text': ['he likes pasta'], 'labels':[o, o, b-food]},
...
]

i want to create a huggingface dataset object from this data so that i can later preprocess it and feed to a transformer model much more easily, but so far i have not found a viable way to do this.","['python', 'nlp', 'huggingface-transformers', 'named-entity-recognition', 'huggingface-datasets']",75717951,"first you'll need some extra libraries to use the metrics and datasets features.
pip install -u transformers datasets evaluate seqeval

to convert list of dict to dataset object
import pandas as pd
from datasets import dataset

sentences = [ 
{'text': 'i live in madrid', 'labels':['o', 'o', 'o', 'b-loc']},
{'text': 'peter lives in spain', 'labels':['b-per', 'o', 'o', 'b-loc']},
{'text': 'he likes pasta', 'labels':['o', 'o', 'b-food']},
]


ds = dataset.from_pandas(pd.dataframe(data=sentences))


convert the dataset into a ""trainer-able"" dataset object
from datasets import dataset
from datasets import classlabel

# define a classlabel object to use to map string labels to integers.
classmap = classlabel(num_classes=4, names=['b-loc', 'b-per', 'b-food', 'o'])


train_sentences = [ 
{'text': 'i live in madrid', 'labels':['o', 'o', 'o', 'b-loc']},
{'text': 'peter lives in spain', 'labels':['b-per', 'o', 'o', 'b-loc']},
{'text': 'he likes pasta', 'labels':['o', 'o', 'b-food']},
]

# map text to tokenizer ids.
ds = ds.map(lambda x: tokenizer(x[""text""], truncation=true))

# map labels to label ids.
ds = ds.map(lambda y: {""labels"": classmap.str2int(y[""labels""])})


to compute metrics with the labeled inputs that you have:
import evaluate

metric = evaluate.load(""seqeval"")


def compute_metrics(p):
    predictions, labels = p
    predictions = predictions.argmax(axis=2)
    # remove ignored index (special tokens)
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    results = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        ""precision"": results[""overall_precision""],
        ""recall"": results[""overall_recall""],
        ""f1"": results[""overall_f1""],
        ""accuracy"": results[""overall_accuracy""],
    }


to use with the trainer object
import pandas as pd
import evaluate

from datasets import dataset
from datasets import classlabel

from transformers import automodelfortokenclassification, trainer, autotokenizer, datacollatorfortokenclassification

# define a classlabel object to use to map string labels to integers.
classmap = classlabel(num_classes=4, names=['b-loc', 'b-per', 'b-food', 'o'])

train_sentences = [ 
{'text': 'i live in madrid', 'labels':['o', 'o', 'o', 'b-loc']},
{'text': 'peter lives in spain', 'labels':['b-per', 'o', 'o', 'b-loc']},
{'text': 'he likes pasta', 'labels':['o', 'o', 'b-food']},
]

eval_sentences = [
    {""text"": ""i like pasta from madrid , spain"", 'labels': ['o', 'o', 'b-food', 'o', 'b-loc', 'o', 'b-loc']}
]

ds_train = dataset.from_pandas(pd.dataframe(data=train_sentences))
ds_eval = dataset.from_pandas(pd.dataframe(data=eval_sentences))

model = automodelfortokenclassification.from_pretrained(""distilbert-base-multilingual-cased"",
                                                        id2label={i:classmap.int2str(i) for i in range(classmap.num_classes)},
                                                        label2id={c:classmap.str2int(c) for c in classmap.names},
                                                        finetuning_task=""ner"")
tokenizer = autotokenizer.from_pretrained(""distilbert-base-multilingual-cased"")
data_collator = datacollatorfortokenclassification(tokenizer)


ds_train = ds_train.map(lambda x: tokenizer(x[""text""], truncation=true))
ds_eval = ds_eval.map(lambda x: tokenizer(x[""text""], truncation=true))

ds_train = ds_train.map(lambda y: {""labels"": classmap.str2int(y[""labels""])})
ds_eval = ds_eval.map(lambda y: {""labels"": classmap.str2int(y[""labels""])})


metric = evaluate.load(""seqeval"")


def compute_metrics(p):
    predictions, labels = p
    predictions = predictions.argmax(axis=2)
    # remove ignored index (special tokens)
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    results = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        ""precision"": results[""overall_precision""],
        ""recall"": results[""overall_recall""],
        ""f1"": results[""overall_f1""],
        ""accuracy"": results[""overall_accuracy""],
    }

# initialize our trainer
trainer = trainer(
    model=model,
    train_dataset=ds_train,
    eval_dataset=ds_eval,
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)


trainer.train()",https://stackoverflow.com/questions/75674773,python,08-03-2023 15:06,3339.0,5.0,1.0,True,13-03-2023 04:02,13-03-2023 04:02
76671494,how to get the embedding of any vocabulary token in gpt?,"i have a gpt model
model = biogptforcausallm.from_pretrained(""microsoft/biogpt"").to(device)

when i send my batch to it i can get the logits and the hidden states:
out = model(batch[""input_ids""].to(device), output_hidden_states=true, return_dict=true)
print(out.keys())
>>> odict_keys(['logits', 'past_key_values', 'hidden_states'])

the logits have shape of
torch.size([2, 1024, 42386]) # batch of size 2, sequence length = 1024, vocab size = 42386

corresponding to (batch, seq_length, vocab_length). if i understand correctly, for each token in the sequence, the logits is a vector of size vocab_length which points the model to which token from the vocabulary to use, after passing it to softmax. i believe that each of these tokens should have an embedding.
from my previous question i found how to get the embeddings of each sequence token (shape [2,1024,1024] in my setting). but, how can i get the embeddings of each token in the vocabulary of the model? this should be of size [2, 1024, 42386, 1024] (biogpt has a hidden size of length 1024).
i'm mainly interested in just a few special tokens (e.g., indices 1,2,6,112 out of the 42386).","['machine-learning', 'pytorch', 'nlp', 'huggingface-transformers', 'language-model']",76672105,"if i understand correctly, you want an embedding representing a single token from the vocabulary. they are two answers that i know for that, depending on which embedding you want exactly.
1st solution
the first layer in the model is a torch.nn.embedding, which is under the hood a linear layer with no bias, so it has a weight parameter of shape [v, d] where v is the vocab size (42386 for you) and d is the dimension of the embedding (1024). you can access to the representation of a token k with : model.biogpt.embed_tokens.weight[k]. this is the 1024-sized vector that directly represents the k-th token.
2nd solution
you can feed the model with a created sequence, containing just the token of which you want the representation. this representation corresponds to the input of the first attention layer of the model. for example, to get the 5th token representation:
inp = torch.tensor([[5]]).long()
output = model(inp, output_hidden_states=true)
print(output.hidden_states[0])

these two representations are not exactly the same, because the first one only represents a token, while the second represents the token in its sentence, which is a sequence of one single token. it is up to you to decide which one suits to what you want to do after.",https://stackoverflow.com/questions/76671494,machine-learning,12-07-2023 14:09,1407.0,0.0,1.0,True,12-07-2023 15:14,12-07-2023 14:16
31270361,why shows error &quot;import nltk&quot;?,"i was trying to build web crawler using python and i'v got following error when it executed.   
import sys
import math
import re
from collections import ordereddict, defaultdict
import os
import nltk //this line shows error
import pickle
from xml.dom import minidom
from xml.dom import empty_namespace
import nltk  //this line shows error
from nltk.stem.porter import porterstemmer
import time`enter code here`

how to overcome this problem?","['python', 'python-2.7', 'nltk']",35679618,"first you can check whether already nltk you have installed or not. 
otherwise you can download from here. 

nltk download link

in ubuntu, you can try followng one: 
sudo apt-get install python-nltk",https://stackoverflow.com/questions/31270361,python,07-07-2015 13:45,7365.0,-1.0,3.0,True,26-01-2023 22:26,01-03-2016 04:47
71512064,error while loading vector from glove in spacy,"i am facing the following attribute error when loading glove model:
code used to load model:
nlp = spacy.load('en_core_web_sm')
tokenizer = spacy.load('en_core_web_sm', disable=['tagger','parser', 'ner', 'textcat'])
nlp.vocab.vectors.from_glove('../models/glove')

getting the following atribute error when trying to load the glove model:
attributeerror: 'spacy.vectors.vectors' object has no attribute 'from_glove'

have tried to search on stackoverflow and elsewhere but can't seem to find the solution. thanks!
from pip list:

spacy version: 3.1.4
spacy-legacy 3.0.8
en-core-web-sm 3.1.0","['python', 'python-3.x', 'nlp', 'spacy', 'stanford-nlp']",71516020,use spacy init vectors to load vectors from word2vec/glove text format into a new pipeline:,https://stackoverflow.com/questions/71512064,python,17-03-2022 12:10,504.0,2.0,2.0,True,08-03-2023 01:46,08-03-2023 01:46
65398096,how to use wordnet 3.1 with nltk on python?,"important edit
as informed by @pengin in comments. nltk is supporting wordnet 3.1 from january 2022. thus this question is deemed irrelevant now.

i need to use wordnet 3.1 for my research work, but nltk (python) ships with the default wordnet version: 3.0. it is important that i use the latest version of wordnet.
>>> from nltk.corpus import wordnet
>>> wordnet.get_version()
'3.0'

but, since nltk 3.1 is the latest version, and i cannot find any way to download and access it using nltk.download(), i am searching for a workaround.
as written in wordnet website (current version link here), i am quoting below:

wordnet 3.1 database files only
you can download the wordnet 3.1 database files. note that this is not a full package as those above, nor does it contain any code for running wordnet. however, you can replace the files in the database directory of your 3.0 local installation with these files and the wordnet interface will run, returning entries from the 3.1 database. this is simply a compressed tar file of the wordnet 3.1 database files.

i tried downloading the wordnet 3.1 database files and replaced them with the default wordnet files at c:\users\<username>\appdata\roaming\nltk_data\corpora (on windows system). i doubted that it won't work as the instructions are to replace the database file in the wordnet software installation, but still, i tried.
on running wordnet.get_version(), i am getting the following error.
---------------------------------------------------------------------------
oserror                                   traceback (most recent call last)
<ipython-input-2-d64ae1e68b36> in <module>
----> 1 wordnet.get_version()

~\anaconda3\lib\site-packages\nltk\corpus\util.py in __getattr__(self, attr)
    118             raise attributeerror(""lazycorpusloader object has no attribute '__bases__'"")
    119 
--> 120         self.__load()
    121         # this looks circular, but its not, since __load() changes our
    122         # __class__ to something new:

~\anaconda3\lib\site-packages\nltk\corpus\util.py in __load(self)
     86 
     87         # load the corpus.
---> 88         corpus = self.__reader_cls(root, *self.__args, **self.__kwargs)
     89 
     90         # this is where the magic happens!  transform ourselves into

~\anaconda3\lib\site-packages\nltk\corpus\reader\wordnet.py in __init__(self, root, omw_reader)
   1136 
   1137         # load the lexnames
-> 1138         for i, line in enumerate(self.open(""lexnames"")):
   1139             index, lexname, _ = line.split()
   1140             assert int(index) == i

~\anaconda3\lib\site-packages\nltk\corpus\reader\api.py in open(self, file)
    206         """"""
    207         encoding = self.encoding(file)
--> 208         stream = self._root.join(file).open(encoding)
    209         return stream
    210 

~\anaconda3\lib\site-packages\nltk\data.py in join(self, fileid)
    335     def join(self, fileid):
    336         _path = os.path.join(self._path, fileid)
--> 337         return filesystempathpointer(_path)
    338 
    339     def __repr__(self):

~\anaconda3\lib\site-packages\nltk\compat.py in _decorator(*args, **kwargs)
     39     def _decorator(*args, **kwargs):
     40         args = (args[0], add_py3_data(args[1])) + args[2:]
---> 41         return init_func(*args, **kwargs)
     42 
     43     return wraps(init_func)(_decorator)

~\anaconda3\lib\site-packages\nltk\data.py in __init__(self, _path)
    313         _path = os.path.abspath(_path)
    314         if not os.path.exists(_path):
--> 315             raise ioerror(""no such file or directory: %r"" % _path)
    316         self._path = _path
    317 

oserror: no such file or directory: 'c:\\users\\punit singh\\appdata\\roaming\\nltk_data\\corpora\\wordnet\\lexnames'

then i checked for the file structure and i am listing the before and after tree below.
file tree in wordnet 3.0
wordnet
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ adj.exc
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ adv.exc
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ citation.bib
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ cntlist.rev
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ data.adj
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ data.adv
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ data.noun
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ data.verb
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ index.adj
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ index.adv
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ index.noun
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ index.sense
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ index.verb
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ lexnames
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ license
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ noun.exc
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½r(ýýýýýýýý index.adv
ýýýýýýýýý index.noun
ýýýýýýýýý index.sense
ýýýýýýýýý index.verb
ýýýýýýýýý log.grind.3.1
ýýýýýýýýý noun.exc
ýýýýýýýýý sentidx.vrb
ýýýýýýýýý dbfiles
    ýýýýýýýýý adj.all
    ýýýýýýýýý adj.pert
    ýýýýýýýýý adj.ppl
    ýýýýýýýýý adv.all
    ýýýýýýýýý cntlist
    ýýýýýýýýý noun.act
    ýýýýýýýýý noun.animal
    ýýýýýýýýý noun.artifact
    ýýýýýýýýý noun.attribute
    ýýýýýýýýý noun.body
    ýýýýýýýýý noun.cognition
    ýýýýýýýýý noun.communication
    ýýýýýýýýý noun.event
    ýýýýýýýýý noun.feeling
    ýýýýýýýýý noun.food
    ýýýýýýýýý noun.group
    ýýýýýýýýý noun.location
    ýýýýýýýýý noun.motive
    ýýýýýýýýý noun.object
    ýýýýýýýýý noun.person
    ýýýýýýýýý noun.phenomenon
    ýýýýýýýýý noun.plant
    ýýýýýýýýý noun.possession
    ýýýýýýýýý noun.process
    ýýýýýýýýý noun.quantity
    ýýýýýýýýý noun.relation
    ýýýýýýýýý noun.shape
    ýýýýýýýýý noun.state
    ýýýýýýýýý noun.substance
    ýýýýýýýýý noun.time
    ýýýýýýýýý noun.tops
    ýýýýýýýýý verb.body
    ýýýýýýýýý verb.change
    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ verb.cognition
    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ verb.communication
    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ verb.competition
    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ verb.consumption
    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ verb.contact
    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ verb.creation
    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ verb.emotion
    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ verb.framestext
    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ verb.motion
    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ verb.perception
    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ verb.possession
    ï¿½ï¿½ï¿½ï¿½ï¿½ï","['python', 'nltk', 'wordnet']",65405555,"after a lot of searching and trial and error, i was able to use wordnet 3.1 on nltk (python). i tweaked this gist to make it work. i am providing the details below.
i divided the code provided in the gist in 3 parts.
part 1. download_extract.py
import os

nltkdata_wn = '/path/to/nltk_data/corpora/wordnet/'
wn31 = ""

if not os.path.exists(nltkdata_wn+'_3.0'):
    os.mkdir(nltkdata_wn+'_3.0')
os.system('mv '+nltkdata_wn+""* ""+nltkdata_wn+""_3.0/"")

if not os.path.exists('wn3.1.dict.tar.gz'):
    os.system('wget '+wn31)

os.system(""tar zxf wn3.1.dict.tar.gz -c ""+nltkdata_wn)
os.system(""mv ""+nltkdata_wn+""dict/* ""+nltkdata_wn)
os.rmdir(nltkdata_wn + 'dict')

this is used to back up the existing wordnet 3.0 folder from wordnet to wordnet_3.0, download the wordnet 3.1 database, and put it in folder wordnet. since i am on a windows system, i did this manually.
part 2. create_lexnames.py
import os

nltkdata_wn = '/path/to/nltk_data/corpora/wordnet/'
dbfiles = nltkdata_wn+'dbfiles'

with open(nltkdata_wn+'lexnames', 'w') as fout:
    for i,j in enumerate(sorted(os.listdir(dbfiles))):
        pos = j.partition('.')[0]
        if pos == ""noun"":
            syncat = 1
        elif pos == ""verb"":
            syncat = 2
        elif pos == ""adj"":
            syncat = 3
        elif pos == ""adv"":
            syncat = 4
        elif j == ""cntlist"":
            syncat = ""cntlist""
        fout.write(""\t"".join([str(i).zfill(2),j,str(syncat)])+""\n"")

this creates the required lexnames file in the wordnet folder.
part 3. testing_wn31.py
from nltk.corpus import wordnet as wn

nltkdata_wn = '/path/to/nltk_data/corpora/wordnet/'

# checking generated lexnames file.
for i, line in enumerate(open(nltkdata_wn + 'lexnames','r')):
    index, lexname, _ = line.split()
    ##print line.split(), int(index), i
    assert int(index) == i

# testing wordnet function.
print(wn.synsets('dog'))
for i in wn.all_synsets():
    print(i, i.pos(), i.definition())

this tested the generated lexname file and also tested if the wordnet functions are working fine.
once i am done with this procedure, i ran following code in python and found that it is actually running version 3.1
>>> from nltk.corpus import wordnet
>>> wordnet.get_version()
'3.1'

a word of caution
once you replace the wordnet 3.1 database, you'll notice that if you run the following code
>>> import nltk
>>> nltk.download()

in the download dialog box, you will see that under corpora tab, wordnet will be shown as out of date, you should not try to update it as it will either replace the wordnet to version 3.0 or break it.",https://stackoverflow.com/questions/65398096,python,21-12-2020 18:27,2537.0,0.0,1.0,True,14-01-2022 13:56,14-01-2022 13:56
32733510,nltk agreement with distance metric,"i have a task to calculate inter-annotator agreement in multi-label classification, where for each example more than one label can be assigned. i found that nltk can measure agreement based on a distance metric.
i am looking for an example of calculating krippendorff alpha with masi distance.
this is what i have.
import nltk
from nltk.metrics import masi_distance


toy_data = [['1', 5723, [1,2]],['2', 5723, [2,3]]]

task = nltk.metrics.agreement.annotationtask(data=toy_data, distance=masi_distance)
print task.alpha()

this code fails with
typeerror: unhashable type: 'list'

the following doesn't work either:
toy_data = [['1', 5723, set([1,2])],['2', 5723, set([2,3])]]

do you have a working example?
thank you!","['python', 'machine-learning', 'nlp', 'nltk']",38877728,"to be more precise, what needs to be a frozenset (as @alexis has pointed out) is just the third member of the triple, this is the labels assigned to the item.
toy_data = [['1', 5723, frozenset([1,2])],['2', 5723, frozenset([2,3])]]",https://stackoverflow.com/questions/32733510,python,23-09-2015 07:28,1607.0,6.0,2.0,True,09-05-2024 14:07,17-04-2022 05:49
75622369,can anyone confirm if  is a real ec2 meta-data endpoint?,"i was recently testing chatgpt on terraform scripts and came across something that i can't tell if it's a gpt ""cannon fodder"" or if this is an undocumented feature.
on a trained chatgpt session i input:

generate a bash script that can use random_pet resources from terraform

and this was the output:
#!/bin/bash

# download metadata for the random_pet resource from the terraform state file
metadata=$(curl -s 

# parse the metadata and extract the value of the ""id"" attribute
id=$(echo $metadata | jq -r '.random_pet_generator_id')

# use the value of the ""id"" attribute in your bash script
echo ""hello, $id!""

this looked bogus to me:



until i read about referring to alternate provider configurations","['amazon-web-services', 'amazon-ec2', 'terraform', 'terraform-provider-aws', 'openai-api']",75678716,"@marcin, i'm pretty sure that's not a real endpoint but hidden features are always a possibility. i ended up with:
foo.tf
  # enables access to instance tags from the instance metadata service.
  metadata_options {
    instance_metadata_tags = ""enabled""
              = ""enabled""
  }

  # apply tags to the resources during launch (does not include default_tags)
  dynamic ""tag_specifications"" {
    for_each = toset([""instance"", ""volume"", ""network-interface""])
    content {
      resource_type = tag_specifications.key
      tags = {
        baz = random_pet.generator.id
      }
    }
  }

bar.sh
baz_tag=$(curl -s 

i'm really curious how chatgpt derived that endpoint. it definitely makes mistakes sometimes but even the mistakes are based on some element of truth.",https://stackoverflow.com/questions/75622369,amazon-web-services,03-03-2023 01:19,160.0,-3.0,2.0,True,08-03-2023 22:13,03-03-2023 01:26
74428413,understanding dimensions in multiheadattention layer of tensorflow,"i'm learning multi-head attention with this article.
as the writer claimed, the structure of mha (by the original paper) is as follows:

but the multiheadattention layer of tensorflow seems to be more flexible:

it does not require key_dim * num_heads = embed_dim. like:

layer = tf.keras.layers.multiheadattention(num_heads = 2, key_dim = 4)
x = tf.keras.input(shape=[3, 5])
layer(x, x)
# no error

does the depth of the weight matrix in tf.mha layer set to key_dim * num_heads regardless of embed_dim? so that q/k/v can still be properly split by num_heads.

however, the output depth of tf.mha layer is (by default) guaranteed to be embed_dim. so there is a final dense layer with embed_dim nodes to ensure the dimensionï¿½ï¿½ï¿½","['tensorflow', 'nlp', 'transformer-model', 'attention-model']",74430271,"yes, for 1 & 2. you can probe the weights by:
layer = tf.keras.layers.multiheadattention(num_heads = 2, key_dim = 4, use_bias=false) #set use_bias=false for simplicity.
x = tf.keras.input(shape=[3, 5])
layer(x, x)

get the weights associated,
weight_names = ['query', 'keys',  'values', 'proj']
for name, out in zip(weight_names,layer.get_weights()):
    print(name, out.shape)

output shapes:
query (5, 2, 4) # (embed_dim, num_heads, key_dim)
keys (5, 2, 4)  # (embed_dim, num_heads, key_dim)
values (5, 2, 4) # (embed_dim, num_heads, value_dim/key_dim)
proj (2, 4, 5)  # (num_heads, key_dim, embed_dim)",https://stackoverflow.com/questions/74428413,tensorflow,14-11-2022 07:50,5038.0,5.0,2.0,True,15-11-2022 09:04,15-11-2022 09:04
48925328,how to get all noun phrases in spacy,"i am new to spacy and i would like to extract ""all"" the noun phrases from a sentence. i'm wondering how i can do it. i have the following code:
import spacy

nlp = spacy.load(""en"")

file = open(""e:/test.txt"", ""r"")
doc = nlp(file.read())
for np in doc.noun_chunks:
    print(np.text)

but it returns only the base noun phrases, that is, phrases which don't have any other np in them. that is, for the following phrase, i get the result below:
phrase: we try to explicitly describe the geometry of the edges of the images.
result: we, the geometry, the edges, the images.
expected result: we, the geometry, the edges, the images, the geometry of the edges of the images, the edges of the images.
how can i get all the noun phrases, including nested phrases?","['python', 'python-3.x', 'nlp', 'spacy']",48941123,"please see commented code below to recursively combine the nouns. code inspired by the spacy docs here
import spacy

nlp = spacy.load(""en"")

doc = nlp(""we try to explicitly describe the geometry of the edges of the images."")

for np in doc.noun_chunks: # use np instead of np.text
    print(np)

print()

# code to recursively combine nouns
# 'we' is actually a pronoun but included in your question
# hence the token.pos_ == ""pron"" part in the last if statement
# suggest you extract pron separately like the noun-chunks above

index = 0
nounindices = []
for token in doc:
    # print(token.text, token.pos_, token.dep_, token.head.text)
    if token.pos_ == 'noun':
        nounindices.append(index)
    index = index + 1


print(nounindices)
for idxvalue in nounindices:
    doc = nlp(""we try to explicitly describe the geometry of the edges of the images."")
    span = doc[doc[idxvalue].left_edge.i : doc[idxvalue].right_edge.i+1]
    span.merge()

    for token in doc:
        if token.dep_ == 'dobj' or token.dep_ == 'pobj' or token.pos_ == ""pron"":
            print(token.text)",https://stackoverflow.com/questions/48925328,python,22-02-2018 10:41,14832.0,12.0,5.0,True,25-04-2024 05:48,25-04-2024 05:48
75266086,how to use gpu for training instead of cpu?,"i was replicating the code which is fine-tuned for domain adaptation. this is the main link to the post for more details:
(
the code is as such:
!pip install -q transformers
!pip install -q datasets

import multiprocessing
import pandas as pd
import numpy as np
import torch
import matplotlib.pyplot as plt
import transformers
from datasets import dataset
from sklearn.model_selection import train_test_split

from transformers import automodelformaskedlm
from transformers import autotokenizer, autoconfig
from transformers import bertformaskedlm, distilbertformaskedlm
from transformers import berttokenizer, distilberttokenizer
from transformers import robertatokenizer, robertaformaskedlm
from transformers import trainer, trainingarguments
from transformers import datacollatorforlanguagemodeling
from tokenizers import bertwordpiecetokenizer

# hyperparams
seed_split = 0
seed_train = 0

max_seq_len = 128
train_batch_size = 16
eval_batch_size = 16
learning_rate = 2e-5 
lr_warmup_steps = 100
weight_decay = 0.01

# load data
dtf_mlm = pd.read_csv('data/jigsaw_train.csv', nrows=1000)
dtf_mlm = dtf_mlm[dtf_mlm[""target""] < 0.5]
dtf_mlm = dtf_mlm.rename(columns={""comment_text"": ""text""})


# train/valid split
df_train, df_valid = train_test_split(
    dtf_mlm, test_size=0.15, random_state=seed_split
)

len(df_train), len(df_valid)

# convert to dataset object
train_dataset = dataset.from_pandas(df_train[['text']].dropna())
valid_dataset = dataset.from_pandas(df_valid[['text']].dropna())

#model selection part
model = 'bert'
bert_type = 'bert-base-cased'

tokenizerclass = berttokenizer
modelclass = bertformaskedlm 


#tokenization part
tokenizer = tokenizerclass.from_pretrained(
            bert_type, use_fast=true, do_lower_case=false, max_len=max_seq_len
            )
model = modelclass.from_pretrained(bert_type)


def tokenize_function(row):
    return tokenizer(
        row['text'],
        padding='max_length',
        truncation=true,
        max_length=max_seq_len,
        return_special_tokens_mask=true)
  
column_names = train_dataset.column_names

train_dataset = train_dataset.map(
    tokenize_function,
    batched=true,
    num_proc=multiprocessing.cpu_count(),
    remove_columns=column_names,
)

valid_dataset = valid_dataset.map(
    tokenize_function,
    batched=true,
    num_proc=multiprocessing.cpu_count(),
    remove_columns=column_names,
)


#training and model saving part
data_collator = datacollatorforlanguagemodeling(
    tokenizer=tokenizer, mlm=true, mlm_probability=0.15
)


steps_per_epoch = int(len(train_dataset) / train_batch_size)

training_args = trainingarguments(
    output_dir='./bert-news',
    logging_dir='./lmlogs',             
    num_train_epochs=2,
    do_train=true,
    do_eval=true,
    per_device_train_batch_size=train_batch_size,
    per_device_eval_batch_size=eval_batch_size,
    warmup_steps=lr_warmup_steps,
    save_steps=steps_per_epoch,
    save_total_limit=3,
    weight_decay=weight_decay,
    learning_rate=learning_rate, 
    evaluation_strategy='epoch',
    save_strategy='epoch',
    load_best_model_at_end=true,
    metric_for_best_model='loss', 
    greater_is_better=false,
    seed=seed_train
)

trainer = trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    tokenizer=tokenizer,
)

trainer.train()
trainer.save_model(""savedmodel/testmodel"") #save your custom model



and this is the gpu that i am using:

i want to use the gpu for training the model on about 1.5 million comments.
i tried doing this:
device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")


#setting the tokenizer and the model
tokenizer = tokenizerclass.from_pretrained(
            bert_type, use_fast=true, do_lower_case=false, max_len=max_seq_len
            )
model = modelclass.from_pretrained(bert_type).to(device)


but i am unsure how to send the inputs and tokens to the gpu.
feel free to give your advice, and i don't owe this code, shout out to marcello politi. thanks!","['pytorch', 'gpu', 'bert-language-model', 'pre-trained-model']",75320388,"after you load the dataset you should add:
device = 'cuda' if torch.cuda.is_available() else 'cpu'
train_dataset = train_dataset.to(device)",https://stackoverflow.com/questions/75266086,pytorch,28-01-2023 07:33,2231.0,0.0,1.0,True,03-02-2023 07:42,28-01-2023 08:23
74773173,"dall e api error: &quot;invalid input image - format must be in [&#39;rgba&#39;, &#39;la&#39;, &#39;l&#39;], got rgb.&quot;","i have an image that i'm retrieving from an aws s3 bucket and then passing to the dall e/openai api. when i attempt i get this error response:
message: ""invalid input image - format must be in ['rgba', 'la', 'l'], got rgb."",

i understand that rgb (the image file type i'm trying to upload) contains an alpha channel, which essentially means transparent areas on the image. is it possible/easy to validate image types in nodejs to catch bad images before i send them to the api?
my s3 gets a .png file like this:
    const data = await s3client.send(
      new getobjectcommand({
        ...bucketparams, // bucket: <bucket name>
        key: `public/dalle/${inputparams.key}`,
      })
    );

and then i pass that to the api via the openai library:
    const response = await openai.createimageedit(
      data.body as unknown as file,
      (maskimagebuffer as unknown as file) || data.body, 
      prompt,
      1,
      ""256x256""
    );","['node.js', 'typescript', 'amazon-s3', 'openai-api']",75686551,"you can use jimp
  let jimage = await jimp.read(imagebuffer);

  const w = jimage.bitmap.width; 
  const h = jimage.bitmap.height;

  if ((w / h) != 1) {
    throw new functions.
        
            ""image must be a square. current ratio = "" + (w/h));
  }

  if (!jimage.hasalpha()) { //check if image has opacity
    jimage = jimage.opacity(1); //add if it doesn't 
  }

  const jsize = (await jimage.getbufferasync(jimp.auto)).bytelength;

  if (jsize >= 4000000) { //check size
    throw new functions.
        
            ""image must be less than 4mg currenty image is "" +
           jsize + "" bytes with alpha"");
  }

  jimage.write(""/tmp/filename.png""); //make png",https://stackoverflow.com/questions/74773173,node.js,12-12-2022 15:08,6386.0,6.0,3.0,True,20-08-2023 16:54,12-12-2022 18:43
75613656,openai chat completions api: how do i extract the message content from the response?,"when receiving a response from openai's text-davinci-003 model, i was able to extract the text from the response with the following php code:
$response = $response->choices[0]->text;

here was the text-davinci-003 response code:
{
  ""id"": ""cmpl-uqkvlqyyk7bgyrrhq0exlwi7"",
  ""object"": ""text_completion"",
  ""created"": 1589478378,
  ""model"": ""text-davinci-003"",
  ""choices"": [
    {
      ""text"": ""\n\nthis is indeed a test"",
      ""index"": 0,
      ""logprobs"": null,
      ""finish_reason"": ""length""
    }
  ],
  ""usage"": {
    ""prompt_tokens"": 5,
    ""completion_tokens"": 7,
    ""total_tokens"": 12
  }
}

i am now trying to alter my code to work with the recently released gpt-3.5-turbo model which returns the response slightly differently:
{
  ""id"": ""chatcmpl-123"",
  ""object"": ""chat.completion"",
  ""created"": 1677652288,
  ""choices"": [{
    ""index"": 0,
    ""message"": {
      ""role"": ""assistant"",
      ""content"": ""\n\nhello there, how may i assist you today?"",
    },
    ""finish_reason"": ""stop""
  }],
  ""usage"": {
    ""prompt_tokens"": 9,
    ""completion_tokens"": 12,
    ""total_tokens"": 21
  }
}

my question is, how can i alter the code:
$response = $response->choices[0]->text;

...so that it can grab the content of the response message?","['php', 'openai-api', 'chatgpt-api']",75613704,"python:
ï¿½ï¿½ï¿½ if you have the openai python sdk <v1:
print(response['choices'][0]['message']['content'])

ï¿½ï¿½ï¿½ if you have the openai python sdk v1:
print(response.choices[0].message.content)

node.js:
ï¿½ï¿½ï¿½ if you have the openai node.js sdk v3:
console.log(response.data.choices[0].message.content);

ï¿½ï¿½ï¿½ if you have the openai node.js sdk v4:
console.log(response.choices[0].message.content);

php:
var_dump($response->choices[0]->message->content);


working example in php
if you run test.php, the openai api will return the following completion:

string(40) ""
the capital city of england is london.""

test.php
<?php
    $ch = curl_init();

openai.com/v1/chat/completions';

    $api_key = 'sk-xxxxxxxxxxxxxxxxxxxx';

    $query = 'what is the capital city of england?';

    $post_fields = array(
        ""model"" => ""gpt-3.5-turbo"",
        ""messages"" => array(
            array(
                ""role"" => ""user"",
                ""content"" => $query
            )
        ),
        ""max_tokens"" => 12,
        ""temperature"" => 0
    );

    $header  = [
        'content-type: application/json',
        'authorization: bearer ' . $api_key
    ];

    curl_setopt($ch, curlopt_url, $url);
    curl_setopt($ch, curlopt_returntransfer, 1);
    curl_setopt($ch, curlopt_post, 1);
    curl_setopt($ch, curlopt_postfields, json_encode($post_fields));
    curl_setopt($ch, curlopt_ $header);

    $result = curl_exec($ch);
    if (curl_errno($ch)) {
        echo 'error: ' . curl_error($ch);
    }
    curl_close($ch);

    $response = json_decode($result);
    var_dump($response->choices[0]->message->content);
?>",https://stackoverflow.com/questions/75613656,php,02-03-2023 09:42,12783.0,2.0,2.0,True,12-06-2024 17:30,12-06-2024 17:30
58057884,what does &quot;text degeneration&quot; mean?,"in machine learning, especially nlp, what does it mean to degenerate a text?
i heard this phrase some days ago in my office and after googling it i saw there are some papers for it, so i thought it might be important and i'm here to aks about the terminology.","['machine-learning', 'deep-learning', 'nlp', 'terminology']",58058501,"this termin, according to this article, means the situations in text generation process when eigther generator model find the state x such that g(x) = x which means that generated text is repeated infinitely, or, according to error state in the middle of generation process, the model starts to reproduce incoherent text patterns.",https://stackoverflow.com/questions/58057884,machine-learning,23-09-2019 07:54,564.0,0.0,1.0,True,07-04-2022 13:06,07-04-2022 13:06
75112896,why open ai api returns a random text,"i'm using apps script to create a video title out of input text. but i get irrelevant gibberish-type text in return, a mixture of some code, symbols, and random text. i have also tried different prompts but it didn't solve the issue. however, it returns good results on the website (chatgpt).
var urlfetchapp = urlfetchapp || import.google.script.url;
var apiurl = ""
var apikey = ""*****************"";

function generatecontentformultiplerows(myrange) {
  var bulletpoints = myrange.getvalues();

  bulletpoints.foreach(function (row, i) {
    var prompt1 = ""create a title of up to 6 words that sums up this content"" + row[0];

    var options1 = {
      ""method"": ""post"",
      ""headers"": {
        ""content-type"": ""application/json"",
        ""authorization"": ""bearer "" + apikey
      },
      ""payload"": json.stringify({
        ""prompt"": prompt1,
        ""max_tokens"": 100
      })
    };
    var response1 = urlfetchapp.fetch(apiurl, options1);
    var json1 = json.parse(response1.getcontenttext());
    var generatedcontent1 = json1.choices[0].text;

    spreadsheetapp.getactivesheet().getrange(2 + i, 4).setvalue(generatedcontent1);
  });
}


here is the input: ""income threshold for social media must be increased
government need to do more""

and here is what i get in output:
"" to provide universal access to the internet access for all people in the developing countries
use a for loop on the words to make similar sentences
s = ["""", government need to do more to provide universal access to the internet access for all people in the developing countries"""",
""""income threshold for social media must be increased, """",]
for l in range(10):
for k in range(3):
new_s""","['google-apps-script', 'google-sheets', 'urlfetch', 'openai-api']",75113265,"i have solved it by changing api url:
when i added 'model' in payload it raised an error that you can't specify engine and model both.
so, i deleted engine from url and kept 'model' in payload only. that's it, it's working ok now.",https://stackoverflow.com/questions/75112896,google-apps-script,13-01-2023 18:00,1314.0,-2.0,3.0,True,03-06-2023 22:22,14-01-2023 05:20
5390397,bleu score implementation for sentence similarity detection,i need to calculate bleu score for identifying whether two sentences are similar or not.i have read some articles which are mostly about bleu score for measuring machine translation accuracy.but i'm in need of a bleu score to find out similarity between sentences in a same language[english].(i.e)(both the sentences are in english).thanks in anticipation.,"java, algorithm, nlp, text-processing, machine-translation",5394035,"well, if you just want to calculate the bleu score, it's straightforward.  treat one sentence as the reference translation and the other as the candidate translation.",https://stackoverflow.com/q/5390397,"java, algorithm, nlp, text-processing, machine-translation",22-03-2011 11:22,14270.0,7.0,6.0,True,20-09-2021 13:05,06-03-2015 15:36
77673353,how to use adapter transformers with a huggingface pipeline,"i tried to run the model ""adapterhub/bert-base-uncased-pf-conll2003"" (model description here) for token classification in nlp.
first i tried to install the adapter transformers
pip install -u adapter-transformers 

the output of the above command was
collecting adapter-transformers

[... see edit history for skipped lines ...]

installing collected packages: tokenizers, huggingface-hub, adapter-transformers
  attempting uninstall: tokenizers
    found existing installation: tokenizers 0.15.0
    uninstalling tokenizers-0.15.0:
      successfully uninstalled tokenizers-0.15.0
  attempting uninstall: huggingface-hub
    found existing installation: huggingface-hub 0.19.4
    uninstalling huggingface-hub-0.19.4:
      successfully uninstalled huggingface-hub-0.19.4
error: pip's dependency resolver does not currently take into account all the packages that are installed. this behaviour is the source of the following dependency conflicts.
transformers 4.35.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.13.4 which is incompatible.
transformers 4.35.2 requires tokenizers<0.19,>=0.14, but you have tokenizers 0.13.3 which is incompatible.
successfully installed adapter-transformers-3.2.1.post0 huggingface-hub-0.13.4 tokenizers-0.13.3


i tried to load the model like this into the pipeline:
from transformers import automodelwithheads
from transformers import pipeline
token_classification = pipeline(""token-classification"", model = ""adapterhub/bert-base-uncased-pf-conll2003"")
res = token_classification(""take out the trash bag from the bin and replace it."")
print(res)

i received the errors
entrynotfounderror: 404 client error. (request id: root=1-657e793c-0ce0c1936aff5e5741676650)

entry not found for url: 

during handling of the above exception, another exception occurred:


oserror                                   traceback (most recent call last)
<ipython-input-3-030dfe0e128d> in <cell line: 3>()
      1 from transformers import automodelwithheads
      2 from transformers import pipeline
----> 3 token_classification = pipeline(""token-classification"", model = ""adapterhub/bert-base-uncased-pf-conll2003"")
      4 res = token_classification(""take out the trash bag from the bin and replace it."")
      5 print(res)

/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py in pipeline(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)
    673         hub_kwargs[""_commit_hash""] = config._commit_hash
    674     elif config is none and isinstance(model, str):
--> 675         config = autoconfig.from_pretrained(model, _from_pipeline=task, **hub_kwargs, **model_kwargs)
    676         hub_kwargs[""_commit_hash""] = config._commit_hash
    677 

 [... see edit history for skipped lines ...]

/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py in _get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    624             try:
    625                 # load from local folder or from cache or download from model hub and cache
--> 626                 resolved_config_file = cached_file(
    627                     pretrained_model_name_or_path,
    628                     configuration_file,

/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)
    452         if revision is none:
    453             revision = ""main""
--> 454         raise environmenterror(
    455             f""{path_or_repo_id} does not appear to have a file named {full_filename}. checkout ""
    456             f""' for available files.""

oserror: adapterhub/bert-base-uncased-pf-conll2003 does not appear to have a file named config.json.
checkout ' for available files.

how do i correctly load this adapter model?","['python', 'machine-learning', 'nlp', 'huggingface-transformers']",77675813,"# be sure you have the dependencies (new)
$ pip install adapters 

the old & legacy package is pip install -u adapter-transformers

create the model outside of the pipeline
from transformers import automodelwithheads
from transformers import pipeline
from transformers import autotokenizer

tokenizer = autotokenizer.from_pretrained(""bert-base-uncased"")
model = automodelwithheads.from_pretrained(""bert-base-uncased"")
adapter_name = model.load_adapter(""adapterhub/bert-base-uncased-pf-conll2003"", source=""hf"")
model.active_adapters = adapter_name

token_classification = pipeline(""token-classification"", model=model, tokenizer=tokenizer)
res = token_classification(""take out the trash bag from the bin and replace it."")
print(res)",https://stackoverflow.com/questions/77673353,python,17-12-2023 04:49,900.0,0.0,1.0,True,13-06-2024 15:03,13-06-2024 15:03
71496947,how can i optimize my code to inverse transform the output of textvectorization?,"i'm using a textvectorization layer in a tf keras sequential model. i need to convert the intermediate textvectorization layer's output to plain text. i've found that there is no direct way to accomplish this. so i used the textvectorization layer's vocabulary to inverse transform the vectors. the code is as follows:
from tensorflow.keras.layers.experimental.preprocessing import textvectorization
text_list = np.array([""this is the first sentence."",""second line of the dataset.""]) # a list of 2 sentences
textvectorizer = textvectorization(max_tokens=none,
            standardize=none,
            ngrams=none,
            output_mode=""int"",
            output_sequence_length=none,
            pad_to_max_tokens=false)
textvectorizer.adapt(text_list)
vectors = textvectorizer(text_list)
vectors 


vectors:
array([[ 3,  7,  2,  9,  4],
       [ 5,  6,  8,  2, 10]])

now, i want to convert the vectors to sentences.
my_vocab = textvectorizer.get_vocabulary()
plain_text_list = []
for line in vectors:
    text = ' '.join(my_vocab[idx] for idx in line)
    plain_text_list.append(text)

print(plain_text_list)

output:
['this is the first sentence.', 'second line of the dataset.']

i was successful in obtaining the desired result. however, due to the naive approach i used in the code, when applied to a large dataset, this method is extremely slow. i'd like to reduce the execution time of this method.","['python', 'tensorflow', 'keras', 'nlp']",71497068,"maybe try np.vectorize:
import numpy as np

my_vocab = textvectorizer.get_vocabulary()
index_vocab =  dict(zip(np.arange(len(my_vocab)), my_vocab))
print(np.vectorize(index_vocab.get)(vectors))

[['this' 'is' 'the' 'first' 'sentence.']
 ['second' 'line' 'of' 'the' 'dataset.']]

performance test:
import numpy as np
import timeit

my_vocab = textvectorizer.get_vocabulary()

def method1(my_vocab, vectors):
  index_vocab =  dict(zip(np.arange(len(my_vocab)), my_vocab))
  return np.vectorize(index_vocab.get)(vectors)

def method2(my_vocab, vectors):
  plain_text_list = []
  for line in vectors:
      text = ' '.join(my_vocab[idx] for idx in line)
      plain_text_list.append(text)
  return plain_text_list

t1 = timeit.timer(lambda: method1(my_vocab, vectors))
t2 = timeit.timer(lambda: method2(my_vocab, vectors)) 

print(t1.timeit(5000))
print(t2.timeit(5000))

0.3139600929998778
19.671524284000043",https://stackoverflow.com/questions/71496947,python,16-03-2022 12:18,439.0,1.0,1.0,True,16-03-2022 13:13,16-03-2022 13:13
69215446,how can i use ensemble learning of two models with different features as an input?,"i have a fake news detection problem and it predicts the binary labels ""1""&""0"" by vectorizing the 'tweet' column, i use three different models for detection but i want to use the ensemble method to increase the accuracy but they use different vectorezer.

i have 3 knn models the first and the second one vectorizes the 'tweet' column using tf-idf.

from sklearn.feature_extraction.text import tfidfvectorizer
    vector = tfidfvectorizer(max_features =5000, ngram_range=(1,3))
    x_train = vector.fit_transform(x_train['tweet']).toarray()
    x_test = vector.fit_transform(x_test['tweet']).toarray()


for the third model i used fasttext for sentence vectorization

%%time
sent_vec = []
for index, row in x_train.iterrows():
    sent_vec.append(avg_feature_vector(row['tweet']))
%%time
sent_vec1 = []
for index, row in x_test.iterrows():
    sent_vec1.append(avg_feature_vector(row['tweet']))


after scaling and... my third model fits the input like this

scaler.fit(sent_vec)
scaled_x_train= scaler.transform(sent_vec)
scaled_x_test= scaler.transform(sent_vec1)
.
.
.
knn_model1.fit(scaled_x_train, y_train)


now i want to combine the three models like this and i want the ensemble method to give me the majority just likevotingclassifier, but i have no idea how can i deal with the different inputs (tf-idf & fasttext) is there another way to do that?","['python', 'machine-learning', 'nlp', 'tf-idf', 'ensemble-learning']",69218787,"you can create a custom myvotingclassifier which takes a fitted model instead of a model instance yet to be trained. in votingclassifier, sklearn takes just the unfitted classifiers as input and train them and then apply voting on the predicted result. you can create something like this. the below function might not be the exact function but you can make quite similar function like below for your purpose.
from collections import counter
clf1 = knn_model_1.fit(x1, y)
clf2 = knn_model_2.fit(x2, y)
clf3 = knn_model_3.fit(x3, y)

class myvotingclassifier:
    def __init__(self, **models):
        self.models = models
    
    def predict(dict_x):
        '''
        dict_x = {'knn_model_1': x1, 'knn_model_2': x2, 'knn_model_3': x3}
        '''
        preds = []
        for model_name in dict_x:
            model = self.models[model_name]
            preds.append(model.predict(dict_x[model_name]))
        preds = list(zip(*preds))
        final_pred = list(map(lambda x: counter(x).most_common(1)[0][0]))
        return final_pred
ensemble_model = myvotingclassifier(knn_model_1=clf1, knn_model_2=clf2, knn_model_3=clf3)
ensemble_model.predict({'knn_model_1': x1, 'knn_model_2': x2, 'knn_model_3': x3}) # input the pre-processed `x`s",https://stackoverflow.com/questions/69215446,python,16-09-2021 21:35,1024.0,1.0,1.0,True,17-09-2021 17:27,16-09-2021 23:55
74664286,converting a dataset to conll format. label remaining tokens with o,"i have a manually annotated dataset that contains records in the following format:
{
    ""id"": 1,
    ""text"": ""at the end of each fiscal quarter, for the four consecutive fiscal quarters ending as of such fiscal quarter end, from the date of the third amendment and until december 30, 1996, the company shall maintain a fixed charge coverage ratio of not less than 1.25 to 1.0."",
    ""label"": [
        [
            209,
            230,
            ""cov_3""
        ],
        [
            379,
            390,
            ""val_3""
        ]
    ],
}

in the example above, ""label"" represents the custom entities i have in my dataset. in the example shown above, the phrase fixed charge coverage is located at position [309, 336] and is given the label cov_3. likewise, the phrase 1.25 to 1.0 is located at [379, 390] and is given the label val_3.
now, i would like to fine-tune some transformer model like bert on this dataset, however, i realised that the dataset must be in conll format. or at least, all the tokens of each datapoint must be labelled. is there any way i can easily label the remaining tokens with label ""o"" or i can transform this dataset in the conll format?","['nlp', 'stanford-nlp', 'huggingface-transformers', 'named-entity-recognition']",74701797,"you use spacy to tokenize and convert character offset annotation to iob tags with built-in utility methods. note that this will skip any spans that don't align to the token boundaries, so you may need to customize the tokenizer or provide the tokenization from another source when creating a doc.
the character offsets in the question don't line up with the text and are modified below.
# tested with spacy v3.4.3, should work with spacy v3.x
import spacy
from spacy.training.iob_utils import biluo_to_iob, doc_to_biluo_tags

data = {
    ""id"": 1,
    ""text"": ""at the end of each fiscal quarter, for the four consecutive fiscal quarters ending as of such fiscal quarter end, from the date of the third amendment and until december 30, 1996, the company shall maintain a fixed charge coverage ratio of not less than 1.25 to 1.0."",
    ""label"": [[209, 230, ""cov_3""], [254, 265, ""val_3""]],
}

nlp = spacy.blank(""en"")

# tokenize the text to create a doc
doc = nlp(data[""text""])

# convert annotation to entity spans and add them to the doc
ents = []
for start, end, label in data[""label""]:
    span = doc.char_span(start, end, label=label)
    if span is not none:
        ents.append(span)
    else:
        print(
            ""skipping span (does not align to tokens):"",
            start,
            end,
            label,
            doc.text[start:end],
        )
doc.ents = ents

# convert doc annotation to iob tags
for token, iob_tag in zip(doc, biluo_to_iob(doc_to_biluo_tags(doc))):
    print(token.text + "" "" + iob_tag)

output:
at o
the o
end o
of o
each o
fiscal o
quarter o
, o
for o
the o
four o
consecutive o
fiscal o
quarters o
ending o
as o
of o
such o
fiscal o
quarter o
end o
, o
from o
the o
date o
of o
the o
third o
amendment o
and o
until o
december o
30 o
, o
1996 o
, o
the o
company o
shall o
maintain o
a o
fixed b-cov_3
charge i-cov_3
coverage i-cov_3
ratio o
of o
not o
less o
than o
1.25 b-val_3
to i-val_3
1.0 i-val_3
. o

these are the 1st and 4th columns from the 4-column conll 2003 format. you may want to insert blank lines for sentence boundaries or add the special document boundary lines, and you may need some real or placeholder values for the 2nd/3rd tag and chunk columns for use with other tools.",https://stackoverflow.com/questions/74664286,nlp,03-12-2022 05:41,1046.0,4.0,1.0,True,07-12-2022 05:17,07-12-2022 05:17
76633368,how does one set the pad token correctly (not to eos) during fine-tuning to avoid model not predicting eos?,"tldr; what i really want to know is what is the official way to set pad token for fine tuning it wasn't set during original training, so that it doesn't not learn to predict eos.
colab: 

the hf falcon tutorial has the following line:
tokenizer.pad_token = tokenizer.eos_token

it looks strange to me. it make sense pad and eos are the same but then why even make a difference between them in the first place in general?
note its wrong to do pad = eos. this means during fine-tuning the model will never be trained to output eos (most likely) since eos is treated as pad token and no back propagated:
i just observed that when i set tokenizer.pad_token = tokenizer.eos_token during training, the model won't stop generating during inference, since it was trained to not output the eos token (per discussions above).

i saw this (here 
tokenizer.add_special_tokens({'pad_token': '[pad]'})

but this assumes the model has a pad_token. i think an additional check has to be done that it does have an embedding for pad_token so that there are no run time errors (~type errors in the matrix extraction from the embedding ""table""/matrix).
but if one does that some care might be needed to initialize the new token so that it dominates the generation: 

code:
def get_model_tokenizer_qlora_falcon7b(model_name: str = ""ybelkada/falcon-7b-sharded-bf16"",
                                       config: wand.config,  # todo
                                       lora_alpha=16,  # todo
                                       lora_dropout=0.1,  # todo
                                       lora_r=64,  # todo
                                       bnb_4bit_compute_dtype=torch.float16,  # changed it from guanaco hf
                                       ) -> tuple:
    """"""
    load the falcon 7b model, quantize it in 4bit and attach lora adapters on it.

    bf16 = 1s, 7exp, 8mantissa

    do:
        pip install bitsandbytes
    ref:
        - 
    """"""
    from transformers import automodelforcausallm, autotokenizer, bitsandbytesconfig, autotokenizer

    # model_id = ""tiiuae/falcon-7b""
    # model_name: str = ""ybelkada/falcon-7b-sharded-bf16""

    # - get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)
    bnb_config = bitsandbytesconfig(
        load_in_4bit=true,  # load (usually huge) base model in 4 bits
        bnb_4bit_quant_type=""nf4"",  # normal float 4 for the (usually huge) base model. introduces error but fixed by ft
        # ref: 
        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,
    )

    # - get falcon 4bit model
    model = automodelforcausallm.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        trust_remote_code=true  # allows to execute custom code you download from the uploaded model code you are using
    )
    model.config.use_cache = false  # todo: why? 

    # get falcon tockenizer
    tokenizer = autotokenizer.from_pretrained(model_name, trust_remote_code=true)  # execs code downloaded from hf hub
    tokenizer.pad_token = tokenizer.eos_token


modifying model gives issues
this still not works:
 userwarning: you have modified the pretrained model configuration to control generation. this is a deprecated strategy to control generation and will be removed soon, in a future version. please use a generation configuration file (see 

code:
""""""
sfttrainer (likely using peft) best practices:


best practices

pay attention to the following best practices when training a model with that trainer:

- sfttrainer always pads by default the sequences to the max_seq_length argument of the sfttrainer. if none is passed, the trainer will retrieve that value from the tokenizer. some tokenizers do not provide default value, so there is a check to retrieve the minimum between 2048 and that value. make sure to check it before training.
- for training adapters in 8bit, you might need to tweak the arguments of the prepare_model_for_int8_training method from peft, hence we advise users to use prepare_in_int8_kwargs field, or create the peftmodel outside the sfttrainer and pass it.
- for a more memory-efficient training using adapters, you can load the base model in 8bit, for that simply add load_in_8bit argument when creating the sfttrainer, or create a base model in 8bit outside the trainer and pass it.
- if you create a model outside the trainer, make sure to not pass to the trainer any additional keyword arguments that are relative to from_pretrained() method.

todo: why trust_remote_code? i want more details.
""""""
import sys

import torch
from peft import loraconfig

from transformers.modeling_utils import pretrainedmodel

from pdb import set_trace as st


def test_bfloat16_int4(compute_dtype: torch.dtype,
                       use_4bit,
                       ):
    """"""
python -c ""import torch; print(torch.cuda.get_device_capability());""
    todo: check other code test_bfloat16() do we need use_4bit?
    """"""
    if compute_dtype == torch.float16 and use_4bit:
        major, _ = torch.cuda.get_device_capability()
        if major >= 8:
            print(""="" * 80)
            print(""your gpu supports bfloat16, you can accelerate training with the argument --bfloat16"")
            print(""="" * 80)


def get_model_tokenizer_qlora_falcon7b(
        # -- mode args
        # model_id = ""tiiuae/falcon-7b""
        pretrained_model_name_or_path: str = ""ybelkada/falcon-7b-sharded-bf16"",
        use_cache: bool = true,
        # -- lora args
        lora_alpha=16,  # todo
        lora_dropout=0.1,  # todo, evidence drop out really help? google, crfm, gpt4
        lora_r=64,  # todo
        bnb_4bit_compute_dtype=torch.float16,  # changed it from guanaco hf

        # -- training args
        output_dir=""./results"",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        # paging so that the sudden mem gpu spikes don't cause the run to shut down
        # (i think usually caused by too long seqs)
        # todo: why 32 bit opt?
        # todo: paged nadamw opt?
        optim=""paged_adamw_32bit"",
        save_steps=10,
        logging_steps=10,
        learning_rate=2e-4,
        max_grad_norm=0.3,
        max_steps=500,
        warmup_ratio=0.03,
        lr_scheduler_type=""constant"",
        # -- quant. args (not recommended to be changed unless you know what your doing?)
        load_in_4bit=true,  # load (usually huge) base model in 4 bits
        bnb_4bit_quant_type=""nf4"",  # normal float 4 for the (large) base models qlora
) -> tuple:
    """"""
    load the falcon 7b model, quantize it in 4bit and attach lora adapters on it.

    bf16 = 1s, 7exp, 8mantissa
    hypothesis: 7b trained due to 6.7 emergence rumour, i still don't think emergence is real.
    notes:
        - ft a model is very specific to the model, tokenizer and training scheme. thus we return
            - model, tokenizer, ft config (peft config), training args

    ref:
        - 
    """"""
    from transformers import automodelforcausallm, autotokenizer, bitsandbytesconfig, autotokenizer

    # - get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)
    bnb_config = bitsandbytesconfig(
        load_in_4bit=load_in_4bit,  # load (usually huge) base model in 4 bits
        bnb_4bit_quant_type=bnb_4bit_quant_type,  # normal float 4 for the (usually huge) base model
        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,  # if you can, during computation use bf16
    )

    # - get falcon 4bit model
    # todo, where is this being saved & how to download quicker
    model = automodelforcausallm.from_pretrained(
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        quantization_config=bnb_config,
        trust_remote_code=true  # allows to execute custom code you download from the uploaded model code you are using
    )
    print(f'{type(model)=}')
    print(f'{model=}')
    # this is here to save gpu vram. likely only needed when using 40b or when oom issues happen ref: 
    model.config.use_cache = use_cache
    print(f'{type(model)=}')

    # - get falcon tokenizer
    tokenizer = autotokenizer.from_pretrained(pretrained_model_name_or_path,
                                              trust_remote_code=true)  # execs code downloaded from hf hub
    # tokenizer.pad_token = tokenizer.eos_token  # ref: 
    # tokenizer.add_special_tokens({'pad_token': '[pad]'})  # i think this is fine if during the training pad is ignored
    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})  # i think this is fine if during the training pad is ignored

    # - modify model
    # add pad token embed
    model.resize_token_embeddings(len(tokenizer))  # todo: i think this is fine if during the training pad is ignored
    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1
    model.config.max_new_tokens = len(tokenizer)
    # model.config.min_length = 1
    print(f'{model=}')
    print(f'{type(tokenizer)=}')
    print(f'{tokenizer.pad_token=}')
    # data_collator = datacollatorforlanguagemodeling(tokenizer=tokenizer, mlm=false) todo

    # - get falcon lora config
    peft_config = loraconfig(
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        r=lora_r,
        bias=""none"",
        task_type=""causal_lm"",
        # model card for falcon tiiuae/falcon-7b: 
        # does seem to include all trainable params as done by qlora on their own paper
        target_modules=[
            # word_embeddings,
            ""query_key_value"",
            ""dense"",
            ""dense_h_to_4h"",
            ""dense_4h_to_h"",
            # ""lm_head""
        ]
    )
    print(f'{type(peft_config)=}')

    # todo: print the num params of the lora = d1*r + d2*r and num of bytes by prec. (bytes) * num params
    return model, tokenizer, peft_config


# -- tests

def example_test_model_already_has_pad_token():
    """"""
    if it already has pad token, it likely has a small prob, so we are done.

    compare it's norm with other tokens to verify this is true.

python ~/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py
    """"""
    # - the get datasets todo: preprocessing, padding, streaming
    from uutils.hf_uu.data_hf.common import get_guanaco_datsets_add_splits_train_test_only
    trainset, _, testset = get_guanaco_datsets_add_splits_train_test_only()

    # qlora flacon7b
    from uutils.hf_uu.model_tokenizer.falcon_uu_mdl_tok import get_model_tokenizer_qlora_falcon7b
    model, tokenizer, peft_config = get_model_tokenizer_qlora_falcon7b()
    model: pretrainedmodel = model
    print(f'{model=}')
    sent = 'dogs are great because they are '
    print()

    # print to see if pad tokens are present and if it ignores the tokens at the end
    encoded_input = tokenizer(sent, padding='max_length', max_length=10, return_tensors='pt')
    print(f'{encoded_input=}')

    # print all special tokens
    print('\n---- start print all special tokens')
    for token_name, token in tokenizer.special_tokens_map.items():
        print(f""{token_name}: {token}"")
    print('\n---- end print all special tokens')

    # get the id for the '[pad]' token
    try:
        pad_token_id = tokenizer.convert_tokens_to_ids('[pad]')
    except keyerror:
        raise valueerror(""token [pad] is not present in the tokenizer vocabulary."")

    # index into the model's embedding table
    try:
        print(f'{model.get_input_embeddings().weight.size()=}')
        pad_embedding = model.get_input_embeddings().weight[pad_token_id]
    except indexerror:
        raise valueerror(f""token id {pad_token_id} is not present in the model's embedding matrix."")

    print(f'{pad_embedding=}')
    print('success!\n')

    # check it generates something sensible
    # tokenizer.decode(model.generate(**tokenizer(sent, return_tensors='pt'), do_sample=true)[0])
    input_ids, attention_mask = encoded_input['input_ids'], encoded_input['attention_mask']
    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=true)
    predicted_tokens_ids = predicted_tokens_ids_options[0]
    predicted_sent = tokenizer.decode(predicted_tokens_ids)
    print(f'original sentence: {sent=}')
    print(f'predicted sentence: {predicted_sent=}')
    print('success2!')


if __name__ == '__main__':
    import time

    start_time = time.time()
    example_test_model_already_has_pad_token()
    print(f""the main function executed in {time.time() - start_time} seconds.\a"")

it doesn't like the modifications to the model:
    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1
    model.config.max_new_tokens = len(tokenizer)

how to fix?
errors:
/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1259: userwarning: you have modified the pretrained model configuration to control generation. this is a deprecated strategy to control generation and will be removed soon, in a future version. please use a generation configuration file (see 
  warnings.warn(
setting `pad_token_id` to `eos_token_id`:11 for open-end generation.
/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1452: userwarning: you are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. you may experience unexpected behaviors or slower generation. please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.
  warnings.warn(
traceback (most recent call last):
  file ""/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py"", line 211, in <module>
    example_test_model_already_has_pad_token()
  file ""/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py"", line 199, in example_test_model_already_has_pad_token
    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=true)
  file ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  file ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py"", line 1572, in generate
    return self.sample(
  file ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py"", line 2633, in sample
    next_token_scores = logits_warper(input_ids, next_token_scores)
  file ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py"", line 92, in __call__
    scores = processor(input_ids, scores)
  file ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py"", line 302, in __call__
    indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, none]
runtimeerror: ""topk_cpu"" not implemented for 'half'


bounty section: small gpt2 code example
yes i agree that pad is assigned to eos. eos is still eos. but during fine-tuning now the weights wrt to eos are unchanged. this might be an issue since the probability of eos has not shifted to the fine-tuning regime. one possibility is that eos is outputed with less chance. yes we can still halt production when we see eos but we've not shifted the probability to output eos according to our fine-tuning distribution -- but all other tokens have changed distribution. i think this could be an issue because it's not like the old probability of eos is conserved since all tokens probs have changed except eos + even if the old eos prob was conserved, it's wrt wrong distribution (not the fine tuning one).
e.g.,
if tokenizer.pad_token_id is none:
    tokenizer.pad_token = tokenizer.eos_token
 ...
raw_text_batch='a'
tokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 0, 0, 0, 0]])}

but it would have been better to have
tokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 0, 0, 0]])}

code
def test_eos_pad():
    from datasets import load_dataset
    import torch
    from transformers import gpt2tokenizer, gpt2lmheadmodel

    raw_text_batch = 'a'

    tokenizer = gpt2tokenizer.from_pretrained(""gpt2"")
    # print(f'{tokenizer.eos_token=}')
    # print(f'{tokenizer.eos_token_id=}')
    # print(f'{tokenizer.pad_token=}')
    # print(f'{tokenizer.pad_token_id=}')

    # print(f'{raw_text_batch=}')
    # tokenize_batch = tokenizer(raw_text_batch, padding=""max_length"", max_length=5, truncation=true, return_tensors=""pt"")
    # print(f'{tokenize_batch=}')

    if tokenizer.pad_token_id is none:
        tokenizer.pad_token = tokenizer.eos_token
    probe_network = gpt2lmheadmodel.from_pretrained(""gpt2"")
    device = torch.device(f""cuda:{0}"" if torch.cuda.is_available() else ""cpu"")
    probe_network = probe_network.to(device)

    print(f'{tokenizer.eos_token=}')
    print(f'{tokenizer.eos_token_id=}')
    print(f'{tokenizer.pad_token=}')
    print(f'{tokenizer.pad_token_id=}')

    print(f'{raw_text_batch=}')
    tokenize_batch = tokenizer(raw_text_batch, padding=""max_length"", max_length=5, truncation=true, return_tensors=""pt"")
    print(f'{tokenize_batch=}')
    print('done')


cross:

hf discuss forum: 
pytorch forum discuss: 

context peft pacman100 code: 
twitter tweet of this:","['machine-learning', 'pytorch', 'huggingface-transformers', 'huggingface', 'huggingface-tokenizers']",79382829,"i feel the simplest answer during training is:

putting the eos string token explicitly then letting the model pad, even if eos == pad, makes sure that we do train on the intended tokens (including eos).

eg see the output:
--> warning: note this model mistralai/mistral-7b-v0.1 didnt have a pad token, we assinged it to eos.
bos/eos/pad --> bos: 1 eos: 2 pad: 2
bos/eos/pad --> bos: <s> eos: </s> pad: </s>
current raw rext (about to be tokenized): ['hello world.</s>', 'the dog is brown.</s>']
tfa encoded text (explict args!): tokenizer(text_examples, add_special_tokens=false)={'input_ids': [[22557, 1526, 28723, 2], [415, 3914, 349, 9060, 28723, 2]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}
training (batch) encoded text args (explict args!): tokenizer(text_examples, padding='max_length', truncation=true, max_length=13, return_tensors='pt', padding_side='right')={'input_ids': tensor([[    1, 22557,  1526, 28723,     2,     2,     2,     2,     2,     2,
             2,     2,     2],
        [    1,   415,  3914,   349,  9060, 28723,     2,     2,     2,     2,
             2,     2,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}

code
# af_tokenizers.py

import os
import time
import torch
from typing import optional
from transformers import (
    autotokenizer,
    automodelforcausallm,
    pretrainedmodel
)
from datasets import load_dataset

def seed_everything(seed: int = 42):
    """"""
    seed python, numpy, and pytorch for reproducibility.
    """"""
    import random
    import numpy as np
    from transformers import set_seed as hf_set_seed

    print(f""setting random seed = {seed}"")
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = true
    torch.backends.cudnn.benchmark = false

    if torch.cuda.is_available():
        hf_set_seed(seed)
    else:
        print(""warning: transformers is only fully deterministic on gpu"")


def main():
    os.environ['cuda_visible_devices'] = '4'  # choose gpu
    seed_everything()

    # 1) our model list (including all desired models, even if some remain commented)
    model_token_configs = [
        {
            ""name"": ""internlm2-math-plus-1_8b"",
            ""repo"": ""internlm/internlm2-math-plus-1_8b"",
        },
        {
            ""name"": ""google/gemma-2-2b"",
            ""repo"": ""google/gemma-2-2b"",
        },
        {
            ""name"": ""mistral-7b-v0.1"",
            ""repo"": ""mistralai/mistral-7b-v0.1"",
        },
    ]

    device = ""cuda"" if torch.cuda.is_available() else ""cpu""

    print('\nloop starting to see model tokenizations...')
    for config in model_token_configs:
        model_name = config[""name""]
        repo = config[""repo""]
        print(f'model repo: {repo}')

        model = automodelforcausallm.from_pretrained(repo, trust_remote_code=true).to(device)

        # putting the eos string token explicitly then letting the model pad, even if eos == pad, makes sure that we do train on the intended tokens (including eos).
        text_examples = [
            ""hello world.</s>"",
            ""the dog is brown.</s>""
        ]
        # 2) get tokenizer & bos/eos/pad info
        # i often have this line and we should be able to see quickly what the tokenizers outputs for the models in question
        tokenizer = autotokenizer.from_pretrained(repo, trust_remote_code=true)
        if tokenizer.pad_token is none:
            tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is none else tokenizer.pad_token
            print(f'--> warning: note this model {repo} didnt have a pad token, we assinged it to eos.')
        print(f'bos/eos/pad --> bos: {tokenizer.bos_token_id} eos: {tokenizer.eos_token_id} pad: {tokenizer.pad_token_id}')
        print(f'bos/eos/pad --> bos: {tokenizer.bos_token} eos: {tokenizer.eos_token} pad: {tokenizer.pad_token}')

        # 3) tokenize according to common ways i do it for thi project
        print(f'current raw rext (about to be tokenized): {text_examples}')

        # tfa encodings
        print(f'tfa encoded text (explict args!): {tokenizer(text_examples, add_special_tokens=false)=}')
        
        # train encodings
        # note: add_eos_token is not always present, so we won't be using: tok = autotokenizer.from_pretrained(pretrained_model_name_or_path, padding_side=""right"", trust_remote_code=true, add_eos_token=true)
        print(f""training (batch) encoded text args (explict args!): {tokenizer(text_examples, padding='max_length', truncation=true, max_length=13, return_tensors='pt', padding_side='right')=}"")
        print()


if __name__ == ""__main__"":
    main()",https://stackoverflow.com/questions/76633368,machine-learning,07-07-2023 01:11,13074.0,12.0,7.0,True,24-02-2025 12:46,24-02-2025 12:46
64464540,how to evaluate word embeddings quality using avgsimc and maxsimc,"i am working in a project of topical word embeddings, where i need to evaluate the quality of word embedidngs based on multi-sense of a word. i have seen in some research papers using  avgsimc and  maxsimc. as per my understanding, sense of a word predict by considering context words using these two methods.  unfortunately i didn't get the clear implementation concepts and source code for these tow methods.
source code (python or c) of implementation avgsimc and  maxsimc using scws data set and any kinds of documentation/tutorial  or any references will be more appreciated.
thank you for your valuable time.","['python', 'similarity', 'word-embedding', 'topic-modeling']",67063591,"for two word vectors word1 and word2 in python
   def avgsimc(word1, word2):
       cosine_similarity = 1 - spatial.distance.cosine(word1, word1)
       return np.mean(cosine_similarity)

   def maxsimc(word1, word2):
       cosine_similarity = 1 - spatial.distance.cosine(word1, word1)
       return np.max(cosine_similarity)",https://stackoverflow.com/questions/64464540,python,21-10-2020 13:25,276.0,0.0,1.0,True,13-04-2021 04:12,23-11-2020 02:20
76523678,cuda out of memory when training is done on multiple gpu,"my nvidia-smi output is as follows:
+---------------------------------------------------------------------------------------+
| nvidia-smi 530.30.02              driver version: 530.30.02    cuda version: 12.1     |
|-----------------------------------------+----------------------+----------------------+
| gpu  name                  persistence-m| bus-id        disp.a | volatile uncorr. ecc |
| fan  temp  perf            pwr:usage/cap|         memory-usage | gpu-util  compute m. |
|                                         |                      |               mig m. |
|=========================================+======================+======================|
|   0  nvidia geforce gtx 1080 ti      off| 00000000:02:00.0 off |                  n/a |
| 20%   54c    p2               83w / 250w|   4692mib / 11264mib |     45%      default |
|                                         |                      |                  n/a |
+-----------------------------------------+----------------------+----------------------+
|   1  nvidia geforce gtx 1080 ti      off| 00000000:03:00.0 off |                  n/a |
| 26%   60c    p2               73w / 250w|   4650mib / 11264mib |     44%      default |
|                                         |                      |                  n/a |
+-----------------------------------------+----------------------+----------------------+
|   2  nvidia geforce gtx 1080 ti      off| 00000000:81:00.0 off |                  n/a |
| 50%   71c    p0               84w / 250w|      0mib / 11264mib |      0%      default |
|                                         |                      |                  n/a |
+-----------------------------------------+----------------------+----------------------+
|   3  nvidia geforce gtx 1080 ti      off| 00000000:82:00.0 off |                  n/a |
| 30%   53c    p0               75w / 250w|      0mib / 11264mib |      0%      default |
|                                         |                      |                  n/a |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| processes:                                                                            |
|  gpu   gi   ci        pid   type   process name                            gpu memory |
|        id   id                                                             usage      |
|=======================================================================================|
|    0   n/a  n/a   3494144      c   python                                     4690mib |
|    1   n/a  n/a   3494896      c   python                                     4648mib |
+---------------------------------------------------------------------------------------+

i'm running a script to train from scratch a roberta model (based on this article and this notebook), but when i run cuda_visible_devices=2,3 python script.py (this is a machine where other researchers run their scripts; kill the processes on gpu 0 and 1 is not an option), i have the following error:
torch.cuda.outofmemoryerror: cuda out of memory. tried to allocate 3.07 gib (gpu 0; 10.91 gib total capacity; 8.36 gib already allocated; 1.93 gib free; 8.40 gib reserved in total by pytorch) if reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  see documentation for memory management and pytorch_cuda_alloc_conf

why is only one gpu's ram being recognized (as seen at 10.91 gib total capacity)? by selecting more than one gpu would i not be able to use the total space made available by them? i would like to use this space as it would allow me to have a larger batch size value for training. because of some time restrictions, i don't intend to use a lower train batch size.","['python', 'pytorch', 'huggingface-transformers']",76523758,"the batch size that you set in torch will be the batch size used by each single gpu. multi-gpu training allows you to distribute each batch to a different gpu to speed up each epoch, the weights learned by each gpu are then integrated into the resulting model.
so you can't use a bigger batch size just because the training employs more gpus.",https://stackoverflow.com/questions/76523678,python,21-06-2023 13:12,1391.0,1.0,1.0,True,21-06-2023 13:21,21-06-2023 13:14
79042708,how to forward openai&#39;s stream response using fastapi in python?,"this is my code to retrieve stream response from openai's model which is event based. (i have shown only core part)
client = openai(api_key=open_ai_api_key)

class eventhandler(assistanteventhandler):
    def on_text_delta(self, delta: textdelta, snapshot: text):
        print(delta.value)

with client.beta.threads.runs.stream(
    thread_id=thread_id,
    assistant_id=assistant_id,
    event_handler=eventhandler()
) as stream:
stream.until_done()

on_text_delta event triggers as tokens arrives from api. i want to forward this response using fastapi instead of printing on output screen.
@app.get(""/stream"")
async def stream():
    return ...something...

i have tried responding result as part of http body:
from fastapi.responses import streamingresponse

...

@app.post(""/stream"")
async def stream():
    with client.beta.threads.runs.stream(
        thread_id=thread_id,
        assistant_id=assistant_id,
        event_handler=eventhandler()
    ) as stream:
        stream.until_done()

    return streamingresponse(eventhandler.generator_function(), media_type=""text/plain"")

i have created generator_function inside eventhandler class but problem is until stream is not over the execution doesn't reach return statement.
i have also tried websockets, but still problem is how should my program execution should flow. the stream doesn't let execution go further until api response is completed.","['python', 'stream', 'fastapi', 'openai-api']",79049704,"i found the solution!
from fastapi import fastapi
from fastapi.responses import streamingresponse
from openai import openai, asyncopenai


open_ai_api_key = 'you_api_key'
async_client = asyncopenai(api_key=open_ai_api_key)
client = openai(api_key=open_ai_api_key)

app = fastapi()

async def stream_assistant_response(assistant_id, thread_id):
    stream =  async_client.beta.threads.runs.stream(
        assistant_id=assistant_id,
        thread_id=thread_id
    )

    async with stream as stream:
        async for text in stream.text_deltas:
            yield f""data: {text}\n\n""


@app.get(""/message"")
async def add_message(assistant_id, thread_id, message):
    # make sure thread exist
    client.beta.threads.messages.create(
        thread_id=thread_id,
        role=""user"",
        content=message
    )

    return streamingresponse(stream_assistant_response(assistant_id, thread_id), media_type=""text/event-stream"")",https://stackoverflow.com/questions/79042708,python,01-10-2024 10:25,1227.0,-1.0,2.0,True,03-10-2024 08:27,01-10-2024 12:52
69384969,does anyone know how to use letsum?,"i am trying to do legal text summarization and found this open source package, but i don't know how to install and use it. i tried using pip install letsum but it's giving me an error.
error: could not find a version that satisfies the requirement letsum (from versions: none)
error: no matching distribution found for letsum
below is the github documentation, any ideas how to use it?

thank you in advance","['python', 'github', 'nlp', 'summarization']",69408274,"you can follow these steps:

clone the repository:

git clone 


install dependencies

pip install -r summarization/supervised/requirements.txt


get to summarization/supervised/legal-specific/letsum:

cd summarization/supervised/legal-specific/letsum


open your ide in the current directory and you should be able to import letsum without any problem!",https://stackoverflow.com/questions/69384969,python,30-09-2021 01:16,138.0,0.0,1.0,True,01-10-2021 15:16,30-09-2021 01:31
76726419,langchain: modulenotfounderror: no module named &#39;langchain&#39;,"when i write code in vs code, beginning with:
import os
from langchain.chains import retrievalqa
from langchain.llms import openai
from langchain.document_loaders import textloader

i am met with the error: modulenotfounderror: no module named 'langchain'
i have updated my python to version 3.11.4, have updated pip, and reinstalled langchain. i have also checked sys.path and the folder c:\\python311\\lib\\site-packages in which the langchain folder is, is appended.
edit: langchain import works when i run it in the python console (functionality works too), but when i run the code from the vscode run button it still provides the modulenotfounderror.
has anyone else run into this issue and found a solution?","['python', 'import', 'module', 'python-import', 'langchain']",76902984,"i had installed packages with python 3.9.7 but this version was causing issues so i switched to python 3.10. when i installed the langhcain it was in python 3.9.7 directory. if yo run pip show langchain, you get this
name: langchain
version: 0.0.220
summary: building applications with llms through composability
home-page: 
author: 
author-email: 
license: mit
location: /home/anaconda3/lib/python3.9/site-packages
requires: aio async-timeout, dataclasses-json, langchainplus-sdk, numexpr, numpy, openapi-schema-pydantic, pydantic, pyyaml, requests, sqlalchemy, tenacity
required-by: jupyter_ai, jupyter_ai_magics

if you look at the location property, you see this /home/anaconda3/lib/python3.9/site-packages. but since i am using pyhton3.10 i had to make sure langchain is in the directory of python 3.10. so installed the langhchain with
python3.10 -m pip install langchain   

now when i run, python3.10 -m pip show langchain i get this
name: langchain
version: 0.0.264
summary: building applications with llms through composability
home-page: 
author: 
author-email: 
license: mit
location: /home/.local/lib/python3.10/site-packages
requires: aio async-timeout, dataclasses-json, langsmith, numexpr, numpy, openapi-schema-pydantic, pydantic, pyyaml, requests, sqlalchemy, tenacity
required-by: 

now new location is referring to python3.10 directory",https://stackoverflow.com/questions/76726419,python,20-07-2023 03:05,106822.0,13.0,8.0,True,08-12-2023 08:46,20-07-2023 03:14
70377385,why the the total number in confusion matrix not same as the data input?,"why the total confusion matrix does not have the same number os samples as the dataset? the dataset contains 7514 but the total at confusion matrix not exceed 2000.

here is the code:
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import porterstemmer

corpus = []
for i in range(len(dataset)):
  text = re.sub('[^a-za-z]', ' ', dataset['text'][i])
  text = text.lower()
  text = text.split()
  ps = porterstemmer()
  text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]
  text = ' '.join(text)
  corpus.append(text)

import sklearn
from sklearn.feature_extraction.text import countvectorizer
cv = countvectorizer(max_features = 10000)
x = cv.fit_transform(corpus).toarray()
y = dataset.iloc[:, 1].values

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)

from sklearn import linear_model
classifier = linear_model.logisticregression(c=10)
classifier.fit(x_train, y_train)

y_pred = classifier.predict(x_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print (""confusion matrix:\n"",cm)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
score1 = accuracy_score(y_test,y_pred)
score2 = precision_score(y_test,y_pred)
score3= recall_score(y_test,y_pred)
print(""\n"")
print(""accuracy is "",round(score1*100,2),""%"")
print(""precision is "",round(score2,2))
print(""recall is "",round(score3,2))","['python', 'machine-learning', 'scikit-learn', 'nlp', 'nltk']",70380056,"after you split data using train_test_split, you are left with 2255 samples in the test portion which is almost equal to  7514 x 0.3, then you determined the confusion matrix using this portion (test-portion). now everything should make sense.",https://stackoverflow.com/questions/70377385,python,16-12-2021 10:26,1130.0,2.0,1.0,True,17-12-2021 01:04,17-12-2021 01:04
76885460,image not loading and api failing with 401 error on production build using vite,"i am using vite for my react project and encountering a couple of issues when i run the production build.

i have an image in my react component, which is loading perfectly fine when i run npm run dev. however, when i build the project using npm run build and then preview it using npm run preview, the image does not load and i see a 404 error in the console for the image file /src/assets/king.png. here's the relevant code snippet:




i am also making a post request to openai api ï¿½ï¿½ï¿½ using axios. this works fine in development, but in production, i am getting a 401 unauthorized error. here's the relevant code snippet:

import { configuration, openaiapi } from ""openai"";

const config = new configuration({
  apikey: ""my_key"",
});

const openai = new openaiapi(config);

const generatequestions = async (topic, language) => {
  const response = await openai.createchatcompletion({
    model: ""gpt-3.5-turbo"",
    messages: [
      {
        role: ""system"",
        content: ""you are a helpful assistant that generates trivia questions."",
      },
      {
        role: ""user"",
        content: `generate trivia question on the topic of ${topic} in ${language} language with a definitive answer and 3 incorrect answers in the following json format:`,
      },
    ],
    temperature: 0.7,
    max_tokens: 256,
    n: 10, 
  });

  const generatedquestions = response.data.choices.map((choice) =>  const questionjson = json.parse(choice.message.content);
    return questionjson;
  });

  console.log(generatedquestions);
  return generatedquestions;
};

i've already ensured that the openai api key is correct and it works in development. i've also tried varying the file path for the image, but without success. any help on these two issues would be greatly appreciated!

remember to replace the api key in your code with a placeholder before posting the question, as sharing the real key in public is a security risk. you should also regenerate your api key if it has been exposed.","['javascript', 'node.js', 'reactjs', 'vite', 'openai-api']",76885788,"try this:

import your image i.e.

import king from ""./src/assets/king.png"";",https://stackoverflow.com/questions/76885460,javascript,11-08-2023 16:56,453.0,2.0,1.0,True,11-08-2023 17:59,11-08-2023 17:59
69581316,label schemes by language in spacy,"from the spacy documentation:

for a list of the fine-grained and coarse-grained part-of-speech tags assigned by spacyï¿½ï¿½ï¿½s models across different languages, see the label schemes documented in the models directory.

i assume this is referring to the parts of speech tags, eg: verb, noun, num etc., and that this list will be different for each language.
is this a correct assumption?
i followed the link in the documentation to the models directory, but could not find a list of the valid pos tags for each language.

answer
thanks to @polm23 for the answer, here's a screen shot with the navigation, in case anyone else can't find it.","['nlp', 'spacy']",69582005,"look for the ""label scheme"" on the page for any individual language.

the verb noun type tags, that go in the .pos attribute, are from universal dependencies, and are mostly the same between languages. the coarse-grained tags, for the .tag attribute, can be anything and are unique to each language as far as i'm aware.",https://stackoverflow.com/questions/69581316,nlp,15-10-2021 07:33,351.0,2.0,1.0,True,15-10-2021 09:31,15-10-2021 09:31
72933472,modulenotfounderror in spacy version 3.3.1 tried previous mentioned solution not working,"import spacy

     from spacy.lemmatizer import lemmatizer

     from spacy.lang.en import lemma_index, lemma_exc, lemma_rules

     lemmatizer = lemmatizer(lemma_index, lemma_exc, lemma_rules)

     lemmatizer('chuckles', 'noun')

the output should be chuckle.
using version 3.1.1","['python', 'nlp', 'chatbot', 'lemmatization', 'spacy-3']",72939110,"it looks like they've changed the way the lemmatizer is instantiated but the following should work...
import spacy
nlp = spacy.load('en_core_web_sm', disable=['ner', 'tagger', 'parser', 'lemmatizer'])
lemmatizer = nlp.get_pipe('lemmatizer')
t = nlp('chuckles')[0]  
t.pos_ = 'noun'
lemma = lemmatizer.lemmatize(t)[0]
print(lemma)
# >> chuckle


it's unfortunate that you have to call the lemmatizer with a token but looking at the code, i don't see a way to call it with (word, pos). i think you're stuck with calling the empty pipeline with a single word to get a token then manually setting the pos_ before calling lemmatize(t).
note that the pos tagger will not work correctly on a single word. it only works in sentences and will probably always assign noun for pos if you only have one word. this is why i've disabled the pipeline and set t.pos_ manually.
btw.. if you only need to lemmatize, you might look at lemminflect which is simpler for single word and also more accurate.",https://stackoverflow.com/questions/72933472,python,11-07-2022 03:41,310.0,0.0,1.0,True,11-07-2022 13:20,11-07-2022 04:29
61903298,cannot execute this command ï¿½ï¿½ï¿½rasa run actions &amp; rasa sh,"using rasa open source i tried to execute (windows powershell) this command rasa run action & rasa shell it generate some error like this: 
at line:1 char:17
+ rasa run action & rasa shell
+                 ~
the ampersand (&) character is not allowed. the & operator is reserved for future use; wrap an ampersand in double quotation marks (""&"") to      
pass it as part of a string.
    + categoryinfo          : parsererror: (:) [], parentcontainserrorrecordexception
    + fullyqualifiederrorid : ampersandnotallowed","['rasa-nlu', 'rasa-core', 'rasa']",61906779,"in powershell you can use a semicolon to run 2 commands (note it should be rasa run actions): 
rasa shell; rasa run actions

in cmd you can still use ampersand.
however this won't work on windows where commands are run sequentially. one solution is to have a terminal session for each command (you also don't mix up logs and can restart only one if  you need to)",https://stackoverflow.com/questions/61903298,rasa-nlu,20-05-2020 00:55,863.0,0.0,2.0,True,03-06-2021 16:41,21-05-2020 12:41
75624961,what does config inside ``super().__init__(config)`` actually do?,"i have the following code to create a custom model for named-entity-recognition. using chatgpt and copilot, i've commented it to understand its functionality.
however, the point with config inside super().__init__(config) is not clear for me. which role does it play since we have already used xlmrobertaconfig at the beginning?
import torch.nn as nn
from transformers import xlmrobertaconfig
from transformers.modeling_outputs import tokenclassifieroutput
from transformers.models.roberta.modeling_roberta import robertamodel
from transformers.models.roberta.modeling_roberta import robertapretrainedmodel

# create a class for a custom model, which inherit from robertapretrainedmodel since we want to use the weights of a pretained model in the body of a custom model
class xlmrobertafortokenclassification(robertapretrainedmodel):
    # common practice in ï¿½ï¿½ï¿½ï¿½ transformers 
    # allows the xlmrobertafortokenclassification class to inherit the configuration functionality and attributes from the xlmrobertaconfig class
    config_class = xlmrobertaconfig

    # initialize the model
    def __init__(self, config):
        # call the initialization function of the parent class (robertapretrainedmodel)
        super().__init__(config)              # config is necessary when working with pretrained models to ensure the initializat the correct configuration of parent class
        self.num_labels = config.num_labels   # number of classes to predict

        # load model body
        self.roberta = robertamodel(config, add_pooling_layer=false) # returns all hidden states not just [cls]
        
        # set up token classification head
        self.dropout = nn.dropout(config.hidden_dropout_prob)             
        self.classifier = nn.linear(config.hidden_size, config.num_labels) # linear transformation layer takes (batch_size, sequence_length, hidden_size) 
                                                                           # to produce output tensor of shape (batch_size, sequence_length, num_labels)
                                                                           # which can be interpreted as probability distribution over the labels for each token in the input sequence.
        
        # load the pretrained weights for the model body and 
        # ... randomly initialize weights of token classification head
        self.init_weights()

    # define the forward pass
    def forward(self, input_ids=none, attention_mask=none, token_type_ids=none, 
                labels=none, **kwargs):
        # feed the data through model body to get encoder representations
        outputs = self.roberta(input_ids, attention_mask=attention_mask,
                               token_type_ids=token_type_ids, **kwargs)
        
        # apply classifier to encoder representation 
        sequence_output = self.dropout(outputs[0]) # apply dropout to the first element of output tensor, i.e., last_hidden_state
        logits = self.classifier(sequence_output)  # apply the linear transformation to get the logits (i.e., raw output of the model)
        # calculate losses if labels are provided
        loss = none
        if labels is not none:
            loss_fct = nn.crossentropyloss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1)) # apply cross entropy function on flattend logits and flattend labels
        # return model output object
        return tokenclassifieroutput(loss=loss, logits=logits, 
                                     hidden_states=outputs.hidden_states, 
                                     attentions=outputs.attentions)

edit: i quote directly from the book i'm working on: ""config_class  ensures that the standard xlmrobertaconfig settings are used when initilize a new model"". if i understand it correctly, could we change these defualt parameters by overwriting the default settings in the config?","['python', 'oop', 'nlp', 'huggingface-transformers']",75625025,"in your code, config_class doesn't contain any configuration parameters. it only contains xlmrobertaconfig, which is a class(/!\ not an instance of that class)
i'm not sure how robertapretrainedmodel model works, but it seems that, when you initialise an instance of xlmrobertafortokenclassification, you need to give it the actual contents of the configuration (maybe as dictionary?)
but the class attribute config_class doesn't know anything about the values set in the configuration
edit: taken from the code of pretrainedmodel, which robertapretrainedmodel inherits from:

if not isinstance(config, pretrainedconfig):
   config_path = config if config is not none else pretrained_model_name_or_path
   config, model_kwargs = cls.config_class.from_pretrained(
       config_path,
       cache_dir=cache_dir,
       return_unused_kwargs=true,
       force_download=force_download,
       resume_download=resume_download,
       proxies=proxies,
       local_files_only=local_files_only,
       use_auth_token=use_auth_token,
       revision=revision,
       subfolder=subfolder,
       _from_auto=from_auto_class,
       _from_pipeline=from_pipeline,
       **kwargs,
   )


(taken from here)
the class_config is used as a fallback, if no valid config is given, to instantiate a config with default values",https://stackoverflow.com/questions/75624961,python,03-03-2023 08:50,199.0,0.0,1.0,True,03-03-2023 09:38,03-03-2023 09:11
68052695,error : typeerror: &#39;int&#39; object is not subscriptable while using lops in python,"i am using python, i have a nested list like this
clusters_appearance_list = [[[0, 0], ['jack']], [[0, 0], ['study']], [[0, 4], ['small', 'modest', 'little']], [[0, 5], ['big', 'large']]]

i want to calculate the frequency of each cluster in a text. my code is like this
for cluster in clusters_appearance_list:
  for word_cluster in cluster:
    for word in words_cluster[1]:
        for sentence in stemmed_sentences_list:
            if word in sentence[1]:
                count = count+1
     clusters_appearance_list.append(count)
     count = 0
print(clusters_appearance_list)

i get error in
for word in cluster[1]:
typeerror: 'int' object is not subscriptable","['python', 'list', 'loops', 'nlp']",68052809,"it should be
for cluster in clusters_appearance_list:
    for word in cluster[1]:

not the
for cluster in clusters_appearance_list:
   for word_cluster in cluster:
     for word in words_cluster[1]:",https://stackoverflow.com/questions/68052695,python,20-06-2021 04:33,67.0,-1.0,1.0,True,20-06-2021 05:01,20-06-2021 04:52
69907682,what are differences between automodelforsequenceclassification vs automodel,"we can create a model from automodel(tfautomodel) function:
from transformers import automodel 
model = automodel.from_pretrained('distilbert-base-uncase')

in other hand, a model is created by automodelforsequenceclassification(tfautomodelforsequenceclassification):
from transformers import automodelforsequenceclassification
model = automodelforsequenceclassification('distilbert-base-uncase')

as i know, both models use distilbert-base-uncase library to create models.
from name of methods, the second class( automodelforsequenceclassification ) is created for sequence classification.
but what are really differences in 2 classes? and how to use them correctly?
(i searched in huggingface but it is not clear)","['nlp', 'text-classification', 'huggingface-transformers']",70232831,the difference between automodel and automodelforsequenceclassification model is that automodelforsequenceclassification  has a classification head on top of the model outputs which can be easily trained with the base model,https://stackoverflow.com/questions/69907682,nlp,10-11-2021 03:33,23066.0,23.0,2.0,True,01-12-2023 21:55,10-11-2021 03:43
56071689,what&#39;s the major difference between glove and word2vec?,"what is the difference between word2vec and glove? 
are both the ways to train a word embedding? if yes then how can we use both?","['machine-learning', 'nlp', 'word2vec', 'word-embedding', 'glove']",56072207,"yes, they're both ways to train a word embedding. they both provide the same core output: one vector per word, with the vectors in a useful arrangement. that is, the vectors' relative distances/directions roughly correspond with human ideas of overall word relatedness, and even relatedness along certain salient semantic dimensions.
word2vec does incremental, 'sparse' training of a neural network, by repeatedly iterating over a training corpus.
glove works to fit vectors to model a giant word co-occurrence matrix built from the corpus.
working from the same corpus, creating word-vectors of the same dimensionality, and devoting the same attention to meta-optimizations, the quality of their resulting word-vectors will be roughly similar. (when i've seen someone confidently claim one or the other is definitely better, they've often compared some tweaked/best-case use of one algorithm against some rough/arbitrary defaults of the other.)
i'm more familiar with word2vec, and my impression is that word2vec's training better scales to larger vocabularies, and has more tweakable settings that, if you have the time, might allow tuning your own trained word-vectors more to your specific application. (for example, using a small-versus-large window parameter can have a strong effect on whether a word's nearest-neighbors are 'drop-in replacement words' or more generally words-used-in-the-same-topics. different downstream applications may prefer word-vectors that skew one way or the other.)
conversely, some proponents of glove tout that it does fairly well without needing metaparameter optimization.
you probably wouldn't use both, unless comparing them against each other, because they play the same role for any downstream applications of word-vectors.",https://stackoverflow.com/questions/56071689,machine-learning,10-05-2019 06:10,18515.0,28.0,2.0,True,22-12-2024 03:09,22-06-2020 06:27
68140256,how can i lemmatize a tokenized column of a dataframe in python?,"i try to lemmatize the column ""tokenized"" in a dataframe. one cell of the column ""tokenized"" looks as follows ""  yeah    simply    zurich    generic    serving    think    media    bland    prepared    curry    kind    paying    well    loves    used    parboiled    oily    place    elaborate    non    tasteful    stay    underspiced    institution    vegetarian    indian    clueless    away    hiltl    anyone    served    support    veg    long    like    normal    strong    worth    insult    not    rice    kitchen    know    wont    food    cuisine    fantastic    fan    time    term    patrons  "".
when i run my code it returns something like this: "",,e,n,d,e,d,,,p,a,y,i"" which is not what i want. how can i lemmatize full words?
this is my code:
reviews_english['tokenized_lem'] = reviews_english['tokenized'].apply(
                    lambda lst:[lmtzr.lemmatize(word) for word in lst])
reviews_english","['python', 'pandas', 'dataframe', 'nltk', 'lemmatization']",68140312,"the problem is that your ""tokenized"" column doesn't look ready to apply the lemmatization step, as it contains a string, not a list of tokens. in other words, instead of having
"" yeah simply zurich generic serving ...""

you should have in your dataframe tokenized cell a list of tokens (generated with a tokenizer from your initial sentence), as in
[""yeah"", ""simply"", ""zurich"", ""generic"", ""serving"", ...]

if you don't have a proper list of tokens in your dataframe cell, python will iterate in your apply/lambda list comprehension character by character, which is clearly not what you want.",https://stackoverflow.com/questions/68140256,python,26-06-2021 07:41,829.0,0.0,1.0,True,26-06-2021 08:15,26-06-2021 08:15
71721694,tensorflow2.x keras embedding layer process tf.dataset error,"this question is a follow-up of tensorflow 2 textvectorization process tensor and dataset error
i would like to make do a word embedding for the processed text with tnesorflow 2.8 on jupyter.
def standardize(input_data):

    input_data = tf.strings.lower(input_data)
    input_data = tf.strings.regex_replace(input_data, f""[{re.escape(string.punctuation)}]"", "" "")
    return input_data

# the input data loaded from text files by tfrecorddataset(file_paths, ""gzip"")
# each file can be 200+mb, totally about 300 files
# each file hold the data with multiple columns
# some columns are text
# after loading, the dataset will be accessed by column name 
# e.g. one column is ""sports"", so the input_dataset[""sports""] 
# return a tensor, which is like the following example

input_data = tf.constant([[""swim 2008-07 baseball""], [""football""]], shape=(2, 1), dtype=tf.string)

text_layer = tf.keras.layers.textvectorization( standardize = standardize, max_tokens = 10, output_mode = 'int', output_sequence_length=10 )

dataset = tf.data.dataset.from_tensors( input_data )

dataset = dataset.batch(2)

text_layer.adapt(dataset)

process_text = dataset.map(text_layer)

emb_layer = layers.embedding(10, 10)

emb_layer(process_text) # error 

error:
 attributeerror: exception encountered when calling layer ""embedding_7"" (type embedding).

'mapdataset' object has no attribute 'dtype'

call arguments received:

 ï¿½ï¿½ï¿½ inputs=<mapdataset element_spec=tensorspec(shape=(none, 2, 10), dtype=tf.int64, name=none)>

how can i convert a tf.dataset to tf.tensor ?
this tensorflow: convert tf.dataset to tf.tensor does not help me.
the above layers will be implemented in a machine learning neural network model.
loading data --> processing features (multiple text columns) --> tokens --> embedding --> average pooling --> some dense layers --> output layer

thanks","['python', 'tensorflow', 'keras', 'text-processing', 'word-embedding']",71732540,"you cannot feed a tf.data.dataset directly to an embedding layer, you can either use .map(...):
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import re
import string 
def standardize(input_data):

    input_data = tf.strings.lower(input_data)
    input_data = tf.strings.regex_replace(input_data, f""[{re.escape(string.punctuation)}]"", "" "")
    return input_data

input_data = tf.constant([[""swim 2008-07 baseball""], [""football""]], shape=(2, 1), dtype=tf.string)

text_layer = tf.keras.layers.textvectorization( standardize = standardize, max_tokens = 10, output_mode = 'int', output_sequence_length=10 )

dataset = tf.data.dataset.from_tensors( input_data )

dataset = dataset.batch(2).map(lambda x: tf.squeeze(x, axis=0))

text_layer.adapt(dataset)

process_text = dataset.map(text_layer)

emb_layer = layers.embedding(10, 10)
process_text = process_text.map(emb_layer)

or define your model and feed your dataset through model.fit(...):
import tensorflow as tf
import re
import string 
def standardize(input_data):

    input_data = tf.strings.lower(input_data)
    input_data = tf.strings.regex_replace(input_data, f""[{re.escape(string.punctuation)}]"", "" "")
    return input_data

input_data = tf.constant([[""swim 2008-07 baseball""], [""football""]], shape=(2, 1), dtype=tf.string)

text_layer = tf.keras.layers.textvectorization( standardize = standardize, max_tokens = 10, output_mode = 'int', output_sequence_length=10 )

dataset = tf.data.dataset.from_tensors( input_data )

dataset = dataset.batch(2)

text_layer.adapt(dataset)

process_text = dataset.map(lambda x: (text_layer(tf.squeeze(x, axis=0)), tf.random.uniform((2, ), maxval=2, dtype=tf.int32))) # add random label to each entry

inputs = tf.keras.layers.input((10, ))
emb_layer = tf.keras.layers.embedding(10, 10)
x = emb_layer(inputs)
x = tf.keras.layers.globalaveragepooling1d()(x)
outputs = tf.keras.layers.dense(1, 'sigmoid')(x)
model = tf.keras.model(inputs, outputs)
model.compile(optimizer='adam', loss='binary_crossentropy')
model.fit(process_text)",https://stackoverflow.com/questions/71721694,python,02-04-2022 22:45,529.0,1.0,2.0,True,16-03-2023 20:29,03-04-2022 00:26
75210324,openai gpt-3 api error: &quot;you didn&#39;t provide an api key. you need to provide your api key in an authorization header using bearer auth&quot;,"i am getting an error for the following php code:
$curl = curl_init(""

$data = array(
  'prompt' => 'how many sundays in 2023',
  'max_tokens' => 256,
  'temperature' => 0.7,
  'model' => 'text-davinci-003'
);

curl_setopt($curl, curlopt_post, 1);
curl_setopt($curl, curlopt_postfields, 
curl_setopt($curl, curlopt_returntransfer, 1);
curl_setopt($curl, curlopt_ ['authorization: bearer sk-my-api-key']);
curl_setopt($curl, curlopt_ ['content-type: application/json']);

$result = curl_exec($curl);
curl_close($curl);

$result = json_decode($result);
print $result->choices[0]->text;

i correctly provided the api key, but getting this error:

error message: you didn't provide an api key. you need to provide your api key in an authorization header using bearer auth (i.e. authorization: bearer your_key)","['php', 'openai-api', 'gpt-3']",75210429,"problem
all engines api endpoints are deprecated.


solution
use the completions api endpoint.
change the url from this...


...to this.



working example
if you run test.php the openai api will return the following completion:

string(23) ""
this is indeed a test""

test.php
<?php
    $ch = curl_init();

    $url = '

    $api_key = 'sk-xxxxxxxxxxxxxxxxxxxx';

    $post_fields = '{
        ""model"": ""text-davinci-003"",
        ""prompt"": ""say this is a test"",
        ""max_tokens"": 7,
        ""temperature"": 0
    }';

    $header  = [
        'content-type: application/json',
        'authorization: bearer ' . $api_key
    ];

    curl_setopt($ch, curlopt_url, $url);
    curl_setopt($ch, curlopt_returntransfer, 1);
    curl_setopt($ch, curlopt_post, 1);
    curl_setopt($ch, curlopt_postfields, $post_fields);
    curl_setopt($ch, curlopt_ $header);

    $result = curl_exec($ch);
    if (curl_errno($ch)) {
        echo 'error: ' . curl_error($ch);
    }
    curl_close($ch);

    $response = json_decode($result);
    var_dump($response->choices[0]->text);
?>",https://stackoverflow.com/questions/75210324,php,23-01-2023 13:39,4508.0,1.0,1.0,True,14-09-2023 14:56,13-03-2023 13:55
72837120,keyword importance over time with tf-idf,"i'm doing research on keyword importance in annual reports of cloud providers. i already extracted the text from the pdfs and i'm mostly looking for the importance of the word ""cloud"" in there.
so i decided to use tf-idf algorithm to define the importance of the keyword across multiple documents. however, i'm not a data scientist, i'm software engineer. i do not know if my solution makes sense. here is what i have:
import glob
from pathlib import path
import pandas as pd
from sklearn.feature_extraction.text import tfidfvectorizer
import numpy as np

x=[2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]

def get_line_and_trend(directory_path):
    # extracts text from files
    text_files = glob.glob(f""{directory_path}/**/*.txt"", recursive=true)
    text_titles = [path(text).stem for text in text_files]
    
    # tf-idf
    tfidf_vectorizer = tfidfvectorizer(input='filename', stop_words=""english"")
    tfidf_vector = tfidf_vectorizer.fit_transform(text_files)
    
    # search for word ""cloud""
    df = pd.dataframe(
        tfidf_vector.toarray(),
        index=text_titles,
        columns=tfidf_vectorizer.get_feature_names()
    ).sort_index()
    tfidf_slice = df[['cloud']]
    tfidf_slice.round(decimals=2)
    
    # draw the trend line
    z = np.polyfit(x, tfidf_slice[""cloud""], 1)
    p = np.poly1d(z)
    return tfidf_slice[""cloud""], p(x)

i'm passing a folder as parameter which contains the annual reports (.txt) in the different folders (2014-2021). with that i can plot something like:

with the following code:
alibaba_cloud, trendline = get_line_and_trend(""./alibaba"")
plt.plot(x, alibaba_cloud, color='b')
plt.plot(x, trendline, ""b--"")

google_cloud, trendline = get_line_and_trend(""./google"")
plt.plot(x, google_cloud, color='r')
plt.plot(x, trendline, ""r--"")

plt.legend([""alibaba"", ""trend line"", ""google"", ""trend line""])
plt.title(""tf-idf for \""cloud\"" in annual reports"")
plt.show()

so my questions are:

does it make sense to use tf-idf to track the importance of keyword over time? should i use something else?
does the chart really represents what i'm trying to do?","['python', 'scikit-learn', 'nlp', 'tf-idf']",72845148,"if i understand what you want, i think ""not really"".
the purpose of the idf factor is to provide a normalized weight for each term. if the term occurs in all your documents, you are ranking it as less important than some other terms which don't.
in so many words, the chart is not wrong in essence; it shows how the frequency of the term has increased over the years. but the y axis is basically meaningless in isolation; you are dividing by a constant which just obscures the actual number which you want to explore, that is, the absolute frequency of your term of interest.
if you were to compare two different terms, idf would make sense: it normalizes the weight of really common words (like ""the"" and ""and"") so their relative use in a specific document can be compared against the relative frequency of less common words (like ""fraught"" and ""outwardly"") in the same document on a normalized scale.
it sounds to me like the number you care about is simply the term frequency, though a normalization which could make sense is to divide by the length of the document (so if ""cloud"" occurs twice in a document with 20,000 words, it's not more significant than if it occurs only once in a smaller document which only contains 10,000 words).",https://stackoverflow.com/questions/72837120,python,02-07-2022 07:03,538.0,0.0,1.0,True,03-07-2022 09:40,03-07-2022 08:54
72995392,use spacy ner to identify person and make person one word?,"i want to use spacy ner to identify the person and make it one word.
my dataset looks like this:
text     
use your superpowers
vote for barack obama
vote for marine le pen
play with michael jordan
support the supporters

i want my final output to look like this:
text     
use your superpowers
vote for barack_obama
vote for marine_le_pen
play with michael_jordan
support the supporters

this is the code i have so far:
 def get_ner (string):
     nlp = spacy.load(""en_core_web_trf"")
     doc = nlp(string)
     for token.text in doc:
         if token.ents==""person"":
         s= ent['start']
         e= ent['end']
         txt = txt[:s] + txt[s:e+1].replace(' ', '_') + txt[e:]
     return txt

 df['text']= df.text.apply(get_ner)

when i use the code above, i'm getting an error message.
attributeerror: name 'token' is not defined","['pandas', 'spacy', 'named-entity-recognition']",73004672,"if you use spacy, you code should be:
nlp = spacy.load('en_core_web_trf')

def get_ner(txt):
    doc = nlp(txt)
    for ent in doc.ents:
        if ent.label_ == 'person':
            s = ent.start_char
            e = ent.end_char
            txt = txt[:s] + txt[s:e+1].replace(' ', '_') + txt[e:]
    return txt

df['text'] = df['text'].apply(get_ner)

output:
>>> df
                       text
0      use your superpowers
1     vote for barack_obama
2    vote for marine_le_pen
3  play with michael_jordan
4    support the supporters",https://stackoverflow.com/questions/72995392,pandas,15-07-2022 14:15,771.0,1.0,1.0,True,16-07-2022 13:38,15-07-2022 14:56
73126202,the output of the pytourch dl network doesn&#39;t match the last layer provided in the network,"i am trying to build a pytorch model that can predict the rank of a text where the output is a float number between 0 and 1.
my input details are

my batch size is 32.
max length for the tokenizer is 116
in addition to the masks and ids generated from the tokenizer, i am adding 11 values that were generated through preprocessing to the input text.

s the entire input shape would be 32 for batch and array with 127 item for each sample text provided
my layers are as follows:

a distilbert uncased transformer. and i am using the distilbert tokenizer over the text.
the following layer is a cnn that takes the output of the distilbert (127 channel) as input and provide 64 channels as output, with kernel=1
after this, 6 cnn layers each input is 64 and output is 64 with a kernel size of 3 and dilation increasing from 2 to 32. on top of each cnn, there is a relu and a maxpooling with 2 as kernal size.
my last cnn layer (and where the issue is happening) have 64 input channels and 32 output channels with a kernel size of 1 and a relu with adaptivemaxpool1d with size of 32 on top of it
linear layer takes 32 and output 16
linear layer takes 16 and output 1

below is my code
class dataset(dataset):
    def __init__(self, df, max_len, bert_model_name, multi=1):
        super().__init__()
        self.df = df.reset_index(drop=true)
        self.max_len = max_len
        
        self.tokenizer = distilberttokenizer.from_pretrained(
            bert_model_name, 
            do_lower_case=true,
            strip_accents=true,
            wordpieces_prefix=none,
            use_fast=true
        )
        self.multiplier = multi

    def __getitem__(self, index):
        row = self.df.iloc[index]
        
        inputs = self.tokenizer.encode_plus(
            row.source,
            none,
            add_special_tokens=true,
            max_length=self.max_len,
            padding=""max_length"",
            return_token_type_ids=true,
            truncation=true
        )
        
        return (
            t.longtensor(t.cat([
                t.longtensor([
                    row.n_total_cells * self.multiplier, 
                    row.n_code_cells * self.multiplier,
                    row.n_markdown_cells * self.multiplier,
                    row.word_counts * self.multiplier,
                    row.line_counts * self.multiplier,
                    row.empty_line_counts * self.multiplier,
                    row.full_lines_count * self.multiplier,
                    row.text_lines_count * self.multiplier,
                    row.tag_lines_count * self.multiplier,
                    row.weight * self.multiplier,
                    row.weight_counts * self.multiplier,
                ]), 
                t.longtensor(inputs['input_ids']),
            ], 0)), 
            
            t.longtensor(t.cat([
                t.ones(11, dtype=t.long),
                t.longtensor(inputs['attention_mask']),
            ], 0)),
        )

class bmodel(nn.module):
    def __init__(self, bert_model_name):
        super(bmodel, self).__init__()
        self.distill_bert = distilbertmodel.from_pretrained(bert_model_name)       

        self.hidden_size = self.distill_bert.config.hidden_size
        print(self.hidden_size) # 768
        self.relu = nn.relu()
        self.dropout = nn.dropout(0.3)
        
        self.cnn_layers()

    def forward(self, inputs):
        dbert = self.cnn_forward(inputs[0], inputs[1])
        return dbert

 def cnn_layers(self):
        self.layers = 4
        kernel_size = 3
        inp = 127
        out = 32
        grades = [2, 4, 8, 16, 32, 64, ]
        
        self.convs = nn.modulelist()
        self.relus = nn.modulelist()
        self.maxs = nn.modulelist()
        self.norms = nn.modulelist()
        
        self.start_conv = nn.conv1d(
            in_channels=inp,
            out_channels=64,
            kernel_size=1,
            bias=true
        )
        
        for i in range(self.layers):
            # dilated convolutions
            self.convs.append(nn.conv1d(
                in_channels=64,
                out_channels=64,
                kernel_size = kernel_size,
                bias=false,
                dilation=grades[i]
            ))

            self.relus.append(nn.relu())

            self.maxs.append(nn.maxpool1d(
                kernel_size=kernel_size-1,
            ))

            self.norms.append(nn.batchnorm1d(
                num_features=64,
            ))


        self.end_conv = nn.conv1d(
            in_channels=64,
            out_channels=out,
            kernel_size=1,
            bias=true
        )
        
        self.max_pool = nn.adaptivemaxpool1d(out)
        
        self.top1 = nn.linear(out, 16) 
        self.top2 = nn.linear(16, 1)
        
    def cnn_forward(self, ids, masks):
        x = self.distill_bert(ids, masks)[0]
        x = self.relu(x)
        x = self.dropout(x)
        print(f""x size after bert:"", x.size())
        
        x = self.start_conv(x)
        print(f""x size after first conv:"", x.size())
        for i in range(self.layers):
            x = self.norms[i](self.maxs[i](self.relus[i](self.convs[i](x))))
            print(f""x size after {i} cnn dilation:"", x.size())
            
        x = self.max_pool(t.abs(self.end_conv(x)))
        print(""x size after adaptivemaxpool1d:"", x.size())
        
        x = self.top1(x)
        print(""x size after before-last linear:"", x.size())
        
        x = self.top2(x)
        print(""x size after last linear:"", x.size())
        return x


printing the output size after each layer would be as below
x size after first conv: torch.size([32, 64, 768])
x size after 0 cnn dilation: torch.size([32, 64, 382])
x size after 1 cnn dilation: torch.size([32, 64, 187])
x size after 2 cnn dilation: torch.size([32, 64, 85])
x size after 3 cnn dilation: torch.size([32, 64, 26])
x size after adaptivemaxpool1d: torch.size([32, 32, 32])
x size after before-last linear: torch.size([32, 32, 16])
x size after last linear: torch.size([32, 32, 1]

the issue i am facing is after the adaptivemaxpool1d, the output of this layer suppose to be 2 dimensions instead of 3 [32, 32] instead of [32, 32, 32]
the output of adaptivemaxpool1d fits into the linear layer but is with one extra dimension causing the output pred to differ from the true input
when i check the pred size vs the true size it would be
y_pred shape (12480,)
y_val shape (390,)

and the code blow with this error
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
input in [13], in <cell line: 21>()
     17 # print(mkdn_train_loader, mkdn_val_loader)
     18 
     19 ########################################################################################################################

file e:\kaggle_comp\pt_model.py:796, in train(model, train_loader, val_loader, epochs, patience, path)
    793 print('y_val shape', y_val.shape)
    794 print(y_pred[:10])
--> 796 print(""validation mse:"", np.round(mean_squared_error(y_val, y_pred), 4))
    797 print()
    799 early_stopping(np.round(mean_squared_error(y_val, y_pred), 4), model)

file ~\appdata\local\programs\python\python310\lib\site-packages\sklearn\metrics\_regression.py:438, in mean_squared_error(y_true, y_pred, sample_weight, multioutput, squared)
    378 def mean_squared_error(
    379     y_true, y_pred, *, sample_weight=none, multioutput=""uniform_average"", squared=true
    380 ):
    381     """"""mean squared error regression loss.
    382 
    383     read more in the :ref:`user guide <mean_squared_error>`.
   (...)
    436     0.825...
    437     """"""
--> 438     y_type, y_true, y_pred, multioutput = _check_reg_targets(
    439         y_true, y_pred, multioutput
    440     )
    441     check_consistent_length(y_true, y_pred, sample_weight)
    442     output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

file ~\appdata\local\programs\python\python310\lib\site-packages\sklearn\metrics\_regression.py:94, in _check_reg_targets(y_true, y_pred, multioutput, dtype)
     60 def _check_reg_targets(y_true, y_pred, multioutput, dtype=""numeric""):
     61     """"""check that y_true and y_pred belong to the same regression task.
     62 
     63     parameters
   (...)
     92         the dtype argument passed to check_array.
     93     """"""
---> 94     check_consistent_length(y_true, y_pred)
     95     y_true = check_array(y_true, ensure_2d=false, dtype=dtype)
     96     y_pred = check_array(y_pred, ensure_2d=false, dtype=dtype)

file ~\appdata\local\programs\python\python310\lib\site-packages\sklearn\utils\validation.py:332, in check_consistent_length(*arrays)
    330 uniques = np.unique(lengths)
    331 if len(uniques) > 1:
--> 332     raise valueerror(
    333         ""found input variables with inconsistent numbers of samples: %r""
    334         % [int(l) for l in lengths]
    335     )

valueerror: found input variables with inconsistent numbers of samples: [390, 12480]


i need to know what i must change to make this run and the size is passed with correct shape.","['python', 'deep-learning', 'pytorch', 'conv-neural-network', 'huggingface-transformers']",73129005,"from adaptivemaxpool1d documentation:
if the input is in shape of (n, c, l_in), then your output would be in the shape of (n, c, l_out).
since your input shape to the adaptivemaxpool1d is in the shape of (32, 32, 26) and you've set the output_size to 32 ( value of ""out"" variable ), your output shape comes out as (32, 32, 32).
i suggest to set output_size as 1 and use squeeze(2) to squish down the dimension.
something like this:
# for initialization of maxpool layer.
nn.adaptivemaxpool1d(1)
# ---------
# in forward add squeeze(2) after max_pool like this:
x = self.max_pool(t.abs(self.end_conv(x))).squeeze(2)",https://stackoverflow.com/questions/73126202,python,26-07-2022 15:36,144.0,-1.0,1.0,True,26-07-2022 19:50,26-07-2022 18:32
44238154,what is the difference between luong attention and bahdanau attention?,these two attentions are used in seq2seq modules. the two different attentions are introduced as multiplicative and additive attentions in this tensorflow documentation. what is the difference?,"['tensorflow', 'deep-learning', 'nlp', 'attention-model']",44239754,"they are very well explained in a pytorch seq2seq tutorial.
the main difference is how to score similarities between the current decoder input and encoder outputs.",https://stackoverflow.com/questions/44238154,tensorflow,29-05-2017 08:43,38577.0,39.0,5.0,True,14-12-2023 20:01,26-10-2020 12:21
78400673,how can i wait for the actual reply of an openai-assistant? (python openai api),"i am interacting with the openai assistant api (python). so far, it works well. but sometimes the api returns the message i sent to the assistant instead of the assistants reply.
from openai import openai
from os import environ


open_api_key = environ.get('open_api_key')
assistant_id = ""xxxxxxx""


client = openai(api_key=open_api_key)
assistant = client.beta.assistants.retrieve(assistant_id)


def ask_assistant(message_text): 
    print(f'received message: {message_text}')
    
    thread = client.beta.threads.create()
    
    message = client.beta.threads.messages.create(
        thread_id=thread.id,
        role=""user"",
        content=message_text
    )

    run = client.beta.threads.runs.create(
        thread_id=thread.id,
        assistant_id=assistant_id
    )
    

    run_retrieve = client.beta.threads.runs.retrieve(
        thread_id=thread.id,
        run_id=run.id
    )

    
    messages = client.beta.threads.messages.list(thread.id)
    final_text = messages.data[0].content[0].text.value
    


    try:
        final_text = messages.data[0].content[0].text.value
        print(final_text)
    except exception as e:
        print(e)
        final_text = ''
    
    return final_text
    
    
if __name__ == ""__main__"":
    ask_assistant('how are you?')

as a hot fix, i implemented a sleep function that waits for 3 seconds before retrieving the reply. this works, but i don't expect it to be the best solution (or reliable at all). any ideas how to wait until the assistant really replied?
thank you very much.
update:
this works:
from openai import openai
from os import environ


open_api_key = environ.get('xxxxx')
assistant_id = ""xxxxx""


client = openai(api_key=open_api_key)
assistant = client.beta.assistants.retrieve(assistant_id)


def get_answer(run, thread):
    while not run.status == ""completed"":
        print(""waiting for answer..."")
        run = client.beta.threads.runs.retrieve(
            thread_id=thread.id,
            run_id=run.id
        )
        

    messages = client.beta.threads.messages.list(thread.id)
    answer = messages.data[0].content[0].text.value
    
    try:
        answer = messages.data[0].content[0].text.value
    except exception as e:
        print(e)
        answer = ''
        
    return answer


def ask_assistant(message_text): 
    print(f'sending to assistant: {message_text}')
    thread = client.beta.threads.create()
    
    message = client.beta.threads.messages.create(
        thread_id=thread.id,
        role=""user"",
        content=message_text
    )

    run = client.beta.threads.runs.create(
        thread_id=thread.id,
        assistant_id=assistant_id
    )
    
    answer = get_answer(run, thread)  
    print(f'assistant response: {answer}')
    return answer
    
    
if __name__ == ""__main__"":
    ask_assistant('hello')","['python', 'openai-api']",78401314,"this should work:
run = openai.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id
)
print(run)


while run.status !=""completed"":
  run = openai.beta.threads.runs.retrieve(
    thread_id=thread.id,
    run_id=run.id
  )
  print(run.status)

messages = openai.beta.threads.messages.list(
  thread_id=thread.id
)

print(messages.data[0].content[0].text.value)

thanks to @joyasree78",https://stackoverflow.com/questions/78400673,python,29-04-2024 05:42,1143.0,2.0,1.0,True,29-04-2024 16:05,29-04-2024 16:05
79313470,getting error when using memory with chain: typeerror: object of type member is not serializable,"i am getting this error: typeerror: object of type member is not serializable when trying to have memory with a chain.
my program is a discord bot that answers with ai on_message, the discord part completely works and i have removed some functions in the shown code.
this works without the memory.
class whitetiger(discord.client):
    def __init__(self, intents):
        super().__init__(intents=intents)
        self.target_channel = none
        self.llm = chatollama(
            model=""llama3.2"",
            temperature=0,
        )
        self.prompt = chatprompttemplate.from_messages(
            [
                (
                    ""system"",
                    ""<context>"",
                ),
                messagesplaceholder(variable_name=""messages""),
            ]
        )
        self.chain = self.prompt | self.llm

        self.init_memory()

    def init_memory(self):
        workflow = stategraph(state_schema=discordmessagesstate)

        def call_model(state: discordmessagesstate):
            response = self.chain.invoke(state)
            return {""messages"": [response]}

        workflow.add_edge(start, ""model"")
        workflow.add_node(""model"", call_model)

        self.memory = memorysaver()
        self.app = workflow.compile(checkpointer=self.memory)
        self.config = {""configurable"": {""thread_id"": ""abc1234""}}


    async def on_message(self, message):
        if (
            self.target_channel
            and message.channel.id == self.target_channel.id
            and message.author.id != self.user.id
        ):
            print(f""message from {message.author}: {message.content}"")

            print(f""{message.author}: {message.content}"")

            reply = self.app.invoke(
                {
                    ""messages"": [humanmessage(f""{message.author}: {message.content}"")],
                    ""author"": message.author,
                },
                self.config
            )

            await message.reply(
                reply[""messages""][-1].pretty_print(), mention_author=true
            )

full traceback: 
thanks in advance!","['discord.py', 'langchain', 'ollama']",79314039,"this error occurs because you are passing a member object to a function which later tries to convert it to json and fails.
to fix that, you need to pass a member's name, id, or other string or int which is serializable. it seems name is the thing you are looking for:
reply = self.app.invoke(
    {
        ""messages"": [humanmessage(f""{message.author}: {message.content}"")],
        ""author"": message.author.name,
    },
    self.config
)",https://stackoverflow.com/questions/79313470,discord.py,28-12-2024 08:46,48.0,0.0,1.0,True,28-12-2024 15:17,28-12-2024 08:52
77269591,"nvidia driver too old error when loading bart model onto cuda, works on other models","i'm getting an error loading a huggingface model on an azureml gpu compute.  loading other models works, such as the first one in the example below:
from transformers import automodelforcausallm

device = ""cuda""

checkpoint1 = ""salesforce/codegen-350m-mono"" 
# this works!!
codegen = automodelforcausallm.from_pretrained(checkpoint1, trust_remote_code=true).to(device)

checkpoint2 = ""facebook/bart-large""
# this doesn't
bart = automodelforcausallm.from_pretrained(checkpoint2, trust_remote_code=true).to(device)

here's the full error:
runtimeerror: the nvidia driver on your system is too old (found version 11040). please update your gpu driver by downloading and installing a new version from the url:  alternatively, go to:  to install a pytorch version that has been compiled with your version of the cuda driver.
i understand the drivers don't match up with the torch version but messing around with the drivers on the machine seems like it would be something that azure would have accounted for.
here are my relevant libraries for references:
transformers 4.34.0
torch 2.1.0
nvidia-cublas-cu12      12.1.3.1
nvidia-cuda-cupti-cu12      12.1.105
nvidia-cuda-nvrtc-cu12      12.1.105
nvidia-cuda-runtime-cu12    12.1.105
nvidia-cudnn-cu12       8.9.2.26
nvidia-cufft-cu12       11.0.2.54
nvidia-curand-cu12      10.3.2.106
nvidia-cusolver-cu12        11.4.5.107
nvidia-cusparse-cu12        12.1.0.106
nvidia-nccl-cu12        2.18.1
nvidia-nvjitlink-cu12       12.2.140
nvidia-nvtx-cu12        12.1.105

note also that i install pytorch when i install transformers like this:
pip install transformers[torch]
i uses pip since that's the recommended way.
is there something about the bart model that requires a different gpu/torch config compared to other models?  is this a problem with some azureml compute configs?","['huggingface-transformers', 'azure-machine-learning-service']",77369879,"installing pytorch through transformers extras probably not the best way to get compatible torch to your environment. based on the cuda drivers in your base image you can try to install torch recommended way  it should be fine with transformers that only pin a lower bound. alternatively you can try more recent cuda image from nvcr.io if you have an option to specify it.
*posting comment on the question as answer",https://stackoverflow.com/questions/77269591,huggingface-transformers,11-10-2023 00:22,3439.0,1.0,1.0,True,26-10-2023 19:34,12-10-2023 14:49
75079926,how to use an api from my mobile app without someone stealing the token,"i'm building an app that makes use of the openai api
they provide me with an api token which i use to make the api calls from my android mobile app (react native)
i know it is a bad practice to store this api token on the mobile client because attackers might still it and use my quota and money.
what are my options? the trivial solution is to build a backend but i don't want to start implementing all the original api methods, i just prefer to use it directly from the client.
i've tried to store the token in a way that it cannot be found, but couldn't find a way.","['android', 'react-native', 'security', 'openai-api']",75109966,"your problem

they provide me with an api token which i use to make the api calls from my android mobile app (react native)
i know it is a bad practice to store this api token on the mobile client because attackers might still it and use my quota and money.

yes, its indeed a very bad practice, but at least you are aware of the risks, while a lot use this approach without realising how easy its for an attacker to grab such secrets (api tokens, api keys, whatever you name them).
in a series of articles i wrote on mobile api security i show how easy it can be done with static analyses and with a mitm attack:
how to extract an api key from a mobile app with static binary analysis:

the range of open source tools available for reverse engineering is huge, and we really can't scratch the surface of this topic in this article, but instead we will focus in using the mobile security framework(mobsf) to demonstrate how to reverse engineer the apk of our mobile app. mobsf is a collection of open source tools that present their results in an attractive dashboard, but the same tools used under the hood within mobsf and elsewhere can be used individually to achieve the same results.
during this article we will use the android hide secrets research repository that is a dummy mobile app with api keys hidden using several different techniques.

some attackers prefer to go straight to mitm attack, because they will learn how the app communicates with the api backend and will extract the secrets used, plus the blueprint they need to use for making the request and to parse the responses.
steal that api key with a man in the middle attack:

in order to help to demonstrate how to steal an api key, i have built and released in github the currency converter demo app for android, which uses the same jni/ndk technique we used in the earlier android hide secrets app to hide the api key.
so, in this article you will learn how to setup and run a mitm attack to intercept  traffic in a mobile device under your control, so that you can steal the api key. finally, you will see at a high level how mitm attacks can be mitigated.

possible solutions
reverse proxy

the trivial solution is to build a backend but i don't want to start implementing all the original api methods, i just prefer to use it directly from the client.

you don't need, you just need that your backend proxy the requests to the third party api you use on your mobile app, that in your case seems to be only for openapi.
for example, when your mobile app needs to make a request to openapi.io/some/resource instead it makes it to your-reverse-proxy.com/some/resource that will then grab the /some/resource part and build the request to openapi openapi.io/some/resource, adding the api token header to it, that now it's securely stored in your reverse proxy server.
using a reverse proxy to protect third party apis

in this article you will start by learning what third party apis are, and why you shouldnï¿½ï¿½ï¿½t access them directly from within your mobile app. next you will learn what a reverse proxy is, followed by when and why you should use it to protect the access to the third party apis used in your mobile app.


a recurring theme in this article was the advice not to access third party apis directly from a mobile app. as we have discussed, once your mobile app is released any secret in it becomes public, thus up for grabs by attackers to use on your behalf. if you are not careful you will be the one paying the bill or finding that your free tier resources have been exhausted by someone else.

the draw back of this approach is that you still have an api key that you need to secure, the one to access the reverse proxy, but at least you are not exposing your openapi secret and you can use s mechanisms to throttle requests and to secure access to your reverse proxy to ensure that only answers to requests from genuine and unmodified instances of your mobile app.
runtime secrets protection
you can devise or use an off the shelf mechanism to deliver the secrets to your mobile app just-in-time of them being required to be used on the api request being made to openapi, but you need to ensure that the secrets are only delivered to genuine and unmodified instances of your mobile app, that are not under a mitm attack, being tampered/instrumented at runtime with tools like frida, otherwise your secret it will be easily extracted by hooking to the function that adds them to an header in the api request or by intercepting the request with mitm attack, even when the communication channel it's secured with certificate pinning, because it's not that hard to bypass in a device the attacker controls.
in my reply to the question storing api keys securely in flutter or sending payment details to my server? i go in more detail on the runtime secret protection approach.
do you want to go the extra mile?
in any response to a security question i always like to reference the excellent work from the owasp foundation.
for apis
owasp api security top 10

the owasp api security project seeks to provide value to software developers and security assessors by underscoring the potential risks in insecure apis, and illustrating how these risks may be mitigated. in order to facilitate this goal, the owasp api security project will create and maintain a top 10 api security risks document, as well as a documentation portal for best practices when creating or assessing apis.

for mobile apps
owasp mobile security project - top 10 risks

the owasp mobile security project is a centralized resource intended to give developers and security teams the resources they need to build and maintain secure mobile applications. through the project, our goal is to classify mobile security risks and provide developmental controls to reduce their impact or likelihood of exploitation.

owasp - mobile security testing guide:

the mobile security testing guide (mstg) is a comprehensive manual for mobile app security development, testing and reverse engineering.",https://stackoverflow.com/questions/75079926,android,11-01-2023 08:06,2581.0,5.0,2.0,True,28-01-2025 21:35,11-01-2023 17:40
56896753,is there a way to get entire constituents using spacy?,"i guess i'm trying to navigate spacy's parse tree in a more blunt way than is provided.
for instance, if i have sentences like: ""he was a genius"" or ""the dog was green,"" i want to be able to save the objects to variables (""a genius"" and ""green""). 
token.children provides the immediate syntactic dependents, so, for the first example, the children of ""was"" are ""he"" and ""genius,"" and then ""a"" is a child of ""genius."" this isn't so helpful if i just want the entire constituent ""a genius."" i'm not sure how to reconstruct it from the token.children or if there's a better way.
i can figure out how to match ""is"" and ""was"" using token.text (part of what i'm trying to do), but i can't figure out how to return the whole constituent ""a genius"" using the info provided about children.
import spacy
nlp = spacy.load('en_core_web_sm')

sent = nlp(""he was a genius."")

for token in sent:
     print(token.text, token.tag_, token.dep_, [child for child in token.children])

this is the output:
he prp nsubj []
was vbd root [he, genius, .]
a dt det []
genius nn attr [a]
. . punct []","['python', 'nlp', 'tokenize', 'spacy']",56901756,"you can use token.subtree (see the docs) to get all dependents of a given node in the dependency tree.
for example, to get all noun phrases:
import spacy

nlp = spacy.load('en')

text = ""he was a genius of the best kind and his dog was green.""

for token in nlp(text):
    if token.pos_ in ['noun', 'adj']:
        if token.dep_ in ['attr', 'acomp'] and token.head.lemma_ == 'be':
            # to test for only verb forms 'is' and 'was' use token.head.lower_ in ['is', 'was']
            print([t.text for t in token.subtree])

outputs:
['a', 'genius', 'of', 'the', 'best', 'kind']
['green']",https://stackoverflow.com/questions/56896753,python,05-07-2019 04:55,4697.0,5.0,2.0,True,21-01-2023 09:54,16-09-2019 08:09
74791093,improve gensim most_similar() return values by using wordnet hypernyms,"import gensim.downloader as api
glove = api.load('glove-wiki-gigaword-200')

i first ran this code to download the pre-trained model.
glove.most_similar(positive=['sushi', 'uae'], negative=['japan'])

would then result in:
[('nahyan', 0.5181387066841125),
 ('caviar', 0.4778318405151367),
 ('paella', 0.4497394263744354),
 ('nahayan', 0.44313961267471313),
 ('zayed', 0.4321245849132538),
 ('omani', 0.4285220503807068),
 ('seafood', 0.4279175102710724),
 ('saif', 0.426000714302063),
 ('dirham', 0.4214130640029907),
 ('sashimi', 0.4165934920310974)]

and in this example, we can see that the method failed to capture the 'type' or 'category' of the query. 'zayed', 'nahyan' are not actually of 'type' food and rather they represent person name.
the approach suggested by my professor is to use wordnet hypernyms to find the 'type'.
with much research, the closest solution i found is to somehow incorporate
lowest_common_hypernyms() that will give the lowest common hypernym between two synsets and use it to filter the results of most_similar().
i am not sure if my idea make sense and would like the community feedback on this.
my idea is compute the hypernym of, e.g. 'sushi' and the hypernyms of all the similar words returned by most_similar() and only choose the word with 'longest' lowest common hypernym path. i expect this should return the word that best matches the 'type'
not sure if it makes sense...","['python', 'nlp', 'nltk', 'gensim', 'wordnet']",74793397,"does your proposed approach give adequate results when you try it?
that's the only test of whether the idea makes sense.
word2vec is generally oblivious to the all the variations of category that a lexicon like wordnet can provide ï¿½ï¿½ï¿½ all the words that are similar to another word, in any aspect, will be neighbors.  even words that people consider opposites ï¿½ï¿½ï¿½ like 'hot' and 'cold' ï¿½ï¿½ï¿½ will be often be fairly close to each other, in some direction in the coordinate space, as they are similar in what they describe and what contexts they're used in. (they can be drop-in replacements for each other.)
word2vec is also fairly oblivious to polysemy in its standard formulation.
some other things worth trying might be:

if you need only answers of a certain type, mix-in some measurement ranking candidate answers by their closeness to a word either describing that type ('food') or representing multiple examples (say an average vector for many food-names you answers)
choose another vector-set, or train your own. there's no universal ""goodness"" for word-vectors: their quality for certain tasks will vary based on their training data & parameters. vectors trained on something broader than wikipedia (your named vector file), or some text corpus more focused on your domain-of-interest ï¿½ï¿½ï¿½ say, food criticism ï¿½ï¿½ï¿½ might do better on some tasks. changing training parameters can also change which kinds of similarity are most emphasized in the resulting vectors. for example, some observers have noticed small context-windows tend to put words that are direct drop-in replacements for each other closer-together, while larger context-windows bring words from the same domains-of-use, even if not drop-in replacements of the same 'type', closer. (it sounds like your current need might be best served with a model trained with smaller windows.)",https://stackoverflow.com/questions/74791093,python,13-12-2022 21:31,147.0,1.0,2.0,True,14-12-2022 08:53,13-12-2022 21:35
68604289,attributeerror: module transformers has no attribute tfgptneoforcausallm,"i cloned this repository/documentation 
i get the below error whether i run it on google collab or locally. i also installed transformers using this
pip install git+

and made sure the configuration file is named as config.json
      5 tokenizer = autotokenizer.from_pretrained(""gpt-neo-125m/"",from_tf=true)
----> 6 model = automodelforcausallm.from_pretrained(""gpt-neo-125m"",from_tf=true)
      7 
      8 

3 frames
/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py in __getattr__(self, name)

attributeerror: module transformers has no attribute tfgptneoforcausallm


full code:
from transformers import autotokenizer, automodelforcausallm 

tokenizer = autotokenizer.from_pretrained(""eleutherai/gpt-neo-125m"",from_tf=true)

model = automodelforcausallm.from_pretrained(""eleutherai/gpt-neo-125m"",from_tf=true)


transformers-cli env results:

transformers version: 4.10.0.dev0
platform: linux-4.4.0-19041-microsoft-x86_64-with-glibc2.29
python version: 3.8.5
pytorch version (gpu?): 1.9.0+cpu (false)
tensorflow version (gpu?): 2.5.0 (false)
flax version (cpu?/gpu?/tpu?): not installed (na)
jax version: not installed
jaxlib version: not installed
using gpu in script?: 
using distributed or parallel set-up in script?: 

both collab and locally have tensorflow 2.5.0 version","['python', 'pytorch', 'huggingface-transformers', 'gpt-3']",68656887,"my solution was to first edit the source code to remove the line that adds ""tf"" in front of the package as the correct transformers module is gptneoforcausallm
, but somewhere in the source code it manually added a ""tf"" in front of it.
secondly, before cloning the repository it is a must to run
 git lfs install. 

this link helped me install git lfs properly",https://stackoverflow.com/questions/68604289,python,31-07-2021 17:14,12606.0,2.0,2.0,True,07-08-2023 21:13,07-08-2023 21:13
19258652,how to get synonyms from nltk wordnet python,"wordnet is great, but i'm having a hard time getting synonyms in nltk. if you search similar to for the word 'small' like here, it shows all of the synonyms.  
basically i just need to know the following:
wn.synsets('word')[i].option() where option can be hypernyms and antonyms, but what is the option for getting synonyms?","['python', 'nltk', 'wordnet']",32718824,"if you want the synonyms in the synset (aka the lemmas that make up the set), you can get them with lemma_names():
>>> for ss in wn.synsets('small'):
>>>     print(ss.name(), ss.lemma_names())

small.n.01 ['small']
small.n.02 ['small']
small.a.01 ['small', 'little']
minor.s.10 ['minor', 'modest', 'small', 'small-scale', 'pocket-size',  'pocket-sized']
little.s.03 ['little', 'small']
small.s.04 ['small']
humble.s.01 ['humble', 'low', 'lowly', 'modest', 'small']    
...",https://stackoverflow.com/questions/19258652,python,08-10-2013 21:20,76867.0,42.0,8.0,True,26-09-2022 06:26,12-04-2015 15:08
77785423,shap value for binary classification using pre-train bert: how to extract summary graph?,"i used pre-train bert model for binary classification. after training my model with my small data, i wanted to extract summary graph like this the graph i want. however, i want to replace these important features  with words.
however, i am not sure everything is okay because the shape of shap_value is only two dimensional. actually, this is sensible. nevertheless, i did not get the graph because i encountered two problems if i use this code:
shap.summary_plot(shap_values[:,:10],feature_names=feature_importance['features'].tolist(),features=comments_text)`

problem is too unsensible: if i change shap_values[:,:10] with shap_values  or shap_values[0] or shap_values.values vb. i always come across
516: assert len(shap_values.shape) != 1, ""summary plots need a matrix of 
shap_values, not a vector."" ==> assertionerror: summary plots need a matrix of 
shap_values, not a vector.

(fist problem)
by the way, my shap_value consist of 10 input(shape_value.shape). if i choose for max value a range from 1 to 147 everything fine for drawing the graph. however,in this time, the graph is not suitable: my graph consist of only blue dot(-second problem-). like this only blue not.
note: shap_values[:,:10] if the number (10) change different number, the graph show diffent word however the total number of the graph same (max 20). only some words order can be changing.
minimal reproducible example:
import nlp
import numpy as np
import pandas as pd
import scipy as sp
import torch
import transformers
import torch
import shap

# load a bert sentiment analysis model
tokenizer = transformers.distilberttokenizerfast.from_pretrained(
    ""distilbert-base-uncased""
)
model = transformers.distilbertforsequenceclassification.from_pretrained(
    ""distilbert-base-uncased-finetuned-sst-2-english""
).cuda()


if torch.cuda.is_available():
    device = torch.device(""cuda"")
    print('we will use the gpu:', torch.cuda.get_device_name(0))

else:
    print('no gpu available, using the cpu instead.')
    device = torch.device(""cpu"")

def f(x):
    # encode the batch of sentenc
    inputs = tokenizer.batch_encode_plus(x.tolist(), max_length=450,add_special_tokens=true, return_attention_mask=true,padding='max_length',truncation=true,return_tensors='pt')

    # send the tensors to the same device as the model
    input_ids = inputs['input_ids'].to(device)
    attention_masks = inputs['attention_mask'].to(device)
    # predict
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_masks)[0].detach().cpu().numpy()
    scores = (np.exp(outputs).t / np.exp(outputs).sum(-1)).t
    val = sp.special.logit(scores[:, 1])  # use one vs rest logit units
    return val
# build an explainer using a token masker
explainer = shap.explainer(f, tokenizer )

imdb_train = nlp.load_dataset(""imdb"")[""train""]
shap_values = explainer(imdb_train[:10], fixed_context=1, batch_size=16)
cohorts = {"""": shap_values}
cohort_labels = list(cohorts.keys())
cohort_exps = list(cohorts.values())
for i in range(len(cohort_exps)):
    if len(cohort_exps[i].shape) == 2:
        cohort_exps[i] = cohort_exps[i].abs.mean(0)
features = cohort_exps[0].data
feature_names = cohort_exps[0].feature_names
#values = np.array([cohort_exps[i].values for i in range(len(cohort_exps))], dtype=object)
values = np.array([cohort_exps[i].values for i in range(len(cohort_exps))])
feature_importance = pd.dataframe(list(zip(feature_names, sum(values))), columns=['features', 'importance'])
feature_importance.sort_values(by=['importance'], ascending=false, inplace=true)
shap.summary_plot(shap_values[:,:10],feature_names=feature_importance['features'].tolist(),features=imdb_train['text'][10:20],show=false)


the above code produce the same result. i spent approximately 200 computer units and i did not succeed it :(. how can i do?","['python', 'machine-learning', 'bert-language-model', 'text-classification', 'shap']",77890455,"will you try:
sv = np.array([arr[:100] for arr in shap_values.values])
data = np.array([arr[:100] for arr in shap_values.data])
shap.summary_plot(sv, data, feature_names=feature_importance['features'].tolist())

i've got a grey plot. this is because your data is non-numeric.",https://stackoverflow.com/questions/77785423,python,09-01-2024 08:49,778.0,0.0,1.0,True,27-01-2024 08:01,20-01-2024 15:28
32331848,create a custom transformer in pyspark ml,"i am new to spark sql dataframes and ml on them (pyspark). 
how can i create a custom tokenizer, which for example removes stop words and uses some libraries from nltk? can i extend the default one?","['python', 'apache-spark', 'nltk', 'pyspark', 'apache-spark-ml']",32337101,"can i extend the default one?

not really. default tokenizer is a subclass of pyspark.ml.wrapper.javatransformer and, same as other transfromers and estimators from pyspark.ml.feature, delegates actual processing to its scala counterpart. since you want to use python you should extend pyspark.ml.pipeline.transformer directly. 
import nltk

from pyspark import keyword_only  ## < 2.0 -> pyspark.ml.util.keyword_only
from pyspark.ml import transformer
from pyspark.ml.param.shared import hasinputcol, hasoutputcol, param, params, typeconverters
# available in pyspark >= 2.3.0 
from pyspark.ml.util import defaultparamsreadable, defaultparamswritable  
from pyspark.sql.functions import udf
from pyspark.sql.types import arraytype, stringtype

class nltkwordpuncttokenizer(
        transformer, hasinputcol, hasoutputcol,
        # credits 
        # by 
        defaultparamsreadable, defaultparamswritable):

    stopwords = param(params._dummy(), ""stopwords"", ""stopwords"",
                      typeconverter=typeconverters.toliststring)


    @keyword_only
    def __init__(self, inputcol=none, outputcol=none, stopwords=none):
        super(nltkwordpuncttokenizer, self).__init__()
        self.stopwords = param(self, ""stopwords"", """")
        self._setdefault(stopwords=[])
        kwargs = self._input_kwargs
        self.setparams(**kwargs)

    @keyword_only
    def setparams(self, inputcol=none, outputcol=none, stopwords=none):
        kwargs = self._input_kwargs
        return self._set(**kwargs)

    def setstopwords(self, value):
        return self._set(stopwords=list(value))

    def getstopwords(self):
        return self.getordefault(self.stopwords)

    # required in spark >= 3.0
    def setinputcol(self, value):
        """"""
        sets the value of :py:attr:`inputcol`.
        """"""
        return self._set(inputcol=value)

    # required in spark >= 3.0
    def setoutputcol(self, value):
        """"""
        sets the value of :py:attr:`outputcol`.
        """"""
        return self._set(outputcol=value)

    def _transform(self, dataset):
        stopwords = set(self.getstopwords())

        def f(s):
            tokens = nltk.tokenize.wordpunct_tokenize(s)
            return [t for t in tokens if t.lower() not in stopwords]

        t = arraytype(stringtype())
        out_col = self.getoutputcol()
        in_col = dataset[self.getinputcol()]
        return dataset.withcolumn(out_col, udf(f, t)(in_col))

example usage (data from ml - features):
sentencedataframe = spark.createdataframe([
  (0, ""hi i heard about spark""),
  (0, ""i wish java could use case classes""),
  (1, ""logistic regression models are neat"")
], [""label"", ""sentence""])

tokenizer = nltkwordpuncttokenizer(
    inputcol=""sentence"", outputcol=""words"",  
    stopwords=nltk.corpus.stopwords.words('english'))

tokenizer.transform(sentencedataframe).show()

for custom python estimator see how to roll a custom estimator in pyspark mllib
ï¿½ï¿½ï¿½ this answer depends on internal api and is compatible with spark 2.0.3, 2.1.1, 2.2.0 or later (spark-19348). for code compatible with previous spark versions please see revision 8.",https://stackoverflow.com/questions/32331848,python,01-09-2015 12:36,28899.0,35.0,1.0,True,27-01-2023 11:46,26-05-2020 08:52
73144451,modulenotfounderror: no module named &#39;setuptools.command.build&#39;,"i am trying to pip install sentence transformers. i am working on a macbook pro with an m1 chip. i am using the following command:

pip3 install -u sentence-transformers

when i run this, i get this error/output and i do not know how to fix it...
defaulting to user installation because normal site-packages is not writeable
collecting sentence-transformers
  using cached sentence-transformers-2.2.2.tar.gz (85 kb)
  preparing metadata (setup.py) ... done
collecting transformers<5.0.0,>=4.6.0
  using cached transformers-4.21.0-py3-none-any.whl (4.7 mb)
collecting tqdm
  using cached tqdm-4.64.0-py2.py3-none-any.whl (78 kb)
requirement already satisfied: torch>=1.6.0 in ./library/python/3.8/lib/python/site-packages (from sentence-transformers) (1.12.0)
collecting torchvision
  using cached torchvision-0.13.0-cp38-cp38-macosx_11_0_arm64.whl (1.2 mb)
requirement already satisfied: numpy in ./library/python/3.8/lib/python/site-packages (from sentence-transformers) (1.23.1)
collecting scikit-learn
  using cached scikit_learn-1.1.1-cp38-cp38-macosx_12_0_arm64.whl (7.6 mb)
collecting scipy
  using cached scipy-1.8.1-cp38-cp38-macosx_12_0_arm64.whl (28.6 mb)
collecting nltk
  using cached nltk-3.7-py3-none-any.whl (1.5 mb)
collecting sentencepiece
  using cached sentencepiece-0.1.96.tar.gz (508 kb)
  preparing metadata (setup.py) ... done
collecting huggingface-hub>=0.4.0
  using cached huggingface_hub-0.8.1-py3-none-any.whl (101 kb)
collecting requests
  using cached requests-2.28.1-py3-none-any.whl (62 kb)
collecting pyyaml>=5.1
  using cached pyyaml-6.0.tar.gz (124 kb)
  installing build dependencies ... done
  getting requirements to build wheel ... done
  preparing metadata (pyproject.toml) ... done
requirement already satisfied: typing-extensions>=3.7.4.3 in ./library/python/3.8/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.3.0)
requirement already satisfied: filelock in ./library/python/3.8/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.1)
requirement already satisfied: packaging>=20.9 in ./library/python/3.8/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)
collecting tokenizers!=0.11.3,<0.13,>=0.11.1
  using cached tokenizers-0.12.1.tar.gz (220 kb)
  installing build dependencies ... done
  getting requirements to build wheel ... error
  error: subprocess-exited-with-error
  
  ï¿½ï¿½ getting requirements to build wheel did not run successfully.
  ï¿½ï¿½ï¿½ exit code: 1
  ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½> [20 lines of output]
      traceback (most recent call last):
        file ""/users/joeyoneill/library/python/3.8/lib/python/site-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 363, in <module>
          main()
        file ""/users/joeyoneill/library/python/3.8/lib/python/site-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 345, in main
          json_out['return_val'] = hos'])
        file ""/users/joeyoneill/library/python/3.8/lib/python/site-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 130, in get_requires_for_build_wheel
          return hook(config_settings)
        file ""/library/developer/commandlinetools/library/frameworks/python3.framework/versions/3.8/lib/python3.8/site-packages/setuptools/build_meta.py"", line 146, in get_requires_for_build_wheel
          return self._get_build_requires(
        file ""/library/developer/commandlinetools/library/frameworks/python3.framework/versions/3.8/lib/python3.8/site-packages/setuptools/build_meta.py"", line 127, in _get_build_requires
          self.run_setup()
        file ""/library/developer/commandlinetools/library/frameworks/python3.framework/versions/3.8/lib/python3.8/site-packages/setuptools/build_meta.py"", line 142, in run_setup
          exec(compile(code, __file__, 'exec'), locals())
        file ""setup.py"", line 2, in <module>
          from setuptools_rust import binding, rustextension
        file ""/private/var/folders/bg/ncfh283n4t39vqhvbd5n9ckh0000gn/t/pip-build-env-vjj6eow8/overlay/lib/python3.8/site-packages/setuptools_rust/__init__.py"", line 1, in <module>
          from .build import build_rust
        file ""/private/var/folders/bg/ncfh283n4t39vqhvbd5n9ckh0000gn/t/pip-build-env-vjj6eow8/overlay/lib/python3.8/site-packages/setuptools_rust/build.py"", line 20, in <module>
          from setuptools.command.build import build as commandbuild  # type: ignore[import]
      modulenotfounderror: no module named 'setuptools.command.build'
      [end of output]
  
  note: this error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

ï¿½ï¿½ getting requirements to build wheel did not run successfully.
ï¿½ï¿½ï¿½ exit code: 1
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½> see above for output.

note: this error originates from a subprocess, and is likely not a problem with pip.

can  should do or what is wrong with what i am currently doing? i factory reset my mac and re-downloaded everything but i still get this same error. i am stumped.","['python', 'macos', 'pip', 'sentence-transformers']",73868734,"i posted this as an issue to the actual sentence transformers github page. around 4 days ago i was given this answer by a ""federico viticci"" which resolved the issue and allowed me to finally install the library:
""for what it is worth, i was having the exact issue. installing it directly from source using
pip install git+

fixed it on my m1 max macbook pro.""
original git issue here:",https://stackoverflow.com/questions/73144451,python,27-07-2022 20:48,18566.0,13.0,3.0,True,14-06-2023 17:37,27-07-2022 20:59
74248881,how to extract phrases from text using specific noun-verb-noun nltk pos tag patterns?,"i have a data frame that has a column containing some text.
i want to extract phrases from the text with the format nn + vb + nn or nn + nn + vb + nn or nn + ... + nn + vb + nn et cetera. basically, i want to get the simple phrases with 1 to n nouns before the first encountered verb, followed by a noun.
i'm using nltk.pos_tag after tokenizing the texts to get the tag of each word, however i cannot find a way to get what i want.
i also thought about bigrams, trigrams, ngrams etc. but couldn't find a way to apply it.
any help, please?","['nlp', 'nltk', 'pos-tagger']",74249680,"here is a solution which utilises nltk.regexparser with a custom grammar rule to match occurrences of any numbers of nouns, followed by a verb, followed by a noun, specifically:
{<n.*>+<v.*><n.*>} 

which is equivalent to,

{<nn|nns|nnp|nnps>+<vb|vbp|vbz|vbg|vbd|vbn><nn|nns|nnp|nnps>}

example
parsing ""prodikos socrates recommended plato, and plato recommended aristotle"" produces the following labelled parse tree:

output:
['prodikos', 'socrates', 'recommended', 'plato']
['plato', 'recommended', 'aristotle']

note: the above rule does not handle symbols and punctuation interrupting the first sequence nouns (e.g. ""prodikos, socrates recommended plato"" will only match ""socrates recommended plato""). there is likely a way to handle this case using some regexp pattern and the nltk pos tags but it is not immediately obvious to me.
solution
from nltk import word_tokenize, pos_tag, regexpparser

# text for testing
text = ""prodikos socrates recommended plato, and plato recommended aristotle""

tokenized = word_tokenize(text)  # tokenize text
tagged = pos_tag(tokenized)  # tag tokenized text with pos tags
print(tagged)
# output: [('prodikos', 'nnp'), ('socrates', 'nnp'), ('recommended', 'vbd'), ('plato', 'nnp'), (',', ','),
# ('and', 'cc'), ('plato', 'nnp'), ('recommended', 'vbd'), ('aristotle', 'nnp')]

# create custom grammar rule to label occurrences of any number of nouns, followed by a verb, followed by a noun
my_grammar = r""""""
nouns_verb_noun: {<n.*>+<v.*><n.*>}""""""


# function to create parse tree using custom grammar rules and pos tagged text
def get_parse_tree(grammar, pos_tagged_text):
    cp = regexpparser(grammar)
    parse_tree = cp.parse(pos_tagged_text)
    parse_tree.draw()  # visualise parse tree
    return parse_tree


# function to get labels from custom grammar:
# takes line separated nltk regexp grammar rules
def get_labels_from_grammar(grammar):
    labels = []
    for line in grammar.splitlines()[1:]:
        labels.append(line.split("":"")[0])
    return labels


# function takes parse tree & list of nltk custom grammar labels as input
# returns phrases which match
def get_phrases_using_custom_labels(parse_tree, custom_labels_to_get):
    matching_phrases = []
    for node in parse_tree.subtrees(filter=lambda x: any(x.label() == custom_l for custom_l in custom_labels_to_get)):
        # get phrases only, drop pos tags
        matching_phrases.append([leaf[0] for leaf in node.leaves()])
    return matching_phrases


text_parse_tree = get_parse_tree(my_grammar, tagged)
my_labels = get_labels_from_grammar(my_grammar)
phrases = get_phrases_using_custom_labels(text_parse_tree, my_labels)

for phrase in phrases:
    print(phrase)
# output:
# ['prodikos', 'socrates', 'recommended', 'plato']
# ['plato', 'recommended', 'aristotle']",https://stackoverflow.com/questions/74248881,nlp,29-10-2022 21:14,608.0,1.0,1.0,True,30-10-2022 12:37,30-10-2022 12:37
73841388,how can i finetune a model from openai&#39;s whisper asr on my own training data?,"i use openai's whisper python lib for speech recognition. i have some training data: either text only, or audio  + corresponding transcription. how can i finetune a model from openai's whisper asr on my own training data?","['python', 'speech-recognition', 'openai-api', 'fine-tuning', 'openai-whisper']",73847374,"from  the released code doesn't contain the training/finetuning part. therefore one would have to write it to be able to train/finetune a model from openai's whisper asr on my own training data.
also, from 

we are open-sourcing models and inference code to serve as a foundation for building useful applications and for further research on robust speech processing.

no training code mentioned.

william castrillon and nizata pointed to the following fine-tuning codes created by third-party developers:

 (code)",https://stackoverflow.com/questions/73841388,python,25-09-2022 00:28,8833.0,3.0,1.0,True,11-05-2023 23:46,25-09-2022 00:33
74596102,pandas append string tokens into list with corresponding column where those column in those string rows having same value,"i'm working on this dataset.

my question is how do i group this dataset based on the same timestamp and merge these strings into one with unique tokens, so, for example, i could have:




date
string




2011-02-01 15:00:00
richmond service index s&p/cs hpi composite - 20 s.a. n.s.a texas services sector outlook tic net long-term transactions including swaps




i don't have any idea on what method should i use to solve this problem. does anyone know how to solve it?","['pandas', 'machine-learning', 'nlp', 'data-science', 'nltk']",74596471,"could this help you?
import pandas as pd
from collections import ordereddict

df['event'] = df['event'].str.replace('amp;', '')
df = df.groupby('date')['event'].apply(lambda x: ' '.join(x)).reset_index()
df['event'] = df['event'].str.split().apply(lambda x: ordereddict.fromkeys(x).keys()).str.join(' ')",https://stackoverflow.com/questions/74596102,pandas,28-11-2022 04:48,206.0,0.0,1.0,True,28-11-2022 11:45,28-11-2022 11:45
78004857,langchain not deploying to firebase function,"i am trying to deploy this function using langchain to firebase functions but i keep getting this error when i deploy it.
node_modules/@langchain/openai/node_modules/@langchain/core/dist/language_models/chat_models.d.ts:64:129 - error ts2304: cannot find name 'this'.

64     _generatecached({ messages, cache, llmstringkey, parsedoptions, handledoptions, }: chatmodelgeneratecachedparameters<typeof this>): promise<llmresult & {
                                                                                                                                   ~~~~

node_modules/@langchain/openai/node_modules/@langchain/core/dist/language_models/llms.d.ts:66:122 - error ts2304: cannot find name 'this'.

66     _generatecached({ prompts, cache, llmstringkey, parsedoptions, handledoptions, }: llmgeneratecachedparameters<typeof this>): promise<llmresult & {
                                                                                                                            ~~~~

src/index.ts:36:31 - error ts2345: argument of type 'chatopenai<chatopenaicalloptions>' is not assignable to parameter of type 'runnablelike<chatpromptvalueinterface, basemessagechunk>'.
  type 'chatopenai<chatopenaicalloptions>' is missing the following properties from type 'runnablemaplike<chatpromptvalueinterface, basemessagechunk>': concat, text, content, additional_kwargs, and 3 more.

36     const chain = prompt.pipe(chatmodel).pipe(outputparser);
                                 ~~~~~~~~~


found 3 errors in 3 files.

errors  files
     1  node_modules/@langchain/openai/node_modules/@langchain/core/dist/language_models/chat_models.d.ts:64
     1  node_modules/@langchain/openai/node_modules/@langchain/core/dist/language_models/llms.d.ts:66
     1  src/index.ts:36

error: functions predeploy error: command terminated with non-zero exit code 2

this is my tsconfig.json
{
  ""compileroptions"": {
    ""module"": ""nodenext"",
    ""noimplicitreturns"": true,
    ""nounusedlocals"": true,
    ""outdir"": ""lib"",
    ""sourcemap"": true,
    ""strict"": true,
    ""target"": ""es2020"",
    ""esmoduleinterop"": true,
    ""moduleresolution"": ""node""
  },
  ""compileonsave"": true,
  ""include"": [""src""]
}

my package.json
{
  ""name"": ""functions"",
  ""scripts"": {
    ""build"": ""tsc"",
    ""build:watch"": ""tsc --watch"",
    ""serve"": ""npm run build && firebase emulators:start --only functions"",
    ""shell"": ""npm run build && firebase functions:shell"",
    ""start"": ""npm run shell"",
    ""deploy"": ""firebase deploy --only functions"",
    ""logs"": ""firebase functions:log""
  },
  ""engines"": {
    ""node"": ""20""
  },
  ""main"": ""lib/index.js"",
  ""dependencies"": {
    ""@langchain/openai"": ""^0.0.14"",
    ""firebase-admin"": ""^11.8.0"",
    ""firebase-functions"": ""^4.3.1"",
    ""langchain"": ""^0.1.18"",
    ""stream-chat"": ""^8.16.0""
  },
  ""overrides"": {
    ""@langchain/core"": ""0.1.5""
  },
  ""devdependencies"": {
    ""firebase-functions-test"": ""^3.1.0"",
    ""typescript"": ""^4.9.0""
  },
  ""private"": true
}

this is the code for my firebase function. it is expected to trigger the ai using langchain and then send the ai message back to the user.
import { onrequest } from ""firebase-functions/v2/
import * as logger from ""firebase-functions/logger"";
import { streamchat } from ""stream-chat"";
import { chatopenai } from ""@langchain/openai"";
import { chatprompttemplate } from ""@langchain/core/prompts"";
import { stringoutputparser } from ""@langchain/core/output_parsers"";

export const myfunction = onrequest(
  { timeoutseconds: 60, region: [""europe-west2""] },
  (request, response) => {
    const client = streamchat.getinstance(
      ""api_key"",
      ""api_secret""
    );
    const signature = request.headers[""x-signature""] as string;
    const valid = client.verifywebhook(request.rawbody, signature);
    if (!valid) response.status(204).end();

    const chatmodel = new chatopenai({
      openaiapikey: ""openai_key"",
    });

    const outputparser = new stringoutputparser();

    const prompt = chatprompttemplate.frommessages([
      [
        ""system"",
        ""you are a helpful assistant"",
      ],
      [""user"", ""{input}""],
    ]);

    const chain = prompt.pipe(chatmodel).pipe(outputparser);

    if (request.body.user.id !== ""system_user"") {
      chain.invoke({ input: request.body.message.text }).then((aires) => {
        const channel = client.channel(
          ""messaging"",
          ""channel_id""
        );
        return channel.sendmessage({
          user_id: ""my-app"",
          text: aires,
        });
      });
    }

    response.status(200).end();
  }
);

update
changed to typescript 5 but got rewarded with new errors
node_modules/@langchain/core/dist/utils/stream.d.ts:1:18 - error ts2320: interface 'iterablereadablestreaminterface<t>' cannot simultaneously extend types 'readablestream<t>' and 'asynciterable<t>'.
  named property '[symbol.asynciterator]' of types 'readablestream<t>' and 'asynciterable<t>' are not identical.

1 export interface iterablereadablestreaminterface<t> extends readablestream<t>, asynciterable<t> {
                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

node_modules/@langchain/core/dist/utils/stream.d.ts:9:5 - error ts2425: class 'readablestream<t>' defines instance member property '[symbol.asynciterator]', but extended class 'iterablereadablestream<t>' defines it as instance member function.

9     [symbol.asynciterator](): this;
      ~~~~~~~~~~~~~~~~~~~~~~


found 2 errors in the same file, starting at: node_modules/@langchain/core/dist/utils/stream.d.ts:1


error: functions predeploy error: command terminated with non-zero exit code 2

any idea why this is happening? i am using langchain ""^0.1.18"". thanks in advance!","['node.js', 'typescript', 'google-cloud-functions', 'es6-modules', 'langchain']",78024278,"after a long ordeal, this finally worked for me. thank you ilya tkachou for the tip on upgrading to typescript 5. that partially solved the problem. another very important thing was to add
 ""esmoduleinterop"": true

to tsconfig.json. i also had to downgrade from node 20 to node 18.
sometimes you need to restart your ide for the changes to take effect.",https://stackoverflow.com/questions/78004857,node.js,16-02-2024 02:00,495.0,0.0,2.0,True,20-02-2024 01:11,16-02-2024 19:28
72095099,can we deduce the relationship b/w a dimension of a word vector with the linguistic characteristic it represents?,"let's imagine we generated a 200 dimension word vector using any pre-trained model of the word ('hello') as shown in the below image.

so, by any means can we tell which linguistic feature is represented by each d_i of this vector?
for example, d1 might be looking at whether the word is a noun; d2 might tell whether the word is a named entity or not and so on.","['nlp', 'stanford-nlp', 'word2vec', 'word-embedding']",72096117,"because these word vectors are dense distributional representations, it is often difficult / impossible to interpret individual neurons, and such models often do not localize interpretable features to a single neuron (though this is an active area of research). for example, see analyzing individual neurons in pre-trained language models
 for a discussion of this with respect to pre-trained language models).
a common method for studying how individual dimensions contribute to a particular phenomenon / task of interest is to train a linear model (i.e., logistic regression if the task is classification) to perform the task from fixed vectors, and then analyze the weights of the trained linear model.
for example, if you're interested in part of speech, you can train a linear model to map from the word vector to the pos [1]. then, the weights of the linear model represent a linear combination of the dimensions that are predictive of the feature. for example, if the weight on the 5th neuron has large magnitude (very positive or very negative), you might expect that neuron to be somewhat correlated with the phenomenon of interest.
[1]: note that defining a pos for a particular word is nontrivial, since the pos often depends on context. for example, ""play"" can be a noun (""he saw a play"") or a verb (""i will play in the grass"").",https://stackoverflow.com/questions/72095099,nlp,03-05-2022 05:25,54.0,0.0,1.0,True,04-05-2022 06:15,03-05-2022 08:36
71717955,how to go through each row with pandas apply() and lambda to clean sentence tokens?,"my goal is to created a cleaned column of the tokenized sentence within the existing dataframe.
the dataset is a pandas dataframe looking like this:




index
tokenized_sents




first
[donald, trump, just, couldn, t, wish, all, am]


second
[on, friday, ,, it, was, revealed, that]




dataset['cleaned_sents'] = dataset.apply(lambda row: [w for w in row[""tokenized_sents""] if len(w)>2 and w.lower() not in stop_words], axis = 1)

my current output is the dataframe without that extra column.
current outout:
    tokenized_sents  \
0  [donald, trump, just, couldn, t, wish, all, am...  

wanted output:
  tokenized_sents  \
0  [donald, trump, just, couldn, wish, all...   

basically removing all the stopwords & short words","['python', 'pandas', 'nlp', 'nltk']",71718173,"create a sentence index
dataset['gid'] = range(1, dataset.shape[0] + 1)

       tokenized_sents  gid
0  [this, is, a, test]    1
1    [and, this, too!]    2

then explode the dataframe
clean_df = dataset.explode('tokenized_sents')

  tokenized_sents  gid
0            this    1
0              is    1
0               a    1
0            test    1
1             and    2
1            this    2
1            too!    2

do all the cleaning on this dataframe and use gid column to group them back. it will be the fastest way to go about doing it.
clean_df = clean_df[clean_df.tokenized_sents.str.len() >= 2]
.
.
.

to get it back,
clean_dataset = clean_df.groupby('gid').agg(list)",https://stackoverflow.com/questions/71717955,python,02-04-2022 13:28,110.0,1.0,2.0,True,03-04-2022 02:56,02-04-2022 13:56
79482283,presidio with langchain experimental does not detect polish names,"i am using presidio/langchain_experimental to anonymize text in polish, but it does not detect names (e.g., ""jan kowalski""). here is my code:
from presidio_anonymizer import presidioanonymizer
from presidio_reversible_anonymizer import presidioreversibleanonymizer

config = {
    ""nlp_engine_name"": ""spacy"",
    ""models"": [{""lang_code"": ""pl"", ""model_name"": ""pl_core_news_lg""}],
}

anonymizer = presidioanonymizer(analyzed_fields=[""person"", ""phone_number"", ""email_address""],
                                languages_config=config)

anonymizer_tool = presidioreversibleanonymizer(analyzed_fields=[""person"", ""phone_number"", ""email_address""],
                                               languages_config=config)

text = ""jan kowalski mieszka w warszawie i ma e-mail jan.kowalski@example.com.""

anonymized_result = anonymizer_tool.anonymize(text)
anon_result = anonymizer.anonymize(text)
deanonymized_result = anonymizer_tool.deanonymize(anonymized_result)

print(""anonymized text:"", anonymized_result)
print(""deanonymized text:"", deanonymized_result)
print(""map:"", anonymizer_tool.deanonymizer_mapping)
print(""anonymized text:"", anon_result)

output:
anonymized text: jan kowalski mieszka w warszawie i ma e-mail jan.kowalski@example.com.
deanonymized text: jan kowalski mieszka w warszawie i ma e-mail jan.kowalski@example.com.
map: {}
anonymized text: jan kowalski mieszka w warszawie i ma e-mail jan.kowalski@example.com.

i expected the name ""jan kowalski"" and the email address to be anonymized, but the output remains unchanged.
i have installed the pl_core_news_lg model using:
python -m spacy download pl_core_news_lg

am i missing something in the configuration, or does presidio not support polish entity recognition properly?
any suggestions on how to make it detect names in polish?
the interesting thing is that when i use only
anonymizer_tool = presidioreversibleanonymizer()

then the output look like this:
anonymized text: elizabeth tate mieszka w warszawie i ma e-mail christinemurray@example.net. 
deanonymized text: jan kowalski mieszka w warszawie i ma e-mail jan.kowalski@example.com. 
map: {'person': {'elizabeth tate': 'jan kowalski'}, 'email_address': {'christinemurray@example.net': 'jan.kowalski@example.com'}}

as mentioned below if i use only spacy:
nlp = spacy.load(""pl_core_news_lg"")
doc = nlp(text)

then the output is correct so i guess that it's the problem with presidio itself. output from spacy:
jan kowalski persname
warszawie placename

so i would not like to create custom analyzer for that but use spacy in  presidio as it works as expected.","['python', 'nlp', 'spacy', 'langchain', 'presidio']",79495969,"after some test i was able to find the solution:
config = {
    ""nlp_engine_name"": ""spacy"",
    ""models"": [{""lang_code"": 'pl', ""model_name"": ""pl_core_news_lg""}],
}
spacy_recognizer = spacyrecognizer(
    supported_language=""pl"",
    supported_entities=[""persname""]
)
anonymizer.add_recognizer(spacy_recognizer)

anonymizer_tool = presidioreversibleanonymizer(analyzed_fields=[""person"", ""phone_number"", ""email_address"", ""credit_card""], languages_config=config)

the output look like this:
anonymized text: <persname> mieszka w warszawie i ma e-mail glenn58@example.org. 
deanonymized text: jan kowalski mieszka w warszawie i ma e-mail jan.kowalski@example.com. 
map: {'persname': {'<persname>': 'jan kowalski', '<persname_2>': 'jana kowalskiego'}, 'email_address': {'glenn58@example.org': 'jan.kowalski@example.com'}}
you need to directly add spacyrecognizer with supported_entities formatted according to spacy's requirements. i believe there's something missing or unclear in the documentation, which is causing the misunderstanding.",https://stackoverflow.com/questions/79482283,python,03-03-2025 22:27,244.0,4.0,2.0,True,15-03-2025 16:18,07-03-2025 06:53
76421591,openai api error when using a fine-tuned model: &quot;the model `xxxxx` does not exist&quot;,"i'm trying to do a chatbot with a fine tuned model.
i'm doing my request like this:
    const api_key = ""/"";
    const org_id = ""/"";
    const headers = {
      ""content-type"": ""application/json"",
      authorization: ""bearer "" + api_key,
      ""openai-organization"": org_id,
    };
    const res = await axios.post(
      ""
      {
        model: ""ft-modelname"",
        messages: [
          {
            role: ""system"",
            content: ""your name is name."",
          },
          {
            role: ""user"",
            content: message,
          },
        ],
      },
      { headers }
    );

and after listing my models with the api i can see it, it does exist.
but i have this error in the console:
{
    ""error"": {
        ""message"": ""the model `ft-modelname` does not exist"",
        ""type"": ""invalid_request_error"",
        ""param"": null,
        ""code"": null
    }
}

after looking up, i can't find a way to do this, some people use open ai engine api instead of completion, but it don't work too.
do you have any ideas ?
thank you.
edit:
it works when i'm not using a fine tune model","['javascript', 'axios', 'openai-api', 'fine-tuning']",76422136,"update: 22 august 2023
fine-tuning for gpt-3.5 is now available, as stated in the official openai blog:

fine-tuning for gpt-3.5 turbo is now available, with fine-tuning for
gpt-4 coming this fall.

consequently, the chat completions api (i.e., the gpt-3.5 api) endpoint can be used for a fine-tuned model, but only for a gpt-3.5 fine-tuned model. for a gpt-3 fine-tuned model, use the completions api (i.e., the gpt-3 api) endpoint.
as stated in the official openai documentation:

when a job has succeeded, you will see the fine_tuned_model field
populated with the name of the model when you retrieve the job
details. you may now specify this model as a parameter to in the chat
completions api (for gpt-3.5-turbo) or legacy completions api (for
babbage-002 and davinci-002), and make requests to it using the
playground.


problem
you're using the wrong api endpoint.
solution
use the completions api (i.e., the gpt-3 api) endpoint instead of the chat completions api (i.e., the gpt-3.5 api) endpoint.
as stated in the official openai documentation:

when a job has succeeded, the fine_tuned_model field will be populated
with the name of the model. you may now specify this model as a
parameter to our completions api, and make requests to it using the
playground.

in general
python
import os
import openai

openai.api_key = os.getenv(""openai_api_key"")

openai.completion.create(
    model = fine_tuned_model,
    prompt = your_prompt)

nodejs
const { configuration, openaiapi } = require(""openai"");

const configuration = new configuration({
  apikey: process.env.openai_api_key,
});

const openai = new openaiapi(configuration);

const response = await openai.createcompletion({
  model: fine_tuned_model
  prompt: your_prompt,
});

curl
curl  \
  -h ""authorization: bearer $openai_api_key"" \
  -h ""content-type: application/json"" \
  -d '{""prompt"": your_prompt, ""model"": fine_tuned_model}'

openai cli
openai api completions.create -m <fine_tuned_model> -p <your_prompt>

your case
try this:
const res = await axios.post(
  "" {
    model: ""ft-modelname"",
    prompt: ""say this is a test"",
  }, {
    headers
  }
);",https://stackoverflow.com/questions/76421591,javascript,07-06-2023 09:01,1067.0,0.0,1.0,True,14-09-2023 10:40,07-06-2023 15:09
74649908,use spacy with pandas,"i'm trying to build a multi-class text classifier using spacy and i have built the model, but facing a problem applying it to my full dataset. the model i have built so far is in the screenshot:
screenshot
below is the code i used to apply to my full dataset using pandas:

messages = pd.read_csv('messages.csv', encoding='cp1252')
    
messages['body'] = messages['body'].astype(str)

messages['nlp_result'] = nlp(messages['body'])._.cats

but it gives me the error:
valueerror: [e1041] expected a string, doc, or bytes as input, but got: <class 'pandas.core.series.series'>

the reason i wanted to use pandas in this case is the dataset has 2 columns: id and body. i want to apply the nlp model only to the body column, but i want the final dataset to have 3 columns: id, body and the nlp result like in the screenshot above.
thanks so much
i tried pandas apply method too, but had no luck. code used:
messages['nlp_result'] = messages['body'].apply(nlp)._.cats

the error i got: attributeerror: 'series' object has no attribute '_'
expectation is to generate 3 columns as described above","['python', 'pandas', 'spacy', 'text-classification']",74652865,"you should provide a callable into series.apply call:
messages['nlp_result'] = messages['body'].apply(lambda x: nlp(x)._.cats)

here, each value in the nlp_result column will be assigned to x variable.
the nlp(x) will create an nlp object that contains the necessary properties you'd like to access. then, the nlp(x)._.cats will return the expected value.
import spacy
import classy classification
import csv
import pandas as pd 

with open ('deliveries.txt', 'r') as d:
    deliveries = d.read().splitlines()
with open (""not spam.txt"", ""r"") as n:
    not_spam = n.read().splitlines()

data = {}
data[""deliveries""] = deliveries
data[""not_spam""] = not_spam

# nlp model
nlp = spacy.blank(""en"")
nlp.add pipe(""text_categorizer"",
    config={
        ""data"": data,
        ""model"": ""sentence-transformers/paraphrase-multilingual-minilm-l12-v2"",
        ""device"": ""gpu""
    }
)

messages['nlp_result'] = messages['body'].apply(lambda x: nlp(x)._.cats)",https://stackoverflow.com/questions/74649908,python,02-12-2022 01:19,723.0,2.0,1.0,True,02-12-2022 08:55,02-12-2022 04:14
74161769,python too slow to find text in string in for loop,"i want to improve the loop performance where it counts word occurrences in text, but it runs around 5 minutes for 5 records now
dataframe
no                  text   
1     i love you forever...*500 other words
2     no , i know that you know xxx *100 words

my word list
wordlist =['i','love','david','mary',......]

my code to count word
for i in wordlist :
    df[i] = df['text'].str.count(i)

result :
no   text                  i    love  other_words
 1    i love you ...       1      1      4
 2    no, i know ...       1      0      5","['python', 'python-3.x', 'performance', 'for-loop', 'nlp']",74162147,"you can do this by making a counter from the words in each text value, then converting that into columns (using pd.series), summing the columns that don't exist in wordlist into other_words and then dropping those columns:
import re
import pandas as pd
from collections import counter

wordlist = list(map(str.lower, wordlist))
counters = df['text'].apply(lambda t:counter(re.findall(r'\b[a-z]+\b', t.lower())))
df = pd.concat([df, counters.apply(pd.series).fillna(0).astype(int)], axis=1)
other_words = list(set(df.columns) - set(wordlist) - { 'no', 'text' })
df['other_words'] = df[other_words].sum(axis=1) 
df = df.drop(other_words, axis=1)

output (for the sample data in your question):
   no                                 text  i  love  other_words
0   1    i love you forever... other words  1     1            4
1   2  no , i know that you know xxx words  1     0            7

note:

i've converted all the words to lower-case so you're not counting i and i separately.
i've used re.findall rather than the more obvious split() so that forever... gets counted as the word forever rather than forever...

if you only want to count the words in wordlist (and don't want an other_words count), you can simplify this to:
wordlist = list(map(str.lower, wordlist))
counters = df['text'].apply(lambda t:counter(w for w in re.findall(r'\b[a-z]+\b', t.lower()) if w in wordlist))
df = pd.concat([df, counters.apply(pd.series).fillna(0).astype(int)], axis=1)

output:
   no                                 text  i  love
0   1    i love you forever... other words  1     1
1   2  no , i know that you know xxx words  1     0

another way of also generating the other_words value is to generate 2 sets of counters, one of all the words, and one only of the words in wordlist. these can then be subtracted from each other to find the count of words in the text which are not in the wordlist:
wordlist = list(map(str.lower, wordlist))
counters = df['text'].apply(lambda t:counter(w for w in re.findall(r'\b[a-z]+\b', t.lower()) if w in wordlist))
df = pd.concat([df, counters.apply(pd.series).fillna(0).astype(int)], axis=1)
c2 = df['text'].apply(lambda t:counter(re.findall(r'\b[a-z]+\b', t.lower())))
df['other_words'] = (c2 - counters).apply(lambda d:sum(d.values()))

output of this is the same as for the first code sample. note that in python 3.10 and later, you should be able to use the new total function:
(c2 - counters).apply(counter.total)",https://stackoverflow.com/questions/74161769,python,22-10-2022 07:22,184.0,3.0,2.0,True,25-01-2023 14:20,25-01-2023 14:20
75640144,openai converting api code from gpt-3 to chatgpt-3.5,"below is my working code for the gpt-3 api. i am having trouble converting it to work with chatgpt-3.5.
<?php include('../config/config.php'); ?>
<!doctype html>
<html lang=""en"">
<head>
<meta charset=""utf-8"">
<meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
<meta  content=""ie=edge"">
<title>chatbot</title>
<link rel=""stylesheet"" href=""
<link href="" rel=""stylesheet"" integrity=""sha384-glhltq8irabdzll6o3ovmwsktqop6b7in1zl3/jr59b6eggoi1afkw7cmda6j6gd"" crossorigin=""anonymous"">
<link href=""style.css"" rel=""stylesheet"">
</head>
<body>
<div class=""container py-5"">
  <h1 class=""mb-5 text-center"">
    <div class=""logo"">  </div>
  </h1>
  <div class=""form-floating mb-3"">
    <select class=""form-select"" id=""tab-select"" aria-label=""select your purpose"">
      <option value=""exam"" selected>exam</option>
      <option value=""feedback"">feedback</option>
    </select>
    <label for=""tab-select"">select your purpose:</label>
  </div>
  <div class=""input-group mb-3"">
    <div class=""form-floating"">
      <textarea class=""form-control"" placeholder=""enter your question or comment here"" id=""prompt""></textarea>
      <label for=""prompt"">enter your question or comment here</label>
    </div>
    <div class=""input-group-append username w-100 mt-3 mb-4"">
      <button class=""btn btn-outline-primary w-100"" type=""button"" id=""send-button"">send</button>
    </div>
  </div>
  <div id=""output"" class=""mb-3"" style=""height: 300px; overflow: auto; border: 1px solid lightgray; padding: 10px;""></div>
  <div id=""exam-instructions"" class=""mb-3"" style=""display: block;"">
    <h3>exam</h3>
    <p>pocketai can create multiple choice and true false questions in a format that enables import into brightspace d2l quizzes using respondus. place pocketai output into a word document before importing with respondus. ask pocketai questions like the following: <br>
      <br>
      create 3 multiple choice questions about carbohydrates for a freshman nutrition online college course.<br>
      create 2 true false questions about business for a sophomore business face to face college course.</p>
  </div>
  <div id=""feedback-instructions"" class=""mb-3"" style=""display: none;"">
    <h3>feedback</h3>
    <p>enter text to receive writing feedback.</p>
  </div>
</div>
<script>
const previousprompts = [];
const username = ""<strong>user</strong>"";
const chatbotname = ""<strong>pocketai</strong>"";

const selectdropdown = document.getelementbyid(""tab-select"");

selectdropdown.addeventlistener(""change"", function() {
  const activetabid = this.value;
  
  // hide all instruction sections
  document.queryselectorall(""[id$='-instructions']"").foreach(function(instructionsection) {
    instructionsection.style.display = ""none"";
  });
  
  // show the instruction section for the active tab
  document.getelementbyid(`${activetabid}-instructions`).style.display = ""block"";
});

document.getelementbyid(""send-button"").addeventlistener(""click"", function() {
  const prompt = document.getelementbyid(""prompt"").value;
  const activetabid = selectdropdown.value;

  const endpoint = ""
  const apikey = ""<?=$open_ai_key;?>"";

  document.getelementbyid(""send-button"").innerhtml = '<span class=""spinner-border spinner-border-sm"" role=""status"" aria-hidden=""true""></span> sending...';

  let prompttext = """";
  
  switch (activetabid) {
    case ""exam"":
        prompttext = ""create quiz questions in the following format: begin each question with a number followed by a period, and then include the question wording. for each question, include four answer choices listed as letters (a, b, c, d) followed by a period and at least one space before the answer wording. designate the correct answer by placing an asterisk (*) directly in front of the answer letter (do not put a space between the asterisk and the answer choice). place the asterisk in front of the answer letter, only the front. it is important that correct answers are identified. don't make up answers, only select factual answers. for example formatting (don't use this specific example), \""1. what is the recommended daily intake of dietary fiber? a. 10 grams b. 25 grams *c. 50 grams d. 75 grams\"". format true false questions the same way. if you are unsure of the correct answer, don't create the question. every quiz question and answer must be 100% correct and factual. do not make up answers. all answers must be correct."";
      break;
     case ""feedback"":
      prompttext = ""can you provide feedback on the writing, grammar, sentence structure, punctuation, and style of this student's paper? the paper should be analyzed for its strengths and weaknesses in terms of written communication. please provide suggestions for improvement and examples to help the student understand how to make the writing better. the feedback should be specific and provide actionable steps that the student can take to improve their writing skills. please include at least three examples of areas that could be improved and specific suggestions for how to improve them, such as correcting grammar errors, restructuring sentences, or improving the use of punctuation."";
      break;
  }
  
  const requestdata = {
    prompt: previousprompts.join(""\n"") + prompttext + ""\n"" + prompt,
    max_tokens: 400,
      model: ""text-davinci-003"",
    n: 1,
    stop: """",
    temperature: 0.5,
      top_p: 0.0,
      frequency_penalty: 0.0,
      presence_penalty: 0
  };
        
  const requestoptions = {
    method: ""post"",
    headers: {
      ""content-type"": ""application/json"",
      ""authorization"": `bearer ${apikey}`,
    },
    body: json.stringify(requestdata),
  };
  
  fetch(endpoint, requestoptions)
    .then(response => response.json())
    .then(data => {
      const reply = data.choices[0].text;
      
      // add the user message to the chat history
      const usermessage = `<div class=""message-container"">
        <div class=""username"">${username}:&nbsp;</div>
        <div class=""user-message"">${prompt}</div>
      </div>`;
      document.getelementbyid(""output"").innerhtml += usermessage;
      
      const chatbotmessage = `<div class=""message-container"">
  <div class=""username"">${chatbotname}:&nbsp;</div>
  <div class=""chatbot-message"" style=""white-space: pre-wrap"">${reply}<i class=""bi bi-clipboard-check copy-button"" data-bs-toggle=""tooltip"" data-bs-placement=""bottom"" title=""copy to clipboard"" data-text=""${reply}"" style=""cursor: pointer;""></i></div>
</div>`; 
document.getelementbyid(""output"").innerhtml += chatbotmessage;

// add an event listener to each ""copy to clipboard"" button
document.addeventlistener(""click"", function(event) {
  if (event.target.classlist.contains(""copy-button"")) {
    const texttocopy = event.target.dataset.text;
    navigator.clipboard.writetext(texttocopy);
  }
});
     // scroll to the bottom of the chat history
      document.getelementbyid(""output"").scrolltop = document.getelementbyid(""output"").scrollheight;
    
      // clear the user input field
      document.getelementbyid(""prompt"").value = """";
    
      previousprompts.push(prompt);
      // clear the spinner and show the ""send"" button again
      document.getelementbyid(""send-button"").innerhtml = 'send';
    })
    .catch(error => {
      console.error(error);
    
      // hide the spinner and show the ""send"" button again
      document.getelementbyid(""send-button"").innerhtml = 'send';
    });
});

document.getelementbyid(""prompt"").addeventlistener(""keydown"", function(event) {
  if (event.keycode === 13) {
    event.preventdefault();
    document.getelementbyid(""send-button"").
click();
  }
});
</script>
</div>
</div>
<script src="" integrity=""sha384-w76aqpfdkmbdxo30js1sgez6pr3x5mlq1zagc+nuzb+eydgrzgiwxhtbtkf7cxvn"" crossorigin=""anonymous""></script>
</body>
</html>

i have read  and referred to this - openai chatgpt (gpt-3.5-turbo) api: how to access the message content? but still can't make it work.
i've tried changing the requestdata to this, but no luck:
const requestdata = {
    model: ""gpt-3.5-turbo"",
    messages: [
      { role: ""user"", content: prompt }
    ],
    max_tokens: 400,
    temperature: 0.5,
    top_p: 1,
    frequency_penalty: 0,
    presence_penalty: 0
  };

any help will be greatly appreciated!","['php', 'openai-api', 'gpt-3', 'chatgpt-api']",75739103,"better check your requestdata object, the gpt 3.5 turbo doesn't need these props

max_tokens,temperature,top_p: 1,frequency_penalty,presence_penalty

i made the same mistake too, gpt 3.5 turbo is wayyyyy easier to use than i expected. here's openai sample:
const { configuration, openaiapi } = require(""openai"");

const configuration = new configuration({
  apikey: process.env.openai_api_key,
});
const openai = new openaiapi(configuration);

const completion = await openai.createchatcompletion({
  model: ""gpt-3.5-turbo"",
  messages: [{role: ""user"", content: ""hello world""}],
});
console.log(completion.data.choices[0].message);",https://stackoverflow.com/questions/75640144,php,05-03-2023 04:10,1665.0,-3.0,1.0,True,14-03-2023 22:56,06-03-2023 01:56
71999817,nltk is installed but nltk_utils returns modulenotfounderror,"i am using virtual env, i have installed nltk module with pip3, when i am trying to import nltk_utils i am getting a modulenotfounderror
>>> import nltk
>>> import nltk_utils
traceback (most recent call last):
 file ""<stdin>"", line 1, in <module>
modulenotfounderror: no module named 'nltk_utils'

i have tried without virtualenv too but no luck
os : ubuntu
python version : 3.9.5
gcc : 10.3.0","['python', 'pip', 'nltk', 'modulenotfounderror']",72001540,"nltk_utils is nothing that comes shipped with nltk. did you mean nltk.util, which is described here?
otherwise nltk_utils is used in some examples using nltk where it is a custom file that contains useful functions in interacting with nltk (e.g. in this chatbot example) so if you are following some tutorial or similar, check if they mention somewhere what nltk_utils should contain",https://stackoverflow.com/questions/71999817,python,25-04-2022 12:49,1396.0,-1.0,3.0,True,24-03-2023 10:37,25-04-2022 14:56
74987587,"r quanteda filtering, counting and grouping features from a customized dictionary","i have the following data set:
library(quanteda)
library(quanteda.textstats)

df_test<-c(""i find water to be so healthy and refreshing"",
           ""nothing like a freshly made burguer to make me feel good"",
           ""i dislike sugar in the morning it tastes horrible"",
           ""a nice burguer is always crispy and spicy"",
           ""it is beyond me to dare to drink soda it's just gross too much sugar"",
           ""yes i will have a hot burguer anytime is so cheap and tasty"")

i want to be able to built a customized dictionary so that i can
classify words/tokens into two categories ""negative"" and ""positive""
after that i want to filter by the most frequent words/tokens and plot
the positive and negative words associated with them
this is my dictionary
dict_custom <- dictionary(list(positive = c(""healthy"", ""refreshing"", ""good"", ""crispy"", 
                                      ""spicy"", ""cheap"", ""tasty""),
                               negative=c(""horrible"",""gross"")))

what are some of the most frequent tokens?
> tok_df<-corpus(df_test) %>% tokens(remove_punct=true) %>% tokens_remove(stopwords(""en""))
> 
> tok_df %>% dfm() %>% 
+   textstat_frequency(5)  
  feature frequency rank docfreq group
1 burguer         3    1       3   all
2   sugar         2    2       2   all
3    find         1    3       1   all
4   water         1    3       1   all
5 healthy         1    3       1   all

i want to choose burger and get all the positive and negative words (after using my dictionary) and count the number of times they appear also perhaps create a word_cloud
i'm using this code:
> tokens_lookup(tok_df,dictionary = dict_custom) %>% 
+   dfm()
document-feature matrix of: 6 documents, 2 features (50.00% sparse) and 0 docvars.
       features
docs    positive negative
  text1        2        0
  text2        1        0
  text3        0        1
  text4        2        0
  text5        0        1
  text6        2        0

but instead of words i get the count of positive and negative tokens per document.
my desired output will contain a matrix/dfm like object filter by burger with all of the negative and positive tokens (crispy, healthy, gross, ect) instead of the count of neg and pos tokens by document (that i do not want).
by the way, what if i want to instead of creating a neg and positive words, rather assign a numeric value lets say gross=-5 and crispy=5 how can i join and merge my tokens with this kind of dictionary so that i afterwards can summarize the numeric output?","['r', 'text-mining', 'quanteda']",75006355,"the best way to do this is using the ability of tokens_select() to filter on dictionaries. by indexing each key separately - below, using lapply - then you can create a list of dfm objects whose features are the value matches for each key.
library(""quanteda"")
#> package version: 3.2.4
#> unicode version: 14.0
#> icu version: 70.1
#> parallel computing: 10 of 10 threads used.
#> see  for tutorials and examples.
library(""quanteda.textstats"")

df_test <- c(""i find water to be so healthy and refreshing"",
             ""nothing like a freshly made burguer to make me feel good"",
             ""i dislike sugar in the morning it tastes horrible"",
             ""a nice burguer is always crispy and spicy"",
             ""it is beyond me to dare to drink soda it's just gross too much sugar"",
             ""yes i will have a hot burguer anytime is so cheap and tasty"")

dict_custom <- dictionary(list(positive = c(""healthy"", ""refreshing"", ""good"", ""crispy"", 
                                            ""spicy"", ""cheap"", ""tasty""),
                               negative = c(""horrible"",""gross"")))

toks <- tokens(df_test)

dfm_list <- lapply(
    names(dict_custom), 
    function(x) {
        tokens_select(toks, dict_custom[x]) |>
            dfm()
    }
)
names(dfm_list) <- names(dict_custom)

now you have a list of dfm objects, named by your dictionary keys, which you can then get frequencies for, or wordclouds, etc.
dfm_list
#> $positive
#> document-feature matrix of: 6 documents, 7 features (83.33% sparse) and 0 docvars.
#>        features
#> docs    healthy refreshing good crispy spicy cheap tasty
#>   text1       1          1    0      0     0     0     0
#>   text2       0          0    1      0     0     0     0
#>   text3       0          0    0      0     0     0     0
#>   text4       0          0    0      1     1     0     0
#>   text5       0          0    0      0     0     0     0
#>   text6       0          0    0      0     0     1     1
#> 
#> $negative
#> document-feature matrix of: 6 documents, 2 features (83.33% sparse) and 0 docvars.
#>        features
#> docs    horrible gross
#>   text1        0     0
#>   text2        0     0
#>   text3        1     0
#>   text4        0     0
#>   text5        0     1
#>   text6        0     0

frequencies:
lapply(dfm_list, textstat_frequency)
#> $positive
#>      feature frequency rank docfreq group
#> 1    healthy         1    1       1   all
#> 2 refreshing         1    1       1   all
#> 3       good         1    1       1   all
#> 4     crispy         1    1       1   all
#> 5      spicy         1    1       1   all
#> 6      cheap         1    1       1   all
#> 7      tasty         1    1       1   all
#> 
#> $negative
#>    feature frequency rank docfreq group
#> 1 horrible         1    1       1   all
#> 2    gross         1    1       1   all

created on 2023-01-04 with reprex v2.0.2",https://stackoverflow.com/questions/74987587,r,02-01-2023 22:31,268.0,1.0,1.0,True,22-01-2023 11:29,22-01-2023 11:29
46064162,extracting dates that are in different formats using regex and sorting them - pandas,"i am new to text mining and i need to extract the dates from a *.txt file and sort them. the dates are in between the sentences ( each line) and their format can potentially be as follows: 
04/20/2009; 04/20/09; 4/20/09; 4/3/09
mar-20-2009; mar 20, 2009; march 20, 2009; mar. 20, 2009; mar 20 2009;
20 mar 2009; 20 march 2009; 20 mar. 2009; 20 march, 2009
mar 20th, 2009; mar 21st, 2009; mar 22nd, 2009
feb 2009; sep 2009; oct 2010
6/2008; 12/2009
2009; 2010

if the day is missing consider the 1st and if the month is missing consider january. 
my idea is to extract all dates and convert that into mm/dd/yyyy format. however i am a bit doubtful on how to find and replace paterns. this is what i have done :
import pandas as pd

doc = []
with open('dates.txt') as file:
    for line in file:
        doc.append(line)

df = pd.series(doc)

df2 = pd.dataframe(df,columns=['text'])

def myfunc(x):
    if len(x)==4:
        x = '01/01/'+x
    else:
        if not re.search('/',x):
            example = re.sub('[-]','/',x)
            terms = re.split('/',x)
            if (len(terms)==2):
                if len(terms[-1])==2:
                    x = '01/'+terms[0]+'/19'+terms[-1]
                else:
                    x = '01/'+terms[0]+'/'+terms[-1] 
            elif len(terms[-1])==2:
                x = terms[0].zfill(2)+'/'+terms[1].zfill(2)+'/19'+terms[-1]
    return x

df2['text'] = df2.text.str.replace(r'(((?:\d+[/-])?\d+[/-]\d+)|\d{4})', lambda x: myfunc(x.groups('date')[0]))

i have done it only for the numerical dates format. but i am a bit confused how to do it with the alfanumerical dates.
i know is a rough code but this is just what i got.","['python', 'pandas', 'date', 'dataframe', 'text-mining']",46069885,"i think this is one of the coursera text mining assignment. well you can use regex and extract to get the solution. dates.txt i.e 
doc = []
with open('dates.txt') as file:
    for line in file:
        doc.append(line)

df = pd.series(doc)

def date_sorter():
    # get the dates in the form of words
    one = df.str.extract(r'((?:\d{,2}\s)?(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*(?:-|\.|\s|,)\s?\d{,2}[a-z]*(?:-|,|\s)?\s?\d{2,4})')
    # get the dates in the form of numbers
    two = df.str.extract(r'((?:\d{1,2})(?:(?:\/|-)\d{1,2})(?:(?:\/|-)\d{2,4}))')
    # get the dates where there is no days i.e only month and year  
    three = df.str.extract(r'((?:\d{1,2}(?:-|\/))?\d{4})')
    #convert the dates to datatime and by filling the nans in two and three. replace month name because of spelling mistake in the text file.
    dates = pd.to_datetime(one.fillna(two).fillna(three).replace('decemeber','december',regex=true).replace('janaury','january',regex=true))
return pd.series(dates.sort_values())

date_sorter()

output:

9     1971-04-10
84    1971-05-18
2     1971-07-08
53    1971-07-11
28    1971-09-12
474   1972-01-01
153   1972-01-13
13    1972-01-26
129   1972-05-06
98    1972-05-13
111   1972-06-10
225   1972-06-15
31    1972-07-20
171   1972-10-04
191   1972-11-30
486   1973-01-01
335   1973-02-01
415   1973-02-01
36    1973-02-14
405   1973-03-01
323   1973-03-01
422   1973-04-01
375   1973-06-01
380   1973-07-01
345   1973-10-01
57    1973-12-01
481   1974-01-01
436   1974-02-01
104   1974-02-24
299   1974-03-01

if you want to return only the index then return pd.series(dates.sort_values().index)
parsing of first regex 

 #?: non-capturing group 

((?:\d{,2}\s)? # the two digits group. `?` refers to preceding token or group. here the digits of 2 or 1 and space occurring once or less.  

 (?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]* # the words in group ending with any letters `[]` occuring any number of times (`*`). 

 (?:-|\.|\s|,) # pattern matching -,.,space 

 \s? #(`?` here it implies only to space i.e the preceding token)

 \d{,2}[a-z]* # less than or equal to two digits having any number of letters at the end (`*`). (eg: may be 1st, 13th , 22nd , jan , december etc ) . 

 (?:-|,|\s)?# the characters -/,/space may occur once and may not occur because of `?` at the end

 \s? # space may occur or may not occur at all (maximum is 1) (`?` here it refers only to space)

 \d{2,4}) # match digit which is 2 or 4   

hope it helps.",https://stackoverflow.com/questions/46064162,python,05-09-2017 22:26,7713.0,2.0,1.0,True,12-10-2022 22:35,06-09-2017 08:40
78379953,accessing langchain lcel variables from prior steps in the chain,"i'm trying to hop onto the lcel and langserve train but i'm having a little trouble understanding a bit of the ""magic"" involved with accessing variables set in the pipeline's dictionary.
the variables appear to be resolvable from prompt templates. i'd like to retrieve these values in custom functions, etc. but it's not clear to me how to access them directly. take the following contrived example which aims to return the source document as well as the answer in the response:
class chatresponse(basemodel):
    answer: str
    sources: list[document]

store = faiss.from_texts(
    [""harrison worked at kensho""], embedding=openaiembeddings()
)
retriever = store.as_retriever()
template = """"""answer the question based only on the following context:
{context}

question: {question}
""""""
prompt = chatprompttemplate.from_template(template)
llm = chatopenai()

def format_response(answer):
    sources = [] # todo lookup source documents (key: 'context')
    return chatresponse(answer=answer, sources=sources)

retrieval_chain = (
        {""context"": retriever, ""question"": runnablepassthrough()}
        | prompt
        | llm
        | stroutputparser()
        | runnablelambda(format_response)
)
app = fastapi()
add_routes(app, retrieval_chain, path=""/chat"", input_type=str, output_type=chatresponse)

in format_response, i've left a todo to lookup the source documents. i'd like to retrieve the source documents from the pipeline's context key. how would i access this key that was set from the first step of the chain?",['langchain'],78385029,"from documentation at 
from langchain_core.runnables import runnableparallel

def format_docs(docs):
    return ""\n\n"".join(doc.page_content for doc in docs)

rag_chain_from_docs = (
    runnablepassthrough.assign(context=(lambda x: format_docs(x[""context""])))
    | prompt
    | llm
    | stroutputparser()
)

rag_chain_with_source = runnableparallel(
    {""context"": retriever, ""question"": runnablepassthrough()}
).assign(answer=rag_chain_from_docs)

then this:
rag_chain_with_source.invoke(""where did harrison work ?"")

returns:
{'context': [document(page_content='harrison worked at kensho')],
 'question': 'where did harrison work ?',
 'answer': 'harrison worked at kensho.'}",https://stackoverflow.com/questions/78379953,langchain,24-04-2024 16:27,702.0,0.0,1.0,True,25-04-2024 13:34,24-04-2024 16:45
76137987,openai completion stream with node.js and express.js,"i'm trying to build a chatgpt website clone and now i need to make the stream completion effect that shows the result word-per-word.
my server is a typescript node.js app that uses the express.js framework.
here's the route:
import express, { request, response } from 'express';
import cors from 'cors';
import { configuration, openaiapi } from 'openai';

// ...

app.post('/api/admin/teststream', async (req: request, res: response) => {
    const { password } = req.body;

    try {
        if (password !== process.env.admin_password) {
            res.send({ message: 'incorrect password' });
            return;
        }
        const completion = await openai.createcompletion({
            model: 'text-davinci-003',
            prompt: 'say this is a test',
            stream: true,
        }, { responsetype: 'stream' });

        completion.data.on('data', (chunk: any) => {
            console.log(chunk.tostring());
        });

        res.send({ message: 'stream started' });
    } catch (err) {
        console.log(err);
        res.send(err);
    }
});

// ...

right now, it gives me an error saying

property 'on' does not exist on type 'createcompletionresponse'.ts(2339)

even if i set the { responsetype: 'stream' }.
how can i solve this problem and send the response chunk-per-chunk to the frontend? (i'm using socket.io.)","['node.js', 'express', 'sockets', 'events', 'openai-api']",76143463,"finally solved it thanks to the help of @uzluisf !
here's what i did:
import express, { request, response } from 'express';
import cors from 'cors';
import { configuration, openaiapi } from 'openai';
import  { incomingmessage } from '

// ...

app.post('/api/admin/teststream', async (req: request, res: response) => {
    const { password } = req.body;

    try {
        if (password !== process.env.admin_password) {
            res.send({ message: 'incorrect password' });
            return;
        }

        const completion = await openai.createchatcompletion({
            model: 'gpt-3.5-turbo',
            messages: [{ role: 'user', content: 'when was america founded?' }],
            stream: true,
        }, { responsetype: 'stream' });

        const stream = completion.data as unknown as incomingmessage;

        stream.on('data', (chunk: buffer) => {
            const payloads = chunk.tostring().split(""\n\n"");
            for (const payload of payloads) {
                if (payload.includes('[done]')) return;
                if (payload.startswith(""data:"")) {
                    const data = json.parse(payload.replace(""data: "", """"));
                    try {
                        const chunk: undefined | string = data.choices[0].delta?.content;
                        if (chunk) {
                            console.log(chunk);
                        }
                    } catch (error) {
                        console.log(`error with json.parse and ${payload}.\n${error}`);
                    }
                }
            }
        });

        stream.on('end', () => {
            settimeout(() => {
                console.log('\nstream done');
                res.send({ message: 'stream done' });
            }, 10);
        });

        stream.on('error', (err: error) => {
            console.log(err);
            res.send(err);
        });
    } catch (err) {
        console.log(err);
        res.send(err);
    }
});

// ...

for more info, visit 
also managed to send chunks of message using socket.io events!

links to example code:

front-end
back-end",https://stackoverflow.com/questions/76137987,node.js,29-04-2023 19:15,16888.0,10.0,2.0,True,15-05-2024 18:28,30-04-2023 20:56
33779230,how to get wordforms in sphinx?,"how i can get all morphology forms of the word?
for example, searching keyword is:
runner
result should be:
run,running ... etc","['nlp', 'sphinx', 'morphological-analysis']",33789699,"you usually don't need one. use a stemmer, which can do the reverse. ie it removes the ""ending"", so that matching works, rather than trying to figure out all the possible endings. 

ie use morphology, rather than worforms.",https://stackoverflow.com/questions/33779230,nlp,18-11-2015 11:46,120.0,0.0,1.0,True,09-10-2024 21:22,09-10-2024 21:22
78269557,vectorstoreindex api key while using azureopenai service,"i'm trying to use vectorstoreindex from llama_index solving the rag problem for chatbot just the following way:
import openai
from llama_index import vectorstoreindex

index = vectorstoreindex.from_documents(docs)
index.storage_context.persist()

when i read documentation, it is recommended to use openai.api_key = os.getenv('openai_api_key') to be able to connect model with openai. but what should i do if i use azureopenai (meaning, that i have api_key, azure_endpoint and api_version and don't have openai api_key)? if i use openai.api_key = my_azure_api_key this also does not work because apparently the model refers to  and not to azureai services..
sorry if the question looks dubious, was unable to find the information on web.
thanks!","['azure', 'openai-api', 'azure-openai', 'llama', 'llama-index']",78269663,"have solved it. actually all you need is to add context parameter into vectorstoreindex.from_documents(). in this parameter one can specify azureopenai or azureopenaiembedding and obviously use api_key, azure_endpoint and api_version.
index = vectorstoreindex.from_documents(documents, service_context=service_context)",https://stackoverflow.com/questions/78269557,azure,03-04-2024 17:47,675.0,-1.0,1.0,True,03-04-2024 18:05,03-04-2024 17:52
22861795,nltk download [error 11004] getadderinfo failed,"this is the error when i try to restart the download again
some times it work but since the new win7 i could not download any data from nltk 
 nltk.download()

showing info 
exception in tkinter callback
traceback (most recent call last):
file ""c:\python27\lib\lib-tk\tkinter.py"", line 1410, in __call__
 return self.func(*args)
file ""c:\python27\lib\lib-tk\tkinter.py"", line 495, in callit
 func(*args)
file ""c:\python27\lib\site-packages\nltk\downloader.py"", line 1893, in_monitor_message_queue
self._select(msg.package.id)
attributeerror: 'str' object has no attribute 'id'","['python', 'nltk']",27765280,"i was facing the same issue. the issue in my case was that when the nltk downloader started it had the server index as - 
this needs to be changed to - 
you can change this by going into the nltk downloader window and the file->change server index.",https://stackoverflow.com/questions/22861795,python,04-04-2014 11:38,4049.0,4.0,2.0,True,04-08-2023 14:03,04-01-2015 13:23
72322545,spacy cust entity training returns nothing,"i have descriptions from which i want to extract colours. hence i thought i would use ner by spacy.
i have data like this for 8000 lines
import spacy
nlp=spacy.load('en_core_web_sm')

# getting the pipeline component
ner=nlp.get_pipe(""ner"")

train_data = 
[
(""bermuda shorts anthracite/black"",{""entities"" : [(15,31,""col"")]}),
(""acrylic antique white"",{""entities"" : [(8,22,""col"")]}),
(""pincer black"",{""entities"" : [(8,13,""col"")]}),
(""cable tie black"",{""entities"" : [(10,15,""col"")]}),
(""water pump pliers blue"",{""entities"" : [(18,22,""col"")]})
]

my code is
for _, annotations in train_data:
    for ent in annotations.get(""entities""):
        ner.add_label(ent[2])

pipe_exceptions = [""ner"", ""trf_wordpiecer"", ""trf_tok2vec""]
unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]


from spacy.training.example import example

for batch in spacy.util.minibatch(train_data, size=2):
    for text, annotations in batch:
        # create example
        doc = nlp.make_doc(text)
        example = example.from_dict(doc, annotations)
        # update the model
        nlp.update([example], losses=losses, drop=0.3)

when i test the model i get nothing.

doc = nlp(""bill gates has a anthracite house worth 10 eur."")
print(""entities"", [(ent.text, ent.label_) for ent in doc.ents])

why am i doing wrong?
please help...","['python', 'python-3.x', 'spacy', 'spacy-3']",72342981,"there are several problems with your code.
where did you save your model?
there is nothing in your code to indicate you saved and reloaded your model. when you train a model like that, you aren't modifying the existing model on disk. if you don't save the model after training it's just gone, which would mean you get no color annotations.
your input doesn't look like your training data!
your input is a complete sentence, but your training data is isolated phrases. this will result in poor performance, as the model isn't sure what to do with colors and, say, verbs. (you would probably still get some annotations though.)

i strongly suggest you go through the spacy course, which covers training your own ner model. i also strongly recommend you use the v3 config-based training instead of writing your own training loop.",https://stackoverflow.com/questions/72322545,python,20-05-2022 17:11,496.0,1.0,2.0,True,23-05-2022 03:28,22-05-2022 19:26
77506707,chatbot with gpt3 in angular,"i'm making my own chat application in angular with chatgpt3. when i tried the api url in the documentation, i got error 429. what do you think could be the reason?
service.ts
export class openaiservice {

  private apikey = 'sk-xxxx';
  private apiurl = '

  constructor(private   { }

  teachgpt(usermessage: string): observable<string> {
    const headers = new 
      'content-type': 'application/json',
      'authorization': `bearer ${this.apikey}`
    });

    const requestdata = {
      model: 'gpt-3.5-turbo',
      messages: [
        { role: 'system', content: 'you are a helpful assistant.' },
        { role: 'user', content: usermessage }
      ]
    };

    return this. requestdata, { headers })
      .pipe(
        map(response => response.choices[0].message.content),
        catcherror(error => {
          console.error('an error occurred:', error);
          return of('');
        })
      );
  }
}

chat.ts
export class chatpagecomponent {

  usermessage: string = '';
  chatgptresponse: string | undefined;

  constructor(private openaiservice: openaiservice) { }

  onsubmit() {
    this.openaiservice.teachgpt(this.usermessage)
      .subscribe(
        response => {
          this.chatgptresponse = response;
          console.log('chatgpt cevabï¿½ï¿½:', response);
        },
        error => {
          console.error('bir hata oluï¿½ï¿½tu:', error);
        }
      );
  }
}

console error:
"" rel=""nofollow noreferrer"">
when i use the url  which is the url in the gpt 3.5 turbo documentation, i get error 429. if the url suggested by gpt was  i got a 400 bad request error.","['angular', 'typescript', 'chatbot', 'openai-api', 'gpt-3']",77514035,"i found the answer to the last one. gpt3 was giving credit to new members for the first 3 months for api trials. when i opened a new account and tried it, the problem was solved.",https://stackoverflow.com/questions/77506707,angular,18-11-2023 11:34,251.0,-2.0,1.0,True,20-11-2023 07:05,19-11-2023 12:08
77491934,is this right way to merge lora weights?,"i have fine-tuned roberta using lora with huggingface libraries, produced multiple lora files.
i want to merge those lora weights without changing original model. so i wrote code like below.
from peft import (
    ...
    peftmodel,
    ...
)

from transformers import (
    ...
    robertaforsequenceclassification
    ...
)


model = robertaforsequenceclassification.from_pretrained(""roberta-base"")

# ""lora-1"" and ""lora-2"" are local directories.
model = peftmodel.from_pretrained(model, ""lora-1"")
model = model.merge_and_unload()

model = peftmodel.from_pretrained(model, ""lora-2"")
model = model.merge_and_unload()

code is working but bit suspicious because i don't know how merge_and_unload() working exactly. guessing by name, i thought perhaps it merge all the lora weights to base model's weight and make it one final single model. but that's not how lora working, as far as i know.
to conclude, i want to produce p(phi_0 + delta phi_1(theta_1) + delta phi_2(theta_2) + ... + delta phi_n(theta_n)), not single merged p(phi_0) (sorry for the equation text, i don't have enough reputation to link online equation svg!)
i couldn't find any more information than using merge_and_unload().

edit
i found there is description about it...shame that i didn't read it.
merge_and_unload()

this method merges the lora layers into the base model. this is needed if someone wants to use the base model as a standalone model.

so basically using merge_and_unload() is definitely not what i want.
is there any other options for merging lora weights?
any advice would be appreciated.","['pytorch', 'nlp', 'huggingface-transformers']",77501731,"perhaps this is what you are looking for add_weighted_adapter. you can refer to this documentation and this issue
model.add_weighted_adapter(
        adapters=['lora-1', 'lora-2'],
        weights=[0.5, 0.5],
        adapter_name=""combined"",
        combination_type=""svd"",
    )",https://stackoverflow.com/questions/77491934,pytorch,16-11-2023 02:20,8806.0,2.0,1.0,True,17-11-2023 12:41,16-11-2023 09:14
78634235,"numpy.dtype size changed, may indicate binary incompatibility. expected 96 from c header, got 88 from pyobject","i want to call my python module from the matlab. i received the error:
error using numpy_ops>init thinc.backends.numpy_ops

python error:
 valueerror: numpy.dtype size changed, may indicate binary incompatibility. expected 96 from c header, got 88 from pyobject.

the python script is as follows
import spacy
def text_recognizer(model_path, text):
try:
    # load the trained model
    nlp = spacy.load(model_path)
    print(""model loaded successfully."")
    
    # process the given text
    doc = nlp(text)
    ent_labels = [(ent.text, ent.label_) for ent in doc.ents]
        return ent_labels

the matlab script is as follows
% set up the python environment
pe = pyenv;
py.importlib.import_module('final_output');

% add the directory containing the python script to the python path
path_add = fileparts(which('final_output.py'));
if count(py.sys.path, path_add) == 0
    insert(py.sys.path, int64(0), path_add);
end
% define model path and text to process
model_path = 'd:\trained_model\\output\\model-best';
text = 'roses are red';
% call the python function
pyout = py.final_output.text_recognizer(model_path, text);
% convert the output to a matlab cell array
entity_labels = cell(pyout);
disp(entity_labels);

i found one solution to update numpy, what i did, but nothing changed. i am using python 3.9 and numpy version 2.0.0
the error was received when i tried to call the python module using a matlab script.
how can i fix the issue?","['python', 'numpy', 'matlab', 'spacy']",78641304,"the reason is that pandas defines its numpy dependency freely as ""anything newer than certain version of numpy"".
the problem occured, when numpy==2.0.0 has been released on june 16th 2024, because it is no longer compatible with your pandas version.
the solution is to pin down the numpy version to any before the 2.0.0. today it could be (this is the most recent numpy 1 release):
numpy==1.26.4

to be added in your requirements or to the pip command you use (but together with installing pandas).
nowadays pip is very flexible and can handle the issue flawesly. you just need to ask it to install both pandas and numpy of given versions in the same pip install invocation.",https://stackoverflow.com/questions/78634235,python,17-06-2024 18:52,204392.0,163.0,8.0,True,23-03-2025 00:22,03-07-2024 08:50
78130662,inserting xml tags at specific part of file without disrupting format,"i'm trying to work with some xml files to do sentence tagging whilst maintaining the original structure of the file. the files look like so:
<text xml:lang="""">
    <body>
      <div>
        <p>
          <p>
            <lb xml:id=""p1z1"" />19.
                    <lb xml:id=""p1z2"" />esse christolam meam te adeo candide et humaniter bullingere colendissime,
                    <lb xml:id=""p1z3"" />esse epistolam meam interpretatum. caeterum, quod scribis te ex consilio consanguine
                    <lb xml:id=""p1z4"" />et affinium generi tui responsum fratri meo coram dedisse, non
                    <lb xml:id=""p1z5"" />possum satis mirari, qui hoc factum sit. res enim ista ad me suum ad
                    <lb xml:id=""p1z6"" />fratrem pertinebat. nec ita fueram abs te dimissus, quod vel tu tale
                    <lb xml:id=""p1z7"" />quid reciperes vel ego probarem, sed ita tua sponte pollicebaris vel te,
                    <lb xml:id=""p1z8"" />vel generum mihi per literas responsurum. frater igitur dixit quidem
                    <lb xml:id=""p1z9"" />mihi te in praesentia nescio quorum (qui namque fuerint excidit) voluisse
                    <lb xml:id=""p1z10"" />respondere se vero voluisse recipere, imo admonuisse te ut, quemadmodum
                    <lb xml:id=""p1z11"" />promisisses, ita faceres. ego simulatque tergiversationem istam cognoscere
                    <lb xml:id=""p1z12"" />non potui aliter interpretari quam ali fortassis aliquid monstri,
                    <lb xml:id=""p1z13"" />ut dicitur. nam quae plana sunt et integra sive dicantur sive scripsisse
                    <lb xml:id=""p1z14"" />nihil refert. utut sit, ego iniuriam illam, ex qua omnes istae
                    <lb xml:id=""p1z15"" />difficultates sunt ortae, iampridem domino deque commendavi, qui
                    <lb xml:id=""p1z16"" />per mosen. mea est ultro et ego retribuam eis in tempore.
                    <lb xml:id=""p1z17"" />de altero etiam capite accipio tuam excusationem. quum enim tam sancte
                    <lb xml:id=""p1z18"" />affirmes te semper erga nos non aliter quam bene et fuisse et
...
...
...
        </p>
      </div>
    </body>
  </text>
</tei>


the sentences i need to tag span over several lines. the lines are tagged with the line break tag ""<lb xml:id=""n"" />"". i need to somehow tag the sentences, and then append them back with their original formal to the file. the issue i encounter is that while the text contains newline characters, as soon as i create an instance of a sentence and try to append to the line break tag, the new line character isn't valid....
the output should look like:
<text xml:lang="""">
    <body>
      <div>
        <p>
          <p>
            <lb xml:id=""p1z1"" /><s n=""1"" xml:lang=""la"">19.</s>
                    <lb xml:id=""p1z2"" /><s n=""1"" xml:lang=""la"">esse christolam meam te adeo candide et humaniter bullingere colendissime,
                    <lb xml:id=""p1z3"" />esse epistolam meam interpretatum.</s><s n=""2"" xml:lang=""la""> caeterum, quod scribis te ex consilio consanguine
                    <lb xml:id=""p1z4"" />et affinium generi tui responsum fratri meo coram dedisse, non
                    <lb xml:id=""p1z5"" />possum satis mirari, qui hoc factum sit.</s><s n=""3"" xml:lang=""la""> res enim ista ad me suum ad
                    <lb xml:id=""p1z6"" />fratrem pertinebat.</s><s n=""4"" xml:lang=""la""> nec ita fueram abs te dimissus, quod vel tu tale
                    <lb xml:id=""p1z7"" />quid reciperes vel ego probarem, sed ita tua sponte pollicebaris vel te,
                    <lb xml:id=""p1z8"" />vel generum mihi per literas responsurum.</s><s n=""5"" xml:lang=""la""> frater igitur dixit quidem
                    <lb xml:id=""p1z9"" />mihi te in praesentia nescio quorum (qui namque fuerint excidit) voluisse
                    <lb xml:id=""p1z10"" />respondere se vero voluisse recipere, imo admonuisse te ut, quemadmodum
                    <lb xml:id=""p1z11"" />promisisses, ita faceres.</s><s n=""6"" xml:lang=""la""> ego simulatque tergiversationem istam cognoscere
                    <lb xml:id=""p1z12"" />non potui aliter interpretari quam ali fortassis aliquid monstri,
                    <lb xml:id=""p1z13"" />ut dicitur.</s><s n=""7"" xml:lang=""la""> nam quae plana sunt et integra sive dicantur sive scripsisse
                    <lb xml:id=""p1z14"" />nihil refert.</s><s n=""8"" xml:lang=""la""> utut sit, ego iniuriam illam, ex qua omnes istae
                    <lb xml:id=""p1z15"" />difficultates sunt ortae, iampridem domino deque commendavi, qui
                    <lb xml:id=""p1z16"" />per mosen.</s><s n=""9"" xml:lang=""la""> mea est ultro et ego retribuam eis in tempore.</s>
                    <lb xml:id=""p1z17"" /><s n=""10"" xml:lang=""la"">de altero etiam capite accipio tuam excusationem.</s><s n=""11"" xml:lang=""la""> quum enim tam sancte
                    <lb xml:id=""p1z18"" />affirmes te semper erga nos non aliter quam bene et fuisse et
...
...
...
       </p>
      </div>
    </body>
  </text>
</tei>

my code looks like:
import xml.etree.elementtree as et
from nltk.tokenize import sent_tokenize
import nltk

# ensure nltk's sentence tokenizer is available
nltk.download('punkt')

def remove_ns_prefix(tree):
    for elem in tree.iter():
        if '}' in elem.tag:
            elem.tag = elem.tag.split('}', 1)[1]  # removing namespace
    return tree

def process_file(input_xml, output_xml):
    tree = et.parse(input_xml)
    root = remove_ns_prefix(tree.getroot())

    for body in root.findall('.//body'):
        for paragraph in body.findall('.//p'):
            # extract all lb elements and following texts
            lb_elements = list(paragraph.findall('.//lb'))
            lb_ids = [lb.attrib.get('xml:id', '') for lb in lb_elements]  # store lb ids
            text_after_lb = [(lb.tail if lb.tail else '') for lb in lb_elements]
            
            # combine the text and tokenize into sentences
            entire_text = ' '.join(text_after_lb)
            sentences = sent_tokenize(entire_text)
            sentences2 = "" "".join(sentences).split(""\n"")
            print(sentences2)
            
            # clear the paragraph's existing content
            paragraph.clear()

            # pair up lb tags and sentences using zip, reinsert them into the paragraph
            for lb_id, sentence in zip(lb_ids, sentences):
                # reinsert lb element
                lb_attrib = {'xml:id': lb_id} if lb_id else {}
                new_lb = et.subelement(paragraph, 'lb', attrib=lb_attrib)
                # attach sentence to this lb
                if sentence:
                    sentence_elem = et.subelement(paragraph, 's', attrib={'xml:lang': 'la'})
                    sentence_elem.text = sentence

    # write the modified tree to a new file
    tree.write(output_xml, encoding='utf-8', xml_declaration=true, method='xml')

i'm losing my mind. hopefully i have an xml pro who is willing to come to my rescue.
i've also tried first tagging, and then reinserting the line break tags afterwards, but due to the nature of xml it's tough. the next thing i would maybe attempt is to create temporary .txt files and go line by line and insert the tags on the lines that don't match...
any and all help appreciated at this point.","['python', 'xml', 'xml-parsing', 'nltk']",78131229,"the job can be done taking advantage of tail attribute of lb elements which are the items with index > 0 in this list (element.tail split by r'(\.|\n)' regexp). label element is placed detecting sentence start and end (dots).
['<lb xml:id=""p1z1""/>', '19', '.', '', '\n', '            ']

that list represents this element; quoted to show whitespace
'<lb xml:id=""p1z1""/>19.
                '

script does no take into account namespaces and is provided as poc of the parsing technique.
it could be cleaner to label sentences with a self closing element
<lb xml:id=""p1z2""/><s n=""2""/>esse christolam meam te adeo candide et humaniter bullingere colendissime,
<lb xml:id=""p1z3""/>esse epistolam meam interpretatum.<s n=""3""/> caeterum, quod scribis te ex consilio consanguine

given this sample
<text xml:lang="""">
  <body>
    <div>
      <p>
        <p>
            <lb xml:id=""p1z1""/>19.
            <lb xml:id=""p1z2""/>esse christolam meam te adeo candide et humaniter bullingere colendissime,
            <lb xml:id=""p1z3""/>esse epistolam meam interpretatum. caeterum, quod scribis te ex consilio consanguine
            <lb xml:id=""p1z4""/>et affinium generi tui responsum fratri meo coram dedisse, non
            <lb xml:id=""p1z5""/>possum satis mirari, qui hoc factum sit. res enim ista ad me suum ad
            <lb xml:id=""p1z6""/>fratrem pertinebat. nec ita fueram abs te dimissus, quod vel tu tale
            <lb xml:id=""p1z7""/>quid reciperes vel ego probarem, sed ita tua sponte pollicebaris vel te,
            <lb xml:id=""p1z8""/>vel generum mihi per literas responsurum. frater igitur dixit quidem
            <lb xml:id=""p1z9""/>mihi te in praesentia nescio quorum (qui namque fuerint excidit) voluisse
            <lb xml:id=""p1z10""/>respondere se vero voluisse recipere, imo admonuisse te ut, quemadmodum
            <lb xml:id=""p1z11""/>promisisses, ita faceres. ego simulatque tergiversationem istam cognoscere
            <lb xml:id=""p1z12""/>non potui aliter interpretari quam ali fortassis aliquid monstri,
            <lb xml:id=""p1z13""/>ut dicitur. nam quae plana sunt et integra sive dicantur sive scripsisse
            <lb xml:id=""p1z14""/>nihil refert. utut sit, ego iniuriam illam, ex qua omnes istae
            <lb xml:id=""p1z15""/>difficultates sunt ortae, iampridem domino deque commendavi, qui
            <lb xml:id=""p1z16""/>per mosen. mea est ultro et ego retribuam eis in tempore.
            <lb xml:id=""p1z17""/>de altero etiam capite accipio tuam excusationem. quum enim tam sancte
            <lb xml:id=""p1z18""/>affirmes te semper erga nos non aliter quam bene et fuisse et
        </p>
      </p>
    </div>
  </body>
</text>

result
<text xml:lang="""">
  <body>
    <div>
      <p>
        <p>
            <lb xml:id=""p1z1""/><s n=""1""/>19.
            <lb xml:id=""p1z2""/><s n=""2""/>esse christolam meam te adeo candide et humaniter bullingere colendissime,
            <lb xml:id=""p1z3""/>esse epistolam meam interpretatum.<s n=""3""/> caeterum, quod scribis te ex consilio consanguine
            <lb xml:id=""p1z4""/>et affinium generi tui responsum fratri meo coram dedisse, non
            <lb xml:id=""p1z5""/>possum satis mirari, qui hoc factum sit.<s n=""4""/> res enim ista ad me suum ad
            <lb xml:id=""p1z6""/>fratrem pertinebat.<s n=""5""/> nec ita fueram abs te dimissus, quod vel tu tale
            <lb xml:id=""p1z7""/>quid reciperes vel ego probarem, sed ita tua sponte pollicebaris vel te,
            <lb xml:id=""p1z8""/>vel generum mihi per literas responsurum.<s n=""6""/> frater igitur dixit quidem
            <lb xml:id=""p1z9""/>mihi te in praesentia nescio quorum (qui namque fuerint excidit) voluisse
            <lb xml:id=""p1z10""/>respondere se vero voluisse recipere, imo admonuisse te ut, quemadmodum
            <lb xml:id=""p1z11""/>promisisses, ita faceres.<s n=""7""/> ego simulatque tergiversationem istam cognoscere
            <lb xml:id=""p1z12""/>non potui aliter interpretari quam ali fortassis aliquid monstri,
            <lb xml:id=""p1z13""/>ut dicitur.<s n=""8""/> nam quae plana sunt et integra sive dicantur sive scripsisse
            <lb xml:id=""p1z14""/>nihil refert.<s n=""9""/> utut sit, ego iniuriam illam, ex qua omnes istae
            <lb xml:id=""p1z15""/>difficultates sunt ortae, iampridem domino deque commendavi, qui
            <lb xml:id=""p1z16""/>per mosen.<s n=""10""/> mea est ultro et ego retribuam eis in tempore.
            <lb xml:id=""p1z17""/><s n=""11""/>de altero etiam capite accipio tuam excusationem.<s n=""12""/> quum enim tam sancte
            <lb xml:id=""p1z18""/>affirmes te semper erga nos non aliter quam bene et fuisse et
        </p>
      </p>
    </div>
  </body>
</text>

set self_close = false to get the op's labels. with restoring parsed elements back to the doc
import re
from lxml import etree
doc = etree.parse('/home/luis/tmp/tmp.xml')
# find parent element
parent = doc.xpath('//div/p/p')[0]

# keep indentation of first lb
all='<p>' + parent.text
i=1
is_open=false
self_close = true
for t in parent.xpath('lb'):
  parts = ['']
  parts.extend(re.split(r'(\.|\n)', t.tail))
  
  t.tail=none
  parts[0]=etree.tostring(t).decode('utf-8')

  #print(parts)
  for p, e in enumerate(parts):
    skip = (e == '' or re.match(r'^(\n|\s+)$', e) is not none)
    
    if p > 0 and not is_open and not skip:
      if self_close:
        parts[p] = f'<s n=""{i}""/>{e}'
      else:
        parts[p] = f'<s n=""{i}"">{e}'
        
      is_open=true
    elif is_open and e == '.':
      if not self_close:
        parts[p] = '.</s>'
      is_open=false
      i += 1
    elif p == len(parts) - 1:
        all += ''.join(parts)
    else:
      continue

# last sentence does not end with a dot?
# hardcoded here but could be detected
if not self_close:
  all+='</s>'

all +='</p>'
# parse back to an element
xfrag = etree.fromstring(all)
xfrag.tail = parent.tail

# replace parent element on document
parent.getparent().replace(parent, xfrag)
print(etree.tostring(doc).decode('utf-8'))",https://stackoverflow.com/questions/78130662,python,08-03-2024 23:13,64.0,1.0,1.0,True,09-03-2024 13:50,08-03-2024 23:24
71039902,huggingface return probability and class label trainer.predict,"is there any way to return probabilities and actual class using trainer.predict ?
i checked the documentation at this page but couldn't figure out. as of now it seems to be returning logits
obviously both probabilities and actual class could be computed using additional coding but wondering if there is any prebuilt method to do the same
my current output as below
new_predictions=trainer.predict(dataset_for_future_predicition_after_tokenizer)

new_predictions


predictionoutput(predictions=array([[-0.43005577,  3.646306  , -0.8073783 , -1.0651836 , -1.3480505 ,
        -1.108013  ],
       [ 3.5415223 , -0.8513837 , -1.8553216 , -0.18011567, -0.35627165,
        -1.8364134 ],
       [-1.0167522 , -0.8911268 , -1.7115675 ,  0.01204597,  1.7177908 ,
         1.0401527 ],
       [-0.82407415, -0.46043932, -1.089274  ,  2.6252217 ,  0.33935028,
        -1.3623345 ]], dtype=float32), label_ids=none, metrics={'test_runtime': 0.0182, 'test_samples_per_second': 219.931, 'test_steps_per_second': 54.983})","['python', 'nlp', 'huggingface-transformers']",71078516,"as you mentioned, trainer.predict returns the output of the model prediction, which are the logits.
if you want to get the different labels and scores for each class, i recommend you to use the corresponding pipeline for your model depending on the task (textclassification, tokenclassification, etc). this pipeline has a return_all_scores parameter on its __call__ method that allows you to get all scores for each label on a prediction.
here's an example:
from transformers import textclassificationpipeline, autotokenizer, automodelforsequenceclassification

model_name = ""...""
tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforsequenceclassification.from_pretrained(model_name)

pipe = textclassificationpipeline(model=model, tokenizer=tokenizer)
prediction = pipe(""the text to predict"", return_all_scores=true)

this is an example of how this prediction variable will look like:
[{label: 'label1', score: 0.80}, {label: 'label2', score: 0.15}, {label: 'label3', score: 0.05}]

the label names can be set on the model's config.json file or when creating the model (before training it) by defining id2label and label2id model parameters:
model = automodelforsequenceclassification.from_pretrained(
    model_name,
    num_labels=num_labels,
    label2id={""greeting"": 0, ""help"": 1, ""farewell"": 2},
    id2label={0: ""greeting"", 1: ""help"", 2: ""farewell""},
)",https://stackoverflow.com/questions/71039902,python,08-02-2022 19:37,7990.0,1.0,1.0,True,11-02-2022 19:01,10-02-2022 19:13
77148560,"python api usage for coreference, semantic graph and nerc","intro
hi, i have been using freeling for a few months now to extract triplets. so far i have succeded in doing so by using the dependency tree and the full parse tree, but i am trying to add nerc.
my work so far
i checked the tutorial for python, but i couldn't find anything beyond depdency parsing. so i went through the class list (since the same classes should be available for python and c++) but it is not very clear how to retrieve the named entities and after checking the output of the analyzer sampler i have a few questions about the performance of the ner module.
problems
so what i'm asking if anyone can help me with is the following:

doubt about entities: using the example ""sobre la mesa marï¿½ï¿½a ve y coge una manzana, un sombrero, una llave y dos paraguas rojo."" i realized that working with capitalized words and lowercase produce different results, but by making it all lowercase the entity recognition stops recognizing ""marï¿½pt as a person. is there are workaround for this or am i going in the wrong direction? the main problem is that ""marï¿½ï¿½a"" not recognized as a named entity (which i need it to be by the way) results in ""marï¿½ï¿½a"" not being the subject of the sentence anymore. im using:

neclass = pyfreeling.ner(lpath + ""/nerc/ner/ner-ab-rich.dat"")

how to retrieve named entities: kind of a follow up of the previous question, how do i get the named entities? i couldn't find any code related to this and the semantic graph i obtain holds 0 entities.

any comments and suggestions are welcomed, thanks in advance.","['python', 'nlp', 'named-entity-recognition', 'freeling']",77155932,"well aparently there are 3 nerc modules, one rule-based and two ml-based. all of them use capitalization as a feature, and since both models are trained on standard text, all nes seen in training are capitalized. therefore lowercase named entities are not likely to be recognized.
about the retrieval it seems that the get_label() from the nodes can provide this info if a word (or multiword) has a pos-tag starting with ""np"", then it means it was recognized by the nerc module.
this is based on freelings authors own explanation which you can find here",https://stackoverflow.com/questions/77148560,python,21-09-2023 08:42,50.0,0.0,1.0,True,22-09-2023 08:20,21-09-2023 10:54
73015102,error in fit_transform while finding tf-idf in python,"import pandas as pd
from sklearn.feature_extraction.text import tfidfvectorizer
mylist = [
    'a a b c',
    'a c c c d e f',
    'a c d d d',
    'a d f',
]
df = pd.dataframe({""texts"": mylist})
tfidf_vectorizer = tfidfvectorizer(ngram_range=[1, 1])
tfidf_separate = tfidf_vectorizer.fit_transform(df[""texts""])

i am trying to find tf-idf value for ï¿½ï¿½ï¿½dï¿½ï¿½ï¿½ in line 3. but, it is showing me empty vocabulary error ""valueerror: empty vocabulary; perhaps the documents only contain stop words"".
any advice on how to resolve the error would be appre","['python', 'nlp', 'tf-idf', 'tfidfvectorizer']",73017360,"you can do it like this:

define analyzer='char' so that tfidfvectorizer works with the letters;
find the index of d in the vocabulary and use it

import pandas as pd
from sklearn.feature_extraction.text import tfidfvectorizer
mylist = [
    'a a b c',
    'a c c c d e f',
    'a c d d d',
    'a d f',
]
df = pd.dataframe({""texts"": mylist})
tfidf_vectorizer = tfidfvectorizer(ngram_range=[1, 1], analyzer='char')
tfidf_separate = tfidf_vectorizer.fit_transform(df[""texts""])
ind = tfidf_vectorizer.vocabulary_['d']
tfidf_separate.todense()[2, ind]
>>> 0.6490674853546846",https://stackoverflow.com/questions/73015102,python,17-07-2022 20:22,208.0,0.0,1.0,True,18-07-2022 04:33,17-07-2022 20:24
73788480,removing punctuations in dataframe using for loop,"i have a dataframe that looks like below
  a        b       c       d      e
0 orange  dad's  x eyes   3d.    navy
1 pink.   mum's  bored.   ooo.   nan
2 yellow  nan    sad      gray   nan

i'm trying to remove punctuations in every column in the dataframe using for loop
import string
string.punctuation

#defining the function to remove punctuation
def remove_punctuation(text):
    punctuationfree="""".join([i for i in text if i not in string.punctuation])
    return punctuationfree

#storing the puntuation free text
col=['a','b','c','d','e']

for i in col:
    df[i].apply(lambda x:remove_punctuation(x))

but i get
    ""typeerror                                 traceback (most recent call last)
    /var/folders/jd/lln92nb4p01g8grr0000gn/t/ipykernel_24651/2417883.py in <module>
         12 
         13 for i in col:
    ---> 14     df[i].apply(lambda x:remove_punctuation(x))
      
typeerror: 'float' object is not iterable"" 

can anyone help me on this please? any help would be greatly appreciated!","['python', 'string', 'nlp', 'punctuation']",73788641,"you are getting the error because of nan values, try to check for nan upfront:
def remove_punctuation(text):
    if pd.isna(text):
        return text
    punctuationfree="""".join([i for i in text if i not in string.punctuation])
    return punctuationfree

for c in df:
    df[c] = df[c].apply(remove_punctuation)

output
# df
          a     b       c     d     e
0   orange   dads  x eyes    3d  navy
1     pink   mums   bored   ooo   nan
2   yellow   nan     sad  gray   nan",https://stackoverflow.com/questions/73788480,python,20-09-2022 14:36,160.0,1.0,3.0,True,20-09-2022 16:03,20-09-2022 16:03
70396367,&#39;translater&#39; r package is not showing the results of the automated translation,"when using the function translate from the translater package, the function keeps the source language, i.e. no translation is occuring.
library(translater)
data(enron)
translate(dataset = enron, content.field = 'email', 
                            google.api.key = mygooglekey, source.lang = 'en', 
                            target.lang = 'de')

what seems to be the problem? are there any alternatives for doing automated translation in r?","['r', 'nlp', 'google-translate']",70465585,"i found out that the problem was the api key was not activated.
after purchasing credit from the google cloud service the command (below) indeed worked
library(translater)
data(enron)
translater::translate(dataset = enron, content.field = 'email', 
                            google.api.key = mygooglekey, source.lang = 'en', 
                            target.lang = 'de')

if you are facing a problem like this, make sure that your google api key is actually activated.
when you verify your account with the help of paypal account or a credit card, google will give you a 300$ for your trial phase to try the services.
after buying credit or simply using the 300$ you get in your trial phase, the code should run smoothly.
how to activate:
if you don't have a google account, go create one here

then sign in to 
there, click on try for free.
from there, follow all the verification steps, including adding a credit card or paypal account. this step won't charge you anything, it is only to make sure you are not a bot.
after verification is done. click on the three horizontal lines to the left hand side corner to open up the navigation menu.
there choose api & services, then choose library.
search for the service you want, in this case for this question, cloud translation api, and click on it.
click on enable.
then click on credentials on the left hand side pane, then click on create credentials and choose api key.
summary:
create a google cloud account, verify it, enable the api service you want to use, create your own api key. use that api key to run the service from your r console.",https://stackoverflow.com/questions/70396367,r,17-12-2021 16:45,248.0,0.0,1.0,True,26-12-2021 10:47,26-12-2021 10:47
77663314,semantickernel throws &quot;unrecognized request argument supplied: tools&quot;,"i've just upgraded to semantickernel 1.0.0-rc4 and this error shows up wherever i'm using native functions:
microsoft.semantickernel.
unrecognized request argument supplied: tools
status: 404 (model_error)

content:
{
  ""error"": {
    ""message"": ""unrecognized request argument supplied: tools"",
    ""type"": ""invalid_request_error"",
    ""param"": null,
    ""code"": null
  }
}


headers:
access-control-allow-origin: redacted
x-ms-region: redacted
x-ratelimit-remaining-tokens: redacted
apim-request-id: redacted
x-request-id: redacted
ms-azureml-model-error-reason: redacted
ms-azureml-model-error-statuscode: redacted
x-ms-client-request-id: 952ee943-35b4-4bd4-883f-a82c80c93ae3
x-ratelimit-remaining-requests: redacted
strict-transport-security: redacted
x-content-type-options: redacted
date: thu, 14 dec 2023 21:59:26 gmt
content-length: 158
content-type: application/json

here is my sample code trying to use lightplugin in the sample docs:
openaipromptexecutionsettings settings = new()
{
    toolcallbehavior = toolcallbehavior.autoinvokekernelfunctions
};

// this line throws the exception
var result = await chatservice.getchatmessagecontentsasync(chathistory, settings, kernel);

it works fine with semantic functions, the problem is just when i'm using native functions.
the native functions were also working fine in 1.0.0-rc3.","['c#', 'openai-api', 'azure-openai', 'semantic-kernel']",77668679,"the problem was that since semantickernel 1.0.0-rc4, it requires a gpt-turbo-35 model newer than 0614. mine was 0301.
good to mention that in some azure regions (like west europe at the time) the newer versions are not available yet. i had to create another resource on uk south so i could create a 1106 version model.",https://stackoverflow.com/questions/77663314,c#,14-12-2023 22:07,1161.0,1.0,1.0,True,15-12-2023 20:31,15-12-2023 08:16
79354534,how to load a finetuned vision llm model? moondream model case,"i've finetuned the moondream model for my own image inference case. i used the finetune script provided on the official repository.
it all went well after learning how to use the script for my dataset.
the script generated the following files when i've ""save_pretrained()"":

config.json;
generation_config.json;
model.safetensors

now i want to load it and make some inference with some images. how can i do it?
i've tried using many ways but i haven't managed to do it, for instance:
model = automodelforcausallm.from_pretrained(
    pretrained_model_name_or_path=model_root_dir,
    use_safetensors=true,
    state_dict=load_file(model_path),
    config=model_config,  # provide a configuration file for the model
    trust_remote_code=true,
)

but i'm getting errors like:
attributeerror: module 'transformers_modules.vikhyatk.moondream2.fb2293ab7450beb1dae536b069f5966becf58e5c.moondream' has no attribute 'moondream'

how can i solve this error?","['python', 'artificial-intelligence', 'huggingface-transformers', 'large-language-model']",79362886,"the issue probably arises due to significant structural changes made to the moondream model's config and renaming in the hf repository, as seen in this commit

if youï¿½ï¿½ï¿½ve saved your checkpoint using save_pretrained(), it no longer align with the new structure of the repository.
hereï¿½ï¿½ï¿½s how i solved the problem temporarly. maybe there needs to be a new ""legacy"" moondream model to solve these changes in the long term.
workaround:
copy the repository content from huggingface a commit prior to the breaking changes into the directory where your saved model resides.
remove any remote references in config.json
for example, replace:
""automodelforcausallm"": ""vikhyatk/moondream2--moondream.moondreammodel""

with:
""automodelforcausallm"": ""moondream.moondreammodel""

if the model complains about missing .py files, paste them into the local directory (in my case this was in the hugginface cache folder).
now, load the modeldated files:
from transformers import automodelforcausallm

model = automodelforcausallm.from_pretrained(
    ""myawesomemodel"",
    trust_remote_code=true,
)

i know this isn't a perfect solution but maybe it helps.
let me know if you have a better one!",https://stackoverflow.com/questions/79354534,python,14-01-2025 09:36,275.0,0.0,1.0,True,16-01-2025 19:45,14-01-2025 10:37
67421308,spacy 3 beam parse for ner probability,"i'm trying to retrieve the probability of my spacy model in assigning the right label to an entity. i have spacy version 3.0.5.
threshold = 0.5
        
for i in testing_raw:
    doc = nlp_updated(i)
    beams = nlp_updated.beam_parse([ doc ], beam_width = 16, beam_density = 0.0001)
    entity_scores = defaultdict(float)

    for beam in beams:
        for score, ents in nlp_updated.entity.moves.get_beam_parses(beam):
            for start, end, label in ents:
                entity_scores[(start, end, label)] += score

        for key in entity_scores:
            start, end, label = key
            score = entity_scores[key]
            if ( score > threshold):
                print ('label: {}, text: {}, score: {}'.format(label, doc[start:end], score))

the following line throws this error:
beams = nlp_updated.beam_parse([ doc ], beam_width = 16, beam_density = 0.0001)

attributeerror: 'english' object has no attribute 'beam_parse'

is this because spacy version 3 doesn't consider beam_parse? if so, how can i do this in this version of spacy as i can't seem to find anything in the documentation?","['python', 'spacy']",67430708,"this workaround for getting ner probabilities doesn't work in v3 because the api has changed, and there's no recommended replacement at the moment.
a spancategorizer is being developed that will allow you to get ner labels with confidence scores.",https://stackoverflow.com/questions/67421308,python,06-05-2021 15:28,1734.0,1.0,2.0,True,24-01-2023 03:41,07-05-2021 08:34
71147799,create new boolean fields based on specific bigrams appearing in a tokenized pandas dataframe,"looping over a list of bigrams to search for, i need to create a boolean field for each bigram according to whether or not it is present in a tokenized pandas series. and i'd appreciate an upvote if you think this is a good question!
list of bigrams:
bigrams = ['data science', 'computer science', 'bachelors degree']

dataframe:
df = pd.dataframe(data={'job_description': [['data', 'science', 'degree', 'expert'],
                                            ['computer', 'science', 'degree', 'masters'],
                                            ['bachelors', 'degree', 'computer', 'vision'],
                                            ['data', 'processing', 'science']]})

desired output:
                         job_description  data science computer science bachelors degree
0        [data, science, degree, expert]          true            false            false
1   [computer, science, degree, masters]         false             true            false
2  [bachelors, degree, computer, vision]         false            false             true
3             [data, bachelors, science]         false            false            false

criteria:

only exact matches should be replaced (for example, flagging for 'data science' should return true for 'data science' but false for 'science data' or 'data bachelors science')
each search term should get it's own field and be concatenated to the original df

what i've tried:
failed:  df = [x for x in df['job_description'] if x in bigrams]
failed: df[bigrams] = [[any(w==term for w in lst) for term in bigrams] for lst in df['job_description']]
failed: could not adapt the approach here -> match trigrams, bigrams, and unigrams to a text; if unigram or bigram a substring of already matched trigram, pass; python
failed: could not get this one to adapt, either -> compare two bigrams lists and return the matching bigram
failed: this method is very close, but couldn't adapt it to bigrams -> create new boolean fields based on specific terms appearing in a tokenized pandas dataframe
thanks for any help you can provide!","['python', 'pandas', 'dataframe', 'nlp', 'boolean']",71148686,"you could also try using numpy and nltk, which should be quite fast:
import pandas as pd
import numpy as np
import nltk

bigrams = ['data science', 'computer science', 'bachelors degree']
df = pd.dataframe(data={'job_description': [['data', 'science', 'degree', 'expert'],
                                            ['computer', 'science', 'degree', 'masters'],
                                            ['bachelors', 'degree', 'computer', 'vision'],
                                            ['data', 'processing', 'science']]})

def find_bigrams(data):
  output = np.zeros((data.shape[0], len(bigrams)), dtype=bool)
  for i, d in enumerate(data):
    possible_bigrams = [' '.join(x) for x in list(nltk.bigrams(d)) + list(nltk.bigrams(d[::-1]))]
    indices = np.where(np.isin(bigrams, list(set(bigrams).intersection(set(possible_bigrams)))))
    output[i, indices] = true
  return list(output.t)

output = find_bigrams(df['job_description'].to_numpy())
df = df.assign(**dict(zip(bigrams, output)))

|    | job_description                               | data science   | computer science   | bachelors degree   |
|---:|:----------------------------------------------|:---------------|:-------------------|:-------------------|
|  0 | ['data', 'science', 'degree', 'expert']       | true           | false              | false              |
|  1 | ['computer', 'science', 'degree', 'masters']  | false          | true               | false              |
|  2 | ['bachelors', 'degree', 'computer', 'vision'] | false          | false              | true               |
|  3 | ['data', 'processing', 'science']             | false          | false              | false              |",https://stackoverflow.com/questions/71147799,python,16-02-2022 18:59,133.0,5.0,2.0,True,16-02-2022 20:47,16-02-2022 20:47
78481278,splitting html file and saving chunks using langchain,"i'm very new to langchain, and i'm working with around 100-150 html files on my local disk that i need to upload to a server for nlp model training. however, i have to divide my information into chunks because each file is only permitted to have a maximum of 20k characters. i'm trying to use the langchain library to do so, but i'm not being successful in splitting my files into my desired chunks.
for reference, i'm using this url:   saved locally as html only.
it's a hadoop faq page that i've downloaded as an html file onto my pc. there are many questions and answers there. i've noticed that sometimes, for some files, it gets split by a mere title, and another split is the paragraph following that title. but my desired output would be to have the title and the specific paragraph or following text from the body of the page, and as metadata, the title of the page.
i'm using this code:
from langchain_community.document_loaders import unstructuredhtmlloader
from langchain_text_splitters import htmlheadertextsplitter
# same example with the url  saved locally as html only
dir_html_file='faq ï¿½ï¿½ï¿½ bigdata.html'

data_html = unstructuredhtmlloader(dir_html_file).load()

headers_to_split_on = [
    (""h1"", ""header 1"")]
html_splitter = htmlheadertextsplitter(headers_to_split_on=headers_to_split_on)
html_header_splits = html_er.split_text(str(data_html))

but is returning a bunch of weird characters and not splitting the document at all.
this is an output:
[document(page_content='[document(page_content=\'bigdata\\n\\n""you can have data without information, but you cannot have information without big data.""\\n\\nsaurabhmcakiet@gmail.com\\n\\n+91-8147644946\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ntoggle navigation\\n\\nhome\\n\\nbigdata\\n\\n\\toverview of bigdata\\n\\tsources of bigdata\\n\\tpros & cons of bigdata\\n\\tsolutions of bigdata\\n\\nhadoop admin\\n\\n\\thadoop\\n\\t\\n\\t\\toverview of hdfs\\n\\t\\toverview of mapreduce\\n\\t\\tapache yarn\\n\\t\\thadoop architecture\\n\\t\\n\\n\\tplanning of hadoop cluster\\n\\tadministration and maintenance\\n\\thadoop ecosystem\\n\\tsetup hdp cluster from scratch\\n\\tinstallation and configuration\\n\\tadvanced cluster configuration\\n\\toverview of ranger\\n\\tkerberos\\n\\t\\n\\t\\tinstalling kerberos/configuring the kdc and enabling kerberos security\\n\\t\\tconfigure spnego authentication for hadoop\\n\\t\\tdisabled kerberos via ambari\\n\\t\\tcommon issues after disabling kerberos via ambari\\n\\t\\tenable  for ambari server\\n\\t\\tenable ssl or  for oozie web ui\\n\\nhadoop dev\\n\\n\\tsolr\\n\\t\\n\\t\\tsolr installation\\n\\t\\tcommits and optimizing in solr and its use for nrt\\n\\t\\tsolr faq\\n\\t\\n\\n\\tapache kafka\\n\\t\\n\\t\\tkafka quickstart\\n\\t\\n\\n\\tget last access time of hdfs files\\n\\tprocess hdfs data with java\\n\\tprocess hdfs data with pig\\n\\tprocess hdfs data with hive\\n\\tprocess hdfs data with sqoop/flume\\n\\nbigdata architect\\n\\n\\tsolution vs enterprise vs technical architectï¿½ï¿½ï¿½s role and responsibilities\\n\\tsolution architect certification\\n\\nabout me\\n\\nfaq\\n\\nask questions\\n\\nfaq\\n\\nhome\\n\\nfaq\\n\\nfrequently\\xa0asked questions about big data\\n\\nmany questions about big data have yet to be answered in a vendor-neutral way. with so many definitions, opinin the gamut. here i will attempt to cut to the heart of the matter by addressing some key questions i often get from readers, clients and industry analysts.\\n\\n1) what is big data?\\n\\n1) what is big data?\\n\\nbig dataï¿½ï¿½ï¿½ is an all-inclusive term used to describe vast amounts of information. in contrast to traditional structured data which is typically stored in a relational database, big data varies in terms of volume, velocity, and variety.\\n\\nbig data\\xa0is characteristically generated in large volumes ï¿½ï¿½ï¿½ on the order of terabytes or exabytes of data (starts with 1 and has 18 zeros after it, or 1 million terabytes) per individual data set.\\n\\nbig data\\xa0is also generated with high velocity ï¿½ï¿½ï¿½ it is collected at frequent intervals ï¿½ï¿½ï¿½ which makes it difficult to analyze (though analyzing it rapidly makes it more valuable).\\n\\nor in simple words we can say ï¿½ï¿½ï¿½big data includes data sets whose size is beyond the ability of traditional software tools to capture, man reasonable time.ï¿½ï¿½ï¿½\\n\\n2) how much data does it take to be called big data?\\n\\nthis question cannot be easily answered absolutely. based on the infrastructure on the market the lower threshold is at about 1 to 3 terabytes.\\n\\nbut using big data technologies can be sensible for smaller databases as well, for example if complex mathematiccal or statistical analyses are run against a database. netezza offers about 200 built in functions and computer languages like revolution r or phyton which can be used in such cases.\\n\\

my expected output will look something like this:
one chunk:

frequently asked questions about big data

many questions about big data have yet to be answered in a vendor-neutral way. with so many definitions, opinions run the gamut. here i will attempt to cut to the heart of the matter by addressing some key questions i often get from readers, clients and industry analysts.

1) what is big data?
ï¿½ï¿½ï¿½big dataï¿½ï¿½ï¿½ is an all-inclusive term ast amounts of information. in contrast to traditional structured data which is typically stored in a relational database, big data varies in terms of volume, velocity, and variety. big data is characteristically generated in large volumes ï¿½ï¿½ï¿½ on the order of terabytes or exabytes of data (starts with 1 and has 18 zeros after it, or 1 million terabytes) per individual data set. big data is also generated with high velocity ï¿½ï¿½ï¿½ it is collected at frequent intervals ï¿½ï¿½ï¿½ which makes it difficult to analyze (though analyzing it rapidly makes it more valuable).
or in simple words we can say ï¿½ï¿½ï¿½big data includes data sets whose size is beyond the ability of traditional software tools to capture, manage, and process the data in a reasonable time.ï¿½ï¿½ï¿½
2) how much data does it take to be called big data?
this question cannot be easily answered absolutely. based on the infrastructure on the market the lower threshold is at about 1 to 3 terabytes.
but using big data technologies can be sensiell, for example if complex mathematical or statistical analyses are run against a database. netezza offers about 200 built in functions and computer languages like revolution r or phyton which can be used in such cases.

metadata: faq


another chunck
7) where is the big data trend going?
eventually the big data hype will wear off, but studies show that big data adoption will continue to grow. with a projected $16.9b market by 2015 (wikibon goes even further to say $50b by 2017), it is clear that big data is here to stay. however, the big data talent pool is lagging behind and will need to catch up to the pace of the market. mckinsey & company estimated in may 2011 that by 2018, the us alone could face a shortage of 140,000 to 190,000 people with deep analytical skills as well as 1.5 million managers and analysts with the know-how to use the analysis of big data to make effective decisions.
the emergence of big data analytics has permanently altered many businessesï¿½ï¿½ï¿½ way of looking at data. big da take companies down a long road of staff, technology, and data storage augmentation, but the payoff ï¿½ï¿½ï¿½ rapid insight into never-before-examined data ï¿½ï¿½ï¿½ can be huge. as more use cases come to light over the coming years and technologies mature, big data will undoubtedly reach critical mass and will no longer be labeled a trend. soon it will simply be another mechanism in the bi ecosystem.
8) who are some of the big data users?
from cloud companies like amazon to healthcare companies to financial firms, it seems as if everyone is developing a strategy to use big data. for example, every mobile phone user has a monthly bill which catalogs every call and every text; processing the sheer volume of that data can be challenging. software logs, remote sensing technologies, information-sensing mobile devices all pose a challenge in terms of the volumes of data created. the size of big data can be relative to the size of the enterprise. for some, it may be hundreds of gigabytes, for others, tens or hundrytes to cause consideration.
9) data visualization is becoming more popular than ever.
in my opinion, it is absolutely essential for organizations to embrace interactive data visualization tools. blame or thank big data for that and these tools are amazing. they are helping employees make sense of the never-ending stream of data hitting them faster than ever. our brains respond much better to visuals than rows on a spreadsheet.
companies like amazon, apple, facebook, google, twitter, netflix and many others understand the cardinal need to visualize data. and this goes way beyond excel charts, graphs or even pivot tables. companies like tableau software have allowed non-technical users to create very interactive and imaginative ways to visually represent information.

metadata: faq  

my thought process is being able to gather all the information and split it into chunks, but i don't want titles without their following paragraphs separated, and i also want as much info as possible (max 20k characters) before creating another chunk.
i would also like to save these chunks and their meta data. is there a function in langchain to do this?
i am open to hearing not to do this in langchain for efficiency reasons.","['python', 'html', 'split', 'langchain', 'py-langchain']",78994909,"check this super amazing html chunking package :package:
pip install html_chunking

our html chunking algorithm operates through a well-structured process that involves several key stages, each tailored to efficiently chunk and merge html content while adhering to a token limit. this approach is highly suitable for scenarios where token limitations are critical, and the need for accurate html parsing is paramount, especially in tasks like web automation or navigation where html content serves as input.

for those of you who are interested in this, here's a demo


from html_chunking import get_html_chunks
merged_chunks = get_html_chunks(your_html_string_here, max_tokens=1000, is_clean_html=true, attr_cutoff_len=25)
merged_chunks


the output should consists of several html chunks, where each chunk contains valid html code with preserved structure and attributes (from root node all the way down to current node), and any excessively long attributes are truncated to the specified length.

check out the html_chunking pypi page and our github page for more example demo!!

for those who are investigating the best way of chunking html for web automation or any other web agent tasks, you should definitely try html_chunking!!

langchain (htmlheadertextsplitter & htmlsectionsplitter) and llamaindex (htmlnodeparser) split text at the element level and add metadata for each header relevant to the chunk. however, they extract only the text content and exclude the html structure, attributes, and other non-text elements, limiting their use for tasks requiring the full html context.

check our github repo below and star :star2:",https://stackoverflow.com/questions/78481278,python,15-05-2024 01:44,4355.0,2.0,2.0,True,17-09-2024 16:03,19-05-2024 22:59
76878134,where to find spacy.py file to rename,"i am attempting to install spacy and have tried a number of methods using pip, conda, and installing directing from git. however, i am running into the same error:
---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
cell in[26], line 3
      1 import spacy
----> 3 nlp = spacy.load(""en_core_web_sm"")

attributeerror: module 'spacy' has no attribute 'load'

from reading various articles online like this one, i see that my error is likely due to a file called ""spacy.py"" that is causing a shadowing error. however, i can't find this file.
using which python in my dir shows me:
/users/my_name/anaconda3/bin/python

looking into my anaconda3/bin/python dir, i do see a unix executable file named ""spacy"" but renaming it has not fixed my error.","['python', 'nlp', 'spacy']",76879261,"make sure you're also installing your preferred package:
for pip:
python -m spacy download en_core_web_sm

also, if you're using a virtual environment in your ide, you should also run the following commands before installing spacy and your preferred package:
for pip:
python -m venv .env
source .env/bin/activate

for conda:
conda create -n venv
conda activate venv

there could also be a mismatch between your environments. if you're using anaconda, try installing spacy in the anaconda powershell. otherwise, try installing spacy using conda and/or pip in your ide's terminal.",https://stackoverflow.com/questions/76878134,python,10-08-2023 17:40,60.0,0.0,1.0,True,08-08-2024 23:32,08-08-2024 23:32
78005393,how can i find out the gpt-4 model version when using openai python library and azure openai?,"i use gpt-4 via openai python library and azure openai. how can i find out the gpt-4 model version by using the openai python library (and not looking at  because for some azure openai instances i only have the credentials to use the api but i can't view them on 
i read:


you can verify this by looking at the response object after sending a request. the response will include the specific model version used (e.g. gpt-3.5-turbo-0613).



gpt-4 currently points to gpt-4-0613.


however, i tried calling gpt-4 version 0314 and gpt-4 version 0125-preview: for both of them, the response object after sending a request only contains gpt-4:
chatcompletion(
    id='chatcmpl-8sln5cbbsdf16s51sdf8yzprxzm1r', 
    choices=[
        choice(
            finish_reason='stop', 
            index=0, 
            logprobs=none, 
            message=chatcompletionmessage(
                content='blahblah', 
                role='assistant', 
                function_call=none, 
                tool_calls=none
            ), 
            content_filter_results={
                'hate': {'filtered': false, 'severity': 'safe'}, 
                'self_harm': {'filtered': false, 'severity': 'safe'}, 
                'sexual': {'filtered': false, 'severity': 'safe'}, 
                'violence': {'filtered': false, 'severity': 'safe'}
            }
        )
    ], 
    created=1708062499, 
    model='gpt-4', 
    object='chat.completion', 
    system_fingerprint='fp_8absdfsdsfs',
    usage=completionusage(
        completion_tokens=185, 
        prompt_tokens=4482, 
        total_tokens=4667
    ), 
    prompt_filter_results=[
        {
            'prompt_index': 0, 
            'content_filter_results': {
                'hate': {'filtered': false, 'severity': 'safe'}, 
                'self_harm': {'filtered': false, 'severity': 'safe'}, 
                'sexual': {'filtered': false, 'severity': 'safe'}, 
                'violence': {'filtered': false, 'severity': 'safe'}
            }
        }
    ]
)

how can i find out the gpt-4 model version when using openai python library and azure openai?","['python', 'version', 'openai-api', 'azure-openai', 'gpt-4']",78738211,"the issue is fixed with newer versions of gpt.
full example:
#note: this code sample was tested with openai python library version 1.35.13 or higher. it does not work with openai==1.8.0 as it is missing the function `.to_json()`.
import json
import pprint
from openai import azureopenai

client = azureopenai(
  azure_endpoint = ""
  api_key='xxxxxxxxxxxxxxxxxxxxx',
  api_version=""2023-07-01-preview""
)

message_text = [{""role"":""system"",""content"":""you are an ai assistant that helps people find information.""}]
completion = client.chat.completions.create(
  model=""gpt-4xxxxxxxx"", 
  messages = message_text,
  temperature=0.7,
  max_tokens=800,
  top_p=0.95,
  frequency_penalty=0,
  presence_penalty=0,
  stop=none
)

print('completion:\n')
print(completion.to_json())

outputs:
{
  ""id"": ""chatcmpl-9jzbinihkrewxh4pizjd3z0h6zaup"",
  ""choices"": [
    {
      ""finish_reason"": ""stop"",
      ""index"": 0,
      ""logprobs"": null,
      ""message"": {
        ""content"": ""sure, i'd be happy to help! what information are you looking for?"",
        ""role"": ""assistant""
      },
      ""content_filter_results"": {
        ""hate"": {
          ""filtered"": false,
          ""severity"": ""safe""
        },
        ""self_harm"": {
          ""filtered"": false,
          ""severity"": ""safe""
        },
        ""sexual"": {
          ""filtered"": false,
          ""severity"": ""safe""
        },
        ""violence"": {
          ""filtered"": false,
          ""severity"": ""safe""
        }
      }
    }
  ],
  ""created"": 1720746994,
  ""model"": ""gpt-4o-2024-05-13"",
  ""object"": ""chat.completion"",
  ""system_fingerprint"": ""fp_abc28019ad"",
  ""usage"": {
    ""completion_tokens"": 15,
    ""prompt_tokens"": 18,
    ""total_tokens"": 33
  },
  ""prompt_filter_results"": [
    {
      ""prompt_index"": 0,
      ""content_filter_results"": {}
    }
  ]
}

notice the ""model"": ""gpt-4o-2024-05-13"" in the output.",https://stackoverflow.com/questions/78005393,python,16-02-2024 05:56,728.0,0.0,1.0,True,12-07-2024 01:21,11-03-2024 09:31
78503492,why is my state not being passed correctly in my langgraph workflow?,"i have a simple series of nodes that are chained together with conditional edges.  the first two are shown here:
def email_router(state: typeddict) -> none:
    node_results, state = get_emails(state)
    if node_results == ""new email"":
        state[""internal_state""] = ""topics_router""
    else:
        state[""errors""] = ""no new mail""
        state[""internal_state""] = ""return_final_status""

def check_internal_state(state: typeddict) -> str:
    logging.debug(f""internal state: {state[""internal_state""]}"")
    return state[""internal_state""]

in the email router, i am setting the state value ""internal_state"" to either ""topics_router"" or ""return_final_status"" which are two other nodes.  the check_internal_state is there simply to enable the conditional edge which looks as follows:
workflow.add_conditional_edges(
    ""email_router"",
    check_internal_state,
    {
        ""topics_router"": ""topics_router"",
        ""return_final_status"": ""return_final_status"",
    },
)

the logging debug statement in the check_internal_state keeps returning an empty string which means that i am not properly carrying the state information across and i cannot figure out what i am doing wrong.
the code is set in classes. the one function compiling the code and saving the output as:
global mailman
mailman = workflow.compile()

i have another function, in the same python file, then invoking mailman and calling functions that all reside in another python file but that are imported.  i only bring this up as maybe separating things into multiple functions is causing the issue?  here is the overall workflow:
workflow = stategraph(graphstate)
workflow.add_node(""email_router"", email_router)
workflow.add_node(""topics_router"", topics_router)
workflow.add_node(""status_router"", status_router)
workflow.add_node(""actions_router"", actions_router)
workflow.add_node(""return_final_status"", return_final_status_node)

workflow.set_entry_point(""email_router"")

workflow.add_conditional_edges(
    ""email_router"",
    check_internal_state,
    {
        ""topics_router"": ""topics_router"",
        ""return_final_status"": ""return_final_status"",
    },
)

workflow.add_conditional_edges(
    ""topics_router"",
    check_internal_state,
    {
        ""status_router"": ""status_router"",
        ""return_final_status"": ""return_final_status"",
    },
)

workflow.add_conditional_edges(
    ""status_router"",
    check_internal_state,
    {
        ""actions_router"": ""actions_router"",
        ""return_final_status"": ""return_final_status"",
    },
)

workflow.add_edge(""actions_router"", ""return_final_status"")
workflow.add_edge(""return_final_status"", end)

does anybody know what i am doing wrong?  thank you!","['python', 'python-3.x', 'langchain', 'langgraph']",78509428,"this ended up being my lack of understanding of how langgraph works and i figure i would explain things to help anybody else out who gets stuck.
langgraph is literally creating a memory object for you called state.  all nodes must return values matching whatever you want to persist from that node into the state object.  for example, you might have ""progress_step"" in your state object.  to keep the value set in your node, you would then return:
return {""progress_step"": progress_step}

the returned json, which can include as many values as you have in your state object, is automatically used to update the global state used across all nodes.
this should not be confused with conditional routers that require a text output matching the name of any other node in your workflow.  i label my nodes with _node and my routing functions with _routing to keep things separate.
i hope that helps anybody else that is confused!",https://stackoverflow.com/questions/78503492,python,19-05-2024 18:05,4272.0,1.0,1.0,True,21-05-2024 02:31,19-05-2024 23:14
75918257,"simple urlsession not functioning in swift, what am i doing wrong?","i'm looking to create a simple cli in swift for working with openai's gpt models. i've tried various approaches. the code below is the simplest version and still not functional -- i believe the closure is not being called.
let endpoint = ""
let headers = [
    ""content-type"": ""application/json"",
    ""authorization"": ""bearer \(oai_key)""
]

let prompt = ""create a simple function in swift that adds two numbers.""

let parameters = [
    ""model"": ""text-curie-001"",
    ""prompt"": prompt,
    ""max_tokens"": 500,
    ""temperature"": 0.5,
    ""n"": 1
] as [string: any]

let urlobject = url(string: endpoint)!

var request = urlrequest(url: urlobject)
request. = ""post""
request.all = headers

do {
    request. = try jsonserialization.data(withjsonobject: parameters)
} catch {
    print(error)
}

func processresponsedata(_ data: data) {
    print(string(data: data, encoding: .utf8) ?? """")
}

let newtask = urlsession.shared.datatask(with: request) {
    data, response, error in
    print(""completion handler closure called"")
    
    if (error != nil) {
        print(""this error occured: \(string(describing: error))"")
    } else {
        print(string(describing: data))
        if let data = data {
            processresponsedata(data)
        }
    }
}

print(newtask.state)
newtask.resume()
print(newtask.state)
print(newtask.response)
print(newtask.error)
print(newtask.progress)

the output in the console from executing this is:
nsurlsessiontaskstate
nsurlsessiontaskstate
nil
nil
<nsprogress: 0x600002c08b80> : parent: 0x0 (portion: 0) / fraction completed: 0.0000 / completed: 0 of 100  
  <nsprogress: 0x600002c08d80> : parent: 0x600002c08b80 (portion: 95) / fraction completed: 0.0000 / completed: 0 of 100  
  <nsprogress: 0x600002c08d00> : parent: 0x600002c08b80 (portion: 5) / fraction completed: 0.0000 / completed: 0 of 100  
program ended with exit code: 0

thanks in advance!","['swift', 'python-requests', 'openai-api', 'urlsession']",75957135,"thanks to @vadian -- a runloop was a great solution.
i ended up cleaning up the code after the runloop got it working and then found a semaphore worked well for my implementation.
here's the code i landed on:
let openai = openaiapi()

func getinput() -> string {
    print(""user:"")
    if let input = readline() {
        return input
    }
    return """"
}

func handlecompletion(responsetext: string?) {
    if let responsetext = responsetext {
        print(""response:\n\(responsetext)"")
    } else {
        print(""error: unable to get response"")
    }
}

while true {
    let prompt = getinput()
    
    if prompt.lowercased() == ""finished"" {
        break
    }
    
    let semaphore = dispatchsemaphore(value: 0)
    
    openai.getcompletion(prompt: prompt) { responsetext in
        handlecompletion(responsetext: responsetext)
        semaphore.signal()
    }
    semaphore.wait()
}

note that i wrapped the previous code i posted in an openaiapi class and added additional functionality to it.",https://stackoverflow.com/questions/75918257,swift,03-04-2023 10:04,188.0,-1.0,1.0,True,18-10-2024 05:28,18-10-2024 05:28
78420498,embedding of llm vs custom embeddings,"i am new to topic of llms (been just 2-3 days) and i've encountered a potential issue in rag pipelines. which assertion is wrong/right?

llm models utilize the most fundamental units of processing as tokens. tokens are created via tokenizers (will be specific to a model)

a token is passed into llm sequentially (from the list of tokens at a time, which also determines the context window)

when ""training"", the ""embeddings"" are randomly initialized. after training , the embedding matrix is created such that there is an embedding for a particular token


now in rag, why is it that we are able to 'customize' our own embedding? i understand this helps with vectorsearch of already stored embeddings, but finally when you send all this to the model, does this ""bypass"" the model's embeddings and starts the inference process as it would? also why don't rag pipelines mention tokenizers often?
went through multiple websites but process is abstracted everywhere
there's a mention of ""we create embeddings"" and then done!","['huggingface-transformers', 'embedding', 'large-language-model', 'huggingface-tokenizers', 'retrieval-augmented-generation']",78608949,"i found the answer....it was a silly little thing. the crux of the matter is , when you're extracting embeddings based on semantic search, the final result(no matter what comes) is ultimately going to be converted into text only, hence the name ""augmented"" generation - essentially a glorified mechanism of enhancing query context as far as this simple application is concerned
so essentially, the two processes (having some custom embedding in vector db) vs embeddings in the model itself, are naturally separate and don't impact each other",https://stackoverflow.com/questions/78420498,huggingface-transformers,02-05-2024 16:24,637.0,-2.0,2.0,True,11-06-2024 17:15,02-05-2024 20:01
71223247,a* search algorithm implementation in python,"i am trying to build a very simple a* search algorithm in python 3. given the following distances  for each node (considering s is the starting node and g the end one)
further_cost = {'s': 11.2, 'a': 9, 'b': 9.3, 'd': 6.2, 'c': 6.7, 'e': 4.2, 'f': 2.1, 'g': 0}

previous_cost = {'s': 0, 'a': 2, 'b': 2.5, 'c': 4, 'd': 3, 'e': 3, 'f': 2.5, 'g': 2}

i want to write a function that finds the best path based on total cost (i.e., f(n) for those familiar with the terminology) for the following search space:
sp = {'s': ['a', 'b'],
      'a': ['b', 'd', 's'],
      'b': ['a', 'e', 'c'],
      'c': ['b'],
      'd': ['s', 'a', 'e'],
      'e': ['d', 'b', 'f'],
      'f': ['e', 'g']}

this is what i have written so far. the issue is that i cannot seem to implement dynamic programming. in other words, i cannot make it so that the algorithm selects the path having the lower f(n).
def extend(queue, state_space):
    current_path = queue[0]
    current_state = current_path[-1]
    extensions = state_space[current_state]
    newpaths = []
    for ext in extensions:
        if not ext in current_path: # check for and don't add loops
            newpaths.append(current_path + ext)
    return newpaths

def a_search(goal, state_space=sp, heuristic=further_cost, a = previous_cost, strategy='best', queue=['s']):




    print (queue)
           
    if not queue:
        print('search failed')
        return false
    elif queue[0][-1] == 'g':
        print('goal found with path {}'.format(queue[0]))
        return true
        
    else:
        
        new_paths = extend(queue, state_space)
        
        cost = {}
        for path in new_paths:
            cost[path] = 0
            for node in path:
                if node == path[-1]:
                    cost[path] += further_cost.get(node) + previous_cost.get(node)
                else:
                    cost[path] += previous_cost.get(node)

            
        if strategy == 'best':
            
            
            #new_queue = sorted(new_paths + queue[1:], key=lambda x: cost[x[-1]])
            new_queue = new_paths + queue[1:]
            new_queue = sorted(new_queue, key=cost.get)

            print(new_queue, cost)
            
        else:
            print('unknown search algorithm')
            return false
        
    a_search(goal, state_space, heuristic, a, strategy, new_queue)   

the function returns the following error:
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
<ipython-input-105-dfe3fabe38db> in <module>
----> 1 a_search(['g'], sp, further_cost, previous_cost, 'best', ['s'])

<ipython-input-104-f872f8359edc> in a_search(goal, state_space, heuristic, a, strategy, queue)
     40             return false
     41 
---> 42     a_search(goal, state_space, heuristic, a, strategy, new_queue)

<ipython-input-104-f872f8359edc> in a_search(goal, state_space, heuristic, a, strategy, queue)
     32             #new_queue = sorted(new_paths + queue[1:], key=lambda x: cost[x[-1]])
     33             new_queue = new_paths + queue[1:]
---> 34             new_queue = sorted(new_queue, key=cost.get)
     35 
     36             print(new_queue, cost)

typeerror: '<' not supported between instances of 'nonetype' and 'float'

i suspect i may be making some conceptual mistake here. ay help would be appreciated!","['python', 'algorithm', 'nlp']",71223410,"apparently you have some values in your cost dictionary that are of type none. perhaps the previous_cost.get call returned none for some early value.
i might suggest a different approach. rather than having separate collections for each piece of data used by a*, you should group them into objects. make a node object that overrides __lt__ and pass in your f and g scores and your parent to its constructor.",https://stackoverflow.com/questions/71223247,python,22-02-2022 14:38,1455.0,1.0,1.0,True,22-02-2022 14:52,22-02-2022 14:52
4470462,associated words,"i am developing a program but stuck on a particular hurdle. i need to find words associated with other words. eg ""green"" might be associated with ""environment"", ""leaf"", ""earth"", ""wind"", ""electric"", ""hybrid"", etc. all i can find is google sets. is there any other resource that is better?",nlp,4483518,"if you have a large text collection (say wikipedia, project gutenberg) you can use co-occurrence scores extract this kind of data. see e.g. padï¿½ï¿½ and lapata and the references therein.
i recently built a tool that mines this kind of associations from wikipedia database dumps by another method. it requires a lot of memory though; other folks have tried to do the same using randomized methods.",https://stackoverflow.com/q/4470462,nlp,17-12-2010 12:31,699.0,2.0,2.0,True,26-03-2022 02:25,17-12-2010 12:33
75709193,how to calculate image similarity of given 2 images by using open ai clip model - which method / ai model is best for calculating image similarity?,"i have prepared a small example code but it is throwing error. can't solve the problem because it is supposed to work.
also do you think are there any better approaches to calculate image similarity? i want to find similar cloth images. e.g. i will give an image of a coat and i want to find similar coats.
also would this code handle all dimensions of images and all types of images?
here the code
import torch
import torchvision.transforms as transforms
import urllib.request
from transformers import clipprocessor, clipmodel, cliptokenizer
from pil import image

# load the clip model
device = ""cuda"" if torch.cuda.is_available() else ""cpu""
model_id = ""openai/clip-vit-base-patch32""
model = clipmodel.from_pretrained(model_id).to(device)

preprocess = clipprocessor.from_pretrained(model_id)


# define a function to load an image and preprocess it for clip
def load_and_preprocess_image(image_path):
    # load the image from the specified path
    image = image.open(image_path)

    # apply the clip preprocessing to the image
    image = preprocess(image).unsqueeze(0).to(device)

    # return the preprocessed image
    return image

# load the two images and preprocess them for clip
image_a = load_and_preprocess_image('/content/a.png')
image_b = load_and_preprocess_image('/content/b.png')

# calculate the embeddings for the images using the clip model
with torch.no_grad():
    embedding_a = model.encode_image(image_a)
    embedding_b = model.encode_image(image_b)

# calculate the cosine similarity between the embeddings
similarity_score = torch.nn.functional.cosine_similarity(embedding_a, embedding_b)

# print the similarity score
print('similarity score:', similarity_score.item())

here the error message
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
[<ipython-input-24-e95a926e1bc8>]( in <module>
     25 
     26 # load the two images and preprocess them for clip
---> 27 image_a = load_and_preprocess_image('/content/a.png')
     28 image_b = load_and_preprocess_image('/content/b.png')
     29 

3 frames
[/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py]( in _call_one(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
   2579 
   2580         if not _is_valid_text_input(text):
-> 2581             raise valueerror(
   2582                 ""text input must of type `str` (single example), `list[str]` (batch or single pretokenized example) ""
   2583                 ""or `list[list[str]]` (batch of pretokenized examples).""

valueerror: text input must of type `str` (single example), `list[str]` (batch or single pretokenized example) or `list[list[str]]` (batch of pretokenized examples)","['python', 'huggingface-transformers', 'clip', 'huggingface']",75709479,"i am not sure why this code is supposed to work since it contains several errors (clipmodel has no encode_image. clipprocessor.__call__'s first argument expects text, the second argument is for images.) please find the corrected code below:
import torch
from transformers import clipimageprocessor, clipmodel, cliptokenizer
from pil import image

# load the clip model
model_id = ""openai/clip-vit-base-patch32""
model = clipmodel.from_pretrained(model_id)

preprocess = clipimageprocessor.from_pretrained(model_id)

# define a function to load an image and preprocess it for clip
def load_and_preprocess_image(image_path):
    # load the image from the specified path
    image = image.open(image_path)

    # apply the clip preprocessing to the image
    image = preprocess(image, return_tensors=""pt"")

    # return the preprocessed image
    return image

# load the two images and preprocess them for clip
image_a = load_and_preprocess_image('/content/bla.png')[""pixel_values""]
image_b = load_and_preprocess_image('/content/bla.png')[""pixel_values""]

# calculate the embeddings for the images using the clip model
with torch.no_grad():
    embedding_a = model.get_image_features(image_a)
    embedding_b = model.get_image_features(image_b)

# calculate the cosine similarity between the embeddings
similarity_score = torch.nn.functional.cosine_similarity(embedding_a, embedding_b)

# print the similarity score
print('similarity score:', similarity_score.item())

output:
similarity score: 1.0000001192092896",https://stackoverflow.com/questions/75709193,python,11-03-2023 20:03,4800.0,2.0,1.0,True,12-03-2023 12:02,12-03-2023 12:02
65279115,how to use &#39;collate_fn&#39; with dataloaders?,"i am trying to train a pretrained roberta model using 3 inputs, 3 input_masks and a label as tensors of my training dataset.
i do this using the following code:
from torch.utils.data import tensordataset, dataloader, randomsampler, sequentialsampler
batch_size = 32
# create the dataloader for our training set.
train_data = tensordataset(train_at, train_bt, train_ct, train_maskat, train_maskbt, train_maskct, labels_traint)
train_dataloader = dataloader(train_data, batch_size=batch_size)

# create the dataloader for our validation set.
validation_data = tensordataset(val_at, val_bt, val_ct, val_maskat, val_maskbt, val_maskct, labels_valt)
val_dataloader = dataloader(validation_data, batch_size=batch_size)

# pytorch training
training_args = trainingarguments(
    output_dir='c:/users/samvd/documents/master/appliedmachinelearning/finalproject/results',          # output directory
    num_train_epochs=1,              # total # of training epochs
    per_device_train_batch_size=32,  # batch size per device during training
    per_device_eval_batch_size=32,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='c:/users/samvd/documents/master/appliedmachinelearning/finalproject/logs',            # directory for storing logs
)

trainer = trainer(
    model=model,                          # the instantiated ï¿½ï¿½ï¿½ï¿½ transformers model to be trained
    args=training_args,                   # training arguments, defined above
    train_dataset = train_data,           # training dataset
    eval_dataset = validation_data,       # evaluation dataset
)

trainer.train()

however this gives me the following error:

typeerror: vars() argument must have dict attribute

now i have found out that it is probably because i don't use collate_fn when using , but i can't really find a source that helps me define this correctly so the trainer understands the different tensors i put in.
can anyone point me in the right direction?","['python', 'pytorch', 'huggingface-transformers', 'dataloader']",65875359,"basically, the collate_fn receives a list of tuples if your __getitem__ function from a dataset subclass returns a tuple, or just a normal list if your dataset subclass returns only one element. its main objective is to create your batch without spending much time implementing it manually. try to see it as a glue that you specify the way examples stick together in a batch. if you donï¿½ï¿½ï¿½t use it, pytorch only put batch_size examples together as you would using torch.stack (not exactly it, but it is simple like that).
suppose for example, you want to create batches of a list of varying dimension tensors. the below code pads sequences with 0 until the maximum sequence size of the batch, that is why we need the collate_fn, because a standard batching algorithm (simply using torch.stack) wonï¿½ï¿½ï¿½t work in this case, and we need to manually pad different sequences with variable length to the same size before creating the batch.
dn(data):
    """"""
       data: is a list of tuples with (example, label, length)
             where 'example' is a tensor of arbitrary shape
             and label/length are scalars
    """"""
    _, labels, lengths = zip(*data)
    max_len = max(lengths)
    n_ftrs = data[0][0].size(1)
    features = torch.zeros((len(data), max_len, n_ftrs))
    labels = torch.tensor(labels)
    lengths = torch.tensor(lengths)

    for i in range(len(data)):
        j, k = data[i][0].size(0), data[i][0].size(1)
        features[i] = torch.cat([data[i][0], torch.zeros((max_len - j, k))])

    return features.float(), labels.long(), lengths.long()

the function above is fed to the collate_fn param in the dataloader, as this example:
dataloader(toy_dataset, collate_fn=collate_fn, batch_size=5)

with this collate_fn function, you always gonna have a tensor where all your examples have the same size. so, when you feed your forward() function with this data, you need to use the length to get the original data back, to not use those meaningless zeros in your computation.
source: pytorch forum",https://stackoverflow.com/questions/65279115,python,13-12-2020 18:23,106394.0,46.0,2.0,True,08-09-2023 12:38,13-12-2020 18:34
66468610,spacy custom tokenizer doesn&#39;t group words,"using spacy,  i'm trying to merge three different tokens into one token.
for example two different tokens ""bell"" ""peper"" into one token ""bell pepper"" by the code below.(i don't think my code is a right approach though)
text='extra-virgin olive oil bell pepper parmesan cheese onion red bell pepper carrots cloves, beans fennel bulb parsnips vegetable broth broccoli florets macaroni parmesan cheese'


import spacy
nlp = spacy.load('en_core_web_sm')
nlp.tokenizer.add_special_case('connection',
    [
        {   'pos': 'noun',
            'f': '-',
            'f': 'oil'},
        {
            'f':'bell',
            'f':'pepper'
        },
        {
            'f':'olive',
            'f':'oil'
        },
        {
            'f':'parmesan',
            'f':'cheese'
        }
    ])

doc= nlp(text)

doc[0]

extra

how do i achieve this?
thank you","['python', 'nlp', 'spacy']",66469649,"so it looks like what you want to do is merge some phrases, like ""olive oil"" or ""bell pepper"", into single tokens. this is usually not something you'd do with the tokenizer exceptions - those are generally more useful for splitting words or dealing with idiosyncratic punctuation. for example, you might want to tokenize ""gimme"" as ""gim me"" (so that ""me"" can be recognized) or to have ""2km"" and ""2 km"" both be two tokens.
in this case i would make a list of all the phrases you want to make into a single token and use the entityruler to assign an entity label to them. this assumes you have a list of the things you want to merge.
if you don't have a list of things you want to make into phrases, given your example text, this is going to be hard because there's no general principle like part of speech patterns behind the merges you're making. spacy models are trained on natural language text, while you seem to just have an unpunctuated list of ingredients, so the part of speech tagger isn't always going to work very well. for example, consider these sentences:

i went to the store and bought olive oil bell peppers and cake mix.

this is not properly punctuated, but it's obviously a list. if it were properly punctuated, spacy's noun_chunks would give you what you want.
the issue is that this is also a valid sentence:

i made olive oil bell pepper pasta for dinner.

this is somewhat awkward but properly punctuated, and in this case ""olive oil bell pepper"" is a modifier of ""pasta"" and not a list of separate items, so it would correctly be a single noun chunk.",https://stackoverflow.com/questions/66468610,python,04-03-2021 04:29,133.0,1.0,1.0,True,04-03-2021 06:22,04-03-2021 05:20
69686930,confidence score of predicted ner entities using spacy,"i am trying to predict entities using a custom trained ner model using spacy. i read  that confidence scores of each entity can be obtained using spancat. but i have a little confusion regarding to make that work. according to my understanding, we have to train a pipeline using spancat component. so while training, within the config file there is a segment,
[nlp]
lang = ""en""
pipeline = [""tok2vec"",""ner""]
batch_size = 1000

should we have to change this to
[nlp]
lang = ""en""
pipeline = [""tok2vec"",""ner"",""spancat""]
batch_size = 1000

for the spancat to work.
then after training, while predicting the entities from unknown text, should we have to use
doc = nlp(data_to_be_predicted)
spans = doc.spans[""spancat""] # spangroup
print(spans.attrs[""scores""]) # list of numbers, span length as spangroup

to get the confidence scores.
i am using spacy 3.1.3. i believe according to the documentation, this feature is rolled out by now.","['python', 'nlp', 'spacy', 'named-entity-recognition']",69693721,"i'm not really sure there's a question in your post, but yes, the spancat is available and you can get entity scores from it.
the spancat is a different component from the ner component. so if you do this:
pipeline = [""tok2vec"",""ner"",""spancat""]

the spancat will not add scores for things your ner component predicted. you probably want to remove the ner component.

about usage, please see the docs and the example project. this is how you get the score:
doc = nlp(text)
span_group = doc.spans[""spans""] # default key, can be changed
scores = span_group.attrs[""scores""]

# note that `scores` is an array with one score for each span in the group
for span, score in zip(span_group, scores):
    print(score, span)",https://stackoverflow.com/questions/69686930,python,23-10-2021 09:42,3536.0,1.0,1.0,True,24-10-2021 09:50,23-10-2021 23:29
66109084,how to convert huggingface&#39;s seq2seq models to onnx format,"i am trying to convert the pegasus newsroom in huggingface's transformers model to the onnx format. i followed this guide published by huggingface. after installing the prereqs, i ran this code:
!rm -rf onnx/
from pathlib import path
from transformers.convert_graph_to_onnx import convert

convert(framework=""pt"", model=""google/pegasus-newsroom"", output=path(""onnx/google/pegasus-newsroom.onnx""), opset=11)


and got these errors:
valueerror                                traceback (most recent call last)
<ipython-input-9-3b37ed1ceda5> in <module>()
      3 from transformers.convert_graph_to_onnx import convert
      4 
----> 5 convert(framework=""pt"", model=""google/pegasus-newsroom"", output=path(""onnx/google/pegasus-newsroom.onnx""), opset=11)
      6 
      7 

6 frames
/usr/local/lib/python3.6/dist-packages/transformers/models/pegasus/modeling_pegasus.py in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, encoder_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)
    938             input_shape = inputs_embeds.size()[:-1]
    939         else:
--> 940             raise valueerror(""you have to specify either decoder_input_ids or decoder_inputs_embeds"")
    941 
    942         # past_key_values_length

valueerror: you have to specify either decoder_input_ids or decoder_inputs_embeds


i have never seen this error before. any ideas?","['python', 'tensorflow', 'pytorch', 'huggingface-transformers', 'onnx']",66117248,"pegasus is a seq2seq model, you can't directly convert a seq2seq model (encoder-decoder model) using this method. the guide is for bert which is an encoder model. any only encoder or only decoder transformer model can be converted using this method.
to convert a seq2seq model (encoder-decoder) you have to split them and convert them separately, an encoder to onnx and a decoder to onnx. you can follow this guide (it was done for t5 which is also a seq2seq model)
why are you getting this error?
while converting pytorch to onnx
_ = torch.onnx._export(
                        model,
                        dummy_input,
                        ...
                       )

you need to provide a dummy variable to both encoder and to the decoder separately. by default when converting using this method it provides the encoder the dummy variable. since this method of conversion didn't accept decoder of this seq2seq model, it won't give a dummy variable to the decoder and you get the above error.
valueerror: you have to specify either decoder_input_ids or decoder_inputs_embeds",https://stackoverflow.com/questions/66109084,python,08-02-2021 20:44,6981.0,8.0,2.0,True,02-01-2023 16:29,14-02-2021 18:53
54905774,why softmax get small gradient when the value is large in paper &#39;attention is all you need&#39;,"this is the screen of the original paper: the screen of the paper. i understand the meaning of the paper is that when the value of dot-product is large, the gradient of softmax will get very small.
however, i tried to calculate the gradient of softmax with the cross entropy loss and found that the gradient of softmax is not directly related to value passed to softmax. even the single value is large, it still can get a large gradient when ather values are large. (sorry about that i don't know how to pose the calculation process here)","['deep-learning', 'nlp', 'softmax', 'attention-model']",54906031,"actually the gradient of cross entropy with softmax on a one hot encoding vector is just grad -log(softmax(x)) = (1 - softmax(x)) at the index of the vector of the corresponding class. ( if the value passed to the softmax is large, the softmax will produce 1 and therefore produce 0 gradient.",https://stackoverflow.com/questions/54905774,deep-learning,27-02-2019 12:42,2935.0,7.0,3.0,True,30-06-2024 23:55,20-06-2020 09:12
67335480,r language : check if two columns containing text are highly correlated,"in r, we can use the cor function to get the correlation between two columns but it doesn't work for non-numerical values.
i ask this because i need to preprocess some data, and i suspect 2 columns being very similar because by looking, i found that when the first column says ""a"", the second column always says ""b"" but i want to be sure that, indeed, if i know the value in the first column, i can deduce the value in the second.
if i'm not clear here's an exemple to illustrate.
dataframe <- read.csv(file = 'data/company_product.csv')

where data/company_product.csv is a table like so
company name     main product       rest of the data    ...
by apple         a phone            some_other_data     ...
by apple         a phone            some_other_data     ...
by microsoft     a computer         some_other_data     ...
by nokia         a tablet           some_other_data     ...
by nokia         a tablet           some_other_data     ...
by nokia         a tablet           some_other_data     ...
...              ...                ...

as you can see in this file, the column main product is useless because if i know the column company name is ""by apple"", the main product will always be ""a phone"".
this means the column company name is highly correlated to the column main product, but i do not find a simple way in r to show that
i'm not sure if the answer will be extremely trivial, or if it is a key problem in text mining, but i do not need precise correlation, all i want is a yes/no for ""every time a value appear in first column, it will always be the same value in the second column""
thanks","['r', 'text-mining']",67335566,"use table to assess this:
table(df[, 1:2])

giving the following which shows only one non-zero in each row and in each column showing by apple is associated with a phone, by microsoft is associated with a computer and by nokia is associated with a tablet.
              second
first          a computer a phone a tablet
  by apple              0       2        0
  by microsoft          1       0        0
  by nokia              0       0        2

or simply count the number of times each unique row appears:
aggregate(list(count = df[[1]]), df, length)
##          first     second count
## 1 by microsoft a computer     1
## 2     by apple    a phone     2
## 3     by nokia   a tablet     2

or
library(dplyr)
count(df, first, second)
##          first     second n
## 1     by apple    a phone 2
## 2 by microsoft a computer 1
## 3     by nokia   a tablet 2

or if you don't care about the count just look at the unique rows:
unique(df[, 1:2])
##          first     second
## 1     by apple    a phone
## 2 by microsoft a computer
## 4     by nokia   a tablet

visualize this as follows:
library(igraph)
g <- graph_from_incidence_matrix(table(df[, 1:2]))
plot(g, layout = layout.bipartite)",https://stackoverflow.com/questions/67335480,r,30-04-2021 14:15,277.0,1.0,2.0,True,30-04-2021 19:31,30-04-2021 14:32
76638928,"regex to detect words based on the words action, object, sumbject, etc in the middle of a text","i have the following text and i would like to detect the words after the subject, action and capabilities using regular expressions:
for this text:
t1  subject num num xxx
t2  action num num  xxx
a1  capability t2 xxx

i have created the following regex but it's not correct:
# regular expressions for pattern matching
action_pattern = r'^t\d+\taction \d+ \d+\t(.+)$'
subject_pattern = r'^t\d+\tsubject \d+ \d+;?\d+? \d+\t(.+)$'
object_pattern = r'^t\d+\tobject \d+ \d+;?\d+? \d+\t(.+)$'
capability_pattern = r'^a\d+\tcapability t\d+ (.+)$'","['python', 'regex', 'nlp']",76639761,"here is what i've come up with:
text = """"""
for this text:
t1  subject 11096 11100 they
t2  action 11101 11106  steal
a1  capability t2 007:malwarecapability-data_theft
t3  object 11107 11111  data
r1  subjaction subject:t1 action:t2 
r2  actionobj action:t2 object:t3   
t4  subject 11127 11132;11140 11148 their implants
t5  action 11152 11156  send
a2  capability t5 006:malwarecapability-data_exfiltration
t6  object 11157 11161  data
t7  modifier 11162 11168    out of
t8  object 11169 11180  the network
t9  modifier 11181 11186    using
t10 object 11187 11195;11203 11224  a victim network`s mail server
""""""
strings = text.split('\n')

action_pattern = r'action\s[\s;\d]+(.*)$'
subject_pattern = r'subject\s[\s;\d]+(.*)$'
object_pattern = r'object\s[\s;\d:]+(.*)$'
capability_pattern = r'capability\s+t[\s\d:]+(.*)$'

def extract(pattern, strings_lst):
    return [re.search(pattern, string).group(1) 
            for string in strings_lst if re.search(pattern, string)]


print(extract(action_pattern, strings))
print(extract(subject_pattern, strings))
print(extract(object_pattern, strings))
print(extract(capability_pattern, strings))

output
['steal', 'send']
['they', 'their implants']
['data', 'data', 'the network', 'a victim network`s mail server']
['malwarecapability-data_theft', 'malwarecapability-data_exfiltration']

you normally don't want to use such congested list comprehensions as in my functions, but for the sake of demonstration and shorter code i did this blasphemy.
edit: simplified regexes",https://stackoverflow.com/questions/76638928,python,07-07-2023 17:05,59.0,0.0,1.0,True,19-09-2023 14:34,19-09-2023 14:34
37106194,how do i calculate tf-idf of a query?,"how do i calculate tf-idf for a query? i understand how to calculate tf-idf for a set of documents with following definitions:

tf = occurances in document/ total words in document
idf = log(#documents / #documents where term occurs

but i don't understand how that correlates to queries.

for example, i read a resource that stated the values of a query ""life learning""

life | tf = .5 | idf =    1.405507153 | tf_idf = 0.702753576
  learning | tf = .5 | idf =    1.405507153 | tf_idf = 0.702753576

the tf values i understand, each term appears only once out of the two possible terms, thus 1/2, but i have no idea where the idf comes from.
i would think that #documents = 1 and occurrence = 1, log(1) = 0, so idf would be 0, but this doesn't seem to be the case. is it based on whatever documents you're using? how do you calculate tf-idf for a query?","['search', 'computer-science', 'tf-idf', 'data-retrieval']",39009806,"only tf(life) depends on the query itself. however, the idf of a query depends on the background documents, so idf(life) = 1+ ln(3/2) ~= 1.405507153. 
that is why tf-idf is defined as multiplying a local component (term frequency) with a global component (inverse document frequency).",https://stackoverflow.com/questions/37106194,search,09-05-2016 00:13,17777.0,18.0,3.0,True,10-04-2021 09:17,18-12-2016 09:29
67976977,use bert under spacy to get sentence embeddings,"i am trying to use bert to get sentence embeddings. here is how i am doing it:
import spacy
nlp = spacy.load(""en_core_web_trf"")
nlp(""the quick brown fox jumps over the lazy dog"").vector 

this outputs an empty vector!!
array([], dtype=float32)

am i missing something?","['python', 'nlp', 'spacy', 'bert-language-model']",67977508,"transformers are a bit different than the other spacy models, but you can use
doc._.trf_data.tensors[1].
the vectors for the individual bpe (byte pair encoding) token-pieces are in doc._.trf_data.tensors[0]. note that i use the term token-pieces  rather than tokens, to prevent confusion between spacy tokens and the tokens that are produced by the bpe tokenizer.
e.g., in our case the spacy-tokens are:
for i, spacy_tok in enumerate(doc):
  print(f""spacy-token {i + 1}: {spacy_tok.text}"")

spacy-token 1: the
spacy-token 2: quick
spacy-token 3: brown
spacy-token 4: fox
spacy-token 5: jumps
spacy-token 6: over
spacy-token 7: the
spacy-token 8: lazy
spacy-token 9: dog

and the token-pieces are:
for i, tok_piece in enumerate(doc._.trf_data.tokens['input_texts'][0]):
  print(f""token-piece {i + 1}: {tok_piece}"")

token-piece 1: <s>
token-piece 2: the
token-piece 3: ï¿½ï¿½quick
token-piece 4: ï¿½ï¿½brown
token-piece 5: ï¿½ï¿½fox
token-piece 6: ï¿½ï¿½jumps
token-piece 7: ï¿½ï¿½over
token-piece 8: ï¿½ï¿½the
token-piece 9: ï¿½ï¿½lazy
token-piece 10: ï¿½ï¿½dog
token-pie",https://stackoverflow.com/questions/67976977,python,14-06-2021 20:42,2627.0,7.0,1.0,True,14-06-2021 22:00,14-06-2021 22:00
56392852,nlp - how to add more features?,"i want to use a sklearn classifier to train a model to classify data entries (yes,no) using a text feature (content), a numerical feature (population) and a categorical feature (location). 

the model below is using only the text data to classify each entry. the text is converted with tf-idf into a sparse matrix before being imported into the classifier.
is there a way to add/use also the other features? these features are not in sparse matrix format so not sure how to combine them with the text sparse matrix. 

    #import libraries
    import string, re, nltk
    import pandas as pd
    from pandas import series, dataframe
    from nltk.corpus import stopwords
    from nltk.stem.porter import porterstemmer
    from sklearn.feature_extraction.text import countvectorizer
    from sklearn.feature_extraction.text import tfidftransformer
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report
    from sklearn.pipeline import pipeline

    # read data and remove empty lines
    dataset = pd.read_csv('sample_data.txt',
                           sep='\t',
                           names=['content','location','population','target'])
                           .dropna(how='all')
                           .dropna(subset=['target'])

    df = dataset[1:]

    #reset dataframe index
    df.reset_index(inplace = true)

    #add an extra column which is the length of text
    df['length'] = df['content'].apply(len)

    #create a dataframe that contains only two columns the text and the target class
    df_cont = df.copy()
    df_cont = df_cont.drop(
        ['location','population','length'],axis = 1)

    # function that takes in a string of text, removes all punctuation, stopwords and returns a list of cleaned text

    def text_process(mess):
        # lower case for string
        mess = mess.lower()

        # check characters and removes urls
       nourl = re.sub(r' ' ', mess)

        # check characters and removes punctuation
        nopunc = [char for char in nourl if char not in string.punctuation]

        # join the characters again to form the string and removes numbers
        nopunc = ''.join([i for i in nopunc if not i.isdigit()])

        # remove stopwords
        return [ps.stem(word) for word in nopunc.split() if word not in set(stopwords.words('english'))]

    #split the data in train and test set and train/test the model

    cont_train, cont_test, target_train, target_test = train_test_split(df_cont['content'],df_cont['target'],test_size = 0.2,shuffle = true, random_state = 1)


    pipeline = pipeline([('bag_of_words',countvectorizer(analyzer=text_process)),
                         ('tfidf',tfidftransformer()),
                         ('classifier',multinomialnb())])

    pipeline.fit(cont_train,target_train)
    predictions = pipeline.predict(cont_test)

    print(classification_report(predictions,target_test))


the model is expected to return the following: accuracy, precision, recall ,f1-score","['python', 'machine-learning', 'scikit-learn', 'nlp', 'tf-idf']",56398033,"i believe you need to use one-hot vectoring for the 'location' feature. 
one-hot vectors for the given data would be,
london - 100
manchester - 010
edinburg - 001
vector length is the number of cities you have in there. note that each bit here would be a feature.
categorical data is usually converted to one-hot vectors before feeding to a machine learning algorithm.
once this is done you can concat the whole row into a 1d array and then feed that to the classifier.",https://stackoverflow.com/questions/56392852,python,31-05-2019 09:59,3211.0,2.0,4.0,True,08-07-2021 19:54,31-05-2019 11:17
73334654,what is better custom training the bert model or use the model with pretrained data?,"i am coding my own models for a time but i saw huggingface and started using it. i wanted to know whether i should use the pretrained model or train model (the same hugging face model) with my own dataset. i am trying to make a question answering model.
i have dataset of 10k-20k questions.","['python', 'nlp', 'huggingface-transformers', 'nlp-question-answering']",73334712,"the state-of-the-art approach is to take a pre-trained model that was pre-trained on tasks that are relevant to your problem and fine-tune the model on your dataset.
so assuming you have your dataset in english, you should take a pre-trained model on natural language english. you can then fine-tune it.
this will most likely work better than training from scratch, but you can experiment on your own. you can also load a model without the pre-trained weights in huggingface.",https://stackoverflow.com/questions/73334654,python,12-08-2022 13:05,640.0,3.0,1.0,True,12-08-2022 13:44,12-08-2022 13:44
61239331,how can i show multiple predictions of the next word in a sentence?,"i am using the gpt-2 pre trained model. the code i am working on will get a sentence and generate the next word for that sentence. i want to print multiple predictions, like the three first predictions with best probabilities! 
for example if i put in the sentence ""i's an interesting ...."" 
predictions:  ""books""    ""story""    ""news""
is there a way i can modify this code to show these predictions instead of one?! 
also there are two parts in the code, i do not understand, what is the meaning of the numbers in  (predictions[0, -1, :])?  and why do we put [0] after predictions = output[0]? 

import torch
from pytorch_transformers import gpt2tokenizer, gpt2lmheadmodel

# load pre-trained model tokenizer (vocabulary)
tokenizer = gpt2tokenizer.from_pretrained('gpt2')

# encode a text inputs
text = ""the fastest car in the  ""
indexed_tokens = tokenizer.encode(text)


# convert indexed tokens in a pytorch tensor
tokens_tensor = torch.tensor([indexed_tokens])


# load pre-trained model (weights)
model = gpt2lmheadmodel.from_pretrained('gpt2')

# set the model in evaluation mode to deactivate the dropout modules
model.eval()

# if you have a gpu, put everything on cuda
#tokens_tensor = tokens_tensor.to('cuda')
#model.to('cuda')

# predict all tokens
with torch.no_grad():
    outputs = model(tokens_tensor)
    predictions = outputs[0]
    #print(predictions)


# get the predicted next sub-word
predicted_index = torch.argmax(predictions[0, -1, :]).item()
predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])

# print the predicted word
#print(predicted_index)
print(predicted_text)

the result for the above code will be :
the fastest car in the world.","['python', 'tensorflow', 'machine-learning', 'nlp', 'pytorch']",61239498,"you can use torch.topk as follows:
predicted_indices = [x.item() for x in torch.topk(predictions[0, -1, :],k=3)]",https://stackoverflow.com/questions/61239331,python,15-04-2020 21:57,1326.0,4.0,1.0,True,26-01-2021 01:00,15-04-2020 22:42
56304109,how to train a spacy model with line number as a feature?,"i'm a newbie to nlp and spacy and i'm working on a project for extracting person and company names from business cards.
in order to extract text i am using a decent ocr function that i've made which gives me something like this:
sunny j. mistry
product design engineer

apple
5 infinite loop, ms 305-1ph
cupertino, ca 95014

t 408 974-5339
m 925 548-4585
sjmistry@apple.com


at first i was trying process line by line using the default english ner for the job and soon realized that it's not enough.  
eventually i've decided to create my own custom ner that will be trained with information about the position of text.
i haven't found any information in the official documentation on how to add custom features for the training data like line numbers, but i've found this answer and example of matthew honnibal which suggested to use a multi-task objective in order to train a model with a costume feature.
i'm still not sure:

how the training data should look like?
how do i use spacy's api to add a custom feature to the training process?
is multi-task objective the right tool to train this kind of model?","['python', 'machine-learning', 'nlp', 'spacy', 'named-entity-recognition']",56851427,"answering my own question:
i didn't find an official way for implementing this kind of task, but in the end i decided on training a model on a normal business card data set containing 200 images. i've extracted the text from each image using google ocr and annotated it using a tool described in this post.
it worked like a charm.",https://stackoverflow.com/questions/56304109,python,25-05-2019 10:47,558.0,4.0,2.0,True,02-06-2021 15:03,25-05-2019 11:46
77074676,importerror: cannot import name &#39;deprecated&#39; from &#39;typing_extensions&#39;,"i want to download spacy, but the version of typing-extensions is lowered in the terminal:
error: pydantic 2.3.0 has requirement typing-extensions>=4.6.1, but you'll have typing-extensions 4.4.0 which is incompatible.
error: pydantic-core 2.6.3 has requirement typing-extensions!=4.7.0,>=4.6.0, but you'll have typing-extensions 4.4.0 which is incompatible.
installing collected packages: typing-extensions
  attempting uninstall: typing-extensions
    found existing installation: typing-extensions 4.7.1
    uninstalling typing-extensions-4.7.1:
      successfully uninstalled typing-extensions-4.7.1
successfully installed typing-extensions-4.4.0

next i want to install the language pack python -m spacy download en, but another error occursï¿½ï¿½ï¿½
(base) e:\anaconda>python -m spacy download en
traceback (most recent call last):
  file ""e:\anaconda\lib\site-packages\confection\__init__.py"", line 38, in <module>
    from pydantimport basemodel, extra, validationerror, create_model
  file ""e:\anaconda\lib\site-packages\pydantic\__init__.py"", line 13, in <module>
    from . import dataclasses
  file ""e:\anaconda\lib\site-packages\pydantic\dataclasses.py"", line 11, in <module>
    from ._internal import _config, _decorators, _typing_extra
  file ""e:\anaconda\lib\site-packages\pydantic\_internal\_config.py"", line 9, in <module>
    from ..config import configdict, extravalues, jsonencoder, jsonschemaextracallable
  file ""e:\anaconda\lib\site-packages\pydantic\config.py"", line 9, in <module>
    from .deprecated.config import baseconfig
  file ""e:\anaconda\lib\site-packages\pydantic\deprecated\config.py"", line 6, in <module>
    from typing_extensions import literal, deprecated
importerror: cannot import name 'deprecated' from 'typing_extensions' (e:\anaconda\lib\site-packages\typing_extensions.py)

my current python version is 3.7, should i update it? or is there any better solution? i'm a newbie in this area, thank you allï¿½ï¿½ï¿","['python', 'pip', 'nlp', 'spacy', 'python-typing']",77075595,"you should use typing_extensions==4.7.1
try :
pip install typing_extensions==4.7.1 --upgrade

i also suggest you to upgrade your python version from 3.7 to 3.10 or 3.11
see a relevant answer:",https://stackoverflow.com/questions/77074676,python,10-09-2023 02:58,59506.0,17.0,4.0,True,27-01-2025 13:55,25-10-2024 19:42
75625675,openai chat completions api error 400: &quot;&#39;user&#39; is not of type &#39;object&#39;&quot;,"i share with you my code below to get a response from a post request with r from the openai chat completions api:
param <- list(model = ""gpt-3.5-turbo"",
              messages = c(""role"" = ""user"", 
                           ""content"" = ""hello""))

result <- post(""
               body = param,
               add_headers(authorization=openai_secret_key),
               encode = ""json"")

here is the result :

response [
date: 2023-03-02 16:28
status: 400
content-type: application/json
size: 158 b
{
ï¿½ï¿½ï¿½errorï¿½ï¿½ï¿½: {
ï¿½ï¿½ï¿½messageï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½userï¿½ï¿½ï¿½ is not of type ï¿½ï¿½ï¿½objectï¿½ï¿½ï¿½ - ï¿½ï¿½ï¿½messages.0ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½typeï¿½ï¿½ï¿½: ï¿½ï¿½ï¿½invalid_request_errorï¿½ï¿½ï¿½,
ï¿½ï¿½ï¿½paramï¿½ï¿½ï¿½: null,
ï¿½ï¿½ï¿½codeï¿½ï¿½ï¿½: null
}
}

{
   ""model"":""gpt-3.5-turbo"",
   ""messages"":[
      {
         ""role"":""user"",
         ""content"":""hello!""
      }
   ]
}","['r', 'openai-api', 'chatgpt-api']",75626044,"if you run test.r the openai api will return the following completion:

[1] ""\n\nhello! how may i assist you today?""

test.r
library(httr)
library(jsonlite)

openai_api_key <- ""sk-xxxxxxxxxxxxxxxxxxxx""

param <- list(model = ""gpt-3.5-turbo"",
              messages = list(list(role = ""user"", content = ""hello""))
         )
    
result <- post(""
               body = param,
               add_headers(""authorization"" = paste(""bearer"", openai_api_key)),
               encode = ""json"")

response_content <- fromjson(rawtochar(result$content))
print(response_content$choices[[1]]$content)",https://stackoverflow.com/questions/75625675,r,03-03-2023 10:07,6801.0,0.0,1.0,True,12-06-2024 17:23,12-06-2024 17:23
77901576,python: nltk: nlp to sql where clause,"i have a python script that loads an sql parser and tries to convert a query string into a where clause. i am having problems with parsing my string into a query. how do i load a regex pattern as grammar?
below is my python code:
import nltk    
from nltk.tokenize import word_tokenize
from nltk import load_parser




nltk.data.show_cfg('./my_sql0.fcfg')    
cp = load_parser('./my_sql0.fcfg')
query = 'find a red car with license plate matching amc'
trees = list(cp.parse(query.split()))
print(trees)
for tree in trees:
    print(tree)

~  

any help is appreciated.","['python-3.x', 'nltk']",77907776,"i figured out how to use the regexp_tagger in tagging my nnp text to my regex pattern. i did some list manipulation in appending and removing, but the idea is to tag a text with regex pattern. this solved my problem of using a regex patter in my code. see the code below:
regexp_tagger = nltk.regexptagger(
        [
                (r""^[a-za-z0-9]+(?:[a-za-z0-9]+)*"", ""aln"")
        ])
tagged_text = regexp_tagger.tag(mysentence.split())",https://stackoverflow.com/questions/77901576,python-3.x,29-01-2024 17:30,98.0,-1.0,1.0,True,30-01-2024 16:33,30-01-2024 16:33
70680290,indexerror: target is out of bounds,"i am currently trying to replicate the article

to get an introduction to pytorch and bert.
i used some own sample corpus and corresponding tragets as practise, but the code throws the following:
---------------------------------------------------------------------------
indexerror                                traceback (most recent call last)
<ipython-input-4-8577755f37de> in <module>()
    201 lr = 1e-6
    202 
--> 203 trainer(model, df_train, df_val, lr, epochs)

3 frames
<ipython-input-4-8577755f37de> in trainer(model, train_data, val_data, learning_rate, epochs)
    162                 output = model(input_id, mask)
    163 
--> 164                 batch_loss = criterion(output, torch.max(train_label,1)[1])
    165                 total_loss_train += batch_loss.item()
    166 

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1100         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1101                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1102             return forward_call(*input, **kwargs)
   1103         # do not call functions when jit is used
   1104         full_backward_hooks, non_full_backward_hooks = [], []

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py in forward(self, input, target)
   1150         return f.cross_entropy(input, target, weight=self.weight,
   1151                                ignore_index=self.ignore_index, reduction=self.reduction,
-> 1152                                label_smoothing=self.label_smoothing)
   1153 
   1154 

/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)
   2844     if size_average is not none or reduce is not none:
   2845         reduction = _reduction.legacy_get_string(size_average, reduce)
-> 2846     return torch._c._nn.cross_entropy_loss(input, target, weight, _reduction.get_enum(reduction), ignore_index, label_smoothing)
   2847 
   2848 

indexerror: target 32 is out of bounds.

the code is mostly identical to the one in the article, except of course the more extensive lable-dict.
orginial:
labels = {'business':0,
          'entertainment':1,
          'sport':2,
          'tech':3,
          'politics':4
          }

mine:
labels = 
{'macroeconomics': 0,
 'microeconomics': 1,
 'labor economics': 2,
 'subnational fiscal issues': 3,
 'econometrics': 4,
 'international economics': 5,
 'financial economics': 6,
 'health, education, and welfare': 7,
 'public economics': 8,
 'development and growth': 9,
 'industrial organization': 10,
 'other': 11,
 'environmental and resource economics': 12,
 'history': 13,
 'regional and urban economics': 14,
 'development economics': 15,
 'corporate finance': 16,
 'children': 17,
 'labor studies': 18,
 'economic fluctuations and growth': 19,
 'economics of aging': 20,
 'economics of education': 21,
 'international trade and investment': 22,
 'asset pricing': 23,
 'health economics': 24,
 'law and economics': 25,
 'international finance and macroeconomics': 26,
 'monetary economics': 27,
 'technical working papers': 28,
 'political economy': 29,
 'development of the american economy': 30,
 'health care': 31,
 'productivity, innovation, and entrepreneurship': 32}

code:
class dataset(torch.utils.data.dataset):

    def __init__(self, df):

        self.labels = torch.longtensor([labels[label] for label in df[""category""]])
        self.texts = [tokenizer(text, 
                               padding='max_length', max_length = 512, truncation=true,
                                return_tensors=""pt"") for text in df['text']]

    def classes(self):
        return self.labels

    def __len__(self):
        return len(self.labels)

    def get_batch_labels(self, idx):
        # fetch a batch of labels
        return np.array(self.labels[idx])

    def get_batch_texts(self, idx):
        # fetch a batch of inputs
        return self.texts[idx]

    def __getitem__(self, idx):
        batch_texts = self.get_batch_texts(idx)
        batch_y = np.array(range(0,len(labels)))

        return batch_texts, batch_y
    
#splitting the sample into trainingset, validationset and testset (80,10,10)
np.random.seed(112)
df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), 
                                     [int(.8*len(df)), int(.9*len(df))])

print(len(df_train),len(df_val), len(df_test))


from torch import nn

class bertclassifier(nn.module):

    def __init__(self, dropout=0.5):

        super(bertclassifier, self).__init__()

        self.bert = bertmodel.from_pretrained('bert-base-cased')
        self.dropout = nn.dropout(dropout)
        self.linear = nn.linear(768, 5)
        self.relu = nn.relu()

    def forward(self, input_id, mask):

        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=false)
        dropout_output = self.dropout(pooled_output)
        linear_output = self.linear(dropout_output)
        final_layer = self.relu(linear_output)

        return final_layer
    
from torch.optim import adam
from tqdm import tqdm

def trainer(model, train_data, val_data, learning_rate, epochs):

    train, val = dataset(train_data), dataset(val_data)
    
    train_dataloader = torch.utils.data.dataloader(train, batch_size=2, shuffle=true)
    val_dataloader = torch.utils.data.dataloader(val, batch_size=2)

    use_cuda = torch.cuda.is_available()
    device = torch.device(""cuda"" if use_cuda else ""cpu"")

    criterion = nn.crossentropyloss()
    optimizer = adam(model.parameters(), lr= learning_rate)

    if use_cuda:

            model = model.cuda()
            criterion = criterion.cuda()

    for epoch_num in range(epochs):

            total_acc_train = 0
            total_loss_train = 0

            for train_input, train_label in tqdm(train_dataloader):

                train_label = train_label.to(device)
                mask = train_input['attention_mask'].to(device)
                input_id = train_input['input_ids'].squeeze(1).to(device)

                output = model(input_id, mask)
                
                batch_loss = criterion(output, torch.max(train_label,1)[1])
                total_loss_train += batch_loss.item()
                
                acc = (output.argmax(dim=1) == train_label).sum().item()
                total_acc_train += acc

                model.zero_grad()
                batch_loss.backward()
                optimizer.step()
            
            total_acc_val = 0
            total_loss_val = 0

            with torch.no_grad():

                for val_input, val_label in val_dataloader:

                    val_label = val_label.to(device)
                    mask = val_input['attention_mask'].to(device)
                    input_id = val_input['input_ids'].squeeze(1).to(device)

                    output = model(input_id, mask)

                    batch_loss = criterion(output, val_label)
                    total_loss_val += batch_loss.item()
                    
                    acc = (output.argmax(dim=1) == val_label).sum().item()
                    total_acc_val += acc
            
            print(
                f'epochs: {epoch_num + 1} | train loss: {total_loss_train / len(train_data): .3f} \
                | train accuracy: {total_acc_train / len(train_data): .3f} \
                | val loss: {total_loss_val / len(val_data): .3f} \
                | val accuracy: {total_acc_val / len(val_data): .3f}')
                  
epochs = 5
model = bertclassifier()
lr = 1e-6
              
trainer(model, df_train, df_val, lr, epochs)","['python', 'pytorch', 'text-classification', 'bert-language-model']",70682744,"you're creating a list of length 33 in your __getitem__ call which is one more than the length of the labels list, hence the out of bounds error. in fact, you create the same list each time this method is called. you're supposed to fetch the associated y with the x found at idx.
if you replace batch_y = np.array(range(...)) with batch_y = np.array(self.labels[idx]), you'll fix your error. indeed, this is already implemented in your get_batch_labels method.",https://stackoverflow.com/questions/70680290,python,12-01-2022 10:53,17137.0,0.0,1.0,True,12-01-2022 14:00,12-01-2022 13:49
73017872,i get an nameerror although i defined my variable,"hello my programmer friends... i'm doing my first nlp project that counts and shows 5 documents tfidf. here's part of the code:
def idf(corpus , unique_words):
    idf_dict = {}
    n = len(corpus)
    for i in unique_words:
        count = 0
        for sen in corpus:
            if i in sen.split():
                count = count+1
            idf_dict[i] = (math.log((1 + n) / (count+1))) + 1
    return idf_dict

def fit(whole_data):
    unique_words = set()
    if isinstance(whole_data, (list,)):
        for x in whole_data:
            for y in x.split():
                if len(y)<2:
                    continue
                unique_words.add(y)
            unique_words = sorted(list(unique_words))
            vocab = {j:i for i,j in enumerate(unique_words)}
    idf_values_of_all_unique_words = idf(whole_data,unique_words)
    return vocab, idf_values_of_all_unique_words
vocabulary, idf_of_vocabulary = fit(corpus)

the word idf in line 22 gives me a nameerror.
is it about positioning?","['python', 'nlp', 'tf-idf', 'nameerror']",73022878,"def fit(whole_data):
    def idf(whole_data, unique_words):
        idf_dict = {}
        n = len(whole_data)
        for i in unique_words:
            count = 0
            for sen in whole_data:
                if i in sen.split():
                    count = count+1
                idf_dict[i] = (math.log((1 + n) / (count+1))) + 1
        return idf_dict

    unique_words = set()

    if isinstance(whole_data, (list,)):
        for x in whole_data:
            for y in x.split():
                if len(y) < 2:
                    continue
                unique_words.add(y)
        unique_words = sorted(list(unique_words))
        vocab = {j: i for i, j in enumerate(unique_words)}

        idf_values_of_all_unique_words = idf(whole_data, unique_words)
    return vocab, idf_values_of_all_unique_words

vocabulary, idf_of_vocabulary = fit(corpus)

just like that!",https://stackoverflow.com/questions/73017872,python,18-07-2022 06:00,61.0,-2.0,1.0,True,19-07-2022 14:01,19-07-2022 14:01
67951891,how to use word2vec cbow in statistical algorithm?,"i have seen few examples of using cbow in neural networks models (although i did not understand it)
i know that word2vec is not similar to bow or tfidf, as there is no single value for cbow
and all examples i saw were using neural network.
i have 2 questions
1- can we convert the vector to a single value and put it in a dataframe so we can use it in logistic regression model?
2- is there any simple code for cbow usage with logistic regression?
more explanation.
in my case, i have a corpus that i want to make a comparison between top features in bow and cbow
after converting to bow
i get this dataset
repid   label   cat   dog   snake   rabbit  apple orange  ...
1       1       5     3     8       2       0 
2       0       1     0     0       6       9
3       1       4     1     5       1       7 

after converting to tfidf
i get this dataset
repid   label   cat   dog   snake   rabbit  apple orange  ...
1       1       0.38     0.42    0.02    0.22   0.00   0.19
2       0       0.75     0.20    0.08    0.12   0.37   0.21
3       1       0.17     0.84    0.88    0.11   0.07   0.44

i am observing the results of top 3 features in each model
so my dataset become like this
bow (i put null here for the values that will be omitted)
repid   label    cat   dog   snake   rabbit  apple orange  ...
1       1        5      null    8    null   null   7
2       0        null   null    null    6   9   2
3       1        4      null    5    null   7   null

tfidf (i put null here for the values that will be omitted)
repid   label   cat   dog   snake   rabbit  apple orange  ...
1       1       0.38     0.42    null    0.22   null   null
2       0       0.75     null    null    null   0.37   0.21
3       1       null     0.84    0.88    null   null   0.44

i want now to do the same with word2ven cbow
i want to take the highest values in the cbow model
repid   label  cat   dog   snake   rabbit  apple orange  ...
1       1      v11     v12    v13    v14   v15   v16
2       0      v21     v22    v23    v24   v25   v26
3       1      v31     v32    v33    v34   v35   v36

to be like this
repid   label    cat   dog   snake   rabbit  apple orange  ...
1       1        v11     null    v13    null   v15   null
2       0        null     null    v23    null   v25   v26
3       1        v31     null    v33    v34   null   null","['nlp', 'logistic-regression', 'word2vec']",67952001,"no matter the internal training method, cbow or skip-gram, a word-vector is always a multidimensional vector: it contains many floating-point numbers.
so at one level, that is one ""value"" - where the ""value"" is a vector. but it's never a single number.
word-vectors, even with all their dimensions, can absolutely serve as inputs for a downstream logistic regression task. but the exact particulars depend on exactly what data you're operating on, and what you intend to achieve - so you may want to expand your question, or ask a more specific followup, with more info about the specific data/task you're considering.
note also: this is done more often with the pipeline of a library like scikit-learn. putting dense high-dimensional word-vectors themselves (or other features derived from word-vectors) directly into ""dataframes"" is often a mistake, adding overhead & indirection compared to working with such large feature-vectors in their more compact/raw format of (say) numpy arrays.",https://stackoverflow.com/questions/67951891,nlp,12-06-2021 18:32,111.0,0.0,1.0,True,12-06-2021 20:13,12-06-2021 20:13
72588288,oserror: [e053] could not read config file from /home/xxxx/.local/lib/python3.9/site-packages/pyresparser/config.cfg,"i am trying to extract data from cv using this tool pyresparser when i installed all the requirements and run my script i got this error.
code
from pyresparser import resumeparser
data = resumeparser('cv.pdf').get_extracted_data()

output
  file ""c:\users\amjad\desktop\extract_data_from_cv\.env\lib\site-packages\spacy\util.py"", line 487, in load_model_from_path
    config = load_config(config_path, overrides=overrides)
  file ""c:\users\amjad\desktop\extract_data_from_cv\.env\lib\site-packages\spacy\util.py"", line 650, in load_config
    raise ioerror(errors.e053.format(path=config_path, name=""config file""))
oserror: [e053] could not read config file from c:\users\amjad\desktop\extract_data_from_cv\.env\lib\site-packages\pyresparser\config.cfg

i am running this inside a venv with python version 3.10.4, spacy version 3.3.1, os windows 11, and pipelines en_core_web_sm (3.3.0).
ps - i did the same steps on google colab and work for me.
thanks in advance.","['python-3.x', 'spacy']",72646424,"i just figured it out, after being stuck with the same error for a while. it's a version issue:
pip install nltk
pip install spacy==2.3.5
pip install 
pip install pyresparser
does the trick. also try different spacy versions and models, because they produce different results. haven't tested any further myself. hope this helps :)
answer from quppi",https://stackoverflow.com/questions/72588288,python-3.x,11-06-2022 22:33,1184.0,2.0,1.0,True,16-06-2022 13:09,12-06-2022 03:36
73311660,what is the difference between asssigning layer and assigning weight of layer?,"imagine i have two code examples:
model.embedding = new_emb

and
model.shared.weight = new_emb.weight

what would be the difference between them?","['deep-learning', 'nlp', 'pytorch', 'embedding']",73312782,"if you assign a layer weight, the assigned layer weight has to have the same shape as the current layer weight.
if you assign a new layer, you can assign any layer (as long as it can handle the input tensor shape and return the correct output tensor shape).",https://stackoverflow.com/questions/73311660,deep-learning,10-08-2022 19:20,44.0,0.0,1.0,True,11-08-2022 22:10,11-08-2022 22:10
16133184,how to detect that two sentences are similar?,"i want to compute how similar two arbitrary sentences are to each other.  for example:


a mathematician found a solution to the problem.
the problem was solved by a young mathematician.


i can use a tagger, a stemmer, and a parser, but i donï¿½ï¿½ï¿½t know how detect that these sentences are similar","['nlp', 'similarity', 'stanford-nlp', 'opennlp']",16133566,"these two sentences are not just similar, they are almost paraphrases, i.e., two alternative ways of expressing the same meaning. it is also a very simple case of paraphrase, in which both utterances use the same words with the only exception of one being in active form while the other is passive. (the two sentences are not exactly paraphrases because in the second sentence the mathematician is ""young"". this additional information makes the semantic relation between the two sentences non symmetric. in these cases, you would say that the second utterance ""entails"" the first one, or in other words that the first can be inferred from the second).
from the example it is not possible to understand whether you are actually interested in paraphrase detection, textual entailment or in sentence similarity in general, which is an even broader and fuzzier problem. for example, is ""people eat food"" more similar to ""people eat bread"" or to ""men eat food""? 
both paraphrase detection and text similarity are complex, open research problems in natural language processing, with a large and active community of researchers working on them. it is not clear what is the extent of your interest in this topic, but consider that even though many brilliant researchers have spent and spend their whole careers trying to crack it, we are still very far from finding sound solutions that just work in general.
unless you are interested in a very superficial solution that would only work in specific cases and that would not capture syntactic alternation (as in this case), i would suggest that you look into the problem of text similarity in more depth. a good starting point would be  the book ""foundations of statistical natural language processing"", which provides a very well organised presentation of most statistical natural language processing topics. once you have clarified your requirements (e.g., under what conditions is your method supposed to work? what levels of precision/recall are you after? what kind of phenomena can you safely ignore, and which ones you need to account for?) you can start looking into specific approaches by diving into recent research work. here, a good place to start would be the online archives of the association for computational linguistics (acl), which is the publisher of most research results in the field.
just to give you something practical to work with, a very rough baseline for sentence similarity would be the cosine similarity between two binary vectors representing the sentences as bags of words. a bag of word is a very simplified representation of text, commonly used for information retrieval, in which you completely disregard syntax and only represent a sentence as a vector whose size is the size of the vocabulary (i.e., the number of words in the language) and whose component ""i"" is valued ""1"" if the word at position ""i"" in the vocabulary appears in the sentence, and ""0"" otherwise.",https://stackoverflow.com/questions/16133184,nlp,21-04-2013 16:04,18317.0,28.0,3.0,True,18-05-2021 11:34,21-04-2013 17:23
78713551,"i load a float32 hugging face model, cast it to float16, and save it. how can i load it as float16?","i load a huggingface-transformers float32 model, cast  it to float16, and save it. how can i load it as float16?
example:
# pip install transformers
from transformers import automodelfortokenclassification, autotokenizer

# load model
model_path = 'huawei-noah/tinybert_general_4l_312d'
model = automodelfortokenclassification.from_pretrained(model_path)
tokenizer = autotokenizer.from_pretrained(model_path)

# convert the model to fp16
model.half()

# check model dtype
def print_model_layer_dtype(model):
    print('\nmodel dtypes:')
    for name, param in model.named_parameters():
        print(f""parameter: {name}, data type: {param.dtype}"")

print_model_layer_dtype(model)
save_directory = 'temp_model_se'
model.save_pretrained(save_directory)

model2 = automodelfortokenclassification.from_pretrained(save_directory, local_files_only=true)
print('\n\n##################')
print(model2)
print_model_layer_dtype(model2)

in this example, model2 loads as a float32 model (as shown by print_model_layer_dtype(model2)), even though model2 was saved as float16 (as shown in config.json). what is the proper way to load it as float16?
tested with transformers==4.36.2 and python 3.11.7 on windows 10.","['python', 'machine-learning', 'huggingface-transformers', 'huggingface', 'half-precision-float']",78713569,"use torch_dtype='auto' in from_pretrained(). example:
model2 = automodelfortokenclassification.from_pretrained(save_directory, 
                                                         local_files_only=true,
                                                         torch_dtype='auto')

full example:
# pip install transformers
from transformers import automodelfortokenclassification, autotokenizer
import torch

# load model
model_path = 'huawei-noah/tinybert_general_4l_312d'
model = automodelfortokenclassification.from_pretrained(model_path)
tokenizer = autotokenizer.from_pretrained(model_path)

# convert the model to fp16
model.half()

# check model dtype
def print_model_layer_dtype(model):
    print('\nmodel dtypes:')
    for name, param in model.named_parameters():
        print(f""parameter: {name}, data type: {param.dtype}"")

print_model_layer_dtype(model)
save_directory = 'temp_model_se'
model.save_pretrained(save_directory)

model2 = automodelfortokenclassification.from_pretrained(save_directory, local_files_only=true, torch_dtype='auto')
print('\n\n##################')
print(model2)
print_model_layer_dtype(model2)

it'll load model2 as torch.float16.",https://stackoverflow.com/questions/78713551,python,05-07-2024 23:58,2036.0,-1.0,1.0,True,06-07-2024 00:47,06-07-2024 00:03
76902970,phrase match from excel list,"i got an excel database containing two columns cliche phrases and type. i need to check a text document for exact match of phrase and return the type for matching phrases. also better to red font matching phrase in original document. my interest is in identifying the type and returning type in a text file.
currently identifying cliche phrase but excel work around eludes me.
import spacy
from spacy.matcher import matcher

nlp = spacy.load('en_core_web_sm')

cliches = ['abandon ship',
'about face',
'above board',
'all ears']

cliche_patterns = [[{'lower':token.text.lower()} for token in nlp(cliche)] for cliche in cliches]

matcher = matcher(nlp.vocab)
for counter, pattern in enumerate(cliche_patterns):
    matcher.add(""cliche ""+str(counter), none, pattern)

example_2 = nlp(""we must abandon ship! it's the only way to stay above board."")
matches_2 = matcher(example_2)
for match in matches_2:
    print(example_2[match[1]:match[2]])

mre:

mock text:
two exquisite objection delighted deficient yet its contained. cordial because are account evident its subject but eat. can properly followed learning prepared you doubtful yet him. over many our good lady feet ask that. expenses own moderate day fat trifling stronger sir domestic feelings. you canï¿½ï¿½ï¿½t judge a book by its cover itself think outside the box at be answer always exeter up do. though or my plenty uneasy do. friendship so considered remarkably be to sentiments. offered mention greater fifteen one promise because nor. why can of worms denoting speaking fat indulged saw dwelling raillery.
expected output:
type a
type b

i tried the code for the mock text.
a sentence containing a phrase called a cat sat on the wall.
a sentence containing a phrase called think outside the box.
a sentence containing a phrase called loose cannon.
a sentence containing a phrase called can of wormsde>

instead of 
a
b
c
d 

as output 
i am getting just getting 
b
c
d","['pandas', 'nltk', 'spacy']",76903704,"did some minor changes to your code in cliche matching side. this writes the types of cliches to a txt file without the color:
import spacy
from spacy.matcher import matcher
from openpyxl import load_workbook

nlp = spacy.load('en_core_web_sm')

wb = load_workbook('cliche_phrases.xlsx')
ws = wb.active
cliche_database = {row[0].lower(): row[1] for row in ws.iter_rows(min_row=2, values_only=true)}

cliches = list(cliche_database.keys())

cliche_patterns = [[{'lower':token.text.lower()} for token in nlp(cliche)] for cliche in cliches]

matcher = matcher(nlp.vocab)

matcher.add(""cliche"", cliche_patterns)

# read and process the mock text
with open('mock_text.txt', 'r') as file:
    text = file.read()

doc = nlp(text)
matches = matcher(doc)

cliche_types_output = []
for match_id, start, end in matches:
    cliche_phrase = doc[start:end].text
    cliche_type = cliche_database.get(cliche_phrase)
    
    if cliche_type:
        cliche_types_output.append(cliche_type)

output_filename = 'output.txt'
with open(output_filename, 'w') as output_file:
    output_file.write(""\n"".join(cliche_types_output))

i'll update the answer to include the coloring of matched words.",https://stackoverflow.com/questions/76902970,pandas,15-08-2023 01:31,98.0,0.0,1.0,True,15-08-2023 13:59,15-08-2023 13:59
76305207,openai api - asynchronous api calls,"i work with the openai api. i have extracted slides text from a powerpoint presentation, and written a prompt for each slide. now, i want to make asynchronous api calls,
so that all the slides are processed at the same time.
this is the code from the async main function:
for prompt in prompted_slides_text:
    task = asyncio.create_task(api_manager.generate_answer(prompt))
    tasks.append(task)
results = await asyncio.gather(*tasks)

and this is generate_answer function:
@staticmethod
    async def generate_answer(prompt):
        """"""
        send a prompt to openai api and get the answer.
        :param prompt: the prompt to send.
        :return: the answer.
        """"""
        completion = await openai.chatcompletion.create(
                model=""gpt-3.5-turbo"",
                messages=[{""role"": ""user"", ""content"": prompt}]

        )
        return completion.choices[0].message.content

the problem is:

object openaiobject can't be used in 'await' expression

and i don't know how to await for the response in generate_answer function
would appreciate any help!","['python', 'python-asyncio', 'openai-api']",77539674,"for those landing here, the error here was probably the instantiation of the object. it has to be:
client = asyncopenai(api_key=api_key)

then you can use:
        response = await client.chat.completions.create(
        model=""gpt-4"",
        messages=custom_prompt,
        temperature=0.9
    )",https://stackoverflow.com/questions/76305207,python,22-05-2023 10:23,36095.0,12.0,3.0,True,18-09-2024 19:54,10-08-2023 05:15
68451246,tm package removewords function concatenate words in r,"am cleaning the sample data using removewords from tm package but removewords function concatenate the words post removal. it should be ""environmental dead frog""  ""environmental dead mouse"". can somebody guide ?
library(tm)
dc<-c(""environmental dead frog still"",""environmental dead mouse come"")

manualremovelist<-c(""the"",""does"",""doesn't"",""please"",""new"",""ok"",""one"",""cant"",
                ""doesnt"",""can"",""still"",""done"",""will"",""without"",""seen"",
                ""also"",""danfoss"",""case"",""doesnï¿½ï¿½t"",""due"",""need"",""occurs"",""made"",
                ""using"",""now"",""make"",""makes"",""needs"",""put"",""okay"",""sno"",""since"",""therefore"",
            ""found"",""milwaukee"",""probably"",""got"",""finally"",""isnt"",""per"",""two"",
                ""obvious"",""unable"",""must"",""nos"",""3nos"",""1no"",""."",""phone"",""tel"",""attached"",
                ""given"",""find"",""have"",""see"",""be"",""give"",""do"",""come"",""use"",""make"",""get"",
                ""try"",""call"",""request"")

dc<-removewords(dc,manualremovelist)

""environmentaldeadfrog""  ""environmentaldeadmouse""","['r', 'nlp', 'tm']",68451498,"removewords works only for words. you may split the string into words and use removewords on individual phrases/sentences.
library(tm)

dc  <- sapply(strsplit(dc, '\\s+'), function(x) 
        trimws(paste0(removewords(x, manualremovelist), collapse = ' ')))

dc

#[1] ""environmental dead frog""  ""environmental dead mouse""",https://stackoverflow.com/questions/68451246,r,20-07-2021 07:41,36.0,1.0,1.0,True,20-07-2021 08:03,20-07-2021 07:49
13269543,why am i getting error? valueerror: chunk structures must contain tagged tokens or trees,"i've been tinkering with nltk with the aim of extracting entities from some news articles, but i keep getting an error:

valueerror: chunk structures must contain tagged tokens or trees.

here's my code:
import lxml.html
import nltk, re, pprint 



def ie_preprocess(document):
    """"""this function takes raw text and chops and then connects the process to break     
       it down into sentences, then words and then complete part-of-speech tagging""""""
    sentences = nltk.sent_tokenize(document)
    sentences = [nltk.word_tokenize(sent) for sent in sentences]
    sentences = [nltk.pos_tag(sent) for sent in sentences]
    return sentences


    #import story
    base_url = ""
    page = lxml.html.parse(base_url)
    story = page.xpath('//*[@id=""story""]/div[2]/div[1]')
    raw_text = story[0].text_content()
    #tokenize
    output = ie_preprocess(raw_text)
    print output
    #chunk
    grammar = r'''
       np: 
       {<dt><nn.*><.*>*<nn.*>} 
       '''
    cp = nltk.regexpparser(grammar)

    chunked = cp.parse(output)

    print chunked

update
here's the error message in full:
traceback (most recent call last):
  file ""geo_locator.py"", line 30, in <module>
    chunked = cp.parse(output)
  file ""/users/davidelks/pythontests/venv/lib/python2.7/site-packages/nltk/chunk/regexp.py"", line 1183, in parse
    chunk_struct = parser.parse(chunk_struct, trace=trace)
  file ""/users/davidelks/pythontests/venv/lib/python2.7/site-packages/nltk/chunk/regexp.py"", line 999, in parse
     chunkstr = chunkstring(chunk_struct)
  file ""/users/davidelks/pythontests/venv/lib/python2.7/site-packages/nltk/chunk/regexp.py"", line 93, in __init__
tags = [self._tag(tok) for tok in self._pieces]
  file ""/users/davidelks/pythontests/venv/lib/python2.7/site-packages/nltk/chunk/regexp.py"", line 103, in _tag
    raise valueerror('chunk structures must contain tagged '
valueerror: chunk structures must contain tagged tokens or trees","['python', 'nltk', 'text-mining']",13289830,"the parse() function can only handle one sentence at a time. 
this works: 
chunked = []
for s in output:
    chunked.append(cp.parse(s))

result:
[tree('s', [(u'police', 'nn'), (u'are', 'vbp'), (u'hunting', 'vbg'), ...",https://stackoverflow.com/questions/13269543,python,07-11-2012 12:11,2873.0,4.0,1.0,True,07-04-2022 22:11,07-04-2022 22:11
72605644,"mobilevit binary classification valueerror: `logits` and `labels` must have the same shape, received ((none, 2) vs (none, 1))","i am using the colab notebook( for mobilevit to train on a dataset i have of 25k pictures for 2 classes. since it's a binary classification, i have used keras.losses.binarycrossentropy and sigmoid as activation function at the last layer:-
def create_mobilevit(num_classes=2):
inputs = keras.input((image_size, image_size, 3))
x = layers.rescaling(scale=1.0 / 255)(inputs)

# initial conv-stem -> mv2 block.
x = conv_block(x, filters=16)
x = inverted_residual_block(
    x, expanded_channels=16 * expansion_factor, output_channels=16
)

# downsampling with mv2 block.
x = inverted_residual_block(
    x, expanded_channels=16 * expansion_factor, output_channels=24, strides=2
)
x = inverted_residual_block(
    x, expanded_channels=24 * expansion_factor, output_channels=24
)
x = inverted_residual_block(
    x, expanded_channels=24 * expansion_factor, output_channels=24
)

# first mv2 -> mobilevit block.
x = inverted_residual_block(
    x, expanded_channels=24 * expansion_factor, output_channels=48, strides=2
)
x = mobilevit_block(x, num_blocks=2, projection_dim=64)

# second mv2 -> mobilevit block.
x = inverted_residual_block(
    x, expanded_channels=64 * expansion_factor, output_channels=64, strides=2
)
x = mobilevit_block(x, num_blocks=4, projection_dim=80)

# third mv2 -> mobilevit block.
x = inverted_residual_block(
    x, expanded_channels=80 * expansion_factor, output_channels=80, strides=2
)
x = mobilevit_block(x, num_blocks=3, projection_dim=96)
x = conv_block(x, filters=320, kernel_size=1, strides=1)

# classification head.
x = layers.globalavgpool2d()(x)
outputs = layers.dense(num_classes, activation=""sigmoid"")(x)

return keras.model(inputs, outputs)

and here's my dataset preparation cell:-
batch_size = 64
auto = tf.data.autotune
resize_bigger = 512
num_classes = 2


def preprocess_dataset(is_training=true):
    def _pp(image, label):
        if is_training:
            # resize to a bigger spatial resolution and take the random
            # crops.
            image = tf.image.resize(image, (resize_bigger, resize_bigger))
            image = tf.image.random_crop(image, (image_size, image_size, 3))
            image = tf.image.random_flip_left_right(image)
        else:
            image = tf.image.resize(image, (image_size, image_size))
        label = tf.one_hot(label, depth=num_classes)
        return image, label

    return _pp


def prepare_dataset(dataset, is_training=true):
    if is_training:
        dataset = dataset.shuffle(batch_size * 10)
    dataset = dataset.map(preprocess_dataset(is_training), num_parallel_calls=auto)
    return dataset.batch(batch_size).prefetch(auto)

and this is the cell for training the model:-
learning_rate = 0.002
label_smoothing_factor = 0.1
epochs = 30

optimizer = keras.optimizers.adam(learning_rate=learning_rate)
loss_fn = keras.losses.binarycrossentropy(label_smoothing=label_smoothing_factor)


def run_experiment(epochs=epochs):
    mobilevit_xxs = create_mobilevit(num_classes=num_classes)
    mobilevit_xxs.compile(optimizer=optimizer, loss=loss_fn, metrics=[""accuracy""])

    checkpoint_filepath = ""/tmp/checkpoint""
    checkpoint_callback = keras.callbacks.modelcheckpoint(
        checkpoint_filepath,
        monitor=""val_accuracy"",
        save_best_only=true,
        save_weights_only=true,
    )

    mobilevit_xxs.fit(
        train_ds,
        validation_data=val_ds,
        epochs=epochs,
        callbacks=[checkpoint_callback],
    )
    mobilevit_xxs.load_weights(checkpoint_filepath)
    _, accuracy = mobilevit_xxs.evaluate(val_ds)
    print(f""validation accuracy: {round(accuracy * 100, 2)}%"")
    return mobilevit_xxs


mobilevit_xxs = run_experiment()

basically the code is identical to  except for the change in binarycrossentropy loss and sigmoid as actv. func. i don't understand why i am getting this even though i am explicitly ont-hot-coded my class labels -
valueerror: `logits` and `labels` must have the same shape, received ((none, 2) vs (none, 1)).","['tensorflow', 'deep-learning', 'pytorch', 'huggingface-transformers', 'image-classification']",72746811,"you need to change the num_classes = 1 instead of num_classes = 2 as you have used sigmoid activation function which returns the values between 0 to 1 for binary classification(0,1).
the values <0.5 will be considered as class 0 and values >0.5 will be as class 1 in between two binary classes (0,1).
please refer to the replicated gist for your reference.",https://stackoverflow.com/questions/72605644,tensorflow,13-06-2022 15:50,226.0,0.0,1.0,True,24-06-2022 16:15,14-06-2022 10:49
49941772,tags in google ngrams dataset,"tl;dr : i can't find a comprehensive list of all tags used in google grams dataset besides that one which only includes pos tags and _start_, _root_ and _end_.  
what do tokens like ,_., ._., _._ mean ? given their frequencies -- see below -- i'd strongly assume they're tags (they can't be proper tokens).

context :
i am trying to extract information from google's n-grams dataset and have troubles understanding some of their tags, and how to take them into account.
ultimately, i would like to approximate how likely a word will follow another one.
for example, calculating how likely the token protection will follow equal would roughly mean calculating count(""equal protection"") / count(""equal *"") where * is the wildcard : any 1gram in the corpus.
the tricky part is calculating that count(""equal *"").
indeed, for example, the bi-gram equal to accounts many times in the google n-grams dataset : 

as equal to, 
as equal to_prt (disambiguated pos version)
as equal _prt_ (aggregated for all prt i.e. particles that might follow equal).

as shows when i compute this on pyspark :
>>> total = ggrams.filter(ggrams.ngram.startswith(""equal "")).groupby(""ngram"") \
             .sum(""match_count"")

>>> total.sort(""sum(match_count)"", ascending=false).show(n=15)

+------------+----------------+  
|       ngram|sum(match_count)|  
+------------+----------------+  
|equal _noun_|        20130934|  
| equal _prt_|        16620727|  
|    equal to|        16598291|  
|equal to_prt|        16598291|  
|   equal _._|         5119672|  
| equal _adp_|         3037747|  
|     equal ,|         2276119|  
|   equal ,_.|         2276119|  
|    equal in|         1682835|  
|equal in_adp|         1682176|  
|     equal .|         1628257|  
|   equal ._.|         1628257|  
|equal _conj_|         1363739|  
|    ...     |             ...|  

so to avoid accounting the same bigram multiple times, my idea was to rather just sum all counts for all patterns like ""equal <pos>"" where <pos> is in the described pos set [_prt_, _noun_, ...] (findable here)
doing this i obtain sum figures that are 1/3rd of the one i'd get from the displayed dataframe above. which strenghthen my hypothesis above that one count will account three times. but i can't help persuading myself what the best way to do it is, especially notifying these weird tokens ,_., ._., _._ which meanings i don't have any clue.","['python-3.x', 'pyspark', 'nlp', 'n-gram', 'part-of-speech']",67710914,"the list of pos tags given in the documentation does not mention two of the tags, but the 2012 paper syntactic annotations for the google books ngram corpus does:

ï¿½ï¿½ï¿½.ï¿½ï¿½ï¿½ (punctuation marks)
x (a catch-all for other categories such as abbreviations or foreign words)

so the token ,_. is a comma appended with its pos tag, just like the token run_verb. similarly, ._. is a full stop appended with its pos tag. finally, _._ means punctuation, any punctuation just like _verb_ is an",https://stackoverflow.com/questions/49941772,python-3.x,20-04-2018 12:34,507.0,1.0,1.0,True,26-05-2021 18:39,20-04-2018 12:40
71402907,how to load and preprocess a dataset by chunks?,"i have a large data frame to which i would like to apply a set of functions to one of its columns using pipeline and progress_apply().
here is my code snippet.
df = # a dataframe object with multiple columns where df.columns[-1] == 'text' 
from tqdm.auto import tqdm
tqdm.pandas()

pipeline = # list of pre-defined methods
prepare(text, pipeline):
   """"""
   a method that cleanup and remove stop words from text input
   """"""
   return # list of clean tokens

# memoryerror! when reaching 50% of cleaning progress
df = df['text'].progress_apply(prepare, pipeline=pipeline)  

i am trying to solve the issue of memoryerror using progress_apply() but loading data by chunks. i have no idea of how i can do this with progress_apply().
i tried the following:
for i in range(0, df.shape[0], 47):
   df = df['text'][i:i+47].progress_apply(prepare, pipeline=pipeline)  

what i have tried doesn't same the previous ranges.","['python', 'python-3.x', 'pandas', 'nlp']",71402986,"n, step = df.shape[0], 47
for i in range(0, n, step):
   df[i:i+step] = df['text'][i:i+step].progress_apply(prepare, pipeline=pipeline)",https://stackoverflow.com/questions/71402907,python,09-03-2022 00:12,299.0,0.0,1.0,True,09-03-2022 00:26,09-03-2022 00:18
75968314,"i&#39;m trying to run the llama index model, but when i get to the index building step - it fails time and time again, how can i fix this?","i'm trying to use the llama_index model which builds an index from your personal documents, and allows you to ask questions about the information from the gpt chat.
this is the full code (of course with my api):
import os
os.environ[""openai_api_key""] = 'your_openai_api_key'

from llama_index import gptsimplevectorindex, simpledirectoryreader
documents = simpledirectoryreader('data').load_data()
index = gptsimplevectorindex.from_documents(documents)

when i run the index build according to the steps in their documentation, it fails at this step:
index = gptsimplevectorindex.from_documents(documents) 
with the following error:
traceback (most recent call last):
  file ""<stdin>"", line 1, in <module>
  file ""c:\users\colmi\appdata\local\programs\python\python310\lib\site-packages\llama_index\indices\base.py"", line 92, in from_documents
    service_context = service_context or servicecontext.from_defaults()
  file ""c:\users\colmi\appdata\local\programs\python\python310\lib\site-packages\llama_index\indices\service_context.py"", line 71, in from_defaults
    embed_model = embed_model or openaiembedding()
  file ""c:\users\colmi\appdata\local\programs\python\python310\lib\site-packages\llama_index\embeddings\openai.py"", line 209, in __init__
    super().__init__(**kwargs)
  file ""c:\users\colmi\appdata\local\programs\python\python310\lib\site-packages\llama_index\embeddings\base.py"", line 55, in __init__
    self._tokenizer: callable = globals_helper.tokenizer
  file ""c:\users\colmi\appdata\local\programs\python\python310\lib\site-packages\llama_index\utils.py"", line 50, in tokenizer
    enc = tiktoken.get_encoding(""gpt2"")
  file ""c:\users\colmi\appdata\local\programs\python\python310\lib\site-packages\tiktoken\registry.py"", line 63, in get_encoding
    enc = encoding(**constructor())
  file ""c:\users\colmi\appdata\local\programs\python\python310\lib\site-packages\tiktoken_ext\openai_public.py"", line 11, in gpt2
    mergeable_ranks = data_gym_to_mergeable_bpe_ranks(
  file ""c:\users\colmi\appdata\local\programs\python\python310\lib\site-packages\tiktoken\load.py"", line 83, in data_gym_to_mergeable_bpe_ranks
    for first, second in bpe_merges:
valueerror: not enough values to unpack (expected 2, got 1)

i should mention that i tried this on docx files inside a specific folder that contains such files and folders, also inside subfolders.","['python', 'python-3.x', 'openai-api', 'chatgpt-api', 'llama-index']",76890846,"i seem to have had a problem with the whole code usage approach.
the value 'data' is not used as a parameter for defining a function, but simply marks an example of a folder name that contains the user's files.
a local path can be used like:
documents = simpledirectoryreader('my_folder').load_data()

or in a fixed path, such as:
documents = simpledirectoryreader('c:\users\user\my_files').load_data()

if you use this approach, everything will work as expected.",https://stackoverflow.com/questions/75968314,python,09-04-2023 00:55,7193.0,0.0,2.0,True,17-08-2023 20:12,17-08-2023 20:12
36731784,how to concatenate word vectors to form sentence vector,"i have learned in some essays (tomas mikolov...) that a better way of forming the vector for a sentence is to concatenate the word-vector.
but due to my clumsy in mathematics, i am still not sure about the details.
for example,
supposing that the dimension of word vector is m; and that a sentence has n words.
what will be the correct result of concatenating operation?
is it a row vector of 1 x m*n ?     or a matrix of m x n ?","['machine-learning', 'deep-learning', 'nlp', 'word2vec']",36834601,"there are at least three common ways to combine embedding vectors; (a) summing, (b) summing & averaging or (c) concatenating. so in your case, with concatenating, that would give you a 1 x m*a vector, where a is the number of sentences. in the other cases, the vector length stays the same. see gensim.models.doc2vec.doc2vec, dm_concat and dm_mean - it allows you to use any of those three options [1,2].
[1] 
[2]",https://stackoverflow.com/questions/36731784,machine-learning,20-04-2016 00:32,5073.0,4.0,1.0,True,06-12-2021 23:21,06-12-2021 23:21
71815866,how do i use gensim to vectorize these words in my dataframe so i can perform clustering on them?,"i am trying to do a clustering analysis (preferably k-means) of poetry words on a pandas dataframe. i am firstly trying to vectorize the words by using the word-to-vector feature in the gensim package. however, the vectors just come out with 0s, so my code is failing to translate the words into vectors. as a result, the clustering doesn't work. here is my code:
# create a gensim model 
model = gensim.models.word2vec(vector_size=100) 
# copy original pandas dataframe with poems
data = poems.copy(deep=true)
# get data ready for kmeans clustering
final_data = [] # empty list 
for i, row in data.iterrows(): 
    poem_vectorized = [] 
    poem = row['main_text']
    poem_all_words = poem.split(sep="" "")
    for poem_w in poem_all_words: #iterate through list of words 
        try:
            poem_vectorized.append(list(model.wv[poem_w]))
        except exception as e:
            pass
    try:
        poem_vectorized = np.asarray(poem_vectorized)
        poem_vectorized_mean = list(np.mean(poem_vectorized, axis=0))
    except exception as e:
        poem_vectorized_mean = list(np.zeros(100))
        pass
    try:
        len(poem_vectorized_mean)
    except:
        poem_vectorized_mean = list(np.zeros(100))
    temp_row = np.asarray(poem_vectorized_mean)
    final_data.append(temp_row)
x = np.asarray(final_data)
print(x)


at closer inspection of:
poem_vectorized.append(list(model.wv[poem_w]))

the problem seems to be this:","['python', 'nlp', 'cluster-analysis', 'gensim']",71818646,"if i understand it correctly you want to use an existing model to get the semantic embeddings of the tokens and then cluster the words, right?
because the way you set the model up you are preparing a new model for training, but then don't feed any training data to it and train it, so your model doesn't know any words and just always throws a keyerror when calling model.wv[poem_w].
use gensim.downloader to load an existing model (check out their repository for a list of all available models):
import gensim.downloader as api
import numpy as np
import pandas

poems = pandas.dataframe({""main_text"": [""this is a sample poem."", ""this is another sample poem.""]})
model = api.load(""glove-wiki-gigaword-100"")

then use it to retrieve the vectors for all words the models knows:
final_data = []
for poem in poems['main_text']:
    poem_all_words = poem.split()
    poem_vectorized = []
    for poem_w in poem_all_words:
        if poem_w in model:
            poem_vectorized.append(model[poem_w])
    poem_vectorized_mean = np.mean(poem_vectorized, axis=0)
    final_data.append(poem_vectorized_mean)

or as list comprehension:
final_data = []
for poem in poems['main_text']:
    poem_vectorized_mean = np.mean([model[poem_w] for poem_w in poem.split() if poem_w in model], axis=0)
    final_data.append(poem_vectorized_mean)

which both will give you:
x = np.asarray(final_data)
print(x)
> [[-3.74696642e-01  3.73661995e-01  4.09943342e-01 -2.07784668e-01
    ...
    -1.85739681e-01 -7.07386672e-01  3.31366658e-01  3.31600010e-01]
   [-3.29973340e-01  4.13213342e-01  5.26199996e-01 -2.29261339e-01
    ...
    -1.25366330e-01 -5.87253332e-01  2.80240029e-01  2.56700337e-01]]

note that attempting to get np.mean() on an empty list will throw an error so you might want to catch that in case there are poems which are empty or where all words are unknown to the model.",https://stackoverflow.com/questions/71815866,python,10-04-2022 10:49,748.0,0.0,1.0,True,10-04-2022 16:43,10-04-2022 11:01
67224105,difference in tense when using spacy pos on the same sentence on different pcs,"i was taking a course on udemy about nlp. so i reached the pos section. so when the instructor was demonstrating the pos tag attribute, i was having a difference in tense on the same sentence which we both were entering.
so i was using the sentence ""i read books on spacy."". he was also using the same sentence but when we applied token.tag_ i got vbd where as he got vbp. can anyone explain why it happened?","['python', 'nlp', 'spacy']",67224719,"in general the models should be deterministic. it's possible that we missed something and that's not quite the case, but first you should check these things:

are you using the same version of spacy?
are the models the same version? (small differences here would explain this)
is the input string actually the same, or is it different? (""spacy"" vs ""spacy"" for example)
is your code the same?

if everything is actually the same, what model are you using?",https://stackoverflow.com/questions/67224105,python,23-04-2021 04:42,94.0,1.0,1.0,True,23-04-2021 07:30,23-04-2021 07:30
68718411,can nltk pos tagger recognize contractions correctly?,"i want to know if i need to write a de-contraction function before sending a given text to nltk's pos tagger. i am reluctant to tokenize words because they might end up like (don't='do',""'nt"") which i suspect would make pos tagging more difficult.
in short, my questions are: does nltk's pos tagger recognize most contractions (from my limited experience it seems to work well w/o word tokenization)? will word tokenization (as opposed to simple word splitting) improve or impair the process? would it just be easier for me to write a de-contraction function? are there any other pos taggers that recognize contractions?
example_text=""i can't and i won't go to the park because i don't like grass.""","['python', 'nltk']",73770444,"i want to know if i need to write a de-contraction function before sending a given text to nltk's pos tagger.

you do not. the default nltk tagger is trained with text that was tokenized with the default nltk tokenization, and works correctly with text that is tokenized the same way. anything else would be a bug in the nltk. so if you change the tokenizer you will make performance worse, not better.
if you try your own example you'll see that it correctly tags ""ca"" and ""wo"" as md (modal verb), even though there are no such words in english; i don't particularly like it (why not just tokenize ""can't"" as ""can n't""?), but the tagger certainly knows what to do with it.
>>> nltk.pos_tag(nltk.word_tokenize(example_text))
[('i', 'prp'), ('ca', 'md'), (""n't"", 'rb'), ('and', 'cc'), ('i', 'prp'),
 ('wo', 'md'), (""n't"", 'rb'), ('go', 'vb'), ('to', 'to'), ('the', 'dt'),
 ('park', 'nn'), ('because', 'in'), ('i', 'prp'), ('do', 'vbp'), (""n't"", 'rb'), 
('like', 'vb'), ('grass', 'nn'), ('.', '.')]

will the tagger get some things wrong? definitely. no tagger is perfect. but if you want better performance, you need to find or train a better tagger. you can't ""improve"" the word tokenizer that the tagger is designed to work with.
ps. you should only pass one (tokenized) sentence at a time to the tagger. if you pass it your entire file as a list of words, you do lose performance unnecessarily. this is how you should do it:
sents = [ nltk.word_tokenize(s) for s in nltk.sent_tokenize(long_text) ]
nltk.pos_tag_sents(sents)",https://stackoverflow.com/questions/68718411,python,09-08-2021 20:52,374.0,1.0,2.0,True,19-09-2022 08:20,09-08-2021 20:57
67397774,adding metadata into stanford corenlp input,"i have a large corpus of sentences (~ 1.1m) to parse through stanford core nlp but in the output i get more sentences than in the input, probably the system segments some sentences beyond the given segmentation into lines.
to control what happens i would like to include ""tags"" into the input. these tags should be recognizable in the output and should not affect parsing.
something like
<0001>
i saw a man with a telescope .
</0001>

or
#0001#
i saw a man with a telescope .
#/0001#

i have tried many formats, in all cases the ""tag"" has been parsed as if it were part of the text.
is there some way to tell the parser ""do not parse this, just keep it as is in the output""?
===a few hours later===
as i'm getting no answer, here is an example: i would like to process the sentence ï¿½ï¿½ï¿½manon espï¿½ï¿½rait secrï¿½ï¿½tement y revoir un garï¿½ï¿½on qui l'avait marquï¿½  carries tag 151_0_4. i imagined to write the tag between two rows of equal signs on a separate line, followed by a period, to be sure that the tag will, at worst, be processed as a separate sentence:
=====151_0_4======.
manon espï¿½ï¿½rait secrï¿½ï¿½tement y revoir un garï¿½ï¿½on qui l'avait marquï¿½ï¿½e autrefois.
=====151_0_4======.

and here is what this produced:
(root (sent (np (sym =)) (np (sym =) (pp (sym =) (np (sym =) (pp (sym =) (np (num 151_0_4) (sym =) (sym =) (np (sym =) (pp (sym =) (np (sym =) (sym =))))))))) (punct .)))

as you see the tags are definitely considered as being part of the sentence, no way to separate them from it.
same thing happened with xml-like tags <x151_0_4> or tags using the hash c",['stanford-nlp'],67410734,"if your current data is strictly one sentence per line, then by far the easiest thing to do is to just leave it like that and to give the option -ssplit.eolonly=true.
there unfortunately isn't an option to pass through certain kinds of meta-data or delimiters without attempting to parse or process them. however, you can indicate that they should not be made part of other sentences by means of the ssplit.boundarytokenregex or ssplit.boundarymultitokenregex properties. however, your choices are then either to just delete them (see ssplit.tokenpatternstodiscard) or else to process them as weird sentences, which you'd then need to clean up.",https://stackoverflow.com/questions/67397774,stanford-nlp,05-05-2021 08:33,51.0,0.0,1.0,True,06-05-2021 01:26,05-05-2021 14:32
74668460,which learning model should be chosen to predict text news tags?,"i have a database of news texts (100000 samples). half of the dataset is tagged, and half is not, what methodology can i use to analyze the remaining news and fill them with tags?
data example:

text = a cap on the price of russian oil will restrict russia's revenues for its ""illegal war ukraine"", the us says. the cap, approved by western allies on friday, is aimed at stopping countries paying more than $60 (ï¿½ï¿½48) for a barrel of seaborne russian crude oil. the measure - due to come into force on monday - intensifies western pressure on russia over the invasionï¿½ï¿½ï¿½ [long test is cut]


tags = ['russian', 'oil', 'war']

i know how to use python, pandas. but i found only methods that predict whether the text is bad or","['python', 'nlp', 'data-science', 'data-analysis', 'tagging']",74763863,"there are quite a few nlp content tagging methods: keyphrase-based, classification-based, custom methods with some rules determined (if you know the principles, how the tags were setup manually). try combining them.

split your datasets into tagged and untagged parts.
tagged dataset is the one you can experiment with: split into train-validation-test, check the metrics.
analyze which tags are missing/added mistakenly and fine-tune the solution.

explore the articles:
[linkedin] automatic content tagging using nlp and machine learning
[medium] k-meanception: how to automatically tag news articles using clustering algorithms?",https://stackoverflow.com/questions/74668460,python,03-12-2022 16:22,592.0,1.0,1.0,True,11-12-2022 19:31,11-12-2022 19:10
72515966,implement metrics using xlmroberta model for text classification,"i have created script for binary (0 and 1) text classification using xlm-roberta model. i would like to put metrics (as binary cross-entropy) but also early stopping with patience of 15.
but i have a problem. i tried to use the path model.compile and model.fit, but xlm-robertaforsequenceclassification doesn't have these parameters. i would't like to use argumentation. it is possible to find some solution?
already i use adamw. finally it is possible to get for each epoch parameters as recall, f1, accuracy? at the moment i get only last data of the last epoch.
below i put the script during training:
from transformers import xlmrobertaforsequenceclassification, adamw, bertconfig

# load bertforsequenceclassification, the pretrained bert model with a single 
# linear classification layer on top. 
model = xlmrobertaforsequenceclassification.from_pretrained(
    ""xlm-roberta-base"", # use the 12-layer bert model, with an uncased vocab.
    num_labels = 2, # the number of output labels--2 for binary classification.
                    # you can increase this for multi-class tasks.   
    output_attentions = false, # whether the model returns attentions weights.
    output_hidden_states = false, # whether the model returns all hidden-states.
)

# tell pytorch to run this model on the gpu.
#model.cuda()
model.to(device)

here start the training!
import random
import numpy as np
import gc
seed_val = 45
epochs = 15
    # set the seed value all over the place to make this reproducible.
    random.seed(seed_val)
    np.random.seed(seed_val)
    torch.manual_seed(seed_val)
    torch.cuda.manual_seed_all(seed_val)
    # store the average loss after each epoch so we can plot them.
    loss_values = []
    training_stats = []
    # measure how long the training epoch takes.
    total_t0 = time.time()
    # for each epoch...
    for epoch in range(0, epochs):
      print("""")
        print('======== epoch {:} / {:} ========'.format(epoch + 1, epochs))
        stacked_val_labels = []
        targets_list = []
        # ========================================
        #               training
        # ========================================
        print('training...')
        # put the model into train mode
        model.train()
        
        # this turns gradient calculations on and off.
        torch.set_grad_enabled(true)
    
        # measure how long the training epoch takes.
        t0 = time.time()
    
        # reset the total loss for this epoch.
        total_train_loss = 0
    
        for i, batch in enumerate(train_dataloader):
            train_status = 'batch ' + str(i) + ' of ' + str(len(train_dataloader))
            print(train_status, end='\r')
        
            b_input_ids = batch[0].to(device)
            b_input_mask = batch[1].to(device)
            b_labels = batch[2].to(device)
    
            model.zero_grad()        
    
            outputs = model(b_input_ids, 
                        attention_mask=b_input_mask,
                        labels=b_labels)
            
            # get the loss from the outputs tuple: (loss, logits)
            loss = outputs[0]
            
            # convert the loss from a torch tensor to a number.
            # calculate the total loss.
            total_train_loss = total_train_loss + loss.item()
            
            # zero the gradients
            optimizer.zero_grad()
            # perform a backward pass to calculate the gradients.
            loss.backward()
            
            # clip the norm of the gradients to 1.0.
            # this is to help prevent the ""exploding gradients"" problem.
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            # use the optimizer to update the weights.
            
            # optimizer for gpu
            optimizer.step() 
            
            # optimizer for tpu
            # 
            #xm.optimizer_step(optimizer, barrier=true)
            # measure how long this epoch took.
            training_time = format_time(time.time() - t0)
        print("""")
        print('train loss:' ,total_train_loss)
        print(""  training epcoh took: {:}"".format(training_time))
     
        # ========================================
        #               validation
        # ========================================
        print('\nvalidation...')
    
        # measure how long the training epoch takes.
        t0 = time.time()
    
        # put the model in evaluation mode.
        model.eval()
    
        # turn off the gradient calculations.
        # this tells the model not to compute or store gradients.
        # this step saves memory and speeds up validation.
        torch.set_grad_enabled(false)
          
        # reset the total loss for this epoch.
        total_val_loss = 0
       
        for j, batch in enumerate(val_dataloader):
            
            val_status = 'batch ' + str(j) + ' of ' + str(len(val_dataloader))
            
            print(val_status, end='\r')
    
            b_input_ids = batch[0].to(device)
            b_input_mask = batch[1].to(device)
            b_labels = batch[2].to(device)      
       
            outputs = model(b_input_ids, 
                    attention_mask=b_input_mask, 
                    labels=b_labels)
            
            # get the loss from the outputs tuple: (loss, logits)
            loss = outputs[0]
            
            # convert the loss from a torch tensor to a number.
            # calculate the total loss.
            total_val_loss = total_val_loss + loss.item()
        
            # get the preds
            preds = outputs[1]
       
            # move preds to the cpu
            val_preds = preds.detach().cpu().numpy()
            
            # move the labels to the cpu
            targets_np = b_labels.to('cpu').numpy()
    
            # append the labels to a numpy list
            targets_list.extend(targets_np)
    
            if j == 0:  # first batch
                stacked_val_preds = val_preds
    
            else:
                stacked_val_preds = np.vstack((stacked_val_preds, val_preds))
    
        # calculate the validation accuracy
        y_true = targets_list
        y_pred = np.argmax(stacked_val_preds, axis=1)
        
        val_acc = accuracy_score(y_true, y_pred)
        # measure how long the validation run took.
        validation_time = format_time(time.time() - t0)
        
        print('val loss:' ,total_val_loss)
        print('val acc: ', val_acc)
        print(""  validation took: {:}"".format(validation_time))
        # record all statistics from this epoch.
        #training_stats = []
        training_stats.append(
            {
                'epoch': epoch + 1,
                'training loss': total_train_loss,
                'valid. loss': total_val_loss,
                'valid. accur.': val_acc,
                'training time': training_time,
                'validation time': validation_time
            }
        )
    
    print("""")
    print(""training complete!"")
    
    print(""total training took {:} (h:mm:ss)"".format(format_time(time.time()-total_t0)))
    
    # save the model
    torch.save(model.state_dict(), '/content/drive/mydrive/model/model.pt')
        
    # use the garbage collector to save memory.
    gc.collect()","['python', 'text-classification']",72543265,"xlmrobertaforsequenceclassification and other classes of the ""forsequenceclassification"" family assume classification into multiple classes and use categorical cross-entropy as the loss function. the class is just a lightweight wrapper of the xlmroberta class.
if you want to use specifically binary cross-entropy, you can either make your own wrapper with a single class output and binary cross-entropy, or you can do the loss computation in the training loop in your code snippet. i.e., instead of using outputs[0], use the logits outputs[1] as an input to the loss function.
regarding other metrics, you have the logits in the outputs variable. it should be enough to compute whatever metric you find useful for your task.",https://stackoverflow.com/questions/72515966,python,06-06-2022 09:57,452.0,0.0,1.0,True,08-06-2022 09:27,06-06-2022 10:19
75648132,openai gpt-3 api: why do i get only partial completion? why is the completion cut off?,"i tried the following code but got only partial results like
[{""light_id"": 0, ""color

i was expecting the full json as suggested on this page:

import json
import os
import time
from json import jsondecodeerror
from typing import list

import openai
openai.api_key =  ""xxx""

header = """"""
i have a hue scale from 0 to 65535. 
red is 0.0
orange is 7281
yellow is 14563
purple is 50971
pink is 54612
green is 23665
blue is 43690

saturation is from 0 to 254
brightness is from 0 to 254

two jsons should be returned in a list. each json should contain a color and a light_id. 
the light ids are 0 and 1. 
the color relates a key ""color"" to a dictionary with the keys ""hue"", ""saturation"" and ""brightness"". 

give me a list of jsons to configure the lights in response to the instructions below. 
give only the json and no additional characters. 
do not attempt to complete the instruction that i give.
only give one json for each light. 
""""""

completion = openai.completion.create(model=""text-davinci-003"", prompt=header)
print(completion.choices[0].text)","['python', 'openai-api', 'gpt-3']",75671537,"in general
gpt-3 api (i.e., completions api)
if you get partial completion (i.e., if the completion is cut off), it's because the max_tokens parameter is set too low or you didn't set it at all (in this case, it defaults to 16). you need to set it higher, but the token count of your prompt and completion together cannot exceed the model's context length.
see the official openai documentation:

gpt-3.5 and gpt-4 api (i.e., chat completions api)
compared to the gpt-3 api, the gpt-3.5 and gpt-4 apis have the max_tokens parameter set to infinite by default.


your case
you're using text-davinci-003 (i.e., the gpt-3 api). if you don't set max_tokens = 1024 the completion you get will be cut off. take a careful look at the tutorial you're referring to once again.
if you run test.py, the openai api will return a completion:

light 0 should be red:  [{""light_id"": 0, ""color"": {""hue"": 0,
""saturation"": 254, ""brightness"": 254}},{""light_id"": 1, ""color"": }]
light 1 should be orange: [{""light_id"": 0, ""color"": {""hue"": 0,
""saturation"": 254, ""brightness"": 254}},{""light_id"": 1, ""color"":
{""hue"": 7281, ""saturation"": 254, ""brightness"": 254}}]

test.py
import openai
import os

openai.api_key = os.getenv('openai_api_key')

header = """"""
i have a hue scale from 0 to 65535. 
red is 0.0
orange is 7281
yellow is 14563
purple is 50971
pink is 54612
green is 23665
blue is 43690

saturation is from 0 to 254
brightness is from 0 to 254

two jsons should be returned in a list. each json should contain a color and a light_id. 
the light ids are 0 and 1. 
the color relates a key ""color"" to a dictionary with the keys ""hue"", ""saturation"" and ""brightness"". 

give me a list of jsons to configure the lights in response to the instructions below. 
give only the json and no additional characters. 
do not attempt to complete the instruction that i give.
only give one json for each light. 
""""""

completion = openai.completion.create(model=""text-davinci-003"", prompt=header, max_tokens=1024)
print(completion.choices[0].text)",https://stackoverflow.com/questions/75648132,python,06-03-2023 07:29,5424.0,4.0,2.0,True,28-11-2023 17:14,24-08-2023 18:03
74091764,string literal matching between words in two different dataframe (dfs) and generate a new dataframe,"i have two dataframes df1 and df2
df1 =




university
school
student first name
last name
nick name




aaa
law
john
mckenzie
stevie


bbb
business
steve
savannah
jo


ccc
engineering
mark
justice
fre


ddd
arts
stuart
little
rah


eee
life science
adam
johnson
meh




120 rows x 5 columns
df2 =




statement




stuart had a headache last nigh which was due to thï¿½ï¿½ï¿½ï¿½ï¿½ï¿½


rah basically found a new found friend which lead to theï¿½ï¿½ï¿½ï¿½ï¿½ï¿½


gerome got a brand new watch which wasï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½.


adam was found chilling all through out his lifeï¿½ï¿½ï¿½ï¿½ï¿½ï¿½


savannah is such a common name thatï¿½ï¿½ï¿½ï¿½ï¿½ï¿½..




3000 rows x1 columns
aim is to form df3
match the string literal and iterate it through every cells in the columns ""student first name"" , ""student last name"" , ""student nick n""s-table-container"">



statement
matching
university
school




stuart had a headache last nigh which was due to thï¿½ï¿½ï¿½
stuart
ddd
arts


rah basically found a new found friend which lead to
rah
ddd
arts


gerome got a brand new watch which wasï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½.
na
na
na


adam was found chilling all through out his lifeï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
adam
eee
life science


savannah is such a common name thatï¿½ï¿½ï¿½ï¿½ï¿½ï¿½..
savannah
bbb
business

</tbod","['python', 'pandas', 'string', 'nltk', 'matching']",74092583,"you can melt and merge:
import re

df1_melt = df1.melt(['university', 'school'], value_name='match')

regex = '|'.join(map(re.escape, df1_melt['match']))

out = df2.join(
 df1_melt[['match', 'university', 'school']]
    .merge(df2['statement']
              .str.extract(f'({regex})', expand=false)
              .rename('match'),
           how='right', on='match'
          )
)

output:
                                                  statement     match university        school
0      stuart had a headache last nigh which was due to the    stuart        ddd          arts
1  rah basically found a new found friend which lead to the       rah        ddd          arts
2                    gerome got a brand new watch which was       nan        nan           nan
3          adam was found chilling all through out his life      adam        eee  life science
4                       savannah is such a common name that  savannah        bbb      business",https://stackoverflow.com/questions/74091764,python,17-10-2022 01:20,136.0,3.0,4.0,True,17-10-2022 04:26,17-10-2022 01:26
13423919,computing n grams using python,"i needed to compute the unigrams,  bigrams and trigrams for a text file containing text like: 
""cystic fibrosis affects 30,000 children and young adults in the us alone
inhaling the mists of salt water can reduce the pus and infection that fills the airways of cystic fibrosis sufferers, although side effects include a nasty coughing fit and a harsh taste. 
that's the conclusion of two studies published in this week's issue of the new england journal of medicine.""
i started in python and used the following code:
#!/usr/bin/env python
# file: n-gram.py
def n_gram(n,text):
nlist = []                      # start with an empty list
if n> 1:
    space = "" "" * (n-1)         # add n - 1 spaces
    text = space + text + space # add both in front and back
# append the slices [i:i+n] to nlist
for i in range( len(text) - (n - 1) ):
    nlist.append(text[i:i+n])
return nlist                    # return the list
# test code
for i in range(5):
print n_gram(i+1,""text"")
# more test code
nlist = n_gram(7,""here is a lot of text to print"")
for ngram in iter(nlist):
print '""' + ngram + '""'


but it works for all the n-grams within a word, when i want it from between words as in cystic and fibrosis or cystic fibrosis. can someone help me out as to how i can get this done?","['python', 'nlp', 'nltk', 'n-gram']",13424002,"assuming input is a string contains space separated words, like x = ""a b c d"" you can use the following function (edit: see the last function for a possibly more complete solution):
def ngrams(input, n):
    input = input.split(' ')
    output = []
    for i in range(len(input)-n+1):
        output.append(input[i:i+n])
    return output

ngrams('a b c d', 2) # [['a', 'b'], ['b', 'c'], ['c', 'd']]

if you want those joined back into strings, you might call something like:
[' '.join(x) for x in ngrams('a b c d', 2)] # ['a b', 'b c', 'c d']

lastly, that doesn't summarize things into totals, so if your input was 'a a a a', you need to count them up into a dict:
for g in (' '.join(x) for x in ngrams(input, 2)):
    grams.setdefault(g, 0)
    grams[g] += 1

putting that all together into one final function gives:
def ngrams(input, n):
   input = input.split(' ')
   output = {}
   for i in range(len(input)-n+1):
       g = ' '.join(input[i:i+n])
       output.setdefault(g, 0)
       output[g] += 1
   return output

ngrams('a a a a', 2) # {'a a': 3}",https://stackoverflow.com/questions/13423919,python,16-11-2012 20:26,108000.0,36.0,8.0,True,26-07-2022 15:48,03-06-2015 00:55
73610869,the expanded size of the tensor (1011) must match the existing size (512) at non-singleton dimension 1,"i have a trained a layoutlmv2 model from huggingface and when i try to inference it on a single image, it gives the runtime error. the code for this is below:
query = '/users/vaihabsaxena/desktop/newfolder/labeled/others/two.pdf26.png'
image = image.open(query).convert(""rgb"")
encoded_inputs = processor(image, return_tensors=""pt"").to(device)
outputs = model(**encoded_inputs)
preds = torch.softmax(outputs.logits, dim=1).tolist()[0]
pred_labels = {label:pred for label, pred in zip(label2idx.keys(), preds)}
pred_labels

the error comes when when i do model(**encoded_inputs). the processor is called directory from huggingface and is initialized as follows along with other apis:
feature_extractor = layoutlmv2featureextractor()
tokenizer = layoutlmv2tokenizer.from_pretrained(""microsoft/layoutlmv2-base-uncased"")
processor = layoutlmv2processor(feature_extractor, tokenizer)

the model is defined and trained as follows:
model = layoutlmv2forsequenceclassification.from_pretrained(
    ""microsoft/layoutlmv2-base-uncased"",  num_labels=len(label2idx)
)
model.to(device);


optimizer = adamw(model.parameters(), lr=5e-5)
num_epochs = 3


for epoch in range(num_epochs):
    print(""epoch:"", epoch)
    training_loss = 0.0
    training_correct = 0
    #put the model in training mode
    model.train()
    for batch in tqdm(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss

        training_loss += loss.item()
        predictions = outputs.logits.argmax(-1)
        training_correct += (predictions == batch['labels']).float().sum()

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    print(""training loss:"", training_loss / batch[""input_ids""].shape[0])
    training_accuracy = 100 * training_correct / len(train_data)
    print(""training accuracy:"", training_accuracy.item())  
        
    validation_loss = 0.0
    validation_correct = 0
    for batch in tqdm(valid_dataloader):
        outputs = model(**batch)
        loss = outputs.loss

        validation_loss += loss.item()
        predictions = outputs.logits.argmax(-1)
        validation_correct += (predictions == batch['labels']).float().sum()

    print(""validation loss:"", validation_loss / batch[""input_ids""].shape[0])
    validation_accuracy = 100 * validation_correct / len(valid_data)
    print(""validation accuracy:"", validation_accuracy.item())

the complete error trace:
runtimeerror                              traceback (most recent call last)
/users/vaihabsaxena/desktop/newfolder/pytorch.ipynb cell 37 in <cell line: 4>()
      2 image = image.open(query).convert(""rgb"")
      3 encoded_inputs = processor(image, return_tensors=""pt"").to(device)
----> 4 outputs = model(**encoded_inputs)
      5 preds = torch.softmax(outputs.logits, dim=1).tolist()[0]
      6 pred_labels = {label:pred for label, pred in zip(label2idx.keys(), preds)}

file ~/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130, in module._call_impl(self, *input, **kwargs)
   1126 # if we don't have any hooks, we want to skip the rest of the logic in
   1127 # this function, and just call forward.
   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1129         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1130     return forward_call(*input, **kwargs)
   1131 # do not call functions when jit is used
   1132 full_backward_hooks, non_full_backward_hooks = [], []

file ~/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py:1071, in layoutlmv2forsequenceclassification.forward(self, input_ids, bbox, image, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)
   1061 visual_position_ids = torch.arange(0, visual_shape[1], dtype=torch.long, device=device).repeat(
   1062     input_shape[0], 1
   1063 )
   1065 initial_image_embeddings = self.layoutlmv2._calc_img_embeddings(
   1066     image=image,
   1067     bbox=visual_bbox,
...
    896     input_shape[0], 1
    897 )
    898 final_position_ids = torch.cat([position_ids, visual_position_ids], dim=1)

runtimeerror: the expanded size of the tensor (1011) must match the existing size (512) at non-singleton dimension 1.  target sizes: [1, 1011].  tensor sizes: [1, 512]

i have tried to set up the tokenizer to cut off the max length but it finds encoded_inputs as nonetype however the image is still there. what is going wrong here?","['python', 'machine-learning', 'computer-vision', 'huggingface-transformers']",73613559,"the error message tells you that the extracted text via ocr is longer (1011 tokens) than the underlying text model is able to handle (512 tokens). depending on your task, you maybe can truncate your text with the tokenizer parameter truncation (the processor will pass this parameter to the tokenizer):
import torch
from transformers import layoutlmv2featureextractor, layoutlmv2tokenizer, layoutlmv2processor, layoutlmv2forsequenceclassification
from pil import image, imagedraw, imagefont

query = ""/content/screenshot_20220905_202551.png""
image = image.open(query).convert(""rgb"")

feature_extractor = layoutlmv2featureextractor()
tokenizer = layoutlmv2tokenizer.from_pretrained(""microsoft/layoutlmv2-base-uncased"")
processor = layoutlmv2processor(feature_extractor, tokenizer)
model = layoutlmv2forsequenceclassification.from_pretrained(""microsoft/layoutlmv2-base-uncased"",  num_labels=2)

encoded_inputs = processor(image, return_tensors=""pt"")
# model will raise an error because the tensor is longer as the trained position embeddings
print(encoded_inputs[""input_ids""].shape)
encoded_inputs = processor(image, return_tensors=""pt"", truncation=true)
print(encoded_inputs[""input_ids""].shape)
outputs = model(**encoded_inputs)
preds = torch.softmax(outputs.logits, dim=1).tolist()[0]

output:
torch.size([1, 644])
torch.size([1, 512])

for this code, i used the following screenshot:",https://stackoverflow.com/questions/73610869,python,05-09-2022 14:23,17266.0,4.0,1.0,True,05-09-2022 18:42,05-09-2022 16:32
78538749,not able to install spacy==2.3.5 version,"i tried to install spacy==2.3.5 for a resume analyser program. encountered with a pip subprocess to install build dependencies did not run successfully error.
using python 3.12.3
also it gives a e053 config file error when running the program regarding pyresparser:
""oserror: [e053] could not read config file from c:\smart_resume_analyser_app-master.venv\lib\site-packages\pyresparser\config.cfg""
`(.venv) ps c:\smart_resume_analyser_app-master> pip install spacy==2.3.5

 
    getting requirements to build wheel did not run successfully.
    exit code: 1

    [267 lines of output]

    error compiling cython file:
    ------------------------------------------------------------
    ...
        len_t* widths
        int i
        int nr_layer
        int batch_size

        __init__(len_t* widths, int nr_layer, int batch_size) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:140:4: function definition in pxd file must be declared 'cdef inline'

    error compiling cython file:
    ------------------------------------------------------------
    ...
            this._nr_feat = <len_t*>calloc(batch_size, sizeof(len_t))
            this._is_valid = <int*>calloc(batch_size * widths[nr_layer-1], sizeof(int))
            this._costs = <weight_t*>calloc(batch_size * widths[nr_layer-1], sizeof(weight_t))
            this.signatures = <uint64_t*>calloc(batch_size, sizeof(uint64_t))

        __dealloc__() nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:157:4: function definition in pxd file must be declared 'cdef inline'

    error compiling cython file:
    ------------------------------------------------------------
    ...
            free(this._nr_feat)
            free(this._is_valid)
            free(this._costs)
            free(this.signatures)

        void reset() nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:172:4: function definition in pxd file must be declared 'cdef inline'

    error compiling cython file:
    ------------------------------------------------------------
    ...
            for i in range(this.i):
                free(this._feats[i])
                this._feats[i] = null
            this.i = 0

        int nr_in() nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:189:4: function definition in pxd file must be declared 'cdef inline'

    error compiling cython file:
    ------------------------------------------------------------
    ...
            this.i = 0

        int nr_in() nogil:
            return this.widths[0]

        int nr_out() nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:192:4: function definition in pxd file must be declared 'cdef inline'

    error compiling cython file:
    ------------------------------------------------------------
    ...
            return this.widths[0]

        int nr_out() nogil:
            return this.widths[this.nr_layer - 1]

        int push_back(const featurec* feats, int nr_feat,
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:195:4: function definition in pxd file must be declared 'cdef inline'

    error compiling cython file:
    ------------------------------------------------------------
    ...
                for i in range(this.nr_out()):
                    this.is_valid(this.i)[i] = 1
            this.i += 1
            return this.i >= this.batch_size

        featurec* features(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:226:4: function definition in pxd file must be declared 'cdef inline'

    error compiling cython file:
    ------------------------------------------------------------
    ...
            return this.i >= this.batch_size

        featurec* features(int i) nogil:
            return this._feats[i]

        int nr_feat(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:229:4: function definition in pxd file must be declared 'cdef inline'

    error compiling cython file:
    ------------------------------------------------------------
    ...
            return this._feats[i]

        int nr_feat(int i) nogil:
            return this._nr_feat[i]

        weight_t* fwd(int i, int j) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:232:4: function definition in pxd file must be declared 'cdef inline'

    error compiling cython file:
    ------------------------------------------------------------
    ...
            return this._nr_feat[i]

        weight_t* fwd(int i, int j) nogil:
            return this._fwd[i] + (j * this.widths[i])

        weight_t* bwd(int i, int j) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:235:4: function definition in pxd file must be declared 'cdef inline'

    error compiling cython file:
    ------------------------------------------------------------
    ...
            return this._fwd[i] + (j * this.widths[i])

        weight_t* bwd(int i, int j) nogil:
            return this._bwd[i] + (j * this.widths[i])

        weight_t* scores(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:238:4: function definition in pxd file must be declared 'cdef inline'

    error compiling cython file:
    ------------------------------------------------------------
    ...
            return this._bwd[i] + (j * this.widths[i])

        weight_t* scores(int i) nogil:
            return this.fwd(this.nr_layer-1, i)

        weight_t* losses(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:241:4: function definition in pxd file must be declared 'cdef inline'

    error compiling cython file:
    ------------------------------------------------------------
    ...
            return this.fwd(this.nr_layer-1, i)

        weight_t* losses(int i) nogil:
            return this.bwd(this.nr_layer-1, i)

        weight_t* costs(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:244:4: function definition in pxd file must be declared 'cdef inline'

    error compiling cython file:
    ------------------------------------------------------------
    ...
            return this.bwd(this.nr_layer-1, i)

        weight_t* costs(int i) nogil:
            return this._costs + (i * this.nr_out())

        int* is_valid(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:247:4: function definition in pxd file must be declared 'cdef inline'

    error compiling cython file:
    ------------------------------------------------------------
    ...
            return this._costs + (i * this.nr_out())

        int* is_valid(int i) nogil:
            return this._is_valid + (i * this.nr_out())

        int guess(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:250:4: function definition in pxd file must be declared 'cdef inline'

    error compiling cython file:
    ------------------------------------------------------------
    ...
            return this._is_valid + (i * this.nr_out())

        int guess(int i) nogil:
            return vecvec.arg_max_if_true(this.scores(i), this.is_valid(i), this.nr_out())

        int best(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:253:4: function definition in pxd file must be declared 'cdef inline'
    warning: thinc\linalg.pxd:14:0: the 'if' statement is deprecated and will be removed in a future cython version. consider using runtime conditions or c macros instead. see 
    warning: thinc\linalg.pxd:90:8: the 'if' statement is deprecated and will be removed in a future cython version. consider using runtime conditions or c macros instead. see 
    warning: thinc\linalg.pxd:174:8: the 'if' statement is deprecated and will be removed in a future cython version. consider using runtime conditions or c macros instead. see 
    compiling thinc/linalg.pyx because it changed.
    compiling thinc/structs.pyx because it changed.
    compiling thinc/typedefs.pyx because it changed.
    compiling thinc/linear/avgtron.pyx because it changed.
    compiling thinc/linear/features.pyx because it changed.
    compiling thinc/linear/serialize.pyx because it changed.
    compiling thinc/linear/sparse.pyx because it changed.
    compiling thinc/linear/linear.pyx because it changed.
    compiling thinc/neural/optimizers.pyx because it changed.
    compiling thinc/neural/ops.pyx because it changed.
    compiling thinc/neural/_aligned_alloc.pyx because it changed.
    compiling thinc/extra/eg.pyx because it changed.
    compiling thinc/extra/mb.pyx because it changed.
    compiling thinc/extra/search.pyx because it changed.
    compiling thinc/extra/cache.pyx because it changed.
    [ 1/15] cythonizing thinc/extra/cache.pyx
    [ 2/15] cythonizing thinc/extra/eg.pyx
    traceback (most recent call last):
      file ""c:\smart_resume_analyser_app-master\.venv\lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 353, in <module>
        main()
      file ""c:\smart_resume_analyser_app-master\.venv\lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 335, in main
        json_out['return_val'] = hook(**hook_input['kwargs'])
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      file ""c:\smart_resume_analyser_app-master\.venv\lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 118, in get_requires_for_build_wheel      
        return hook(config_settings)
               ^^^^^^^^^^^^^^^^^^^^^
      file ""c:\users\vasud\appdata\local\temp\pip-build-env-iv7ops9s\overlay\lib\site-packages\setuptools\build_meta.py"", line 325, in get_requires_for_build_wheel
        return self._get_build_requires(config_settings, requirements=['wheel'])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      file ""c:\users\vasud\appdata\local\temp\pip-build-env-iv7ops9s\overlay\lib\site-packages\setuptools\build_meta.py"", line 295, in _get_build_requires
        self.run_setup()
      file ""c:\users\vasud\appdata\local\temp\pip-build-env-iv7ops9s\overlay\lib\site-packages\setuptools\build_meta.py"", line 311, in run_setup
        exec(code, locals())
      file ""<string>"", line 258, in <module>
      file ""<string>"", line 195, in setup_package
      file ""c:\users\vasud\appdata\local\temp\pip-build-env-iv7ops9s\overlay\lib\site-packages\cython\build\dependencies.py"", line 1154, in cythonize
        cythonize_one(*args)
      file ""c:\users\vasud\appdata\local\temp\pip-build-env-iv7ops9s\overlay\lib\site-packages\cython\build\dependencies.py"", line 1321, in cythonize_one
        raise compileerror(none, pyx_file)
    cython.compiler.errors.compileerror: thinc/extra/eg.pyx
    [end of output]

    note: this error originates from a subprocess, and is likely not a problem with pip.
  error: subprocess-exited-with-error

  getting requirements to build wheel did not run successfully.
  exit code: 1

  see above for output.

  note: this error originates from a subprocess, and is likely not a problem with pip.
  [end of output]

note: this error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error
ï¿½ï¿½ pip subprocess to install build dependencies did not run successfully.
ï¿½c:code: 1
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½> see above for output.
note: this error originates from a subprocess, and is likely not a problem with","['python', 'parsing', 'pip', 'nlp', 'spacy']",78541534,"try:
pip install 

or
pip install",https://stackoverflow.com/questions/78538749,python,27-05-2024 11:20,213.0,1.0,1.0,True,28-05-2024 21:29,28-05-2024 21:29
73515496,"getting &quot;valueerror: shapes (none, 2) and (none, 1) are incompatible&quot; for binary classification when i am using &quot;tf.keras.metrics.recall()&quot;","i am working distillbert project for binary classification. i am trying to run the following code using the spam sms data set (you can also use the imdb dataset, it is also giving the same issue), which runs fine using 'accuracy' and 'sparse_categorical_accuracy' but gives an error when i am using tf.keras.metrics.recall() or tf.keras.metrics.auc(). here i am using the sparsecategoricalcrossentropy loss function and ada optimizer.
dataset - the dataset used here is the spam sms dataset which has binary labels 0 for normal sms, and 1 for spam sms. the same error can be reproduced using the imdb data set for this code.
error -
valueerror: shapes (none, 2) and (none, 1) are incompatible

code -
import pandas as pd
import tensorflow as tf
import transformers
from transformers import distilberttokenizer
from transformers import tfautomodelforsequenceclassification
pd.set_option('display.max_colwidth', none)
model_name = 'distilbert-base-uncased'
batch_size = 8
n_epochs = 3

train = pd.read_csv(""train_set.csv"", error_bad_lines=false)
test = pd.read_csv(""test_set.csv"", error_bad_lines=false)

x_train = train.text
x_test = test.text
y_train = train.label
y_test = test.label

tokenizer = distilberttokenizer.from_pretrained(model_name)

train_encodings = tokenizer(list(x_train.values),
                        truncation=true, 
                        padding=true)
test_encodings = tokenizer(list(x_test.values),
                       truncation=true, 
                       padding=true)

train_dataset = 
tf.data.dataset.from_tensor_slices((dict(train_encodings),list(y_train.values)))

test_dataset = 
tf.data.dataset.from_tensor_slices((dict(test_encodings),list(y_test.values)))
test_dataset2 = test_dataset.shuffle(buffer_size=1024).take(1000).batch(16)

model = tfautomodelforsequenceclassification.from_pretrained(model_name)

optimizerr = tf.keras.optimizers.adam(learning_rate=5e-5)

losss = tf.keras.losses.sparsecategoricalcrossentropy(from_logits=true)

model.compile(optimizer=optimizerr,
          loss=losss,
          metrics= 
['accuracy','sparse_categorical_accuracy',tf.keras.metrics.recall()])

print(""evaluate base model on test data"")
results = model.evaluate(test_dataset2)
print(""test loss, test acc:"", results)

how can i get the recall, precision, auc and other metrics scores for this code?
edit - when i am using loss function
loss=tf.keras.losses.binarycrossentropy(from_logits=true)

the error that i am getting is -
valueerror: logits and labels must have the same shape ((none, 2) vs (none, 1))","['python', 'tensorflow', 'machine-learning', 'keras', 'bert-language-model']",73515780,you typically don't use sparse categorical cross entropy for a binary classification problem. just use binary cross entropy. that is most likely why the tensor sizes don't match up.,https://stackoverflow.com/questions/73515496,python,28-08-2022 01:54,344.0,1.0,1.0,True,28-08-2022 23:05,28-08-2022 23:05
77380358,retrieve page from the pdf in pdf-chatbot using langchain,"i have developed a small app based on langchain and streamlit, where user can ask queries using pdf files. the code is mentioned as below:
from dotenv import load_dotenv
import streamlit as st
from pypdf2 import pdfreader
from langchain.text_splitter import charactertextsplitter
from langchain.embeddings.openai import openaiembeddings
from langchain.vectorstores import faiss
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import openai
from langchain.callbacks import get_openai_callback


def main():
    load_dotenv()
    st.set_page_config(page_title=""ask your pdf"")
    st.header(""ask your pdf ï¿½ï¿½ï¿½ï¿½"")
    
    # upload file
    pdf = st.file_uploader(""upload your pdf"", type=""pdf"")
    
    # extract the text
    if pdf is not none:
      pdf_reader = pdfreader(pdf)
      text = """"
      for page in pdf_reader.pages:
        text += page.extract_text()
        
      # split into chunks
      text_spliharactertextsplitter(
        separator=""\n"",
        chunk_size=500,
        chunk_overlap=100,
        length_function=len
      )
      chunks = text_splitter.split_text(text)
      
      # create embeddings
      embeddings = openaiembeddings()
      knowledge_base = faiss.from_texts(chunks, embeddings)
      
      # show user input
      user_question = st.text_input(""ask a question about your pdf:"")
      if user_question:
        docs = knowledge_base.similarity_search(user_question)
        
        llm = openai()
        chain = load_qa_chain(llm)
        with get_openai_callback() as cb:
          response = chain.run(input_documents=docs, question=user_question)
          print(cb)
           
        st.write(response)
    

if __name__ == '__main__':
    main()

can someone suggest that how i can retrieve or render the page of the pdf from where answer or information has been extracted?
i have came across this but won't able to implement it properly.","['streamlit', 'openai-api', 'langchain', 'large-language-model', 'nlp-question-answering']",77421139,"here is a simple approach.

while reading the pdf, also save the content per page and the page number.

    # extract the text
    if pdf is not none:
        pdf_reader = pdfreader(pdf)
        text = """"

        page_dict = {}
        for i, page in enumerate(pdf_reader.pages):
            page_content = page.extract_text()
            text += page_content + '\n\n'
            page_dict[page_content] = i+1

once we get the response, we will compare it with the content of each page that we have saved before. the idea is to get which page gets the highest similarity to the response. it can be page 1, page 2, etc.
            # get the similarity between each page and response.
            # use spacy model (free). openai similarity can be expensive
            # but maybe more accurate.
            data = []
            for page_content, page_num in page_dict.items():
                similarity = spacy_sim(response, page_content)
                data.append([similarity, page_num])

sort the data and get the page with highest similarity.
            # sort the similarity score fron high to low.
            data = sorted(data, key=lambda x: x[0], reverse=true)
            print(data)

            # get the top page number.
            top_page_num = data[0][1]

now generate all the images per page, using the library pdf2image. we are going to show the content of the page as an image. you can do other methods as we already have the content of the page. but in this approach i will show the image via streamlit image widget.
            # generate images per page in the pdf.
            images = convert_from_path(pdf.name)

now that we have a list of images, get the index that corresponds to the page that we want to show.
            # show the the page image with the highest similarity.
            st.image(images[top_page_num-1])

here is the code to get the similarity score between page content and response.
def spacy_sim(str1, str2):
    """"""model en_core_web_lg should be better""""""
    nlp = spacy.load(""en_core_web_md"")
    doc_1 = nlp(str1)
    doc_2 = nlp(str2)
    return doc_1.similarity(doc_2)

sample output

you can download a sample pdf from my google drive.
full code
from dotenv import load_dotenv
import streamlit as st
from pypdf2 import pdfreader
from langchain.text_splitter import charactertextsplitter
from langchain.embeddings.openai import openaiembeddings
from langchain.vectorstores import faiss
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import openai
from langchain.callbacks import get_openai_callback
from pdf2image import convert_from_path
import spacy


secret = 'abc'


def spacy_sim(str1, str2):
    nlp = spacy.load(""en_core_web_md"")
    doc_1 = nlp(str1)
    doc_2 = nlp(str2)
    return doc_1.similarity(doc_2)


def main():
    load_dotenv()
    st.set_page_config(page_title=""ask your pdf"")
    st.header(""ask your pdf ï¿½ï¿½ï¿½ï¿½"")

    # upload file, should be in the same location with the streamlit script.
    pdf = st.file_uploader(""upload your pdf"", type=""pdf"")

    # extract the text
    if pdf one:
        pdf_reader = pdfreader(pdf)
        text = """"

        page_dict = {}
        for i, page in enumerate(pdf_reader.pages):
            page_content = page.extract_text()
            text += page_content + '\n\n'
            page_dict[page_content] = i+1

        # split into chunks
        text_splitter = charactertextsplitter(
            separator=""\n"",
            chunk_size=500,
            chunk_overlap=100,
            length_function=len
        )
        chunks = text_splitter.split_text(text)

        # create embeddings
        embeddings = openaiembeddings(openai_api_key=secret)
        knowledge_base = faiss.from_texts(chunks, embeddings)

        # show user input
        user_question = st.text_input(""ask a question about your pdf:"")
        if user_question:
            docs = knowledge_base.similarity_search(user_question)

            llm = openai(openai_api_key=secret)
            chain = load_qa_chain(llm)
            with get_openai_callback() as cb:
                response = chain.run(input_documents=docs,
                                     question=user_question)
                print(f'billing details: {cb}')

            # get the similarity between each page and response.
            # use spacy model (free). openai similarity can be expensive
            # but maybe more accurate.
            data = []
            for page_content, page_num in page_dict.items():
                similarity = spacy_sim(response, page_content)
                data.append([similarity, page_num])

            # sort the similarity score from high to low.
            data = sorted(data, key=lambda x: x[0], reverse=true)
            print(data)

            # get the top page number.
            top_page_num = data[0][1]

            st.write(f""answer: {response}"")

            # generate images per page from the pdf.
            images = convert_from_path(pdf.name)

            # show the page image with the highest similarity.
            st.image(images[top_page_num-1])


if __name__ == '__main__':
    main()

use similarity from openai api.
import openai

openai.api_key  = secret

def openai_sim(str1, str2):
    # call the api
    response = openai.embedding.create(
        input=[str1, str2],
        model=""text-embedding-ada-002""
    )

    # extract the embeddings
    embedding1 = response['data'][0]['embedding']
    embedding2 = response['data'][1]['embedding']

    # calculate cosine similarity
    similarity_score = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))

    return similarity_score

use sentence-transfomer for similarity.
def transformer_sim(str1, str2):
    """"""
    install pytorch:
        

    install sentence-transformers:
        pip install -u sentence-transformers

    from sentence_transformers import sentencetransformer, util
    """"""
    model = sentencetransformer('all-minilm-l6-v2')
    embeddings1 = model.encode(str1, convert_to_tensor=true)
    embeddings2 = model.encode(str2, convert_to_tensor=true)
    cosine_score = util.cos_sim(embeddings1, embeddings2)
    simscore = float(cosine_score[0][0])

    return simscore

solution 2
uses pymupdf to save the text and save the images per page. uploaded file can be from anywhere not necessarily from the location of the streamlit script because while we are saving the text on each pdf page, we also save the images as data bytes.
this also uses the sentence-transformer to measure similarity of two text strings useful for page content and response comparison.
full code
""""""using sentence-transfomer for similarity score.""""""


from dotenv import load_dotenv
import streamlit as st
from langchain.text_splitter import charactertextsplitter
from langchain.embeddings.openai import openaiembeddings
from langchain.vectorstores import faiss
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import openai
from langchain.callbacks import get_openai_callback
from sentence_transformers import sentencetransformer, util
import fitz  # pymupdf


secret = 'abc'


def transformer_sim(str1, str2):
    """"""
    install pytorch:
        

    install sentence-transformers:
        pip install -u sentence-transformers

    from sentence_transformers import sentencetransformer, util
    """"""
    model = sentencetransformer('all-minilm-l6-v2')
    embeddings1 = model.encode(str1, convert_to_tensor=true)
    embeddings2 = model.encode(str2, convert_to_tensor=true)
    cosine_score = util.cos_sim(embeddings1, embeddings2)
    simscore = float(cosine_score[0][0])

    return simscore


def main():
    load_dotenv()
    st.set_page_config(page_title=""ask your pdf"")
    st.header(""ask your pdf ï¿½ï¿½ï¿½ï¿½"")

    # upload file, should be in thocation with the streamlit script.
    pdf = st.file_uploader(""upload your pdf"", type=""pdf"")

    # extract the text
    if pdf is not none:
        text = """"
        images = []
        page_dict = {}

        with fitz.open(stream=pdf.read(), filetype=""pdf"") as pdf_pages:            
            for i, page in enumerate(pdf_pages):
                page_content = page.get_text()
                text += page_content + '\n\n'
                page_dict[page_content] = i+1

                # images
                pix = page.get_pixmap()
                bytes_data = pix.tobytes(""png"")
                images.append(bytes_data)

        # split into chunks
        text_splitter = charactertextsplitter(
            separator=""\n"",
            chunk_size=500,
            chunk_overlap=100,
            length_function=len
        )
        chunks = text_splitter.split_text(text)

        # create embeddings
        embeddings = openaiembeddings(openai_api_key=secret)
        knowledge_base = faiss.from_texts(chunks, embeddings)

        # show user input
        user_question = st.text_input(""ask a question about your pdf:"")
        if user_question:
            docs = knowledge_base.similarity_search(user_question)

            llm = openai(openai_api_key=secret)
            chain = load_qa_chain(llm)
            with get_openai_callback() as cb:
                response = chain.run(input_documents=docs,
                                     question=user_question)
                print(f'billing details: {cb}')

            # get the similarity between each page and response.
            data = []
            for page_content, page_num in page_dict.items():
                similarity = transformer_sim(response, page_content)
                data.append([similarity, page_num, page_content])

            # sort the similarity score fron high to low.
            data = sorted(data, key=lambda x: x[0], reverse=true)

            # get the top page number.
            top_page_num = data[0][1]
            top_sim_score = data[0][0]

            st.write(f""answer: {response}"")
            st.markdown(f'**there is a top similarity score of {top_sim_score} that the response is from page {top_page_num}**')

            # show the the page image with the highest similarity.
            st.image(images[top_page_num-1])


if __name__ == '__main__':
    main()",https://stackoverflow.com/questions/77380358,streamlit,28-10-2023 16:46,1849.0,1.0,2.0,True,07-11-2023 12:35,01-11-2023 05:18
77708996,how to convert model.safetensor to pytorch_model.bin?,"i'm fine tuning a pre-trained bert model and i have a weird problem:
when i'm fine tuning using the cpu, the code saves the model like this:

with the ""pytorch_model.bin"". but when i use cuda (that i have to), the model is saved like this:

when i try to load this ""model.safetensors"" in the future, it raises an error ""pytorch_model.bin"" not found. i'm using two differents venvs to test using the cpu and cuda.
how to solve this? is some version problem?
i'm using sentence_transformers framework to fine-tune the model.
here's my training code:
checkpoint = 'sentence-transformers/paraphrase-multilingual-minilm-l12-v2'

word_embedding_model = models.transformer(checkpoint, cache_dir=f'model/{checkpoint}')
pooling_model = models.pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode='mean')
model = sentencetransformer(modules=[word_embedding_model, pooling_model], device='cuda')


train_loss = losses.cosinesimilarityloss(model)

evaluator = evaluation.embeddingsimilarityevaluator.from_input_examples(val_examples, name='sbert')

model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=5, evaluator=evaluator, show_progress_bar=true, output_path=f'model_ft/{checkpoint}', save_best_model=true)

i did try the tests in two differentes venvs, and i'm expecting the code to save a ""pytorch_model.bin"" not a ""model.safetensors"".
edit: i really don't know yet, but it seems that is the newer versions of transformers library that causes this problem. i saw that with hugging-face is possible to load the safetensors, but with sentence-transformers (that i need to use) it's not.","['machine-learning', 'pytorch', 'bert-language-model', 'sentence-transformers']",77759913,"probably you figured it out already but updating the transformer library now to the newest version resolves the issue.
pip install -u transformers

u don't need to transform the model anymore you can load the load the model.safetensor with sentencetransformer(""modelpath"")",https://stackoverflow.com/questions/77708996,machine-learning,23-12-2023 20:43,13319.0,6.0,2.0,True,21-03-2024 08:48,24-12-2023 00:21
69443805,how to find the similarity of sentences in 2 columns of a dataframe using spacy,"i pulled this code from 
import spacy_sentence_bert
# load one of the models listed at 
nlp = spacy_sentence_bert.load_model('en_roberta_large_nli_stsb_mean_tokens')
# get two documents
doc_1 = nlp('hi there, how are you?')
doc_2 = nlp('hello there, how are you doing today?')
# use the similarity method that is based on the vectors, on doc, span or token
print(doc_1.similarity(doc_2[0:7]))

i have a dataframe with 2 columns containing sentences like below. i'm trying to find the similarity between the sentences in each row. i've tried a few different methods but not having much luck so figured i would ask here. thank you all.
current df
sentence1 | sentence2

another-sentence1 | another-sentence2

yet-another-sentence1 | yet-another-sentence2

goal output:
sentence1 | sentence2 | similarity-score-sentence1-sentence2

another-sentence1 | another-sentence2 | similarity-score-another-sentence1-another-sentence2

yet-another-sentence1 | yet-another-sentence2 | similarity-score-yet-another-sentence1-yet-another-sentence2","['python', 'pandas', 'spacy', 'similarity', 'bert-language-model']",69444338,"i assume that your first row consists of headers, the data will start from the next row after header, and also assume that you are using panda to convert csv to dataframe, the below code works in my environment.
import spacy_sentence_bert
import pandas as pd
nlp = spacy_sentence_bert.load_model('en_roberta_large_nli_stsb_mean_tokens')
df = pd.read_csv('testing.csv')
similarityvalue = []

for i in range(df.count()[0]):
    sentence_1 = nlp(df.iloc[i][0])
    sentence_2 = nlp(df.iloc[i][1])
    similarityvalue.append(sentence_1.similarity(sentence_2))
    print(sentence_1, '|', sentence_2, '|', sentence_1.similarity(sentence_2))

df['similarity'] = similarityvalue
print(df)

input csv:

output:",https://stackoverflow.com/questions/69443805,python,05-10-2021 00:56,2333.0,3.0,1.0,True,04-03-2024 01:34,04-03-2024 01:34
70893269,keras 1d segmentation model always classifies even number of items,"i'm trying to train a 1d cnn to identify specific parts of a text string.
the inputs are arrays of shape (128,1) containing 128 characters, and the aim is for the network to classify each of the characters into a particular class. for purposes of illustration, an input array could look like this:
array(['3', '!', 'd', 'o', 'g', '.', '?', '8', '7', 'a', 'p', 'p', 'l',
       'e', 'f', 'd', '$', '5'], dtype='<u1')

and the corresponding label looks like this:
array([0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0])

the idea being that the network will classify the characters ""d"", ""o"", ""g"" into class 1 (say, animals) and ""a"", ""p"", ""p"", ""l"", ""e"" into class 2 (fruits) and the rest into class 0.
the plan is to build a network with an architecture similar to u-net, but for now i'm experimenting with a very simple downsample/upsample network which looks like this:
def get_model(seq_size,n_classes):
    
    inputs = tf.keras.input(shape=seq_size)
    
#     downsample phase

    x = tf.keras.layers.conv1d(32,11,padding=""same"")(inputs)
    x = tf.keras.layers.batchnormalization()(x)
    x = tf.keras.layers.activation(""relu"")(x)
    
    x = tf.keras.layers.maxpooling1d(2,padding=""same"")(x)    
    
    x = tf.keras.layers.conv1d(64,5,padding=""same"")(x)
    x = tf.keras.layers.batchnormalization()(x)
    x = tf.keras.layers.activation(""relu"")(x)
    
    x = tf.keras.layers.maxpooling1d(2,padding=""same"")(x)  
    
#     upsample phase    
    
    x = tf.keras.layers.conv1dtranspose(128,5,padding=""same"")(x)
    x = tf.keras.layers.batchnormalization()(x)
    x = tf.keras.layers.activation(""relu"")(x)
    
    x = tf.keras.layers.upsampling1d(2)(x)  
    
    x = tf.keras.layers.conv1dtranspose(256,7,padding=""same"")(x)
    x = tf.keras.layers.batchnormalization()(x)
    x = tf.keras.layers.activation(""relu"")(x)
    
    x = tf.keras.layers.upsampling1d(2)(x)     
    
    outputs = tf.keras.layers.conv1d(n_classes,1,activation=""softmax"",padding=""same"")(x)
    
    model = tf.keras.model(inputs,outputs)
    return model 

with an input shape of (128,1) and n_classes = 5.
the model works quite well for a baseline, but it has an interesting quirk which i'm trying to get my head around: when it makes predictions over characters, it always classifies an even number of characters (or ""pixels"" if thinking about this as analogous to an image segmentation task). so in the above example it would identify !dog or dog. as belonging to class 1, and 7apple or applef as belonging to class 2.
this is only a problem if the word contains an odd number of characters, which makes me think that it's probably something to do with max-pooling and upsampling operations. i've tried to find an answer by understanding how these operations work in keras, but this hasn't been fruitful. so if anybody could shed some light onto why the predictions are always an even number of characters, and how i might rectify that, i would be very grateful!
edit from advice in the comments:
to clarify, the arrays are encoded simply using the ord function, and then min/max normalized to the range 0:1.
i'm using sparse categorical cross-entropy for the loss function, and the training setup is as follows:
loss = tf.keras.losses.sparsecategoricalcrossentropy()
opt = tf.keras.optimizers.adam()

model.compile(optimizer=opt,loss=loss,metrics=[""accuracy""])

callbacks = [tf.keras.callbacks.modelcheckpoint(""trial.h5"",save_best_only=true)]

epochs = 10
model.fit(train_gen, epochs=epochs, validation_data=test_gen, callbacks=callbacks)

where train_gen and test_gen are data generators built as a tf.keras.utils.sequence subclass.","['python', 'tensorflow', 'keras', 'nlp', 'conv-neural-network']",70963872,"i think when you use upsampling1d each value is repeated twice. which means the input to the last step contains pair-wise duplicated value. it would then give the same predicted class for adjancent characters. if my guess is correct, you would always see the same prediction for the 2k and 2k+1 characters.
you could confirm by inspecting the input x in
outputs = tf.keras.layers.conv1d(n_classes,1,activation=""softmax"",padding=""same"")(x)

it should look like [a, a, b, b, c, c, ...]
to solve the issue you probably can add an additional step between outputs = ... and x = tf.keras.layers.upsampling1d(2)(x)",https://stackoverflow.com/questions/70893269,python,28-01-2022 11:47,493.0,1.0,1.0,True,03-02-2022 09:11,03-02-2022 09:11
77433100,how to get perplexity per token rather than average perplexity?,"i can get the perplexity of a whole sentence from here:
device = ""cuda""
from transformers import gpt2lmheadmodel, gpt2tokenizerfast

device = ""cuda""
model_id = ""gpt2""
model = gpt2lmheadmodel.from_pretrained(model_id).to(device)
tokenizer = gpt2tokenizerfast.from_pretrained(model_id)
sent = 'happy birthday!'
input_ids = tokenizer(sent, return_tensors='pt')['input_ids']
target_ids = input_ids.clone()
outputs = model(input_ids.to(device), labels=target_ids)
ppl = torch.exp(outputs.loss)
print(ppl)
>>>tensor(1499.6934, device='cuda:0', grad_fn=<expbackward0>)

but how can i get the perplexity value for each token, instead of of the average perplexity of the entire sequence of tokens? the input sentence in this example, 'happy birthday!' is composed of 3 tokens. based on the formula for perplexity:

this should result in 3 values: log probability of the first token, log probability of the second token given the first, and the log probability of the third token given the first 2. each should be exponentiated to get the perplexity value of each token.
i currently have the following:
import torch
from transformers import gpt2lmheadmodel, gpt2tokenizerfast

device = ""cuda""
model_id = ""gpt2""
model = gpt2lmheadmodel.from_pretrained(model_id).to(device)
tokenizer = gpt2tokenizerfast.from_pretrained(model_id)

sent = 'happy birthday!'
input_ids = tokenizer(sent, return_tensors='pt')['input_ids'].to(device)
target_ids = input_ids.clone()

# initialize an empty list to store perplexities for each token
perplexities = []

# calculate perplexity for each token
for i in range(input_ids.shape[1]):
    output = model(input_ids[:, :i+1], labels=target_ids[:, :i+1])
    log_prob = output.loss.item()
    perplexity = torch.exp(torch.tensor(log_prob))
    perplexities.append(perplexity.item())

# perplexities is now a list containing the perplexity values for each token
for i, token in enumerate([tokenizer.decode(i) for i in input_ids[0]]):
    print(f""token: {token}, perplexity: {perplexities[i]}"")
    >>> token: happy, perplexity: nan
token:  birthday, perplexity: 54192.46484375
token: !, perplexity: 1499.693359375

but i'm not sure what i'm doing wrong, as the last token seem to have the same perplexity as the entire sentence.","['machine-learning', 'pytorch', 'nlp', 'huggingface-transformers', 'perplexity']",77433933,"this is happening because in the second code snippet, you loop over the input sequence by adding a new token at each iteration:
i=0: input_ids[:, :i+1] := tensor([[25082]], device='cuda:0')
i=1: input_ids[:, :i+1] := tensor([[25082, 33511]], device='cuda:0')
i=2: input_ids[:, :i+1] := tensor([[25082, 33511,     0]], device='cuda:0')

then, the computation of the perplexity in the last iteration of the loop is essentially identical to doing this:
outputs = model(input_ids.to(device), labels=target_ids)
ppl = torch.exp(outputs.loss)

here's how you can compute the perplexity and per-token perplexity (see 
import torch.nn.functional as f
[...]
sent = 'happy birthday!'
input_ids = tokenizer(sent, return_tensors='pt')['input_ids'].to(device)
labels = input_ids.clone()

output = model(input_ids, labels=labels)
logits = output.logits
shift_logits = logits[..., :-1, :].contiguous()
shift_labels = labels[..., 1:].contiguous()
loss = f.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1), reduction='none')
per_token_perplexity = torch.exp(loss)
average_perplexity = torch.exp(torch.mean(loss))
print(per_token_perplexity)
print(average_perplexity)

the output:
tensor([5.4192e+04, 4.1502e+01], device='cuda:0', grad_fn=<expbackward0>)
tensor(1499.6934, device='cuda:0', grad_fn=<expbackward0>)",https://stackoverflow.com/questions/77433100,machine-learning,06-11-2023 17:30,1750.0,2.0,2.0,True,09-01-2024 07:12,06-11-2023 18:03
73518635,can&#39;t use bloomai locally,"so i just finished installing bloom's model from huggingface & i tried to run it in my notebook.
here's the code:
from transformers import autotokenizer, automodel
model_path = ""d:/bloom""
tokenizer = autotokenizer.from_pretrained(model_path)
model = automodel.from_pretrained(model_path)

and i get this error:
defaultcpuallocator: not enough memory: you tried to allocate xxx bytes

so then i went on to google searching and found this article:

however, i get this error:
typeerror: build_alibi_tensor() missing 1 required positional argument: 'device'

how to run bloom locally?","['python', 'machine-learning', 'huggingface-transformers', 'huggingface', 'bloom']",73539954,i had that problem (and others beyond it) because i was using a more recent version of the transformers library than the one used in the article you linked.  try using transformers version 4.20.0 as the article mentions.,https://stackoverflow.com/questions/73518635,python,28-08-2022 12:22,882.0,0.0,1.0,True,30-08-2022 09:17,29-08-2022 14:08
76922671,why openai key is not working in spring boot,"@service
public class openaitranslationservice {

    private resttemplate resttemplate;

    public openaitranslationservice(resttemplate resttemplate) {
        this.resttemplate = resttemplate;
    }

    public openairesponse translate(string language, string text) {
         headers = new 
        headers.add(openaiconstants.authorization, openaiconstants.token);

        openairequest ai = new openairequest();
        ai.setmodel(openaiconstants.model);
        ai.setprompt(openaiconstants.translate + language + "": "" + text);
        ai.setfrequency_penalty(0);
        ai.setpresence_penalty(0);
         entity = new  headers);

        responseentity<openairesponse> data = resttemplate.exchange(""  entity, openairesponse.class);
        return data.getbody();
    }
}

@value(""${api.key}"")
    public static final string api_key = """";

    public static final string token = ""bearer "" + api_key;

@postmapping(""/translate"")
    public list<string> translate(@requestparam string language, @requestparam string text) {
        openairesponse response = service.translate(language, text);
        return response.getchoiceslist().stream().map(choices::gettext).collect(collectors.tolist());
    }

api key is defined as api.key in application.properties file. i send the request but response code is 401. where could there be a mistake?
i tried to create a new api key but the result is the same.","['spring', 'spring-boot', 'openai-api']",76922852,"most likely a copy of this:
how to assign a value from application.properties to a static variable?
here is a janky solution that i would not recommend: 
best use the accepted solution from the stackoverflow question itself. it nicely explains why its bad to inject values onto a static variable and what you can actually do.",https://stackoverflow.com/questions/76922671,spring,17-08-2023 14:49,1107.0,0.0,2.0,True,15-04-2024 11:57,17-08-2023 19:51
72552550,spacy dependency matcher parsing with pandas dataframe,"i am having difficulties passing a dataframe column through the spacy dependency matcher. i attempted to modify the solution found in a pervious question, 'spacy dependency parsing with pandas dataframe' but no luck.
import pandas as pd
import spacy
from spacy import displacy
from spacy.matcher import dependencymatcher
from spacy.symbols import nsubj, verb, dobj, noun

nlp = spacy.load(""en_core_web_lg"")
text = 'repaired connector on j3 sms. replaced the primary computer.'.lower()
dep_matcher  = dependencymatcher(vocab = nlp.vocab)
dep_pattern = [
    {
        ""right_id"": ""action"",
        ""right_attrs"": {'lemma' : {""in"": [""reseat"", ""cycle"", 'replace' , 'repair', 'reinstall' , 'clean', ' treat', 'splice', 'swap', 'read', 'inspect','installed' ]}}
    },

    {
        ""left_id"": ""action"",
        ""rel_op"": "">"",
        ""right_id"": ""component"",
        ""right_attrs"": {""dep"":{""in"": [ 'dobj']}},     
    }]

dep_matcher.add('maint_action' , patterns = [dep_pattern])
dep_matches = dep_matcher(doc)

for match in dep_matches:
    dep_pattern = match[0]
    matches = match[1]
    verb , subject = matches[0], matches[1] 
    print (nlp.vocab[dep_pattern].text, '\t' ,doc[verb] , doc[subject])
>>>maint_action   repaired connector
>>>maint_action   replaced computer 

passing a string, the above works perfectly. but when try passing a df the new column returns blank.
heres the function for df:
import pandas as pd
    import spacy
    from spacy import displacy
    from spacy.matcher import dependencymatcher
    from spacy.symbols import nsubj, verb, dobj, noun

nlp = spacy.load(""en_core_web_lg"")
data = {'new':  ['repaired computer and replaced connector.', 'spliced wire on connector.', 'cycled power and reseated connectors and replaced computer on transmitter.']}

df = pd.dataframe(data)    

dep_matcher  = dependencymatcher(vocab = nlp.vocab)
    dep_pattern = [
        {
            ""right_id"": ""action"",
            ""right_attrs"": {'lemma' : {""in"": [""reseat"", ""cycle"", 'replace' , 'repair', 'reinstall' , 'clean', ' treat', 'splice', 'swap', 'read', 'inspect','installed' ]}}
        },
    
        {
            ""left_id"": ""action"",
            ""rel_op"": "">"",
            ""right_id"": ""component"",
            ""right_attrs"": {""dep"":{""in"": [ 'dobj']}},     
        }]
    
    dep_matcher.add('maint_action' , patterns = [dep_pattern])
    dep_matches = dep_matcher(doc)
def find_matches(text):
        doc = nlp(text)
        rule3_pairs = []
        for match in dep_matches:
            dep_pattern = match[0]
            matches = match[1]
            verb , subject = matches[0], matches[1] 
            a = (nlp.vocab[dep_pattern].text, '\t' ,doc[verb] , doc[subject])
            rule3_pairs.append(a)
            return rule3_pairs
      
df['three_tuples'] = df['new'].apply(find_matches) 

i am trying to have each row that meets the pattern output the respective noun and verb combo.  such as:
|three_tuples|
|maint_action    repaired computer  replaced connector|
|maint_action    spliced wire|
|maint_action    cycled power  reseated connectors  replaced computer|","['python', 'dataframe', 'dependencies', 'spacy', 'matcher']",72552807,"i have executed your code exactly as it is (the second sample) and it's already providing the results that you want (image below).
you have one small problem in the first code sample, you are not doing:
doc = nlp(text) 
but i don't think that's what's causing the issue, maybe try restarting your kernel if you're using jupyter.

update
after your edit, i noticed that you had a lot of indentation errors please fix those. 
also, you are calling the dep_matcher from outside the function not from within, that's why it won't work. 
finally, you are breaking the loop with the return statement there. you should get the return out of the for loop if you want to get all the results. 
here's the code that worked for me:
def find_matches(text):
    doc = nlp(text)
    dep_matches = dep_matcher(doc)
    rule3_pairs = []
    for match in dep_matches:
        dep_pattern = match[0]
        matches = match[1]
        verb , subject = matches[0], matches[1]
        a = (nlp.vocab[dep_pattern].text, doc[verb] , doc[subject])
        rule3_pairs.append(a)
    return rule3_pairs",https://stackoverflow.com/questions/72552550,python,08-06-2022 21:44,847.0,1.0,1.0,True,05-11-2023 03:22,09-06-2022 01:20
72489570,getting random output every time on running next sentence prediction code using bert,"based on the code provided below, i am trying to run nsp (next sentence prediction) on a custom dataset. the loss after training the model is different every time and the model give different accuracies every time. what am i missing or doing wrong?
pip install transformers[torch]
from transformers import berttokenizer, bertfornextsentenceprediction 
import torch  
tokenizer = berttokenizer.from_pretrained('bert-base-uncased') 
model = bertfornextsentenceprediction.from_pretrained('bert-base-uncased')
with open('clean.txt', 'r') as fp:
    text = fp.read().split('\n')
bag = [item for sentence in text for item in sentence.split('.') if item != '']
bag_size = len(bag)
import random
 
sentence_a = []
sentence_b = []
label = []
 
for paragraph in text:
    sentences = [
        sentence for sentence in paragraph.split('.') if sentence != ''
    ]
    num_sentences = len(sentences)
    if num_sentences > 1:
        start = random.randint(0, num_sentences-2)
        # 50/50 whether is isnextsentence or notnextsentence
        if random.random() >= 0.5:
            # this is isnextsentence
            sentence_a.append(sentences[start])
            sentence_b.append(sentences[start+1])
            label.append(0)
        else:
            index = random.randint(0, bag_size-1)
            # this is notnextsentence
            sentence_a.append(sentences[start])
            sentence_b.append(bag[index])
            label.append(1)
 
inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt', max_length=512, truncation=true, padding='max_length')
inputs['labels'] = torch.longtensor([label]).t
class meditationsdataset(torch.utils.data.dataset):
    def __init__(self, encodings):
        self.encodings = encodings
    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
    def __len__(self):
        return len(self.encodings.input_ids)
 
dataset = meditationsdataset(inputs)
loader = torch.utils.data.dataloader(dataset, batch_size=16, shuffle=true)
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)
from transformers import adamw
 
# activate training mode
model.train()
# initialize optimizer
optim = adamw(model.parameters(), lr=5e-6)
 
from tqdm import tqdm  # for our progress bar
 
epochs = 2
 
for epoch in range(epochs):
    # setup loop with tqdm and dataloader
    loop = tqdm(loader, leave=true)
    for batch in loop:
        # initialize calculated gradients (from prev step)
        optim.zero_grad()
        # pull all tensor batches required for training
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        labels = batch['labels'].to(device)
        # process
        outputs = model(input_ids, attention_mask=attention_mask,
                        token_type_ids=token_type_ids,
                        labels=labels)
        # extract loss
        loss = outputs.loss
        # calculate loss for every parameter that needs grad update
        loss.backward()
        # update parameters
        optim.step()
        # print relevant info to progress bar
        loop.set_description(f'epoch {epoch}')
        loop.set_postfix(loss=loss.item())

in the code below i am testing the model on unseen data:
from torch.nn import functional as f
from torch.nn.functional import softmax
prompt = ""sentence 1 text""
prompt2 = ""sentence 2 text""
output = tokenizer.encode_plus(prompt,prompt2, return_tensors=""pt"")
result = model(**output)[0]
prob = softmax(result, dim=1)
print(prob)

so, the value of prob and loss is different every single time for the same unseen data which to the best of my knowledge should be same.","['nlp', 'pytorch', 'huggingface-transformers', 'bert-language-model', 'attention-model']",72559434,"you need to put the model in evaluation mode. if you use i.e. dropout layers while testing the model you should turn it off.
you can do this with
model.eval()

if you don't use this, you will get a different output and loss value because the dropout in your model will close different neurons each time.",https://stackoverflow.com/questions/72489570,nlp,03-06-2022 12:28,393.0,0.0,1.0,True,09-06-2022 11:22,05-06-2022 14:08
74358056,tensor to dataframe for each sentence,"for a 6 class sentence classification task, i have a list of sentences where i retrieve the absolute values before the softmax is applied. example list of sentences:
s = ['i like the weather today', 'the movie was very scary', 'love is in the air']

i get the values the following way:
from transformers import automodelforsequenceclassification, autotokenizer

model_name = ""emanuel/bertweet-emotion-base""
model = automodelforsequenceclassification.from_pretrained(model_name)
tokenizer = autotokenizer.from_pretrained(model_name)

for i in s:
  sentence = tokenizer(i, return_tensors=""pt"")
  output = model(sentence[""input_ids""])
  print(output.logits.detach().numpy())

  # returns [[-0.8390876   2.9480567  -0.5134539   0.70386493 -0.5019671  -2.619496  ]]
  #[[-0.8847909  -0.9642067  -2.2108874  -0.43932158  4.3386173  -0.37383893]]
  #[[-0.48750368  3.2949197   2.1660519  -0.6453249  -1.7101991  -2.817954  ]]

how do i create a data frame with columns sentence, class_1, class_2, class_3, class_4, class_5, class_6 where i add values iteratively or maybe in a more optimal way where i append each new sentence and its absolute values? what would be the best way?
expected output:
     sentence                   class_1        class_2    class_3      ....
0    i like the weather today   -0.8390876     2.9480567  -0.5134539   ....
1    the movie was very scary   -0.8847909     -0.9642067 -2.2108874   ....
2    love is in the air         -0.48750368    3.2949197   2.1660519   ....  
...

if i only had one sentence, i could transform it to a data frame like this, but i would still need to append the sentence somehow
sentence = tokenizer(""love is in the air"", return_tensors=""pt"")
output = model(sentence[""input_ids""])

px = pd.dataframe(output.logits.detach().numpy())

maybe creating two separate data frames and then appending them would be one plausible way of doing this?","['pandas', 'numpy', 'huggingface-transformers', 'torch']",74529049,"i managed to come up with a solution and i am posting it as someone might find it useful.
the idea is to initialize a data frame and to append the absolute values for every sentence while iterating
absolute_vals = pd.dataframe()

for i in s:
  sentence = tokenizer(i, return_tensors=""pt"")
  output = model(sentence[""input_ids""]) 
  px = pd.dataframe(output.logits.detach().numpy())
  absolute_vals = absolute_vals.append(px, ignore_index = true)

absolute_vals

returns:
     sentence                   class_1        class_2    class_3      ....
0    i like the weather today   -0.8390876     2.9480567  -0.5134539   ....
1    the movie was very scary   -0.8847909     -0.9642067 -2.2108874   ....
2    love is in the air         -0.48750368    3.2949197   2.1660519   ....  
...",https://stackoverflow.com/questions/74358056,pandas,08-11-2022 09:01,414.0,0.0,2.0,True,22-11-2022 07:45,08-11-2022 14:00
74944407,using fetch to call the openai api throws error 400 &quot;you must provide a model parameter&quot;,"followed the openai api docs and but there is something wrong with this post request... i get an ""you must provide a model parameter"" error ... what is wrong with the request body?
try {
        const response = await fetch(
            `
            {
                body: json.stringify({""model"": ""text-davinci-003"", ""prompt"": ""say this is a test"", ""temperature"": 0, ""max_tokens"": 7}),
                method: ""post"",
                headers: {
                    accept: ""application/json"",
                    authorization: ""bearer [api-key]"",
                },
                    }
        ).then((response) => {
            if (response.ok) {
                response.json().then((json) => {
                    terminal.echo(json);
                });
            }
        });
      
        console.log(""completed!"");
    } catch (err) { console.error(`error: ${err}`) }
}```","['javascript', 'fetch-api', 'openai-api']",74944822,"as per api reference a valid request looks like this:
curl  \
  -h 'content-type: application/json' \
  -h 'authorization: bearer your_api_key' \
  -d '{
  ""model"": ""text-davinci-003"",
  ""prompt"": ""say this is a test"",
  ""max_tokens"": 7,
  ""temperature"": 0
}'

we can see that model is passed as a property in the request body. the request body has to be a json string. additionally, the api requires two request headers: content-type and authorization.
the request body shared in your example is correct. the authorization request header is also there. however, the content-type header is missing in your example as it was - apparently and probably mistakenly - replaced by the accept header.
due to the missing content-type header, the api does not know what content type the request body body has (could be json, yaml, xml, etc..). the api therefore, cannot process the request body and responds with the fact that the model parameter is missing.
the following example works in chrome:
    await fetch(
        `
        {
            body: json.stringify({""model"": ""text-davinci-003"", ""prompt"": ""say this is a test"", ""temperature"": 0, ""max_tokens"": 7}),
            method: ""post"",
            headers: {
                ""content-type"": ""application/json"",
                authorization: ""bearer  api_key_here"",
            },
                }
    ).then((response) => {
        if (response.ok) {
            response.json().then((json) => {
                terminal.echo(json);
            });
        }
    });",https://stackoverflow.com/questions/74944407,javascript,28-12-2022 19:45,5420.0,1.0,3.0,True,26-06-2024 23:55,16-07-2023 09:47
79404571,why am i getting &quot;model_not_found&quot; error when using openai&#39;s o3-mini model?,"openai recently released the o3-mini model (announced around january 31, 2025). i am trying to use it via the api, but i receive the following error:
error: error code: 404 - {'error': {'message': 'the model o3-mini does not exist or you do not have access to it.', 
'type': 'invalid_request_error', 'param': none, 'code': 'model_not_found'}}

i have verified that my api key is valid and works for other models. however, o3-mini is not accessible.
things iï¿½ï¿½ï¿½ve checked:

my openai api key is valid.
i can successfully call other models like gpt-4-turbo or gpt-3.5-turbo.
i am using the correct model name (o3-mini as per the announcement).

questions:

is o3-mini available to all api users, or is it in limited access/testing?
could the model name be different from what was announced?
is there a delay between the announcement and api availability?

any insights would be appreciated! thn advance.","['error-handling', 'openai-api', 'chatgpt-api', 'openaiembeddings']",79407657,"yeah, apparently its use is limited to tier 3 organizations as mentioned by openai staff here on their forums. you can check your organization's current tier on  and looking at the bottom of the ""limits"" page under ""organization"" on the left side bar.",https://stackoverflow.com/questions/79404571,error-handling,01-02-2025 07:13,8013.0,8.0,2.0,True,10-02-2025 07:22,01-02-2025 08:04
78557135,how do i print each element in an openai chatcompletion response as json on a separate line?,"i'm making a simple call to openai using python asking about where a baseball game was played.
completion = openai.chat.completions.create(
  model=""gpt-3.5-turbo"",
  messages = [
  {""role"": ""system"", ""content"": ""you are a helpful assistant.""},
  {""role"": ""user"", ""content"": ""who won the world series in 2020?""},
  {""role"": ""assistant"", ""content"": ""the los angeles dodgers won the world series in 2020.""},
  {""role"": ""user"", ""content"": ""where was it played?""}
  ]
)

print (completion)

the output shows up like this:
chatcompletion(id='chatcmpl-9ugp85b0gybjeaiymcf3ryt9y3fdz', choices=[choice(finish_reason='stop', index=0, logprobs=none, message=chatcompletionmessage(content='the 2020 world series was played at globe life field in arlington, texas.', role='assistant', function_call=none, tool_calls=none))], created=1717099870, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=none, usage=completionusage(completion_tokens=17, prompt_tokens=53, total_tokens=70))

but i'd like it to show up like this:
{
  ""choices"": [
    {
      ""finish_reason"": ""stop"",
      ""index"": 0,
      ""message"": {
        ""content"": ""the 2020 world series was played in texas at globe life field in arlington."",
        ""role"": ""assistant""
      },
      ""logprobs"": null
    }
  ],
  ""created"": 1677664795,
  ""id"": ""chatcmpl-7qyqpwdfhqwajicieznoc6q47xayw"",
  ""model"": ""gpt-3.5-turbo-0613"",
  ""object"": ""chat.completion"",
  ""usage"": {
    ""completion_tokens"": 17,
    ""prompt_tokens"": 57,
    ""total_tokens"": 74
  }
}

fwiw, i'm using python 3.12 and windows terminal.","['python', 'json', 'openai-api', 'pretty-print']",78563543,"use .model_dump_json()
completion = client.chat.completions.create(
  model=""gpt-3.5-turbo"",
  messages = [
  {""role"": ""system"", ""content"": ""you are a helpful assistant""},
  {""role"": ""user"", ""content"": ""who won the world series in 2020?""},
  {""role"": ""assistant"", ""content"": ""the los angeles dodgers won the world series in 2020.""},
  {""role"": ""user"", ""content"": ""where was it played?""}
  ]
)

data = completion.model_dump_json()
print(data)

output:
{""id"":""chatcmpl-9vgebgyeosyhcww19f6f5b6lmfhpl"",""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""message"":{""content"":""the world series in 2020 was played at globe life field in arlington, texas."",""role"":""assistant"",""function_call"":null,""tool_calls"":null}}],""created"":1717239213,""model"":""gpt-3.5-turbo-0125"",""object"":""chat.completion"",""system_fingerprint"":null,""usage"":{""completion_tokens"":18,""prompt_tokens"":52,""total_tokens"":70}}

if you need to pretty print, you can just use the ident parameter:
data = completion.model_dump_json(indent=3)
print(data)

output:
{
   ""id"": ""chatcmpl-9vgphvuzlkh2quqt9vsaudwcxoifg"",
   ""choices"": [
      {
         ""finish_reason"": ""stop"",
         ""index"": 0,
         ""logprobs"": null,
         ""message"": {
            ""content"": ""the 2020 world series was played at globe life field in arlington, texas, home of the texas rangers."",
            ""role"": ""assistant"",
            ""function_call"": null,
            ""tool_calls"": null
         }
      }
   ],
   ""created"": 1717239901,
   ""model"": ""gpt-3.5-turbo-0125"",
   ""object"": ""chat.completion"",
   ""system_fingerprint"": null,
   ""usage"": {
      ""completion_tokens"": 23,
      ""prompt_tokens"": 52,
      ""total_tokens"": 75
   }
}",https://stackoverflow.com/questions/78557135,python,30-05-2024 20:29,2193.0,2.0,1.0,True,07-08-2024 10:34,01-06-2024 11:28
1876103,i have a list of country codes and a list of language codes. how do i map from country code to language code?,"when the user visits the site, i can get their country code.  i want to use this to set the default language (which they can later modify if necessary, just a general guess as to what language they might speak based on what country they are in).  
is there a definitive mapping from country codes to language codes that exists somewhere?  i could not find it.  i know that not everyone in a particular country speaks the same language, but i just need a general mapping, the user can select their language manually later.","localization, internationalization, nlp, country-codes",1876170,"a search for ""country language mapping"" turns up this languages by countries table, which you could probably convert to language codes with a bit of work. (there are less than 200 countries.)
a warning though: doing this is pretty much guaranteed to annoy many users in multi-lingual countries like switzerland or canada. for such countries it's customary to let the user choose their language on the main page of the ui. see google switzerland or best buy canada for some examples. this also annoys expatriates and travelers.
better yet, why don't you use the accept-language http header to determine which language to use? the accept-language header is the right thing to look at when determining the user's language. it even gives you a list in order of preference, so if the user is most fluent in some language you don't support, but is reasonably fluent in some other language that you do support you can fall back correctly. the one problem with accept-language is that most users leave it at the default setting. most browser should default to platform's language setting, though, so it isn't too bad. (some older browsers would default to english, which was kind of a disaster because ""en"" effectively meant the accept-language was unset.)",https://stackoverflow.com/q/1876103,"localization, internationalization, nlp, country-codes",09-12-2009 19:06,11985.0,12.0,2.0,True,28-08-2023 13:38,20-06-2014 17:01
66304420,how to encode data of variable input length?,"i was doing some data science work when i get stuck with this issue, i'm trying to create a model for a supervised task, where both input and output are of variable length.
here is an example on how the input and output look like:
input[0]: [2.42, 0.43, -5.2, -54.9]
output[0]: ['class1', 'class3', 'class12']

the problem is that in the dataset that i have, the inputs and outputs are of variable length, so in order to be able to use the entire dataset, i need to find a way of encoding this data.
first i encoded the outputs classes and added a padding to equal all outputs length in the dataset, (let's say of length=(6,)):
encoded_output[0]: [1, 3, 12, 0, 0, 0]

but i can't figure out a way of encoding the input, because, as the original input data are floats, i cannot create an encoding and add padding. i don't know what other options i have and i would like to hear how would you solve this.","['python', 'tensorflow', 'machine-learning', 'nlp']",66306305,"a way to solve this is to:

find out what is the max length that the variable data can be.
find out what the true length of each training instance is.

from these two things you can create a mask and have your network compute zero gradients for the stuff you want to ignore.
example:
import tensorflow as tf

# pretend the longest data instance we will have is 5
max_seq_len = 5

# batch of indices indicating true length of each instance
idxs = tf.constant([3, 2, 1])

# batch of variable-length data
rt = tf.ragged.constant(
  [
    [0.234, 0.123, 0.654],
    [0.111, 0.222],
    [0.777],
  ], dtype=tf.float32)

t = rt.to_tensor(shape=[3, max_seq_len])
print(t)
# tf.tensor(
# [[0.234 0.123 0.654 0.    0.   ]
#  [0.111 0.222 0.    0.    0.   ]
#  [0.777 0.    0.    0.    0.   ]], shape=(3, 5), dtype=float32)

# use indices to create a boolean mask. we can use this mask in
# layers in our network to ignore gradients
mask = tf.sequence_mask(idxs, max_seq_len)
print(mask)
# <tf.tensor: shape=(3, 5), dtype=bool, numpy=
# array([[ true,  true,  true, false, false],
#        [ true,  true, false, false, false],
#        [ true, false, false, false, false]])>

this use case commonly occurs in rnns. you can see there is a mask option in the call() method where you can pass a binary mask for variable length time-series data.",https://stackoverflow.com/questions/66304420,python,21-02-2021 16:22,453.0,1.0,1.0,True,21-02-2021 19:47,21-02-2021 16:30
65373223,how to find named entity relationship,"i was doing named entity relationship from the text of a book. i have done the recognition using ""spacy"" using its module ""en_core_web_sm"" as:
import en_core_web_sm
nlp = en_core_web_sm.load()
def namedentityrecognition(text):
    doc = nlp(text)
    list1 = []
    for ent in doc.ents:
        if ent.label_=='person':
            list1.append((ent.text, ent.label_))
    return list1

i only wanted to find the persons from it. now i wish to find the relationship between different persons (how one person is related to another person, like ""is a father of"", ""is a brother of"" kind of relationships) but i don't have any idea about how to do it.","['python', 'spacy']",65374123,"probably you are referring to relation extraction.
what relationship between the entity ""person"" you seek to find out? answering that question, and depending on your task you could customize your search.
in general, you should have a look here and here if you haven't done that so far.
i hope those help with your problem.",https://stackoverflow.com/questions/65373223,python,19-12-2020 18:40,612.0,1.0,1.0,True,20-12-2020 12:07,20-12-2020 12:07
62801070,detect checkboxes from a form using opencv python,"given a dental form as input, need to find all the checkboxes present in the form using image processing. i have answered my current approach below. is there any better approach to find the checkboxes for low-quality docs as well?
sample input:","['python', 'image-processing', 'information-retrieval', 'opencv']",62801071,"this is one approach in which we can solve the issue,
import cv2
import numpy as np
image=cv2.imread('path/to/image.jpg')

### binarising image
gray_scale=cv2.cvtcolor(image,cv2.color_bgr2gray)
th1,img_bin = cv2.threshold(gray_scale,150,225,cv2.thresh_binary)


defining vertical and horizontal kernels
linewidth = 7
lineminwidth = 55
kernal1 = np.ones((linewidth,linewidth), np.uint8)
kernal1h = np.ones((1,linewidth), np.uint8)
kernal1v = np.ones((linewidth,1), np.uint8)

kernal6 = np.ones((lineminwidth,lineminwidth), np.uint8)
kernal6h = np.ones((1,lineminwidth), np.uint8)
kernal6v = np.ones((lineminwidth,1), np.uint8)

detect horizontal lines
img_bin_h = cv2.morphologyex(~img_bin, cv2.morph_close, kernal1h) # bridge small gap in horizonntal lines
img_bin_h = cv2.morphologyex(img_bin_h, cv2.morph_open, kernal6h) # kep ony horiz lines by eroding everything else in hor direction


finding vertical lines
## detect vert lines
img_bin_v = cv2.morphologyex(~img_bin, cv2.morph_close, kernal1v)  # bridge small gap in vert lines
img_bin_v = cv2.morphologyex(img_bin_v, cv2.morph_open, kernal6v)# kep ony vert lines by eroding everything else in vert direction


merging vertical and horizontal lines to get blocks. adding a layer of dilation to remove small gaps
### function to fix image as binary
def fix(img):
    img[img>127]=255
    img[img<127]=0
    return img

img_bin_final = fix(fix(img_bin_h)|fix(img_bin_v))

finalkernel = np.ones((5,5), np.uint8)
img_bin_final=cv2.dilate(img_bin_final,finalkernel,iterations=1)


apply connected component analysis on the binary image to get the blocks required.
ret, labels, stats,centroids = cv2.connectedcomponentswithstats(~img_bin_final, connectivity=8, ltype=cv2.cv_32s)

### skipping first two stats as background
for x,y,w,h,area in stats[2:]:
    cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),2)",https://stackoverflow.com/questions/62801070,python,08-07-2020 18:11,4920.0,4.0,2.0,True,03-05-2021 05:40,03-05-2021 05:40
62739737,replace spaces with new lines if part of a specific pattern using sed and regex with extended syntax,"so i have a text file with multiple instances looking like this:
word.  word or words [something:'else]
i need to replace with a new line the double space after every period followed by a sequence of words and then a ""["", like so:
word.\nword or words [something:'else]
i thought about using the sed command in bash with extended regex syntax, but nothing has worked so far... i've tried different variations of this:
sed -e 's/(\.)(  )(.*)(.\[)/\1\n\3\4/g' old.txt > new.txt
i'm an absolute beginner at this, so i'm not sure at all about what i'm doing ï¿½ï¿½ï¿½","['regex', 'sed', 'nlp', 'text-processing']",62740698,"this might work for you (gnu sed):
sed -e 's/\.  ((\w+ )+\[)/\.\n\1/g' file

replace globally a period followed by two spaces and one or more words space separated followed by an opening square bracket by; a period followed by a newline followed by the matching back reference from the regexp.",https://stackoverflow.com/questions/62739737,regex,05-07-2020 10:54,70.0,0.0,2.0,True,05-07-2020 18:19,05-07-2020 18:19
77328508,qa_chain from langchain does not recognize azure openai engine&#39; or &#39;deployment_id,"i am retrieving the results from my internal db but for this example i have added an open url. i am using azure openai and langchain in conjunction to build this retrieval engine. i checked in the azure portal that deployment is successful and i am able to run in a stand alone prompt.
the last query throws this error:

invalidrequesterror: must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.completion.completion'>.

since we can see i have already supplied a deployment_id above. what am i missing?
here is the entire code
from langchain.vectorstores import chroma
from langchain.embeddings import openaiembeddings
from langchain.text_splitter import recursivecharactertextsplitter
from langchain.llms import openai
from langchain.chains import retrievalqa
from langchain.document_loaders import textloader
from langchain.document_loaders import directoryloader  
loader= textloader(r'./rawtext.txt', encoding='utf-8')
documents = loader.load()    
text_splitter= recursivecharactertextsplitter(chunk_size= 70, chunk_overlap=0)
texts= text_splitter.split_documents(documents)

#create the db
    persist_directory = 'db'
#embedding= openaiembeddings()
   embeddings = openaiembeddings(deployment=""text-embedding-ada-002"",model=""text-embedding-ada-002"", chunk_size = 1)
   vectordb= chroma.from_documents(documents=texts,
                               embedding=embeddings,
                               persist_directory=persist_directory)

post this i am creating a retriever as below:
  vectordb= chroma(persist_directory=persist_directory, embedding_function=embeddings)
    retriever= vectordb.as_retriever()
    docs=retriever.get_relevant_documents(""databricks"")

#creating a chain:
 from langchain.chains import retrievalqa
 import openai

#specify the name of the engine you want to use
engine = ""test_chat""
qa_chain=retrievalqa.from_chain_type(llm=openai(),
                                chain_type=""stuff"",
                                retriever= retriever,
                                return_source_documents=true)

#test_chat here for reference is text-embedding-ada-002
#cite source
def process_llm_responses(llm_response):
print(llm_response['result'])
print('\n\nsources:')
for source in llm_response[""source_documents""]:
    print(source.metadata[""source""])

#full retrieval in process
query = ""what is a medallion architecture""
llm_response= qa_chain(query)
process_llm_responses","['azure', 'langchain', 'azure-openai']",77333192,"since not knowing your langchain version, first check with below code
embeddings = openaiembeddings(deployment=""text-embedding-ada-002"",model=""text-embedding-ada-002"",engine=""text-embedding-ada-002"", chunk_size = 1)

if the above code doesn't work you try for below
 embeddings = openaiembeddings(deployment=""text-embedding-ada-002"",model=""text-embedding-ada-002"",openai_api_type='azure', chunk_size = 1)",https://stackoverflow.com/questions/77328508,azure,20-10-2023 05:29,1003.0,-1.0,1.0,True,24-04-2024 03:09,21-10-2023 14:33
68383634,cuda error: cublas_status_invalid_value error when training bert model using huggingface,"i am working on sentiment analysis on steam reviews dataset using bert model where i have 2 labels: positive and negative. i have fine-tuned the model with 2 linear layers and the code for that is as below.
 bert = bertforsequenceclassification.from_pretrained(""bert-base-uncased"",
                                                 num_labels = len(label_dict),
                                                 output_attentions = false,
                                                 output_hidden_states = false)

 class bertmodel(nn.module):
   def __init__(self, bert):
     super(bertmodel, self).__init__()
     self.bert = bert
     self.dropout1 = nn.dropout(0.1)
     self.relu =  nn.relu()
     self.fc1 = nn.linear(768, 512)
     self.fc2 = nn.linear(512, 2)
     self.softmax = nn.logsoftmax(dim = 1)

  def forward(self, **inputs):
     _, x = self.bert(**inputs)
    x = self.fc1(x)
    x = self.relu(x)
    x = self.dropout1(x)
    x = self.fc2(x)
    x = self.softmax(x)

  return x

this is my train function:
def model_train(model, device, criterion, scheduler, optimizer, n_epochs):
  train_loss = []
  model.train()
 for epoch in range(1, epochs+1):
   total_train_loss, training_loss = 0,0 
  for idx, batch in enumerate(dataloader_train):
     model.zero_grad()
     data = tuple(b.to(device) for b in batch)
     inputs = {'input_ids':      data[0],'attention_mask': data[1],'labels':data[2]}
     outputs = model(**inputs)
     loss = criterion(outputs, labels)
     loss.backward()
     torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
     #update the weights
     optimizer.step()
     scheduler.step()
     training_loss += loss.item()
     total_train_loss += training_loss
     if idx % 25 == 0:
        print('epoch: {}, batch: {}, training loss: {}'.format(epoch, idx, training_loss/10))
        training_loss = 0      
  #avg training loss
  avg_train_loss = total_train_loss/len(dataloader_train)
  #validation data loss
  avg_pred_loss = model_evaluate(dataloader_val)
  #print for every end of epoch
  print('end of epoch {}, avg. training loss: {}, avg. validation loss: {} \n'.format(epoch, avg_train_loss, avg_pred_loss))

i am running this code on google colab. when i run the train function, i get the following the error, i have tried with batch sizes 32, 256, 512.
runtimeerror: cuda error: cublas_status_invalid_value when calling `cublassgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`

can anyone please help me on this? thank you.
update on the code: i tried running the code on the cpu and the error is in the matrix shapes mismatch. the input shape, shape after the self.bert is printed in the image. since the first linear layer (fc1) is not getting executed, the shape after that is not printed.","['python', 'pytorch', 'sentiment-analysis', 'bert-language-model']",68383837,"i suggest trying out couple of things that can possibly solve the error.
as shown in this forum, one possible solution is to lower the batch size of how you load data. since it might be a memory error.
if that does not work then i suggest as shown in this github issue to update to a new version of pytorch cuda that fixes a matrix multiplication bug that releases this same error that your code could be doing. hence, as shown in this forum you can update pytorch to the nightly pip wheel, or use the cuda10.2 or conda binaries. you can find information on such installations on the pytorch home page where it mentions how to install pytorch.
if none of that works, then the best thing to do is to run a smaller version of the process on cpu and recreate the error. when running it on cpu instead of cuda, you will get a more useful traceback that can solve your error.
edit (based on comments):
you have a matrix error in your model.
the problem stems in your forward func then
the model bert outputs a tensor that has torch.size (64, 2) which means if you put it in the linear layer you have it will error since that linear layer requires input of (?, 768) b/c you initialized it as nn.linear(768, 512). in order to make the error disappear you need to either do some transformation on the tensor or initialize another linear layer as shown below:
somewhere defined in __init__: self.fc0 = nn.linear(2, 768)
def forward(self, **inputs):
     _, x = self.bert(**inputs)
     
    x = self.fc0(x)
    x = self.fc1(x)
    x = self.relu(x)
    x = self.dropout1(x)
    x = self.fc2(x)
    x = self.softmax(x)


sarthak jain",https://stackoverflow.com/questions/68383634,python,14-07-2021 18:47,6645.0,2.0,1.0,True,16-07-2021 06:33,16-07-2021 01:07
52205475,custom sentence segmentation using spacy,"i am new to spacy and nlp. i'm facing the below issue while doing sentence segmentation using spacy.
the text i am trying to tokenise into sentences contains numbered lists (with space between numbering and actual text), like below.
import spacy
nlp = spacy.load('en_core_web_sm')
text = ""this is first sentence.\nnext is numbered list.\n1. hello world!\n2. hello world2!\n3. hello world!""
text_sentences = nlp(text)
for sentence in text_sentences.sents:
    print(sentence.text)

output (1.,2.,3. are considered as separate lines) is:
this is first sentence.
  
next is numbered list.
    
1.
hello world!
 
2.
hello world2!
  
3.
hello world!

but if there is no space between numbering and actual text, then sentence tokenisation is fine. like below:
import spacy
nlp = spacy.load('en_core_web_sm')
text = ""this is first sentence.\nnext is numbered list.\n1.hello world!\n2.hello world2!\n3.hello world!""
text_sentences = nlp(text)
for sentence in text_sentences.sents:
    print(sentence.text)

output(desired) is:
this is first sentence.
    
next is numbered list.
   
1.hello world!
    
2.hello world2!
    
3.hello world!

please suggest whether we can customise sentence detector to do this.","['nlp', 'tokenize', 'spacy', 'sentence']",52539370,"when you use a pretrained model with spacy, the sentences get splitted based on training data that were provided during the training procedure of the model.
of course, there are cases like yours, that may somebody want to use a custom sentence segmentation logic. this is possible by adding a component to spacy pipeline.
for your case, you can add a rule that prevents sentence splitting when there is a {number}. pattern. 
a workaround for your problem:
import spacy
import re

nlp = spacy.load('en')
boundary = re.compile('^[0-9]$')

def custom_seg(doc):
    prev = doc[0].text
    length = len(doc)
    for index, token in enumerate(doc):
        if (token.text == '.' and boundary.match(prev) and index!=(length - 1)):
            doc[index+1].sent_start = false
        prev = token.text
    return doc

nlp.add_pipe(custom_seg, before='parser')
text = u'this is first sentence.\nnext is numbered list.\n1. hello world!\n2. hello world2!\n3. hello world!'
doc = nlp(text)
for sentence in doc.sents:
    print(sentence.text)

hope it helps!",https://stackoverflow.com/questions/52205475,nlp,06-09-2018 13:43,18846.0,10.0,1.0,True,22-06-2021 17:05,22-06-2021 17:05
69423621,how to put spaces in between every emojis,"i have a dataset of tweets where it contains at least one occurrence of emoji. but sometimes there are more. emojis can be in the middle of the sentence, or it could be at the start or at the end. hence for each tweet the case is different. i am having difficulties trying to split only the emojis in the sentence. if i loop through each word, the multiple emojis are also considered as one word.
she is too hot for congress.  vote her out!  #sarcasm ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½

expected output: she is too hot for congress.  vote her out!  #sarcasm ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½
the struggle is real ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ #struggle #struggleisreal #struggles #funny #humor #saying #sarcasm #lifestruggles #sarcastic #funnysaying #sayings #thestruggleisreal 

expected output: the struggle is real ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ de>
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½  for more funny post follow

expected output: ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½  f"" for words and emoji
answer from the above post gives me a list and toknized words for each tweet in the dataset which i don't want, it also does not solve my problem. i do not get space between the emojis.","['python', 'nlp', 'sentiment-analysis']",69423881,"using emoji library 'v1.5.0' it's an easy job.
import emoji

def extract_emojis(s):
    return ''.join((' '+c+' ') if c in emoji.unicode_emoji['en'] else c for c in s)

test:
s = ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ me asï¿½ï¿½, seï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ds ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ hello ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ emoji hello ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ how are ï¿½ï¿½ï¿½ï¿½ you todayï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½""

extract_emojis(s)

output:
' ï¿½ï¿½ï¿½ï¿½  ï¿½ï¿½ï¿½ï¿",https://stackoverflow.com/questions/69423621,python,03-10-2021 09:36,556.0,0.0,1.0,True,03-10-2021 10:16,03-10-2021 09:51
78991862,getting model not found for text completion (g4f),"i want to prompt g4f with a simple question but the exception i get each time was model not found:
import g4f

allowed_models = [
    'code-davinci-002',
    'text-ada-001',
    'text-babbage-001',
    'text-curie-001',
    'text-davinci-002',
    'text-davinci-003'
]

response = g4f.completion.create(
    model='text-davinci-003',
    prompt='say this is a test'
)

print(response)

any idea how to remedy to this?","['openai-api', 'chatgpt-api']",78992788,"for openai provider, g4f only support 3 models: gpt-3.5-turbo, gpt-4, gpt-4-turbo.",https://stackoverflow.com/questions/78991862,openai-api,16-09-2024 21:30,201.0,0.0,1.0,True,18-01-2025 06:18,17-09-2024 06:30
72522174,tokenize paragraphs by special characters; then rejoin so tokenized segments to reach certain length,"i have this long paragraph:
paragraph = ""the weakening of the papacy by the avignon exile and the papal schism; the breakdown of monastic discipline and clerical celibacy; the luxury of prelates, the corruption of the curia, the worldly activities of the popes; the morals of alexander vi, the wars of julius ii, the careless gaiety of leo x; the relicmongering and peddling of indulgences; the triumph of islam over christendom in the crusades and the turkish wars; the spreading acquaintance with non-christian faiths; the influx of arabic science and philosophy; the collapse of scholasticism in the irrationalism of scotus and the skepticism of ockham; the failure of the conciliar movement to effect reform; the discovery of pagan antiquity and of america; the invention of printing; the extension of literacy and education; the translation and reading of the bible; the newly realized contrast between the poverty and simplicity of the apostles and the ceremonious opulence of the church; the rising wealth and economic independence of germany and england; the growth of a middle class resentful of ecclesiastical restrictions and claims; the protests against the flow of money to rome; the secularization of law and government; the intensification of nationalism and the strengthening of monarchies; the nationalistic influence of vernacular languages and literatures; the fermenting legacies of the waldenses, wyclif, and huss; the mystic demand for a less ritualistic, more personal and inward and direct religion: all these were now uniting in a torrent of forces that would crack the crust of medieval custom, loosen all standards and bonds, shatter europe into nations and sects, sweep away more and more of the supports and comforts of traditional beliefs, and perhaps mark the beginning of the end for the dominance of christianity in the mental life of european man.""

my goal is to split this long paragraph into multiple sentences keeping the sentences around 18 - 30 words each.
there is only one full-stop at the end; so nltk tokenizer is of no use. i can use regex to tokenize; i have this pattern that works in splitting:
regex_special_chars = '([ï¿½ï¿½ï¿½;*""(ï¿½ï¿½=!ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\\?\\]ï¿½ï¿½ï¿½)ï¿½ï¿½ï¿½ï¿½ï¿½[]+)'
new_text = re.split(regex_special_chars, paragraph)


the question is how to join this paragraph into a list of multiple sentences that would be around 18 to 30; where possible; because sometimes it's not possible with this regex.
the end result will look like the following list below:
tokenized_paragraph = ['the weakening of the papacy by the avignon exile and the papal schism; the breakdown of monastic discipline and clerical celibacy;',
 'the luxury of prelates, the corruption of the curia, the worldly activities of the popes; the morals of alexander vi, the wars of julius ii, the careless gaiety of leo x;',
 'the relicmongering and peddling of inddom in the crusades and the turkish wars; the spreading acquaintance with non-christian faiths; ',
 'the influx of arabic science and philosophy; the collapse of scholasticism in the irrationalism of scotus and the skepticism of ockham; the failure of the conciliar movement to effect reform; ',
 'the discovery of pagan antiquity and of america; the invention of printing; the extension of literacy and education; the translation and reading of the bible; ',
 'the newly realized contrast between the poverty and simplicity of the apostles and the ceremonious opulence of the church; the rising wealth and economic independence of germany and england;',
 'the growth of a middle class resentful of ecclesiastical restrictions and claims; the protests against the flow of money to rome; the secularization of law and government; ',
 'the intensification of nationalism and the strengthening of monarchies; the nationalistic influence of vernacular languages and literatures; the fermenting legacies of the waldenses, wyclif, and huss;',
 'the mystic demand for a less ritualistic, more personal and inward and direct religion: all these were now uniting in a torrent of forces that would crack the crust of medieval custom, loosen all standards and bonds, shatter europe into nations and sects, sweep away more and more of the supports and comforts of traditional beliefs, and perhaps mark the beginning of the end for the dominance of christianity in the mental life of european man.']

if we check the lengths of the end result; we get this many words into each tokenized segment:
[len(sent.split()) for sent in tokenized_paragraph]
[21, 31, 25, 30, 25, 29, 27, 26, 76]


only the last segment exceeded 30 words (76 words), and that's okay!
edit
the regex could include a colon : so the last segment would be less than 76","['python', 'regex', 'nlp']",72522742,"i would suggest using findall instead of split.
then the regex could be:
(?:\s+\s+)*?(?:\s+\s+){17,29}\s+(?:$|[ï¿½ï¿½ï¿½;*""(ï¿½ï¿½=!ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\?\]ï¿½ï¿½ï¿½)ï¿½ï¿½ï¿½ï¿½ï¿½[]+)

break-down:

\s+\s+ a word and the space(s) that follow it
(?:\s+\s+)*?(?:\s+\s+){17,29}: lazily match some words followed by a space (so initially it wont match any) and then greedily match as many words as possible up to 29, but at least 17, and all that ending with white space. the first lazy match is needed for when no match completes with just the greedy part.
\s+(?:$|[ï¿½ï¿½ï¿½;*""(ï¿½ï¿½=!ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\?\]ï¿½ï¿½ï¿½)ï¿½ï¿½ï¿½ï¿½ï¿½[]+): match one more word, terminated by a terminator character, or the end of the string.

so:
regex = r'(?:\s+\s+)*?(?:\s+\s+){18,30}\s+(?:$|[ï¿½ï¿½ï¿½;*""(ï¿½ï¿½=!ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\?\],de>
the number of words per paragraph are:
[21, 31, 25, 30, 25, 29, 27, 26, 76]",https://stackoverflow.com/questions/72522174,python,06-06-2022 18:44,38.0,0.0,1.0,True,06-06-2022 20:20,06-06-2022 20:20
50369400,how can i process persian texts using rapid miner?,"i am working on a persian classification project. persian texts is very similar to arabic texts. when i use tokenize, it does not show any word in its wordlist page and in example set page, the image below will be shown:

i need to classify persian text to some category, but i dont know how?.
i follow some steps like this:
1- read excel(using read excel component) dataset with 2 column => col1:persian text ,col2: category
2- i use set role component to labeling data
3- i use process documents from data component containing :(tokenize(with any mode not change anythings) and filter token(min:5,max:25) inside it)
4- then i use cross validation component to train with svm or basian and in test mode to get performance.
the program runs correctly and performance is not bad for e.g accuracy is 50% but i think my work is wrong.
any help would be appreciated.","['classification', 'text-processing', 'text-classification', 'rapidminer']",50894908,"first, make sure your text data have utf-8 encoding 
and if u use filter tokens(by length) 5 is too much for minimum try 2 or at least 3 
also, i recommend using filter stopwords (dictionary) operator and the dictionary should have persian stopwords in each line 
hope it will help u",https://stackoverflow.com/questions/50369400,classification,16-05-2018 11:13,191.0,0.0,1.0,True,21-06-2020 11:42,18-05-2018 22:10
59355529,is there any order in wordnet&#39;s synsets?,"i am using wordnet to access synonyms that share a common meaning. here is an example:
from itertools import chain
from nltk.corpus import wordnet as wn

synsets = wn.synsets(""drink"")
# synsets = [synset('drink.n.01'), synset('drink.n.02'), synset('beverage.n.01'), ...]
synonyms = set(chain(*(x.lemma_names() for x in synsets)))
# synonyms = {'drinking', 'drinkable', 'crapulence', 'toast', 'drink', 'drunkenness', ...}

are synsets sorted? and, in case they are, what are the criteria? are the first synsets of the list those which have higher chances to be correlated to the given word?
i would like to limit the number of synonyms by keeping only the ""most important"" ones (what ""important"" means in this context is to be defined, but i wonder whether wordnet has its own concept of ""important"").
if synsets are not sorted, what could be an alternative way to find the most appropriate synonyms of a word?","['python', 'nlp', 'nltk', 'wordnet']",59406807,"the documentation has a relevant section: 
various similarity finding methods are provided: path_similarity, lch_similarity, wup_similarity, res_similarity, etc.
for example, from the documentation (for path_similarity): 

synset1.path_similarity(synset2): return a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy. the score is in the range 0 to 1.

you can use the method in the following format: 
# assuming we are comparing with 0th synset of ""drink""
syn_to_compare = wn.synsets(""drink"")[0]
all_synsets = wn.synsets(""drink"")
corr = [(all_synsets[i],syn_to_compare.path_similarity(all_synsets[i])) for i in range(len(all_synsets))]

will generate an output like: 
[(synset('drink.n.01'), 1.0), (synset('drink.n.02'), 0.06666666666666667), (synset('beverage.n.01'), 0.08333333333333333), (synset('drink.n.04'), 0.09090909090909091), (synset('swallow.n.02'), 0.07692307692307693), (synset('drink.v.01'), none), (synset('drink.v.02'), none), (synset('toast.v.02'), none), (synset('drink_in.v.01'), none), (synset('drink.v.05'), none)]

you can then sort them using sorted() method providing the similarity_score as value.
sorted(corr, key=lambda x: x[1] if x[1] != none else 0, reverse=true)
[(synset('drink.n.01'), 1.0), (synset('drink.n.04'), 0.09090909090909091), (synset('beverage.n.01'), 0.08333333333333333), (synset('swallow.n.02'), 0.07692307692307693), (synset('drink.n.02'), 0.06666666666666667), (synset('drink.v.01'), none), (synset('drink.v.02'), none), (synset('toast.v.02'), none), (synset('drink_in.v.01'), none), (synset('drink.v.05'), none)]

if you want to deal with proper nouns, i suggest looking into gensim's most_similar() method.

are synsets sorted? and, in case they are, what are the criteria? are the first synsets of the list those which have higher chances to be correlated to the given word?

i cannot answer this question decisively, however i don't think there is a criteria. you can use the above method to find most similar words based on a particular synset.
edit: as mentioned in the comments below, the author of the question was looking for an order in the list returned by wordnet's synsets() method.
from the code available on github:  for the method synset()
if lang == ""eng"":
    get_synset = self.synset_from_pos_and_offset
    index = self._lemma_pos_offset_map
    if pos is none:
        pos = pos_list
    return [
        get_synset(p, offset)
        for p in pos
        for form in self._morphy(lemma, p, check_exceptions)
        for offset in index[form].get(p, [])
    ]

where pos_list has the value: pos_list = [noun, verb, adj, adv].
therefore, preference is given the order mentioned above. furthermore, according to their code: noun=""n"", verb=""v"", adj=""a"", adv=""r""
so the order primarily depends on nltk's pos tag based on pos_list, followed by what the method _morphy() returns with lemma and pos tag, followed by what _lemma_pos_offset_map() returns.
for example: 
>>> pos_list = [""n"", ""v"", ""a"", ""r""]
>>> syn = list()
>>> lemma = ""drink""
>>> for p in pos_list:
...     for form in wn._morphy(lemma, p, true):
...             for offset in wn._lemma_pos_offset_map[form].get(p, []):
...                     syn.append(wn.synset_from_pos_and_offset(p, offset))
... 
>>> syn
[synset('drink.n.01'), synset('drink.n.02'), synset('beverage.n.01'), synset('drink.n.04'), synset('swallow.n.02'), synset('drink.v.01'), synset('drink.v.02'), synset('toast.v.02'), synset('drink_in.v.01'), synset('drink.v.05')]
>>> # you can verify it with what synsets() is providing
... 
keyboardinterrupt
>>> wn.synsets(""drink"")
[synset('drink.n.01'), synset('drink.n.02'), synset('beverage.n.01'), synset('drink.n.04'), synset('swallow.n.02'), synset('drink.v.01'), synset('drink.v.02'), synset('toast.v.02'), synset('drink_in.v.01'), synset('drink.v.05')]
>>> 

hope the updated answer is helpful!",https://stackoverflow.com/questions/59355529,python,16-12-2019 11:15,1829.0,0.0,2.0,True,27-02-2022 23:02,19-12-2019 09:53
74972344,snscrape error - twitter scrape crashes after a long time giving &#39;215&#39; error,"i got the following error:




snscrape.base:4 requests to 

i used this code:
import snscrape.modules.twitter as sntwitter
import pandas as pd
import re

# setting variables to be used below
maxtweets = 350000

# creating list to append tweet data to
tweets_list1 = []
user_list = ['lanacionmas','lanacion','ambitocom','infobae','clarincom']
# using twittersearchscraper to scrape data and append tweets to list
for user in user_list:
  for i,tweet in enumerate(sntwitter.twittersearchscraper('from:' + user +' since:2018-01-01 until:2018-12-31').get_items()):
    if i>maxtweets:
        break

    tweets_list1.append([tweet.date, tweet.id, tweet.content])

print('done' + user)


do you know how to fix this error?
thanks a lot!","['python', 'web-scraping', 'twitter', 'nlp', 'twitter-api-v2']",75096054,"feel like your lucky day, because i created an account here just to reply to you.
in in this issue, the module creator explains what happened and how to solve the problem, but you just need add top=true in commands that have twitter**scraper, or if you are using it from the command line, use the flag --top
(i just found out that i could have posted as a guest)",https://stackoverflow.com/questions/74972344,python,31-12-2022 22:30,1258.0,0.0,1.0,True,12-01-2023 12:02,03-01-2023 07:00
70441578,obtain a square similarity matrix from a list of words,"i am trying to compute a similarity matrix from a list of words of 12k elements.
i am using a wordnet similarity using sematch tool. with a few words i am using this line of code:
    wns_matrix = [[wns.word_similarity(w1, w2, 'li') for w1 in words] for w2 in words]
the thing is, this code is ok with a few words but with 12k words would be a very long process, like more than a day.
is there a lean and faster way to compute a square matrix (12k x 12k) of this similarity scores without create a list of list as i am doing?
i tried this solution:
wns_matrix = [wns.word_similarity(w1, w2, 'li') for (w1, w2) in itertools.combinations(words,2)]

but still it is really slow!
i hope you can help me","['python-3.x', 'string', 'list', 'nlp']",70454030,"wns.word_similarity is a very slow function. no matter how you arrange your loops, their performance is limited by the function calls. assuming that the similarity is symmetric, you can reduce the time by a factor of 2 by adding the condition if w1<w2. that's all you can do, i am afraid.
wns_matrix = [[(wns.word_similarity(w1, w2, 'li') if w1 < w2 else np.nan)
               for w1 in words] for w2 in words]",https://stackoverflow.com/questions/70441578,python-3.x,21-12-2021 21:04,128.0,1.0,1.0,True,07-12-2022 02:34,07-12-2022 02:34
71871613,number of matches for keywords in specified categories,"for a large scale text analysis problem, i have a data frame containing words that fall into different categories, and a data frame containing a column with strings and (empty) counting columns for each category. i now want to take each individual string, check which of the defined words appear, and count them within the appropriate category.
as a simplified example, given the two data frames below, i want to count how many of each animal type appear in the text cell.
df_texts <- tibble(
  text=c(""the ape and the fox"", ""the tortoise and the hare"", ""the owl and the the 
  grasshopper""),
  mammals=na,
  reptiles=na,
  birds=na,
  insects=na
)

df_animals <- tibble(animals=c(""ape"", ""fox"", ""tortoise"", ""hare"", ""owl"", ""grasshopper""),
           type=c(""mammal"", ""mammal"", ""reptile"", ""mammal"", ""bird"", ""insect""))

so my desired result would be:
df_result <- tibble(
  text=c(""the ape and the fox"", ""the tortoise and the hare"", ""the owl and the the 
  grasshopper""),
  mammals=c(2,1,0),
  reptiles=c(0,1,0),
  birds=c(0,0,1),
  insects=c(0,0,1)
)

is there a straightforward way to achieve this keyword-matching-and-counting that would be applicable to a much larger dataset?
thanks in advance!","['r', 'nlp']",71871842,"here's a way do to it in the tidyverse. first look at whether strings in df_texts$text contain animals, then count them and sum by text and type.
library(tidyverse)

cbind(df_texts[, 1], sapply(df_animals$animals, grepl, df_texts$text)) %>% 
  pivot_longer(-text, names_to = ""animals"") %>% 
  left_join(df_animals) %>% 
  group_by(text, type) %>% 
  summarise(sum = sum(value)) %>% 
  pivot_wider(id_cols = text, names_from = type, values_from = sum)

  text                                   bird insect mammal reptile
  <chr>                                 <int>  <int>  <int>   <int>
1 ""the ape and the fox""                     0      0      2       0
2 ""the owl and the the \n  grasshopper""     1      0      0       0
3 ""the tortoise and the hare""               0      0      1       1


to account for the several occurrences per text:
cbind(df_texts[, 1], t(sapply(df_texts$text, str_count, df_animals$animals, use.names = f))) %>% 
  setnames(c(""text"", df_animals$animals)) %>% 
  pivot_longer(-text, names_to = ""animals"") %>% 
  left_join(df_animals) %>% 
  group_by(text, type) %>% 
  summarise(sum = sum(value)) %>% 
  pivot_wider(id_cols = text, names_from = type, values_from = sum)",https://stackoverflow.com/questions/71871613,r,14-04-2022 12:27,147.0,3.0,1.0,True,14-04-2022 13:32,14-04-2022 13:08
17753182,getting a large list of nouns (or adjectives) in python with nltk; or python mad libs,"like this question, i am interested in getting a large list of words by part of speech (a long list of nouns; a list of adjectives) to be used programmatically elsewhere. this answer has a solution using the wordnet database (in sql) format. 
is there a way to get at such list using the corpora/tools built into the python nltk. i could take a large bunch of text, parse it and then store the nouns and adjectives. but given the dictionaries and other tools built in, is there a smarter way to simply extract the words that are already present in the nltk datasets, encoded as nouns/adjectives (whatever)?
thanks.","['python', 'machine-learning', 'nltk']",17753310,"it's worth noting that wordnet is actually one of the corpora included in the nltk downloader by default. so you could conceivably just use the solution you already found without having to reinvent any wheels.
for instance, you could just do something like this to get all noun synsets:
from nltk.corpus import wordnet as wn

for synset in list(wn.all_synsets('n')):
    print synset

# or, equivalently
for synset in list(wn.all_synsets(wn.noun)):
    print synset

that example will give you every noun that you want and it will even group them into their synsets so you can try to be sure that they're being used in the correct context.
if you want to get them all into a list you can do something like the following (though this will vary quite a bit based on how you want to use the words and synsets):
all_nouns = []
for synset in wn.all_synsets('n'):
    all_nouns.extend(synset.lemma_names())

or as a one-liner:
all_nouns = [word for synset in wn.all_synsets('n') for word in synset.lemma_names()]",https://stackoverflow.com/questions/17753182,python,19-07-2013 18:33,24816.0,11.0,3.0,True,21-02-2021 17:09,23-05-2017 11:46
