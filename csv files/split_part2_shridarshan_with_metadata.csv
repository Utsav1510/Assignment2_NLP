question_id,title,body,tags,accepted_answer_id,accepted_answer_body,link,tag,creation_date,view_count,score,answer_count,is_answered,last_activity_date,last_edit_date
36629329,how do i keep intra-word periods in unigrams? r quanteda,"i would like to preserve two letter acronyms in my unigram frequency table that are separated by periods such as ""t.v."" and ""u.s."".  when i build my unigram frequency table with quanteda, the teminating period is getting truncated.  here is a small test corpus to illustrate.  i have removed periods as sentence separators:
sos this is the u.s. where our politics is crazy eos
sos in the us we watch a lot of t.v. aka tv eos
sos tv is an important part of life in the us eos
sos folks outside the u.s. probably don't watch so much t.v. eos
sos living in other countries is probably not any less crazy eos
sos i enjoy my sanity when it comes to visit eos
which i load into r as character vector:
acro.test <- c(""sos this is the u.s. where our politics is crazy eos"", ""sos in the us we watch a lot of t.v. aka tv eos"", ""sos tv is an important part of life in the us eos"", ""sos folks outside the u.s. probably don't watch so much t.v. eos"", ""sos living in other countries is probably not any less crazy eos"", ""sos i enjoy my sanity when it comes to visit eos"")

here is the code i use to build my unigram frequency table:
library(quanteda)
dat.dfm <- dfm(acro.test, ngrams=1, verbose=true, concatenator="" "",  tolower=false, removenumbers=true, removepunct=false, stopwords=false)
dat.mat <- as.data.frame(as.matrix(docfreq(dat.dfm)))
ng.sorted <- sort(rowsums(dat.mat), decreasing=true)
freqtable <- data.frame(ngram=names(ng.sorted), frequency = ng.sorted)
row.names(freqtable) <- null
freqtable

this produces the following:
       ngram frequency
1        sos         6
2        eos         6
3        the         4
4         is         3
5          .         3
6        u.s         2
7      crazy         2
8         us         2
9      watch         2
10        of         2
11       t.v         2
12        tv         2
13        in         2
14  probably         2
15      this         1
16     where         1
17       our         1
18  politics         1
19        in         1
20        we         1
21         a         1
22       lot         1
23       aka         1

etc...
i would like to keep the terminal periods on t.v. and u.s. as well as eliminate the entry in the table for . with a frequency of 3.
i also don't understand why the period (.) would have a count of 3 in this table while counting the u.s and t.v unigrams correctly (2 each).","['r', 'nlp', 'n-gram', 'quanteda']",36631672,"the reason for this behaviour is that quanteda's default word tokeniser uses the icu-based definition for word boundaries (from the stringi package).  u.s. appears as the word u.s. followed by a period . token.  this is great if your name is will.i.am but maybe not so great for your purposes.  but you can easily switch to the white-space tokeniser, using the argument what = ""fasterword"" passed to tokens(), an option available in dfm() through the ... part of the function call.
tokens(acro.test, what = ""fasterword"")[[1]]
## [1] ""sos""      ""this""     ""is""       ""the""      ""u.s.""     ""where""    ""our""      ""politics"" ""is""       ""crazy""    ""eos"" 

you can see that here, u.s. is preserved.  in response to your last question, the terminal . had a document frequency of 3 because it appeared in three documents as a separate token, which is the default word tokeniser behaviour when remove_punct = false.
to pass this through to dfm() and then construct your data.frame of the document frequency of the words, the following code works (i've tidied it up a bit for efficiency).  note the comment about the difference between document and term frequency - i've noted that some users are a bit confused about docfreq().
# i removed the options that were the same as the default 
# note also that stopwords = true is not a valid argument - see remove parameter
dat.dfm <- dfm(acro.test, tolower = false, remove_punct = false, what = ""fasterword"")

# sort in descending document frequency
dat.dfm <- dat.dfm[, names(sort(docfreq(dat.dfm), decreasing = true))]
# note: this would sort the dfm in descending total term frequency
#       not the same as docfreq
# dat.dfm <- sort(dat.dfm)

# this creates the data.frame in one more efficient step
freqtable <- data.frame(ngram = featnames(dat.dfm), frequency = docfreq(dat.dfm),
                        row.names = null, stringsasfactors = false)
head(freqtable, 10)
##    ngram frequency
## 1    sos         6
## 2    eos         6
## 3    the         4
## 4     is         3
## 5   u.s.         2
## 6  crazy         2
## 7     us         2
## 8  watch         2
## 9     of         2
## 10  t.v.         2

in my view the named vector produced by docfreq() on the dfm is a more efficient method for storing the results than your data.frame approach, but you may wish to add other variables.",https://stackoverflow.com/questions/36629329,r,14-04-2016 16:53,311.0,1.0,1.0,True,19-10-2022 08:00,14-04-2016 18:56
70282594,text classification from html with beautifulsoup,"i have obtained html page source code and then parsed it using 'html5lib' with beautifulsoup.
i have got something like this:
<div class=""v0h1ob-haaclf opzbo-ke6vqe o0s21d-hiayvf"" jsaction=""mouseover:pane.wfvdle40;mouseout:pane.wfvdle40"" jsan=""7.v0h1ob-haaclf,7.opzbo-ke6vqe,7.o0s21d-hiayvf,0.jsaction"" jstcache=""824"">
    <a aria-label=""muzeum londynu"" class=""a4gq8e-avtxab-haaclf-jrmmhf-hsrgpd"" href="" jsaction=""pane.wfvdle40;focus:pane.wfvdle40;blur:pane.wfvdle40;auxclick:pane.wfvdle40;contextmenu:pane.wfvdle40;keydown:pane.wfvdle40;clickmod:pane.wfvdle40"" jsan=""7.a4gq8e-avtxab-haaclf-jrmmhf-hsrgpd,0.aria-label,8.href,0.jsaction"" jstcache=""825""></a>
    <div class=""cjy91c-jrmmhf-avtxab-haaclf-wfkmr"" jstcache=""826""></div>
    <div aria-label=""muzeum londynu"" class=""mvvflb-haaclf v0h1ob-haaclf-d6wfac mvvflb-haaclf-uxvfw-hsrgpd"" jsan=""7.mvvflb-haaclf,7.v0h1ob-haaclf-d6wfac,7.mvvflb-haaclf-uxvfw-hsrgpd,0.aria-label"" jstcache=""827"">
        <div class=""cjy91c-jrmmhf-avtxab-haaclf-biwrp"" jstcache=""828""></div>
        <div class=""li9ife"">
            <div class=""cjy91c-jrmmhf-avtxab-haaclf-hsrblb"" jstcache=""829"">
                <div class=""rnefrd-jrmmhf-hsrblb b9hcub-qflw2"" jsan=""t-pddsp4p8dqq,7.rnefrd-jrmmhf-hsrblb,7.b9hcub-qflw2"" jstcache=""933"">
                    <button jstcache=""842"" style=""display:none""></button>
                    <div class=""z8fk3b"" jsan=""7.z8fk3b,t-mjeqqy5xodm"" jstcache=""843""> 
                        <div class=""cuwbzc-content gm2-body-2""> <div class=""qbf1pd-haaclf"">
                            <div class=""qbf1pd gm2-subtitle-alt-1"" jsan=""7.qbf1pd,7.gm2-subtitle-alt-1,t-u3p6pfxaxm4"" jstcache=""845"">
                                <span jstcache=""858"">muzeum londynu</span> 
                            </div>
                            <h1 jstcache=""846"" style=""display:none""></h1> 
                            <span class=""rnefrd-jrmmhf-hsrblb-title-btuy5e-haaclf""></span> 
                        </div> 
                        <div class=""section-subtitle-extension"" jstcache=""847""></div> 
                        <div class=""zy2y6b-rwgcyc"" jsan=""7.zy2y6b-rwgcyc,t-heqdox2ffv0"" jstcache=""848""> 
                        <div class=""oevfgc-wcwwm-haaclf""> 
                            <span class=""rnefrd-jrmmhf-hsrblb-wpzpjb-btuy5e-haaclf"" jstcache=""860""></span>
                            <span class=""gm2-body-2"" jsan=""t-cj3gw1vpbaa,7.gm2-body-2"" jstcache=""861"">
                            <span jstcache=""868"" style=""display:none""></span>
                            <span aria-label="" 4,6-gwiazdkowy  opinie (13 898)  "" class=""zkp5je"" jsan=""7.zkp5je,0.aria-label,0.role,t-kqtgnps-9g0"" jstcache=""869"" role=""group"">
                            <span aria-hidden=""true"" class=""mw4etd"" jsan=""7.mw4etd,0.aria-hidden"" jstcache=""872"">4,6</span>
                            <div jstcache=""873"" style=""display:none""></div>
                            <div class=""qbul8c"" jsan=""7.qbul8c"" jsinstance=""0"" jstcache=""874""></div>
                            <div class=""qbul8c"" jsan=""7.qbul8c"" jsinstance=""1"" jstcache=""874""></div>
                            <div class=""qbul8c"" jsan=""7.qbul8c"" jsinstance=""2"" jstcache=""874""></div>
                            <div class=""qbul8c"" jsan=""7.qbul8c"" jsinstance=""3"" jstcache=""874""></div>
                            <div class=""qbul8c cxokeb-s62q7b"" jsan=""7.qbul8c,7.cxokeb-s62q7b"" jsinstance=""*4"" jstcache=""874""></div> 
                            <span aria-hidden=""true"" class=""uy7f9"" jsan=""7.uy7f9,0.aria-hidden"" jstcache=""875"">(13 898)</span>
                        </span>
                     </span> 
                     <span jstcache=""862"" style=""display:none"">
                         <jsl jstcache=""863"" style=""display:none""></jsl> 
                     </span> 
                 </div> 
             </div> 
             <div class=""zy2y6b-rwgcyc""> 
                 <span jstcache=""849"" style=""display:none""></span> 
                 <div class=""zy2y6b-rwgcyc"" jsinstance=""0"" jstcache=""850""> 
                     <span jsinstance=""0"" jstcache=""851"">
                          <jsl jstcache=""852""> <span jstcache=""884"" style=""display:none"">ï¿½ï¿½</span> 
                          <span jstcache=""885"">muzeum</span> <span jstcache=""886"" style=""display:none""></span> </jsl> </span><span jsinstance=""*1"" jstcache=""851""> <jsl jstcache=""852""> <span aria-hidden=""true"" class=""bxlt7b-hgduwe"" jsan=""lt7b-hgduwe,0.aria-hidden"" jstcache=""884"">ï¿½ï¿½</span> <span jstcache=""885"">150 london wall</span> <span jstcache=""886"" style=""display:none""></span> </jsl> </span> </div><div class=""zy2y6b-rwgcyc"" jsinstance=""1"" jstcache=""850""> <span jsinstance=""*0"" jstcache=""851""> <jsl jstcache=""852""> <span jstcache=""884"" style=""display:none"">ï¿½ï¿½</span> <span jstcache=""885"">historia londynu od staroï¿½ï¿½ytnoï¿½ï¿½ci do dziï¿½ï¿½</span> <span jstcache=""886"" style=""display:none""></span> </jsl> </span> </div><div class=""zy2y6b-rwgcyc"" jsinstance=""*2"" jstcache=""850""> <span jsinstance=""*0"" jstcache=""851""> <jsl jstcache=""852""> <span jstcache=""884"" style=""disp</span> <span jstcache=""885"">zamkniï¿½ï¿½cie: 17:00</span> <span jstcache=""886"" style=""display:none""></span> </jsl> </span> </div> </div> </div> </div></div></div><div class=""cjy91c-jrmmhf-avtxab-haaclf-jibuqc"" jstcache=""830""></div><div class=""cjy91c-jrmmhf-avtxab-haaclf-hiayvf"" jstcache=""831""><div class=""xwpmrb qisnde"" jsan=""t-pls0ilpsy_c,7.xwpmrb,7.qisnde,5.width,5.height,5.margin-top,5.margin-bottom,5.margin-left,5.margin-right"" jstcache=""932"" style=""width: 84px; height: 84px; margin: 0px;""><div class=""vig8jf-haaclf p0hhde"" jsan=""7.p0hhde,7.vig8jf-haaclf,5.min-width,5.min-height"" jstcache=""836"" style=""min-width:84px;min-height:84px""></div><button jstcache=""837"" style=""display:none""></button><div class=""badge-container""></div></div></div><div class=""cjy91c-jrmmhf-avtxab-haaclf-hxbzzc"" jstcache=""832""></div></div><div class=""cjy91c-jrmmhf-avtxab-haaclf-iowfhc"" jstcache=""833""></div></div></div>

the last part was running methong .find_all('a', href=true) which got me something like this:
[<a aria-label=""muzeum londynu"" class=""a4gq8e-avtxab-haaclf-jrmmhf-hsrgpd"" href="" jsaction=""pane.wfvdle40;focus:pane.wfvdle40;blur:pane.wfvdle40;auxclick:pane.wfvdle40;contextmenu:pane.wfvdle40;keydown:pane.wfvdle40;clickmod:pane.wfvdle40"" jsan=""7.a4gq8e-avtxab-haaclf-jrmmhf-hsrgpd,0.aria-label,8.href,0.jsaction"" jstcache=""825""></a>]

i am trying to specifically extract longitude and latitude which are [51.5176183, -0.0967782] present in the href.
i've tried using .href method similar to .text method but when i am using .href 'none' is being returned. could you tell me how to extract those two velues from href body?
running .text method on the html code returning output like this:
museum of london         4,6(13 898)           ï¿½ï¿½ museum     ï¿½ï¿½ 150 london wall       ï¿½ï¿½ the history of london from antiquity to today       ï¿½ï¿½ closing: 17:00","['python', 'html', 'beautifulsoup', 'href', 'text-classification']",70283012,"according to your question, i use split() method to get the desired output.
script
html='''
<html>
 <head>
 </head>
 <body>
  <a aria-label=""muzeum londynu"" class=""a4gq8e-avtxab-haaclf-jrmmhf-hsrgpd"" href="" jsaction=""pane.wfvdle40;focus:pane.wfvdle40;blur:pane.wfvdle40;auxclick:pane.wfvdle40;contextmenu:pane.wfvdle40;keydown:pane.wfvdle40;clickmod:pane.wfvdle40"" jsan=""7.a4gq8e-avtxab-haaclf-jrmmhf-hsrgpd,0.aria-label,8.href,0.jsaction"" jstcache=""825"">
  </a>
 </body>
</html>
'''
from bs4 import beautifulsoup
soup = beautifulsoup(html,'html5lib')
#print(soup.prettify())
href=soup.find(""a"",class_=""a4gq8e-avtxab-haaclf-jrmmhf-hsrgpd"").get('href')
lat_lan=','.join(href.split('/')[-1].split('?')[0].split(':')[-1].split('!')[2:]).replace('3d','').replace('4d','').split()
print(lat_lan)

output
['51.5176183', '-0.0967782']",https://stackoverflow.com/questions/70282594,python,08-12-2021 22:39,389.0,1.0,1.0,True,08-12-2021 23:34,08-12-2021 23:14
77922817,importerror: cannot import name &#39;iterator&#39; from &#39;typing_extensions&#39;,"when i try to install openai on my google colab, i get this error:
from openai import openai


---------------------------------------------------------------------------
importerror                               traceback (most recent call last)
<ipython-input-6-5ee9a4e68b89> in <cell line: 1>()
----> 1 from openai import openai

5 frames
/usr/local/lib/python3.10/dist-packages/openai/_utils/_streams.py in <module>
      1 from typing import any
----> 2 from typing_extensions import iterator, asynciterator
      3 
      4 
      5 def consume_sync_iterator(iterator: iterator[any]) -> none:

importerror: cannot import name 'iterator' from 'typing_extensions' (/usr/local/lib/python3.10/dist-packages/typing_extensions.py)

---------------------------------------------------------------------------
note: if your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

to view examples of installing some common dependencies, click the
""open examples"" button below.

my details to reproduce:
python 3.10.12
os is windows 11
typing_extensions is 4.9.0
edit:
my typing_extensions details:","['python', 'python-3.x', 'pip', 'openai-api']",77922914,"i found a related issue on openai forum:

solution 1:
credit to @aymane_oub
pip install --force-reinstall typing-extensions==4.5
pip install --force-reinstall openai==1.8


solution 2:
credit to @shannon-21
after navigating through the answers i found that i can change the import of the file that is trying to import iterator from typing_extensions in this way (on google colab):
%%writefile /usr/local/lib/python3.10/dist-packages/openai/_utils/_streams.py
from typing import any
from typing_extensions import asynciterator
from typing import iterator # import iterator from the correct library

def consume_sync_iterator(iterator: iterator[any]) -> none:
    for _ in iterator:
        ...

async def consume_async_iterator(iterator: asynciterator[any]) -> none:
    async for _ in iterator:
        ...

#output

after running the above code on my colab cell, i was able to use:

link to answer :",https://stackoverflow.com/questions/77922817,python,01-02-2024 19:18,8586.0,2.0,1.0,True,13-02-2024 10:18,13-02-2024 10:18
73744658,resource punkt not found. please use the nltk downloader to obtain the resource: &gt;&gt;&gt; import nltk &gt;&gt;&gt; nltk.download(&#39;punkt&#39;),"i have nltk installed and it is giving me an error:
resource punkt not found.
please use the nltk downloader to obtain the resource:

import nltk
nltk.download('punkt')
for more information see: 

attempted to load tokenizers/punkt/py3/english.pickle

  searched in:
    - '/users/divyanshundley/nltk_data'
    - '/library/frameworks/python.framework/versions/3.10/nltk_data'
    - '/library/frameworks/python.framework/versions/3.10/share/nltk_data'
    - '/library/frameworks/python.framework/versions/3.10/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
    - ''
**********************************************************************

and my code is:
import nltk
nltk.download('punkt')

def tokenize(token):
    return nltk.word_tokenize(token);
tokenize(""why is this not working?"");","['python', 'nltk']",74060135,"adding these code will resolve your issue:
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')",https://stackoverflow.com/questions/73744658,python,16-09-2022 12:10,56427.0,17.0,7.0,True,11-01-2025 03:08,11-01-2025 03:00
71720038,runtimeerror: boolean value of tensor with more than one value is ambiguous in python,"i'm facing the following error, and i don't know why:
this code is on github, i ran it correctly on collab, but it gives me the following error here:
device=""cpu""
lr=3e-5#1e-3
num_training_steps=int(len(dataset) / train_batch_size * epoch)

model=bert_classification_model().to(device)
optimizer=adamw(model.parameters(), lr=lr)
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                        num_warmup_steps = 0,
                                        num_training_steps = num_training_steps)
val_losses=[]
batches_losses=[]
val_acc=[]
for epoch in range(epoch):
    t0 = time.time()    
    print(f""\n=============== epoch {epoch+1} / {epoch} ===============\n"")
    batches_losses_tmp=train_loop_fun1(train_data_loader, model, optimizer, device)
    epoch_loss=np.mean(batches_losses_tmp)
    print(f""\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\n"")
    t1=time.time()
    output, target, val_losses_tmp=eval_loop_fun1(valid_data_loader, model, device)
    print(f""==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\n"")
    tmp_evaluate=evaluate(target.reshape(-1), output)
    print(f""=====>\t{tmp_evaluate}"")
    val_acc.append(tmp_evaluate['accuracy'])
    val_losses.append(val_losses_tmp)
    batches_losses.append(batches_losses_tmp)
    print(""\tï¿½ï¿½ï¿½ï¿½ model has been saved ï¿½ï¿½ï¿½ï¿½"")
    torch.save(model, f""model1/model_epoch{epoch+1}.pt"")    




some weights of the model checkpoint at bert-base-uncased were not used when initializing bertmodel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.layernorm.bias', 'cls.predictions.transform.layernorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- this is expected if you are initializing bertmodel from the model trained on another task or with another architecture (e.g. initializing a bertforsequenceclassification model from a bertforpretraining model).
- this is not expected if you are initializing bertmodel from the checkpoint of a model that you expect to be exactly identical (initializing a bertforsequenceclassification model from a bertforsequenceclassification model).


=============== epoch 1 / 3 ===============

---------------------------------------------------------------------------

runtimeerror                              traceback (most recent call last)

<ipython-input-33-aa98faac385e> in <module>()
     14     t0 = time.time()
     15     print(f""\n=============== epoch {epoch+1} / {epoch} ===============\n"")
---> 16     batches_losses_tmp=train_loop_fun1(train_data_loader, model, optimizer, device)
     17     epoch_loss=np.mean(batches_losses_tmp)
     18     print(f""\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\n"")

6 frames

/content/robert_recurrence_over_bert/custom_dataset_class.py in long_terms_tokenizer(self, data_tokenize, targets)
    158         targets_list.append(targets)
    159 
--> 160         if remain and self.approach != 'head':
    161             remain = torch.tensor(remain, dtype=torch.long)
    162             idxs = range(len(remain)+self.chunk_len)

runtimeerror: boolean value of tensor with more than one value is ambiguous

this is the file link:","['python', 'pytorch', 'bert-language-model']",71732394,"your tensor remain (in your dataset class) is a boolean tensor and not a boolean variable. therefore, the condition if remain is not well-defined.",https://stackoverflow.com/questions/71720038,python,02-04-2022 18:14,3697.0,0.0,1.0,True,04-04-2022 05:24,04-04-2022 05:12
73102008,how to convert a dictionary with a tuple inside of a nested list?,"i'm trying to create a bigram from a dictionary with a specific condition. below is the example of the dictionary:
dict_example = {'keywords1': ['africa',
  'basic service',
  'class',
  'develop country',
  'disadvantage',
  'economic resource',
  'social protection system']

the specific condition is that i want to create a bigram if the words in each element are more than 1. below is the code that i have been working on so far:
keywords_bigram_temp = {}
keywords_bigram = {}
for k, v in dict_example.items():
    keywords_bigram_temp.update({k: [word_tokenize(w) for w in v]})
    for k2, v2 in keywords_bigram_temp.items():
        keywords_bigram.update({k2: [list(nltk.bigrams(v3)) for v3 in v2 if len(v3) > 1]})

this code works, but instead of returning a normal tuple within a list (i think this is what bigram normally looked like), it returns a tuple within a nested list. below is an example of the result:
'keywords1': [[('basic', 'service')],
  [('develop', 'country')],
  [('economic', 'resource')],
  [('social', 'protection'),
   ('social', 'system'),
   ('protection', 'system'),
   ('social', 'protection')]}

what i need is a normal bigram structure, a tuple within a list like so:
'keywords1': [('basic', 'service'),
  ('develop', 'country'),
  ('economic', 'resource'),
  ('social', 'protection'),
  ('protection', 'system')]}","['python', 'nltk']",73102177,"here's a way to do what your question asks using itertools.combinations():
from itertools import combinations
keywords_bigram = {'keywords1': [x for elem in dict_example['keywords1'] if ' ' in elem for x in combinations(elem.split(), 2)]}

output:
{'keywords1': [('basic', 'service'), ('develop', 'country'), ('economic', 'resource'), ('social', 'protection'), ('social', 'system'), ('protection', 'system')]}

explanation:

in the dict comprehension, use for elem in dict_example['keywords1'] if ' ' in elem to iterate over all items in the list associated with keywords1 that contain a ' ' character, meaning the words in the element number more than 1
use the nested loop for x in combinations(elem.split(), 2) to include every unique combination of 2 words within the multi-word item

update:
based on op's clarification that original question contained an extra entry,  and that what is required is ""in a 'a b c d' context, it will become ('a','b'),('b','c'),('c','d')"", here are three alternative solutions.
solution #1 using walrus operator := and dict comprehension:
keywords_bigram = {'keywords1': [x for elem in dict_example['keywords1'] if len(words := elem.split()) > 1 for x in zip(words, words[1:])]}

solution #2 using a long-hand for loop:
keywords_bigram = {'keywords1': []}
for elem in dict_example['keywords1']:
    words = elem.split()
    if len(words) > 1:
        keywords_bigram['keywords1'].extend(zip(words, words[1:]))

solution #3 without zip():
keywords_bigram = {'keywords1': []}
for elem in dict_example['keywords1']:
    words = elem.split()
    if len(words) > 1:
        for i in range(len(words) - 1):
            keywords_bigram['keywords1'].append(tuple(words[i:i+2]))

all three solutions give identical output:
{'keywords1': [('basic', 'service'), ('develop', 'country'), ('economic', 'resource'), ('social', 'protection'), ('protection', 'system')]}",https://stackoverflow.com/questions/73102008,python,24-07-2022 20:58,78.0,1.0,1.0,True,25-07-2022 19:25,25-07-2022 13:10
74846628,why is openai image generation api returning 400 bad request in unity?,"i'm testing out dynamically generating images using openai api in unity. amusingly, i actually generated most of this code from chatgpt.
the error response is: ""your request contained invalid json: expecting value: line 1 column 1 (char 0)"". but i cant see anything wrong with the json formatting of my requestbody...
i also found this other question which is probably failing for the same reason, whatever it is: why does post request to openai in unity result in error 400?
here is my code:
public class playerscript : monobehaviour
{
    // replace api_key with your actual api key
    private string api_key = ""<api_key>"";
    private string api_url = ""

    void start()
    {
        startcoroutine(getimage());
    }

    ienumerator getimage()
    {
        // create a request body with the prompt ""player""
        string requestbody = ""{\""prompt\"": \""player\"",\""n\"": 1,\""size\"": \""128x128\""}"";
        // create a unitywebrequest and set the request method to post
        unitywebrequest www = unitywebrequest.post(api_url, requestbody);

        // set the authorization header with the api key
         ""bearer "" + api_key);

        // set the content type header
         ""application/json"");

        // send the request
        yield return 

        // check for errors
        if ( || 
        {
            debug.logerror(
        }
        else
        {
            // do stuff
        }
    }
}

any idea what's going wrong? apologies if it's something obvious, never made web requests in unity before.","['json', 'unity-game-engine', 'post', 'openai-api', 'unitywebrequest']",75045869,"unitywebrequests url-encodes content before submitting the request. this messes up the formatting. see: 
using  worked for me.",https://stackoverflow.com/questions/74846628,json,19-12-2022 05:40,1105.0,0.0,1.0,True,08-01-2023 13:51,08-01-2023 13:51
78856532,how to make huggingface transformer for translation return n translation inferences?,"so i am trying to use this transformer from huggingface  the issue is that i want n translations returned and not just one. how can i  do that? i mean, i want to have ordered translations, that means the translation with index 0 would have the highest confidence, this is important for my use case, which is about translating natural language to commands language (about 40 commands without subcommands).
the github repo and exact model is this one 
this is the huggingface api:
translator = pipeline(""translation_xx_to_yy"", model=""my_awesome_opus_books_model"")
translator(text)

but i am intending to use the model directly from the google search github repo, so it seems some tweaking should be done here:
predictions = []
    for batch in dataset:
      predicted_tokens = self._model.generate(
          input_ids=self.to_tensor(batch[""inputs""]), **generate_kwargs
      )
      predicted_tokens = predicted_tokens.cpu().numpy().tolist()
      predictions.extend(
          [vocabs[""targets""].decode(p) for p in predicted_tokens]
      )

    for inp, pred in zip(inputs, predictions):
      logging.info(""%s\n  -> %s"", inp, pred)

    if output_file is not none:
      utils.write_lines_to_file(predictions, output_file)

also any suggestion on some other model option to solve this natural language to cmd is welcomed!","['python', 'huggingface-transformers', 'transformer-model']",78856815,"check out the documentation of the generate method: 
the parameter to use is num_return_sequences. but t5 by default does a greedy search, meaning it generates word by word and discards the options on its path there. to generate multiple options you need a selection of alternative paths. there are basically two ways to do this (my guess would be that for your case the first option works better):
if you activate do_sample, the model will not just pick the highest probability token at each time, but instead take a weighted sample from the distribution of next word probabilities.
predicted_tokens = self._model.generate(
          input_ids=self.to_tensor(batch[""inputs""]), num_return_sequences=3, do_sample=true, **generate_kwargs
      )

if you set num_beams to anything larger than 1, you switch to beam search, where for each further token the model follows multiple alternatives of next tokens.
predicted_tokens = self._model.generate(
          input_ids=self.to_tensor(batch[""inputs""]), num_return_sequences=3, num_beams=4, **generate_kwargs
      )  # note that num_return_sequences has to be smaller or equal to num_beans

to also get the scores of generated outputs you can additionally use the arguments output_scores=true and return_dict_in_generate=true, although you should note that these will return the logits of all individual tokens, which you then would have to put together to the overall probability yourself, check out 
in general, t5 might not be the best model for code synthesis, as of my knowledge it wasn't pretrained or fine-tuned on it in its multi-task instruction fine-tuning. there is however flan-t5, which was fine-tuned on a wider range of tasks, including code synthesis. there are also codet5 and many other models relating to code synthesis.",https://stackoverflow.com/questions/78856532,python,10-08-2024 16:37,331.0,1.0,1.0,True,10-08-2024 19:06,10-08-2024 19:06
75179250,is splitting a long document of a dataset for bert considered bad practice?,"i am fine-tuning a bert model on a labeled dataset with many documents longer than the 512 token limit set by the tokenizer.
since truncating would lose a lot of data i would rather use, i started looking for a workaround. however i noticed that simply splitting the documents after 512 tokens (or another heuristic) and creating new entries in the dataset with the same label was never mentioned.
in this answer, someone mentioned that you would need to recombine the predictions, is that necessary when splitting the documents?
is this generally considered bad practice or does it mess with the integrity of the results?","['machine-learning', 'nlp', 'classification', 'bert-language-model', 'text-classification']",75255155,"you have not mentioned if your intention is to classify, but given that you refer to an article on classification i will refer to an approach where you classify the whole text.
the main question is - which part of the text is the most informative for your purpose - or - in other words - does it make sense to use more than the first / last split of text?
when considering long passages of text, frequently, it is enough to consider the first (or last) 512 tokens to correctly predict the class in substantial majority of cases (say 90%). even though you may loose some precision, you gain on speed and performance of the overall solution and you are getting rid of a nasty problem of figuring out the correct class out of a set of classifications. why?
consider an example of text 2100 tokens long. you split it by 512 tokens, obtaining pieces: 512, 512, 512, 512, 52 (notice the small last piece - should you even consider it?). your target class for this text is, say, a, however you get the following predictions on the pieces: a, b, a, b, c. so you have now a headache to figure out the right method to determine the class. you can:

use majority voting but it is not conclusive here.
weight the predictions by the length of the piece. again non conclusive.
check that prediction of the last piece is class c but it is barely above the threshold and class c is kinda a. so you are leaning towards a.
re-classify starting the split from the end. in the same order as before you get: a, b, c, a, a. so, clearly a. you also get it when you majority vote combining all of the classifications (forward and backward splits).
consider the confidence of the classifications, e.g. a: 80, b: 70, a: 90, b: 60, c: 55% - avg. 85% for a vs. 65% for b.
reconfirm the correction of labelling of the last piece manually: if it turns out to be b, then it changes all of the above.
then you can train an additional network to classify out of the raw classifications of pieces. getting again into trouble of figuring out what to do with particularly long sequences or non-conclusive combinations of predictions resulting in poor confidence of the additional classification layer.

it turns out that there is no easy way. and you will notice that text is a strange classification material exhibiting all of the above (and more) issues while typically the difference in agreement between the first piece prediction and the annotation vs. the ultimate, perfect classifier is slim at best.
so, spare the effort and strive for simplicity, performance, and heuristic... and clip it!
on details of the best practices you should probably refer to the article from this answer.",https://stackoverflow.com/questions/75179250,machine-learning,19-01-2023 23:20,1580.0,1.0,1.0,True,27-01-2023 07:37,27-01-2023 07:37
71591765,facebook&#39;s duckling cannot identify time dimension correctly,"i'm using facebook's duckling to parse text. when i pass the text: 13h 47m it correctly classifies the entire text as duration (= 13 hours 47 minutes).
however, when i pass the text: 13h 47m 13s it cannot identify the 13s part of the string as being part of the duration. i was expecting it to parse it as 13 hours, 47 minutes and 13 seconds but it essentially ignores the 13s part as not being part of the duration.
command: curl -xpost  --data locale=en_us&text=""13h 47m 13s""
json array: 
[
  {
    ""latent"": false,
    ""start"": 0,
    ""dim"": ""duration"",
    ""end"": 7,
    ""body"": ""13h 47m"",
    ""value"": {
      ""unit"": ""minute"",
      ""normalized"": {
        ""unit"": ""second"",
        ""value"": 49620
      },
      ""type"": ""value"",
      ""value"": 827,
      ""minute"": 827
    }
  },
  {
    ""latent"": false,
    ""start"": 8,
    ""dim"": ""number"",
    ""end"": 10,
    ""body"": ""13"",
    ""value"": {
      ""type"": ""value"",
      ""value"": 13
    }
  }
]

is this a bug? how can i update duckling so that it parses the text as described above?","['haskell', 'nlp', 'duckling']",71593144,"the documentation seems pretty clear about this:

to extend duckling's support for a dimension in a given language, typically 4 files need to be updated:

duckling/<dimension>/<lang>/rules.hs
duckling/<dimension>/<lang>/corpus.hs
duckling/dimensions/<lang>.hs (if not already present in duckling/dimensions/common.hs)
duckling/rules/<lang>.hs


taking a look in duckling/duration/rules.hs, i see:
ruleintegerunitofduration = rule
  { name = ""<integer> <unit-of-duration>""
  , pattern =
    [ predicate isnatural
    , dimension timegrain
    ]
  -- ...

so next i peeked in duckling/timegrain/en/rules.hs (because duckling/timegrain/rules.hs did not exist), and see:
grains :: [(text, string, tg.grain)]
grains = [ (""second (grain) "", ""sec(ond)?s?"",      tg.second)
         -- ...

presumably this means 13h 47m 13sec would parse the way you want. to make 13h 47m 13s parse in the same way, i guess the first thing i would try would be to make the regex above a bit more permissive, maybe something like s(ec(ond)?s?)?, and see if that does the trick without breaking anything else you care about.",https://stackoverflow.com/questions/71591765,haskell,23-03-2022 17:44,544.0,2.0,1.0,True,23-03-2022 19:43,23-03-2022 19:33
76655508,how to get the vector embedding of a token in gpt?,"i have a gpt model
model = biogptforcausallm.from_pretrained(""microsoft/biogpt"").to(device)

when i send my batch to it i can get the logits and the hidden states:
out = model(batch[""input_ids""].to(device), output_hidden_states=true, return_dict=true)
print(out.keys())
>>> odict_keys(['logits', 'past_key_values', 'hidden_states'])

the logits have shape of
torch.size([2, 1024, 42386])

corresponding to (batch, seq_length, vocab_length)
how can i get the vector embedding of the first (i.e., dim=0) token in the last layer (i.e., after the fully connected layer)? i believe it should be of size [2, 1024, 1024]
from here it seems like it should be under last_hidden_state, but i can't seem to generate it. out.hidden_states seems to be a tuple of length 25, where each is of dimension [2, 1024, 1024]. i'm wondering if the last one is the one i'm looking for, but i'm not sure.","['machine-learning', 'pytorch', 'huggingface-transformers', 'language-model']",76661403,"you are right with output_hidden_state=true and watching out.hidden_states. this element is a tuple of length 25 as you mentioned. according to biogpt paper and huggingface doc, your model contains 24 transformer layers, and the 25 elements in the tuple are the first embedding layer output and the outputs of each of the 24 layers.
the shape of each of these tensors is [b, l, e] where b is your batch size, l is the length of the input and e is the dimension of your embedding. it seems that you are padding your input to 1024 regarding the shape you indicated. so, the representation of your first token (in the first batched sentence) would be out.hidden_states[k][0,0,:], which is of shape [1024]. here, k denotes the layer you want to use and it is up to you to decide which one you want depending on what you will do with it.",https://stackoverflow.com/questions/76655508,machine-learning,10-07-2023 16:06,737.0,0.0,1.0,True,11-07-2023 11:14,10-07-2023 16:15
49085673,download all nltk packages in google colaboratory at once,"i want to use stopwords in my code on google colab, there are no errors when i import stuff regarding nltk but when i use stopwords in my code google colab gives this error:-
resource 'corpora/stopwords.zip/stopwords/' not found.  please
use the nltk downloader to obtain the resource:  >>>
nltk.download()

but when i do:-
 import nltk
 nltk.download()

it gives me all the packages list so i have to select 1 to download, in terminal i could do ""all"" to download all packages but how an i do that in google colab? i don't want to add a name everytime to download stuff. this is what colab shows me when i do ""nltk.download()"":-
     nltk downloader
     d) download    l) list     u) update    c) config    h) help    q) quit
 downloader> d

 download which package (l=list; x=cancel)?

is there any way i can download all packages of nltk at once to my project in google colab?","['python', 'package', 'nltk', 'google-colaboratory']",51442969,"i reached this page when i faced same problem.
i can use ""popular"" with this code at google colab.
import nltk
nltk.download(""popular"")",https://stackoverflow.com/questions/49085673,python,03-03-2018 15:18,29780.0,11.0,4.0,True,24-10-2023 15:57,05-03-2018 12:31
68239361,cant install tensorflow for huggingface transformers library,"im trying to use huggingface transformers library in my python project. i am a first time python programmer, and i am stuck on this error message, even though tensorflow has been installed on my machine:
>>> from transformers import pipeline

none of pytorch, tensorflow >= 2.0, or flax have been found. models won't be available and only tokenizers, configuration and file/data utilities can be used.

i have discovered that tensorflow does not exist, even though i have installed it via pip. i have tried uninstalling it and reinstalling it, and but when i try to import the package, it just comes back as a modulenotfounderror
>>> import tensorflow

traceback (most recent call last):
  file ""<pyshell#2>"", line 1, in <module>
    import tensorflow
  file ""c:\users\######\appdata\local\packages\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\localcache\local-packages\python39\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
modulenotfounderror: no module named 'tensorflow.python

i have tried uninstalling and re-installing using pip and conda. i even tried installing pytorch using the same methods. it always says that the package was succesfully installed, and yet the error persists.
i am using python 3.9 and my os is windows 10. i dont know what i am doing wrong. but i know that a solution will definitely not be to uninstall and reinstall a package.
pip version (pip -v):
pip 21.1.3 from c:\users\######\appdata\local\programs\python\python39\lib\site-packages\pip (python 3.9)

python version (python -v):
python 3.9.5

python path list
i tried comparing the output of sys.path with the output of pip -v.
the closest path i saw for the pip -v path is down at the bottom, however i did not find the exact directory.
>>> import sys
>>> sys.path

['', 'c:\\windows\\system32', 'c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.9_3.9.1520.0_x64__qbz5n2kfra8p0', 'c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.9_3.9.1520.0_x64__qbz5n2kfra8p0\\python39.zip', 'c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.9_3.9.1520.0_x64__qbz5n2kfra8p0\\dlls', 'c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.9_3.9.1520.0_x64__qbz5n2kfra8p0\\lib', 'c:\\users\\######\\appdata\\local\\microsoft\\windowsapps\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0', 'c:\\users\\######\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages', 'c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.9_3.9.1520.0_x64__qbz5n2kfra8p0', 'c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.9_3.9.1520.0_x64__qbz5n2kfra8p0\\lib\\site-packages']

closest path:
c:\users\######\appdata\local\packages\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\localcache\local-packages\python39\site-packages","['python', 'tensorflow', 'pytorch', 'package', 'huggingface-transformers']",68379175,"from comments

you have multiple python interpreters installed, that is why
installing stuff does not show in your python interpreter, use pip -v and compare it to the python version that appears in the interpreter. remove one and use only one then your issue will be
resolved (paraphrased from dr.snoopy)",https://stackoverflow.com/questions/68239361,python,03-07-2021 19:26,2732.0,2.0,1.0,True,14-07-2021 13:37,04-07-2021 03:08
72463970,spacy: can&#39;t find model &#39;en_core_web_sm&#39;. it doesn&#39;t seem to be a python package or a valid path to a data directory,"i'm trying to load the en_core_web_sm spacy model, but i have been unsuccessful in doing so.
the error that occurs is the following:
oserror: [e050] can't find model 'en_core_web_sm'. it doesn't seem to be a python package or a valid path to a data directory.

i'm working in a anaconda virtual environment. the following checkboxes are ticked:

did conda activate gcp-env prior to installing spacy and the english language model
have run conda install -c conda-forge spacy while on the right environment
then, have run python -m spacy download en, still while on the right environment
also tried adding spacy to the requirements.txt , and installing dependencies via that route, after first attempts failed

spacy info produces this output:
spacy info

============================== info about spacy ==============================

spacy version    3.3.0                         
location         /users/simonmortensen/opt/anaconda3/envs/gcp-env/lib/python3.10/site-packages/spacy
platform         macos-11.6.5-x86_64-i386-64bit
python version   3.10.4                        
pipelines        en_core_web_sm (3.3.0)

python -m spacy validate produces this output:
================= installed pipeline packages (spacy v3.3.0) =================
ï¿½ï¿½ï¿½ spacy installation:
/users/simonmortensen/opt/anaconda3/envs/gcp-env/lib/python3.10/site-packages/spacy

name             spacy                 version                            
en_core_web_sm   >=3.3.0.dev0,<3.4.0   3.3.0   ï¿½ï¿½ï¿½

i've been through several previous stackoverflow posts on the same topic. those have often been solved, but my issue remains.
any advice would be very much appreciated. thanks in advance!
simon
edit:
for additional context, pip list on the environment conp>
spacy                         3.3.0
spacy-legacy                  3.0.9
spacy-loggers                 1.0.2

and
en-core-web-sm                3.3.0

even so, import en_core_web_sm also doesn't work:
import en_core_web_sm
traceback (most recent call last):

  input in [65] in <cell line: 1>
    import en_core_web_sm

modulenotfounderror: no module named 'en_core_web_sm'","['python', 'nlp', 'anaconda', 'conda', 'spacy']",72498112,"spyder was the villain.
all packages were correctly installed on the virtual environment, but spyder was not running that environment (even if the ide was launched with the spyder command from a terminal where the environment was in fact activated).
in order to make spyder run the correct environment, you needed to change the python interpreter in the spyder preferences:

... and then restart the kernel.
i got an error prompting me to pip install spyder-kernels==2.1.*, but once that was done (make sure to do it on the right venv), i restarted spyder, and it finally worked!
see discussions in thread:",https://stackoverflow.com/questions/72463970,python,01-06-2022 14:54,3441.0,2.0,3.0,True,10-04-2024 11:07,01-06-2022 19:46
75542728,find if one column of dataframe contains text from column of another dataframe and add a third column where there is a match,"i am working in an nlp problem statement in python. i have two dataframes -
df1 -




problem
region




i have wrong product
a


i have excess payment
a


address problem
b


i have delayed delivery
c




df2 -




key
category




wrong
accuracy


pay
pay related


delay
delay related




i need a final dataframe that checks if 'problem' contains 'key'. eg. ""pay"" exists in ""excess payment""
if yes then, 'category' is assigned. so the resultant dataframe will be -
df3 -




problem
region
category




i have wrong product
a
accuracy


i have excess payment
a
pay related


address problem
b



i have delayed delivery
c
delay related




have found solutions where index needs to match but that is not the case here. another vague solution is writing multiple str.contains statement but then that is not scalable. any leads how to solve this?","['python', 'pandas', 'string', 'numpy', 'nlp']",75542793,"you can use a merge on susbtring:
import re

# build the regex
# ensure longer strings are used first
# and escape potential special characters
pattern = '|'.join(map(re.escape, df2['key'].sort_values(key=lambda s: s.str.len(), ascending=false)))
# 'wrong|delay|pay'

# perform a left merge
out = df1.merge(df2,
                left_on=df1['problem'].str.lower()
                           .extract(fr'\b({pattern})',
                                    expand=false),
                right_on='key', how='left').drop(columns='key')

nb. here we used a left word boundary (\b) to only match the words that start with the substring (this way papaya wouldn't match pay). if this is unwanted and you prefer to match anywhere use f'({pattern})' in extract.
output:
                   problem region       category
0     i have wrong product      a       accuracy
1    i have excess payment      a    pay related
2          address problem      b            nan
3  i have delayed delivery      c  delay related",https://stackoverflow.com/questions/75542728,python,23-02-2023 09:15,183.0,1.0,1.0,True,23-02-2023 09:31,23-02-2023 09:22
77311518,spacy custom component function is never called,"i am adding a custom component to spacy but it never gets called:
@language.component(""custom_sentence_boundaries"")
def custom_sentence_boundaries(doc):
    print(""."")
    for token in doc[:-1]:
        if token.text == ""\n"":
            doc[token.i + 1].is_sent_start = true
    return doc

nlp = spacy.load(""de_core_web_sm"")
nlp.add_pipe(""custom_sentence_boundaries"", after=""parser"")
nlp.analyze_pipes(pretty=true)
doc = nlp(text)
sentences = [sent.text for sent in doc.sents]

i get a result in sentences and the analyzer does list my component but my custom component seams to have no effect and i never see the dots from the print appearing...
any ideas?","['python', 'python-3.x', 'spacy', 'spacy-3']",77311758,"in the code which you have pasted:
you are doing :
nlp = spacy.load(""de_core_web_sm"")

however, it should be :
nlp = spacy.load(""en_core_web_sm"")


i tried to reproduce your code and i got the result
@language.component(""custom_sentence_boundaries"")
def custom_sentence_boundaries(doc):
    print(""...$..."")                     # i am printing ""...$..."" so that it is visible easily 
    for token in doc[:-1]:
        if token.text == ""\n"":
            doc[token.i + 1].is_sent_start = true
    return doc

nlp = spacy.load(""en_core_web_sm"")
nlp.add_pipe(""custom_sentence_boundaries"", after=""parser"")
nlp.analyze_pipes(pretty=true)
text = (""when sebastian thrun started working on self-driving cars at ""
        ""google in 2007, few people outside of the company took him ""
        ""seriously. ï¿½ï¿½ï¿½i can tell yy senior ceos of major american ""
        ""car companies would shake my hand and turn away because i wasnï¿½ï¿½ï¿½t ""
        ""worth talking to,ï¿½ï¿½ï¿½ said thrun, in an interview with recode earlier ""
        ""this week."")
doc = nlp(text)
sentences = [sent.text for sent in doc.sents]


#output
(please see at the bottom ...$... is printed and custom_sentence_boundaries is printed after parser as we have stated after=""parser"" in keyword argument)
============================= pipeline overview =============================

#   component                    assigns               requires   scores             retokenizes
-   --------------------------   -------------------   --------   ----------------   -----------
0   tok2vec                      doc.tensor                                          false      
                                                                                    
1   tagger                       token.tag                        tag_acc            false      
                                                                                                
2   parser                       token.dep                        dep_uas            false      
                                 token.head                       dep_las                       
                                 token.is_sent_start              dep_las_per_type              
                                 doc.sents                        sents_p                       
                                                                  sents_r                       
                                                                  sents_f                       
                                                                                                
3   custom_sentence_boundaries                                                       false      
                                                                                                
4   attribute_ruler                                                                  false      
                                                                                                
5   lemmatizer                   token.lemma                      lemma_acc          false      
                                                                                                
6   ner                          doc.ents                         ents_f             false      
                                 token.ent_iob                    ents_p                        
                                 token.ent_type                   ents_r                        
                                                                  ents_per_type                 

ï¿½ï¿½ï¿½ no problems found.
...$...
<",https://stackoverflow.com/questions/77311518,python,17-10-2023 18:43,70.0,1.0,1.0,True,17-10-2023 19:36,17-10-2023 19:36
77087460,langchain - azure openai api - returning additional information than the asked question,"i am trying to develop a chatbot using streamlit,langchain azure openai api.
i have been successful in deploying the model and invoking an response but it is not what i expect.
here is the prompt and the code that to invoke the api
llm = azureopenai(model_name=""gpt-35-turbo"",
                  temperature=0,
                  openai_api_key=open_api_key,
                  openai_api_base=openai_api_base,
                  openai_api_type=openai_api_type,
                  openai_api_version=openai_api_version,
                  deployment_name=openai_deployment) 

embedding = openaiembeddings(openai_api_key=open_api_key,
                             deployment=open_ai_embed_model,
                             openai_api_base=openai_api_base,
                             openai_api_type=openai_api_type,
                             )
prompt_template = """"""use the following pieces of context to answer the question at the end. if you don't know the answer, just say that you don't know, don't try to make up an answer.
just answer the question asked. do not provide additional question and helpful answers
{context}

question: {question}
""""""
prompt = prompttemplate(
    template=prompt_template, input_variables=[""context"", ""question""]
)

code to invoke the chain and pass the prompt
chain_type_kwargs = {""prompt"": prompt}
qa_chain = retrievalqa.from_chain_type(
       llm,
       retriever=retriever,
       chain_type=""stuff"",
       chain_type_kwargs=chain_type_kwargs
)
result = qa_chain.run(user_input)

now if i ask it a question such as

what is the limit of breakfast

i get the correct response such as

a: 20$

but the issue is after that i get bunch of additional information in a question answer format which although is produced using the context but not requested such as

question: what is the limit for laundary? answer: 10$ question:
question: what is the guideline for selecting flights for domestic
flights? answer: helpful answer

it seems like model is trying to use maximum tokens allowed despite clear instruction in prompt to not provide any additional information.
how can i control this any inputs will be helpful?
p.s: it work properly if i use openai model rather than azure openai model","['azure', 'openai-api', 'langchain']",77088925,please try by replacing ï¿½ï¿½ï¿½azureopenaiï¿½ï¿½ï¿½ with ï¿½ï¿½ï¿½azurechatopenaiï¿½ï¿½ï¿½. i ran into this similar problem and making this change,https://stackoverflow.com/questions/77087460,azure,12-09-2023 08:49,3811.0,1.0,1.0,True,12-09-2023 12:03,12-09-2023 09:46
76744939,importerror: using the trainer with pytorch requires accelerate = 0.20.1,"please help me when i tried to use it in my google colab for transformers
error:

importerror: using the trainer with pytorch requires
accelerate=0.20.1: please run pip install transformers[torch] or pip
install accelerate -u`

note: if your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.
to view examples of installing some common dependencies, click the
""open examples"" button below.
transformers - gpt - pytorch","['python', 'pytorch', 'artificial-intelligence', 'google-colaboratory', 'huggingface-transformers']",76744946,"steps:
pip install accelerate -u
then
restart the notebook
run all
and you'll see it'll solve",https://stackoverflow.com/questions/76744939,python,22-07-2023 16:59,2032.0,3.0,2.0,True,20-09-2023 17:31,20-09-2023 17:02
72746948,subsampling formula skipgram nlp,"i'm studying how to implement a skip-gram model using pytorch, i follow this tutorial, in the subsampling part the author used this formula:
import random
import math

def subsample_prob(word, t=1e-3):
    z = freq[word_to_ix[word]] / sum_freq
    return (math.sqrt(z/t) + 1) * t/z

words_subsample = [w for w in words if random.random() < subsample_prob(w)]

where z variable is the proportion of counts of a certain word by the total of words in the corpus. my doubt is that depending on the proportion of words this formula gives a result greater than one, then the word is always added to the sub sample corpus, shouldn't it return a value between zero and one?","['nlp', 'word2vec', 'subsampling']",72747728,"the frequent-word downsampling option ('subsampling') introduced in the original word2vec (as a -sample argument) indeed applies downsampling only to a small subset of the very-most-frequent words. (and, given the 'tall head'/zipfian distributions of words in natural-language texts, that's plenty.)
typical values leave most words fully sampled, as reflected in this formula by a sampling-probability greater-than 1.0.
so: there's no error here. it's how the original word2vec implementation, and others, interpret the sample parameter. most words are exempt from any thinning, but some of the most-common words are heavily dropped. (but, there's still plenty of their varied usage examples in the training set ï¿½ï¿½ï¿½ and indeed spending fewer training updates redundantly on those words lets other words get better vectors, facing less contention/dilution from overtraining of common words.",https://stackoverflow.com/questions/72746948,nlp,24-06-2022 16:28,298.0,0.0,1.0,True,24-06-2022 18:53,24-06-2022 18:53
57994144,extract pos_tag_sents from pandas series,"following the advice from the thread how to apply pos_tag_sents() to pandas dataframe efficiently i run the code to identify different pos for the text in one of my variables. 
now that i managed to create the column of interest - sub['pos'] - how do i extract my relevant information - all the nn - and create a column for each of them?
print(sub['pos'])

5     [(e-mail, jj), (new, jj), (delhi, nn), ((, (),...
4     [(bangladesh, jj), (garment, nn), (unions, nns...
41    [(listen, vb), (blaze, nn), (wrecks, nns), (te...
10    [(11:49, cd), (am, vbp), (,, ,), (september, v...
17    [(listen, jj), (two, cd), (events, nns), (plan...

as an output, i would like a new column (here as 'nn'), that contains all the nn for each row. 
df = pd.dataframe([""delhi"", 
                   ""garment"" , 
                   ""blaze"", 
                   nan], columns=['nn'])","['python', 'pandas', 'nltk', 'part-of-speech']",57994696,"so i am assuming you have one column in the dataframe where each row is a list of tuples. please correct me if i am wrong. from that column you want to create new columns for each 'tag'. do you think following is what will achieve what you want to do?
import pandas as pd
import numpy as np

df = pd.dataframe({""line"":[[('e-mail', 'jj'), ('new', 'jj'), ('delhi', 'nn')]]})

def extract_pos(line,pos):
    return [word[0] for word in line if word[1] == pos]

df['nn'] = [extract_pos(line,'nn') for line in df['line']]
df['jj'] = [extract_pos(line,'jj') for line in df['line']]

this way you can add many column as you want and the result might look as some thing like following.

hope this helps,
cheers",https://stackoverflow.com/questions/57994144,python,18-09-2019 13:40,230.0,0.0,1.0,True,26-09-2021 03:55,26-09-2021 03:55
53669956,why i&#39;m unable to pass list to word_counts in textblob?,"as shown in the quickstart.
i have a list of words to be searched programmatically, but .word_counts['ekki'] (as shown in the quickstart), is giving me an error.
>>> import textblob
>>> str = textblob.textblob(""hello im programmer"")
>>> lis = [""hi"",""hello""]
>>> str.word_counts[i for i in lis]
  file ""<stdin>"", line 1
    str.word_counts[i for i in lis]
                        ^
syntaxerror: invalid syntax

code snippets or helpful links appreciated.","['python', 'python-3.x', 'nltk', 'textblob']",53670003,"you need to replace this;
str.word_counts[i for i in lis]

with this;
for i in lis:
    print(str.word_counts[i])

your original line doesn't make any sense since you're trying to use a list comprehension as an index to acces a list.",https://stackoverflow.com/questions/53669956,python,07-12-2018 12:52,186.0,0.0,1.0,True,25-05-2022 16:21,25-05-2022 16:21
77407632,how can i get llm to only respond in json strings?,"this is how i am defining the executor
const executor = await initializeagentexecutorwithoptions(tools, model, {
  agenttype: 'chat-conversational-react-description',
  verbose: false,
});

whenever i prompt the ai i have this statement at the end.
type someobject = {
  field1: number,
  field2: number,
}

- it is very critical that you answer only as the above object and json stringify it as a single string.
  don't include any other verbose explanatiouns and don't include the markdown syntax anywhere.

the someobject is just an example. usually it will have a proper object type.
when i use the executor to get a response from the ai, half the time i get the proper json string, but the other half the times are the ai completely ignoring my instructions and gives me a long verbose answer in just plain english...
how can i make sure i always get the structured data answer i want?
maybe using the agenttype: 'chat-conversational-react-description' isn't the right approach here?","['langchain', 'langchain-js']",77419154,"update nov. 6, 2023
openai announced today a new ï¿½ï¿½ï¿½json modeï¿½ï¿½ï¿½ at the devday keynote. when activated the model will only generate responses using the json format.
you can refer to the official d"" rel=""nofollow noreferrer"">here.
original answer
that's a great question and langchain provides an easy solution. look at langchain's output parsers if you want a quick answer. it is the recommended way to process llm output into a specified format.
here's the official link from the docs:

javascript: 
python: 


side note: i wrote an introductory tutorial about this particular issue but for python, so if anyone else is interested in more details you can check it out here.
the example below does not use initializeagentexecutorwithoptions, but will ensure that the output is processed as json without specifying this explicitly in your system prompt.
how it works
in order to tell langchain that we'll need to convert the llm response to a json output, we'll need to define a structuredoutputparser and pass it to our chain.
defining our parser:
here's an example:
// let's define our parser
const parser = structuredoutputparser.fromzodschema(
  z.object({
    field1: z.string().describe(""first field""),
    field2: z.string().describe(""second field"")
  })
);

adding it to our chain:
// we can then add it to our chain
const chain = runnablesequence.from([
  prompttemplate.fromtemplate(...),
  new openai({ temperature: 0 }),
  parser, // <-- this line
]);

invoking our chain with format_instructions:
// finally, we'll pass the format instructions to the invoke method
const response = await chain.invoke({
  question: ""what is the capital of france?"",
  format_instructions: parser.getformatinstructions(), // <-- this line
});

go ahead and log the parser.getformatinstructions() method before you call invoke if you'd like to see the output.
when we pass parser.getformatinstructions() to the format_instructions property, this lets langchain append the desired json schema that we defined in step 1 to our prompt before sending it to the large language model.
as a final point, it is absolutely critical to make sure your query/prompt is relevant and produces values that could be interpreted as the properties in your object someobject that are defined in the parser.
please give this a try, and let me know if you're able to consistently output json.",https://stackoverflow.com/questions/77407632,langchain,02-11-2023 07:26,49629.0,14.0,5.0,True,23-11-2024 14:49,16-06-2024 00:10
69169595,should feature embeddings be taken before or after dropout layer in neural network?,"i am training a binary text classification model using bert as follows:
def create_model():
   text_input = tf.keras.layers.input(shape=(), dtype=tf.string, name='text')
   preprocessed_text = bert_preprocess(text_input)
   outputs = bert_encoder(preprocessed_text)

   # neural network layers
   l1 = tf.keras.layers.dropout(0.1, name=""dropout"")(outputs['pooled_output'])
   l2 = tf.keras.layers.dense(1, activation='sigmoid', name=""output"")(l1)

   # use inputs and outputs to construct a final model
   model = tf.keras.model(inputs=[text_input], outputs=[l2])
   return model

this code is borrowed from the example on tfhub: 
i want to extract feature embeddings from the penultimate layer and use them for comparison, clustering, visualization, etc between examples. should this be done before dropout (l1 in the model above) or after dropout (l2 in the model above)?
i am trying to figure out whether this choice makes a significant difference, or is it fine either way? for example, if i extract feature embeddings after dropout and compute feature similarities between two examples, this might be affected by which nodes are randomly set to 0 (but perhaps this is okay).","['tensorflow', 'neural-network', 'embedding', 'bert-language-model', 'dropout']",69336770,"in order to answer your question let's recall how a dropout layer works:
the dropout layer is usually used as a means to mitigate overfitting. suppose two layers, a and b, are connected through a dropout layer. then during the training phase, neurons in layer a are being randomly dropped. that prevents layer b from becoming too dependent upon specific neurons in layer a, as these neurons are not always available. therefore, layer b has to take into consideration the overall signal coming from layer a, and (hopefully) cannot cling to some noise which is specific to the training set.
an important point to note is that the dropout mechanism is activated only during the training phase. while predicting, dropout does nothing.
if i understand you correctly, you want to know whether to take the features before or after the dropout (note that in your network l1 denotes the features after dropout has been applied). if so, i would take the features before dropout, because technically it does not really matter (dropout is inactive during prediction) and it is more reasonable to do so (dropout is not meaningful without a following layer).",https://stackoverflow.com/questions/69169595,tensorflow,13-09-2021 22:03,1067.0,-1.0,1.0,True,26-09-2021 16:20,14-09-2021 14:41
79145419,is it possible to get embeddings from nv-embed using candle?,"what i want to do is a cli program that outputs embeddings of an arbitrary input.
to do that, i want to do an inference with an embeddings model, and i chose nv-embed-v2. my framework of choice is candle, but i also looked at mistral-rs.
basically, what i'm trying to do is this code fragment:

but with rust and candle.
what i tried is to start off with mistral candle's example because the nv-embed's hf page says: model details / base decoder-only llm: mistral-7b-v0.1.
i replaced the model id in the original code with nvidia/nv-embed-v2, and was able to download the weights from hugging face, but upon loading the config, i got this:
error: missing field `vocab_size` at line 101 column 1

then i hardcoded the values from the json config loaded from hf to a newly created candle_transformers::models::mistral::config instance. and after that, mistral::new(&config, vb) fails with:
error: cannot find tensor model.embed_tokens.weight

is there a way around that ï¿½ï¿½ï¿½ maybe there are some other candle-based open source works that i could use as an inspiration? or, maybe that's a common mistake that could easily be diagnosed","['machine-learning', 'rust', 'nlp']",79156470,"candle looking for model.embed_tokens.weight whereas the original tensor name is embedding_model.embed_tokens.weight. you just have to change this line of mistral.rs in candle_transformers.
// from
let vb_m = vb.pp(""model"");
//to
let vb_m = vb.pp(""embedding_model"");",https://stackoverflow.com/questions/79145419,machine-learning,31-10-2024 15:55,336.0,0.0,1.0,True,04-11-2024 17:45,01-11-2024 08:59
78094644,invalid url when using chatgpt sdk (betalgo.openai),"i'm playing around with the chatgpt sdk (betalgo.openai) in c# and using one of their samples:
var openaiservice = new openaiservice(new openaioptions()
{
    apikey = ""abc"",
    organization = ""org-def""
});

var completionresult = await openaiservice.completions
  .createcompletion(new completioncreaterequest()
  {
    prompt = ""once upon a time"",
    maxtokens = 20,
    model = models.gpt_4
  }); 

i've change my apikey and organization here but they are correct in my code but i'm getting the following error:

this is a chat model and not supported in the v1/completions endpoint.
did you mean to use v1/chat/completions?

but i'm not specifying the url so clearly this is determined by the sdk.
i've also tried it with chatgpt 3.5 by setting the model to models.gpt_3_5_turbo but i'm getting the very same error.
any ideas how to circumvent this problem?
thanks.
update-1
as per @shaharshokrani recommendation, i changed from
var completionresult = await openaiservice.completion
to
var completionresult = await openaiservice.chatcompletion
but it also meant changing the provided request. here is the full code:
var openaiservice = new openaiservice(new openaioptions()
{
    apikey = ""abc"",
    organization = ""org-def""
});

var completionresult = await openaiservice.chatcompletion
    .createcompletion(new chatcompletioncreaterequest()
    {
       messages = new list<chatmessage>()
        {
            new chatmessage(""user"", ""once upon a time"")
        },
       maxtokens = 20,
       model = models.gpt_3_5_turbo
    });

as you can see, the completioncreaterequest has been changed to chatcompletioncreaterequest and prompt has been replaced by messages which is a list<chatmessages>.","['openai-api', 'chatgpt-api']",78094705,change it to var completionresult = await openaiservice.chatcompletion instead of var completionresult = await openaiservice.completion,https://stackoverflow.com/questions/78094644,openai-api,02-03-2024 23:30,152.0,0.0,1.0,True,04-03-2024 10:22,04-03-2024 10:22
68417735,"bind_tf_idf() error: in tapply(n, documents, sum) : arguments must have same length","i am trying to do bind_tf_idf() for the following df. my df has two documents/classes: y or n.
> test_2
# a tibble: 3,295 x 2
   class word    
   <fct> <chr>   
 1 y     nature
 2 y     great
 3 y     are     
 4 y     present 
 5 n     in      
 6 n     weather   
 7 y     moisture   
 8 n     humidity     
 9 y     and     
10 y     pollen
# ï¿½ï¿½ï¿½ with 3,285 more rows
warning message:
`...` is not empty.

we detected these problematic arguments:
* `needs_dots`

these dots only exist to allow future extensions and should be empty.
did you misspecify an argument?

this is what i am using:
test_2_tf_idf <- test_2 %>%
  bind_tf_idf(word, class, sum)

but i get the error message:
> test_2_tf_idf <- test_2 %>%
+   bind_tf_idf(word, class, sum)

'error in tapply(n, documents, sum) : arguments must have same length'

what i ultimately want in the end is a table of calculations analogousis (disregard ""total"" column):
#> # a tibble: 40,379 x 7
#>    book              word      n  total     tf   idf tf_idf
#>    <fct>             <chr> <int>  <int>  <dbl> <dbl>  <dbl>
#>  1 mansfield park    the    6206 160460 0.0387     0      0
#>  2 mansfield park    to     5475 160460 0.0341     0      0
#>  3 mansfield park    and    5438 160460 0.0339     0      0
#>  4 emma              to     5239 160996 0.0325     0      0
#>  5 emma              the    5201 160996 0.0323     0      0
#>  6 emma              and    4896 160996 0.0304     0      0
#>  7 mansfield park    of     4778 160460 0.0298     0      0
#>  8 pride & prejudice the    4331 122204 0.0354     0      0
#>  9 emma              of     4291 160996 0.0267     0      0
#> 10 pride & prejudice to     4162 122204 0.0341     0      0
#> # ï¿½ï¿½ï¿½ with 40,369 more rows

except in my case the ""book"" cis analogous to ""y"" or ""n"" class for each word.
what can i do to fix this tapply error?","['r', 'tf-idf', 'tapply', 'tidytext']",68418128,"the fourth argument of tidytext::bind_tf_idf is not a function but a

column containing document-term counts as string or symbol (?tidytext::bind_tf_idf)

hence you first have to aggregate your data by class and word using e.g. dplyr::count:
test_2 <- structure(list(class = c(
  ""y"", ""y"", ""y"", ""y"", ""n"", ""n"", ""y"", ""n"",
  ""y"", ""y""
), word = c(
  ""vesicles"", ""exosomes"", ""are"", ""present"",
  ""in"", ""blood"", ""urine"", ""and"", ""and"", ""proteins""
)), class = ""data.frame"", row.names = c(
  ""1"",
  ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10""
))

library(tidytext)
library(dplyr)

test_2_tf_idf <- test_2 %>%
  count(word, class) %>%
  bind_tf_idf(word, class, n)

test_2_tf_idf
#>        word class n        tf       idf     tf_idf
#> 1       and     n 1 0.3333333 0.0000000 0.00000000
#> 2       and     y 1 0.1428571 0.0000000 0.00000000
#> 3       are     y 1 0.1428571 0.6931472 0.09902103
#> 4     blood     n 1 0.3333333 0.6931472 0.23104906
#> 5  exosomes     y 1 0.1428571 0.6931472 0.09902103
#> 6        in     n 1 0.3333333 0.6931472 0.23104906
#> 7   present     y 1 0.1428571 0.6931472 0.09902103
#> 8  proteins     y 1 0.1428571 0.6931472 0.09902103
#> 9     urine     y 1 0.1428571 0.6931472 0.09902103
#> 10 vesicles     y 1 0.1428571 0.6931472 0.09902103",https://stackoverflow.com/questions/68417735,r,17-07-2021 05:40,569.0,1.0,1.0,True,25-07-2021 19:07,25-07-2021 19:07
76099140,hugging face transformers bart cuda error: cublas_status_not_initialize,"i'm trying to finetune the facebook bart model, i'm following this article in order to classify text using my own dataset.
and i'm using the trainer object in order to train:
training_args = trainingarguments(
    output_dir=model_directory,      # output directory
    num_train_epochs=1,              # total number of training epochs - 3
    per_device_train_batch_size=4,  # batch size per device during training - 16
    per_device_eval_batch_size=16,   # batch size for evaluation - 64
    warmup_steps=50,                # number of warmup steps for learning rate scheduler - 500
    weight_decay=0.01,               # strength of weight decay
    logging_dir=model_logs,          # directory for storing logs
    logging_steps=10,
)

model = bartforsequenceclassification.from_pretrained(""facebook/bart-large-mnli"") # bart-large-mnli

trainer = trainer(
    model=model,                          # the instantiated ï¿½ï¿½ï¿½ï¿½ transformers model to be trained
    args=training_args,                   # training arguments, defined above
    compute_metrics=new_compute_metrics,  # a function to compute the metrics
    train_dataset=train_dataset,          # training dataeval_dataset=val_dataset              # evaluation dataset
)

this is the tokenizer i used:
from transformers import barttokenizerfast
tokenizer = barttokenizerfast.from_pretrained('facebook/bart-large-mnli')

but when i use trainer.train() i get the following:
printing the following:
***** running training *****
  num examples = 172
  num epochs = 1
  instantaneous batch size per device = 4
  total train batch size (w. parallel, distributed & accumulation) = 16
  gradient accumulation steps = 1
  total optimization steps = 11

followed by this error:
runtimeerror: caught runtimeerror in replica 1 on device 1.
original traceback (most recent call last):
  file ""/databricks/python/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py"", line 61, in _worker
    output = module(*input, **kwargs)
  file ""/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  file ""/databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py"", line 1496, in forward
    outputs = self.model(
  file ""/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  file ""/databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py"", line 1222, in forward
    encoder_outputs = self.encoder(
  file ""/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  file ""/databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py"", line 846, in forward
    layer_outputs = encoder_layer(
  file ""/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  file ""/databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py"", line 323, in forward
    hidden_states, attn_weights, _ = self.self_attn(
  file ""/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  file ""/databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py"", line 191, in forward
    query_states = self.q_proj(hidden_states) * self.scaling
  file ""/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  file ""/databricks/python/lib/python3.9/site-packages/torch/nn/modules/linear.py"", line 114, in forward
    return f.linear(input, self.weight, self.bias)
runtimeerror: cuda error: cublas_status_not_initialized when calling `cublascreate(handle)`

i've searched this site and github and hugging face forum but still didn't find anything that helped me fix this for me (i tried adding more memory, lowering batches and warmup, restarting, specifying cpu or gpu, and more, but none worked for me)
databricks clusters:

runtime: 12.2 lts ml (includes apache spark 3.3.2, gpu, scala 2.12)
worker type: standard_nc24s_v3 with 4 gpus, 2 to 10 workers, i think 16gb ram and 448gb memory for the host
runtime: 12.1 ml (includes apache spark 3.3.1, scala 2.12)
worker type: standard_l8s (memory optimized), 2 to 10 workers, 64gb memory with 8 cores

update: with the second cluster, depending on the flag combination, i sometimes get the error indexerror: target {i} is out of bounds where i change from time to time

if you require any other information, comment and i'll add it up asap


my dataset is holding private information but here is an image of how it's built:


updates:
i also tried setting the:

fp16=true
gradient_checkpointing=true
gradient_accumulation_steps=4

flags but still had the same error when putting each separately and together
second cluster error (it get this error only sometimes, based on the flag combination):
indexerror                                traceback (most recent call last)
file <command-2692616476221798>:1
----> 1 trainer.train()

file /databricks/python/lib/python3.9/site-packages/transformers/trainer.py:1527, in trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1522     self.model_wrapped = self.model
   1524 inner_training_loop = find_executable_batch_size(
   1525     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1526 )
-> 1527 return inner_training_loop(
   1528     args=args,
   1529     resume_from_checkpoint=resume_from_checkpoint,
   1530     trial=trial,
   1531     ignore_keys_for_eval=ignore_keys_for_eval,
   1532 )

file /databricks/python/lib/python3.9/site-packages/transformers/trainer.py:1775, in trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1773         tr_loss_step = self.training_step(model, inputs)
   1774 else:
-> 1775     tr_loss_step = self.training_step(model, inputs)
   1777 if (
   1778     args.logging_nan_inf_filter
   1779     and not is_torch_tpu_available()
   1780     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
   1781 ):
   1782     # if loss is nan or inf simply add the average of previous logged losses
   1783     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)

file /databricks/python/lib/python3.9/site-packages/transformers/trainer.py:2523, in trainer.training_step(self, model, inputs)
   2520     return loss_mb.reduce_mean().detach().to(self.args.device)
   2522 with self.compute_loss_context_manager():
-> 2523     loss = self.compute_loss(model, inputs)
   2525 if self.args.n_gpu > 1:
   2526     loss = loss.mean()  # mean() to average on multi-gpu parallel training

file /databricks/python/lib/python3.9/site-packages/transformers/trainer.py:2555, in trainer.compute_loss(self, model, inputs, return_outputs)
   2553 else:
   2554     labels = none
-> 2555 outputs = model(**inputs)
   2556 # save past state if it exists
   2557 # todo: this needs to be fixed and made cleaner later.
   2558 if self.args.past_index >= 0:

file /databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py:1190, in module._call_impl(self, *input, **kwargs)
   1186 # if we don't have any hooks, we want to skip the rest of the logic in
   1187 # this function, and just call forward.
   1188 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1189         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1190     return forward_call(*input, **kwargs)
   1191 # do not call functions when jit is used
   1192 full_backward_hooks, non_full_backward_hooks = [], []

file /databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:1561, in bartforsequenceclassification.forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
   1559 elif self.config.problem_type == ""single_label_classification"":
   1560     loss_fct = crossentropyloss()
-> 1561     loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))
   1562 elif self.config.problem_type == ""multi_label_classification"":
   1563     loss_fct = bcewithlogitsloss()

file /databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py:1190, in module._call_impl(self, *input, **kwargs)
   1186 # if we don't have any hooks, we want to skip the rest of the logic in
   1187 # this function, and just call forward.
   1188 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1189         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1190     return forward_call(*input, **kwargs)
   1191 # do not call functions when jit is used
   1192 full_backward_hooks, non_full_backward_hooks = [], []

file /databricks/python/lib/python3.9/site-packages/torch/nn/modules/loss.py:1174, in crossentropyloss.forward(self, input, target)
   1173 def forward(self, input: tensor, target: tensor) -> tensor:
-> 1174     return f.cross_entropy(input, target, weight=self.weight,
   1175                            ignore_index=self.ignore_index, reduction=self.reduction,
   1176                            label_smoothing=self.label_smoothing)

file /databricks/python/lib/python3.9/site-packages/torch/nn/functional.py:3026, in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)
   3024 if size_average is not none or reduce is not none:
   3025     reduction = _reduction.legacy_get_string(size_average, reduce)
-> 3026 return torch._c._nn.cross_entropy_loss(input, target, weight, _reduction.get_enum(reduction), ignore_index, label_smoothing)

indexerror: target 11 is out of bounds.

the number 11 changes from time to time.","['python', 'pytorch', 'huggingface-transformers', 'text-classification', 'huggingface']",76163186,"i was able to reproduce your problem, here is how i solved it (on both of the clusters you provided).
in order to solve it i used (at the from_pretrained call):

ignore_mismatched_sizes=true: because the model you use has fewer labels than what you have
num_labels={}: insert the number of your labels, i used 16 just to be sure


it's based on both your errors but mainly the second one, i suspect it was also the source of the second error on the gpu, please test and confirm it

i also used the following (at the trainingarguments call for memory optimizations):

fp16=true
gradient_checkpointing=true

i tested it with up until num_train_epochs=12, per_device_train_batch_size=16, per_device_eval_batch_size=64, warmup_steps=500
and it worked just fine, hopefully, it will help you get the desired results.

you can look at the 2 final links i provided for more details about how to optimize the speed and memory while training on both gpu and cpu

for reference:

huggingface models (search for ignore_mismatched_sizes)
huggingface configuration (search for num_labels)
fine-tuning with custom datasets
efficient training on a single gpu
efficient training on cpu",https://stackoverflow.com/questions/76099140,python,25-04-2023 08:36,2062.0,3.0,1.0,True,03-05-2023 11:23,03-05-2023 10:08
72392438,"in r, how to find the location of a word in a string?","how can i find the first location of specific words in a dataframe cell, and save the output in a new column in the same dataframe?
ideally i want the first match for each of the words in dictionary.
df <- data.frame(text = c(""omg coke is so awsme"",""i always preferred pepsi"", ""mozart is so overrated by yeah fanta makes my day, always""))

dict <- c(""coke"", ""pepsi"", ""fanta"")

location can be n of characters or words preceding the dictionary word.
i've been playing around with the code found here, but i can't make it work.
for example, this code does the job, but only for one word, and for one string (rather than a df and dictionary)
my_string = ""omg coke is so awsme""
unlist(gregexpr(""coke"", my_string))[1]

desired output:
                                                       text  location
1                                      omg coke is so awsme         2
2                                  i always preferred pepsi         4
3 mozart is so overrated by yeah fanta makes my day, always         7

like i said, the location can also be string rather than word, if that is easier.","['r', 'text', 'nlp', 'text-mining', 'quanteda']",72392613,"here's a simple for loop:
for(i in dict) {
  df[[i]] = stringi::stri_locate_first_fixed(df$text, i)[, 1]
}
df
#                                                        text coke pepsi fanta
# 1                                      omg coke is so awsme    5    na    na
# 2                                  i always preferred pepsi   na    20    na
# 3 mozart is so overrated by yeah fanta makes my day, always   na    na    32

or with regexpr (part of base, so no dependencies):
for(i in dict) {
  df[[i]] = regexpr(i, df$text, fixed = true)
}
df
#                                                        text coke pepsi fanta
# 1                                      omg coke is so awsme    5    -1    -1
# 2                                  i always preferred pepsi   -1    20    -1
# 3 mozart is so overrated by yeah fanta makes my day, always   -1    -1    32

and here's a solution for word number, though i would recommend deleting all the punctuation before using this:
df$words = strsplit(df$text, split = "" "")
for(i in dict) {
  df[[i]] = sapply(df$words, \(x) match(i, unlist(x)))
}
df
#                                                        text coke pepsi fanta
# 1                                      omg coke is so awsme    2    na    na
# 2                                  i always preferred pepsi   na     4    na
# 3 mozart is so overrated by yeah fanta makes my day, always   na    na     7
#                                                                 words
# 1                                            omg, coke, is, so, awsme
# 2                                         i, always, preferred, pepsi
# 3 mozart, is, so, overrated, by, yeah, fanta, makes, my, day,, always",https://stackoverflow.com/questions/72392438,r,26-05-2022 13:14,694.0,1.0,3.0,True,27-05-2022 04:02,26-05-2022 13:31
61631446,attributeerror: &#39;list&#39; object has no attribute &#39;lower&#39; with countvectorizer,"i am trying to make a prediction on a pandas dataframe in python. somehow the countvectorizer can't convert the data. does anyone know what's causing the problem?
this is my code:
filename = 'final_model.sav'
print(response.status_code)
data = response.json()
print(data)

dictionary = pd.read_json('rating_company_small.json', lines=true)

dictionary_df = pd.dataframe()
dictionary_df[""comment text""] = dictionary[""comment""]

data = pd.dataframe.from_dict(json_normalize(data), orient='columns')
print(data)

df = pd.dataframe()

df[""comment text""] = data[""text""]
df[""status""] = data[""status""]

print(df)
processing.dataframe_cleaning(df)

comment_data = df['comment text']

tfidf = countvectorizer()
tfidf.fit(dictionary_df[""comment text""])
test_x_tfidf = tfidf.transform(df[""comment text""])


print(comment_data)
print(test_x_tfidf)
loaded_model = pickle.load(open(filename, 'rb'))
predictions_nb = loaded_model.predict(test_x_tfidf)


this is the dataframe:
                         comment text    status
0                   [slecht, bedrijf]    string
1  [leuk, bedrijfje, goed, behandeld]  approved
2  [leuk, bedrijfje, goed, behandeld]  approved
3                   [leuk, bedrijfje]  approved 

full error message:
traceback (most recent call last):
  file ""request.py"", line 36, in <module>
    test_x_tfidf = tfidf.transform(df[""comment text""])
  file ""c:\users\junio\anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"", line 1112, in transform
    _, x = self._count_vocab(raw_documents, fixed_vocab=true)
  file ""c:\users\junio\anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"", line 970, in _count_vocab
    for feature in analyze(doc):
  file ""c:\users\junio\anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"", line 352, in <lambda>
    tokenize(preprocess(self.decode(doc))), stop_words)
  file ""c:\users\junio\anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"", line 256, in <lambda>
    return lambda x: strip_accents(x.lower())
attributeerror: 'list' object has no attribute 'lower'

i'm expecting it to return the predictions on the dataframe.","['python', 'pandas', 'machine-learning', 'nlp']",61631664,"countvectorizer cannot directly handle a series of lists, which is why you're getting that error (lower is a string method).
i looks like you want a multilabelbinarizer instead, which can handle this input structure:
from sklearn.preprocessing import multilabelbinarizer

count_vec = multilabelbinarizer()
mlb = count_vec.fit(df[""comment text""])
pd.dataframe(mlb.transform(df[""comment text""]), columns=[mlb.classes_])

  bedrijf bedrijfje behandeld goed leuk slecht
0       1         0         0    0    0      1
1       0         1         1    1    1      0
2       0         1         1    1    1      0
3       0         1         0    0    1      0


however the above approach won't account for duplicate elements in the lists, the output elements can either be 0 or 1. if that is the behavior you're expecting instead, you could join the lists into strings and then use a countvectorizer, since it is expecting strings:
text = df[""comment text""].map(' '.join)
count_vec = countvectorizer()
cv = count_vec.fit(text)

pd.dataframe(cv.transform(text).toarray(), columns=[mlb.classes_])

  bedrijf bedrijfje behandeld goed leuk slecht
0       1         0         0    0    0      1
1       0         1         1    1    1      0
2       0         1         1    1    1      0
3       0         1         0    0    1      0

note that this is not the same as a tf-idf of the input strings. here you just have the actual counts. for that you have tfidfvectorizer, which for the same example would produce:
    bedrijf bedrijfje behandeld      goed      leuk    slecht
0  0.707107  0.000000  0.000000  0.000000  0.000000  0.707107
1  0.000000  0.444931  0.549578  0.549578  0.444931  0.000000
2  0.000000  0.444931  0.549578  0.549578  0.444931  0.000000
3  0.000000  0.707107  0.000000  0.000000  0.707107  0.000000",https://stackoverflow.com/questions/61631446,python,06-05-2020 09:11,4951.0,2.0,2.0,True,15-08-2022 21:15,06-05-2020 09:17
78094346,how can i find out the location of the endpoint when using openai python library and azure openai?,"e.g., when i make a basic azure openai request, i don't see the endpoint in the response object:
#note: this code sample requires openai python library version 1.0.0 or higher.
import json
import pprint
from openai import azureopenai

client = azureopenai(
  azure_endpoint = ""
  api_key='xxxxxxxxxxxxxxxxxxxxx',
  api_version=""2023-07-01-preview""
)

message_text = [{""role"":""system"",""content"":""you are an ai assistant that helps people find information.""}]
completion = client.chat.completions.create(
  model=""gpt-4xxxxxxxx"", 
  messages = message_text,
  temperature=0.7,
  max_tokens=800,
  top_p=0.95,
  frequency_penalty=0,
  presence_penalty=0,
  stop=none
)

print('completion:\n')
pprint.pprint(completion)

# convert python object to json
json_data = json.dumps(completion, default=lambda o: o.__dict__, indent=4)

# print json
print(json_data)

looking at  the output, the response object completion contains:
chatcompletion(id='chatcmpl-xxxxxxxxx', choices=[choice(finish_reason='stop', index=0, logprobs=none, message=chatcompletionmessage(content='great! how can i assist you today?', role='assistant', function_call=none, tool_calls=none), content_filter_results={'hate': {'filtered': false, 'severity': 'safe'}, 'self_harm': {'filtered': false, 'severity': 'safe'}, 'sexual': {'filtered': false, 'severity': 'safe'}, 'violence': {'filtered': false, 'severity': 'safe'}})], created=1709313222, model='gpt-4', object='chat.completion', system_fingerprint='fp_xxxxx', usage=completionusage(completion_tokens=9, prompt_tokens=18, total_tokens=27), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': false, 'severity': 'safe'}, 'self_harm': {'filtered': false, 'severity': 'safe'}, 'sexual': {'filtered': false, 'severity': 'safe'}, 'violence': {'filtered': false, 'severity': 'safe'}}}])

how can i find out the location of the endpoint when using openai python library and azure openai?

i know that one may view the location on 

but i don't have access to all azure openai instances that i work with in my account.","['python', 'openai-api', 'azure-openai', 'gpt-4']",78095051,"as mentioned in the comments, azureopenai is not the right sdk/library to get the information about the geo-location of your azure openai service instance. it is essentially a wrapper over openai rest api.
the functionality you are looking for is a control-plane activity and you will need to use azure resource manager rest api for that. the correct sdk/library for that would be azure-ai-resources. you can learn more about it here:",https://stackoverflow.com/questions/78094346,python,02-03-2024 21:20,419.0,-1.0,1.0,True,03-03-2024 03:46,02-03-2024 23:19
75884448,"langchain: logprobs, best_of and echo parameters are not available on gpt-35-turbo model","i am trying to use langchain with gpt-35-turbo.
it seems that the new gpt3.5 turbo is not using certain parameters anymore as per the link learn how to work with the chatgpt and gpt-4 models (preview)

the following parameters aren't available with the new chatgpt and gpt-4 models: logprobs, best_of, and echo. if you set any of these parameters, you'll get an error.

now each time i initialise an llm model through langchain with gpt-3.5-turbo, it gives me this error:
invalidrequesterror: logprobs, best_of and echo parameters are not available on gpt-35-turbo model. please remove the parameter and try again. for more details, see 

i don't know how to 'unset' these parameters in langchain.
this is my code:
from langchain.chains.llm import llmchain
from langchain.llms.openai import openai
from langchain.prompts.prompt import prompttemplate

llm = openai(temperature=0, engine=deployment_name)

template = """"""
you are a helpful assistant that translates english to french. translate this sentence from english to french: {text}
""""""
prompt = prompttemplate(input_variables=[""text""], template=template)
llm_chain = llmchain(llm=llm, prompt=prompt)
response = llm_chain.generate([
        {""text"": ""i love ai""},
        {""text"": ""i love the ocean""},
    ])

for g in response.generations:
    print(g[0].text)

note that i am using openai in azure,
i also tried this code and it's still giving me the same error
deployment_name = ""my-deployment-name""
from langchain.llms import azureopenai
llm = azureopenai(deployment_name=deployment_name )
print(llm)
llm(""tell me a joke"")","['openai-api', 'langchain']",75915673,"so, i finally was able to fix it by creating an extension of azureopenai class and nullifying those arguments. code that works is below:
from langchain.llms import azureopenai
from typing import list
class newazureopenai(azureopenai):
    stop: list[str] = none
    @property
    def _invocation_params(self):
        params = super()._invocation_params
        # fix invalidrequesterror: logprobs, best_of and echo parameters are not available on gpt-35-turbo model.
        params.pop('logprobs', none)
        params.pop('best_of', none)
        params.pop('echo', none)
        #params['stop'] = self.stop
        return params
    
llm = newazureopenai(deployment_name=deployment_name,temperature=0.9)
llm(""tell me a joke"")

the answer was actually found in this link and it worked for me.",https://stackoverflow.com/questions/75884448,openai-api,30-03-2023 05:35,6341.0,6.0,2.0,True,03-04-2023 02:33,31-03-2023 02:32
76421921,using gpt 4 or gpt 3.5 with sql database agent throws outputparserexception: could not parse llm output:,"i am using the sql database agent to query a postgres database. i want to use gpt 4 or gpt 3.5 models in the openai llm passed to the agent, but it says i must use chatopenai. using chatopenai throws parsing errors.
the reason for wanting to switch models is reduced cost, better performance and most importantly - token limit. the max token size is 4k for 'text-davinci-003' and i need at least double that.
here is my code
from langchain.agents.agent_toolkits import sqldatabasetoolkit
from langchain.sql_database import sqldatabase
from langchain.agents import create_sql_agent
from langchain.llms import openai
from langchain.chat_models import chatopenai
import os

os.environ[""openai_api_key""] = """"
db = sqldatabase.from_uri(
    ""postgresql://<my-db-uri>"",
    engine_args={
        ""connect_args"": {""sslmode"": ""require""},
    },
)

llm = chatopenai(model_name=""gpt-3.5-turbo"")
toolkit = sqldatabasetoolkit(db=db, llm=llm)

agent_executor = create_sql_agent(
    llm=llm,
    toolkit=toolkit,
    verbose=true,
)

agent_executor.run(""list the tables in the db. give the answer in a table json format."")

when i do, it throws an error in the chain midway saying
> entering new agentexecutor chain...
traceback (most recent call last):
  file ""/home/ramlah/documents/projects/langchain-test/sql.py"", line 96, in <module>
    agent_executor.run(""list the tables in the db. give the answer in a table json format."")
  file ""/home/ramlah/documents/projects/langchain/langchain/chains/base.py"", line 236, in run
    return self(args[0], callbacks=callbacks)[self.output_keys[0]]
  file ""/home/ramlah/documents/projects/langchain/langchain/chains/base.py"", line 140, in __call__
    raise e
  file ""/home/ramlah/documents/projects/langchain/langchain/chains/base.py"", line 134, in __call__
    self._call(inputs, run_manager=run_manager)
  file ""/home/ramlah/documents/projects/langchain/langchain/agents/agent.py"", line 953, in _call
    next_step_output = self._take_next_step(
  file ""/home/ramlah/documents/projects/langchain/langchain/agents/agent.py"", line 773, in _take_next_step
    raise e
  file ""/home/ramlah/documents/projects/langchain/langchain/agents/agent.py"", line 762, in _take_next_step
    output = self.agent.plan(
  file ""/home/ramlah/documents/projects/langchain/langchain/agents/agent.py"", line 444, in plan
    return self.output_parser.parse(full_output)
  file ""/home/ramlah/documents/projects/langchain/langchain/agents/mrkl/output_parser.py"", line 51, in parse
    raise outputparserexception(
langchain.schema.outputparserexception: could not parse llm output: `action: list_tables_sql_db, ''`

please help. thanks!
update
the recent updates to langchain version 0.0.215 seem to have fixed this issue, for me at least.","['openai-api', 'gpt-3', 'langchain', 'gpt-4']",78176065,"the previously accepted answer never seemed to work for me. however, as per a github thread on the same issue, the way to handle this is like so.
agent_executor = create_sql_agent(llm,
                                   db=db,
                                   agent_type=""openai-tools"",
                                   agent_executor_kwargs={'handle_parsing_errors':true},
                               verbose=true)",https://stackoverflow.com/questions/76421921,openai-api,07-06-2023 09:40,3931.0,5.0,2.0,True,17-03-2024 16:36,26-06-2023 07:14
77797163,how to fix the warning from pydantic in langchain: the `dict` method is deprecated; use `model_dump` instead,"when i run the rag chain code with openai from langchain it gives me warning like this:
pydanticdeprecatedsince20: the `dict` method is deprecated; use `model_dump` instead. deprecated in pydantic v2.0 to be removed in v3.0. see pydantic v2 migration guide at 
  warnings.warn('the `dict` method is deprecated; use `model_dump` instead.', deprecationwarning)

i have not place to replace dict with model_dump and i even have not encode it anywhere in my code. any idea how to solve this warning?
here is my code:
from client_setup import get_client
#from langchain_community.vectorstores import weaviate
#from langchain_openai.openai import openai
from langchain_openai import openai
from langchain_community.vectorstores import weaviate
from langchain.retrievers.weaviate_hybrid_search import weaviatehybridsearchretriever
from langchain.prompts import chatprompttemplate
from langchain.schema.runnable import runnablepassthrough
from langchain.schema.output_parser import stroutputparser


client = get_client()

retriever = weaviatehybridsearchretriever(
        client=client, 
        index_name=""material"",
        text_key=""su"",
        attributes=[""material"", ""heat_treatment"", ""su"", ""sy""],
        create_schema_if_missing=true,
        )

llm = openai(api_key="""", model_name=""gpt-3.5-turbo-instruct"")


template = """"""you are an assistant for question-answering tasks. 
use the following pieces of retrieved context to answer the question. 
if you don't know the answer, just say that you don't know. 
use three sentences maximum and keep the answer concise.
question: {question} 
context: {context} 
answer:
""""""
prompt = chatprompttemplate.from_template(template)

#print(prompt)
query = ""what heat treatment is used for steel sae 1040?""

rag_chain = (
        {""context"": retriever,
         ""question"": runnablepassthrough()}
        | prompt
        | llm
        | stroutputparser()
        )


result = rag_chain.invoke(query)
print(result)","['python', 'langchain', 'pydantic-v2']",77807242,"i have the same issue. it's a problem with the langchain implementation itself. if you are using langchain 0.1.0 as i do, then it seems that the pydantic package it usess is 2.5.2. but then some langchain code (in my case langchain_community/chat_models/openai.py:456) uses a deprecated method from an earlier version of pydantic.
the developers should be aware of this and if they ever need to upgrade pydantic to 3.0, they should migrate their code accordingly, for us.
so for now we are good, unless at some point your code depends on pydantic 3 but langchain does not and there will be a conflict...",https://stackoverflow.com/questions/77797163,python,11-01-2024 01:08,2243.0,-3.0,1.0,True,12-01-2024 14:25,11-01-2024 03:12
72957008,no module named &#39;google trans&#39;,"i've got warning while installing the pip googletrans.
the warning says in the picture:

does it affect my code? the error says ""no module named 'google trans'?
error-screenshot:

from tkinter import *
import googletrans
import textblob
from tkinter import ttk, messagebox

root = tk ()
root.title('codemy.com - translator')
root.geometry(""880x300"")

def translate_it():
    pass

# text boxes
original_text = text(root, height=10, width=40)
original_text.grid(row=0, column=0, pady=20, padx=10)

translate_button = button(root, text=""translate!"", font=(""helvetica"", 24), command=translate_it)
translate_button.grid(row=0, column=1, padx=10)

translated_text = text(root, height=10, width=40)
translated_text.grid(row=0, column=2, pady=20, padx=10)

root.mainloop()","['python', 'nlp', 'importerror']",72957091,"check if the directory of the googletrans module and make sure it's within the list.
you can check the directories that python reads with:
import sys
sys.path

you can add the directory of googletrans with:
sys.path.append('/users/name/documents')",https://stackoverflow.com/questions/72957008,python,12-07-2022 18:43,603.0,1.0,1.0,True,13-07-2022 03:25,13-07-2022 03:25
78470683,"valueerror: arguments `target` and `output` must have the same shape. received: target.shape=(none, 512), output.shape=(none, 3)","i was trying to train a bert model to solve a multi-classification problem.
i got this error while run the code below:
arguments `target` and `output` must have the same shape. received: target.shape=(none, 512), output.shape=(none, 3)

import tensorflow as tf

epochs = 4

train_dataloader = train_dataset.shuffle(buffer_size=10000).batch(batch_size)
validation_dataloader = val_dataset.batch(batch_size)

# start training 
history = model.fit(
    train_dataloader,  # train_data
    validation_data=validation_dataloader,  # validation_data
    epochs=epochs,  
    verbose=1 
)
# save the model
model.save(""bert_model.h5"")

this is a test:
for batch in train_dataloader.take(1):
    input_ids, attention_masks, labels = batch
    print(""batch input_ids shape:"", input_ids.shape) 
    print(""batch attention_masks shape:"", attention_masks.shape) 
    print(""batch labels shape:"", labels.shape)  

# i got this output
batch input_ids shape: (16, 512)
batch attention_masks shape: (16, 512)
batch labels shape: (16,)

i already checked the tensor shape.","['python', 'tensorflow', 'artificial-intelligence', 'classification', 'bert-language-model']",78471329,"your labels have a shape of (16,), while your model's output has a shape of (none,3).
probably the issue is that your labels are not one-hot encoded. they should have the same second dimension as your output layer:
from tensorflow.keras.utils import to_categorical
num_classes = 3
labels = to_categorical(labels, num_classes=num_classes)
print(labels.shape)",https://stackoverflow.com/questions/78470683,python,13-05-2024 07:32,8094.0,2.0,1.0,True,13-05-2024 21:23,13-05-2024 21:23
53541327,error while embedding: could not convert string to float: &#39;ng&#39;,"i am working on pre trained word vectors using glove method. data contains vectors on wikipedia data. while embedding data i am getting error stating that could not convert string to float: 'ng'
i tried going through data but there i was not able to find symbol 'ng'
# load embedding as a dict
def load_embedding(filename):
    # load embedding into memory, skip first line
    file = open(filename,'r', errors = 'ignore')
    # create a map of words to vectors
    embedding = dict()
    for line in file:
        parts = line.split()
        # key is string word, value is numpy array for vector
        embedding[parts[0]] = np.array(parts[1:], dtype='float32')
    file.close()
    return embedding

here is the error report. please guide me further.
runfile('c:/users/akshay/desktop/nlp/pre-trained glove.py', wdir='c:/users/akshay/desktop/nlp')
c:\users\akshay\appdata\local\conda\conda\envs\py355\lib\site-packages\h5py\__init__.py:36: futurewarning: conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. in future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
using tensorflow backend.
traceback (most recent call last):

  file ""<ipython-input-1-d91aa5ebf9f8>"", line 1, in <module>
    runfile('c:/users/akshay/desktop/nlp/pre-trained glove.py', wdir='c:/users/akshay/desktop/nlp')

  file ""c:\users\akshay\appdata\local\conda\conda\envs\py355\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 705, in runfile
    execfile(filename, namespace)

  file ""c:\users\akshay\appdata\local\conda\conda\envs\py355\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  file ""c:/users/akshay/desktop/nlp/pre-trained glove.py"", line 123, in <module>
    raw_embedding = load_embedding('glove.6b.50d.txt')

  file ""c:/users/akshay/desktop/nlp/pre-trained glove.py"", line 67, in load_embedding
    embedding[parts[0]] = np.array(parts[1:], dtype='float32')

valueerror: could not convert string to float: 'ng'","['python', 'machine-learning', 'nlp', 'glove']",54633735,"valueerror: could not convert string to float: 'ng' 
for addressing the problem above, add encoding='utf8' to the function as follows:
file = open(filename,'r', errors = 'ignore', encoding='utf8')",https://stackoverflow.com/questions/53541327,python,29-11-2018 14:32,4885.0,2.0,4.0,True,30-03-2023 12:44,29-11-2018 16:11
62400205,what is the correct way of using rasa&#39;s api (rasa.core.processor - encountered an exception)?,"i installed the rasa-demo code sample. for turning on the rasa api, i did:
user@user:~/rasa-demo ï¿½ï¿½ï¿½master*ï¿½ï¿½ï¿½$ rasa run
no chat connector configured, falling back to the rest input channel. to connect your bot to another channel, read the docs here: 
2020-06-19 13:20:02 info     root  - starting rasa server on 
2020-06-19 13:20:08 info     absl  - using /var/folders/h5/9rj1zn8x4s59bk_mg_ktzv740000gn/t/tfhub_modules to cache modules.
2020-06-19 13:20:08 info     absl  - downloading tf-hub module '
2020-06-19 13:20:26 info     absl  - downloading  22.35mb
2020-06-19 13:20:43 info     absl  - downloading  42.35mb
2020-06-19 13:21:02 info     absl  - downloading  82.35mb
2020-06-19 13:21:21 info    nloading  118.59mb
2020-06-19 13:21:40 info     absl  - downloading  148.59mb
2020-06-19 13:21:41 info     absl  - downloaded  total size: 152.02mb
2020-06-19 13:21:41 info     absl  - downloaded tf-hub module '
2020-06-19 13:22:14 info     root  - rasa server is up and running.

how can i query as an api the chatbot? i would like be able to make a request and do the conversation through requests, instead of using the shell. so far, when i tried to make a curl to the rasa server:
in:
user@user:~ $ curl -xpost localhost:5005/webhooks/rest/webhook -d '{""message"":""hi""}'

out:
[]%

on the rasa run server, i get this response:
2020-06-19 13:23:17 error    rasa.core.actions.action  - failed to run custom action 'action_greet_user'. couldn't connect to the server at ' is the server running? error: cannot connect to host localhost:5055 ssl:default [connection refused]
2020-06-19 13:23:17 error    rasa.core.processor  - encountered an exception while running action 'action_greet_user'. bot will continue, but the actions events are lost. please check the logs of your action server for more information.
2020-06-19 13:24:04 error    rasa.core.actions.action  - failed to run custom action 'action_greet_user'. couldn't connect to the server at ' is the server running? error: cannot connect to host localhost:5055 ssl:default [connection refused]
2020-06-19 13:24:04 error    rasa.core.processor  - encountered an exception while running action 'action_greet_user'. bot will continue, but the actions events are lost. please check the logs of your action server for more information.

it is not working. what is the correct way to request rasa server as an api? after reading the docs, it is not clear to me how to make correct usage of the api.
i also tried this:
in: 
import requests

response = requests.get(' 
print(response) 
print(response.headers) 
print(response.content)

out:
<response [405]>
{'connection': 'keep-alive', 'keep-alive': '5', 'allow': 'post', 'access-control-allow-credentials': 'true', 'content-length': '60', 'content-type': 'text/plain; charset=utf-8'}
b'error: method get not allowed for url /webhooks/rest/webhook'","['python', 'nlp', 'python-requests', 'rasa-core', 'rasa']",62538198,"i end up figuring out that this is how you request the end point:
localhost:5005/model/parse -s -d '{ ""text"": ""hi"" }'

the documentation should be clearer.",https://stackoverflow.com/questions/62400205,python,16-06-2020 02:53,9493.0,2.0,5.0,True,11-04-2023 12:51,19-06-2020 19:30
75003737,trying to insert an input function using python in openai,"import os
import openai
openai.api_key = 'api_key'
question = ""\\n\\nq: input(""enter question"")?\\na:"",
response = openai.completion.create(
model=""text-davinci-003"",
prompt= question,
temperature=0,
max_tokens=100,
top_p=1,
frequency_penalty=0.0,
presence_penalty=0.0,
stop=\[""\\n""\]
)
print( str(response\['choices'\]\[0\]\['text'\]))`

error
file ""c:\\users\\yus\\pycharmprojects\\pythonproject\\test.py"", line 6
question = ""\\n\\nq: ""input_question""  \\na:""
^
syntaxerror: invalid syntax
process finished with exit code 1

expecting an input function to ask to enter a question and then print the answer using openai.","['python', 'openai-api']",75023379,"this is how is done
 def main():
      form = cgi.fieldstorage()
      in = form.getvalue(""query"")
      qt = f""q: {in}  ? a:""
      response = openai.completion.create(
        model=""text-davinci-003"",
        prompt=qt,
        temperature=0,
        max_tokens=64,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0,
        stop=[""\n\n""]
      )
      print(str(response['choices'][0]['text']))
   
  if __name__ == ""__main__"":
      main()",https://stackoverflow.com/questions/75003737,python,04-01-2023 09:56,462.0,0.0,1.0,True,05-01-2023 19:20,04-01-2023 14:41
76491056,i get  error in c# openai library,"i am using the openai library in my c# project, but i get the following error if it does not receive a response for more than 100 seconds. i cannot add a custom  element. how can i solve this problem. thanks in advance.
ï¿½ï¿½ï¿½system threading tasks.taskcanceledexception: the request was
canceled due to the configured  of 100 seconds
elapsing,ï¿½ï¿½ï¿½
the library i u"" rel=""nofollow noreferrer"">
my code:
   openaiapi api = new openaiapi(apikey);
                var result = await api.chat.createchatcompletionasync(new chatrequest()
                {
                    model = model.chatgptturbo,
                    temperature = 0.5,
                    messages = new chatmessage[]
                    {
            new chatmessage(chatmessagerole.system, """"),
            new chatmessage(chatmessagerole.user, prompt)
                    }
                });","['c#', 'timeout', 'httpclient', 'openai-api', 'chatgpt-api']",76496823,"solution for those who have other problems:
using system.net.

public class custom : i
{
    public  createclient(string name)
    {
        var  = new 
         = timespan.fromseconds(200);

        return 
    }
}


openaiapi api = new openaiapi(apikey);  
api. = new custom
var result = await api.chat.createchatcompletionasync(new chatrequest()
{
    model = model.chatgptturbo,
    temperature = 0.5,
    messages = new chatmessage[]
{
    new chatmessage(chatmessagerole.system, """"),
    new chatmessage(chatmessagerole.user, prompt)
}
});",https://stackoverflow.com/questions/76491056,c#,16-06-2023 14:16,1281.0,1.0,1.0,True,17-06-2023 15:10,16-06-2023 14:21
74162510,"when i run the code, it stops when the sanity check dataloader, but no error is prompted","through debugging, i found that the problem occurred when i ran to the line of trainer. fit (model).it seems that there are some problems when loading data.
here's my code
weight = ""bert-base-uncased""

class classifier(pl.lightningmodule): 
    
    def __init__(self, 
                 num_classes: int,
                 train_dataloader_: dataloader,
                 val_dataloader_: dataloader,
                 weights: str = weight):
        
        super(classifier, self).__init__()
        self.train_dataloader_ = train_dataloader_
        self.val_dataloader_ = val_dataloader_
        
        self.bert = automodel.from_pretrained(weights)
        self.num_classes = num_classes
        self.classifier = nn.linear(self.bert.config.hidden_size, self.num_classes)
    
    def forward(self, input_ids: torch.tensor):
        bert_logits, bert_pooled = self.bert(input_ids = input_ids)
        out = self.classifier(bert_pooled)
        return out
    
    def training_step(self, batch, batch_idx):
        # batch
        input_ids, labels = batch
    
        # predict
        y_hat = self.forward(input_ids=input_ids)
        
        # loss 
        loss = f.cross_entropy(y_hat, labels)

        # logs
        tensorboard_logs = {'train_loss': loss}
        return {'loss': loss, 'log': tensorboard_logs}
    
    def validation_step(self, batch, batch_idx):
        input_ids, labels = batch
        
        y_hat = self.forward(input_ids = input_ids)
        
        loss = f.cross_entropy(y_hat, labels)
        
        a, y_hat = torch.max(y_hat, dim=1)
        y_hat = y_hat.cpu()
        labels = labels.cpu()

        val_acc = accuracy_score(labels, y_hat)
        val_acc = torch.tensor(val_acc)
        
        val_f1 = f1_score(labels, y_hat, average='micro')
        val_f1 = torch.tensor(val_f1)

        return {'val_loss': loss, 'val_acc': val_acc, 'val_f1': val_f1}
    
    def validation_end(self, outputs):
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()
        avg_val_f1 = torch.stack([x['val_f1'] for x in outputs]).mean()
        
        tensorboard_logs = {'val_loss': avg_loss, 'avg_val_acc': avg_val_acc, 'avg_val_f1': avg_val_f1}
        
        return {'avg_val_loss': avg_loss, 'avg_val_f1':avg_val_f1 ,'progress_bar': tensorboard_logs}
    
    def configure_optimizers(self):
        return torch.optim.adam([p for p in self.parameters() if p.requires_grad], 
                                lr=2e-05, eps=1e-08)
    
    def train_dataloader(self):
        return self.train_dataloader_
    
    def val_dataloader(self):
        return self.val_dataloader_

train  = pd.read_csv(""data/practice/task1.csv"", names =[""index"", ""text"", ""gold""], sep="";"", header=0)
test   = pd.read_csv(""data/trial/task1.csv"", names =[""index"", ""text"", ""gold""], sep="";"", header=0)

weights = [""distilroberta-base"", ""bert-base-uncased"", ""roberta-base"", ""t5-base""]
batch_size = 12

random_seed = 1988
train, val = train_test_split(train, stratify=train[""gold""], random_state=random_seed)
# from transformers import logging

# logging.set_verbosity_warning()
# logging.set_verbosity_error()
for weight in weights:
    try:
        tokenizer = autotokenizer.from_pretrained(weight)
        x_train = [torch.tensor(tokenizer.encode(text, max_length=200, truncation=true)) for text in train[""text""]]
        x_train = pad_sequence(x_train, batch_first=true, padding_value=0)
        y_train = torch.tensor(train[""gold""].tolist())

        x_val = [torch.tensor(tokenizer.encode(text, max_length=200, truncation=true)) for text in val[""text""]]
        x_val = pad_sequence(x_val, batch_first=true, padding_value=0)
        y_val = torch.tensor(val[""gold""].tolist())

        ros = randomoversampler(random_state=random_seed)
        x_train_resampled, y_train_resampled = ros.fit_resample(x_train, y_train)

        x_train_resampled = torch.tensor(x_train_resampled)
        y_train_resampled = torch.tensor(y_train_resampled)

        train_dataset = tensordataset(x_train_resampled, y_train_resampled)
        train_dataloader_ = dataloader(train_dataset,
                                    sampler=randomsampler(train_dataset),
                                    batch_size=batch_size,
                                    num_workers=24,
                                    pin_memory=true)

        val_dataset = tensordataset(x_val, y_val)
        val_dataloader_ = dataloader(val_dataset,
                                    batch_size=batch_size,
                                    num_workers=24,
                                    pin_memory=true)
        
        model = classifier(num_classes=2,
                            train_dataloader_=train_dataloader_,
                            val_dataloader_ = val_dataloader_,
                            weights=weight)

        trainer = pl.trainer(devices=1,accelerator=""gpu"",
                            max_epochs=30)
        
        trainer.fit(model)
        
        x_test = [torch.tensor(tokenizer.encode(text, max_length=200, truncation=true)) for text in test[""text""].tolist()]
        x_test = pad_sequence(x_test, batch_first=true, padding_value=0)
        y_test = torch.tensor(test[""gold""].tolist())

        test_dataset = tensordataset(x_test, y_test)
        test_dataloader_ = dataloader(test_dataset, batch_size=16, num_workers=4)

        device = ""cuda:0""
        model.eval()
        model = model.to(device)

        test_preds = []
        for batch in tqdm(test_dataloader_, total=len(list(test_dataloader_))):
            ii, _ = batch
            ii = ii.to(device)
            preds = model(input_ids = ii)
            preds = torch.argmax(preds, axis=1).detach().cpu().tolist()
            test_preds.extend(preds)    

        from sklearn.metrics import classification_report

        report = classification_report(test[""gold""].tolist(), test_preds)

        with open(""task1_experiments/""+weight+""_baseline.txt"", ""w"") as f:
            f.write(report)
    except:
        continue

when the code stops running, the output of the terminal is shown in the following.i don't know what caused this problem. i hope someone can help me solve this problem.
how can i solve this problem.
thanks in advance for helping me
gpu available: true (cuda), used: true
tpu available: false, using: 0 tpu cores
ipu available: false, using: 0 ipus
hpu available: false, using: 0 hpus
local_rank: 0 - cuda_visible_devices: [0]
| name       | type         | params
0 | bert       | robertamodel | 124 m
1 | classifier | linear       | 1.5 k
124 m     trainable params
0         non-trainable params
124 m     total params
498.589   total estimated model params size (mb)
sanity checking dataloader 0:   0%|                                                                                                                                     | 0/2 [00:00<?, ?it/s]
enter image description here","['python', 'pytorch', 'bert-language-model', 'pytorch-lightning']",74311847,"ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½forwardï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½classfierï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½tensorï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
print(self.bert(input_ids = i",https://stackoverflow.com/questions/74162510,python,22-10-2022 09:47,2203.0,0.0,1.0,True,04-11-2022 03:25,22-10-2022 09:58
78881661,how to chain azureopenai in langchain?,"from openai import azureopenai
client = azureopenai(
        azure_endpoint=os.getenv(""azure_openai_endpoint""),
        api_key=os.getenv(""azure_openai_api_key""),
        api_version=""2024-05-01-preview""
    )

normal_chain = (
    chatprompttemplate.from_messages([(""system"", ""write a tweet about {topic} in the style of elon musk"") ])
    | client
    | stroutputparser()
)


in this code, i am getting expected type 'runnable[any, other] | (any) -> other | (iterator) -> iterator[other] | mapping[str, runnable[any, other] | (any) -> other | any]', got 'azureopenai' instead. how to make the client as runnable? is there any other way to do the same thing?
i tried all the available options from langchain, but it is not working.","['python', 'azure', 'openai-api', 'langchain']",78906013,"client = azureopenai(
        azure_endpoint=os.getenv(""azure_openai_endpoint""),
        api_key=os.getenv(""azure_openai_api_key""),
        api_version=""2024-05-01-preview""
    )

    completion = client.chat.completions.create(
        model=finetuned_deployment,
        messages=[
            {
                ""role"": ""user"",
                ""content"": f""write a tweet about {topic}""
            }],
        max_tokens=800,
        temperature=0.7,
        top_p=0.95,
    )
    
completion.to_json()    

this code snippet worked.",https://stackoverflow.com/questions/78881661,python,17-08-2024 07:50,205.0,0.0,1.0,True,23-08-2024 13:20,17-08-2024 09:19
73074462,how to find closest embedding vectors?,"i have 100k known embedding i.e.
[emb_1, emb_2, ..., emb_100000]

each of this embedding is derived from gpt-3 sentence embedding with dimension 2048.
my task is given an embedding(embedding_new) find the closest 10 embedding from the above 100k embedding.
the way i am approaching this problem is brute force.
every time a query asks to find the closest embeddings, i compare embedding_new with [emb_1, emb_2, ..., emb_100000] and get the similarity score.
then i do quicksort of the similarity score to get the top 10 closest embedding.
alternatively, i have also thought about using faiss.
is there a better way to achieve this?","['deep-learning', 'embedding', 'word-embedding', 'vector-database']",73102747,"i found a solution using vector database lite (vdblite)
vdblite here: 
import vdblite
from time import time
from uuid import uuid4
import sys
from pprint import pprint as pp


if __name__ == '__main__':
    vdb = vdblite.vdb()
    dimension = 12    # dimensions of each vector                         
    n = 200    # number of vectors                   
    np.random.seed(1)             
    db_vectors = np.random.random((n, dimension)).astype('float32')
    print(db_vectors[0])
    for vector in db_vectors:
        info = {'vector': vector, 'time': time(), 'uuid': str(uuid4())}
        vdb.add(info)
    vdb.details()
    results = vdb.search(db_vectors[10])
    pp(results)

looks like it uses faiss behind the scene.",https://stackoverflow.com/questions/73074462,deep-learning,22-07-2022 02:22,3411.0,3.0,2.0,True,20-09-2022 08:20,24-07-2022 06:50
78661728,implement a search engine chain using tavily in langchain,"i want to implement a search engine chain using tavily in langchain. this chain gives user's query as an input and returns up to 5 related documents. each retrieved document must have the content of the document as page_content and the url of the corresponding site as metadata under the definition of langchain documents. i must use langchain_core.documents.base.document class to define documents. so this chain will have two main parts:

tavily search platform
parser with the aim of converting search output data into standard langchai documents.

i wrote this code but i don't know how to change tavily output format into standard form of document:
from langchain_core.documents.base import document
from langchain_community.tools.tavily_search import tavilysearchresults

search = tavilysearchresults(max_results=5)

class parseddocument(basemodel):
    content: str = field(description=""this refers to the content of the search."")
    url: str = field(description=""this refers to the url of the search."")

search_parser = pydanticoutputparser(pydantic_object=parseddocument)
search_engine_chain = search | search_parser


i would be grateful if you could help me how to change this code.","['python', 'nlp', 'search-engine', 'langchain', 'chain']",78910571,"i finally found the answer:
class parseddocument(basemodel):
    content: str = field(description=""this refers to the content of the search."")
    url: str = field(description=""this refers to the url of the search."")

# define a custom parser
def custom_parser(search_results):
    parsed_documents = []
    for result in search_results:  # adjust this line based on the actual structure of search_results
        parsed_document = parseddocument(content=result['content'], url=result['url'])
        document = document(page_content=parsed_document.content, metadata={'url': parsed_document.url})
        parsed_documents.append(document)
    return parsed_documents

search_engine_chain = search | custom_parser",https://stackoverflow.com/questions/78661728,python,24-06-2024 10:01,524.0,2.0,1.0,True,25-08-2024 06:24,24-06-2024 13:52
77468918,winerror 10060 connection timeout error when downloading &#39;punkt&#39; in nltk,"i'm experiencing an issue with the nltk library in python, specifically when trying to download the 'punkt' tokenizer models. here's the error message i receive:
context:
i'm attempting to download the punkt package using the following code:
import nltk
nltk.download('punkt')

troubleshooting steps i've taken:

checked my internet connection, which seems to be working fine.
temporarily disabled firewall and antivirus software, but the issue persists.
attempted to use a different internet connection (e.g., mobile hotspot), but faced the same error.","['python', 'python-3.x', 'jupyter-notebook', 'nltk']",77469069,"a very strange error :



it seems to be that on some networks, i've been told ""jio"" is one of them, raw.githubusercontent.com is not accessible, e.g.:",https://stackoverflow.com/questions/77468918,python,12-11-2023 12:31,492.0,0.0,2.0,True,22-11-2023 15:28,12-11-2023 15:11
74360282,bos token for encoder decoder models,"iï¿½ï¿½ï¿½m using t5-base for my model, and it seems to be generating something reasonable when i do model.generate. but my question is how?
the decoder part of this model needs a starting token to start decoding doesnï¿½ï¿½ï¿½t it? how does it figure out what the very first token is supposed to look like?
or am i doing the training wrong where i should have included a token?
if nee"" rel=""nofollow noreferrer"">here is a code sample where i used model.generate.
edit 1
apart from the answer below, i found that there was a model.config.decoder_start_token_id. this is not necessarily <bos>. in the case of t5/flan-t5 it ended up being <pad>.","['deep-learning', 'huggingface-transformers', 'transformer-model', 'huggingface-tokenizers']",74386139,"this is already done within the generate method that is implemented using the generationmixin in the pytorch part of huggingface transformers. even if you provide a prefix for the generation, the bos (beginning of a sequence) token is explicitly added. the default generation algorithm is then the beam search.",https://stackoverflow.com/questions/74360282,deep-learning,08-11-2022 11:52,1720.0,3.0,1.0,True,14-11-2022 10:38,14-11-2022 10:38
77461857,&#39;openai&#39; function_calling not execute the function,"using function calling, 'openai' does not execute the function, but it prints the function with parameters. please see below:
i am using a local llm (llamaï¿½ï¿½2) in colaboratory.
'openai' version installed: 0.27.8
!pip install openai==0.27.8

os.environ['openai_api_key'] = 'null'
os.environ['openai_api_base'] = ""

#import openai from ""openai"";
openai.api_base = os.getenv('openai_api_base')
openai.api_key = os.getenv(""openai_api_key"")
openai.api_version = ""2023-07-01-preview""

here is the function:
def get_current_weather(location):
""""""get the current weather in a given location""""""

url = ""

querystring = {""q"":location, ""days"":""1""}

headers = {
    ""x-rapidapi-key"": ""xxxxx"",
    ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
}

response = requests.get(url, headers=headers, params=querystring)

weather_info = {
    ""location"": response.json()['location']['name'],
    ""temperature"": response.json()['current']['temp_c'],
}

print(json.dumps(weather_info))

return json.dumps(weather_info)

it returns the following result:
response=get_current_weather('naples, italy')
    {""location"": ""naples"", ""temperature"": 17.0}

here is the schema for the function:
function_descriptions = [{
    ""name"": ""get_current_weather"",
    ""description"": ""get the current weather"",
    ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
            ""location"": {
                ""type"": ""string"",
                ""description"": ""the city and state, e.g. san francisco, ca""
            },
        },
        ""required"": [""location""],
    },
}]

here is the chat completion:
completion = openai.chatcompletion.create(
model=""gpt-35-turbo-0613"",
messages=[{""role"": ""user"", ""content"": user_prompt}],
functions=function_descriptions,
function_call={""name"": ""get_current_weather""}
#function_call=""auto""
)
output = completion.choices[0].message
print(output)
{
   ""role"": ""assistant"",
   ""content"": ""ah, thank you for asking! the current temperature in naples, italy is approximately {get_current_weather({location: \""naples, italy\""})}. would you like me to tell you more about the weather there?<|end_of_assistant|></s>""
 }

why does it not execute the function, but it just prints the call {get_current_weather({location: \""naples, italy\""})}?","['openai-api', 'function-call', 'large-language-model']",77480757,"large language models (llms) are not able to directly run code. their expertise lies in processing and generating text, which includes code snippets but doesn't extend to actual execution. if the goal is to execute code, there are two alternatives:

server-side execution:
in this approach, the code is executed on a server owned by entities like openai or google. however, this method poses security concerns due to the risk of arbitrary code execution. it's important to note that you are not passing the actual function get_current_weather to the model, you are just providing a description in the function_descriptions.

client-side processing:
many companies opt for client-side processing, where the parameters identified by the model are sent to the client so it can be handle by the user (the function selection and identification of parameter is the actual benefit of the feature). to implement this, the model's output needs to be carefully managed on the client side.
after receiving the model's response, you can parse the result by examining finish_reason in response.choices[0].finish_reason. if you are using function_call=""auto"" and the model identifies the need for a function call, it returns finish_reason='function_call'. you can use this information to parse the output intelligently and execute the code on your server, maintaining control over the execution process.",https://stackoverflow.com/questions/77461857,openai-api,10-11-2023 17:33,593.0,0.0,1.0,True,29-11-2023 19:10,29-11-2023 19:10
74563252,match the content of sentence/sequence in python,"suppose we 2 sequence of words
sentence1 = 'ram is eating'

sentence2 = 'is ram  eating'

sentence3 = 'is ram playing'

sentence4 = 'movie ram watching is'

how to get match% of such 2 sequences .
difflib sequencematcher matches letter by letter . any way to find match % in these cases.
match% between sentence1 and sentence2 = 3/3 i.e. 100%
match% between sentence1 and sentence3 = 2/3 i.e. 66.66%
match% between sentence1 and sentence4 = 2/3 i.e. 66.66%
match% = (number of words matching in sentence1 and sentence2 irrespective of position/total number of words in sentence1)*100","['python', 'nlp', 'sequence', 'sentence', 'difflib']",74563449,"how about converting string to list and find matching percentage.
sentence1 = 'ram is eating'
sentence2 = 'is ram  eating'

sentence1 = sentence1.split()
sentence2 = sentence2.split()

longest = max(sentence1, sentence2, key=len)

per = len(set(sentence1) & set(sentence2))  
result = per/len(longest)
print (f'{result *100}% matched')
   

gives #
100.0% matched

case 2
sentence1 = 'ram is eating' 
sentence2 = 'is ram'

sentence1 = sentence1.split()
sentence2 = sentence2.split()
longest = max(sentence1, sentence2, key=len)

per = len(set(sentence1) & set(sentence2))
result = per/len(longest)
print (f'{result *100}% matched')
       

gives #
66.66666666666666% matched

case 3
sentence1 = 'ram is eating'

sentence2 = 'is ram'
sentence3 = 'is ram playing'

sentence1 = sentence1.split()
sentence2 = sentence2.split()
sentence3 = sentence3.split()


longest = max(sentence1, sentence3, key=len)

per = len(set(sentence1) & set(sentence3)) 
result = per/len(longest)
print (f'{result *100}% matched')

gives #
66.66666666666666% matched",https://stackoverflow.com/questions/74563252,python,24-11-2022 15:44,104.0,0.0,1.0,True,24-11-2022 16:18,24-11-2022 16:16
78979528,send google drive api file download to openai,"i'm trying to send a file from google drive to openai api via chat completion
i've already authenticated and i'm able to list files and get the file in question. the google api returns the data as a blob.
the problem i'm running into is that when i try to convert the blob to a base64 encoded string, i get a rangeerror or domexception from the supabase edge function
here's a snippet from the code i have right now (post auth and files.get):
const filedownload = await driveapi.files.get({
  fileid: 'file_id_string',
  alt: 'media',
});
console.log('file download', filedownload.data);

const blob: blob = filedownload.data;
const arraybuffer = await blob.arraybuffer();
const uint8array = new uint8array(arraybuffer);

// attempt 1
const charcodestring = string.fromcharcode(...uint8array); // <-- fails here
const base64string = btoa(charcodestring);

// attempt 2
const decodedstring = new textdecoder().decode(uint8array);
const base64string = btoa(decodedstring); // <-- fails here

// uncomment once i get the base64string right
// await openai.chat.completions.create({
//   model: 'gpt-4o-mini',
//   response_format: 'json_object',
//   messages: [
//     { role: 'user', content: [{ url: base64string, type: 'image_url' }] },
//   ],
// });

# attempt 1 error message
 rangeerror: maximum call stack size exceeded
    at object.handler (file:///repos/supabase-personal/supabase/functions/openai/index.ts:32:38)
    at object.runmicrotasks (ext:core/01_core.js:642:26)
    at processticksandrejections (ext:deno_node/_next_tick.ts:39:10)
    at runnextticks (ext:deno_node/_next_tick.ts:48:3)
    at eventlooptick (ext:core/01_core.js:175:21)
    at async respond (ext:sb_core_main_js/js/

# attempt 2 error message
domexception: the string to be encoded contains characters outside of the latin1 range.
    at new domexception (ext:deno_web/01_dom_exception.js:116:20)
    at btoa (ext:deno_web/05_base64.js:52:13)
    at object.handler (file:///repos/supabase-personal/supabase/functions/openai/index.ts:37:26)
    at object.runmicrotasks (ext:core/01_core.js:642:26)
    at processticksandrejections (ext:deno_node/_next_tick.ts:39:10)
    at runnextticks (ext:deno_node/_next_tick.ts:48:3)
    at eventlooptick (ext:core/01_core.js:175:21)
    at async respond (ext:sb_core_main_js/js/

i really appreciate any help that can be given!
i'm expecting, based on the openai documentation, to convert the file to base64 to send to the chat completion endpoint. is there an easier, or better way to do this?","['node.js', 'google-drive-api', 'openai-api', 'supabase', 'deno']",78980182,"i believe your goal is as follows.

you want to download files except for google docs files (document, spreadsheet, slides and so on) as base64 data.
you want to achieve this using googelapis for node.js.

in this case, how about the following modification?
modified script:
const fileid = ""file_id_string""; // please set your file id.

const filedownload = await driveapi.files.get(
  { fileid, alt: ""media"" },
  { responsetype: ""stream"" }
);
let buf = [];
filedownload.data.on(""data"", (e) => buf.push(e));
filedownload.data.on(""end"", () => {
  const buffer = buffer.concat(buf);
  const base64string = buffer.tostring(""base64"");
  console.log(base64string);
});

or, when responsetype: ""arraybuffer"" is used, how about the following modification?
const fileid = ""file_id_string""; // please set your file id.

const filedownload = await driveapi.files.get(
  { fileid, alt: ""media"" },
  { responsetype: ""arraybuffer"" }
);
const base64string = buffer.from(filedownload.data).tostring(""base64"");
console.log(base64string);


by this, the downloaded file is converted to the base64 data in base64string.

reference:

google apis node.js client",https://stackoverflow.com/questions/78979528,node.js,12-09-2024 18:37,171.0,0.0,1.0,True,13-09-2024 01:19,12-09-2024 21:06
76869283,program raised stopiteration error when using pattern library on python,"i want to use suggest function from pattern library in python but i encountered an error.
i expected to see the output but it raised stopiteration error.
my code:
from pattern.en import sentiment, suggest

print(sentiment(""what a mess day!""))
print(suggest(""aerplane""))


terminal output:
(-0.21875, 0.175)
traceback (most recent call last):
  file ""/library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/pattern/text/__init__.py"", line 609, in _read
    raise stopiteration
stopiteration
the above exception was the direct cause of the following exception:

traceback (most recent call last):
  file ""/users/umutsatir/desktop/nda/sentiment.py"", line 4, in <module>
    print(suggest(""aerplane""))
          ^^^^^^^^^^^^^^^^^^^
  file ""/library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/pattern/text/en/__init__.py"", line 207, in suggest
    return spelling.suggest(w)
           ^^^^^^^^^^^^^^^^^^^
  file ""/library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/pattern/text/__init__.py"", line 2677, in suggest
    if len(self) == 0:
       ^^^^^^^^^
  file ""/library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/pattern/text/__init__.py"", line 376, in __len__
    return self._lazy(""__len__"")
           ^^^^^^^^^^^^^^^^^^^^^
  file ""/library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/pattern/text/__init__.py"", line 368, in _lazy
    self.load()
  file ""/library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/pattern/text/__init__.py"", line 2621, in load
    for x in _read(self._path):
runtimeerror: generator raised stopiteration

i am newbie at stackoverflow. if any data is missing, please let me know. thanks!","['python', 'nlp', 'stopiteration']",76878083,"(i am posting this as an answer to allow me to nicely format the code suggestions.)
try editing your /library/frameworks/python.framework/versions/3.11/lib/python3.11/site-packages/pattern/text/__init__.py file.  find the definition of the _read method.  for me this is line 588.  then wrap the
for i, line in enumerate(f):
    ...

block with a try/except like
try:
    for i, line in enumerate(f):
        ...
except stopiteration:
    return",https://stackoverflow.com/questions/76869283,python,09-08-2023 15:33,337.0,1.0,1.0,True,10-08-2023 17:33,09-08-2023 16:01
42548300,no module named pipeline,"i am trying to train some data in rasa-nlu. 
so, i installed anaconda, then rasa-nlu and spacy.
but, whenever i try to run
 python -m rasa_nlu.train -c config.json

i get
traceback (most recent call last):
  file ""d:\ddrive\lib\runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  file ""d:\ddrive\lib\runpy.py"", line 72, in _run_code
    exec code in run_globals
  file ""d:\ddrive\lib\site-packages\rasa_nlu\train.py"", line 65, in <module>
    do_train(config)
  file ""d:\ddrive\lib\site-packages\rasa_nlu\train.py"", line 54, in do_train
    trainer = create_trainer(config)
  file ""d:\ddrive\lib\site-packages\rasa_nlu\train.py"", line 31, in create_trainer
    from trainers.spacy_sklearn_trainer import spacysklearntrainer
  file ""d:\ddrive\lib\site-packages\rasa_nlu\trainers\spacy_sklearn_trainer.py"", line 9, in <module>
    from rasa_nlu.extractors.spacy_entity_extractor import spacyentityextractor
  file ""d:\ddrive\lib\site-packages\rasa_nlu\extractors\spacy_entity_extractor.py"", line 7, in <module>
    from spacy.pipeline import entityrecognizer
importerror: no module named pipeline

i am not able to install pipeline using conda or pip.","['python', 'pip', 'anaconda', 'spacy', 'rasa-nlu']",43427865,"make sure your spacyversion is up to date (at the time of this writing this would be 1.7). 

you can install the latest version using 
pip install -u spacy.",https://stackoverflow.com/questions/42548300,python,02-03-2017 06:29,11212.0,1.0,2.0,True,30-12-2023 07:43,02-05-2017 11:41
27860302,is there any way to classify text without target labels?,"i was wondering if there was any way to classify text data into different groups/categories based on the words in the text using a combination of python and sklearn machine learning?
for example:
text = [[""request approval for access"", ""request approval to enter premises"", ""laptop not working""], [""completed bw table loading""]]

so can i get categories like:
category_label = [[0,0,2], [1]]
categories = [[""approval request"", ""approval request"", ""laptop working""], [""bw table""]]

where
  0 = approval request
  2 = laptop working
  1 = bw table

basically the above would imply that there is no labelled training data or target labels.","['python', 'machine-learning', 'scikit-learn', 'nlp']",27867351,"this is readily possible in scikit-learn as well as in nltk.
the features that you list:
0 = approval request
2 = laptop working
1 = bw table

are not ones that a clustering algorithm would naturally choose and its worthwhile to caution you against the possible mistake of clouding your statistical learning algorithm with heuristics.  i suggest that you first try some clustering and classification and then consider semi-supervised learning methods whereby you can label your clusters and propagate those labels.",https://stackoverflow.com/questions/27860302,python,09-01-2015 12:11,2616.0,2.0,2.0,True,23-05-2023 16:01,23-05-2023 16:01
77025306,openai api error: &quot;property &#39;data&#39; does not exist on type &#39;chatcompletion&#39;&quot;,"import openai from ""@/openai"";
import { nextresponse } from ""next/server"";

export async function post(request: request){
    const {todos} = await request.json();

    //communicate with openai api
    const response = await openai.chat.completions.create({
        model: ""gpt-3.5-turbo"",
        temperature: 0.8,
        n: 1,
        stream: false,
        messages: [
            {
                role: ""system"",
                content: `when responding, welcome the user always as hello user and say welcome to the todo list app!
                limit text to 200 characters.`
            },
            {
                role: ""user"",
                content: `hi there, provide a summary of the following todos. count how many todos are in each category such as to do, in progress and done, 
                then tell the user to have a productive day! here's the data: ${json.stringify(
                    todos
                )}`
            }
        ]
    });

    const {data} = response;
    
}

the last line shows the error property 'data' does not exist on type 'chatcompletion'
i wanted to destructure the object to fetch the data from the response object received.","['openai-api', 'chatgpt-api']",77027585,"youï¿½ï¿½ï¿½re using openai nodejs sdk v4.
extract the message as follows:
console.log(response.choices[0].message);

extract the message content as follows:
console.log(response.choices[0].message.content);
<",https://stackoverflow.com/questions/77025306,openai-api,01-09-2023 18:22,983.0,1.0,1.0,True,13-09-2023 18:47,13-09-2023 18:47
74860137,how to regex to match sentences in a pdf?,"i have the following pdf file i use pypdf2 to extract text from it pdf image
and i'm looking for a regex to capture numbered sentences in the pdf file
i tried a couple of regex in the following code  but the output is not as needed i need to capture the numbered points each as one sentence like this
expected output
['1. please admit that plaintiff, joshua pink, received benefits from a collateral
source, as defined by ï¿½ï¿½768.76, florida statutes, for medical bills alleged to have been incurred as
a result of the incident described in the complaint.',2. please.....] 

instead of two regexes i tried either doesn't capture the full sentence or capture it in multiple lines and consider every \n as a new sentence
extracted text
"" \n in the circuit court, of the \neighteenth judicial circuit, in \nand for seminole county, \nflorida  \n \ncase no: 2022 -ca-002235  \n \njoshua pink,  \n \n plaintiff,  \nvs. \n \nmathew zumbrum , \n \n defendant.  \n                                                                  / \n \ndefendant's request for admissions to plaintiff, joshua pink  \n \n \ncome now the defendant , mathew zumbrum , by and through the undersigned \nattorneys, and pursuant to rule 1.370, florida rul es of civil procedure, requests the plaintiff, \njoshua pink, admit in this action that each of the following statements are true:  \n1. please admit that plaintiff, joshua pink , received benefits from a collateral \nsource, as defined by ï¿½ï¿½768.76, florida statute s, for medical bills alleged to have been incurred as \na result of the incident described in the complaint.  \n2. please admit that plaintiff, joshua pink , received benefits from a collateral \nsource, as defined by ï¿½ï¿½768.76, florida statutes, for loss of wages o r income alleged to have been \nsustained as a result of the incident described in the complaint.  \n3. please admit that plaintiff, joshua pink , received benefits under the personal \ninjury protection portion of an automobile policy for medical bills alleged toeen incurred \nas a result of the incident described in the complaint.  \n filing # 162442429 e-filed 12/06/2022 09:46:49 am\n \n2 4. please admit that plaintiff, joshua pink , received benefits under the personal \ninjury protection portion of an automobile insurance policy for loss of wages or income alleged \nto have been sustained as a result of the incident described in the complaint.  \n5. please admit that plaintiff, joshua pink , received benefits under the medical \npayments provisions of an automobile insurance policy for medical bills alleged to have been \nincurred as a result of the incident described in the complaint.  \n6. please admit that plaintiff, joshua pink , is subject to a deductible under the \npersonal injury protection portion of an automobile insurance policy.  \n7. please admit that plaintiff, joshua pink  received benefits pursuant to personal \nor group health insurance policy, for medical bills alleged to have been incurred as a result of the \nincident described in the complaint.  \n8. please admit that plaintiff, joshua  pink , received benefits pursuant to a \npersonal or group wage continuation plan or policy, for loss of wages or income alleged to have \nbeen sustained as a result of the incident described in the complaint.  \n 9. please admit that on the date of the accident alleged in your complaint, defendant, \nmathew zumbrum , complied with and met the security requirements under chapter \n627.730 - 627.7405, florida statutes.  \n10. please admit that plaintiff, joshua pink , was partially responsible for the \nsubject accident.  \n11. please admit that plaintiff, joshua pink , did not  suffer a permanent injury as \na result of the subject accident.  \ni hereby certify that on the 6th day of december, 2022 a true and correct copy of \nthe foregoing was electronically filed with the florida court s e-filing portal system which will \n \n3 send a notice of electronic filing to michael r. vaughn, esq., morgan & morgan, p.a., 20 n. \norange ave, 16th floor, orlando, fl 32801 at mvaughn@forthepeople.com; \njburnham@forthepeople.com; mserrano@forthepeople.com.  \nand rew j. gorman & associates  \n \nby: \n \n(original signed electronically by attorney.)  \nlourdes calvo -paquette, esq.  \nattorney for defendant, zumbrum  \n390 n. orange avenue, suite 1700  \norlando, fl 32801  \ntelephone:  (407) 872 -2498  \nfacsï¿½ï¿½mile:  (855) 369 -8989  \nflorida bar no.  0817295  \ne-mail for service (fl r. jud. admin. 2.516) : \nflor.law -mlslaw.172o19@statefarm.com  \n \nattorneys and staff of andrew j. gorman & \nassociates are employees of the law department \nof state farm mutual automobile insurance \ncompany.  \n \n \n\n""

sample output of regex2 (sentence is captured in 2 lines)

[('2022', 'ca-002235 '),
 ('1', 'florida rul es of civil procedure, requests the plaintiff,'),
 ('1',
  'please admit that plaintiff, joshua pink , received benefits from a collateral'),
 ('768',
  'florida statute s, for medical bills alleged to have been incurred as..]

sample output of regex1 (not capturing full sentence) 

['1. please admit that plaintiff, joshua pink , received benefits from a collateral ',
 '2. please admit that plaintiff, joshua pink , received benefits from a collateral ',
 '3. please admit that plaintiff, joshua pink , received benefits under the personal ',
 '2 4. please admit that plaintiff, joshua pink , received benefits under the personal ',
 '5. please admit that plaintiff, joshua pink , received benefits under the medical ',....]

code:
def read_pdf(name):
    reader = pdfreader(name,""rb"")
    text = """"
    for page in reader.pages:
        text += page.extract_text() + ""\n""

    #regex1 = r'(^[0-9].*)'
    regex2 = r'([\d]+).+?([a-za-z].+).'
    pat = re.compile(regex, re.m)

    extracted_text = pat.findall(text)

    return text,extracted_text

text,pdf1 = read_pdf(names[0])","['python', 'regex', 'pdf', 'nlp', 'pypdf']",74862269,"i'll provide an answer to go over a couple of different patterns you can use to approach text items like that. let's say you have a text that is structured like this:
test_str = """"""
some preamble.
    1. very
long
sentence.
    2. one-line sentence.
    3. another
longer sentence.
a new paragraph.
""""""

first scenario: you want to match items that begin with a number followed by a period at the beginning of a line (with optional leading space) and end with a period at the end of a line - irrespective of how many characters it takes, but as few as possible. that's what your question reads like. one pattern that describes this is ^[ \t]*\d+\.[\s\s]*?\.$. the heavy lifting here is done by [\s\s]*? which is a lazy class that just matches any character (by including all spaces and all non-spaces) as few times as possible.
regex1 = re.compile(r""^[ \t]*\d+\.[\s\s]*?\.$"", re.multiline)
print(re.findall(regex1, test_str))

which returns:
['    1. very\nlong\nsentence.', '    2. one-line sentence.', '    3. another\nlonger sentence.']
if you want to exclude leading space, you could add a capturing group ^[ \t]*(\d+\.[\s\s]*?\.)$ in which case findall() will only return the captured part. in python:
regex2 = re.compile(r""^[ \t]*(\d+\.[\s\s]*?\.)$"", re.multiline)
print(re.findall(regex2, test_str))

which returns:
['1. very\nlong\nsentence.', '2. one-line sentence.', '3. another\nlonger sentence.']
first scenario, alternative expression: after the leading number, express the match in terms of lines; always get the first line and add every following line as long as the preceding line does not end in a period: ^[ \t]*(\d+\..*(?:[^.]$\r?\n.*)*\.)$. this will be faster than the lazy class in the first example and returns the same as with regex2.
regex3 = re.compile(r""^[ \t]*(\d+\..*(?:[^.]$\r?\n.*)*\.)$"", re.multiline)
print(re.findall(regex3, test_str))

second scenario: we don't care what the sentence(s) end in. just get complete items, which we'll interpret as the leading number followed by all lines that do not start with another leading number or an entirely new paragraph: ^[ \t]*(\d+\..+$(?:\r?\n(?![ \t]*\d+\.|a new).*)*).
this makes use of a negative lookahead (?![ \t]*\d+\.|a new) to prevent matching lines that start either with a new item number or some non-item text and allows more control over what kind of lines may constitute an item. return values are the same.
regex4 = re.compile(r""^[ \t]*(\d+\..+$(?:\r?\n(?![ \t]*\d+\.|a new).*)*)"", re.multiline)
print(re.findall(regex4, test_str))",https://stackoverflow.com/questions/74860137,python,20-12-2022 08:13,1627.0,1.0,4.0,True,22-12-2022 12:53,21-12-2022 10:02
76349622,training a bartforsequenceclassification returns data with ununiform dimentsions,"i am  trying to fine-tune a bart-base model on a dataset that i have. the dataset looks like this: it has columns ""id"", ""text"", ""label"" and ""dataset_id"". the ""text"" column is what i want to use as inputs to the model, and it is plain text. ""label"" is a value of either 0 or 1.
i've already written the code for training, using transfomers==4.28.0.
this is the code for the dataset class:
class textdataset(dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}

    def __len__(self):
        return len(self.encodings['input_ids'])

this is the code for loading and encoding of the data:
def load_data(directory):
    files = os.listdir(directory)
    dfs = []
    for file in files:
        if file.endswith('train.csv'):
            df = pd.read_csv(os.path.join(directory, file))
            dfs.append(df)
    return pd.concat(dfs, ignore_index=true)
print(len(load_data(""splitted_data/gender-bias"")))

def encode_data(tokenizer, text, labels):
    inputs = tokenizer(text, padding=""max_length"", truncation=true, max_length=128, return_tensors=""pt"")
    inputs['labels'] = torch.tensor(labels)
    return inputs

this is the code for the metrics for evaluation. i use the f1_score function from scikit.
def compute_metrics(eval_pred):
    logits = eval_pred.predictions
    labels = eval_pred.label_ids
    predictions = np.argmax(logits, axis=-1)
    return {""f1"": f1_score(labels, predictions)}

this is the training function:
def train_model(train_dataset, eval_dataset):
    # define the training arguments
    training_args = trainingarguments(
        output_dir='./baseline/results',           # output directory
        num_train_epochs=5,               # total number of training epochs
        per_device_train_batch_size=32,   # batch size per device during training
        per_device_eval_batch_size=64,    # batch size for evaluation
        warmup_steps=500,                 # number of warmup steps for learning rate scheduler
        weight_decay=0.01,                # strength of weight decay
        evaluation_strategy=""steps"",      # evaluation is done at each training step
        eval_steps=50,                    # number of training steps between evaluations
        load_best_model_at_end=true,      # load the best model when finished training (defaults to `false`)
        save_strategy='steps',            # save the model after each training step
        save_steps=500,                   # number of training steps between saves
        metric_for_best_model='f1',       # metric to use to compare models
        greater_is_better=true            # whether a larger metric value is better
    )

    # define the trainer
    trainer = trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics
    )

    # train the model
    trainer.train()

    return trainer

this is how i defined the model and etc.
model = bartforsequenceclassification.from_pretrained('facebook/bart-base', num_labels=2)
tokenizer = barttokenizer.from_pretrained('facebook/bart-base')

train_df = load_data(""splitted_data/gender-bias"")
train_encodings = encode_data(tokenizer, train_df['text'].tolist(), train_df['label'].tolist())

# for simplicity, let's split our training data to create a pseudo-evaluation set
train_size = int(0.9 * len(train_encodings['input_ids']))  # 90% for training
train_dataset = {k: v[:train_size] for k, v in train_encodings.items()}
print(train_dataset)
print(len(train_dataset))

eval_dataset = {k: v[train_size:] for k, v in train_encodings.items()}  # 10% for evaluation

# convert the dictionary data to pytorch dataset
train_dataset = textdataset(train_dataset)
eval_dataset = textdataset(eval_dataset)

trainer = train_model(train_dataset, eval_dataset)

the training looks just fine. however, when it comes to evaluation during training, an error is raised from my compute_metrics function, which takes a parameter as the output of the model. the model should be a binary classification model, returning the probabilistic of each label in its output i believe.
np.argmax(np.array(logits), axis=-1) 21 
valueerror: could not broadcast input array from shape (3208,2) into shape (3208,)

i've tried to output the type of the logits, and it turns out that type(logits) return tuple. considering that this might be caused the fact that evaluation dataset might be split into batches, and the returned tuple is a number of separate numpy arrays, i've also tried to concatenate the tuple.
def compute_metrics(eval_pred):
    logits = eval_pred.predictions
    labels = eval_pred.label_ids
    logits = np.concatenate(logits, axis=0)
    predictions = np.argmax(logits, axis=-1)
    return {""f1"": f1_score(labels, predictions)}

but this raised a new error:
packages/numpy/core/overrides.py in concatenate(*args, **kwargs) 

valueerror: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 3 dimension(s)

how can i solve this issue?","['python', 'numpy', 'machine-learning', 'pytorch', 'huggingface-transformers']",76373648,"i've found the answer. since the returned tuple has a shape of [(3208, 2), (3208, 128, 768)], it is returning two things simultaneously. the first element of this tuple represents the binary logits for the predictions, while the second element seems to be an output of a layer of my bart model. hence, the code works well when i write it as below:
def compute_metrics(eval_pred):
    logits = eval_pred.predictions[0]
    labels = eval_pred.label_ids
    predictions = np.argmax(logits, axis=-1)
    return {""f1"": f1_score(labels, predictions)}",https://stackoverflow.com/questions/76349622,python,28-05-2023 01:27,445.0,0.0,1.0,True,31-05-2023 12:47,28-05-2023 22:13
75907062,proc http sas macro issue,"i'm using a macro that allows you to prompt chat-gpt 3.5 (or trying to at least) within a sas environment. here is the code.
   options mprint mlogic symbolgen;



obviously not going to show you my api-key (fake example provided) but the code works fine outside of being wrapped in a macro but when i run it inside a macro i get this error.
symbolgen:  macro variable sys_proc resolves to 404
 mlogic(chatgpt):  %if condition (&sys_proc ne 200) is true
 mlogic(chatgpt):  %put an error occurred. http &sys_proc &sys_proc
 symbolgen:  macro variable sys_proc resolves to 404
 symbolgen:  macro variable sys_proc resolves to not found
 an error occurred. http 404: not found
 mlogic(chatgpt):  %abort 
 error: execution terminated by an %abort statement.

is it something to do with the headers authorisation statement in proc  here is the code outside of the macro.
    %let api_key= ; 
    %let question = %str(%""sas code to transpose data%"");
    %let question = %str(%""debug 'proc print data=mydf; vars myvar; run;' %"");
    
    /* body of the post request */
    filename in temp;
    data _null_;
    file in;
    put;
    put ""{"";
    put  '""model"": ""gpt-3.5-turbo"", ""messages"": [{""role"": ""user"", ""content"": '""&question }]"";
    put ""}"";
    run;","['sas', 'openai-api']",75908540,"if you want to use a data step to write the value of a macro variable to a text file it is probably best to first get the value from the macro variable into an actual variable. (note: dataset variables can only hold 32k bytes and macro variables can hold 64k bytes.)  this will mean that the macro does not need to worry about macro quoting the value to use it. so if the text includes & or % characters the macro processor will not try to resolve the references.
data _null_;
  file in;
  length text $32767;
  text=symget('text');
  put '{""model"": ""gpt-3.5"", ""messages"": [{""role"": ""user"", ""content"": ' 
      text :$quote. '}]}'
  ;
run;

if you want to generate the authorization headername value in the proc http call then perhaps it is better to parameterize the macro to expect the full value, instead of just part of the value. so include the text bearer (if that is required) in the macro call.
%macro chatgpt(authorization= ......);
   ...

headers ""authorization"" = &authorization. ;
   ...

and then when calling the macro pass the authorization value, including the quotes.  again if the value might include macro triggers like & or % that you don't want the macro processor to modify you can enclose the string in single quotes in the macro call.
%chatgpt(authorization='bearer xxxxxx',......);",https://stackoverflow.com/questions/75907062,sas,01-04-2023 15:13,596.0,0.0,1.0,True,21-05-2023 16:44,28-04-2023 12:20
48225845,how to generate word embeddings in portuguese using gensim?,"i have the following problem:
in english language my code generates successful word embeddings with gensim, and similar phrases are close to each other considering cosine distance:
the angle between ""response time and error measurement"" and ""relation of user perceived response time to error measurement"" is very small, thus they are the most similar phrases in the set.

however, when i use the same phrases in portuguese, it doesn't work:

my code as follows:
import logging
logging.basicconfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.info)
import matplotlib.pyplot as plt
from gensim import corpora
documents = [""interface mï¿½ï¿½quina humana para aplicaï¿½ï¿½ï¿½ï¿½es computacionais de labo"",
          ""um levantamento da opiniï¿½ï¿½o do usuï¿½ï¿½rio sobre o tempo de resposta do sistema info"",
           ""o sistema de gerenciamento de interface do usuï¿½ï¿½rio"",
           ""sistema e testes de engenharia de sistemas humanos de eps"",
           ""relaï¿½ï¿½ï¿½ï¿½o do tempo de resposta percebido pelo usuï¿½ï¿½rio para a mediï"",
           ""a geraï¿½ï¿½ï¿½ï¿½o de ï¿½ï¿½rvores nï¿½ï¿½o ordenadas binï¿"",
           ""o grï¿½ï¿½fico de interseï¿½ï¿½ï¿½ï¿½o dos caminhos "",
           ""grï¿½ï¿½fico de menores iv largura de ï¿½ï¿½rvores e bem quase enc"",
           ""grï¿½ï¿½ficos menores uma pesq""]

stoplist = set('for a of the and to in on'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
for document in documents]
texts

from collections import defaultdict
frequency = defaultdict(int)

for text in texts:
    for token in text:
        frequency[token] += 1
frequency

from nltk import tokenize  
texts=[tokenize.word_tokenize(documents[i], language='portuguese') for i in range(0,len(documents))]

from pprint import pprint
pprint(texts)

dictionary = corpora.dictionary(texts)
dictionary.save('/tmp/deerwester.dict')
print(dictionary)

print(dictionary.token2id)


# vector
new_doc = ""tempo de resposta e mediï¿½ï¿½ï¿½ï¿½o""
new_vec = dictionary.doc2bow(new_doc.lower().split())
print(new_vec)

## vetor of phrases
corpus = [dictionary.doc2bow(text) for text in texts]
corpora.mmcorpus.serialize('/tmp/deerwester.mm', corpus)  
print(corpus)

from gensim import corpora, models, similarities
tfidf = models.tfidfmodel(corpus) # step 1 -- initialize a model

### phrase coordinates
frase=tfidf[new_vec]
print(frase)

corpus_tfidf = tfidf[corpus]
for doc in corpus_tfidf:
    print(doc)

lsi = models.lsimodel(corpus_tfidf, id2word=dictionary, num_topics=2)
corpus_lsi = lsi[corpus_tfidf]

lsi.print_topics(2)

## text coordinates
todas=[]
for doc in corpus_lsi:
    todas.append(doc)
todas

from gensim import corpora, models, similarities
dictionary = corpora.dictionary.load('/tmp/deerwester.dict')
corpus = corpora.mmcorpus('/tmp/deerwester.mm')
print(corpus)

lsi = models.lsimodel(corpus, id2word=dictionary, num_topics=2)

doc = new_doc
vec_bow = dictionary.doc2bow(doc.lower().split())
vec_lsi = lsi[vec_bow]
print(vec_lsi)

p=[]
for i in range(0,len(documents)):
    doc1 = documents[i]
    vec_bow2 = dictionary.doc2bow(doc1.lower().split())
    vec_lsi2 = lsi[vec_bow2]
    p.append(vec_lsi2)

p

index = similarities.matrixsimilarity(lsi[corpus])

index.save('/tmp/deerwester.index')
index = similarities.matrixsimilarity.load('/tmp/deerwester.index')

sims = index[vec_lsi]
print(list(enumerate(sims)))

sims = sorted(enumerate(sims), key=lambda item: -item[1])
print(sims) 

#################

import gensim
import numpy as np
import matplotlib.colors as colors
import matplotlib.cm as cmx
import matplotlib as mpl

matrix1 = gensim.matutils.corpus2dense(p, num_terms=2)
matrix3=matrix1.t
matrix3[0]
ss=[]
for i in range(0,9):
    ss.append(np.insert(matrix3[i],0,[0,0]))
matrix4=ss
matrix4

matrix2 = gensim.matutils.corpus2dense([vec_lsi], num_terms=2)
matrix2=np.insert(matrix2,0,[0,0])
matrix2

data=np.insert(matrix4,0,matrix2)
data=data.reshape(10,4)
data

names=np.array(documents)
names=np.insert(names,0,new_doc)
new_doc
cmap = plt.cm.jet

cnorm  = colors.normalize(vmin=np.min(data[:,3])+.2, vmax=np.max(data[:,3]))

scalarmap = cmx.scalarmappable(norm=cnorm,cmap=cmap)
len(data[:,1])

plt.subplots()
plt.figure(figsize=(12,9))
plt.scatter(matrix1[0],matrix1[1],s=60)
plt.scatter(matrix2[2],matrix2[3],color='r',s=95)
for idx in range(0,len(data[:,1])):
    colorval = scalarmap.to_rgba(data[idx,3])
    plt.arrow(data[idx,0],
          data[idx,1], 
          data[idx,2], 
          data[idx,3], 
          color=colorval,head_width=0.002, head_length=0.001)
for i,names in enumerate (names):
    plt.annotate(names, (data[i][2],data[i][3]),va='top')
plt.title(""phrase similarity - word2vec with gensim library"")
plt.xlim(min(data[:,2]-.2),max(data[:,2]+1))
plt.ylim(min(data[:,3]-.2),max(data[:,3]+.3))
plt.show()

my question is: is there any additional set up for gensim to generate proper word embeddings in portuguese language or gensim does not support this language?","['python', 'nlp', 'nltk', 'gensim']",59000722,"one year and 10 months later, i got the response by myself: use bert embeddings in pytorch:
phrases:

i adapted pytorch extract_features.py at 
class main:
    def main(self,input_file,output_file):
        self.input_file=input_file
        self.output_file=output_file
        self.bert_model='bert-base-multilingual-uncased'
        self.do_lower_case=true
        self.layers=""-1""
        self.max_seq_length=128
        self.batch_size=32
        self.local_rank=-1
        self.no_cuda=false

        if self.local_rank == -1 or self.no_cuda:
            device = torch.device(""cuda"" if torch.cuda.is_available() and not self.no_cuda else ""cpu"")
            n_gpu = torch.cuda.device_count()
        else:
            device = torch.device(""cuda"", self.local_rank)
            n_gpu = 1
            # initializes the distributed backend which will take care of sychronizing nodes/gpus
            torch.distributed.init_process_group(backend='nccl')
        logger.info(""device: {} n_gpu: {} distributed training: {}"".format(device, n_gpu, bool(self.local_rank != -1)))

        layer_indexes = [int(x) for x in self.layers.split("","")]

        tokenizer = berttokenizer.from_pretrained(self.bert_model, do_lower_case=self.do_lower_case)

        examples = read_examples(self.input_file)

        features = convert_examples_to_features(
            examples=examples, seq_length=self.max_seq_length, tokenizer=tokenizer)

        unique_id_to_feature = {}
        for feature in features:
            unique_id_to_feature[feature.unique_id] = feature

        model = bertmodel.from_pretrained(self.bert_model)
        model.to(device)

        if self.local_rank != -1:
            model = torch.nn.parallel.distributeddataparallel(model, device_ids=[self.local_rank],
                                                            output_device=self.local_rank)
        elif n_gpu > 1:
            model = torch.nn.dataparallel(model)

        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)
        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)
        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)

        eval_data = tensordataset(all_input_ids, all_input_mask, all_example_index)
        if self.local_rank == -1:
            eval_sampler = sequentialsampler(eval_data)
        else:
            eval_sampler = distributedsampler(eval_data)
        eval_dataloader = dataloader(eval_data, sampler=eval_sampler, batch_size=self.batch_size)

        model.eval()
        with open(self.output_file, ""w"", encoding='utf-8') as writer:
            for input_ids, input_mask, example_indices in eval_dataloader:
                input_ids = input_ids.to(device)
                input_mask = input_mask.to(device)

                all_encoder_layers, _ = model(input_ids, token_type_ids=none, attention_mask=input_mask)
                all_encoder_layers = all_encoder_layers

                for b, example_index in enumerate(example_indices):
                    feature = features[example_index.item()]
                    unique_id = int(feature.unique_id)
                    # feature = unique_id_to_feature[unique_id]
                    output_json = collections.ordereddict()
                    output_json[""linex_index""] = unique_id
                    all_out_features = []
                    for (i, token) in enumerate(feature.tokens):
                        all_layers = []
                        for (j, layer_index) in enumerate(layer_indexes):
                            layer_output = all_encoder_layers[int(layer_index)].detach().cpu().numpy()
                            layer_output = layer_output[b]
                            layers = collections.ordereddict()
                            layers[""index""] = layer_index
                            print(layer_output.shape)
                            layers[""values""] = [
                                round(x.item(), 6) for x in layer_output[i]
                            ]
                            all_layers.append(layers)
                        out_features = collections.ordereddict()
                        out_features[""token""] = token
                        out_features[""layers""] = all_layers
                        all_out_features.append(out_features)
                    output_json[""features""] = all_out_features
                    writer.write(json.dumps(output_json) + ""\n"")

and then run:
embeddings=extrair.main()
embeddings.main(input_file='gensim.csv',output_file='gensim.json')

parsing the json file:
import json
from pprint import pprint
import numpy as np

data = [json.loads(line) for line in open('gensim.json', 'r')]

xx=[]
for parte in range(0,len(data)):
    xx.append(np.mean([data[parte]['features'][i]['layers'][0]['values'] for i in range(0,len(data[parte]['features']))],axis=0))

from scipy.spatial.distance import cosine as cos

for i in range(0,len(xx)):
    print(cos(xx[2],xx[i]))

getting as output:",https://stackoverflow.com/questions/48225845,python,12-01-2018 11:56,2028.0,3.0,1.0,True,02-04-2023 13:12,12-01-2018 12:11
77131172,openai api key not working in my react app,"i am trying to create a chatbot in my react app, and i'm not able to generate an llm powered response. i've been studying documentation and checking out tutorials but am unable to fix.
i tried setting up a function for my chatbot, calling the api key, importing openai, setting up the parameters for the gpt-3.5-turbo model including temperature. the catch (error) section has a setresponse of 'error generating response' and that's all i get after user inputs a question.
try {
    const response = await openai.createcompletion({
        model: 'gpt-3.5-turbo',
        prompt: question,
        max_tokens: 100,
        n: 1,
        stop: '\n',
        temperature: 1.17,
        headers: {
            authorization: `bearer ${api_key}`,
        }
});","['javascript', 'reactjs', 'chatbot', 'openai-api']",77132867,"first of all, as @kenwhite suggested, fix the fundamentals. use the try...catch statement properly as follows:
try {
  // your code here
} catch (error) {
  console.error(error);
}

problem
note: openai nodejs sdk v4 was released on august 16, 2023, and is a complete rewrite of the sdk. see the v3 to v4 migration guide.
there are a few problems with the code you posted in your question. the solutions to these problems i provide below differ depending on whether you use openai nodejs sdk v3 or v4.
to check your openai nodejs sdk version, run the following command:
npm info openai version


problem 1: passing an invalid parameter to the api endpoint
you're trying to pass headers as a parameter to the api endpoint, which is not a valid parameter. remove it.
solution
you need to set the bearer token as follows...
ï¿½ï¿½ï¿½ if you have the openai nodejs sdk v3:
import { configuration, openaiapi } from 'openai';

const configuration = new configuration({
  apikey: process.env.openai_api_key,
});

const openai = new openaiapi(configuration);

ï¿½ï¿½ï¿½ if you have the openai nodejs sdk v4:
import openai from 'openai'nai = new openai({
  apikey: process.env.openai_api_key,
});


problem 2: using the wrong method name
you want to use the gpt-3.5-turbo model (i.e., chat completions api). use the proper method name.
solution
ï¿½ï¿½ï¿½ if you have the openai nodejs sdk v3:

openai.createcompletion <-- wrong ï¿½ï¿½ï¿½
openai.createchatcompletion <-- correct (works with the chat completions api) ï¿½ï¿½ï¿½

ï¿½ï¿½ï¿½ if you have the openai nodejs sdk v4:

openai.completions.create <-- wrong ï¿½ï¿½ï¿½
openai.chat.completions.create <-- correct (works with the chat completions api) ï¿½ï¿½ï¿½


problem 3: using the prompt parameter
you want to use the gpt-3."" rel=""nofollow noreferrer"">chat completions api).
the chat completions api uses the messages parameter, while the completions api uses the prompt parameter.
solution
use the messages parameter instead of the prompt parameter.

final solution
ï¿½ï¿½ï¿½ if you have the openai nodejs sdk v3, try this:
import { configuration, openaiapi } from 'openai';

const configuration = new configuration({
  apikey: process.env.openai_api_key,
});

const openai = new openaiapi(configuration);

try {
  const chatcompletion = await openai.createchatcompletion({
    model: 'gpt-3.5-turbo',
    messages: [{ role: 'user', content: 'hello world' }],
    max_tokens: 100,
    n: 1,
    stop: '\n',
    temperature: 1.17,
  });

  console.log(chatcompletion.data.choices[0].message);
} catch (error) {
  console.error(error);
}

ï¿½ï¿½ï¿½ if you have the openai nodejs sdk v4, try this:
import openai from 'openai';

const openai = new openai({
  apikey: process.env.openai_api_key,
});

try {
  const on = await openai.chat.completions.create({
    model: 'gpt-3.5-turbo',
    messages: [{ role: 'user', content: 'hello world' }],
    max_tokens: 100,
    n: 1,
    stop: '\n',
    temperature: 1.17,
  });

  console.log(chatcompletion.choices[0].message);
} catch (error) {
  console.error(error);
}",https://stackoverflow.com/questions/77131172,javascript,19-09-2023 01:19,1527.0,0.0,1.0,True,19-09-2023 17:00,19-09-2023 07:01
74524530,how to get the items inside of an openaiobject in python?,"i would like to get the text inside this data structure that is outputted via gpt3 openai. i'm using python. when i print the object i get:
<openaiobject text_completion id=cmpl-6f7sczdu2ukkjgpxtitpnkgfrikz at 0x7f7648cacef0> json: {
  ""choices"": [
    {
      ""finish_reason"": ""stop"",
      ""index"": 0,
      ""logprobs"": null,
      ""text"": ""\nwhat was malcolm x's original name?\nmalcolm x's original name was malcolm little.\n\nwhere was malcolm x born?\nmalcolm x was born in omaha, nebraska.\n\nwhat was the profession of malcolm x's father?\nmalcolm x's father was a baptist minister.\n\nwhat did malcolm x do after he stopped attending school?\nmalcolm x became involved in petty criminal activities.""
    }
  ],
  ""created"": 1669061618,
  ""id"": ""cmpl-6f7sczdu2gjjhkzspxtitpnkgfrikz"",
  ""model"": ""text-davinci-002"",
  ""object"": ""text_completion"",
  ""usage"": {
    ""completion_tokens"": 86,
    ""prompt_tokens"": 1200,
    ""total_tokens"": 1286
  }
}

how do i get the 'text' component of this?
for example, if this object is called: qa ... i can output
qa['choices']

and i get the same items as above... but adding a .text or ['text'] to this does not do it, and gets an error.
but not sure how to isolate the 'text'
i've read the docs, but cannot find this...","['python', 'openai-api', 'gpt-3']",74524554,"x = {&quot;choices&quot;: [{&quot;finish_reason&quot;: &quot;length&quot;,
                  &quot;text&quot;: &quot;, everyone, and welcome to the first installment of the new opening&quot;}], }

text = x['choices'][0]['text']
print(text)  # , everyone, and welcome to the first installment of the new opening",https://stackoverflow.com/questions/74524530,python,21-11-2022 20:26,11904.0,7.0,5.0,True,07-08-2023 20:10,02-04-2023 20:17
74756815,python - loop to replace elements in string with second element of elements in second string?,"i am trying to replace elements that are in one string with the second element (also string type) of elements in a second string.
a_list and replace are short versions of the lists that i am working with. i want to replace the elements in the first list a_list with the first elements of each list in the second list, if the second element (pos tags) of each list in the second list matches and then check the rest of the list.
here is an example part of the lists that i am working with and my not working function:
a_list = [""jj"",""mm"",""jj""]

replace = [['innovative', 'jj'],['methods', 'nns'],['test', 'mm'],['extra', 'jj']]

new_list = []

for item in a_list:
    if a_list[i] == replace[i][1]:
        new_list.append(item.replace(replace[i][1], replace[i][0]))
        
    else:
        new_list.append(item)

print(new_list)  

['jj', 'mm', 'jj']

what i want: ['innovative', ""test"", ""extra""]
a_list, a_list[1], replace, replace[1], and replace[1][1] are all strings. they were tuples but those were harder for me to work with.
not sure how to remove that specific string after it was used to replace and not sure how to iterate over both strings. replace is a lot longer than a_list. i have been working on this for a couple of hours now i'm sure there is something that can do this - just need help, it seems so close! there are many similar questions but i have tried and they have not worked for me
thank you!","['python', 'list', 'replace', 'nlp', 'append']",74756975,"you have to iterate over replace too. if you find a matching entry then add the corresponding value to your list, delete this entry from replace and break from the inner loop. if you don't find a match, then add your original item. one of the cases where for-else comes in handy.
a_list = [""jj"",""mm"",""jj""]
replace = [['innovative', 'jj'],['methods', 'nns'],['test', 'mm'],['extra', 'jj']]
new_list = []
for item in a_list:
    for i, (value, key) in enumerate(replace):
        if key == item:
            new_list.append(value)
            del replace[i]
            break
    else:
        new_list.append(item)

print(new_list)

result:
['innovative', 'test', 'extra']

if you define your list with a_list = [""hello"", ""jj"", ""mm"", ""jj"", ""jj""] you'll get ['hello', 'innovative', 'test', 'extra', 'jj']. there is no replacement for ""hello"" and there's no replacement left for the last ""jj"".",https://stackoverflow.com/questions/74756815,python,10-12-2022 21:56,393.0,2.0,2.0,True,10-12-2022 23:08,10-12-2022 22:25
77892549,azure openai and token limit,"i want to use gpt model to analyze my data. data is a suite of records (e.g. 1000 records) with 10 or even more properties. i want to say gpt (or other model):

""please, analyze this data and find and exceptions, extremums etc.
anything, what is different than common""

i use azure.ai.openai nuget package 
when i try model ""gpt-35-turbo with the following code:
        var chatcompletionsoptions = new chatcompletionsoptions()
        {
            deploymentname = ""gpt-35-turbo"", // use deploymentname for ""model"" with non-azure clients
            messages =
            {
                new chatrequestsystemmessage(""you are data specialist""),
                new chatrequestusermessage(@""analyze this data and find exceptions""),
                new chatrequestusermessage(stringbuilder.tostring())
            }
        };

        response<chatcompletions> aichatresponse = await _openaiclient.getchatcompletionsasync(chatcompletionsoptions);
        chatresponsemessage responsechatmessage = aichatresponse.value.choices[0].message;

where stringbuilder has jsonl model with 1000 records and even 2 columns
i get
{
  ""error"": {
    ""message"": ""this model's maximum context length is 8192 tokens. however, your messages resulted in 17901 tokens. please reduce the length of the messages."",
    ""type"": ""invalid_request_error"",
    ""param"": ""messages"",
    ""code"": ""context_length_exceeded""
  }
}

so, as we can see, limitation is small to analyze data via chat
when i try to use model text-embedding-ada-002:
        embeddingsoptions embeddingsoptions = new(""text-embedding-ada-002"", strings);
        response<embeddings> responseembeddings = await _openaiclient.getembeddingsasync(embeddingsoptions);

        embeddingitem eitem = responseembeddings.value.data[0];
        readonlymemory<float> embedding = eitem.embedding;

but it being executed long time and i cancelled it for cost increasing :)
with 10 records it returns only number list...
added #1
e.g. i have list of the people and all of them from chicago, except 2, which are from other cities. or most of them has salary approx $100000 per year, but some of them has $10000 (much less) and $100000 (much more, than approx). or any other different exceptions and deviations, i don't know which, because otherwise i can develop it directly. i want to have ability to analyze all data as model and find anything (probably, not only by one parameter, probably, linked parameters). and, even find relations inside data, between one parameter from another (e.g. salary in new york much more, that city x). there are only examples, main goal - i don't know which concrete relations and exceptions, ai should point me it
how to solve my task?","['.net', 'azure', 'openai-api', 'chatgpt-api', 'chat-gpt-4']",77893021,"i'll try to answer questions to the issues you are facing. hopefully it will give you some ideas about how to proceed.
first, regarding text-embedding-ada-002 model, you cannot use it for chat completions. it is used when you want to create vector representation of the data and feed to a vector database.
regarding the error you are getting for token length with gpt 3.5 model, it is expected as the max token sizes (also known as context size i.e. the amount of data that can be sent to azure openai) are fixed for a model and you cannot exceed them. a few things you can do to take care of this error:

use a model that supports larger context length. for example, you can use gpt-35-turbo-16k which supports 16k context size (double the context size of the model you are currently using) or gpt-4-32k (four time the context size of the model you are currently using). this will allow you to pass larger text to the model for comprehension. however, please note that the cost for gpt-4 models are significantly higher than gpt-3.5 models so please also take that into consideration.
use prompt patterns. one possible solution would be make use of map-reduce kind of pattern where you chunk your data in smaller pieces and send each chunk to llm with your question. once you get response for each chunk, send the responses back to llm to comprehend and come up with a final answer (without knowing details about the business problem you are trying to solve, i am not 100% sure that if this would solve your problem though so you may want to try it out).",https://stackoverflow.com/questions/77892549,.net,27-01-2024 19:37,4244.0,0.0,1.0,True,27-01-2024 22:22,27-01-2024 21:54
56010551,pytorch embedding index out of range,"i'm following this tutorial here  in there a neural model is created, using nn.module, with an embedding layer, which is initialized here
self.embedding = nn.embedding(params['vocab_size'], params['embedding_dim'])

vocab_size is the total number of training samples, which is 4000.  embedding_dim is 50.  the relevant piece of the forward method is below
def forward(self, s):
        # apply the embedding layer that maps each token to its embedding
        s = self.embedding(s)   # dim: batch_size x batch_max_len x embedding_dim

i get this exception when passing a batch to the model like so
model(train_batch)
train_batch is a numpy array of dimension batch_sizexbatch_max_len.  each sample is a sentence, and each sentence is padded so that it has the length of the longest sentence in the batch.

file
  ""/users/liam_adams/documents/cs512/research_project/custom/model.py"",
  line 34, in forward
      s = self.embedding(s)   # dim: batch_size x batch_max_len x embedding_dim   file
  ""/users/liam_adams/documents/cs512/venv_research/lib/python3.7/site-packages/torch/nn/modules/module.py"",
  line 493, in call
      result = self.forward(*input, **kwargs)   file ""/users/liam_adams/documents/cs512/venv_research/lib/python3.7/site-packages/torch/nn/modules/sparse.py"",
  line 117, in forward
      self.norm_type, self.scale_grad_by_freq, self.sparse)   file ""/users/liam_adams/documents/cs512/venv_research/lib/python3.7/site-packages/torch/nn/functional.py"",
  line 1506, in embedding
      return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) runtimeerror: index out of range at
  ../aten/src/th/generic/thtensorevenmoremath.cpp:193

is the problem here that the embedding is initialized with different dimensions than those of my batch array?  my batch_size will be constant but batch_max_len will change with every batch. this is how its done in the tutorial.","['python', 'neural-network', 'nlp', 'pytorch', 'recurrent-neural-network']",56010862,"found the answer here 
i'm converting words to indexes, but i had the indexes based off the total number of words, not vocab_size which is a smaller set of the most frequent words.",https://stackoverflow.com/questions/56010551,python,06-05-2019 18:28,36386.0,13.0,4.0,True,20-02-2023 13:11,06-05-2019 18:58
71396540,how to save and load custom siamese bert model,"i am following this tutorial on how to train a siamese bert network:

all good, but i am not sure what is the best way to save the model after train it and save it.
any suggestion?
i was trying with
model.save('models/bert_siamese_v1')
which creates a folder with save_model.bp keras_metadata.bp and two subfolders (variables and assets)
then i try to load it with:
model.load_weights('models/bert_siamese_v1/')

and it gives me this error:
2022-03-08 14:11:52.567762: w tensorflow/core/util/tensor_slice_reader.cc:95] could not open models/bert_siamese_v1/: failed precondition: models/bert_siamese_v1; is a directory: perhaps your file is in a different file format and you need to use a different restore operator?

what is the best way to proceed?","['python', 'tensorflow', 'keras', 'deep-learning', 'bert-language-model']",71397026,"try using tf.saved_model.save to save your model:
tf.saved_model.save(model, 'models/bert_siamese_v1')
model = tf.saved_model.load('models/bert_siamese_v1')

the warning you get during saving can apparently be ignored. after loading your model, you can use it for inference f(test_data):
f = model.signatures[""serving_default""]
x1 = tf.random.uniform((1, 128), maxval=100, dtype=tf.int32)
x2 = tf.random.uniform((1, 128), maxval=100, dtype=tf.int32)
x3 = tf.random.uniform((1, 128), maxval=100, dtype=tf.int32)
print(f)
print(f(attention_masks = x1, input_ids = x2, token_type_ids = x3))

concretefunction signature_wrapper(*, token_type_ids, attention_masks, input_ids)
  args:
    attention_masks: int32 tensor, shape=(none, 128)
    input_ids: int32 tensor, shape=(none, 128)
    token_type_ids: int32 tensor, shape=(none, 128)
  returns:
    {'dense': <1>}
      <1>: float32 tensor, shape=(none, 3)
{'dense': <tf.tensor: shape=(1, 3), dtype=float32, numpy=array([[0.40711606, 0.13456087, 0.45832306]], dtype=float32)>}",https://stackoverflow.com/questions/71396540,python,08-03-2022 14:20,1909.0,3.0,2.0,True,09-03-2022 10:34,08-03-2022 16:18
70056791,normalization words for sentiment analysis,"i'm currently doing sentiment analysis and having a problem.
i have a big normalization for word and i want to normalization text before tokenize like this example:




data
normal




kamu knp sayang
kamu kenapa sayang


drpd sedih mending belajar
dari pada sedih mending belajar


dmna sekarang
di mana sekarang





knp: kenapa
drpd: dari pada
dmna: di mana

this is my code:
import pandas as pd

slang = pd.dataframe({'before': ['knp', 'dmna', 'drpd'], 'after': ['kenapa', 'di mana', 'dari pada']})
df = pd.dataframe({'data': ['kamu knp sayang', 'drpd sedih mending bermain']})
                  
normalisasi = {}

for index, row in slang.iterrows():
  if row[0] not in normalisasi:
    normalisasi[row[0]] = row[1]


def normalized_term(document):
    return [normalisasi[term] if term in normalisasi else term for term in document]

df['normal'] = df['data'].apply(normalized_term)
df

but, the result like this:
result
i want the result like the example table.","['python', 'pandas', 'nlp']",70057142,"there is a utility named str.replace in pandas that allows us to replace a substring with another or even find/replace patterns. you can find full documentation here. your desired output would have appeared like this:
update
there were two things wrong with the answer:

you must only replace in whole word mode, not subword
after each entry in the slang file you must keep the changes not discard them


so it would be like this:
import pandas as pd
df = pd.read_excel('data bersih.xlsx')
slang = pd.read_excel('slang.xlsx')
df['normal'] = df.text
for idx, row in slang.iterrows():
    df['normal'] = df.normal.str.replace(r""\b""+row['before']+r""\b"", row['after'], regex=true)

output:
                                                   text  \
0                           hari ini udh mulai ppkm yaa   
1     mohon info apakah pgs pasar turi selama ppkm b...   
2     di rumah aja soalnya lagi ppkm entah bakal nga...   
3     pangkal penanganan pandemi di indonesia yang t...   
4                              ppkm mikro anjingggggggg   
...                                                 ...   
9808  drpd nonton sinetron mending bagi duit kayak g...   
9809                     ppkm pelan pelan kalau masukin   
9810  masih ada kepala desa camat bahkan kepala daer...   
9811            aku suka ppkm tapi tanpa pp di depannya   
9812  menteri ini perlu tidak dibayarkan gajinya set...   

                                                 normal  
0                         hari ini sudah mulai ppkm yaa  
1     mohon informasi apakah pgs pasar turi selama p...  
2     di rumah saja soalnya lagi ppkm entah bakal se...  
3     pangkal penanganan pandemi di indonesia yang t...  
4                              ppkm mikro anjingggggggg  
...                                                 ...  
9808  dari pada nonton sinema elektronik lebih baik ...  
9809                    ppkm pelan pelan kalau masukkan  
9810  masih ada kepala desa camat bahkan kepala daer...  
9811  aku suka ppkm tapi tanpa pulang pergi di depannya  
9812  menteri ini perlu tidak dibayarkan gajinya set...  

[9813 rows x 2 columns]",https://stackoverflow.com/questions/70056791,python,21-11-2021 17:08,541.0,0.0,1.0,True,23-11-2021 13:11,21-11-2021 17:13
1639855,pos tagging in german,"i am using nltk to extract nouns from a text-string starting with the following command:
tagged_text = nltk.pos_tag(nltk.text(nltk.word_tokenize(some_string)))

it works fine in english. is there an easy way to make it work for german as well? 
(i have no experience with natural language programming, but i managed to use the python nltk library which is great so far.)","['python', 'nlp', 'nltk']",1640001,"natural language software does its magic by leveraging corpora and the statistics they provide. you'll need to tell nltk about some german corpus to help it tokenize german correctly. i believe the europarl corpus might help get you going. 
see nltk.corpus.europarl_raw and this answer for example configuration.
also, consider tagging this question with ""nlp"".",https://stackoverflow.com/questions/1639855,python,28-10-2009 20:17,23832.0,28.0,6.0,True,07-03-2022 18:47,26-11-2015 16:25
78943401,fine-tuning a pretrained model with quantization and amp: scaler error &quot;attempting to unscale fp16 gradients&quot;,"i am trying to fine-tune a pretrained model with limited vram. to achieve this, i am using quantization and automatic mixed precision (amp). however, i am encountering an issue that i can't seem to resolve. could you please help me identify the problem?
here is a minimal example:
import os
from transformers import bitsandbytesconfig, optforcausallm, gpt2tokenizerfast
import torch
from torch.cuda.amp import gradscaler, autocast

model_name = ""facebook/opt-1.3b""
cache_dir = './models'
os.environ[""cuda_visible_devices""] = ""7""

quantization_config = bitsandbytesconfig(
    load_in_4bit=true,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

pretrained_model:optforcausallm = optforcausallm.from_pretrained(model_name, 
                                                    cache_dir=cache_dir,                                                     
                                                    quantization_config=quantization_config)
tokenizer:gpt2tokenizerfast = gpt2tokenizerfast.from_pretrained(model_name,
                                                    cache_dir=cache_dir)
optimizer = torch.optim.adamw(pretrained_model.parameters(), lr=1e-4)
scaler = gradscaler()
input_ids = torch.longtensor([[0, 1, 2, 3]]).to(0)
labels = torch.longtensor([[1, 2, 3, 4]]).to(0)
with torch.autocast(device_type='cuda'):
    out = pretrained_model(input_ids=input_ids, labels=labels)
    loss = out.loss
scaler.scale(out.loss).backward()
scaler.step(optimizer) 
scaler.update()
optimizer.zero_grad()

print(f'end')

at the line scaler.step(optimizer), an error occurs:
exception has occurred: valueerror: attempting to unscale fp16 gradients.","['python', 'pytorch', 'nlp', 'huggingface-transformers', 'fine-tuning']",78945455,"you can't fine-tune a fp16/uint8 model with amp. amp uses fp32 parameters. the params are autocast to fp16 for the forward pass, but amp expects the master set of parameters to be fp32.
you also shouldn't fine-tune a quantized model in the first place. the quantization causes all sorts of numerical issues and instability during training.
what you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model. you can find more details here",https://stackoverflow.com/questions/78943401,python,03-09-2024 08:38,522.0,1.0,1.0,True,25-02-2025 22:39,25-02-2025 22:39
40840731,valueerror: cannot compute lda over an empty collection (no terms),"getting this error in python when trying to compute lda for a smaller size of corpus but works fine in other cases.
the size of corpus is 15 and i tried setting the number of topic to 5 then reduced it to 2 but it still gives the same error : valueerror: cannot compute lda over an empty collection (no terms)
getting error at this line :     lda = models.ldamodel(corpus, num_topics=topic_number, id2word=dictionary, passes=passes)
where corpus is corpus = [dictionary.doc2bow(text) for a, id, text, s_date, e_date, qd, qd_perc in texts]
why is it giving no terms?","['python', 'gensim', 'lda', 'topic-modeling']",40842347,"finally figured it out. the issue with small documents is that if you try to filter the extremes from dictionary, you might end up with empty lists in corpus.corpus = [dictionary.doc2bow(text)].
so the values of parameters in dictionary.filter_extremes(no_below=2, no_above=0.1) needs to be selected accordingly and carefully before corpus = [dictionary.doc2bow(text)]
i just removed the filter extremes and lda model runs fine now. though i will change the parameter values in filter extreme and use it later.",https://stackoverflow.com/questions/40840731,python,28-11-2016 09:18,7327.0,2.0,1.0,True,03-03-2022 06:14,03-03-2022 06:14
77864121,how to split delimiter with numbers,"i am using openai to get subject lines, choices returning string not array/object, i need to split string by numbers.
var choices = ""1. ""don't miss out: exclusive deals inside!"" 2. ""hurry! limited time offer inside"" 3. ""unlock savings: open this email now"" 4. ""last chance to save big"""";
choices = choices.split('/[0-9]+\./');
console.log(choices)

this is not working for me. its showing all string into single array index.
i needed output as
[""don't miss out: exclusive deals inside!"",""hurry! limited time offer inside"",""unlock savings: open this email now"",""last chance to save big""]","['jquery', 'openai-api']",77864155,"define the regexp without quotes:
const choices = `1. ""don't miss out: exclusive deals inside!"" 2. ""hurry! limited time offer inside"" 3. ""unlock savings: open this email now"" 4. ""last chance to save big""`;
const choicesarr = choices.split(/\d+\./);
console.log(choicesarr);

edit: changed regexp to use \d according to roko suggestion.",https://stackoverflow.com/questions/77864121,jquery,23-01-2024 05:23,84.0,-4.0,2.0,True,28-03-2024 04:30,28-03-2024 04:30
53212374,how to get token ids using spacy (i want to map a text sentence to sequence of integers),"i want to use spacy to tokenize sentences to get a sequence of integer token-ids that i can use for downstream tasks. i expect to use it something like below. please fill in ???
import spacy

# load english tokenizer, tagger, parser, ner and word vectors
nlp = spacy.load('en_core_web_lg')

# process whole documents
text = (u""when sebastian thrun started working on self-driving cars at "")

doc = nlp(text)

idxs = ???

print(idxs)

i want the output to be something like:

array([ 8045, 70727, 24304, 96127, 44091, 37596, 24524, 35224, 36253])

preferably the integers refers to some special embedding id in en_core_web_lg..
spacy.io/usage/vectors-similarity does not give a hint what attribute in doc to look for.
i asked this on crossvalidated but it was determined as ot. proper terms for googling/describing this problem is also helpful.","['nlp', 'spacy', 'word-embedding']",53223349,"solution;
import spacy
nlp = spacy.load('en_core_web_md')
text = (u""when sebastian thrun started working on self-driving cars at "")

doc = nlp(text)

ids = []
for token in doc:
    if token.has_vector:
        id = nlp.vocab.vectors.key2row[token.norm]
    else:
        id = none
    ids.append(id)

print([token for token in doc])
print(ids)
#>> [when, sebastian, thrun, started, working, on, self, -, driving, cars, at]
#>> [71, 19994, none, 369, 422, 19, 587, 32, 1169, 1153, 41]

breaking this down;
# a vocabulary for which __getitem__ can take a chunk of text and returns a hash
nlp.vocab 
# >>  <spacy.vocab.vocab at 0x12bcdce48>
nlp.vocab['hello'].norm # hash
# >> 5983625672228268878


# the tensor holding the word-vector
nlp.vocab.vectors.data.shape
# >> (20000, 300)

# a dict mapping hash -> row in this array
nlp.vocab.vectors.key2row
# >> {12646065887601541794: 0,
# >>  2593208677638477497: 1,
# >>  ...}

# so to get int id of 'earth'; 
i = nlp.vocab.vectors.key2row[nlp.vocab['earth'].norm]
nlp.vocab.vectors.data[i]

# note that tokens have hashes but may not have vector
# (hence no entry in .key2row)
nlp.vocab['thrun'].has_vector
# >> false",https://stackoverflow.com/questions/53212374,nlp,08-11-2018 16:45,3908.0,5.0,2.0,True,25-11-2023 12:21,25-11-2023 12:21
75898276,"openai api error 429: &quot;you exceeded your current quota, please check your plan and billing details&quot;","i'm making a python script to use openai via its api. however, i'm getting this error:

openai.error.ratelimiterror: you exceeded your current quota, please check your plan and billing details

my script is the following:
#!/usr/bin/env python3.8
# -*- coding: utf-8 -*-

import openai
openai.api_key = ""<my pai key>""

completion = openai.chatcompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
    {""role"": ""user"", ""content"": ""tell the world about the chatgpt api in the style of a pirate.""}
  ]
)

print(completion.choices[0].message.content)

i'm declaring the shebang python3.8, because i'm using pyenv. i think it should work, since i did 0 api requests, so i'm assuming there's an error in my code.","['python', 'prompt', 'openai-api', 'completion', 'chatgpt-api']",75898717,"tl;dr: you need to upgrade to a paid plan. set up a paid account, add a credit or debit card, and generate a new api key if your old one was generated before the upgrade. it might take 10 minutes or so after you upgrade to a paid plan before the paid account becomes active and the error disappears.
problem
as stated in the official openai documentation:



type
overview




ratelimiterror
cause: you have hit your assigned rate limit. solution: pace your requests. read more in our rate limit guide.



also, read more about error code 429 - you exceeded your current quota, please check your plan and billing details:

this (i.e., 429) error message indicates that you have hit your maximum monthly
spend (hard limit) for the api. this means that you have consumed all
the credits or units allocated to your plan and have reached the limit
of your billing cycle. this could happen for several reasons, such as:

you are using a high-volume or complex service that consumes a lot of credits or units per request.

you are using a large or diverse data set that requires a lot of requests to process.

your limit is set too low for your organizationï¿½ï¿½ï¿½s usage.




did you sign up some time ago?
you're getting error 429 because either you used all your free tokens or 3 months have passed since you signed up.
as stated in the official openai article:

to explore and experiment with the api, all new users get $5
worth of free tokens. these tokens expire after 3 months.
after the quota has passed you can choose to enter billing information
to upgrade to a paid plan and continue your use of the api on
pay-as-you-go basis. if no billing information is entered you will
still have login access, but will be unable to make any further api
requests.
please see the pricing page for the latest information on
pay-as-you-go pricing.

note: if you signed up earlier (e.g., in december 2022), you got $18 worth of free tokens.
check your api usage in the usage dashboard.
for example, my free trial expires tomorrow and this is what i see right now in the usage dashboard:

this is how my dashboard looks after expiration:

if i run a simple script after my free trial has expired, i get the following error:

openai.error.ratelimiterror: you exceeded your current quota, please check your plan and billing details.


did you create your second openai account?
you're getting error 429 because you created a second openai account with the same phone number. it seems like free credit is given based on phone numbers.
as explained on the official openai forum by @sapphirefelinebytes:

i created an open ai account in november and my $18 credits expired on
march 1st. so, like many of you here, i tried creating a new account
with a different email address, but same number. they gave me $0
credits.
i tried now with a different phone number and email. this time i got
$5 credits.

it's confirmed that free credit is given based on phone numbers, as explained on the official openai forum by @logankilpatrick:

also note, you only get free credits for the first account associated
with your phone number. subsequent accounts are not granted free credits.

solution
try to do the following:

set up paid account.
add a credit or debit card.
generate a new api key if your old api key was generated before you upgraded to the paid plan.

when you upgrade to a paid plan, don't expect the error to disappear immediately, as @dcferreira mentioned in the comment above. it might take a few minutes to more than an hour after the upgrade before the error disappears.
in the comment below, @joemornin confirmed that it took 10 minutes for his paid account to become active. in the meantime, he was getting the following error:

you've reached your usage limit. see your usage dashboard and billing settings for more details. if you have further questions, please contact us through our help center at help.openai.com.",https://stackoverflow.com/questions/75898276,python,31-03-2023 11:58,433511.0,176.0,5.0,True,10-01-2025 04:16,03-01-2024 15:06
70060847,how to work with openai maximum context length is 2049 tokens?,"i'd like to send the text from various pdf's to openai's api. specifically the summarize for a 2nd grader or the tl;dr summarization api's.
i can extract the text from pdf's using pymupdf and prepare the openai prompt.
question: how best to prepare the prompt when the token count is longer than the allowed 2049?

do i just truncate the text then send multiple requests?
or is there a way to sample the text to ""compress"" it to lose key points?","['python', 'openai-api']",74955497,"i faced the same problem. here is the strategy i used to send text that is much, much longer than openais gpt3 token limit.
depending on the model (davinci, curie, etc.) used, requests can use up to 4097 tokens shared between prompt and completion.

prompt being the input you send to openai, i.e. your ""command"", e.g. ""summarize the following text"" plus the text itself
completion being the response, i.e. the entire summary of your text

if your prompt is 4000 tokens, your completion can be 97 tokens at most. for more information on openai tokens and how to count them, see here.
to ensure that we donï¿½ï¿½ï¿½t exceed the maximum length limit for prompt plus completion, we need to ensure that prompt (i.e. your text) and completion (i.e. the summary) put together always fits into the 4097 token boundary.
for that reason we split the entire text into multiple text chunks, summarize each chunk independently and finally merge all summarized chunks using a simple "" "".join() function.
maximum number of words - token-to-word conversion
openai has a fixed limit on the number of tokens. however, a token is not the same as a word. hence, we first need to calculate the maximum number of words we can send to openai. the documentation says:

given the token-to-word ratio, we can send approximately 2900 words to openai's gpt3 assuming a 5 sentence summary per text chunk.

max tokens per request: 4000 tokens (leaving 97 tokens as a safety buffer) = 3000 words
max prompt tokens: ï¿½ï¿½ï¿½summarize the following text in five sentencesï¿½ï¿½ï¿½ has 7 words = 10 tokens
max tokens of returned summary (5 sentences): 20 words per sentence. 5 * 20 = 100 words = 133 tokens
max tokens of text chunk: 4000 - 10 - 133 = 3857 tokens = 2900 words

text chunking
we can choose from a plethora of strategies to split up the entire text into smaller chunks.
the simplest approach is creating a single list of all words by splitting the entire text on whitespaces, and then creating buckets of words with words evenly distributed across all buckets. the downside is that we are likely to split a sentence half-way through and lose the meaning of the sentence because gpt ends ug the first half of the sentence independently from the second half ï¿½ï¿½ï¿½ ignoring any relations between the two chunks.
other options include tokenizers such as sentencepiece and spacyï¿½ï¿½ï¿½s sentence splitter. choosing the later generates the most stable results.
implementation of text chunking with spacy
the following example splits the text ï¿½ï¿½ï¿½my first birthday was great. my 2. was even better.ï¿½ï¿½ï¿½ into a list of two se""lang-bash prettyprint-override"">python -m spacy download en_core_web_sm

import spacy
from spacy.lang.en import english

nlp = spacy.load(""en_core_web_sm"")

text = ""my first birthday was great. my 2. was even better.""
    
for sentence in nlp(text).sents:
  print(sentence.text)

output
my first birthday was great.
my 2. was even better.

spacy correctly detected the second sentence instead of splitting it after the ï¿½ï¿½ï¿½2.ï¿½ï¿½ï¿½.
now, letï¿½ï¿½ï¿½s write a text_to_chunks helper function to generate chunks of sentences where each chunk holds at most 2700 words. 2900 words was the initially calculated word limit, but we want to ensure to have enough buffer for words that are longer than 1.33 token""lang-py prettyprint-override"">def text_to_chunks(text):
  chunks = [[]]
  chunk_total_words = 0

  sentences = nlp(text)

  for sentence in sentences.sents:
    chunk_total_words += len(sentence.text.split("" ""))

    if chunk_total_words > 2700:
      chunks.append([])
      chunk_total_words = len(sentence.text.split("" ""))

    chunks[len(chunks)-1].append(sentence.text)
  
  return chunks


an alternative approach to determine the number of tokens of a text was recently introduced by openai. the approach uses tiktoken and is tailored towards openai's models.
import tiktoken

encoding = tiktoken.encoding_for_model(""gpt-3.5-turbo"")
number_of_tokens = len(encoding.encode(""tiktoken is great!""))
print(number_of_tokens)


next, we wrap the text summarization logic into a summarize_text function.
def summarize_text(text):
  prompt = f""summarize the following text in 5 sentences:\n{text}""

  response = openai.completion.create(
      engine=""text-davinci-003"", 
      prompt=prompt,
      temperature=0.3, 
      max_tokens=150, # = 112 words
      top_p=1, 
      frequency_penalty=0,
      presence_penalty=1
  )

  return response[""choices""][0][""text""]

our final piece of code looks like this:
chunks = text_to_chunks(one_large_text)

chunk_summaries = []

for chunk in chunks:
  chunk_summary = summarize_text("" "".join(chunk))
  chunk_summaries.append(chunk_summary)

summary = "" "".join(chunk_summaries)

references

how to count tokens with tiktoken, openai cookbook",https://stackoverflow.com/questions/70060847,python,22-11-2021 04:19,52267.0,25.0,4.0,True,09-12-2024 14:00,29-04-2022 15:35
68573795,using spacy to extract tensor by token id,"i'm using spacy 3.0 to vectorize a text with a transformer model. due to data privacy reason the vectorization has to be on a different machine than the one that trains the model. to reduce the amount of data i generate and would have to transfer between machines, i extract the token ids of the text like this:
import spacy
nlp = spacy.load(""de_dep_news_trf"")
doc = nlp(""eine bank steht im park."")
print(doc._.trf_data.tokens[""input_ids""])

which returns
tensor([[    3,   917,  2565,  1302,   106,  3087, 26914,     4]])

having the ids now, is it possible to extract the correct tensors from the language model (de_dep_news_trf) using spacy?","['python', 'nlp', 'spacy', 'transformer-model', 'spacy-3']",68576309,"unfortunately, this is not possible. the problem is that transformer models generate their embeddings for individual tokens on the context. meaning, if you have he same token_id in two different sentences, they will likely have a (significantly) different embedding. the only way is to return the tensor associated with each of the tokens, but you cannot generate them solely based on the input_ids.",https://stackoverflow.com/questions/68573795,python,29-07-2021 09:54,423.0,1.0,1.0,True,29-07-2021 12:53,29-07-2021 12:52
61913010,can not import pipeline from transformers,"i have installed pytorch with conda and transformers with pip.
i can import transformers without a problem but when i try to import pipeline from transformers i get an exception:
from transformers import pipeline
---------------------------------------------------------------------------
importerror                               traceback (most recent call last)
<ipython-input-4-69a9fd07ccac> in <module>
----> 1 from transformers import pipeline

importerror: cannot import name 'pipeline' from 'transformers' (c:\users\alienware\anaconda3\envs\tf2\lib\site-packages\transformers\__init__.py)

this is a view of the directory where it searches for the init.py file:

what is causing the problem and how can i resolve it?","['python', 'python-3.x', 'pipeline', 'huggingface-transformers', 'anaconda3']",61913108,"check transformers version. make sure you are on latest. pipelines were introduced quite recently, you may have older version.",https://stackoverflow.com/questions/61913010,python,20-05-2020 12:20,45988.0,10.0,3.0,True,06-04-2022 11:01,21-05-2020 07:40
77896681,efficient many-to-many embedding comparisons,"i am trying to recommend a user the top ""articles"" given embeddings of ""interests"" they have.
each ""user"" will have 5-10 embeddings associated with their profile, represented as arrays of doubles.
each ""article"" will also have 5-10 embeddings associated with it (each embedding represents a distinct topic).
i want to write a postgresql query that returns the top 20 ""articles"" that are most aligned to a users interests. since each user 5-10 embeddings representing their interests and each article has 5-10 embeddings representing the content it covers, i can't trivially apply an extension like pgvector to solve this issue.
i wrote an algorithm in sql where i compute pairwise similarities between user embeddings and article embeddings and then take a max along each row and then average those values. it helps to imagine a uxt matrix (where u represents number of user embeddings and t is article embeddings) and fill each entry by the cosine similarity between the corresponding user embedding and articles embedding.
i wrote helper functions to compute products between two arrays, another to compute cosine similarity, and a third to compute ""vectors_similarity"" -- which computes the similarity between a set of user vectors and a set of article vectors.
the query itself applies a few joins to get the required information, filters out for articles in the last ten days and articles that have already been ""read"" by the user, and returns the top 20 most similar articles using this methodology.
this takes over 30sec to search through 1000 articles. i am not a sql expert, and am struggling to debug this. below, i have posted my sql query and the results of ""explain analysis.
is this just computationally intractable, or am i missing some obvious optimization opportunities?
create or replace function array_product(arr double precision[])
returns double precision as
$$
declare
    result double precision := 1;
    i integer;
begin
    for i in array_lower(arr, 1) .. array_upper(arr, 1)
    loop
        result := result * arr[i];
    end loop;
    return result;
end;
$$
language plpgsql;

create or replace function cosine_similarity(a double precision[], b double precision[])
returns double precision as $$
declare
    dot_product double precision;
    norm_a double precision;
    norm_b double precision;
    a_length int;
    b_length int;
begin
    a_length := array_length(a, 1);
    b_length := array_length(b, 1);

    dot_product := 0;
    norm_a := 0;
    norm_b := 0;
    
    for i in 1..a_length loop
        dot_product := dot_product + a[i] * b[i];
        norm_a := norm_a + a[i] * a[i];
        norm_b := norm_b + b[i] * b[i];
    end loop;

    norm_a := sqrt(norm_a);
    norm_b := sqrt(norm_b);

    if norm_a = 0 or norm_b = 0 then
        return 0;
    else
        return dot_product / (norm_a * norm_b);
    end if;
end;
$$ language plpgsql;

create or replace function vectors_similarity(
    user_vectors float[][],
    article_vectors float[][]
) returns float as $$
declare
    num_user_vectors int;
    num_article_vectors int;
    scores float[][];
    row_weights float[];
    row_values float[];
    col_weights float[];
    similarity float;
    article_vector float[][];
    user_vector float[][];
    i int;
    j int;
begin
    num_user_vectors := array_length(user_vectors, 1);
    num_article_vectors := array_length(article_vectors, 1);

    scores := array(select array(select 0.0 from generate_series(1, num_article_vectors)) from generate_series(1, num_user_vectors));
    
    i := 1;
    foreach user_vector slice 1 in array user_vectors
    loop
        j := 1;
        foreach article_vector slice 1 in array article_vectors
        loop
            scores[i][j] := cosine_similarity(user_vector, article_vector);
            scores[i][j] := exp(scores[i][j] * 7);
        j := j+1;
        end loop;
    i := i + 1;
    end loop;
        
    select 
      avg(
        (select max(row_val) from unnest(row_array) as row_val)
      ) into similarity
    from 
      (
        select scores[row_index][:] as row_array
        from generate_series(1, array_length(scores, 1)) as row_index
      ) as subquery;
    
    return similarity;
end;
$$ language plpgsql;

explain analyze
select
        art.*,
        vectors_similarity(array_agg(topic.vector), array[array[ -0.0026961329858750105,0.004657252691686153, -0.011298391036689281, ...], array[...]]) as similatory_score
    from
        article art     
    join
        article_topic art_top on art.id = art_top.article_id
    join
        topic topic on art_top.topic_id = topic.id
    where
        art.date_published > current_date - interval '5' day
        and not exists (
        select 1
        from user_article_read usr_art_read
        where usr_art_read.article_id = art.id
        and usr_art_read.profile_id = 1 -- :user_id to be inputted by the user with the actual user_id
        )
    group by
        art.id
    order by
        similatory_score desc, art.date_published desc, art.id desc
    limit 20;

and here is the analysis:
""limit  (cost=945.53..945.55 rows=5 width=518) (actual time=27873.197..27873.227 rows=5 loops=1)""
""  output: art.id, art.created_timestamp, art.modified_timestamp, art.title, art.url, art.date_published, art.summary, art.author, art.thumbnail_url, art.thumbnail_credit, art.source_name, art.feed_type, art.source_type, (scaled_geometric_similarity_vectors(array_agg(topic.vector), '{{-0.0026961329858750105,...}, {...}, ...}'::double precision[])""
""              group key: art.id""
""              batches: 25  memory usage: 8524kb  disk usage: 3400kb""
""              buffers: shared hit=14535 read=19, temp read=401 written=750""
""              ->  hash join  (cost=395.19..687.79 rows=4491 width=528) (actual time=6.746..20.875 rows=4638 loops=1)""
""                    output: art.id, art.created_timestamp, art.modified_timestamp, art.title, art.url, art.date_published, art.summary, art.author, art.thumbnail_url, art.thumbnail_credit, art.source_name, art.feed_type, art.source_type, topic.vector""
""                    inner unique: true""
""                    hash cond: (art_top.topic_id = topic.id)""
""                    buffers: shared hit=289""
""                    ->  hash anti join  (cost=202.53..483.33 rows=4491 width=518) (actual time=3.190..15.589 rows=4638 loops=1)""
""                          output: art.id, art.created_timestamp, art.modified_timestamp, art.title, art.url, art.date_published, art.summary, art.author, art.thumbnail_url, art.thumbnail_credit, art.source_name, art.feed_type, art.source_type, art_top.topic_id""
""                          hash cond: (art.id = usr_art_read.article_id)""
""                          buffers: shared hit=229""
""                          ->  hash join  (cost=188.09..412.13 rows=4506 width=518) (actual time=3.106..14.853 rows=4638 loops=1)""
""                                output: art.id, art.created_timestamp, art.modified_timestamp, art.title, art.url, art.date_published, art.summary, art.author, art.thumbnail_url, art.thumbnail_credit, art.source_name, art.feed_type, art.source_type, art_top.topic_id""
""                                inner unique: true""
""                                hash cond: (art_top.article_id = art.id)""
""                                buffers: shared hit=224""
""                                ->  seq scan on public.article_topic art_top  (cost=0.00..194.67 rows=11167 width=16) (actual time=0.018..7.589 rows=11178 loops=1)""
""                                      output: art_top.id, art_top.created_timestamp, art_top.article_id, art_top.topic_id""
""                                      buffers: shared hit=83""
""                                ->  hash  (cost=177.56..177.56 rows=843 width=510) (actual time=3.005..3.011 rows=818 loops=1)""
""                                      output: art.id, art.created_timestamp, art.modified_timestamp, art.title, art.url, art.date_published, art.summary, art.author, art.thumbnail_url, art.thumbnail_credit, art.source_name, art.feed_type, art.source_type""
""                                      buckets: 1024  batches: 1  memory usage: 433kb""
""                                      buffers: shared hit=141""
""                                      ->  seq scan on public.article art  (cost=0.00..177.56 rows=843 width=510) (actual time=0.082..1.585 rows=818 loops=1)""
""                                            output: art.id, art.created_timestamp, art.modified_timestamp, art.title, art.url, art.date_published, art.summary, art.author, art.thumbnail_url, art.thumbnail_credit, art.source_name, art.feed_type, art.source_type""
""                                            filter: (art.date_published > (current_date - '5 days'::interval day))""
""                                            rows removed by filter: 1191""
""                                            buffers: shared hit=141""
""                          ->  hash  (cost=14.35..14.35 rows=7 width=8) (actual time=0.052..0.052 rows=0 loops=1)""
""                                output: usr_art_read.article_id""
""                                buckets: 1024  batches: 1  memory usage: 8kb""
""                                buffers: shared hit=5""
""                                ->  bitmap heap scan on public.user_article_read usr_art_read  (cost=4.21..14.35 rows=7 width=8) (actual time=0.051..0.052 rows=0 loops=1)""
""                                      output: usr_art_read.article_id""
""                                      recheck cond: (usr_art_read.profile_id = 1)""
""                                      buffers: shared hit=5""
""                                      ->  bitmap index scan on user_article_read_profile_id_d4edd4f6  (cost=0.00..4.21 rows=7 width=0) (actual time=0.050..0.050 rows=0 loops=1)""
""                                            index cond: (usr_art_read.profile_id = 1)""
""                                            buffers: shared hit=5""
""                    ->  hash  (cost=118.96..118.96 rows=5896 width=26) (actual time=3.436..3.440 rows=5918 loops=1)""
""                          output: topic.vector, topic.id""
""                          buckets: 8192  batches: 1  memory usage: 434kb""
""                          buffers: shared hit=60""
""                          ->  seq scan on public.topic  (cost=0.00..118.96 rows=5896 width=26) (actual time=0.009..2.100 rows=5918 loops=1)""
""                                output: topic.vector, topic.id""
""                                buffers: shared hit=60""
""planning:""
""  buffers: shared hit=406 read=7""
""planning time: 52.507 ms""
""execution time: 27875.522 ms""","['sql', 'postgresql', 'search', 'sql-execution-plan', 'word-embedding']",77897739,"currently, almost all cost is accrued in the outer select, and that is not due to sorting. it's the hugely expensive function vectors_similarity() which calls the nested function cosine_similarity() many times, and that nested function is as inefficient as the first.
(you also show the function array_product(), but that's unused in the query, so just a distraction. also inefficient, btw.)
this part in your query plan indicates you need more work_mem:

memory usage: 8,524kb disk usage: 3,400kb


indeed, your server seems to be at default settings, or else explain(analyze, verbose, buffers, settings) (like you claim to have used) would report custom settings. that won't do for a non-trivial workload.
i started out with ""currently"", because this is only getting worse. you filter the past 5 days or your data, but don't have an index on article.date_published. currently, almost half of your 2000 articles qualify, but that ratio is bound to change dramatically. then you need an index on article (date_published).
also, your limit is combined with an order by on the computed similarity. so there is no way around computing the similarity score for all qualifying rows. a small limit barely helps.
(aside: your query plan reports rows=5 though there are more than enough candidate rows, which disagrees with limit 20 in your query.)
your course of action should be:
1.) optimize both ""similarity"" functions, most importantly cosine_similarity().
2.) reduce the number of rows for which to do the expensive calculation, maybe by pre-filtering rows with a much cheaper filter.
3.) optimize server configuration.
4.) optimize indexes.",https://stackoverflow.com/questions/77896681,sql,28-01-2024 22:38,214.0,1.0,1.0,True,29-01-2024 06:30,28-01-2024 23:14
61574364,reaching the main word from the suffix list,"i have a word like ""itibarsï¿½ï¿½zlaï¿½ï¿"".
the stem is ""itibar"" and the suffix list is  ""a, ak, ar, ï¿½ï¿½, laï¿½ï¿½, m,"".
the suffix list is missing. ""ma, tï¿½ï¿½r, ï¿½ï¿½z, i, ï¿½ï¿½, a, m, sï¿½ï¿½, mak, tï¿½ï¿½"" is the right one.
how can i to reach ""itibarsï¿½ï¿½zlaï¿½ï¿"" with suffix list in which order?
for example : itibar + suffixlist[6] -> itibarsï¿½ï¿½z
itibar + suffixlist[6] + suffixlist[5] -> itibarsï¿½ï¿½zlaï¿½ï¿½
words and suffix list changes all time. so i need a algorithm for it. i tried merge suffix one by one with stem and comparision but it is not work for all list.","['c#', 'nlp', 'morphological-analysis']",61574827,"using a dictionary with assigned suffixes to words is a way of achieving this.
this code does however needs to be tweaked to discern suffixes not in the list and suffixes that are similar (like 'a' and 'ak' in your example).
update
fixed the search pattern for the suffixes.
using system;
using system.collections.generic;
using system.linq;
                
public class program
{
    public static void main()
    {
        var words = new dictionary<string, list<string>>();
        words.add(""itibar"", new list<string>(){""ma"", ""tï¿½ï¿½r"", ""ï¿½ï¿½z"", ""i"", ""ï¿½ï¿½"", ""a"", ""m"", ""sï¿½ï¿½"", ""mak"", ""tï¿½ï¿½"", ""sï¿½ï¿½z"", ""ak"", ""ar"", ""laï¿½ï¿½""}.orderby(e => e.length).tolist());
        
        var word = ""itibarsï¿½ï¿½zlaï¿½ï¿½tï¿½ï¿½rmak"";
        
        var wordused = words.firstordefault(e =>  var suffixesusedinorder = new list<string>();
        var charstosearch = """";
        
        foreach (var character in word.substring(wordused.key.length))
        {
            var a = character.tostring();
            if (charstosearch.length > 0) 
            {
                a = charstosearch + a;  
            }
            
            if (!wordused.value.any(e => e == a) || wordused.value.count(e => e.startswith(a)) > 1)
            {
                charstosearch += character.tostring();
            }
            else 
            {
                suffixesusedinorder.add(wordused.value.firstordefault(e => e == a));
                charstosearch = """";
            }
        }
        
        console.writeline(string.join("","", suffixesusedinorder));
            
    }
}

the result of this code run: sï¿½ï¿½z,laï¿½ï¿½,tï¿½ï¿½r,mak",https://stackoverflow.com/questions/61574364,c#,03-05-2020 12:22,63.0,0.0,1.0,True,09-10-2024 21:25,09-10-2024 21:25
77667537,arm64 - how do i install sudachipy - needed for japanese spacy,"i should preface by saying that i did follow the spacy documentation to install the spacy library and the models of interest.
pip install -u pip setuptools wheel
pip install -u 'spacy[apple]'
python -m spacy download zh_core_web_sm
python -m spacy download en_core_web_sm
python -m spacy download fr_core_news_sm
python -m spacy download de_core_news_sm
python -m spacy download ja_core_news_sm
python -m spacy download es_core_news_sm

currently stuck at installing ja_core_news_sm in my docker with a python image of python:3.11-slim in my docker environment. i am installing a few other spacy pipeline modules in tandem on my arm64 docker image and this is the source of conflict. the other pipeline models are :
i get the following error:
    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½: eta 0:00:00
 building wheels for collected packages: sudachipy
   building wheel for sudachipy (pyproject.toml): started
   building wheel for sudachipy (pyproject.toml): finished with status 'error'
   error: subprocess-exited-with-error
   
   ï¿½ï¿½ building wheel for sudachipy (pyproject.toml) did not run successfully.
   ï¿½ï¿½ï¿½ exit code: 1
   ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½> [39 lines of output]
       running bdist_wheel
       running build
       running build_py
       creating build
       creating build/lib.linux-aarch64-cpython-311
       creating build/lib.linux-aarch64-cpython-311/sudachipy
       copying py_src/sudachipy/config.py -> build/lib.linux-aarch64-cpython-311/sudachipy
       copying py_src/sudachipy/__init__.py -> build/lib.linux-aarch64-cpython-311/sudachipy
       copying py_src/sudachipy/command_line.py -> build/lib.linux-aarch64-cpython-311/sudachipy
       copying py_src/sudachipy/errors.py -> build/lib.linux-aarch64-cpython-311/sudachipy
       creating build/lib.linux-aachipy/dictionary
       copying py_src/sudachipy/dictionary/__init__.py -> build/lib.linux-aarch64-cpython-311/sudachipy/dictionary
       creating build/lib.linux-aarch64-cpython-311/sudachipy/tokenizer
       copying py_src/sudachipy/tokenizer/__init__.py -> build/lib.linux-aarch64-cpython-311/sudachipy/tokenizer
       creating build/lib.linux-aarch64-cpython-311/sudachipy/morphemelist
       copying py_src/sudachipy/morphemelist/__init__.py -> build/lib.linux-aarch64-cpython-311/sudachipy/morphemelist
       creating build/lib.linux-aarch64-cpython-311/sudachipy/morpheme
       copying py_src/sudachipy/morpheme/__init__.py -> build/lib.linux-aarch64-cpython-311/sudachipy/morpheme
       copying py_src/sudachipy/sudachipy.pyi -> build/lib.linux-aarch64-cpython-311/sudachipy
       creating build/lib.linux-aarch64-cpython-311/sudachipy/resources
       copying py_src/sudachipy/resources/sudachi.json -> build/lib.linux-aarch64-cpython-311/sudachipy/resources
       copying py_src/sudachipy/resources/rewrite.def -> build/lib.linux-aarch64-cpython-311/sudachipy/resources
       copying py_src/sudachipy/resources/unk.def -> build/lib.linux-aarch64-cpython-311/sudachipy/resources
       copying py_src/sudachipy/resources/char.def -> build/lib.linux-aarch64-cpython-311/sudachipy/resources
       warning: build_py: byte-compiling is disabled, skipping.
       
       running build_ext
       running build_rust
       error: can't find rust compiler
       
       if you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. installing from the wheel would avoid the need for a rust compiler.
       
       to update pip, run:
       
           pip install --upgrade pip
       
       and then retry package installation.
       
       if you did intend to build this package from source, try installing a rust compiler from your system package manager and ensure it is on the path during installation. alternatively, rustup (available at  is the recommended way to download and update the rust compiler toolchain.
       [end of output]
   
   note: this error originates from a subprocess, and is likely not a problem with pip.
   error: failed building wheel for sudachipy
 failed to build sudachipy
 error: could not build wheels for sudachipy, which is required to install pyproject.toml-based projects

here's my dockerfile:
# use the official python image, with python 3.11
from python:3.11-slim

# set environment variables to reduce python bytecode generation and buffering
env pythonunbuffered=1 \
    pythondontwritebytecode=1

# set working directory
workdir /app

# install essential dependencies including python development headers and gcc
run apt-get update && \
    apt-get install -y --no-install-recommends \
    libpq-dev \
    gcc \
    ffmpeg \
    python3-dev \
    libc-dev \
    && apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# update pip and install python packages
copy ./docker-requirements.txt /app/
run pip install --upgrade pip && \
    pip install --no-cache-dir -r docker-requirements.txt

# copy application code to container
copy . /app

# expose the port the app runs on
expose 5000

# make the entrypoint script executable
run chmod +x /app/shell_scripts/entrypoint.sh /app/shell_scripts/wait-for-it.sh /app/shell_scripts/docker-ngrok-tunnel.sh

# define entrypoint
entrypoint [""/app/shell_scripts/entrypoint.sh""]


read further, it's an archived module.
here's the note from pypi, which explains the issue, given my mac having an m2 chip:
binary wheels
we provide binary builds for macos (10.14+), windows and linux only for x86_64 architecture. x86 32-bit architecture is not supported and is not tested. macos source builds seem to work on arm-based (aarch64) macs, but this architecture also is not tested and require installing rust toolchain and cargo.

i cannot seem to install all of the models","['python', 'docker', 'spacy', 'rust-cargo', 'arm64']",77684219,"credit to @aab for nudging in the direction of the rust compiler.
silver bullet was upgrading my es & fr spacy pipelines in addition to installing the rust compiler, because sudachipy relies on a rust compiler.
# use the official python image, with python 3.11
from python:3.11-slim

# set environment variables to reduce python bytecode generation and buffering
env pythonunbuffered=1 \
    pythondontwritebytecode=1

# set working directory
workdir /app

# install essential dependencies including python development headers and gcc
run apt-get update && \
    apt-get install -y --no-install-recommends \
    python3-dev \
    build-essential \
    git \
    libpq-dev \
    gcc \
    ffmpeg \
    libc-dev \
    curl \
    && apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# install rust
run curl --proto '= --tlsv1.2 -ssf  | sh -s -- -y
env path=""/root/.cargo/bin:${path}""

# update pip and install python packages
copy ./docker-requirements.txt /app/
run pip install --upgrade pip && \
    pip install --no-cache-dir -r docker-requirements.txt

# install cython, spacy and language models
run pip install -u pip setuptools wheel && \
    pip install -u spacy && \
    pip install --upgrade 'sudachipy>=0.6.8' && \
    python -m spacy download zh_core_web_sm && \
    python -m spacy download en_core_web_sm && \
    python -m spacy download fr_core_news_md && \
    python -m spacy download de_core_news_sm && \
    python -m spacy download es_core_news_md && \
    python -m spacy download ja_core_news_sm 

# copy application code to container
copy . /app

# expose the port the app runs on
expose 5000

# make the entrypoint script executable
run chmod +x /app/shell_scripts/entrypoint.sh /app/shell_scripts/wait-for-it.sh /app/shell_scripts/docker-ngrok-tunnel.sh

# define entrypoint
entrypoint [""/app/shell_scripts/entrypoint.sh""]",https://stackoverflow.com/questions/77667537,python,15-12-2023 16:20,1171.0,-2.0,1.0,True,19-12-2023 10:00,17-12-2023 19:49
25150316,convert numbers to english strings,"websites like  and  tries to convert a numerical string into an english strings, but they are giving natural sounding output.
for example, on 
[in]: 100456
[out]:  one hundred  thousand four hundred fifty-six

this website is a little better, 
[in]: 100456
[out]: one hundred thousand, four hundred and fifty-six

[in]: 10123124001
[out]: ten billion, one hundred and twenty-three million, one hundred and twenty-four thousand, one 

but it breaks at some point:
[in]: 10000000001
[out]: ten billion, , , one 

i've wrote my own version but it involves lots of rules and it caps at one billion, from 
import codecs

def num2word (num):
  ones = {1:""one"",2:""two"",3:""three"",4:""four"",
          5:""five"",6:""six"",7:""seven"",8:""eight"",
          9:""nine"",0:""zero"",10:""ten""}
  teens = {11:""eleven"",12:""twelve"",13:""thirteen"",
           14:""fourteen"",15:""fifteen""}
  tens = {2:""twenty"",3:""thirty"",4:""forty"",
          5:""fifty"",6:""sixty"",7:""seventy"",
          8:""eighty"",9:""ninety""}
  lens = {3:""hundred"",4:""thousand"",6:""hundred"",7:""million"",
          8:""million"", 9:""million"",10:""billion""#,13:""trillion"",11:""googol"",
          }

  if num > 999999999:
    return ""number more than 1 billion""

  # ones
  if num < 11:
    return ones[num]
  # teens
  if num < 20:
    word = ones[num%10] + ""teen"" if num > 15 else teens[num]
    return word
  # tens
  if num > 19 and num < 100:
    word = tens[int(str(num)[0])]
    if str(num)[1] == ""0"":
      return word
    else:
      word = word + "" "" + ones[num%10]
      return word

  # first digit for thousands,hundred-thousands.
  if len(str(num)) in lens and len(str(num)) != 3:
    word = ones[int(str(num)[0])] + "" "" + lens[len(str(num))]
  else:
    word = """"

  # hundred to million  
  if num < 1000000:
    # first and second digit for ten thousands.  
    if len(str(num)) == 5:
      word = num2word(int(str(num)[0:2])) + "" thousand""
    # how many hundred-thousand(s).
    if len(str(num)) == 6:
      word = word + "" "" + num2word(int(str(num)[1:3])) + \
            "" "" + lens[len(str(num))-2]
    # how many hundred(s)?
    thousand_pt = len(str(num)) - 3
    word = word + "" "" + ones[int(str(num)[thousand_pt])] + \
            "" "" + lens[len(str(num))-thousand_pt]
    # last 2 digits.
    last2 = num2word(int(str(num)[-2:]))
    if last2 != ""zero"":
      word = word + "" and "" + last2
    word = word.replace("" zero hundred"","""")
    return word.strip()

  left, right = '',''  
  # less than 1 million.
  if num < 100000000:
    left = num2word(int(str(num)[:-6])) + "" "" + lens[len(str(num))]
    right = num2word(int(str(num)[-6:]))
  # from 1 million to 1 billion.
  if num > 100000000 and num < 1000000000:
    left = num2word(int(str(num)[:3])) +  "" "" + lens[len(str(num))]
    right = num2word(int(str(num)[-6:]))
  if int(str(num)[-6:]) < 100:
    word = left + "" and "" + right
  else:  
    word = left + "" "" + right
  word = word.replace("" zero hundred"","""").replace("" zero thousand"","" thousand"")
  return word

print num2word(int(raw_input(""give me a number:\n"")))

how can i make the script i've wrote accept > billion?
is there any other way to get the same output?
can my code be written in a less verbose way?","['python', 'nlp']",25150515,"a more general approach to this problem uses repeated division (i.e. divmod) and only hardcodes the special/edge cases necessary.
for example, divmod(1034393, 1000000) -> (1, 34393), so you've effectively found the number of millions and are left with a remainder for further calculations.
possibly more illustrative example: divmod(1034393, 1000) -> (1034, 393) which allows you to take off groups of 3 decimal digits at a time from the right.
in english we tend to group digits in threes, and similar rules apply. this should be parameterized and not hard coded. for example, ""303"" could be three hundred and three million, three hundred and three thousand, or three hundred and three. the logic should be the same except for the suffix, depending on what place you're in. edit: looks like this is sort of there due to recursion.
here is a partial example of the kind of approach i mean, using a generator and operating on integers rather than doing lots of int(str(i)[..]) everywhere.
say_base = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven',
    'eight', 'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen',
    'fifteen', 'sixteen', 'seventeen', 'eighteen', 'nineteen']

say_tens = ['', '', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy',
    'eighty', 'ninety']

def hundreds_i(num):
    hundreds, rest = divmod(num, 100)
    if hundreds:
        yield say_base[hundreds]
        yield ' hundred'
    if 0 < rest < len(say_base):
        yield ' and '
        yield say_base[rest]
    elif rest != 0:
        tens, ones = divmod(rest, 10)
        yield ' and '
        yield say_tens[tens]
        if ones > 0:
            yield '-'
            yield say_base[ones]

assert """".join(hundreds_i(245)) == ""two hundred and forty-five""
assert """".join(hundreds_i(999)) == 'nine hundred and ninety-nine'
assert """".join(hundreds_i(200)) == 'two hundred'",https://stackoverflow.com/questions/25150316,python,06-08-2014 00:08,738.0,0.0,3.0,True,05-02-2024 18:01,06-08-2014 00:11
74851810,python sentence transformer - get matching sentence by index order,"i have a database table with lot of records. and i am comparing the sentence to find a best match.
lets say the table contains 4 columns: id, sentence, info, updated_date.
the data contains as below:




id
sentence
info
updated_info_date




1
what is the name of your company
some distinct info
19/12/2022


2
company name
some distinct info
18/12/2022


3
what is the name of your company
some distinct info
17/12/2022


4
what is the name of your company
some distinct info
16/12/2022


5
what is the name of your company
some distinct info
15/12/2022


6
what is the name of your company
some distinct info
14/12/2022


7
what is the name of your company
some distinct info
13/12/2022


8
what is the phone number of your company
some distinct info
12/12/2022


9
what is the name of your company
some distinct info
11/12/2022


10
what is the name of your company
some distinct info
10/12/2022




i have converted these sentences to tensors.
and i am passing this as an example ""what is the name of your company""(tensor) to match.
sentence = ""what is the name of your company"" # in tensor format
cos_scores = util.pytorch_cos_sim(sentence, all_sentences_tensors)[0]

top_results = torch.topk(cos_scores, k=5) 
or
top_results = np.argpartition(cos_scores, range(5))[0:5]

top_results does not return the top results index wise.
as the sentences are same, all will have a score of ""1"". and it returns the results arbitrarily.

what i want is to get the top 5 matches with the latest updated_date order or the index order.
is this possible to achieve ?
any suggestions ?","['python', 'numpy', 'pytorch', 'sentence-transformers']",74852150,"what i would do is as follows:

get the cosine similarity scores for each sentence and store them in an array.
sort the array based on the updated_date
get the top 5 indices from the sorted array
get the corresponding sentences from the database table using the indices
this should give you the top 5 matches with the latest updated_date or the index order.
something like this should work:

from sklearn.feature_extraction.text import countvectorizer
from sklearn.metrics.pairwise import cosine_similarity

# create a countvectorizer object to transform the sentences into vectors
vectorizer = countvectorizer()

# transform the sentences into vectors using the countvectorizer object
vectors = vectorizer.fit_transform(sentences)

# calculate the cosine similarity scores using the cosine_similarity function
cosine_scores = cosine_similarity(vectors)

# convert the cosine similarity scores to a 1-dimensional numpy array
cosine_scores = cosine_scores.flatten()

# sort the array of cosine similarity scores in ascending order
sorted_indices = cosine_scores.argsort()

# get the top 5 indices from the sorted array
top_5_indices = sorted_indices[-5:]

# get the corresponding sentences from the database table using the indices
top_5_sentences = [sentences[i] for i in top_5_indices]",https://stackoverflow.com/questions/74851810,python,19-12-2022 14:25,838.0,1.0,1.0,True,19-12-2022 15:07,19-12-2022 15:07
76450328,build a chatbot with custom data using langchain,"i am trying to understand gpt/langchain . i want to use my own data only but i am not able to find a basic example.
for example, i envision my chat to be something like this:
user: show me way to build a tree house
gpt : to build a tree house you need the following materials and tools.....
my owns data in a file mydata.txt  with the following content
to build a tree house you need the following tool hammer , nails and materials wood...
....
.....

can you please show a simple example of how this can be done ..","['openai-api', 'langchain']",76461024,"summary
you need to use the vector db text generation tool in langchain, this tool will allow you to use your own documents as context for the chatbot to use for its answers.the example i will give below is slightly different from the chain in the documentation but i found it works better, not to mention the documentation talks mostly about getting text from a github repo, which isnt your case i suppose.
code below is written in python
load the imports, llm model and the document splitter
 from langchain.embeddings.openai import openaiembeddings
from langchain.vectorstores import chroma
from langchain.text_splitter import charactertextsplitter
from langchain.llms import openai
from langchain.chains import retrievalqa
from langchain.document_loaders import textloader
from langchain.prompts import prompttemplate
from langchain.chat_models import chatopenai

loader = textloader("""") #put the path and name of the file here, if its in the same directory of the code file you can just use the target file name
documents = loader.load()
llm = chatopenai(model = ""gpt-3.5-turbo"", temperature=0) //change the model to the one you want to use, tweak the temperature to see which one gives better answers
text_splitter = charactertextsplitter(chunk_size=500, chunk_overlap=0) # you can set the size of each doc chunk from your own doc
texts = text_splitter.split_documents(documents)
embeddings = openaiembeddings() #this will create the vector embeddings of your text
docsearch = chroma.from_documents(texts, embeddings)

create the prompt template
 from langchain.chains import llmchain
    prompt_template = """"""use the context below to write a 400 word blog post about the topic below:
        context: {context}
        topic: {topic}
        blog post:""""""
    #this is the standard prompt template, you can change and experiment with it
    prompt = prompttemplate(
        template=prompt_template, input_variables=[""context"", ""topic""]
    )
    
    chain = llmchain(llm=llm, prompt=prompt)

create the function to make the post and run it
def generate_blog_post(topic):
    docs = search_index.similarity_search(topic, k=4)
#k is basically how many chunks of context will be given to the llm for each search, more could give more context, but it could cost more tokens or someties even confuse the model, test it and be aware
    inputs = [{""context"": doc.page_content, ""topic"": topic} for doc in docs]
    print(chain.apply(inputs))
generate_blog_post(""your question/subject"")",https://stackoverflow.com/questions/76450328,openai-api,11-06-2023 11:41,2238.0,3.0,1.0,True,13-06-2023 00:28,11-06-2023 15:26
62235153,huggingface transformers bert model without classification layer,"i want to do a joint-embedding from vgg16 and bert for classification.
the thing with huggingface transformers bert is that it has the classification layer which has num_labels dimension.
but, i want the output from bertpooler (768 dimensions) which i will use as a text-embedding for an extended model.
from transformers import bertforsequenceclassification

model = bertforsequenceclassification.from_pretrained('bert-base-uncased')

this gives the following model:
bertforsequenceclassification(
...
...
        (11): bertlayer(
          (attention): bertattention(
            (self): bertselfattention(
              (query): linear(in_features=768, out_features=768, bias=true)
              (key): linear(in_features=768, out_features=768, bias=true)
              (value): linear(in_features=768, out_features=768, bias=true)
              (dropout): dropout(p=0.1, inplace=false)
            )
            (output): bertselfoutput(
              (dense): linear(in_features=768, out_features=768, bias=true)
              (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
              (dropout): dropout(p=0.1, inplace=false)
            )
          )
          (intermediate): bertintermediate(
            (dense): linear(in_features=768, out_features=3072, bias=true)
          )
          (output): bertoutput(
            (dense): linear(in_features=3072, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
      )
    )
    (pooler): bertpooler(
      (dense): linear(in_features=768, out_features=768, bias=true)
      (activation): tanh()
    )
  )
  (dropout): dropout(p=0.1, inplace=false)
  (classifier): linear(in_features=768, out_features=2, bias=true)
)

how can i get rid of the classifier layer?","['pytorch', 'huggingface-transformers', 'bert-language-model']",62235318,"from transformers import bertmodel
model = bertmodel.from_pretrained('bert-base-uncased')

output
(11): bertlayer(
        (attention): bertattention(
          (self): bertselfattention(
            (query): linear(in_features=768, out_features=768, bias=true)
            (key): linear(in_features=768, out_features=768, bias=true)
            (value): linear(in_features=768, out_features=768, bias=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
          (output): bertselfoutput(
            (dense): linear(in_features=768, out_features=768, bias=true)
            (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
            (dropout): dropout(p=0.1, inplace=false)
          )
        )
        (intermediate): bertintermediate(
          (dense): linear(in_features=768, out_features=3072, bias=true)
        )
        (output): bertoutput(
          (dense): linear(in_features=3072, out_features=768, bias=true)
          (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.1, inplace=false)
        )
      )
    )
  )
  (pooler): bertpooler(
    (dense): linear(in_features=768, out_features=768, bias=true)
    (activation): tanh()
  )
)

checkout the bertmodel definition here.",https://stackoverflow.com/questions/62235153,pytorch,06-06-2020 17:05,2242.0,3.0,1.0,True,10-12-2023 16:02,06-06-2020 17:13
76674439,deleting certain segments of strings in r?,"in r, suppose i have the following string:
x<-""the bank is going after bank one and pizza and corn.""

i would like to delete all segments of the string before the final time bank appears in the sentence, thus obtaining the string ""one and pizza and corn."" more generally, if i want to delete all text before the final time a specific word appears in a string, if there a way to do this?","['r', 'string', 'text', 'text-mining', 'sentiment-analysis']",76674525,"you can use group cature regex as shown below:
words <- ""bank""
pat <- sprintf(""^.*?(\\b%s\\b).*\\1 ?"", paste0(words, collapse = ""|""))
sub(pat, """", x, perl = true)

explanation ^ from begining the sentence, match .*? anything 0 or many times lazily until the first appearance of the bounded word. from here match greedily everything until you meet the word the last time. replace everything with a empty string ''",https://stackoverflow.com/questions/76674439,r,12-07-2023 21:05,35.0,0.0,1.0,True,12-07-2023 21:20,12-07-2023 21:10
69861444,typeerror: &quot;hypothesis&quot; expects pre-tokenized hypothesis (iterable[str]):,"i am trying to calculate the meteor score for the following:
print (nltk.translate.meteor_score.meteor_score(
    [""this is an apple"", ""that is an apple""], ""an apple on this tree""))

however i am getting this error every time and i am not sure how to fix it.
typeerror: ""hypothesis"" expects pre-tokenized hypothesis (iterable[str]): an apple on this tree

i also tried to put ""an apple on this tree"" in a list
    from nltk.translate.meteor_score import meteor_score
import nltk 
print (nltk.translate.meteor_score.meteor_score(
    [""this is an apple"", ""that is an apple""], [""an apple on this tree""]))

but it gave me this error.
typeerror: ""reference"" expects pre-tokenized reference (iterable[str]): this is an apple","['python', 'nlp', 'nltk', 'metrics']",69861501,"looking at the library code, it looks like hypothesis should be an iterable.  the error is coming from:
if isinstance(hypothesis, str):
        raise typeerror(
            f'""hypothesis"" expects pre-tokenized hypothesis (iterable[str]): {hypothesis}'
        )

try putting ""an apple on this tree"" in a list.",https://stackoverflow.com/questions/69861444,python,06-11-2021 04:26,2218.0,1.0,3.0,True,24-10-2024 05:15,06-11-2021 15:37
78999652,error during the compilation of the tokenizers package when trying to install transformers 4.27,"the use of chatglm-6b requires the installation of transformers==4.27.1.

i'm trying to install transformers==4.27.1, but i'm encountering an error during the compilation of the tokenizers package, which prevents the successful building of the wheel file. the error message indicates that the cargo rustc command failed and returned code 101.
here is the complete error message:


pip install -v transformers==4.27.1

....

error: `cargo rustc --lib --message-format=json-render-diagnostics --manifest-path cargo.toml --release -v --features pyo3/extension-module --crate-type cdylib -- -c 'link-args=-undefined dynamic_lookup -wl,-install_name,@rpath/tokenizers.cpython-312-darwin.so'` failed with code 101
  error: subprocess-exited-with-error
  
  ï¿½ï¿½ building wheel for tokenizers (pyproject.toml) did not run successfully.
  ï¿½ï¿½ï¿½ exit code: 1
  ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½> see above for output.
  
  note: this error originates from a subprocess, and is likely not a problem with pip.
  full command: /users/dragonfang/****/venv_agi/bin/python3.12 /users/dragonfang/****/venv_agi/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py build_wheel /var/folders/ll/9dtz3vg150vfv8t75ppq_nr00000gn/t/tmpymy6ke0d
  cwd: /private/var/folders/ll/9dtz3vg150vfv8t75ppq_nr00000gn/t/pip-install-tz2dgt67/tokenizers_11ac58d2069c4ec1985eae0d4528f0ec
  building wheel for tokl) ... error
  error: failed building wheel for tokenizers
failed to build tokenizers
error: error: failed to build installable wheels for some pyproject.toml based projects (tokenizers)

how to resolve this issue?

supplementaryï¿½ï¿½ï¿½

cargo rustc failed with code 101
'chatglmtokenizer' object has no attribute 'sp_tokenizer'

i have tried some methods from the two links above, but none of them have solved my problem.

the rustc 1.72.1 version is too low, package clap_lex v0.7.2 cannot be built because it requires rustc 1.74 or newer.
transformers==4.34.0 no longer needs to install the rust compiler, but the error occurs: attributeerror: 'chatglmtokenizer' object has no attribute 'sp_tokenizer'.
transformers==4.33.2 requires the installation of the rust compiler, and the error is: cargo rustc --lib --message-format=json-render-diagnostics --manifest-path cargo.toml --release -v --features pyo3/extension-module --crate-type cdylib -- -c 'link-args=-undefined dynamic_lookup -wl,-install_name,@rpath/tokenizers.cpython-312-darwin.so' failed with code 101.","['artificial-intelligence', 'huggingface-transformers', 'large-language-model']",79002519,"chatgpt suggested

you can try using python 3.10 or 3.11 to see if the issue is resolved

since my python version was 3.12.5, i downgraded to 3.11.9 and re-ran pip install. this successfully resolved the problem.
i've noticed that gemini 1.5 flash only suggests me to update rust and cargo, while gpt-4o mini additionally mentions the issue of python version. i have been using gemini before, it seems i should compare these two models more in the future.",https://stackoverflow.com/questions/78999652,artificial-intelligence,18-09-2024 18:13,185.0,0.0,1.0,True,19-09-2024 12:50,19-09-2024 09:38
78689702,different embeddings for same sentences with torch transformer,"hey all and apologies in advance for what is probably a fairly basic question - i have a theory about what's causing the issue here, but would be great to confirm with people who know more about this than i do.
i've been trying to implement this python code snippet in google colab. the snippet is meant to work out similarity for sentences. the code runs fine, but what i'm finding is that the embeddings and distances change every time i run it, which isn't ideal for my intended use case.
import torch
from scipy.spatial.distance import cosine
from transformers import automodel, autotokenizer

# import our models. the package will take care of downloading the models automatically
tokenizer = autotokenizer.from_pretrained(""qiyuw/pcl-bert-base-uncased"")
model = automodel.from_pretrained(""qiyuw/pcl-bert-base-uncased"")

# tokenize input texts
texts = [
    ""there's a kid on a skateboard."",
    ""a kid is skateboarding."",
    ""a kid is inside the house.""
]
inputs = tokenizer(texts, padding=true, truncation=true, return_tensors=""pt"")

# get the embeddings
with torch.no_grad():
    embeddings = model(**inputs, output_hidden_states=true, return_dict=true).pooler_output

# calculate cosine similarities
# cosine similarities are in [-1, 1]. higher means more similar
cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])
cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])

print(""cosine similarity between \""%s\"" and \""%s\"" is: %.3f"" % (texts[0], texts[1], cosine_sim_0_1))
print(""cosine similarity between \""%s\"" and \""%s\"" is: %.3f"" % (texts[0], texts[2], cosine_sim_0_2))

i think the issue must be model specific since i receive the warning about newly initialized pooler weights, and pooler_output is ultimately what the code reads to inform similarity:
some weights of robertamodel were not initialized from the model checkpoint at qiyuw/pcl-roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
you should probably train this model on a down-stream task to be able to use it for predictions and inference.

switching to an alternative model which does not give this warning (for example, sentence-transformers/all-mpnet-base-v2) makes the outputs reproducible, so i think it is because of the above warning about initialization of weights.  so here are my questions:

can i make the output reproducible by initialising/seeding the model differently?
if i can't make the outputs reproducible, is there a way in which i can improve the accuracy to reduce the amount of variation between runs?
is there a way to search huggingface models for those which will initialise the pooler weights so i can find a model which does suit my purposes?

thanks in advance","['python', 'pytorch', 'huggingface-transformers', 'bert-language-model']",78707989,"you are correct the model layer weights for bert.pooler.dense.bias and bert.pooler.dense.weight are initialized randomly. you can initialize these layers always the same way for a reproducible output, but i doubt the inference code that you have copied from there readme is correct. as already mentioned by you the pooling layers are not initialized and their model class also makes sure that the pooling_layer is not added:
...
self.bert = bertmodel(config, add_pooling_layer=false)
...

the evaluation script of the repo should be called, according to the readme with the following command:
python evaluation.py --model_name_or_path qiyuw/pcl-bert-base-uncased --mode test --pooler cls_before_pooler

when you look into it, your inference code for qiyuw/pcl-bert-base-uncased should be the following way:
import torch
from scipy.spatial.distance import cosine
from transformers import automodel, autotokenizer

# import our models. the package will take care of downloading the models automatically
tokenizer = autotokenizer.from_pretrained(""qiyuw/pcl-bert-base-uncased"")
model = automodel.from_pretrained(""qiyuw/pcl-bert-base-uncased"")

# tokenize input texts
texts = [
    ""there's a kid on a skateboard."",
    ""a kid is skateboarding."",
    ""a kid is inside the house.""
]
inputs = tokenizer(texts, padding=true, truncation=true, return_tensors=""pt"")

# get the embeddings
with torch.inference_mode():
    embeddings = model(**inputs)
    embeddings = embeddings.last_hidden_state[:, 0]

# calculate cosine similarities
# cosine similarities are in [-1, 1]. higher means more similar
cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])
cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])

print(""cosine similarity between \""%s\"" and \""%s\"" is: %.3f"" % (texts[0], texts[1], cosine_sim_0_1))
print(""cosine similarity between \""%s\"" and \""%s\"" is: %.3f"" % (texts[0], texts[2], cosine_sim_0_2))

output:
cosine similarity between ""there's a kid on a skateboard."" and ""a kid is skateboarding."" is: 0.941
cosine similarity between ""there's a kid on a skateboard."" and ""a kid is inside the house."" is: 0.779


can i make the output reproducible by initialising/seeding the model differently?

yes, you can. use torch.maunal_seed:
import torch
from transformers import automodel, autotokenizer

model_random = automodel.from_pretrained(""qiyuw/pcl-bert-base-uncased"")
torch.manual_seed(42)
model_repoducible1 = automodel.from_pretrained(""qiyuw/pcl-bert-base-uncased"")

torch.manual_seed(42)
model_repoducible2 = automodel.from_pretrained(""qiyuw/pcl-bert-base-uncased"")

print(torch.allclose(model_random.pooler.dense.weight, model_repoducible1.pooler.dense.weight))
print(torch.allclose(model_random.pooler.dense.weight, model_repoducible2.pooler.dense.weight))
print(torch.allclose(model_repoducible1.pooler.dense.weight, model_repoducible2.pooler.dense.weight))

output:
false
false
true",https://stackoverflow.com/questions/78689702,python,30-06-2024 20:21,533.0,3.0,2.0,True,06-07-2024 20:12,01-07-2024 18:53
61309527,unable to do stacking for a multi-label classifier,"i am working on a multi-label text classification problem (total target labels 90). the data distribution has a long tail and class imbalance and around 100k records. i am using the oaa strategy (one against all). i am trying to create an ensemble using stacking.
text features : hashingvectorizer(number of features 2**20, char analyzer)
tsvd to reduce the dimensionality (n_components=200). 
text_pipeline = pipeline([
    ('hashing_vectorizer', hashingvectorizer(n_features=2**20,
                                             analyzer='char')),
    ('svd', truncatedsvd(algorithm='randomized',
                         n_components=200, random_state=19204))])

feat_pipeline = featureunion([('text', text_pipeline)])

estimators_list = [('extratrees',
                    onevsrestclassifier(extratreesclassifier(n_estimators=30,
                                                             class_weight=""balanced"",
                                                             random_state=4621))),
                   ('linearsvc',
                    onevsrestclassifier(linearsvc(class_weight='balanced')))]
estimators_ensemble = stackingclassifier(estimators=estimators_list,
                                         final_estimator=onevsrestclassifier(
                                             logisticregression(solver='lbfgs',
                                                                max_iter=300)))

classifier_pipeline = pipeline([
    ('features', feat_pipeline),
    ('clf', estimators_ensemble)])

error 
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
<ipython-input-41-ad4e769a0a78> in <module>()
      1 start = time.time()
----> 2 classifier_pipeline.fit(x_train.values, y_train_encoded)
      3 print(f""execution time {time.time()-start}"")
      4 

3 frames
/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py in column_or_1d(y, warn)
    795         return np.ravel(y)
    796 
--> 797     raise valueerror(""bad input shape {0}"".format(shape))
    798 
    799 

valueerror: bad input shape (89792, 83)","['machine-learning', 'scikit-learn', 'nlp', 'multilabel-classification', 'ensemble-learning']",61336334,"stackingclassifier does not support multi label classification as of now. you could get to understand these functionalities by looking at the shape value for the fit parameters such as here.
solution would be to put the onevsrestclassifier wrapper on top of stackingclassifier rather on the individual models.
example:
from sklearn.datasets import make_multilabel_classification
from sklearn.linear_model import logisticregression
from sklearn.model_selection import train_test_split
from sklearn.ensemble import extratreesclassifier
from sklearn.svm import linearsvc
from sklearn.ensemble import stackingclassifier
from sklearn.multiclass import onevsrestclassifier

x, y = make_multilabel_classification(n_classes=3, random_state=42)
x_train, x_test, y_train, y_test = train_test_split(x, y, 
                                                    test_size=0.33,
                                                    random_state=42)

estimators_list = [('extratrees', extratreesclassifier(n_estimators=30, 
                                                       class_weight=""balanced"", 
                                                       random_state=4621)),
                   ('linearsvc', linearsvc(class_weight='balanced'))]

estimators_ensemble = stackingclassifier(estimators=estimators_list,
                                         final_estimator = logisticregression(solver='lbfgs', max_iter=300))

ovr_model = onevsrestclassifier(estimators_ensemble)

ovr_model.fit(x_train, y_train)
ovr_model.score(x_test, y_test)

# 0.45454545454545453

from sklearn.metrics import confusion_matrix
confusion_matrix(
    y_train[:, 0], 
    ovr_model.estimators_[0].estimators_[0].predict(x_train),)

#array([[818,   0],
#       [  0, 522]])

ovr_model.estimators_[0].estimators_[0].feature_importances_

#array([0.05049793, 0.07232525, 0.05278524, 0.08005984, 0.05036507,
#       0.03674032, 0.06144285, 0.03473714, 0.04080104, 0.05120309,
#       0.05311589, 0.04119592, 0.03239608, 0.08101098, 0.03522335,
#       0.03676684, 0.04613645, 0.04755277, 0.05268342, 0.04296053])",https://stackoverflow.com/questions/61309527,machine-learning,19-04-2020 18:26,1322.0,1.0,1.0,True,10-04-2023 13:13,21-04-2020 05:29
75633751,find the co-ordinate and highlight the required text in image in python,"i have the photo in which i wanted to get the co-ordinate of text and highlight the text.

text to highlight and get co-ordinate = 'was the age of wisdom'
i try to get the co-ordinate by providing the first and last word but didn't get the required solution
import regex as re
import pytesseract
from pil import image
pytesseract.pytesseract.tesseract_cmd = ""c:\\program files\\tesseract-ocr\\tesseract.exe""

import cv2
import matplotlib.pyplot as plt

filename = 'c:\\users\\vicky\\downloads\\image.png'


img = cv2.imread(filename)

from pytesseract import output
d = pytesseract.image_to_data(img, output_type=output.dict)
#print(d)

n_boxes = len(d['level'])
word1 = 'was'
word2 = 'wisdom,'
for i in range(n_boxes):
    #print(d['text'][i])   ## each word 
    if(d['text'][i] == word1):
        (x, y) = (d['left'][i], d['top'][i])
        #print(x,y)


    if(d['text'][i] == word2):
        (x1, y1,w1,h1) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        #print(x1, y1, w1, h1)
        
        
        
        print(f'the coordinates of text are ({x}, {y}) and ({x1+w1}, {y1+h1})')
        cv2.rectangle(img, (x, y), (x1 + w1, y1 + h1), (0, 0, 139), 5)
        break

cv2.imwrite('result2.png', img)

the resulted image i got

if anybody know any other approach than this and also  want to highlight and  fill the text with pale color.","['python', 'image-processing', 'nlp', 'computer-vision', 'data-science']",75636119,"you have to figure out the postion of the words:
import pytesseract
from pytesseract import output
import cv2

pytesseract.pytesseract.tesseract_cmd=r'c:\program files\tesseract-ocr\tesseract.exe'

img = cv2.imread('text.png')

d = pytesseract.image_to_data(img, output_type=output.dict)
#print(d.keys())

overlay = img.copy()
output = img.copy()
n_boxes = len(d['text'])
for i in range(18,25):
    if int(d['conf'][i]) > 60:
        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
        # print((x, y, w, h))
        img = cv2.rectangle(overlay, (x, y), (x + w, y + h), (0, 255, 0), -1)

alpha = 0.1
image_new = cv2.addweighted(overlay, alpha, output, 1 - alpha, 0, output)

cv2.imshow('img', image_new)
cv2.waitkey(0)
cv2.destroyallwindows()

output:",https://stackoverflow.com/questions/75633751,python,04-03-2023 05:02,10929.0,2.0,1.0,True,04-03-2023 16:07,04-03-2023 16:07
75084272,openai gpt-3 api: how to keep the format of the response?,"when i use gpt3's playground, i often get results that are formatted with numbered lists and paragraphs like below:
here's what the above class is doing:

1. it creates a directory for the log file if it doesn't exist.
2. it checks that the log file is newline-terminated.
3. it writes a newline-terminated json object to the log file.
4. it reads the log file and returns a dictionary with the following

- list 1
- list 2
- list 3
- list 4

however, when i directly use their api and extract the response from json result, i get the crammed text version that is very hard to read, something like this:
here's what the above class is doing:1. it creates a directory for the log file if it doesn't exist.2. it checks that the log file is newline-terminated.3. it writes a newline-terminated json object to the log file.4. it reads the log file and returns a dictionary with the following-list 1-list 2-list 3- list4

my question is, how do people keep the formats from gpt results so they are displayed in a neater, more readable way?","['openai-api', 'gpt-3']",76444852,"the problem was on my side's frontend. the openai api was returning the correct response, and i was rending the result with the wrong whitespace css settings",https://stackoverflow.com/questions/75084272,openai-api,11-01-2023 14:02,7835.0,3.0,4.0,True,14-09-2023 11:03,14-09-2023 10:47
1271918,"ruby, count syllables","i am using ruby to calculate the gunning fog index of some content that i have, i can successfully implement the algorithm described here: 
gunning fog index
i am using the below method to count the number of syllables in each word:
tokenizer = /([aeiouy]{1,3})/

def count_syllables(word)

  len = 0

  if word[-3..-1] == 'ing' then
    len += 1
    word = word[0...-3]
  end

  got = word.scan(tokenizer)
  len += got.size()

  if got.size() > 1 and got[-1] == ['e'] and
      word[-1].chr() == 'e' and
      word[-2].chr() != 'l' then
    len -= 1
  end

  return len

end

it sometimes picks up words with only 2 syllables as having 3 syllables.  can anyone give any advice or is aware of a better method?
text = ""the word logorrhoea is often used pejoratively to describe prose that is highly abstract and contains little concrete language. since abstract writing is hard to visualize, it often seems as though it makes no sense and all the words are excessive. writers in academic fields that concern themselves mostly with the abstract, such as philosophy and especially postmodernism, often fail to include extensive concrete examples of their ideas, and so a superficial examination of their work might lead one to believe that it is all nonsense.""

# used to get rid of any puncuation
text = text.gsub!(/\w+/, ' ')

word_array = text.split(' ')

word_array.each do |word|
    puts word if count_syllables(word) > 2
end

""themselves"" is being counted as 3 but it's only 2","ruby, nlp",1272072,"the function i give you before is based upon these simple rules outlined here:

each vowel (a, e, i, o, u, y) in a
  word counts as one syllable subject to
  the following sub-rules: 

ignore final -es, -ed, -e (except
  for -le)   
words of three letters or
  less count as one syllable
consecutive vowels count as one
  syllable.


here's the code:
def new_count(word)
  word.downcase!
  return 1 if word.length <= 3
  word.sub!(/(?:[^laeiouy]es|ed|[^laeiouy]e)$/, '')
  word.sub!(/^y/, '')
  word.scan(/[aeiouy]{1,2}/).size
end

obviously, this isn't perfect either, but all you'll ever get with something like this is a heuristic.
edit:
i changed the code slightly to handle a leading 'y' and fixed the regex to handle 'les' endings better (such as in ""candles"").
here's a comparison using the text in the question:
# used to get rid of any puncuation
text = text.gsub!(/\w+/, ' ')

words = text.split(' ')

words.each do |word|
  old = count_syllables(word.dup)
  new = new_count(word.dup)
  puts ""#{word}: \t#{old}\t#{new}"" if old != new
end

the output is:
logorrhoea:     3   4
used:   2   1
makes:  2   1
themselves:     3   2

so it appears to be an improvement.",https://stackoverflow.com/q/1271918,"ruby, nlp",13-08-2009 13:23,5673.0,8.0,4.0,True,06-11-2021 14:17,20-02-2015 03:13
35857519,efficiently count word frequencies in python,"i'd like to count frequencies of all words in a text file.
>>> countinfile('test.txt')

should return {'aaa':1, 'bbb': 2, 'ccc':1} if the target text file is like:
# test.txt
aaa bbb ccc
bbb

i've implemented it with pure python following some posts. however, i've found out pure-python ways are insufficient due to huge file size (> 1gb).
i think borrowing sklearn's power is a candidate.
if you let countvectorizer count frequencies for each line, i guess you will get word frequencies by summing up each column. but, it sounds a bit indirect way.
what is the most efficient and straightforward way to count words in a file with python?
update
my (very slow) code is here:
from collections import counter

def get_term_frequency_in_file(source_file_path):
    wordcount = {}
    with open(source_file_path) as f:
        for line in f:
            line = line.lower().translate(none, string.punctuation)
            this_wordcount = counter(line.split())
            wordcount = add_merge_two_dict(wordcount, this_wordcount)
    return wordcount

def add_merge_two_dict(x, y):
    return { k: x.get(k, 0) + y.get(k, 0) for k in set(x) | set(y) }","['python', 'nlp', 'scikit-learn', 'word-count', 'frequency-distribution']",35857833,"the most succinct approach is to use the tools python gives you.
from future_builtins import map  # only on python 2

from collections import counter
from itertools import chain

def countinfile(filename):
    with open(filename) as f:
        return counter(chain.from_iterable(map(str.split, f)))

that's it. map(str.split, f) is making an iterator that returns lists of words from each line. wrapping in chain.from_iterable converts that to a single iterator that produces a word at a time. counter takes an input iterable and counts all unique values in it. at the end, you return a  dict-like object (a counter) that stores all unique words and their counts, and during creation, you only store a line of data at a time and the total counts, not the whole file at once.
in theory, on python 2.7 and 3.1, you might do slightly better looping over the chained results yourself and using a dict or collections.defaultdict(int) to count (because counter is implemented in python, which can make it slower in some cases), but letting counter do the work is simpler and more self-documenting (i mean, the whole goal is counting, so use a counter). beyond that, on cpython (the reference interpreter) 3.2 and higher counter has a c level accelerator for counting iterable inputs that will run faster than anything you could write in pure python.
update: you seem to want punctuation stripped and case-insensitivity, so here's a variant of my earlier code that does that:
from string import punctuation

def countinfile(filename):
    with open(filename) as f:
        linewords = (line.translate(none, punctuation).lower().split() for line in f)
        return counter(chain.from_iterable(linewords))

your code runs much more slowly because it's creating and destroying many small counter and set objects, rather than .update-ing a single counter once per line (which, while slightly slower than what i gave in the updated code block, would be at least algorithmically similar in scaling factor).",https://stackoverflow.com/questions/35857519,python,08-03-2016 01:52,39588.0,38.0,8.0,True,24-11-2023 17:55,23-05-2017 10:31
21410490,which stanford nlp package to use for content categorization&gt;,"i have about 5000 terms in a table and i want to group them into categories that make sense.
for example some terms are:
nissan
ford
arrested
jeep
court

the result should be that nissan, ford, jeep get grouped into one category and that arrested and court are in another category. i looked at the stanford classifier nlp. am i right to assume that this is the right one to choose to do this for me?","['machine-learning', 'nlp', 'stanford-nlp', 'categorization']",21468999,"i would suggest you to use nltk if there weren't many proper nouns. you can use the semantic similarity from wordnet as features and try to cluster the words. here's a discussion about how to do that.
to use the stanford classifier, you need to know how many buckets (classes) of words you want. besides i think that is for documents rather than words.",https://stackoverflow.com/questions/21410490,machine-learning,28-01-2014 15:56,240.0,-1.0,2.0,True,09-03-2024 00:19,09-03-2024 00:19
75401992,openai api error: &quot;you didn&#39;t provide an api key. you need to provide your api key in an authorization header using bearer auth&quot;,"i am creating a php script to access open ai's api, to ask a query and get a response.
i am getting the following error:

you didn't provide an api key. you need to provide your api key in an
authorization header using bearer auth (i.e. authorization: bearer
your_key)

...but i thought i was providing the api key in the first variable?
here is my code:
$api_key = ""sk-u3b.........7mil"";

$query = ""how are you?"";

$url = ""

// set up the api request headers
$headers = array(
    ""content-type: application/json"",
    ""authorization: bearer "" . $api_key
);

// set up the api request body
$data = array(
    ""prompt"" => $query,
    ""max_tokens"" => 100,
    ""temperature"" => 0.5
);

// use wordpress's built-in http api to send the api request
$response = wp_remote_post( $url, array(
    'headers' => $headers,
    'body' => json_encode( $data )
) );

// check if the api request was successful
if ( is_wp_error( $response ) ) {
    // if the api request failed, display an error message
    echo ""error communicating with openai api: "" . $response->get_error_message();
} else {
    // if the api request was successful, extract the response text
    $response_body = json_decode( $response['body'] );
    //$response_text = $response_body->choices[0]->text;
    var_dump($response_body);
    // display the response text on the web page
    echo $response_body;","['php', 'curl', 'openai-api', 'gpt-3', 'gpt-4']",75402073,"all engines endpoints are deprecated.

use the completions api endpoint:


working example
if you run test.php the openai api will return the following completion:

string(23) ""
this is indeed a test""

test.php
<?php
    $ch = curl_init();

    $url = '

    $api_key = 'sk-xxxxxxxxxxxxxxxxxxxx';

    $post_fields = '{
        ""model"": ""text-davinci-003"",
        ""prompt"": ""say this is a test"",
        ""max_tokens"": 7,
        ""temperature"": 0
    }';

    $header  = [
        'content-type: application/json',
        'authorization: bearer ' . $api_key
    ];

    curl_setopt($ch, curlopt_url, $url);
    curl_setopt($ch, curlopt_returntransfer, 1);
    curl_setopt($ch, curlopt_post, 1);
    curl_setopt($ch, curlopt_postfields, $post_fields);
    curl_setopt($ch, curlopt_ $header);

    $result = curl_exec($ch);
    if (curl_errno($ch)) {
        echo 'error: ' . curl_error($ch);
    }
    curl_close($ch);

    $response = json_decode($result);
    var_dump($response->choices[0]->text);
?>",https://stackoverflow.com/questions/75401992,php,09-02-2023 17:00,6925.0,0.0,2.0,True,27-09-2023 10:39,12-08-2023 15:26
72187949,extracting start and end indices of a token using spacy,"i am looking at lots of sentences and looking to extract the start and end indices of a word in a given sentence.
for example, the input is as follows:
""this is a sentence written in english by a native english speaker.""
and what i want is the span of the word 'english' which in this case is : (30,37) and (50, 57).
note: i was pointed to this answer (get position of word in sentence with spacy)
but this answer doesn't solve my problem. it can help me in getting the start character of the token but not the end index.
all help appreciated","['python', 'python-3.x', 'spacy']",72188259,"you can do this with re in pure python:
s=""this is a sentence written in english by a native english speaker.""

import re
[(i.start(), i.end()) for i in re.finditer('english', s.upper())]

#output
[(30, 37), (50, 57)]

you can do in spacy as well:
import spacy
nlp=spacy.load(""en_core_web_sm"")
doc=nlp(""this is a sentence written in english by a native english speaker."")
for ent in doc.ents:
    if ent.text.upper()=='english':
      print(ent.start_char,ent.end_char)",https://stackoverflow.com/questions/72187949,python,10-05-2022 14:11,3206.0,5.0,3.0,True,31-01-2023 21:16,11-05-2022 15:53
77822185,format responses from open-ai ai in typescript/javascript css,"i'm using nextjs to hit the open ai api and using the ai npm package to tie it all together. everything works out of the box so to speak and is amazing. however, i am not seeing line breaks and so the responses are just a single line that piles up on top of itself.

for instance this response:
this is a paragraph.
this is a new paragraph.
looks like this:
this is a paragraph. this is a new paragraph.
so what i am trying to figure out is how to make it so that i am able to either insert <br> tags around the areas that need them. or <p> tags around the paragraphs themselves. i am probably missing something obvious here but felt it was worth asking as i could not get the answer from chatgpt itself, at least not with the questions i was asking.
is there a default way to do this from open-ai that i missed?
here is the sample code from the package for reference:
import openai from 'openai';
import { openaistream, streamingtextresponse } from 'ai';

const openai = new openai({
  apikey: process.env.openai_api_key,
});

export const runtime = 'edge';

export async function post(req) {
  const { messages } = await req.json();
  const response = await openai.chat.completions.create({
    model: 'gpt-4',
    stream: true,
    messages,
  });
  const stream = openaistream(response);
  return new streamingtextresponse(stream);
}

// ./app/page.js
'use client';

import { usechat } from 'ai/react';

export default function chat() {
  const { messages, input, handleinputchange, handlesubmit } = usechat();

  return (
    <div>
      {messages.map(m => (
        <div key={m.id}>
          {m.role}: {m.content}
        </div>
      ))}

      <form onsubmit={handlesubmit}>
        <input
          value={input}
          placeholder=""say something...""
          onchange={handleinputchange}
        />
      </form>
    </div>
  );
}

in this exmple, i want to format the m.content to see line breaks where it currently does not. although in the console if you log the m.content and then click into the the string on the value it will show you that the paragraph is there, it just isn't formatted to work in html","['javascript', 'css', 'reactjs', 'tailwind-css', 'openai-api']",77826752,"i feel almost silly at how easy the answer to this is.
{messages.map(m => (
   <div key={m.id} classname='pb-2'>
      <div classname=""font-bold"">{m.role}</div>
      <pre classname=""overflow-x-auto whitespace-pre-wrap"">{m.content}</pre>
    </div>
 ))}

but hopefully it will help someone who would have otherwise wasted time on this issue. by using the <pre> tags, it formats it exactly as it should be, not only creating the proper line breaks but also all spaces and tabs in output code to and from open ai api stay intact.
in addition to <pre>tag a little css helps the text wrap where you want it to as opposed to the text running off endlessly to the right of the page. in the above example tailwind is used to add
overflow-x: auto and whitespace: pre-wrap",https://stackoverflow.com/questions/77822185,javascript,15-01-2024 20:26,2212.0,1.0,2.0,True,17-01-2024 15:29,17-01-2024 15:29
73059189,why does comparing two images take longer when running the procedure in parallel using python&#39;s pool module?,"i'm developing a program that involves computing similarity scores for around 480 pairs of images (20 directories with around 24 images in each). i'm utilizing the sentence_transformers python module for image comparison, and it takes around 0.1 - 0.2 seconds on my windows 11 machine to compare two images when running in serial, but for some reason, that time gets increased to between 1.5 and 3.0 seconds when running in parallel using a process pool. so, either a), there's something going on behind the scenes that i'm not yet aware of, or b) i just did it wrong.
here's a rough structure of the image comparison function:
def compare_images(image_one, image_two, clip_model):
    start = time()
    images = [image_one, image_two]
    # clip_model is set to sentencetransformer('clip-vit-b-32') elsewhere in the code
    encoded_images = clip_model.encode(images, batch_size = 2, convert_to_tensor = true, show_progress_bar = false)
    processed_images = util.paraphrase_mining_embeddings(encoded_images)
    stop = time()
    print(""comparison time: %f"" % (stop - start) )
    score, image_id1, image_id2 = processed_images[0]
    return score

here's a rough structure of the serial version of the code to compare every image:
def compare_all_images(candidate_image, directory, clip_model):
    for dir_entry in os.scandir(directory):
        dir_image_path = dir_entry.path
        dir_image = image.open(dir_image_path)
        similiarity_score = compare_images(candidate_image, dir_image, clip_model)

        # ... code to determine whether this is the maximum score the program has seen...

here is a rough structure of the parallel version:
def compare_all_images(candidate_image, directory, clip_model):
    pool_results = dict()
    pool = pool()

    for dir_entry in os.scandir(directory):
        dir_image_path = dir_entry.path
        dir_image = image.open(dir_image_path)
        pool_results[dir_image_path] = pool.apply_async(compare_images, args = (candidate_image, dir_image, clip_model)

    # added everything to the pool, close it and wait for everything to finish
    pool.close()
    pool.join()

    # ... remaining code to determine which image has the highest similarity rating

i'm not sure where i might be erring.
the interesting thing here is that i also developed a smaller program to verify whether i was doing things correctly:
def func():
    sleep(6)

def main():
    pool = pool()
    for i in range(20):
        pool.apply_async(func)
    pool.close()

    start = time()
    pool.join()
    stop = time()
    print(""time: %f"" % (stop - start) ) # this gave an average of 12 seconds 
                                        # across multiple runs on my windows 11 
                                        # machine, on which multiprocessing.cpu_count=12

is this a problem with trying to make things parallel with sentence transformers, or does the problem lie elsewhere?
update: now i'm especially confused. i'm now only passing str objects to the comparison function and have temporarily slapped a return 0 as the very first line in the function to see if i can further isolate the issue. oddly, even though the parallel function is doing absolutely nothing now, several seconds (usually around 5) still seem to pass between the time that the pool is closed and the time that pool.join() finishes. any thoughts?
update 2: i've done some more playing around, and have found out that an empty pool still has some overhead. this is the code i'm testing out currently:
            # ...
            pool = pool()

            pool.close()
            start = time()
            debuggingutilities.debug(""empty pool closed, doing a join on the empty pool to see if directory traversal is messing things up"")
            pool.join()
            stop = time()

            debuggingutilities.debug(""empty pool join time: %f"" % (stop - start) )

this gives me an ""empty pool join time"" of about 5 seconds. moving this snippet to the very first part of my main function still yields the same. perhaps pool works differently on windows? in wsl (ubuntu 20.04), the same code runs in about 0.02 seconds. so, what would cause even an empty pool to hang for such a long time on windows?
update 3: i've made another discovery. the empty pool problem goes away if the only imports i have are from multiprocessing import pool and from time import time. however, the program uses a boatload of import statements across several source files, which causes the program to hang a bit when it first starts. i suspect that this is propagating down into the pool for some reason. unfortunately, i need all of the import statements that are in the source files, so i'm not sure how to get around this (or why the imports would affect an empty pool).
update 4: so, apparently it's the from sentence_transformers import sentencetransformer line that's causing issues (without that import, the pool.join() call happens relatively quickly. i think the easiest solution now is to simply move the compare_images function into a separate file. i'll update this question again with updates as i implement this.
update 5: i've done a little more playing around, and it seems like on windows, the import statements get executed multiple times whenever a pool gets created, which i think is just weird. here's the code i used to verify this:
from multiprocessing import pool
from datetime import datetime
from time import time
from utils import test

print(""outside function lol"")

def get_time():

    now = datetime.now()

    return ""%02d/%02d/%04d - %02d:%02d:%02d"" % (now.month, now.day, now.year, now.hour, now.minute, now.second)


def main():
    pool = pool()

    print(""starting pool"")

    """"""
    for i in range(4):
        print(""applying %d to pool %s"" % (i, get_time() ) )
        pool.apply_async(test, args = (i, ) )
    """"""

    pool.close()
    print(""pool closed, waiting for all processes to finish"")
    start = time()
    pool.join()

    stop = time()

    print(""pool done: %f"" % (stop - start) )

if __name__ == ""__main__"":

    main()

running through windows command prompt:
outside function lol
starting pool
pool closed, waiting for all processes to finish
outside function lol
outside function lol
outside function lol
outside function lol
outside function lol
outside function lol
outside function lol
outside function lol
outside function lol
outside function lol
outside function lol
outside function lol
pool done: 4.794051

running through wsl:
outside function lol
starting pool
pool closed, waiting for all processes to finish
pool done: 0.048856

update 6: i think i might have a workaround, which is to create the pool in a file that doesn't directly or indirectly import anything from sentence_transformers. i then pass the model and anything else i need from sentence_transformers as parameters to a function that handles the pool and kicks off all of the parallel processes. since the sentence_transformers import seems to be the only problematic one, i'll wrap that import statement in an if __name__ == ""__main__"" so it only runs once, which will be fine, as i'm passing the things i need from it as parameters. it's a rather janky solution, and probably not what others would consider as ""pythonic"", but i have a feeling this will work.
update 7: the workaround was successful. i've managed to get the pool join time on an empty pool down to something reasonable (0.2 - 0.4 seconds). the downside of this approach is that there is definitely considerable overhead in passing the entire model as a parameter to the parallel function, which i needed to do as a result of creating the pool in a different place than the model was being imported. i'm quite close, though.","['python', 'image-processing', 'parallel-processing', 'pool', 'sentence-transformers']",73397004,"i've done a little more digging, and think i've finally discovered the root of the problem, and it has everything to do with what's described here.
to summarize, on linux systems, processes are forked from the main process, meaning that the current process state is copied (which is why the import statements don't run multiple times). on windows (and macos), processes are spawned, meaning that interpreter starts at the beginning of the ""main"" file, thus running all import statements again. so, the behavior i'm seeing is not a bug, but i will need to rethink my program design to account for this.",https://stackoverflow.com/questions/73059189,python,20-07-2022 23:38,338.0,1.0,1.0,True,18-08-2022 03:05,22-07-2022 19:12
76616520,textdecoderstream is producing partial chunk,"i am constructing a reader using response.body.pipethrough(new textdecoderstream()).getreader().
i then loop through the read output and parse what is supposed to be valid json.
the complete implementation can be seen here:

it is supposed to produce output similar to:
data: {""id"":""chatcmpl-7ykvp6xeacyebja9awrheeenraqnc"",""object"":""chat.completion.chunk"",""created"":1688517887,""model"":""gpt-3.5-turbo-0613"",""choices"":[{""index"":0,""delta"":{""role"":""assistant"",""content"":null,""function_call"":{""name"":""print_tags"",""arguments"":""""}},""finish_reason"":null}]}

data: {""id"":""chatcmpl-7ykvp6xeacyebja9awrheeenraqnc"",""object"":""chat.completion.chunk"",""created"":1688517887,""model"":""gpt-3.5-turbo-0613"",""choices"":[{""index"":0,""delta"":{""function_call"":{""arguments"":""{\n""}},""finish_reason"":null}]}

data: {""id"":""chatcmpl-7ykvp6xeacyebja9awrheeenraqnc"",""object"":""chat.completion.chunk"",""created"":168851788

the above is what i get from reader.read() here.
however, notice that the last line got truncated.
i assume this is happening because of some sort of misconfiguration of textdecoderstream or the reader.
what needs to be done to ensure that i am receiving the complete chunk?
adding openai-api tag because this might be a bug in their/specific to their api.","['javascript', 'openai-api']",76616551,"each chunk you receive from a textdecoderstream is supposed to be valid utf-8 text not valid json.
to be 100% sure you have all the data, you need to wait for it all to be downloaded, at which point you may just use the response#json() method instead.
another possible solution would be to use a streaming json parser and feed it with each chunk. you can check this q/a.",https://stackoverflow.com/questions/76616520,javascript,05-07-2023 01:01,671.0,0.0,1.0,True,05-07-2023 01:26,05-07-2023 01:26
66433496,"how do i fix valueerror when doing nlp.add_pipe(languagedetector(), name=&#39;language_detector&#39;, last=true) with spacy 3","every time i run the following code i found on kaggle, i get valueerror. this is because of new version v3 of spacy:
import scispacy
import spacy
import en_core_sci_lg
from spacy_langdetect import languagedetector

nlp = en_core_sci_lg.load(disable=[""tagger"", ""ner""])
nlp.max_length = 2000000
nlp.add_pipe(languagedetector(), name='language_detector', last=true)

valueerror: [e966] nlp.add_pipe now takes the string name of the registered component factory, not a callable component. expected string, but got <spacy_langdetect.spacy_langdetect.languagedetector object at 0x00000216bb4c8d30> (name: 'language_detector').

if you created your component with nlp.create_pipe('name'): remove nlp.create_pipe and call nlp.add_pipe('name') instead.

if you passed in a component like textcategorizer(): call nlp.add_pipe with the string name instead, e.g. nlp.add_pipe('textcat').

if you're using a custom component: add the decorator @language.component (for function components) or @language.factory (for class components / factories) to your custom component and assign it a name, e.g. @language.component('your_name'). you can then run nlp.add_pipe('your_name') to add it to the pipeline.


i have installed these versions:
python_version : 3.8.5
spacy.version  : '3.0.3'
scispacy.version  :  '0.4.0'
en_core_sci_lg.version  :  '0.4.0'","['python', 'spacy', 'language-detection']",66433607,"the way add_pipe works changed in v3; components have to be registered, and can then be added to a pipeline just using their name. in this case you have to wrap the languagedetector like so:
import scispacy
import spacy
import en_core_sci_lg
from spacy_langdetect import languagedetector

from spacy.language import language

def create_lang_detector(nlp, name):
    return languagedetector()

language.factory(""language_detector"", func=create_lang_detector)

nlp = en_core_sci_lg.load(disable=[""tagger"", ""ner""])
nlp.max_length = 2000000
nlp.add_pipe('language_detector', last=true)

you can read more about how this works in the spacy docs.",https://stackoverflow.com/questions/66433496,python,02-03-2021 04:48,14910.0,9.0,2.0,True,02-10-2022 19:16,02-10-2022 19:16
49941646,python counter() function to count words in documents with more then one occurrence,"i am working on an nlp (natural language processing) project where i used the python counter() function from collections library. i am getting the results in the following form:
output:
counter({'due': 23, 'support': 20, 'atm': 16, 'come': 12, 'case': 11, 'sallu': 10, 'tough,': 9, 'team': 8, 'evident': , 'likely': 6, 'rupee': 4, 'depreciated': 2, 'senior': 1, 'neutral': 1, 'told': 1, 'tour\n\nrussiaï¿½ï¿½ï¿½s': 1, 'vladimir': 1, 'indeed,': 1, 'welcome,ï¿½ï¿½ï¿½': 1, 'player': 1, 'added': 1, 'games,': 1, 'russia': 1, 'arrest': 1, 'system.\nbut': 1, 'rate': 1, 'tuesday': 1, 'february,': 1, 'idea': 1, 'ban': 1, 'data': 1, 'consecutive': 1, 'interbank': 1, 'man,': 1, 'involved': 1, 'aggressive': 1, 'took': 1, 'sure': 1, 'market': 1, 'custody': 1, 'gang.\nwithholding': 1, 'cricketer': 1})

the problem is, i want to extract the words having count more than 1. in other words, i am trying to get only those words whose count is greater than 1 or 2.
i the output to make a vocabulary list after reducing the words with low frequency.
ps: i have more than 100 documents to test my data with almost 2000 distinct words.
pps: i have tried everything to get the results but unable to do so. i only need a logic and will be able to implement.","['python', 'python-3.x', 'nlp', 'words', 'python-collections']",49941763,"you can iterate over the key, value pairs in the dict and add them to a separate list. this is just that you wanted to produce a list in the end, otherwise @jpp has the better solution.
from collections import counter

mystr = ""this this this is really really good.""
mydict = counter(mystr.split())

mylist = [k for k, v in mydict.items() if v > 1]

# ['this', 'really']",https://stackoverflow.com/questions/49941646,python,20-04-2018 12:27,13382.0,3.0,3.0,True,03-04-2024 06:22,20-04-2018 12:32
73309040,how to select string rows extracted from pdf starting from a specific row that meets a condition,"i'm using python. i have many pdftexts and i have used pdfminer to extract and arrange the elements i'm interested in (ltchar objects). what i need now is to isolate text from a starting row until the end.
consider the pdf attached to this thread. in the last page there's a row that says ""tabla 1. indicadores..."". this is the starting row i'm interested in. i need to extract all rows that there are from row ""tabla 1"" until the end.
the example pdf can be found here:

my code up to now is the following:
# import libraries

import pdfminer
from io import stringio
from pdfminer.pdfparser import pdfparser
from pdfminer.pdfdocument import pdfdocument
from pdfminer.pdfpage import pdfpage, pdftextextractionnotallowed
from pdfminer.pdfinterp import pdfresourcemanager, pdfpageinterpreter
from pdfminer.converter import pdfpageaggregator, textconverter
from pdfminer.pdfdevice import pdfdevice
from pdfminer.layout import laparams
from pdfminer.layout import ltpage, lttextboxhorizontal, lttextboxvertical, lttextlinehorizontal, lttextlinevertical, lttextcontainer, ltchar, lttext, lttextbox, ltanno
from pdfquery import pdfquery
import matplotlib.pyplot as plt
from matplotlib import patches
%matplotlib inline
import pandas as pd
import numpy as np
import itertools

# iterate util i get ltchar objects

fp = open('example.pdf', 'rb')
manager = pdfresourcemanager()
laparams = laparams()
dev = pdfpageaggregator(manager, laparams=laparams)
interpreter = pdfpageinterpreter(manager, dev)
pages = pdfpage.get_pages(fp)

for page in pages:
    interpreter.process_page(page)
    layout = dev.get_result()
    characters = []
    for textbox in layout:
        if isinstance(textbox, lttextbox):
            for line in textbox:
                if isinstance(line, lttextlinehorizontal):
                    for char in line:
                      if isinstance(char, ltchar):
                        if char.get_text() != ' ':
                          characters.append(char)

# arrange characters

def arrange_text(x):
    rows = sorted(list(set(c.bbox[1] for c in x)), reverse=true)
    sorted_rows = []
    for row in rows:
        sorted_row = sorted([c for c in x if c.bbox[1] == row], key=lambda c: c.bbox[0])
        sorted_rows.append(sorted_row)
    return sorted_rows

sorted_rows = arrange_text(characters)

# get text lines

rows = sorted_rows
for row in rows:          
  text_line = """".join([c.get_text() for c in row])
  print(text_line)

current output is
considerarestosnuevosenfoquesyexperienciasparaunaintervenciï¿½ï¿½nsostenibleque
generemayorbienestar.
ï¿½ï¿½ï¿½
sinembargo,collinsetal.,(1999)adviertenqueelconocerelcontextolocaladetallees
necesarioparaunacorrectaimplementaciï¿½ï¿½n.enesesentido,losmodeloscomolatriple
hï¿½ï¿½liceyexperienciascomoelprogramabioculturadebenconsiderarse,perono
importarsedeformaacrï¿½ï¿½tica.enesesentido,esnecesarioquelacomisiï¿½ï¿½nconvocada
paralaenderevalï¿½ï¿½econdetalleelcontextolocal,yevite,enloposible,intervenciones
decarï¿½ï¿½cteruniversalista.
tabla1.indicadoressocioeconï¿½ï¿½micosenlosï¿½ï¿½mbitosurbanoyrural
indicadorurbanoruralaï¿½ï¿½o
porcentajedeasistenciaescolara85.480.72021
educaciï¿½ï¿½nsecundaria*
porcentajedepoblaciï¿½ï¿½nconaccesoa87222020-2021
unaredpï¿½ï¿½blicadealcantarillado*
porcentajedepoblaciï¿½ï¿½nconaccesoa47.925.82018
algï¿½ï¿½n8
porcentajedepoblaciï¿½ï¿½npobrepor26.645.72020
ï¿½ï¿½readeresidencia***
fuente:inei(2019,2021a,2021b)
*obtenidodeinei(2021a)
**obtenidodeinei(2019)
***obtenidodeinei(2021b)

expected output is
tabla1.indicadoressocioeconï¿½ï¿½micosenlosï¿½ï¿½mbitosurbanoyrural
indicadorurbanoruralaï¿½ï¿½o
porcentajedeasistenciaescolara85.480.72021
educaciï¿½ï¿½nsecundaria*
porcentajedepoblaciï¿½ï¿½nconaccesoa87222020-2021
unaredpï¿½ï¿½blicadealcantarillado*
porcentajedepoblaciï¿½ï¿½nconaccesoa47.925.82018
algï¿½ï¿½nserviciofinanciero**
ingresopromediomensualensoles**1557.4711.42018
porcentajedepoblaciï¿½ï¿½npobrepor26.645.72020
ï¿½ï¿½readeresidencia***
fuente:inei(2019,2021a,2021b)
*obtenidodeinei(2021a)
**obtenidodeinei(2019)
***obtenidodeinei(2021b)

it should be noted that i have many pdfs and they all have the same structure. the starting row always starts with the words ""tabla 1."" in the same row, the part of ""indicadores socioeconï¿½ï¿½micos..."" vary ae function startswith(). however, if i use only this function, i just get the row of ""tabla 1."" and nothing else while i'm interested in all rows that come before ""tabla 1"" row. in all cases, the condition is that the starting row starts with characters ""tabla 1."".
any suggestions?","['python', 'string', 'pdf', 'nlp', 'pdfminer']",73318355,"you should first create one string with all text_line and later search tabla 1, and slice it text[start:], and later print it
# --- before loop ---

rows = sorted_rows

lines = []

# --- loop ---

for row in rows:          
    text_line = """".join([c.get_text() for c in row])
    lines.append(text_line)
  
# --- after loop ---

text = ""\n"".join(lines)

start = text.find('tabla 1')

print(text[start:])

if you want to make sure that tabla 1 is at the beginning of line then you can search with \n - find(""\ntabla 1"") - and later skip \n using [start+1:]

if you may have may parts with tabla 1 then you can use text.split('\ntabla 1') to get list of parts - and later you will have to add tabla 1 at the beginning of every part.
parts = text.split('\ntabla 1')

parts = parts[1:]  # skip first part (before first `tabla 1`)

parts = ['tabla 1'+text for text in parts]

for text in parts:
    print(text)
    print('---')

or you may try to use regex to search text between two \ntabla 1",https://stackoverflow.com/questions/73309040,python,10-08-2022 15:35,833.0,0.0,1.0,True,12-08-2022 05:36,10-08-2022 18:17
24098658,morphological text analysis with python using *.dic *.aff,"i have 2 files in hunspell format(.dic and .aff) for ukrainian language. my program has to get base form of the input word. so, it can use word form from .dic file and affices from .aff files. i don't know how to achieve this even with hunspell util, but suppose it is possible.
which python libraries can get base form of the word using .dic and .aff files?","['python', 'nlp', 'hunspell', 'morphological-analysis']",25703644,"as said before hunspell is the library you require.
examples from 
import hunspell
hobj = hunspell.hunspell('/usr/share/myspell/en_us.dic', '/usr/share/myspell/en_us.aff')
hobj.spell('spookie')
>>>>false

hobj.suggest('spookie')
>>>>['spookier', 'spookiness', 'spooky', 'spook', 'spoonbill']

hobj.spell('spooky')
>>>>true

hobj.analyze('linked')
>>>>[' st:link fl:d']
hobj.stem('linked')
>>>>['link']",https://stackoverflow.com/questions/24098658,python,07-06-2014 15:18,2347.0,2.0,2.0,True,09-10-2024 21:12,09-10-2024 21:12
72736034,cannot download glove embeddings. have they been moved or is downloads.cs.stanford.edu down temporarily?,"i am attempting to download glove.840b.300d.zip. i used the link at  and also ran wget  the output from wget looks as follows:
--2022-06-23 15:50:30--  (try: 2)  
connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... failed: connection timed out.
retrying.

does anyone know if this is a temporary issue? thank you!","['nlp', 'stanford-nlp', 'word-embedding']",72736695,"just found that someone opened an issue for this: 
downloading from  is not currently possible.
however, huggingface has mirrors for all of the glove sets that can be downloaded. links to these are provided by a comment made on the github issue by joelsewhere on june 22nd.",https://stackoverflow.com/questions/72736034,nlp,23-06-2022 20:07,640.0,2.0,1.0,True,23-06-2022 21:21,23-06-2022 21:05
76927485,how to calculate the quality of comments using nlp,"how can i calculate the quality of comments a user receives on their posts and assign ratings based on the comments' quality score in a dataset i'm analyzing as part of my data science work?
i've studied this, but nlp seems complex for beginners. any suggestions for a simpler approach?",['nlp'],76927519,"consider using sentiment analysis on each comment to determine sentiment (positive, negative, neutral), and assign scores accordingly. it's a simpler approach suitable for beginners. look up resources to implement it easily. is this helpful for your initial stage analysis.",https://stackoverflow.com/questions/76927485,nlp,18-08-2023 08:01,180.0,-2.0,1.0,True,13-06-2024 19:34,13-06-2024 19:34
77193642,deploy aws sagemaker endpoint for hugging face embedding model,"i would like to deploy a huggingface text embedding model endpoint via aws sagemaker.
here is my code so far:
import sagemaker
from sagemaker.huggingface.model import huggingfacemodel

# sess = sagemaker.session()
role = sagemaker.get_execution_role()

# hub model configuration. <
hub = {
  'hf_model_id':'sentence-transformers/all-minilm-l12-v2', # model_id from hf.co/models
  'hf_task':'feature-extraction' # nlp task you want to use for predictions
}

# create hugging face model class
huggingface_model = huggingfacemodel(
    env=hub, # configuration for loading model from hub
    role=role, # iam role with permissions to create an endpoint
    py_version='py36',
    transformers_version=""4.6"", # transformers version used
    pytorch_version=""1.7"", # pytorch version used
)

predictor = huggingface_model.deploy(
    initial_instance_count=1,
    instance_type=""ml.m5.xlarge""
)

data = {
""inputs"": [""this is an example sentence"", ""each sentence is converted""]
}

result = predictor.predict(data)
print(len(result[0]))
print(result[0])

while this does deploy a endpoint successfully, it does not behave the way it should.
i expect for each string in the input list to get a 1x384 list of floats as output.
but instead i get 7x384 lists for each sentence.
did i maybe use the wrong pipeline?","['python', 'huggingface-transformers', 'amazon-sagemaker', 'endpoint']",78274670,"there are two ways to deploy huggingface models as sagemaker endpoints:

the way you have done, defining env=hub inside huggingfacemodel class. this is a nice and quick way to get inferences from the model without any custom preprocessing. you send a request and you will get a response in the raw form with which the model was created.
if you want to do more with each request sent to model i.e. preprocess the inputs, change the behaviour of model and/or postprocess the output, you will need to use custom scripts. sample huggingfacemodel class parameters are:

huggingface_model = huggingfacemodel(
   model_data=s3_location,       # path to your model and script
   role=role,                    # iam role with permissions to create an endpoint
   transformers_version=""4.37.0"",  # transformers version used
   pytorch_version=""2.1.0"",        # pytorch version used
   py_version='py310',            # python version used
    #model_server_workers=1,
    #image_uri=""763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:2.1.0-transformers4.37.0-cpu-py310-ubuntu22.04""
   #env=hub
)

this is the complete reference you need:

additional info: the handler file that will run with each request your endpoint receives:",https://stackoverflow.com/questions/77193642,python,28-09-2023 09:11,781.0,0.0,2.0,True,04-04-2024 14:48,28-09-2023 12:09
73607406,"valueerror: x has 3 features, but linearsvc is expecting 64852 features as input","i get the following error when i try to deploy this model.
valueerror: x has 3 features, but linearsvc is expecting 64852 features as input

example of data below.
data = [[3409, false, 'lorum ipsum'], [0409, true, 'dolor sit amet consectetuer'], [7869, false, 'aenean commodo ligula eget dolor']]
df = pd.dataframe(data, columns=['id', 'booleanv', 'text'] 

the code where the model gets created below.
import pandas as pd
from sklearn.feature_extraction.text import tfidfvectorizer
from sklearn.svm import linearsvc
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.feature_extraction.text import countvectorizer
from sklearn.feature_extraction.text import tfidftransformer

df = pd.read_csv('cleandata.csv')

# split dataset into training and validation set
train_size = int(df.shape[0] * 0.8)

train_df = df[:train_size]
val_df = df[train_size:]

# split text and labels
x_train = train_df.text.to_numpy()
y_train = train_df.booleanv.to_numpy()
x_test = val_df.text.to_numpy()
y_test = val_df.booleanv.to_numpy()


tfidf = tfidfvectorizer(ngram_range=(1,1))
x_train_tf = tfidf.fit_transform(x_train)
x_test_tf = tfidf.transform(x_test)

model1 = linearsvc(random_state=0, tol=1e-5)
model1.fit(x_train_tf, y_train)

import pickle

pickle.dump(model1, open('classification.pickle','wb'))
pickle.dump(tfidf, open('vectorizer.pickle','wb'))

x_train and x_test are both arrays. the input i feed in the api i created is in json format. i suspect that i need to transform my input somehow. is this correct? if so, how can i do that?","['python', 'machine-learning', 'nlp', 'svm', 'text-classification']",73740191,"to obtain predictions from your model, you need to follow the same transformation steps that were undertaken during the training phase.
the valueerror you are encountering indicates that you are passing raw data to the classifier without vectorization. as the model has been trained on a sparse matrix consisting of 64852 features (the outcome of tfidf.fit_transform(x_train) ), it expects a vectorized input with the same number of features. here is how it can be done:
input_data = {
               'id': 1234,  
               'booleanv': false, 
               'text' : 'your input text goes here'
              }

#vectorize 
input_vectorized = tfidf.transform([input_data['text']]) 

#get predictions 
predictions = model.predict(input_vectorized)

this can, of course, be modified to work with batches instead of single inputs. moreover, the use of pipelines is highly recommended to assemble all the different steps.",https://stackoverflow.com/questions/73607406,python,05-09-2022 09:42,2499.0,2.0,1.0,True,18-09-2022 19:57,05-09-2022 09:58
38916452,nltk download ssl: certificate verify failed,"i get the following error when trying to install punkt for nltk:
nltk.download('punkt')    
 [nltk_data] error loading punkt: <urlopen error [ssl:
 [nltk_data]     certificate_verify_failed] certificate verify failed
 [nltk_data]     (_ssl.c:590)>
false","['python', 'ssl-certificate', 'nltk']",39142816,"the downloader script is broken. as a temporal workaround can manually download the punkt tokenizer from here and then place the unzipped folder in the corresponding location. the default folders for each os are:

windows: c:\nltk_data\tokenizers
osx: /usr/local/share/nltk_data/tokenizers
unix: /usr/share/nltk_data/tokenizers",https://stackoverflow.com/questions/38916452,python,12-08-2016 11:04,143610.0,110.0,16.0,True,03-11-2024 21:03,27-12-2020 23:39
76047251,stop spacy from deleting stopwords in split strings,"i'm trying to use spacy to remove stopwords from a panda dataframe created from a csv.
my issue is that i'm trying to account for words that might have a mix of words and numbers.
my issue:
if a number separates a word so that it contains a stop word,
it will delete that portion of the word.
    ex. with stop word at the end
        input: 'co555in'
        breaks up the word, separating it in 'co'+ 555 + 'in'
        removes 'in' because it is a stop word.
        output: 'co555'

    ex. without stop word at the end
        input: 'co555inn'
        breaks up the word, separating it in 'co'+ 555 + 'inn'
        will not remove 'inn' because it is not a stop word.
        output: 'co555inn'

current implementation:
    df[col] = df[col].apply(lambda text: 
            """".join(token.lemma_ for token in nlp(text) 
            if not token.is_stop))

so what i'd like is to be able to account for numbers and words mixed without spacy filtering out the portion of the word if the number separates the string so that it contains a stopword.
update: according to the devs this is a feature and not a bug. so a workaround, like the answer below, is necessary to account for these edge cases.","['python', 'nlp', 'spacy', 'stop-words']",76051427,"edit #2: simplified the code. added functionality to remove any words that have numerical characters from the text using python's regular expressions library then tokenized all the other text. also added additional safeguards to ensure that punctuation does not cause an error.
here is my remove_stopwords method, with some additional code included that i used for testing.
import spacy
import pandas as pd
import re

nlp = spacy.load('en_core_web_sm')

def remove_stopwords(text):
    """"""
    removes stop words from a text source
    """"""
    number_words = re.findall(r'\w*\d+\w*', text)
    remove_numbers = re.sub(r'\w*\d+\w*', '', text)
    split_text = re.split(r'\w+', remove_numbers)
    remove_stop_words = [word for word in split_text if not nlp.vocab[word].is_stop]
    final_words = number_words + remove_stop_words
    return "" "".join(final_words)

df = pd.read_csv('input_file.csv', sep='\t') # replace with your csv file
df['text'] = df['text'].apply(remove_stopwords)
df.to_csv('output_file.csv', index=false) # replace with your desired output file name",https://stackoverflow.com/questions/76047251,python,18-04-2023 17:01,179.0,3.0,1.0,True,11-05-2023 15:31,11-05-2023 15:31
76014701,how to avoid adding double start of token in trocr finetune model,"describe the bug
the model i am using (trocr model):
the problem arises when using:

[x] the official example scripts: done by the nice tutorial  (fine_tune) @nielsrogge
[x] my own modified scripts: (as the script below )

processor = trocrprocessor.from_pretrained(""microsoft/trocr-large-handwritten"")

class dataset(dataset):
    def __init__(self, root_dir, df, processor, max_target_length=128):
        self.root_dir = root_dir
        self.df = df
        self.processor = processor
        self.max_target_length = max_target_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # get file name + text 
        file_name = self.df['file_name'][idx]
        text = self.df['text'][idx]
        # prepare image (i.e. resize + normalize)
        image = image.open(self.root_dir + file_name).convert(""rgb"")
        pixel_values = self.processor(image, return_tensors=""pt"").pixel_values
        # add labels (input_ids) by encoding the text
        labels = self.processor.tokenizer(text, 
                                          padding=""max_length"",
                                                         max_length=self.max_target_length).input_ids
        # important: make sure that pad tokens are ignored by the loss function
        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]
        # encoding  
        return {""pixel_values"": pixel_values.squeeze(), ""labels"": torch.tensor(labels)}

model = visionencoderdecodermodel.from_pretrained(""microsoft/trocr-large-handwritten"")
model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
model.config.pad_token_id = processor.tokenizer.pad_token_id
model.config.vocab_size = model.config.decoder.vocab_size

model.config.eos_token_id = processor.tokenizer.sep_token_id


# python3 train.py path/to/labels  path/to/images/


platform: linux  ubuntu  distribution  [gcc 9.4.0] on linux
pytorch version (gpu?):  0.8.2+cu110
transformers: 4.22.2
python version:3.8.10

a clear and concise description of what the bug is.
to reproduce steps to reproduce the behavior:

after training the model or during the training phase  when evaluating metrics calculate i see the model added the double start of token <s><s> or ids [0,0, ......,2,1,1, 1 ]
here is an example during the training phase the show generated tokens in compute_metrics
input predictions: [[0,0,506,4422,8046,2,1,1,1,1,1]]
input references: [[0,597,2747 ...,1,1,1]] 
other examples during testing models []

expected behavior a clear and concise description of what you expected to happen.
in 2 reproduced problems:
i am expecting during training input predictions: [[,0,,506,4422,8046,2,1,1,1,1,1 ]] 
in addition during the testing phase:  generated text without double 
tensor([[0,11867,405,22379,1277,..........,368,2]]) 
<s>ennyit errï¿½ï¿½l, tï¿½ï¿½lem fï¿½ï¿½nykï¿½ï¿½pezz amennyit akarsz, a vï¿½ï¿½lemï¿½ï¿½nyem akk","['python', 'deep-learning', 'pytorch', 'huggingface-transformers', 'huggingface']",76044662,"the problem comes from passed token ids. i am adding  start token from tokenizer  +  another start token from the trocr model so the duplication happens. the solution is super easy by just skipping  start token coming from the tokenizer  by using labels = labels[1:]
def __getitem__(self, idx):
        # get file name + text 
        file_name = self.df['file_name'][idx]
        text = self.df['text'][idx]
        # prepare image (i.e. resize + normalize)
        image = image.open(self.root_dir + file_name).convert(""rgb"")
        pixel_values = self.processor(image, return_tensors=""pt"").pixel_values
        # add labels (input_ids) by encoding the text
        labels = self.processor.tokenizer(text, 
                                          padding=""max_length"",
                                                                                    max_length=self.max_target_length).input_ids
        # important: make sure that pad tokens are ignored by the loss function
        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]
        # encoding
        labels = labels[1:]   
        return {""pixel_values"": pixel_values.squeeze(), ""labels"": torch.tensor(labels)}",https://stackoverflow.com/questions/76014701,python,14-04-2023 11:37,255.0,1.0,1.0,True,18-04-2023 12:54,18-04-2023 12:53
61681613,"construct the unigrams, bi-grams and tri-grams in python","how to construct the unigrams, bi-grams and tri-grams for large corpora then to compute the frequency for each of them. arrange the results by the most frequent to the least frequent grams.
from nltk import word_tokenize
from nltk.util import ngrams
from collections import counter

text = ""i need to write a program in nltk that breaks a corpus (a large collection of \
txt files) into unigrams, bigrams, trigrams, fourgrams and fivegrams.\ 
i need to write a program in nltk that breaks a corpus""
token = nltk.word_tokenize(text)
bigrams = ngrams(token,2)
trigrams = ngrams(token,3)```","['python', 'nltk']",61682100,"try this:
import nltk
from nltk import word_tokenize
from nltk.util import ngrams
from collections import counter

text = '''i need to write a program in nltk that breaks a corpus (a large 
collection of txt files) into unigrams, bigrams, trigrams, fourgrams and 
fivegrams. i need to write a program in nltk that breaks a corpus'''

token = nltk.word_tokenize(text)
most_frequent_bigrams = counter(list(ngrams(token,2))).most_common()
most_frequent_trigrams = counter(list(ngrams(token,3))).most_common()
for k, v in most_frequent_bigrams:
    print (k,v)
for k, v in most_frequent_trigrams:
    print (k,v)",https://stackoverflow.com/questions/61681613,python,08-05-2020 14:38,426.0,0.0,2.0,True,29-02-2024 11:07,08-05-2020 14:47
72452850,quanteda : remove empty documents to compute tfidf but keep them in the final dfm,"i am trying to compute tfidf on a dataset with a lot of empty documents. i wanted to compute tfidf without the empty documents, but still have as an output a dfm object with the original number of documents.
here's an example :
texts = c("""", ""bonjour!"", ""hello, how are you"", """", ""good"", """", """", """")
a = texts %>%
    tokens(tolower=t, remove_punct=t) %>%
    dfm() %>%
    dfm_wordstem() %>%
    dfm_remove(stopwords(""en"")) %>%
    dfm_tfidf()
print(a, max_ndoc=10)
document-feature matrix of: 8 documents, 3 features (87.50% sparse) and 0 docvars.
       features
docs    bonjour   hello    good
  text1 0       0       0      
  text2 0.90309 0       0      
  text3 0       0.90309 0      
  text4 0       0       0      
  text5 0       0       0.90309
  text6 0       0       0      
  text7 0       0       0      
  text8 0       0       0    

but idf is affected by the number of empty documents, which i do not want. therefore, i compute tfidf on the subset of non-empty documents like so :
a2 = texts %>%
    tokens(tolower=t, remove_punct=t) %>%
    dfm() %>%
    dfm_subset(ntoken(.) > 0) %>%
    dfm_wordstem() %>%
    dfm_remove(stopwords(""en"")) %>%
    dfm_tfidf()
print(a2, max_ndoc=10)
document-feature matrix of: 3 documents, 3 features (66.67% sparse) and 0 docvars.
       features
docs      bonjour     hello      good
  text2 0.4771213 0         0        
  text3 0         0.4771213 0        
  text5 0         0         0.4771213

i now want to have a sparse matrix with the same format as the first matrix, but with the previous values for the texts. i found this code on stackoverflow: 
add_rows_2 <- function(m,v) {
    oldind <- unique(m@i)
    ## new row indices
    newind <- oldind + as.integer(rowsums(outer(oldind,v,"">="")))
    ## modify dimensions
    m@dim <- m@dim + c(length(v),0l)
    m@i <- newind[match(m@i,oldind)]
    m
}
empty_texts_idx = which(texts=="""")
position_after_insertion = empty_texts_idx - 1:(length(empty_texts_idx))

a3 = add_rows_2(a2, position_after_insertion)
print(a3, max_ndoc=10)
document-feature matrix of: 8 documents, 3 features (87.50% sparse) and 0 docvars.
         features
docs        bonjour     hello      good
  text2.1 0         0         0        
  text3.1 0.4771213 0         0        
  text5.1 0         0.4771213 0        
  na.na   0         0         0        
  na.na   0         0         0.4771213
  na.na   0         0         0        
  na.na   0         0         0        
  na.na   0         0         0        

which is what i want, and the empty texts have been added at the appropriate row in the matrix.
question 1: i was wondering if there is a more efficient way to do this directly with the quanteda package...
question 2: ...or at least a way that would not change the structure of the dfm object, since a3 and a do not have the same docvars attribute.
print(a3@docvars)
  docname_ docid_ segid_
1    text2  text2      1
2    text3  text3      1
3    text5  text5      1

print(docnames(a3))
[1] ""text2"" ""text3"" ""text5""

print(a@docvars)
  docname_ docid_ segid_
1    text1  text1      1
2    text2  text2      1
3    text3  text3      1
4    text4  text4      1
5    text5  text5      1
6    text6  text6      1
7    text7  text7      1
8    text8  text8      1

i was able to have a ""correct"" format for a3 by running the following lines of code
# necessary to print proper names in 'docs' column
new_docvars = data.frame(docname_=paste0(""text"",1:length(textes3)) %>% as.factor(), docid_=paste0(""text"",1:length(textes3))%>% as.factor(), segid_=rep(1,length(textes3)))
a3@docvars = new_docvars

# the following line is necessary for cv.glmnet to run using a3 as covariates
docnames(a3) <- paste0(""text"",1:length(textes3)) 
# seems equivalent to a3@dimnames$docs <- paste0(""text"",1:length(textes3))

print(a3, max_ndoc=10)
document-feature matrix of: 8 documents, 3 features (87.50% sparse) and 0 docvars.
       features
docs      bonjour     hello      good
  text1 0         0         0        
  text2 0.4771213 0         0        
  text3 0         0.4771213 0        
  text4 0         0         0        
  text5 0         0         0.4771213
  text6 0         0         0        
  text7 0         0         0        
  text8 0         0         0

print(a3@docvars) # this is now as expected
  docname_ docid_ segid_
1    text1  text1      1
2    text2  text2      1
3    text3  text3      1
4    text4  text4      1
5    text5  text5      1
6    text6  text6      1
7    text7  text7      1
8    text8  text8      1
print(docnames(a3)) # this is now as expected
[1] ""text1"" ""text2"" ""text3"" ""text4"" ""text5"" ""text6"" ""text7"" ""text8""

i need  to change docnames(a3) because i want to use a3 as covariates for a model i want to train with cv.glmet, but i get an error if i don't change the document names for a3. again, is this the correct way to proceed with quanteda? i felt like manually changing docvars was not the proper way to do it, and i could not find anything online about that. any insights on that would be appreciated.
thanks!","['r', 'sparse-matrix', 'tf-idf', 'quanteda']",72582572,"i do not know if it is a good idea to remove empty documents before computing tf-idf, but it easy to do restore removed documents with drop_docid = false and fill = true because quanteda keeps track of them.
require(quanteda)
#> loading required package: quanteda
#> package version: 3.2.1
#> unicode version: 13.0
#> icu version: 66.1
#> parallel computing: 10 of 10 threads used.
#> see  for tutorials and examples.
txt <- c("""", ""bonjour!"", ""hello, how are you"", """", ""good"", """", """", """")
corp <- corpus(txt)
dfmt <- dfm(tokens(corp))
dfmt
#> document-feature matrix of: 8 documents, 8 features (87.50% sparse) and 0 docvars.
#>        features
#> docs    bonjour ! hello , how are you good
#>   text1       0 0     0 0   0   0   0    0
#>   text2       1 1     0 0   0   0   0    0
#>   text3       0 0     1 1   1   1   1    0
#>   text4       0 0     0 0   0   0   0    0
#>   text5       0 0     0 0   0   0   0    1
#>   text6       0 0     0 0   0   0   0    0
#> [ reached max_ndoc ... 2 more documents ]


dfmt2 <- dfm_subset(dfmt, ntoken(dfmt) > 0, drop_docid = false) %>% 
  dfm_tfidf()
dfmt2
#> document-feature matrix of: 3 documents, 8 features (66.67% sparse) and 0 docvars.
#>        features
#> docs      bonjour         !     hello         ,       how       are       you
#>   text2 0.4771213 0.4771213 0         0         0         0         0        
#>   text3 0         0         0.4771213 0.4771213 0.4771213 0.4771213 0.4771213
#>   text5 0         0         0         0         0         0         0        
#>        features
#> docs         good
#>   text2 0        
#>   text3 0        
#>   text5 0.4771213

dfmt3 <- dfm_group(dfmt2, fill = true, force = true)
dfmt3
#> document-feature matrix of: 8 documents, 8 features (87.50% sparse) and 0 docvars.
#>        features
#> docs      bonjour         !     hello         ,       how       are       you
#>   text1 0         0         0         0         0         0         0        
#>   text2 0.4771213 0.4771213 0         0         0         0         0        
#>   text3 0         0         0.4771213 0.4771213 0.4771213 0.4771213 0.4771213
#>   text4 0         0         0         0         0         0         0        
#>   text5 0         0         0         0         0         0         0        
#>   text6 0         0         0         0         0         0         0        
#>        features
#> docs         good
#>   text1 0        
#>   text2 0        
#>   text3 0        
#>   text4 0        
#>   text5 0.4771213
#>   text6 0        
#> [ reached max_ndoc ... 2 more documents ]

created on 2022-06-16 by the reprex package (v2.0.1)",https://stackoverflow.com/questions/72452850,r,31-05-2022 18:53,538.0,0.0,1.0,True,15-06-2022 21:22,02-06-2022 20:04
77588646,truncating a training dataset so that it fits exactly within the context window,"i have a dataset where the total tokens once tokenised is around 5000. i was to feed that into a bert-style model so i have to shrink it down to 512 tokens but i want to rearrange the text to train it on fill-in-the-middle tasks using the techniques outlined in this paper: 
my issue is that i want to take the last 512 - 1 tokens and then prepend my <pre> token to the beginning, but i'm finding it difficult to simply prepend a single token to my tokenised text without going through the process of encoding the text to tokens, truncating off the text on the left, then decoding the text, adding my <pre> token and then re-encoding the text again. is there a simpler way?
here's what i have so far:
additional_special_tokens = [""<pre>"", ""<suf>"", ""<mid>""]

tokenizer = autotokenizer.from_pretrained(model_name, truncation_side=""left"")
tokenizer.additional_special_tokens = additional_special_tokens

small_eval_dataset = full_dataset[""validation""].shuffle(42).select(range(1))

def build_training_data(examples):
    to_tokenized = examples[""context""] + ""<suf><mid>"" + examples[""gt""]
    tokenized = tokenizer(to_tokenized, truncation=true)
    tokenized[""input_ids""][0] = tokenizer(""<pre>"")
    return tokenized


small_eval_dataset = small_eval_dataset.map(build_training_data)

i would like to have the text truncated on the left to 512 tokens, so that then i can feed that into my bert-style model and have it train on this specific task.","['huggingface-transformers', 'bert-language-model', 'huggingface-tokenizers']",77598369,"first of all, to add special tokens to your tokenizer, you should use add_tokens method. simply setting tokenizer.additional_special_tokens has no effect.
tokenizer = autotokenizer.from_pretrained(model_name, truncation_side=""left"")
tokenizer.add_tokens(additional_special_tokens, special_tokens=true)

print(tokenizer.added_tokens_encoder)
>>> {'[pad]': 0, '[unk]': 100, '[cls]': 101, '[sep]': 102, '[mask]': 103, '<pre>': 28996, '<suf>': 28997, '<mid>': 28998}

after doing this, you should also resize the model embeddings (i.e. initialize random embeddings for the new tokens we just added) (source: how to add new special token to the tokenizer? ):
model.resize_token_embeddings(len(tokenizer))

note that after training the model, it's a good idea to save the tokenizer using tokenizer.save_pretrained method, so that you can easily load it later.
second, to prepend the <pre> token to the tokenized text, you can directly modify the tokenized object:
def build_training_example(text):
    
    pre_token_id = tokenizer.get_vocab()['<pre>']
    
    tokenized = tokenizer(text)
    tokenized['input_ids'] = [pre_token_id] + tokenized['input_ids'][-511:]
    tokenized['attention_mask'] = [1] + tokenized['attention_mask'][-511:]
    
    return tokenized

this function will truncate the last 511 tokens from the tokenized ids, and prepend the id of the <pre> token.
i also added re-assigning the attention_mask, this might be useful if you have examples shorter than 512 and use padding to extend them to length 512, but you may or may not need this depending on how you plan to use the tokenized data. you may also consider using setting add_special_tokens=false in the tokenizer call, because if you want your data to always start with <pre> token, it may be a good idea to avoid the [cls] token, with which the tokenization starts if add_special_tokens is true.",https://stackoverflow.com/questions/77588646,huggingface-transformers,02-12-2023 00:05,612.0,1.0,1.0,True,04-12-2023 09:29,04-12-2023 00:05
69820318,"predicting sentiment of raw text using trained bert model, hugging face","i'm predicting sentiment analysis of tweets with positive, negative, and neutral classes. i've trained a bert model using hugging face. now i'd like to make predictions on a dataframe of unlabeled twitter text and i'm having difficulty.
i've followed the following tutorial ( and was able to train a bert model using hugging face.
here's an example of predicting on raw text however it's only one sentence and i would like to use a column of tweets. 
review_text = ""i love completing my todos! best app ever!!!""

encoded_review = tokenizer.encode_plus(
  review_text,
  max_length=max_len,
  add_special_tokens=true,
  return_token_type_ids=false,
  pad_to_max_length=true,
  return_attention_mask=true,
  return_tensors='pt',
)

input_ids = encoded_review['input_ids'].to(device)
attention_mask = encoded_review['attention_mask'].to(device)
output = model(input_ids, attention_mask)
_, prediction = torch.max(output, dim=1)
print(f'review text: {review_text}')
print(f'sentiment  : {class_names[prediction]}')

review text: i love completing my todos! best app ever!!!
sentiment  : positive


bill's response works. here's the solution.
def predictionpipeline(text):
  encoded_review = tokenizer.encode_plus(
      text,
      max_length=max_len,
      add_special_tokens=true,
      return_token_type_ids=false,
      pad_to_max_length=true,
      return_attention_mask=true,
      return_tensors='pt',
    )

  input_ids = encoded_review['input_ids'].to(device)
  attention_mask = encoded_review['attention_mask'].to(device)

  output = model(input_ids, attention_mask)
  _, prediction = torch.max(output, dim=1)

  return(class_names[prediction])

df2['prediction']=df2['cleaned_tweet'].apply(predictionpipeline)","['pytorch', 'sentiment-analysis', 'huggingface-transformers', 'pytorch-dataloader']",70023942,"you can use the same code to predict texts from the dataframe column.
model = ...
tokenizer = ...
    
def predict(review_text):
    encoded_review = tokenizer.encode_plus(
    review_text,
    max_length=max_len,
    add_special_tokens=true,
    return_token_type_ids=false,
    pad_to_max_length=true,
    return_attention_mask=true,
    return_tensors='pt',
    )

    input_ids = encoded_review['input_ids'].to(device)
    attention_mask = encoded_review['attention_mask'].to(device)
    output = model(input_ids, attention_mask)
    _, prediction = torch.max(output, dim=1)
    print(f'review text: {review_text}')
    print(f'sentiment  : {class_names[prediction]}')
    return class_names[prediction]


df = pd.dataframe({
            'texts': [""text1"", ""text2"", ""....""]
        })

df_dataset[""sentiments""] = df.apply(lambda l: predict(l.texts), axis=1)",https://stackoverflow.com/questions/69820318,pytorch,03-11-2021 06:07,2623.0,2.0,2.0,True,14-05-2022 14:43,19-11-2021 20:33
75795474,why did the bart-large-cnn summarization model giving funny output with different length settings?,"i have a piece of text of 4226 characters (316 words + special characters)
i am trying different combinations of min_length and max_length to get summary
print(summarizer(input, max_length = 1000, min_length=500, do_sample=false))

with the code:
the code is
summarizer = pipeline(""summarization"", model=""facebook/bart-large-cnn"")

input = """"""we see chatgpt as an engine that will eventually power human interactions with computer systems in a familiar, natural, and intuitive way. as chatgpt stated, large language models can be put to work as a communication engine in a variety of applications across a number of vertical markets. glaringly absent in its answer is the use of chatgpt in search engines. microsoft, which is an investor in openai, is integrating chatgpt into its bing search engine. the use of a large language model enables more complex and more natural searches and extract deeper meaning and better context from source material. this is ultimately expected to deliver more robust and useful results. is ai coming for your job? every wave of new and disruptive technology has incited fears of mass job losses due to automation, and we are already seeing those fears expressed relative to ai generally and chatgpt specifically. the year 1896, when henry ford rolled out his first automobile, was probably not a good year for buggy whip makers. when ibm introduced its first mainframe, the system/360, in 1964, office workers feared replacement by mechanical brains that never made mistakes, never called in sick, and never took vacations. there are certainly historical cases of job displacement due to new technology adoption, and chatgpt may unseat some office workers or customer service reps. however, we think ai tools broadly will end up as part of the solution in an economy that has more job openings than available workers. however, economic history shows that technology of any sort (i.e., manufacturing technology, communications technology, information technology) ultimately makes productive workers more productive and is net additive to employment and economic growth. how big is the opportunity? the broad ai hardware and services market was nearly usd 36bn in 2020, based on idc and bloomberg intelligence data. we expect the market to grow by 20% cagr to reach usd 90bn by 2025. given the relatively early monetization stage of conversational ai, we estimate that the segment accounted for 10% of the broader aiï¿½ï¿½ï¿½s addressable market in 2020, predominantly from enterprise and consumer subscriptions. that said, user adoption is rapidly rising. chatgpt reached its first 1 million user milestone in a week, surpassing instagram to become the quickest application to do so. similarly, we see strong interest from enterprises to integrate conservational ai into their existing ecosystem. as a result, we believe conversational aiï¿½ï¿½ï¿½s share in the broader aiï¿½ï¿½ï¿½s addressable market can climb to 20% by 2025 (usd 18ï¿½ï¿½uy prove to be conservative; they could be even higher if conversational ai improvements (in terms of computing power, machine learning, and deep learning capabilities), availability of talent, enterprise adoption, spending from governments, and incentives are stronger than expected. how to invest in ai? we see artificial intelligence as a horizontal technology that will have important use cases across a number of applications and industries. from a broader perspective, ai, along with big data and cybersecurity, forms what we call the abcs of technology. we believe these three major foundational technologies are at inflection points and should see faster adoption over the next few years as enterprises and governments increase their focus and investments in these areas. conservational ai is currently in its early stages of monetization and costs remain high as it is expensive to run. instead of investing directly in such platforms, interested investors in the short term can consider semiconductor companies, and cloud-service providers that provides the infrastructure needed for generative ai to take off. in the medium to long term, companies can integrate generative ai to improve margins across industries and sectors, such as within healthcare and traditional manufacturing. outside of public equities, investors can also consider opportunities in private equity (pe). we believe the tech sector is currently undergoing a new innovation cycle after 12ï¿½ï¿½ï¿½18 months of muted activity, which provides interesting and new opportunities that pe can capture through early-stage investments.""""""

print(summarizer(input, max_length = 1000, min_length=500, do_sample=false))



questions i have are:
q1: what does the following warning message mean?  your max_length is set to 1000, ...
your max_length is set to 1000, but you input_length is only 856. you might consider decreasing max_length manually, e.g. summarizer(ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½, max_length=428)
q3: of the above 2211 characters, first 933 characters are valid content from text but then it publishes text like

for confidential support call the samaritans on 08457 90 90 90 or
visit a local samaritans branch, see  for details.
for support ï¿½ï¿½ï¿½

q4: how does min_length and max_length actually work (it does not seems to follow the restrictions given to it)?
q5: what is the max input that i can actually give to this summarizer","['python', 'nlp', 'huggingface-transformers', 'summarization', 'large-language-model']",75796257,"q2: after above message this it publishes a summary of total 2211 characters. how did it get that?
a: the length that the model sees is not the no. of characters, so q2 is out-of-scope question. it's more appropriate to determine if the output of the model is shorter than the input no. of subwords tokens.
how we humanly decide no. of words is kinda different from how the model sees no. of tokens, i.e.
from transformers import autotokenizer

tokenizer = autotokenizer.from_pretrained(""facebook/bart-large-cnn"")

text = """"""we see chatgpt as an engine that will eventually power human interactions with computer systems in a familiar, natural, and intuitive way. as chatgpt stated, large language models can be put to work as a communication engine in a variety of applications across a number of vertical markets. glaringly absent in its answer is the use of chatgpt in search engines. microsoft, which is an investor in openai, is integrating chatgpt into its bing search engine. the use of a large language model enables more complex and more natural searches and extract deeper meaning and better context from source material. this is ultimately expected to deliver more robust and useful results. is ai coming for your job? every wave of new and disruptive technology has incited fears of mass job losses due to automation, and we are already seeing those fears expressed relative to ai generally and chatgpt specifically. the year 1896, when henry ford rolled out his first automobile, was probably not a good year for buggy whip makers. when ibm introduced its first mainframe, the system/360, in 1964, office workers feared replacement by mechanical brains that never made mistakes, never called in sick, and never took vacations. there are certainly historical cases of job displacement due to new technology adoption, and chatgpt may unseat some office workers or customer service reps. however, we think ai tools broadly will end up as part of the solution in an economy that has more job openings than available workers. however, economic history shows that technology of any sort (i.e., manufacturing technology, communications technology, information technology) ultimately makes productive workers more productive and is net additive to employment and economic growth. how big is the opportunity? the broad ai hardware and services market was nearly usd 36bn in 2020, based on idc and bloomberg intelligence data. we expect the market to grow by 20% cagr to reach usd 90bn by 2025. given the relatively early monetization stage of conversational ai, we estimate that the segment accounted for 10% of the broader aiï¿½ï¿½ï¿½s addressable market in 2020, predominantly from enterprise and consumer subscriptions. that said, user adoption is rapidly rising. chatgpt reached its first 1 million user milestone in a week, surpassing instagram to become the quickest application to do so. similarly, we see strong interest from enterprises to integrate conservational ai into their existing ecosystem.result, we believe conversational aiï¿½ï¿½ï¿½s share in the broader aiï¿½ï¿½ï¿½s addressable market can climb to 20% by 2025 (usd 18ï¿½ï¿½ï¿½20bn). our estimate may prove to be conservative; they could be even higher if conversational ai improvements (in terms of computing power, machine learning, and deep learning capabilities), availability of talent, enterprise adoption, spending from governments, and incentives are stronger than expected. how to invest in ai? we see artificial intelligence as a horizontal technology that will have important use cases across a number of applications and industries. from a broader perspective, ai, along with big data and cybersecurity, forms what we call the abcs of technology. we believe these three major foundational technologies are at inflection points and should see faster adoption over the next few years as enterprises and governments increase their focus and investments in these areas. conservational ai is currently in its early stages of monetization and costs remaipensive to run. instead of investing directly in such platforms, interested investors in the short term can consider semiconductor companies, and cloud-service providers that provides the infrastructure needed for generative ai to take off. in the medium to long term, companies can integrate generative ai to improve margins across industries and sectors, such as within healthcare and traditional manufacturing. outside of public equities, investors can also consider opportunities in private equity (pe). we believe the tech sector is currently undergoing a new innovation cycle after 12ï¿½ï¿½ï¿½18 months of muted activity, which provides interesting and new opportunities that pe can capture through early-stage investments.""""""

tokenized_text = tokenizer(text)

print(len(tokenized_text['input_ids']))

[out]:
800

we see that the input text you have in the example has 800 input subwords tokens, not 300 words.

q1: what does the following your max_length is set to 1000 ...
the warning message is as such:

your max_length is set to 1000, but you input_length is only 856. you might consider decreasing max_length manually, e.g. summarizer(ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½, max_length=428)

lets first try to put the input into the model and see the no. of tokens it outputs (without pipeline)
[code]:

from transformers import autotokenizer, automodelforseq2seqlm

tokenizer = autotokenizer.from_pretrained(""facebook/bart-large-cnn"")
model = automodelforseq2seqlm.from_pretrained(""facebook/bart-large-cnn"")


text = """"""we see chatgpt as an engine that will eventually power human interactions with computer systems in a familiar, natural, and intuitive way. as chatgpt stated, large language models can be put to work as a communication engine in a variety of applications across a number of vertical markets. glaringly absent in its of chatgpt in search engines. microsoft, which is an investor in openai, is integrating chatgpt into its bing search engine. the use of a large language model enables more complex and more natural searches and extract deeper meaning and better context from source material. this is ultimately expected to deliver more robust and useful results. is ai coming for your job? every wave of new and disruptive technology has incited fears of mass job losses due to automation, and we are already seeing those fears expressed relative to ai generally and chatgpt specifically. the year 1896, when henry ford rolled out his first automobile, was probably not a good year for buggy whip makers. when ibm introduced its first mainframe, the system/360, in 1964, office workers feared replacement by mechanical brains that never made mistakes, never called in sick, and never took vacations. there are certainly historical cases of job displacement due to new technology adoption, and chatgpt may unseat some office workers or customer service reps. however, we think ai tools broadly will end up as part of the solution in an economy that has more job openings than available workers. however, economic history shows that technology of any sort (i.e., manufacturing technology, communications technology, information technology) ultimately makes productive workers more productive and is net additive to employment and economic growth. how big is the opportunity? the broad ai hardware and services market was nearly usd 36bn in 2020, based on idc and bloomberg intelligence data. we expect the market to grow by 20% cagr to reach usd 90bn by 2025. given the relatively early monetization stage of conversational ai, we estimate that the segment accounted for 10% of the broader aiï¿½ï¿½ï¿½s addressable market in 2020, predominantly from enterprise and consumer subscriptions. that said, user adoption is rapidly rising. chatgpt reached its first 1 million user milestone in a week, surpassing instagram to become the quickest application to do so. similar see strong interest from enterprises to integrate conservational ai into their existing ecosystem. as a result, we believe conversational aiï¿½ï¿½ï¿½s share in the broader aiï¿½ï¿½ï¿½s addressable market can climb to 20% by 2025 (usd 18ï¿½ï¿½ï¿½20bn). our estimate may prove to be conservative; they could be even higher if conversational ai improvements (in terms of computing power, machine learning, and deep learning capabilities), availability of talent, enterprise adoption, spending from governments, and incentives are stronger than expected. how to invest in ai? we see artificial intelligence as a horizontal technology that will have important use cases across a number of applications and industries. from a broader perspective, ai, along with big data and cybersecurity, forms what we call the abcs of technology. we believe these three major foundational technologies are at inflection points and should see faster adoption over the next few years as enterprises and governments increase their focus and inveseas. conservational ai is currently in its early stages of monetization and costs remain high as it is expensive to run. instead of investing directly in such platforms, interested investors in the short term can consider semiconductor companies, and cloud-service providers that provides the infrastructure needed for generative ai to take off. in the medium to long term, companies can integrate generative ai to improve margins across industries and sectors, such as within healthcare and traditional manufacturing. outside of public equities, investors can also consider opportunities in private equity (pe). we believe the tech sector is currently undergoing a new innovation cycle after 12ï¿½ï¿½ï¿½18 months of muted activity, which provides interesting and new opportunities that pe can capture through early-stage investments.""""""

tokenized_text = tokenizer(text, return_tensors=""pt"")

outputs = model.generate(tokenized_text['input_ids'])

tokenizer.decode(outputs[0], skip_special_tokens=t
[stderr]:
/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1288: 

userwarning: using `max_length`'s default (142) to control the generation length. this behaviour is deprecated and will be removed from the config in v5 of transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.

[stdout]:

chatgpt is an engine that will eventually power human interactions with computer systems in a familiar, natural, and intuitive way. microsoft, which is an investor in openai, is integrating chatgpt into its bing search engine. the broad ai hardware and services market was nearly usd 36bn in 2020, based on idc and bloomberg intelligence data.

checking the output no. of tokens:
print(outputs.shape)

print(len(tokenizer.decode(outputs[0], skip_special_tokens=true)))


[out]:
torch.size([1, 73])
343

thus, the model summarizes the 800 subwords tokens input to an output of 73 subwords made up of 343 characters
not sure how you got an output of 2k+ chars though, so lets try with pipeline.
[code]:
from transformers import pipeline

summarizer = pipeline(""summarization"", model=""facebook/bart-large-cnn"")

text = """"""we see chatgpt as an engine that will eventually power human interactions with computer systems in a familiar, natural, and intuitive way. as chatgpt stated, large language models can be put to work as a communication engine in a variety of applications across a number of vertical markets. glaringly absent in its answer is the use of chatgpt in search engines. microsoft, which is an investor in openai, is integrating chatgpt into its bing search engine. the use of a large language model enables more complex and more natural searches and extract deeper meaning and better context from source material. this is ultimately expected to deliver more robust and useful results. is ai coming for your job? every wave of new and disruptive technology has incited fears of mass job losses due to automation, and we are already seeing those fears expressed relative to ai generally and chatgpt specifically. the year 1896, when henry ford rolled out his first automobile, was probably not a good year for buggy whip makers. when ibm introduced its first mainframe, the system/360, in 1964, office workers feared replacement by mechanical brains that never made mistakes, never called in sick, and never took vacations. there are certainly historical cases of job displacement due to new technology adoption, and chatgpt may unseat some office workers or customer service reps. however, we think ai tools broadly will end up as part of the solution in an economy that has more job openings than available workers. however, economic history shows that technology of any sort (i.e., manufacturing technology, communications technology, information technology) ultimately makes productive workers more productive and is net additive to employment and economic growth. how big is the opportunity? the broad ai hardware and services market was nearly usd 36bn in 2020, based on idc and bloomberg intelligence data. we expect the market to grow by 20% cagr to reach usd 90bn by 2025. given the relatively early monetization stage of conversational ai, we estimate that the segment accounted for 10% of the broader aiï¿½ï¿½ï¿½s addressable market in 2020, predominantly from enterprise and consumer subscriptions. that said, user adoption is rapidly rising. chatgpt reached its first 1 million user milestone in a week, surpassing instagram to become the quickest application to do so. similarly, we see strong interest from enterprises to integrate conservational ai into their existing ecosystem. as a result, we believe conversational aiï¿½ï¿½ï¿½s share in the broader aiï¿½ï¿½ï¿½s addressable market can climb to 20% by 2025 (usd 18ï¿½ï¿½ï¿½20bn). our estimate may prove to be conservative; they could be even hai improvements (in terms of computing power, machine learning, and deep learning capabilities), availability of talent, enterprise adoption, spending from governments, and incentives are stronger than expected. how to invest in ai? we see artificial intelligence as a horizontal technology that will have important use cases across a number of applications and industries. from a broader perspective, ai, along with big data and cybersecurity, forms what we call the abcs of technology. we believe these three major foundational technologies are at inflection points and should see faster adoption over the next few years as enterprises and governments increase their focus and investments in these areas. conservational ai is currently in its early stages of monetization and costs remain high as it is expensive to run. instead of investing directly in such platforms, interested investors in the short term can consider semiconductor companies, and cloud-service providers that provides the infrastructure needed for generative ai to take off. in the medium to long term, companies can integrate generative ai to improve margins across industries and sectors, such as within healthcare and traditional manufacturing. outside of public equities, investors can also consider opportunities in private equity (pe). we believe the tech sector is currently undergoing a new innovation cycle after 12ï¿½ï¿½ï¿½18 months of muted activity, which provides interesting and new opportunities that pe can capture through early-stage investments.""""""

output = summarizer(text)

print(output)

[out]:
[{'summary_text': 'chatgpt is an engine that will eventually power human interactions with computer systems in a familiar, natural, and intuitive way. microsoft, which is an investor in openai, is integrating chatgpt into its bing search engine. the broad ai hardware and services market was nearly usd 36bn in 2020, based on idc and bloomberg intelligence data.'}]

checking the size of the outp>
print(output[0]['summary_text'])

[out]:
343

this is consistent with how we use the model without pipeline, 343 characters summary.
q: does that mean that i don't have to set the max_new_tokens?
yeah, kind-of, you don't have to do anything since the summary is already shorter than the input text.
q: what does setting the max_new_tokens do?
we know that the default output summary gives us 73 tokens. lets try and see what happens if we set it down to 30 tokens!

from transformers import autotokenizer, automodelforseq2seqlm

tokenizer = autotokenizer.from_pretrained(""facebook/bart-large-cnn"")
model = automodelforseq2seqlm.from_pretrained(""facebook/bart-large-cnn"")


text = """"""we see chatgpt as an engine that will eventually power human interactions with computer systems in a familiar, natural, and intuitive way. as chatgpt stated, large language models can be put to work as a communication engine in a variety of applications across a number of vertical markets. glaringly absent in its answer is the use of chatgpt in search engines. microsoft, which is an investor in openai, is integrating chatgpt into its bing search engine. the use of a large language model enables more complex and more natural searches and extract deeper meaning and better context from source material. this is ultimately expected to deliver more robust and useful results. is ai coming for your job? every wave of new and disruptive technology has incited fears of mass job losses due to automation, and we are already seeing those fears expressed relative to ai generally and chatgpt specifically. the year 1896, when henry ford rolled out his first automobile, was probably not a good year for buggy whip makers. when ibm introduced its first mainframe, the system/360, in 1964, office workers feared replacement by mechanical brains that never made mistakes, never called in sick, and never took vacations. there are certainly historical cases of job displacement due to new technology adoption, and chatgpt may unseat some office workers or customer service reps. however, we think ai tools broadly will end up as part of the solution in an economy that has more job openings than available workers. however, economic history shows that technology of any sort (i.e., manufacturing technology, communications technology, information technology) ultimately makes productive workers more productive and is net additive to employment and economic growth. how big is the opportunity? the broad ai hardware and services market was nearly usd 36bn in 2020, based on idc and bloomberg intelligence data. we expect the market to grow by 20% cagr to reach usd 90bn by 2025. given the relatively early monetization stage of conversational ai, we estimate that the segment accounted for 10% of the broader aiï¿½ï¿½ï¿½s addressable market in 2020, predominantly from enterprise and consumer subscriptions. that said, user adois rapidly rising. chatgpt reached its first 1 million user milestone in a week, surpassing instagram to become the quickest application to do so. similarly, we see strong interest from enterprises to integrate conservational ai into their existing ecosystem. as a result, we believe conversational aiï¿½ï¿½ï¿½s share in the broader aiï¿½ï¿½ï¿½s addressable market can climb to 20% by 2025 (usd 18ï¿½ï¿½ï¿½20bn). our estimate may prove to be conservative; they could be even higher if conversational ai improvements (in terms of computing power, machine learning, and deep learning capabilities), availability of talent, enterprise adoption, spending from governments, and incentives are stronger than expected. how to invest in ai? we see artificial intelligence as a horizontal technology that will have important use cases across a number of applications and industries. from a broader perspective, ai, along with big data and cybersecurity, forms what we call the abcs of technology. we believe these three major foundat are at inflection points and should see faster adoption over the next few years as enterprises and governments increase their focus and investments in these areas. conservational ai is currently in its early stages of monetization and costs remain high as it is expensive to run. instead of investing directly in such platforms, interested investors in the short term can consider semiconductor companies, and cloud-service providers that provides the infrastructure needed for generative ai to take off. in the medium to long term, companies can integrate generative ai to improve margins across industries and sectors, such as within healthcare and traditional manufacturing. outside of public equities, investors can also consider opportunities in private equity (pe). we believe the tech sector is currently undergoing a new innovation cycle after 12ï¿½ï¿½ï¿½18 months of muted activity, which provides interesting and new opportunities that pe can capture through early-stage investments.""""""

tokenized_ tokenizer(text, return_tensors=""pt"")

outputs = model.generate(tokenized_text['input_ids'], max_new_tokens=30)

[stderr]:
valueerror                                traceback (most recent call last)
<ipython-input-26-665cd5fbe802> in <module>
      3 tokenized_text = tokenizer(text, return_tensors=""pt"")
      4 
----> 5 model.generate(tokenized_text['input_ids'], max_new_tokens=30)

1 frames
/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py in generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)
   1304 
   1305         if generation_config.min_length is not none and generation_config.min_length > generation_config.max_length:
-> 1306             raise valueerror(
   1307                 f""unfeasible length constraints: the minimum length ({generation_config.min_length}) is larger than""
   1308                 f"" the maximum length ({generation_config.max_length})""

valueerror: unfeasible length constraints: the minimum length (56) is larger than the maximum length (31)

ah ha, there's some minimum length that the model wants to output as the summary!
so lets just try to set it to 60
tokenized_text = tokenizer(text, return_tensors=""pt"")

outputs = model.generate(tokenized_text['input_ids'], max_new_tokens=60)

print(tokenizer.decode(outputs[0], skip_special_tokens=true))

[out]:
chatgpt is an engine that will eventually power human interactions with computer systems in a familiar, natural, and intuitive way. microsoft, which is an investor in openai, is integrating chatgpt into its bing search engine. the broad ai hardware and services market was nearly usd 36bn

we see that now the summarized output is shorter than the 73 default output and fits into the 60 max_new_tokens limit we set.
and if we check the print(len(outputs[0])), we get 61 subwords tokens, the additional one off the max_new_tokens is to account for the end of sentence symbol. if you print the outputs, you'll see that the first token id is 2 which is represented by the </s> token.
when you specify the skip_special_tokens=true it will delete the </s> token, as well as the start of sentence tokens <s>.

q4: how does min_length and max_length actually work (it does not seems to follow the restrictions given to it)?
given the above examples, the min_length is actually hard to determine since the model has to decide the minimum subwords tokens it needs to get a good summary output. remember the unfeasible length constraints: the minimum length (56) ... warning?

q5: what is the max input that i can actually give to this summarizer?
the sensible max_length or more appropriately max_new_tokens is most probably going to lower than your input length and if there's some sort of ui limitations or compute/latency limitations, it's best to keep it low and close to whatever is needed.
i.e., to set the max_new_tokens, just make sure it's lower than the input text no. of tokens and sensible enough for your application. if you want to know a ballpark no. try the model without setting the limit and see if the summary output is how you expect the model to behave, then adjust appropriately.
like seasoning while cooking, ""add/reduce max_new_tokens as desired""

q3: of the above 2211 characters, first 933 characters are valid content from text but then it publishes text like ...
when setting the min_length to some arbitrarily large number, way larger than the default output of the model, i.e. 73 subwords,
print(summarizer(text, max_length=900, min_length=300, do_sample=false))

print(summarizer(text, max_length=900, min_length=500, do_sample=false))

then it will warn you,
[sterr]:
your max_length is set to 900, but you input_length is only 800. you might consider decreasing max_length manually, e.g. summarizer('...', max_length=400)

it will start hallucinating things beyond the first 300-ish subwords tokens. possibly, the model thinks that beyond 300-ish subwords, nothing else from the input text is important.
and output looks something like:
[{'summary_text': 'chatgpt is an engine that will eventually power human interactions with computer systems in a familiar, natural, and intuitive way. microsoft, which is an investor in openai, is integrating chatgpt into its bing search engine. ... they recommend semiconductor companies, cloud-service providers that provides the infrastructure needed for generative ai to take off, and private equity firms that provide the infrastructure for cloud-based services. they also suggest investors can consider opportunities in private equity (pe) to invest in ai platforms in the short-term and in the medium to long-term.'}]

[{'summary_text': ""chatgpt is an engine that will eventually power human interactions with computer systems in a familiar, natural, and intuitive way. microsoft, which is an investor in openai, is integrating chatgpt into its bing search engine. ... they say ai tools broadly will end up as part of the solution in an economy that has more job openings than available workers. the technology of any sort (i.e., manufacturing technology, communications technology, information technology) ultimately makes productive workers more productive and is net additive to employment and economic growth, they say. the authors believe the tech sector is currently undergoing a new innovation cycle after 12ï¿½ï¿½ï¿½18 months of muted activity, which provides interesting and new opportunities that pe can capture through early-stage investments. they recommend semiconductor comp cloud-service providers that provides the infrastructure needed for generative ai to take off, and private equity firms that provide the infrastructure for cloud-based services. they also suggest investors can consider opportunities in private equity (pe) to invest in ai platforms in the short-term and in the medium to long-term, such as within healthcare and traditional manufacturing. the author's firm is based in new york and they have worked with microsoft, google, facebook, and others on ai projects in the past. the firm has also worked with google, microsoft, facebook and others to develop ai products and services in the u.s. and abroad. for confidential support, call the national suicide prevention lifeline at 1-800-273-8255 or visit  for confidential. support on suicide matters call the samaritans on 08457 90 90 90 or visit a local samaritans branch or click here for details. in the uk, contact samaritans at 08457 909090 or visit\xa0the samaritansï¿½ï¿½ï¿½\xa0 helpline at   for details on how to get involved in the ukï¿½ï¿½ï¿½s national suicide prevention lifeline (in the uk or the uk). for confidential help in the united states, call\xa0the national suicide prevention line at\xa0800\xa0273\xa08255.""}]

q: why did the model start hallucinating beyond 300 subwords?
good question and also an active research area, see  and there are many more in that area.
[opinion]: personally, hunch says, it's most probably because most of the data that the model learnt from where the text is 800-ish subwords, the summary it trained are between the length of 80-300 subwords. and training data points where there are 300-500 subwords in the summary, it always contains the sos helpline. so the model starts to overfit whenever it reaches that min_length that is >300.
to prove the hunch pudding, try another random text of 800-ish subwords, and then set the min_length again to 500, it will most probably hallucinate the sos sentence again beyond 300-ish subwords.",https://stackoverflow.com/questions/75795474,python,20-03-2023 21:26,3270.0,1.0,1.0,True,21-03-2023 00:41,21-03-2023 00:24
70791550,running a for loop or .apply with a pandas series,"i'm trying to run a for loop or .apply using lambdas for my pandas series. here's the code:
df['sentiment_score'] = df.apply(lambda x: analyzer.polarity_scores(x['filtered_text']), axis=1)

what i'm trying to achieve is for each word in df['filtered_text'], apply the analyzer.polartiy_scores(x['filtered_text']) through the column.
an example of what is stored in df['filtered_text']:
[website, needs, convinient, terrible]
[filters, mobile, version, site]

so for every one of those words, i'd like it to be applied to the analyzer.polarity_scores
i've also tried this:
df['sentiment_score'] = df['filtered_text'].apply(lambda x: analyzer.polarity_scores(x))

but i get this error:

attributeerror: 'list' object has no attribute 'encode'

and this:
df['sentiment_score'] = df['filtered_text'].apply(lambda x: [ft for ft in x: analyzer.polarity_scores(x)])

thanks","['python', 'pandas', 'nltk']",70791800,"i would use a list comprehension to solve this:
df['sentiment_score'] = df['filtered_text'].apply(lambda x: [analyzer.polarity_scores(e) for e in x])",https://stackoverflow.com/questions/70791550,python,20-01-2022 18:28,497.0,0.0,1.0,True,20-01-2022 18:46,20-01-2022 18:40
69286889,transformers and bert downloading to your local machine,"i am trying to replicates the code from this page.
at my workplace we have access to transformers and pytorch library but cannot connect to internet from our python environment. could anyone help with how we could get the script working after manually downloading files to my machine?
my specific questions are -

should i go to the location bert-base-uncased at main and download all the files? do i have put them in a folder with a specific name?



how should i change the below code
# load pre-trained model tokenizer (vocabulary)
tokenizer = berttokenizer.from_pretrained('bert-base-uncased')
# tokenize our sentence with the bert tokenizer.
tokenized_text = tokenizer.tokenize(marked_text)




how should i change the below code
# load pre-trained model (weights)
model = bertmodel.from_pretrained('bert-base-uncased',

                                  output_hidden_states = true, # whether the model returns all hidden-states.

                              )

please let me know if anyone has done thisï¿½ï¿½ï¿½thanks
###update1
i went to the link and manually downloaded all files to a folder and specified path of that folder in my code. tokenizer works but this line model = bertmodel.from_pretrained('bert-base-uncased', output_hidden_states = true, # whether the model returns all hidden-states. ) fails. any idea what should i do? i noticed that 4 big files when downloaded have very strange name...should i rename them to same names as shown on the above page? do i need to download any other files?
the error message is oserrr: unable to load weights from pytorch checkpoint file for bert-base-uncased2/ at bert-base-uncased/pytorch_model.bin if you tried to load a pytroch model from a tf 2 checkpoint, please set from_tf=true","['python', 'torch', 'bert-language-model', 'transformer-model', 'doc2vec']",69287116,"clone the model repo for downloading all the files
git lfs install
git clone 

# if you want to clone without large files ï¿½ï¿½ï¿½ just their pointers
# prepend your git clone with the following env var:
git_lfs_skip_smudge=1

git usage:

download git from here 

paste these to your cli(terminal):
a. git lfs install
b. git clone 

wait for download, it will take time. if you want monitor your web performance

find the current directory simply pasting cd to your cli and get the file path(e.g ""c:/users/........./bert-base-uncased"" )

use it as:
 from transformers import bertmodel, berttokenizer
 model = bertmodel.from_pretrained(""c:/users/........./bert-base-uncased"")
 tokenizer = berttokenizer.from_pretrained(""c:/users/........./bert-base-uncased"")



manual download, without git:

download all the files from here 

put them in a folder named ""yourfoldername""

use it as:
 model = bertmodel.from_pretrained(""c:/users/........./yourfoldername"")
 tokenizer = berttokenizer.from_pretrained(""c:/users/........./yourfoldername"")



for only model(manual download, without git):

just click the download button here and download only pytorch pretrained model. its about 420mb


download config.json file from here 

put both of them in a folder named ""yourfilename""

use it as:
 model = bertmodel.from_pretrained(""c:/users/........./yourfilename"")",https://stackoverflow.com/questions/69286889,python,22-09-2021 15:07,20079.0,1.0,2.0,True,01-05-2022 15:15,23-09-2021 13:40
71417026,make spacy tokenizer not split on /,"how do i modify the english tokenizer to prevent splitting tokens on the '/' character?
for example, the following string should be one token:

import spacy

nlp = spacy.load('en_core_web_md')
doc = nlp(""12/ab/568793"")

for t in doc:
    print(f""[{t.pos_} {t.text}]"")

# produces
#[num 12]
#[sym /]
#[adj ab/568793]","['python', 'nlp', 'spacy']",71417718,"the approach is a variation on removing a rule in the ""modifying existing rule sets"" from spacy documentation:

nlp = spacy.load('en_core_web_md')
infixes = nlp.defaults.infixes
assert(len([x for x in infixes if '/' in x])==1)  # there seems to just be one rule that splits on /'s
# remove that rule; then modify the tokenizer
infixes = [x for x in infixes if '/' not in x]
nlp.tokenizer.infix_finditer = spacy.util.compile_infix_regex(infixes).finditer",https://stackoverflow.com/questions/71417026,python,09-03-2022 23:08,969.0,2.0,2.0,True,01-11-2022 16:35,09-03-2022 23:15
44855603,typeerror: can&#39;t pickle _thread.lock objects in seq2seq,"i'm having trouble using buckets in my tensorflow model. when i run it with buckets = [(100, 100)], it works fine. when i run it with buckets = [(100, 100), (200, 200)] it doesn't work at all (stacktrace at bottom).
interestingly, running tensorflow's seq2seq tutorial gives the same kind of issue with a nearly identical stacktrace. for testing purposes, the link to the repository is here. 
i'm not sure what the issue is, but having more than one bucket always seems to trigger it.
this code won't work as a standalone, but this is the function where it is crashing - remember that changing buckets from [(100, 100)] to [(100, 100), (200, 200)] triggers the crash.
class myseq2seq(object):
    def __init__(self, source_vocab_size, target_vocab_size, buckets, size, num_layers, batch_size, learning_rate):
        self.source_vocab_size = source_vocab_size
        self.target_vocab_size = target_vocab_size
        self.buckets = buckets
        self.batch_size = batch_size

        cell = single_cell = tf.nn.rnn_cell.grucell(size)
        if num_layers > 1:
            cell = tf.nn.rnn_cell.multirnncell([single_cell] * num_layers)

        # the seq2seq function: we use embedding for the input and attention
        def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):
            return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(
                encoder_inputs, decoder_inputs, cell,
                num_encoder_symbols=source_vocab_size,
                num_decoder_symbols=target_vocab_size,
                embedding_size=size,
                feed_previous=do_decode)

        # feeds for inputs
        self.encoder_inputs = []
        self.decoder_inputs = []
        self.target_weights = []
        for i in range(buckets[-1][0]):
            self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[none], name=""encoder{0}"".format(i)))
        for i in range(buckets[-1][1] + 1):
            self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[none], name=""decoder{0}"".format(i)))
            self.target_weights.append(tf.placeholder(tf.float32, shape=[none], name=""weight{0}"".format(i)))

        # our targets are decoder inputs shifted by one
        targets = [self.decoder_inputs[i + 1] for i in range(len(self.decoder_inputs) - 1)]
        self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(
            self.encoder_inputs, self.decoder_inputs, targets,
            self.target_weights, [(100, 100)],
            lambda x, y: seq2seq_f(x, y, false))

        # gradients update operation for training the model
        params = tf.trainable_variables()
        self.updates = []
        for b in range(len(buckets)):
            self.updates.append(tf.train.adamoptimizer(learning_rate).minimize(self.losses[b]))

        self.saver = tf.train.saver(tf.global_variables())

stacktrace:
    traceback (most recent call last):
  file ""d:/stuff/ideaprojects/myproject/src/main.py"", line 38, in <module>
    model = predict.make_model(input_vocab_size, output_vocab_size, buckets, cell_size, model_layers, batch_size, learning_rate)
  file ""d:\stuff\ideaprojects\myproject\src\predictor.py"", line 88, in make_model
    size=cell_size, num_layers=model_layers, batch_size=batch_size, learning_rate=learning_rate)
  file ""d:\stuff\ideaprojects\myproject\src\predictor.py"", line 45, in __init__
    lambda x, y: seq2seq_f(x, y, false))
  file ""c:\users\user\appdata\local\programs\python\python36\lib\site-packages\tensorflow\contrib\legacy_seq2seq\python\ops\seq2seq.py"", line 1206, in model_with_buckets
    decoder_inputs[:bucket[1]])
  file ""d:\stuff\ideaprojects\myproject\src\predictor.py"", line 45, in <lambda>
    lambda x, y: seq2seq_f(x, y, false))
  file ""d:\stuff\ideaprojects\myproject\src\predictor.py"", line 28, in seq2seq_f
    feed_previous=do_decode)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\site-packages\tensorflow\contrib\legacy_seq2seq\python\ops\seq2seq.py"", line 848, in embedding_attention_seq2seq
    encoder_cell = copy.deepcopy(cell)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 161, in deepcopy
    y = copier(memo)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\layers\base.py"", line 476, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 215, in _deepcopy_list
    append(deepcopy(a, memo))
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  file ""c:\users\user\appdata\local\programs\python\python36\lib\copy.py"", line 169, in deepcopy
    rv = reductor(4)
typeerror: can't pickle _thread.lock objects","['python-3.x', 'tensorflow', 'nlp', 'lstm', 'sequence-to-sequence']",47952913,"the problem is with latest changes in seq2seq.py. add this to your script and it will avoid deep-coping of the cells:
setattr(tf.contrib.rnn.grucell, '__deepcopy__', lambda self, _: self)
setattr(tf.contrib.rnn.basiclstmcell, '__deepcopy__', lambda self, _: self)
setattr(tf.contrib.rnn.multirnncell, '__deepcopy__', lambda self, _: self)",https://stackoverflow.com/questions/44855603,python-3.x,30-06-2017 22:31,28008.0,16.0,2.0,True,09-10-2022 20:53,23-12-2017 14:24
74834890,how to detect the language used in a column and put it in a new column?,"i have the following df:
df = pd.dataframe({
    'user': ['id159', 'id758', 'id146', 'id477', 'id212', 'id999'],
    'comment' : [""i inboxed you"", '123', 123, 'je suis fatiguï¿½ï¿½', ""j'aime"", 'ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½']  
})

it has the following display:
    user    comment
0   id159   i inboxed you
1   id758   123
2   id146   123
3   id477   je suis fatiguï¿½ï¿½
4   id212   j'aime
5   id999   ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½

my goal is to get a new column containing language used in the column df['comment'] as follows:
    user    comment         language
0   id159   i inboxed you   en
1   id758   123             unknown
2   id146   123             unknown
3   id477   je suis fatiguï¿½ï""lang-py prettyprint-override"">from langdetect import detect

df['language'] = [detect(x) for x in df['comment']]

when i tried to use detect i faced the following message error:
langdetectexception: no features in text.

i tried to add an if else statement but douldn't solve the issue.
any help from your side will be highly appreciated (i upvote all answers)
than you!","['python', 'pandas', 'dataframe', 'language-detection']",74835079,"it would be better if you clarify all exception cases you want to set as unknown.
anyway, i assume you want to set non-string and numeric into unknown.
then,
df[""language""] = [
    detect(x) if isinstance(x, str) and not x.isnumeric() else ""unknown""
    for x in df[""comment""]
]

edit:
or for more general approach (though not really recommended) you can just use exception handling
def f(x):
    try:
        return detect(x)
    except:
        return ""unknown""

df[""language""] = [f(x) for x in df[""comment""]]",https://stackoverflow.com/questions/74834890,python,17-12-2022 14:29,862.0,2.0,3.0,True,17-12-2022 15:05,17-12-2022 14:39
64445784,how can i apply pruning on a bert model?,"i have trained a bert model using ktrain (tensorflow wrapper) to recognize emotion on text. it works, but it suffers from really slow inference. that makes my model not suitable for a production environment. i have done some research, and it seems pruning could help.
tensorflow provides some options for pruning, e.g., tf.contrib.model_pruning. the problem is that it is not a not a widely used technique. what would be a simple enough example that could help me to understand how to use it?
i provide my working code below for reference.
import pandas as pd
import numpy as np
import preprocessor as p
import emoji
import re
import ktrain
from ktrain import text
from unidecode import unidecode
import nltk

# text preprocessing class
class textpreprocessing:
    def __init__(self):
        p.set_options(p.opt.mention, p.opt.url)

    def _punctuation(self, val):
        val = re.sub(r'[^\w\s]', ' ', val)
        val = re.sub('_', ' ', val)
        return val

    def _whitespace(self, val):
        return "" "".join(val.split())

    def _removenumbers(self, val):
        val = re.sub('[0-9] + ', '', val)
        return val

    def _remove_unicode(self, text):
        text = unidecode(text).encode(""ascii"")
        text = str(text, ""ascii"")
        return text

    def _split_to_sentences(self, body_text):
        sentences = re.split(r""(?<!\w\.\w.)(?<![a-z][a-z]\.)(?<=\.|\?)\s"", body_text)
        return sentences

    def _clean_text(self, val):
        val = val.lower()
        val = self._removenumbers(val)
        val = p.clean(val)
        val = ' '.join(self._punctuation(emoji.demojize(val)).split())
        val = self._remove_unicode(val)
        val = self._whitespace(val)
        return val

    def text_preprocessor(self, body_text):

        body_text_df = pd.dataframe({""body_text"": body_text}, index=[1])

        sentence_split_df = body_text_df.copy()

        sentence_split_df[""body_text""] = sentence_split_df[""body_text""].apply(
            self._split_to_sentences)

        lst_col = ""body_text""
        sentence_split_df = pd.dataframe(
            {
                col: np.repeat(
                    sentence_split_df[col].values, sentence_split_df[lst_col].str.len(
                    )
                )
                for col in sentence_split_df.columns.drop(lst_col)
            }
        ).assign(**{lst_col: np.concatenate(sentence_split_df[lst_col].values)})[
            sentence_split_df.columns
        ]

        body_text_df[""body_text""] = body_text_df[""body_text""].apply(self._clean_text)

        final_df = (
            pd.concat([sentence_split_df, body_text_df])
            .reset_index()
            .drop(columns=[""index""])
        )

        return final_df[""body_text""]

# instantiate data preprocessing object
text1 = textpreprocessing()

# import data
data_train = pd.read_csv('data_train_v5.csv', encoding='utf8', engine='python')
data_test = pd.read_csv('data_test_v5.csv', encoding='utf8', engine='python')

# clean the data
data_train['text'] = data_train['text'].apply(text1._clean_text)
data_test['text'] = data_test['text'].apply(text1._clean_text)

x_train = data_train.text.tolist()
x_test = data_test.text.tolist()

y_train = data_train.emotion.tolist()
y_test = data_test.emotion.tolist()

data = data_train.append(data_test, ignore_index=true)

class_names = ['joy', 'sadness', 'fear', 'anger', 'neutral']

encoding = {
    'joy': 0,
    'sadness': 1,
    'fear': 2,
    'anger': 3,
    'neutral': 4
}

# integer values for each class
y_train = [encoding[x] for x in y_train]
y_test = [encoding[x] for x in y_test]

trn, val, preproc = text.texts_from_array(x_train=x_train, y_train=y_train,
                                          x_test=x_test, y_test=y_test,
                                          class_names=class_names,
                                          preprocess_mode='distilbert',
                                          maxlen=350)

model = text.text_classifier('distilbert', train_data=trn, preproc=preproc)

learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)

predictor = ktrain.get_predictor(learner.model, preproc)

# save the model on a file for later use
predictor.save(""models/bert_model"")

message = ""this is a happy message""

# cleaning - takes 5 ms to run
clean = text1._clean_text(message)

# prediction - takes 325 ms to run
predictor.predict_proba(clean)","['python', 'tensorflow', 'nlp', 'bert-language-model', 'huggingface-transformers']",64450767,"the distilbert model in ktrain is created using hugging face transformers, which means you can use that library to prune the model.  see this link for more information and the example script. you may need to convert the model to pytorch before using the script (in addition to making some modifications to the script itself). the approach is based on the paper are sixteen heads really better than one?.",https://stackoverflow.com/questions/64445784,python,20-10-2020 13:04,1711.0,4.0,1.0,True,11-01-2022 10:09,10-01-2022 13:21
76012755,how can i remove some unwanted characters in dictionary values,"i extracted some search keywords and their corresponding text and put them into a dictionary using python. a sample of dictionary looks as follows:
{'id': 'id', 'he': 'q1 - lth', 'lt': 'q2 - la tor', 'hip': 'q3a - hh sure', 'mhbp': 'q3.1.a -  pressure ', 'dite': 'q3b - dates'}

how can i remove all the q and the letters to - (including - )?
output should looks like :

{'id': 'id', 'he': 'lth', 'lt': 'la tor', 'hip': 'hh sure', 'mhbp': 'pressure ', 'dite': 'dates'}","['python', 'nlp']",76012866,"you can do the following
import re
pattern = re.compile(r""(^\s*q.*\-\s*)"")
d = {'id': 'id', 'he': 'q1 - lth', 'lt': 'q2 - la tor', 
     'hip': 'q3a - hh sure', 'mhbp': 'q3.1.a -  pressure ', 'dite': 'q3b - dates'}
d = {k:pattern.sub("""", v) for (k, v) in d.items()}
d",https://stackoverflow.com/questions/76012755,python,14-04-2023 07:52,72.0,-2.0,1.0,True,17-04-2023 20:21,17-04-2023 20:21
75442916,openai gpt-3 api: how to preserve formatting when pasting content into an excel cell?,"i'm trying to create a fine tuned gpt-3 model.  to that end, i have some content that i've formatted in word that im trying to bring to excel (in order to import the training dataset).
my inputs are in the format:
*point a
    *subpoint a1
    *subpoint a2
*point b
    *subpoint b1
    *subpoint b2

however, when i copy the contents into excel, the excel cell converts this to:
*point a
*subpoint a1
*subpoint a2
*point b
*subpoint b1
*subpoint b2

is there any way for me to preserve my original formatting?
is there any other way better way than this?
any help is appreciated greatly :)
regards,
galeej","['excel', 'openai-api', 'gpt-3']",75447943,"it's definitely possible to copy-paste to excel and keep the original formatting. what you need to do is choose keep source formatting under paste options.
but, i don't think the fine-tuned model will return a completion with the original formatting (if that's your goal).

edit
try using the cli preparation tool to get some suggestions on how to structure the training dataset.",https://stackoverflow.com/questions/75442916,excel,14-02-2023 02:24,546.0,0.0,1.0,True,13-03-2023 14:38,13-03-2023 14:38
71755917,how do i load a fine-tuned allennlp bert-srl model using bertpretrainedmodel.from_pretrained()?,"i have fine-tuned a bert model for semantic role labeling, using allennlp. this produces a model directory (serialization directory, if i recall?) that contains the following:
best.th
config.json
meta.json
metrics_epoch_0.json
metrics_epoch_10.json
metrics_epoch_11.json
metrics_epoch_12.json
metrics_epoch_13.json
metrics_epoch_14.json
metrics_epoch_1.json
metrics_epoch_2.json
metrics_epoch_3.json
metrics_epoch_4.json
metrics_epoch_5.json
metrics_epoch_6.json
metrics_epoch_7.json
metrics_epoch_8.json
metrics_epoch_9.json
metrics.json
model_state_e14_b0.th
model_state_e15_b0.th
model.tar.gz
out.log
training_state_e14_b0.th
training_state_e15_b0.th
vocabulary

where vocabulary is a folder with labels.txt and non_padded_namespaces.txt.
i'd now like to use this fine-tuned model bert model as the initialization when learning a related task, event extraction, using this library:  (ie i want to exploit some transfer learning). the config.ini file has a line for fine_tuned_path, where i can specify an already-fine-tuned model that i want to use here. i provided the path to the allennlp serialization directory, and i got the following error:
2022-04-05 13:07:28,112 -  info - setting seed 23
2022-04-05 13:07:28,113 -  info - loading fine tuned model in /data/projects/srl/ser_pure_clinical_bert-large_thyme_and_ontonotes/
traceback (most recent call last):
  file ""main.py"", line 65, in <module>
    model = bert_ee()
  file ""/data/projects/srl/bert-ee/model.py"", line 88, in __init__
    self.__build(self.use_fine_tuned)
  file ""/data/projects/srl/bert-ee/model.py"", line 118, in __build
    self.__get_pretrained(self.fine_tuned_path)
  file ""/data/projects/srl/bert-ee/model.py"", line 110, in __get_pretrained
    self.__model = bert_ee_model.from_pretrained(path)
  file ""/home/richier/anaconda3/envs/allennlp/lib/python3.7/site-packages/transformers/modeling_utils.py"", line 1109, in from_pretrained
    f""error no file named {[weights_name, tf2_weights_name, tf_weights_name + '.index', flax_weights_name]} found in ""
oserror: error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack'] found in directory /data/projects/srl/ser_pure_clinical_bert-large_thyme_and_ontonotes/ or `from_tf` and `from_flax` set to false.

of course, the serialization directory doesn't have any of those files, hence the error. i tried unzipping model.tar.gz but it only has:
config.json
weights.th
vocabulary/
vocabulary/.lock
vocabulary/labels.txt
vocabulary/non_padded_namespaces.txt
meta.json

digging into the codebase of the github repo i linked above, i can see that bert_ee_model inherits from bertpretrainedmodel from the transformers library, so the trick would seem to be getting the allennlp model into a format that bertpretrainedmodel.from_pretrained() can load...?
any help would be greatly appreciated!","['python', 'pytorch', 'huggingface-transformers', 'bert-language-model', 'allennlp']",71787657,"i believe i have figured this out. basically, i had to re-load my model archive, access the underlying model and tokenizer, and then save those:
from allennlp.models.archival import load_archive
from allennlp_models.structured_prediction import semanticrolelabeler, srl, srl_bert

archive = load_archive('ser_pure_clinical_bert-large_thyme_and_ontonotes/model.tar.gz')

bert_model = archive.model.bert_model #type is transformers.models.bert.modeling_bert.bertmodel
bert_model.save_pretrained('ser_pure_clinical_bert-large_thyme_and_ontonotes_save_pretrained/')

bert_tokenizer = archive.dataset_reader.bert_tokenizer
bert_tokenizer.save_pretrained('ser_pure_clinical_bert-large_thyme_and_ontonotes_save_pretrained/')

(this last part is probably less interesting to most folks, but also, in the config.ini i mentioned, the directory 'ser_pure_clinical_bert-large_thyme_and_ontonotes_save_pretrained' needed to be passed to the line pretrained_model_name_or_path not to fine_tuned_path.)",https://stackoverflow.com/questions/71755917,python,05-04-2022 17:21,666.0,0.0,1.0,True,07-04-2022 19:04,06-04-2022 23:20
75400926,openai chatgpt api: cors policy error when fetching data,"i am trying to write a simple javascript script which uses the chatgpt api to ask a question and get a response.
however i am getting the following error message:

""access to fetch at
'
from origin ' has been
blocked by cors policy: no 'access-control-allow-origin' header is
present on the requested resource. if an opaque response serves your
needs, set the request's mode to 'no-cors' to fetch the resource with
cors disabled.""

i have enabled cors headers server side in my hosting environment. but the error remains.
what is the reason for this issue and how can i fix this issue?
here is my code:
<html>
<head>
  <script>
    function askquestion() {
      var question = document.getelementbyid(""questioninput"").value;
      var apikey = document.getelementbyid(""apikey"").value;
      // access chatgpt's api and pass in the question and api key as parameters
      fetch("" + question + ""&api_key="" + apikey)
        .then(response => {
          if (!response.ok) {
            throw new error(""failed to fetch answer from api"");
          }
          return response.json();
        })
        .then(data => {
          // get the answer from the api response and display it in the textbox
          document.getelementbyid(""answerbox"").value = data.answer;
        })
        .catch(error => {
          console.error(""error fetching answer from api: "", error);
        });
    }

    function askfollowupquestion() {
      var followupquestion = document.getelementbyid(""followupquestioninput"").value;
      var apikey = document.getelementbyid(""apikey"").value;
      // access chatgpt's api and pass in the follow-up question and api key as parameters
      fetch("" + followupquestion + ""&api_key="" + apikey)
        .then(response => {
          if (!response.ok) {
            throw new error(""failed to fetch answer from api"");
          }
          return response.json();
        })
        .then(data => {
          // get the answer from the api response and display it in the textbox
          document.getelementbyid(""followupanswerbox"").value = data.answer;
        })
        .catch(error => {
          console.error(""error fetching answer from api: "", error);
        });
    }
  </script>
</head>
<body>
  <input type=""text"" id=""questioninput"" placeholder=""enter your question here""></input>
  <br>
  <input type=""text"" id=""apikey"" placeholder=""enter your api key""></input>
  <br>
  <button onclick=""askquestion()"">ask</button>
  <br>
  <textarea id=""answerbox"" readonly></textarea>
  <br>
  <input type=""text"" id=""followupquestioninput"" placeholder=""enter your follow-up question here""></input>
  <br>
  <button onclick=""askfollowupquestion()"">ask follow-up</button>
  <br>
  <textarea id=""followupanswerbox"" readonly></textarea>
</body>
</html>","['javascript', 'openai-api', 'gpt-3']",75401250,"update: 1 march 2023
chatgpt api (i.e., chat completions api) is now available
as stated in the official openai blog:

chatgpt and whisper models are now available on our api, giving
developers access to cutting-edge language (not just chat!) and
speech-to-text capabilities. through a series of system-wide
optimizations, weï¿½ï¿½ï¿½ve achieved 90% cost reduction for chatgpt since
december; weï¿½ï¿½ï¿½re now passing through those savings to api users.
developers can now use our open-source whisper large-v2 model in the
api with much faster and cost-effective results. chatgpt api users can
expect continuous model improvements and the option to choose
dedicated capacity for deeper control over the models. weï¿½ï¿½ï¿½ve also
listened closely to feedback from our developers and refined our api
terms of service to better meet their needs.

<"" rel=""nofollow noreferrer"">documentation.

chatgpt api is not available yet
as stated on the official openai twitter profile:

we've learned a lot from the chatgpt research preview and have been
making important updates based on user feedback. chatgpt will be
coming to our api and microsoft's azure openai service soon.

did you mean the gpt-3 api? if yes, then read the documentation, see the list of all available gpt-3 models, and learn how to write the code using the completions endpoint.",https://stackoverflow.com/questions/75400926,javascript,09-02-2023 15:37,9036.0,3.0,1.0,True,29-08-2023 17:29,13-03-2023 14:24
74623917,how to tokenize block of text as one token in python?,"recently i am working on a genome data set which consists of many blocks of genomes. on previous works on natural language processing, i have used sent_tokenize and word_tokenize from nltk to tokenize the sentences and words. but when i use these functions on genome data set, it is not able to tokenize the genomes correctly. the text below shows some part of the genome data set.
>nr_004049 1
tattattatacacaatcccggggcgttctatatagttatgtataatgtat
atttatattatttatgcctctaactggaacgtaccttgagcatatatgct
gtgacccgaaagatggtgaactatacttgatcaggttgaagtcaggggaa
accctgatggaagaccgaaacagttctgacgtgcaaatcgattgtcagaa
ttgagtataggggcgaaagaccaatcgaaccatctagtagctggttcctt
ccgaagtttccctcaggatagctggtgcattttaatattatataaaataa
tcttatctggtaaagcgaatgattagaggccttagggtcgaaacgatctt
aacctattctcaaactttaaatgggtaagaaccttaactttcttgatatg
aagttcaaggttatgatataatgtgcccagtgggccacttttggtaagca
gaactggcgctgtgggatgaaccaaacgtaatgttacggtgcccaaataa
caact
>nr_004048 1
aatgttttatataaattgcagtatgtgtcacccaaaatagcaaaccccat
aaccaaccagattattatgatacataatgcttatatgaaactaagacatt
tcgcaacatttattttaggtatataaatacatttattgaaggaattgata
tatgccagtaaaatggtgtatttttaatttctttcaataaaaacataatt
gacattatataaaaatgaattataaaactctaagcggtggatcactcggc
tcatgggtcgatgaagaacgcagcaaactgtgcgtcatcgtgtgaactgc
aggacacatgaacatcgacattttgaacgcatatcgcagtccatgctgtt
atgtactttaattaattttatagtgctgcttggactacatatggttgagg
gttgtaagactatgctaattaagttgcttataaatttttataagcatatg
gtatattattggataaatataataatttttattcataatattaaaaaata
aatgaaaaacattatctcacatttgaatgt
>nr_004047 1
atattcaggttcatcgggcttaacctctaagcagtttcacgtactgttta
actctctattcagagttcttttcaactttccctcacggtacttgtttact
atcggtctcatggttatatttagtgtttagatggagtttaccacccactt
agtgctgcactatcaagcaacactgactctttggaaacatcatctagtaa
tcattaacgttatacgggcctggcaccctctatgggtaaatggcctcatt
taagaaggacttaaatcgctaatttctcatactagaatattgacgctcca
tacactgcatctcacatttgccatatagacaaagtgacttagtgctgaac
tgtcttctttacggtcgccgctactaagaaaatccttggtagttactttt
cctcccctaattaatatgcttaaattcagggggtagtcccatatgagttg
>nr_004052 1

when the tokenizer of ntlk is applied on this dataset, each line of text (for example tattattatacacaatcccggggcgttctatatagttatgtataatgtat ) becomes one token which is not correct. and a block of sequences should be considered as one token. for example in this case contents between >nr_004049 1  and >nr_004048 1 should be consider as one token:
>nr_004049 1
tattattatacacaatcccggggcgttctatatagttatgtataatgtat
atttatattatttatgcctctaactggaacgtaccttgagcatatatgct
gtgacccgaaagatggtgaactatacttgatcaggttgaagtcaggggaa
accctgatggaagaccgaaacagttctgacgtgcaaatcgattgtcagaa
ttgagtataggggcgaaagaccaatcgaaccatctagtagctggttcctt
ccgaagtttccctcaggatagctggtgcattttaatattatataaaataa
tcttatctggtaaagcgaatgattagaggccttagggtcgaaacgatctt
aacctattctcaaactttaaatgggtaagaaccttaactttcttgatatg
aagttcaaggttatgatataatgtgcccagtgggccacttttggtaagca
gaactggcgctgtgggatgaaccaaacgtaatgttacggtgcccaaataa
caact
>nr_004048 1 

so each block starting with special words such as >nr_004049 1 until the next special character should be considered as one token. the problem here is tokenizing this kind of data set and i dont have any idea how can i correctly tokenize them.
i really appreciate answers which helps me to solve this.
update:
one way to solve this problem is to append al lines within each block, and then using the nltk tokenizer. for example this means that to append all lines between >nr_004049 1 and >nr_004048 1 to make one string from several lines, so the nltk tokenizer will consider it as one token. can any one help me how can i append lines within each block?","['python', 'nlp', 'nltk', 'tokenize']",74632007,"you just need to concatenate the lines between two ids apparently. there should be no need for nltk or any tokenizer, just a bit of programming ;)

patterns = {}
with open('data', ""r"") as f:
    id = none
    current = """"
    for line0 in f:
        line= line0.rstrip()
        if line[0] == '>' :  # new pattern
            if len(current)>0:
#                print(""adding ""+id+""  ""+current)
                patterns[id] = current
                current = """"
            # to find the next id:
            tokens = line.split("" "")
            id = tokens[0][1:]
        else: # continuing pattern
            current = current + line
    if len(current)>0:
        patterns[id] = current
#        print(""adding ""+id+""  ""+current)


# do whatever with the patterns:
for id, pattern in patterns.items():
    print(f""{id}\t{pattern}"")",https://stackoverflow.com/questions/74623917,python,30-11-2022 07:00,211.0,3.0,1.0,True,30-11-2022 17:42,30-11-2022 07:42
72722957,trie is using too much memory,"i'm trying to get all the words made from the letters, 'crbtfopkgevyqdzsh' from a file called web2.txt. the posted cell below follows a block of code which improperly returned the whole run up to a full word e.g. for the word shocked it would return s, sh, sho, shoc, shock, shocke, shocked
so i tried a trie (know pun intended).
web2.txt is 2.5 mb in size, and contains 2,493,838 words of varying length. the trie in the cell below is breaking my google colab notebook. i even upgraded to google colab pro, and then to google colab pro+ to try and accommodate the block of code, but it's still too much. any more efficient ideas besides trie to get the same result?
# find the words3 word list here:  svnweb.freebsd.org/base/head/share/dict/web2?view=co

trie = {}

with open('/content/web2.txt') as words3:


    for word in words3:
        cur = trie
        for l in word:
            cur  = cur.setdefault(l, {})
            cur['word'] = true # defined if this node indicates a complete word
        
def findwords(word, trie = trie, cur = '', words3 = []):
    for i, letter in enumerate(word):
        if letter in trie:
            if 'word' in trie[letter]:
                words3.append(cur)
            findwords(word, trie[letter], cur+letter, words3 )    
            # first example: findwords(word[:i] + word[i+1:], trie[letter], cur+letter, word_list )

    return [word for word in words3 if word in words3]

words3 = findwords(""crbtfopkgevyqdzsh"")

i'm using pyhton3","['python', 'list', 'for-loop', 'nlp', 'trie']",72723071,"a trie is overkill. there's about 200 thousand words, so you can just make one pass through all of them to see if you can form the word using the letters in the base string.
this is a good use case for collections.counter, which gives us a clean way to get the frequencies (i.e. ""counters"") of the letters of an arbitrary string:
from collections import counter

base_counter = counter(""crbtfopkgevyqdzsh"")
with open(""data.txt"") as input_file:
    for line in input_file:
        line = line.rstrip()
        line_counter = counter(line.lower())
        # can use <= instead if on python 3.10
        if line_counter & base_counter == line_counter:
            print(line)",https://stackoverflow.com/questions/72722957,python,22-06-2022 23:20,206.0,1.0,1.0,True,23-06-2022 00:12,22-06-2022 23:58
72979886,fasttext top10 similar words,"i am working in an nlp task using the following fasttext model,
# fasttext
ft_model = fasttext(word_tokenized_corpus,
                    max_n=0,
                    vector_size=64,
                    window=5,
                    min_count=1,
                    sg=1,
                    workers=20,
                    epochs=1,
                    seed=42)

i have the following code to see the top10 similar words for each word
random_words = ['ready', 'stopping', 'myself', 'follow', 'instagram',
                'people', 'stories', 'coffee', 'hang', 'tumblr', 'snapchat']

full_semantically_similar_words_wv_2 = {words: [item[0] for item in ft_model.wv.most_similar([words], topn=10)]
                  for words in random_words}

for k,v in full_semantically_similar_words_wv_2.items():
    print(k + "": "" + str(v)) 

the output looks like this
ready: ['prepared', 'ready.', 'gonna', 'gunna', 'going', 'guaranteed', 'ganna', 'waiting', 'preparing', 'gona']
stopping: ['preventing', 'noticing', 'disappearing', 'calling.', 'resorting', 'slowing', 'stoping', 'distracting', 'sane', 'pushing']
myself: ['myself.', 'myself...', 'myself..', 'myself....', 'myseld', 'myslef', 'them....', 'it.....', 'myself-', 'them']
follow: ['unfollow', 'unlisten', 'reblog', 'stalk', 'block', 'softblock', 'dm', 'follow.', 'join', 'accordingly']
instagram: ['insta', 'twitter', 'facebook', 'tumblr', 'snapchat', 'musical.ly', 'instagram.', 'fb', 'twitter.', 'insta.']
people: ['people.', 'people..', 'ppl', 'people...', 'people....', 'ppl.', 'people-', 'strangers', 'them.', 'others']
stories: ['stories.', 'poems', 'storys', 'story', 'inspirational', 'reactions', 'characters.', 'videos.', 'animations', 'story.']
coffee: ['tea', 'coffe', 'soda.', 'iced', 'soda', 'chai', 'wine', 'coffee.', 'creamer', 'boba']
hang: ['hangout', 'invite', 'bail', 'reconnect', 'hangin', 'blurt', 'sneak', 'reunite', 'clubbing', 'arrange']
tumblr: ['twitter', 'instagram', 'musical.ly', 'pinterest', 'insta', 'ao3', 'twitter.', 'facebook', 'deviantart', 'tumblr.']
snapchat: ['instagram', 'insta', 'sc', 'facebook', 'snapchat.', 'insta.', 'instagram.', 'fb', 'twitter', 'kik.']

the output that i want to create should be a list with all the words, like
['prepared', 'ready.', 'gonna', ..., 'fb', 'twitter', 'kik.']

i woyld be grateful if you could help me.","['python', 'nlp']",72979981,"i assume full_semantically_similar_words_wv_2 is a dictionary. then, you can add all the words to a single list and print it like this:
all_words = []
for v in full_semantically_similar_words_wv_2.values():
    all_words += v
print(all_words)

or in one line:
all_words = [word for word_list in full_semantically_similar_words_wv_2.values() for word in word_list]
print(all_words)",https://stackoverflow.com/questions/72979886,python,14-07-2022 11:38,248.0,1.0,1.0,True,14-07-2022 20:49,14-07-2022 20:49
69087044,early stopping in bert trainer instances,"i am fine-tuning a bert model for a multiclass classification task. my problem is that i don't know how to add ""early stopping"" to those trainer instances. any ideas?","['python', 'deep-learning', 'neural-network', 'huggingface-transformers', 'huggingface']",69087153,"there are a couple of modifications you need to perform, prior to correctly using the earlystoppingcallback().
from transformers import earlystoppingcallback, intervalstrategy
...
...
# defining the trainingarguments() arguments
args = trainingarguments(
   output_dir = ""training_with_callbacks"",
   evaluation_strategy = intervalstrategy.steps, # ""steps""
   eval_steps = 50, # evaluation and save happens every 50 steps
   save_total_limit = 5, # only last 5 models are saved. older ones are deleted.
   learning_rate=2e-5,
   per_device_train_batch_size=batch_size,
   per_device_eval_batch_size=batch_size,
   num_train_epochs=5,
   weight_decay=0.01,
   push_to_hub=false,
   metric_for_best_model = 'f1',
   load_best_model_at_end=true)

you need to:

use load_best_model_at_end = true (earlystoppingcallback() requires this to be true).
evaluation_strategy = 'steps' or intervalstrategy.steps instead of 'epoch'.
eval_steps = 50 (evaluate the metrics after n steps).
metric_for_best_model = 'f1'

in your trainer():
trainer = trainer(
    model,
    args,
    ...
    compute_metrics=compute_metrics,
    callbacks = [earlystoppingcallback(early_stopping_patience=3)]
)

of course, when you use compute_metrics(), for example it can be a function like:
def compute_metrics(p):    
    pred, labels = p
    pred = np.argmax(pred, axis=1)
    accuracy = accuracy_score(y_true=labels, y_pred=pred)
    recall = recall_score(y_true=labels, y_pred=pred)
    precision = precision_score(y_true=labels, y_pred=pred)
    f1 = f1_score(y_true=labels, y_pred=pred)    
return {""accuracy"": accuracy, ""precision"": precision, ""recall"": recall, ""f1"": f1}

the return of the compute_metrics() should be a dictionary and you can access whatever metric you want/compute inside the function and return.
note: in newer transformers version, the usage of enum intervalstrategy.steps is recommended (see trainingarguments()) instead of plain steps string, the latter being soon subject to deprecation.",https://stackoverflow.com/questions/69087044,python,07-09-2021 11:02,23946.0,32.0,1.0,True,23-11-2024 07:25,06-05-2023 09:20
78031519,"how to resolve valueerror: you should supply an encoding or a list of encodings to this method that includes input_ids, but you provided [&#39;label&#39;]","i am working on sentiment analysis using the imdb dataset and a gpt-2-based model. this is a toy project to understand peft and lora as well as get some experience with the huggingface library.
this is what i've tried out:
from datasets import load_dataset

splits = [""train"", ""test""]
ds = {split: ds for split, ds in zip(splits, load_dataset(""imdb"", split=splits))}

from transformers import autotokenizer

tokenizer = autotokenizer.from_pretrained(""gpt2"")
# gpt-2 tokenizer doesn't have a padding token.
tokenizer.pad_token = tokenizer.eos_token

def preprocess_function(examples):
    """"""preprocess the imdb dataset by returning tokenized examples.""""""
    tokens = tokenizer(examples['text'],padding='max_length',truncation=true)
    return tokens


tokenized_ds = {}
for split in splits:
    tokenized_ds[split] = ds[split].map(preprocess_function, batched=true)


model2 = automodelforsequenceclassification.from_pretrained(
    ""gpt2"",
    num_labels=2,
    id2label={0: ""negative"", 1: ""positive""},  # for converting predictions to strings
    label2id={""negative"": 0, ""positive"":1},
)
model2.config.pad_token_id = model.config.eos_token_id

from peft import loraconfig
from peft import get_peft_model
lora_config = loraconfig(""lora_gpt2"", fan_in_fan_out=true,)
lora_model = get_peft_model(model2, lora_config)

trainer_lora = trainer(
    model=lora_model,
    args=trainingarguments(
        output_dir=""./data/sentiment_analysis2"",
        learning_rate=2e-3,
        # reduce the batch size if you don't have enough memory
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        num_train_epochs=5,
        weight_decay=0.01,
        evaluation_strategy=""epoch"",
        save_strategy=""epoch"",
        load_best_model_at_end=true,
    ),
    train_dataset=tokenized_ds[""train""],
    eval_dataset=tokenized_ds[""test""],
    tokenizer=tokenizer,
    data_collator=datacollatorwithpadding(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
)

trainer_lora.train()

when i run this code, im getting the following error and have had some difficulty debugging the issue:
file /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3018, in pretrainedtokenizerbase.pad(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)
   3016 # the model's main input name, usually `input_ids`, has be passed for padding
   3017 if self.model_input_names[0] not in encoded_inputs:
-> 3018     raise valueerror(
   3019         ""you should supply an encoding or a list of encodings to this method ""
   3020         f""that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}""
   3021     )
   3023 required_input = encoded_inputs[self.model_input_names[0]]
   3025 if required_input is none or (isinstance(required_input, sized) and len(required_input) == 0):

valueerror: you should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']

i'm not sure how to resolve this and haven't been able to find many examples for this online and was hoping the so community could help out.","['nlp', 'huggingface-transformers', 'huggingface-tokenizers', 'peft']",78032173,"turns out the lora model changes the name of the column expected from label, to labels.
in order to fix it, you need
# lora takes in ""labels"", not ""label"" so we need to rename the 
# training and testing sets
train_lora = tokenized_ds['train'].rename_column('label', 'labels')
test_lora = tokenized_ds['test'].rename_column('label', 'labels')

also needed was the tasktype in the config:
loraconfig(
r=8, 
lora_alpha=32,
target_modules=['c_attn', 'c_proj'],
lora_dropout=0.1,
bias=""none"",
fan_in_fan_out=true,
task_type=tasktype.seq_cls
)",https://stackoverflow.com/questions/78031519,nlp,21-02-2024 04:11,4469.0,2.0,1.0,True,21-02-2024 07:15,21-02-2024 06:01
77010673,how to use different dataset for training and test in text classification while avoiding # of features mismatch?,"i'm working on text classification using two distinct dataset, with the aim to use one dataset for training and other other for testing. please note i do not wish to merge the dataset to prevent leakage (i think that's what it's called). the test dataset is much smaller (~1000 rows) compared to the training dataset (16k rows)
i'm using countvectorizer and as the two datasets have different vocabularies, it results in different number of columns - which leads to error during prediction step.
valueerror: x has 55229 features, but decisiontreeclassifier is expecting 387964 
features as input.

i've been gpting and googling for some time and i'm getting mixed guidance e.g:

add zero-filled columns to the smaller x_test
use scikit-learn pipeline

code snippets below:
# read dfs
df_1 = pd.read_csv(""data1.csv"",header=0) # for training, has text, and class columns
df_2 = pd.read_csv(""data2.csv"",header=0) # for testing,  has text, and class columns

# vectorise
cv1 = countvectorizer(ngram_range=(1,3), stop_words=""english"").fit(df_1['text']) 
x_train = cv1.transform(df_1['text'])
y_train = df_1['class']

cv2 = countvectorizer(ngram_range=(1,3), stop_words=""english"").fit(df_2['text']) 
x_test = cv2.transform(df_2['text'])
y_test = df_test['class']

## shapes of objects
## x_test (1589, 55229), y_test(1589,)
## x_train (16716, 387964), y_train(16716,)

# build classifier and predict
classifier = decisiontreeclassifier(random_state=1234)
model = classifier.fit(x_train,y_train)
y_pred = model.predict(x_test)

# error valueerror: x has 55229 features, but decisiontreeclassifier is expecting 387964 features as input.","['python', 'scikit-learn', 'nlp', 'text-classification']",77011587,"as with every preprocessing step, do not fit on the test set.  you should have one instance of countvectorizer that you fit_transform the training set and transform the test set with.
in your case:
cv = countvectorizer(ngram_range=(1,3), stop_words=""english"")
x_train = cv.fit_transform(df_1['text'])
y_train = df_1['class']

x_test = cv.transform(df_2['text'])
y_test = df_test['class']",https://stackoverflow.com/questions/77010673,python,30-08-2023 18:31,266.0,1.0,1.0,True,30-08-2023 23:00,30-08-2023 23:00
71763580,indexing/search algorithm stability between versions,"i'm migrating from elasticsearch 1.5 to 7.10 there are multiple required changes, the most relevant one is the removal of the document type concept in version 6, to deal with it i introduced a new field doc_type and then i match with it when i search.
my question is, when i make the same (or equivalent because there are some changes) search query should i expect to have the exact same result set? because i'm having some differences, so i would like to figure out if i broke something in the new mappings or in the search query.
thank you in advance
edit after first question:
in general: i have a service that communicates with es 1.5 and i have to migrate it to es 7.10 keeping the external api as stable as possible.

i'm not using scoring.
previously i had document types a and b, when i make a query like this for example: host/indexname/a,b/_search, after the migration i keep a or b in doc_type, and the query becomes host/indexname/_search with a ""bool"":{""should"":[{""terms"":{""doc_type"":[""a""],""boost"":1.0}},{""terms"":{""doc_type"":[""b""],""boost"":1.0}}],""adjust_pure_negative"":true,""boost"":1.0} in the body. if i put it in different indexes for a and b and the user want to match in both of them i'll have to ""merge"" the search response for both queries and i don't know which strategy should i follow for that, so keeping it all together i get a response with mixed (doc_type) results from es. i followed this specific approach 
the differences are not so big, difficult to show a concrete example because it's a complex data/doc structure but the idea is, having for 1.5 this response for a giving query for example:
[a, b, c, d, e, f, g, h, i, j] (where each one may have any of types a or b)
with 7.10 i'm having responses like:
[a, b, e, c, d, f, g, h, i, j] or [a, b, c, d, e, g, i, j, k]

second edit:
this query has been generated from the java client.
{
   ""from"":0,
   ""size"":100,
   ""query"":{
      ""bool"":{
         ""must"":[
            {
               ""query_string"":{
                  ""query"":""mark_deleted:false"",
                  ""fields"":[
                     
                  ],
                  ""type"":""best_fields"",
                  ""default_operator"":""or"",
                  ""max_determinized_states"":10000,
                  ""enable_position_increments"":true,
                  ""fuzziness"":""auto"",
                  ""fuzzy_prefix_length"":0,
                  ""fuzzy_max_expansions"":50,
                  ""phrase_slop"":0,
                  ""escape"":false,
                  ""auto_generate_synonyms_phrase_query"":true,
                  ""fuzzy_transpositions"":true,
                  ""boost"":1.0
               }
            },
            {
               ""bool"":{
                  ""should"":[
                     {
                        ""terms"":{
                           ""type"":[
                              ""a""
                           ],
                           ""boost"":1.0
                        }
                     },
                     {
                        ""terms"":{
                           ""type"":[
                              ""b""
                           ],
                           ""boost"":1.0
                        }
                     },
                     {
                        ""terms"":{
                           ""type"":[
                              ""d""
                           ],
                           ""boost"":1.0
                        }
                     }
                  ],
                  ""adjust_pure_negative"":true,
                  ""boost"":1.0
               }
            }
         ],
         ""adjust_pure_negative"":true,
         ""boost"":1.0
      }
   },
   ""post_filter"":{
      ""term"":{
         ""mark_deleted"":{
            ""value"":false,
            ""boost"":1.0
         }
      }
   },
   ""sort"":[
      {
         ""a_specific_date"":{
            ""order"":""desc""
         }
      }
   ],
   ""highlight"":{
      ""pre_tags"":[
         ""<b>""
      ],
      ""post_tags"":[
         ""</b>""
      ],
      ""no_match_size"":120,
      ""fields"":{
         ""body"":{
            ""fragment_size"":120,
            ""number_of_fragments"":1
         }
      }
   }
}","['elasticsearch', 'indexing', 'tf-idf']",71766300,"first, since you don't care about scoring you should use bool/filter instead of bool/must at the top level, otherwise your results are sorted by _score  by default and between 1.7 et 7.10, there have been so many changes that it would explain the differences you get. so you're better off simply sorting the results using any other field than _score
second, instead of the bool/should on type you can use a simple terms query, which does exactly the same job, yet in a simpler way:
{
  ""from"": 0,
  ""size"": 100,
  ""query"": {
    ""bool"": {
      ""filter"": [
        {
          ""query_string"": {
            ""query"": ""mark_deleted:false"",
            ""fields"": [],
            ""type"": ""best_fields"",
            ""default_operator"": ""or"",
            ""max_determinized_states"": 10000,
            ""enable_position_increments"": true,
            ""fuzziness"": ""auto"",
            ""fuzzy_prefix_length"": 0,
            ""fuzzy_max_expansions"": 50,
            ""phrase_slop"": 0,
            ""escape"": false,
            ""auto_generate_synonyms_phrase_query"": true,
            ""fuzzy_transpositions"": true,
            ""boost"": 1
          }
        },
        {
          ""terms"": {
            ""type"": [
              ""a"",
              ""b"",
              ""c""
            ]
          }
        }
      ]
    }
  },
  ""post_filter"": {
    ""term"": {
      ""mark_deleted"": {
        ""value"": false,
        ""boost"": 1
      }
    }
  },
  ""sort"": [
    {
      ""a_specific_date"": {
        ""order"": ""desc""
      }
    }
  ],
  ""highlight"": {
    ""pre_tags"": [
      ""<b>""
    ],
    ""post_tags"": [
      ""</b>""
    ],
    ""no_match_size"": 120,
    ""fields"": {
      ""body"": {
        ""fragment_size"": 120,
        ""number_of_fragments"": 1
      }
    }
  }
}

finally, i'm not sure why you're using a query_string query to do an exact match on mark_deleted:false, it doesn't make sense to me. a simple term query would be better and more adequate here.
also not clear why you have remove all results that also have mark_deleted:false in your post_filter, since it's the same condition as in your query_string constraint.",https://stackoverflow.com/questions/71763580,elasticsearch,06-04-2022 08:40,46.0,0.0,1.0,True,06-04-2022 11:53,06-04-2022 11:45
76091659,failed lemmatization,"i'm trying to lemmatize german texts which are in a dataframe.
i use german library to succesfully handle with specific grammatic structure: 
my code:
from german import preprocess

df = pd.read_csv('afd.csv', sep=',')

lemma = open('messageafd_lemma.txt', 'w')
for i in df['message']:
    preprocess (i, remove_stop=true)
    lemma.write(i)
lemma.close()


the process of lemmatization goes successfully, there's no any error in the terminal, but openning the file ""messageafd_lemma.txt"", i get this : (nothing was lemmatized)
the expected result is like:
input:
preprocess(['johpannes war einer von vielen guten schï¿½ï¿½lern.', 'julia trinkt gern tee.'], remove_stop=true)

output:
['johannes gut schï¿½ï¿½ler', 'julia trinken tee']
what goes wro","['python', 'nlp', 'lemmatization']",76091954,"the preprocess function returns a copy of the texts, instead of modifying the input. so you need to write the result of preprocess to the file, not the original i messages.
furthermore, preprocess accepts a list of texts to process, so you must wrap your message in [message], and extract the single result from the returned list with result, = ...
from german import preprocess

df = pd.read_csv('afd.csv', sep=',')

lemma = open('messageafd_lemma.txt', 'w')
for message in df['message']:
    result, = preprocess([message], remove_stop=true)
    lemma.write(result)
lemma.close()

# or, to process all messages in one go:
with open('messageafd_lemma.txt', 'w') as f:
    for result in preprocess(df['message'], remove_stop=true):
        f.write(result)",https://stackoverflow.com/questions/76091659,python,24-04-2023 12:03,54.0,0.0,1.0,True,24-04-2023 12:58,24-04-2023 12:29
70362716,stopword segmentation,"i have a problem with nltk stopwords: if i do a cycle, stopword check on letter and not on word. how i can change this behaviour?
an example:
import pandas as pd
import nltk

stopword = nltk.corpus.stopwords.words('italian')
pd.set_option('display.max_colwidth', none)

df = pd.read_csv('esempiotweet.csv', sep =',')

def remove_stop(text):
    text = [word for word in text if word not in stopword]
    return text
df['testo_no_stop'] = df['testo_token'].apply(lambda x: remove_stop(x))
df.head()

given a previous column like this:
[covid, calano, i, nuovi, contagi, e, tamponi]

i expect an output like this:
[covid, calano, nuovi, contagi, tamponi]

but i have an output like:
[v,d,n, ...]

i understand that stopword is operating on a single letter and not on the whole word, but why? i'm sure that my remove_stop function works in a right way, but why stopword operates in a wrong one?","['python', 'nlp', 'nltk', 'stop-words']",70364134,"your code uses for word in text which if text is a string returns one letter at a time.
i simplified the code removing pandas as irrelevant - changed your remove_stop slightly to use word in text.split(), although i imagine nltk may have a method to split text into words which maybe you should use as for example it might remove punctuation which split() won't.
import nltk

stopwords = nltk.corpus.stopwords.words('italian')

phrase = ""oggi piove e non esco""

def remove_stop(text):
    global stopwords
    text = [word for word in text.split() if word not in stopwords]
    return text

res = remove_stop(phrase)
print( f""{res=}"" )

output:
res=['oggi', 'piove', 'esco']

btw i don't think you need the lambda, just use:
df['testo_no_stop'] = df['testo_token'].apply(remove_stop)

don't forget you can add debugging to a function like remove_stop(), which tbh is a good reason to use for loops rather than undebuggable comprehensions.
similarly you can print stopwords to check it is a list. it is.",https://stackoverflow.com/questions/70362716,python,15-12-2021 11:14,101.0,0.0,1.0,True,25-01-2024 16:03,25-01-2024 16:03
77028326,searching for specific words in corpus with r (tm package),"i have a corpus (tm package), containing a collection of 1.300 different text documents [content:  documents: 1.300].
my goal is now to search the frequency of a specific wordlist in each of those documents. e.g. if my wordlist contains the words ""january, february, march,...."". i want to analyze how often the documents refer to these words.
example: 
text 1: i like going on holiday in january and not in february.
text 2: i went on a holiday in march.
text 3: i like going on vacation.

the result should look like this:
text 1: 2 
text 2: 1
text 3: 0

i tried using the following codes:
library(quanteda)
toks <- tokens(x) 
toks <- tokens_wordstem(toks) 

dtm <- dfm(toks)

dict1 <- dictionary(list(c(""january"", ""february"", ""march"")))

dict_dtm2 <- dfm_lookup(dtm, dict1, nomatch=""_unmatched"")                                 
tail(dict_dtm2)  

this code was proposed in a different chat, however it does not work on mine and an error, saying it is only applicaple on text or corpus elements occurs.
how can i search for my wordlist using my existing corpus in tm package in r?","['r', 'nlp', 'tm', 'corpus']",77028673,"to make your quanteda code work, you first have to convert your tm vcorpus object x + fix few other minor issues:

dictionary() expects a named list
english stemmer will return ""januari"", ""februari"" instead of ""january"", ""february"".

library(tm)
library(quanteda)

## prepare reprex, create tm vcorpus:
docs <- c(""i like going on holiday in january and not in february."",
          ""i went on a holiday in march."",
          ""i like going on vacation."")
x <- vcorpus(vectorsource(docs))
class(x)
#> [1] ""vcorpus"" ""corpus""

### tm vcorpus object to quanteda corpus:
x <- corpus(x)
class(x)
#> [1] ""corpus""    ""character""

### continue with tokenization and stemmming
toks <- tokens(x) 
toks <- tokens_wordstem(toks) 
dtm <- dfm(toks)

# dictionary() takes a named list, i.e. list(months = c(..))
# and ""january"", ""february"" are stemmed to ""januari"", ""februari""
dict1 <- dictionary(list(months = c(""januar*"", ""februar*"", ""march"")))
dict_dtm2 <- dfm_lookup(dtm, dict1, nomatch=""_unmatched"")                                 
dict_dtm2
#> document-feature matrix of: 3 documents, 2 features (16.67% sparse) and 7 docvars.
#>        features
#> docs    months _unmatched
#>   text1      2         10
#>   text2      1          7
#>   text3      0          6

created on 2023-09-02 with reprex v2.0.2",https://stackoverflow.com/questions/77028326,r,02-09-2023 12:41,139.0,0.0,1.0,True,14-12-2024 14:58,14-12-2024 14:58
77455122,aws lambda fails when using playwright to navigate browser (python),"i have created an llm agent with tools using langchain and python. i build my lambda with with aws sam with docker. after building, i test the docker image using sam local invoke, and the lambda function executes as expected (on m1 max chip), using langchain's playwright toolkit to navigate a headless, asynchronous chromium browser. after i deploy with sam, i test the function in the aws lambda ui, but there is an issue with navigating the browser: target page, context or browser has been closed. i do not experience this error when testing the lambda locally with sam local invoke.
additionally, if the llm agent does not use playwright tools, then the lambda executes as expected. this confirms that there is an issue using playwright (via the langchain package) within a deployed aws lambda function that is not experienced with local testing.

used image with playwright in dockerfile: mcr.microsoft.com/playwright/python:v1.21.0-focal. expected playwright browsers to properly configured. browser still closes prematurely.
increased to 2048 mb. i hoped that more memory might solve the issue, but error still occurs.
followed this guide for dockerfile. i thought by adding aws-lambda-cpp build dependencies the issue might resolve, but it did not.
tried --platform=linux/x86_64 and --platform=linux/arm64 with playwright image. architecture set in docker to correspond to choice. error still occurs.","['docker', 'aws-lambda', 'playwright', 'langchain', 'playwright-python']",77455580,"the issue was related to langchain's initialization of the async playwright browser. the imported function from langchain.tools.playwright.utils import create_async_playwright_browser does not allow for arguments to be passed to browser.chromium.launch(). for the browser to function properly on aws, browser.chromium.launch(headless=true, args=[""--disable-gpu"", ""--single-process""]).
here is the modification i made to get the playwright browser to function properly in my aws lambda:
from playwright.async_api import async_playwright
from playwright.async_api import browser as asyncbrowser
from typing import list
from langchain.tools.playwright.utils import run_async

def create_async_playwright_browser(headless: bool = true, args: list[str] = none) -> asyncbrowser:
    """"""
    create an async playwright browser.

    args:
        headless: whether to run the browser in headless mode. defaults to true.
        args: a list of arguments for the browser instance.

    returns:
        asyncbrowser: the playwright browser.
    """"""
    browser = run_async(async_playwright().start())
    if args:
        return run_async(browser.chromium.launch(headless=headless, args=args))
    else:
        return run_async(browser.chromium.launch(headless=headless))

using this function to create the browser instead of the one found langchain.tools.playwright.utils results in the aws lambda functioning as desired",https://stackoverflow.com/questions/77455122,docker,09-11-2023 17:10,2005.0,0.0,1.0,True,09-11-2023 18:35,09-11-2023 17:40
77218224,using whisper api to generate .srt transcripts?,"i'm exploring the capabilities of the whisper api and was wondering if it can be used to generate an .srt file with transcriptions. from what i understand, this transcription to .srt can be achieved when running the model locally using the whisper package. unfortunately, i don't possess the computational resources to run the model locally, so i'm leaning towards using the api directly.
has anyone had experience with this or can provide guidance on how to approach it through the api?
the following python script can be used a starting point, but the question is about capabilities of the model itself, not specific to any programming language.
import os
import openai
openai.api_key = api_key
audio_file = open(""audio.mp3"", ""rb"")
transcript = openai.audio.transcribe(""whisper-1"", audio_file)
print(transcript.text)","['openai-api', 'openai-whisper']",77218259,"a cursory look at openai's docs shows that srt is a supported value for the response_format parameter on the /v1/audio/transcriptions endpoint.
with the official python bindings you're using in your example, you should be able to pass this as a named parameter to your openai.audio.transcribe() invocation:
transcript = openai.audio.transcribe(""whisper-1"", audio_file, response_format=""srt"")",https://stackoverflow.com/questions/77218224,openai-api,02-10-2023 20:02,7164.0,6.0,2.0,True,20-10-2024 13:31,11-08-2024 17:10
56656153,remove loops for sentence comparison in nlp,"i'm using bert to compare text similarity, with the following code: 
from bert_embedding import bertembedding
import numpy as np
from scipy.spatial.distance  import cosine as cosine_similarity

bert_embedding = bertembedding()
text1 = ""as expected from mit-level of course: it's interesting, challenging, engaging, and for me personally quite enlightening. this course is second part of 5 courses in  micromasters program. i was interested in learning about supply chain (purely personal interest, my work touch this topic but not directly) and stumbled upon this course, took it, and man-oh-man...i just couldn't stop learning. now i'm planning to take the rest of the courses. average time/effort per week should be around 8-10 hours, but i tried to squeeze everything into just 5 hours since i have very limited free time. you will need 2-3 hours per week for the lecture videos, 2 hours for practice problems, and another 2 hours for the weekly homework. this course offers several topics around demand forecasting and inventory. basic knowledge of probability and statistics is needed. it will help if you take the prerequisite course: supply chain analytics. but if you've already familiar with basic concept of statistics, you can pick yourself along the way. the lectures are very interesting and engaging, it gives you a lot of knowledge but also throw in some business perspective, so it's very relatable and applicable! the practice problems can help strengthen the understanding of the given knowledge and the homework are very challenging compared to other online-courses i have taken. this course is the best quality i have taken so far, and i have taken several (3-4 moocs) from other provider.""
text1 = text1.split('.')

sentence2 = [""challenging course ""]

from there i want to find the best match of sentence2 in one of the sentences of text1, using cosine distance
best_match = {'sentence':'','score':''}
best = 0
for sentence in text1: 
  #sentence = sentence.replace('supply chain','')
  if len(sentence) < 5:
    continue
  avg_vec1 = calculate_avg_vec([sentence])
  avg_vec2 = calculate_avg_vec(sentence2)

  score = cosine_similarity(avg_vec1,avg_vec2)
  if score > best:
    best_match['sentence'] =  sentence
    best_match['score'] =  score
    best = score

best_match

the code is working, but since i want to compare the sentence2 not only with text1 with but n texts i need to improve the speed. is it possible to vectorice this loop? or any way to speed it up?","['python', 'numpy', 'bert-language-model']",56737830,"cosine_similarity is defined as a dot product of two normalized vectors.
this is essentially a matrix multiplication, followed by an argmax to get the best index.
i'll be using numpy, even though - as mentioned in the comments - you could probably plug it in to the bert model with pytorch or tensorflow.
first, we define a normalized average vector:
def calculate_avg_norm_vec(sentence):
    vs = sentence2vectors(sentence) # todo: use bert embedding
    vm = vs.mean(axis=0)
    return vm/np.linalg.norm(vm)

then, we build a matrix of all sentences and their vectors
x = np.apply_along_axis(calculate_avg_norm_vec, 1, all_sentences)
target = calculate_avg_norm_vec(target_sentence)

finally, we'll need to multiply the target vector with the x matrix, and take the argmax
index_of_sentence = np.dot(x,target.t).argmax(axis=1)

you might want to make sure that the axis and indexing fit your data, but this is the overall scheme",https://stackoverflow.com/questions/56656153,python,18-06-2019 19:52,257.0,1.0,1.0,True,24-03-2022 00:12,24-03-2022 00:12
71781813,how does the finetune on transformer (t5) work?,"i am using pytorch lightning to finetune t5 transformer on a specific task. however, i was not able to understand how the finetuning works. i always see this code :
tokenizer = autotokenizer.from_pretrained(hparams.model_name_or_path)                                     model = automodelforseq2seqlm.from_pretrained(hparams.model_name_or_path)
i don't get how the finetuning is done, are they freezing the whole model and training the head only, (if so how can i change the head) or are they using the pre-trained model as a weight initializing? i have been looking for an answer for couple days already. any links or help are appreciated.","['deep-learning', 'nlp', 'pytorch', 'huggingface-transformers', 'seq2seq']",71797603,"if you are using pytorch lightning, then it won't freeze the head until you specify it do so. lightning has a callback which you can use to freeze your backbone and training only the head module. see backbone finetuning
also checkout ligthning-flash, it allows you to quickly build model for various text tasks and uses transformers library for backbone. you can use the trainer to specify which kind of finetuning you want to apply for your training.
thanks",https://stackoverflow.com/questions/71781813,deep-learning,07-04-2022 12:00,643.0,0.0,1.0,True,08-04-2022 13:08,07-04-2022 12:08
71936286,spacy python how to count numbers with /,"i am using spacy for text analysis. i need to count the number of occurrences of numbers in this text where the text contains the expression '1/2'.
how to calculate the numbers ""1"" and ""2"" separately without resorting to operations with regular expressions?
my code:
for token in doc:
    if token.pos_ =='num':
        m.append(token.text)
    
    for item in set(m):
        print(f'""{item}"" was found {m.count(item)} times in text')","['python', 'spacy']",72642846,"in terms of your question, you want to count the number of occurrences of numbers in a text where the expression '1/2' pops up. in that case, the following will work:
test_str = ""i like 1/2 to code 1/2 a lot""
search_str = ""1/2""
print(f""number of occurrences: {test_str.count(search_str)}"")

calc = eval(""1/2"") # evaluates the expression as pythonic code
print(f""the expression '1/2', calculated to float: {calc}"")

# >>> number of occurrences: 2
# >>> the expression '1/2', calculated to float: 0.5

i also added a way to 'calculate' the expression. you can use the eval function, which will evaluate the string as pythonic code. hopefully this helps!
edit: to count the total number of expressions, you can do something like this:
test_str = ""i 1 would 1 like 1/2 to code 1/2 a lot""
search_str = ""1/2""
digit_list = search_str.split('/') # creates list ['1', '2']

print(f""number of occurrences of {search_str}: {test_str.count(search_str)}"")
for digit in digit_list:
    print(f""number of occurrences of digit {digit}: {test_str.count(digit)}"")

# >>> number of occurrences of 1/2: 2
# >>> number of occurrences of digit 1: 4
# >>> number of occurrences of digit 2: 2",https://stackoverflow.com/questions/71936286,python,20-04-2022 08:17,417.0,0.0,1.0,True,16-06-2022 08:54,16-06-2022 08:24
65041338,how to tokenize double dots as separate tokens in spacy?,"i have code like that:
import spacy
nlp = spacy.load('de_core_news_md')
doc = nlp('92637 weiden i.d.opf..')
tokens = list(doc)

so, tokens looks like that:
92637
weiden
i.d
.
opf
..

how can i split last (double-dot) token into two tokens with single-dot?
it's necessary for fit into ner-labeling.
so, i expecting that:
92637
weiden
i.d
.
opf
.
.","['python', 'nlp', 'spacy']",65077811,"when you do:
import spacy
nlp = spacy.load('de_core_news_md')
nlp.tokenizer.explain('92637 weiden i.d.opf..')

you'll find out:
[('token', '92637'),
 ('token', 'weiden'),
 ('token', 'i.d'),
 ('infix', '.'),
 ('token', 'opf'),
 ('suffix', '..')]

meaning you have splitting problems due to suffix rules.
then you may achieve what you want with redefining suffix patterns:
import spacy
nlp = spacy.load('de_core_news_md')
suffixes = (r""[\.]"")
# suffixes = [r""[\.]""] + nlp.defaults.suffixes[:38] + nlp.defaults.suffixes[39:]

suffix_regex = spacy.util.compile_suffix_regex(suffixes)
nlp.tokenizer.suffix_search = suffix_regex.search

doc = nlp('92637 weiden i.d.opf..')
[tok for tok in doc]
# expected result
[92637, weiden, i.d, ., opf, ., .]",https://stackoverflow.com/questions/65041338,python,27-11-2020 17:19,704.0,0.0,1.0,True,27-06-2022 15:47,27-06-2022 15:47
74016749,reading csv file for machine learning scan,"i've been stuck for 2 days making this machine learning code to work, to read a youtube comments csv file and scan it for hate speech.
i got this code from: 
i added a code for reading a new csv file of youtube comments that will be cleaned at the def function.
the first sample of testing a sentence works without any issues.
but after that i want to read the csv file and scan it with the machine learning who has already learned hate speech from the twitter csv file if i'm correct.
unfortunately i am not able to scan the csv file and get a correct output i wanted.
tried many things but every time i get an different error or issue.
i think i may be doing something simple wrong. the error occures at the last 3 lines of this code.
(i've only been programming for 4 weeks so i'm kind of new to all of this.)
csv files:

from nltk.util import pr
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import countvectorizer
from sklearn.model_selection import train_test_split
from sklearn.tree import decisiontreeclassifier
import re
import nltk
stemmer = nltk.snowballstemmer(""english"")
from nltk.corpus import stopwords
import string
stopword = set(stopwords.words('english'))
data = pd.read_csv(""twitter.csv"")  # read the twitter csv file which the model learns from.

test2 = pd.read_csv(""youtube-comments.csv"")  # let the model scan this file to see if the model is smart enough to scan this.
test2.drop([""time"", ""likes"", ""reply count"", ""name""], axis=1, inplace=true)  # drop columns we don't need in youtube comments csv file

# nltk.download() # use this if your missing packages of ntlk
data[""labels""] = data[""class""].map({0: ""hate speech"",  # create labels
                                    1: ""offensive language"",
                                    2: ""no hate and offensive""})

data = data[[""tweet"", ""labels""]]
# test2 = test2[[""comment"", [data[""labels""]]]]

print(""\n"")
print(data.head())

def clean(text):
    text = str(text).lower()
    text = re.sub(r'[^\w]', ' ', text)
    text = re.sub('\[.*?\]', '', text)
    text = re.sub(' '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    text = [word for word in text.split(' ') if word not in stopword]
    text ="" "".join(text)
    text = [stemmer.stem(word) for word in text.split(' ')]
    text ="" "".join(text)
    return text
data[""tweet""] = data[""tweet""].apply(clean)
test2[""comment""] = test2[""comment""].apply(clean) # cleaning csv file data of youtube comments.
print(""\n"")
print(test2) # printing out csv test data to see if its cleaned

x = np.array(data[""tweet""])
y = np.array(data[""labels""])

cv = countvectorizer()
x = cv.fit_transform(x) # fit the data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)

clf = decisiontreeclassifier()
clf.fit(x_train,y_train)

sample = ""let's unite and kill all the people who shot are fucking protesting against the government""
data = cv.transform([sample]).toarray()
print(clf.predict(data))  # this example works with a sentence

print(""\n"")  # now i want to try to read the csv file and scan that for hate speech. here comes the error.
sample = test2
data = cv.transform([sample]).toarray()
print(clf.predict(data))

output error (removed some print statements at start):
[40933 rows x 1 columns] ['no hate and offensive']

traceback (most recent call last):   file ""c:\users\mango\youtube test
hate speech.py"", line 64, in <module>

data = cv.transform([sample]).toarray()   file ""c:\users\mango\anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"",

line 1254, in transform
_, x = self._count_vocab(raw_documents, fixed_vocab=true)   file ""c:\users\mango\anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"",

line 1114, in _count_vocab
for feature in analyze(doc):   file ""c:\users\mango\anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"",
line 104, in _analyze

doc = preprocessor(doc)   file ""c:\users\mango\anaconda3\lib\site-packages\sklearn\feature_extraction\text.py"",

line 69, in _preprocess
doc = doc.lower()   file ""c:\users\mango\anaconda3\lib\site-packages\pandas\core\generic.py"",

line 5487, in __getattr__
return object.__getattribute__(self, name) attributeerror: 'dataframe' object has no attribute 'lower'","['python', 'pandas', 'scikit-learn', 'nltk']",74021783,"you have problem in
sample = test2
data = cv.transform([sample]).toarray()

test2 is dataframe and it doesn't need []
sample = test2
data = cv.transform( sample ).toarray()


frankly, you should get only one column from dataframe  test2['comment']
sample = test2['comment']
data = cv.transform( sample ).toarray()


that's all.

edit:
full working code
import re
import string

import pandas as pd
import numpy as np

import nltk
from nltk.corpus import stopwords
from nltk.util import pr

from sklearn.feature_extraction.text import countvectorizer
from sklearn.model_selection import train_test_split
from sklearn.tree import decisiontreeclassifier

# --- functions --- 

def clean(text):
    text = text.lower()
    text = re.sub(r'[^\w]', ' ', text)
    text = re.sub('\[.*?\]', '', text)
    text = re.sub(' '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    
    #text = [word for word in text.split(' ') if word not in stopword]
    #text = "" "".join(text)
    #text = [stemmer.stem(word) for word in text.split(' ')]
    # shorter 
    text = [stemmer.stem(word) for word in text.split(' ') if word not in stopword]
    
    text = "" "".join(text)
    
    return text

# --- main ---

stemmer = nltk.snowballstemmer(""english"")

stopword = set(stopwords.words('english'))

data = pd.read_csv(""twitter.csv"")  # read the twitter csv file which the model learns from.
test = pd.read_csv(""youtube-comments.csv"")  # let the model scan this file to see if the model is smart enough to scan this.

# - clean -

test.drop([""time"", ""likes"", ""reply count"", ""name""], axis=1, inplace=true)  # drop columns we don't need in youtube comments csv file

# nltk.download() # use this if your missing packages of ntlk
data[""labels""] = data[""class""].map({0: ""hate speech"",  # create labels
                                    1: ""offensive language"",
                                    2: ""no hate and offensive""})

data = data[[""tweet"", ""labels""]]
# test = test[[""comment"", [data[""labels""]]]]

data[""tweet""] = data[""tweet""].apply(clean)

test[""comment""] = test[""comment""].apply(clean) # cleaning csv file data of youtube comments.

print(""\n--- data ---\n"")
print(data.head())

print(""\n--- test ---"")
print(test.head())

# - train -

x = np.array(data[""tweet""])
y = np.array(data[""labels""])

cv = countvectorizer()
x = cv.fit_transform(x)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)

clf = decisiontreeclassifier()
clf.fit(x_train, y_train)

# - prediction 1 -

print('\n--- prediction 1 ---\n')

sample = [""let's unite and kill all the people who shot are fucking protesting against the government""]
data = cv.transform( sample )#.toarray()

print('- sample -\n')
print(sample)
print('\n- prediction -\n')
print(clf.predict(data))

# - prediction 2 -

print('\n--- prediction 2 ---\n')

sample = test['comment']
data = cv.transform( sample )#.toarray()

print('- sample -\n')
print(sample)
print('\n- prediction -\n')
print(clf.predict(data))

result:
# ...

--- prediction 1 ---

- sample -

[""let's unite and kill all the people who shot are fucking protesting against the government""]

- prediction -

['no hate and offensive']

--- prediction 2 ---

- sample -

0        join us countdown reveal next battlefield  sta...
1         darrel swatten thx ill recruit new helper vol...
2         prez mrm great innov onlin industri call look...
3              darrel swatten copi clone bfv game base bfv
4         darrel swatten ill make  game base bfv new we...
                               ...                        
40928    origin twist steel trailer teas new plane cras...
40929                                  banzaaaaaaiiiii    
40930                           kamikaz instead v  rocket 
40931                                           emperor   
40932                                         baanzzaaii  
name: comment, length: 40933, dtype: object

- prediction -

['no hate and offensive' 'no hate and offensive' 'no hate and offensive'
 ... 'no hate and offensive' 'no hate and offensive'
 'no hate and offensive']",https://stackoverflow.com/questions/74016749,python,10-10-2022 14:48,377.0,-1.0,1.0,True,11-10-2022 00:40,10-10-2022 16:03
77142013,langchain pypdfloader read from azure blob storage mount point in azure databricks,"i am working on azure databricks and trying to read a pdf file located in azure blob storage. also, i am using langchain pypdfloader to read the pdf. according to the examples i have checked, pypdfloader gets in the path of the pdf file, so i provide the path in the mount point, but it does not work.
here, i show the code snapshot:
blob_path = f""wasbs://{container_name}@{storage_account}.blob.core.windows.net""
account_url = f""fs.azure.account.key.{storage_account}.blob.core.windows.net""
account_key = ""******************""


dbutils.fs.mount(source = blob_path,
             mount_point = f""/mnt/{mount_point}/"",
             extra_configs = {account_url: account_key})

loader = pypdfloader(f'/mnt/{mount_point}/raw-docs/doc.pdf')
pages = loader.load()

i get the following error:
""valueerror: file path /mnt/mpoint/raw-docs/doc.pdf is not a valid file or url""
i have tried changing the pdf location and also with some other functions to read the file, but still get the same error.","['azure', 'azure-blob-storage', 'databricks', 'langchain']",77142282,"you need to prefix the pdf path with dbfs since your not using any spark context.
the pypdfloader searches the path through driver filesystem so you need to give path from root.
/dbfs/mount_path.
alter the path like below.
  loader = pypdfloader(f'/dbfs/mnt/{mount_point}/testfolder/sample.pdf')
  pages = loader.load()
  pages[0]

output:

refer this documentation to know more about working with files in databricks.",https://stackoverflow.com/questions/77142013,azure,20-09-2023 11:36,994.0,1.0,1.0,True,20-09-2023 12:09,20-09-2023 11:42
60843698,how to define ration of summary with hugging face transformers pipeline?,"i am using the following code to summarize an article from using huggingface-transformer's pipeline. using this code:
from transformers import pipeline
summarizer = pipeline(task=""summarization"" )
summary = summarizer(text)
print(summary[0]['summary_text'])

how can i define a ratio between the summary and the original article? for example, 20% of the original article?
edit 1: i implemented the solution you suggested, but got the following error. this is the code i used: 
summarizer(text, min_length = int(0.1 * len(text)), max_length = int(0.2 * len(text)))
print(summary[0]['summary_text'])

the error i got: 
runtimeerror                              traceback (most recent call last)
<ipython-input-9-bc11c5d8eb66> in <module>()
----> 1 summarizer(text, min_length = int(0.1 * len(text)), max_length = int(0.2 * len(text)))
      2 print(summary[0]['summary_text'])

13 frames
/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   1482         # remove once script supports set_grad_enabled
   1483         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-> 1484     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   1485 
   1486 

runtimeerror: index out of range: tried to access index 1026 out of table with 1025 rows. at /pytorch/aten/src/th/generic/thtensorevenmoremath.cpp:418","['pytorch', 'huggingface-transformers']",60846776,"(note that this answer is based on the documentation for version 2.6 of transformers)
it seems that as of yet the documentation on the pipeline feature is still very shallow, which is why we have to dig a bit deeper. when calling a python object, it internally references its own __call__ property, which we can find here for the summarization pipeline.
note that it allows us (similar to the underlying bartforconditionalgeneration model) to specifiy the min_length and max_length, which is why we can simply call with something like
summarizer(text, min_length = int(0.1 * len(text)), max_length = int(0.2 * len(text)))

this would give you a summary of about 10-20% length of the original data, but of course you can change that to your liking. note that the default value for bartforconditionalgeneration for max_length is 20 (as of now, min_length is undocumented, but defaults to 0), whereas the summarization pipeline has values min_length=21 and max_length=142.",https://stackoverflow.com/questions/60843698,pytorch,25-03-2020 06:08,2055.0,2.0,1.0,True,02-08-2023 11:42,25-03-2020 12:26
67158554,fine-tuning model&#39;s classifier layer with new label,"i would like to fine-tune already fine-tuned bertforsequenceclassification model with new dataset containing just 1 additional label which hasn't been seen by model before.
by that, i would like to add 1 new label to the set of labels that model is currently able of classifying properly.
moreover, i don't want classifier weights to be randomly initialized, i'd like to keep them intact and just update them accordingly to the dataset examples while increasing the size of classifier layer by 1.
the dataset used for further fine-tuning could look like this:
sentece,label
intent example 1,new_label
intent example 2,new_label
...
intent example 10,new_label

my model's current classifier layer looks like this:
linear(in_features=768, out_features=135, bias=true)

how could i achieve it?
is it even a good approach?","['pytorch', 'huggingface-transformers']",67187807,"you can just extend the weights and bias of your model with new values. please have a look at the commented example below:
#this is the section that loads your model
#i will just use an pretrained model for this example
import torch
from torch import nn
from transformers import automodelforsequenceclassification, autotokenizer
tokenizer = autotokenizer.from_pretrained(""jpcorb20/toxic-detector-distilroberta"")
model = automodelforsequenceclassification.from_pretrained(""jpcorb20/toxic-detector-distilroberta"")
#we check the output of one sample to compare it later with the extended layer
#to verify that we kept the previous learnt ""knowledge""
f = tokenizer.encode_plus(""this is an example"", return_tensors='pt')
print(model(**f).logits)

#now we need to find out the name of the linear layer you want to extend
#the layers on top of distilroberta are wrapped inside a classifier section
#this name can differ for you because it can be chosen randomly
#use model.parameters instead find the classification layer
print(model.classifier)

#the output shows us that the classification layer is called `out_proj`
#we can now extend the weights by creating a new tensor that consists of the
#old weights and a randomly initialized tensor for the new label 
model.classifier.out_proj.weight = nn.parameter(torch.cat((model.classifier.out_proj.weight, torch.randn(1,768)),0))

#we do the same for the bias:
model.classifier.out_proj.bias = nn.parameter(torch.cat((model.classifier.out_proj.bias, torch.randn(1)),0))

#and be happy when we compare the output with our expectation 
print(model(**f).logits)

output:
tensor([[-7.3604, -9.4899, -8.4170, -9.7688, -8.4067, -9.3895]],
       grad_fn=<addmmbackward>)
robertaclassificationhead(
  (dense): linear(in_features=768, out_features=768, bias=true)
  (dropout): dropout(p=0.1, inplace=false)
  (out_proj): linear(in_features=768, out_features=6, bias=true)
)
tensor([[-7.3604, -9.4899, -8.4170, -9.7688, -8.4067, -9.3895,  2.2124]],
       grad_fn=<addmmbackward>)

please note, that you should fine-tune your model. the new weights are randomly initialized and will therefore negatively impact the performance.",https://stackoverflow.com/questions/67158554,pytorch,19-04-2021 08:32,3206.0,3.0,2.0,True,24-06-2024 22:01,21-04-2021 00:20
75825362,&quot;attributeerror: encode&quot; when returning streamingresponse in fastapi,"i am using python 3.10 and fastapi 0.92.0 to write a server-sent events (sse) stream api. this is how the python code looks like:
from fastapi import apirouter, fastapi, header

from src.chat.completions import chat_stream
from fastapi.responses import streamingresponse

router = apirouter()

@router.get(""/v1/completions"",response_class=streamingresponse)
def stream_chat(q: str, authorization: str = header(none)):
    auth_mode, auth_token = authorization.split(' ')
    if auth_token is none:
        return ""authorization token is missing""
    answer = chat_stream(q, auth_token)
    return streamingresponse(answer, media_type=""text/event-stream"")

and this is the chat_stream function:
import openai

def chat_stream(question: str, key: str):
    openai.api_key = key
    # create a completion
    completion = openai.completion.create(model=""text-davinci-003"",
                                          prompt=question,
                                          stream=true)
    return completion

when i am using this command to invoke the api:
curl -n -h ""authorization: bearer sk-the openai key"" 

the server side shows the following error:
info:     123.146.17.54:0 - ""get /v1/completions?q=hello  200 ok
error:    exception in asgi application
traceback (most recent call last):
  file ""/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/ line 429, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  file ""/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py"", line 78, in __call__
    return await self.app(scope, receive, send)
  file ""/usr/local/lib/python3.10/dist-packages/fastapi/applications.py"", line 276, in __call__
    await super().__call__(scope, receive, send)
  file ""/usr/local/lib/python3.10/dist-packages/starlette/applications.py"", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  file ""/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py"", line 184, in __call__
    raise exc
  file ""/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py"", line 162, in __call__
    await self.app(scope, receive, _send)
  file ""/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py"", line 79, in __call__
    raise exc
  file ""/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py"", line 68, in __call__
    await self.app(scope, receive, sender)
  file ""/usr/local/lib/python3.10/dist-packages/fastapi/middleware/asyncexitstack.py"", line 21, in __call__
    raise e
  file ""/usr/local/lib/python3.10/dist-packages/fastapi/middleware/asyncexitstack.py"", line 18, in __call__
    await self.app(scope, receive, send)
  file ""/usr/local/lib/python3.10/dist-packages/starlette/routing.py"", line 718, in __call__
    await route.handle(scope, receive, send)
  file ""/usr/local/lib/python3.10/dist-packages/starlette/routing.py"", line 276, in handle
    await self.app(scope, receive, send)
  file ""/usr/local/lib/python3.10/dist-packages/starlette/routing.py"", line 69, in app
    await response(scope, receive, send)
  file ""/usr/local/lib/python3.10/dist-packages/starlette/responses.py"", line 270, in __call__
    async with anyio.create_task_group() as task_group:
  file ""/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py"", line 662, in __aexit__
    raise exceptions[0]
  file ""/usr/local/lib/python3.10/dist-packages/starlette/responses.py"", line 273, in wrap
    await func()
  file ""/usr/local/lib/python3.10/dist-packages/starlette/responses.py"", line 264, in stream_response
    chunk = chunk.encode(self.charset)
  file ""/usr/local/lib/python3.10/dist-packages/openai/openai_object.py"", line 61, in __getattr__
    raise attributeerror(*err.args)
attributeerror: encode

why did this error happen? what should i do to fixed it?","['python-3.x', 'streaming', 'fastapi', 'server-sent-events', 'openai-api']",75837557,"as described in fastapi's documentation, streamingresponse takes an asynchronous (async def) generator or a normal (def) generator/iterator and streams the response body. as explained in this answer, in either case, fastapi will still work asynchronouslyï¿½ï¿½ï¿½if the generator that is passed to streamingresponse isn't asynchronous, fastapi/starlette will then run the generator in a separate thread (see the relevant implementation here), using iterate_in_threadpool(), which will then be awaited (see iterate_in_threadpool() implementation). for more details on def vs async def in fastapi, as well as solutions for running blocking operations inside async def endpoints, please have a look at this detailed answer.
streamingresponse, a subclass of response, streams the response body in bytes. hence, if the content passed through the generator is not in bytes format, fastapi/starlette will attempt to encode/convert it into bytes (using the default utf-8 encoding scheme). below is a code snippet from the relevant implementation:
async for chunk in self.body_iterator:
    if not isinstance(chunk, bytes):
        chunk = chunk.encode(self.charset)
    await send({""type"": "" ""body"": chunk, ""more_body"": true})

however, if a chunk in the iterator/generator is not in str format that could be encoded, an attributeerror will be raised (e.g., attributeerror: ... has no attribute 'encode'), similar to what is described in this answer (see option 2, note 3). also, if a chunk includes characters outside the range of utf-8 encoding, you might as well be faced with unicodeencodeerror: ... codec can't encode character. thus, given the attributeerror: encode in the error traceback you provided, it is very likely that you are passing an object other than str.
the example below demonstrates an asynchronous generator (i.e., async def gen()), streaming json data, which, in this case, requires dict objects first to be converted into str (using json.dumps(), or any other json encoder, such as orjson, see here) and then, optionally, to the corrsponding byte value (using .encode('utf-8')), which, as explained earlier, if omitted, fastapi/starlette will take care of that. in addition, the example below uses text/event-stream media type (also known as mime type), which is usually used when sending events from the server (see event stream format as well). if you are confident that the data sent from the server are always in json format, you could use application/json media type as well. note that, as explained in this answer, using text/plain media type instead might result in not displaying the streaming data in the browser immediately, as browsers use to buffer text/plain responses for what is called ""mime sniffing"" (see the linked answer above on how to disable it, if you would like to use text/plain).
working example
from fastapi import fastapi
from fastapi.responses import streamingresponse
import asyncio
import json

app = fastapi()


@app.get('/')
async def main():
    async def gen():
        while true:
            #yield (json.dumps({'msg': 'hello world!'}) + '\n\n').encode('utf-8')
            # or, simply use the below, and fastapi/starlette will take care of the encoding
            yield json.dumps({'msg': 'hello world!'}) + '\n\n'
            await asyncio.sleep(0.5)

    return streamingresponse(gen(), media_type='text/event-stream')


if you are confident that the data sent from the server are in json format, you could instead use:
# ...

@app.get('/')
async def main():
    async def gen():
        while true:
            yield json.dumps({'msg': 'hello world!'})
            await asyncio.sleep(0.5)

    return streamingresponse(gen(), media_type='application/json')",https://stackoverflow.com/questions/75825362,python-3.x,23-03-2023 16:05,8910.0,4.0,1.0,True,03-06-2023 04:12,26-03-2023 09:32
70479299,how do i extract only abbreviation following acronyms inside the brackets by mapping each capital letter,"a = ""the process maps are similar to manual excellence process framework (mepf)""

input = ""the process maps are similar to manual excellence process framework (mepf)""
output = manual excellence process framework (mepf)
i want to write a python scripts where i have that piece of text, from that i want to extract full for of given acronyms inside the brackets (mepf) and full form is manual excellence process framework i want to append only full from by match each uppercase letter from inside the brackets.
my idea was when ever acronyms appears inside the bracket that will map each capital letter for example (mepf) starting from last letter f that will match last word befoure the bracket here it is framwork, then p (pocess) then e(excellence ) finaly m (manual) so final output will be full form(manual excellence process framework) can you try once this way that will be realy helpfull for me","['python', 'python-3.x', 'text', 'nlp', 'artificial-intelligence']",70479524,"using a simple regex and a bit of post-processing:
a = ""i like international business machines (ibm). the manual excellence process framework (mepf)""

import re
m = re.findall(r'([^)]+) \(([a-z]+)\)', a)
out = {b: ' '.join(a.split()[-len(b):]) for a,b in m}

out

output:
{'ibm': 'international business machines',
 'mepf': 'manual excellence process framework'}

if you want to check the the acronym actually matches the words:
out = {b: ' '.join(a.split()[-len(b):]) for a,b in m
       if all(x[0]==y for x,y in zip(a.split()[-len(b):], b))
       }

example
a = ""no match (abc). i like international business machines (ibm). the manual excellence process framework (mepf).""

m = re.findall(r'([^)]+) \(([a-z]+)\)', a)
{b: ' '.join(a.split()[-len(b):]) for a,b in m
 if all(x[0]==y for x,y in zip(a.split()[-len(b):], b))
}

# {'ibm': 'international business machines',
#  'mepf': 'manual excellence process framework'}",https://stackoverflow.com/questions/70479299,python,25-12-2021 10:14,142.0,1.0,2.0,True,25-12-2021 11:08,25-12-2021 11:08
75609157,openai whisper api error: &quot;attributeerror: module &#39;openai&#39; has no attribute &#39;audio&#39;&quot;,"chatgpt api is announced with speech-to-text whisper api and i was so excited to give it a try. here's the link
i have tried their sample code
# note: you need to be using openai python v0.27.0 for the code below to work
import openai

audio_file= open(""/path/to/file/audio.mp3"", ""rb"")

and got the following error
attributeerror: module 'openai' has no attribute 'audio'

i'm sure i'm using the version 0.27.0
pip list | grep openai
openai                0.27.0

do you think openai is not updated yet?","['python', 'openai-api', 'openai-whisper']",75609167,"there are three possible reasons why you get attributeerror: module 'openai' has no attribute 'audio'.
reason 1: you didn't upgrade python
the required python version is 3.7.1 or newer, as stated in the official openai github repository.
reason 2: you didn't upgrade the openai python package
first, check your openai package version by running the following command in the terminal:
pip show openai

you need to use version 0.27.0 or newer if you want to use the openai whisper api.
if you have an older package, run the following command in the terminal to update the openai package:
pip install --upgrade openai

reason 3: your code isn't correct
try the code below.
option 1 (recommended):
test.py
import openai
import os

openai.api_key = os.getenv('openai_api_key')

audio_file = open('audio.mp3', 'rb')
transcript = openai.audio.transcribe('whisper-1', audio_file)

option 2:
test.py
import openai

openai.api_key = 'sk-xxxxxxxxxxxxxxxxxxxx'

audio_file = open('audio.mp3', 'rb')
transcript = openai.audio.transcribe('whisper-1', audio_file)",https://stackoverflow.com/questions/75609157,python,01-03-2023 21:25,5905.0,2.0,2.0,True,05-09-2023 14:35,12-08-2023 15:27
77253558,localentrynotfounderror while building docker image with hugging face model,"i am trying to create a aws lambda function using docker image as source.
i am executing the following code as part of the image build phase to download all the dependencies
import logging
logging.basicconfig(level=logging.debug)
from langchain.embeddings import huggingfaceinstructembeddings

from transformers import automodelforsequenceclassification, autotokenizer

instructor_embeddings = huggingfaceinstructembeddings(model_name=""hkunlp/instructor-large"",
                                                      model_kwargs={""device"": ""cpu""},encode_kwargs={""batch_size"": 1})

# replace 'model_id' with your specific model identifier
model_id = ""samlowe/roberta-base-go_emotions""

# download the model and tokenizer
model = automodelforsequenceclassification.from_pretrained(model_id,force_download=true)
tokenizer = autotokenizer.from_pretrained(model_id,force_download=true)

# save the model and tokenizer to a directory (e.g., ""./models/"")
model.save_pretrained(""./model"")
tokenizer.save_pretrained(""./tokenizer"")


here is the dockerfile
# lambda base image for docker from aws
from public.ecr.aws/lambda/python:latest

# update the package list and install development tools for c++ support
run yum update -y && \
    yum install deltarpm -y && \
    yum groupinstall ""development tools"" -y 

# export cxxflags environment variable for c++11 support
env cxxflags=""-std=c++11""
env aws_default_region=""eu-central-1""
env transformers_offline=""1""
env sentence_transformers_home=""/root/.cache/huggingface/""

# install packages
copy requirements.txt ./
run python3 -m pip install -r requirements.txt

# setup directories
run mkdir -p /tmp/content/pickles/ ~/.cached
run chmod 0700 ~/.cached

# copy all code and lambda handler
copy *.py ./

run python3 dependency.py

# run lambda handler
cmd [""function.handler""]

while runnging the docker build command, the phase for executing the file dependency.py leads to the below error
docker build . -t ml-server
[+] building 6.5s (13/13) finished                                                                                          
 => [internal] load build definition from dockerfile                                                                   0.0s
 => => transferring dockerfile: 1.38kb                                                                                 0.0s
 => [internal] load .dockerignore                                                                                      0.0s
 => => transferring context: 32b                                                                                       0.0s
 => [internal] load metadata for public.ecr.aws/lambda/python:latest                                                   1.3s
 => [internal] load build context                                                                                      0.0s
 => => transferring context: 287b                                                                                      0.0s
 => [1/9] from public.ecr.aws/lambda/python:latest@sha256:d8a8324834a079dbdfc6551831325113512a147bf70003622412565f216  0.0s
 => cached [2/9] run yum update -y &&     yum install deltarpm -y &&     yum groupinstall ""development tools"" -y       0.0s
 => cached [3/9] copy requirements.txt ./                                                                              0.0s
 => cached [4/9] run echo `whoami`                                                                                     0.0s
 => cached [5/9] run python3 -m pip install -r requirements.txt                                                        0.0s
 => [6/9] run mkdir -p /tmp/content/pickles/ ~/.cached                                                                 0.3s
 => [7/9] run chmod 0700 ~/.cached                                                                                     0.2s
 => [8/9] copy *.py ./                                                                                                 0.0s
 => error [9/9] run python3 dependency.py                                                                              4.6s
------
 > [9/9] run python3 dependency.py:
#13 1.260 info:numexpr.utils:numexpr defaulting to 4 threads.
#13 2.806 info:sentence_transformers.sentencetransformer:load pretrained sentencetransformer: hkunlp/instructor-large
#13 2.807 debug:urllib3.connectionpool:starting new  connection (1): huggingface.co:443
#13 3.658 debug:urllib3.connectionpool: ""get /api/models/hkunlp/instructor-large  200 140783
#13 4.016 traceback (most recent call last):
#13 4.016   file ""/var/task/dependency.py"", line 7, in <module>
#13 4.016     instructor_embeddings = huggingfaceinstructembeddings(model_name=""hkunlp/instructor-large"",
#13 4.016                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
#13 4.016   file ""/var/lang/lib/python3.11/site-packages/langchain/embeddings/huggingface.py"", line 150, in __init__
#13 4.016     self.client = instructor(
#13 4.016                   ^^^^^^^^^^^
#13 4.016   file ""/var/lang/lib/python3.11/site-packages/sentence_transformers/sentencetransformer.py"", line 87, in __init__
#13 4.017     snapshot_download(model_name_or_path,
#13 4.017   file ""/var/lang/lib/python3.11/site-packages/sentence_transformers/util.py"", line 491, in snapshot_download
#13 4.017     path = cached_download(**cached_download_args)
#13 4.017            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
#13 4.017   file ""/var/lang/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py"", line 118, in _inner_fn
#13 4.017     return fn(*args, **kwargs)
#13 4.017            ^^^^^^^^^^^^^^^^^^^
#13 4.017   file ""/var/lang/lib/python3.11/site-packages/huggingface_hub/file_download.py"", line 749, in cached_download
#13 4.018     raise localentrynotfounderror(
#13 4.018 huggingface_hub.utils._errors.localentrynotfounderror: connection error, and we cannot find the requested files in the cached path. please try again or make sure your internet connection is on.
------
executor failed running [/bin/sh -c python3 dependency.py]: exit code: 1

when i try to execute the same file on macos it runs without an issue. i am able to get packages from pip within the image so network connection should not be an issue
judging from the error message i thought this would be an error related to the embeddings method, so i add the force_download download flag and set the download batch_size to 1.
that also resulted in the same error
the size of the downloaded dependencies is not larger than 3 gb.
is there a cpu bottleneck while running the download pass?","['python', 'docker', 'aws-lambda', 'dockerfile', 'huggingface-transformers']",77253843,"so the issue was because the environment variable was set to use transformers in offline mode
env transformers_offline=""1""",https://stackoverflow.com/questions/77253558,python,08-10-2023 12:12,840.0,0.0,1.0,True,08-10-2023 13:48,08-10-2023 13:48
70697470,"valueerror: unrecognized model in ./mrpc/. should have a `model_type` key in its config.json, or contain one of the following strings in its name","goal: amend this notebook to work with albert and distilbert models
kernel: conda_pytorch_p36. i did restart & run all, and refreshed file view in working directory.
error occurs in section 1.2, only for these 2 new models.
for filenames etc., i've created a variable used everywhere:
model_name = 'albert-base-v2'  # 'distilbert-base-uncased', 'bert-base-uncased'

i replaced imports with:
from transformers import (autoconfig, automodel, autotokenizer)
#from transformers import (bertconfig, bertforsequenceclassification, berttokenizer,)

as suggested in transformers documentation - auto classes.

instantiating one of autoconfig, automodel, and autotokenizer will directly create a class of the relevant architecture.


section 1.2:
# load model
model = automodel.from_pretrained(configs.output_dir)  # bertforsequenceclassification
model.to(configs.device)


# quantize model
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.linear}, dtype=torch.qint8
)

#print(quantized_model)

def print_size_of_model(model):
    torch.save(model.state_dict(), ""temp.p"")
    print('size (mb):', os.path.getsize(""temp.p"")/(1024*1024))
    os.remove('temp.p')

print_size_of_model(model)
print_size_of_model(quantized_model)

traceback:
valueerror: unrecognized model in ./mrpc/. should have a `model_type` key in its config.json, or contain one of the following strings in its name: imagegpt, qdqbert, vision-encoder-decoder, trocr, fnet, segformer, vision-text-dual-encoder, perceiver, gptj, layoutlmv2, beit, rembert, visual_bert, canine, roformer, clip, bigbird_pegasus, deit, luke, detr, gpt_neo, big_bird, speech_to_text_2, speech_to_text, vit, wav2vec2, m2m_100, convbert, led, blenderbot-small, retribert, ibert, mt5, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, megatron-bert, mpnet, bart, blenderbot, reformer, longformer, roberta, deberta-v2, deberta, flaubert, fsmt, squeezebert, hubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, speech-encoder-decoder, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag, tapas, splinter, sew-d, sew, unispeech-sat, unispeech, wavlm

please let me know if there's anything else i can add to post.","['python', 'tensorflow', 'huggingface-transformers', 'bert-language-model', 'onnx']",70698011,"explanation:
when instantiating automodel, you must specify a model_type parameter in ./mrpc/config.json file (downloaded during notebook runtime).
list of model_types can be found here.

solution:
code that appends model_type to config.json, in the same format:
import json

json_filename = './mrpc/config.json'

with open(json_filename) as json_file:
    json_decoded = json.load(json_file)

json_decoded['model_type'] = # !!

with open(json_filename, 'w') as json_file:
    json.dump(json_decoded, json_file, indent=2, separators=(',', ': '))

config.json:
{
  ""attention_probs_dropout_prob"": 0.1,
  ""finetuning_task"": ""mrpc"",
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 768,
  ""initializer_range"": 0.02,
  ""intermediate_size"": 3072,
  ""layer_norm_eps"": 1e-12,
  ""max_position_embeddings"": 512,
  ""num_attention_heads"": 12,
  ""num_hidden_layers"": 12,
  ""num_labels"": 2,
  ""output_attentions"": false,
  ""output_hidden_states"": false,
  ""pruned_heads"": {},
  ""torchscript"": false,
  ""type_vocab_size"": 2,
  ""vocab_size"": 30522,
  ""model_type"": ""albert""
}",https://stackoverflow.com/questions/70697470,python,13-01-2022 13:32,8458.0,3.0,2.0,True,14-10-2024 17:34,13-01-2022 14:03
79165226,&quot;openai.badrequesterror: invalid role&quot; error while using ollama + litellm + autogen,"introduction
i am running an ollama server with litellm proxy. i aim to design a group chat using autogen. the designated agents are:

user_proxy: ask question
sql_generator: generate sql
sql_runner: run sql (tool calling)
result_validator: validate result

problems

the group chat works fine up until sql_runner returns the data from db. but when the next speaker is expected to be the result_validator, the sql_generator is assigned as next speaker, although i explicitly state in the system message that sql_runner should pass the result to result_validator.

for some reason, sql_generator agent's role remains as tool following the activity of sql_runner instead of one of [system, user, assistant] and it leads to this error:


***** response from calling tool (call_6493f510-d318-4e85-b552-8963bece5fcb) *****
[
    // data from db
]
**********************************************************************************
next speaker: sql_generator

// error trace

openai.badrequesterror: error code: 400 - {'error': {'message': 'invalid role: tool, role must be one of [system, user, assistant]', 'type': 'api_error', 'param': none, 'code': none}}

code
llm configs
llama3_config_list = [
    {
        ""model"": ""llama3:latest"",
        ""base_url"": f""{ollama_base_url}:{ollama_port}/v1"",
        ""api_key"": ""ollama"",
    }
]

litellm_config_list = [
    {
        ""model"": ""notrequired"",
        ""api_key"": ""notrequired"",
        ""base_url"": f""{litellm_base_url}:{litellm_port}"",
        ""price"": [0, 0],
    }
]

local_llm_config = {""config_list"": litellm_config_list, ""cache_seed"": none}

groupchat
user_proxy = autogen.userproxyagent(
    name=role.user_proxy,
    system_message=prompt.user_proxy,
    human_input_mode=""never"",
    is_termination_msg=is_termination_msg,
    code_execution_config={""use_docker"": false, ""work_dir"": ""test""},
)

sql_generator = autogen.assistantagent(
    name=role.sql_generator,
    system_message=prompt.sql_generator,
    llm_config=get_custom_llm_config(config_list=llama3_config_list),
)

sql_runner = autogen.assistantagent(
    name=role.sql_runner,
    system_message=prompt.sql_runner,
    llm_config=local_llm_config,
)

@user_proxy.register_for_execution()
@sql_runner.register_for_llm(description=""sql query runner"")
def run_sql(sql: annotated[str, ""sql query string to run""]) -> str:
    with postgresmanager() as db:
        db.get_rds_connection()
        return db.run_sql(sql)

result_validator = autogen.assistantagent(
    name=role.result_validator,
    system_message=prompt.result_validator,
    llm_config=get_custom_llm_config(config_list=llama3_config_list)
)

groupchat = autogen.groupchat(
    agents=[
        user_proxy,
        sql_generator,
        sql_runner,
        result_validator,
    ],
    messages=[],
)

manager = autogen.groupchatmanager(groupchat=groupchat)

user_proxy.initiate_chat(manager, message=question)

for the problem number 1, i have already tried to set autogen.groupchat's speaker_selection_method to ""round_robin"" but the result did not change at all.
for the problem number 2, i am guessing that it might be overcome if i can somehow prevent agent role from being cached, but i cannot really figure out how.","['artificial-intelligence', 'openai-api', 'ollama', 'ms-autogen']",79208623,"i was encountering this error since my ollama version did not support tool calling.
after upgrading ollama server version (to 0.4.2) now the role ""tool"" is available.",https://stackoverflow.com/questions/79165226,artificial-intelligence,07-11-2024 06:21,345.0,0.0,1.0,True,05-12-2024 13:37,05-12-2024 13:37
78844205,"how to develop a generalized rag pipeline for text, images, and structured data","i'm trying to find a general solution for rag to solve problems involving both text, images, chart, tables,.., they are in many different formats such as .docx, .xlsx, .pdf.
the requirement for the answer:

some answers are just images
some answers only contain text and need to be absolutely accurate because it relates to a process,...
on the other hand, the answers may not need to be absolutely accurate but should still ensure logical consistency; this is something i am already working on

the features of the documents:

some documents in docx and excel formats contain only text; this is the simplest form. my task is to determine the embedding model and llm, in addition to selecting hyperparameters such as chunk size, chunk overlap, etc., and experimenting to find the appropriate values
if the documents have more complex content, such as docx files containing text and images, or pdf files containing text, images, charts, tables, etc., i haven't found a general solution to handle them yet.

below are some documents i have read but feel i don't fully understand, i'm not sure how it can help me.




i want to be able to outline a pipeline to answer questions according to the requirements of my system. any help would be greatly appreciated!
system:

llm was run locally (llama 3.1 13n instruct, qwen2-7b-instruct,...)","['python', 'langchain', 'large-language-model', 'python-embedding', 'rag']",78847823,"here is a sample of the code you will need to implement a rag-fusion. you would have to structure your requirements with this code, this serves as a guide for json files, you can implement others such as pdf, images following the same procedure.
def determine_extension(file):
    if file.endswith("".jpg"", "".png""):
       send_image_to_rag_classifier(file)
    elif ...
    else ...


"""""" implement the rag fusion using the langchain library""""""

import asyncio
import json
import logging
import os
import pathlib as path
from operator import itemgetter
from typing import any

from dotenv import find_dotenv, load_dotenv
from langchain.prompts import chatprompttemplate
from langchain_community.vectorstores import chroma
from langchain_core.documents import document
from langchain_core.output_parsers import stroutputparser
from langchain_core.vectorstores import vectorstoreretriever
from langchain_openai import chatopenai, openaiembeddings
from langchain_text_splitters import recursivejsonsplitter

logger = logging.getlogger(__name__)


# read openai_api_key from the environment
load_dotenv(find_dotenv())

# define a prompt for the rag model
system_prompt = """""" 
                  your prompt
                """"""


# recursive pass data in the retriever_data
def collect_data_files(filepath: path) -> list:
    """"""walk through the file path and collect the files

    args: filepath: the file path to be walked through

    returns:
    list: list of files
    """"""
    
    return store_file


# create a recursive json splitter to split the data into chunks
def retrieve_data(data) -> list[chroma.document]:
    """"""
    retrieve the data from the file

    args: data: the data to be retrieved

    returns: list: list of documents
    """"""

    docs = collect_data_files(data)

    for file in docs:
        with open(file, ""r"") as f:
            data = json.loads(f.read())
            # split the data into chunks
            splitter = recursivejsonsplitter(max_chunk_size=300)

            # create documents from the vector database
            documents = splitter.create_documents(texts=data, convert_lists=true)

    return documents


# vectorstore database from chroma
def vectorstore_db(data) -> vectorstoreretriever:
    """"""
    create a vectorstore database from the data
    args: data: the data to be indexed

    returns: vectorstoreretriever: the vectorstore retriever
    """"""


    return vector_retriever


# create a function to generate queries from the rag model
def get_unique_union_of_documents(docs: list[list]) -> list[any]:
    """"""
    get the unique union of the documents
    args:
    docs: the documents to be processed

    returns:
    list: the unique union of the documents""""""

    return [json.loads(doc) for doc in unique_union]


# rag fusion
class ragfusion:
    """"""
    implement the rag fusion
    args:
    data: the data to be used for the rag fusion
    """"""

    def __init__(self, data) -> none:

        self.data = data

    def __call__(self, question: str) -> str:
        """"""
        implement the rag fusion
        args:
        question: the question to be answered

        returns:
        str: the answer to the question
        """"""

        try:
            # create a retrieval chain
            prompt_for_rag_fusion = chatprompttemplate.from_template(system_prompt)

            generate_query = (
                prompt_for_rag_fusion
                | chatopenai(temperature=0.5, max_tokens=4096)
                | stroutputparser()
                | (lambda x: x.split(""\n""))
            )

            vb = vectorstore_db(self.data)

            # create a retrieval chain
            retrieval_chain = generate_query | vb.map() | get_unique_union_of_documents

            chat_template = """"""
                        answer the following questions{question} \n
                        based on the data and context provided {context} \n
                        question: {question} \n
                    """"""

            # get the chat prompt template
            prompt = chatprompttemplate.from_template(chat_template)

            # use this llm
            llm = chatopenai(temperature=0.5, max_tokens=4096)

            # implement the final rag fusion
            final_rag_fusion = (
                {""context"": retrieval_chain, ""question"": itemgetter(""question"")}
                | prompt
                | llm
                | stroutputparser()
            )

            return final_rag_fusion.invoke({""question"": question})
        except exception as e:
            logger.error(f""an error occurred: {e}"")",https://stackoverflow.com/questions/78844205,python,07-08-2024 14:22,973.0,1.0,1.0,True,08-08-2024 14:01,08-08-2024 14:01
70007129,output from bert gives str not tensor in forward function,"class bertmodel(nn.module):
    def __init__(self,pre_trained='bert-base-uncased'):
        super().__init__()        
        self.bert = automodel.from_pretrained(pre_trained)
        self.dropout = nn.dropout(0.1)
        self.relu =  nn.relu()
        self.fc1 = nn.linear(768,512)
        self.fc2 = nn.linear(512,6)
      
    
        
    def forward(self,inputs, mask, labels):
        
        pooled, cls_hs = self.bert(input_ids=inputs,attention_mask=mask)
        print(pooled)
        print(cls_hs)  
        print(inputs) 
        print(mask)   
        x = self.fc1(cls_hs)
        print(1) 
        x = self.relu(x)
        print(2) 
        x = self.dropout(x)
        print(3) 
      # output layer
        x = self.fc2(x)
        print(4)
      # apply softmax activation
        x = self.softmax(x)
        print(5)


last_hidden_state
pooler_output


tensor([[  101,  2342,  2393,  ...,     0,     0,     0],
[  101, 14477,  4779,  ...,  4839,  6513,   102],
[  101, 14777,  2111,  ..., 13677,  3613,   102],
...,
[  101,  2113, 14047,  ...,     0,     0,     0],
[  101,  5683,  3008,  ...,     0,     0,     0],
[  101, 19046,  2075,  ...,  2050,  3308,   102]])
tensor([[1, 1, 1,  ..., 0, 0, 0],
[1, 1, 1,  ..., 1, 1, 1],
[1, 1, 1,  ..., 1, 1, 1],
...,
[1, 1, 1,  ..., 0, 0, 0],
[1, 1, 1,  ..., 0, 0, 0],
[1, 1, 1,  ..., 1, 1, 1]])

in linear(input, weight, bias)
if has_torch_function_variadic(input, weight, bias):
return handle_torch_function(linear, (input, weight, bias), input, weight,
bias=bias)
return torch._c._nn.linear(input, weight, bias)
typeerror: linear(): argument 'input' (position 1) must be tensor, not str

pooled, cls_hs printed as string last_hidden_state, pooler_output tensor
with out any tensor","['python', 'function', 'model', 'bert-language-model']",70080462,"try to replace the line where you downloaded the pretrained bert model with:
self.model = automodel.from_pretrained(pre_trained, return_dict=false)",https://stackoverflow.com/questions/70007129,python,17-11-2021 15:23,213.0,0.0,1.0,True,23-11-2021 11:55,17-11-2021 15:55
73282911,how to load huggingface model/resource from local disk?,"i am behind firewall, and have a very limited access to outer world from my server. i wanted to load huggingface model/resource from local disk.
from sentence_transformers import sentencetransformer
# initialize sentence transformer model

# how to load 'bert-base-nli-mean-tokens' from local disk?
model = sentencetransformer('bert-base-nli-mean-tokens')
# create sentence embeddings
sentence_embeddings = model.encode(sentences)

i came across some comments about

load_pretrained()

, etc. however, could not get the above problem sorted. any suggestion is welcome. thank you in advance.","['local-storage', 'huggingface-transformers', 'sentence-transformers']",73292853,"first, clone the model you want to load with git clone
in your example:
git clone 
you can of course download it from another pc and pass it, to avoid the firewall problem.
after that, simply replace the name of the model with the path of the file you
just downloaded:
from sentence_transformers import sentencetransformer
# initialize sentence transformer model

# how to load 'bert-base-nli-mean-tokens' from local disk?
model = sentencetransformer('/path/to/cloned/git/repo')
# create sentence embeddings
sentence_embeddings = model.encode(sentences)

side note: as mentioned here:

this model is deprecated. please don't use it as it produces sentence
embeddings of low quality. you can find recommended sentence embedding
models here: sbert.net - pretrained models",https://stackoverflow.com/questions/73282911,local-storage,08-08-2022 19:11,8889.0,3.0,1.0,True,09-08-2022 19:03,09-08-2022 19:03
70555307,find words out of vocabulary,"i have some texts in a pandas dataframe df['mytext']
i have also got a vocabulary vocab (list of words).
i am trying to list and count the words out of vocabulary for each document
i have tried the following but it is quite slow for 10k documents.
how to quickly and efficiently quantify the out of vocabulary tokens in collection of texts in pandas?
oov_text=df['mytext'].apply(lambda s: ' '.join([ word  for word in s.split() if (word not in vocab) ]))
oov=df['mytext'].apply(lambda s: sum([(word in vocab) for word in s.split()])/len(s.split()))

df.shape[0] is quite large
len(vocab) is large
len(unique words in df.mytext)<<len(vocab)","['python', 'pandas', 'nlp', 'vocabulary', 'oov']",70555392,"you can use
from collections import counter
vocab=['word1','word2','word3','2021']
df['mytext_list']=df['mytext'].str.split(' ')
df['count']=df['mytext_list'].apply(lambda c:sum([counter(c)[w] for w in vocab]))

it should be faster than your solution because it uses pandas vectorization and then the counter method.
you can skip saving the helper column as ""mytest_list"" for saving memory usage.",https://stackoverflow.com/questions/70555307,python,02-01-2022 10:42,391.0,0.0,1.0,True,11-03-2024 20:49,11-03-2024 20:49
72761858,formatting our data into pytorch dataset object for fine-tuning bert,"i'm using an already existing code from towards data science for fine-tuning a bert model.
the problem i'm facing belongs to this part of the code which where try to format our data into a pytorch data.dataset object:
class meditationsdataset(torch.utils.data.dataset):
    def _init_(self, encodings, *args, **kwargs):
        self.encodings = encodings
    def _getitem_(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
    def _len_(self):
        return len(self.encodings.input_ids)


dataset = meditationsdataset(inputs)

when i run the code, i face this error:
typeerror                                 traceback (most recent call last)
<ipython-input-144-41fc3213bc25> in <module>()
----> 1 dataset = meditationsdataset(inputs)

/usr/lib/python3.7/typing.py in __new__(cls, *args, **kwds)
    819             obj = super().__new__(cls)
    820         else:
--> 821             obj = super().__new__(cls, *args, **kwds)
    822         return obj
    823 

typeerror: object.__new__() takes exactly one argument (the type to instantiate)

i already searched for this error but the problem here is that sadly i'm not familiar with either pytorch or oop so i couldn't fix this problem. could you please let me know what should i add or remove from this code so i can run it? thanks a lot in advance.
also if needed, our data is as below:
{'input_ids': tensor([[   2, 1021, 1005,  ...,    0,    0,    0],
                      [   2, 1021, 1005,  ...,    0,    0,    0],
                      [   2, 1021, 1005,  ...,    0,    0,    0],
                      ...,
                      [   2, 1021, 1005,  ...,    0,    0,    0],
                      [   2,  103, 1005,  ...,    0,    0,    0],
                      [   2,    4,    0,  ...,    0,    0,    0]]), 
 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],
                           [0, 0, 0,  ..., 0, 0, 0],
                           [0, 0, 0,  ..., 0, 0, 0],
                           ...,
                           [0, 0, 0,  ..., 0, 0, 0],
                           [0, 0, 0,  ..., 0, 0, 0],
                           [0, 0, 0,  ..., 0, 0, 0]]), 
 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
                           [1, 1, 1,  ..., 0, 0, 0],
                           [1, 1, 1,  ..., 0, 0, 0],
                           ...,
                           [1, 1, 1,  ..., 0, 0, 0],
                           [1, 1, 1,  ..., 0, 0, 0],
                           [1, 1, 0,  ..., 0, 0, 0]]), 
 'labels': tensor([[   2, 1021, 1005,  ...,    0,    0,    0],
                   [   2, 1021, 1005,  ...,    0,    0,    0],
                   [   2, 1021, 1005,  ...,    0,    0,    0],
                   ...,
                   [   2, 1021, 1005,  ...,    0,    0,    0],
                   [   2, 1021, 1005,  ...,    0,    0,    0],
                   [   2,    4,    0,  ...,    0,    0,    0]])}","['python', 'oop', 'pytorch', 'bert-language-model']",72761932,"special functions in python use double underscores prefix and suffix. in your case, to implement a data.dataset, you must have __init__, __getitem__, and __len__:
class meditationsdataset(torch.utils.data.dataset):
    def __init__(self, encodings, *args, **kwargs):
        self.encodings = encodings
    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
    def __len__(self):
        return len(self.encodings.input_ids)",https://stackoverflow.com/questions/72761858,python,26-06-2022 13:07,171.0,0.0,1.0,True,26-06-2022 13:32,26-06-2022 13:32
74244702,"how to split input text into equal size of tokens, not character length, and then concatenate the summarization results for hugging face transformers","i am using the below methodology to summarize longer than 1024 token size long texts.
current method splits the text by half. i took this from another user's post and modified it slightly.
so what i want to do is, instead of splitting into half, split whole text into 1024 equal sized tokens and get summarization each of them and then at the end, concatenate them with the correct order and write into file. how can i do this tokenization and getting the correct output?
text split with split("" "") doesn't work same as tokenization. it produces different count.
import logging
from transformers import pipeline

f = open(""textfile1.txt"", ""r"")

article = f.read()

summarizer = pipeline(""summarization"", model=""facebook/bart-large-cnn"" )

counter = 1

def summarize_text(text: str, max_len: int) -> str:
    global counter
    try:
        #logging.warning(""max_len "" + str(max_len))
        summary = summarizer(text, min_length=30, do_sample=false)
        with open('parsed_'+str(counter)+'.txt', 'w') as f:
            f.write(text)
        counter += 1
        return summary[0][""summary_text""]
    except indexerror as ex:
        logging.warning(""sequence length too large for model, cutting text in half and calling again"")
        return summarize_text(text=text[:(len(text) // 2)], max_len=max_len) + "" "" + summarize_text(text=text[(len(text) // 2):], max_len=max_len)

gg = summarize_text(article, 1024)

with open('summarized.txt', 'w') as f:
    f.write(gg)","['python', 'nlp', 'huggingface-transformers', 'huggingface-tokenizers', 'huggingface']",74524543,"i like splitting text using nltk. you can also do it with spacy and the quality is better, but it takes a bit longer. nltk and spacy allow you to cut text into sentences and this is better because the text pieces are more coherent. you want to cut it less than 1024 to be on the safe side. 512 should be better and it's what the original bert uses, so it shouldn't be too bad. you just summarize the summarizations in the end. here's an example:
import nltk
from nltk.tokenize import sent_tokenize

def split_in_segments(text):
    tokens = 0
    mystring = list()
    segments = []
    for sent in sent_tokenize(text):
        newtokens = len(sent.split())
        tokens += newtokens
        mystring.append(str(sent).strip())
        if tokens > 512:
            segments.append("" "".join(mystring))
            mystring = []
            tokens = 0
    if mystring:
        segments.append("" "".join(mystring))
    return(segments)

def summarize_4_plotly(text):
    segments = split_in_segments(text)
    summarylist = summarizer(segments, max_length=100, min_length=30, do_sample=false)
    summary = summarizer("" "".join([summarylist[i]['summary_text'] for i in range(len(summarylist))]), max_length = 120, min_length = 30, do_sample = false)
    return(summary)

summarize_4_plotly(text)",https://stackoverflow.com/questions/74244702,python,29-10-2022 10:56,3593.0,3.0,1.0,True,22-11-2022 01:44,29-10-2022 11:08
77013746,how to release memory correctly in streamlit app?,"recently, while developing a streamlit app, the app frequently crashes and requires manual rebooting.
after spending some time, i identified the issue as ""exceeding ram"". the free version of ram is only 1gb, and my app easily surpasses this limit when multiple users are using it simultaneously.
application of the app

using langchain to build a document gpt.
users upload pdfs and start asking questions.

main problematic code
complete code from github
app.py
model = none

doc_container = st.container()
with doc_container:
    # when user upload pdf base on upload_and_process_pdf()
    # the create_doc_gpt() can execute successfully
    docs = upload_and_process_pdf()
    model = create_doc_gpt(docs)
    del docs
    st.write('---')

def create_doc_gpt(docs):
    if not docs:
        return

    ... instance docgpt which will use huggingfaceembedding

what i've tried
i attempted to identify where the issue in the code lies and whether optimization is possible. i conducted the following experiments:

used windows task manager's detailed view.

executed the app (streamlit run app.py) and simultaneously identified its pid, observing memory usage.

when opening the app, memory usage occupied 150,000 kb.

based on the simplified code above, after uploading a pdf, the docgpt instance (my model) is instantiated. at this point, memory rapidly spikes to 1,000,000 kb. i suspect this is due to huggingfaceembedding causing this. (when i switched to a lighter embedding, memory decreased significantly)

since memory's main source is the model instance, but when i re-upload the same pdf, memory increases again to 1,750,000 kb. this seems like two models are occupying memory.

additionally, i have attempted to repeatedly upload the same pdf on my app. after uploading the 8000kb file approximately 4 times, the app crashes.


question
how should i correctly release the initially instantiated model?
if i use st.cache_resource to decorate create_doc_gpt(docs), i have a few points of confusion as follows:

when the same user uploads the first pdf, the embedding is performed, and the model is returned. at this point, does the app create a cache and occupy memory? if the user uploads a new pdf again, will the app go through embedding and returning the model, creating a new cache and occupying memory again?

if the assumption in #1 is correct, can i use the ttl and max_entries parameters to avoid excessive caching?

if the assumptions in #1 and #2 are correct, when there are two users simultaneously, and my max_entries is set to 2, will the cached models they create be counted separately?



i'm unsure if this type of question is appropriate to ask here. if it's against the rules, i'm willing to delete the post and seek help elsewhere.","['python', 'memory', 'streamlit', 'langchain']",77016325,"i would recommend you to use session states in your application as much as possible. in combination with that you should use @st.cache_data and  @st.cache_resource for example in your function:
@st.cache_resource
def create_doc_gpt(docs):
    if not docs:
        return

you could then also delete the session state and refresh the site like this
for key in st.session_state.keys():
                del st.session_state[key]

you can read more about it here",https://stackoverflow.com/questions/77013746,python,31-08-2023 07:41,3511.0,0.0,1.0,True,01-09-2023 03:27,01-09-2023 03:27
70740565,optimize albert huggingface model,"goal: amend this notebook to work with albert-base-v2 model
kernel: conda_pytorch_p36.
section 2.1 exports the finalised model. it too uses a bert specific function. however, i cannot find an equivalent for albert.
i've successfully implemented alternatives for albert up until this section.
code:
# optimize transformer-based models with onnxruntime-tools
from onnxruntime_tools import optimizer
from onnxruntime_tools.transformers.onnx_model_bert import bertoptimizationoptions

# disable embedding layer norm optimization for better model size reduction
opt_options = bertoptimizationoptions('bert')
opt_options.enable_embed_layer_norm = false
...

do functions for optimizing and quantizing an albert model exist?
update: you can run quantization in the notebook, without running optimization. you just need to remove '.opt.' from code, that is an indicative of optimised filenames.","['python', 'huggingface-transformers', 'bert-language-model', 'onnx', 'huggingface-tokenizers']",70758315,"optimise any pytorch model, using torch_optimizer.
installation:
pip install torch_optimizer

implementation:
import torch_optimizer as optim

# model = ...
optimizer = optim.diffgrad(model.parameters(), lr=0.001)
optimizer.step()

source
torch.save(model.state_dict(), path)

source",https://stackoverflow.com/questions/70740565,python,17-01-2022 11:25,293.0,1.0,1.0,True,18-01-2022 15:35,17-01-2022 15:24
77970163,error message while installing spacy 2.3.5 - &quot;microsoft visual c++ 14.0 or greater is required&quot; even though build tools is already installed,"i want to use spacy version 2.3.5 since i have a trained an nlp model in this version.
i am able to install latest spacy using ""pip install spacy""
however installing any specific version fails
>>pip install spacy==2.3.5
  copying spacy\tests\tokenizer\sun.txt -> build\lib.win-amd64-cpython-310\spacy\tests\tokenizer
  running build_ext
  building 'spacy.parts_of_speech' extension
  error: microsoft visual c++ 14.0 or greater is required. get it with ""microsoft c++ build tools"":    
  [end of output]

  note: this error originates from a subprocess, and is likely not a problem with pip.   error: failed building wheel for spacy failed to build spacy error: could not build wheels for spacy, which is required to install pyproject.toml-based projects

build tools exists in my system:

i have tried reinstalling pip, build tools and restarted my system several times. build tools didn't exist in my environment variables path so i've added the path yet the spacy installation fails.
apologies if this appears like a duplicate question, but i've gone through solutions on stackoverflow and nothing works.
this answer suggested to install third part wheels instead but the web page isn't available.
my environment info:
virtual environment on python 3.10.10
windows 10 home edition","['python', 'python-3.x', 'pip', 'spacy', 'build-tools']",77974051,"spacy 2.3.5 is compatible with ""2.3.1"", ""2.3.0""
""2.3.5"": {
      ""en_core_web_sm"": [""2.3.1"", ""2.3.0""],
      ""en_core_web_md"": [""2.3.1"", ""2.3.0""],
      ""en_core_web_lg"": [""2.3.1"", ""2.3.0""]
}

you can install by:
pip install 

or
pip install 

here is the complete link :",https://stackoverflow.com/questions/77970163,python,09-02-2024 18:31,1194.0,1.0,2.0,True,27-05-2024 21:49,10-02-2024 18:39
78631769,problems with named entity recognition in spacy using german de_dep_news_trf pipeline,"i'm currently working on a project using spacy with the german trained pipeline de_dep_news_trf.
unfortunately, i'm having issues with named entity recognition (ner).
when i run a simple sentence like ""berlin ist die hauptstadt von deutschland. angela merkel war die bundeskanzlerin."", no entities are detected.
i've followed these steps to set up my python environment (3.12)(windows) in a pycharm community project:
python.exe -m pip install --upgrade pip
pip install -u pip setuptools wheel
pip install -u spacy
python -m spacy download de_dep_news_trf --timeout 600
pip install spacy[transformers]

here is a snippet of my code:
import spacy


def process_text_with_spacy(text_to_process):
    doc = nlp(text_to_process)
    data = {
        ""text"": text_to_process,
        ""sentences"": []
    }
    for sent in doc.sents:
        process_sentence_data = {
            ""sentence"": sent.text,
            ""entities"": []
        }
        for ent in sent.ents:
            process_sentence_data[""entities""].append({
                ""text"": ent.text,
                ""start"": ent.start_char,
                ""end"": ent.end_char,
                ""label"": ent.label_
            })
        data[""sentences""].append(process_sentence_data)
    return data


nlp = spacy.load('de_dep_news_trf')

sample_text = ""berlin ist die hauptstadt von deutschland. angela merkel war die bundeskanzlerin.""

processed_data = process_text_with_spacy(sample_text)

print(""text:"", sample_text)
for sentence_data in processed_data[""sentences""]:
    print(""sentence:"", sentence_data[""sentence""])
    print(""entities:"", sentence_data[""entities""])

output:
text: berlin ist die hauptstadt von deutschland. angela merkel war die bundeskanzlerin.
sentence: berlin ist die hauptstadt von deutschland.
entities: []
sentence: angela merkel war die bundeskanzlerin.
entities: []

when using de_core_news_lg, the output for each sentence is:
text: berlin ist die hauptstadt von deutschland. angela merkel war die bundeskanzlerin.
sentence: berlin ist die hauptstadt von deutschland.
entities: [{'text': 'berlin', 'start': 0, 'end': 6, 'label': 'loc'}, {'text': 'deutschland', 'start': 30, 'end': 41, 'label': 'loc'}]
sentence: angela merkel war die bundeskanzlerin.
entities: [{'text': 'angela merkel', 'start': 43, 'end': 56, 'label': 'per'}]

however, when i use de_dep_news_trf, the results are empty.
model de_dep_news_trf is selected based on ""accuracy"" from the spacy website.
could someone explain why de_dep_news_trf does not return the same result? is there a specific reason or setting that could cause this difference?
thank you for your help!","['python', 'nlp', 'spacy']",78632377,"problem is because this model doesn't have function to recognize entities.
see documentation for de_dep_news_trf - it has components transformer, tagger, morphologizer, parser, lemmatizer, attribute_ruler but no ner for entityrecognizer
so it may need to use one of other models :

de_core_news_sm
de_core_news_md
de_core_news_lg",https://stackoverflow.com/questions/78631769,python,17-06-2024 09:14,201.0,0.0,1.0,True,17-06-2024 11:34,17-06-2024 11:08
20880365,how can i apply feature reduction methods in weka?,"how can i apply feature reduction methods like lsi etc in weka for text classification?

can feature reduction methods like lsi etc improve the accuracy of classification?","['machine-learning', 'weka', 'text-classification', 'feature-selection']",20880617,"take a look at filteredclassifier class or at attributeselectedclassifier. with filteredclassifier you can use such features reduction method as principal component analysis (pca). here is a video how to filter your dataset using pca, so that you could try different classifiers on reduced dataset.
it can help, but there is no guarantee about that. if you remove redundant features, or transform features in some way (like svm or pca do) classification task can become simpler. anyway big number of features usually lead to curse of dimensionality and attribute selection is a way to avoid it.",https://stackoverflow.com/questions/20880365,machine-learning,02-01-2014 10:00,4497.0,-1.0,1.0,True,13-01-2025 01:04,13-01-2025 01:04
59472245,azure python deployment - spacy nomodule found exception,"i'm using linux app service. i'm trying to deploy python 3.6 flask application through the azure devops pipeline. it worked fine for a basic app but when i add an additional code (spacy module), it started to throw
2019-12-24t18:07:33.079953940z     __import__(module)
2019-12-24t18:07:33.079961840z   file ""/home/site/ line 3, in <module>
2019-12-24t18:07:33.079970340z     from data_cleanup_utility.clear_content_utility import clearcontent
2019-12-24t18:07:33.079978440z   file ""/home/site/ line 12, in <module>
2019-12-24t18:07:33.079986741z     import spacy
2019-12-24t18:07:33.079994741z **modulenotfounderror: no module named 'spacy'**
2019-12-24t18:07:33.084726683z [2019-12-24 18:07:33 +0000] [51] [info] worker exiting (pid: 51)
2019-12-24t18:07:33.170423056z [2019-12-24 18:07:33 +0000] [48] [info] shutting down: master
2019-12-24t18:07:33.172257711z [2019-12-24 18:07:33 +0000] [48] [info] reason: worker failed to boot.

i have added the dependency modules in the requirement.txt
flask==1.0.2
flask-cors==3.0.8
flask-restful==0.3.7
fastai==1.0.59
numpy==1.17.4
pandas==0.25.3
requests==2.22.0
spacy==2.2.3
spacy-langdetect==0.1.2

and azurepipeline.yml
- script: |
        python -m venv antenv
        source antenv/bin/activate
        python -m pip install --upgrade pip
        pip install setup
        pip install -r requirements.txt
        python -m spacy download es
      workingdirectory: $(projectroot)
      displayname: ""install requirements""

and my code clear_content_utility.py
import spacy
from spacy_langdetect import languagedetector

nlp = spacy.load('es')
nlp.add_pipe(languagedetector(), name='language_detector', last=true)

did anyone face the above issue? appreciate your help.","['python', 'python-3.x', 'spacy']",59484349,"here is something you should do:

please ensure to activate the virtual environment before executing any code, also make sure to check if your installed package is actually in there in your venv.

i would suggest you to create new venv and try activating it.

secondly , you can language data has been moved to a submodule **spacy.lang** to keep thing cleaner and better organised.
e.g. instead of using spacy.en, you now import from spacy.lang.en



    python -m spacy download en_core_web_sm
    import spacy
    nlp = spacy.load(""en_core_web_sm"")



additional reference:

importerror: no module named 'spacy.en'
hope it helps.",https://stackoverflow.com/questions/59472245,python,24-12-2019 18:19,713.0,0.0,1.0,True,22-08-2022 00:05,25-12-2019 09:40
56527814,stanford typed dependencies using corenlp in python,"in stanford dependency manual they mention  ""stanford typed dependencies"" and particularly the type ""neg"" - negation  modifier. it is also available when using stanford enhanced++ parser using the website. for example, the sentence: 

""barack obama was not born in hawaii""

 
the parser indeed find neg(born,not) 
but when i'm using the stanfordnlp python library, the only dependency parser i can get will parse the sentence as  follow:
('barack', '5', 'nsubj:pass')

('obama', '1', 'flat')

('was', '5', 'aux:pass')

('not', '5', 'advmod')

('born', '0', 'root')

('in', '7', 'case')

('hawaii', '5', 'obl')

and the code that generates it: 
import stanfordnlp
stanfordnlp.download('en')  
nlp = stanfordnlp.pipeline()
doc = nlp(""barack obama was not born in hawaii"")
a  = doc.sentences[0]
a.print_dependencies()

is there a way to get similar results to the enhanced dependency parser or any other stanford parser that result in typed dependencies that will give me the negation modifier?","['python', 'parsing', 'nlp', 'stanford-nlp']",56635121,"it is to note the python library stanfordnlp is not just a python wrapper for stanfordcorenlp. 
1. difference stanfordnlp / corenlp
as said on the stanfordnlp github repo:

the stanford nlp group's official python nlp library. it contains
  packages for running our latest fully neural pipeline from the conll
  2018 shared task and for accessing the java stanford corenlp server.

stanfordnlp contains a new set of neural networks models, trained on the conll 2018 shared task. the online parser is based on the corenlp 3.9.2 java library. those are two different pipelines and sets of models, as explained here. 
your code only accesses their neural pipeline trained on conll 2018 data. this explains the differences you saw compared to the online version. those are basically two different models.
what adds to the confusion i believe is that both repositories belong to the user named stanfordnlp (which is the team name). don't be fooled between the java stanfordnlp/corenlp and the python stanfordnlp/stanfordnlp. 
concerning your 'neg' issue, it seems that in the python libabry stanfordnlp, they decided to consider the negation with an 'advmod' annotation altogether. at least that is what i ran into for a few example sentences.
2. using corenlp via stanfordnlp package
however, you can still get access to the corenlp through the stanfordnlp package. it requires a few more steps, though. citing the github repo,

there are a few initial setup steps.

download stanford corenlp and models for the language you wish to    use. (you can download corenlp and the language models here)
put the model jars in the distribution folder
tell the python code where stanford corenlp is located: export corenlp_home=/path/to/stanford-corenlp-full-2018-10-05


once that is done, you can start a client, with code that can be found in the demo  :
from stanfordnlp.server import corenlpclient 

with corenlpclient(annotators=['tokenize','ssplit','pos','depparse'], timeout=60000, memory='16g') as client:
    # submit the request to the server
    ann = client.annotate(text)

    # get the first sentence
    sentence = ann.sentence[0]

    # get the dependency parse of the first sentence
    print('---')
    print('dependency parse of first sentence')
    dependency_parse = sentence.basicdependencies
    print(dependency_parse)

    #get the tokens of the first sentence
    #note that 1 token is 1 node in the parse tree, nodes start at 1
    print('---')
    print('tokens of first sentence')
    for token in sentence.token :
        print(token)

your sentence will therefore be parsed if you specify the 'depparse' annotator (as well as the prerequisite annotators tokenize, ssplit, and pos). 
reading the demo, it feels that we can only access basicdependencies. i have not managed to make enhanced++ dependencies work via stanfordnlp.
but the negations will still appear if you use basicdependencies !
here is the output i obtained using stanfordnlp and your example sentence. it is a dependencygraph object, not pretty, but it is unfortunately always the case when we use the very deep corenlp tools. you will see that between nodes 4 and 5 ('not' and 'born'), there is and edge 'neg'. 
node {
  sentenceindex: 0
  index: 1
}
node {
  sentenceindex: 0
  index: 2
}
node {
  sentenceindex: 0
  index: 3
}
node {
  sentenceindex: 0
  index: 4
}
node {
  sentenceindex: 0
  index: 5
}
node {
  sentenceindex: 0
  index: 6
}
node {
  sentenceindex: 0
  index: 7
}
node {
  sentenceindex: 0
  index: 8
}
edge {
  source: 2
  target: 1
  dep: ""compound""
  isextra: false
  sourcecopy: 0
  targetcopy: 0
  language: universalenglish
}
edge {
  source: 5
  target: 2
  dep: ""nsubjpass""
  isextra: false
  sourcecopy: 0
  targetcopy: 0
  language: universalenglish
}
edge {
  source: 5
  target: 3
  dep: ""auxpass""
  isextra: false
  sourcecopy: 0
  targetcopy: 0
  language: universalenglish
}
edge {
  source: 5
  target: 4
  dep: ""neg""
  isextra: false
  sourcecopy: 0
  targetcopy: 0
  language: universalenglish
}
edge {
  source: 5
  target: 7
  dep: ""nmod""
  isextra: false
  sourcecopy: 0
  targetcopy: 0
  language: universalenglish
}
edge {
  source: 5
  target: 8
  dep: ""punct""
  isextra: false
  sourcecopy: 0
  targetcopy: 0
  language: universalenglish
}
edge {
  source: 7
  target: 6
  dep: ""case""
  isextra: false
  sourcecopy: 0
  targetcopy: 0
  language: universalenglish
}
root: 5

---
tokens of first sentence
word: ""barack""
pos: ""nnp""
value: ""barack""
before: """"
after: "" ""
originaltext: ""barack""
beginchar: 0
endchar: 6
tokenbeginindex: 0
tokenendindex: 1
hasxmlcontext: false
isnewline: false

word: ""obama""
pos: ""nnp""
value: ""obama""
before: "" ""
after: "" ""
originaltext: ""obama""
beginchar: 7
endchar: 12
tokenbeginindex: 1
tokenendindex: 2
hasxmlcontext: false
isnewline: false

word: ""was""
pos: ""vbd""
value: ""was""
before: "" ""
after: "" ""
originaltext: ""was""
beginchar: 13
endchar: 16
tokenbeginindex: 2
tokenendindex: 3
hasxmlcontext: false
isnewline: false

word: ""not""
pos: ""rb""
value: ""not""
before: "" ""
after: "" ""
originaltext: ""not""
beginchar: 17
endchar: 20
tokenbeginindex: 3
tokenendindex: 4
hasxmlcontext: false
isnewline: false

word: ""born""
pos: ""vbn""
value: ""born""
before: "" ""
after: "" ""
originaltext: ""born""
beginchar: 21
endchar: 25
tokenbeginindex: 4
tokenendindex: 5
hasxmlcontext: false
isnewline: false

word: ""in""
pos: ""in""
value: ""in""
before: "" ""
after: "" ""
originaltext: ""in""
beginchar: 26
endchar: 28
tokenbeginindex: 5
tokenendindex: 6
hasxmlcontext: false
isnewline: false

word: ""hawaii""
pos: ""nnp""
value: ""hawaii""
before: "" ""
after: """"
originaltext: ""hawaii""
beginchar: 29
endchar: 35
tokenbeginindex: 6
tokenendindex: 7
hasxmlcontext: false
isnewline: false

word: "".""
pos: "".""
value: "".""
before: """"
after: """"
originaltext: "".""
beginchar: 35
endchar: 36
tokenbeginindex: 7
tokenendindex: 8
hasxmlcontext: false
isnewline: false

2. using corenlp via nltk package
i will not go into details on this one, but there is also a solution to access the corenlp server via the nltk library , if all else fails. it does output the negations, but requires a little more work to start the servers.
details on this page
edit
i figured i could also share with you the code to get the dependencygraph into a nice list of 'dependency, argument1, argument2' in a shape similar to what stanfordnlp outputs.
from stanfordnlp.server import corenlpclient

text = ""barack obama was not born in hawaii.""

# set up the client
with corenlpclient(annotators=['tokenize','ssplit','pos','depparse'], timeout=60000, memory='16g') as client:
    # submit the request to the server
    ann = client.annotate(text)

    # get the first sentence
    sentence = ann.sentence[0]

    # get the dependency parse of the first sentence
    dependency_parse = sentence.basicdependencies

    #print(dir(sentence.token[0])) #to find all the attributes and methods of a token object
    #print(dir(dependency_parse)) #to find all the attributes and methods of a dependencygraph object
    #print(dir(dependency_parse.edge))

    #get a dictionary associating each token/node with its label
    token_dict = {}
    for i in range(0, len(sentence.token)) :
        token_dict[sentence.token[i].tokenendindex] = sentence.token[i].word

    #get a list of the dependencies with the words they connect
    list_dep=[]
    for i in range(0, len(dependency_parse.edge)):

        source_node = dependency_parse.edge[i].source
        source_name = token_dict[source_node]

        target_node = dependency_parse.edge[i].target
        target_name = token_dict[target_node]

        dep = dependency_parse.edge[i].dep

        list_dep.append((dep, 
            str(source_node)+'-'+source_name, 
            str(target_node)+'-'+target_name))
    print(list_dep)

it ouputs the following 
[('compound', '2-obama', '1-barack'), ('nsubjpass', '5-born', '2-obama'), ('auxpass', '5-born', '3-was'), ('neg', '5-born', '4-not'), ('nmod', '5-born', '7-hawaii'), ('punct', '5-born', '8-.'), ('case', '7-hawaii', '6-in')]",https://stackoverflow.com/questions/56527814,python,10-06-2019 13:54,3901.0,6.0,5.0,True,17-03-2021 16:56,12-06-2019 15:08
67849833,how to truncate input in the huggingface pipeline?,"i currently use a huggingface pipeline for sentiment-analysis like so:
from transformers import pipeline
classifier = pipeline('sentiment-analysis', device=0)

the problem is that when i pass texts larger than 512 tokens, it just crashes saying that the input is too long.  is there any way of passing the max_length and truncate parameters from the tokenizer directly to the pipeline?
my work around is to do:
from transformers import autotokenizer, automodelforsequenceclassification
model_name = ""nlptown/bert-base-multilingual-uncased-sentiment""
model = automodelforsequenceclassification.from_pretrained(model_name)
tokenizer = autotokenizer.from_pretrained(model_name)
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, device=0)

and then when i call the tokenizer:
pt_batch = tokenizer(text, padding=true, truncation=true, max_length=512, return_tensors=""pt"")

but it would be much nicer to simply be able to call the pipeline directly like so:
classifier(text, padding=true, truncation=true, max_length=512)","['huggingface-transformers', 'huggingface-tokenizers']",68609735,"this way should work:
classifier(text, padding=true, truncation=true)

if it doesn't try to load tokenizer as:
tokenizer = autotokenizer.from_pretrained(model_name, model_max_len=512)",https://stackoverflow.com/questions/67849833,huggingface-transformers,05-06-2021 12:56,16795.0,26.0,4.0,True,27-11-2023 16:33,17-01-2022 15:57
76073565,gpt2 special tokens: ignore word(s) in input text when predicting next word,"i just started using gpt2 and i have a question concerning special tokens:
i'd like to predict the next word given a text input, but i want to mask some words in my input chunk using a special token. i don't want gpt2 to predict the masked words, i just don't want to use them for the prediction and i want gpt2 to ""know"" that it doesn't ""see"" all the input words.
here's an example:
i have ""the quick brown fox jumps over the lazy"" as an input sentence.
i want gpt2 to predict the last word (correct would be ""dog"" in this case).
i also want to mask the words ""the lazy"", but gpt2 should ""know"" there is something at the end of the input sentence. so basically for gpt2, the input should look like this: ""the quick brown fox jumps over _ _"", and not like this: ""the quick brown fox jumps over"", so it knows not to predict the word after ""over"".
i thought about using special tokens to replace the ""hidden"" words, but i think neither mask nor pad make sense in this case.
does anyone have an idea how to solve this?
thanks in advance for your help!","['python', 'nlp', 'token', 'predict', 'gpt-2']",76424001,"solved this, masking the tokens did the trick. i used an attention mask and set all attention mask values of tokens i wanted to ignore to 0, so their attention weights are 0 on all layers.",https://stackoverflow.com/questions/76073565,python,21-04-2023 13:20,360.0,0.0,1.0,True,07-06-2023 13:46,21-04-2023 13:26
69805465,partial matching of name in a corpus to names in another column in a pandas dataframe,"i have a dataframe like this
                            name                            corpus
0  james bond junior bristleback     agent james bond went missing
1            batman bin superman      superman saves the day again
2                  thor s/o odin  loki was last seen in march 2020

i wish to get this output.
                            name                            corpus  value
0  james bond junior bristleback     agent james bond went missing   true
1            batman bin superman      superman saves the day again   true
2                  thor s/o odin  loki was last seen in march 2020  false

i have previously tried regex but it seems i can't get the desired output. is there anyway to achieve this with regex or some other libraries/packages?","['python', 'python-3.x', 'pandas', 'dataframe', 'nlp']",69805526,"not sure if this exactly fits your needs. it essentially converts each sentence into a set of words, and checks if there is any overlap:
df.name.str.split().apply(set) & df.corpus.str.split().apply(set)

output:
0     true
1     true
2    false
dtype: bool",https://stackoverflow.com/questions/69805465,python,02-11-2021 04:40,59.0,-1.0,1.0,True,08-04-2022 22:07,08-04-2022 22:07
51012476,spacy custom tokenizer to include only hyphen words as tokens using infix regex,"i want to include hyphenated words for example: long-term, self-esteem, etc. as a single token in spacy. after looking at some similar posts on stackoverflow, github, its documentation and elsewhere, i also wrote a custom tokenizer as below:
import re
from spacy.tokenizer import tokenizer

prefix_re = re.compile(r'''^[\[\(""']''')
suffix_re = re.compile(r'''[\]\)""']$''')
infix_re = re.compile(r'''[.\,\?\:\;\...\ï¿½ï¿½ï¿½\ï¿½ï¿½ï¿""\'~]''')

def custom_tokenizer(nlp):
    return tokenizer(nlp.vocab, prefix_search=prefix_re.search,
                                suffix_search=suffix_re.search,
                                infix_finditer=infix_re.finditer,
                                token_match=none)

nlp = spacy.load('en_core_web_lg')
nlp.tokenizer = custom_tokenizer(nlp)

doc = nlp(u'note: since the fourteenth century the practice of ï¿½ï¿½ï¿½medicineï¿½ï¿½ï¿½ has become a profession; and more importantly, it\'s a male-dominated profession.')
[token.text for token in doc]

so for this sentence: 
'note: since the fourteenth century the practice of ï¿½ï¿½ï¿½medicineï¿½ï¿½ï¿½ has become a profession; and more importantly, it\'s a male-dominated profession.'
now, the tokens after incorporating the custom spacy tokenizer are: 
'note', ':', 'since', 'the', 'fourteenth', 'century', 'the', 'practice', 'of',
'ï¿½ï¿½ï¿½medicine',  'ï¿½ï¿½ï¿½', 'has', ';''and', 'more', 'importantly', ',', 
""it's"", 'a', 'male-dominated', 'profession', '.'
earlier, the tokens before this change were: 
'note',  ':',  'since',  'the',  'fourteenth',  'century',  'the',  'practice',  'of',  'ï¿½ï¿½ï¿½',  'medicine',  'ï¿½ï¿½ï¿½',  'has', 'become',  'a',  'profession',  ';',  'and',  'more',  'importantly',  ',',  'it<""'s"",  'a',  'male',  '-',  'dominated',  'profession',  '.'
and, the expected tokens should be:
'note',  ':',  'since',  'the',  'fourteenth',  'century',  'the',  'practice',  'of',  'ï¿½ï¿½ï¿½',  'medicine',  'ï¿½ï¿½ï¿½',  'has', 'become',  'a',  'profession',  ';',  'and',  'more',  'importantly',  ',',  'it<""'s"",  'a',  'male-dominated',  'profession',  '.'
summary: as one can see...

the hyphen word is included and so are the other punctuation marks except for the double quotes and apostrophe...
...but now, the apostrophe and double quotes don't have the earlier or expected behaviour.
i have tried different permutations and combinations for the regex compile for the infix but no progress to fix this issue.","['regex', 'nlp', 'tokenize', 'spacy', 'linguistics']",51022249,"using the default prefix_re and suffix_re gives me the expected output:
import re
import spacy
from spacy.tokenizer import tokenizer
from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex

def custom_tokenizer(nlp):
    infix_re = re.compile(r'''[.\,\?\:\;\...\ï¿½ï¿½ï¿½\ï¿½ï¿½ï¿½\`\ï¿½ï¿½ï¿½\ï¿½ï¿½ï¿½\""\'~]''')
    prefix_re = compile_prefix_regex(nlp.defaults.prefixes)
    suffix_re = compile_suffix_regex(nlp.defaults.suffixes)

    return tokenizer(nlp.vocab, prefix_search=prefix_re.search,
                                suffix_search=suffix_re.search,
                                infix_finditer=infix_re.finditer,
                                token_match=none)

nlp = spacy.load('en')
nlp.tokenizer = custom_tokenizer(nlp)

doc = nlp(u'note: since the fourteenth century the practice of ï¿½ï¿½ï¿½medicineï¿½ï¿½ï¿½ has become a profession; and more importantly, it\'s a male-dominated profession.')
[token.text for tok""lang-py prettyprint-override"">['note', ':', 'since', 'the', 'fourteenth', 'century', 'the', 'practice', 'of', 'ï¿½ï¿½ï¿½', 'medicine', 'ï¿½ï¿½ï¿½', 'has', 'become', 'a', 'profession', ';', 'and', 'more', 'importantly', ',', 'it', ""'s"", 'a', 'male-dominated', 'profession', '.']

if you want to dig into to why your regexes weren't working like spacy's, here are links to the relevant source code:
prefixes and suffixes defined here:"" rel=""nofollow noreferrer"">
with reference to characters (e.g, quotes, hyphens, etc.) defined here:

and the functions used to compile them (e.g., compile_prefix_regex):",https://stackoverflow.com/questions/51012476,regex,24-06-2018 17:45,11049.0,16.0,1.0,True,11-07-2023 14:33,16-06-2020 07:59
58320922,how to speed up a spacy pipeline with the nlp.pipe pattern?,"following spacy's pipeline documentation i have been trying to use nlp.pipe pattern to speed up my pipeline. what i have found though is that for whatever batch_size i set there is no speed up compared to a sequential run.
i was wondering if the issue is on my end or if batching doesn't work? 
i am testing this behaviour on 30000 texts which are on average 1500 characters long, i have tested batch sizes of 5,50,500,5000 with no avail.
so i timed:
for text in texts:
   doc = nlp(text)

vs
doc_gen = nlp.pipe(texts, batch_size, n_threads)

with n_threads -1 & 2
testing batch size 5, 50, 500, 5000
with texts containing 30000 documents with an average 1500 char length
my timing results don't show any significant difference between using the pipe pattern and not.
i am running python 3 with spacy 2.0.12","['python-3.x', 'spacy']",66822776,"the batch size is parameter specific to nlp.pipe, and again, a good value depends on the data being worked on. for reasonably long-sized text such as news articles, it makes sense to keep the batch size reasonably small (so that each batch doesn't contain really long texts), so in this case 20 was chosen for the batch size. for other cases (e.g. tweets) where each document is much shorter in length, a larger batch size can be used.

-prashanth rao, 
in addition to including this helpful quote above, the article i've linked above talks about 3 different ways to speed up text preprocessing with spacy.
if using a pipeline is not speeding up the process, i'd recommend using the .apply function instead as mentioned below and on that website! i've seen it shorten a process from taking 9+ hours down to taking 47 minutes.
the page linked above provides the following code:
def lemmatize(text):
    """"""perform lemmatization and stopword removal in the clean text
       returns a list of lemmas
    """"""
    doc = nlp(text)
    lemma_list = [str(tok.lemma_).lower() for tok in doc
                  if tok.is_alpha and tok.text.lower() not in stopwords]
    return lemma_list

the resulting lemmas are stored as a list in a separate column preproc as shown below.
%%time
df_preproc['preproc'] = df_preproc['clean'].apply(lemmatize)
df_preproc[['date', 'content', 'preproc']].head(3)",https://stackoverflow.com/questions/58320922,python-3.x,10-10-2019 10:34,5297.0,4.0,1.0,True,16-09-2021 13:41,13-10-2019 12:15
79196245,how do i format an openai api request in swift?,"i need to use the openai api and am struggling to format the request. i started by referencing the curl code that is listed in the openai api playground, and this code worked in my terminal. that request is here:
curl  \
  -h ""content-type: application/json"" \
  -h ""authorization: bearer $openai_api_key"" \
  -d '{
  ""model"": ""gpt-4o"",
  ""messages"": [
    {
      ""role"": ""user"",
      ""content"": [
        {
          ""type"": ""text"",
          ""text"": ""write a haiku about weightlifting""
        }
      ]
    }
  ],
  ""temperature"": 1,
  ""max_tokens"": 2048,
  ""top_p"": 1,
  ""frequency_penalty"": 0,
  ""presence_penalty"": 0,
  ""response_format"": {
    ""type"": ""text""
  }
}'

then, i started working on translating that request into swift instead of curl. i've been referencing some youtube tutorials, and here is my code so far.
let openaiurl = url(string: ""
var request = urlrequest(url: openaiurl)
request.setvalue(""application/json"", for ""content-type"")
request.addvalue(""bearer \(openaiapikey)"", for ""authorization"")
request. = ""post""
let  [string: any] = [
    ""messages"" : [
        {
            ""role"": ""user"", //error: consecutive statements on a line must be separated by ';'
            ""content"": ""write a haiku about weightlifting""
        }
    ],
    ""model"": ""gpt-4o"",
    ""max_tokens"" : 100,
    ""temperature"": string(temperature),
    ""frequency_penalty"": 0,
    ""presence_penalty"": 0,
    ""response_format"": { //error: consecutive statements on a line must be separated by ';'
        ""type"": ""text""
    }
]

i am getting the error that says ""consecutive statements on a line must be separated by ';'"" on the two lines that are using curly braces.
what do i need to do to format the api request? the temperature variable is a double that is currently set to 1.0.","['swift', 'openai-api']",79196281,"try this approach using a jsonencoder to encode the json parameters, such as:
            let body = """"""
{
                    ""messages"" : [
                        {
                            ""role"": ""user"",
                            ""content"": ""write a haiku about weightlifting""
                        }
                    ],
                    ""model"": ""gpt-4o"",
                    ""max_tokens"" : 100,
                    ""temperature"": 1.0,
                    ""frequency_penalty"": 0,
                    ""presence_penalty"": 0,
                    ""response_format"": { 
                        ""type"": ""text""
                    }
}
""""""
            
            do {
                request. = try jsonencoder().encode(body)

                let (responsedata, response) = try await urlsession.shared.data(for: request)

                print(""-----> \n \(string(data: responsedata, encoding: .utf8) as anyobject) \n"")

            } catch {
                print(""---> error: \(error)"")
            }

edit-1
here is my test code using swiftui. since i don't have a key i cannot fully test this code, but it compiles and gives some info about the response.
import swiftui

struct contentview: view {
    let openaiapikey = ""for testing""
    
    var body: some view {
        text(""testing"")
            .task {
                await dopost()
            }
    }
    
    func dopost() async {
        let openaiurl = url(string: ""
        
        var request = urlrequest(url: openaiurl)
        request.setvalue(""application/json"", for ""content-type"")
        request.addvalue(""bearer \(openaiapikey)"", for ""authorization"")
        request. = ""post""
        
        let body = """"""
{
""messages"": [
{
""role"": ""user"",
""content"": ""write a haiku about weightlifting""
}
],
""model"": ""gpt-4o"",
""max_tokens"" : 100,
""temperature"": 1.0,
""frequency_penalty"": 0,
""presence_penalty"": 0,
""response_format"": { 
    ""type"": ""text""
}
}
""""""
        do {
            request. = try jsonencoder().encode(body)
            
            let (responsedata, response) = try await urlsession.shared.data(for: request)
            print(""-----> responsedata \n \(string(data: responsedata, encoding: .utf8) as anyobject) \n"")
        }
        catch { print(error) }
    }
}

edit-2
another alternative,
struct contentview: view {
    let openaiapikey = ""for testing""
    
    var body: some view {
        text(""testing"")
            .task {
                let url = url(string: ""
                var request = urlrequest(url: url)
                request. = ""post""
                request.setvalue(""application/json"", for ""content-type"")
                request.setvalue(""bearer \(openaiapikey)"", for ""authorization"")
                
                let requestbody: [string: any] = [
                    ""model"": ""gpt-4o"",
                    ""messages"": [
                        [
                            ""role"": ""user"",
                            ""content"": [
                                [
                                    ""type"": ""text"",
                                    ""text"": ""write a haiku about weightlifting""
                                ]
                            ]
                        ]
                    ],
                    ""temperature"": 1,
                    ""max_tokens"": 2048,
                    ""top_p"": 1,
                    ""frequency_penalty"": 0,
                    ""presence_penalty"": 0,
                    ""response_format"": [
                        ""type"": ""text""
                    ]
                ]
                
                do {
                    request. = try jsonserialization.data(withjsonobject: requestbody)
                    
                    let (responsedata, response) = try await urlsession.shared.data(for: request)
                    print(""-----> responsedata \n \(string(data: responsedata, encoding: .utf8) as anyobject) \n"")
                }
                catch { print(error) }
                
            }
    }
}",https://stackoverflow.com/questions/79196245,swift,16-11-2024 23:31,463.0,1.0,1.0,True,17-11-2024 22:56,17-11-2024 00:52
72545744,how to convert small dataset into word embeddings instead of one-hot encoding?,"i have a dataset of 33 words that are a mix of verbs and nouns, for eg. father, sing, etc. i have tried converting them to 1-hot encoding but for my use case, it has been suggested to look into word2vec embedding. i have looked in gensim and glove but struggling to make it work.
how could i convert my data into an embedding? such that two words that may be semantically closer may have a lesser distance between their respective vectors. how may this be achieved or any helpful material on the same?
such as this","['nlp', 'stanford-nlp', 'gensim', 'word2vec']",72547065,"since your dataset is quite small, and i'm assuming it doesn't contain any jargon, it's best to use a pre-trained model in order to save up on training time.
with gensim, it's as simple as:
import gensim.downloader as api
wv = api.load('word2vec-google-news-300')

the 'word2vec-google-news-300' model has been pre-trained on a part of the google news dataset and generalizes well enough to most tasks. following this, you can create word embeddings/vectors like so:
vec = wv['father']

and, finally, for computing word similarity:
similarity_score = wv.similarity('father', 'sing')

lastly, one major limitation of word2vec is it's inability to deal with words that are oov(out of vocabulary). for such cases, it's best to train a custom model for your corpus.",https://stackoverflow.com/questions/72545744,nlp,08-06-2022 12:30,1018.0,1.0,1.0,True,08-06-2022 14:00,08-06-2022 13:18
71033726,"when applying word2vec, should i standardize the cosine values within each year, when comparing them across years?","i'm a researcher, and i'm trying to apply npl to understand the temporal changes of the meaning of some words.
so far i have obtained the trained embeddings (word2vec, sgn) of several years with identical parameters in the training.
for example, if i want to test the change of cosine similarity of word a and word b over 5 years, should i just compute them and plot the cosine values?
the reason i'm asking this is that i found the overall cosine values (mean of all possible pairs within that year) differ across the 5 years. **for example, 1990:0.21, 1991:0.19, 1992:0.31, 1993:0.22, 1994:0.31. does it mean in some years, all words are more similar to each other than other years??
base on my limited understanding, i think the vectors are odds in logistic functions, so they shouldn't be significantly affected by the size of the corpus? is it necessary for me to standardize the cosine values (of all pairs within each year) so i can compare the relative ranking change across years? or just trust the raw cosine values and compare them across years?","['word2vec', 'word-embedding', 'cosine-similarity']",71039759,"in general you should not think of cosine-similarities as an absolute measure that'd be comparable between models. that is, you should not think of ""0.7"" cosine-similarity as anything like ""70%"" similar, and choose some arbitrary ""70%"" threshold to be used across models.
instead, it's only a measure within a single model's induced space - with its effective 'scale' affected by all the parameters & the training data.
one small exercise that may help illustrate this: with the exact same data, train a 100d model, then a 200d model. then look at some word pairs, or words alongside their nearest-neighbors ranked by cosine-similarity.
with enough training/data, generally the same highly-related words will be nearest-neighbors of each other. but the effective ranges of cosine-similarity values will be very different. if you chose a specific threshold in one model as meaning, ""close enough to feed some other analysis"", the same threshold would not be sufficient in the other. every model is its own world, induced by the training data & parameters, as well as some sources of explicit or implicit randomness during training. (several parts of the word2vec algorithm use random sampling, but also any efficient multi-threaded training will encounter arbitray differences in training-order via host os thread-scheduling vagaries.)
if your parameters are identical, & the corpora very-alike in every measurable internal proportion, these effects might be minimized, but never eliminated.
for example, even if people's intended word meanings were perfectly identical, one year's training data might include more discussion of 'war' or 'politics' or some medical-topic, than another. in that case, the iterative, interleaved tug-of-war in training updates will mean words from that overrepresented domain have far more push-pull influence on the final model word positions ï¿½ï¿½ï¿½ essentially warping subregions of the final space for  distinctions some places, and thus *coarser distinctions in the less-updated zones.
that is, you shouldn't expect any global-per-model scaling factor (as you've implied might apply) to correct for any model-to-model differences. the influences of different data & training runs are far more subtle, and might affect different 'neighborhoods' of words differently.
instead, when comparing different models, a more stable grounds for comparison is relative rankings or relative-proportions of words with respect to their closeness-to-others. did words move into, or out of, each others' top-n neighbors? did a move more closely to b than c did to d? etc.
even there, you might want to be careful about differences in the full vocabulary: if a & b were each others' closest neighbors year 1, but 5 other words squeeze between them in year 2, did any word's meaning really change? or might it simply be because those other words weren't even suitably represented in year 1 to receive any position, or previously had somewhat 'noisier' positions nearby? (as words get rarer their positions from run to run will be more idiosyncratic, based on their few usage examples, and the influences of those other sources of run-to-run 'noise'.)
limiting all such analyses to very-well-represented words will minimize misinterpreting noise-in-the-models as something meaningful. re-running models more than once, either with same parameters or slightly-different ones, or slightly-different training data subsets, and seeing which comparisons hold up across such changes, may also help determine which observed changes are robust, versus methodological artifacts such as jitter from run-to-run, or other sampling effects.
a few previous answers on similar questions about comparing word-vectors across different source corpora may have other useful ideas or caveats for you:
how calculate distance between 2 node2vec model
word embeddings for the same word from two different texts
how to compare cosine similarities across three pretrained models?",https://stackoverflow.com/questions/71033726,word2vec,08-02-2022 12:13,490.0,1.0,1.0,True,09-02-2022 19:01,09-02-2022 08:16
69730273,how to sum up the word frequencies after stemming in racket?,"as background i'm trying to make a nlp application in racket and i arrived at the part where i have to stem the words (i also obtained their frequency).
i am using the (planet dyoo/porter-stemmer) package in order to stem, and as an example we can write:
(map (ï¿½ï¿½(x) (list (stem (first x)) (second x)))
     '((""cryed"" 1)
       (""racketeer"" 2)
       (""crying"" 3)
       (""playing"" 4)
       (""racketing"" 5)
       (""plays"" 6)
       (""racket"" 7)))

which produces: '((""cry"" 1) (""racket"" 2) (""cry"" 3) (""plai"" 4) (""racket"" 5) (""plai"" 6) (""racket"" 7))
now my goal is to sum up the frequency for each term, aka to arrive at: '((""cry"" 4) (""racket"" 14) (""plai"" 10))
i came up with a way to do it, but i don't like my solution:
(define (frequencying)
  (map (ï¿½ï¿½(x) (list (first x) (length x)))
       (group-by (ï¿½ï¿½(x) x) (string-split string))))

(define (recalculate lst)
  (frequency
   (string-join
    (flatten
     (map (ï¿½ï¿½(x) (make-list (second x) (first x))) lst)))))

basically i retype each word as many times as it's frequency, then make a single string containing all words and finally compute the frequency again. is there a simpler(faster) way to achieve this?
i should perhaps add that the order doesn't matter (""plai"" can come up before ""cry"" and so on). also i'm looking for a simpler solution because i'm gonna have to use larger datasets and i want to make this faster (i'd also be glad even if the frequency function can be made more f","['nlp', 'racket']",69731203,"you could create an add-count procedure that takes a list of counts and a new count as arguments, and adds the count to the list if there are no similarly tagged counts already in the list, or combines the new count with an existing count.
#lang racket

(define (get-tag c) (first c))

(define (get-val c) (second c))

(define (add-count cs c)
  (let* ((k (get-tag c))
         (v (get-val c))
         (old-count (assoc k cs)))
    (if old-count
        (cons (list k (+ v (get-val old-count)))
              (remove old-count cs))
        (cons c cs))))

here get-tag and get-val are just convenience procedures to access the tag and value stored in a count. the assoc procedure is used to extract a copy of the first count in cs matching the new count c to be added. this count is stored in old-count, the value of which is used to create a new count which is added to the list after removing old-count from the original list cs.
with the add-count procedure defined, a procedure reduce-counts could be defined that goes through all of the counts and accumulates them to an empty list by using add-count. the resulting list will have the counts combined.
(define (reduce-counts cs (acc '()))
  (if (null? cs)
      acc
      (reduce-counts (rest cs) (add-count acc (first cs)))))

here is a test run:
reduce-counts.rkt> (define test-counts '((""cry"" 1) (""racket"" 2) (""cry"" 3) (""play"" 4) (""racket"" 5) (""play"" 6) (""racket"" 7)))
reduce-counts.rkt> (reduce-counts test-counts)
'((""racket"" 14) (""play"" 10) (""cry"" 4))

as an alternative approach you could use filter to collect counts with similar tags in a list, and combine those into a new count after summing the values. the combined counts can be collected in an accumulator before filtering the input to remove the tags which were just combined. this process can be repeated recursively until all counts have been combined, removed, and collected.
;;; an alternate solution
(define (combine-like-counts cs)
  (list (get-tag (first cs))
        (foldl (lambda (c x) (+ x (get-val c))) 0 cs)))

(define (reduce-counts cs (acc '()))
  (if (null? cs)
      acc
      (let* ((k (get-tag (first cs)))
             (k-tag? (lambda (c) (equal? k (get-tag c))))
             (like (filter k-tag? cs))
             (remaining (filter (negate k-tag?) cs)))
        (reduce-counts remaining
                       (cons (combine-like-counts like) acc)))))

here the combine-like-counts procedure assumes that all counts in the input list share the same tag, so a new count is formed by taking the tag and the sum of all values into a list.
the new reduce-counts procedure returns whatever has been placed in the accumulator when the input is the empty list, otherwise the tag of the first count is saved and used to create the k-tag? predicate, which is then used with filter to create a list of matching counts and a list of the remaining counts with all matching counts removed. the list of matching counts is combined into a single count with combine-like-counts and added to the accumulator, which is passed along with remaining recursively to reduce-counts.
this works as before, although the ordering has changed:
reduce-counts.rkt> (define test-counts '((""cry"" 1) (""racket"" 2) (""cry"" 3) (""play"" 4) (""racket"" 5) (""play"" 6) (""racket"" 7)))
reduce-counts.rkt> (reduce-counts test-counts)
'((""play"" 10) (""racket"" 14) (""cry"" 4))

i would suspect that these two implementations would have different performance characteristics depending on the particulars of their input data. my hunch is that the second would fare better for large input that contained large quantities of each tag, but the real answer would come from testing on some representative data samples.
if you are really concerned about performance for large amounts of data, you might consider converting the data to a hash table and using some of the built-in dictionary procedures to arrive at a similar solution.",https://stackoverflow.com/questions/69730273,nlp,26-10-2021 21:38,113.0,1.0,1.0,True,27-10-2021 02:40,26-10-2021 22:01
79355967,huggingface pretrained model for fine-tuning has 100% trainable parameters,"i believe iï¿½ï¿½ï¿½m correctly following huggingfaceï¿½ï¿½ï¿½s documentation on fine-tuning pretrained models, but i get a model with 100% trainable parameters. i thought only some layers would be unfrozen and optimized, but it looks like all of them are.""lang-py prettyprint-override"">def print_trainable_parameters(model):
    """"""
    prints the number of trainable parameters in the model.
    """"""
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f""trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}""
    )

...
# id2label and label2id represent 3 classes in my current problem

model_name = ""nvidia/segformer-b5-finetuned-cityscapes-1024-1024""
model = automodelforsemanticsegmentation.from_pretrained(model_name, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=true)
print_trainable_parameters(model)

prints the following:
some weights of segformerforsemanticsegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-cityscapes-1024-1024 and are newly initialized because the shapes did not match:
- decode_head.classifier.weight: found shape torch.size([19, 768, 1, 1]) in the checkpoint and torch.size([3, 768, 1, 1]) in the model instantiated
- decode_head.classifier.bias: found shape torch.size([19]) in the checkpoint and torch.size([3]) in the model instantiated
you should probably train this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 84595651 || all params: 84595651 || trainable%: 100.00

why 100% trainable parameters? i could use peft to reduce the number of trainable parameters, but i thought that only a small subset of the parameters would be free to be optimized based on the warning message of layer decode_head.classifier.","['pytorch', 'huggingface-transformers', 'fine-tuning']",79416575,"this is the expected behavior. the library can't freeze the layers for you. you can freeze them yourself by setting requires_grad to false for certain layers as shown below:
from transformers import automodelforsemanticsegmentation

model_name = ""nvidia/segformer-b5-finetuned-cityscapes-1024-1024""
model = automodelforsemanticsegmentation.from_pretrained(model_name)

print_trainable_parameters(model)
# freezing everything except the decoder head
for name, param in model.named_parameters():
    if not name.startswith(""decode_head""): 
      param.requires_grad = false
print_trainable_parameters(model)

output:
trainable params: 84607955 || all params: 84607955 || trainable%: 100.00
trainable params: 3164947 || all params: 84607955 || trainable%: 3.74",https://stackoverflow.com/questions/79355967,pytorch,14-01-2025 18:09,80.0,3.0,1.0,True,06-02-2025 01:12,06-02-2025 01:09
65851158,how to add the count vectorizer to simple rnn model?,"for my nlp project i used countvectorizer to extract features from a dataset using vectorizer = countvectorizer(stop_words='english') and all_features = vectorizer.fit_transform(data.text)  and i also wrote a simple rnn model using keras but i am not sure how to do the padding and the tokeniser step and get the data be trained on the model.
my code for rnn is:
model.add(keras.layers.recurrent.simplernn(units = 1000, activation='relu',
use_bias=true))
model.add(keras.layers.dense(units=1000, input_dim = 2000, activation='sigmoid'))
model.add(keras.layers.dense(units=500, input_dim=1000, activation='relu'))
model.add(keras.layers.dense(units=2, input_dim=500,activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

return model

can someone please give me some advice on this?
thank you","['keras', 'nlp', 'recurrent-neural-network', 'countvectorizer']",65865469,"add ensemble - you don't count vectorize, you use ensemble

 docs=ufo_df[""summary""] #text
 labels=['egg', 'cross','sphere', 'triangle','disk','oval','rectangle','teardrop']
 #labels=['triangle']
 target=ufo_df[labels]
 #print([len(d) for d in docs])
 encoded_docs=[one_hot(d,vocab_size) for d in docs]
 #print([np.max(d) for d in encoded_docs])
 padded_docs = pad_sequences(encoded_docs,   maxlen=max_length, padding='post')

 #print([d for d in padded_docs])
 model=sequential()
 model.add(embedding(vocab_size, 8,   input_length=max_length))
 model.add(flatten())
 model.add(dense(8, activation='softmax'))
#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
 model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

 model.fit(padded_docs, target, epochs=50, verbose=0)",https://stackoverflow.com/questions/65851158,keras,22-01-2021 18:55,887.0,0.0,1.0,True,18-03-2021 16:14,18-03-2021 16:14
69933708,how do i compute and add meta data to an existing elasticsearch index?,"i loaded over 38 million documents (text strings) to an elasticsearch index on my local machine. i would like to compute the length of each string and add that value as meta data in the index.
should i have computed the string lengths as meta data before loading the documents to elasticsearch? or, can i update the meta data with a computed value after the fact?
i'm relatively new to elasticsearch/kibana and these questions arose because of the following python experiments:

data as a list of strings
 mylist = ['string_1', 'string_2',..., 'string_n']
 l = [len(s) for s in mylist]  # this computation takes about 1 minute on my machine

the downside of option 1 is that i'm not leveraging elasticsearch and 'mylist' is occupying a large chunk of memory.

data as an elasticsearch index where each string in 'mylist' was loaded into the field 'text'.
 from haystack.document_store.elasticsearch import elasticsearchdocumentstore
 document_store = elasticsearchdocumentstore(host='localhost', username='', password='', index='myindex')
 docs = document_store.get_all_documents_generator()
 l = [len(d.text) for d in docs]  # this computation takes about 6 minutes on my machine

the downside of option 2 is that it took much longer to compute. the upside is the generator() freed up memory. the long computation time is why i thought storing the string length (and other analytics) as meta data in elasticsearch would be a good solution.


are there other options i should consider? what am i missing?","['python-3.x', 'elasticsearch', 'kibana', 'nlp-question-answering', 'haystack']",69939166,"if you want to store the size of the whole document, i suggest installing the mapper-size plugin, which will store the size of the source document in the _size field.
if you only want to store the size of a specific field of your source document, then you need to proceed differently.
what i suggest is to create an ingest pipeline that will process each document just before it gets indexed. that ingest pipeline can then be used either when indexing the documents the first time or after having loaded the documents. i'll show you how.
first, create the ingest pipeline with a script processor that will store the size of the string in the text field in another field called textlength.
put _ingest/pipeline/string-length
{
  ""description"": ""my optional pipeline description"",
  ""processors"": [
    {
      ""script"": {
        ""source"": ""ctx.textlength = ctx.text.length()""
      }
    }
  ]
}

so, in the case you've already loaded the documents into elasticsearch and would like to enrich each document with the length of one of its fields, you can do it after the fact by using the update by query api, like this:
post myindex/_update_by_query?pipeline=string-length&wait_for_completion=false

it is also possible to leverage that ingest pipeline at indexing time when the documents get indexed the first time, simply by referencing the pipeline in your index query, like this:
put myindex/_doc/123?pipeline=string-length

both options will work, try it out and pick the one that best suits your needs.",https://stackoverflow.com/questions/69933708,python-3.x,11-11-2021 19:16,877.0,1.0,1.0,True,08-07-2022 16:18,08-07-2022 16:18
73201858,&quot;no module named keras&quot; error in transformers,"i'm trying to load a pretrained bert model in a sagemaker training job using the transformers library and i'm getting ""no modul named keras error"". you can find the relevant code, imports and requirements.txt below
import tensorflow as tf
from tensorflow.keras.models import sequential
from tensorflow.keras.layers import dense
from tensorflow.keras.wrappers.scikit_learn import kerasclassifier
from tensorflow.keras import applications
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras import optimizers
from tensorflow.keras import metrics
from tensorflow.keras import model
from transformers import tfautomodel
from transformers import berttokenizer
from tensorflow.keras.layers import leakyrelu

bert1 = tfautomodel.from_pretrained('path/to/bert', from_pt=true)

requirements.txt (i haven't set any versions for these):
transformers
torch
sentencepiece

other env settings:
python= 3.7
tensorflow= 2.3

i had used these exact settings a few months back and faced no issues, so not sure why i'm getting this error now
edit: based on the answers i received, i added keras to my requirements and also added from tensorflow import keras statement, and now i'm getting the following error:
modulenotfounderror: no module named 'keras.saving'","['python', 'keras', 'tensorflow2.0', 'huggingface-transformers']",73202801,"turns out a new version of the huggingface-transformers library was released a few days ago. so setting the transformers version to 4.20.1 solved the issue.
maybe upgrading tensorflow to 2.7 might work as well.",https://stackoverflow.com/questions/73201858,python,02-08-2022 04:05,3839.0,1.0,4.0,True,11-11-2022 15:21,02-08-2022 09:28
70663782,"valueerror: layer weight shape (30522, 768) not compatible with provided weight shape ()","i got word-embedding using bert and need to feed it as an embedding layer in the keras model, and the error i got is
valueerror: layer weight shape (30522, 768) not compatible with provided weight shape ()

the model is
embedding = embedding(30522, 768, mask_zero=true)(sentence)
model.layers[1].set_weights([embedding_matrix])","['tensorflow', 'keras', 'deep-learning', 'huggingface-transformers', 'bert-language-model']",70664004,"you are passing to set_weights a list of list:
embedding_matrix = [np.random.uniform(0,1, (30522, 768))]

sentence = input((20,))
embedding = embedding(30522, 768, mask_zero=true)(sentence)
model = model(sentence, embedding)

model.layers[1].set_weights([embedding_matrix])

while you should simply pass a list of arrays:
embedding_matrix = np.random.uniform(0,1, (30522, 768))

sentence = input((20,))
embedding = embedding(30522, 768, mask_zero=true)(sentence)
model = model(sentence, embedding)

model.layers[1].set_weights([embedding_matrix])",https://stackoverflow.com/questions/70663782,tensorflow,11-01-2022 08:34,670.0,0.0,1.0,True,10-07-2023 15:23,10-07-2023 15:23
76284412,how can i stream a response from langchain&#39;s openai using flask api?,"i am using python flask app for chat over data. in the console i am getting streamable response directly from the openai since i can enable streming with a flag streaming=true.
the problem is, that i can't ""forward"" the stream or ""show"" the strem than in my api call.
code for the processing openai and chain is:
def askquestion(self, collection_id, question):
    collection_name = ""collection-"" + str(collection_id)
    self.llm = chatopenai(model_name=self.model_name, temperature=self.temperature, openai_api_key=os.environ.get('openai_api_key'), streaming=true, callback_manager=callbackmanager([streamingstdoutcallbackhandler()]))
    self.memory = conversationbuffermemory(memory_key=""chat_history"", return_messages=true,  output_key='answer')
    
    chroma_vectorstore = chroma(collection_name=collection_name, embedding_function=self.embeddingsopenai, client=self.chroma_client)


    self.chain = conversationalretrievalchain.from_llm(self.llm, chroma_vectorstore.as_retriever(similarity_search_with_score=true),
                                                        return_source_documents=true,verbose=verbose, 
                                                        memory=self.memory)
    

    result = self.chain({""question"": question})
    
    res_dict = {
        ""answer"": result[""answer""],
    }

    res_dict[""source_documents""] = []

    for source in result[""source_documents""]:
        res_dict[""source_documents""].append({
            ""page_content"": source.page_content,
            ""metadata"":  source.metadata
        })

    return res_dict

and the api route code:
@app.route(""/collection/<int:collection_id>/ask_question"", methods=[""post""])
def ask_question(collection_id):
    question = request.form[""question""]
    # response_generator = document_thread.askquestion(collection_id, question)
    # return jsonify(response_generator)

    def stream(question):
        completion = document_thread.askquestion(collection_id, question)
        for line in completion['answer']:
            yield line

    return app.response_class(stream_with_context(stream(question)))

i am testing my endpoint with curl and i am passing flag -n to curl, so i should get the streamable response, if it is possible.
when i make api call first the endpoint is waiting to process the data (i can see in my terminal in vs code the streamable answer) and when finished, i get everything displayed in one go.","['python', 'flask', 'openai-api', 'langchain']",76361636,"with the usage of threading and callback we can have a streaming response from flask api.
in flask api, you may create a queue to register tokens through langchain's callback.
class streaminghandler(basecallbackhandler):
    ...

    def on_llm_new_token(self, token: str, **kwargs) -> none:
        self.queue.put(token)

you may get tokens from the same queue in your flask route.
from flask import response, stream_with_context
import threading 

@app.route(....):
def stream_output():
   q = queue()
   
   def generate(rq: queue):
      ...
      # add your logic to prevent while loop
      # to run indefinitely  
      while( ...):
          yield rq.get()
   
   callback_fn = streaminghandler(q)
   
   threading.thread(target= askquestion, args=(collection_id, question, callback_fn))
   return response(stream_with_context(generate(q))


in your langchain's chatopenai add the above custom callback streaminghandler.
self.llm = chatopenai(
  model_name=self.model_name, 
  temperature=self.temperature, 
  openai_api_key=os.environ.get('openai_api_key'), 
  streaming=true, 
  callback=[callback_fn,]
)

for reference:",https://stackoverflow.com/questions/76284412,python,18-05-2023 20:53,6481.0,7.0,2.0,True,17-06-2024 19:13,17-06-2024 19:13
68499164,add known matches to spacy document with character offsets,"i would like to run some analysis on documents using different spacy tools, though i am interested in the dependency matcher in particular.
it just so happens that for these documents, i already have the character offsets of some difficult-to-parse entities. a somewhat-contrived example:
from spacy.lang.en import english

nlp = english()
text = ""apple is opening its first big office in san francisco.""
already_known_entities = [
    {""offsets"":(0,5), ""id"": ""apple""}, 
    {""offsets"":(41,54), ""id"": ""san-francisco""}
]

# do something here so that `nlp` knows about those entities 

doc = nlp(text)

i've thought about doing something like this:
from spacy.lang.en import english

nlp = english()
text = ""apple is opening its first big office in san francisco.""
already_known_entities = [{""offsets"":(0,5), ""id"": ""apple""}, {""offsets"":(41,54), ""id"": ""san-francisco""}]

ruler = nlp.add_pipe(""entity_ruler"")
patterns = []
for e in already_known_entities:
    patterns.append({
        ""label"": ""gpe"",
        ""pattern"": text[e[""offsets""][0]:e[""offsets""][1]]
    })
ruler.add_patterns(patterns)

doc = nlp(text)

this technically works, and it's not the worst solution in the world, but i was still wondering if offsets can be added to the nlp object directly. as far as i can tell, the matcher docs don't show anything like this. i also understand this might be a bit of a departure from typical matcher behavior, where a pattern can be applied to all documents in a corpus--whereas here i want to tag entities at certain offsets only for particular documents. offsets from one document do not apply to other documents.","['python', 'machine-learning', 'nlp', 'spacy', 'dependency-parsing']",68524306,"you are looking for doc.char_span.
doc = ""blah blah blah""
span = doc.char_span(0, 4, label=""blah"")
doc.ents = [span]

note that doc.ents is a tuple, so you can't append to it, but you can convert it to a list and set the ents, for example.",https://stackoverflow.com/questions/68499164,python,23-07-2021 12:25,254.0,1.0,1.0,True,26-07-2021 03:51,23-07-2021 14:32
71024254,jupyter kernel dies when importing pipeline function from transformers class on mac os,"i'm unable to import pipeline function of transformers class as my jupyter kernel keeps dying. tried on transformers-4.15.0 and 4.16.2. anyone faced this issue?
i tried importing the class in a new notebook as you can see in the image and it keeps killing the kernel.","['python', 'jupyter-notebook', 'jupyter-lab', 'huggingface-transformers']",71029030,"it works fine for me.

you could try creating a fresh conda environment and reinstalling the app.
you could also try using jupyterlab instead of jupyter-notebook.
are you on mac os? i couldn't get it to run at first using conda install transformers my jupyterlab kept hanging as well.
then i did this, conda install -c huggingface transformers and here is the result.

it works fine for me on linux and mac now.",https://stackoverflow.com/questions/71024254,python,07-02-2022 19:24,9248.0,5.0,2.0,True,03-12-2022 18:15,03-12-2022 18:15
71746985,is there a way to filter out retweets from results of search_tweets using tweepy?,"i'm using tweepy to query tweets with certain keywords and am able to a get a list of tweet objects (or status objects?) (
i'm wondering if there's a way to remove retweets from all the tweets since i want to look at original tweets and replies specifically. if there's an attribute in the tweet object, that'd solve my problem but didn't figure that out so far.
thanks in advance!","['python', 'twitter', 'nlp', 'tweepy']",71757223,"if you're using the standard search api with api.search_tweets, you can use the -filter:retweets operator in your query.
also, if you're using api.search_tweets, you're using twitter api v1.1, and  is the corresponding documentation for the tweet object. you can distinguish retweets by the existence of a retweeted_status attribute.",https://stackoverflow.com/questions/71746985,python,05-04-2022 06:08,1128.0,0.0,1.0,True,05-04-2022 19:13,05-04-2022 19:13
72795591,tfidf vector into lstm model,"i am trying to feed my tfidf vector into an lstm model.
tfidfvectorizer(ngram_range=(1,2), use_idf=true, analyzer='word', max_features = 5000)
here's the vector shapes
train_vector.shape = (22895, 5000)
test_vector.shape = (5724, 5000)
i am defining a model like below:
model = models.sequential()

model.add(layers.lstm(64, input_shape=(5000, 1), activation='relu'))
model.add(layers.dropout(0.2))

model.add(layers.dense(32, activation='relu'))
model.add(layers.dropout(0.2))
model.add(layers.dense(1, activation='sigmoid'))


other parameters
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(train_vector, y_train, validation_data=(test_vector, y_test), epochs=10, batch_size=1024)

tensorflow is being used here.
i am getting this error

valueerror: input 0 of layer sequential_2 is incompatible with the layer: expected ndim=3, found ndim=2. full shape received: (none, 5000)

i am trying to reshape the arrays, but the errors are still showing. i know lstm needs 3d array. so how can i shape my arrays in a way that can be fed into lstm???","['python', 'tensorflow', 'keras', 'lstm', 'tf-idf']",72796337,"to add a new dimension to your training and test data, you can try:
train_vector = train_vector[..., none] # or tf.newaxis instead of none
test_vector = test_vector[..., none] # or tf.newaxis instead of none

or
train_vector = tf.expand_dims(train_vector, axis=-1)
test_vector = tf.expand_dims(test_vector, axis=-1)

also, note that if you have one node in your output layer and you are using a sigmoid activation function, you usually combine it with the binary_crossentropy loss function instead of sparse_categorical_crossentropy, which is usually used for more than 2 classes.",https://stackoverflow.com/questions/72795591,python,29-06-2022 03:47,443.0,1.0,1.0,True,29-06-2022 05:47,29-06-2022 05:47
71581197,what is the loss function used in trainer from the transformers library of hugging face?,"what is the loss function used in trainer from the transformers library of hugging face?
i am trying to fine tune a bert model using the trainer class from the transformers library of hugging face.
in their documentation, they mention that one can specify a customized loss function by overriding the compute_loss method in the class. however, if i do not do the method override and use the trainer to fine tine a bert model directly for sentiment classification, what is the default loss function being use? is it the categorical crossentropy? thanks!","['python', 'machine-learning', 'nlp', 'artificial-intelligence', 'huggingface-transformers']",71585375,"it depends!
especially given your relatively vague setup description, it is not clear what loss will be used. but to start from the beginning, let's first check how the default compute_loss() function in the trainer class looks like.
you can find the corresponding function here, if you want to have a look for yourself (current version at time of writing is 4.17).
the actual loss that will be returned with default parameters is taken from the model's output values:

         loss = outputs[""loss""] if isinstance(outputs, dict) else outputs[0]

which means that the model itself is (by default) responsible for computing some sort of loss and returning it in outputs.
following this, we can then look into the actual model definitions for bert (source: here, and in particular check out the model that will be used in your sentiment analysis task (i assume a bertforsequenceclassification model.
the code relevant for defining a loss function looks like this:
if labels is not none:
    if self.config.problem_type is none:
        if self.num_labels == 1:
            self.config.problem_type = ""regression""
        elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
            self.config.problem_type = ""single_label_classification""
        else:
            self.config.problem_type = ""multi_label_classification""

    if self.config.problem_type == ""regression"":
        loss_fct = mseloss()
        if self.num_labels == 1:
            loss = loss_fct(logits.squeeze(), labels.squeeze())
        else:
            loss = loss_fct(logits, labels)
    elif self.config.problem_type == ""single_label_classification"":
        loss_fct = crossentropyloss()
        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
    elif self.config.problem_type == ""multi_label_classification"":
        loss_fct = bcewithlogitsloss()
        loss = loss_fct(logits, labels)


based on this information, you should be able to either set the correct loss function yourself (by changing model.config.problem_type accordingly), or otherwise at least be able to determine whichever loss will be chosen, based on the hyperparameters of your task (number of labels, label scores, etc.)",https://stackoverflow.com/questions/71581197,python,23-03-2022 02:35,28405.0,16.0,1.0,True,28-01-2025 18:54,28-01-2025 18:54
75533159,can stop phrases be removed while doing text processing in python?,"on the task that i'm working on, involves finding the cosine similarity using tfidf between a base transcript and other sample transcripts.
i am removing stop words for this. but i would also like to remove certain stop phrases that are unique to the sample transcripts.
for example - i would like to retain words like 'sounds' , 'like'. but want to remove the phrase 'sounds like' when it occurs together.
i am using sklearn tfidfvectorizer package currently. is there an efficient way to do the above?","['python', 'nlp', 'cosine-similarity', 'stop-words', 'tfidfvectorizer']",75538580,"yes, you can achieve this by defining function custom_preprocessor that removes the stop phrases and passing it to the tfidfvectorizer constructor using the preprocessor argument.
def custom_preprocessor(text):
    for stop_phrase in stop_phrases:
        text = text.replace(stop_phrase, '')
    return text
vectorizer = tfidfvectorizer(preprocessor=custom_preprocessor)",https://stackoverflow.com/questions/75533159,python,22-02-2023 13:10,61.0,1.0,1.0,True,22-02-2023 22:01,22-02-2023 13:12
58454157,pytorch bert typeerror: forward() got an unexpected keyword argument &#39;labels&#39;,"training a bert model using pytorch transformers (following the tutorial here).
following statement in the tutorial
loss = model(b_input_ids, token_type_ids=none, attention_mask=b_input_mask, labels=b_labels)

leads to
typeerror: forward() got an unexpected keyword argument 'labels'

here is the full error,
typeerror                                 traceback (most recent call last)
<ipython-input-53-56aa2f57dcaf> in <module>
     26         optimizer.zero_grad()
     27         # forward pass
---> 28         loss = model(b_input_ids, token_type_ids=none, attention_mask=b_input_mask, labels=b_labels)
     29         train_loss_set.append(loss.item())
     30         # backward pass

~/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--> 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

typeerror: forward() got an unexpected keyword argument 'labels'

i cant seem to figure out what kind of argument the forward() function expects.
there is a similar problem here, but i still do not get what the solution is.
system information:

os: ubuntu 16.04 lts
python version: 3.6.x
torch version: 1.3.0
torch vision version: 0.4.1
pytorch transformers version: 1.2.0","['python', 'pytorch', 'bert-language-model', 'huggingface-transformers']",58456379,"as far as i know, the bertmodel does not take labels in the forward() function. check out the forward function parameters.
i suspect you are trying to fine-tune the bertmodel for sequence classification task and the api provides a class for that which is bertforsequenceclassification. as you can see its forward() function definition:
def forward(self, input_ids, attention_mask=none, token_type_ids=none,
            position_ids=none, head_mask=none, labels=none):

please note, the forward() method returns the followings.
outputs: `tuple` comprising various elements depending on the configuration (config) and inputs:
        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.floattensor`` of shape ``(1,)``:
            classification (or regression if config.num_labels==1) loss.
        **logits**: ``torch.floattensor`` of shape ``(batch_size, config.num_labels)``
            classification (or regression if config.num_labels==1) scores (before softmax).
        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=true``)
            list of ``torch.floattensor`` (one for the output of each layer + the output of the embeddings)
            of shape ``(batch_size, sequence_length, hidden_size)``:
            hidden-states of the model at the output of each layer plus the initial embedding outputs.
        **attentions**: (`optional`, returned when ``config.output_attentions=true``)
            list of ``torch.floattensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:
            attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads. 

hope this helps!",https://stackoverflow.com/questions/58454157,python,18-10-2019 15:42,72026.0,26.0,1.0,True,14-01-2022 19:32,16-01-2020 10:23
72097848,how to load customized ner model from disk with spacy?,"i have customized ner pipeline with following procedure
doc = nlp(""i am going to vallila. i am going to sï¿½ï¿½rnï¿½ï¿½inen."")
for ent in doc.ents:
    print(ent.text, ent.label_)

label = 'district'
train_data = [
    (
    'we need to deliver it to vallila', {
        'entities': [(25, 32, 'district')]
    }),
    (
    'we need to deliver it to somewhere', {
        'entities': []
    }),
]

ner = nlp.get_pipe(""ner"")
ner.add_label(label)

nlp.disable_pipes(""tagger"")
nlp.disable_pipes(""parser"")
nlp.disable_pipes(""attribute_ruler"")
nlp.disable_pipes(""lemmatizer"")
nlp.disable_pipes(""tok2vec"")

optimizer = nlp.get_pipe(""ner"").create_optimizer()
import random
from spacy.training import example

for i in range(25):
    random.shuffle(train_data)
    for text, annotation in train_data:
        example = example.from_dict(nlp.make_doc(text), annotation)
        nlp.update([example], sgd=optimizer)

i tsave that customized ner to disk and load it again with following code
ner.to_disk('/home/feru/ner')

import spacy
from spacy.pipeline import entityrecognizer
nlp = spacy.load(""en_core_web_lg"", disable=['ner'])

ner = entityrecognizer(nlp.vocab)
ner.from_disk('/home/feru/ner')
nlp.add_pipe(ner)

i got however following error:

---> 10 ner = entityrecognizer(nlp.vocab)
11 ner.from_disk('/home/feru/ner')
12 nlp.add_pipe(ner)
~/.local/lib/python3.8/site-packages/spacy/pipeline/ner.pyx in
spacy.pipeline.ner.entityrecognizer.init()
typeerror: init() takes at least 2 positional arguments (1 given)

this method to save and load custom component from disk seems to be from some erly spacy version. what's the second argument entityrecognizer needs?","['spacy', 'spacy-3']",72100706,"the general process you are following of serializing a single component and reloading it is not the recommended way to do this in spacy. you can do it - it has to be done internally, of course - but you generally want to save and load pipelines using high-level wrappers. in this case this means that you would save like this:
nlp.to_disk(""my_model"") # not ner.to_disk

and then load it with spacy.load(""my_model"").
you can find more detail about this in the saving and loading docs. since it seems you're just getting started with spacy, you might want to go through the course too. it covers the new config-based training in v3, which is much easier than using your own custom training loop like in your code sample.
if you want to mix and match components from different pipelines, you still will generally want to save entire pipelines, and you can then combine components from them using the ""sourcing"" feature.",https://stackoverflow.com/questions/72097848,spacy,03-05-2022 10:07,2455.0,2.0,1.0,True,03-05-2022 14:09,03-05-2022 11:24
42711749,"replace the words &quot;can&#39;t, don&#39;t&quot; by &quot;can not, do not&quot; using python","i need to replace words like ""{can't, don't, won't }""  by  ""{can not, do not, would not}"" using python
the problem is:
""can't"" can be detected by checking suffix ""n't"", so we can replace ""n't"" by ""not""
but how can we transform ""ca"" to ""can"" as when we split ""can't"" it should be transformed to ""can not""?","['regex', 'python-2.7', 'nlp', 'nltk']",42711824,"since the rules of english are large and sometimes inconsistent, your best bet is probably just to set up full word maps rather than trying to figure out on the fly which letters are represented by the apostrophe.
in other words, a dictionary with values like:
can't    -> can not
don't    -> do not
won't    -> will not
:
oughtn't -> ought not",https://stackoverflow.com/questions/42711749,regex,10-03-2017 06:29,1528.0,0.0,1.0,True,17-10-2021 08:28,17-10-2021 08:28
72338808,how to calculate per document probabilities under respective topics with bertopics?,"i am trying to use bertopic to analyze the topic distribution of documents, after bertopic is performed, i would like to calculate the probabilities under respective topics per document, how should i did it?
# define model
model = bertopic(verbose=true,
                 vectorizer_model=vectorizer_model,
                 embedding_model='paraphrase-minilm-l3-v2',
                 min_topic_size= 50,
                 nr_topics=10)

#  train model
headline_topics, _ = model.fit_transform(df1.review_processed3)

# examine one of the topic
a_topic = freq.iloc[0][""topic""] # select the 1st topic
model.get_topic(a_topic) # show the words and their c-tf-idf scores

below is the words and their c-tf-idf scores for one of the topics
image 1
how should i change the result into topic distribution as below in order to calculate the topic distribution score and also identify the main topic?
image 2","['python', 'nlp', 'bert-language-model', 'topic-modeling']",72347576,"first, to compute probabilities, you have to add to your model definition calculate_probabilities=true (this could slow down the extraction of topics if you have many documents, > 100000).
# define model
model = bertopic(verbose=true,
                 vectorizer_model=vectorizer_model,
                 embedding_model='paraphrase-minilm-l3-v2',
                 min_topic_size= 50,
                 nr_topics=10,
                 calculate_probabilities=true)

then, calling fit_transform, you should save the probabilities:
headline_topics, probs = model.fit_transform(df1.review_processed3)

now, you can create a pandas dataframe which shows probabilities under respective topics per document.
import pandas as pd
probs_df=pd.dataframe(probs)
probs_df['main percentage'] = pd.dataframe({'max': probs_df.max(axis=1)})",https://stackoverflow.com/questions/72338808,python,22-05-2022 15:11,2838.0,5.0,1.0,True,23-05-2022 14:48,23-05-2022 14:48
66762169,"for integer/dates values annotated using prodigy, does the spacy model learn the range of values as well?","i have a prodigy session set up to annotate certain numeric values in a document for age (ranges from 0 to 100). i am only annotating the number. my question is, suppose there is a corrupt value which crept in (age being 1000 or 22.7), will the model understand that even though it is close to the age text in the document, it should not be picked up?
in other words, can it learn the range of integer values, and if it does, will that work for date format as well?
for instance a date in the format dd/mm/yyyy which is dob (all the annotated ones are < 01/01/2000) and there is a date 31/12/2020, will that get picked up as well since all the annotated dates are nowhere close to this range?
thank you","['nlp', 'spacy', 'named-entity-recognition', 'prodigy']",66792779,"good question! spacy does not internally represent numeric tokens as numbers, so it doesn't have an explicit concept of the values. in that sense it can't tell between valid and invalid values for age.
however, spacy does use ""shape"" features when representing tokens that will help it recognize valid ages. there are different kinds of shape tokens, but the one spacy uses will represent words by converting characters to a representation of the character type. it works like this:

spacy ï¿½ï¿½ï¿½ xxxxx
fish ï¿½ï¿½ï¿½ xxxx
fish ï¿½ï¿½ï¿½ xxxx
23 ï¿½ï¿½ï¿½ dd
1000 ï¿½ï¿½ï¿½ dddd
22.7 ï¿½ï¿½ï¿½ dd.d

because of this you could expect that spacy learns that two-digit numbers are likely to be ages, but numbers with decimals or four digits aren't likely. on the other hand, this doesn't help it differentiate between 100 and 999.
for dates this will not help with determining valid or invalid birthdates. shape is just one of  like prefix and suffix aren't really going to help with this either.
since it's easy to verify numeric values in code, what i would suggest is matching broadly in spacy and then using your own function to check whether dates or ages are valid by parsing them.

outside of spacy in particular, the question of how nlp models represent numeric values is actually an increasingly popular research topic - if you'd like to know more about it this is a recent article on the topic: do language models know how heavy an elephant is?",https://stackoverflow.com/questions/66762169,nlp,23-03-2021 11:23,168.0,1.0,1.0,True,14-04-2022 18:27,14-04-2022 18:27
27860652,what is the concept of negative-sampling in word2vec?,"i'm reading the 2014 paper word2vec explained: deriving mikolov et al.ï¿½ï¿½ï¿½s
negative-sampling word-embedding method (note: direct download link) and it references the concept of ""negative-sampling"":

mikolov et al. present the negative-sampling approach as a more efficient
way of deriving word embeddings. while negative-sampling is based on the
skip-gram model, it is in fact optimizing a different objective.

i have some issue understanding the concept of negative-sampling.

can anyone explain in layman's terms what negative-sampling is?","['machine-learning', 'nlp', 'word2vec']",27864657,"the idea of word2vec is to maximise the similarity (dot product) between the vectors for words which appear close together (in the context of each other) in text, and minimise the similarity of words that do not. in equation (3) of the paper you link to, ignore the exponentiation for a moment. you have
      v_c . v_w
 -------------------
   sum_i(v_ci . v_w)

the numerator is basically the similarity between words c (the context) and w (the target) word. the denominator computes the similarity of all other contexts ci and the target word w. maximising this ratio ensures words that appear closer together in text have more similar vectors than words that do not. however, computing this can be very slow, because there are many contexts ci. negative sampling is one of the ways of addressing this problem- just select a couple of contexts ci at random. the end result is that if cat appears in the context of food, then the vector of food is more similar to the vector of cat (as measures by their dot product) than the vectors of several other randomly chosen words (e.g. democracy, greed, freddy), instead of all other words in language. this makes word2vec much much faster to train.",https://stackoverflow.com/questions/27860652,machine-learning,09-01-2015 12:31,63771.0,124.0,3.0,True,06-03-2025 14:52,06-03-2025 14:52
79293889,catelog sentences into 5 words that represent them,"i have dataframe with 1000 text rows. df['text']
i also have 5 words that i want to know for each one of them how much they represnt the text  (between 0 to 1)
every score will be in df[""word1""] ,df[""word2""] and etc
i will glad for recomendations how to do that
edit
represnt = the semantic distance between the word to the text.
for example -
lets say in row 1 the text is ""i want to eat""
and i have 2 words : food and house.
so in df[""food ""] it would be higher score than in df[""house""]","['python', 'pandas', 'nlp', 'text-mining', 'similarity']",79294099,"you could use a pre-trained sentence transformer model from sentence_transformers:
import pandas as pd
from sentence_transformers import sentencetransformer, util


class semanticsimilaritycalculator:
  def __init__(self, model_name: str = 'all-minilm-l6-v2') -> none:
    self.model = sentencetransformer(model_name)
    self.word_embeddings = none

  def encode_words(self, words: list[str]) -> none:
    self.word_embeddings = self.model.encode(words, convert_to_tensor=true)
    self.words = words

  def calculate_similarity(self, text: str) -> list[float]:
    if self.word_embeddings is none:
      raise valueerror('words must be encoded before calculating similarity.')
    text_embedding = self.model.encode(text, convert_to_tensor=true)
    similarities = util.cos_sim(text_embedding, self.word_embeddings)[
      0
    ].tolist()
    return similarities

  def add_similarity_scores_to_df(
    self, df: pd.dataframe, text_column: str
  ) -> pd.dataframe:
    if self.words is none:
      raise valueerror(
        'words must be encoded before adding scores to the dataframe.'
      )
    similarity_columns = ['word_' + word for word in self.words]
    df[similarity_columns] = df[text_column].apply(
      lambda text: pd.series(self.calculate_similarity(text))
    )
    return df


def main():
  data = {'text': ['i want to eat', 'the house is big', 'i need to sleep']}
  df = pd.dataframe(data)
  words = ['food', 'house', 'sleep', 'drink', 'run']
  calculator = semanticsimilaritycalculator()
  calculator.encode_words(words)
  df_with_scores = calculator.add_similarity_scores_to_df(
    df, text_column='text'
  )
  print(df_with_scores)


if __name__ == '__main__':
  main()

output:
               text  word_food  word_house  word_sleep  word_drink  word_run
0     i want to eat   0.592410    0.215032    0.254065    0.370329  0.259350
1  the house is big   0.243262    0.672110    0.170785    0.213780  0.119716
2   i need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838",https://stackoverflow.com/questions/79293889,python,19-12-2024 10:16,59.0,0.0,1.0,True,19-12-2024 13:52,19-12-2024 13:52
70492407,bilou tagging scheme for multi-word entities in spacy&#39;s ner,"i am working on building a custom ner using spacy for recognizing new entities apart from spacy's ner. now i have my training data to be tagged and added using spacy.example. i am using the bilou scheme. my doubt is that i have entities which have more than 3 words. for example:
housing development finance corporation reported heavy losses in the past quarter.

i want to tag housing development finance corporation as a single entity using the bilou scheme. something like
'housing'     b-entity
'development' i-entity
'finance'     i-entity
'corporation' l-entity

is this tagging correct?how will the model interpret the order within each entity?any guidance would be much appreciated.","['python', 'nlp', 'named-entity-recognition', 'spacy-3']",70492505,"the tagging you have is correct while all outside words which are not entities would be marked with o.
the model will be depending on the same order within the entity to match it towards a previous entity of the same name, ex:
'housing'     b-entity
'development' i-entity
'finance'     i-entity
'corporation' l-entity

and
'housing'     b-entity
'finance'     i-entity
'development' i-entity
'corporation' l-entity

will not be linked as the same entity, although if you want this to be the case, you could look into a classification model to classify your foud entities towards your previously known entities and work from there.",https://stackoverflow.com/questions/70492407,python,27-12-2021 06:45,474.0,2.0,1.0,True,27-12-2021 07:34,27-12-2021 07:34
77111087,openai api error: &quot;the model &#39;curie:ft-personal-2023-09-15-06-05-01&#39; does not exist&quot; when trying to delete a fine-tuned model,"created a model from a jsonl file with prompts. i enter the command:
openai -k sk-blablablablablabla api fine_tunes.list

it shows information about this model:
{
  ""object"": ""list"",
  ""data"": [
    {
      ""object"": ""fine-tune"",
      ""id"": ""ft-yrlmcjagdwijfrnvkldsn"",
      ""hyperparams"": {
        ""n_epochs"": 4,
        ""batch_size"": 1,
        ""prompt_loss_weight"": 0.01,
        ""learning_rate_multiplier"": 0.1
      },
      ""organization_id"": ""org-kouf8eihlsamclsl"",
      ""model"": ""curie"",
      ""training_files"": [
        {
          ""object"": ""file"",
          ""id"": ""file-iyrkyuiefjwkq"",
          ""purpose"": ""fine-tune"",
          ""filename"": ""data_prepared.jsonl"",
          ""bytes"": 417,
          ""created_at"": 1694757803,
          ""status"": ""processed"",
          ""status_details"": null
        }
      ],
      ""validation_files"": [],
      ""result_files"": [
        {
          ""object"": ""file"",
          ""id"": ""file-jofepw8489hfidkdwfiw"",
          ""purpose"": ""fine-tune-results"",
          ""filename"": ""compiled_results.csv"",
          ""bytes"": 742,
          ""created_at"": 1694757902,
          ""status"": ""processed"",
          ""status_details"": null
        }
      ],
      ""created_at"": 1694757804,
      ""updated_at"": 1694757903,
      ""status"": ""succeeded"",
      ""fine_tuned_model"": ""curie:ft-personal-2023-09-15-06-05-01""
    }
  ],
  ""next_starting_after"": null
}

but i can't remove it.
i'm trying different things:
openai -k sk-blablablablablabla api models.delete -i <fine_tuned_model>
openai -k sk-blablablablablabla api models.delete -i curie:ft-personal-2023-09-15-06-05-01
openai -k sk-blablablablablabla api models.delete -i ft-wyysqbyrfko2p0eznxa6f5sc

tried it through python code too:
import os
import openai
from dotenv import load_dotenv, find_dotenv

load_dotenv(find_dotenv())

def init_api():
    openai.api_key = os.getenv(""ai_api_key"")

init_api()

openai.model.delete(""curie:ft-personal-2023-09-15-06-05-01"")

i got the following error:
the model 'curie:ft-personal-2023-09-15-06-05-01' does not exist (http status code: 404)","['python', 'openai-api', 'chatgpt-api']",77111780,"you said that you got the following error: the model 'curie:ft-personal-2023-09-15-06-05-01' does not exist.
meaning you have probably already successfully deleted your fine-tuned model. consequently, it doesn't exist anymore when you try to delete it for the second, third, fourth, etc. time.
use the models api to list all models. you shouldn't see curie:ft-personal-2023-09-15-06-05-01.
python:
import os
import openai

openai.api_key = os.getenv(""openai_api_key"")

openai.model.list()

nodejs:
import openai from ""openai"";

const openai = new openai();

async function main() {
  const list = await openai.models.list();

  for await (const model of list) {
    console.log(model);
  }
}
main();",https://stackoverflow.com/questions/77111087,python,15-09-2023 09:09,203.0,1.0,1.0,True,18-09-2023 08:17,18-09-2023 08:13
43894416,wordcloud showing colour based on continuous metadata in r,"i'm creating a wordcloud in which the size of the words is based on frequency, but i want the colour of the words to be mapped to a third variable (stress, which is the amount of stress associated with each word, a numerical or continuous variable). 
i tried the following, which gave me only two different colours (yellow and purple) while i want something more smooth. i would like some color range like a palette that goes from green to red for example.
df = data.frame(word = c(""calling"", ""meeting"", ""conference"", ""contract"", ""negotiation"", ""email""),
n = c(20, 12, 4, 8, 10, 43),
stress = c(23, 30, 15, 40, 35, 15))
df = tbl_df(df) 
wordcloud(words = df$word, freq = df$n, col = df$stress)

does anyone know how to deal with this continous metadata and get some smoothly changing colour for the words when stress goes up? thanks!","['r', 'colors', 'text-mining', 'word-cloud', 'color-palette']",43938528,"here is a potential solution. you want to use the wordcloud2 package for your task. then, you can solve your issue, i suppose. since i do not know your real data, i created a sample data to demonstrate a prototype.
if you have many words, i am not sure if adding colors with a continuous variable (stress) is a good idea. one thing you could do is to create a new group variable using cut(). in this way, you can reduce the numbers of colors you would use in your graphics. here, i created a new column called color with five colors from the viridis package.
when you use wordcloud2(), you have only two things to supply. one is data and the other is color. font size reflects frequency of the words without specifying it.
mydf = data.frame(word = c(""calling"", ""meeting"", ""conference"", ""contract"", ""negotiation"",
                           ""email"", ""friends"", ""chat"", ""text"", ""deal"",
                           ""business"", ""promotion"", ""discount"", ""users"", ""family""),
                  n = c(20, 12, 4, 8, 10, 43, 33, 5, 47, 28, 12, 9, 50, 31, 22),
                  stress = c(23, 30, 15, 40, 35, 15, 30, 18, 10, 5, 29, 38, 45, 8, 3))


          word  n stress
1      calling 20     23
2      meeting 12     30
3   conference  4     15
4     contract  8     40
5  negotiation 10     35
6        email 43     15
7      friends 33     30
8         chat  5     18
9         text 47     10
10        deal 28      5
11    business 12     29
12   promotion  9     38
13    discount 50     45
14       users 31      8
15      family 22      3

library(dplyr)
library(wordcloud2)
library(viridis)

mutate(mydf, color = cut(stress, breaks = c(0, 10, 20, 30, 40, inf),
             labels = c(""#fde725ff"", ""#73d055ff"", ""#1f968bff"",
                        ""#2d708eff"", ""#481567ff""),
             include.lowest = true)) -> temp

wordcloud2(data = temp, color = temp$color)",https://stackoverflow.com/questions/43894416,r,10-05-2017 13:48,3221.0,3.0,2.0,True,11-06-2022 09:59,11-06-2022 09:59
79401652,checkpoints valueerror with downloading huggingface models,"i am having trouble downloading deepseek_vl_v2 into my computer.
here is the error in my terminal

valueerror: the checkpoint you are trying to load has model type
deepseek_vl_v2 but transformers does not recognize this
architecture. this could be because of an issue with the checkpoint,
or because your version of transformers is out of date.
you can update transformers with the command pip install --upgrade transformers. if this does not work, and the checkpoint is very new,
then there may not be a release version that supports this model yet.
in this case, you can get the most up-to-date code by installing
transformers from source with the command pip install git+

however, i have downloaded many huggingface models before and never seemed to have had this issue, which means i should have all the libraries all correctly downloaded. i have already tried updating the transformers library and installing from the direct source, but neither have resolved the issue.
please let me know if there any more information i can provide.
thanks so much in advance!","['python', 'terminal', 'model', 'huggingface-transformers', 'huggingface']",79419644,"following up on this question i asked, i found the solution here
basically, deepseek is not a model supported by huggingface's transformer library, so the only option for downloading this model is through importing the model source code directly as of now.",https://stackoverflow.com/questions/79401652,python,31-01-2025 02:34,471.0,0.0,1.0,True,06-02-2025 23:47,31-01-2025 02:41
78985137,alternative to device_map = &quot;auto&quot; in huggingface pretrained,"i have a model that i was reading from huggingface using the following code:
from transformers import autotokenizer, automodelforcausallm

tokenizer = autotokenizer.from_pretrained(model_path)
model = automodelforcausallm.from_pretrained(model_path, device_map=""auto"", trust_remote_code=true)

now i read the model and i did some modifications to the internal layers and added more layers. when i started the training/fine-tuning i get that not everything is on the same model.
now after more investigations, i found that my custom layers aren't distributed on multi gpus as the original model. so i need something like device_map=""auto"" but after reading the model.
so simply something like
tokenizer = autotokenizer.from_pretrained(model_path)
model = automodelforcausallm.from_pretrained(model_path, device_map=""auto"", trust_remote_code=true)

model.device_map = ""auto""","['machine-learning', 'deep-learning', 'nlp', 'huggingface-transformers']",79007343,"i found out that there are actually several methods in accelerate for this. the first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model:

the second one is used to match your model with the devices:

so basically, in your case, you can use the following code:
from accelerate import dispatch_model, infer_auto_device_map

model = automodelforcausallm.from_pretrained(model_path, device_map=""auto"", trust_remote_code=true)

***
...
new_model = custommodel(model)
...
***

device_map_dict = infer_auto_device_map(new_model)
dispatch_model(new_model, device_map_dict)

p.s. this code still needs to be tested on fine-tuning.",https://stackoverflow.com/questions/78985137,machine-learning,14-09-2024 12:42,1088.0,2.0,1.0,True,20-09-2024 15:16,14-09-2024 14:28
77107785,fastapi custom validator error: fastapi/pydantic not recognizing custom validator functions (runtimeerror: no validator found for &lt;class&gt;),"i'm working on a fastapi project on an amazon ec2 instance running ubuntu 20.04.5. the nature of the project requires me to have several custom types (written by me) and third party types (from huggingface transformers, hf peft, langchain) as fields in my model schema. when i try to run my fastapi application, though, i consistently get this error for each custom/third party type i use as a field:
runtimeerror: no validator found for <class 'peft.peft_model.peftmodelforcausallm'>, see `arbitrary_types_allowed` in config

i've done everything i can find online/in docs to fix this error. i'll show some snippets of my code that should show what i've tried so far.
my custom model schema look like this
class saicmodelfortextgen(basemodel):
    """"""
    loads a model for text generation. will require some time to load checkpoint shards 
    once called. 
    """"""

    model_config = configdict(arbitrary_types_allowed=true)

    prompt: str = base_prompt_textgen
    
    model, tokenizer = load_text_generation()

    conversation: conversation = conversation()

    @validator('model', check_fields=false)
    def validate_model(cls, value):
        return validate(value)

the field 'model' is of type peftmodelforcausallm (shown in the error). the function validate(value) redirects to a simple validation function that immediately returns the object when called.
in the api itself, a get request that uses this class would look like this:
@app.get(""/"", response_model=none)
def init_text_gen(base_prompt: str = base_prompt_textgen) -> saicmodelfortextgen:
    return saicmodelfortextgen(prompt=base_prompt)

setting response_model=none was a suggestion i saw on docs/internet that would provide a workaround for my error, but so far hasn't changed anything.
i've also experimented with older/newer versions of fastapi and pydantic without success either. i haven't been able to find anyone online who has experienced the same issue as me after implementing custom validator functions and setting response_model=none.
does anyone know of a workaround/solution to this issue? thanks in advance for your help :) - i'll list the versions/libraries i'm working with below
i'm working with llms on a gpu, so there's a lot of other libraries involved in the project, but i mentioned the ones that i thought would be helpful since they seem to be directly involved in this issue. thanks again!
edit: forgot to include the actual libraries. worried this is a dependency issues somehow.
fastapi 0.95.0
unicorn 0.23.2
pydantic 1.10.12
python 3.11
transformers 4.31.0
peft 0.4.0","['fastapi', 'huggingface-transformers', 'pydantic', 'peft']",77143339,"i eventually reached a solution to this error. i was working with custom types i trusted, so i bypassed validation entirely with the basemodel.construct() method. for other with a similar issue, you should only do this if you can already trust the data that's going into your custom model, because it's definitely a dangerous method (although safe in my case as i mentioned).",https://stackoverflow.com/questions/77107785,fastapi,14-09-2023 19:11,1387.0,0.0,2.0,True,20-09-2023 14:15,18-09-2023 14:14
67969998,stanford corenlp output is very slow in python,"i'm using nltk's stanforddependencyparser to generate dependency trees. here is the code
cpath = ""path to stanford-corenlp-4.2.0-models-english.jar"" + os.pathsep + ""path to stanford-parser.jar""

if cpath not in os.environ['classpath']:
     os.environ['classpath'] = cpath + os.pathsep + os.environ['classpath']

# todo: deprecated
# self.dependency_parser_instance_corenlp = stanforddependencyparser(path_to_models_jar=""path to stanford-corenlp-4.2.0-models-english.jar"", encoding='utf8')

dependencies = [list(parse.triples()) for parse in self.dependency_parser_instance_corenlp.raw_parse(query)]

# encode every string in tree to utf8 so string matching will work
for dependency in dependencies[0]:
    dependency[0][0].encode('utf-8')
    dependency[0][1].encode('utf-8')
    dependency[1].encode('utf-8')
    dependency[2][0].encode('utf-8')
    dependency[2][1].encode('utf-8')

for a sentence with 10 words, it takes around 1.5 seconds to generate the output. is this expected? can you please steps to improve speed?
i've already tried using sr parser and removing all the extra folders(like coref, lexparser, ner, tagger) from the models jar.","['python', 'nltk', 'stanford-nlp']",67977791,"yes, the (old nltk implementation) stanforddependencyparser is very slow. you should not use it. you should use the classes and methods in the nltk.parse.corenlp module, which are much faster.
for more info, see the notes in the corenlp documentation or this tutorial.",https://stackoverflow.com/questions/67969998,python,14-06-2021 12:12,337.0,4.0,1.0,True,07-07-2021 09:43,07-07-2021 09:43
70048302,difference between tokenization and segmentation,"what is the difference between tokenization and segmentation in nlp. i searched about them but i didn't really find any differences
.","['machine-learning', 'nlp', 'artificial-intelligence', 'terminology', 'text-segmentation']",70049058,"short answer: all tokenization is segmentation, but not all segmentation is tokenization. 
long answer: 
while segmentation is a more generic concept of splitting the input text, tokenization is a type of segmentation and it is carried out based on a well defined criteria.
for example - in a hypothetical scenario if all your input sentences are compound sentences of two sub-sentences, then splitting them into two independent sentences can be termed as segmentation (but not tokenization). tokenization is a form of segmentation which is performed on the basis of a semantic criteria or using a token dictionary - e.g. a word or sub-word tokenization, mainly with an intention of assigning them token ids for downstream processing.",https://stackoverflow.com/questions/70048302,machine-learning,20-11-2021 17:32,2509.0,9.0,1.0,True,25-12-2022 09:35,25-12-2022 09:35
71398882,cuda: runtimeerror: cuda out of memory - bert sagemaker,"i have been trying to train a bertsequenceforclassification model using aws sagemaker. i'm using hugging face estimators. but i keep getting the error: runtimeerror: cuda out of memory. tried to allocate 192.00 mib (gpu 0; 11.17 gib total capacity; 10.73 gib already allocated; 87.88 mib free; 10.77 gib reserved in total by pytorch) the same code runs fine on my laptop.

how do i check what is occupying that 10gb of memory? my dataset is pretty small (68kb), so is my batch size (8) and epochs (1). when i run nvidia-smi, i can only see ""no processes running"" and the gpu memory usage is zero. when i run print(torch.cuda.memory_summary(device=none, abbreviated=false)) from within my training script (right before it throws the error) it prints

|===========================================================================|
|                  pytorch cuda memory summary, device id 0                 |
|---------------------------------------------------------------------------|
|            cuda ooms: 0            |        cudamalloc retries: 0         |
|===========================================================================|
|        metric         | cur usage  | peak usage | tot alloc  | tot freed  |
|---------------------------------------------------------------------------|
| allocated memory      |       0 b  |       0 b  |       0 b  |       0 b  |
|       from large pool |       0 b  |       0 b  |       0 b  |       0 b  |
|       from small pool |       0 b  |       0 b  |       0 b  |       0 b  |
|---------------------------------------------------------------------------|
| active memory         |       0 b  |       0 b  |       0 b  |       0 b  |
|       from large pool |       0 b  |       0 b  |       0 b  |       0 b  |
|       from small pool |       0 b  |       0 b  |       0 b  |       0 b  |
|---------------------------------------------------------------------------|
| gpu reserved memory   |       0 b  |       0 b  |       0 b  |       0 b  |
|       from large pool |       0 b  |       0 b  |       0 b  |       0 b  |
|       from small pool |       0 b  |       0 b  |       0 b  |       0 b  |
|---------------------------------------------------------------------------|
| non-releasable memory |       0 b  |       0 b  |       0 b  |       0 b  |
|       from large pool |       0 b  |       0 b  |       0 b  |       0 b  |
|       from small pool |       0 b  |       0 b  |       0 b  |       0 b  |
|---------------------------------------------------------------------------|
| allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| gpu reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

but i have no idea what it means or how to interpret it

when i run !df -h i can see:

filesystem      size  used avail use% mounted on
devtmpfs         30g   72k   30g   1% /dev
tmpfs            30g     0   30g   0% /dev/shm
/dev/xvda1      109g   93g   16g  86% /
/dev/xvdf       196g   61m  186g   1% /home/ec2-user/sagemaker

how is this memory different from the gpu? if theres 200gb in /dev/xvdf is there anyway i can just use that..? in my test script i tried
model = bertforsequenceclassification.from_pretrained(args.model_name,num_labels=args.num_labels).to(""cpu"")
but that just gives the same error","['python', 'gpu', 'amazon-sagemaker', 'huggingface-transformers']",71399097,"a cuda out of memory error indicates that your gpu ram (random access memory) is full. this is different from the storage on your device (which is the info you get following the df -h command).
this memory is occupied by the model that you load into gpu memory, which is independent of  your dataset size. the gpu memory required by the model is at least twice the actual size of the model, but most likely closer to 4 times (initial weights, checkpoint, gradients, optimizer states, etc).
things you can try:

provision an instance with more gpu memory
decrease batch size
use a different (smaller) model",https://stackoverflow.com/questions/71398882,python,08-03-2022 17:06,8945.0,2.0,1.0,True,08-03-2022 19:29,08-03-2022 19:29
76173414,openai gpt-3 api: what is the difference between davinci and text-davinci-003?,"i'm testing the different models for openai, and i noticed that not all of them are developed or trained enough to give a reliable response.
the models i tested are the following:
model_engine = ""text-davinci-003""
model_engine = ""davinci"" 
model_engine = ""curie"" 
model_engine = ""babbage"" 
model_engine = ""ada"" 

i need to understand what the difference is between davinci and text-davinci-003, and how to improve the responses to match that response when you use chatgpt.","['openai-api', 'gpt-3']",76173794,"tl;dr

text-davinci-003 is the newer and more capable model than davinci
text-davinci-003 supports a longer context window than davinci
text-davinci-003 was trained on a more recent dataset than davinci
text-davinci-003 is cheaper than davinci
text-davinci-003 is not available for fine-tuning, while davinci is

capabilities
as stated in the official openai article:

while both davinci and text-davinci-003 are powerful models, they
differ in a few key ways.
text-davinci-003 is the newer and more capable model, designed
specifically for instruction-following tasks. this enables it to
respond concisely and more accurately - even in zero-shot scenarios,
i.e. without the need for any examples given in the prompt. davinci,
on the other hand, can be fine-tuned on a specific task, which can
make it very effective if you have access to at least a few hundred
training examples.
additionally, text-davinci-003 supports a longer context window (max
prompt+completion length) than davinci - 4097 tokens compared to
davinci's 2049.
finally, text-davinci-003 was trained on a more recent dataset,
containing data up to june 2021. these updates, along with its support
for inserting text, make text-davinci-003 a particularly versatile and
powerful model we recommend for most use-cases.

use text-davinci-003 because the other models you mentioned in your question are less capable.
if you buy a chatgpt plus subscription, you can also use gpt-3.5-turbo or gpt-4. so, to get similar responses as you get from chatgpt, it depends on whether you are subscribed or not. for sure, gpt-3.5-turbo and gpt-4 are even more capable than text-davinci-003.
costs
text-davinci-003 is cheaper than davinci, as stated on the official openai website:




model
usage




davinci
$0.1200 / 1k tokens


text-davinci-003
$0.0200 / 1k tokens



fine-tuning availability
text-davinci-003 is not available for fine-tuning, while davinci is, as stated in the official openai documentation:

fine-tuning is currently only available for the following base models:
davinci, curie, babbage, and ada. these are the original models that
do not have any instruction following training (like text-davinci-003
does for example).





model
fine-tuning availability
training




davinci
yes
$0.0300 / 1k tokens


text-davinci-003
no",https://stackoverflow.com/questions/76173414,openai-api,04-05-2023 12:51,20642.0,7.0,2.0,True,11-07-2023 08:34,05-05-2023 21:16
68193584,how encode text can be converted to main text (without special character created by encoding),"i am going to extract text from a series of pdf files to do topic modeling. after extracting text from pdf files, i am going to save the text of each pdf file in a .txt file or .doc file. to do this, i had an error that i should add .encode('utf-8') for saving extracted text in a .txt file. so, i added txt = str(txt.encode('utf-8')). the problem is reading the .txt files, when i read the .txt files, they have special characters due to utf-8, i don't know how i can have the main text without that characters. i applied to decode but it didn't work.
i applied another approach to avoid saving in .txt format, i was going to save the extracted text in a data frame, but i found that the few first pages were saved in data frame!
i would appreciate it if you could share your solutions to read from the .txt file and removing characters relating to encoding ('utf-8') and how i can save the extracted text in a data frame.
import pdfplumber
import pandas as pd
import  codecs

txt = ''

with pdfplumber.open(r'c:\users\thmag\3rdpaperlda\a1.pdf') as pdf:
    pages = pdf.pages
    for i, pg in enumerate (pages):
            txt += pages [i].extract_text()
        
print (txt)

data = {'text': [txt]}
df = pd.dataframe(data)


####write in .txt file
text_file = open(""test.txt"", ""wt"")
txt = str(txt.encode('utf-8'))
n = text_file.write(txt)
text_file.close()

####read from .txt file
with codecs.open('test.txt', 'r', 'utf-8') as f:
    for line in f:
        print (line)","['python', 'encoding', 'utf-8', 'nlp']",68193773,"you are writing the file incorrectly.  rather than encoding the text, declare an encoding when you open the file, and write the text without encoding - python will automatically encode it.
it should be

####write in .txt file
with open(""test.txt"", ""wt"", encoding='utf-8') as text_file:
    n = text_file.write(txt)

unless you are using python 2 you don't need to use codecs to open encoded files, again you can declare the encoding in the open function:
with open(""test.txt"", ""rt"", encoding='utf-8') as f:
    for line in f:
        print(line)",https://stackoverflow.com/questions/68193584,python,30-06-2021 11:05,839.0,1.0,1.0,True,30-06-2021 11:18,30-06-2021 11:18
76398258,openai gpt-3.5 &quot;prompt&quot; argument not working,"i am trying to make a flutter app with the openai api that works like a chatbot, and i want to add a prompt so that the responses are more specialized, like in the openai playground on their website.
i am testing the api post function with postman and it worked perfectly fine for me before i tried adding a prompt. i assumed that to add a prompt you just have to add a ""prompt"": line in the body like when you work with the text-davinci model, but when i do that i get this message returned:
{
    ""error"": {
        ""message"": ""unrecognized request argument supplied: prompt"",
        ""type"": ""invalid_request_error"",
        ""param"": null,
        ""code"": null
    }
}

is there a different way you need to do this with the gpt models, or does the prompt argument just not exist for them?","['http', 'openai-api', 'gpt-3']",76398870,"the open ai 3.5-turbo model only supports the newer chat completion api which does not have a 'prompt' json body field.
i would assume you are using the older competion api json body format aginst the newer chat completion api endpoint and that is the reason for your error.
--- update ---
i can reproduce your exact error response with:

curl  -h ""content-type:
application/json""  -h ""authorization: bearer $openai_api_key""  -d '{
""model"": ""gpt-3.5-turbo"", ""messages"": [{""role"": ""user"", ""content"":
""hello!""}], ""prompt"": ""testing"" }'

this is just a modified example from the api link above. removing the added ""prompt"" field from the json makes it work fine.
so my advice in the comment stands.  figure out what you are sending in the body of your http get request.  it will have a ""prompt"" field, remove it and any other field that should not be there and it should work.",https://stackoverflow.com/questions/76398258,http,03-06-2023 22:26,1292.0,0.0,1.0,True,04-06-2023 22:14,04-06-2023 22:14
69806385,why does viterbi algorithm (pos tagging) always predict one tag?,"here is my hmm model class:
class hiddenmarkovmodel:    
    def __init__(self):
    
        pass
        
    def fit(self, train_tokens_tags_list):
        """"""
        train_tokens_tags_list: array of sentences of pairs word-tag (for train) 
        """"""
        tags = [tag for sent in train_tokens_tags_list
                for (word, tag) in sent]
        words = [word for sent in train_tokens_tags_list
                 for (word, tag) in sent]
        
        tag_num = pd.series(data = nltk.freqdist(tags)).sort_index()
        word_num = pd.series(data = nltk.freqdist(words)).sort_values(ascending=false)
         
        self.tags = tag_num.index
        self.words = word_num.index
        
        a = pd.dataframe({'{}'.format(tag) : [0] * len(tag_num) for tag in tag_num.index}, index=tag_num.index)
        b = pd.dataframe({'{}'.format(tag) : [0] * len(word_num) for tag in tag_num.index}, index=word_num.index)
        
        # compure matrixes a and b from count of words and tags
        
        # sent - sentence
        # sent[i][0] - i word in this sentence, sent[i][1] - i tag in this sentence
        for sent in train_tokens_tags_list:
            for i in range(len(sent)):
                b.loc[sent[i][0], sent[i][1]] += 1 # current i-pair word-tag
                if len(sent) - 1 != i: # for last tag there is no next tag
                    a.loc[sent[i][1], sent[i + 1][1]] += 1 # pair tag-tag
                
        
        # now to probabilities
        
        # normalize along the row, i.e along all possible next tags
        a = a.divide(a.sum(axis=1), axis=0)
        
        # normalize along column, i.e along all possible current words
        b = b / np.sum(b, axis=0)
        
        self.a = a
        self.b = b
        
        return self
        
    
    def predict(self, test_tokens_list):
        """"""
        test_tokens_list : array of sentences of pairs word-tag (for test)
        """"""
        predict_tags = ordereddict({i : np.array([]) for i in range(len(test_tokens_list))})
        
        for i_sent in range(len(test_tokens_list)):
            
            current_sent = test_tokens_list[i_sent] # current sentence
            len_sent = len(current_sent) # lenght of sentence 
            
            q = np.zeros(shape=(len_sent + 1, len(self.tags)))
            q[0] = 1 # zero state (equal distribution for all s)
            back_point = np.zeros(shape=(len_sent + 1, len(self.tags))) # # argmax
            
            for t in range(len_sent):
                
                # if we haven't met this word during training, 
                # we'll use the most popular word with most popular tag instead
                if current_sent[t] not in self.words:
                  # print('this word not in dictionary', current_sent[t])
                  tagdict = findtags('noun', brown_tagged_words)
                  current_sent[t] = tagdict['noun'][0][0]
                    
                # using max we choose next tag
                for i_s in range(len(self.tags)):
                    # i_s - index for tag
                    s = self.tags[i_s] # tag
                    
                    # formula (1)
                    probability = np.max(q[t][i_s] * self.a.loc[:, s] * self.b.loc[current_sent[t], s])

                    q[t + 1][i_s] = np.max(probability)
               
                    # argmax formula(1)
                    
                    # argmax for recreating the sequence of tags
                    back_point[t + 1][i_s] = (probability).reset_index()[s].idxmax() # index
                    
            back_point = back_point.astype('int')

            # saving tags and changing their order for real one
            back_tag = deque()
            current_tag = np.argmax(q[len_sent])

            for t in range(len_sent, 0, -1):
                back_tag.appendleft(self.tags[current_tag])
                current_tag = back_point[t, current_tag]
             
            predict_tags[i_sent] = np.array(back_tag)
        
        
        return predict_tags

it was actually almost completely written before me. i was to fill in some gaps (above all in the viterbi algorithm where we compute q and back_point matrices). but i think i did something wrong as my model always predicts something like that:
ordereddict([(0, array(['.', '.', '.'], dtype='<u1'))]) 

in the example above i gave it this sentence:  [['he', 'can', 'stay']]
i've checked : all these words are in the train data.
i've printed out matrices a and b. here they are:
a:

b:

this shows that the first part of model is working fine.
these are folmulas i had for help:


this is the data:
brown_tagged_sents = brown.tagged_sents(tagset=""universal"", categories='humor')

my_brown_tagged_sents = []
for sent in brown_tagged_sents:
    my_brown_tagged_sents.append(list(map(lambda x: (x[0].lower(), x[1]), sent)))
my_brown_tagged_sents = np.array(my_brown_tagged_sents)

from sklearn.model_selection import train_test_split
train_sents, test_sents = train_test_split(my_brown_tagged_sents, random_state=0, test_size=0.1)

what could be wrong with my model?","['python', 'pandas', 'pos-tagger', 'hidden-markov-models', 'viterbi']",69807910,"probability should look like that:
probability = q[t, :] * self.a.loc[:, s] * self.b.loc[current_sent[t], s]

instead of this:
probability = q[t][i_s] * self.a.loc[:, s] * self.b.loc[current_sent[t], s]",https://stackoverflow.com/questions/69806385,python,02-11-2021 06:52,326.0,1.0,1.0,True,02-11-2021 09:15,02-11-2021 07:11
76363706,got exception &#39;eagertensor&#39; object has no attribute &#39;size&#39; when generating bert embeddings,"i am new to huggingface transforms and i have this little test program:
from transformers import berttokenizer, bertmodel

tokenizer = berttokenizer.from_pretrained('bert-base-uncased')
model = bertmodel.from_pretrained('bert-base-uncased')

title = ""today is monday""
tokens = tokenizer([title], return_tensors=""tf"", truncation=true, padding=true)
outputs = model(**tokens)

it will fail with this exception:
traceback (most recent call last):
  file ""/anaconda3/envs/recommenders/lib/python3.9/runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, none,
  file ""/anaconda3/envs/recommenders/lib/python3.9/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  file ""/root/.vscode-server/extensions/ms-python.python-2023.8.0/pythonfiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in <module>
    cli.main()
  file ""/root/.vscode-server/extensions/ms-python.python-2023.8.0/pythonfiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main
    run()
  file ""/root/.vscode-server/extensions/ms-python.python-2023.8.0/pythonfiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file
    runpy.run_path(target, run_name=""__main__"")
  file ""/root/.vscode-server/extensions/ms-python.python-2023.8.0/pythonfiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path
    return _run_module_code(code, init_globals, run_name,
  file ""/root/.vscode-server/extensions/ms-python.python-2023.8.0/pythonfiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  file ""/root/.vscode-server/extensions/ms-python.python-2023.8.0/pythonfiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 124, in _run_code
    exec(code, run_globals)
  file ""/opt/nwdata/tests/recommenders/examples/00_quick_start/test.py"", line 9, in <module>
    outputs = model(**tokens)
  file ""/anaconda3/envs/recommenders/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  file ""/anaconda3/envs/recommenders/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py"", line 962, in forward
    input_shape = input_ids.size()
  file ""/anaconda3/envs/recommenders/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 437, in __getattr__
    raise attributeerror(""""""
attributeerror: 
        'eagertensor' object has no attribute 'size'.
        if you are looking for numpy-related methods, please run the following:
        from tensorflow.python.ops.numpy_ops import np_config
        np_config.enable_numpy_behavior()

it will work if i pass ""pt"" to the param return_tensors. but i need to set the param to ""tf"" because i will use the result in a tensorflow program.  how can i solve this problem?","['tensorflow2.0', 'huggingface-transformers', 'huggingface']",76363759,"oh, i found the solution.  the solution is to import a different set of classes (tfautomodel, autotokenizer) to do the job:
from transformers import tfautomodel, autotokenizer

tokenizer = autotokenizer.from_pretrained('bert-base-uncased')
model = tfautomodel.from_pretrained('bert-base-uncased')",https://stackoverflow.com/questions/76363706,tensorflow2.0,30-05-2023 10:04,1305.0,1.0,1.0,True,30-05-2023 11:17,30-05-2023 11:17
78685685,when i am using langchain chatopenai model and invoking method it&#39;s working but when using in crewai same llm models it gives invalid api key,"import os
from langchain_openai import openai
from langchain_openai import chatopenai
from langchain.prompts import prompttemplate


chat_model = chatopenai( openai_api_base = ""
                    model_name=""meta/llama3-70b-instruct"",
                    openai_api_key = ""api key"",
                    streaming=true) 

question = ""what is the meaning of life?""

answer = chat_model.invoke(question)
print(question)
print(answer)

when i am using above code it's working fine but when i am trying to user llm nvidea model with crew it gives me error
from crewai import crew,process
from agents import aiblogcreationagent
from tasks import aiblogcreationtasks
from langchain_openai import chatopenai
import os
from blog_io import save_blog
from dotenv import load_dotenv

load_dotenv()
print(os.environ['openai_api_key'])
print(os.environ['serper_api_key'])
agents=aiblogcreationagent()
tasks=aiblogcreationtasks()

llm = chatopenai(
    model = ""meta/llama3-70b-instruct"",
    base_url = ""
    streaming=true)


editor=agents.editor_agent()
blog_fetcher=agents.blog_fetch_agent()
blog_analyzer=agents.blog_analyzer_agent()
blog_complier=agents.blog_compiler_agent()


fetched_blog_task=tasks.fetch_blog_task(blog_fetcher)
analyzed_blog_task=tasks.analyze_blog_task(blog_analyzer,[fetched_blog_task])
compiled_blog_task=tasks.compile_blog_task(blog_complier,[analyzed_blog_task],save_blog)


crew=crew(
    agents=[editor,blog_fetcher,blog_analyzer,blog_complier],
    tasks=[fetched_blog_task,analyzed_blog_task,compiled_blog_task],
    process=process.hierarchical,
    manager_llm=llm,
    memory=true,
)

results=crew.kickoff()

print(results)

error:i encountered an error while trying to use the tool. this was the error:
error code: 401 - {'error': {'message': 'incorrect api key provided: nvapi-jz**********************************************************mns7. you can find your api key at  'type': 'invalid_request_error', 'param': none, 'code': 'invalid_api_key'}}.

what's the solution for it","['python-3.x', 'openai-api', 'large-language-model']",78690254,"you are using an url for nvidia, not openai, use the expanded chatopenai and replace with your nvidia key?
llm = chatopenai(
api_key=api_key
model = ""meta/llama3-70b-instruct"",
base_url = ""
streaming=true)",https://stackoverflow.com/questions/78685685,python-3.x,29-06-2024 10:12,507.0,0.0,1.0,True,01-07-2024 13:22,01-07-2024 13:22
78013937,autogen studio 2 - api_key is not present in llm_config or openai_api_key env variable for agent ** primary_assistant,"running autogen studio 2, i have my model setup and tested successful - when i run the travel demo i get the following error:

api_key is not present in llm_config or openai_api_key env variable for agent ** primary_assistant**. update your workflow to provide an api_key to use the llm.

i have tested the model successfully and the agent is pointing to the correct model.","['python', 'openai-api', 'ms-autogen']",78029864,"i am using a conda environment and was able to resolve the issue by setting my env variable and deactivating / reactivating my environment. i used the command below to set my api key:
conda env config vars set openai_api_key=value

conda deactivate envname
conda activate envname",https://stackoverflow.com/questions/78013937,python,17-02-2024 22:30,2240.0,1.0,2.0,True,05-12-2024 19:17,05-12-2024 19:17
79057082,avoiding overlap in frequency and document frequency count in quanteda,"below is a dummy corpus of 4 documents.
the dictionary was developed to identify the frequency of words or phrases in the corpus, as well as the number of documents a word or phrases occurs in.
the world 'australians' occurs in two dictionary keys (peep, indig). key content is intended to be mutually exclusive.
similarly 'australia' (oz and australia post), foreign (foreign and multinat) and farm/farmers (dairy and farmers) occur in two dictionary keys each,
but are intended to be counted once, according to the dictionary.
the expected overall frequency count is (extracted from the 'pattern"" column of the kwic table) and reported as x2 below. note the word industry appears but is not allocated to industry because it is define din the indig key.
dairy is the most frequency occuring key, occuring in three documents. this can calculated from unique rows in the kwic table 'doc names' column for each key.
i have three questions:

are there any problems/issues that could affect output accuracy using this approach?
is there a better/more parsimonius approach to achieve what i am trying to do?
what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table?

        library (quanteda)
        library(quanteda.textstats)

        txt <- c(doc1 = ""a significant percent of all farms in australia, are dairy. 
         although there are a lot of dairy farms in this country, 
         it is not the biggest farm industry. the life of a farmer is not easy, a dairy 
        farmer has to be an early riser. "",
         doc2 = ""australian people like milk so a healthy dairy industry is important in 
         our country"",
         doc3 = ""dairy and sheep farms developed at the expense of indigenous 
         australians. further many companies  are now foreign-owned"",
         doc4 = ""some farmers are lucky to receive a service from australia post. mail is 
         sent to many foreign countries and received more quickly than 
         delivered in some locations in australia."")



         x <- x %>%
         tokens_compound(phrase(""dairy farmers""), concatenator = "" "") %>%
         tokens_compound(phrase(""dairy farms""), concatenator = "" "") %>%
         tokens_compound(phrase(""dairy farm""), concatenator = "" "") %>%
         tokens_compound(phrase(""dairy farming""), concatenator = "" "") %>%
         tokens_compound(phrase(""dairy industry""), concatenator = "" "") %>%
         tokens_compound(phrase(""indigenous australians""), concatenator = "" "") %>%
         tokens_compound(phrase(""australia post""), concatenator = "" "") %>%
         tokens_compound(phrase(""dairy farmer""), concatenator = "" "")
              x

         dict <- dictionary(list(multinat = c(""offshore petroleum companies"", ""foreign- 
         owned"", ""foreign owned"", ""foreign companies"", ""multinational"", ""multinational 
         oil companies"", ""multinationals"", ""transnational""),
         dairy = c(""dairy farmers"", ""dairy farms"",""dairy farm"",""dairy farming"",""dairy 
         industry"", ""dairy farmer"",""dairy"", ""milk""),
         auspost = ""australia post"",
         oz = c(""australia"", ""this country"", ""our country""),
         farmers = c(""farmers"", ""farmer"", ""farm"", ""farms""),
         foreign = c(""foreign"", ""foreigner"", ""foreigners""), 
         business =c(""small business"", ""business"", ""businesses"", ""company"", ""companies""),
         indig = c(""aboriginal"", ""aboriginals"", ""indigenous australians"", ""torres 
         strait""),
         peep = c(""australians"", ""people of australia"", ""australian people"", ""people of 
         this nation"", ""people of this country""),
         industry = c(""industry"", ""industries"")))

        kwicdict <- kwic(x, pattern = dict, window = 4)
        write.csv (kwicdict, ""d:/output/test.csv"")

       df <- read.csv(""d://output/test.csv"",header=t)

       ## obtaining frequency count of kwic table 'pattern ' values
       > x2 <- df[,8]
       > 
       > table (x2)
       x2
       auspost business    dairy  farmers  foreign    indig industry multinat  oz  peep    
          1        1        6        5        1        1        1        1     5    2","['r', 'count', 'nlp', 'overlap', 'quanteda']",79058791,"i don't think that kwic() is what you want here. tokens_lookup() lets you specify that the nested scope should be mutually exclusive across keys, not just within keys. observe the difference below. (and note the use of wildcarding for dairy key.)
library(quanteda)
#> package version: 4.1.0
#> unicode version: 14.0
#> icu version: 71.1
#> parallel computing: 10 of 10 threads used.
#> see  for tutorials and examples.
library(quanteda.textstats)

txt <- c(doc1 = ""a significant percent of all farms in australia, are dairy. 
         although there are a lot of dairy farms in this country, 
         it is not the biggest farm industry. the life of a farmer is not easy, a dairy 
        farmer has to be an early riser. "",
         doc2 = ""australian people like milk so a healthy dairy industry is important in 
         our country"",
         doc3 = ""dairy and sheep farms developed at the expense of indigenous 
         australians. further many companies  are now foreign-owned"",
         doc4 = ""some farmers are lucky to receive a service from australia post. mail is 
         sent to many foreign countries and received more quickly than 
         delivered in some locations in australia."")

dict <- dictionary(list(multinat = c(""offshore petroleum companies"", ""foreign-owned"", 
                                     ""foreign owned"", ""foreign companies"", ""multinational"", 
                                     ""multinational oil companies"", ""multinationals"", ""transnational""),
                        dairy = c(""dairy farm*"", ""dairy industry"", ""dairy"", ""milk""),
                        auspost = ""australia post"",
                        oz = c(""australia"", ""this country"", ""our country""),
                        farmers = c(""farmers"", ""farmer"", ""farm"", ""farms""),
                        foreign = c(""foreign"", ""foreigner"", ""foreigners""), 
                        business =c(""small business"", ""business"", ""businesses"", ""company"", ""companies""),
                        indig = c(""aboriginal"", ""aboriginals"", ""indigenous australians"", ""torres strait""),
                        peep = c(""australians"", ""people of australia"", ""australian people"", 
                                 ""people of this nation"", ""people of this country""),
                        industry = c(""industry"", ""industries"")))

x <- tokens(txt)

# with overlap
tokens_lookup(x, dict) |>
    dfm()
#> document-feature matrix of: 4 documents, 10 features (55.00% sparse) and 0 docvars.
#>       features
#> docs   multinat dairy auspost oz farmers foreign business indig peep industry
#>   doc1        0     3       0  2       5       0        0     0    0        1
#>   doc2        0     2       0  1       0       0        0     0    1        1
#>   doc3        1     1       0  0       1       0        1     1    1        0
#>   doc4        0     0       1  2       1       1        0     0    0        0

# without overlap
tokens_lookup(x, dict, nested_scope = ""dictionary"") |>
    dfm()
#> document-feature matrix of: 4 documents, 10 features (60.00% sparse) and 0 docvars.
#>       features
#> docs   multinat dairy auspost oz farmers foreign business indig peep industry
#>   doc1        0     3       0  2       3       0        0     0    0        1
#>   doc2        0     2       0  1       0       0        0     0    1        0
#>   doc3        1     1       0  0       1       0        1     1    0        0
#>   doc4        0     0       1  1       1       1        0     0    0        0

created on 2024-10-06 with reprex v2.1.1",https://stackoverflow.com/questions/79057082,r,05-10-2024 12:43,61.0,1.0,1.0,True,14-12-2024 09:57,14-12-2024 09:57
71557275,emotion detection in text using emoroberta,"i'm using emoroberta for emotion detection and i want the output to be all emotions, each with its assigned score and not only the final emotion and its score. how can i do that?
this is the code i'm using:
tokenizer = robertatokenizerfast.from_pretrained(""arpanghoshal/emoroberta"")
model = tfrobertaforsequenceclassification.from_pretrained(""arpanghoshal/emoroberta"")
emotion = pipeline('sentiment-analysis', model='arpanghoshal/emoroberta')
def get_emotion_label(text):
  return(emotion(text)[0]['label'])

df['text']= df['text'].apply(remove_html).apply(remove_url).apply(remove_stopwords)
df['emotion']= df['text'].apply(get_emotion_label)","['python', 'deep-learning', 'huggingface-transformers']",71558260,"so the solution is to add a parameter in the pipeline for it to become:
emotion = pipeline('sentiment-analysis', model='arpanghoshal/emoroberta' , return_all_scores= true)",https://stackoverflow.com/questions/71557275,python,21-03-2022 11:59,1454.0,1.0,2.0,True,14-10-2022 17:35,21-03-2022 12:01
77280761,"openai api error: &quot;module &#39;openai&#39; has no attribute &#39;chatcompletion&#39;, did you mean &#39;completion&#39;?&quot;","i can't seem to figure out what the issue is here, i am running version 0.28.1:

from what i have read i should be using chatcompletion rather than completion as that's what gpt-4 and 3.5-turbo supports.
response = openai.chatcompletion.create(
    prompt=question,
    temperature=0,
    max_tokens=3700,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0,
    stop=none,
    model=""gpt-4"",
)

looking at other answers i can also tell you that my file isn't named openai.py or anything like that.
thanks for any help in advance.","['python', 'openai-api', 'chatgpt-api', 'chat-gpt-4']",77292230,"first of all, be sure you have an up-to-date openai package version.
if not, upgrade the openai package.
python:
pip install --upgrade openai

nodejs:
npm update openai


the code posted in your question above has a mistake. the chat completions api doesn't have the prompt parameter as the completions api does. instead, it has the messages parameter. see the official openai documentation.
try the following:
import os
import openai
openai.api_key = os.getenv(""openai_api_key"")

completion = openai.chatcompletion.create(
  model = ""gpt-4"",
  messages = [
    {""role"": ""system"", ""content"": ""you are a helpful assistant.""},
    {""role"": ""user"", ""content"": ""hello!""}
  ]
)

print(completion.choices[0].message)",https://stackoverflow.com/questions/77280761,python,12-10-2023 12:52,5903.0,2.0,4.0,True,02-04-2024 15:08,24-10-2023 13:55
55983874,how to detect language of a dataframe object?,"i want to create a new column in my dataframe review giving the language of the column text which is of type object.
i try to convert to string and then use the detect function from langdetect but, there still a type error when i run the code.
i do not understand the problem lol 
my code : 
from langdetect import detect


review['langue'] = detect((review['text']).astype(str))

actual result : 
--------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)


typeerror: expected string or bytes-like object","['python', 'typeerror', 'language-detection']",55983968,"if i correctly understood your question you needs
from langdetect import detect
review['langue'] = review['text'].apply(detect)

detect function expect str as argument, not pd.series. instead, you should apply detect function to each element of review['text'] pd.series.",https://stackoverflow.com/questions/55983874,python,04-05-2019 15:09,5328.0,4.0,3.0,True,29-07-2021 09:57,04-05-2019 15:11
76094865,openai api: how to count tokens before api request,"i would like to count the tokens of my openai api request in r before sending it (version gpt-3.5-turbo). since the openai api has rate limits, this seems important to me.
example:
the function i use to send requests:
ask_chatgpt <- function(prompt) {
      response <- post(
        url = "" 
        add_headers(authorization = paste(""bearer"", api_key)),
        content_type_json(),
        encode = ""json"",
        body = list(
          model = ""gpt-3.5-turbo"",
          messages = list(list(
            role = ""user"", 
            content = prompt
          ))
        )
      )
      str_trim(content(response)$choices[[1]]$message$content)
    }

example

api_key <- ""your_openai_api_key"" 

library(httr)
library(tidyverse)

#calls the chatgpt api with the given prompt and returns the answer
ask_chatgpt <- function(prompt) {
  response <- post(
    url = "" 
    add_headers(authorization = paste(""bearer"", api_key)),
    content_type_json(),
    encode = ""json"",
    body = list(
      model = ""gpt-3.5-turbo"",
      messages = list(list(
        role = ""user"", 
        content = prompt
      ))
    )
  )
  str_trim(content(response)$choices[[1]]$message$content)
}

prompt <- ""how do i count the token in r for gpt-3.45-turbo?""

ask_chatgpt(prompt)
#> [1] ""as an ai language model, i am not sure what you mean by \""count the token in r for gpt-3.5-turbo.\"" please provide more context or clarification so that i can better understand your question and provide an appropriate answer.""

created on 2023-04-24 with reprex v2.0.2
i would like to calculate/estimate as how many tokens prompt will need with gtp-3.5-turbo
there is a similar question for gtp-3 and python, where the tiktoken library is recommended. however, i could not find a similar library in r.
openai also recommends tiktoken or gpt-3-encoder package for javascript.","['r', 'nlp', 'openai-api']",76098874,"openai has their own tokenizer so you probably won't be able to reproduce it. instead, i would just recommend using their python api via the reticulate package
first, install the tiktoken package via the command line using:
pip install tiktoken

then, in r
library(reticulate)
tiktoken <- import(""tiktoken"")
encoding <- tiktoken$encoding_for_model(""gpt-3.5-turbo"")
prompt <- ""how do i count the token in r for gpt-3.45-turbo?""
length(encoding$encode(prompt))
# [1] 19",https://stackoverflow.com/questions/76094865,r,24-04-2023 18:10,3998.0,2.0,2.0,True,28-06-2023 06:51,25-04-2023 07:38
42986405,how to speed up gensim word2vec model load time?,"i'm building a chatbot so i need to vectorize the user's input using word2vec. 
i'm using a pre-trained model with 3 million words by google (googlenews-vectors-negative300).
so i load the model using gensim:
import gensim
model = gensim.models.keyedvectors.load_word2vec_format('googlenews-vectors-negative300.bin', binary=true)

the problem is that it takes about 2 minutes to load the model. i can't let the user wait that long.
so what can i do to speed up the load time?
i thought about putting each of the 3 million words and their corresponding vector into a mongodb database. that would certainly speed things up but intuition tells me it's not a good idea.","['python', 'nlp', 'gensim', 'word2vec']",43067907,"in recent gensim versions you can load a subset starting from the front of the file using the optional limit parameter to load_word2vec_format(). (the googlenews vectors seem to be in roughly most- to least- frequent order, so the first n are usually the n-sized subset you'd want. so use limit=500000 to get the most-frequent 500,000 words' vectors ï¿½ï¿½ï¿½ still a fairly large vocabulary ï¿½ï¿½ï¿½ saving 5/6ths of the memory/load-time.)
so that may help a bit. but if you're re-loading for every web-request, you'll still be hurting from loading's io-bound speed, and the redundant memory overhead of storing each re-load. 
there are some tricks you can use in combination to help. 
note that after loading such vectors in their original word2vec.c-originated format, you can re-save them using gensim's native save(). if you save them uncompressed, and the backing array is large enough (and the googlenews set is definitely large enough), array gets dumped in a separate file in a raw binary format. that file can later be memory-mapped from disk, using gensim's native [load(filename, mmap='r')][1] option.
initially, this will make the load seem snappy ï¿½ï¿½ï¿½ rather than reading all the array from disk, the os will just map virtual address regions to disk data, so that some time later, when code accesses those memory locations, the necessary ranges will be read-from-disk. so far so good!
however, if you are doing typical operations like most_similar(), you'll still face big lags, just a little later. that's because this operation requires both an initial scan-and-calculation over all the vectors (on first call, to create unit-length-normalized vectors for every word), and then another scan-and-calculation over all the normed vectors (on every call, to find the n-most-similar vectors). those full-scan accesses will page-into-ram the whole array ï¿½ï¿½ï¿½ again costing the couple-of-minutes of disk io.at you want is to avoid redundantly doing that unit-normalization, and to pay the io cost just once. that requires keeping the vectors in memory for re-use by all subsequent web requestes (or even multiple parallel web requests). fortunately memory-mapping can also help here, albeit with a few extra prep steps.
first, load the word2vec.c-format vectors, with load_word2vec_format(). then, use model.init_sims(replace=true) to force the unit-normalization, destructively in-place (clobbering the non-normalized vectors). 
then, save the model to a new filename-prefix: model.save('googlenews-vectors-gensim-normed.bin'`. (note that this actually creates multiple files on disk that need to be kept together for the model to be re-loaded.)
now, we'll make a short python program that serves to both memory-map load the vectors, and force the full array into memory. we also want this program to hang until externally terminated (keeping the mapping alive), and be careful not to re-calculate the already-normed vectors. this requires another trick because the loaded keyedvectors actually don't know that the vectors are normed. (usually only the raw vectors are saved, and normed versions re-calculated whenever needed.)
roughly the following should work:
from gensim.models import keyedvectors
from threading import semaphore
model = keyedvectors.load('googlenews-vectors-gensim-normed.bin', mmap='r')
model.syn0norm = model.syn0  # prevent recalc of normed vectors
model.most_similar('stuff')  # any word will do: just to page all in
semaphore(0).acquire()  # just hang until process killed

this will still take a while, but only needs to be done once, before/outside any web requests. while the process is alive, the vectors stay mapped into memory. further, unless/until there's other virtual-memory pressure, the vectors should stay loaded in memory. that's important for what's next.
finally, in your web request-handling code, you can now just do the following:
model = keyedvectors.load('googlenews-vectors-gensim-normed.bin', mmap='r')
model.syn0norm = model.syn0  # prevent recalc of normed vectors
# ï¿½ï¿½ï¿½ plus whatever else you wanted to do with the model

multiple processes can share read-only memory-mapped files. (that is, once the os knows that file x is in ram at a certain position, every other process that also wants a read-only mapped version of x will be directed to re-use that data, at that position.). 
so this web-reqeust load(), and any subsequent accesses, can all re-use the data that the prior process already brought into address-space and active-memory. operations requiring similarity-calcs against every vector will still take the time to access multiple gb of ram, and do the calculations/sorting, but will no longer require extra disk-io and redundant re-normalization. 
if the system is facing other memory pressure, ranges of the array mayout of memory until the next read pages them back in. and if the machine lacks the ram to ever fully load the vectors, then every scan will require a mixing of paging-in-and-out, and performance will be frustratingly bad not matter what. (in such a case: get more ram or work with a smaller vector set.)
but if you do have enough ram, this winds up making the original/natural load-and-use-directly code ""just work"" in a quite fast manner, without an extra web service interface, because the machine's shared file-mapped memory functions as the service interface.",https://stackoverflow.com/questions/42986405,python,23-03-2017 20:30,30098.0,28.0,4.0,True,29-06-2023 16:54,29-06-2023 16:54
71143240,normalizing topic vectors in top2vec,"i am trying to understand how top2vec works. i have some questions about the code that i could not find an answer for in the paper. a summary of what the algorithm does is that it:

embeds words and vectors in the same semantic space and normalizes them. this usually has more than 300 dimensions.
projects them into 5-dimensional space using umap and cosine similarity.
creates topics as centroids of clusters using hdbscan with euclidean metric on the projected data.

what troubles me is that they normalize the topic vectors. however, the output from umap is not normalized, and normalizing the topic vectors will probably move them out of their clusters. this is inconsistent with what they described in their paper as the topic vectors are the arithmetic mean of all documents vectors that belong to the same topic.
this leads to two questions:
how are they going to calculate the nearest words to find the keywords of each topic given that they altered the topic vector by normalization?
after creating the topics as clusters, they try to deduplicate the very similar topics. to do so, they use cosine similarity. this makes sense with the normalized topic vectors. in the same time, it is an extension of the inconsistency that normalizing topic vectors introduced. am i missing something here?","['python', 'nlp', 'topic-modeling', 'doc2vec', 'hdbscan']",71145381,"i got the answer to my questions from the source code. i was going to delete the question but i will leave the answer any way.
it is the part i missed and is wrong in my question. topic vectors are the arithmetic mean of all documents vectors that belong to the same topic. topic vectors belong to the same semantic space where words and documents vector live.
that is why it makes sense to normalize them since all words and documents vectors are normalized, and to use the cosine metric when looking for duplicated topics in the higher original semantic space.",https://stackoverflow.com/questions/71143240,python,16-02-2022 13:59,623.0,0.0,1.0,True,16-02-2022 16:13,16-02-2022 14:34
77830490,spacy import error: cannot import name &#39;combining_diacritics&#39; from &#39;spacy.lang.char_classes&#39;,"when i try importing the nlp library 'spacy' using import spacy i'm getting the following error:
importerror: cannot import name 'combining_diacritics' from 'spacy.lang.char_classes'

here are my versions:
spacy==3.7.2
spacy-legacy==3.0.12

thinc==8.2.2

pydantic==1.8.2
pydantic_core==2.14.6

python version - 3.9.18

i tried installing, uninstalling spacy, upgrading the library & all the other common checks. where is the issue?
complete error:
importerror                               traceback (most recent call last)
~\appdata\local\temp\ipykernel_33180\572880994.py in 
----> 1 import spacy

c:\users\anaconda3\lib\site-packages\spacy\__init__.py in 
     11 from thinc.api import config, prefer_gpu, require_cpu, require_gpu  # noqa: f401
     12 
---> 13 from . import pipeline  # noqa: f401
     14 from . import util
     15 from .about import __version__  # noqa: f401

c:\users\anaconda3\lib\site-packages\spacy\pipeline\__init__.py in 
----> 1 from .attributeruler import attributeruler
      2 from .dep_parser import dependencyparser
      3 from .edit_tree_lemmatizer import edittreelemmatizer
      4 from .entity_linker import entitylinker
      5 from .entityruler import entityruler

c:\users\anaconda3\lib\site-packages\spacy\pipeline\attributeruler.py in 
      6 from .. import util
      7 from ..errors import errors
----> 8 from ..language import language
      9 from ..matcher import matcher
     10 from ..scorer import scorer
...
      3     alpha_lower,
      4     alpha_upper,
      5     combining_diacritics,

importerror: cannot import name 'combining_diacritics' from 'spacy.lang.char_classes' (c:\users\anaconda3\lib\site-packages\spacy\lang\char_classes.py)","['python', 'pip', 'nlp', 'spacy', 'pydantic']",77835083,"create a new enviroments and install scipy, you can use the following command:
conda create -n myenv

activate myenv:
conda activate myenv

install scipy check the instalation page:
conda install -c conda-forge spacy

then you can use scipy.",https://stackoverflow.com/questions/77830490,python,17-01-2024 07:03,234.0,1.0,1.0,True,17-01-2024 19:38,17-01-2024 16:27
53035563,"sentiment analysis, naive bayes accuracy","i'm trying to form a naive bayes classifier script for sentiment classification of tweets. i'm pasting my whole code here, because i know i will get hell if i don't. so i basically i use nltk's corpuses as training data, and then some tweets i scraped as test data. i pre-process them and do a bag of words extraction. the classifier is trained with no problem and when i do the following
print(classifier.classify(bag_of_words('this is magnificent')))  

it correctly outputs 'pos'.
now my problem is how to calculate accuracy using ntlk.util accuracy. i do
print(nltk.classify.accuracy(classifier, proc_set))

and i get the following error:
  file ""/library/frameworks/python.framework/versions/3.7/lib/python3.7/site-   packages/nltk/classify/util.py"", line 87, in accuracy
  results = classifier.classify_many([fs for (fs, l) in gold])
  attributeerror: 'naivebayesclassifier' object has no attribute 'classify_many'

i also tried this
test_set_final=[]
for tweet in proc_test:
test_set_final.append((bag_of_words(tweet),   classifier.classify(bag_of_words(tweet))))

print(nltk.classify.accuracy(classifier, test_set_final))

and i get the same kind of error
print(nltk.classify.accuracy(classifier, test_set_final))
file ""/library/frameworks/python.framework/versions/3.7/lib/python3.7/site-packages/nltk/classify/util.py"", line 87, in accuracy
results = classifier.classify_many([fs for (fs, l) in gold])
attributeerror: 'naivebayesclassifier' object has no attribute 'classify_many'

code:
import nltk
import ast
import string
import re
import csv
import textblob
import pandas as pd
import numpy as np
import itertools
from textblob import textblob
from textblob import word
from textblob.classifiers import naivebayesclassifier
from nltk.corpus import twitter_samples
from nltk.corpus import stopwords
from nltk.corpus import wordnet as wd
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import countvectorizer
from random import shuffle
from nltk.classify.util import accuracy
from autocorrect import spell

stopwords = stopwords.words('english')
lemmatizer = nltk.wordnetlemmatizer().lemmatize
punct=['""','$','%','&','\',''','(',')','+',',','-     ','.','/',':',';','<','=','>','@','[','\',','^','_','`','{','|','}','~']

emoticons_happy = set([
':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',
':^)', ':-d', ':d', ': d','8-d', '8d', 'x-d', 'xd', 'x-d', 'xd', '=-d', '=d',
'=-3', '=3', ':-))', ':-)', "":'-)"", "":')"", ':*', ':^*', '>:p', ':-p', ':p', 'x-p',
'x-p', 'xp', 'xp', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',
'<3',':*', ':p'
])

emoticons_sad = set([
':l', ':-/', '>:/', ':s', '>:[', ':@', ':-(', ':[', ':-||', '=l', ':<',
':-[', ':-<', '=\\', '=/', '>:(', ':-(', '>.<', "":'-("", "":'("", ':\\', ':-c',
':c', ':{', '>:\\', ';('
])
emoticons = emoticons_happy.union(emoticons_sad)


def pre_process(tweet):

    tweet = re.sub(r' '', tweet)

    tweet = re.sub(r'#', '', tweet)

    tweet=''.join([i for i in tweet if not i.isdigit()])

    tweet=re.sub(r'([.,/#!$%^&*;:{}=_`~-])([.,/#!$%^&*;:{}=_`~-]+)\1+', r'\1',tweet)

    tweet = re.sub(r'@[a-za-z0-9]+', '', tweet)

    tweet=''.join([i for i in tweet if i not in emoticons])

    tweet=''.join([i for i in tweet if i not in punct])

    tweet=' '.join([i for i in tweet.split() if i not in stopwords])

    tweet=tweet.lower()

    tweet=lemmatize(tweet)

    return tweet

def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('j'):
        return wd.adj
    elif treebank_tag.startswith('v'):
        return wd.verb
    elif treebank_tag.startswith('n'):
        return wd.noun
    elif treebank_tag.startswith('r'):
        return wd.adv
    else:
        return wd.noun

def lemmatize(tt):
    pos = nltk.pos_tag(nltk.word_tokenize(tt))
    lemm = [lemmatizer(sw[0], get_wordnet_pos(sw[1])) for sw in pos]
    sentence= ' '.join([i for i in lemm])

    return sentence


test_tweets=[]
file=open('scraped_tweets.csv', 'r')
reader = csv.reader(file)
for line in reader:
    line=line[1]
    test_tweets.append(line)

pos_tweets = twitter_samples.strings('positive_tweets.json')
neg_tweets = twitter_samples.strings('negative_tweets.json')



proc_train_pos=[]
for tweet in pos_tweets:
    proc_train_pos.append(pre_process(tweet))
proc_train_neg=[]
for tweet in neg_tweets:
    proc_train_neg.append(pre_process(tweet))
proc_test=[]
for tweet in test_tweets:
    proc_test.append(pre_process(tweet))


def bag_of_words(tweet):
    words_dictionary = dict([word, true] for word in tweet.split())    
    return words_dictionary

pos_tweets_set = []
for tweet in proc_train_pos:
    pos_tweets_set.append((bag_of_words(tweet), 'pos'))    

neg_tweets_set = []
for tweet in proc_train_neg:
    neg_tweets_set.append((bag_of_words(tweet), 'neg'))

shuffle(pos_tweets_set)
shuffle(neg_tweets_set)
train_set = pos_tweets_set+neg_tweets_set

classifier = naivebayesclassifier(train_set)
print('training is done')

#print(classifier.classify(bag_of_words('this is magnificent'))) #output 'pos'

print(nltk.classify.accuracy(classifier, proc_set))","['python', 'scikit-learn', 'nltk', 'sentiment-analysis', 'naivebayes']",53035719,"well, as the error message says, the classifier you are trying to use (naivebayesclassifier) doesn't have the method classify_many that the nltk.classify.util.accuracy function requires.
(reference: 
now, that looks like an nltk bug, but you can get your answer easily on your own:
from sklearn.metrics import accuracy_score

y_predicted = [classifier.classify(x) for x in proc_set]

accuracy = accuracy_score(y_true, y_predicted)

where y_true are the sentiment values corresponding to proc_set inputs (which i don't see you actually creating in your code shown above, though).
or, without using the sklearn accuracy function, but pure python:
hits = [yp == yt for yp, yt in zip(y_predicted, y_true)]

accuracy = sum(hits)/len(hits)",https://stackoverflow.com/questions/53035563,python,28-10-2018 19:59,825.0,0.0,2.0,True,24-03-2025 12:43,24-03-2025 12:43
67595500,how to download a model from huggingface?,"for example, i want to download bert-base-uncased on  but can't find a 'download' link. or is it not downloadable?","['huggingface-transformers', 'transformer-model']",67599169,"the models are automatically cached locally when you first use it.
so, to download a model, all you have to do is run the code that is provided in the model card (i chose the corresponding model card for bert-base-uncased).
at the top right of the page you can find a button called ""use in transformers"", which even gives you the sample code, showing you how to use it in python. again, for bert-base-uncased, this gives you the following code snippet:
from transformers import autotokenizer, automodelformaskedlm
  
tokenizer = autotokenizer.from_pretrained(""bert-base-uncased"")
model = automodelformaskedlm.from_pretrained(""bert-base-uncased"")

when you run this code for the first time, you will see a download bar appear on screen. see this post (disclaimer: i gave one of the answers) if you want to find the actual folder where huggingface stores their models.",https://stackoverflow.com/questions/67595500,huggingface-transformers,19-05-2021 00:34,297739.0,92.0,16.0,True,09-02-2025 04:17,04-04-2024 01:46
75314242,extracting text from pdf scientific papers,"i would like to extract text from a scientific document in pdf format.
i first used pypdf2 but random spaces appear in the middle of several words.
i am currently using pymupdf
import fitz
import re

def extract_pdf_text(pdf_file_path):
    doc = fitz.open(pdf_file_path)
    text = """"
    for page in doc:
        text += page.get_text(""text"")#.replace(""\n"", "" "")
    return text

pdf_path = ""/home/xxx/papers/xxxxx.pdf""
text = extract_pdf_text(pdf_path)
text = re.sub(r""ï¿½ï¿½ï¿½"", "" "", text)
url_pattern = re.compile(r'
text = re.sub(url_pattern, 'replaced_link.', text)
text = re.sub(r""\s+"", "" "", text)

removing ï¿½ï¿½ï¿½, replacing url by fix word and remove extra space
the goal is to separate the text into sentences (i use spacy).
but it failed in some places because the extracted text sticks with a space two distinct parts of the pdf (e.g. title and author).
i would like to paste them with a ""\n"" instead."" rel=""nofollow noreferrer"">
if i extract and dispatch in sentences i get
[""see discussions, stats, and author profiles for this publication at:  hdskg:"", ""harvesting domain speciï¿½ï¿½ï¿½c knowledge graph from content of webpages conference paper ï¿½ï¿½ february 2017""]
instead of
[""see discussions, stats, and author profiles for this publication at:  ""hdskg:harvesting domain speciï¿½ï¿½ï¿½c knowledge graph from content of webpages"", ""conference paper ï¿½ï¿½ febru","['python', 'pdf', 'spacy']",75317265,"thanks to jorj mckie, get_text(sort=true) worked a bit, mb for text = re.sub(r""\s+"", "" "", text) that was removing the ""\n"".",https://stackoverflow.com/questions/75314242,python,01-02-2023 17:52,1951.0,1.0,1.0,True,01-02-2023 23:30,01-02-2023 21:23
76897509,dynamic filtering with conversationalretrievalchain,"i'm trying to add metadata filtering of the underlying vector store (chroma).
db = chroma.from_documents(texts, embeddings)

it works like this:
qa = conversationalretrievalchain.from_llm(
    openai(openai_api_key=get_random_key(openai_api_key_pool), cache=true, temperature=0),
    vectorstoreretriever(vectorstore=db, search_kwargs={""filter"": {""source"": ""data/my.pdf""}}),
    verbose=true,
    return_source_documents=true) 
result = qa({""question"": query, ""chat_history"": []})

but this would imply creating a separate chain for each document which seems weird. however, when i try to pass the filter to the existing chain, it doesn't seem to have any effect, it returns results for all the documents in the db.
qa = conversationalretrievalchain.from_llm(
    openai(openai_api_key=get_random_key(openai_api_key_pool), cache=true, temperature=0),
    vectorstoreretriever(vectorstore=db),
    verbose=true,
    return_source_documents=true)

filter = {'source': 'my.pdf'}

result = qa({""question"": query, ""chat_history"": [], ""filter"": filter})

am i missing something? or it really won't work without extending the existing classes/modifying source code of langchain?",['langchain'],76899080,"i ended up extending both classes i used to pass the filter:
class conversationalretrievalchainpassargs(conversationalretrievalchain):

    def _get_docs(self, question: str, inputs: dict[str, any], *,
        run_manager: callbackmanagerforchainrun) -> list[document]:
        """"""get docs.""""""
        docs = self.retriever._get_relevant_documents(
            question, inputs['filter']
        )
        return self._reduce_tokens_below_limit(docs)


class vectorstoreretrieverwithfiltering(vectorstoreretriever):

    def _get_relevant_documents(self, query: str, filter: dict) -> list[document]:
        if self.search_type == ""similarity"":
            docs = self.vectorstore.similarity_search(query, filter=filter, **self.search_kwargs)
        ...
        return docs

if anyone has a more elegant solution, please share.",https://stackoverflow.com/questions/76897509,langchain,14-08-2023 08:46,2331.0,4.0,2.0,True,13-03-2024 11:34,12-03-2024 02:50
71292945,find string matching among columns,"i have a dataframe that looks like this:
id      sentences                                           ind                 tar
0       in samples of depression injected intraneously...   depression        albumin
0       monomethylmethacrylate in whole blood was asso...   depression        albumin
1       in samples of depression injected intraneously...   depression          hip
1       monomethylmethacrylate in whole blood was asso...   depression          hip
2       the gvh kinetics and cellular characteristics ...   gvh,gvh,gvh,gvh...  pfc
2       effects on pfcgeneword responses to thymus-dep...   gvh,gvh,gvh,gvh...  pfc
2       the unresponsive state which developed in gvhg...   gvh,gvh,gvh,gvh...  pfc
2       furthermore, gvhgeneword spleen cells suppress...   gvh,gvh,gvh,gvh...  pfc
2       this active suppressor effect was found to be ...   gvh,gvh,gvh,gvh...  pfc
2       the delayed transfer of gvhgeneword cells to i...   gvh,gvh,gvh,gvh...  pfc

i want to keep only the rows that have either an ind or a tar value in the corresponding sentence.
the problem is that when i have more than one elements in either ind or tar, even if one of those elements exists on sentence, it doesn't match it, because it uses the whole string as a term. for example, at the 5th row, even though the word gvh exists in the sentence, it uses as ind the whole value gvh,gvh,gvh,gvh and not each gvh term separately. can someone help how to fix this issue? here's my code so far :
df['check_ind'] = df.apply(lambda x: x.ind in x.sentences, axis=1)
df['check_tar'] = df.apply(lambda x: x.tar in x.sentences, axis=1)
df = df.loc[(df['check_ind'] == true) | (df['check_tar'] == true)]

print(df.sentences.iloc[4], '\n')

print(df.indications.iloc[4], '\n')

print(df.targets.iloc[4], '\n')

print(df.check_ind.iloc[4], '\n')

print(df.check_tar.iloc[4], '\n')


>>>> the gvh kinetics and cellular characteristics indicated that suppressor t cells exert an anti-mitotic influence on antigen-stimulated b-cell proliferation. . 

>>>> gvh,gvh,gvh,gvh,gvh,gvh 

>>>> pfc 

>>>> false (this should return true since gvh is in the sentence)

>>>> false 

data:
{'id': [0, 0, 1, 1, 2, 2, 2, 2, 2, 2],
 'sentences': ['in samples of depression injected intraneously...',
  'monomethylmethacrylate in whole blood was asso...',
  'in samples of depression injected intraneously...',
  'monomethylmethacrylate in whole blood was asso...',
  'the gvh kinetics and cellular characteristics ...',
  'effects on pfcgeneword responses to thymus-dep...',
  'the unresponsive state which developed in gvhg...',
  'furthermore, gvhgeneword spleen cells suppress...',
  'this active suppressor effect was found to be ...',
  'the delayed transfer of gvhgeneword cells to i...'],
 'ind': ['depression', 'depression', 'depression',
         'depression', 'gvh,gvh,gvh,gvh...',
         'gvh,gvh,gvh,gvh...', 'gvh,gvh,gvh,gvh...',
         'gvh,gvh,gvh,gvh...', 'gvh,gvh,gvh,gvh...',
         'gvh,gvh,gvh,gvh...'],
 'tar': ['albumin', 'albumin', 'hip', 'hip', 'pfc', 'pfc',
         'pfc', 'pfc', 'pfc', 'pfc']}","['python', 'pandas', 'dataframe', 'nlp']",71293108,"your code is currently treating x.ind as if it were a simple value.
conceptually x.ind is not a single value, but rather a comma-separated list of values.
in python, you can transform a comma-separated list into an actual python list using x.split(','). in addition, str.strip() is useful to remove possible spaces (for instance, if you have ""gvh ,gvh "", the spaces should probably be ignored).
finally, builtin function any and all are convenient to broadcast a condition to a list.
df['check_ind'] = df.apply(lambda x: any(v.strip() in x.sentences for v in x.split(',')), axis=1)",https://stackoverflow.com/questions/71292945,python,28-02-2022 09:33,62.0,1.0,5.0,True,28-02-2022 09:57,28-02-2022 09:55
55140308,does stemming and fuzzy search work together in apache solr,"i am using porter filter factory for a field which has 3 to 4 words in it.
eg : ""abc blossom company""
i expect to fetch the above document when i search for abc blossoming company as well. 

when i query this:

name:abc and name:blossoming and name:company


i get my result
this is what the parsed query looks like

+name:southern +name:blossom +name:compani
  (stemmer works fine)
  

but when i add the fuzzy syntax and query like this,

name:abc~1 and name:blossoming~1 and name:company~1


the search does not give any documents as result and the parsed query looks like this


+name:abc~1 +name:blossoming~1 +name:company~2 
  

this clearly shows that stemming is not happening.
kindly review and give feedback.","['solr', 'stemming', 'fuzzy', 'porter-stemmer']",55157129,"tl;dr
stemming is not happening, since you have used the porterfilter, which is not a multitermawarecomponent.
what to do?
use one of the filters/normalizers that implements the multitermawarecomponent interface.
explanation
you, like many others, are caught by solr's and lucense multiterm behaviour. there is a good article about this topic on the solr wiki. all though this article is dated, it still holds true

one of the surprises for most solr users is that wildcards queries haven't gone through any analysis. practically, this means that wildcard (and prefix and range) queries are case sensitive, which is at odds with expectations. as of this solr-2438, solr-2918, and perhaps solr-2921, this behavior is changed.
what's a multiterm you ask? essentially it's any term that may ""point to"" more than one real term. for instance, run* could expand to runs, runner, running, runt, etc. likewise, a range query is really a ""multiterm"" query as well. before solr 3.6, these were completely unprocessed, the application layer usually had to apply any transformations required, for instance lower-casing the input. running these types of terms through a ""normal"" query analysis chain leads to all sorts of interesting behavior so was avoided.",https://stackoverflow.com/questions/55140308,solr,13-03-2019 11:00,833.0,5.0,2.0,True,18-12-2021 11:14,13-03-2019 12:11
66625389,attributeerror: &#39;list&#39; object has no attribute &#39;size&#39; hugging-face transformers,"i am trying to use huggingface to transform stuff from english to hindi.
this is the code snippet
from transformers import autotokenizer, automodelforseq2seqlm

tokenizer = autotokenizer.from_pretrained(""helsinki-nlp/opus-mt-en-hi"")

model = automodelforseq2seqlm.from_pretrained(""helsinki-nlp/opus-mt-en-hi"")
text = ""hello my friends! how are you doing today?""
tokenized_text = tokenizer.prepare_seq2seq_batch([text])

# perform translation and decode the output
translation = model.generate(**tokenized_text)
translated_text = tokenizer.batch_decode(translation, skip_special_tokens=true)[0]

# print translated text
print(translated_text)

i am getting this error while trying to call the method generate on 'model'.

attributeerror: 'list' object has no attribute 'size'.

i am running on transformer version 4.3.3.","['python-3.x', 'nlp', 'huggingface-transformers']",66626813,"the model requires pytorch tensors and not a python list. simply add return_tensors='pt' to prepare_seq2seq:
from transformers import autotokenizer, automodelforseq2seqlm

tokenizer = autotokenizer.from_pretrained(""helsinki-nlp/opus-mt-en-hi"")

model = automodelforseq2seqlm.from_pretrained(""helsinki-nlp/opus-mt-en-hi"")
text = ""hello my friends! how are you doing today?""
tokenized_text = tokenizer.prepare_seq2seq_batch([text], return_tensors='pt')

# perform translation and decode the output
translation = model.generate(**tokenized_text)
translated_text = tokenizer.batch_decode(translation, skip_special_tokens=true)[0]

# print translated text
print(translated_text)

output:
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï",https://stackoverflow.com/questions/66625389,python-3.x,14-03-2021 14:00,13911.0,6.0,1.0,True,14-03-2021 16:21,14-03-2021 16:18
78766967,how do i pass streaming data from nitro api to pinia store so i can use it in nuxt component?,"in my nuxt app i have a pinia store and am making an api call via the nitro server that gets streaming data in response. when i try to update the pinia store with the streaming data it does not update, i.e. i can call
const streamoutput = ref({ value: """", annotations: [] })
const setstreamoutput = (output) => {
    console.log(""setstreamoutput"", output)
    streamoutput.value = output
    console.log(""setstreamoutput"", output)
}

and will see the console log but in my server logs, not in browser and in my template and in the browser tool i still see the initial value for streamoutput.
export default defineeventhandler(async (event) => {
    const config = useruntimeconfig()
    const body = await readbody(event)
    const chatstore = usechatstore()
    const readable = new readable({
        read() {} // no-op implementation for read method
    })

    const run = sendstream(
        event,
        await openai.beta.threads
            .createandrunstream({
                assistant_id: *id*,
                thread: {
                    messages: [
                        { role: ""user"", content: ""explain deep learning to a 5 year old."" }
                    ]
                }
            })
            .on(""messagecreated"", async (text) => {
                console.log(""\n\n messagecreated"", text)
            })
            .on(""textdelta"", async (textdelta, snapshot) => {
                console.log(""\n\n textdelta"", snapshot)
                await chatstore.setstreamoutput(snapshot)
            })
            .on(""toolcallcreated"", async (toolcall) => {
                console.log(""\n\n toolcallcreated"", toolcall)
            })
            .on(""toolcalldelta"", async (toolcalldelta, snapshot) => {
                console.log(""\n\n toolcalldelta"", snapshot)
                if (toolcalldelta.type === ""code_interpreter"") {
                    if (toolcalldelta.code_interpreter.input) {
                        chatstore.setstreamoutput(
                            chatstore.streamoutput + toolcalldelta.code_interpreter.input
                        )
                    }
                    if (toolcalldelta.code_interpreter.outputs) {
                        chatstore.setstreamoutput(chatstore.streamoutput + ""\noutput >\n"")
                        toolcalldelta.code_interpreter.outputs.foreach((output) => {
                            if (output.type === ""logs"") {
                                chatstore.setstreamoutput(
                                    chatstore.streamoutput + `\n${output.logs}\n`
                                )
                            }
                        })
                    }
                }
            })
            .on(""textdone"", async (content, snapshot) => {
                console.log(""\n\n text done"")
            })
    )

    return sendstream(event, readable)
})","['vue.js', 'nuxt.js', 'openai-api', 'pinia', 'nitro']",78790762,"for anyone running into this the solution has two parts:

creating and returning a readable
updating your pinia store to read the data stream

for creating a readable
import { readable } from ""stream""
import { sendstream } from ""h3""

const readable = new readable({
        read() {} // no-op implementation for read method
    })
await openai.beta.threads
        .createandrunstream({
            ...
        })
        .on(""messagecreated"", async (text) => {
            readable.push(`{""messagecreated"": ${json.stringify(text)}}\n`)
        })
//handle other events similarly
...
return sendstream(event, readable)
})

for updating your pinia store to read the data stream
(this is a bit tedious and i'm sure it can be optimized but i will forget about this answer in a bit and figure i should post before that)
const streamoutput = ref({ value: """", annotations: [] })

const setstreamoutput = (output) => {
    streamoutput.value = output
}

const createrun = async () => {
    try {
        const response = await fetch(""/api/run/create"", {
            method: ""post"",
            body: json.stringify({
                threadid: thread.value.id
            })
        })

        if (!response.ok) {
            throw new error(""network response was not ok"")
        }

        const reader = response.body.getreader()
        const decoder = new textdecoder(""utf-8"")

        let buffer = """"

        let done = false
        while (!done) {
            const { value, done: readerdone } = await reader.read()
            done = readerdone
            if (value) {
                buffer += decoder.decode(value, { stream: true })

                // process complete json objects in the buffer
                let boundary
                while ((boundary = buffer.indexof(""\n"")) !== -1) {
                    const chunk = buffer.slice(0, boundary).trim()
                    buffer = buffer.slice(boundary + 1)

                    if (chunk) {
                        try {
                            const json = json.parse(chunk)
                            if (json.textdelta) {
                                // vvvvv here you can hook into your store's function to use the data
                                setstreamoutput(json.textdelta.value)
                            } else if (json.messagecreated) {
                                ... // handle other streaming events",https://stackoverflow.com/questions/78766967,vue.js,18-07-2024 23:06,311.0,1.0,1.0,True,24-07-2024 22:05,19-07-2024 16:09
73188799,gensim 4.2.0 downloader function is missing,"i'm using the gensim package. however, when i want to load the word2vec model, the gensim.downloader function seems not to exist.
w2v = gensim.downloader.load('word2vec-google-news-300')

got error message:
attributeerror: module 'gensim' has no attribute 'downloader'

i checked the directory of gensim using dir() method and here's what i got:
['__builtins__','__cached__','__doc__','__file__','__loader__','__name__','__package__','__path__','__spec__','__version__','_matutils','corpora','interfaces','logger','logging','matutils','models','parsing','similarities','topic_coherence','utils']

seems like the downloader method is not in the directory. i wonder if there's another way to download a specific pretrained model with gensim library and also what's wrong with the gensim downloader.
my gensim version is 4.2.0.","['python', 'nlp', 'gensim', 'word2vec']",73188926,"if you're following some example code, you should copy its imports & code exactly. i don't think you'll find any docs/examples suggesting to use the gensim.downloader module the way you've attempted.
more generally: i recommend against using gensim.downloader. it hides the actual sources, local paths, & return types of the data it retrieves, and also runs new code, from the net, that's not part of the gensim project source-control nor part of versioned gensim releases. (it's a sketchy software-engineering practice.)
instead, download the googlenews dataset directly from some host, saving the exact original file(s) to a specific place of your choosing. examine the downloads to understand their filenames/formats (decompressing if necessary).
then use other gensim methods ï¿½ï¿½ï¿½ such as keyedvectors.load_word2vec_format() ï¿½ï¿½ï¿½ to load from a specific known local file path, with a returned object of a specific documented tyour code (and your own understanding) will be more clear, robust, & secure.",https://stackoverflow.com/questions/73188799,python,01-08-2022 04:19,473.0,0.0,1.0,True,01-08-2022 14:14,01-08-2022 14:14
67857840,how to access to fasttext classifier pipeline?,"as we know facebook's fasttext is a great open-source, free, lightweight library which can be used for text classification. but here a problem is the pipeline seem to be end-to end black-box. yes, we can change the hyper-parameters from these options for setting training configuration. but i couldn't manage to find a way to access to the vector embedding it generates internally.
actually i want to do some manipulation on the vector embedding - like introducing tf-idf weighting apart from these word2vec representations and another thing i want to to is oversampling using smote which requires numerical representation. for these reasons i need to introduce my custom code in between the overall pipeline which seems to be inaccessible for me. how introduce custom steps in this pipeline?","['machine-learning', 'nlp', 'pipeline', 'word2vec', 'fasttext']",67861368,"the full source code is available:

so, you can make any changes or extensions you can imagine - if you're comfortable reading & modifying its c++ source code. nothing is hidden or inaccessible.
note that both fasttext, and its supervised classification mode, are chiefly conventions for training a shallow neural-network. it may not be helpful to think of it as a ""pipeline"" like in the architecture of other classifier libraries - as none of the internal interfaces use that sort of language or modular layout.
specifically, if you get the gist of word2vec training, fasttext classifier mode really just replaces attempted-predictions of neighboring (in-context-window) vocabulary words, with attempted-predictions of known labels instead.
for the sake of understanding fasttext's relationship to other techniques, and potential aspects for further extension, i think it's useful to also review:

this skeptical blog post comparing fasttext to the much-earlier 'vowpal wabbit' tool: ""fast & easy baseline text categorization with vw""
facebook's far-less discussed extension of such vector-training for more generic categorical or numerical tasks, ""starspace""",https://stackoverflow.com/questions/67857840,machine-learning,06-06-2021 09:55,617.0,0.0,1.0,True,06-06-2021 16:30,06-06-2021 11:14
72675260,"in the rouge metrics, what do the low, mid and high values mean?","the rouge metrics were introduced to ""automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans"" [1].
when calculating any rouge metric you get an aggregatescore object with 3 parameters: low, mid, high.
how are these aggregate values calculated?
for example, from the huggingface implementation [2]:
>>> rouge = evaluate.load('rouge')
>>> predictions = [""hello there"", ""general kenobi""]
>>> references = [""hello there"", ""general kenobi""]
>>> results = rouge.compute(predictions=predictions,
...                         references=references)
>>> print(list(results.keys()))
['rouge1', 'rouge2', 'rougel', 'rougelsum']
>>> print(results[""rouge1""])
aggregatescore(low=score(precision=1.0, recall=1.0, fmeasure=1.0), mid=score(precision=1.0, recall=1.0, fmeasure=1.0), high=score(precision=1.0, recall=1.0, fmeasure=1.0))
>>> print(results[""rouge1""].mid.fmeasure)
1.0


edit: on july 7th, the huggingface  implementation was simplified to return a cleaner and easier to understand dict:","['machine-learning', 'nlp', 'data-science', 'evaluation', 'summarization']",72675261,"given a list of (summary, gold_summary) pairs, any rouge metric is calculated per each item in the list. in huggingface, you can opt-out of the aggregation part by adding  use_aggregator=false and get these values returned.
for the aggregation, a bootstrap resampling is used [1, 2]. bootstrap resampling is a technique used to extract confidence intervals [3, 4]. the idea is that for n samples, you draw x times a sample with replacement of size n, and then calculate some statistic for each resample. now you get a new distribution called the empirical bootstrap distribution, which can be used to extract confidence intervals.
in the rouge implementation by google [4], they used:

n for the number of resamples to run
mean for the resample statistic
2.5th, 50th and 97.5th percentiles to calculate the values for low, mid and high, respectively (can be controlled with the confidence_interval param)

randomness in rouge
note that due to the bootstrapping technique used in rouge, it is non-deterministic, and can return different results for each run (see [5]). if you don't want to opt out from using the bootstrapping technique, you can set the seed in the load function, as such: evaluate.load('rouge', seed=42).",https://stackoverflow.com/questions/72675260,machine-learning,19-06-2022 08:10,2157.0,2.0,1.0,True,04-06-2023 11:48,25-11-2022 09:32
53870599,disabling part of the nlp pipeline,"i am running spacy v2.x on a windows box with python3. i do not have admin privelages, so i have to call the pipeline as:
nlp = en_core_web_sm.load()
when i run my same script on a *nix box, i can load the pipeline as:
nlp = spacy.load('en', disable = ['ner', 'tagger', 'parser', 'textcat'])
all i am do is tokenizing, so i do not need the entire pipeline. on the windows box, if i load the pipeline like:
nlp = en_core_web_sm.load(disable = ['ner', 'tagger', 'parser', 'textcat'])
does that actually disable the components?
spacy information on the nlp pipeline","['python-3.x', 'nlp', 'spacy']",53885009,"you can check the current pipeline components by
print(nlp.pipe_names)

if you are not convinced by the output, you can manually check by trying to use the component and try to print the output. e.g try to disable parser and print dependency tags.",https://stackoverflow.com/questions/53870599,python-3.x,20-12-2018 14:28,15856.0,18.0,2.0,True,30-08-2023 05:56,05-10-2020 15:39
72357588,the added layer must be an instance of class layer,"i am merging two embding layers for two lstm models as follows:
code here in this image
when i was building the sequential model, it gave me an error.
model = sequential()
merged = concatenate(axis=1)([s1rnn.output,s2rnn.output])
model.add(merged)
model.add(dense(1))
model.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])
model.fit([x1,x2], y,batch_size=128, nb_epoch=20, validation_split=0.05)


typeerror: the added layer must be an instance of class layer. received: layer=kerastensor(type_spec=tensorspec(shape=(none, 110, 1), dtype=tf.float32, name=none), name='concatenate/concat:0', description=""created by layer 'concatenate'"") of type <class 'keras.engine.keras_tensor.kerastensor'>.","['keras', 'nlp', 'lstm']",72357625,"your model in which you want to add the concatenate() layer as input needs to be of functional() not sequential() type (that would be a first step to modify your code).
the structure should look something like (notice the brackets at the end: how you add layers in the functional api):
input_s1rnn= input(shape=(...))
input_s2rnn= input(shape=(...))
merged = concatenate([s1_rnnmodel(input_s1rnn), s2_rnnmodel(input_s2rnn)],axis=1)
layer_1_model = some_layer()(merged)
layer_2_model = some_layer()(layer_1_model)
...
output_layer = dense(1,activation='sigmoid')(layer_2_model)
model= model([input_s1rnn, input_s2rnn], output_layer)",https://stackoverflow.com/questions/72357588,keras,24-05-2022 05:27,940.0,0.0,1.0,True,24-05-2022 05:32,24-05-2022 05:32
9294926,"how does apple find dates, times and addresses in emails?","in the ios email client, when an email contains a date, time or location, the text becomes a hyperlink and it is possible to create an appointment or look at a map simply by tapping the link. it not only works for emails in english, but in other languages also. i love this feature and would like to understand how they do it. 
the naive way to do this would be to have many regular expressions and run them all. however i  this is not going to scale very well and will work for only a specific language or date format, etc. i think that apple must be using some concept of machine learning to extract entities (8:00pm, 8pm, 8:00, 0800, 20:00, 20h, 20h00, 2000 etc.).
any idea how apple is able to extract entities so quickly in its email client? what machine learning algorithm would you to apply accomplish such task?","['machine-learning', 'nlp', 'information-extraction', 'named-entity-recognition']",9344555,"they likely use information extraction techniques for this.
here is a demo of stanford's sutime tool:

you would extract attributes about n-grams (consecutive words) in a document:

numberofletters
numberofsymbols
length
previousword
nextword
nextwordnumberofsymbols
...

and then use a classification algorithm, and feed it positive and negative examples:
observation  nletters  nsymbols  length  prevword  nextword ispartofdate  
""feb.""       3         1         4       ""wed""     ""29th""   true  
""dec""        3         0         3       ""company"" ""went""   false  
...

you might get away with 50 examples of each, but the more the merrier. then, the algorithm learns based on those examples, and can apply to future examples that it hasn't seen before.
it might learn rules such as

if previous word is only characters and maybe periods...
and current word is in ""february"", ""mar."", ""the"" ...
and next word is in ""twelfth"", any_number ...
then is date

here is a decent video by a google engineer on the subject",https://stackoverflow.com/questions/9294926,machine-learning,15-02-2012 14:12,25936.0,133.0,6.0,True,16-05-2023 23:09,30-09-2012 20:36
75839825,how to prevent transformer generate function to produce certain words?,"i have the following code:
from transformers import t5tokenizer, t5forconditionalgeneration

tokenizer = t5tokenizer.from_pretrained(""t5-small"")
model = t5forconditionalgeneration.from_pretrained(""t5-small"")

input_ids = tokenizer(""the <extra_id_0> walks in <extra_id_1> park"", return_tensors=""pt"").input_ids

sequence_ids = model.generate(input_ids)
sequences = tokenizer.batch_decode(sequence_ids)
sequences

currently it produces this:
['<pad><extra_id_0> park offers<extra_id_1> the<extra_id_2> park.</s>']

is there a way to prevent the generator to produce certain words (e.g. stopwords = [""park"", ""offer""])?","['python', 'nlp', 'huggingface-transformers', 'generative-pretrained-transformer']",75863052,"after looking at the docs found out there is a bad_words_ids parameter that you can pass in the generate()
given a bad word list you can create the id list using
tokenizer(bad_words, add_special_tokens=false).input_ids

input_ids = tokenizer(""the <extra_id_0> walks in <extra_id_1> park"", return_tensors=""pt"").input_ids
bad_words = [""park"", ""offers""]
bad_words_ids = tokenizer(bad_words, add_special_tokens=false).input_ids 
#[[2447], [704]]

sequence_ids = model.generate(input_ids, bad_words_ids=bad_words_ids)
#tensor([[    0, 32099,  1061,    19,     3,     9,   710,  1482,   550,    45, 32098,     8, 32097,  1061,     5,     1]])

sequences = tokenizer.batch_decode(sequence_ids)
print(sequences) 
#['<pad><extra_id_0> park is a short walk away from<extra_id_1> the<extra_id_2> park.</s>']


notice how the word ""park"" is appearing now. this is because the tokenizer identifies park (id 2447) and park (id 1061) as 2 different tokens. this may depend on the tokenizer you use (there are case-insensitive tokenizers). if you don't want this to happen you can add park into the bad word list as well.
colab demo",https://stackoverflow.com/questions/75839825,python,25-03-2023 05:39,1128.0,3.0,1.0,True,29-03-2023 15:07,28-03-2023 02:18
73012354,using &#39;isin&#39; in python for three filters,"i have the following dataframe
# import pandas library
import pandas as pd
import numpy as np

# initialize list elements
data = ['george',
        'instagram',
        'nick',
        'basketball',
        'tennis']
  
# create the pandas dataframe with column name is provided explicitly
df = pd.dataframe(data, columns=['unique words'])
  
# print dataframe.
df

and i want to create a new column based on the following two lists that looks like this
key_words = [""football"", ""basketball"", ""tennis""]
usernames = [""instagram"", ""facebook"", ""snapchat""]

label
-----
0
2
0
1
1

so the words that are in the list key_words take the label 1, in the list usernames take the label 2 and all the other the label 0.
thank you so much for your time and help!","['python', 'numpy', 'nlp']",73012462,"one way to do this is to create a label map by numbering all of the elements in the first list as 1, and the other as 2.  then you can use .map in pandas to map the values and fillna with 0.
# import pandas library
import pandas as pd
import numpy as np

# initialize list elements
data = ['george',
        'instagram',
        'nick',
        'basketball',
        'tennis']
  
# create the pandas dataframe with column name is provided explicitly
df = pd.dataframe(data, columns=['unique words'])
  
key_words = [""football"", ""basketball"", ""tennis""]
usernames = [""instagram"", ""facebook"", ""snapchat""]


label_map = {e: i+1 for i, l in enumerate([key_words,usernames]) for e in l}
print(label_map)

df['label'] = df['unique words'].map(label_map).fillna(0).astype(int)

print(df)

output
{'football': 1, 'basketball': 1, 'tennis': 1, 'instagram': 2, 'facebook': 2, 'snapchat': 2}

  unique words  label
0       george      0
1    instagram      2
2         nick      0
3   basketball      1
4       tennis      1",https://stackoverflow.com/questions/73012354,python,17-07-2022 13:52,49.0,-1.0,1.0,True,17-07-2022 14:06,17-07-2022 14:06
68489759,huggingface ner with custom data,"i have a csv data as below.
**token**      **label**
0.45""      length
1-12       size
2.6""       length
8-9-78     size
6mm        length

whenever i get the text as below
6mm 8-9-78 silver head

i should be able to say length = 6mm and size = 8-9-78. i'm new to nlp world, i'm trying to solve this using huggingface ner. i have gone through various articles. i'm not getting how to train with my own data. which model/tokeniser should i make use of? or should i build my own? any help would be appreciated.","['python', 'nlp', 'huggingface-transformers', 'named-entity-recognition']",68952649,i had two options one is spacy (as suggested by @scarpacci) and other one is sparknlp. i opted for sparknlp and found a solution. i formatted the data in conll format and trained using spark's nerdlapproach and glove word embedding.,https://stackoverflow.com/questions/68489759,python,22-07-2021 18:16,1081.0,1.0,2.0,True,27-08-2021 11:28,23-07-2021 10:07
75490436,what causes this attribute error encountered when implementing langchain&#39;s openai llm wrapper?,"this is my first post here. i'm building a python window application with pyqt5 that implements interactions with the openai completions endpoint. so far, any code that i've written myself has performed fine, and i was reaching the point where i wanted to start implementing long-term memory for conversational interactions. i started by just running my own chain of prompts for categorizing and writing topical subjects and summaries to text files, but i decided it best to try exploring open source options to see how the programming community is managing things. this led me to langchain, which seems to have some popular support behind it and already implements many features that i intend.
however, i have not had even the tiniest bit of success with it yet. even the most simple examples don't perform, regardless of what context i'm implementing it in (within a class, outside a class, in an asynchronous loop, to the console, to my text browsers within the main window, whatever) i always get the same error message.
the simplest possible example:
import os
from langchain.llms import openai
from local import constants #for api key
os.environ[""openai_api_key""] = constants.openai_api_key
davinci = openai(model_name= 'text-davinci-003', verbose=true, temperature=0.6)
text = ""write me a story about a guy who is frustrated with python.""
print(""prompt: "" + text)
print(davinci(text))

it capably instantiates the wrapper and prints the prompt to the console, but at any point a command is sent through the wrapper's functions to receive generated text, it encounters this attributeerror.
here is the traceback:
traceback (most recent call last):
  file ""d:\dropbox\pycharm projects\workspace\main.py"", line 16, in <module>
    print(davinci(text))
  file ""d:\dropbox\pycharm projects\workspace\venv\lib\site-packages\langchain\llms\base.py"", line 255, in __call__
    return self.generate([prompt], stop=stop).generations[0][0].text
  file ""d:\dropbox\pycharm projects\workspace\venv\lib\site-packages\langchain\llms\base.py"", line 128, in generate
    raise e
  file ""d:\dropbox\pycharm projects\workspace\venv\lib\site-packages\langchain\llms\base.py"", line 125, in generate
    output = self._generate(prompts, stop=stop)
  file ""d:\dropbox\pycharm projects\workspace\venv\lib\site-packages\langchain\llms\openai.py"", line 259, in _generate
    response = self.completion_with_retry(prompt=_prompts, **params)
  file ""d:\dropbox\pycharm projects\workspace\venv\lib\site-packages\langchain\llms\openai.py"", line 200, in completion_with_retry
    retry_decorator = self._create_retry_decorator()
  file ""d:\dropbox\pycharm projects\workspace\venv\lib\site-packages\langchain\llms\openai.py"", line 189, in _create_retry_decorator
    retry_if_exception_type(openai.error.timeout)
attributeerror: module 'openai.error' has no attribute 'timeout'

i don't expect that there is a fault in the langchain library, because it seems like nobody else has experienced this problem. i imagine i may have some dependency issue? or i do notice that others using the langchain library are doing so in a notebook development environment, and my lack of familiarity in that regard is making me overlook some fundamental expectation of the library's use?
any advice is welcome! thanks!
what i tried: i initially just replaced my own function for managing calls to the completion endpoint with one that issued the calls through langchain's llm wrapper. i expected it to work as easily as my own code had, but i received that error. i then stripped everything apart layer by layer attempting to instantiate the wrapper at every scope of the program, then i attempted to make the calls in an asynchronous function through a loop that waited to completion, and no matter what, i always get that same error message.","['python', 'dependencies', 'openai-api']",75494401,"i think it might be something about your current installed versions of python, openai, and/or langchain. maybe try using a newer version of python and openai. i'm new to python and these things but hopefully i could help.",https://stackoverflow.com/questions/75490436,python,18-02-2023 00:42,3924.0,1.0,1.0,True,06-04-2023 14:59,18-02-2023 00:43
78163924,openai assistant run always fails when called from php,"the runs i create with the openai-php library fail direct in 100% of cases. what am i doing wrong? i do not have much experience with php but this is the test script.
<?php
require '../../../dev/vendor/autoload.php';

$apikey = getenv('assistant_api_key');

@ini_set('zlib.output_compression',0);
@ini_set('implicit_flush',1);
@ob_end_clean();
set_time_limit(0);
ob_implicit_flush(1);

$client = openai::client($apikey);

echo ""creating thread...<br/>"";
$response = $client->threads()->create([]);

$array = $response->toarray(); 
$threadid = $response->id;

echo ""thread created, id:"" . $response->id . ""<br/>"";

echo ""adding message hello world...<br/>"";

$response = $client->threads()->messages()->create($threadid, 
    [
        'role' => 'user',
        'content' => 'hello world!',
    ]
);

echo ""message created, id:"" . $response->id . ""<br/>"";

echo ""creating a run...<br/>"";

$response = $client->threads()->runs()->create(
    threadid: $threadid, 
    parameters: [
        'assistant_id' => 'asst_wyzcgybrwvpcky3g4o0fem5a',
    ],
);

echo ""run created, id:"" . $response->id . ""<br/>"";
$runid = $response->id;

echo ""waiting for run result...<br/>"";

for ($i=0; $i <= 20; $i++) {
    $response = $client->threads()->runs()->retrieve(
        threadid: $threadid,
        runid: $runid,
    );
    $runstatus = $response->status;
    if(strcmp($runstatus, ""in_progress"")==0){
        echo ""in progres... <br/>"";
    }

    if(strcmp($runstatus, ""failed"")==0){
        echo ""failed... <br/>"";
        echo json_encode($response->toarray(), json_pretty_print);
        echo ""<br/>"";
        break;
    }
    if(strcmp($runstatus, ""completed"") ==0){
        echo ""in completed... <br/>"";
        break;
    }
    sleep(1); 
}

echo ""run complete<br/>"";

?>

the result:
creating thread...
thread created, id:thread_zvhjywzjoyxf04cr6bjbw67c
adding message hello world...
message created, id:msg_z1y6ei4j9b5eki48btszhvot
creating a run...
run created, id:run_jr2oc8y3wlqws8jwfcizk70i
waiting for run result...
in progres...
failed...
{
  ""id"": ""run_jr2oc8y3wlqws8jwfcizk70i"",
  ""object"": ""thread.run"",
  ""created_at"": 1710458772,
  ""assistant_id"": ""asst_wyzcgybrwvpcky3g4o0fem5a"",
  ""thread_id"": ""thread_zvhjywzjoyxf04cr6bjbw67c"",
  ""status"": ""failed"",
  ""started_at"": 1710458772,
  ""expires_at"": null,
  ""cancelled_at"": null,
  ""failed_at"": 1710458773,
  ""completed_at"": null,
  ""last_error"": {
    ""code"": ""server_error"",
    ""message"": ""sorry, something went wrong.""
  },
  ""model"": ""gpt-4-1106-preview"",
  ""instructions"": ""you're helping debugging why the api calls to you are not working. please share everything , all technical details you know about the interaction and the communication aspects. "",
  ""tools"": [],
  ""file_ids"": [],
  ""metadata"": [],
  ""usage"": {
    ""prompt_tokens"": 0,
    ""completion_tokens"": 0,
    ""total_tokens"": 0
  }
}
run complete

when i use the api to create the tread and message and use the platform.openai.com website the run completes fine. (i used an online json formatter to format it nicely as above.)
the error message is: sorry, something went wrong.
thanks for any help!","['php', 'openai-api']",78171459,"the problem was with openai and not with the code. i used an api key restricted to threads write access, thread write access and assistant read access and these did not work.
i set the api key to full access and now it works.",https://stackoverflow.com/questions/78163924,php,14-03-2024 23:11,554.0,0.0,1.0,True,16-03-2024 10:48,16-03-2024 10:48
72252937,nlp using xlm dataset,"i am trying to do nlp on the dataset consisting of the following row
00001 b 74457
00002 c 12804123 16026213 14627885
00004 a 15329425 9058342 11279767

where 1st element in the row is the identifier  2nd on is a label recommends, it can have only three labels $a, b, c$ and the number for examples 12804123 represent the id of the xml, it contains data, for example, text, location, etc. based on this i need to extract the data from the xml file and use it to make a model. so first of all i want to extract some of the data from the xml file and make a data frame of structure data. an example of the xml file is below.
when i run the command pd.read_xml(xml) it gives
    medlinecitation     pubmeddata
0   nan     nan

any example from kaggle or any other source etc i can follow to do the analysis.
74457.xml = '''
<pubmedarticleset>
<pubmedarticle>
<medlinecitation owner=""nlm"" status=""medline"">
<pmid version=""1""> 74457 </pmid>
<datecreated>
<year> 1978 </year>
<month> 03 </month>
<day> 21 </day>
</datecreated>
<datecompleted>
<year> 1978 </year>
<month> 03 </month>
<day> 21 </day>
</datecompleted>
<daterevised>
<year> 2007 </year>
<month> 11 </month>
<day> 15 </day>
</daterevised>
<article pubmodel=""print"">
<journal>
<issn issntype=""print""> 0140-6736 </issn>
<journalissue citedmedium=""print"">
<volume> 1 </volume>
<issue> 7984 </issue>
<pubdate>
<year> 1976 </year>
<month> sep </month>
<day> 4 </day>
</pubdate>
</journalissue>
<title> lancet </title>
<isoabbreviation> lancet </isoabbreviation>
</journal>
<articletitle>
prophylactic treatment of alcoholism by lithium carbonate. a controlled study.
</articletitle>
<pagination>
<medlinepgn> 481-2 </medlinepgn>
</pagination>
<abstract>
<abstracttext>
lithium therapy has been shown to have a therapeutic influence in reducing the drinking and incapacity by alcohol in depressive alcoholics in a prospective double-blind placebo-controlled trial conducted over one year, but it had no significant effect on non-depressed patients. patients in the trial treated by placebo had significantly greater alcoholic morbidity if they were depressive than if they were non-depressive.
</abstracttext>
</abstract>
<authorlist completeyn=""y"">
<author validyn=""y"">
<lastname> merry </lastname>
<forename> j </forename>
<initials> j </initials>
</author>
<author validyn=""y"">
<lastname> reynolds </lastname>
<forename> c m </forename>
<initials> cm </initials>
</author>
<author validyn=""y"">
<lastname> bailey </lastname>
<forename> j </forename>
<initials> j </initials>
</author>
<author validyn=""y"">
<lastname> coppen </lastname>
<forename> a </forename>
<initials> a </initials>
</author>
</authorlist>
<language> eng </language>
<publicationtypelist>
<publicationtype> clinical trial </publicationtype>
<publicationtype> comparative study </publicationtype>
<publicationtype> journal article </publicationtype>
<publicationtype> randomized controlled trial </publicationtype>
</publicationtypelist>
</article>
<medlinejournalinfo>
<country> england </country>
<medlineta> lancet </medlineta>
<nlmuniqueid> 2985213r </nlmuniqueid>
<issnlinking> 0140-6736 </issnlinking>
</medlinejournalinfo>
<chemicallist>
<chemical>
<registrynumber> 0 </registrynumber>
<nameofsubstance> placebos </nameofsubstance>
</chemical>
<chemical>
<registrynumber> 7439-93-2 </registrynumber>
<nameofsubstance> lithium </nameofsubstance>
</chemical>
</chemicallist>
<citationsubset> aim </citationsubset>
<citationsubset> im </citationsubset>
<meshheadinglist>
<meshheading>
<descriptorname majortopicyn=""n""> adult </descriptorname>
</meshheading>
<meshheading>
<descriptorname majortopicyn=""n""> alcohol drinking </descriptorname>
</meshheading>
<meshheading>
<descriptorname majortopicyn=""n""> alcoholism </descriptorname>
<qualifiername majortopicyn=""y""> drug therapy </qualifiername>
</meshheading>
<meshheading>
<descriptorname majortopicyn=""n""> clinical trials as topic </descriptorname>
</meshheading>
<meshheading>
<descriptorname majortopicyn=""n""> depression </descriptorname>
<qualifiername majortopicyn=""n""> chemically induced </qualifiername>
<qualifiername majortopicyn=""y""> prevention & control </qualifiername>
</meshheading>
<meshheading>
<descriptorname majortopicyn=""n""> double-blind method </descriptorname>
</meshheading>
<meshheading>
<descriptorname majortopicyn=""n""> drug evaluation </descriptorname>
</meshheading>
<meshheading>
<descriptorname majortopicyn=""n""> female </descriptorname>
</meshheading>
<meshheading>
<descriptorname majortopicyn=""n""> humans </descriptorname>
</meshheading>
<meshheading>
<descriptorname majortopicyn=""n""> lithium </descriptorname>
<qualifiername majortopicyn=""y""> therapeutic use </qualifiername>
</meshheading>
<meshheading>
<descriptorname majortopicyn=""n""> male </descriptorname>
</meshheading>
<meshheading>
<descriptorname majortopicyn=""n""> middle aged </descriptorname>
</meshheading>
<meshheading>
<descriptorname majortopicyn=""n""> placebos </descriptorname>
</meshheading>
</meshheadinglist>
</medlinecitation>
<pubmeddata>
<history>
<pubmedpubdate pubstatus=""pubmed"">
<year> 1976 </year>
<month> 9 </month>
<day> 4 </day>
</pubmedpubdate>
<pubmedpubdate pubstatus=""medline"">
<year> 1976 </year>
<month> 9 </month>
<day> 4 </day>
<hour> 0 </hour>
<minute> 1 </minute>
</pubmedpubdate>
<pubmedpubdate pubstatus=""entrez"">
<year> 1976 </year>
<month> 9 </month>
<day> 4 </day>
<hour> 0 </hour>
<minute> 0 </minute>
</pubmedpubdate>
</history>
<publicationstatus> ppublish </publicationstatus>
<articleidlist>
<articleid idtype=""pubmed""> 74457 </articleid>
</articleidlist>
</pubmeddata>
</pubmedarticle>
</pubmedarticleset>'''

please help me to understand what is happening? and how can i make it a data frame?","['python', 'pandas', 'xml', 'jupyter-notebook', 'nlp']",72336520,"here is one way to do it:
import pandas as pd

try:
    medlinecitation = pd.read_xml(""74457.xml"", xpath="".//medlinecitation"").dropna(
        axis=1
    )
except valueerror:
    medlinecitation = pd.dataframe()

try:
    pubmedpubdate = pd.read_xml(""74457.xml"", xpath="".//pubmedpubdate"")
except valueerror:
    pubmedpubdate = pd.dataframe()

df = pd.merge(
    left=medlinecitation,
    right=pubmedpubdate,
    how=""outer"",
    left_index=true,
    right_index=true,
).fillna(method=""ffill"")

print(df)
# output
  owner   status     pmid citationsubset pubstatus  year  month  day  hour  \
0   nlm  medline  74457.0             im    pubmed  1976      9    4   nan   
1   nlm  medline  74457.0             im   medline  1976      9    4   0.0   
2   nlm  medline  74457.0             im    entrez  1976      9    4   0.0   

   minute  
0     nan  
1     1.0  
2     0.0",https://stackoverflow.com/questions/72252937,python,15-05-2022 23:40,171.0,1.0,1.0,True,24-05-2022 15:37,16-05-2022 07:42
76239184,python text classification accuracy measurement inconsistency,"i'm trying to get accuracy, recall and precision measurements form nltk movie review corpus but i get three undesirable outcomes:



i follow nltk guide that has random.shuffle method, which makes accuracy, etc., different each time. this isn't good.




i delete the shuffle line, but precision and recall don't work anymore and show 0.0 and ""none"" respectively.




i delete the shuffle line and change the training and test sets to [500:1500] and [:1500] respectively, like in this thread: how to get the precision and recall from a nltk classifier? the recall and precision do work now, but by doing so, the testing set is larger than the training one, which works at first glance but i believe you can't do that.



import nltk
import random
import collections
from nltk.corpus import movie_reviews
from nltk.tokenize import word_tokenize

documents = [(list(movie_reviews.words(fileid)), category)
            for category in movie_reviews.categories()
            for fileid in movie_reviews.fileids(category)]
random.shuffle(documents)



all_words = nltk.freqdist(w.lower() for w in movie_reviews.words())
word_features = list(all_words)[:2000]

def document_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
        features[word] = (word in document_words)
    return features

featuresets = [(document_features(d), c) for (d,c) in documents]
train_set, test_set = featuresets[300:], featuresets[:300]
print ('train on', len(train_set), 'instances, test on', len(test_set), 'instances')
classifier = nltk.naivebayesclassifier.train(train_set)
print(nltk.classify.accuracy(classifier, test_set))
classifier.show_most_informative_features(10)

refsets = collections.defaultdict(set)
testsets = collections.defaultdict(set)
for i, (feats, label) in enumerate(test_set):
    refsets[label].add(i)
    observed = classifier.classify(feats)
    testsets[observed].add(i)
print('precision:', nltk.precision(refsets['pos'], testsets['pos']))
print('recall:', nltk.recall(refsets['pos'], testsets['pos']))
print('f_measure:', nltk.f_measure(refsets['pos'], testsets['pos']))

is there anything i can do, or am i just misunderstanding something?
edit: a sufficient solution is to use random.seed(x) before shuffling, or to average several runs. it doesn't explain, however, why deleting the shuffle breaks the program.","['python-3.x', 'nlp', 'nltk', 'text-classification']",76241250,"the reason why deleting the shuffle breaks the program is that the naivebayesclassifier implementation in nltk assumes that the data is randomly shuffled before splitting into training and testing sets. if you don't shuffle the data, the training and testing sets will have a biased distribution and may not generalize well to new data.
to ensure that you get consistent results, you can set the seed for the random number generator before shuffling the data. this will ensure that the shuffling is done in a deterministic way and you get the same train/test splits every time you run the code. you can use random.seed() function to set the seed.
here's an updated code with the random seed set to 42:
import nltk
import random
import collections

from nltk.corpus import movie_reviews
from nltk.tokenize import word_tokenize

random.seed(42)  # set the random seed for reproducibility

documents = [(list(movie_reviews.words(fileid)), category)
            for category in movie_reviews.categories()
            for fileid in movie_reviews.fileids(category)]

random.shuffle(documents)

all_words = nltk.freqdist(w.lower() for w in movie_reviews.words())
word_features = list(all_words)[:2000]

def document_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
        features[word] = (word in document_words)
    return features

featuresets = [(document_features(d), c) for (d,c) in documents]
train_set, test_set = featuresets[300:], featuresets[:300]
print ('train on', len(train_set), 'instances, test on', len(test_set), 'instances')

classifier = nltk.naivebayesclassifier.train(train_set)
print(nltk.classify.accuracy(classifier, test_set))
classifier.show_most_informative_features(10)

refsets = collections.defaultdict(set)
testsets = collections.defaultdict(set)
for i, (feats, label) in enumerate(test_set):
    refsets[label].add(i)
    observed = classifier.classify(feats)
    testsets[observed].add(i)
print('precision:', nltk.precision(refsets['pos'], testsets['pos']))
print('recall:', nltk.recall(refsets['pos'], testsets['pos']))
print('f_measure:', nltk.f_measure(refsets['pos'], testsets['pos']))

this should give you consistent results each time you run the code.",https://stackoverflow.com/questions/76239184,python-3.x,12-05-2023 18:52,81.0,0.0,1.0,True,13-05-2023 05:39,12-05-2023 22:03
75637545,accessing chatgpt api through firebase cloud function,"below is some code for a simple firebase cloud function that hits the chatgpt api. deploying this code and accessing it from my app results in a cors error.
import * as functions from ""firebase-functions"";
import {definestring} from ""firebase-functions/v2/params"";
import {configuration, openaiapi} from ""openai"";

const openaikey = definestring(""open_api_key"");

export const getsummary = functions. (data) => {
  const configuration = new configuration({
    apikey: openaikey.value(),
  });
  const openai = new openaiapi(configuration);
  const completion = await openai.createchatcompletion({
    model: ""gpt-3.5-turbo"",
    messages: [
      {
        role: ""user"",
        content: data.prompt,
      },
    ],
  });
  const [choice] = completion.data.choices;
  return {
    response: choice.message ?? ""no response"",
  };
});

this cloud function works perfectly when i access it from my app using the functions emulator. i only get the cors error when i deploy it to the cloud and try to use it.
also, i have a helloworld function deployed alongside this one so that i can check that there's nothing wrong with my whole functions setup, and it works fine also. furthermore, when i go into my cloud functions console and test the function directly, it also works. so the issue clearly has to do with accessing the api specifically via the cloud function production environment and specifically from the app.
update: here's the client code and the exact error:
const getsummary =  ""getsummary"");
async function askgpt() {
    const result = await getsummary({
      prompt: ""please summarize the question in the following text. phrase your response in the form of a question, and use markdown for any formatting you might need.\n\n"" + question.text
    });
    question.question_summary = (
      (question.question_summary ?? """") // @ts-ignore
      + (result?.data?.response?.content || """").trim()
    );
  }

error:

access to fetch at ' from origin ' has been blocked by cors policy: response to preflight request doesn't pass access control check: no 'access-control-allow-origin' header is present on the requested resource. if an opaque response serves your needs, set the request's mode to 'no-cors' to fetch the resource with cors disabled.","['javascript', 'firebase', 'google-cloud-functions', 'cors', 'openai-api']",76342002,"the best solution to this problem is to avoid using oncall when you have cors issues. instead:

come up with a name for a new firestore collection.
set up a cloud function that is triggered by the creation of a new document in that collection.
put the logic that talks to the openai api in the cloud function. when you have the response, write it to a collection in firestore (wherever you want).
from the client:

write to the aforementioned collection when you want to make a request.
subscribe to the collection that the cloud function writes to.



all together, a new request from the client causes this cascade:

a write to the firestore collection for requests.
a cloud function read of that document, plus an api call and any other necessary logic.
a write to a firestore collection for responses.
an update on the client in response to that document update.

the reason this alternative approach is required is that an oncall cloud function has different cors behavior than a cloud function triggered by a firestore event.",https://stackoverflow.com/questions/75637545,javascript,04-03-2023 17:54,2657.0,7.0,3.0,True,31-12-2023 04:16,04-03-2023 20:30
77936280,how to create embedding vectors on-premise for openai models?,"i want to create a chatbot for confidential company documents. due to security concerns, i want to make the embeddings locally and store the vectors locally. iï¿½ï¿½ï¿½ll use openaiï¿½ï¿½ï¿½s api to communicate with the llm. how do i make the embeddings locally without using openai's embeddings w","['openai-api', 'chatgpt-api', 'gpt-3', 'openaiembeddings']",77947613,"you can use spacy to create embedding vectors. as stated in the official spacy documentation:

spacy is a free, open-source library for advanced natural language
processing (nlp) in python.

if you run get_embedding.py, you'll get the following response (i.e., a 128-dimensional embedding vector):

[-0.4769258499145508, 0.3881034553050995, 2.819754123687744,
0.1371886432170868, 7.742671966552734, -1.2794740200042725, 3.1346824169158936, 1.4228293895721436, 0.27339574694633484, -0.4797571003437042, 6.9937872886657715, 1.4315242767333984, -3.4885427951812744, 0.8452857136726379, 0.8568257689476013, 2.3755757808685303, 5.252387523651123, 1.0385771989822388, -2.077688217163086, 0.7908112406730652, -2.546300172805786, 0.7618517279624939, -0.42370277643203735, 3.1288869380950928, 0.6323999762535095, -3.8961079120635986, -0.4977128207683563, -1.5293715000152588, -1.5151740312576294, 1.6068800687789917, -2.0303213596343994, -2.4576945304870605, 1.4395530223846436, 0.7422757148742676, -2.270634174346924, -0.15845993161201477, 0.07029717415571213, 0.672839343547821, 5.159962177276611, -0.06168988719582558, -3.129868745803833, -1.227286696434021, -2.006021499633789, 0.4333727955818176, -1.2434427738189697, 2.46277117729187, 3.537201404571533, 0.2767142653465271, -0.7451871633529663, -2.5755043029785156, -1.1589397192001343, 1.5788328647613525, -0.508418619632721, -2.740482807159424, -2.119898557662964, 0.5995656847953796, 1.1638715267181396, 4.050228595733643, -1.1868728399276733, -4.347542762756348, 4.015085697174072, -0.23206289112567902, 0.10843563079833984, -0.5687889456748962, -0.5912571549415588, 6.662228584289551, -2.3623156547546387, -4.9967570304870605, 3.283771514892578, 3.147571563720703, -2.7288429737091064, 2.373138666152954, -3.020965576171875, -0.8559457063674927, -0.9629656672477722, 1.2457185983657837, -3.6433043479919434, 3.081699848175049, -5.936647891998291, 3.153151273727417, -4.296724319458008, 0.23952282965183258, -0.616602897644043, 1.9953927993774414, 2.9439570903778076, 1.9284285306930542, 0.47489139437675476, -1.0409842729568481, 2.129765748977661, 1.1889699697494507, -2.2774386405944824, 0.35642704367637634, 3.420785903930664, -3.1786859035491943, -2.098905563354492, -2.9918370246887207, 2.8626744747161865, -1.6979585886001587, -1.1304199695587158, 0.6608514785766602, 3.7660014629364014, 2.038205623626709, 3.123993158340454, 3.6879427433013916, -2.20119309425354, 4.754899501800537, 4.687614440917969, -2.214437246322632, 0.32483574748039246, 0.5160357356071472, 4.57424259185791, 1.8791313171386719, -3.228891372680664, 2.3561816215515137, 1.2214956283569336, -1.1263086795806885, 0.9208157658576965, 0.022158537060022354, -2.070528507232666, 0.5088043808937073, -0.2275187224149704, -1.7481428384780884, 2.601383686065674, 2.5015127658843994, -0.7987513542175293, -3.5253543853759766, 0.6400442123413086, -1.7350285053253174]

get_embedding.py
import spacy

spacy_model = spacy.load('en_core_web_lg')

user_input = 'make an embedding vector of this text'

vector = spacy_model(user_input).vector[:128].tolist()

print(vector)",https://stackoverflow.com/questions/77936280,openai-api,04-02-2024 14:56,1040.0,1.0,1.0,True,22-02-2024 01:57,22-02-2024 01:57
73161662,how to get the word on which the text classification has been made?,"i am doing a multi-label text classification using a pre-trained model of bert. here is an example of the prediction that has been made for one sentence-
pred_image
i want to get those words from the sentence on which the prediction has been made. like this one - right_one
if anyone has any idea, please enlighten me.","['nlp', 'text-classification', 'bert-language-model', 'multilabel-classification']",73162168,"multi-label text classification (first image) and token classification (second image) are two different tasks for each which the model needs to be specifally trained for.
the first one returns a probability for each label considering the entire sentence. the second returns such predictions for each single word in the sentence while usually considering the rest of the sentence as context.
so you can not really use the output from a text classifier and use it for token classification because the information you get is not detailed enough.
what you can and should do is train a token classification model, although you obviously will need token-level-annotated data to do so.",https://stackoverflow.com/questions/73161662,nlp,29-07-2022 04:29,263.0,0.0,1.0,True,29-07-2022 05:54,29-07-2022 04:31
47011991,apache open nlp vs nltk,"we have a spring boot application integrated with node.js and socket.io chat application , to which we want to integrate natural language processing. not getting any direction on which of these two apache-opennlp or nltk would be a better choice for us as both of the frameworks offer the kind of processing we need. 
wrt to the features provided by the frameworks , they both are good. both have features that we are looking for. more than how to choose between features , what would suit our architecture better is a perspective i would like.. 
any suggestions ?","['architecture', 'nlp', 'nltk', 'opennlp']",47021689,"it is tough to answer a question about which product will meet your needs better without know what your needs are.  opennlp can perform tokenization, sentence detection, pos tagging, named entity detection, language detection, document classification, chunking, and sentence parsing.  it also has lower-level access to maximum entropy and naive-bayes classifiers.  i use opennlp often.  nltk appears to do the same stuff (i don't really use it, so i can't tell you all its benefits).  a small difference is that opennlp is java whereas nltk is python. so your preference can come into play. another difference is that nltk has build in methods for downloading corpora.
if you were a little more specific about what you wanted, people could give you better advice.",https://stackoverflow.com/questions/47011991,architecture,30-10-2017 09:38,3920.0,4.0,1.0,True,15-12-2021 10:57,31-10-2017 07:25
74943838,split several sentences in pandas dataframe,"i have a pandas dataframe with a column that looks like this.




sentences




['this is text.', 'this is another text.', 'this is also text.', 'even more text.']


['this is the same in another row.', 'another row another text.', 'text in second row.', 'last text in second row.']




in every row there are 10 sentences in ' ' or "" "" separated by commas. the column type is ""str"". i was not able to transform it to a list of strings.
i want to transform the values of this dataframe that they look like this:
[['this', 'is', 'text'], ['this', 'is', 'another', 'text'], ['this', 'is', 'also', 'text'], ['even', 'more', 'text']]

i tried something like this:
    new_splits = []
    for num in range(len(refs)):
      komma = refs[num].replace("" "", ""\', \'"")#regex=true)
      new_splits.append(komma)

and this:
    new_splits = []
    for num in range(len(refs)):
      splitted = refs[num].split(""', '"")
      new_splits.append(splitted)

disclaimer: i need this for evaluating bleu score and haven't found a way to do this for this kind of dataset. thanks in advance!","['python', 'pandas', 'nlp', 'bleu']",74944042,"you can use np.char.split in one line:
df['separated'] = np.char.split(df['sentences'].tolist()).tolist()

@kata if you think the sentences column type is str meaning the element in each row is a string instead of a list, for e.g. ""['this is text.', 'this is another text.', 'this is also text.', 'even more text.']"" then you need to try to convert them into lists first. one way is to use ast.literal_eval.
from ast import literal_eval
df['sentences'] = df['sentences'].apply(literal_eval)
df['separated'] = np.char.split(df['sentences'].tolist()).tolist()

note on data: this is not a recommended way of storing data. if possible fix the source from which data is coming. it needs to be strings in each cell not lists preferably, or at least just lists, and not a string representing list.",https://stackoverflow.com/questions/74943838,python,28-12-2022 18:37,168.0,0.0,2.0,True,29-12-2022 12:34,28-12-2022 19:01
72926089,how make multi word entities a single word?,"i have a pandas column with multiple word entities. i want to make the entities labeled as person a single word. my input looks like this:
    text
 vote for donald trump
 vote for barack obama
 vote for bernie sanders
 move to another location
 support lavaughn robinson
 support michelle lavaughn robinson
 support sanders

i need my output to look like this:
   text
vote for donald_trump
vote for barack_obama
vote for bernie_sanders
move to another location
support lavaughn_robinson
support michelle_lavaughn_robinson
support sanders

my first though was to use spacy ner, return the person and later combined the words returned, but i'm getting words that are not ner.  can i do it using bilou? is there any other way to do it?
updated question
i have a new dataset that have 2 columns. spacy ner seems to performe best, but it only identifies the ner(persons) on the column complete_text. when i run it on the column incomplete_text, it cannot identify the ner(persons). is there a way to map the person identify on the column complete_text and matched  it with the person on column incomplete_text. i'm sorry if this sounds confusing. this is how my dataset looks like:
complete_text                             incomplete_text                            
everyone to vote for marine le pen     vote for marine le pen

when i use spacy to get the person on both the complete_text column and the incomplete_text column, it only returns the person on the complete_text not on the incomplete_text. i want  want to match the person identified on the complete_text column with the person on the incomplete_text column and return the person identified as a single word.
complete_text                           spacy_complete_text_person   incomplete_text                spacy_incomplete_text__person      result                                         
everyone to vote for marine le pen     [marine le pen]                 vote for marine le pen                  []                      vote for marine_le_pen","['python', 'named-entity-recognition']",72926417,"you can use bert model from dslim/bert-base-ner using transformers:
# python env: pip install transformers
# anaconda env: conda install transformers
from transformers import autotokenizer, automodelfortokenclassification
from transformers import pipeline

def process_person(txt):
    # currently not optimized for pandas
    l = nlp(txt)
    for d in l:
        if d['entity_group'] == 'per':
            s = d['start']
            e = d['end']
            txt = txt[:s] + txt[s:e+1].replace(' ', '_') + txt[e:]
    return txt

tokenizer = autotokenizer.from_pretrained(""dslim/bert-large-ner"")
model = automodelfortokenclassification.from_pretrained(""dslim/bert-large-ner"")

nlp = pipeline(""ner"", model=model, tokenizer=tokenizer, aggregation_strategy='max')
df['text2'] = df['text'].apply(process_person)

output:
>>> df
                                 text                               text2
0               vote for donald trump               vote for donald_trump
1               vote for barack obama               vote for barack_obama
2             vote for bernie sanders             vote for bernie_sanders
3            move to another location            move to another location
4           support lavaughn robinson           support lavaughn_robinson
5  support michelle lavaughn robinson  support michelle_lavaughn_robinson
6                     support sanders                     support sanders
7                           rt please                           rt please
8                            stop luc                            stop luc
9                           kick some                           kick some",https://stackoverflow.com/questions/72926089,python,10-07-2022 04:09,725.0,1.0,2.0,True,14-07-2022 06:06,14-07-2022 06:06
70954157,modulenotfounderror: no module named &#39;milvus&#39;,"goal: to run this auto labelling notebook on aws sagemaker jupyter labs.
kernels tried: conda_pytorch_p36, conda_python3, conda_amazonei_mxnet_p27.

! pip install farm-haystack -q
# install the latest master of haystack
!pip install grpcio-tools==1.34.1 -q
!pip install git+ -q
!wget --no-check-certificate 
!tar -xvf xpdf-tools-linux-4.03.tar.gz && sudo cp xpdf-tools-linux-4.03/bin64/pdftotext /usr/local/bin
!pip install git+ -q

# here are the imports we need
from haystack.document_stores.elasticsearch import elasticsearchdocumentstore
from haystack.nodes import preprocessor, transformersdocumentclassifier, farmreader, elasticsearchretriever
from haystack.schema import document
from haystack.utils import convert_files_to_dicts, fetch_archive_from_ print_answers

traceback:
02/02/2022 10:36:29 - info - faiss.loader -   loading faiss with avx2 support.
02/02/2022 10:36:29 - info - faiss.loader -   could not load library with avx2 support due to:
modulenotfounderror(""no module named 'faiss.swigfaiss_avx2'"",)
02/02/2022 10:36:29 - info - faiss.loader -   loading faiss.
02/02/2022 10:36:29 - info - faiss.loader -   successfully loaded faiss.
02/02/2022 10:36:33 - info - farm.modeling.prediction_head -   better speed can be achieved with apex installed from  .
---------------------------------------------------------------------------
modulenotfounderror                       traceback (most recent call last)
<ipython-input-4-6ff421127e9c> in <module>
      1 # here are the imports we need
----> 2 from haystack.document_stores.elasticsearch import elasticsearchdocumentstore
      3 from haystack.nodes import preprocessor, transformersdocumentclassifier, farmreader, elasticsearchretriever
      4 from haystack.schema import document
      5 from haystack.utils import convert_files_to_dicts, fetch_archive_from_ print_answers

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/__init__.py in <module>
      3 import pandas as pd
      4 from haystack.schema import document, label, multilabel, basecomponent
----> 5 from haystack.finder import finder
      6 from haystack.pipeline import pipeline
      7 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/finder.py in <module>
      6 from collections import defaultdict
      7 
----> 8 from haystack.reader.base import basereader
      9 from haystack.retriever.base import baseretriever
     10 from haystack import multilabel

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/reader/__init__.py in <module>
----> 1 from haystack.reader.farm import farmreader
      2 from haystack.reader.transformers import transformersreader

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/reader/farm.py in <module>
     22 
     23 from haystack import document
---> 24 from haystack.document_store.base import basedocumentstore
     25 from haystack.reader.base import basereader
     26 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/document_store/__init__.py in <module>
      2 from haystack.document_store.faiss import faissdocumentstore
      3 from haystack.document_store.memory import inmemorydocumentstore
----> 4 from haystack.document_store.milvus import milvusdocumentstore
      5 from haystack.document_store.sql import sqldocumentstore

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/haystack/document_store/milvus.py in <module>
      5 import numpy as np
      6 
----> 7 from milvus import indextype, metrictype, milvus, status
      8 from scipy.special import expit
      9 from tqdm import tqdm

modulenotfounderror: no module named 'milvus'


pip install milvus

import milvus

traceback:
---------------------------------------------------------------------------
modulenotfounderror                       traceback (most recent call last)
<ipython-input-3-91c33e248077> in <module>
----> 1 import milvus

modulenotfounderror: no module named 'milvus'","['elasticsearch', 'nlp', 'document-classification', 'milvus', 'haystack']",70968746,i would recommend to downgrade your milvus version to a version before the 2.0 release just a week ago. here is a discussion on that topic:,https://stackoverflow.com/questions/70954157,elasticsearch,02-02-2022 10:40,9101.0,6.0,2.0,True,06-07-2022 10:02,06-07-2022 10:02
73822353,how can i get word-level timestamps in openai&#39;s whisper asr?,"i use openai's whisper python lib for speech recognition. how can i get word-level timestamps?

to transcribe with openai's whisper (tested on ubuntu 20.04 x64 lts with an nvidia geforce rtx 3090):
conda create -y --name whisperpy39 python==3.9
conda activate whisperpy39
pip install git+ 
sudo apt update && sudo apt install ffmpeg
whisper recording.wav
whisper recording.wav --model large

if using an nvidia geforce rtx 3090, add the following after conda activate whisperpy39:
pip install -f 
conda install pytorch==1.10.1 torchvision torchaudio cudatoolkit=11.0 -c pytorch","['python', 'timestamp', 'speech-recognition', 'openai-api', 'openai-whisper']",77784868,"in openai-whisper version 20231117, you can get word level timestamps by setting word_timestamps=true when calling transcribe():
pip install openai-whisper

import whisper
model = whisper.load_model(""large"")
transcript = model.transcribe(
    word_timestamps=true,
    audio=""toto.mp3""
)
for segment in transcript['segments']:
    print(''.join(f""{word['word']}[{word['start']}/{word['end']}]"" 
                    for word in segment['words']))

prints:

toto,[2.98/3.4] i[3.4/3.82] have[3.82/3.96] a[3.96/4.02] feeling[4.02/4.22] we're[4.22/4.44] not[4.44/4.56] in[4.56/4.72] kansas[4.72/5.14] anymore.[5.14/5.48]",https://stackoverflow.com/questions/73822353,python,23-09-2022 02:15,37947.0,22.0,4.0,True,05-09-2024 18:28,23-09-2022 16:37
76716798,streaming a response with langchain in javascript,"i am writing a little application in javascript using the langchain library. i have the following snippet:
/* langchain imports */
import { openai } from ""langchain/llms/openai"";
import { buffermemory } from ""langchain/memory"";
import { conversationchain } from ""langchain/chains"";

// ========================================================================================= //
  // ============= use langchain to send request to openai api =============================== //
  // ========================================================================================= //

  const openaillmoptions = {
    modelname: chatmodel.value,
    openaiapikey: decryptedstring,
    temperature: parsefloat(temperaturevalue.value),
    topp: parsefloat(topp.value),
    maxtokens: parseint(maxtokens.value),
    stop: stopsequences.value.length > 0 ? stopsequences.value : null,
    streaming: true,
};

  const model = new openai(openaillmoptions);
  const memory = new buffermemory();
  const chain = new conversationchain({ llm: model, memory: memory });

  try {
    const response = await chain.call({ input: content.value, signal: signal }, undefined,
      [
        {

          handlellmnewtoken(token) {
            process.stdout.write(token);
          },
        },
      ]
    );

// handle the response

}

this does not work (i tried both using the token via typescript and without typing). i have scoured various forums and they are either implementing streaming with python or their solution is not relevant to this problem. so to summarize, i can successfully pull the response from openai via the langchain conversationchain() api call, but i canï¿½ï¿½ï¿½t stream the response. is there a solution","['openai-api', 'langchain-js']",77230371,"for reference here is how i got streaming working:
const openaillmoptions = {
    modelname: chatmodel.value,
    cache: true,
    openaiapikey: openaidecryptedstring,
    temperature: parsefloat(temperaturevalue.value),
    topp: parsefloat(topp.value),
    maxtokens: parseint(maxtokens.value),
    stop: stopsequences.value.length > 0 ? stopsequences.value : null,
    streaming: true,
    verbose: true,
  };

const chat = new chatopenai(openaillmoptions);
      const chatprompt = chatprompttemplate.frommessages([
        [
          ""system"",
          systemprompt.value,
        ],
        new messagesplaceholder(""history""),
        [""human"", content.value],
      ]);

      const chain = new conversationchain({
        memory: new buffermemory({ returnmessages: true, memorykey: ""history"" }),
        prompt: chatprompt,
        llm: chat,
      });

      await chain.call({
        input: content.value,
        signal: signal,
        callbacks: [
          {
            handlellmnewtoken(token) {
              airesponse.value += token;
            }
          }
        ]

      });",https://stackoverflow.com/questions/76716798,openai-api,18-07-2023 22:07,4380.0,2.0,2.0,True,02-02-2024 00:49,02-02-2024 00:49
75773786,"why can&#39;t i access gpt-4 models via api, although gpt-3.5 models work?","i'm able to use the gpt-3.5-turbo-0301 model to access the chatgpt api, but not any of the gpt-4 models. here is the code i am using to test this (it excludes my openai api key). the code runs as written, but when i replace ""gpt-3.5-turbo-0301"" with ""gpt-4"", ""gpt-4-0314"", or ""gpt-4-32k-0314"", it gives me an error
openai.error.invalidrequesterror: the model: `gpt-4` does not exist

i have a chatgpt+ subscription, am using my own api key, and can use gpt-4 successfully via openai's own interface.
it's the same error if i use gpt-4-0314 or gpt-4-32k-0314. i've seen a couple articles claiming this or similar code works using 'gpt-4' works as the model specification, and the code i pasted below is from one of them.
is it possible to access the gpt-4 model via python + api, and if so, how?
openai_key = ""sk...""
openai.api_key = openai_key
system_intel = ""you are gpt-4, answer my questions as if you were an expert in the field.""
prompt = ""write a blog on how to use gpt-4 with python in a jupyter notebook""
# function that calls the gpt-4 api

def ask_gpt4(system_intel, prompt): 
    result = openai.chatcompletion.create(model=""gpt-3.5-turbo-0301"",
                                 messages=[{""role"": ""system"", ""content"": system_intel},
                                           {""role"": ""user"", ""content"": prompt}])
    print(result['choices'][0]['message']['content'])

# call the function above
ask_gpt4(system_intel, prompt)","['python', 'openai-api', 'gpt-4']",75774187,"currently the gpt 4 api is restricted, even to users with a chat gpt + subscription.
you may need to join the waitlist for the api.",https://stackoverflow.com/questions/75773786,python,18-03-2023 03:59,32540.0,20.0,4.0,True,06-03-2024 15:45,06-03-2024 15:45
7551262,training data for sentiment analysis,"where can i get a corpus of documents that have already been classified as positive/negative for sentiment in the corporate domain? i want a large corpus of documents that provide reviews for companies, like reviews of companies provided by analysts and media.
i find corpora that have reviews of products and movies. is there a corpus for the business domain including reviews of companies, that match the language of business?","['nlp', 'machine-learning', 'text-analysis', 'sentiment-analysis', 'training-data']",7554965,"you can use twitter, with its smileys, like this: 
hope that gets you started.  there's more in the literature, if you're interested in specific subtasks like negation, sentiment scope, etc.
to get a focus on companies, you might pair a method with topic detection, or cheaply just a lot of mentions of a given company.  or you could get your data annotated by mechanical turkers.",https://stackoverflow.com/questions/7551262,nlp,26-09-2011 06:18,42910.0,57.0,6.0,True,02-03-2023 11:09,27-09-2011 01:36
76189213,how can i find the cosine similarity between two song lyrics represented as strings?,"my friends and i are doing an nlp project on song recommendation.
context: we originally planned on giving the model a recommended song playlist that has the most similar lyrics based on the random input corpus(from the literature etc), however we didn't really have a concrete idea of its implementation.
currently our task is to find similar lyrics to a random lyric fed as a string input. we are using sentence bert model(sbert) and cosine similarity to find the similarity between the songs and it seems like the output numbers are meaningful enough to find the most similar song lyrics.
is there any other way that we can improve this approach?
we'd like to use bert model and are open to suggestions that can be used on top of bert if possible, but if there is any other models that should be used instead of bert, we'd be happy to learn. thanks.","['nlp', 'stanford-nlp', 'bert-language-model', 'cosine-similarity', 'nlp-question-answering']",76189401,"computing cosine similarity
you can use the util.cos_sim(embeddings1, embeddings2) from the sentence-transformers package to compute the cosine similarity of two embeddings.
alternatively, you can also use sklearn.metrics.pairwise.cosine_similarity(x, y, dense_output=true) from the scikit-learn package.
improvements for representation and models
since you want recommendations just on top of bert, you can consider roberta as well with byte-pair encoding for tokenizer over bert's wordpeice tokenizers.  consider the roberta-base model as a feature extractor from the huggingfacetransformers package.
from transformers import robertatokenizer, robertamodel
tokenizer = robertatokenizer.from_pretrained('roberta-base')
model = robertamodel.from_pretrained('roberta-base')
text = ""song lyrics in text.""
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)

tokenizers work at various text granularity level of syntax & semantics. they help generate quality vectors/embeddings. each can yield different and better results if fine-tunned for the correct task and model.
some other tokenizers you can consider are:
character level bpe, byte-level bpe, wordpiece (bert uses this), sentencepiece, and unigram tokenizer with lm character.
also consider exploring the huggingface official tokenizer library guide here.",https://stackoverflow.com/questions/76189213,nlp,06-05-2023 13:43,712.0,3.0,1.0,True,08-05-2023 21:49,08-05-2023 21:49
59428931,running jupyter notebook on binder with spacy dependencies,"i want to try out spacy in a jupyter notebook using binder. when trying to run load on a model like:
nlp = en_core_web_sm.load()

i get the following error:
---------------------------------------------------------------------------
modulenotfounderror                       traceback (most recent call last)
<ipython-input-4-8a5aa70d40b9> in <module>
----> 1 import en_core_web_sm
      2 nlp = en_core_web_sm.load()

modulenotfounderror: no module named 'en_core_web_sm'

i tried downloading the model using the requirements.txt, but that didn't work or the model was download in an area i don't have access to. not sure.
here's the github repo. thank you.","['python', 'jupyter-notebook', 'spacy']",59471932,"it looks like you are trying to use environment.yml and requirements.txt. when your needs necessitate moving beyond a requirements.txt configuration file for binderhub-served sessions, you should move the contents of requirements.txt to environment.yml following this example repo. in your case though one of your current requirements.txt lines is redundant (and conflicting) with the spacy line in environment.yml.",https://stackoverflow.com/questions/59428931,python,20-12-2019 17:02,214.0,1.0,2.0,True,30-01-2023 06:38,30-01-2023 06:38
70473212,google translate python package not working after x calls?,"so i am translating almost 1755 english sentences (short ones each are less than 10 words). the code below works fine.
problem faced: however, after translating almost 500 rows (sentences) in my data frame, it stops translating (without getting an error), and 'newlanguage' is the same as the original sentence. (i tried italian 'it' instead of arabic also faced the same problem).
do i have a limit to the # of api calls to translate? any ideas how to fix this ?
!pip install googletrans==3.1.0a0


from googletrans import translator

translator = translator()
backtrans_sentences=[]
backtrans_labels=[]

for sentence,label in zip(df_en_train['sentence'],df_en_train['labels']):
        newlanguage= translator.translate(text=sentence, dest='ar').text
        eng=translator.translate(text=newlanguage, dest='en').text
        backtrans_sentences.append(eng)
        backtrans_labels.append(label)","['python', 'google-cloud-platform', 'text', 'nlp', 'google-translation-api']",70493988,"for each gcp api there's a limit to a number requests per minute. in this case this is a rate quota that applies:

rate quotas are typically used for limiting the number of requests you can make to an api or service. rate quotas reset after a time interval that is specific to the serviceï¿½ï¿½ï¿½for example, the number of api requests per day.

in your case translation is handled by a translate.googleapis.com api.
go to your quotas page and there you can see the numbers.
you can also view the quotas for the api with the gcloud:
gcloud alpha services quota list \
    --service=translate.googleapis.com \
    --consumer=projects/your-project-name

however since this feature is in alpha it may not work as expected.
you can also reqest higher quotas in your translate api quota page.
to see which cap you're hitting go to cloud monitoring and follow the instructions described in the documentation.
when you establish which quota you need to raise just edit the value and click submit request button.",https://stackoverflow.com/questions/70473212,python,24-12-2021 13:04,453.0,0.0,1.0,True,27-12-2021 09:57,25-12-2021 10:18
29332851,what does nn vbd in dt nns rb means in nltk?,"when i chunk text, i get lots of codes in the output like
nn, vbd, in, dt, nns, rb.
is there a list documented somewhere which tells me the meaning of these? 
i have tried googling nltk chunk code nltk chunk grammar nltk chunk tokens.
but i am not able to find any documentation which explains what these codes mean.","['python', 'nlp', 'nltk', 'text-parsing', 'pos-tagger']",29333217,"the tags that you see are not a result of the chunks but the pos tagging that happens before chunking. it's the penn treebank tagset, see 
>>> from nltk import word_tokenize, pos_tag, ne_chunk
>>> sent = ""this is a foo bar sentence.""
# pos tag.
>>> nltk.pos_tag(word_tokenize(sent))
[('this', 'dt'), ('is', 'vbz'), ('a', 'dt'), ('foo', 'nnp'), ('bar', 'nnp'), ('sentence', 'nn'), ('.', '.')]
>>> tagged_sent = nltk.pos_tag(word_tokenize(sent))
# chunk.
>>> ne_chunk(tagged_sent)
tree('s', [('this', 'dt'), ('is', 'vbz'), ('a', 'dt'), tree('organization', [('foo', 'nnp'), ('bar', 'nnp')]), ('sentence', 'nn'), ('.', '.')])

to get the chunks look for subtrees within the chunked outputs. from the above output, the tree('organization', [('foo', 'nnp'), ('bar', 'nnp')]) indicates the chunk.
this tutorial site is pretty helpful to explain the chunking process in nltk:  
for official documentation, see",https://stackoverflow.com/questions/29332851,python,29-03-2015 18:08,33349.0,34.0,4.0,True,10-03-2023 10:27,29-03-2015 18:39
76854685,how to get &quot;text-embedding-ada-002&quot;?,"i can get this model only with openai api-key?
i can get full vocabulary of token embeddings?
i can download this model or no?
i cant get information for this in different sources for a long time.
sorry, my english is bad...","['python', 'artificial-intelligence', 'openai-api', 'embedding']",76988857,"he's asking how to run this locally. to the best of my knowledge, this model is only available via the openai service, ie behind their api.",https://stackoverflow.com/questions/76854685,python,07-08-2023 19:32,2578.0,-2.0,1.0,True,27-08-2023 20:37,07-08-2023 20:59
76353315,how to fix &#39;fs module not found&#39; error when using langchain document loaders in next.js?,"i am working on an ai project. i am using langchain and next.js 13.
i am trying to use the document loaders in langchain to load my pdf, however when i call a loader eg
import { pdfloader } from ""langchain/document_loaders/fs/pdf"";

immediately i get an error:
fs module not found

as per langchain documentation, this should not occur as it states that the apis support next.js enviroment.
i have tried using a package(pdf.js) to handle pdf file upload and parsing, but i get an error with this as well.
how can i use langchain document loaders in next.js?","['next.js', 'artificial-intelligence', 'langchain']",76354088,"i finally understand the issue here.
i was calling it in the browser context but once i moved my code to the api routes(node) environment, everything worked as expected.",https://stackoverflow.com/questions/76353315,next.js,28-05-2023 19:42,1619.0,4.0,2.0,True,25-09-2024 12:26,25-09-2024 12:26
78505196,syntaxerror: cannot assign to function call here. maybe you meant &#39;==&#39; when using os.environ in python,"i am creating a ai agent in my jupyter notebook on anaconda. when i enter the following settings i get an error.
import os
os.environ(""openai_api_key"")=openai_api_key
os.environ(""openai_model_name"")=""gpt-3.5-turbo""

 cell in[47], line 1
    os.environ(""openai_api_key"")=openai_api_key
    ^
syntaxerror: cannot assign to function call here. maybe you meant '==' instead of '='?

this seems to be a standard statement that works on lightning.ai or even vscode editor but why doesnt it work on my jupyter notebook running on localhost? i am running this on my macbook.
next i was planning to create a few agents to create a bot to read and respond to a blog.","['python', 'jupyter-notebook', 'openai-api']",78505222,"use square brackets instead of parentheses.
os.environ[""openai_api_key""]=openai_api_key
os.environ[""openai_model_name""]=""gpt-3.5-turbo""

it'll solve the issue.",https://stackoverflow.com/questions/78505196,python,20-05-2024 07:18,326.0,2.0,2.0,True,20-05-2024 11:47,20-05-2024 11:47
41904197,data frame of tfidf with python,"i have to classify some sentiments my data frame is like this
phrase                      sentiment    
is it  good movie          positive    
wooow is it very goode      positive    
bad movie                  negative

i did some preprocessing as tokenisation stop words stemming etc ...  and i get
phrase                      sentiment    
[ good , movie  ]        positive    
[wooow ,is , it ,very, good  ]   positive 
[bad , movie ]            negative

i need finally to get a dataframe in which the line are the text which the value is the tf_idf and the columns are the words like that
good     movie   wooow    very      bad                sentiment
tf idf    tfidf_  tfidf    tf_idf    tf_idf               positive
(same thing for the 2 remaining lines)","['python', 'pandas', 'dataframe', 'text-mining', 'tf-idf']",41908152,"i'd use sklearn.feature_extraction.text.tfidfvectorizer, which is specifically designed for such tasks:
demo:
in [63]: df
out[63]:
                   phrase sentiment
0       is it  good movie  positive
1  wooow is it very goode  positive
2               bad movie  negative

solution:
from sklearn.feature_extraction.text import countvectorizer, tfidfvectorizer

vect = tfidfvectorizer(sublinear_tf=true, max_df=0.5, analyzer='word', stop_words='english')

x = vect.fit_transform(df.pop('phrase')).toarray()

r = df[['sentiment']].copy()

del df

df = pd.dataframe(x, columns=vect.get_feature_names())

del x
del vect

r.join(df)

result:
in [31]: r.join(df)
out[31]:
  sentiment  bad  good     goode     wooow
0  positive  0.0   1.0  0.000000  0.000000
1  positive  0.0   0.0  0.707107  0.707107
2  negative  1.0   0.0  0.000000  0.000000

update: memory saving solution:
from sklearn.feature_extraction.text import countvectorizer, tfidfvectorizer

vect = tfidfvectorizer(sublinear_tf=true, max_df=0.5, analyzer='word', stop_words='english')

x = vect.fit_transform(df.pop('phrase')).toarray()

for i, col in enumerate(vect.get_feature_names()):
    df[col] = x[:, i]

update2: related question where the memory issue was finally solved",https://stackoverflow.com/questions/41904197,python,27-01-2017 22:40,4952.0,9.0,2.0,True,10-04-2022 08:04,08-08-2020 17:24
72658548,how to create a abstractive summary using supervised machine learning?,"i want to summarise my document to about 7-8% of total words in document.the summary should be abstractive not extractive. i've referred some of the previous abstractive summarisation strategies however they're using deep learning models like seq2seq, lstm, etc. and i would like to do this task using some basic supervised machine learning algorithms like svm, logistic regression, etc. the accuracy is not a concern for me. i did a lot of research but was not able to get something relevant.","['machine-learning', 'nlp', 'summarization']",73273763,you can find something similar here,https://stackoverflow.com/questions/72658548,machine-learning,17-06-2022 11:14,64.0,0.0,1.0,True,08-08-2022 06:42,17-06-2022 12:12
78129126,typeerror: exception encountered when calling layer &#39;embeddings&#39; (type tfbertembeddings),"my model was wholly workable two weeks back, but now it's showing the following error:
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
<ipython-input-23-a3e5a45f06c9> in <cell line: 14>()
     12 
     13 # encode input using bert model
---> 14 bert_output = bert_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
     15 
     16 # get pooled output and pass through dropout layer

8 frames
/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # to get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---> 70             raise e.with_traceback(filtered_tb) from none
     71         finally:
     72             del filtered_tb

typeerror: exception encountered when calling layer 'embeddings' (type tfbertembeddings).

could not build a typespec for name: ""tf.debugging.assert_less/assert_less/assert/assert""
op: ""assert""
input: ""tf.debugging.assert_less/assert_less/all""
input: ""tf.debugging.assert_less/assert_less/assert/assert/data_0""
input: ""tf.debugging.assert_less/assert_less/assert/assert/data_1""
input: ""tf.debugging.assert_less/assert_less/assert/assert/data_2""
input: ""placeholder""
input: ""tf.debugging.assert_less/assert_less/assert/assert/data_4""
input: ""tf.debugging.assert_less/assert_less/y""
attr {
  key: ""t""
  value {
    list {
      type: dt_string
      type: dt_string
      type: dt_string
      type: dt_int32
      type: dt_string
      type: dt_int32
    }
  }
}
attr {
  key: ""summarize""
  value {
    i: 3
  }
}
 of unsupported type <class 'tensorflow.python.framework.ops.operation'>.

call arguments received by layer 'embeddings' (type tfbertembeddings):
  ï¿½ï¿½ï¿½ input_ids=<kerastensor: shape=(none,type=int32 (created by layer 'input_ids')>
  ï¿½ï¿½ï¿½ position_ids=none
  ï¿½ï¿½ï¿½ token_type_ids=<kerastensor: shape=(none, 50) dtype=int32 (created by layer 'token_type_ids')>
  ï¿½ï¿½ï¿½ inputs_embeds=none
  ï¿½ï¿½ï¿½ past_key_values_length=0
  ï¿½ï¿½ï¿½ training=false

i think this error occurs when the input layers are sent to the corresponding bert layer. if i use old versions of tensorflow instead of 2.15.0, the error is resolved. however, with those old versions, i did not get gpu and faced a graph execution error.
can","['tensorflow', 'deep-learning', 'nlp', 'bert-language-model', 'transformer-model']",78135776,"it's a problem with the transformers library, i had the same problem and solved it using version 4.31.0",https://stackoverflow.com/questions/78129126,tensorflow,08-03-2024 16:48,1388.0,0.0,2.0,True,17-05-2024 04:33,08-03-2024 16:55
73853551,complex regex not working in spacy entity ruler,"i'm trying to identify the entities by passing the regular expression (regex) to the spacy model using entity ruler but, spacy is unable to identify based on the below regex.
but, i tested the regex here and it's working.
import model_training
import spacy

nlp = spacy.load('en_core_web_trf')
nlp.add_pipe(""spacytextblob"")

nlp = model_training.train_model_with_regex(nlp)

model_training.py
def train_model_with_regex(nlp):
ruler = nlp.add_pipe(""entity_ruler"", before=""ner"")
patterns = [
    {
        ""label"": ""volume"",
        ""pattern"": [{""lower"": {'regex': ""(?:\d+\s(?:million|hundred|thousand|billion)*\s*)+""}}]
    }
]

ruler.add_patterns(patterns)
return nlp

i wanted to achieve this, for the below example
text = ""i have spent 5 million to buy house and 70 thousand for the furniture""

expected output:
{'result': [
    {'label': 'volume', 'text': '5 million'},
    {'label': 'volume', 'text': '70 thousand'}
]}","['python', 'regex', 'nlp', 'spacy', 'named-entity-recognition']",73855126,"the problem is that your pattern is supposed to match at least two tokens, while the regex operator is applied to a single token.
a solution can look like
""pattern"": [
    {""text"": {""regex"": r""^\d+(?:[,.]\d+)*$""}},
    {""text"": {""regex"": r""^(?:million|hundred|thousand|billion)s?$""}}
]

the like_num entity is defined  in spacy source code mostly as a string of digits with all dots and commas removed, so the ^\d+(?:[,.]\d+)*$ pattern looks good enough. it matches a token that starts with one or more digits and then contains zero or more occurrences of a comma or dot and then one or more digits till the end of the token.",https://stackoverflow.com/questions/73853551,python,26-09-2022 11:32,379.0,2.0,1.0,True,26-09-2022 13:47,26-09-2022 12:14
70529754,python: how to speed up lemmatisation if i check the pos for each word?,"i am new to nlp. i wish to lemmatise. but understand that for wordnetlemmatizer, it depends on the type of words passed in noun, verb, etc.
hence i tried the below code but it is very slow. basically all my text are saved in a column called ""text"" in df. i use the pre_process(text) function by looping each row (option 1) but it is v slow.
i tried apply (option 2) , but it just as slow.
any way to speed up? thank you!
from nltk import wordnetlemmatizer, pos_tag
import pandas as pd

def pre_process(text):
 
    words_only = words_only.lower().split()    

    lem = wordnetlemmatizer()
    words_only1=[]
    for j in range(0, len(words_only)):
        
        pos_label = (pos_tag(words_only)[j][1][0]).lower()
        word=words_only[j]
        
        if pos_label == 'j': pos_label = 'a'    # 'j' <--> 'a' reassignment
        
        if pos_label in ['r']:  # for adverbs it's a bit different
            try:
                word=wordnet.synset(word+'.r.1').lemmas()[0].pertainyms()[0].name() # could have errors for words like 'not'
            except:
                word=lem.lemmatize(word)

        elif pos_label in ['a', 's', 'v']: # for adjectives and verbs
            word=lem.lemmatize(word, pos=pos_label)

        else:   # for nouns and everything else as it is the default kwarg
            word=lem.lemmatize(word)
        
        words_only1.append(word)
    
    words_only=words_only1
    return( "" "".join(words_only)) 


df=pd.read_excel( 'c:/users/desktop/test.xlsx', 
                   sheet_name='text', 
                   engine='openpyxl')

**option 1**
num_text = df.shape[0]
clean_text= []
for i in range(0, num_text):
    clean_text.append(pre_process(df['text'].iloc[i]))


**option 2**
df_bd['processed text']=df['text'].apply(pre_process_bow)
clean_text= df['processed text'].tolist()","['python', 'nlp', 'lemmatization']",70538225,"from a quick review of your method, i suggest you to call pos_tag outside of the for loop. otherwise, you call this method for every word, which could be slow. this could already speed up the process a bit, depending on the complexity of pos_tag.
note: i suggest you using tqdm. this gives you a nice progress bar and lets you estimate how long it takes.
from tqdm import tqdm

def pre_process(text):
    words_only = words_only.lower().split()    

    lem = wordnetlemmatizer()
    words_only1=[]
    pos_tags = pos_tag(words_only)
    for word, word_pos_tag in tqdm(zip(words_only, pos_tags), total=len(words_only)):
        pos_label = word_pos_tag[1][0].lower()
        if pos_label == 'j': 
            pos_label = 'a'    # 'j' <--> 'a' reassignment
        
        if pos_label in ['r']:  # for adverbs it's a bit different
            try:
                word=wordnet.synset(word+'.r.1').lemmas()[0].pertainyms()[0].name() # could have errors for words like 'not'
            except:
                word=lem.lemmatize(word)

        elif pos_label in ['a', 's', 'v']: # for adjectives and verbs
            word=lem.lemmatize(word, pos=pos_label)

        else:   # for nouns and everything else as it is the default kwarg
            word=lem.lemmatize(word)
        
        words_only1.append(word)
    
    return("" "".join(words_only1))",https://stackoverflow.com/questions/70529754,python,30-12-2021 09:32,273.0,0.0,1.0,True,31-12-2021 01:30,31-12-2021 01:08
66675261,how can i work with example for nlp.update problem with spacy3.0,"i am trying to train my data with spacy v3.0 and appareantly the nlp.update do not accept any tuples. here is the piece of code:
import spacy
import random
import json
nlp = spacy.blank(""en"")
ner = nlp.create_pipe(""ner"")
nlp.add_pipe('ner')
ner.add_label(""label"")
# start the training
nlp.begin_training()
# loop for 40 iterations
for itn in range(40):
    # shuffle the training data
    random.shuffle(training_data)
    losses = {}
# batch the examples and iterate over them
    for batch in spacy.util.minibatch(training_data, size=2):
        texts = [text for text, entities in batch]
        annotations = [entities for text, entities in batch]
# update the model
        nlp.update(texts, annotations, losses=losses, drop=0.3)
    print(losses)

and i am receiving error
valueerror                                traceback (most recent call last)
<ipython-input-79-27d69961629b> in <module>
     18         annotations = [entities for text, entities in batch]
     19 # update the model
---> 20         nlp.update(texts, annotations, losses=losses, drop=0.3)
     21     print(losses)

~\anaconda3\lib\site-packages\spacy\language.py in update(self, examples, _, drop, sgd, losses, component_cfg, exclude)
   1086         """"""
   1087         if _ is not none:
-> 1088             raise valueerror(errors.e989)
   1089         if losses is none:
   1090             losses = {}

valueerror: [e989] `nlp.update()` was called with two positional arguments. this may be due to a backwards-incompatible change to the format of the training data in spacy 3.0 onwards. the 'update' function should now be called with a batch of example objects, instead of `(text, annotation)` tuples. 

i set my train data format:
training_data = []
for entry in labeled_data:
    entities = []
    for e in entry['labels']:
        entities.append((e[0], e[1],e[2]))
    spacy_entry = (entry['text'], {""entities"": entities})
    training_data.append(spacy_entry)

my train data looks like this:
[('part list', {'entities': []}), ('pending', {'entities': []}), ('3d printing', {'entities': [(0, 11, 'process')]}), ('recommended to use a fdm 3d printer with pla material.', {'entities': [(25, 36, 'process'), (41, 44, 'material')]}), ('ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½', {'entities': []}), ('no need supports or rafts.', {'entities': []}), ('resolution: 0.20mm', {'entities': []}), ('fill density 20%', {'entities': []}), ('as follows from the analysis, part of the project is devoted to 3d', {'entities': [(64, 66, 'process')]}), ('printing, as all static components were created using 3d modelling and', {'entities': [(54, 66, 'process')]}), ('subsequent printing.', {'entities': []}), ('ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½', {'entities': []}), ('in our project, we created several versions of the', {'entities': []}), ('model during modelling, which we will describe and document in the', {'entities': []}), ('following subchapters.ing, we used the sketchup', {'entities': [(37, 49, 'process')]}), ('make tool, version from 2017. the main reason was the high degree of', {'entities': []}), ('intuitiveness and simplicity of the tool, as we had not encountered 3d', {'entities': [(68, 70, 'process')]}), ('modelling before and needed a relatively flexible and efficient tool to', {'entities': []}), ('guarantee the desired result. with zero previous experience.', {'entities': []}), ('in this version, which is shown in the figures figure 13 - version no. 2 side view and figure 24 - version no. 2 - front view, for the first time, the specific dimensions of the infuser were clarified and', {'entities': []}), ('modelled. the details of the lower servo attachment, the cable hole in', {'entities': []}), ('the main mast, the winding cylinder mounting, the protrusion on the', {'entities': [(36, 44, 'process')]}), ('winding cylinder for holding the tea bag, the preparation for fitting', {'entities': []}), ('the wooden and aluminium plate and the shape of the cylinder end that', {'entities': [(15, 25, 'material')]}), ('exactly fit the servo were also reworked.', {'entities': []}), ('after the creation of this', {'entities': []}), ('version of the model, this model was subsequently officially consulted', {'entities': []}), ('and commented on for the first time.', {'entities': []}), ('in this version, which is shown in the figures figure 13 - version no. 2 side view and figure 24 - version no. 2 - front view, for the first time, the specific dimensions of the infuser were clarified and', {'entities': []}), ('modelled. the details of the lower servo attachment, the cable hole in', {'entities': []}), ('the main mast, the winding cylinder mounting, the protrusion on the', {'entities': [(36, 44, 'process')]})]

i would appreciate your help as a new contributor. thanks a lot!","['nlp', 'spacy', 'named-entity-recognition']",66679910,"you didn't provide your train_data, so i cannot reproduce it. however, you should try something like this:
from spacy.training.example import example

for batch in spacy.util.minibatch(training_data, size=2):
    for text, annotations in batch:
        # create example
        doc = nlp.make_doc(text)
        example = example.from_dict(doc, annotations)
        # update the model
        nlp.update([example], losses=losses, drop=0.3)",https://stackoverflow.com/questions/66675261,nlp,17-03-2021 14:36,20748.0,21.0,4.0,True,06-08-2023 14:41,18-03-2021 09:23
52123026,sklearn pipeline valueerror: could not convert string to float,"i'm playing around with sklearn and nlp for the first time, and thought i understood everything i was doing up until i didn't know how to fix this error. here is the relevant code (largely adapted from 
from sklearn.feature_extraction.text import tfidfvectorizer
from sklearn.pipeline import pipeline, featureunion
from sklearn.preprocessing import standardscaler
from sklearn.decomposition import truncatedsvd
from sgboost import xgbclassifier
from pandas import dataframe

def read_files(path):
    for article in os.listdir(path):
        with open(os.path.join(path, doc)) as f:
            text = f.read()
        yield os.path.join(path, article), text

def build_data_frame(path, classification)
    rows = []
    index = []
    for filename, text in read_files(path):
        rows.append({'text': text, 'class': classification})
        index.append(filename)
    df = dataframe(rows, index=index)
    return df

data = dataframe({'text': [], 'class': []})
for path, classification in sources: # sources is a list of tuples
    data = data.append(build_data_frame(path, classification))
data = data.reindex(np.random.permutation(data.index))

classifier = pipeline([
    ('features', featureunion([
        ('text', pipeline([
            ('tfidf', tfidfvectorizer()),
            ('svd', truncatedsvd(algorithm='randomized', n_components=300)
            ])),
        ('words', pipeline([('wscaler', standardscaler())])),
    ])),
    ('clf, xgbclassifier(silent=false)),
])
classifier.fit(data['text'].values, data['class'].values)

the data loaded into the dataframe is preprocessed text with all stopwords, punctuation, unicode, capitals, etc. taken care of. this is the error i'm getting once i call fit on the classifier where the ... represents one of the documents that should have been vecorized in the pipeline:
valueerror: could not convert string to float: ...

i first thought the tfidfvectorizer() is not working, causing an error on the svd algorithm, but after i extracted each step out of the pipeline and implemented them sequentially, the same error only came up on xgbclassifer.fit().
even more confusing to me, i tried to piece this script apart step-by-step in the interpreter, but when i tried to import either read_files or build_data_frame, the same valueerror came up with one of my strings, but this was merely after:
from classifier import read_files

i have no idea how that could be happening, if anyone has any idea what my glaring errors may be, i'd really appreciate it. trying to wrap my head around these concepts on my own but coming across a problem likes this leaves me feeling pretty incapacitated.","['python', 'scikit-learn', 'nlp', 'text-classification']",52124552,"first part of your pipeline is a featureunion. featureunion will pass all the data it gets parallely to all internal parts. the second part of your featureunion is a pipeline containing single standardscaler. thats the source of error.
this is your data flow:
x --> classifier, pipeline
            |
            |  <== x is passed to featureunion
            \/
      features, featureunion
                      |
                      |  <== x is duplicated and passed to both parts
        ______________|__________________
       |                                 |
       |  <===   x contains text  ===>   |                         
       \/                               \/
   text, pipeline                   words, pipeline
           |                                  |   
           |  <===    text is passed  ===>    |
          \/                                 \/ 
       tfidf, tfidfvectorizer            wscaler, standardscaler  <== error
                 |                                   |
                 | <==text converted to floats       |
                \/                                   |
              svd, truncatedsvd                      |
                       |                             |
                       |                             |
                      \/____________________________\/
                                      |
                                      |
                                     \/
                                   clf, xgbclassifier

since text is passed to standardscaler, the error is thrown, standardscaler can only work with numerical features.
just as you are converting text to numbers using tfidfvectorizer, before sending that to truncatedsvd, you need to do the same before standardscaler, or else only provide numerical features to it.
looking at the description in question, did you intend to keep standardscaler after the results of truncatedsvd?",https://stackoverflow.com/questions/52123026,python,31-08-2018 21:59,4992.0,7.0,2.0,True,10-02-2023 10:18,31-08-2018 22:08
71326920,extracting spacy date entities and adding to new pandas column,"i have a collection of social media comments that i want to explore based on their reference to dates. for this purpose, i am using spacy's named entity recognizer to search for date entities. i have the comments in a pandas dataframe called df_test under the column comment. i would like to add a new column dates to this dataframe consisting of all the date entities found in each comment. some comments are not going to have any date entities in which case none should be added here instead.
so for example:
comment
'bla bla 21st century'
'bla 1999 bla bla 2022'
'bla bla bla'

should be:
comment                        dates
'bla bla 21st century'         '21st century'
'bla 1999 bla bla 2022'        '1999', '2022'
'bla bla bla'                  'none'

based on is their a way to add the new ner tag found in a new column? i have tried a list approach:
date_label = ['date']
dates_list = []

def get_dates(row):
    comment = str(df_test.comment.tolist())
    doc = nlp(comment)
    for ent in doc.ents:
        if ent.label_ in date_label:
            dates_list.append([ent.text])
        else:
            dates_list.append(['none'])

df_test.apply(lambda row: get_dates(row))
date_df_test = pd.dataframe(dates_list, columns=['dates'])

however, this then produces a column that would be longer than the original dataframe, like:
comment                        dates
'bla bla 21st century'         '21st century'
'bla 1999 bla bla 2022'        '1999'
'bla bla bla'                  '2022'
                               'none'

which doesn't work, since the entries of dates no longer matches with their corresponding comments. i understand that it is because i am for-looping across all entities, but i don't know how to work around this. is there any way to solve this, so that i can extract all date entities and connect them in some way to the comment their were found in for the purpose of later analysis? any help is much appreciated!","['python', 'pandas', 'spacy', 'named-entity-recognition', 'named-entity-extraction']",71328623,"i managed to find a solution to my own problem by using this function.
date_label = ['date']

def extract_dates(text):
    doc = nlp(text)
    results = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in date_label]
    return results

df_test['dates'] = df_test['comment'].apply(extract_dates)

i hope this may help anyone who face a similar issue.",https://stackoverflow.com/questions/71326920,python,02-03-2022 17:36,3515.0,1.0,1.0,True,03-03-2022 14:54,02-03-2022 17:58
49395939,"smote initialisation expects n_neighbors &lt;= n_samples, but n_samples &lt; n_neighbors","i have already pre-cleaned the data, and below shows the format of the top 4 rows:
     [in] df.head()

    [out]   year    cleaned
         0  1909    acquaint hous receiv follow letter clerk crown...
         1  1909    ask secretari state war whether issu statement...
         2  1909    i beg present petit sign upward motor car driv...
         3  1909    i desir ask secretari state war second lieuten...
         4  1909    ask secretari state war whether would introduc...

i have called train_test_split() as follows:
     [in] x_train, x_test, y_train, y_test = train_test_split(df['cleaned'], df['year'], random_state=2)
   [note*] `x_train` and `y_train` are now pandas.core.series.series of shape (1785,) and `x_test` and `y_test` are also pandas.core.series.series of shape (595,)

i have then vectorized the x training and testing data using the following tfidfvectorizer and fit/transform procedures:
     [in] v = tfidfvectorizer(decode_error='replace', encoding='utf-8', stop_words='english', ngram_range=(1, 1), sublinear_tf=true)
          x_train = v.fit_transform(x_train)
          x_test = v.transform(x_test)

i'm now at the stage where i would normally apply a classifier, etc (if this were a balanced set of data). however, i initialize imblearn's smote() class (to perform over-sampling)...
     [in] smote_pipeline = make_pipeline_imb(smote(), classifier(random_state=42))
          smote_model = smote_pipeline.fit(x_train, y_train)
          smote_prediction = smote_model.predict(x_test)

... but this results in:
     [out] valueerror: ""expected n_neighbors <= n_samples, but n_samples = 5, n_neighbors = 6.

i've attempted to whittle down the number of n_neighbors but to no avail, any tips or advice would be much appreciated. thanks for reading.
------------------------------------------------------------------------------------------------------------------------------------
edit:
full traceback
the dataset/dataframe (df) contains 2380 rows across two columns, as shown in df.head() above. x_train contains 1785 of these rows in the format of a list of strings (df['cleaned']) and y_train also contains 1785 rows in the format of strings (df['year']). 
post-vectorization using tfidfvectorizer(): x_train and x_test are converted from pandas.core.series.series of shape '(1785,)' and '(595,)' respectively, to scipy.sparse.csr.csr_matrix of shape '(1785, 126459)' and '(595, 126459)' respectively.
as for the number of classes: using counter(), i've calculated that there are 199 classes (years), each instance of a class is attached to one element of aforementioned df['cleaned'] data which contains a list of strings extracted from a textual corpus.
the objective of this process is to automatically determine/guess the year, decade or century (any degree of classification will do!) of input textual data based on vocabularly present.","['scikit-learn', 'knn', 'tf-idf', 'oversampling', 'imblearn']",49418705,"since there are approximately 200 classes and 1800 samples in the training set, you have on average 9 samples per class. the reason for the error message is that a) probably the data are not perfectly balanced and there are classes with less than 6 samples and b) the number of neighbors is 6. a few solutions for your problem:

calculate the minimum number of samples (n_samples) among the 199 classes and select n_neighbors parameter of smote class less or equal to n_samples.
exclude from oversampling the classes with n_samples < n_neighbors using the ratio parameter of smote class.
use randomoversampler class which does not have a similar restriction.
combine 3 and 4 solutions: create a pipeline that is using smote and randomoversampler in a way that satisfies the condition n_neighbors <= n_samples for smoted classes and uses random oversampling when the condition is not satisfied.",https://stackoverflow.com/questions/49395939,scikit-learn,20-03-2018 23:48,58737.0,17.0,5.0,True,13-12-2022 01:15,18-02-2019 19:28
77265396,how does sqldatabase chain work internally? (langchain),"langchain doc
i want to understand underlying implementation. i know it uses nlp. but how it is determining whether requested thing is table or column. maybe they are using spacy but customised a bit to understand database terms.

what does it store in memory? obviously they are not storing whole database. from this answer,i got to know they are storing ddl of database.  
but huge database will probably have large ddl. won't that create issue?","['nlp', 'langchain']",77266382,"this is the implementation for sqldatabasechain
 

regarding your queries
what does it store in memory? obviously they are not storing whole database.
answer : yes sqldatabasechain does not store entire database, it works based on metadata
from this answer,i got to know they are storing ddl of database.
but huge database will mostly have large ddl. won't that create issue?
answer : metadata mostly includes table names, column names, primary and foreign keys, all these information together sums up to  very small compared to ddl.",https://stackoverflow.com/questions/77265396,nlp,10-10-2023 11:31,1124.0,0.0,2.0,True,28-10-2023 03:41,10-10-2023 13:56
72463717,is there a way in r to find a combination of words (or sentences) within a certain range in a string,"i'm trying to find all strings with a combination of words/sentences with other words separating them but with a fixed limit.
example : i want the combination of ""bought"" and ""watch"" but with, at maximum, 2 words separating them.

i bought a beautiful and shiny watch
-> not ok because there is 4 words between ""bought"" and ""watch"" (""a beautiful and shiny"")
i bought a shiny watch -> ok because there is 2 words between ""bought"" and ""watch"" (""a shiny"")

i haven't found anything close to what i wanted on r.
to find simple words/sentences in strings i'm using str_extract_all from stringr as here :
my_analysis <- str_c(""\\b("", str_c(my_list_of_words_and_sentences, collapse=""|""), "")\\b"")
df$words_and_sentences_found <- str_extract_all(df$my_strings, my_analysis)","['r', 'text-mining', 'stringr', 'tidytext']",72495406,"you can use skip-grams for this:
library(tidyverse)
library(tidytext)

df <- tibble(id = 1:3,
             txt = c(""i bought a beautiful and shiny watch"", 
                     ""i bought a shiny watch"", 
                     ""the watch is very shiny""))

tidy_ngrams <- df %>%
  ## use k for the skip, and n for what degree of n-gram:
  unnest_tokens(ngram, txt, token = ""skip_ngrams"", n_min = 2, n = 2, k = 2) 

tidy_ngrams
#> # a tibble: 33 ï¿½ï¿½ 2
#>       id ngram           
#>    <int> <chr>           
#>  1     1 i bought        
#>  2     1 i a             
#>  3     1 i beautiful     
#>  4     1 bought a        
#>  5     1 bought beautiful
#>  6     1 bought and      
#>  7     1 a beautiful     
#>  8     1 a and           
#>  9     1 a shiny         
#> 10     1 beautiful and   
#> # ï¿½ï¿½ï¿½ with 23 more rows

tidy_ngrams %>%
  filter(ngram == ""bought watch"")
#> # a tibble: 1 ï¿½ï¿½ 2
#>      id ngram   lt;int> <chr>       
#> 1     2 bought watch

created on 2022-06-03 by the reprex package (v2.0.1)",https://stackoverflow.com/questions/72463717,r,01-06-2022 14:38,449.0,0.0,2.0,True,03-06-2022 21:38,01-06-2022 14:39
76337058,how to generate sentiment scores using predefined aspects with deberta-v3-base-absa-v1.1 huggingface model?,"i have a dataframe , where there is text in 1st column and predefine aspect in another column however there is no aspects defined for few text  ,for example row 2.
data = {
    'text': [
        ""the camera quality of this phone is amazing."",
        ""the belt is poor quality"",
        ""the battery life could be improved."",
        ""the display is sharp and vibrant."",
        ""the customer service was disappointing.""
    ],
    'aspects': [
        [""camera"", ""phone""],
        [],
        [""battery"", ""life""],
        [""display""],
        [""customer service""]
    ]
}

df = pd.dataframe(data)


i want to generate two things

using pre define aspect for the text, generate sentiment score
using text generate aspect and also the sentiment score from the package

note: this package yangheng/deberta-v3-base-absa-v1.1
1)generate sentiment score based on predefine aspects
2)generate both aspect and it's respective sentiments
note row 2 does not have predefine aspect
i tried and getting error
import torch
from transformers import autotokenizer, automodelforsequenceclassification
import pandas as pd

# load the absa model and tokenizer
model_name = ""yangheng/deberta-v3-base-absa-v1.1""
tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforsequenceclassification.from_pretrained(model_name)




# generate aspects and sentiments
aspects = []
sentiments = []

for index, row in df.iterrows():
    text = row['text']
    row_aspects = row['aspects']
    
    aspect_sentiments = []
    
    for aspect in row_aspects:
        inputs = tokenizer(text, aspect, return_tensors=""pt"")
        
        with torch.inference_mode():
            outputs = model(**inputs)
        
        predicted_sentiment = torch.argmax(outputs.logits).item()
        sentiment_label = model.config.id2label[predicted_sentiment]
        
        aspect_sentiments.append(f""{aspect}: {sentiment_label}"")
    
    aspects.append(row_aspects)
    sentiments.append(aspect_sentiments)

# add the generated aspects and sentiments to the dataframe
df['generated_aspects'] = aspects
df['generated_sentiments'] = sentiments

# print the updated dataframe
print(df)




generic example to use the package
import torch
import torch.nn.functional as f
from transformers import autotokenizer, automodelforsequenceclassification

model_name = ""yangheng/deberta-v3-base-absa-v1.1""
tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforsequenceclassification.from_pretrained(model_name)

aspects = [""food"", ""service""]
text = ""the food was great but the service was terrible.""
sentiment_aspect = {}
for aspect in aspects:
  inputs = tokenizer(text, aspect, return_tensors=""pt"")

  with torch.inference_mode():
    outputs = model(**inputs)

  scores = f.softmax(outputs.logits[0], dim=-1)
  label_id = torch.argmax(scores).item()
  sentiment_aspect[aspect] = (model.config.id2label[label_id], scores[label_id].item())

print(sentiment_aspect)


desired output","['python', 'nlp', 'huggingface-transformers', 'sentiment-analysis', 'large-language-model']",76337458,"specific to the yangheng/deberta-v3-base-absa-v1.1 model this is the usage and you have to loop through the model one time per aspect:
# load the absa model and tokenizer
model_name = ""yangheng/deberta-v3-base-absa-v1.1""
tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforsequenceclassification.from_pretrained(model_name)

classifier = pipeline(""text-classification"", model=model, tokenizer=tokenizer)


for aspect in ['camera', 'phone']:
   print(aspect, classifier('the camera quality of this phone is amazing.',  text_pair=aspect))

[out]:
camera [{'label': 'positive', 'score': 0.9967294931411743}]
phone [{'label': 'neutral', 'score': 0.9472787380218506}]


to get the zero-shot classification scores in general, try using pipeline:
from transformers import autotokenizer, automodelforsequenceclassification
from transformers import pipeline


# load the absa model and tokenizer
model_name = ""yangheng/deberta-v3-base-absa-v1.1""
tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforsequenceclassification.from_pretrained(model_name)


pipe = pipeline(""zero-shot-classification"", model=model, tokenizer=tokenizer)

pipe(""the camera quality of this phone is amazing."", candidate_labels=[""camera"", ""phone""])

[out]:
{'sequence': 'the camera quality of this phone is amazing.',
 'labels': ['camera', 'phone'],
 'scores': [0.9036691784858704, 0.09633082151412964]}


depending on what ""text generated aspect"" means, perhaps it's keyword extraction, and if so, doing a search on  gives this as the top downloaded model, 
from transformers import autotokenizer, automodelfortokenclassification

tokenizer2 = autotokenizer.from_pretrained(""yanekyuk/bert-uncased-keyword-extractor"")
model2 = automodelfortokenclassification.from_pretrained(""yanekyuk/bert-uncased-keyword-extractor"")



def extract_aspect(text):
    extractor = pipeline(""ner"", model=model2, tokenizer=tokenizer2)
    phrasesids = []
    for tag in extractor(text):
        if tag['entity'].startswith('b'):
            phrasesids.append([tag['start'], tag['end']])
        if tag['entity'].startswith('i'):
            phrasesids[-1][-1] = tag['end']
    phrases = [text[p[0]:p[1]] for p in phrasesids]
    return phrases

text = ""the camera quality of this phone is amazing.""

extract_aspect(text)

[out]:
camera

putting the extractor and classifier together:
from transformers import autotokenizer, automodelforsequenceclassification, automodelfortokenclassification
from transformers import pipeline


# load the absa model and tokenizer
model_name = ""yangheng/deberta-v3-base-absa-v1.1""
tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforsequenceclassification.from_pretrained(model_name)

classifier = pipeline(""zero-shot-classification"", model=model, tokenizer=tokenizer)

tokenizer2 = autotokenizer.from_pretrained(""yanekyuk/bert-uncased-keyword-extractor"")
model2 = automodelfortokenclassification.from_pretrained(""yanekyuk/bert-uncased-keyword-extractor"")


def extract_aspect(text):
    extractor = pipeline(""ner"", model=model2, tokenizer=tokenizer2)
    phrasesids = []
    for tag in extractor(text):
        if tag['entity'].startswith('b'):
            phrasesids.append([tag['start'], tag['end']])
        if tag['entity'].startswith('i'):
            phrasesids[-1][-1] = tag['end']
    phrases = [text[p[0]:p[1]] for p in phrasesids]
    return phrases

text = ""the camera quality of this phone is amazing.""

pipe(text, candidate_labels=extract_aspect(text))

[out]:
{'sequence': 'the camera quality of this phone is amazing.',
 'labels': ['camera'],
 'scores': [0.9983300566673279]}


q: but the extracted keywords is not ""right"" or doesn't match the pre-defined ones?
a: no model is perfect and the model example above is a keyword extractor not a product aspect extractor. ymmv.
q: why isn't the zero-shot classifier giving me negative / positive labels?
a: the zero-shot classifier is labelling the data based on the extracted labels. not a sentiment classifier.",https://stackoverflow.com/questions/76337058,python,26-05-2023 01:06,1047.0,1.0,1.0,True,26-05-2023 08:53,26-05-2023 03:54
69866866,what is the best way to compute metrics for the transformers results?,"here is simple example of hugging face transformers for ner:
from transformers import autotokenizer, automodelfortokenclassification
from transformers import pipeline

tokenizer = autotokenizer.from_pretrained(""dslim/bert-large-ner"")
model = automodelfortokenclassification.from_pretrained(""dslim/bert-large-ner"")

nlp = pipeline(""ner"", model=model, tokenizer=tokenizer)
example = ""my name is jonathan davis and i live in chicago, illinois""

ner_results = nlp(example)
print(ner_results)

    output: [{'entity': 'b-per', 'score': 0.95571744, 'index': 4, 'word': 'j', 'start': 11, 'end':
 12}, {'entity': 'b-per', 'score': 0.6131773, 'index': 5, 'word': '##ona', 'start': 12, 'end': 
15}, {'entity': 'i-per', 'score': 0.6707376, 'index': 6, 'word': '##than', 'start': 15, 'end':
 19}, {'entity': 'i-per', 'score': 0.97754997, 'index': 7, 'word': 'da', 'start': 20, 'end': 22},
 {'entity': 'i-per', 'score': 0.4608973, 'index': 8, 'word': '##vis', 'start': 22, 'end': 25}, 
{'entity': 'b-loc', 'score': 0.9990302, 'index': 13, 'word': 'chicago', 'start': 40, 'end': 47}]

for example i have information about my sentence:
jonathan davis - per
chicago - loc
illinois - loc (the model did not recognize this entity)

how do i correctly calculate the precision and recall, given that my data is split as follows:
j, ##ona, ##than

before that, i used regular expressions and used a metric, the point of which is described in this article. but i do not know if it is suitable for this task.
please help me find the correct way to calculate the metric. perhaps there are some built-in features in hugging-face i'm missing out on?","['python', 'nlp', 'huggingface-transformers']",69870442,"in my opinion, there is something wrong with the model (dslim/bert-large-ner) you're using. according to documents, they have introduced an argument named aggregation_strategy for the exact same purpose (full explanation).
but for some reason, this is not working properly here. now there are two options for the quick fix
first: change the model to one which is working fine.
from transformers import autotokenizer, automodelfortokenclassification
tokenizer = autotokenizer.from_pretrained(""jean-baptiste/camembert-ner"")
model = automodelfortokenclassification.from_pretrained(""jean-baptiste/camembert-ner"")
from transformers import pipeline
nlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=""simple"")
sequence = ""my name is jonathan davis and i live in chicago, illinois""
nlp(sequence)

output:
[{'end': 25,
  'entity_group': 'per',
  'score': 0.9983611,
  'start': 10,
  'word': 'jonathan davis'},
 {'end': 47,
  'entity_group': 'loc',
  'score': 0.9982808,
  'start': 39,
  'word': 'chicago'},
 {'end': 57,
  'entity_group': 'loc',
  'score': 0.99840826,
  'start': 48,
  'word': 'illinois'}]

second translate the output to a more comfortable format to do the rest of the process (probably with the aid of a state machine).",https://stackoverflow.com/questions/69866866,python,06-11-2021 18:40,770.0,1.0,1.0,True,07-11-2021 07:12,06-11-2021 19:06
78309394,how do i make the text bold and have space in between in tkinter python?,"in my code, i only want to make the sender name bold. this means i only want to make ""user"" and ""gpt-3.5-turbo"" bold. for some reason, messages sent by ""user"" is always bold. additionally, there is no new line between ""user"" and ""gpt-3.5-turbo"" as seen in the image i've attached, although there is space in between the very first message by ""user"" and ""gpt-3.5-turbo"". i've tried many commands to fix this issue but no luck. can someone help me out in fixing these 2 issues? below is my code:
import tkinter as tk
from tkinter import ttk
from datetime import datetime
import openai
import json
import requests

history = []

# create a function to use chatgpt 3.5 turbo to answer a question based on the prompt
def get_answer_from_chatgpt(prompt, history):
    openai.api_key = ""sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""
    print(""trying"")

    messages = [
            {""role"": ""system"", ""content"": ""you are a helpful assistant.""}
        ]

    for sender, message in history:
            messages.append({""role"": sender, ""content"": message})

    try:
        stream = openai.chat.completions.create(
            model=""gpt-3.5-turbo"",
            messages=messages,
            stream=true,
            
        )
        append_to_chat_log(""gpt-3.5-turbo"")
        for chunk in stream:
            if chunk.choices[0].delta.content is not none:
               chunk = chunk.choices[0].delta.content
               append_to_chat_log(message=chunk) 
               #history.append((""assistant"", chunk))
        append_to_chat_log(message=""\n"")
        
        #chat_log.insert(""end"",'\n\n')
        
        print(""streamig complete"")        
    except exception as e:
        print(e)
        return ""sorry, an error occurred while processing your request.""


def append_to_chat_log(sender=none, message=none):
   
    chat_log.config(state=tk.normal)
    if sender:
        chat_log.insert(""end"", f""{sender}\n\n"", ""sender"")
        
        #chat_log.insert(""end"",'\n\n')
    if message:
        chat_log.insert(""end"", message)
    chat_log.tag_config(""sender"", font=('arial', 12, 'bold'))
    chat_log.config(state=tk.disabled)
    chat_log.see(""end"")
    chat_log.update()


def send_message(event=none):
    global history
    message = message_entry.get(1.0, ""end-1c"") 
    message = message.strip()
    message_entry.delete(1.0, tk.end)
    message_entry.update()
    
    if not message:
        pass 
    else:
        append_to_chat_log(""user"")
        append_to_chat_log(message)
        history.append((""user"", message))
        if len(history) >4:
            history = history[-4:]

        get_answer_from_chatgpt(message, history)
        print(history)

root = tk.tk()

root.title(""chat"")

# maximize the window
root.attributes('-zoomed', true)

chat_frame = tk.frame(root)
chat_frame.pack(expand=true, fill=tk.both)

chat_log = tk.text(chat_frame, state='disabled', wrap='word', width=70, height=30, font=('arial', 12), highlightthickness=0, borderwidth=0)
chat_log.pack(side=tk.left, padx=(500,0), pady=10)


message_entry = tk.text(root, padx=17, insertbackground='white', width=70, height=1, spacing1=20, spacing3=20, font=('open sans', 14))
message_entry.pack(side=tk.left, padx=(500, 0), pady=(0, 70))  # adjust pady to move it slightly above the bottom
#message_entry.insert(0, ""ask me anything..."")
message_entry.insert(1.0, ""ask me anything..."")
message_entry.mark_set(""insert"", ""%d.%d"" % (0,0))
message_entry.bind(""<return>"", send_message)
#message_entry.bind(""<button-1>"", click)

root.mainloop()

i've tried using commands like chat_log.insert(""end"",'\n\n') and append_to_chat_log(message=""\n"") in def get_answer_from_chatgpt(prompt, history): function but none seem to work.","['python', 'tkinter', 'openai-api', 'tkinter-entry', 'tkinter-layout']",78309616,"the message from user is bold because you call append_to_chat_log() wrong on message and you need to use message option as below:
def send_message(event=none):
    ...
    else:
        append_to_chat_log(""user"")  # log the sender
        append_to_chat_log(message=message)  # log the message
        ...

to add newlines between messages, you can add newlines messages before and after gpt response:
def get_answer_from_chatgpt(message, history):
    ...
    try:
        ...
        # add newlines before response
        append_to_chat_log(message=""\n\n"")
        append_to_chat_log(""gpt-3.5-turbo"")
        for chunk in stream:
            ...
        # add newlines after response
        append_to_chat_log(message=""\n\n"")
        ...
    ...",https://stackoverflow.com/questions/78309394,python,11-04-2024 09:33,112.0,0.0,1.0,True,11-04-2024 10:11,11-04-2024 09:47
59991226,ways of obtaining a similarity metric between two full text documents?,"so imagine i have three text documents, for example (let 3 randomly generated texts).
document 1:

""whole every miles as tiled at seven or. wished he entire esteem mr oh by. possible bed you pleasure civility boy elegance ham. he prevent request by if in pleased. picture too and concern has was comfort. ten difficult resembled eagerness nor. same park bore on be....""

document 2:

""style too own civil out along. perfectly offending attempted add arranging age gentleman concluded. get who uncommonly our expression ten increasing considered occasional travelling. ever read tell year give may men call its. piqued son turned fat income played end wicket...""

if i want to obtain in python (using libraries) a metric on how similar these 2 documents are to a third one (in other words, which one of the 2 documents is more similar to a third one) , what would be the best way to proceed?
edit: i have observed other questions that they answer by comparing individual sentences to other sentences, but i am not interested on that, as i want to compare a full text (consisting on related sentences) against another full text, and obtaining a number (which for example may be bigger than another comparison obtained with a different document which is less similar to the target one)","['python', 'nlp', 'artificial-intelligence', 'topic-modeling']",59991398,"there is no simple answer to this question. as similarities will perform better or worse depending on the particular task you want to perform. 
having said that, you do have a couple of options regarding comparing blocks of text. this post compares and ranks several different ways of computing sentence similarity, which you can then aggregate to perform full document similarity. how to aggregate this? will also depend on your particular task. a simple, but often well-performing approach is to compute the average sentence similarities of the 2 (or more) documents.
other useful links for this topics include:

introduction to information retrieval (free book)
doc2vec (from gensim, for paragraph embeddings, which is probably very suitable for your case)",https://stackoverflow.com/questions/59991226,python,30-01-2020 17:16,1281.0,1.0,2.0,True,30-09-2022 12:10,30-01-2020 17:23
66398873,lookuperror: resource stopwords not found. please use the nltk downloader to obtain the resource,"i wanted to use nltk library in python.
but when i run the code i have this error:
lookuperror: 
**********************************************************************
  resource stopwords not found.
  please use the nltk downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('stopwords')
  
  for more information see: 

  attempted to load corpora/stopwords

  searched in:
    - 'c:\\users\\hossein m/nltk_data'
    - 'c:\\users\\hossein m\\appdata\\local\\programs\\python\\python39\\nltk_data'
    - 'c:\\users\\hossein m\\appdata\\local\\programs\\python\\python39\\share\\nltk_data'
    - 'c:\\users\\hossein m\\appdata\\local\\programs\\python\\python39\\lib\\nltk_data'
    - 'c:\\users\\hossein m\\appdata\\roaming\\nltk_data'
    - 'c:\\nltk_data'
    - 'd:\\nltk_data'
    - 'e:\\nltk_data'
**********************************************************************

but i did it before by nltk.download() command:
import nltk
nltk.download()
nltk.download('stopwords')

from nltk.corpus import stopwords

set(stopwords.words(""english""))","['python', 'text', 'nlp', 'nltk', 'text-mining']",68692990,i solved this problem by download the corresponding zip file on nltk.org then manually setup the c:\nltk_data\corpora dir.,https://stackoverflow.com/questions/66398873,python,27-02-2021 12:36,6512.0,1.0,1.0,True,08-08-2021 19:32,28-02-2021 10:02
67677033,how to count collocations in quanteda based on grouping variables?,"i have been working on identifying and classfying collocations over quenteda package in r.
for instance;
i create token object from a list of documents, and apply collocation analysis.
toks <- tokens(text$abstracts)
collocations <- textstat_collocations(toks)

however, as far as i can see, there is not a clear method to see which collocation(s) is frequent/exist in which document. even if i apply kwic(toks, pattern = phrase(collocations), selection = 'keep') result will only include rowid as text1, text2 etc.
i would like to group collocation analysis results based on docvars. is it possible with quanteda ?","['r', 'nlp', 'quanteda']",67685223,"it sounds like you wish to tally collocations by document.  the output from textstat_collocations() already provides counts for each collocation, but these are for the entire corpus.
so the solution to group by document (or any other variable) is to

get the collocations using textstat_collocations().  below, i've done that after removing stopwords and punctuation.
compound the tokens from which the stopwords were formed, using tokens_compound().  this converts each collocation sequence into a single token.
form a dfm from the compounded tokens, and use textstat_frequency() to count the compounds by document.
this is a bit trickier

implementation using the built-in inaugural corpus:
library(""quanteda"")
## package version: 3.0
## unicode version: 13.0
## icu version: 69.1
## parallel computing: 12 of 12 threads used.
## see  for tutorials and examples.
library(""quanteda.textstats"")

toks <- data_corpus_inaugural %>%
  tail(10) %>%
  tokens(remove_punct = true, padding = true) %>%
  tokens_remove(stopwords(""en""), padding = true)

colls <- textstat_collocations(toks)
head(colls)
##        collocation count count_nested length   lambda        z
## 1           let us    34            0      2 6.257000 17.80637
## 2  fellow citizens    14            0      2 6.451738 16.18314
## 3 fellow americans    15            0      2 6.221678 16.16410
## 4      one another    14            0      2 6.592755 14.56082
## 5        god bless    15            0      2 8.628894 13.57027
## 6    united states    12            0      2 9.192044 13.22077

now we compound them and keep only the collocations, then get the frequencies by document:
dfmat <- tokens_compound(toks, colls, concatenator = "" "") %>%
  dfm() %>%
  dfm_keep(""* *"")

that dfm already contains the counts by document of each collocation, but if you want counts in a data.frame format, with a grouping option, use textstat_frequency().  here i've only output the top two by document, but if you remove the n = 2 then it will give you the frequencies of all collocations by document.
textstat_frequency(dfmat, groups = docnames(dfmat), n = 2) %>%
  head(10)
##             feature frequency rank docfreq        group
## 1   nuclear weapons         4    1       1  1985-reagan
## 2     human freedom         3    2       1  1985-reagan
## 3        new breeze         4    1       1    1989-bush
## 4    new engagement         3    2       1    1989-bush
## 5            let us         7    1       1 1993-clinton
## 6  fellow americans         4    2       1 1993-clinton
## 7            let us         6    1       1 1997-clinton
## 8       new century         6    1       1 1997-clinton
## 9  nation's promise         2    1       1    2001-bush
## 10      common good         2    1       1    2001-bush",https://stackoverflow.com/questions/67677033,r,24-05-2021 18:18,619.0,1.0,1.0,True,21-06-2021 14:04,25-05-2021 09:28
78366661,i get an empty array from vector serch in mongodb with langchain,"i have the code:
loader = pypdfloader(ï¿½ï¿½ï¿½
data = loader.load()
docs = text_splitter1.split_documents(data)
vector_search_index = ï¿½ï¿½ï¿½vector_indexï¿½ï¿½ï¿½

vector_search = mongodbatlasvectorsearch.from_documents(
  documents=docs,
  embedding=openaiembeddings(disallowed_special=()),
  collection=atlas_collection,
  index_name=vector_search_index,
)

query = ""what were the compute requirements for training gpt 4""
results = vector_search1.similarity_search(query)
print(""result: "", results)

and in results i have every time only empty array. i don't understand what i do wro"" rel=""nofollow noreferrer"">link  on the langchain documentation with examples. information is saved normally in database, but i cannot search info in this collection.","['python', 'mongodb', 'langchain', 'py-langchain', 'vector-search']",78368576,"so i was able to get this to work in mongodb with the following code:
text_splitter = recursivecharactertextsplitter(chunk_size=1000, chunk_overlap=150)

loader = pypdfloader(""
data = loader.load()
docs = text_splitter.split_documents(data)

db_name = ""langchain_db""
collection_name = ""atlas_collection""
atlas_vector_search_index_name = ""vector_index""
mongodb_atlas_cluster_uri = uri = os.environ.get(""mongo_db_endpoint"")

client = mongoclient(mongodb_atlas_cluster_uri)
mongodb_collection = client[db_name][collection_name]

vector_search = mongodbatlasvectorsearch.from_documents(
    documents=docs,
    embedding=openaiembeddings(disallowed_special=()),
    collection=mongodb_collection,
    index_name=atlas_vector_search_index_name,
)

query = ""what were the compute requirements for training gpt 4""
results = vector_search.similarity_search(query)
print(""result: "", results)

at this point, i did get the same results that you did. before it would work, i had to create the vector search index and i made sure it was named the same as what is specified in atlas_vector_search_index_name:

fwiw - it was easier for me to do in astra db (i tried this first, because i am a datastax employee):
text_splitter = recursivecharactertextsplitter(chunk_size=1000, chunk_overlap=150)

loader = pypdfloader(""
data = loader.load()
docs = text_splitter.split_documents(data)
atlas_collection = ""atlas_collection""

astra_db_api_endpoint = os.environ.get(""astra_db_api_endpoint"")
astra_db_application_token = os.environ.get(""astra_db_application_token"")

vector_search = astradbvectorstore.from_documents(
  documents=docs,
  embedding=openaiembeddings(disallowed_special=()),
  collection_name=atlas_collection,
  api_endpoint=astra_db_api_endpoint,
  token=astra_db_application_token,
)

query = ""what were the compute requirements for training gpt 4""
results = vector_search.similarity_search(query)
print(""result: "", results)

worth noting, that astra db will create your vector index automatically based on the dimensions of the embedding model.",https://stackoverflow.com/questions/78366661,python,22-04-2024 13:37,444.0,3.0,2.0,True,12-08-2024 05:38,22-04-2024 13:45
68512112,tokenize characters except when encapsulated by brackets and keep brackets,"i am parsing my keystroke data. it looks something like this:
> key_data = 'stuff[up][left][return]end'

i want to tokenize the characters, but treat the modifiers surrounded by [] as a single token.
> print(key_tokens)
['s','t','u','f','f','[up]','[left]','[return]','e','n','d']

i know i can do something like this to find the encapsulated sections:
> key_tokens = re.split(r'([\[\]])', key_data)
> print(key_tokens)
['stuff','[','up',']','[','left',']','[','return',']','end']

i can also of course do something like this to separate each character:
> key_tokens = [c for c in key_data]
> print(key_tokens)
['s','t','u','f','f','[','u','p',']','[','l','e','f','t',']','[','r','e','t','u','r','n',']','e','n','d']

i am just having trouble putting it all together.
edit: now i am seeing a corner case where the opening square bracket is used as text. unfortunately, it is not escaped or anything.
> key_data = 'stuff[but[up][left][return]end'
> key_tokens = re.findall('\[.*?\]|.', key_data)
> print(key_tokens)
['s','t','u','f','f','[but[up]','[left]','[return]','e','n','d']

what i want to see is:
> print(key_tokens)
['s','t','u','f','f','[','b','u','t','[up]','[left]','[return]','e','n','d']","['python', 'regex', 'nlp']",68512160,"if you don't mind using re.findall, instead of re.split, you can first try to match the pattern for anything inside squared bracket using \[.*?\], if not, then you can just take a single character that's what |. is doing, it will match 1-length any character, if you just have word characters (i.e. alphabets) as you have in sample data, you can consider using |\w:
>>> re.findall('\[.*?\]|.', key_data)

['s', 't', 'u', 'f', 'f', '[up]', '[left]', '[return]', 'e', 'n', 'd']

for updated question:
if that's the case, you may consider using alternatives to regex since it is not so good at handling these types of nesting, since value needs to compared back and forth. here is a non-regex solution:
result = []
idx = 0
while true:
    c = key_data[idx]
    if c != '[':
        idx += 1
        result.append(c)  #append if not [
    else:
        closingindex = key_data[idx+1:].find(']') # find if ] exist after current [
        if closingindex == -1:
            #append the rest sub-srting and break since no ] after current [
            result.extend(key_data[idx:])
            break
        else:
            # check if [ in the  middle, append only c if true
            if '[' in key_data[idx+1:idx+closingindex+2]:
                result.append(c)
                idx += 1
            else:
                #extend from [ to the nearest ]
                result.append(key_data[idx:idx+closingindex+2])
                idx += closingindex+2
    if idx>=len(key_data): break  #break loop if idx exceeds maximum value",https://stackoverflow.com/questions/68512112,python,24-07-2021 17:00,269.0,1.0,1.0,True,24-07-2021 21:23,24-07-2021 20:25
475033,detecting programming language from a snippet,what would be the best way to detect what programming language is used in a snippet of code?,"['programming-languages', 'language-detection']",475041,"i think that the method used in spam filters would work very well. you split the snippet into words. then you compare the occurences of these words with known snippets, and compute the probability that this snippet is written in language x for every language you're interested in.

if you have the basic mechanism then it's very easy to add new languages: just train the detector with a few snippets in the new language (you could feed it an open source project). this way it learns that ""system"" is likely to appear in c# snippets and ""puts"" in ruby snippets.
i've actually used this method to add language detection to code snippets for forum software. it worked 100% of the time, except in ambiguous cases:
print ""hello""

let me find the code.
i couldn't find the code so i made a new one. it's a bit simplistic but it works for my tests. currently if you feed it much more python code than ruby code it's likely to say that this code:
def foo
   puts ""hi""
end

is python code (although it really is ruby). this is because python has a def keyword too. so if it has seen 1000x def in python and 100x def in ruby then it may still say python even though puts and end is ruby-specific. you could fix this by keeping track of the words seen per language and dividing by that somewhere (or by feeding it equal amounts of code in each language).
class classifier
  def initialize
    @data = {}
    @totals = hash.new(1)
  end

  def words(code)
    code.split(/[^a-z]/).reject{|w| w.empty?}
  end

  def train(code,lang)
    @totals[lang] += 1
    @data[lang] ||= hash.new(1)
    words(code).each {|w| @data[lang][w] += 1 }
  end

  def classify(code)
    ws = words(code)
    @data.keys.max_by do |lang|
      # we really want to multiply here but i use logs 
      # to avoid floating point underflow
      # (adding logs is equivalent to multiplication)
      math.log(@totals[lang]) +
      ws.map{|w| math.log(@data[lang][w])}.reduce(:+)
    end
  end
end

# example usage

c = classifier.new

# train from files
c.train(open(""code.rb"").read, :ruby)
c.train(open(""code.py"").read, :python)
c.train(open(""code.cs"").read, :csharp)

# test it on another file
c.classify(open(""code2.py"").read) # => :python (hopefully)",https://stackoverflow.com/questions/475033,programming-languages,23-01-2009 23:16,126220.0,134.0,16.0,True,26-12-2022 06:38,26-10-2013 17:31
67058709,bert - is that needed to add new tokens to be trained in a domain specific environment?,"my question here is no how to add new tokens, or how to train using a domain-specific corpus, i'm already doing that.
the thing is, am i supposed to add the domain-specific tokens before the mlm training, or i just let bert figure out the context? if i choose to not include the tokens, am i going to get a poor task-specific model like ner?
to give you more background of my situation, i'm training a bert model on medical text using portuguese language, so, deceased names, drug names, and other stuff are present in my corpus, but i'm not sure i have to add those tokens before the training.
i saw this one: using pretrained bert model to add additional words that are not recognized by the model
but the doubts remain, as other sources say otherwise.
thanks in advance.","['nlp', 'bert-language-model', 'huggingface-transformers', 'huggingface-tokenizers']",67138993,"yes, you have to add them to the models vocabulary.
tokenizer = berttokenizer.from_pretrained(model_name)
tokenizer.add_tokens(['new', 'rdemorais', 'blabla'])
model = bert.from_pretrained(model_name, return_dict=false)
     
model.resize_token_embeddings(len(tokenizer))

the last line is important and needed since you change the numbers of tokens in the model's vocabulary, you also need to update the model correspondingly.",https://stackoverflow.com/questions/67058709,nlp,12-04-2021 12:51,1532.0,3.0,1.0,True,17-04-2021 14:01,14-04-2021 09:52
66655023,longformer get last_hidden_state,"i am trying to follow this example in the huggingface documentation here 
import torch
from transformers import longformermodel, longformertokenizer
model = longformermodel.from_pretrained('allenai/longformer-base-4096')
tokenizer = longformertokenizer.from_pretrained('allenai/longformer-base-4096')
sample_text = ' '.join(['hello world! '] * 1000)  # long input document
input_ids = torch.tensor(tokenizer.encode(sample_text)).unsqueeze(0)  # batch of size 1
# attention mask values -- 0: no attention, 1: local attention, 2: global attention
attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to local attention
global_attention_mask = torch.zeros(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to global attention to be deactivated for all tokens
global_attention_mask[:, [1, 4, 21,]] = 1  # set global attention to random tokens for the sake of this example
                                    # usually, set global attention based on the task. for example,
                                    # classification: the <s> token
                                    # qa: question tokens
                                    # lm: potentially on the beginning of sentences and paragraphs
outputs = model(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask, output_hidden_states= true)
sequence_output = outputs[0].last_hidden_state
pooled_output = outputs.pooler_output

i suppose that this would return a document embedding for the sample text.
however, i run into the following error:
attributeerror: 'tensor' object has no attribute 'last_hidden_state'

why isnt it possible to call last_hidden_state?","['python', 'nlp', 'pytorch', 'huggingface-transformers']",66658405,"do not select via index:
sequence_output = outputs.last_hidden_state

outputs is a longformerbasemodeloutputwithpooling object with the following properties:
print(outputs.keys())

output:
odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])

calling outputs[0] or outputs.last_hidden_state will both give you the same tensor, but this tensor does not have a property called last_hidden_state.",https://stackoverflow.com/questions/66655023,python,16-03-2021 12:14,7406.0,3.0,2.0,True,21-05-2024 20:11,16-03-2021 15:34
69006154,how would i detect a pattern in text?,"i am creating a python script that is able to understand human inputted text for actions to make on the stock market. for example all of these mean the same thing:

$example 12 aug 21 $22.5 call average price = $0.8
$example $22.5 calls exp. 8/12 @ $0.8
$example $22.5 calls exp. 8/12 @ $0.8
$example $22.5c 8/12 @ .8ï¿½ï¿½
$example 22.5c 8/12 @ .8
lorem ipsum dolor sit amet, consectetur $example 22.5c 8/12 @ .8 adipiscing elit, sed do eiusmod

the components that i need to extract: the ticker ($example), the call price ($22.5), expiration (8/12 or august 12), and the average price ($0.8). the issue is that sometimes formatting differs as shown in the examples above. sometimes the 'calls' will be denoted as 22.5c and sometimes the average price may be written as average price = 0.8 or @ 0.trong>. something else to note: sometimes the string may have text prefixing or following the parts i would like to extract (as shown in the last example).
how should i approach this? will machine learning be useful in this case since the formatting isn't the same every time?","['python', 'nlp', 'stock']",69006577,"this is not a trivial problem, and i assume there will be cases where two humans would give different answers for the same text. especially if you don't want to specify the allowed formats.
i would recommend going with an approach similar to what pw1990 suggested, it's the most predictable and maintainable solution. however, instead of having only one regexp per semantic structure, you could have a method that tries its best to extract a property with a set of regexps.
if you have enough data, you could also do machine learning. for that, you need to have a dataset with a mapping of each ""text"" to each {action}. from my experience, i would start with 100k-1m data points (mappings).
you could try a bidirectional rnn that returns a number for each character, assigning it to one of the classes [ticker, price, avg, date, type, other]. then you could parse and validate each of the substrings manually. this is straightforward if you know what you are parsing.
of course, you could try an end-to-end approach where an rnn returns you a reformatted string that you feed into a regexp. however, it significantly increases the complexity of the model and the amount of data you need.
there was a similar stackoverflow thread here: 
my personal recommendation is to avoid machine learning if it is not absolutely needed. ml is not a silver bullet, and it requires a lot of work to get it correctly. so it will be a project of its own. on the other hand, regular expressions are a much more straightforward solution for this problem, although i understand that it is tedious to hardcode all possible regexps. just keep in mind that most likely, you are aiming at getting 99% of the cases.",https://stackoverflow.com/questions/69006154,python,31-08-2021 23:10,2006.0,0.0,2.0,True,01-09-2021 17:17,01-09-2021 17:17
78335181,wrong behavior in stemming for nonenglish languages?,"i am working in a project with spanish text, summing up, none of the stemmers that i have seen in the documentation for spanish give me good results (only 2, snowball and the normal one), to give an example.
{
  ""tokenizer"": ""standard"",
  ""filter"": [ 
    {
      ""type"": ""snowball"",
      ""language"": ""spanish""
    }
  ],
  ""text"": ""alimento, alimentacion""
}

the previous query returns the following:
{
  ""tokens"" : [
    {
      ""token"" : ""aliment"",
      ""start_offset"" : 0,
      ""end_offset"" : 8,
      ""type"" : ""<alphanum>"",
      ""position"" : 0
    },
    {
      ""token"" : ""alimentacion"",
      ""start_offset"" : 10,
      ""end_offset"" : 22,
      ""type"" : ""<alphanum>"",
      ""position"" : 1
    }
  ]

when clearly ""alimento"" and ""alimentacion"" should have the same root, is there a way to look for other stemmers?","['elasticsearch', 'stemming', 'spanish']",78774389,"as it's been mentioned, while alimentaciï¿½ï¿½n gets properly stemmed as aliment, and alimentacion doesn't. nevertheless, i found this link from the official documentation of elastic search that allows to defined custom stemming patters.
in your case, you just need to add a new filter just before the stemmer:
""filter"": {
    ""custom_stems"": {
      ""type"": ""stemmer_override"",
      ""rules"": [
        ""alimentacion => aliment""
      ]
    }
  }",https://stackoverflow.com/questions/78335181,elasticsearch,16-04-2024 14:08,24.0,1.0,2.0,True,21-07-2024 04:37,16-04-2024 14:10
70632384,pandas: text analysis: transfer raw data to dataframe,"i need to read lines from a text file and extract the
quoted person name and quoted text from each line.
lines look similar to this:

""am i ever!"", homer simpson responded.


remarks:
hint: use the returned object from the 'open' method to get the file
handler. each line you read is expected to contain a new-line in the
end of the line. remove the new-line as following: line_cln =line.strip()


there are the options for each line (assume one of these
three options): the first set of patterns, for which the person name
appears before the quoted text. the second set of patterns, for which
the quoted text appears before the person. empty lines.


complete the transfer_raw_text_to_dataframe function to return a
dataframe    with the extracted person name and text as explained
above.  the information is expected to be extracted from the lines of
the given 'filename' file.


the returned dataframe should include two columns:

person_name - containing the extracted person name for each line.
extracted_text - containing the extracted quoted text for each line.

the returned values:

dataframe - the dataframe with the extracted information as described above.


important note: if a line does not contain any quotation pattern, no information should be saved in the
corresponding row in the dataframe.


what i got so far: [edited]
def transfer_raw_text_to_dataframe(filename):

    data = open(filename)
    
    quote_pattern ='""(.*)""'
    name_pattern = ""\w+\s\w+""
    
    df = open(filename, encoding='utf8')
    lines = df.readlines()
    df.close()
    dataframe = pd.dataframe(columns=('person_name', 'extracted_text'))
    i = 0  

    for line in lines:
        quote = re.search(quote_pattern,line)
        extracted_quotation = quote.group(1)

        name = re.search(name_pattern,line)
        extracted_person_name = name.group(0)
        
        df2 = {'person_name': extracted_person_name, 'extracted_text': extracted_quotation}
        dataframe = dataframe.append(df2, ignore_index = true)

        dataframe.loc[i] = [person_name, extracted_text]
        i =i+1
            
    return dataframe

the dataframe is created with the correct shape, problem is, the person name in each row is: 'oh man' and the quote is 'oh man, that guy's tough to love.' (in all of them)
which is weird because it's not even in the txt file...
can anyone help me fix this?
edit: i need to extract from a simple txt file that contains these lines only:
""am i ever!"", homer simpson responded.
""hmmm. so... is it okay if i go to the women's conference with chloe?"", lisa simpson answered.
""really? uh, sure."", bart simpson answered.
""sounds great."", bart simpson replied.
homer simpson responded: ""danica patrick in my thoughts!""
c. montgomery burns: ""trust me, he'll say it, or i'll bust him down to thursday night vespers.""
""gimme that torch."" lisa simpson said.
""no! no, i've got a lot more mothering left in me!"", marge simpson said.
""oh, homie, i don't care if you're a billionaire. i love you just because you're..."" marge simpson said.
""damn you, e-bay!"" homer simpson answered.","['python', 'pandas', 'dataframe', 'nlp', 'python-re']",70634341,"possibly in such a way:
import pandas as pd
import re

# do smth
with open(""12.txt"", ""r"") as f:
    data = f.read()
    # print(data)
    
    # ########## findall text in quotes
    m = re.findall(r'\""(.+)\""', data)
    print(""result: \n"", m)
    df = pd.dataframe({'rep': m})
    print(df)
    
    # ##########  retrieve and replace text in quotes for nothing
    m = re.sub(r'\""(.+)\""', r'', data)
    
    # ##########  get first name & last name from the rest text in each line
    regex = re.compile(""([a-z]{1}[a-z]+ [a-z]{1}[a-z]+)"")
    mm = regex.findall(m)
    df1 = pd.dataframe({'author': mm})
    print(df1)

    # ########## join 2 dataframes
    fin = pd.concat([df, df1], axis=1)
    print(fin)

all print just for checking (get them away for cleaner code).
just ""c. montgomery burns"" is loosing his first letter...",https://stackoverflow.com/questions/70632384,python,08-01-2022 12:39,738.0,2.0,3.0,True,09-10-2022 20:28,08-01-2022 17:00
67715251,"how do i make spacy choose noun chunks separated by &quot;and&quot; or &quot;,&quot; as one","i'm sorry about the title, i really didn't know how to phrase it, but hopefully this example will make it clear.
basically,
for the following sentence:
ashley and brian are drinking water.
i want the noun chunk to be ""ashley and brian"" instead it is, ""ashley"", ""brian""
another example is:
types of clothes include shirts, pants and trousers.
i want the noun chunk to be ""shirts, pants and trousers"" instead of ""shirts"" ""pants"" ""trousers""
how do i solve this problem?","['spacy', 'spacy-3']",67716428,"what you are describing is not a noun chunk. the conjuncts feature is closer to what you want.
this might not work for complex sentences, but at least it'll cover your examples and typical cases.
import spacy

nlp = spacy.load(""en_core_web_sm"")

texts = [
        ""ashley and brian are drinking water."",
        ""types of clothes include shirts, pants and trousers."",
        ]

for text in texts:
    print(""-----"")
    print(text)
    checked = 0
    doc = nlp(text)
    for tok in doc:
        if tok.i < checked: continue
        if tok.pos_ not in ('noun', 'propn'): continue

        if tok.conjuncts:
            print(doc[tok.left_edge.i:tok.right_edge.i+1])
            checked = tok.right_edge.i + 1",https://stackoverflow.com/questions/67715251,spacy,27-05-2021 03:22,993.0,2.0,1.0,True,27-05-2021 05:56,27-05-2021 05:44
76238212,bert model splits words by its own,"i am tokenizing the input words using bert model.
the code is :
tokenizer = berttokenizer.from_pretrained('bert-base-multilingual-cased',do_lower_case = false)
model = bertmodel.from_pretrained(""bert-base-multilingual-cased"", add_pooling_layer=false, output_hidden_states=true, output_attentions=true)

marked_text =  text + "" [sep]""
    tokenized_text = tokenizer.tokenize(marked_text)
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
    print(tokenized_text)
    print(indexed_tokens)

the model i used is from huggingface.
my goal is to print the embedded vectors of all words bert model has, so i searched and found that this model has 119296 tokens available.
i don't know this number of the tokens is reason, but the model splits the words by its own, which is unwanted for me.
for example,

only -> [only]
only -> [on,l,y]

stradivarius -> ['st', '##radi', '##vari', '##us']

is this natural bert thing or i am doing something wrong ?","['python', 'nlp', 'huggingface-transformers', 'bert-language-model']",76242730,"you are not doing anything wrong. bert uses a so-called wordpiece subword tokenizer as a compromise for meaningful embeddings and acceptable memory consumption between a character-level (small vocabulary) and a word-level tokenizer (large vocabulary).
a common approach to retrieve word embeddings from a subword-based model is to take the mean of the respective tokens. the code below shows you have you can retrieve the word embeddings (non-contextualized and contextualized) by taking the mean. it uses a fasttokenizer to utilize the methods of the batchencoding object.
import torch
from transformers import berttokenizerfast, bertmodel

t = berttokenizerfast.from_pretrained('bert-base-multilingual-cased')
# whole model
m = bertmodel.from_pretrained(""bert-base-multilingual-cased"")
# token embedding layer
embedding_layer = m.embeddings.word_embeddings

sample_sentence = 'this is an example with token-embeddings and word-embeddings'
encoded = t([sample_sentence])
# the batchencoding object allows us to map the token back to the string indices
print(*[(token_id, encoded.token_to_chars(idx)) for idx, token_id in enumerate(encoded.input_ids[0])], sep=""\n"")
# and we can also check the mapping of word to token indices
print(*[(word, encoded.word_to_tokens(idx)) for idx, word in enumerate(sample_sentence.split())], sep=""\n"")

output:
(101, none)
(10747, charspan(start=0, end=4))
(10124, charspan(start=5, end=7))
(10151, charspan(start=8, end=10))
(14351, charspan(start=11, end=18))
(10169, charspan(start=19, end=23))
(18436, charspan(start=24, end=27))
(10136, charspan(start=27, end=29))
(118, charspan(start=29, end=30))
(10266, charspan(start=30, end=32))
(33627, charspan(start=32, end=35))
(13971, charspan(start=35, end=39))
(10107, charspan(start=39, end=40))
(10111, charspan(start=41, end=44))
(12307, charspan(start=45, end=49))
(118, charspan(start=49, end=50))
(10266, charspan(start=50, end=52))
(33627, charspan(start=52, end=55))
(13971, charspan(start=55, end=59))
(10107, charspan(start=59, end=60))
(102, none)
('this', tokenspan(start=1, end=2))
('is', tokenspan(start=2, end=3))
('an', tokenspan(start=3, end=4))
('example', tokenspan(start=4, end=5))
('with', tokenspan(start=5, end=6))
('token-embeddings', tokenspan(start=6, end=8))
('and', tokenspan(start=8, end=9))
('word-embeddings', tokenspan(start=9, end=13))

to retrieve the word embeddings:
with torch.inference_mode():
  token_embeddings = embedding_layer(encoded.convert_to_tensors(""pt"").input_ids).squeeze()
  # we need the attention mechanism of the whole model to get the contextualized token representations
  contextualized_token_embeddings = m(**encoded.convert_to_tensors(""pt"")).last_hidden_state.squeeze()

def fetch_word_embeddings(sample_sentence:str, encoded, embeddings:torch.tensor) -> dict[str,torch.tensor]:
  word_embeddings = {}
  for idx, word in enumerate(sample_sentence.split()):
    start, end = encoded.word_to_tokens(idx)
    word_embeddings[word] = embeddings[start:end].mean(dim=0)
  return word_embeddings

word_embeddings = fetch_word_embeddings(sample_sentence, encoded, token_embeddings)
contextualized_word_embeddings = fetch_word_embeddings(sample_sentence, encoded, contextualized_token_embeddings)
print(word_embeddings[""token-embeddings""])
print(contextualized_word_embeddings[""token-embeddings""])

output:
tensor([ 1.2455e-02, -3.8478e-02,  8.0834e-03, ..., -1.8502e-02,  1.1511e-02, -6.5307e-02])
tensor([-5.1564e-01, -1.6266e-01, -3.9420e-01, ..., -5.9969e-02,  3.0784e-01, -3.4451e-01])",https://stackoverflow.com/questions/76238212,python,12-05-2023 16:17,1936.0,2.0,2.0,True,17-12-2024 10:04,13-05-2023 12:28
76165522,using openai api from firebase cloud functions in flutter app,"updated
i can't figure out how to make this work.
this is my cloud function in javascript. i'm trying a simple code to see if the connection works (other cloud functions not using the openai package do work fine).
the function does work on gcp when i test it, but won't connect to my flutter function for some reason:
error:
flutter: error calling firebase function: internal response is missing data field.
cloud function
const functions = require('firebase-functions');
const admin = require('firebase-admin');
const axios = require('axios');
const cors = require('cors')({ origin: true });
admin.initializeapp();

exports.openairesponse2 = functions. (request, response) => {
  const apikey = 'sk-';
  cors(request, response, async () => {
    try {
      const apiresponse = await axios.post(
        '
        {
          model: 'gpt-3.5-turbo',
          messages: [{ role: 'user', content: 'say this is a test!' }],
          temperature: 0.7,
        },
        {
          headers: {
            'authorization': `bearer ${apikey}`,
            'content-type': 'application/json',
          },
        },
      );

     // response.status(200).json({ message: 'openai api connection successful.', engines: apiresponse.data });
        response.status(200).json({
              message: 'openai api connection successful.',
              completion: apiresponse.data.choices[0].message.content,
            });
    } catch (error) {
      console.error('error connecting to openai api:', error);
      response.status(500).send('error connecting to openai api');
    }
  });
});

flutter:
future<void> talktome2() async {
  try {
     callable = firebasefunctions.instance.
    print(callable);
    final response = await callable.call();
    print('response is $response');
    print('external api response: ${response.data}');
  } on firebasefunctionsexception catch (e) {
    print('error calling firebase function: ${e.code} ${e.message}');
  } catch (e) {
    print('error calling firebase function: $e');
  }
}

this is my package.json
{
  ""name"": ""functions"",
  ""description"": ""cloud functions for firebase"",
  ""scripts"": {
    ""serve"": ""firebase emulators:start --only functions"",
    ""shell"": ""firebase functions:shell"",
    ""start"": ""npm run shell"",
    ""deploy"": ""firebase deploy --only functions"",
    ""logs"": ""firebase functions:log""
  },
  ""engines"": {
    ""node"": ""16""
  },
  ""main"": ""index.js"",
  ""dependencies"": {
    ""firebase-admin"": ""^11.5.0"",
    ""firebase-functions"": ""^4.2.0"",
    ""openai"": ""^3.2.1""
  },
  ""devdependencies"": {
    ""firebase-functions-test"": ""^3.0.0""
  },
  ""private"": true
}


and my flutter doctor
[ï¿½ï¿½ï¿½] flutter (channel stable, 3.7.12, on macos 13.2 22d49 darwin-arm64, locale en-es)
[ï¿½ï¿½ï¿½] android toolchain - develop for android devices (android sdk version 33.0.0-rc1)
[ï¿½ï¿½ï¿½] xcode - develop for ios and macos (xcode 14.1)
[ï¿½ï¿½ï¿½] chrome - develop for the web
[ï¿½ï¿½ï¿½] android studio (version 2021.1)
[ï¿½ï¿½ï¿½] vs code (version 1.77.3)
[ï¿½ï¿½ï¿½] connected device (4 available)
[ï¿½ï¿½ï¿½] http host availability","['node.js', 'firebase', 'google-cloud-functions', 'openai-api']",76277392,"i am not a flutter developer but  is for oncall not onrequest.


if you look at the flutter sample, you should have something like below for  callable
const functions = require('firebase-functions');

exports.listfruit = functions. context) => { //<<<-- not onrequest
  return [""apple"", ""banana"", ""cherry"", ""date"", ""fig"", ""grapes""]
});",https://stackoverflow.com/questions/76165522,node.js,03-05-2023 15:13,975.0,0.0,1.0,True,18-05-2023 02:43,04-05-2023 14:03
58908050,my text classifier model doens&#39;t improve with multiple classes,"i'm trying to train a model for a text classification and the model take a list of maximum 300 integer embedded from articles. the model trains without problem and all but the accuracy won't go up. 
the target consists of 41 categories encoded into int from 0 to 41 and were then normalized.
the table would look like this

also, i don't know how my model should look like since i refered on two different example as per below

a binary classifier with one input column and one output column example 1
multiple class classifier with multiple columns as input example 2

i have tried modifying my model based on both model but the model accuracy won't change and even getting lower per epoch
should i add more layers to my model or i have done something stupid that i haven't realized?
note: if the 'df.pickle' download link broken, use this link
from sklearn.model_selection import train_test_split
from urllib.request import urlopen
from os.path import exists
from os import mkdir
import tensorflow as tf
import pandas as pd
import pickle

# define dataframe path
df_path = 'df.pickle'

# check if local dataframe exists
if not exists(df_path):
  # download binary from dropbox
  content = urlopen('

  # write to file
  with open(df_path, 'wb') as file: file.write(content)

  # load the dataframe from bytes
  df = pickle.loads(content)
# if the file exists (aka. downloaded)
else:
  # load the dataframe from file
  df = pickle.load(open(df_path, 'rb'))

# normalize the category
df['category_code'] = df['category_code'].apply(lambda x: x / 41)

train_df, test_df = [pd.dataframe() for _ in range(2)]

x_train, x_test, y_train, y_test = train_test_split(df['content_parsed'], df['category_code'], test_size=0.15, random_state=8)
train_df['content_parsed'], train_df['category_code'] = x_train, y_train
test_df['content_parsed'], test_df['category_code'] = x_test, y_test

# variable containing the number of words we want to keep in our vocabulary
num_words = 10000
# input/token length
seq_len = 300

# create tokenizer for our data
tokenizer = tf.keras.preprocessing.text.tokenizer(num_words=num_words, oov_token='<unk>')
tokenizer.fit_on_texts(train_df['content_parsed'])

# convert text data to numerical indexes
train_seqs=tokenizer.texts_to_sequences(train_df['content_parsed'])
test_seqs=tokenizer.texts_to_sequences(test_df['content_parsed'])

# pad data up to seq_len (note that we truncate if there are more than seq_len tokens)
train_seqs=tf.keras.preprocessing.sequence.pad_sequences(train_seqs, maxlen=seq_len, padding=""post"")
test_seqs=tf.keras.preprocessing.sequence.pad_sequences(test_seqs, maxlen=seq_len, padding=""post"")

# create models folder if not exists
if not exists('models'): mkdir('models')

# define local model path
model_path = 'models/model.pickle'

# check if model exists/pre-trained
if not exists(model_path):
  # define word embedding size
  embedding_size = 16

  # create new model
  '''
  model = tf.keras.sequential([
    tf.keras.layers.embedding(num_words, embedding_size),
    tf.keras.layers.bidirectional(tf.keras.layers.lstm(embedding_size)),
    # tf.keras.layers.dense(embedding_size, activation='relu'),
    tf.keras.layers.dense(1, activation='sigmoid')
  ])
  '''
  model = tf.keras.sequential([
      tf.keras.layers.embedding(num_words, embedding_size),
      # tf.keras.layers.bidirectional(tf.keras.layers.lstm(embedding_size)),
      tf.keras.layers.globalaveragepooling1d(),
      tf.keras.layers.dense(embedding_size, activation='relu'),
      tf.keras.layers.dense(1, activation='sigmoid')
  ])

  # compile the model
  model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
  )

  # stop training when a monitored quantity has stopped improving.
  es = tf.keras.callbacks.earlystopping(monitor='val_acc', mode='max', patience=1)

  # define batch size (can be tuned to improve model accuracy)
  batch_size = 16
  # define number or cycle to train
  epochs = 20

  # using gpu (if error means you don't have gpu. use cpu instead)
  with tf.device('/gpu:0'):
    # train/fit the model
    history = model.fit(
      train_seqs, 
      train_df['category_code'].values, 
      batch_size=batch_size, 
      epochs=epochs, 
      validation_split=0.2,
      validation_steps=30,
      callbacks=[es]
    )

  # evaluate the model
  model.evaluate(test_seqs, test_df['category_code'].values)

  # save the model into a file
  with open(model_path, 'wb') as file: file.write(pickle.dumps(model))
else:
  # load the model
  model = pickle.load(open(model_path, 'rb'))

# check the model
model.summary()","['python', 'pandas', 'tensorflow', 'machine-learning', 'text-classification']",58969447,"after 2 days of tweaking and understanding more examples i found this website which explains quite well about the multi-class classification.
the details of changes i made are as follows:

since i'm going to build a model for multiple classes, during the model compilation the model should use categorical_crossentropy as it's loss function instead of binary_crossentropy.

the model should produce number of output with similar length as your total class you're going to classify which in my case 41. (one hot encoding)

the last layer's activation function should be ""softmax"" since we're choosing a label with the highest confidence level (closest to 1.0).

you will need to tweak the layers accordingly based on the number of classes you're going to classify. see here on how to improve your model.


my final code would look something just like this
from sklearn.model_selection import train_test_split
from urllib.request import urlopen
from functools import reduce
from os.path import exists
from os import listdir
from sys import exit
import tensorflow as tf
import pandas as pd
import pickle
import re

# specify dataframe path
df_path = 'df.pickle'
# check if the file exists
if not exists(df_path):
  # specify url of the dataframe binary
  url = '
  # read the byte content from url
  content = urlopen(url).read()
  # write to a file to save up time
  with open(df_path, 'wb') as file: file.write(pickle.dumps(content))
  # unpickle the dataframe
  df = pickle.loads(content)
else:
  # load the pickle dataframe
  df = pickle.load(open(df_path, 'rb'))

# useful variables
max_num_words = 50000                        # vocabulary size for our tokenizer
max_seq_length = 600                         # maximum length of tokens (for padding later)
embedding_size = 256                         # embedding size (tweak to improve accuracy)
output_length = len(df['category'].unique()) # number of class to be classified

# create our tokenizer
tokenizer = tf.keras.preprocessing.text.tokenizer(num_words=max_num_words, lower=true)
# fit our tokenizer with words/tokens
tokenizer.fit_on_texts(df['content_parsed'].values)
# get our token vocabulary
word_index = tokenizer.word_index
print('found {} unique tokens'.format(len(word_index)))

# parse our text into sequence of numbers using our tokenizer
x = tokenizer.texts_to_sequences(df['content_parsed'].values)
# pad the sequence up to the max_seq_length
x = tf.keras.preprocessing.sequence.pad_sequences(x, maxlen=max_seq_length)
print('shape of feature tensor: {}'.format(x.shape))

# convert our labels into dummy variable (more info on the link provided above)
y = pd.get_dummies(df['category']).values
print('shape of label tensor: {}'.format(y.shape))

# split our features and labels into test and train dataset
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

# creating our model
model = tf.keras.sequential()
model.add(tf.keras.layers.embedding(max_num_words, embedding_size, input_length=max_seq_length))
model.add(tf.keras.layers.spatialdropout1d(0.2))
# the number 64 could be changed based on your model performance
model.add(tf.keras.layers.lstm(64, dropout=0.2, recurrent_dropout=0.2))
# our output layer with length similar to the output_length
model.add(tf.keras.layers.dense(output_length, activation='softmax'))
# compile our model with ""categorical_crossentropy"" loss function
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# model variables
epochs = 100                          # number of cycle to run (the early stopping may stop the training process accordingly)
batch_size = 64                       # batch size (tweaking this may improve model performance a bit)
checkpoint_path = 'model_checkpoints' # checkpoint path of our model

# use gpu if available
with tf.device('/gpu:0'):
  # fit/train our model
  history = model.fit(
    x_train, y_train,
    epochs=epochs,
    batch_size=batch_size,
    validation_split=0.1,
    callbacks=[
      tf.keras.callbacks.earlystopping(monitor='val_loss', min_delta=0.0001),
      tf.keras.callbacks.modelcheckpoint(
        checkpoint_path, 
        monitor='val_acc', 
        save_best_only=true, 
        save_weights_only=false
      )
    ],
    verbose=1
  )

now, my model accuracies perform well and are increasing each epoch but since the validation accuracies (val_acc around 76~77 percent) are not performing well, i may need to tweak the model/layers a bit.
the output snapshot is provided below",https://stackoverflow.com/questions/58908050,python,18-11-2019 04:39,131.0,1.0,1.0,True,03-05-2022 17:16,18-11-2019 08:35
71437696,valueerror: classification metrics can&#39;t handle a mix of multilabel-indicator and multiclass targets,"i am trying to predict a model using independent variable (arabic sentence) and dependent variables (multiclass but using one hot encoding technique. i used tokenizer technique for train and test set
the model:
model = sequential()

model.add(embedding(num_words,32,input_length=max_length))
model.add(lstm(64,dropout=0.1))
model.add(dense(4,activation='sigmoid'))

model.compile(loss='binary_crossentropy',optimizer=optimizer, metrics=['accuracy'])

# some code here

model.fit(train_padded,y_train,epochs=1, validation_data=(test_padded,y_test))

the problem is when i use score = f1_score(y_test, ynew, average='weighted') as evaluation. it shows the following error:
valueerror: classification metrics can't handle a mix of multilabel-indicator and multiclass targets

ynew and y_test values are the following:
ynew= array([2, 1, 3, ..., 3, 0, 1]`, dtype=int64)

y_test = array([[0, 0, 1, 0],
       [0, 1, 0, 0],
       [0, 0, 0, 1],
       ...,
       [0, 0, 0, 1],
       [1, 0, 0, 0],
       [0, 1, 0, 0]], dtype=uint8)","['python', 'keras', 'nlp', 'evaluation']",71443865,"both arguments of f1_score() must be in the same format: either one-hot encoding or label encoding. you cannot pass two differently encoded arguments. use one of the following options.
option 1: you could convert ynew to one-hot encoding.
# one-hot encode ynew, before calculating f1_score
ynew = keras.utils.to_categorical(ynew)
f1_score(y_test, ynew, average='weighted')

option 2: you could convert y_new to one-hot encoding using labelbinarizer.
from sklearn.preprocessing import labelbinarizer
# one-hot encode ynew, before calculating f1_score
ynew = labelbinarizer().fit_transform(ynew)
f1_score(y_test, ynew, average='weighted')

option 3: you could convert y_test from one-hot encoding to label encoding.
import numpy as np
# label encode y_test, before calculating f1_score
y_test = np.argmax(y_test, axis=1)
f1_score(y_test, ynew, average='weighted')",https://stackoverflow.com/questions/71437696,python,11-03-2022 11:17,6069.0,1.0,1.0,True,12-03-2022 14:23,12-03-2022 14:23
75811594,openai chat completions api: can i fine-tune a gpt-3.5 model?,"i have fine-tuned an openai language model (curie) and was able to access the model via the openai.completion.create method, but i could not access the fine-tuned model via openai.chatcompletion.create.
by researching a bit, i have found out that the problem is not in the fine-tuning but in the fact that the original curie model is not accessible via openai.chatcompletion.create.
by looping over these models:
models = ['gpt-3.5-turbo', 'davinci', 'curie', 'babbage', 'ada']

i found out that only gpt-3.5-turbo model is accessible via openai.chatcompletion.create and it is not accessible via openai.completion.create. in contrast, the remaining four models are accessible via openai.completion.create but are not accessible via openai.chatcompletion.create.
so, my first question is: can someone confirm my finding? is what i found out written somewhere in the openai documentation?
my second question is: is it possible to fine-tune a model that supports chat?
for example, on the official page, i see that:

fine-tuning is currently only available for the following base models:
davinci, curie, babbage, and ada.

so, did i get it right that we can only fine-tune models that do not support chat?","['openai-api', 'chatgpt-api']",75811803,"question 1:

i found out that only gpt-3.5-turbo model is accessible via
openai.chatcompletion.create and it is not accessible via
openai.completion.create. in contrast, the remaining four models are
accessible via openai.completion.create but are not accessible via
openai.chatcompletion.create.
so, my first question if someone can confirm my finding?

answer 1:
yes, correct. the reason why this is the case is that the gpt-3.5.-turbo model is a gpt-3.5 model. all the other models you mentioned (i.e., davinci, curie, babbage, and ada) are gpt-3 models.
gpt-3.5 models use a different api endpoint than gpt-3 models. this is not explicitly written in the documentation, but it's very clear if you read the whole documentation.

question 2:

my second question is if it is possible to fine-tune a model that
supports chat / dialogue?

answer 2:
no, it's not possible. you want to fine-tune a gpt-3.5 model, which is not possible as of march 2023. also, it doesn't seem this will change in the near future, if ever. why?
i strongly recommend you to read the official openai article on how chatgpt's behavior is shaped to understand why you can't fine-tune a gpt-3.5 model. i want to emphasize that the article doesn't discuss specifically the fine-tuning of a gpt-3.5 model, or better yet, its inability to do so, but rather chatgpt's behavior. it's important to emphasize that chatgpt is not the same as the gpt-3.5 model, but chatgpt uses chat models, which gpt-3.5 belongs to, along with gpt-4 models.
as stated in the article:

unlike ordinary software, our models are massive neural networks.
their behaviors are learned from a broad range of data, not programmed
explicitly. /.../ an initial ï¿½ï¿½ï¿½pre-trainingï¿½ï¿½ï¿½ phase comes first, in
which the model learns to predict the next word in a sentence,
informed by its exposure to lots of internet text (and to a vast array
of perspectives). this is followed by a second phase in which we
ï¿½ï¿½ï¿½fine-tuneï¿½ï¿½ï¿½ our models to narrow down system behavior.
first, s by having them predict what comes next in
a big dataset that contains parts of the internet. they might learn to
complete the sentence ï¿½ï¿½ï¿½instead of turning left, she turned ___.ï¿½ï¿½ï¿½ by
learning from billions of sentences, our models learn grammar, many
facts about the world, and some reasoning abilities. they also learn
some of the biases present in those billions of sentences.
then, we ï¿½ï¿½ï¿½fine-tuneï¿½ï¿½ï¿½ these models on a more narrow dataset that we
carefully generate with human reviewers who follow guidelines that we
provide them. /.../ then, while they are in use, the models generalize
from this reviewer feedback in order to respond to a wide array of
specific inputs provided by a given user.

visual "" rel=""nofollow noreferrer"">source):

as you can see, chat models (i.e., gpt-3.5 and gpt-4 models) are already ""fine-tuned"" by the openai. this is the reason why you can only fine-tune base models: davinci, curie, babbage, and ada. these are the original models that do not have any instruction following training.",https://stackoverflow.com/questions/75811594,openai-api,22-03-2023 11:20,2212.0,-4.0,1.0,True,12-06-2024 17:11,12-06-2024 17:11
66608073,how to setup tf 2.4 training data with generator or other means,"i have a model setup with one input and two outputs. i am trying to use any of

tf.data.dataset.from_generator
fit with regular python
generator
tf.data.tfrecorddataset

so far all my attempts have run into errors, which i can only assume is based on the shape/types involved in my output from the generators i've tried setting up. what format should the output of such a generator be?
i am also super open to suggestions for doing this differently
you can download my whole notebook here if you'd like to look through
the input
the input to the model is of shape
(none,)

and is of type
tf.string

i am able to get model output with
model(tf.constant(['hello tensorflow!']))

the outputs
there are two output heads for the model, the first is of shape
(none, 128, 5)

the second is of shape
(none, 128, 3)

they both are of type
tf.float32

the loss for my model is sparse categorical crossentropy. (i want a softmax across 5 or 3 classes depending on the head, for each of the 128 outputs, with the none being there for the batch size). i believed for this the proper output format would be a tuple of batch_size instances of the following format
(input_string, (output_for_head1, output_for_head2))

where input_string is a string, output_for_head1 and output_for_head2 are both numpy arrays of shape (128) and type int.
some random things i've tried for fitting on generator directly
yield single item rather than whole batch (using batch size 10 for all testing)
gets index out of bounds error- pretty sure this needs to be batched
yield whole batch
get error
    data is expected to be in format `x`, `(x,)`, `(x, y)`, or `(x, y, sample_weight)`, found: ((<tf.tensor: shape=(), dtype=string, numpy=b'ya yeet'>, (<tf.tensor: shape=(128,), dtype=int64, numpy=... ( a very long set of (128,) tensors which is too large to post here)


     [[{{node pyfunc}}]]
     [[iteratorgetnext]] [op:__inference_train_function_95064]

function call stack:
train_function

ï¿½ï¿½ï¿","['python', 'tensorflow', 'nlp', 'tensorflow2.x']",66658607,"i figured out the solution to this using generators. i was able to first create a generator yielding numpy arrays that the model could be trained on directly, and then create a tf.data dataset from a slightly modified version of that generator.
the solution was to output just 3 numpy arrays per batch like
input_arr, (output_arr1, output_arr2) the shape of each array was expanded to have the batch size on the left, rather than having a tuple of length batch_size.
the final generators looked like this
def text_data_generator(dataset_path, batch_size, input_text_col='text', output_classes_col='labels', classes=classes, continuity_classes=continuity_classes, pad_length=128, sep=' '):
    while true:
        for chunk in pd.read_csv(dataset_path, chunksize=batch_size):
            #todo : should probably shuffle the dataset somehow
            texts = chunk['text'].values
            c_classes = np.stack(chunk['classes'].apply(lambda x : pad([classes.index(item) for item in x.split(sep)])).values)
            c_continuity = np.stack(chunk['continuity'].apply(lambda x : pad([continuity_classes.index(item) for item in x.split(sep)])).values)
            texts = np.array(texts)
            c_classes = np.array(c_classes)
            c_continuity = np.array(c_continuity)
            yield texts, (c_classes, c_continuity)

and
def tf_text_data_generator(dataset_path, batch_size, input_text_col='text', output_classes_col='labels', classes=classes, continuity_classes=continuity_classes, pad_length=128, sep=' '):
    for chunk in pd.read_csv(dataset_path, chunksize=batch_size):
        texts = chunk['text'].values
        c_classes = np.stack(chunk['classes'].apply(lambda x : pad([classes.index(item) for item in x.split(sep)])).values)
        c_continuity = np.stack(chunk['continuity'].apply(lambda x : pad([continuity_classes.index(item) for item in x.split(sep)])).values)
        texts = np.array(texts)
        c_classes = np.array(c_classes)
        c_continuity = np.array(c_continuity)
        yield texts, (c_classes, c_continuity)

the model could be trained directly on an instance of text_data_generator. to train on the other generator i created a tf.data.dataset by
def wrapped_gen():
    return tf_text_data_generator(""test.csv"", 10)
dataset = tf.data.dataset.from_generator(wrapped_gen, (tf.string, (tf.int64, tf.int64)))

which then can be passed directly to model.train just as the instantiated generator could be.",https://stackoverflow.com/questions/66608073,python,12-03-2021 22:32,280.0,1.0,1.0,True,16-03-2021 15:44,13-03-2021 08:42
61279917,multi-classification of text using nlp python - recall is relatively very less for 2 categories out of total categories,"i am having almost balanced dataset of 9 unique categories, each having almost 2200 rows with difference of +/-100 rows. to create model , i have used below mentioned urls approach but in each case my model accuracy is coming around 58% and precision/recall is also around 54%. can you please let me know what wrong am i doing?



my dataset is having only 2 columns , 1 as feature and other as label.
from pandas import excelfile

df = pd.read_excel('prediction.xlsx', 
                   sheet_name='sheet1')
df.head()
bad_symbols_re = re.compile('[^0-9a-z #+_]')
stopwords = set(stopwords.words('english'))
import sys
!{sys.executable} -m pip install lxml

def clean_text(text):
    """"""
        text: a string

        return: modified initial string
    """"""
    text = beautifulsoup(text, ""html.parser"").text # html decoding
    text = text.lower() # lowercase text
    text = replace_by_space_re.sub(' ', text) # replace replace_by_space_re symbols by space in text
    text = bad_symbols_re.sub('', text) # delete symbols which are in bad_symbols_re from text
    text = ' '.join(word for word in text.split() if word not in stopwords) # delete stopwors from text
    return text

df['notes_issuedesc'] = df['notes_issuedesc'].apply(clean_text)
print_plot(10)
df['notes_issuedesc'].apply(lambda x: len(x.split(' '))).sum()
x = df.notes_issuedesc
y = df.final
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state = 42)
%%time
from sklearn.naive_bayes import multinomialnb
from sklearn.pipeline import pipeline
from sklearn.feature_extraction.text import tfidftransformer

nb = pipeline([('vect', countvectorizer()),
               ('tfidf', tfidftransformer()),
               ('clf', multinomialnb()),
              ])
nb.fit(x_train, y_train)

from sklearn.metrics import classification_report
y_pred = nb.predict(x_test)

print('accuracy %s' % accuracy_score(y_pred, y_test))
print(classification_report(y_test, y_pred,target_names=my_tags))","['python', 'machine-learning', 'nlp']",78742758,"i was able to get my code to work by first correcting my data.
the issue was that there was lot of missing data, so i used mean values to fill these missing values. i also utilized a scatter chart to identify outlier data and then removed those as well.
i have performed few data wrangling operations and it generated with higher accuracy.",https://stackoverflow.com/questions/61279917,python,17-04-2020 20:12,51.0,-1.0,1.0,True,13-07-2024 03:56,22-04-2020 23:28
69343395,hugging face h5 load model error : no model found in config file,"i'm trying to load model from hugging face and i downloaded h5 model from here: 
from flask import flask, jsonify, request  # import objects from the flask model
from keras.models import load_model
from transformers import autotokenizer, automodelforsequenceclassification,textclassificationpipeline

model = load_model('./tf_model.h5') # trying to load model here

and the error shows up:
file ""c:\d\learning\flask\flask-pp-rest\main.py"", line 11, in <module>
    model = load_model('./tf_model.h5') file ""c:\users\ndrez\appdata\local\programs\python\python39\lib\site-packages\keras\saving\save.py"",
line 200, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, file
""c:\users\ndrez\appdata\local\programs\python\python39\lib\site-packages\keras\saving\hdf5_format.py"",
line 176, in load_model_from_hdf5
    raise valueerror('no model found in config file.') valueerror: **no model found in config file.**

does anyone know how to solve this? if you know please help me out. i will monitor this question and try to implement your solution's answer.","['python', 'tensorflow', 'keras', 'huggingface-transformers']",69352027,"you can load the tensorflow version of distilbert-base-uncased-finetuned-sst-2-english with the tfautomodelforsequenceclassification class:
from transformers import autotokenizer, tfautomodelforsequenceclassification
tokenizer = autotokenizer.from_pretrained(""distilbert-base-uncased-finetuned-sst-2-english"")
model = tfautomodelforsequenceclassification.from_pretrained(""distilbert-base-uncased-finetuned-sst-2-english"")",https://stackoverflow.com/questions/69343395,python,27-09-2021 08:25,2577.0,1.0,2.0,True,27-09-2021 19:04,27-09-2021 12:46
71963038,word2vec + lstm good training and validation but poor on test,"currently i'am training my word2vec + lstm for twitter sentiment analysis. i use the pre-trained googlenewsvectornegative300 word embedding. the reason i used the pre-trained googlenewsvectornegative300 because the performance much worse when i trained my own word2vec using own dataset. the problem is why my training process had validation acc and loss stuck at 0.88 and 0.34 respectively. then, my confussion matrix also seems wrong. here several processes that i have done before fitting the model
text pre processing:

lower casing
remove hashtag, mentions, urls, numbers, change words to numbers, non-ascii characters, retweets ""rt""
expand contractions
replace negations with antonyms
remove puncutations
remove stopwords
lemmatization

i split my dataset into 90:10 for train:test as follows:
def split_data(x, y):
    x_train, x_test, y_train, y_test = train_test_split(x, 
                                                        y,
                                                        train_size=0.9, 
                                                        test_size=0.1, 
                                                        stratify=y,
                                                        random_state=0)
    return x_train, x_test, y_train, y_test

the split data resulting in training has 2060 samples with 708 positive sentiment class, 837 negative sentiment class, and 515 sentiment neutral class
then, i implemented the text augmentation that is eda (easy data augmentation) on all the training data as follows:
class textaugmentation:
    def __init__(self):
        self.augmenter = eda()

    def replace_synonym(self, text):
        augmented_text_portion = int(len(text)*0.1) 
        synonym_replaced = self.augmenter.synonym_replacement(text, n=augmented_text_portion)
        return synonym_replaced

    def random_insert(self, text):
        augmented_text_portion = int(len(text)*0.1) 
        random_inserted = self.augmenter.random_insertion(text, n=augmented_text_portion)
        return random_inserted

    def random_swap(self, text):
        augmented_text_portion = int(len(text)*0.1)
        random_swaped = self.augmenter.random_swap(text, n=augmented_text_portion)
        return random_swaped

    def random_delete(self, text):
        random_deleted = self.augmenter.random_deletion(text, p=0.5)
        return random_deleted

text_augmentation = textaugmentation()

the data augmentation resulting in training has 10300 samples with 3540 positive sentiment class, 4185 negative sentiment class, and 2575 sentiment neutral class
then, i tokenized the sequence as follows:
# tokenize the sequence
pfizer_tokenizer = tokenizer(oov_token='oov')
pfizer_tokenizer.fit_on_texts(df_pfizer_train['text'].values)

x_pfizer_train_tokenized = pfizer_tokenizer.texts_to_sequences(df_pfizer_train['text'].values)
x_pfizer_test_tokenized = pfizer_tokenizer.texts_to_sequences(df_pfizer_test['text'].values)

# pad the sequence
x_pfizer_train_padded = pad_sequences(x_pfizer_train_tokenized, maxlen=100)
x_pfizer_test_padded = pad_sequences(x_pfizer_test_tokenized, maxlen=100)

pfizer_max_length = 100
pfizer_num_words = len(pfizer_tokenizer.word_index) + 1

# encode label
y_pfizer_train_encoded = df_pfizer_train['sentiment'].factorize()[0]
y_pfizer_test_encoded = df_pfizer_test['sentiment'].factorize()[0]

y_pfizer_train_category = to_categorical(y_pfizer_train_encoded)
y_pfizer_test_category = to_categorical(y_pfizer_test_encoded)

resulting in 8869 unique words and 100 maximum sequence length
finally, i fit the into my model using pre trained googlenewsvectornegative300 word embedding but only use the weight and lstm, and i split my training data again with 10% for validation as follows:
# build single lstm model
def build_lstm_model(embedding_matrix, max_sequence_length):
    # input layer
    input_layer = input(shape=(max_sequence_length,), dtype='int32')
    
    # word embedding layer
    embedding_layer = embedding(input_dim=embedding_matrix.shape[0],
                                output_dim=embedding_matrix.shape[1],
                                weights=[embedding_matrix],
                                input_length=max_sequence_length,
                                trainable=true)(input_layer)
    
    # lstm model layer
    lstm_layer = lstm(units=128,
                      dropout=0.5,
                      return_sequences=true)(embedding_layer)
    batch_normalization = batchnormalization()(lstm_layer)
    
    lstm_layer = lstm(units=128,
                      dropout=0.5,
                      return_sequences=false)(batch_normalization)
    batch_normalization = batchnormalization()(lstm_layer)

    # dense model layer
    dense_layer = dense(units=128, activation='relu')(batch_normalization)
    dropout_layer = dropout(rate=0.5)(dense_layer)
    batch_normalization = batchnormalization()(dropout_layer)
    
    output_layer = dense(units=3, activation='softmax')(batch_normalization)

    lstm_model = model(inputs=input_layer, outputs=output_layer)

    return lstm_model

# building single lstm model
sinovac_lstm_model = build_lstm_model(sinovac_embedding_matrix, sinovac_max_sequence)
sinovac_lstm_model.summary()
sinovac_lstm_model.compile(loss='categorical_crossentropy',
                               optimizer=adam(learning_rate=0.001),
                               metrics=['accuracy'])
sinovac_lstm_history = sinovac_lstm_model.fit(x=x_sinovac_train,
                                                  y=y_sinovac_train,
                                                  batch_size=64,
                                                  epochs=20,
                                                  validation_split=0.1,
                                                  verbose=1)


the training result:

the evaluation result:


i really need some suggestions or insights to have a good accuracy on my test","['python', 'keras', 'nlp', 'lstm', 'word2vec']",71981414,"without reviewing everything, a few high-order things that may be limiting your results:

the googlenews vectors were trained on media-outlet news stories from 2012 and earlier. tweets in 2020+ use a very different style of language. i wouldn't necessarily expect those pretrained vectors, from a different era & domain-of-writing, to be very good at modeling the words you'll need. a well-trained word2vec model (using plenty of modern tweet data, with good preprocessing/tokenization & parameterization choices) has a good chance of working better, so you may want to revisit that choice.

the googlenews training texts preprocessing, while as far as i can tell never fully-documented, did not appear to flatten all casing, nor remove stopwords, nor involve lemmatization. it didn't mutate obvious negations into antonyms, but it did perform a statistical combinations of some single-words into multigram tokens instead. so some of your steps are potentially causing your tokens to have less concordance with that set's vectors ï¿½ï¿½ï¿½ even throwing away info, like inflectional variations of words, that could be beneficially retained. be sure every step you're taking is worth the trouble ï¿½ï¿½ï¿½ and note that a suffiicient modern word2vec moel, on tweets, built using the same preprocessing for word2vec training then later steps, would match vocabularies perfectly.

both the word2vec model, and any deeper neural network, often need lots of data to train well, and avoid overfitting. even disregarding the 900 million parameters from googlenews, you're trying to train ~130k parameters ï¿½ï¿½ï¿½ at least 520kb of state ï¿½ï¿½ï¿½ from an initial set of merely 2060 tweet-sized texts (maybe 100kb of data). models that learn generalizable things tend to be compressions of the data, in some sense, and a model that's much larger than the training data brings risk of severe overfitting. (your mechanistic process for replacing wordbe really giving the model any info that the word-vector similarity between synonyms didn't already provide.) so: consider shrinking your model, and getting much more training data - potentially even from other domains than your main classification interest, as long as the use-of-language is similar.",https://stackoverflow.com/questions/71963038,python,22-04-2022 03:06,815.0,0.0,1.0,True,25-04-2022 05:07,25-04-2022 05:07
67644891,how do i create embeddings for every sentence in a list and not for the list as a whole?,"i need to generate embeddings for documents in lists, calculate the cosine similarity between every sentence of corpus 1 with every sentence of corpus2, rank them and give out the best fit:
embed = hub.load(""

embeddings1 = [""i'd like an apple juice"",
                                ""an apple a day keeps the doctor away"",
                                 ""eat apple every day"",
                                 ""we buy apples every week"",
                                 ""we use machine learning for text classification"",
                                 ""text classification is subfield of machine learning""]
embeddings1 = embed(embeddings1)

embeddings2 = [""i'd like an orange juice"",
                                ""an orange a day keeps the doctor away"",
                                 ""eat orange every day"",
                                 ""we buy orange every week"",
                                 ""we use machine learning for document classification"",
                                 ""text classification is some subfield of machine learning""]
embeddings2 = embed(embeddings2)

print(cosine_similarity(embeddings1, embeddings2))

the vectors seem to work fine (due to the shape of the array) and also the calculation of the cosine similarity.
my problem is that the universal sentence encoder does not give them out with the respective strings which is crucial. it always has to find the right fit and i must be able to order after the value of cosine similarity
array([[ 0.7882168 ,  0.3366559 ,  0.22973989,  0.15428472, -0.10180502,
                                                         -0.04344492],
       [ 0.256085  ,  0.7713026 ,  0.32120776,  0.17834462, -0.10769081,
                                                         -0.09398925],
       [ 0.23850328,  0.446203  ,  0.62606746,  0.25242645, -0.03946173,
                                                         -0.00908459],
       [ 0.24337521,  0.35571027,  0.32963073,  0.6373588 ,  0.08571904,
                                                         -0.01240187],
       [-0.07001016, -0.12002315, -0.02002328,  0.09045915,  0.9141338 ,
                                                          0.8373743 ],
       [-0.04525191, -0.09421931, -0.00631144, -0.00199519,  0.75919366,
                                                          0.9686416 ]]

the goal is that the code finds out itself that the highest cosine similarity of ""i'd like an apple juice"" in the second corpus is ""i'd like an orange juice"" and matches them.
i tried for loops, for instance:
for sentence in embeddings1:
    print(sentence, embed(sentence))

resulting in this error:
tensorflow.python.framework.errors_impl.invalidargumenterror:  input must be a vector, got shape: []
     [[{{node statefulpartitionedcall/statefulpartitionedcall/text_preprocessor/tokenize/stringsplit/stringsplit}}]] [op:__inference_restored_function_body_5285]

function call stack:
restored_function_body","['python', 'tensorflow', 'nlp', 'cosine-similarity', 'sentence-similarity']",67663618,"as i mentioned in the comment, you should write the for loop as follows:
for sentence in embeddings1:
    print(sentence, embed([sentence]))

the reason is simply that embed is expecting a list of strings as an input. no more detailed explanation than that.",https://stackoverflow.com/questions/67644891,python,21-05-2021 23:44,1228.0,0.0,1.0,True,26-05-2021 14:33,22-05-2021 00:04
77948682,how to stop at 512 tokens when sending text to pipeline? huggingface and transformers,"i want to test my model using pipeline by transformers. my model is a pretrained bert, which works great if the given text is < 512 tokens. however, when sending the a larger text to the pipeline, it breaks, because it's too long. i tried to search, but couldn't figure out how to solve this issue.
this is my code:
def get_predicted_folder(text, model):
    pipe = pipeline(""text-classification"", model=model)
    if text:
        predicted_folder = pipe(text)
        label = predicted_folder[0]['label']
        score = predicted_folder[0]['score']
        return label, score
    else:
        err = ""error: the provided text is empty.""
        return err, none

my_saved_model = ""model/danish_bert_model"" (it is saved locally)
label, score = get_predicted_folder(text, my_saved_model)

it gives me this error: runtimeerror: the size of tensor a (1593) must match the size of tensor b (512) at non-singleton dimension 1
i tried to give tokenizer=model to the pipeline, and have this tokenizer = autotokenizer.from_pretrained(model_ckpt) before calling the get_predicted_folder method, but it doesn't solve the issue.
the tokenizer inside the model is this:
def tokenize_dataset(tokenizer, examples):
    return tokenizer(examples['text'], truncation=true, max_length=512)
tokenizer = autotokenizer.from_pretrained(model_ckpt)
    dataset = dataset.map(lambda examples: tokenize_dataset(tokenizer, examples), batched=true)

can someone please help me?
thanks so much in advance!","['deep-learning', 'huggingface-transformers', 'huggingface', 'huggingface-tokenizers']",77949948,"only add the tokenizer, maximum length and truncation to the pipe as well and it will work well.
 pipe = pipeline(""text-classification"", model=model, tokenizer=model_path, max_length=512, truncation=true)",https://stackoverflow.com/questions/77948682,deep-learning,06-02-2024 14:55,585.0,2.0,1.0,True,06-02-2024 18:06,06-02-2024 15:02
47499675,find named entities from tokenized sentences in spacy v2.0,"i am trying to do:

tokenize sentences from text
compute named entities for each word present in sentence

this is what i have done so far:
nlp = spacy.load('en')
sentence = ""germany and u.s.a are popular countries. i am going to gym tonight""
sentence = nlp(sentence)
tokenized_sentences = []
for sent in sentence.sents:
        tokenized_sentences.append(sent)
for s in tokenized_sentences:
        labels = [ent.label_ for ent in s.ents]
        entities = [ent.text for ent in s.ents]

error: 
    labels = [ent.label_ for ent in s.ents]
    attributeerror: 'spacy.tokens.span.span' object has no attribute 'ents'

is there any alternative way to find named entities of tokenized sentence?
thanks in advance","['python', 'nlp', 'spacy']",47502965,"note that you only have two entities - usa and germany.
the simple version:
sentence = nlp(""germany and u.s.a are popular countries. i am going to gym tonight"")    
for ent in sentence.ents:
        print(ent.text, ent.label_)

what i think you are tying to do:
sentence = nlp(""germany and u.s.a are popular countries. i am going to gym tonight"")
for sent in sentence.sents:
    tmp = nlp(str(sent))
    for ent in tmp.ents:
        print(ent.text, ent.label_)",https://stackoverflow.com/questions/47499675,python,26-11-2017 18:41,2234.0,2.0,2.0,True,12-02-2022 09:38,26-11-2017 19:31
73826710,how to check if a column has a word based on words from another column with three different conditions in pandas?,"input:

import pandas as pd

df_input = pd.dataframe({'keyword': {0: 'apple banana, orange',
  1: 'apple orange ?banana ""',
  2: 'potato, piercing pot hole',
  3: 'armor hard known'},
 'returns': {0: 'fruit; banana vendor',
  1: 'blendor :kaka orange',
  2: 'piercing fruit banana takes a lot',
  3: 'bullet jacket gun'}})

df_input

for every word in keyword column,

if any of it appears in returns column, score = 1
if none of it appears in returns column, score = 0
if any of it appears in first half of the words in returns column, score_before = 1
if any of it appears in second half of the words in returns column, score_after = 1

output:

import pandas as pd

df_output = pd.dataframe({'keyword': {0: 'apple banana, orange',
  1: 'apple orange ?banana ""',
  2: 'potato, piercing pot hole',
  3: 'armor hard known'},
 'returns': {0: 'fruit; banana vendor',
  1: 'blendor :kaka orange',
  2: 'piercing fruit banana takes a lot',
  3: 'bullet jacket gun'},
 'score': {0: 1, 1: 1, 2: 1, 3: 0},
 'score_before': {0: 0, 1: 0, 2: 1, 3: 0},
 'score_after': {0: 0, 1: 1, 2: 0, 3: 0}})

df_output

original data frame has a million rows, how do i even tokenize the words efficiently? should i use string operations instead?
(i've used from nltk.tokenize import word_tokenize before, but how to apply it on the whole data frame if that's the way?)
edit: my custom function for tokenization:
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
import string

def tokenize(s):
  translator = str.maketrans('', '', string.punctuation)
  s = s.translate(translator)
  s = word_tokenize(s)
  return s

tokenize('finity: stocks%, direct$ mf, etf')","['python', 'pandas', 'string', 'dataframe', 'nlp']",73826930,"as you have as many conditions as you have rows, you need to loop here.
you can use a custom function:
import re
def get_scores(s1, s2):
    words1 = set(re.findall('\w+', s1.casefold()))
    words2 = re.findall('\w+', s2.casefold())
    n = len(words2)//2
    half1 = words2[:n]
    half2 = words2[n:]
    score_before = any(w in words1 for w in half1)
    score_after = any(w in words1 for w in half2)
    score = score_before or score_after
    return (score, score_before, score_after)

df2 = pd.dataframe([get_scores(s1, s2) for s1, s2 in
                    zip(df_input['keyword'], df_input['returns'])],
                    dtype=int, columns=['score', 'score_before', 'score_after'])

out = df_input.join(df2)

output:
                     keyword                            returns  score  score_before  score_after
0       apple banana, orange               fruit; banana vendor      1             0            1
1     apple orange ?banana ""               blendor :kaka orange      1             0            1
2  potato, piercing pot hole  piercing fruit banana takes a lot      1             1            0
3           armor hard known                  bullet jacket gun      0             0            0",https://stackoverflow.com/questions/73826710,python,23-09-2022 10:58,282.0,0.0,1.0,True,23-09-2022 11:23,23-09-2022 11:23
79257046,cannot install llama-index-embeddings-huggingface==0.1.3 because these package versions have conflicting dependencies,"i am unable to install the huggingfaceembedding \
getting the followng error:
error: cannot install llama-index-embeddings-huggingface==0.1.3, llama-index-embeddings-huggingface==0.1.4 and llama-index-embeddings-huggingface==0.1.5 because these package versions have conflicting dependencies.
error: resolutionimpossible: for help visit 

python version: 3.13","['python', 'huggingface-transformers', 'large-language-model', 'huggingface', 'llama']",79279977,"several things i had to do in order to make this work.

downgrade to python 3.9.  i went specifically to 3.9.13.  without that some of your imports in code will not work as well.
use a virtual environment.  i can't tell if you have done this.  it may not be necessary but it worked for me.
use uv to install packages.  it seems to sort out the dependencies a bit easier than straight pip.  uv can be installed via pip install uv.  then uv pip instal 

this got the llama_index_embedded_huggingface package to install.",https://stackoverflow.com/questions/79257046,python,06-12-2024 06:33,278.0,-1.0,1.0,True,14-12-2024 02:52,06-12-2024 06:42
67267220,read and write large text file python too slow,"this code goes over a large 5.1gb text file and checks if there are words that appear less than 100 times . then rewrites the 5.1gb into an output text file and replaces those words with unk. the main problem is that the creation of output.txt is taking a very long time.
i suspect that the method write_text() is causing an issue by the way its opening the dataset file and the output file.
the goal behind this script : i have a prebuilt vocab and i have a text. the text might have new words that are not in my vocab so i would like to add them to my vocab. but i only want to add new words that are relevant (appears more than 100 times). the new words that appear in the text less than 100 times are disposable and not important so i would like to change them to ""unk"".

from collections import counter

extra_words = []
new_words = []
add_words = []


def get_vocab():
    vocab = set()
    with open('vocab.txt', 'r', encoding='utf-8') as rd:
        lines = rd.readlines()

    for line in lines:
        tokens = line.split(' ')
        word = tokens[0]
        vocab.add(word)

    return vocab


def _count(text):

    vocab = get_vocab()

    with open(text, 'r', encoding='utf-8') as fd:

        for line in fd.readlines():

            for token in line.split():

                if token not in vocab:
                    extra_words.append(token)

    word_count = counter(extra_words)

    # add del word_count[punctuation] to remove it from list

    #del word_count['""']

    for word in word_count:

        if word_count[word] < 100:
            new_words.append(word)

        else:
            add_words.append(word)

    write_text()

    #return len(new_words), word_count.most_common()[0]


def write_text():

    with open('dataset', 'r', encoding='utf-8') as fd:

        f = fd.readlines()

    with open('output.txt', 'w', encoding='utf-8') as rd:
        new_text = []
        for line in f:
            new_line = []
            for token in line.split():

                

                if token in new_words:

                    new_line.append('<unk>')

                else:

                    new_line.append(token)

            new_text.append(' '.join(new_line))
        print('\n'.join(new_text), file=rd)
            #print(' '.join(new_line), file=rd)


def add_vocab():

    ln = len(get_vocab())

    with open('vocab.txt', 'w', encoding='utf-8') as fd:

        for idx, word in add_words:

            print(f'{word} {ln + idx + 1}\n', file=fd)

    pass


print(_count('dataset'))
add_vocab()","['python-3.x', 'text', 'nlp', 'corpus', 'data-preprocessing']",67270025,"i tested this with the complete works of shakespeare. you have a bunch of work still ahead of you related to case and punctuation. it does 100 copies of his works (500meg) in about 15 seconds for me. you might want to look at profiling your code if this takes more an an unacceptable time. note that i used a simplified version of your vocabulary file as i did not follow what you wanted to see in it. the version i used is just words line by line.
import collections

def get_vocabulary(path):
    with open(path, 'r', encoding='utf-8') as file_in:
        tokens = [line.strip(""\n"") for line in file_in]
    return set(tokens)

def get_interesting_word_counts(path, vocabulary):
    word_counts = collections.counter()
    with open(path, 'r', encoding='utf-8') as file_in:
        for line in file_in:
            word_counts.update([token for token in line.split() if token not in vocabulary])
    return word_counts

def get_cleaned_text(path, vocabulary, uncommon_words):
    with open(path, 'r', encoding='utf-8') as file_in:
        for line in file_in:
            #line_out = "" "".join([""<unk>"" if token in uncommon_words else token for token in line.strip(""\n"").split()])
            line_out = "" "".join([
                token if token in vocabulary or token not in uncommon_words else ""<unk>""
                for token in line.strip(""\n"").split()
            ])
            yield ""{}\n"".format(line_out)

vocabulary = get_vocabulary(""vocabulary.txt"")
word_counts = get_interesting_word_counts(""shakespeare.txt"", vocabulary)

## --------------------------------------
## add frequent but missing words to vocabulary
## --------------------------------------
common_words = set([item[0] for item in word_counts.items() if item[1] >= 100])
with open('vocabulary.txt', 'a', encoding='utf-8') as file_out:
    for word in common_words:
        file_out.write(""{}\n"".format(word))
## --------------------------------------

## --------------------------------------
## rewite the text censuring uncommon words
## --------------------------------------
uncommon_words = set([item[0] for item in word_counts.items() if item[1] < 100])
cleaned_text = get_cleaned_text(""shakespeare.txt"", vocabulary, uncommon_words)
with open('shakespeare_out.txt', 'w', encoding='utf-8') as file_out:
    file_out.writelines(cleaned_text)
## --------------------------------------

you can get the text i used here: 
the source begins:
the project gutenberg ebook of the complete works of william shakespeare, by william shakespeare

the resulting file begins:
<unk> <unk> <unk> <unk> of the <unk> <unk> of <unk> <unk> by <unk> <unk>

the updated vocabulary file begins:
as
run
heï¿½ï¿½ï¿½s
this.
thereï¿½ï¿½ï¿½s
like
you.e>",https://stackoverflow.com/questions/67267220,python-3.x,26-04-2021 13:06,1759.0,1.0,1.0,True,26-04-2021 21:03,26-04-2021 15:39
74847953,memoryerror when extracting articles into list using gensim wikicorpus,"i wish to build a corpus from a wikipedia dump (~19gb compressed .bz2 file). but, i encountered memoryerror when i try to run the code as shown. is there any solution that can solve this issue?
import warnings
warnings.filterwarnings(action='ignore', category=userwarning, module='gensim')
from gensim.corpora import wikicorpus
import sys


def make_corpus(in_f, out_f):
    output = open(out_f, 'w')
    print(""file created!"")
    wiki = wikicorpus(in_f)
    print(""wiki opened!"")
    i = 0

    for text in wiki.get_texts():
        output.write(bytes(' '.join(text).encode('utf-8')).decode('utf-8') + '\n')
        i = i + 1
        if (i % 10000 == 0):
            print('processed ' + str(i) + ' articles...')

    output.close()
    print('processing completed!')


if __name__ == '__main__':
    if len(sys.argv) !=3:
        sys.exit(1)

    in_f = sys.argv[1]
    out_f = sys.argv[2]
    make_corpus(in_f, out_f)


traceback (most recent call last):
file """", line 1, in 
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\multiprocessing\spawn.py"", line 105, in spawn_main
exitcode = _main(fd)
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\multiprocessing\spawn.py"", line 114, in _main
prepare(preparation_data)
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\multiprocessing\spawn.py"", line 225, in prepare
fixup_main_from_path(data['init_main_from_path'])
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\multiprocessing\spawn.py"", line 277, in fixup_main_from_path
run_name=""mp_main"")
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\runpy.py"", line 263, in run_path
pkg_name=pkg_name, script_name=fname)
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\runpy.py"", line 96, in run_module_code
mod_name, mod_spec, pkg_name, script_name)
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\runpy.py"", line 85, in run_code
exec(code, run_globals)
file ""d:\leongjc\fyp_code\code\wikipedia_transformation.py"", line 3, in 
import gensim
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim_init.py"", line 11, in 
from gensim import parsing, corpora, matutils, interfaces, models, similarities, utils  # noqa:f401
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora_init.py"", line 6, in 
from .indexedcorpus import indexedcorpus  # noqa:f401 must appear before the other classes
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora\indexedcorpus.py"", line 14, in 
from gensim import interfaces, utils
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\interfaces.py"", line 19, in 
from gensim import utils, matutils
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\matutils.py"", line 19, in 
from scipy.stats import entropy
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\scipy\stats_init.py"", line 388, in 
from .stats import *
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\scipy\stats\stats.py"", line 174, in 
from scipy.spatial.distance import cdist
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\scipy\spatial_init.py"", line 101, in 
from .procrustes import procrustes
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\scipy\spatial_procrustes.py"", line 9, in 
from scipy.linalg import orthogonal_procrustes
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\scipy\linalg_init.py"", line 194, in 
from .misc import *
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\scipy\linalg\misc.py"", line 4, in 
from .lapack import get_lapack_funcs
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\scipy\linalg\lapack.py"", line 783, in 
from scipy.linalg import _flapack
importerror: dll load failed: the paging file is too small for this operation to complete.
multiprocessing.pool.remotetraceback:
""""""
traceback (most recent call last):
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\multiprocessing\pool.py"", line 119, in worker
result = (true, func(*args, **kwds))
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora\wikicorpus.py"", line 530, in _process_article
token_max_len=token_max_len, lower=lower,
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora\wikicorpus.py"", line 490, in process_article
result = tokenizer_func(text, token_min_len, token_max_len, lower)
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora\wikicorpus.py"", line 361, in tokenize
utils.to_unicode(token) for token in utils.tokenize(content, lower=lower, errors='ignore')
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\utils.py"", line 264, in tokenize
text = text.lower()
memoryerror
""""""


the above exception was the direct cause of the following exception:
traceback (most recent call last):
file ""d:/leongjc/fyp_code/code/wikipedia_transformation.py"", line 31, in 
make_corpus(in_f, out_f)
file ""d:/leongjc/fyp_code/code/wikipedia_transformation.py"", line 11, in make_corpus
wiki = wikicorpus(in_f)
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora\wikicorpus.py"", line 639, in init
self.dictionary = dictionary(self.get_texts())
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora\dictionary.py"", line 78, in init
self.add_documents(documents, prune_at=prune_at)
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora\dictionary.py"", line 196, in add_documents
for docno, document in enumerate(documents):
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora\wikicorpus.py"", line 693, in get_texts
for tokens, title, pageid in pool.imap(_process_article, group):
file ""c:\users\asus\anaconda3\envs\tensorflow\lib\multiprocessing\pool.py"", line 735, in next
raise value
memoryerror

cmd error message1
cmd error message2","['nlp', 'gensim']",74859675,"by default, the wikicorpus class surveys the entire dump file's vocabulary upon creation, even though most users don't need that. and, it's during that step you're hitting this memoryerror.
however, if you supply an empty python dict at wikicorpus creation, it'll skip this time-consuming & memory-consuming step. a
specifically, change your line...
    wiki = wikicorpus(in_f)

...to...
    wiki = wikicorpus(in_f, dictionary={})

after this change, you may not have any further problems, as it looks like your code is otherwise doing things in an incremental fashion that shouldn't use much memory even on a small-memory machine.",https://stackoverflow.com/questions/74847953,nlp,19-12-2022 08:37,140.0,2.0,1.0,True,20-12-2022 07:21,19-12-2022 15:30
62317723,tokens to words mapping in the tokenizer decode step huggingface?,"is there a way to know the mapping from the tokens back to the original words in the tokenizer.decode() function?
for example:
from transformers.tokenization_roberta import robertatokenizer

tokenizer = robertatokenizer.from_pretrained('roberta-large', do_lower_case=true)

str = ""this is a tokenization example""
tokenized = tokenizer.tokenize(str) 
## ['this', 'ï¿½ï¿½is', 'ï¿½ï¿½a', 'ï¿½ï¿½token', 'ization', 'ï¿½ï¿½example']

encoded = tokenizer.encode_plus(str) 
## encoded['input_ids']=[0, 42, 16, 10, 19233, 1938, 1246, 2]

decoded = tokenizer.decode(encoded['input_ids']) 
## '<s> this is a tokenization example</s>'

and the objective is to have a function that maps each token in the decode process to the correct input word, for here it will be:
desired_output = [[1],[2],[3],[4,5],[6]] as this corresponds to id 42, while token and ization corresponds to ids [19244,1938] which are at indexes 4,5 of the input_ids</c","['pytorch', 'tokenize', 'huggingface-transformers']",70386864,"if you use the fast tokenizers, i.e. the rust backed versions from the tokenizers library the encoding contains a word_ids method that can be used to map sub-words back to their original word. what constitutes a word vs a subword depends on the tokenizer, a word is something generated by the pre-tokenization stage, i.e. split by whitespace, a subword is generated by the actual model (bpe or unigram for example).
the code below should work in general, even if the pre-tokenization performs additional splitting. for example i created my own custom step that splits based on pascalcase - the words here are pascal and case, the accepted answer wont work in this case since it assumes words are whitespace delimited.
from transformers import autotokenizer

tokenizer = autotokenizer.from_pretrained('roberta-large', do_lower_case=true)

example = ""this is a tokenization example""

encoded = tokenizer(example)

desired_output = []
for word_id in encoded.word_ids():
    if word_id is not none:
        start, end = encoded.word_to_tokens(word_id)
        if start == end - 1:
            tokens = [start]
        else:
            tokens = [start, end-1]
        if len(desired_output) == 0 or desired_output[-1] != tokens:
            desired_output.append(tokens)
desired_output",https://stackoverflow.com/questions/62317723,pytorch,11-06-2020 05:33,41212.0,26.0,2.0,True,02-11-2023 22:07,12-06-2020 11:06
71289683,can i use a different corpus for fasttext build_vocab than train in gensim fasttext?,"i am curious to know if there are any implications of using a different source while calling the build_vocab and train of gensim fasttext model. will this impact the contextual representation of the word embedding?
my intention for doing this is that there is a specific set of words i am interested to get the vector representation for and when calling model.wv.most_similar. i only want words defined in this vocab list to get returned rather than all possible words in the training corpus. i would use the result of this to decide if i want to group those words to be relevant to each other based on similarity threshold.
following is the code snippet that i am using, appreciate your thoughts if there are any concerns or implication with this approach.

vocab.txt contains a list of unique words of interest
corpus.txt contains full conversation text (i.e. chat messages) where each line represents a paragraph/sentence per chat

a follow up question to this is what values should i set for total_examples & total_words during training in this case?
from gensim.models.fasttext import fasttext

model = fasttext(min_count=1, vector_size=300,)

corpus_path = f'data/{client}-corpus.txt'
vocab_path = f'data/{client}-vocab.txt'
# unsure if below counts should be based on the training corpus or vocab
corpus_count = get_lines_count(corpus_path)
total_words = get_words_count(corpus_path)

# build the vocabulary
model.build_vocab(corpus_file=vocab_path)

# train the model
model.train(corpus_file=corpus.corpus_path, epochs=100, 
    total_examples=corpus_count, total_words=total_words,
)

# save the model
model.save(f'models/gensim-fastext-model-{client}')","['python', 'nlp', 'gensim', 'word-embedding', 'fasttext']",71388143,"incase someone has similar question, i'll paste the reply i got when asking this question in the gensim disussion group for reference:

you can try it, but i wouldn't expect it to work well for most
purposes.
the build_vocab() call establishes the known vocabulary of the
model, & caches some stats about the corpus.
if you then supply another corpus ï¿½ï¿½ï¿½ & especially one with more words
ï¿½ï¿½ï¿½ then:

you'll want your train() parameters to reflect the actual size of your training corpus. you'll want to provide a true total_examples and total_words count that are accurate for the training-corpus.
every word in the training corpus that's not in the know vocabulary is ignored completely, as if it wasn't even there. so you might as
well filter your corpus down to just the words-of-interest first, then
use that same filtered corpus for both steps. will the example texts
still make sense? will that be enough data to train meaningful,
generalizable word-vectors for just the words, alongside
other words-of-interest, without the full texts? (you could look at
your pref-filtered corpus to get a sense of that.) i'm not sure - it
could depend on how severely trimming to just the words-of-interest
changed the corpus. in particular, to train high-dimensional dense
vectors ï¿½ï¿½ï¿½ as with vector_size=300 ï¿½ï¿½ï¿½ you need a lot of varied data.
such pre-trimming might thin the corpus so much as to make the
word-vectors for the words-of-interest far less useful.

you could certainly try it both ways ï¿½ï¿½ï¿½ pre-filtered to just your
words-of-interest, or with the full original corpus ï¿½ï¿½ï¿½ and see which
works better on downstream evaluations.
more generally, if the concern is training time with the full corpus,
there are likely other ways to get an adequate model in an acceptable
amount of time.
if using corpus_file mode, you can increase workers to equal the
local cpu core count for a nearly-linear speedup from number l corpus_iterable mode, max throughput is usually
somewhere in the 6-12 workers threads, as long as you ahve that many
cores.)
min_count=1 is usually a bad idea for these algorithms: they tend to
train faster, in less memory, leaving better vectors for the remaining
words when you discard the lowest-frequency words, as the default
min_count=5 does. (it's possible fasttext can eke a little bit of
benefit out of lower-frequency words via their contribution to
character-n-gram-training, but i'd only ever lower the default
min_count if i could confirm it was actually improving relevant
results.
if your corpus is so large that training time is a concern, often a
more-aggressive (smaller) sample parameter value not only speeds
training (by dropping many redundant high-frequency words), but ofthen
improves final word-vector quality for downstream purposes as well (by
letting the rarer words have relatively more influence on the model in
the absense of the downsampled words).
and again if the corpus is so large that training time is a concern,
than epochs=100 is likely overkill. i believe the googlenews
vectors were trained using only 3 passes ï¿½ï¿½ï¿½ over a gigantic corpus. a
sufficiently large & varied corpus, with plenty of examples of all
words all throughout, could potentially train in 1 pass ï¿½ï¿½ï¿½ because each
word-vector can then get more total training-updates than many epochs
with a small corpus. (in general larger epochs values are more often
used when the corpus is thin, to eke out something ï¿½ï¿½ï¿½ not on a corpus
so large you're considering non-standard shortcuts to speed the
steps.)
-- gordon<",https://stackoverflow.com/questions/71289683,python,28-02-2022 01:06,428.0,2.0,1.0,True,07-03-2022 22:50,28-02-2022 01:11
73232413,why was bert&#39;s default vocabulary size set to 30522?,"i have been trying to build a bert model for a specific domain. however, my model is trained on non-english text, so i'm worried that the default token size, 30522, won't fit my model.
does anyone know where the number 30522 came from?
i expect that researchers were fine-tuning their model by focusing on training time and vocabulary coverage, but a more clear explanation will be appreciated.","['tokenize', 'bert-language-model']",75130419,"the number of 30522 is not ""token size."" it's the size of wordpiece vocabulary bert was trained on. see this link for an explanation of wordpiece. the number 30522 likely means the base character set was 522 characters in size and the wordpiece algorithm was trained on 30,000 iterations.",https://stackoverflow.com/questions/73232413,tokenize,04-08-2022 08:06,2671.0,2.0,1.0,True,30-09-2024 14:18,04-08-2022 14:24
77823105,i am classifying each word in a sentence (named entity recognition) but i receive ...an unexpected keyword argument &#39;grouped_entities&#39;,"sentence = 'american airlines was the first airline to fly every a380 flight perfectly when president george bush was in office. the woodlands texas is a great place to be.'
ner = pipeline('text-classification', model='dbmdz/bert-large-cased-finetuned-conll03-english', grouped_entities=true)
ners = ner(sentence)
print('\nsentence:')
print(wrapper.fill(sentence))
print('\n')
for n in ners:
  print(f""{n['word']} -> {n['entity_group']}"")

i am inside google colab.
i tried
!pip install transformers --upgrade # the error is caused by a bug in the transformers library. the fix is to install the latest version of the library.
but i received the following:
/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py in _encode_plus(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
    574     ) -> batchencoding:
    575         batched_input = [(text, text_pair)] if text_pair else [text]
--> 576         batched_output = self._batch_encode_plus(
    577             batched_input,
    578             is_split_into_words=is_split_into_words,

typeerror: pretrainedtokenizerfast._batch_encode_plus() got an unexpected keyword argument 'grouped_entities'","['python', 'nlp', 'google-cloud-colab-enterprise']",77825273,"there may be a confusion , the named entity recognition task is a token-classification task, not a text-classification task. please update your code:
ner = pipeline(
    'token-classification',
    model='dbmdz/bert-large-cased-finetuned-conll03-english',
    grouped_entities=true
)  # alias ""ner"" available

that will raise a warning :
userwarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=""simple""` instead.

updated code with aggregation_strategy:
# updated code with 'aggregation_strategy'
ner = pipeline(
    'ner',
    model='dbmdz/bert-large-cased-finetuned-conll03-english',
    aggregation_strategy='simple'
)",https://stackoverflow.com/questions/77823105,python,16-01-2024 01:47,173.0,0.0,1.0,True,20-01-2024 23:46,20-01-2024 23:46
74666233,how to handle abbreviation when reading nltk corpus,"i am reading nltk corpus using
def read_corpus(package, category):
    """""" read files from corpus(package)'s category.
        params:
            package (nltk.corpus): corpus
            category (string): category name
        return:
            list of lists, with words from each of the processed files assigned with start and end tokens
    """"""
    files = package.fileids(category)
    return [[start_token] + [w.lower() for w in list(package.words(f))] + [end_token] for f in files]

but i find that it process 'u.s.' to ['u','.','s','.'] and 'i'm' to ['i', ""'"", 'm'].
how can i get an abbreviation as a whole or restore it?","['python', 'nltk']",74666256,"to treat abbreviations such as ""u.s."" and contractions such as ""i'm"" as a single token when processing text, you can use the treebankwordtokenizer from the nltk library. this tokenizer is designed to tokenize text in a way that is similar to how humans would naturally write and speak, so it will treat abbreviations and contractions as single tokens.",https://stackoverflow.com/questions/74666233,python,03-12-2022 11:20,271.0,0.0,1.0,True,03-12-2022 11:23,03-12-2022 11:21
74060732,create api with flask that receives 2 strings and returns the similarity between them with spacy,"i have this code that compute the similarity between 2 strings:
import spacy
from spacy.lang.pt.examples import sentences 

x =""some string 1""
y =""some string 2""

nlp = spacy.load('pt_core_news_sm')
x_nlp = nlp(x)
y_nlp = nlp(y)

token_x = [token.text for token in x_nlp]
token_y = [token.text for token in y_nlp]

print(""similarity:"", x_nlp.similarity(y_nlp))

now i want to transform this code in an api with flask, i tried to follow a tutorial:
from flask import flask,render_template,url_for,request
import re
import spacy
from spacy.lang.pt.examples import sentences 

nlp = spacy.load('pt_core_news_sm')

app = flask(__name__)

@app.route('/',methods=[""post""])
def process():
    x_nlp = nlp(input())
    y_nlp = nlp(input())
        
    print(""similarity:"", x_nlp.similarity(y_nlp))


if __name__ == '__main__':
    app.run(debug=true)

the code above returns: ""get /  405 -","['python', 'flask', 'spacy', 'similarity']",74061382,"you are trying to reach the url ""/"". however, within your code there is no route defined for this path. thus, the error 404 is returned.
you need a route that accepts both a get and a post request to both display a form and receive data from a submitted form sent via post.
otherwise, a 405 error is returned because the request method is not allowed.
@app.route('/', method=['get', 'post'])
def index():
    if request.method == 'post':
         # handle a post request and the data sent here.
    # ...

within flask it is not possible to request input with input().
as mentioned above, you need a form within an html page. within this form you can define input fields that must be provided with a name attribute in order to query them on the server side.
<form method=""post"">
    <input type=""text"" name=""x"" />
    <input type=""text"" name=""y"" />
    <input type=""submit"">
</form>

if the submit button is now pressed, the data of the form, as defined in the method attribute, is sent to the server via post and can be queried here. the input fields are queried using the name attribute.
finally, the endpoint must have a return value. in your case, this is the template that displays the page with the form and outputs the possible result.
@app.route('/', methods=['get', 'post'])
def index():
    if request.method == 'post':
        x = request.form.get('x', '')
        y = reutest.form.get('y', '')
        # ...
    return render_template('index.html')

so the entire code of your application should look something like this.
flask (app.py)
from flask import (
    flask,
    render_template,
    request
)
import spacy

nlp = spacy.load('pt_core_news_sm')

app = flask(__name__)

@app.route('/', methods=['get', 'post'])
def index():
    if request.method == 'post':
        # handle a post request and the data sent here.
        x_nlp = nlp(request.form.get('x', ''))
        y_nlp = nlp(request.form.get('y', ''))
        
        resultado = x_nlp.similarity(y_nlp)
        
    # return a rendered template and pass defined variables to the template.
    return render_template('index.html', **locals())

html (templates/index.html)
<!doctype html>
<html>
<head>
    <meta charset=""utf-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1"">
    <title>index</title>
</head>
<body>
    <form method=""post"">
        <input type=""text"" name=""x"" />
        <input type=""text"" name=""y"" />
        <input type=""submit"">
    </form>

    {% if resultado -%}
    <p>similaridade: {{ resultado }}</p>
    {% endif -%}
</body>
</html>",https://stackoverflow.com/questions/74060732,python,13-10-2022 19:15,107.0,-1.0,1.0,True,13-10-2022 20:22,13-10-2022 20:11
75790862,openai gpt-3 api: why do i get a response that makes no sense in relation to the question?,"when i ask a question in parameters of the request, the response has no sentence, and i get other questions in the response. i tried with every ""temperature"" and the response is never the same that i could get on chatgpt-3. i also tried with every models like davinci-codex, davinci, curie, babbage, etc. do you have any idea of why ?
here are the parameters  :
{
    ""prompt"": ""what's the capital of usa ?"",
    ""max_tokens"": 100,
    ""n"": 1,
    ""stop"": null,
    ""temperature"": 0
}

this is the api response :
{
    ""id"": ""cmpl-6wa6d1bcnyju7cbqljkrtoooi8ts2"",
    ""object"": ""text_completion"",
    ""created"": 1679319891,
    ""model"": ""davinci"",
    ""choices"": [
        {
            ""text"": ""\n\na: washington d.c.\n\nq: what's the capital of canada ?\n\na: ottawa\n\nq: what's the capital of australia ?\n\na: canberra\n\nq: what's the capital of england ?\n\na: london\n\nq: what's the capital of france ?\n\na: paris\n\nq: what's the capital of germany ?\n\na: berlin\n\nq: what's the capital of italy ?"",
            ""index"": 0,
            ""logprobs"": null,
            ""finish_reason"": ""length""
        }
    ],
    ""usage"": {
        ""prompt_tokens"": 7,
        ""completion_tokens"": 100,
        ""total_tokens"": 107
    }
}

with 0.5 temperature, the reponse is :
{
    ""id"": ""cmpl-6wa3zuuafgre8ox6dmy2m9tqgoxar"",
    ""object"": ""text_completion"",
    ""created"": 1679319701,
    ""model"": ""davinci"",
    ""choices"": [
        {
            ""text"": ""\n\na: washington d.c.\n\nq: what's the capital of france ?\n\na: paris.\n\nq: what's the capital of germany ?\n\na: berlin.\n\nq: what's the capital of china ?\n\na: beijing.\n\nq: what's the capital of japan ?\n\na: tokyo.\n\nq: what's the capital of russia ?\n\na: moscow.\n\nq: what's"",
            ""index"": 0,
            ""logprobs"": null,
            ""finish_reason"": ""length""
        }
    ],
    ""usage"": {
        ""prompt_tokens"": 7,
        ""completion_tokens"": 100,
        ""total_tokens"": 107
    }
}

and with a more difficult question this is what i get :
questions :
{
    ""prompt"": ""what job could i do if i like computers and video games?"",
    ""max_tokens"": 100,
    ""n"": 1,
    ""stop"": null,
    ""temperature"": 0
}

answer:
{
    ""id"": ""cmpl-6waacq91vboohawmbqqvjyoaznu6i"",
    ""object"": ""text_completion"",
    ""created"": 1679320112,
    ""model"": ""davinci"",
    ""choices"": [
        {
            ""text"": ""\n\nwhat job could i do if i like to work with my hands?\n\nwhat job could i do if i like to work with animals?\n\nwhat job could i do if i like to work with plants?\n\nwhat job could i do if i like to work with people?\n\nwhat job could i do if i like to work with numbers?\n\nwhat job could i do if i like to work with words?\n\nwhat job could i do if i"",
            ""index"": 0,
            ""logprobs"": null,
            ""finish_reason"": ""length""
        }
    ],
    ""usage"": {
        ""prompt_tokens"": 13,
        ""completion_tokens"": 100,
        ""total_tokens"": 113
    }
}","['openai-api', 'gpt-3']",75791101,"you're using an old gpt-3 model (i.e., davinci). use a newer gpt-3 model.
for example, use the model text-davinci-003 instead of davinci.
as stated in the official openai article:

how do davinci and text-davinci-003 differ?
while both davinci and text-davinci-003 are powerful models, they
differ in a few key ways.
text-davinci-003 is the newer and more capable model, designed
specifically for instruction-following tasks. this enables it to
respond concisely and more accurately - even in zero-shot scenarios,
i.e. without the need for any examples given in the prompt.
additionally, text-davinci-003 supports a longer context window (max
prompt+completion length) than davinci - 4097 tokens compared to
davinci's 2049.
finally, text-davinci-003 was trained on a more recent dataset,
containing data up to june 2021. these updates, along with its support
for inserting text, make text-davinci-003 a particularly versatile and
powerful model we recommend for most use-cases.",https://stackoverflow.com/questions/75790862,openai-api,20-03-2023 13:27,1619.0,-1.0,1.0,True,21-12-2024 20:33,21-12-2024 20:33
78485621,passing multiple arguments during langchain chain.invoke(),"i am experimenting with a langchain chain by passing multiple arguments.
here is a scenario:
template = """"""task: generate cypher statement to query a graph database.
instructions:
use only the provided relationship types and properties in the schema.
do not use any other relationship types or properties that are not provided.

you are also provided with contexts to generate cypher queries. these contexts are the node ids from the schema.
{schema}

some examples of contexts:
{context}

the question is:
{question}""""""

prompt = prompttemplate.from_template(template=template)
chain = (
    {
     ""schema"": schema,
     ""context"": vector_retriever_chain | extract_relevant_docs, 
     ""question"": runnablepassthrough()
    }
    | prompt
)
chain.invoke(""my question?"")

in this chain, i am getting some context from a vector retriever which i am passing to a function called extract_relevant_docs() that will parse the result and get format i want.
the tricky part here is the variable 'schema' which i also want to supply to design my prompt. how can i pass these variables during the chain.invoke().
thank you","['python', 'langchain', 'retrievalqa']",78567037,"you can pass multiple inputs in chain.invoke() by forming them as a dict, for example chain.invoke(input={""context"": context, ""schema"": schema})",https://stackoverflow.com/questions/78485621,python,15-05-2024 17:37,10460.0,0.0,1.0,True,02-06-2024 16:25,15-05-2024 17:49
70834489,choosing a good prompt for gpt-3,"i am trying to generate a quiz from a text that look like this:
text: ""mary has little lamb and john has a cow. the lamb is one month old. it eats grass and milk, which mary brings from the the farm.""

keywords: ""lamb"", ""cow"", ""one month old"", ""farm.""

1. what does mary have?
a. lamb
b. cow
c. dog
d. cat

a. lamb

2. what does john have?
a. cow
b. lamb
c. dog
d. cat

a. cow

3. how old is mary's lamb?
a. one month old
b. one year old
c. two months old
d. two years old

etc.

it works perfectly when i don't give keywords, with multiple prompts, with all these answer options, correct answer, everything. the problem is when i want to generate these questions such that the correct answer is the keyword. i tried all kinds of prompts, even giving examples like what i did above, but it doesn't work.","['nlp', 'openai-api', 'gpt-3']",70881747,"using gpt-j i tested the following:
input:
create a quiz about the following text according to the keywords: mary has little lamb and john has a cow. the lamb is one month old. it eats grass and milk, which mary brings from the the farm.

keywords: ""lamb"", ""cow"", ""one month old"", ""farm.""

question 1: what does mary have?
a. lamb
b. cow
c. dog
d. cat

answer: a. lamb

question 2:

output:
create a quiz about the following text according to the keywords: mary has little lamb and john has a cow. the lamb is one month old. it eats grass and milk, which mary brings from the the farm.

keywords: ""lamb"", ""cow"", ""one month old"", ""farm.""

question 1: what does mary have?
a. lamb
b. cow
c. dog
d. cat

answer: a. lamb

question 2: what does john have?
a. lamb
b. cow
c. dog
d. cat

answer: b. cow

question 3: how old is the lamb?
a. 1 month
b. 1 year
c. 2 months
d. 1 year and a half

answer: c. 2 months

giving an example of what you want in the input (1 question + answer) can help gpt to understand the structure of the desired output. explicitly explaining the gpt what the desired task is in the input (see line 1) can help it to understand the task in my experience.
it does not execute the task perfectly, but using gpt-3 might help and i hope this is a step in the right direction for you.",https://stackoverflow.com/questions/70834489,nlp,24-01-2022 13:16,617.0,0.0,1.0,True,13-12-2022 15:42,13-12-2022 15:42
77201376,problem with text find and replacement in python,"i have very specific function. i have 2 strings, one that is backup of input of the code, and second one, that is modified by steps like replacing spaces, extract of information etc (not important for this case).
i need to find a match in those strings, even when the first one is modified. after the match is found, i need to store the match from original string (without modification), and remove it from ""sub_str""/""modified_sub_str"".
def find_and_save(sub_str, main_str):
    # convert both strings to lowercase and remove spaces, commas, and hyphens for case-insensitive matching
    sub_str_mod = re.escape(sub_str.lower().replace("" "", """").replace("","", """").replace(""-"", """"))
    main_str_mod = main_str.lower().replace("" "", """").replace("","", """").replace(""-"", """")

    # use re.search() to find the substring in the modified main string
    match = re.search(sub_str_mod, main_str_mod)

    if match:
        start = match.start()
        end = match.end()

        count = 0
        original_start = 0
        original_end = 0

        for i, c in enumerate(main_str):
            if c not in [' ', ',', '-']:
                count += 1
            if count == start + 1:
                original_start = i
            if count == end:
                original_end = i + 1
                break

        original_sub_str = main_str[original_start:original_end]

        # if the whole sub_str is matching with some part of main_str, return an empty string as modified_sub_str
        if original_sub_str.lower().replace("" "", """").replace("","", """").replace(""-"", """") == sub_str_mod:
            modified_sub_str = """"
        else:
            # remove the matching part from sub_str in a case-insensitive manner
            modified_sub_str = re.sub(re.escape(original_sub_str), '', sub_str, flags=re.ignorecase)

        return modified_sub_str, original_sub_str  # returns the modified sub_str and the matched string in its original form
    else:
        return sub_str, none  # returns sub_str as it was and none if no match is found

but i have a specific problems with this code. for example if i have inputs like
sub_str = ""internationalworkshopongraphene/ceramiccomposites2016,wgcc2016""

and
main_str = ""roï¿½ï¿½. 37, ï¿½ï¿½. 12, international workshop on graphene/ceramic composites 2016, wgcc 2016 (2017), s. 3773-3780 [print, online]"" 

this code can find match, can return ""original_sub_str"", but cannot remove the match from ""modified_sub_str"".
the same problem for those inputs:
""sub_str"" - ""main_str""
""isnnm-2016,internationalsymposiumon""
""roï¿½ï¿½. 2017, ï¿½ï¿½. 65, isnnm-2016, international symposium on novel and nano materials (2017), s. 76-82 [print, online]""

""fractographyofadvancedceramics5ï¿½ï¿½ï¿½fractographyfrommacro-tonano-scaleï¿½ï¿½ï¿½"" 
""roï¿½ï¿½. 37, ï¿½ï¿½. 14, fractography of advanced ceramics 5 ï¿½ï¿½ï¿½fractography from macro- to nano-scaleï¿½ï¿½ï¿½ (2017), s. 4315-4322 [print, online]""

""73.zjazdchemikov,zbornï¿½ï¿½kabstraktov""
""roï¿½ï¿½. 17, ï¿½ï¿½. 1, 73. zjazd chemikov, zbornï¿½ï¿½k abstraktov (2021), s. 246-246 [print, online]"" 

i cant find a solution even with use of ai, but i know theres a problem with","['python', 'replace', 'extract', 'text-mining']",77201840,"your sub_str_mod was a regex escaped string. . is converted to \., now original_sub_str can not be found because original_sub_str has no backslash. (next time use a debugger)
removed re and do all with literal string find.
removed the else because the if test is always true
def clean_str(s) -> str:
    return s.lower().replace("" "", """").replace("","", """").replace(""-"", """")

def find_and_save(sub_str, main_str):
    # convert both strings to lowercase and remove spaces, commas, and hyphens for case-insensitive matching
    sub_str_mod = clean_str(sub_str)
    main_str_mod = clean_str(main_str)

    # find the substring in the modified main string
    start = main_str_mod.find(sub_str_mod)
    if start == -1:
        return sub_str, none  # returns sub_str as it was and none if no match is found

    end = start + len(sub_str_mod)

    count = 0
    original_start = 0
    original_end = 0

    for i, c in enumerate(main_str):
        if c not in [' ', ',', '-']:
            count += 1
        if count == start + 1:
            original_start = i
        if count == end:
            original_end = i + 1
            break

    original_sub_str = main_str[original_start:original_end]

    # if the whole sub_str is matching with some part of main_str, return an empty string as modified_sub_str
    modified_sub_str = """"
    if clean_str(original_sub_str) == sub_str_mod:  # always true
        modified_sub_str = """"
    return modified_sub_str, original_sub_str  # returns the modified sub_str and the matched string in its original form

output of the 4 cases:
('', 'international workshop on graphene/ceramic composites 2016, wgcc 2016')
('', 'isnnm-2016, international symposium on')
('', 'fractography of advanced ceramics 5 ï¿½ï¿½ï¿½fractography from macro- to nano-scaleï¿½ï¿½ï¿½')
('', '73. zjazd chemikov, zbornï¿½ï¿½k abstraktov'",https://stackoverflow.com/questions/77201376,python,29-09-2023 10:55,77.0,1.0,1.0,True,29-09-2023 12:13,29-09-2023 11:07
62175452,topic modeling on short texts python,i want to do topic modeling on short texts. i did some research on lda and found that it doesn't go well with short texts. what methods would be better and do they have python implementations?,"['python', 'python-3.x', 'nlp', 'lda', 'topic-modeling']",62179245,"you can try short text topic modelling (refer to this  (code available at  . it combine state-of-the-art algorithms and traditional topics modelling for long text which can conveniently be used for short text.
for more specialised libraries, try lda2vec-tf, which combines word vectors with lda topic vectors. it is branched from the original lda2vec and improved upon and gives better results than the original library.",https://stackoverflow.com/questions/62175452,python,03-06-2020 14:32,5620.0,5.0,4.0,True,09-09-2022 06:29,03-06-2020 14:53
74671279,train t5/bart to convert a string into multiple strings,"is it possible to train a seq2seq model like t5 or bart to convert a string into a list of strings? on my first attempt, the tokenizer complained that my 2d list of labels isnï¿½ï¿½ï¿½t the correct data type:
file ""/home/matt/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py"", line 429, in _batch_encode_plus
    encodings = self._tokenizer.encode_batch(
typeerror: textencodeinput must be union[textinputsequence, tuple[inputsequence, inputsequence]]

i suppose i could concatenate the multiple strings in each of my training examples, but then iï¿½ï¿½ï¿½d have to use a potentially error-prone splitter to split them up again. maybe using a special character as a delimiter is the answer here?
it's not super relevant, but here's how i'm invoking the tokenizer. also, i'm using a subclass of torch.utils.data.dataset:
tokenizer = autotokenizer.from_pretrained(args.model_name)
encodings = tokenizer(textn=true, padding=true, return_tensors='pt')
decodings = tokenizer(labels, truncation=true, padding=true, return_tensors='pt')
dataset_tokenized = dataset(encodings, decodings)

what is relevant is that my texts variable is a list of strings, and my labels variable is a 2d list of strings, which obviously isn't allowed.","['python', 'nlp', 'huggingface-transformers']",74694893,"using a special delimiter worked great! i chose the pipe character.
pairs = [(source, ' | '.join(target)) for source, target in pairs]",https://stackoverflow.com/questions/74671279,python,03-12-2022 22:26,299.0,0.0,1.0,True,10-12-2022 01:24,04-12-2022 04:38
69874436,pyinstaller problem making exe files that using transformers and pyqt5 library,"so i'm working on an ai project using huggingface library, and i need to convert it into an exe file. i'm using pyqt5 for the interface, and transformers and datasets library from huggingface. i tried using pyinstaller to convert it into an exe file, it does finish building the exe files of the project, but it gives me this error when i run the exe file:
traceback (most recent call last):
  file ""transformers\utils\versions.py"", line 105, in require_version
  file ""importlib\metadata.py"", line 530, in version
  file ""importlib\metadata.py"", line 503, in distribution
  file ""importlib\metadata.py"", line 177, in from_name
importlib.metadata.packagenotfounderror: tqdm

during handling of the above exception, another exception occurred:

traceback (most recent call last):
  file ""app.py"", line 5, in <module>
  file ""pyinstaller\loader\pyimod03_importers.py"", line 476, in exec_module
  file ""transformers\__init__.py"", line 43, in <module>
  file ""pyinstaller\loader\pyimod03_importers.py"", line 476, in exec_module
  file ""transformers\dependency_versions_check.py"", line 41, in <module>
  file ""transformers\utils\versions.py"", line 120, in require_version_core
  file ""transformers\utils\versions.py"", line 107, in require_version
importlib.metadata.packagenotfounderror: the 'tqdm>=4.27' distribution was not found and is required by this application.
try: pip install transformers -u or pip install -e '.[dev]' if you're working with git master
[736] failed to execute script 'app' due to unhandled exception!

[process exited with code 1]

line 5 on my code was a line of code for importing the transformers library.
...
4| from pyqt5.qtcore import qthread, qobject, pyqtsignal
5| from transformers import autotokenizer, automodelforquestionanswering, pipeline
...
...

and this is my .spec file:
# -*- mode: python ; coding: utf-8 -*-

block_cipher = none

a = analysis(['app.py'],
             pathex=[],
             binaries=[],
             datas=[
                ('./resources/images/logo.png', '.'), 
                ('./resources/model/config.json', '.'), 
                ('./resources/model/pytorch_model.bin', '.'), 
                ('./resources/model/special_tokens_map.json', '.'), 
                ('./resources/model/tokenizer.json', '.'), 
                ('./resources/model/tokenizer_config.json', '.'), 
                ('./resources/model/vocab.txt', '.')
            ],
             hiddenimports=[],
             hookspath=[],
             hooksconfig={},
             runtime_hooks=[],
             excludes=[],
             win_no_prefer_redirects=false,
             win_private_assemblies=false,
             cipher=block_cipher,
             noarchive=false)
pyz = pyz(a.pure, a.zipped_data,
             cipher=block_cipher)

exe = exe(pyz,
          a.scripts, 
          [],
          exclude_binaries=true,
          name='app',
          debug=false,
          bootloader_ignore_signals=false,
          strip=false,
          upx=true,
          console=true,
          disable_windowed_traceback=false,
          target_arch=none,
          codesign_identity=none,
          entitlements_file=none , icon='logo.ico')
coll = collect(exe,
               a.binaries,
               a.zipfiles,
               a.datas, 
               strip=false,
               upx=true,
               upx_exclude=[],
               name='app')

i would really appreciate any help given, thanks :d","['python', 'pyqt', 'pyside', 'huggingface-transformers']",70229204,"first, pip install tqdm if you haven't already. second, specify the path to your lib/site-packages. you can do this by either:

adding an argument to pathex in your .spec file
(.venv for a virtual environment at some folder .venv in your local directory, or the absolute path to your global python install lib/site-packages if you're not using a virtual environment):

pathex=['.venv/lib/site-packages']


specifying the path to lib/site-packages from the command-line:

pyinstaller --paths '.venv/lib/site-packages' my_program.py

from the pyinstaller docs

pathex: a list of paths to search for imports (like using pythonpath), including paths given by the --paths option.

some python scripts import modules in ways that pyinstaller cannot detect: for example, by using the __import__() function with variable data, using importlib.import_module(), or manipulating the sys.path value at run time.",https://stackoverflow.com/questions/69874436,python,07-11-2021 16:34,2677.0,0.0,1.0,True,04-12-2022 19:23,08-11-2021 02:10
43018030,replace apostrophe/short words in python,"i am using python to clean a given sentence. suppose that my sentence is:
what's the best way to ensure this?

i want to convert:
what's -> what is

similarly,
 must've -> must have

also, verbs to original form,
told -> tell

singular to plural, and so on.
i am currently exploring textblob. but not all of the above is possible using it.","['python', 'nlp', 'textblob']",43023503,"for the first question, there isn't a direct module that does that for you so you will have to build your own, first you will need a contraction dictionary like this one:
contractions = {
""ain't"": ""am not / are not"",
""aren't"": ""are not / am not"",
""can't"": ""cannot"",
""can't've"": ""cannot have"",
""'cause"": ""because"",
""could've"": ""could have"",
""couldn't"": ""could not"",
""couldn't've"": ""could not have"",
""didn't"": ""did not"",
""doesn't"": ""does not"",
""don't"": ""do not"",
""hadn't"": ""had not"",
""hadn't've"": ""had not have"",
""hasn't"": ""has not"",
""haven't"": ""have not"",
""he'd"": ""he had / he would"",
""he'd've"": ""he would have"",
""he'll"": ""he shall / he will"",
""he'll've"": ""he shall have / he will have"",
""he's"": ""he has / he is"",
""how'd"": ""how did"",
""how'd'y"": ""how do you"",
""how'll"": ""how will"",
""how's"": ""how has / how is"",
""i'd"": ""i had / i would"",
""i'd've"": ""i would have"",
""i'll"": ""i shall / i will"",
""i'll've"": ""i shall have / i will have"",
""i'm"": ""i am"",
""i've"": ""i have"",
""isn't"": ""is not"",
""it'd"": ""it had / it would"",
""it'd've"": ""it would have"",
""it'll"": ""it shall / it will"",
""it'll've"": ""it shall have / it will have"",
""it's"": ""it has / it is"",
""let's"": ""let us"",
""ma'am"": ""madam"",
""mayn't"": ""may not"",
""might've"": ""might have"",
""mightn't"": ""might not"",
""mightn't've"": ""might not have"",
""must've"": ""must have"",
""mustn't"": ""must not"",
""mustn't've"": ""must not have"",
""needn't"": ""need not"",
""needn't've"": ""need not have"",
""o'clock"": ""of the clock"",
""oughtn't"": ""ought not"",
""oughtn't've"": ""ought not have"",
""shan't"": ""shall not"",
""sha'n't"": ""shall not"",
""shan't've"": ""shall not have"",
""she'd"": ""she had / she would"",
""she'd've"": ""she would have"",
""she'll"": ""she shall / she will"",
""she'll've"": ""she shall have / she will have"",
""she's"": ""she has / she is"",
""should've"": ""should have"",
""shouldn't"": ""should not"",
""shouldn't've"": ""should not have"",
""so've"": ""so have"",
""so's"": ""so as / so is"",
""that'd"": ""that would / that had"",
""that'd've"": ""that would have"",
""that's"": ""that has / that is"",
""there'd"": ""there had / there would"",
""there'd've"": ""there would have"",
""there's"": ""there has / there is"",
""they'd"": ""they had / they would"",
""they'd've"": ""they would have"",
""they'll"": ""they shall / they will"",
""they'll've"": ""they shall have / they will have"",
""they're"": ""they are"",
""they've"": ""they have"",
""to've"": ""to have"",
""wasn't"": ""was not"",
""we'd"": ""we had / we would"",
""we'd've"": ""we would have"",
""we'll"": ""we will"",
""we'll've"": ""we will have"",
""we're"": ""we are"",
""we've"": ""we have"",
""weren't"": ""were not"",
""what'll"": ""what shall / what will"",
""what'll've"": ""what shall have / what will have"",
""what're"": ""what are"",
""what's"": ""what has / what is"",
""what've"": ""what have"",
""when's"": ""when has / when is"",
""when've"": ""when have"",
""where'd"": ""where did"",
""where's"": ""where has / where is"",
""where've"": ""where have"",
""who'll"": ""who shall / who will"",
""who'll've"": ""who shall have / who will have"",
""who's"": ""who has / who is"",
""who've"": ""who have"",
""why's"": ""why has / why is"",
""why've"": ""why have"",
""will've"": ""will have"",
""won't"": ""will not"",
""won't've"": ""will not have"",
""would've"": ""would have"",
""wouldn't"": ""would not"",
""wouldn't've"": ""would not have"",
""y'all"": ""you all"",
""y'all'd"": ""you all would"",
""y'all'd've"": ""you all would have"",
""y'all're"": ""you all are"",
""y'all've"": ""you all have"",
""you'd"": ""you had / you would"",
""you'd've"": ""you would have"",
""you'll"": ""you shall / you will"",
""you'll've"": ""you shall have / you will have"",
""you're"": ""you are"",
""you've"": ""you have""
}

then write some code to modify your text according to the dictionary, something like this:
text=""what's the best way to ensure this?""
for word in text.split():
    if word.lower() in contractions:
        text = text.replace(word, contractions[word.lower()])
print(text)

for your second question on changing verb tense, nodebox's linguistics library is very popular and highly recommended for such tasks. after downloading their zip file, unzip it and copy it to python's site-package directory. after doing that, you can write something like this:
import en
for word in text.split():
    if en.is_verb(word.lower()):
        text = text.replace(word, en.verb.present(word.lower()))
print text

note: this library is only for python 2 since it does not yet offer support for python 3.",https://stackoverflow.com/questions/43018030,python,25-03-2017 15:11,18503.0,15.0,4.0,True,08-08-2021 06:15,03-11-2018 20:25
75535679,implementation of adamw is deprecated and will be removed in a future version. use the pytorch implementation torch.optim.adamw,"how to fix this deprecated adamw model?
i tried to use the bert model to perform a sentiment analysis on the hotel reviews, when i run this piece of code, it prompts the following warning. i am still studying the transformers and i don't want the code to be deprecated very soon. i searched on the web and i can't find the solution yet.
i found this piece of information, but i don't know how to apply it to my code.

to switch optimizer, put optim=""adamw_torch"" in your trainingarguments
(the default is ""adamw_hf"")

could anyone kindly help with this?
from transformers import berttokenizer, bertforsequenceclassification
import torch_optimizer as optim
from torch.utils.data import dataloader
from transformers import adamw
import pandas as pd
import torch
import random
import numpy as np
import torch.nn as nn
from torch.nn import crossentropyloss
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from tqdm.notebook import tqdm
import json
from collections import ordereddict
import logging
from torch.utils.tensorboard import summarywriter

skip some code...
param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'gamma', 'beta']
optimizer_grouped_parameters = [{
    'params':
    [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
    'weight_decay_rate':
    0.01
}, {
    'params':
    [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
    'weight_decay_rate':
    0.0
}]

#
optimizer = adamw(optimizer_grouped_parameters, lr=1e-5)           ##deprecated 
#optimizer = optim.adamw(optimizer_grouped_parameters, lr=1e-5)      ##torch.optim.adamw  (not working)

step = 0
best_acc = 0
epoch = 10
writer = summarywriter(log_dir='model_best')
for epoch in tqdm(range(epoch)):
    for idx, batch in tqdm(enumerate(train_loader),
                           total=len(train_texts) // batch_size,
                           leave=false):
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs[0]  # calculate loss
        logging.info(
            f'epoch-{epoch}, step-{step}, loss: {loss.cpu().detach().numpy()}')
        step += 1
        loss.backward()
        optimizer.step()
        writer.add_scalar('train_loss', loss.item(), step)
    logging.info(f'epoch {epoch}, present best acc: {best_acc}, start evaluating.')
    accuracy, precision, recall, f1 = eval_model(model, eval_loader)  # evaluate model
    writer.add_scalar('dev_accuracy', accuracy, step)
    writer.add_scalar('dev_precision', precision, step)
    writer.add_scalar('dev_recall', recall, step)
    writer.add_scalar('dev_f1', f1, step)
    if accuracy > best_acc:
        model.save_pretrained('model_best')  # save model
        tokenizer.save_pretrained('model_best')
        best_acc = accuracy","['python', 'pytorch', 'huggingface-transformers', 'sentiment-analysis']",75543088,"if you comment out both these lines:
import torch_optimizer as optim
from transformers import adamw

and then use:
optimizer = torch.optim.adamw(optimizer_grouped_parameters, lr=1e-5)

does it work? if not, what is the error?

to switch optimizer, put optim=""adamw_torch"" in your trainingarguments (the default is ""adamw_hf"")

this is referring to huggingface trainer, which is configured with a trainingarguments instance. but as you are using your own training loop, this is not applicable to you.",https://stackoverflow.com/questions/75535679,python,22-02-2023 16:37,6560.0,5.0,1.0,True,23-02-2023 09:48,22-02-2023 18:18
74062036,keras seq2seq model output shapes,"i am working on keras seq2seq example here:
what i have understood from the text is in decoder model each cell's output is input to the next cell.
however i didnt understand implementing this recursion to the model.in the link it makes the decoder model as follows.
decoder_model = model(
        [decoder_inputs] + decoder_states_inputs,
        [decoder_outputs] + decoder_states)

how does this syntax work to tell the model that each cells output is input to next cell?
in general how does this syntax work?
edit:
when you check keras.model documentation you will realize that a model can take a  list of keras.input objects as input argument, notice that [decoder_inputs] + decoder_states_inputs is a list.","['keras', 'deep-learning', 'nlp', 'lstm', 'seq2seq']",74062999,"if you look at the documentation for the keras model class here, you'll see that the model() function takes in inputs and outputs as its first and second arguments respectively (model(inputs, outputs)). this specifies the input and output layers of the model (in your case, a decoder that will be used in the inference loop of the decode_sequence() function at the end of the article you linked).
to elaborate more on the code snippet you posted, you are providing decoder_inputs and decoder_states_inputs together as the inputs argument of model(inputs, outputs) to specify the input layer of the decoder model:

decoder_inputs is an input object (keras tensor) with length num_decoder_tokens, instantiated using the input() function (see input) that simply accepts the input tokens (characters).

similarly, decoder_states_inputs is a list of two input tensors for the decoder's hidden input state and cell state, both of length latent_dim.


and again, you provide the decoder_outputs and decoder_states together as the outputs argument of model(inputs, outputs) to specify the output layer of the model:

decoder_outputs ends up being a densely connected nn layer used for output activation (see dense).
decoder_states is a list containing the hidden state state_h and cell state state_c of the decoder_lstm.",https://stackoverflow.com/questions/74062036,keras,13-10-2022 21:32,124.0,1.0,1.0,True,17-10-2022 18:20,17-10-2022 18:20
77251249,getting &#39;valueerror: file path is not a valid file or url&#39; when using pypdfloader with a valid file path,"import os 
import openai
import sys
sys.path.append('../..')
from dotenv import load_dotenv, find_dotenv
_=load_dotenv(find_dotenv())
openai.api_key =""my_key""
from langchain.document_loaders import pypdfloader
loader = pypdfloader(""c:\\users\\lenovo\\documents\\machinelearning-lecture01.pdf"")

in my code, i've made sure to use the 'r' prefix to treat backslashes as literal characters in the file path, used double backslashes to escape them properly, and double-checked that the file indeed exists at the specified location ""c:\users\lenovo\desktop\machinelearning-lecture01.pdf."" however, despite these precautions, i'm consistently receiving a 'valueerror' stating that the file path is not recognized as a valid file or url when using the pypdfloader.
how to resolve this issue and successfully load the pdf file?","['langchain', 'py-langchain']",77254336,"to address the issue, i uploaded the file directly to google colab and then utilized the following line of code to read it:
loader = pypdfloader(""machinelearning-lecture01.pdf"")

this allowed me to access and work with the file in colab seamlessly.",https://stackoverflow.com/questions/77251249,langchain,07-10-2023 19:34,5200.0,0.0,2.0,True,08-10-2023 16:06,08-10-2023 00:11
68197664,how to take just the score from huggingface pipeline sentiment analysis,"i'm quite new to the whole huggingface pipeline world, and i have stumbled upon something which i can't figure out. i have googled quite a bit for an answer, but haven't found anything yet, so any help would be great.
i am trying to get just the score from the hf pipeline sentiment classifier, not the label, as i want to apply the scores to a dataframe containing many cells of text.
i know how to achieve this on just a single sentence, namely like so:
from transformers import pipeline
classifier = pipeline(""sentiment-analysis"")

result = classifier(""this is a positive sentence"")[0]
(result['score'])

this gives me the following output:

0.9994597434997559

i know how to apply the classifier to my dataframe. however, when i adapt the code above to the dataframe, like so:
result = df['text'].apply(lambda x: classifier(x[:512]))[0]
df['sentiment'] = result['score']

my code fails on the second line, with the following error:
typeerror: list indices must be integers or slices, not str

does anyone know how to fix this? i have tried a few things, but i haven't been able to figure it out so far. any help would be immensely appreciated!","['python', 'pandas', 'sentiment-analysis', 'huggingface-transformers']",72998972,"if your classifier output looks like this:
[{'label': '1', 'score': 0.9999555349349976}]

then you could extract the score with the following:
result['sentiment'] = df['text'].apply(lambda x: classifier(x[:512]).apply(
  lambda x: classifier(x)).str[0].str['score']


alternatively:
get the classifier output:
df['result'] = df['text'].apply(lambda x: classifier(x[:512]))

extract the score from the output:
df['sentiment'] = df['result'].str[0].str['score']",https://stackoverflow.com/questions/68197664,python,30-06-2021 15:28,1932.0,3.0,2.0,True,18-07-2022 03:23,26-05-2022 13:13
70725486,error: cannot install en-core-web-trf because these package versions have conflicting dependencies,"i use the following commands (from spacy website here) to install spacy and en_core_web_trf under windows 10 home 64 bit, however, i have encountered problems while running the last (third line) command.
pip install -u pip setuptools wheel
pip install -u spacy
python -m spacy download en_core_web_trf

error:
info: pip is looking at multiple versions of en-core-web-trf to determine which version is compatible with other requirements. this could take a while.
error: cannot install en-core-web-trf because these package versions have conflicting dependencies.

the conflict is caused by:
    spacy-transformers 1.1.4 depends on torch>=1.6.0
    spacy-transformers 1.1.3 depends on torch>=1.6.0
    spacy-transformers 1.1.2 depends on torch>=1.6.0

to fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict

error: resolutionimpossible: for help visit 

anyone knows how to solve it? thanks! i was stuck here for the whole evening.
error update after uninstalling the spacy and reinstall it with pip install spacy-transformers -f 
error: cannot install spacy-transformers==0.5.0, spacy-transformers==0.5.1, spacy-transformers==0.5.2, spacy-transformers==0.5.3, spacy-transformers==0.6.1, spacy-transformers==0.6.2, spacy-transformers==1.0.0, spacy-transformers==1.0.1, spacy-transformers==1.0.2, spacy-transformers==1.0.3, spacy-transformers==1.0.4, spacy-transformers==1.0.5, spacy-transformers==1.0.6, spacy-transformers==1.1.0, spacy-transformers==1.1.1, spacy-transformers==1.1.2, spacy-transformers==1.1.3 and spacy-transformers==1.1.4 because these package versions have conflicting dependencies.

the conflict is caused by:
    spacy-transformers 1.1.4 depends on torch>=1.6.0
    spacy-transformers 1.1.3 depends on torch>=1.6.0
    spacy-transformers 1.1.2 depends on torch>=1.6.0
    spacy-transformers 1.1.1 depends on torch>=1.6.0
    spacy-transformers 1.1.0 depends on torch>=1.6.0
    spacy-transformers 1.0.6 depends on torch>=1.5.0
    spacy-transformers 1.0.5 depends on torch>=1.5.0
    spacy-transformers 1.0.4 depends on torch>=1.5.0
    spacy-transformers 1.0.3 depends on torch>=1.5.0
    spacy-transformers 1.0.2 depends on torch>=1.5.0
    spacy-transformers 1.0.1 depends on torch>=1.5.0
    spacy-transformers 1.0.0 depends on torch>=1.5.0
    spacy-transformers 0.6.2 depends on spacy<2.4.0 and >=2.3.0
    spacy-transformers 0.6.1 depends on spacy<2.4.0 and >=2.3.0
    spacy-transformers 0.5.3 depends on spacy<2.3.0 and >=2.2.1
    spacy-transformers 0.5.2 depends on spacy<2.3.0 and >=2.2.1
    spacy-transformers 0.5.1 depends on spacy<2.3.0 and >=2.2.1
    spacy-transformers 0.5.0 depends on spacy<2.3.0 and >=2.2.1

to fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict

error: resolutionimpossible: for help visit","['python', 'pip', 'nlp', 'spacy']",70730232,"information for someone who will have the same problem with me.
the mentioned problem was finally not solved, at least i tried to reinstall packages, reinstall python (totally uninstalled it before reinstalling). nothing worked for me.
how did i avoid this finally?
i uninstalled the python completely.
install anaconda (latest version), while installing the anaconda, the pip and python are also installed by default. then i use pip install xx in the windows command line, no matter to install spacy, or to install the requirements.txt, all work well. that's how did i avoid the above mentioned problem.
hope it is helpful for someone who also have this problem.",https://stackoverflow.com/questions/70725486,python,15-01-2022 21:13,13264.0,4.0,3.0,True,17-12-2023 04:26,16-01-2022 09:59
76915495,shape of my dataframe(#rows) and that of final embeddings array doesn&#39;t match,"i generated the word embeddings for my corpus(2-d list) then tried to generate the average word2vec embeddings for each of the individual word list(that is for each comment which have been converted into a list though split() method) inside my corpus but the final length of my average word2vec embeddings numpy array and that of the #rows doesn't match i.e. 159571, which is the number of comments.
here's the code for generating the 'final_embeddings' array:
#building vocabulary
vocabulary = set(model.wv.index_to_key)

final_embeddings = []
for i in flatten_corpus:
    avg_embeddings = none
    for j in i:
      
         if j in vocabulary:

            if avg_embeddings is none:
                avg_embeddings = model.wv[j]
            else:
                avg_embeddings = avg_embeddings + model.wv[j]
    if avg_embeddings is not none:
        avg_embeddings = avg_embeddings / len(avg_embeddings)
        final_embeddings.append(avg_embeddings)



length of flatten_corpus: 159571
length of the above array: 159487 (doesn't match to above number)

what am i doing wrong?","['python', 'machine-learning', 'nlp', 'word2vec']",76917467,"you are only appending into your final_embeddings in a code branch that's only sometimes reached: if there's at least one known word in the text.
if any element of flatten_corpus only includes words that aren't in the model, it will simply proceed to the next item in flatten_corpus.
and then, you'll not only be missing those 84 items, but the average vectors in final_embeddings will no longer be aligned at the same slot indexes as their matching texts.
a quick and dirty fix would be to initialize your avg_embeddings to some value that stands-in, as the default, even if none of the words are known. for example:
    avg_embeddings = np.zeros(model.vector_size, dtype=np.float32)

of course, having 84 of your per-text summary average vector be zero-vectors may cause other problems down the way, so you may want to think more about what, if anything you should be doing for such texts. maybe, without word-vectors to model them, they should just be ignored.
other notes on making code that is easier to debug:

using descriptive temporary variable names like 'text' & 'word' instead of 'i' & 'j' makes code clearer

you can already test whether a word is inside a set of word-vectors (model.wv, of gensim class type keyedvectors) with idiomatic python membership-checking, so there's no need to create your vocabulary set ï¿½ï¿½ï¿½ instead just check with if word in model.wv:.

the keyedvectors object has a utility method for getting the average of  the word-vectors of a list-of-words, with other options that could prove helpful: .get_mean_vector() ï¿½ï¿½ï¿½ and if you combine that with a python list comprehension, your code can be replaced by a 1-liner:


final_embeddings = [model.wv.get_mean_vector(text) for text in flatten_corpus]
<",https://stackoverflow.com/questions/76915495,python,16-08-2023 16:33,64.0,0.0,1.0,True,16-08-2023 22:35,16-08-2023 18:30
75743057,openai chat completions api: how to implement a for loop with a list of questions in python?,"i've been trying to run a for loop to run through the openai chat completions api, but i don't seem to make it work. i'm puzzled. my goal is to have a list of all the responses.
basically, i have a list of sentences. let's call this list input_list. here's an example of what this would look like:
['who won the champions league in 2017?', 'who won the world cup in 2014?', ...]

and here's how i tried to loop through the input:
output = []
for i in range(len(input_list)):
  response = openai.chatcompletion.create(
      model=""gpt-3.5-turbo"",
      messages=[
          {""role"": ""system"", ""content"": ""you are a chatbot.""},
          {""role"": ""user"", ""content"": input_list[i]},
          ]
          )
  
  chat_response = response['choices'][0]['message']['content']
  output.append(chat_response)

when running this, however, the responses don't seem to append. i only ever see the very first answer in the output list. why is this the case? and how can i fix it? i would like to see all the responses.
many thanks in advance for your help!","['python', 'list', 'for-loop', 'openai-api', 'chatgpt-api']",75743229,"you need to print the output.
if you run test.py the openai api will return a completion:

['the winner of the uefa champions league in 2017 was real madrid.',
'the 2014 fifa world cup was won by germany.']

test.py
import openai
import os

openai.api_key = os.getenv('openai_api_key')

input_list = ['who won the champions league in 2017?', 'who won the world cup in 2014?']

output = []

for i in range(len(input_list)):
  response = openai.chatcompletion.create(
    model = 'gpt-3.5-turbo',
    messages = [
      {'role': 'system', 'content': 'you are a chatbot.'},
      {'role': 'user', 'content': input_list[i]},
    ]
  )
  
  chat_response = response['choices'][0]['message']['content']
  output.append(chat_response)

print(output)",https://stackoverflow.com/questions/75743057,python,15-03-2023 09:54,4536.0,0.0,1.0,True,12-06-2024 17:14,12-06-2024 17:14
74212658,convert spacy `doc` into conll 2003 sample,"i was planning to train a spark nlp custom ner model, which uses the conll 2003 format to do so (this blog even leaves some traning sample data to speed-up the follow-up). this ""sample data"" is not useful for me, as i have my own training data to train a model with; this data however, consists of a list of spacy doc objects and quite honestly, i don't know how to carry on with this conversion. i have found three approaches so far, each with some considerable weakness:

in spacy's documentation, i have found an example code about how to build a single doc to conll using spacy_conll project, but notice it uses a blank spacy model, so it is not clear where ""my own labeled data"" comes to play; furthermore, it seems conll_formatter component is ""added at the end of the pipeline"", so it seems ""no direct conversion from doc to conll is actually done""... is my grasping correct?

in prodigy forum (another product of the same designers of spacy), i found this purposal, however that ""conll"" (2003 i suppose?) format seems to be incomplete: the pos tag seems to be missing (which can be easily obtained via token.pos_, as well as the ""syntactic chunk"" (whose spacy equivalent, does not seem to exist). these four fields are mentioned in conll 2003 official documentation.

speaking of a ""direct conversion from doc to conll"", i have also found this implementation based on textacy library, but it seems this implementation got deprecated by version 0.11.0, because ""conll-u [...] wasn't enforced or guaranteed"" , so i am not sure whether to use it or not (btw, the most up-to-date textacy implementation when writing these lines, is 0.12.0)


my current code looks like:
import spacy
from spacy.training import offsets_to_biluo_tags
from spacy.tokens import span

print(""spacy helper model"")
base_model = ""en_core_web_sm""
nlp = spacy.load(base_model)
to_disable= ['parser', 'lemmatizer', 'ner']
_ = [nlp.remove_pipe(item) for item in to_disable]
print(""base model used: "", base_model)
print(""removed components: "", to_disable)
print(""enabled components: "", nlp.pipe_names)

# assume text is already available as sentences...
# so no need for spacy `sentencizer` or similar
print(""\ndemo spacy doc list building..."", end="""")
doc1 = nlp(""iphone x is coming."")
doc1.ents = [span(doc1, 0, 2, label=""gadget"")]
doc2 = nlp(""space x is nice."")
doc2.ents = [span(doc1, 0, 2, label=""brand"")]
docs = [doc1, doc2]
print(""done!"")

print(""\nconll 2003 conversion:\n"")
results = []
for doc in docs:
    # preliminary: whole sentence
    whole_sentence = doc.text
    # 1st item (conll 2003): word
    words = [token.text for token in doc]
    # 2nd item (conll 2003): pos
    pos = [token.tag_ for token in doc]
    # 3rd item (conll 2003): syntactic chunk tag
    sct = [""[unknown]"" for token in doc]
    # 4th item (conll 2003): named entities
    spacy_entities = [
        (ent.start_char, ent.end_char, ent.label_)
        for ent in doc.ents
    ]
    biluo_entities = offsets_to_biluo_tags(doc, spacy_entities)
    results.append((whole_sentence, words, pos, sct, biluo_entities))

for result in results:
    print(
        ""\ndoc text (not included in conll 2003, just for demo): "",
        result[0], ""\n""
    )
    print(""-docstart- -x- -x- o"")
    for w,x,y,z in zip(result[1], result[2], result[3], result[4]):
        print(w,x,y,z)

# pending: write to a file, but that's easy, and out of topic.

which gives as output:
doc text (not included in conll 2003, just for demo):  iphone x is coming.

-docstart- -x- -x- o
iphone nnp [unknown] b-gadget
x nnp [unknown] l-gadget
is vbz [unknown] o
coming vbg [unknown] o
. . [unknown] o

doc text (not included in conll 2003, just for demo):  space x is nice.

-docstart- -x- -x- o
space nnp [unknown] b-brand
x nnp [unknown] l-brand
is vbz [unknown] o
nice jj [unknown] o
. . [unknown] o

have you done something like this before?
thanks!","['python', 'nlp', 'spacy-3', 'conll']",74294187,"with @albertoandreotti's help, i managed to get to a functional workaround:
import spacy
from spacy.training import offsets_to_biluo_tags
from spacy.tokens import span

print(""spacy helper model"")
base_model = ""en_core_web_sm""
nlp = spacy.load(base_model)
to_disable= ['parser', 'lemmatizer', 'ner']
_ = [nlp.remove_pipe(item) for item in to_disable]
print(""base model used: "", base_model)
print(""removed components: "", to_disable)
print(""enabled components: "", nlp.pipe_names)

# assume text is already available as sentences...
# so no need for spacy `sentencizer` or similar
print(""\ndemo spacy doc list building..."", end="""")
doc1 = nlp(""iphone x is coming."")
doc1.ents = [span(doc1, 0, 2, label=""gadget"")]
doc2 = nlp(""space x is nice."")
doc2.ents = [span(doc1, 0, 2, label=""brand"")]
docs = [doc1, doc2]
print(""done!"")

print(""\nconll 2003 conversion:\n"")
results = []
for doc in docs:
    # preliminary: whole sentence
    whole_sentence = doc.text
    # 1st item (conll 2003): word
    words = [token.text for token in doc]
    # 2nd item (conll 2003): pos
    pos = [token.tag_ for token in doc]
    # 3rd item (conll 2003): syntactic chunk tag
    # sct = pos  # redundant, so will be left out
    # 4th item (conll 2003): named entities
    spacy_entities = [
        (ent.start_char, ent.end_char, ent.label_)
        for ent in doc.ents
    ]
    biluo_entities = offsets_to_biluo_tags(doc, spacy_entities)
    results.append((whole_sentence, words, pos, biluo_entities))

for result in results:
    print(
        ""\ndoc text (not included in conll 2003, just for demo): "",
        result[0], ""\n""
    )
    print(""-docstart- -x- -x- o"")
    for w,x,y,z in zip(result[1], result[2], result[2], result[3]):
        print(w,x,y,z)

as complementary information, i found out that the 3rd missing item, ""syntactic chunking tag"", is related to a broader problem called ""phrase chunking"", that happens to be an unsolved problem in computer science, for which only aproximations have been got, so regardless of the library used, the conversion of that 3rd item specifically, into conll 2033, might have errors. however, it seems spark nlp does not care at all about 2nd & 3rd items, so the workaround suggested here, is acceptable.
for more details, you might want to put an eye on this thread.",https://stackoverflow.com/questions/74212658,python,26-10-2022 18:35,775.0,1.0,3.0,True,17-11-2022 23:26,01-11-2022 17:51
72164415,how to convert each word of each row to numeric value of a dataframe,"this dataframe is given to me.

my desired output using a dictionary is like this
**given the following dictionary:-** 
d = {'i': 30,'am': 45,'good': 90,'boy': 50,'we':100,'are':70,'going':110}


how to do this using python ..
i have tried like this but have failed :(
dataframe['new'] = data['documents'].apply(lambda x: dictionary[x]) 

kindly help me out. thanks in advance.","['python', 'pandas', 'dataframe', 'nlp']",72164461,"you can use explode to get words then map with your dict and reshape your dataframe:
mapping = {'i': 30,'am': 45,'good': 90,'boy': 50,'we':100,'are':70,'going':110}

df['documents'] = (df['documents'].str.split().explode().map(mapping).astype(str)
                                  .groupby(level=0).agg(list).str.join(' '))
print(df)

# output
   id    documents
0   0  30 45 90 50
1   1   100 70 110
2   2    30 45 110

step by step
phase 1: explode
# split phrase into words
>>> out = df['documents'].str.split()
0    [i, am, good, boy]
1      [we, are, going]
2        [i, am, going]
name: documents, dtype: object

# explode lists into scalar values
>>> out = out.explode()
0        i
0       am
0     good
0      boy
1       we
1      are
1    going
2        i
2       am
2    going
name: documents, dtype: object

phase 2: transform
# convert words with your dict mapping and convert as string
>>> out = out.map(mapping).astype(str)
0     30
0     45
0     90
0     50
1    100
1     70
1    110
2     30
2     45
2    110
name: documents, dtype: object  # <- .astype(str)

phase 3: reshape
# group by index (level=0) then aggregate to a list
>>> out = out.groupby(level=0).agg(list)
0    [30, 45, 90, 50]
1      [100, 70, 110]
2       [30, 45, 110]
name: documents, dtype: object

# join your list of words
>>> out = out.str.join(' ')
0    30 45 90 50
1     100 70 110
2      30 45 110
name: documents, dtype: object",https://stackoverflow.com/questions/72164415,python,08-05-2022 19:22,265.0,1.0,2.0,True,09-05-2022 21:51,08-05-2022 19:52
78324647,sklearm featurehasher not working on a single column in a dataframe,"i tried performing the feature hasher on a single column in my dataframe but it keeps on giving the error:

valueerror: samples can not be a single string. the input must be an iterable over iterables of strings.

from sklearn.feature_extraction import featurehasher

hash_vector_size = 50
fh = featurehasher(n_features=hash_vector_size, input_type='string')
hashed_df = pd.dataframe(fh.transform(x_train[""item_identifier""]).toarray(),
                         columns=['h'+str(i) for i in range (hash_vector_size)])

i was expecting a dataframe of 50 columns where the data would have been hashed in","['pandas', 'machine-learning', 'scikit-learn', 'nlp']",78324733,"you were almost there:
from sklearn.feature_extraction import featurehasher
import pandas as pd

data = {""item_identifier"": [""id1"", ""id2"", ""id3"", ""id4"", ""id5""]}
x_train = pd.dataframe(data)

hash_vector_size = 50
fh = featurehasher(n_features=hash_vector_size, input_type='string')
hashed_features = fh.transform([[item] for item in x_train[""item_identifier""]])

hashed_df = pd.dataframe(hashed_features.toarray(),
                         columns=['h'+str(i) for i in range(hash_vector_size)])

print(hashed_df)

which gives
h0   h1   h2   h3   h4   h5   h6   h7   h8   h9  ...  h40  h41  h42  h43  \
0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -1.0  ...  0.0  0.0  0.0  0.0   
1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   
2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   
3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   
4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   

   h44  h45  h46  h47  h48  h49  
0  0.0  0.0  0.0  0.0  0.0  0.0  
1  0.0  0.0  0.0  0.0  0.0  0.0  
2  0.0  0.0  0.0  0.0  0.0  0.0  
3  0.0  0.0  0.0  0.0  0.0  0.0  
4  0.0  0.0  0.0  0.0  0.0  0.0  

[5 rows x 50 columns]",https://stackoverflow.com/questions/78324647,pandas,14-04-2024 17:20,111.0,2.0,1.0,True,14-04-2024 22:20,14-04-2024 22:20
75873428,how to install diff version of a package (transformers) without internet in kaggle notebook w/o killing the kernel while keeping variables in memory?,"i have prepared an inference pipeline for a kaggle competition and it has to be executed without internet connection.
i'm trying to use different versions of transformers but i had some issues regarding the installation part.
kaggle's default transformers version is 4.26.1. i start with installing a different branch of transformers (4.18.0.dev0) like this.
!pip install ./packages/sacremoses-0.0.53
!pip install /directory/to/packages/transformers-4.18.0.dev0-py3-none-any.whl --find-links /directory/to/packages

it installs transformers-4.18.0.dev0 without any problem. i use this version of the package and do the inference with some models. then i want to use another package open_clip_torch-2.16.0 which is compatible with transformers-4.27.3, so i install them by simply doing
!pip install /directory/to/packages/transformers-4.27.3-py3-none-any.whl --no-index --find-links /directory/to/packages
!pip install /directory/to/packages/open_clip_torch-2.16.0-py3-none-any.whl --no-index --find-links /directory/to/packages/

i get a prompt of successfully installed transformers-4.27.3 and open_clip_torch-2.16.0.
!pip list | grep transformers outputs transformers 4.27.3 but when i do
import transformers
transformers.__version__

the version is '4.18.0.dev0'. i can't use open_clip because of that reason. some of the codes are breaking because it uses the old version of transformers even though i installed a newer version. how can i resolve this issue?","['python', 'jupyter-notebook', 'pip', 'huggingface-transformers', 'python-importlib']",75874102,"when you initially import a module in a python environment it is cached in sys.modules. subsequent imports are not read from the disk but from the cache, for this reason you are not seeing the new version of the module being loaded.
import sys
import transformers
sys.modules['transformers'].__version__

a possible solution is to attempt to reload the module using importlib.reload.
import importlib
importlib.reload(transformers)
sys.modules['transformers'].__version__

read the documentation so that you are aware of the caveats of using this method.",https://stackoverflow.com/questions/75873428,python,29-03-2023 05:26,1879.0,1.0,2.0,True,29-03-2023 07:31,29-03-2023 06:47
71826119,search for phrases in text with pos tagging,"i want to extract phrases that have ""of"" between two nouns.
this is my code:
import nltk

text = ""i live in kingdom of spain""

tokens = nltk.word_tokenize(text)
tag = nltk.pos_tag(tokens, tagset='universal')
print(tag)

regexes = '''phrase:{<noun>of<noun>}'''

noun_phrase_regex = nltk.regexpparser(regexes)
result = noun_phrase_regex.parse(tag)
result = list(result)
print(result)

unfortunately, i do not get tree in my result, so my regex do not work good.
i have tried:
{<noun>(of)<noun>}
{<noun>{of}<noun>}
{<noun>of<noun>}
{<noun><of><noun>}

but result is the same.
also, when i get the result, how can i extract the tree values from the list, for now, im doing that like this:
result = [element for element in result if type(element) != tuple]
result = ["" "".join([word[0] for word in tup_phrase]) for tup_phrase in result]
print(result)","['python', 'nltk']",71831503,"it isn't possible to mix words and pos tags in an nltk parser grammar.
you can still achieve what you want by other means though. for example you can match all pos tags that match your requirement and then check the result set for those which contain 'of', and whichever variations of that word that you want (e.g. w/some capitals). that would work like so:
import nltk


text = ""i live in the kingdom of spain""
tokens = nltk.word_tokenize(text)
tag = nltk.pos_tag(tokens, tagset='universal')
print(tag)

regexes = 'chunk: {<noun> <adp> <noun>}'
noun_phrase_regex = nltk.regexpparser(regexes)
result = noun_phrase_regex.parse(tag)

tree = noun_phrase_regex.parse(tag)
chunks = []
for subtree in tree.subtrees():
    if subtree.label() == 'chunk': 
        chunks.append(subtree)

found = []
for chunk in chunks:
    leaves = chunk.leaves()
    if leaves[1][0] == 'of':
        found.append(' '.join([word for word, _ in leaves]))

print(found)

this will give you:
>>> print(found)
['kingdom of spain']
>>> nltk.__version__
'3.7'",https://stackoverflow.com/questions/71826119,python,11-04-2022 10:18,515.0,1.0,1.0,True,11-04-2022 16:48,11-04-2022 14:19
78836208,removing bi-grams after tokenization for tfidfvectorizer,"i'm attempting to remove bi-grams that are created by tfidfvectorizer.  i'm using text.tfidfvectorizer so that i can use my own preprocessor function.
test strings and preprocessor function:
doc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', 
        'another that has aa aa and start date and hurricane hitting south carolina']

def remove_bigrams(doc):
    gram_2 = ['past performance', 'start date', 'aa aa']
    res = []
    for record in doc:
        the_string = record
        for phrase in gram_2:
            the_string = the_string.replace(phrase, """")
        res.append(the_string)
    return res

remove_bigrams(doc2)

my tfidfvectorizer instantiation and fit_transform:
from sklearn.feature_extraction.text import english_stop_words as stop_words
from sklearn.feature_extraction.text import tfidfvectorizer
from sklearn.feature_extraction import text

custom_stop_words = [i for i in stop_words]

vec = text.tfidfvectorizer(stop_words=custom_stop_words,
                           analyzer='word',
                           ngram_range=(2, 2),
                           preprocessor=remove_bigrams,
                          )

features = vec.fit_transform(doc2)

here is my error:
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
input in [49], in <cell line: 5>()
      3 #t3_cv = countvectorizer(t2, stop_words = stop_words)
      4 vec = text.tfidfvectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams)
----> 5 features = vec.fit_transform(doc2)

file c:\development_solutions\sandbox\sbve\lib\site-packages\sklearn\feature_extraction\text.py:2079, in tfidfvectorizer.fit_transform(self, raw_documents, y)
   2072 self._check_params()
   2073 self._tfidf = tfidftransformer(
   2074     norm=self.norm,
   2075     use_idf=self.use_idf,
   2076     smooth_idf=self.smooth_idf,
   2077     sublinear_tf=self.sublinear_tf,
   2078 )
-> 2079 x = super().fit_transform(raw_documents)
   2080 self._tfidf.fit(x)
   2081 # x is already a transformed view of raw_documents so
   2082 # we set copy to false

file c:\development_solutions\sandbox\sbve\lib\site-packages\sklearn\feature_extraction\text.py:1338, in countvectorizer.fit_transform(self, raw_documents, y)
   1330             warnings.warn(
   1331                 ""upper case characters found in""
   1332                 "" vocabulary while 'lowercase'""
   1333                 "" is true. these entries will not""
   1334                 "" be matched with any documents""
   1335             )
   1336             break
-> 1338 vocabulary, x = self._count_vocab(raw_documents, self.fixed_vocabulary_)
   1340 if self.binary:
   1341     x.data.fill(1)

file c:\development_solutions\sandbox\sbve\lib\site-packages\sklearn\feature_extraction\text.py:1209, in countvectorizer._count_vocab(self, raw_documents, fixed_vocab)
   1207 for doc in raw_documents:
   1208     feature_counter = {}
-> 1209     for feature in analyze(doc):
   1210         try:
   1211             feature_idx = vocabulary[feature]

file c:\development_solutions\sandbox\sbve\lib\site-packages\sklearn\feature_extraction\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)
    111     doc = preprocessor(doc)
    112 if tokenizer is not none:
--> 113     doc = tokenizer(doc)
    114 if ngrams is not none:
    115     if stop_words is not none:

typeerror: expected string or bytes-like object

how to resolve it?","['python', 'scikit-learn', 'nlp', 'preprocessor', 'tfidfvectorizer']",78837616,"the preprocessor should handle documents, not the whole corpus. (the clues are the ""expected string"" in the error, and the fact that the tfidfvectorizer docs refer to ""the preprocessing (string transformation) stage"". the docs could definitely be clearer.)
this should fix it:
def remove_bigrams(doc: str) -> str:
    """"""remove certain bi-grams from a document.""""""
    gram_2 = ['past performance', 'start date', 'aa aa']
    for phrase in gram_2:
        doc = doc.replace(phrase, """")
    return doc",https://stackoverflow.com/questions/78836208,python,05-08-2024 19:46,42.0,1.0,1.0,True,07-08-2024 08:15,07-08-2024 00:03
78322637,langchain: how to view the context my retriever used when invoke,"i am trying to make a private llm with rag capabilities. i successfully followed a few tutorials and made one. but i wish to view the context the multivectorretriever retriever used when langchain invokes my query.
this is my code:
from langchain_core.output_parsers import stroutputparser
from langchain_core.prompts import chatprompttemplate
from langchain.retrievers.multi_vector import multivectorretriever
from langchain.storage import inmemorystore
from langchain_community.chat_models import chatollama
from langchain_community.embeddings import gpt4allembeddings
from langchain_community.vectorstores import chroma
from langchain_core.documents import document
from langchain_core.runnables import runnablepassthrough
from pil import image
import io
import os
import uuid
import json
import base64

def convert_bytes_to_base64(image_bytes):
    encoded_string=  base64.b64encode(image_bytes).decode(""utf-8"")
    return ""data:image/jpeg;base64,"" + encoded_string

#load retriever

path=""./vectorstore/pdf_test_file.pdf""

#load from json files
texts = json.load(open(os.path.join(path, ""json"", ""texts.json"")))
text_summaries = json.load(open(os.path.join(path, ""json"", ""text_summaries.json"")))
tables = json.load(open(os.path.join(path, ""json"", ""tables.json"")))
table_summaries = json.load(open(os.path.join(path, ""json"", ""table_summaries.json"")))
img_summaries = json.load(open(os.path.join(path, ""json"", ""img_summaries.json"")))

#load from figures
images_base64_list = []
for image in (os.listdir(os.path.join(path, ""figures""))):
    
    img = image.open(os.path.join(path, ""figures"",image))
    buffered = io.bytesio()
    img.save(buffered,format=""png"")
    image_base64 = convert_bytes_to_base64(buffered.getvalue())
    #warning: this section of the code does not support external ides like spyder and will break. run it loccally in the native terminal
    images_base64_list.append(image_base64)


#add to vectorstore

# the vectorstore to use to index the child chunks
vectorstore = chroma(
    collection_name=""summaries"", embedding_function=gpt4allembeddings()
)

# the storage layer for the parent documents
store = inmemorystore()  # <- can we extend this to images
id_key = ""doc_id""

# the retriever (empty to start)
retriever = multivectorretriever(
    vectorstore=vectorstore,
    docstore=store,
    id_key=id_key,
)

# add texts
doc_ids = [str(uuid.uuid4()) for _ in texts]
summary_texts = [
    document(page_content=s, metadata={id_key: doc_ids[i]})
    for i, s in enumerate(text_summaries)
]
retriever.vectorstore.add_documents(summary_texts)
retriever.docstore.mset(list(zip(doc_ids, texts)))

# add tables
table_ids = [str(uuid.uuid4()) for _ in tables]
summary_tables = [
    document(page_content=s, metadata={id_key: table_ids[i]})
    for i, s in enumerate(table_summaries)
]
retriever.vectorstore.add_documents(summary_tables)
retriever.docstore.mset(list(zip(table_ids, tables)))

# add images
img_ids = [str(uuid.uuid4()) for _ in img_summaries]
summary_img = [
    document(page_content=s, metadata={id_key: img_ids[i]})
    for i, s in enumerate(img_summaries)
]
retriever.vectorstore.add_documents(summary_img)
retriever.docstore.mset(
    list(zip(img_ids, img_summaries))
)  # store the image summary as the raw document


img_summaries_ids_and_images_base64=[]
count=0
for img in images_base64_list:
    new_summary = [img_ids[count],img]
    img_summaries_ids_and_images_base64.append(new_summary)
    count+=1



# check response

# question example: ""what is the issues plagueing the acres?""

""""""
testing retrival

print(""\ntesting retrival: \n"")
prompt = ""images / figures with playful and creative examples""
responce = retriever.get_relevant_documents(prompt)[0]
print(responce)

""""""

""""""
retriever.vectorstore.similarity_search(""what is the issues plagueing the acres? show any relevant tables"",k=10)
""""""

# prompt template
template = """"""answer the question based only on the following context, which can include text, tables and images/figures:
{context}
question: {question}
""""""

prompt = chatprompttemplate.from_template(template)

# multi-modal llm
# model = llava
model = chatollama(model=""custom-mistral"")

# rag pipeline
chain = (
    {""context"": retriever, ""question"": runnablepassthrough()}
    | prompt
    | model
    | stroutputparser()
)

print(""\n\n\ntesting responce: \n"")

print(chain.invoke(
    ""what is the issues plagueing the acres? show any relevant tables""
))

the output will look something like this:

testing responce:

in the provided text, the main issue with acres is related to wildfires and their impact on various lands and properties. the text discusses the number of fires, acreage burned, and the level of destruction caused by wildfires in the united states from 2018 to 2022. it also highlights that most wildfires are human-caused (89% of the average number of wildfires from 2018 to 2022) and that fires caused by lightning tend to be slightly larger and burn more acreage than those caused by humans.

here's the table provided in the text, which shows the number of fires and acres burned on federal lands (by different organizations), other non-federal lands, and total:

| year | number of fires (thousands) | acres burned (millions) |
|------|-----------------------------|--------------------------|
| 2018 | 58.1                        | 8.8                      |
| 2019 | 58.1                        | 4.7                      |
| 2020 | 58.1                        | 10.1                     |
| 2021 | 58.1                        | 10.1                     |
| 2022 | 58.1                        | 3.6                      |

the table also breaks down the acreage burned by federal lands (doi and fs) and other non-federal lands, as well as showing the total acreage burned each year.<|im_end|>

from the rag pipline i wish to print out the the context used from the retriever which stores tons of vector embeddings. i wish to know which ones it uses for the query. something like :
chain.invoke(""what is the issues plagueing the acres? show any relevant tables"").get_context_used()

i know there are functions like
retriever.get_relevant_documents(prompt) 

and
retriever.vectorstore.similarity_search(prompt) 

which provides the most relevant context to the query but i'm unsure whether the invoke function pulls the same context with the other 2 functions.
the retriver im using from langchain is the multivectorretriever","['python', 'langchain', 'large-language-model', 'retrieval-augmented-generation']",78323617,"you can tap into langchains with a runnablelambda and print the state passed from the retriever to the prompt
from langchain_core.runnables import runnablelambda

def inspect(state):
    """"""print the state passed between runnables in a langchain and pass it on""""""
    print(state)
    return state

# rag pipeline
chain = (
    {""context"": retriever, ""question"": runnablepassthrough()}
    | runnablelambda(inspect)  # add the inspector here to print the intermediate results
    | prompt
    | model
    | stroutputparser()
)",https://stackoverflow.com/questions/78322637,python,14-04-2024 03:35,5136.0,2.0,1.0,True,14-04-2024 11:25,14-04-2024 03:47
75817155,openai api in a laravel project (backend implementation doesn&#39;t work),"i am currently trying to use this api :  in a personal laravel project.
i donï¿½ï¿½ï¿½t really know what is currently wrong in my code, all i can say is that the code detailed below gets my message displayed in the chatbox but returns an error 419 on my post request : xhr post  as well as a fetch error: server error.
when not using the route and controller but giving the direct api endpoint to the javascript, i manage to get a json response to my message...
please note that i would really like to make the backend part to work, and not just doing it in js.
also, i didn't want to use jquery but i would if it might help.
the javascript :
//csrf token
    const csrftoken = document.queryselector('meta[name=""csrf-token""]').getattribute('content');
    const xhr = new xml
    xhr.open('get', ' true);
    xhr.setrequesth'x-csrf-token', csrftoken);

    const button = document.getelementbyid('button-submit');
    const chatwindow = document.getelementbyid('chat-window');
    const url = '{{ url('send') }}';

//api fetch on button click
    button.addeventlistener('click', function (){
        const input = document.getelementbyid('input').value;

        chatwindow.innerhtml += `<div class=""messages-user"">
        <div class=""__user"">
            <p>${input}</p>
        </div>
        
        <div style=""clear: both""></div>
        </div>`;

        fetch(url, {
            method: 'post',
            body: json.stringify(input),
            headers: {
                'content-type': 'application/json',
                'authorization': 'bearer ""my_openai_key""'
            }
        }).then(function(response) {
            if (response.ok) {
                return response.text();
            } else {
                throw new error('server error.');
            }
        }).then(function(data) {
            chatwindow.innerhtml += `<div class=""messages-bot"">
                            <div class=""__bot"">
                                <p>${data}</p>
                            </div>
                            
                            </div>
                            `;
        }).catch(function(error) {
            console.log('fetch error:', error.message);
        });
    });

the web.php :
use illuminate\support\facades\route;
use app\

route::get(ï¿½ï¿½ï¿½/ï¿½ï¿½ï¿½, function () {
    return view(ï¿½ï¿½ï¿½welcomeï¿½ï¿½ï¿½);
});

route::post(ï¿½ï¿½ï¿½sendï¿½ï¿½ï¿½, [chatbotcontroller::class, ï¿½ï¿½ï¿½sendchatï¿½ï¿½ï¿½]);
public function sendchat(request $request){
    $result = openai::completions()->create([
        ï¿½ï¿½ï¿½max-tokenï¿½ï¿½ï¿½ => 100,
        ï¿½ï¿½ï¿½modelï¿½ï¿½ï¿½ => ï¿½ï¿½ï¿½text-davinci-003ï¿½ï¿½ï¿½,
        ï¿½ï¿½ï¿½promptï¿½ï¿½ï¿½ => $request->input
    ]);

    $response = array_reduce(
        $result->toarray()['choices'],
        fn(string $result, array $choice) => $result . $choice['text'], """"
    );
    
    return $response","['javascript', 'php', 'laravel', 'openai-api']",75817246,"in laravel th 419 error, means thet the csrf token field is not resent or it is incorrect.
in this case you probably dosen't need to use the crsf token for a api call.
so you need to move this line of code:
route::post('send', [chatbotcontroller::class, ï¿½ï¿½ï¿½sendchatï¿½ï¿½ï¿½]);

from web.php (which uses the csrf protection) to the api.php file under the routes folder in your laravel project.
at this point you need to call the /api/send endpoint instead of the /send e",https://stackoverflow.com/questions/75817155,javascript,22-03-2023 20:49,961.0,-3.0,1.0,True,12-12-2024 10:10,12-12-2024 10:10
76388150,does the sqldatabase agent for langchain store the database in memory?,"say i have a huge database stored in bigquery, and i'd like to use the sql database agent to query this database using natural language prompts. i can't store the data in memory because it's too huge. does the sqldatabase function store this in memory?
if so, can i directly query the source data without loading everything? i'm comfortable with the latencies involved with read operations on disk. this question might sound novice but i'm gradually exploring this package as i go","['python', 'langchain', 'py-langchain']",76854677,"quoting from dariel dato-on's answer to does anyone know what is roughly the database size limit for sql agent? #3730:

the sqldatabasechain will still work if you have a really large database. sqldatabasechain doesn't load the entire database into memory. instead, it formulates a sql query based on your input and then runs it against the database. the only time you may run into issues is if the sql query result is too long and exceeds the prompt token limit for your chosen llm. for example, davinci has a limit of about 4,000 tokens.
if you're using the sql database agent (rather than the chain), be aware that it will start with listing all the tables in the db. this can also potentially cause problems if the number of tables is too large and exceeds the prompt token limit.

but quoting from follow-up discussion, it does ""load up the ddl of the database into the context of the current session. [...] so [...] if the database has a large ddl [...] that [can] also create an issue"".",https://stackoverflow.com/questions/76388150,python,02-06-2023 07:47,1037.0,1.0,1.0,True,07-08-2023 19:31,07-08-2023 19:31
61680408,huggingface transformers not getting imported in vs code,"my vs code editor for python is not able to import transformers even though i have done a conda install and giving me the following error
traceback (most recent call last):
  file ""c:/users/i323017/documents/studies/question_answering_kinnal/src/main.py"", line 3, in <module>
    import transformers
  file ""c:\users\i323017\appdata\local\continuum\anaconda3\lib\site-packages\transformers\__init__.py"", line 107, in <module>
    from .pipelines import (
  file ""c:\users\i323017\appdata\local\continuum\anaconda3\lib\site-packages\transformers\pipelines.py"", line 40, in <module>
    from .tokenization_auto import autotokenizer
  file ""c:\users\i323017\appdata\local\continuum\anaconda3\lib\site-packages\transformers\tokenization_auto.py"", line 49, in <module>
    from .tokenization_flaubert import flauberttokenizer
  file ""c:\users\i323017\appdata\local\continuum\anaconda3\lib\site-packages\transformers\tokenization_flaubert.py"", line 23, in <module>
    from .tokenization_xlm import xlmtokenizer
  file ""c:\users\i323017\appdata\local\continuum\anaconda3\lib\site-packages\transformers\tokenization_xlm.py"", line 26, in <module>
    import sacremoses as sm
  file ""c:\users\i323017\appdata\local\continuum\anaconda3\lib\site-packages\sacremoses\__init__.py"", line 2, in <module>
    from sacremoses.tokenize import *
  file ""c:\users\i323017\appdata\local\continuum\anaconda3\lib\site-packages\sacremoses\tokenize.py"", line 10, in <module>
    from sacremoses.util import is_cjk
  file ""c:\users\i323017\appdata\local\continuum\anaconda3\lib\site-packages\sacremoses\util.py"", line 11, in <module>
    from joblib import parallel, delayed
modulenotfounderror: no module named 'joblib'

may i know the problem here?","['python', 'python-import', 'visual-studio-code', 'huggingface-transformers']",61695918,"this seems to be an installation issue. if you have already installed transformers using conda install -c conda-forge transformers, an additional upgradation from the source using the below resolved my issue.
git clone 
cd transformers
pip install .",https://stackoverflow.com/questions/61680408,python,08-05-2020 13:35,8172.0,0.0,4.0,True,17-10-2024 13:48,08-05-2020 13:41
30843011,save and reuse tfidfvectorizer in scikit learn,"i am using tfidfvectorizer in scikit learn to create a matrix from text data. now i need to save this object to reuse it later. i tried to use pickle, but it gave the following error.
loc=open('vectorizer.obj','w')
pickle.dump(self.vectorizer,loc)
*** typeerror: can't pickle instancemethod objects

i tried using joblib in sklearn.externals, which again gave similar error. is there any way to save this object so that i can reuse it later?
here is my full object:
class changetomatrix(object):
    def __init__(self,ngram_range=(1,1),tokenizer=stemtokenizer()):
        from sklearn.feature_extraction.text import tfidfvectorizer
        self.vectorizer = tfidfvectorizer(ngram_range=ngram_range,analyzer='word',lowercase=true,
                                          token_pattern='[a-za-z0-9]+',strip_accents='unicode',
                                          tokenizer=tokenizer)

    def load_ref_text(self,text_file):
        textfile = open(text_file,'r')
        lines = textfile.readlines()
        textfile.close()
        sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
        sentences = [item.strip().strip('.') for item in sent_tokenizer.tokenize(' '.join(lines).strip())]
        #vectorizer is transformed in this step
        chk2 = pd.dataframe(self.vectorizer.fit_transform(sentences1).toarray())
        return sentences, [chk2]

    def get_processed_data(self,data_loc):
        ref_sentences,ref_dataframes=self.load_ref_text(data_loc)
        loc = open(""indexeddata/vectorizer.obj"",""w"")
        pickle.dump(self.vectorizer,loc) #getting error here
        loc.close()
        return ref_sentences, ref_dataframes","['python', 'nlp', 'scikit-learn', 'pickle', 'text-mining']",30845049,"firstly, it's better to leave the import at the top of your code instead of within your class:
from sklearn.feature_extraction.text import tfidfvectorizer
class changetomatrix(object):
  def __init__(self,ngram_range=(1,1),tokenizer=stemtokenizer()):
    ...

next stemtokenizer don't seem to be a canonical class. possibly you've got it from  or maybe somewhere else so we'll assume it returns a list of strings.
class stemtokenizer(object):
    def __init__(self):
        self.ignore_set = {'footnote', 'nietzsche', 'plato', 'mr.'}

    def __call__(self, doc):
        words = []
        for word in word_tokenize(doc):
            word = word.lower()
            w = wn.morphy(word)
            if w and len(w) > 1 and w not in self.ignore_set:
                words.append(w)
        return words

now to answer your actual question, it's possible that you need to open a file in byte mode before dumping a pickle, i.e.:
>>> from sklearn.feature_extraction.text import tfidfvectorizer
>>> from nltk import word_tokenize
>>> import cpickle as pickle
>>> vectorizer = tfidfvectorizer(ngram_range=(0,2),analyzer='word',lowercase=true, token_pattern='[a-za-z0-9]+',strip_accents='unicode',tokenizer=word_tokenize)
>>> vectorizer
tfidfvectorizer(analyzer='word', binary=false, decode_error=u'strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=true, max_df=1.0, max_features=none, min_df=1,
        ngram_range=(0, 2), norm=u'l2', preprocessor=none, smooth_idf=true,
        stop_words=none, strip_accents='unicode', sublinear_tf=false,
        token_pattern='[a-za-z0-9]+',
        tokenizer=<function word_tokenize at 0x7f5ea68e88c0>, use_idf=true,
        vocabulary=none)
>>> with open('vectorizer.pk', 'wb') as fin:
...     pickle.dump(vectorizer, fin)
... 
>>> exit()
alvas@ubi:~$ ls -lah vectorizer.pk 
-rw-rw-r-- 1 alvas alvas 763 jun 15 14:18 vectorizer.pk

note: using the with idiom for i/o file access automatically closes the file once you get out of the with scope.
regarding the issue with snowballstemmer(), note that snowballstemmer('english') is an object while the stemming function is snowballstemmer('english').stem. 
important:

tfidfvectorizer's tokenizer parameter expects to take a string and return a list of string
but snowball stemmer does not take a string as input and return a list of string.

so you will need to do this:
>>> from nltk.stem import snowballstemmer
>>> from nltk import word_tokenize
>>> stemmer = snowballstemmer('english').stem
>>> def stem_tokenize(text):
...     return [stemmer(i) for i in word_tokenize(text)]
... 
>>> vectorizer = tfidfvectorizer(ngram_range=(0,2),analyzer='word',lowercase=true, token_pattern='[a-za-z0-9]+',strip_accents='unicode',tokenizer=stem_tokenize)
>>> with open('vectorizer.pk', 'wb') as fin:
...     pickle.dump(vectorizer, fin)
...
>>> exit()
alvas@ubi:~$ ls -lah vectorizer.pk 
-rw-rw-r-- 1 alvas alvas 758 jun 15 15:55 vectorizer.pk",https://stackoverflow.com/questions/30843011,python,15-06-2015 10:35,43978.0,28.0,2.0,True,17-04-2025 17:04,17-04-2025 17:04
74710732,what are the differences between adapter tuning and prefix tuning?,"i am trying to understand the concept of adapter-tuning, prompt-tuning, and prefix-tuning in the context of few-shot learning.
it appears to me that i can apply prompt tuning to a black box language model.
i read for prompt tuning the entire pre-trained language model is frozen. if that's the case prompt tuning could be applied for an openai model like gpt-3 and codex.
how could i do prompt tuning with openai codex? i don't find any way so far.
how these techniques are different than in-context example that could be given by few-shot learning.
can anyone please guide me in the correct direction?","['machine-learning', 'deep-learning', 'artificial-intelligence', 'fine-tuning', 'few-shot-learning']",74837058,"these are alternatives to fine-tuning model. they are essentially solutions that reside between few-shot learning and complete fine-tuning of models.
the other answer in this so post is completely wrong. fine-tuning has nothing to do with neither prompt tuning nor prefix tuning. these two are completely different techniques than fine-tuning.
correct reference to prompt tuning and prefix tuning are given below:

prompt tuning: for prompt tuning k learnable parameter i.e. continuous token embeddings is appended to the input. but the entire pre-trained language model is frozen.

prefix tuning: for k positions prepended to the input, concatenate additional learnable weights for keys and values at every attention layer. different to prompt tuning (only learnable input vectors).


papers that introduced these techniques are given below:

prompt tuning: 
prefix-tuning:",https://stackoverflow.com/questions/74710732,machine-learning,07-12-2022 01:16,2721.0,4.0,2.0,True,19-12-2022 18:17,16-12-2022 04:50
56639938,bert output not deterministic,"bert output is not deterministic.
i expect the output values are deterministic when i put a same input, but my bert model the values are changing. sounds awkwardly, the same value is returned twice, once. that is, once another value comes out, the same value comes out and it repeats.
how i can make the output deterministic?
let me show snippets of my code.
i use the model as below.
for the bert implementation, i use huggingface implemented bert pytorch implementation. which is quite fameous model ri implementation in the pytorch area. [link] 
        tokenizer = berttokenizer.from_pretrained(self.bert_type, do_lower_case=self.do_lower_case, cache_dir=self.bert_cache_path)
        pretrain_bert = bertmodel.from_pretrained(self.bert_type, cache_dir=self.bert_cache_path)
        bert_config = pretrain_bert.config

get the output like this
        all_encoder_layer, pooled_output = self.model_bert(all_input_ids, all_segment_ids, all_input_mask)

        # all_encoder_layer: bert outputs from all layers.
        # pooled_output: output of [cls] vec.


pooled_output
tensor([[-3.3997e-01,  2.6870e-01, -2.8109e-01, -2.0018e-01, -8.6849e-02,

tensor([[ 7.4340e-02, -3.4894e-03, -4.9583e-03,  6.0806e-02,  8.5685e-02,

tensor([[-3.3997e-01,  2.6870e-01, -2.8109e-01, -2.0018e-01, -8.6849e-02,

tensor([[ 7.4340e-02, -3.4894e-03, -4.9583e-03,  6.0806e-02,  8.5685e-02,

for the all encoder layer, the situation is same, - same in twice an once.
i extract word embedding feature from the bert, and the situation is same. 
wemb_n
tensor([[[ 0.1623,  0.4293,  0.1031,  ..., -0.0434, -0.5156, -1.0220],

tensor([[[ 0.0389,  0.5050,  0.1327,  ...,  0.3232,  0.2232, -0.5383],

tensor([[[ 0.1623,  0.4293,  0.1031,  ..., -0.0434, -0.5156, -1.0220],

tensor([[[ 0.0389,  0.5050,  0.1327,  ...,  0.3232,  0.2232, -0.5383],","['deep-learning', 'nlp', 'transformer-model', 'bert-language-model']",56646351,please try to set the seed. i faced the same issue and set the seed to make sure we get same values every time. one of the possible reasons could be dropout taking place in bert.,https://stackoverflow.com/questions/56639938,deep-learning,17-06-2019 23:17,3506.0,9.0,2.0,True,01-04-2023 07:58,25-06-2019 09:00
74576157,nlp stemming with javascript and php pages in the browser,"i'm trying to figure out, how to implement and use stemming results with javascript and php pages in the browser.
by using node index.js in the vs code terminal i got the output word using natural:
var natural = require(""natural"");
console.log(natural.porterstemmer.stem(""words"")); 

with nlp.js librarie:
const { stemmerit } = require(""@nlpjs/lang-it"");
const stemmer = new stemmerit();
const input = [""ho"", ""visto"", ""uno"", ""sviluppatore""];
console.log(stemmer.stem(input));

they are node.js libraries, and i got an error in the chrome browser console log using require:

uncaught referenceerror: require is not defined
at index.js:320:15

i'm not sure if there is some way to use libraries with the browser, like for example brain.js providing npm and cnd.
i did not try to use compromise.cool library yet, which can be used to run nlp on the browser, looking for a stemming method.
i've tried porters stemming algorithm javascript but can't figure out, how to use it as a function. seems like i'm doing it incorrectly:
var result = stemmer(""words"");
console.log(result);


uncaught typeerror: stemmer is not a function
at index.js:346:14

also, i've tried to follow natural language processing in the browser guide, separately installing each dependency listed in package.json:
""@nlpjs/core"": ""^4.14.0"",
""@nlpjs/lang-en-min"": ""^4.14.0"",
""@nlpjs/nlp"": ""^4.15.0"",
""@nlpjs/request-rn"": ""^4.14.3"",
""browserify"": ""^17.0.0"",
""terser"": ""^5.3.8""

but npm run build throws the error:

10 error missing script: build

any advice, guide, or example would be useful.","['javascript', 'node.js', 'browser', 'nlp', 'stemming']",74580029,for browser side nlp & stemming look at the example of stemming and lemmatization. this observable notebook and the associated collection contain details of making pure browser side nlp apps.,https://stackoverflow.com/questions/74576157,javascript,25-11-2022 17:23,764.0,0.0,1.0,True,26-11-2022 05:48,25-11-2022 17:52
73467509,how do i use pytorch models in deep java library(djl)?,"i would like to run easynmt in java.
however i don't know how to load and run the model.
i loaded the model as follows:
uri uri = new uri(""file:////users/.../prior.pth"");
path modeldir = paths.get(uri);
model model = model.newinstance(""model.pth"", device.cpu(), ""pytorch"");
model.load(modeldir);

however, i do not know what to do after this.
easynmt performs the following:
model.translate(""dies ist ein satz in deutsch."", target_lang='en', max_new_tokens=1000)

how does djl perform translations?","['python', 'java', 'nlp', 'pytorch', 'djl']",73481119,"you need create your own translator to do pre-processing and post-processing. you can find this jupyter notebook that explains how translator works in djl.
for nmt model, you can find this example in djl:",https://stackoverflow.com/questions/73467509,python,24-08-2022 04:41,1792.0,1.0,1.0,True,25-08-2022 01:57,24-08-2022 06:32
69780823,tokenizers change vocabulary entry,"i have some text which i want to perform nlp on. to do so, i download a pre-trained tokenizer like so:
import transformers as ts

pr_tokenizer = ts.autotokenizer.from_pretrained('distilbert-base-uncased', cache_dir='tmp')

then i create my own tokenizer with my data like this:
from tokenizers import tokenizer
from tokenizers.models import bpe
tokenizer = tokenizer(bpe(unk_token=""[unk]""))

from tokenizers.trainers import bpetrainer
trainer = bpetrainer(special_tokens=[""[unk]"", ""[cls]"", ""[sep]"", ""[pad]"", ""[mask]""])

from tokenizers.pre_tokenizers import whitespace
tokenizer.pre_tokenizer = whitespace()

tokenizer.train(['transcripts.raw'], trainer)

now comes the part where i get confused... i need to update the entries in the pretraned tokenizer (pr_tokenizer) where they are the keys are the same as in my tokenizer (tokenizer). i have tried several methods, so here is one of them:
new_vocab = pr_tokenizer.vocab
v = tokenizer.get_vocab()

for i in v:
    if i in new_vocab:
        new_vocab[i] = v[i]

so what do i do now? i was thinking something like:
pr_tokenizer.vocab.update(new_vocab)

or
pr_tokenizer.vocab = new_vocab

neither work. does anyone know a good way of doing this?","['python', 'python-3.x', 'nlp', 'huggingface-transformers', 'huggingface-tokenizers']",69809122,"to do that, you can just download the tokenizer source from github or the huggingface website into the same folder as your code, and then edit the vocabulary before the tokenizer is loaded:
new_vocab = {}

# getting the vocabulary entries
for i, row in enumerate(open('./distilbert-base-uncased/vocab.txt', 'r')): 
    new_vocab[row[:-1]] = i

# your vocabulary entries
v = tokenizer.get_vocab()

# replace common (your code)
for i in v:
    if i in new_vocab:
        new_vocab[i] = v[i]

with open('./distilbert-base-uncased/vocabb.txt', 'w') as f:
    # reversed vocabulary
    rev_vocab = {j:i for i,j in zip(new_vocab.keys(), new_vocab.values())}
    # adding vocabulary entries to file
    for i in range(len(rev_vocab)):
        if i not in rev_vocab: continue
        f.write(rev_vocab[i] + '\n')

# loading the new tokenizer
pr_tokenizer = ts.autotokenizer.from_pretrained('./distilbert-base-uncased')",https://stackoverflow.com/questions/69780823,python,30-10-2021 18:02,4064.0,2.0,2.0,True,02-11-2021 10:48,30-10-2021 18:30
79088393,how to add eos when training t5?,"i'm a little puzzled where (and if) eos tokens are being added when using huggignface's trainer classes to train a t5 (longt5 actually) model.
the data set contains pairs of text like this:



from
to




some text
some corresponding text


some other text
some other corresponding text



the tokenizer has been custom trained:
tokenizer = sentencepieceunigramtokenizer()
tokenizer.train_from_iterator(iterator=iterator, vocab_size=32_128, show_progress=true, unk_token=""<unk>"")

and is loaded like this:
tokenizer = t5tokenizerfast(tokenizer_file=""data-rb-25000/tokenizer.json"",  
                            padding=true, bos_token=""<s>"", 
                            eos_token=""</s>"",unk_token=""<unk>"", 
                            pad_token=""<pad>"")

before training, the data set is tokenized and examples that have a too high token count are filtered out, like so:
max_sequence_length = 16_384 / 2

def preprocess_function(examples):
    inputs = tokenizer(
        examples['from'],
        truncation=false,  # don't truncate yet
        padding=false,     # don't pad yet
        return_length=true,
    )
    labels = tokenizer(
        examples['to'],
        truncation=false,
        padding=false,
        return_length=true,
    )

    inputs[""input_length""] = inputs[""length""]
    inputs[""labels""] = labels[""input_ids""]
    inputs[""label_length""] = labels[""length""]

    inputs.pop(""length"", none)

    return inputs

tokenized_data = dataset.map(preprocess_function, batched=true, remove_columns=dataset[""train""].column_names)

def filter_function(example):
    return example['input_length'] <= max_sequence_length and example['label_length'] <= max_sequence_length

filtered_data = tokenized_data.filter(filter_function)

training is done like this:
from transformers import datacollatorforseq2seq

data_collator = datacollatorforseq2seq(tokenizer=tokenizer, model=""google/long-t5-tglobal-base"")

from transformers import automodelforseq2seqlm, autoconfig

config = autoconfig.from_pretrained(
    ""google/long-t5-tglobal-base"",
    vocab_size=len(tokenizer),
    pad_token_id=tokenizer.pad_token_id,
    eos_token_id=tokenizer.eos_token_id,
    decoder_start_token_id=tokenizer.pad_token_id,
)

model = automodelforseq2seqlm.from_config(config)

from transformers import generationconfig

generation_config = generationconfig.from_model_config(model.config)
generation_config._from_model_config = false
generation_config.max_new_tokens = 16_384

from transformers import seq2seqtrainer, seq2seqtrainingarguments

training_args = seq2seqtrainingarguments(
    output_dir=""rb-25000-model"",
    eval_strategy=""epoch"",
    save_strategy=""epoch"",
    learning_rate=2e-5,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=16,
    gradient_checkpointing=true,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=5,
    logging_steps=1,
    predict_with_generate=true,
    load_best_model_at_end=true,
    bf16=true,
)

trainer = seq2seqtrainer(
    model=model,
    args=training_args,
    train_dataset=filtered_data[""train""],
    eval_dataset=filtered_data[""test""],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    generation_config=generation_config,
)

trainer.train()

i know that the tokenizer doesn't add the eos token:
inputs = tokenizer(['hello world', 'hello'], padding=true, truncation=true, max_length=100, return_tensors=""pt"")
labels = inputs[""input_ids""]

print(labels)
print(tokenizer.convert_tokens_to_ids(['<s>'])[0])
print(tokenizer.convert_tokens_to_ids(['<pad>'])[0])
print(tokenizer.convert_tokens_to_ids(['<unk>'])[0])
print(tokenizer.convert_tokens_to_ids(['</s>'])[0])

print(tokenizer.convert_ids_to_tokens([1]))

output:
tensor([[1, 10356, 1, 5056],
        [1, 10356, 16002, 16002]])
16000
16002
0
16001
['ï¿½ï¿½ï¿½']

(i don't really understand what's that strange token with index 1.
anyway, i was wondering if the trainer class or the datacollator actually adds the eos. i did not find any examples online of how and where to add eos.
i suspect it's not there, because after training the model it doesn't stop generating until it reaches max_new_tokens (set to pretty high).
what's the best practice here? where should i add eos? is there anything else about this code that should be checked or that looks weird for more experienced eyes","['machine-learning', 'huggingface-transformers', 'huggingface', 'huggingface-tokenizers', 'huggingface-trainer']",79101756,"the t5 tokenizer should end sequences by eos token by default. pretrained t5 tokenizer on huggingface does that by default. in fact, i found the function that is responsible for that in the source code on line 256:
def _add_eos_if_not_present(self, token_ids: list[int]) -> list[int]:
        """"""do not add eos again if user already added it.""""""
        if len(token_ids) > 0 and token_ids[-1] == self.eos_token_id:
            warnings.warn(
                f""this sequence already has {self.eos_token}. in future versions this behavior may lead to duplicated""
                "" eos tokens being added.""
            )
            return token_ids
        else:
            return token_ids + [self.eos_token_id]

if eos token is not appended by default, you can add a post processor to your tokenizer using templateprocessing:
from tokenizers.processors import templateprocessing

tokenizer._tokenizer.post_processor = templateprocessing(
    single=""$a </s>"",
    pair=""$a </s> $b </s>"",
    special_tokens=[(""</s>"", tokenizer.eos_token_id)]
)

inputs = tokenizer(['hello world', 'hello'], padding=true, truncation=true, max_length=100, return_tensors=""pt"")
labels = inputs[""input_ids""]
print(labels)

this should give:
tensor([[1, 10356, 1, 5056, 16001],
        [1, 10356, 16001, 16002, 16002]])",https://stackoverflow.com/questions/79088393,machine-learning,15-10-2024 04:22,235.0,1.0,1.0,True,21-10-2024 12:57,15-10-2024 08:57
64057803,assertion failed: [condition x == y did not hold element-wise:],"i have built a bilstm model with an attention layer for sentence classification task but i am getting an error that my assertion has failed due to mismatch in number of parameters. the attention layer code is here and the error is below the code.
class attention(layer):
    
    def __init__(self, return_sequences=true):
        self.return_sequences = return_sequences
        super(attention,self).__init__()
        
    def build(self, input_shape):
        
        self.w=self.add_weight(name=""att_weight"", shape=(input_shape[-1],1),
                               initializer=""normal"")
        self.b=self.add_weight(name=""att_bias"", shape=(input_shape[1],1),
                               initializer=""zeros"")
        
        super(attention,self).build(input_shape)
        
    def call(self, x):
        
        e = k.tanh(k.dot(x,self.w)+self.b)
        a = k.softmax(e, axis=1)
        output = x*a
        
        if self.return_sequences:
            return output
        
        return k.sum(output, axis=1)

when i am training the model with attention layer included, it is giving an error that assertion failed.
epoch 1/10
---------------------------------------------------------------------------
invalidargumenterror                      traceback (most recent call last)
<ipython-input-45-ac310033130c> in <module>()
      1 #early stopping, adam, dropout = 0.3, 0.5, 0.5
      2 #history = model.fit(sequences_matrix, y_train, batch_size=256, epochs=5, validation_split=0.1,  callbacks=[earlystopping(monitor='val_loss', min_delta=0.0001)])
----> 3 history = model.fit(sequences_matrix, y_train, batch_size=32, epochs=10, validation_split=0.1)

8 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     ctx.ensure_initialized()
     59     tensors = pywrap_tfe.tfe_py_execute(ctx._handle, device_name, op_name,
---> 60                                         inputs, attrs, num_outputs)
     61   except core._notokstatusexception as e:
     62     if name is not none:

invalidargumenterror:  assertion failed: [condition x == y did not hold element-wise:] [x (sparse_categorical_crossentropy/sparsesoftmaxcrossentropywithlogits/shape_1:0) = ] [32 1] [y (sparse_categorical_crossentropy/sparsesoftmaxcrossentropywithlogits/strided_slice:0) = ] [32 758]
     [[node sparse_categorical_crossentropy/sparsesoftmaxcrossentropywithlogits/assert_equal_1/assert/assert (defined at <ipython-input-45-ac310033130c>:3) ]] [op:__inference_train_function_19854]

function call stack:
train_function

my model is
model = sequential()
model.add(embedding(max_words, 768, input_length=max_len, weights=[embedding]))
model.add(batchnormalization())
model.add(activation('tanh'))
model.add(spatialdropout1d(0.1))
model.add(conv1d(16, kernel_size=11, activation='relu'))
model.add(bidirectional(lstm(16, return_sequences=true)))
model.add(attention(return_sequences=true))
model.add(batchnormalization())
model.add(activation('tanh'))
model.add(dropout(0.2))
model.add(dense(2, activation='softmax', use_bias=true, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4), bias_regularizer=regularizers.l2(1e-4),
    activity_regularizer=regularizers.l2(1e-5)))
model.summary()

shape of y_train is
max_words = 48369
max_len = 768
tok = tokenizer(num_words = max_words)
tok.fit_on_texts(x_train)
sequences = tok.texts_to_sequences(x_train)
sequences_matrix = sequence.pad_sequences(sequences, maxlen = max_len)
y_train = np.array(y_train)
y_test = np.array(y_test)

print(y_train.shape)

(43532, 1)","['python-3.x', 'tensorflow', 'keras', 'nlp', 'attention-model']",64061835,your target is in 2d so you need to set return_sequences=false in the last attention layer in order to return output in 2d format,https://stackoverflow.com/questions/64057803,python-3.x,25-09-2020 04:26,3477.0,2.0,2.0,True,06-05-2022 07:39,25-09-2020 09:24
76164749,"use python, autogpt and chatgpt to extract data from downloaded html page","note: if you're downvoting at least share why. i put in a lot of effort to write this question, shared my code and did my own research first, so not sure what else i could add.
i already use scrapy to crawl websites successfully. i extract specific data from a webpage using css selectors. however, it's time consuming to setup and error prone.
i want to be able to pass the raw html to chatgpt and ask a question like

""give me in a json object format the price, array of photos, description, key features, street address, and zipcode of the object""

desired output below.
i truncated description, key features and photos for legibility.
{
""price"":""$945,000"",
""photos"":""
""description"":""<div>this spacious 2 bedroom 1 bath home easily converts to 3 bedrooms. featuring a bright and quiet southern exposure, the expansive great room (with 9ft ceilings) is what sets (...)"",
""key features"":""center island;central air;dining in living room;dishwasher"",
""street address"":""170 west 89th street, 2d"",
""zipcode"":""ny 10024"",
}

right now i run into the max chat length of 4096 characters. so i decided to send the page in chunks. however even with a simple question like ""what is the price of this object?"" i'd expect the answer to be ""$945,000"" but i'm just getting a whole bunch of text.
i'm wondering what i'm doing wrong. i heard that autogpt offers a new layer of flexibility so was also wondering if that could be a solution here.
my code:
import requests
from bs4 import beautifulsoup, comment
import openai
import json

# set up your openai api key
openai.api_key = ""mykey""

# fetch the html from the page
url = ""
response = requests.get(url)

# parse and clean the html
soup = beautifulsoup(response.text, ""html.parser"")

# remove unnecessary tags, comments, and scripts
for script in soup([""script"", ""style""]):
    script.extract()

# for comment in soup.find_all(text=lambda text: isinstance(text, comment)):
#     comment.extract()

text = soup.get_text(strip=true)

# divide the cleaned text into chunks of 4096 characters
def chunk_text(text, chunk_size=4096):
    chunks = []
    for i in range(0, len(text), chunk_size):
        chunks.append(text[i:i+chunk_size])
    return chunks

print(text)

text_chunks = chunk_text(text)

# send text chunks to chatgpt api and ask for the price
def get_price_from_gpt(text_chunks, question):
    for chunk in text_chunks:
        prompt = f""{question}\n\n{chunk}""
        response = openai.completion.create(
            engine=""text-davinci-002"",
            prompt=prompt,
            max_tokens=50,
            n=1,
            stop=none,
            temperature=0.5,
        )

        answer = response.choices[0].text.strip()
        if answer.lower() != ""unknown"" and len(answer) > 0:
            return answer

    return ""price not found""

question = ""what is the price of this object?""
price = get_price_from_gpt(text_chunks, question)
print(price)","['python', 'openai-api', 'chatgpt-api', 'autogpt']",76546876,"updated answer 06.28.2023

your question was very interesting, so i wanted to try to improve my previous answer that you have already accepted.
i noted that my previous answer cost around .05 cents to query the openai api.  these costs was directly related to the text chunking function and asking the questions in a for loop. i have removed the text chunking function and the for loop because i was able to reduced the tokens to a condensed size.
one of the core items that was required to reduce the cost is text cleaning, which is a standard nlp and data science problem.  i added some more code to remove additional unneeded text from the soup object. there is a performance hit when doing this, but not enough to lose sleep over.
refining the query prompt was also needed to submit everything in a single request. doing this reduces the query costs.
the code below can be refined more. currently it cost .02 cents per query using text-davinci-003.  the prompt will need to reworked to use text-davinci-002, which is a little cheaper than text-davinci-003.
the api query time for the code below can exceed 15 seconds.  there are numerous discussions on the community forums at openai about query performance. from my research there is no solid technique on how to improve query performance.
import json
import spacy
import openai
import requests
import re as regex
from bs4 import beautifulsoup

openai.api_key = 'my_key'


# this code can be refined. 
def remove_unwanted_tags(soup):
    for disclaimer_tag in soup.select('div[class*=""disclaimer__textcontainer""]'):
        disclaimer_tag.decompose()

    for footer_tag in soup.select('div[class*=""globalfooter__""]'):
        footer_tag.decompose()

    for hidden_tag in soup.select('p[class*=""visually-hidden""]'):
        hidden_tag.decompose()

    for agent_contact_card_tag in soup.select('div[class*=""agentcontactcard__""]'):
        agent_contact_card_tag.decompose()

    for listing_agent_tag in soup.select('div[class*=""detailsandagents__listingfirmname""]'):
        listing_agent_tag.decompose()

    for property_sale_history_tag in soup.select('section[class*=""saleshistory__""]'):
        property_sale_history_tag.decompose()

    for data in soup(['style', 'script', 'iframe', 'footer', 'h2', 'a']):
        data.decompose()

    dirty_soup = ' '.join(soup.find_all(string=true))
    remove_allcaps_words = regex.sub(r'\b[a-z]+\b', '', dirty_soup)
    clean_soup = regex.sub(""\s\s+"", "" "", str(remove_allcaps_words))
    return  clean_soup


def tokenize_text(text):
    nlp = spacy.load(""en_core_web_sm"")
    sentences = nlp(text)
    return [sentence.text for sentence in sentences.sents]


def get_property_details_from_gpt(text, question):
        prompt = f""{question}\n\n{text}""
        response = openai.completion.create(
            engine=""text-davinci-003"",
            prompt=prompt,
            max_tokens=200,
            n=1,
            stop=none,
            temperature=0.7,
            frequency_penalty=0,
            presence_penalty=0.6
        )

        answer = response.choices[0].text.strip()
        if answer.lower() != ""unknown"" and len(answer) > 0:
            return answer

url = ""
response = requests.get(url)
soup = beautifulsoup(response.content, ""html.parser"")
property_photos = [element.find('img').attrs['src'] for element in soup.select('div[class*=""carousel-item""]')]

strained_soup = remove_unwanted_tags(soup)
text = u''.join(tokenize_text(strained_soup))

question = """"""please extract the following details from the provided items:
{""price"": ""(1) exact selling price of this property with dollar sign and thousands formatting."",
""street_address"": ""(2) exact street address of this property without state or zip code."",
""description"": ""(3) short description of this property."",
""key_features"": ""(4) key features of this property. provide these items in a semicolon delimiter string."",
""state"": ""(5) state abbreviation for this property."",
""zipcode"": ""(6) zip code for this property, if available."",
""photos"": """"},
(8) provide this data back in a python dictionary that can be processed by json.loads""""""

query_results = get_property_details_from_gpt(text, question)
query_data = query_results.replace('answer:', '')
corcoran_properties = json.loads(query_data)
corcoran_properties[""photos""] = f""{property_photos}""
corcoran_json = json.dumps(corcoran_properties, indent=4)
print(corcoran_json)


this is the output:
{
    ""price"": ""$945,000"",
    ""street_address"": ""170 west 89th street"",
    ""description"": ""this spacious 2 bedroom 1 bath home easily converts to 3 bedrooms. featuring a and quiet southern exposure, the expansive great room (with 9ft ceilings) is what sets this home apart from others. paired with a renovated open kitchen, new bathroom, and washer/dryer, this is an upper west side gem."",
    ""key_features"": ""center island; central air; dining in living room; dishwasher; en suite; excellent light light; hardwood floors; high ceilings; modern kitchen; new windows; open kitchen; pet friendly; prewar detail; storage space; washer/dryer; window /"",
    ""state"": ""ny"",
    ""zipcode"": ""10024"",
    ""photos"": ""[' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '
}


updated answer 06.26.2023

i'm trying to refine this answer.  i decided to clean the data slightly before sending it the api.  doing this allowed me to get some cleaner answers. i removed the code from my previous answer, but i left my notes, which i consider important to anyone trying to do something similar.
i found that text-davinci-003 gives more precise answers to the questions than text-davinci-002, but it costs more to use text-davinci-003.
updated code:
import json
import spacy
import openai
import requests
import re as regex
from bs4 import beautifulsoup

#####################################################
# the nlp package en_core_web_sm has to be download 
# once to your environment. use the following command 
# to accomplish this. 
# 
# spacy.cli.download(""en_core_web_sm"")
#
######################################################

openai.api_key = 'my_key'

# i needed to deep clean the soup by removing some excess text. doing this allowed for better responses for openai. 
def remove_unwanted_tags(soup):
    for disclaimer_tag in soup.select('div[class*=""disclaimer__text""]'):
        disclaimer_tag.decompose()

    for footer_tag in soup.select('div[class*=""globalfooter__footer""]'):
        footer_tag.decompose()

    for hidden_tag in soup.select('p[class*=""visually-hidden""]'):
        hidden_tag.decompose()

    for data in soup(['style', 'script', 'iframe']):
        data.decompose()

    dirty_soup = ' '.join(soup.find_all(string=true))
    
    # i removed all the upper case words in the soup, 
    # because they provided no value. 
    clean_soup = regex.sub(r'\b[a-z]+\b', '', dirty_soup)
    return clean_soup

def tokenize_text(text):
    nlp = spacy.load(""en_core_web_sm"")
    sentences = nlp(text)
    return [sentence.text for sentence in sentences.sents]

def get_text_chunks(text, max_tokens_per_chunk=3000):
    chunks = []
    current_chunk = []
    current_token_count = 0
    sentences = tokenize_text(text)
    for sentence in sentences:
        current_chunk.append(sentence)
        current_token_count += len(sentence.split("" ""))

        if current_token_count >= max_tokens_per_chunk:
            chunks.append(current_chunk)
            current_chunk = []
            current_token_count = 0

    if current_chunk:
        chunks.append(current_chunk)
    return chunks

def get_property_details_from_gpt(text_chunks, question):
    for chunk in text_chunks:
        prompt = f""{question}\n\n{chunk}""
        response = openai.completion.create(
            engine=""text-davinci-003"",
            prompt=prompt,
            max_tokens=400,
            n=1,
            stop=none,
            temperature=0.5,
        )

        answer = response.choices[0].text.strip()
        if answer.lower() != ""unknown"" and len(answer) > 0:
            return answer


url = ""
response = requests.get(url)
soup = beautifulsoup(response.content, ""html.parser"")

questions = {'price': 'provide only the price the for this property',
             'description': 'provide a short description about this property?\nonly provide complete '
                            'sentences. please correct the spelling for each word provide.',
             'key features':'what are the key features of this property?\nprovide these items in a semicolon delimiter string.',
             'street address': 'what is the exact street address of this propery?\nonly provide the street address '
                               'and nothing additional.',
             'zipcode': 'what is the state abbreviation and zip code for this property?\nonly provide the state abbreviation and zipcode.'}

corcoran_properties = {}

strained_soup = remove_unwanted_tags(soup)
text_chunks = get_text_chunks(strained_soup)
for json_key, question in questions.items():
    question_response = get_property_details_from_gpt(text_chunks, question)
    corcoran_properties[json_key] = question_response

property_photos = [element.find('img').attrs['src'] for element in soup.select('div[class*=""carousel-item""]')]
corcoran_properties['photos'] = property_photos

corcoran_json = json.dumps(corcoran_properties, indent=4)
print(corcoran_json)



this was the output from the code above:
{
    ""price"": ""$945,000"",
    ""description"": ""this property is a 2 bedroom, 1 bathroom co-op located at 170 west 89th street in the upper west side of manhattan, new york. built in 1910, this spacious home features 9ft ceilings, a renovated open kitchen, new bathroom, and washer/dryer. building amenities include a storage unit, stroller parking and bicycle storage. it has excellent natural light and southern exposure. the neighborhood is full of iconic architecture, cultural institutions, and historical sites. it is close to central park and riverside park."",
    ""key features"": ""9ft ceilings; 2 beds; 1 bath; southern exposure; renovated open kitchen; new bathroom; washer/dryer; storage unit; convenient stroller parking; bicycle storage; center island; central air; dining in living room; dishwasher; en suite; excellent light; hardwood floors; high ceilings; modern kitchen; new windows; open kitchen; pet friendly; prewar detail; storage space; washer/dryer; window/listing agent; iconic architecture; city-defining structures; cultural institutions; historical sites; central park; riverside park."",
    ""street address"": ""170 west 89th street"",
    ""zipcode"": ""ny, 10024"",
    ""photos"": [
        ""
        ""
        ""
        ""
        ""
        ""
        ""
        ""
        ""
        ""
        ""
        ""
        ""
        ""
        ""
        ""
        ""
        ""
        ""
        ""
        ""
        ""
        ""
    ]
}

original answer 06.24.2023 (code removed for readability)

i noted that one of the core issues in your code was in this line:

text = soup.get_text(strip=true)

this line was removing some of the needed spaces that openai chatgpt needed for processing.

built in 19102 beds1 bath$945,000maintenance/common charges:$1,68010%

doing this allows for the spaces:

text = u' '.join(soup.find_all(string=true))
built in 1910     2 beds     1 bath $945,000 maintenance/common charges:

also the api for openai deals with tokens and not characters, so your chunking code needs to be replace with one that handles tokenization.
i'm unsure of the scalability of this answer, because you will definitely need to think through all the applicable questions related to your data source.
for instance:
what is the address of this property? the address of this property is 170 west 89th street #2d, new york, ny 10024.
what year was the property built? the property was built in 1910.
what are the maintenance fees for this property? the maintenance fees for this property are $1,680.
** are there any property amenities?** there are a few property amenities, including a storage unit, stroller parking, and bicycle storage.
one of the core issues with using openai api is extracting a clear description from the text provided.
this line text = ' '.join(soup.find_all(string=true)) produces this text:
170 west 89th street #2d, new york, ny 10024 property for sale skip to main content sign in agent sign in preferences open visitors preferences modal buy rent sell new developments commercial search all commercial wexler healthcare properties agents search local agents browse all agents become an agent offices search local offices search all offices about us about corcoran corcoran leadership corcoran brand browse affiliates become an affiliate explore market reports neighborhood guides inhabit blog media coverage exclusive rental buildings save share contact buy search in contract web id:    22053660 170 west 89th street, 2d   upper west side,  manhattan, ny 10024 upper west side,  manhattan, ny 10024 in contract |    co-op |    built in 1910     2 beds     1 bath $945,000 maintenance/common charges:     $1,680 10   % down:     $94,500 available    immediately   this is a carousel. use next and previous buttons to navigate. click on image or ""expand"" button to open the fullscreen carousel.
     not all information is available from these images. previous next floorplan map buy search in contract web id:    22053660 170 west 89th street, 2d   upper west side,  manhattan, ny 10024 upper west side,  manhattan, ny 10024 in contract |    co-op |    built in 1910     2 beds     1 bath $945,000 maintenance/common charges:     $1,680 10   % down:     $94,500 available    immediately   the details about  170 west 89th street, 2d, upper west side, manhattan, ny 10024 columbus avenue and amsterdam avenue this spacious 2 bedroom 1 bath home easily converts to 3 bedrooms. featuring a bright and quiet southern exposure, the expansive great room (with 9ft ceilings) is what sets this home apart from others. paired with a renovated open kitchen, new bathroom, and washer/dryer, this is an upper west side gem. building amenities include a storage unit ($25/month), convenient stroller parking and bicycle storage. in... see more listing courtesy of    corcoran   , stuart    moss   ,  (212) 821-9140 , rls data display by corcoran group key features   interior center island central air dining in living room dishwasher en suite excellent light light hardwood floors high ceilings modern kitchen new windows open kitchen pet friendly prewar detail storage space washer/dryer window a/c listing agent stuart moss licensed associate real estate broker business   :  (212) 821-9140 mobile   :  (646) 642-0603 contact me agent-email=""sim@corcoran.com"" upper west side ever wonder why an incalculable number of creative works are set somewhere between 59th and 110th streets, within central park west and the hudson river? all new york city neighborhoods are created equal, but thereï¿½ï¿½ï¿½s just something about the upper west side. honestly, itï¿½ï¿½ï¿½s all in the details: iconic architecture, city-defining structures like the dakota, the san remo, and the el dorado. cultural institutions and historical sites of immense international renown line the streets and avenues. having two beloved greenspaces ï¿½ï¿½ï¿½ central park and riverside park ï¿½ï¿½ï¿½ at its horizontal edges certainly doesnï¿½ï¿½ï¿½t hurt the reputation either. all of it and more is why so many new yorkers choose to call the uws home. itï¿½ï¿½ï¿½s also why, at times, this neighborhood cet as it does a physical place. about the building about the building 170 west 89th street apartment building    in  upper west side columbus avenue and amsterdam avenue 1       units 5       stories 1910       built view building details building sales history for 170 west 89th street for 170 west 89th street for 170 west 89th street, 2d sales history for  170 west 89th street date unit price approx. sq. ft. beds baths 08/03/2021 3a $985,000 0 2 1 02/28/2020 1b $972,000 1000 3 1 02/19/2020 5a $800,000 0 2 1 see 3 more rows sales history for  170 west 89th street, 2d date price listing status 08/12/2016 $925,000 sold 02/09/2012 $652,500 sold 08/17/2006 $606,000 sold all information furnished regarding property for sale, rental or financing is from sources deemed reliable, but no warranty or representation is made as to the accuracy thereof and same is submitted subject to errors, omissions, change of price, rental or other conditions, prior sale, lease or financing or withdrawal without notice. all dimensions are approximate. for exact dimensions, you must hire your own architect or engineer. images may be digitally enhanced photos, virtually staged photos, artists' renderings of future conditions, or otherwise modified, and therefore may not necessarily reflect actual site conditions. accordingly, interested parties must confirm actual site conditions for themselves, in person. relocation careers healthcare real estate real estate agents homes for sale homes for rent copyright ï¿½ï¿½    2023    the corcoran group. all rights reserved. terms & conditions privacy notice fair housing policy issues dmca notice accessibility statement licensing sitemap do not sell or share my personal information 590 madison avenue new york, ny 10022  |  800.544.4055  |  212.355.3550  |  fax: 212.223.6381  |  info@corcoran.com corcoran and the corcoran logos are trademarks of corcoran group llc. the corcoranï¿½ï¿½ system is comprised of company owned offices which are owned by a subsidiary of anywhere real estate inc. and franchices which are independently owned and operated. the corcoran system fully supports the principles of the fair housing act and the equal opportunity act. listing information is deemed reliable, but is not guaranteed. licensed in the state of california as ca dre# 02109201

when processing you might get this:

""description"": ""this property is a 2 bedroom, 1 bathroom co-op in upper west side, manhattan. it features a bright and quiet southern exposure, an expansive great room, and a renovated open kitchen."",

or this:

""description"": ""this is a carousel. use next and previous buttons to navigate. click on image or ""expand"" button to open the fullscreen carousel.\n\nthis is a 2 bedroom, 1 bath carousel that easily converts to 3 bedrooms. it has a bright and quiet southern exposure, an expansive great room with 9ft ceilings, a renovated open kitchen, new bathroom, and washer/dryer. building amenities include a storage unit ($25/month), convenient stroller parking and bicycle storage."",

getting a clear and concise description will require lots of testing. it might require you to do something like this details = ''.join([element.text for element in soup.select('div[class*=""detailsandagents""]')])
doing this also creates an issue with obtaining a clear description.

""description"": ""errace gardens is a historic apartment building in the st. louis place neighborhood of st. louis, missouri. the building was constructed in 1892 and was added to the national register of historic places in 2002. the building is four stories tall and is constructed of brick and limestone. the building features a terracotta cornice and a projecting bay window. the building has a u-shaped floor plan and contains 24 apartments."",

or this:

""description"": ""otally renovated 3 bedroom, 2.5 bath home with an attached 2 car garage. this home has a brand new kitchen with granite countertops and stainless steel appliances. the bathrooms have also been updated with new vanities and fixtures. there is new flooring and fresh paint throughout the home. the home is located on a cul-de-sac and has a large, fenced in backyard."",",https://stackoverflow.com/questions/76164749,python,03-05-2023 13:59,3847.0,1.0,1.0,True,30-06-2023 18:01,25-06-2023 09:34
69626196,train hugging face automodel defined using autoconfig,"i have defined the configration for a model in transformers. later, i have used this configration to initialise the classifier as follows
from transformers import autoconfig, automodel

config = autoconfig.from_pretrained('bert-base-uncased')
classifier = automodel.from_config(config)

i have check the list of functions available for this class which are
>>> dir(classifier)

>>>
['add_memory_hooks',
 'add_module',
 'adjust_logits_during_generation',
 'apply',
 'base_model',
 'base_model_prefix',
 'beam_sample',
 'beam_search',
 'bfloat16',
 'buffers',
 'children',
 'config',
 'config_class',
 'cpu',
 'cuda',
 'device',
 'double',
 'dtype',
 'dummy_inputs',
 'dump_patches',
 'embeddings',
 'encoder',
 'estimate_tokens',
 'eval',
 'extra_repr',
 'float',
 'floating_point_ops',
 'forward',
 'from_pretrained',
 'generate',
 'get_buffer',
 'get_extended_attention_mask',
 'get_head_mask',
 'get_input_embeddings',
 'get_output_embeddings',
 'get_parameter',
 'get_position_embeddings',
 'get_submodule',
 'gradient_checkpointing_disable',
 'gradient_checkpointing_enable',
 'greedy_search',
 'group_beam_search',
 'half',
 'init_weights',
 'invert_attention_mask',
 'is_parallelizable',
 'load_state_dict',
 'load_tf_weights',
 'modules',
 'name_or_path',
 'named_buffers',
 'named_children',
 'named_modules',
 'named_parameters',
 'num_parameters',
 'parameters',
 'pooler',
 'prepare_inputs_for_generation',
 'prune_heads',
 'push_to_hub',
 'register_backward_hook',
 'register_buffer',
 'register_forward_hook',
 'register_forward_pre_hook',
 'register_full_backward_hook',
 'register_parameter',
 'requires_grad_',
 'reset_memory_hooks_state',
 'resize_position_embeddings',
 'resize_token_embeddings',
 'retrieve_modules_from_names',
 'sample',
 'save_pretrained',
 'set_input_embeddings',
 'share_memory',
 'state_dict',
 'supports_gradient_checkpointing',
 'tie_weights',
 'to',
 'to_empty',
 'train',
 'training',
 'type',
 'xpu',
 'zero_grad']

out of this only train method seemed relevant. however, upon checking the doc string for the function, i got
>>> print(classifier.train.__doc__)
>>> sets the module in training mode.

        this has any effect only on certain modules. see documentations of
        particular modules for details of their behaviors in training/evaluation
        mode, if they are affected, e.g. :class:`dropout`, :class:`batchnorm`,
        etc.

        args:
            mode (bool): whether to set training mode (``true``) or evaluation
                         mode (``false``). default: ``true``.

        returns:
            module: self

how do i train this classifier on custom dataset (preferably in the transformers or in tensorflow)?","['deep-learning', 'nlp', 'tensorflow2.0', 'huggingface-transformers']",69626761,"tfautomodel was needed in the above code.
from transformers import autoconfig, tfautomodel

config = autoconfig.from_pretrained('bert-base-uncased')

model = tfautomodel.from_config(config)

model.compile(
    loss=tf.keras.losses.binarycrossentropy(from_logits=true),
    optimizer=tf.keras.optimizers.rmsprop(),
    metrics=[""accuracy""],
)

then, we call model.fit and model.predict functions to train on the custom dataset",https://stackoverflow.com/questions/69626196,deep-learning,19-10-2021 06:48,6207.0,2.0,1.0,True,19-10-2021 08:18,19-10-2021 08:18
56780427,how should i install the english model of spacy on my jupyter notebook which runs on a google cloud instance?,"i am trying to use the english model of spacy in jyputer notebook (python 3) which runs on a google cloud instance. i have installed spacy, but my problem is that i cannot install/import its english model.
i have already tried the following codes:
!pip3 install en_core_web_sm
!python -m spacy download en

and many other codes,  but none have worked and every time i got a different error. i use spacy hassle-free when i am working on my local machine, but i do not know how to install the english model on a jyputer notebook which runs on cloud. any suggestions?
thanks!
fyi: when trying: !pip3 install en_core_web_sm,
i get the following error:
collecting spacy-model-en_core_web_sm
exception:
traceback (most recent call last):
  file ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  file ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 353, in run
    wb.build(autobuilding=true)
  file ""/usr/lib/python3/dist-packages/pip/wheel.py"", line 749, in build
    self.requirement_set.prepare_files(self.finder)
  file ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files
    ignore_dependencies=self.ignore_dependencies))
  file ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 554, in _prepare_file
    require_hashes
  file ""/usr/lib/python3/dist-packages/pip/req/req_install.py"", line 278, in populate_link
    self.link = finder.find_requirement(self, upgrade)
  file ""/usr/lib/python3/dist-packages/pip/index.py"", line 465, in find_requirement
    all_candidates = self.find_all_candidates(req.name)
  file ""/usr/lib/python3/dist-packages/pip/index.py"", line 423, in find_all_candidates
    for page in self._get_pages(url_locations, project_name):
  file ""/usr/lib/python3/dist-packages/pip/index.py"", line 568, in _get_pages
    page = self._get_page(location)
  file ""/usr/lib/python3/dist-packages/pip/index.py"", line 683, in _get_page
    return htmlpage.get_page(link, session=self.session)
  file ""/usr/lib/python3/dist-packages/pip/index.py"", line 795, in get_page
    resp.raise_for_status()
  file ""/usr/share/python-wheels/requests-2.12.4-py2.py3-none-any.whl/requests/models.py"", line 893, in raise_for_status
    raise  response=self)
requests.exceptions. 404 client error: not found for url:","['python-3.x', 'google-cloud-platform', 'jupyter-notebook', 'spacy']",56782861,"i found the answer thanks to @dustin ingram. i should type in:
!python3 -m spacy download en_core_web_sm

if you use python 2, drop ""3"" from the end of python in the command above!
you can run this within python as in a jupyter notebook via:
import spacy
spacy.cli.download('en_core_web_sm')",https://stackoverflow.com/questions/56780427,python-3.x,26-06-2019 20:29,14248.0,9.0,2.0,True,27-03-2024 08:37,26-06-2019 21:49
54717449,mapping word vector to the most similar/closest word using spacy,"i am using spacy as part of a topic modelling solution and i have a situation where i need to map a derived word vector to the ""closest"" or ""most similar"" word in a vocabulary of word vectors.
i see gensim has a function (wordembeddingskeyedvectors.similar_by_vector) to calculate this, but i was wondering if spacy has something like this to map a vector to a word within its vocabulary (nlp.vocab)?","['nlp', 'spacy', 'word2vec', 'word-embedding']",54726446,"after a bit of experimentation, i found a scikit function (cdist in scikit.spatial.distance) that finds a ""close"" vector in a vector space to the input vector. 
# imports
from scipy.spatial import distance
import spacy

# load the spacy vocabulary
nlp = spacy.load(""en_core_web_lg"")

# format the input vector for use in the distance function
# in this case we will artificially create a word vector from a real word (""frog"")
# but any derived word vector could be used
input_word = ""frog""
p = np.array([nlp.vocab[input_word].vector])

# format the vocabulary for use in the distance function
ids = [x for x in nlp.vocab.vectors.keys()]
vectors = [nlp.vocab.vectors[x] for x in ids]
vectors = np.array(vectors)

# *** find the closest word below ***
closest_index = distance.cdist(p, vectors).argmin()
word_id = ids[closest_index]
output_word = nlp.vocab[word_id].text
# output_word is identical, or very close, to the input word",https://stackoverflow.com/questions/54717449,nlp,15-02-2019 21:43,9036.0,12.0,5.0,True,24-05-2024 08:24,14-10-2020 11:47
41170726,add/remove custom stop words with spacy,what is the best way to add/remove stop words with spacy? i am using token.is_stop function and would like to make some custom changes to the set. i was looking at the documentation but could not find anything regarding of stop words. thanks!,"['python', 'nlp', 'stop-words', 'spacy']",41172279,"you can edit them before processing your text like this (see this post):
>>> import spacy
>>> nlp = spacy.load(""en"")
>>> nlp.vocab[""the""].is_stop = false
>>> nlp.vocab[""definitelynotastopword""].is_stop = true
>>> sentence = nlp(""the word is definitelynotastopword"")
>>> sentence[0].is_stop
false
>>> sentence[3].is_stop
true

note: this seems to work <=v1.8. for newer versions, see other answers.",https://stackoverflow.com/questions/41170726,python,15-12-2016 18:11,80837.0,69.0,8.0,True,03-01-2023 05:19,20-05-2020 16:55
70978468,how to get mentions in pytorch ner instead of toknes?,"i am using pytorch and a pre-trained model.
here is my code:
class ner(object):
    def __init__(self, model_name_or_path, tokenizer_name_or_path):
        self.tokenizer = autotokenizer.from_pretrained(tokenizer_name_or_path)
        self.model = automodelfortokenclassification.from_pretrained(
            model_name_or_path)
        self.nlp = pipeline(""ner"", model=self.model, tokenizer=self.tokenizer)

    def get_mention_entities(self, query):
        return self.nlp(query)


when i call get_mention_entities and print its output for ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½.""
it gives:
[{'entity': 'b-fac', 'score': 0.9454591, 'index': 2, 'word': 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½', 'start': 6, 'end': 13}, {'entity': 'i-fac', 'score': 0.9713519, 'index': 3, 'word': 'ï¿½ï¿½ide>
as you can see, it can recognize the university name, but there are three tokens in the list.
is there any standard way to combine these tokens based on the ""entity"" attribute?
desired output is something like:
[{'entity': 'fac', 'word': 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½', 'start': 6, 'end': 28}]

finally, i can write a function to iterate, compare, and merge the tokens based on the ""entity"" attribute, but i want a standard way like an internal py"" question.
ps: ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï","['python', 'pytorch', 'huggingface-transformers', 'named-entity-recognition', 'mention']",70978579,"huggingface's ner pipeline has an argument grouped_entities=true which will do exactly what you seek: group bi into unified entities.
adding
self.nlp = pipeline(""ner"", model=self.model, tokenizer=self.tokenizer, grouped_entities=true)

should do the trick",https://stackoverflow.com/questions/70978468,python,03-02-2022 21:21,101.0,0.0,1.0,True,03-02-2022 21:31,03-02-2022 21:31
70677179,why is this accuracy of this random forest sentiment classification so low?,"i want to use randomforestclassifier for sentiment classification. the x contains data in string text, so i used labelencoder to convert strings. y contains data in numbers. and my code is this:
import pandas as pd
import numpy as np
from sklearn.model_selection import *
from sklearn.ensemble import *
from sklearn import *
from sklearn.preprocessing.label import labelencoder

data = pd.read_csv('data.csv')

x = data['reviews']
y = data['ratings']

le = labelencoder()
x_encoded = le.fit_transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_encoded,y, test_size = 0.2)

x_train = x_train.reshape(-1,1)
x_test = x_test.reshape(-1,1)

clf = randomforestclassifier(n_estimators=100)

clf.fit(x_train, y_train)

y_pred = clf.predict(x_test)

then i printed out the accuracy like below:
print(""accuracy:"", metrics.accuracy_score(y_test, y_pred))

and here's the output:
accuracy: 0.5975

i have read that random forests has high accuracy, because of the number of decision trees participating in the process. but i think that the accuracy is much lower than it should be. i have looked for some similar questions on stack overflow, but i couldn't find a solution for my problem.
is there any problem in my code using random forest library? or is there any exceptions of cases when using random forest?","['python', 'scikit-learn', 'random-forest', 'text-classification']",70677500,"it is not a problem regarding random forests or the library, it is rather a problem how you transform your text input into a feature or feature vector.
what labelencoding does is; given some labels like [""a"", ""b"", ""c""] it transforms those labels into numeric values between 0 and n-1 with n-being the number of distinct input labels. however, i assume reviews contain texts and not pure labels so to say. this means, all your reviews (if not 100% identical) are transformed into different labels. eventually, this leads to your classifier doing random stuff. give that input. this means you need something different to transform your textual input into a numeric input that random forests can work on.
as a simple start, you can try something like tfidf or also some simple count vectorizer. those are available from sklearn  section 6.2.3. text feature extraction. there are more sophisticated ways of transforming texts into numeric vectors but that should be a good start for you to understand what has to happen conceptually.
a last important note is that you fit those vectorizers only on the training set and not on the full dataset. otherwise, you might leak information from training to evaluation/testing. a good way of doing this would be to build a sklearn pipeline that consists of a feature transformation step and the classifier.",https://stackoverflow.com/questions/70677179,python,12-01-2022 06:24,418.0,1.0,1.0,True,14-01-2022 18:31,14-01-2022 18:31
76136216,how do i reshape data to calculate roc and auc for binary text classification?,"i'm very new to python and need to calculate the roc and auc of two binary classification models using nlp data. i can't seem to get my head around sparse vs dense arrays (i mean, i get that sparse arrays contain a ton of zeros, and dense arrays do not), data shape, and dimensionality.
i think i can produce pretty good preprocessed data, but inputting that into my classifiers in a way they can read has me stymied.
in my code below, you'll note that i have tried more than one train test split. i get
typeerror: a sparse matrix was passed, but dense data is required. use x.toarray() to convert to a dense numpy array.

if i don't convert x and y to dense.
i get
valueerror: y should be a 1d array, got an array of shape (1594, 286579) instead

undefinedmetricwarning: no positive samples in y_true, true positive value should be meaningless

when i do the dense conversion.
and i get
valueerror: found input variables with inconsistent numbers of samples: [1594, 399]

when (if i'm remembering correctly) using the commented out train test split.
here is my messy, redundant code:
import joblib
import re
import string
import numpy as np
import pandas as pd
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import countvectorizer
from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report
from sklearn.model_selection import stratifiedkfold, train_test_split
from sklearn.naive_bayes import multinomialnb

categories = ['rec.sport.baseball', 'rec.sport.hockey']

news_group_data = fetch_20newsgroups(subset=""all"", remove=(""headers"", ""footers"", ""quotes""), categories=categories)

df = pd.dataframe(dict(text=news_group_data[""data""],target=news_group_data[""target""]))
df[""target""] = df.target.map(lambda x: categories[x])

def process_text(text):
    text = str(text).lower()
    text = re.sub(f""[{re.escape(string.punctuation)}]"", "" "", text)
    text = "" "".join(text.split())
    return text

df[""clean_text""] = df.text.map(process_text)

#df_train, df_test = train_test_split(df, test_size=0.20, stratify=df.target)

vec = countvectorizer(ngram_range=(1, 3), stop_words=""english"",)

x = vec.fit_transform(df.clean_text)
y = vec.transform(df.clean_text)


#x = vec.fit_transform(df_train.clean_text)
#y = vec.transform(df_test.clean_text)

x = x.toarray()
y = y.toarray()

#y_train = df_train.target
#y_test = df_test.target

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2,
                                                    random_state=0)

from sklearn.ensemble import randomforestclassifier
from sklearn.naive_bayes import gaussiannb

randomforestclassifier(bootstrap=true, ccp_alpha=0.0, class_weight=none,
                       criterion='gini', max_depth=none, max_features=5,
                       max_leaf_nodes=none, max_samples=none,
                       min_impurity_decrease=0.0, #min_impurity_split=none,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=500,
                       n_jobs=none, oob_score=false, random_state=none,
                       verbose=0, warm_start=false)

nb = gaussiannb()
nb.fit(x_train, y_train)

r_probs = [0 for _ in range(len(y_test))]
rf_probs = rf.predict_proba(x_test)
nb_probs = nb.predict_proba(x_test)

rf_probs = rf_probs[:, 1]
nb_probs = nb_probs[:, 1]

from sklearn.metrics import roc_curve, roc_auc_score

r_auc = roc_auc_score(y_test, r_probs)
rf_auc = roc_auc_score(y_test, rf_probs)
nb_auc = roc_auc_score(y_test, nb_probs)

print('random (chance) prediction: auroc = %.3f' % (r_auc))
print('random forest: auroc = %.3f' % (rf_auc))
print('naive bayes: auroc = %.3f' % (nb_auc))

r_fpr, r_tpr, _ = roc_curve(y_test, r_probs)
rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)
nb_fpr, nb_tpr, _ = roc_curve(y_test, nb_probs)

import matplotlib.pyplot as plt

plt.plot(r_fpr, r_tpr, linestyle='--', label='random prediction (auroc = %0.3f)' % r_auc)
plt.plot(rf_fpr, rf_tpr, marker='.', label='random forest (auroc = %0.3f)' % rf_auc)
plt.plot(nb_fpr, nb_tpr, marker='.', label='naive bayes (auroc = %0.3f)' % nb_auc)

plt.title('roc plot')
plt.xlabel('false positive rate')
plt.ylabel('true positive rate')
plt.legend()
plt.show()","['python', 'machine-learning', 'scikit-learn', 'nlp', 'auc']",76136550,"the problem is that you are not using the correct target. you are basically encoding two times the text with the countvectorizer, in these lines:
x = vec.fit_transform(df.clean_text)
y = vec.transform(df.clean_text)

instead you should encode the binary class in df.target as target for the model (your y)
def labeling(v):
    if v == categories[0]:
        return 0
    else:
        return 1

df[""target_encod""] = df.target.map(labeling)
print(df['target_encod'])

after that you can use the correct y for your machine learning problem
x = x.toarray()
y = df[""target_encod""].values

my result after the changes:

for the next question, you forgot to assign a variable to the randomforest instance
randomforestclassifier(bootstrap=true, ccp_alpha=0.0, class_weight=none,
                   criterion='gini', max_depth=none, max_features=5,
                   max_leaf_nodes=none, max_samples=none,
                   min_impurity_decrease=0.0, #min_impurity_split=none,
                   min_samples_leaf=1, min_samples_split=2,
                   min_weight_fraction_leaf=0.0, n_estimators=500,
                   n_jobs=none, oob_score=false, random_state=none,
                   verbose=0, warm_start=false)

instead of
rf = randomforestclassifier(bootstrap=true, ccp_alpha=0.0, class_weight=none,
                   criterion='gini', max_depth=none, max_features=5,
                   max_leaf_nodes=none, max_samples=none,
                   min_impurity_decrease=0.0, #min_impurity_split=none,
                   min_samples_leaf=1, min_samples_split=2,
                   min_weight_fraction_leaf=0.0, n_estimators=500,
                   n_jobs=none, oob_score=false, random_state=none,
                   verbose=0, warm_start=false)",https://stackoverflow.com/questions/76136216,python,29-04-2023 12:27,200.0,2.0,1.0,True,30-04-2023 15:44,29-04-2023 13:58
68538365,issues converting text to lower case even after making sure it is a string,"i'm struggling to convert my text to lower case even after making sure it is a string. i'm not sure why it says that it's a ""series"" object when i clearly converted it to a string and even checked that it was converted to string.","['python', 'string', 'text', 'nlp', 'lowercase']",68538386,"use this
corpus = data['text'].str.lower()",https://stackoverflow.com/questions/68538365,python,27-07-2021 02:50,180.0,0.0,1.0,True,27-07-2021 03:24,27-07-2021 03:24
75033776,is there an alternative to the command line &quot;python -m module app ...&quot; that works inside python code?,"for example i'm downloading a model for spacy using the command:
python -m spacy download en_core_web_sm

is there an acceptable way to access this functionality within python, something like:
import spacy

spacy.download('en_core_web_sm') # or whatever command that would work

?
i was also wondering about how to access this functionalities for other cases.
i tried to look directly at spacy source code but it wasn't clear at all to me how this worked (there seems to be an ""app"" system of some sort, i have no idea of this works).","['python', 'command-line-arguments', 'spacy', 'python-module']",75034036,"you can access the same functionality within a script using spacy.cli.download. it is essentially an internal call to pip. for example:
import spacy
spacy.cli.download(""en_core_web_sm"")
# lots of output from pip as model is installed
nlp = spacy.load(""en_core_web_sm"")
...

please note that this is specific to spacy. other modules that are run via the command line - python -m some_module command ... won't necessarily have the command functionality exposed in exactly this way.",https://stackoverflow.com/questions/75033776,python,06-01-2023 16:47,234.0,1.0,1.0,True,06-01-2023 17:12,06-01-2023 16:48
77839628,loading en_core_web_sm results in attributeerror: module &#39;transformers&#39; has no attribute &#39;berttokenizerfast&#39;,"i (a beginner at programming and anything pc related) am clueless how to solve the following problem:
i had spacy 3.7.2 installed, including en_core_web_sm. running the code
nlp = spacy.load(""en_core_web_sm"")

resulted in the traceback mentioned in the title question.
i tried downgrading spacy to version 3.6.1 (via pip install), then ran the code again and got the traceback:
registryerror: [e892] unknown function registry: 'vectors'. available names: architectures, augmenters, batchers, callbacks, cli, datasets, displacy_colors, factories, initializers, languages, layers, lemmatizers, loggers, lookups, losses, misc, models, ops, optimizers, readers, schedules, scorers, tokenizers

same happend with spacy version 3.6.0.
some other person ran my code with spacy 3.4.3 and that worked. so tried downgrading spacy to that, but got the attributeerror again.
i noticed, that on my pc (windows) under ...anaconda3\lib\site-packages there were still folders with en_core_web_sm version 3.7.2 (despite the downgrading). i randomly deleted those folders.
then installed spacy again, and also tried to manually install en_core_web_sm with
python -m spacy download en_core_web_sm-3.4.3
but got the attributeerror in cmd. tried installing different older/newer versions of spacy as well as en_core_web_sm but can't successfully install the latter.
so now i can't even load en_core_web_sm since it's not installed. does anyone have an idea what else i could try to install en_core_web_sm?","['python', 'pip', 'nlp', 'anaconda', 'spacy']",77839778,"i think that there is older version of the transformers in your global environment that cause the problem.
to avoid version conflict, create a new virtual environment using conda :
 conda create -n myenv


activate myenv:
conda activate myenv

install scipy check the instalation page:
conda install -c conda-forge spacy

download en_core_web_sm :
python -m spacy download en_core_web_sm

now you can run your code.",https://stackoverflow.com/questions/77839628,python,18-01-2024 13:04,285.0,0.0,1.0,True,18-01-2024 13:31,18-01-2024 13:17
77644218,passing in &quot;embedding&quot; as a callable is deprecated,"this piece of code seems to not work. even though this is the way that pinecone have stated in their documentation that it should look like.
vectorstore = pinecone(index, embeddings.embed_query, text_field)
the error/warning is
c:\users\ndira\casetext-test-server\lib\site-packages\langchain\vectorstores\pinecone.py:59: userwarning: passing in ""embedding"" as a callable is deprecated. please pass in an embeddings object instead.   warnings.warn(
i don't know any other way of solving this. kindly help thanks.","['langchain', 'py-langchain', 'pinecone']",77658275,"replace embeddings.embed_query with embeddings and the warning should clear.
the documentation has not been updated it seems",https://stackoverflow.com/questions/77644218,langchain,12-12-2023 07:04,722.0,0.0,1.0,True,14-12-2023 06:56,12-12-2023 07:45
7851937,extract relationships using nltk,"this is a follow-up of my question. i am using nltk to parse out persons, organizations, and their relationships. using this example, i was able to create chunks of persons and organizations; however, i am getting an error in the nltk.sem.extract_rel command:
attributeerror: 'tree' object has no attribute 'text'

here is the complete code:
import nltk
import re
#billgatesbio from 
with open('billgatesbio.txt', 'r') as f:
    sample = f.read()

sentences = nltk.sent_tokenize(sample)
tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]
tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]
chunked_sentences = nltk.batch_ne_chunk(tagged_sentences)

# tried plain ne_chunk instead of batch_ne_chunk as given in the book
#chunked_sentences = [nltk.ne_chunk(sentence) for sentence in tagged_sentences]

# pattern to find <person> served as <title> in <org>
in = re.compile(r'.+\s+as\s+')
for doc in chunked_sentences:
    for rel in nltk.sem.extract_rels('org', 'person', doc,corpus='ieer', pattern=in):
        print nltk.sem.show_raw_rtuple(rel)

this example is very similar to the one given in the book, but the example uses prepared 'parsed docs,' which appears of nowhere and i don't know where to find its object type. i scoured thru the git libraries as well. any help is appreciated.
my ultimate goal is to extract persons, organizations, titles (dates) for some companies; then create network maps of persons and organizations.","['python', 'nlp', 'nltk']",7853030,"it looks like to be a ""parsed doc"" an object needs to have a headline member and a text member both of which are lists of tokens, where some of the tokens are marked up as trees. for example this (hacky) example works:
import nltk
import re

in = re.compile (r'.*\bin\b(?!\b.+ing)')

class doc():
  pass

doc.headline=['foo']
doc.text=[nltk.tree('organization', ['whyy']), 'in', nltk.tree('location',['philadelphia']), '.', 'ms.', nltk.tree('person', ['gross']), ',']

for rel in  nltk.sem.extract_rels('org','loc',doc,corpus='ieer',pattern=in):
   print nltk.sem.relextract.show_raw_rtuple(rel)

when run this provides the output:
[org: 'whyy'] 'in' [loc: 'philadelphia']

obviously you wouldn't actually code it like this, but it provides a working example of the data format expected by extract_rels, you just need to determine how to do your preprocessing steps to get your data massaged into that format.",https://stackoverflow.com/questions/7851937,python,21-10-2011 15:46,10529.0,10.0,4.0,True,25-01-2024 18:04,23-05-2017 12:17
75245706,what is log level in rasa? what is its purpose?,"i am  new to this environment. help me find out what this is for
some commands are also given.
log_level_libraries=error log_level_matplotlib=warning log_level_kafka=debug rasa shell --debug

log_level_libraries=debug log_level_matplotlib=debug rasa shell --verbose","['rasa', 'rasa-nlu']",75295382,"this is about the messages you could probably see when you run or train your rasa. rasa offers you the option to activate/desactivate logs for external libraries. you can do so by setting these environment variable.
for example for macos you can do so by typing in your cli: export variable_name=variable_value
you can read more about this here:

for macos
for windows",https://stackoverflow.com/questions/75245706,rasa,26-01-2023 11:49,338.0,0.0,1.0,True,14-02-2023 10:30,14-02-2023 10:30
78233240,pubtator api doesn&#39;t return session number as expected,"currently trying to validate that pubtator's api for named entity recognition (ner) works and returns expected output format. i downloaded the example files included in the sample python code at  using one of these example input files , i tried the following curl command as indicated in their documentation:
curl -x post --data-binary @input/ex2.pubtator 

i just get html code for an error-500 webpage. input/ex.pubtator is simply a single abstract, and it doesn't contain any special characters that i can discern:
20085714|t|autosomal-dominant striatal degeneration is caused by a mutation in the phosphodiesterase 8b gene.
20085714|a|autosomal-dominant striatal degeneration (adsd) is an autosomal-dominant movement disorder affecting the striatal part of the basal ganglia. adsd is characterized by bradykinesia, dysarthria, and muscle rigidity. these symptoms resemble idiopathic parkinson disease, but tremor is not present. using genetic linkage analysis, we have mapped the causative genetic defect to a 3.25 megabase candidate region on chromosome 5q13.3-q14.1. a maximum lod score of 4.1 (theta = 0) was obtained at marker d5s1962. here we show that adsd is caused by a complex frameshift mutation (c.94g>c+c.95delt) in the phosphodiesterase 8b (pde8b) gene, which results in a loss of enzymatic phosphodiesterase activity. we found that pde8b is highly expressed in the brain, especially in the putamen, which is affected by adsd. pde8b degrades cyclic amp, a second messenger implied in dopamine signaling. dopamine is one of the main neurotransmitters involved in movement control and is deficient in parkinson disease. we believe that the functional analysis of pde8b will help to further elucidate the pathomechanism of adsd as well as contribute to a better understanding of movement disorders.

what am i doing wrong?","['named-entity-recognition', 'ncbi', 'pubmed-api']",78373596,"the url i'm using for my named entity recognition api request appears to be deprecated now that pubtator3 is in beta.
if submitting a short string of text, use ...
curl -x post  -h ""content-type: application/x- -d ""text=possible role of valvular serotonin 5-ht receptors in the cardiopathy associated with fenfluramine.&bioconcept=gene""

if submitting entire contents of a plain text file, which is probably the more relevant use case, one needs to work around the curl character limit on arguments. best i could manage was ...
printf ""text=%s&bioconcept=gene"" ""$(cat ___data/plain_txts/yang2020.txt)"" | curl -x post  -h ""content-type: application/x- --data-binary @-

regrettably, the output annotation can still be truncated if the input file has too much content.",https://stackoverflow.com/questions/78233240,named-entity-recognition,27-03-2024 16:34,138.0,0.0,1.0,True,26-06-2024 17:53,26-06-2024 17:53
78016973,check the difference in pretrained and finetuned model,"so i am finetuning a pretrained llama2 model. i want to check if the model that i have finetuned is different from the original. i want to check the difference between base_model and model. is there a way to check if there is a difference in weights or parameters after training?
from google.colab import drive

# mount google drive
drive.mount('/content/drive')

# path to your saved model in google drive
model_path_in_drive = '/content/drive/mydrive/mod/llama-2-7b-miniguanaco'

# reload model in fp16 and merge it with lora weights
base_model = automodelforcausallm.from_pretrained(
    model_name,
    low_cpu_mem_usage=true,
    return_dict=true,
    torch_dtype=torch.float16,
    device_map=device_map,
)

# load your peftmodel from the saved checkpoint in google drive
model = peftmodel.from_pretrained(base_model, model_path_in_drive)
model = model.merge_and_unload()
#mark_only_lora_as_trainable(lora_model)
# reload tokenizer to save it
tokenizer = autotokenizer.from_pretrained(model_name, trust_remote_code=true)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = ""right""

i tried some methods in this article but it didn't help me at all.","['pytorch', 'huggingface-transformers', 'llama']",78023090,"just loop over the parameters and compare them with torch.allclose. i used distilbertmodel for the answer, please use the respective classes from your example that are also mentioned in the comments:
import torch
from transformers import  distilbertmodel

# automodelforcausallm in your case
base_model =  distilbertmodel.from_pretrained(""distilbert/distilbert-base-uncased"")
# peftmodel.merge_and_unload() in your case
finetuned_model = distilbertmodel.from_pretrained(""distilbert/distilbert-base-uncased-finetuned-sst-2-english"")

for base_param, finetuned_param in zip(base_model.named_parameters(), finetuned_model.named_parameters()):
  if not torch.allclose(base_param[1], finetuned_param[1]):
    print(base_param[0])

output:
embeddings.word_embeddings.weight
embeddings.position_embeddings.weight
embeddings.layernorm.weight
embeddings.layernorm.bias
transformer.layer.0.attention.q_lin.weight
transformer.layer.0.attention.q_lin.bias
transformer.layer.0.attention.k_lin.weight
transformer.layer.0.attention.k_lin.bias
transformer.layer.0.attention.v_lin.weight
transformer.layer.0.attention.v_lin.bias
transformer.layer.0.attention.out_lin.weight
transformer.layer.0.attention.out_lin.bias
transformer.layer.0.sa_layer_norm.weight
transformer.layer.0.sa_layer_norm.bias
transformer.layer.0.ffn.lin1.weight
transformer.layer.0.ffn.lin1.bias
transformer.layer.0.ffn.lin2.weight
transformer.layer.0.ffn.lin2.bias
transformer.layer.0.output_layer_norm.weight
transformer.layer.0.output_layer_norm.bias
...
transformer.layer.5.attention.q_lin.weight
transformer.layer.5.attention.q_lin.bias
transformer.layer.5.attention.k_lin.weight
transformer.layer.5.attention.k_lin.bias
transformer.layer.5.attention.v_lin.weight
transformer.layer.5.attention.v_lin.bias
transformer.layer.5.attention.out_lin.weight
transformer.layer.5.attention.out_lin.bias
transformer.layer.5.sa_layer_norm.weight
transformer.layer.5.sa_layer_norm.bias
transformer.layer.5.ffn.lin1.weight
transformer.layer.5.ffn.lin1.bias
transformer.layer.5.ffn.lin2.weight
transformer.layer.5.ffn.lin2.bias
transformer.layer.5.output_layer_norm.weight
transformer.layer.5.output_layer_norm.bias",https://stackoverflow.com/questions/78016973,pytorch,18-02-2024 18:44,211.0,1.0,1.0,True,19-02-2024 22:41,19-02-2024 22:41
35747245,bigram vector representations using word2vec,"i want to construct word embeddings for documents using the word2vec tool. i know how to find a vector embedding corresponding to a single word (unigram). now, i want to find a vector for a bigram. is it possible to construct a bigram word embedding using word2vec? if yes, how?","['nlp', 'word2vec', 'word-embedding']",35915218,"the following snippet will get you the vector representation of a bigram. note that the bigram you want to convert to a vector needs to have an underscore instead of a space between the words, e.g. bigram2vec(unigrams, ""this report"") is wrong, it should be bigram2vec(unigrams, ""this_report""). for more details on generating the unigrams, please see the gensim.models.word2vec.word2vec class here.
from gensim.models import word2vec

def bigram2vec(unigrams, bigram_to_search):
    bigrams = phrases(unigrams)
    model = word2vec.word2vec(bigrams[unigrams])
    if bigram_to_search in model.vocab.keys():
        return model[bigram_to_search]
    else:
        return none",https://stackoverflow.com/questions/35747245,nlp,02-03-2016 12:27,6742.0,10.0,1.0,True,07-09-2022 19:55,07-09-2022 19:55
73567055,extend bert or any transformer model using manual features,"i have been doing a thesis in my citation classifications. i just implemented bert model for the classification of citations. i have 4 output classes and i give an input sentence and my model returns an output that tells the category of citation. now my supervisor gave me another task.
you have to search that whether it is possible to extend bert or any transformer model using manual features. e.g. you are currently giving a sentence as the only input followed by its class. what if you can give a sentence, and some other features as input; as we do in other classifiers?
i need some guidance about this problem. how can i add an extra feature in my bert model and the feature would be categorical not numerical.","['huggingface-transformers', 'text-classification', 'bert-language-model']",73576407,"the are several ways to achieve that. i will explain just two in the following answer:

add category as a token:
the idea of this approach is rather simple when transformer models like bert are able to produce contextualized embeddings for a given sentence, why can't we incorporate categorical features as text as well? for example, you use the title of a cited paper as input and also want to incorporate the research area of the paper to provide more context:

""attention is all you need. [computer science] [machine translation]"" -> bert

to do that, i would add the categories of your new feature as separate tokens to bert (that is not required but reduces the sequence length) and fine-tune it for a few epochs:
from transformers import berttokenizer, bertforsequenceclassification

my_categories = [""[computer science]"", ""[machine translation]""]
sentence=""attention is all you need. [computer science] [machine translation]""

t= berttokenizer.from_pretrained(""bert-base-cased"")
m=bertforsequenceclassification.from_pretrained(""bert-base-cased"")
# tokenized without separate tokens
print(len(t(sentence)[""input_ids""]))

# tokenized without separate tokens
t.add_tokens(my_categories)
print(len(t(sentence)[""input_ids""]))

# extend embedding layer of model
m.resize_token_embeddings(len(t.get_vocab()))

# training...

output:
18
12
embedding(28998, 768, padding_idx=0)


separate embedding layer:
a more traditional way is to hold an embedding for each category and concatenate (or any other method to combine features) it with the contextualized output of bert before you feed it to the classification layer. for this approach, you can simply copy the code from huggingfaces bertforsequenceclassification class (or whatever class you are using) and make the required changes:

import torch
from torch.nn import bcewithlogitsloss, crossentropyloss, mseloss
from transformers import bertpretrainedmodel, bertmodel
from typing import optional

class mybertforsequenceclassification(bertpretrainedmodel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.config = config

        self.bert = bertmodel(config)
        classifier_dropout = (
            config.classifier_dropout if config.classifier_dropout is not none else config.hidden_dropout_prob
        )
        self.dropout = torch.nn.dropout(classifier_dropout)
        
        # modified +20
        self.classifier = torch.nn.linear(config.hidden_size +20, config.num_labels)

        # modified 50 different categories embedding dimension 20 
        self.my_categorical_feature = torch.nn.embedding(50,20)

        # initialize weights and apply final processing
        self.post_init()

    # modified new parameter categorical_feature_ids
    def forward(
        self,
        input_ids: optional[torch.tensor] = none,
        attention_mask: optional[torch.tensor] = none,
        token_type_ids: optional[torch.tensor] = none,
        position_ids: optional[torch.tensor] = none,
        head_mask: optional[torch.tensor] = none,
        inputs_embeds: optional[torch.tensor] = none,
        labels: optional[torch.tensor] = none,
        output_attentions: optional[bool] = none,
        output_hidden_states: optional[bool] = none,
        return_dict: optional[bool] = none,
        categorical_feature_ids = none,
    ):

        return_dict = return_dict if return_dict is not none else self.config.use_return_dict

        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        pooled_output = outputs[1]

        # modified get embeddings
        my_categorical_embedding = self.my_categorical_feature(categorical_feature_ids)
        my_categorical_embedding = self.dropout(my_categorical_embedding)

        pooled_output = self.dropout(pooled_output)
        
        # modified concatenate contextualized embeddings from bert and your categorical embedding
        pooled_output = torch.cat((pooled_output, my_categorical_embedding), dim=-1)

        logits = self.classifier(pooled_output)

        loss = none
        if labels is not none:
            if self.config.problem_type is none:
                if self.num_labels == 1:
                    self.config.problem_type = ""regression""
                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
                    self.config.problem_type = ""single_label_classification""
                else:
                    self.config.problem_type = ""multi_label_classification""

            if self.config.problem_type == ""regression"":
                loss_fct = mseloss()
                if self.num_labels == 1:
                    loss = loss_fct(logits.squeeze(), labels.squeeze())
                else:
                    loss = loss_fct(logits, labels)
            elif self.config.problem_type == ""single_label_classification"":
                loss_fct = crossentropyloss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            elif self.config.problem_type == ""multi_label_classification"":
                loss_fct = bcewithlogitsloss()
                loss = loss_fct(logits, labels)
        if not return_dict:
            output = (logits,) + outputs[2:]
            return ((loss,) + output) if loss is not none else output

        return {
            ""loss"":loss,
            ""logits"":logits,
            ""hidden_states"":outputs.hidden_states,
            ""attentions"":outputs.attentions,
        }

you can use this class just as the bertforserquenceclassification class, the only difference is, that it expects categorical_feature_ids as additional input:
from transformers import berttokenizer, bertforsequenceclassification

t= berttokenizer.from_pretrained(""bert-base-cased"")
m= mybertforsequenceclassification.from_pretrained(""bert-base-cased"")

# batch with two sentences (i.e. the citation text you have already used) 
i = t([""paper title 1"", ""paper title 2""], padding=true, return_tensors=""pt"")

# we assume that the first sentence (i.e. paper title 1) belongs to category 23 and the second sentence to category 42
# you probably want to use a dictionary in your own code 
i[""categorical_feature_ids""] = torch.tensor([23,42])

print(m(**i))

output:
{'loss': none, 
'logits': tensor([[ 0.6069, -0.1878], [ 0.6347, -0.2608]], grad_fn=<addmmbackward0>), 
'hidden_states': none, 
'attentions': none}",https://stackoverflow.com/questions/73567055,huggingface-transformers,01-09-2022 09:25,2761.0,6.0,1.0,True,03-09-2022 13:31,03-09-2022 13:31
73357612,python &amp; spacy: visualizing spans in a notebook,"i am following the instructions of spacy in order to visulaize the spans in a notebook and copying the code :

import spacy
from spacy import displacy
from spacy.tokens import span

text = ""welcome to the bank of china.""

nlp = spacy.blank(""en"")
doc = nlp(text)

doc.spans[""sc""] = [
    span(doc, 3, 6, ""org""), 
    span(doc, 5, 6, ""gpe""),
]

displacy.serve(doc, style=""span"")

the result in my notebook is as follows:

it is weird that when using displacy.render the rendering is of another quality as when using ""serve"". serve can not be used properly in a notebook.
i would like to know what i need to do to get the result shown in the documentation within a notebook. that means, the underlying.
note: display(displacy.render(doc, style=""span"")) does not work neither","['python', 'jupyter-notebook', 'visualization', 'spacy']",73815890,"in notebook you have to pass jupyter=true argument.
displacy.render(doc, style=""span"", jupyter=true)",https://stackoverflow.com/questions/73357612,python,15-08-2022 06:37,398.0,1.0,1.0,True,30-09-2022 12:13,30-09-2022 12:13
74860397,"use three transformations (average, max, min) of pretrained embeddings to a single output layer in pytorch","i have developed a trivial feed forward neural network with pytorch.
the neural network uses glove pre-trained embeddings in a freezed nn.embeddings layer.
next, the embedding layer splits into three embeddings. each split is a different transformation applied to the initial embedding layer. then the embeddings layer feed three nn.linear layers. and finally i have a single output layer for a binary classification target.
the shape of the embedding tensor is [64,150,50]
-> 64: sentences in the batch,
-> 150: words per sentence,
-> 50: vector-size of a single word (pre-trained glove vector)
so after the transformation, the embedding layer splits into three layers with shape [64,50], where 50 = either the torch.mean(),  torch.max() or torch.min() of the 150 words per sentence.
my questions are:

how could i feed the output layer from three different nn.linear layers to predict a single target value [0,1].

is this efficient and helpful to the total predictive power of the model? or just selecting the average of the embeddings is sufficient and no improvement will be observed.


the forward() method of my pytorch model is:
  def forward(self, text):

    embedded = self.embedding(text)
    if self.use_pretrained_embeddings:
      embedded_average = torch.mean(embedded, dim=1)
      embedded_max = torch.max(embedded, dim=1)[0]
      embedded_min = torch.min(embedded, dim=1)[0]
    else:
      embedded = self.flatten_layer(embedded)

    input_layer = self.input_layer(embedded_average) #each linear layer has the same value of hidden unit
    input_layer = self.activation(input_layer)

    input_layer_max = self.input_layer(embedded_max)
    input_layer_max = self.activation(input_layer_max)

    input_layer_min = self.input_layer(embedded_min)
    input_layer_min = self.activation(input_layer_min)
    
    #what should i do here? to exploit the weights of the 3 hidden layers
    output_layer = self.output_layer(input_layer)
    output_layer = self.activation_output(output_layer) #sigmoid()
    
    return output_layer

after the proposed answer the function is:
  def forward(self, text):

    embedded = self.embedding(text)
    if self.use_pretrained_embeddings:
      embedded_average = torch.mean(embedded, dim=1)
      embedded_max = torch.max(embedded, dim=1)[0]
      embedded_min = torch.min(embedded, dim=1)[0]

      #use of average embeddings transformation
      input_layer_average = self.input_layer(embedded_average)
      input_layer_average = self.activation(input_layer_average)
      
      #use of max embeddings transformation
      input_layer_max = self.input_layer(embedded_max)
      input_layer_max = self.activation(input_layer_max)

      #use of min embeddings transformation
      input_layer_min = self.input_layer(embedded_min)
      input_layer_min = self.activation(input_layer_min)

    else:
      embedded = self.flatten_layer(embedded)

    input_layer = torch.concat([input_layer_average, input_layer_max, input_layer_min], dim=1)
    input_layer = self.activation(input_layer)

    print(""3"",input_layer.shape) #[192,1] vs [64,1] -> output layer

    if self.n_layers !=0:
      for layer in self.layers:
          input_layer = layer(input_layer)

    output_layer = self.output_layer(input_layer)
    output_layer = self.activation_output(output_layer)
    
    return output_layer

this generates the following error:

valueerror: using a target size (torch.size([64, 1])) that is different to the input size (torch.size([192, 1])) is deprecated. please ensure they have the same size.

expected outcome since the concatenated layer is 3x the size of the sentences (64). any fix that could resolve it?","['python', 'machine-learning', 'pytorch', 'neural-network', 'word-embedding']",74860529,"regarding 1: you can use torch.concat to concatenate the outputs along the appropriate dimension, and then e.g. map them to a single output using another linear layer.
regarding 2: you will have to try it yourself and see whether this is useful.",https://stackoverflow.com/questions/74860397,python,20-12-2022 08:40,531.0,1.0,1.0,True,20-12-2022 10:22,20-12-2022 10:22
69801576,how to set vocabulary size in python tokenizers library?,"i would like to fit my own tokenizer and use it further for the pre-trained model, however, when fitting a new tokenizer there seems to be no way to choose the vocabulary size. so when i call tokenizer.get_vocab() it always returns a dictionary with 30000 elements. how do i change that? here is what i do:
from tokenizers import tokenizer
from tokenizers.models import bpe
tokenizer = tokenizer(bpe(unk_token=""[unk]""))

from tokenizers.trainers import bpetrainer
trainer = bpetrainer(special_tokens=[""[unk]"", ""[cls]"", ""[sep]"", ""[pad]"", ""[mask]""])

from tokenizers.pre_tokenizers import whitespace
tokenizer.pre_tokenizer = whitespace()

tokenizer.train(['transcripts.raw'], trainer) # here there are no additional arguments for some reason","['python', 'machine-learning', 'nlp', 'huggingface-tokenizers']",69801741,"what you can do is use the vocab_size parameter of the bpetrainer, which is set by default to 30000:
trainer = bpetrainer(special_tokens=[""[unk]"", ""[cls]"", ""[sep]"", ""[pad]"", ""[mask]""], vocab_size=10)

for more information, you can check out the docs.",https://stackoverflow.com/questions/69801576,python,01-11-2021 19:00,1424.0,0.0,1.0,True,08-12-2021 09:08,08-12-2021 09:08
77998898,troubleshooting gpt-4 integration with sqldatabasetoolkit and create_sql_agent for prompt passing error,"i was previously using sqldatabasechain to connect llm (language model) with my database, and it was functioning correctly with gpt-3.5. however, when attempting the same process with gpt-4, i encountered an error stating ""incorrect syntax near 's""
to address this issue, i opted to use sqldatabasetoolkit and the create_sql_agent function. however, i encountered a problem with this approach as i was unable to pass a prompt. when attempting to include a prompttemplate in the create_sql_agent argument, it resulted in errors.
valueerror: prompt missing required variables: {'tool_names', 'agent_scratchpad', 'tools'}
below is my code:
toolkit = sqldatabasetoolkit(db=db, llm=llm)

agent_executor = create_sql_agent(
    llm=llm,
    toolkit=toolkit,
    verbose=true,
    prompt=mssql_prompt,
)","['openai-api', 'prompt', 'langchain', 'gpt-4', 'mssql-tools']",78040867,"i found solution for it.
agent_executor = create_sql_agent(llm, db=db, agent_type=""openai-tools"", verbose=false)

will worked for me. and it also work with prompt based approach. so if you want to add prompt in it then it should be like
agent_executor = create_sql_agent(llm, db=db, agent_type=""openai-tools"", verbose=false, prompt=mssql_prompt)

where
    mssql_prompt = """"""you are an ms sql expert. given an input question, first create a syntactically correct ms sql query to run, then look at the results of the query and return the answer to the input question.

use the following domain knowledge about database: one order can have multiple shipments & shipment containers. 

use the following format:

question: question here
sqlquery: sql query to run
sqlresult: result of the sqlquery
answer: final answer here

only use the following tables:
{table_info}

thought: i should look at the tables in the database to see what i can query. then i should query the schema of the most relevant tables.
{agent_scratchpad}

question: {input}""""""

you can add more data in prompt as per your use cases.",https://stackoverflow.com/questions/77998898,openai-api,15-02-2024 07:03,790.0,5.0,1.0,True,12-04-2024 11:33,12-04-2024 11:33
77048121,how to format ragged tensor for encoder-decoder model?,"i'm working on building a seq2seq model using encoder-decoder architecture for which i have built a tf.data.dataset pipeline that reads the text from the directories, vectorizes using them tf.keras.layers.textvectorization and preprocess it to be fed for model training. i'm not able to format my labels such that it is of the shape (none, seq_len, target_vocab_size). i tried using to map tf.utils.to_categorical to the labels but it won't work on the tensors. strangely there is no material out there where there was a similar problem discussed. below is my implementation:
buffer_size = len(articles)
batch_size = 64

train_raw = (tf.data.dataset
             .from_tensor_slices((articles[is_train], summaries[is_train]))
             .shuffle(buffer_size)
             .batch(batch_size))

val_raw = (tf.data.dataset
           .from_tensor_slices((articles[~is_train], summaries[~is_train]))
           .shuffle(buffer_size)
           .batch(batch_size))

context_vectorizer = tf.keras.layers.textvectorization(
    standardize = tf_lower_and_split_punct,
    max_tokens = max_vocab_size,
    ragged=true)

target_vectorizer = tf.keras.layers.textvectorization(
    standardize=tf_lower_and_split_punct,
    max_tokens=max_vocab_size,
    ragged=true)

context_vectorizer.adapt(train_raw.map(lambda context, target: context))
target_vectorizer.adapt(train_raw.map(lambda context, target: target))

def preprocess_text(context, target):
    context = context_vectorizer(context).to_tensor()
    target = target_vectorizer(target)

    target_in = target[:,:-1].to_tensor()
    target_out = target[:,1:].to_tensor()
    # target_out = target[:,:-1]
    return (context, target_in), target_out

train_ds = train_raw.map(preprocess_text, tf.data.autotune)
val_ds = val_raw.map(preprocess_text, tf.data.autotune)

def encoder(hsize, embed_dim=200):
    en_input_layer = input(shape=(none,), name='encoder_input_layer', ragged=true)
    en_embed = embedding(context_vectorizer.vocabulary_size()+1, output_dim=embed_dim, name='encoder_embedding_layer')
    en_embed_out = en_embed(en_input_layer)
    en_gru_1 = gru(hsize, return_sequences=true, return_state=true, name='encoder_gru_layer_1')
    en_gru_1_out, en_gru_states = en_gru_1(en_embed_out)
    return en_input_layer, en_gru_1_out, en_gru_states

def decoder(hsize, encoder_states, embed_dim=200):
    de_input_layer = input(shape=(none,), name='decoder_input_layer', ragged=true)
    de_embed = embedding(target_vectorizer.vocabulary_size()+1, output_dim=embed_dim, name='decode_embedding_layer')
    de_embed_out = de_embed(de_input_layer)
    de_gru_1 = gru(hsize, return_sequences=true, name='decoder_gru_layer_1')
    de_gru_1_out = de_gru_1(de_embed_out, initial_state=encoder_states)
    de_dense = timedistributed(dense(target_vectorizer.vocabulary_size(), activation='softmax'), name='time_distributed_output_layer')
    de_preds = de_dense(de_gru_1_out)
    return de_input_layer, de_preds

hsize = 256

def create_model(hsize):
    en_input_layer, enc_out, enc_states = encoder(hsize)
    de_input_layer, de_preds = decoder(hsize, enc_states)
    model = model(inputs=[en_input_layer, de_input_layer], outputs=de_preds)
    model.compile(optimizer='adam', loss='categorical_crossentropy',
                    metrics=[""acc""])
    return model

### model training

m = create_model(hsize)

history = m.fit(
        train_ds.repeat(),
        steps_per_epoch=100,
        epochs=100,
        validation_data=val_ds,
        callbacks=[
            tf.keras.callbacks.modelcheckpoint('./checkpoints_trial_1',
                                                save_weights_only=true),
            tf.keras.callbacks.earlystopping(patience=3)])

the model summary is below:

 layer (type)                output shape                 param #   connected to                  
==================================================================================================
 encoder_input_layer (input  [(none, none)]               0         []                            
 layer)                                                                                           
                                                                                                  
 decoder_input_layer (input  [(none, none)]               0         []                            
 layer)                                                                                           
                                                                                                  
 encoder_embedding_layer (e  (none, none, 200)            437200    ['encoder_input_layer[0][0]'] 
 mbedding)                                                                                        
                                                                                                  
 decode_embedding_layer (em  (none, none, 200)            244200    ['decoder_input_layer[0][0]'] 
 bedding)                                                                                         
                                                                                                  
 encoder_gru_layer_1 (gru)   [(none, none, 256),          351744    ['encoder_embedding_layer[0][0
                              (none, 256)]                          ]']                           
                                                                                                  
 decoder_gru_layer_1 (gru)   (none, none, 256)            351744    ['decode_embedding_layer[0][0]
                                                                    ',                            
                                                                     'encoder_gru_layer_1[0][1]'] 
                                                                                                  
 time_distributed_output_la  (none, none, 1220)           313540    ['decoder_gru_layer_1[0][0]'] 
 yer (timedistributed)                                                                            
                                                                                                  
==================================================================================================
total params: 1698428 (6.48 mb)
trainable params: 1698428 (6.48 mb)
non-trainable params: 0 (0.00 byte)
__________________________________________________________________________________________________

the model compile's fine but when i run the fit method i get the following error:
valueerror: shapes (none, none) and (none, none, 1220) are incompatible

i'm struggling with defining the model's input layers correctly, or preprocess_text output that would work with the model definition.","['python', 'tensorflow', 'keras', 'deep-learning', 'nlp']",77057103,"reposting from above comment, to fix the above issue, we can either change the loss method that works on sparse vector or transform the target label to one-hot encoded. below is the complete working code with some dummy data.
make_one_hot = false # params: true, false

num_articles = 1000
num_summaries = 1000
max_vocab_size = 5000
articles = np.array([f""article {i}"" for i in range(num_articles)])
summaries = np.array([f""summary {i}"" for i in range(num_summaries)])
is_train = np.random.rand(len(articles)) < 0.8

def tf_lower_and_split_punct(text):
    text = tf.strings.lower(text)
    text = tf.strings.regex_replace(text, '[.?!,ï¿½ï¿½]', ' ')
    text = tf.strings.strip(text)
    text = tf.strings.join([' ', text, ' '])
    return text

buffer_size = len(articles)
batch_size = 64

train_raw = (tf.data.dataset
             .from_tensor_slices((articles[is_train], summaries[is_train]))
             .shuffle(buffer_size)
             .batch(batch_size))

val_raw = (tf.data.dataset
           .from_tensor_slices((articles[~is_train], summaries[~is_train]))
           .shuffle(buffer_size)
           .batch(batch_size))

context_vectorizer = tf.keras.layers.textvectorization(
    standardize = tf_lower_and_split_punct,
    max_tokens = max_vocab_size,
    ragged=true)

target_vectorizer = tf.keras.layers.textvectorization(
    standardize=tf_lower_and_split_punct,
    max_tokens=max_vocab_size,
    ragged=true)

context_vectorizer.adapt(train_raw.map(lambda context, target: context))
target_vectorizer.adapt(train_raw.map(lambda context, target: target))

def preprocess_text(context, target):
    context = context_vectorizer(context).to_tensor()
    target = target_vectorizer(target)

    target_in = target[:,:-1].to_tensor()
    target_out = target[:,1:].to_tensor()
    
    if make_one_hot:
        target_out = tf.one_hot(
            target_out, 
            depth=tf.cast(
                target_vectorizer.vocabulary_size(), dtype='int32'
            )
        )
    return (context, target_in), target_out

train_ds = train_raw.map(preprocess_text, tf.data.autotune)
val_ds = val_raw.map(preprocess_text, tf.data.autotune)

def create_model(hsize):
    en_input_layer, enc_out, enc_states = encoder(hsize)
    de_input_layer, de_preds = decoder(hsize, enc_states)
    model = model(inputs=[en_input_layer, de_input_layer], outputs=de_preds)
    
    if make_one_hot:
        loss_fn = 'categorical_crossentropy'
    else:
        loss_fn = 'sparse_categorical_crossentropy'
    
    model.compile(
        optimizer='adam', 
        loss=loss_fn,
        metrics=[""acc""]
    )
    return model


model.fit(train_ds)
5s 24ms/step - loss: 6.7114 - acc: 0.003
<keras.callbacks.history at 0x7bfef0423f40>


reference
selecting loss and metrics for tensorflow model",https://stackoverflow.com/questions/77048121,python,05-09-2023 22:46,118.0,0.0,1.0,True,12-09-2023 22:01,12-09-2023 22:01
77555312,langchain / chromadb: why does vectorstore return so many duplicates?,"import os
from langchain.llms import openai
import bs4
import langchain
from langchain import hub
from langchain.document_loaders import unstructuredfileloader
from langchain.embeddings import openaiembeddings
from langchain.text_splitter import recursivecharactertextsplitter
from langchain.vectorstores import chroma

os.environ[""openai_api_key""] = ""key""

loader = unstructuredfileloader(
    'path_to_file'
)
docs = loader.load()

text_splitter = recursivecharactertextsplitter(
    chunk_size=1000, chunk_overlap=200, add_start_index=true
)
all_splits = text_splitter.split_documents(docs)
vectorstore = chroma.from_documents(documents=all_splits, embedding=openaiembeddings())
retriever = vectorstore.as_retriever(search_type=""similarity"", search_kwargs={""k"": 6})

retrieved_docs = retriever.get_relevant_documents(
    ""what is x?""
)

this returns:
[document(page_content=""..."", metadata={'source': 'path_to_text', 'start_index': 16932}),
 document(page_content=""..."", metadata={'source': 'path_to_text', 'start_index': 16932}),
 document(page_content=""..."", metadata={'source': 'path_to_text', 'start_index': 16932}),
 document(page_content=""..."", metadata={'source': 'path_to_text', 'start_index': 16932}),
 document(page_content=""..."", metadata={'source': 'path_to_text', 'start_index': 16932}),
 document(page_content=""..."", metadata={'source': 'path_to_text', 'start_index': 16932})]

which is all seemingly the same document.
when i first ran this code in google colab/jupyter notebook, it returned different documents...as i ran it more, it started returning the same documents. makes me feel like this is a database issue, where the same entry is being inserted into the database with each run.
how do i return 6 different unique documents?","['python', 'openai-api', 'langchain', 'py-langchain', 'chromadb']",77568404,"the issue is here:
chroma.from_documents(documents=all_splits, embedding=openaiembeddings())

everytime you execute the file, you are inserting the same documents into the database.
you could comment out that part of code if you are inserting from same file. or you could detect the similar vectors using embeddingsredundantfilter

filter that drops redundant documents by comparing their embeddings.",https://stackoverflow.com/questions/77555312,python,27-11-2023 08:01,4836.0,7.0,2.0,True,21-04-2024 04:11,29-11-2023 03:29
21361073,tokenize words in a list of sentences python,"i currently have a file that contains a list that is looks like 
example = ['mary had a little lamb' , 
           'jack went up the hill' , 
           'jill followed suit' ,    
           'i woke up suddenly' ,
           'it was a really bad dream...']

""example"" is a list of such sentences , and i want the output to look as :
mod_example = [""'mary' 'had' 'a' 'little' 'lamb'"" , 'jack' 'went' 'up' 'the' 'hill' ....]
and so on. 
i need the sentences to be separate with each word tokenized so that i can compare each word from a sentence of mod_example (at a time using for loop) with a reference sentence.
i tried this :
for sentence in example:
    text3 = sentence.split()
    print text3 

and got the follwing as output : 
['it', 'was', 'a', 'really', 'bad', 'dream...']

how do i get this for all the sentences? 
it keeps overwriting . and yes , also mention whether my approach is right?
this should remain a list of sentences with the words tokenized.. thanks","['python', 'python-2.7', 'text', 'nltk']",21361623,"you could use the word tokenizer in nltk ( with a list comprehension, see 
>>> from nltk.tokenize import word_tokenize
>>> example = ['mary had a little lamb' , 
...            'jack went up the hill' , 
...            'jill followed suit' ,    
...            'i woke up suddenly' ,
...            'it was a really bad dream...']
>>> tokenized_sents = [word_tokenize(i) for i in example]
>>> for i in tokenized_sents:
...     print i
... 
['mary', 'had', 'a', 'little', 'lamb']
['jack', 'went', 'up', 'the', 'hill']
['jill', 'followed', 'suit']
['i', 'woke', 'up', 'suddenly']
['it', 'was', 'a', 'really', 'bad', 'dream', '...']",https://stackoverflow.com/questions/21361073,python,26-01-2014 07:53,108845.0,16.0,8.0,True,18-10-2023 16:34,18-10-2023 16:34
75630689,how to clear map entry after certain time of inactivity,"i'm working on a chatbot powered by openai. i'm using the new gpt-3.5-turbo model with chatcompletion requests. i've already come up with a way for the bot to remember conversations for each individual user using a hashmap and it all works. the bot is able to remember all the previous responses and generate a new response using the history as context. however, the more responses in the query, the more tokens that it'll use, increasing costs. i want make it where if there is a certain amount of inactivity (like 2 minutes of not talking to the bot), it'll clear the history and start a fresh new conversation to save on tokens. what would be the best way to accomplish this?
here is my code:
private static openaiservice service;
    
private static map<string, list<chatmessage>> conversations = new hashmap<string, list<chatmessage>>();
            
public static void initializeopenai() {
    service = new openaiservice(settings.openai_access_key);
}

public static string generatechatresponse(user user, string prompt) {
    list<chatmessage> conversation;
        
    if (conversations.containskey(user.getid())) {
        conversation = conversations.get(user.getid());
    } else {
        conversation = new arraylist<chatmessage>();
    }
        
    if (conversation.size() < 1) {
        chatmessage system = new chatmessage();
        system.setrole(""system"");
        system.setcontent(""omnis is a chatbot with sarcasm"");
        conversation.add(system);
    }
        
    chatmessage userprompt = new chatmessage();
    userprompt.setrole(""user"");
    userprompt.setcontent(prompt);
    conversation.add(userprompt);
        
    chatcompletionrequest chatrequest = chatcompletionrequest.builder()
            .model(""gpt-3.5-turbo"")
            .maxtokens(1000)
            .temperature(0.9)
            .topp(0.3)
            .frequencypenalty(0.9)
            .presencepenalty(0.0)
            .messages(conversation)
            .build();
        
    chatcompletionresult chatresult = null;
        
    try {
        chatresult = service.createchatcompletion(chatrequest);
    } catch (exception e) {
        system.out.println(""an openai request failed!"");
            
        if (e.getmessage().contains(""timeout"")) {
            return ""your request timed out. please try again after a breif wait for try a different request."";
        }
            
        if (e.getmessage().contains(""rate limit"")) {
            return ""rate limit for chat requests has been reached!"";
        }
            
        return ""something went wrong with your request. cause of error is "" + e.getmessage();
    }
        
    if (chatresult != null) {
        system.out.println(""created chat completion request that used "" + chatresult.getusage().gettotaltokens() + "" tokens with "" + chatresult.getmodel());
    }
        
    string response = chatresult.getchoices().get(0).getmessage().getcontent();
        
    chatmessage assistantprompt = new chatmessage();
    assistantprompt.setrole(""assistant"");
    assistantprompt.setcontent(response);
    conversation.add(assistantprompt);
        
    conversations.put(user.getid(), conversation);
        
    return response;
}

the ""generatechatresonse"" method gets called each time a message is sent by a user. it get's the user's id and pulls the conversation from the hashmap if it exists, if not it creates a new conversation.
i don't know what to try.
i don't want to clear the entire hashmap, just the entry associated with the inactive user.","['java', 'openai-api']",75634654,"i've added guava to my project and replaced the map with a loadingcache object. it takes the same type of parameters and allows me to easily set the time before expiration.
private static loadingcache<string, list<chatmessage>> conversations;

public static void initializeopenai() {
    service = new openaiservice(settings.openai_access_key);
        
    conversations = cachebuilder.newbuilder()
            .maximumsize(10000)
            .expireafterwrite(2, timeunit.minutes)
            .build(
                new cacheloader<string, list<chatmessage>>() {
                    @override
                    public list<chatmessage> load(string key) throws exception {
                        return conversations.get(key);
                    }
                });
}

public static string generatechatresponse(user user, string prompt) {
    list<chatmessage> conversation = conversations.getifpresent(user.getid());
        
    if (conversation == null) {
        conversation = new arraylist<chatmessage>();
            
        chatmessage system = new chatmessage();
        system.setrole(""system"");
        system.setcontent(""omnis is a chatbot with sarcasm"");
            
        conversation.add(system);
    }

    // remaining code is unchanged
}",https://stackoverflow.com/questions/75630689,java,03-03-2023 18:32,549.0,2.0,2.0,True,04-03-2023 18:44,04-03-2023 18:44
77178370,how to retrieve source documents via langchain&#39;s get_relevant_documents method only if the answer is from the custom knowledge base,"i am making a chatbot which accesses an external knowledge base docs. i want to get the relevant documents the bot accessed for its answer, but this shouldn't be the case when the user input is something like ""hello"", ""how are you"", ""what's 2+2"", or any answer that is not retrieved from the external knowledge base docs. in this case, i want
retriever.get_relevant_documents(query) or any other line to return an empty list or something similar.
import os
from langchain.embeddings.openai import openaiembeddings
from langchain.vectorstores import faiss
from langchain.chains import conversationalretrievalchain 
from langchain.memory import conversationbuffermemory
from langchain.chat_models import chatopenai
from langchain.prompts import prompttemplate

os.environ['openai_api_key'] = ''

custom_template = """"""
this is conversation with a human. answer the questions you get based on the knowledge you have.
if you don't know the answer, just say that you don't, don't try to make up an answer.
chat history:
{chat_history}
follow up input: {question}
""""""
custom_question_prompt = prompttemplate.from_template(custom_template)

llm = chatopenai(
    model_name=""gpt-3.5-turbo"",  # name of the language model
    temperature=0  # parameter that controls the randomness of the generated responses
)

embeddings = openaiembeddings()

docs = [
    ""buildings are made out of brick"",
    ""buildings are made out of wood"",
    ""buildings are made out of stone"",
    ""buildings are made out of atoms"",
    ""buildings are made out of building materials"",
    ""cars are made out of metal"",
    ""cars are made out of plastic"",
  ]

vectorstore = faiss.from_texts(docs, embeddings)

retriever = vectorstore.as_retriever()

memory = conversationbuffermemory(memory_key=""chat_history"", return_messages=true)

qa = conversationalretrievalchain.from_llm(
    llm,
    retriever,
    condense_question_prompt=custom_question_prompt,
    memory=memory
)

query = ""what are cars made of?""
result = qa({""question"": query})
print(result)
print(retriever.get_relevant_documents(query))

i tried setting a threshold for the retriever but i still get relevant documents with high similarity scores. and in other user prompts where there is a relevant document, i do not get back any relevant documents.
retriever = vectorstore.as_retriever(search_type=""similarity_score_threshold"", search_kwargs={""score_threshold"": .9})","['python', 'openai-api', 'langchain', 'py-langchain']",77227773,"to solve this problem, i had to change the chain type to retrievalqa and introduce agents and tools.
import os
from langchain.embeddings.openai import openaiembeddings
from langchain.vectorstores import faiss
from langchain.chains import retrievalqa
from langchain.memory import conversationbuffermemory
from langchain.chat_models import chatopenai
from langchain.prompts import prompttemplate
from langchain.agents import agentexecutor, tool,initialize_agent
from langchain.agents.types import agenttype

os.environ['openai_api_key'] = ''

system_message = """"""
""you are the xyz bot.""
""this is conversation with a human. answer the questions you get based on the knowledge you have.""
""if you don't know the answer, just say that you don't, don't try to make up an answer.""
""""""

llm = chatopenai(
    model_name=""gpt-3.5-turbo"",  # name of the language model
    temperature=0  # parameter that controls the randomness of the generated responses
)

embeddings = openaiembeddings()

docs = [
    ""buildings are made out of brick"",
    ""buildings are made out of wood"",
    ""buildings are made out of stone"",
    ""buildings are made out of atoms"",
    ""buildings are made out of building materials"",
    ""cars are made out of metal"",
    ""cars are made out of plastic"",
  ]

vectorstore = faiss.from_texts(docs, embeddings)

retriever = vectorstore.as_retriever()

memory = conversationbuffermemory(memory_key=""chat_history"", input_key='input', return_messages=true, output_key='output')

qa = retrievalqa.from_chain_type(
        llm=llm,
        chain_type=""stuff"",
        retriever=vectorstore.as_retriever(),
        verbose=true,
        return_source_documents=true
    )

tools = [
        tool(
            name=""doc_search_tool"",
            func=qa,
            description=(
               ""this tool is used to retrieve information from the knowledge base""
            )
        )
    ]

agent = initialize_agent(
        agent = agenttype.chat_conversational_react_description,
        tools=tools,
        llm=llm,
        memory=memory,
        return_source_documents=true,
        return_intermediate_steps=true,
        agent_kwargs={""system_message"": system_message}
        )

query1 = ""what are buildings made of?""
result1 = agent(query1)


query2 = ""who are you?""
result2 = agent(query2)

if result accessed sources, it will have values for the key ""intermediate_steps"" then source documents can be accessed through result1[""intermediate_steps""][0][1][""source_documents""]
otherwise, when the query didn't need sources, result2[""intermediate_steps""] will be empty.",https://stackoverflow.com/questions/77178370,python,26-09-2023 08:41,19155.0,3.0,4.0,True,15-03-2024 20:45,29-09-2023 09:01
77862678,"simple python beginner chatgpt bot program bombs on compatibility issues, cannot find workaround","i am trying to learn both python and simple chatgpt api programming. the code i came up with was suggested by gpt itself, and in researching the problem i'm having, every code example i can find on the web is basically doing the same thing as the code i'm trying (below).
i started with a ""pip3 install openai"" and have no errors from that. it's when i run what should be the simplest code i've ever seen that the errors occur.
the errors i'm receiving are essentially telling me that ""openai.chatcompletion.create"" is deprecated, so i've tried the recommended alternative call, which is just ""openai.completion.create"". that fails too. both calls are deprecated according to the runtime warnings. unfortunately, every piece of sample code i can find on google uses one of these two methods.
i was given two suggestions by the runtime. one was run ""openai migrate"" which does nothing but give me some strange permissions error on my pictures folder of all things.
the more sensible suggestion was digging into the recommended openai api spec and it suggested using ""openai.chat.completions.create"". when i try that, i get a different error, this time suggesting i've sent too many requests to the api. this is utter nonsense -- i've made one and exactly one only call to the api to get the error message.
this should not be this hard. i'm running out of options. i've tried every suggested sample i can find; none work; the call i find in the latest api spec (unless i'm misinterpreting) also does not work.
any ideas would be appreciated.
import openai

def chat_with_gpt(api_key):
    openai.api_key = api_key

    # starting a new chat session
    session = openai.chatcompletion.create(
        model=""gpt-3.5-turbo"",  # or another model of your choice
        messages=[{""role"": ""system"", ""content"": ""you are a helpful assistant.""}]
    )

    while true:
        prompt = input(""prompt: "")
        if prompt == ""/quit"":
            break

        try:
            response = openai.chatcompletion.create(
                model=""gpt-3.5-turbo"",
                session_id=session[""id""],  # using the same session for continuity
                messages=[{""role"": ""user"", ""content"": prompt}]
            )
            print(response[""choices""][0][""message""][""content""])
        except exception as e:
            print(f""an error occurred: {e}"")

if __name__ == ""__main__"":
    api_key = ""mykey""  # i replace with my actual key
    chat_with_gpt(api_key)","['python', 'openai-api', 'chatgpt-api']",77997913,"i'm on the 'free tier' with openai and use my assisant program multiple times daily, but have never seen a 429 error before. i realize that's not helping you figure out why you were|are getting them. i agree with @larsks that the github openai repo has good working code examples. here is my assistant program that works well for my needs. i got much of the code from the openai repo. replace <botname> with a name for your assistant, if you like. :-)
#!/usr/bin/env python3
# openai chatgpt 3.5 chat client

import os
import sys
import openai
from openai import openai


intro = ""you are '<botname>' my helpful personal assitant. where possible, answer the following query with citations from a reference text.""

try:
    client = openai(api_key=os.environ['openai_api_key'])
except exception as e:
    print(e.__cause__)
    sys.exit(1)


print('how can i help you <your name>?\n')
while true:
    query = input(""> "")
    if query == ""done"":
        print('bye! -<botname>')
        break

    try:
        chat_completion = client.chat.completions.create(
                messages=[{""role"": ""user"", ""content"": intro+'query:""""""'+query+'""""""',}],
            model=""gpt-3.5-turbo"",
            temperature=0.2,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0
        )

    except openai.apiconnectionerror as e:
        print(""the server could not be reached"")
        print(e.__cause__)  # an underlying exception, likely raised within 

    except openai.ratelimiterror as e:
        print(""a 429 status code was received; we should back off a bit."")

    except openai.apistatuserror as e:
        print(""another non-200-range status code was received"")
        print(e.status_code)
        print(e.response)

    # print out our results
    print('<botname>:')
    print('',chat_completion.choices[0].message.content, '\n')",https://stackoverflow.com/questions/77862678,python,22-01-2024 20:49,393.0,0.0,2.0,True,28-02-2024 15:05,22-01-2024 22:29
75081879,dall-e api in node.js always returns 400 bad request,"i'm trying to implement openai's dall-e api in my next.js project but every reload returns 400 bad request.
i'm following the steps exactly as they are in the documentation but i still get this error.
im using next.js 13's app directory. this is my page.tsx file below. i'm calling the predict function in an async function.
const { configuration, openaiapi } = require(""openai"");

async function predict() {
  const configuration = new configuration({
    apikey: process.env.open_ai,
  });
  const openai = new openaiapi(configuration);

  const response = await openai.createimage(
    {
      prompt: ""a white siamese cat"",
    },
    {
      validatestatus: function (status: number) {
        return status < 500;
      },
    }
  );

  console.log(response);
}

export default async function home() {
  const response = await predict();

  return (
    <div classname=""flex h-full flex-col items-center justify-center px-8"">
      <h1>get a picture of a cat</h1>
    </div>
  );
}

this is the response
    status: 400,
    statustext: 'bad request',
    headers: {
      date: 'wed, 11 jan 2023 10:42:12 gmt',
      'content-type': 'application/json',
      'content-length': '172',
      connection: 'keep-alive',
      'access-control-allow-origin': '*',
      'openai-version': '2020-10-01',
      'openai-organization': 'user-***************',
      'x-request-id': '6f3b88d1c538d56b102d76d2f1dc6aee',
      'openai-processing-ms': '55',
      'strict-transport-security': 'max-age=15724800; includesubdomains'
    },","['node.js', 'next.js', 'openai-api']",75083205,i found out that my code is fine the issue was that my api key was under a trial license with no available money to spend! i added my card and now i'm receiving 200.,https://stackoverflow.com/questions/75081879,node.js,11-01-2023 10:51,784.0,0.0,1.0,True,11-01-2023 12:39,11-01-2023 11:48
73011992,filter the words that in the same sentence with other words python,"i have a dataframe, df, in the following form:
df = 
index   text 
----------------
0       my name is george and i don't like football 
1       hey find me on instagram to play basketball 
2       i am down for saturday night
...
1000    text1000

i have also one more list which contains some key words, the name of this list is,
key_word = [football, basketball, tennis]

i have also one another list which contains some usernames, like
usernames = [instagram, facebook, snapchat]

i want to find the usernames that are in the same sentence of dataframe, df, with the key words, key_words.
in my example the output should be the following list
output = [instagram]","['python', 'nlp']",73012159,"one approach using str.extract and str.contains:
key_words = [""football"", ""basketball"", ""tennis""]
usernames = [""instagram"", ""facebook"", ""snapchat""]

# create a boolean mask that contains the key_words and the usernames
mask = df[""text""].str.contains(""|"".join(key_words)) & df[""text""].str.contains(""|"".join(usernames))

# extract the usernames from those rows that contains both usernames and key_words
res = df.loc[mask, ""text""].str.extract(f'({""|"".join(usernames)})')
print(res)

output
           0
1  instagram

the expression:
""|"".join(key_words) # key_words can be any list of strings

creates a regular expression that will match a string that contains any of strings in the list.
for creating really efficient regular expressions for this type of problem see this.",https://stackoverflow.com/questions/73011992,python,17-07-2022 12:58,102.0,0.0,1.0,True,17-07-2022 13:32,17-07-2022 13:08
71372732,create a new line if cells equals a value from a dataframe,"using this dataframe:
            ##    doc_id paragraph_id sentence_id token_id       token      lemma  upos xpos
            ## 1    doc1            1           1        1 linguistics linguistic  noun  nns
            ## 2    doc1            1           1        2        also       also   adv   rb
            ## 3    doc1            1           1        3       deals       deal  noun  nns
            ## 4    doc1            1           2        1   something   something noun  nns
            ## 5    doc1            1           2        2        else        else noun  nns

i'd like to have something like this in a .txt file:
linguistic_nns also_r deal_nn
something_nn else_nn

except that using this code:
paste(text_anndf$lemma, ""_"", text_anndf$xpos, collapse = "" "", sep = """")

i have this:
linguistic_nn also_r deal_nn something_nn else_nn

because it doesn't take into account the ""sentence_id"" values. do i need to use a if or something similar ? thanks","['r', 'dataframe', 'lemmatization']",71373423,"something like this?
library(dplyr)
df %>%
    unite(col = lemma_and_position,
          lemma, xpos, sep = '_') %>%
    group_by(sentence_id) %>%
    summarise(lemma_and_position = paste(lemma_and_position, collapse = ' '))

re @stompers advice: providing test data with dput(your_data) is always helpful!",https://stackoverflow.com/questions/71372732,r,06-03-2022 17:39,44.0,0.0,1.0,True,06-03-2022 19:11,06-03-2022 19:01
17317418,stemmers vs lemmatizers,"natural language processing (nlp), especially for english, has evolved into the stage where stemming would become an archaic technology if ""perfect"" lemmatizers exist. it's because stemmers change the surface form of a word/token into some meaningless stems. 
then again the definition of the ""perfect"" lemmatizer is questionable because different nlp task would have required different level of lemmatization. e.g. convert words between verb/noun/adjective forms. 
stemmers 
[in]: having
[out]: hav

lemmatizers
[in]: having
[out]: have


so the question is, are english stemmers any useful at all today? since we have a plethora of lemmatization tools for english
if not, then how should we move on to build robust lemmatizers that
can take on nounify, verbify, adjectify and adverbify
preprocesses?
how could the lemmatization task be easily scaled to other languages
that have similar morphological structures as english?","['nlp', 'wordnet', 'stemming', 'text-analysis', 'lemmatization']",17320458,"q1: ""[..] are english stemmers any useful at all today? since we have a plethora of lemmatization tools for english""
yes. stemmers are much simpler, smaller, and usually faster than lemmatizers, and for many applications, their results are good enough. using a lemmatizer for that is a waste of resources. consider, for example, dimensionality reduction in information retrieval. you replace all drive/driving with driv in both the searched documents and the query. you do not care if it is drive or driv or x17a$ as long as it clusters inflectionally related words together.
q2: ""[..]how should we move on to build robust lemmatizers that can take on nounify, verbify, adjectify, and adverbify preprocesses?
what is your definition of a lemma, does it include derivation (drive - driver) or only inflection (drive - drives - drove)? does it take into account semantics?
if you want to include derivation (which most people would say includes verbing nouns etc.) then keep in mind that derivation is far more irregular than inflection. there are many idiosyncracies, gaps, etc. do you really want for to change (change trains) and change (as coins) to have the same lemma? if not, where do you draw the boundary? how about nerve - unnerve, earth -- unearth - earthling, ...  it really depends on the application.
if you take into account semantics (bank would be labeled as bank-money or bank-river depending on context), how deep do you go (do you distinguish bank-institution from bank-building)? some apps may not care about this at all, some might want to distinguish basic semantics, and some might want it fined-grained.
q3: ""how could the lemmatization task be easily scaled to other languages that have similar morphological structures as english?""
what do you mean by ""similar morphological structures as english""? english has very little inflectional morphology. there are good lemmatizers for languages of other morphological types (truly inflectional, agglutinative, template, ...).
with a possible exception of agglutinative languages, i would argue that a lookup table (say a compressed trie) is the best solution. (possibly with some backup rules for unknown words such as proper names). the lookup is followed by some kind of disambiguation (ranging from trivial - take the first one, or take the first one consistent with the words pos tag, to much more sophisticated). the more sophisticated disambiguations are usually supervised stochastical algorithms (e.g. treetagger or faster), although a combination of machine learning and manually created rules has been done too (see e.g. this).
obviously, for most languages, you do not want to create the lookup table by
hand, but instead, generate it from a description of the morphology of
that language. for inflectional languages, you can go the engineering
way of hajic for czech or mikheev for russian, or, if you are daring,
you use two-level morphology. or you can do something in between,
such as hana (myself) (note that these are all full
morphological analyzers that include lemmatization as one of their features). or you can learn
the lemmatizer in an unsupervised manner a la yarowsky and
wicentowski, possibly with manual post-processing, correcting the
most frequent words.
there are way too many options and it really all depends on what you want to do with the results.",https://stackoverflow.com/questions/17317418,nlp,26-06-2013 10:19,26086.0,81.0,4.0,True,14-01-2023 14:43,23-05-2017 12:02
72554328,how to fine tune fine tune github copilot?,"we can fine tune language models like bert, gpt-3.
can i fine tune github copilot model?
i have already looked into examples from  but cant find the details.
would really appreciate if someone had fine tuned github copilot.","['github', 'deep-learning', 'openai-api', 'gpt-3', 'github-copilot']",72718976,"there does not seem to be a client-facing feature allowing you to fine-tune copilot directly.
here are two illustration as to why this feature is, for now (q2 2022) missing.
the copilot feature page initially included this:

how will github copilot get better over time?
github copilot doesnï¿½ï¿½ï¿½t actually test the code it suggests, so the code may not even compile or run. github copilot can only hold a very limited context, so even single source files longer than a few hundred lines are clipped and only the immediately preceding context is used. and github copilot may suggest old or deprecated uses of libraries and languages. you can use the code anywhere, but you do so at your own risk.

as tomek korbak explains on twitter:

actually, copilot's completions will always be optimised for human's liking, not necessarily compiler's liking.
that's because the language model training objective (predicting the next token in text) is great at capturing short-term dependencies (which explains the human feel of generated snippets).
but it struggles to capture long-term, global, semantic properties of generated sequences such as compilability. and there's no easy way of including compilability as a signal for their training.
the standard way -- fine-tuning language models using rl with compilability as a reward -- notoriously leads to catastrophic forgetting: less diverse and less accurate completions.

tomek references ""energy-based models for code generation under compilability constraints (pdf)""


our solution (kl-dpg) boosts compilability rate of generated sequences from 55% to 70%.
rl fine-tuning can do better but at a cost of catastrophic forgetting.
overall, energy-based models (ebms) turn out to be great at expressing weird, sequence-level constraints that would be super hard as to express as normalised priors for autoregressive language models.
ebms provide a way of injecting our structured, symbolic knowledge into large language models without breaking them down or sacrificing their uncanny abilities.
the space of further applications in controllable generation is huge.

so not so easy.
tanishq mathew abraham explains in ""coding with github copilot""

i wonder if the github team might also develop a way of perhaps fine-tuning github copilot to specific use-cases.
for example, there may be a specific github copilot models for fastai, jax, etc. they would be fine-tuned on the source code of of these libraries and codebases that use these libraries.
but making sure that the tool does not provide outdated suggestions would still be a challenge.
i donï¿½ï¿½ï¿½t think it would be possible to provide suggestions for a brand-new library that does not have enough codebases using it to train on.
additionally, for situations like fastai where there are older apis and newer apis, when fine-tuning a model, the codebases using the older apis would have to be filtered out.
</blockq",https://stackoverflow.com/questions/72554328,github,09-06-2022 03:12,6527.0,4.0,3.0,True,18-01-2023 21:35,18-01-2023 21:35
69517460,bert get sentence embedding,"i am replicating code from this page. i have downloaded the bert model to my local system and getting sentence embedding.
i have around 500,000 sentences for which i need sentence embedding and it is taking a lot of time.

is there a way to expedite the process?
would sending batches of sentences rather than one sentence at a time help?

.
#!pip install transformers
import torch
import transformers
from transformers import berttokenizer, bertmodel
tokenizer = berttokenizer.from_pretrained('bert-base-uncased')
model = bertmodel.from_pretrained('bert-base-uncased',
                                  output_hidden_states = true, # whether the model returns all hidden-states.
                                  )

# put the model in ""evaluation"" mode, meaning feed-forward operation.
model.eval()

corpa=[""i am a boy"",""i live in a city""]



storage=[]#list to store all embeddings

for text in corpa:
    # add the special tokens.
    marked_text = ""[cls] "" + text + "" [sep]""

    # split the sentence into tokens.
    tokenized_text = tokenizer.tokenize(marked_text)

    # map the token strings to their vocabulary indeces.
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)

    segments_ids = [1] * len(tokenized_text)

    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensors = torch.tensor([segments_ids])

    # run the text through bert, and collect all of the hidden states produced
    # from all 12 layers. 
    with torch.no_grad():

        outputs = model(tokens_tensor, segments_tensors)

        # evaluating the model will return a different number of objects based on 
        # how it's  configured in the `from_pretrained` call earlier. in this case, 
        # becase we set `output_hidden_states = true`, the third item will be the 
        # hidden states from all layers. see the documentation for more details:
        # 
        hidden_states = outputs[2]


    # `hidden_states` has shape [13 x 1 x 22 x 768]

    # `token_vecs` is a tensor with shape [22 x 768]
    token_vecs = hidden_states[-2][0]

    # calculate the average of all 22 token vectors.
    sentence_embedding = torch.mean(token_vecs, dim=0)

    storage.append((text,sentence_embedding))

######update 1
i modified my code based upon the answer provided. it is not doing full batch processing
#!pip install transformers
import torch
import transformers
from transformers import berttokenizer, bertmodel
tokenizer = berttokenizer.from_pretrained('bert-base-uncased')
model = bertmodel.from_pretrained('bert-base-uncased',
                                  output_hidden_states = true, # whether the model returns all hidden-states.
                                  )

# put the model in ""evaluation"" mode, meaning feed-forward operation.
model.eval()

batch_sentences = [""hello i'm a single sentence"",
                    ""and another sentence"",
                    ""and the very very last one""]
encoded_inputs = tokenizer(batch_sentences)


storage=[]#list to store all embeddings
for i,text in enumerate(encoded_inputs['input_ids']):
    
    tokens_tensor = torch.tensor([encoded_inputs['input_ids'][i]])
    segments_tensors = torch.tensor([encoded_inputs['attention_mask'][i]])
    print (tokens_tensor)
    print (segments_tensors)

    # run the text through bert, and collect all of the hidden states produced
    # from all 12 layers. 
    with torch.no_grad():

        outputs = model(tokens_tensor, segments_tensors)

        # evaluating the model will return a different number of objects based on 
        # how it's  configured in the `from_pretrained` call earlier. in this case, 
        # becase we set `output_hidden_states = true`, the third item will be the 
        # hidden states from all layers. see the documentation for more details:
        # 
        hidden_states = outputs[2]


    # `hidden_states` has shape [13 x 1 x 22 x 768]

    # `token_vecs` is a tensor with shape [22 x 768]
    token_vecs = hidden_states[-2][0]

    # calculate the average of all 22 token vectors.
    sentence_embedding = torch.mean(token_vecs, dim=0)
    print (sentence_embedding[:10])
    storage.append((text,sentence_embedding))

i could update first 2 lines from the for loop to below. but they work only if all sentences have same length after tokenization
tokens_tensor = torch.tensor([encoded_inputs['input_ids']])
segments_tensors = torch.tensor([encoded_inputs['attention_mask']])

moreover in that case outputs = model(tokens_tensor, segments_tensors)  fails.
how could i fully perform batch processing in such case?","['python', 'nlp', 'huggingface-transformers', 'bert-language-model', 'huggingface-tokenizers']",69629902,"about your original question: there is not much you can do. bert is pretty computationally demanding algorithm. your best shot is to use berttokenizerfast instead of the regular berttokenizer. the ""fast"" version is much more efficient and you will see the difference for large amounts of text.
saying that, i have to warn you that averaging bert word embeddings  does not create good embeddings for the sentence. see this post. from your questions i assume you want to do some kind of semantic similarity search. try using one of those open-sourced models.",https://stackoverflow.com/questions/69517460,python,10-10-2021 17:32,12666.0,6.0,2.0,True,19-10-2021 11:23,11-10-2021 22:22
32666732,tools for calculating annotators agreement,"i am looking for a tool for calculating inter annotators agreement for two annotators in the case of multi-label classification. 
i was trying to use recal online tool, but it seems that recal doesn't support multi-label cllassification.
are you aware of any tool for multi-label classification?","['machine-learning', 'statistics', 'nlp', 'computer-science']",32678909,"look at  . however, it is not a 'ready-to-use tool', it is a library and it requires a bit of coding to fit your data into its data model.",https://stackoverflow.com/questions/32666732,machine-learning,19-09-2015 09:47,203.0,0.0,2.0,True,07-08-2022 04:24,21-09-2015 10:16
78793356,openai assistants api error: &quot;&#39;assistants&#39; has no attribute &#39;files&#39;&quot;,"i have the code below, which i have to integrate, but it's showing up that 'assistants' have no attribute 'file'. this code is around 8 months old, and i understand that the api probably changed since then, but i cannot find any alternatives to this in the documentation. does anyone have any experience migrating this code?
assistant_file = client.beta.assistants.files.create(
    assistant_id = st.session_state.assistant_id,
    file_id = st.session_state.file_id,
)","['python-3.x', 'openai-api', 'openai-assistants-api']",78793656,"the method you're trying to use (i.e., .files.create) doesn't exist.
moreover, this code wouldn't work even with the openai assistants api v1.
if you want to create an assistant, use the following code (works with the openai assistants api v2):
from openai import openai
client = openai()

my_assistant = client.beta.assistants.create(
    instructions=""you are a personal math tutor. when asked a question, write and run python code to answer the question."",
    name=""math tutor"",
    tools=[{""type"": ""code_interpreter""}],
    model=""gpt-4o"",
)

print(my_assistant)

if you want to create a file for an assistant, use the following code (works with the openai assistants api v2):
from openai import openai
client = openai()

my_file = client.files.create(
  file=open(""mydata.jsonl"", ""rb""),
  purpose=""assistants""
)

print(my_file)",https://stackoverflow.com/questions/78793356,python-3.x,25-07-2024 12:39,412.0,0.0,1.0,True,22-09-2024 08:06,21-09-2024 16:02
73096672,how do we predict 1 single datapoint with logisticregression,"i have been working on an nlp problem (text classification).
i first preprocessed the text, then trained a model on that data (after tfidf)
x_train, x_test, y_train, y_test = train_test_split(ref_red, uniqueoutput...)

clf = logisticregression(c = 0.9, penalty = 'l1', solver = 'liblinear') # grid search    
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)

print(accuracy_score(y_pred, y_test))

now, i want to try and test the model on a single datapoint (ml inference).
how do pass a single datapt to predict function?
please let me know.","['nlp', 'logistic-regression']",73097022,"the only thing to keep in mind is dimension agreement between training and inference data.
x, y = make_classification()
x_train, x_test, y_train, y_test = train_test_split(x, y)
dim = x_train.shape[-1]
clf = logisticregression(c = 0.9, penalty = 'l1', solver = 'liblinear') # grid search    
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
sample_count = 1
clf.predict(np.random.rand(sample_count, dim))",https://stackoverflow.com/questions/73096672,nlp,24-07-2022 07:58,66.0,1.0,1.0,True,24-07-2022 09:03,24-07-2022 08:25
78258373,get previous sentence while using spacy matcher,"i am running a spacy matcher line-by-line on a text file. my file has each text entry on a separate line. i am trying to extract 1) the matched instance, 2) the full sentence, and 3) the previous sentence. i am able to get the first two, but i am having trouble getting the previous sentence, given that there isn't a sentence index (from this post). here is my code:
with open('file.txt', 'r') as f:
    for line in iter(f.readline, ''):
        doc = nlp(line)
        matcher = matcher(nlp.vocab)
        matcher.add(""pattern_of_interest"", [pattern])
        matches = matcher(doc)
    
        for match_id, start, end in matches:
            string_id = nlp.vocab.strings[match_id]
            span = doc[start:end] 
            
        for sent in doc.sents:
            if matcher(sent):
                instances.append(pd.series({""instance"":str(span.text), 
                                        ""sentence"":str(sent.text),
                                        ""previous_sentence"":str(sent[-1].text)}))

i understand that the bolded part is giving me the previous token, not sentence (i tried to get around this with the list, but it doesn't work). any advice for retrieving the previous sentence would be greatly appreciated. thank you!","['python', 'nlp', 'spacy']",78259079,"adjustments:
track previous sentence: we now maintain a prev_sent variable that tracks the previous sentence as we iterate through all sentences in a document. matcher usage: we only need to create the matcher instance once, outside the loop through lines in the file, and then apply it to each sentence within the loop. this is more efficient than recreating it for every line. check for previous sentence: we handle cases where there might not be a previous sentence (e.g., the match is found in the first sentence of the document) by checking if prev_sent is none. if it is, we set the ""previous_sentence"" field to ""n/a"" or any placeholder text you find suitable.
import spacy
from spacy.matcher import matcher
import pandas as pd

# load the spacy model
nlp = spacy.load(""en_core_web_sm"")  # adjust model as necessary

# define your pattern here
pattern = [{""lower"": ""example""}]  # example pattern

# initialize matcher with the vocab
matcher = matcher(nlp.vocab)
matcher.add(""pattern_of_interest"", [pattern])

instances = []  # list to hold match details

with open('file.txt', 'r') as f:
    for line in iter(f.readline, ''):
        doc = nlp(line)
        
        prev_sent = none  # variable to keep track of the previous sentence
        for sent in doc.sents:
            matches = matcher(sent)
            if matches:
                for match_id, start, end in matches:
                    instance_text = sent[start:end].text  # the matched instance
                    current_sentence = sent.text
                    previous_sentence = prev_sent.text if prev_sent else ""n/a""  # handle the case where there's no previous sentence
                    
                    # append the extracted information to your instances list
                    instances.append(pd.series({""instance"": instance_text, 
                                                 ""sentence"": current_sentence,
                                                 ""previous_sentence"": previous_sentence}))
            prev_sent = sent  # update the previous sentence for the next iteration

# convert instances list to dataframe
df_instances = pd.dataframe(instances)
print(df_instances)",https://stackoverflow.com/questions/78258373,python,02-04-2024 00:14,50.0,1.0,1.0,True,02-04-2024 05:16,02-04-2024 00:35
77544292,how properly store and load own embeddings in redis vector db,"here is a simple code to use redis and embeddings but it's not clear how can i build and load own embeddings and then pull it from redis and use in search
from langchain.embeddings import openaiembeddings
from langchain.vectorstores.redis import redis

embeddings = openaiembeddings
metadata = [
    {
        ""user"": ""john"",
        ""age"": 18,
        ""job"": ""engineer"",
        ""credit_score"": ""high""
    }
]
texts = [""foo"", ""foo"", ""foo"", ""bar"", ""bar""]

rds = redis.from_texts(
    texts,
    embeddings,
    metadata,
    redis_url=""redis://localhost:6379"",
    index_name=""users"",
)

results = rds.similarity_search(""foo"")
print(results[0].page_content)

but i want to load a text from e.g. text file, create embedings and load into redis for later use. something like this:
from openai import openai
client = openai()

def get_embedding(text, model=""text-embedding-ada-002""):
    text = text.replace(""\n"", "" "")
    return client.embeddings.create(input = [text], model=model).data[0].embedding

does anyone have good example to implement this approach? also wondering about ttl for embedings in redis","['python', 'openai-api', 'langchain', 'large-language-model']",77562141,"helloï¿½ï¿½ï¿½ you can use the textloader to load txt and split it into documents!
just like below:
from langchain.embeddings import openaiembeddings
from langchain.vectorstores.redis import redis
from langchain.document_loaders import textloader
from langchain.embeddings.sentence_transformer import sentencetransformerembeddings
from langchain.text_splitter import charactertextsplitter


embeddings = openaiembeddings()

loader = textloader(""union.txt"", encoding=""utf-8"")

documents = loader.load()

text_splitter = charactertextsplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

vectorstore = redis.from_documents(
    docs,
    embeddings,
    redis_url=""redis://localhost:6379"",
    index_name=""users"",
)


results = rds.similarity_search_with_score(""he met the ukrainian people."")
print(results)
<",https://stackoverflow.com/questions/77544292,python,24-11-2023 15:44,1378.0,0.0,2.0,True,28-11-2023 08:01,27-11-2023 21:41
77064579,module &#39;numpy&#39; has no attribute &#39;_no_nep50_warning&#39;,"when load huggingfaceembeddings, always shows error like below.
---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-27-04d1583103a5> in <cell line: 2>()
      1 get_ipython().system('pip install --force-reinstall numpy==1.24.0')
----> 2 embeddings = huggingfaceembeddings(
      3     model_name=""oshizo/sbert-jsnli-luke-japanese-base-lite""
      4 )

12 frames
/usr/local/lib/python3.10/dist-packages/numpy/__init__.py in __getattr__(attr)
    309         """"""
    310         try:
--> 311             x = ones(2, dtype=float32)
    312             if not abs(x.dot(x) - float32(2.0)) < 1e-5:
    313                 raise assertionerror()

attributeerror: module 'numpy' has no attribute '_no_nep50_warning'

i tried to downgrade numpy version to
1.25.2 -> 1.24.4 -> 1.24.2 -> 1.24.1 -> 1.24.0

but always same errors happen. and googles has no answer about this.
if anyone know how to fixing it this problem... please tell me any information,
heres my pip installed source code.
numpy versions 1.24.0 was trying to downgrade version last time.
!pip install --force-reinstall numpy==1.24.0
from langchain.embeddings import huggingfaceembeddings
embeddings = huggingfaceembeddings(
    model_name=""intfloat/multilingual-e5-large""
)

heres my pip list below
package                          version
-------------------------------- ---------------------
absl-py                          1.4.0
accelerate                       0.22.0
aiohttp                          3.8.5
aiosignal                        1.3.1
alabaster                        0.7.13
albumentations                   1.3.1
altair                           4.2.2
annotated-types                  0.5.0
anyio                            3.7.1
appdirs                          1.4.4
argon2-cffi                      23.1.0
argon2-cffi-bindings             21.2.0
array-record                     0.4.1
arviz                            0.15.1
astropy                          5.3.2
astunparse                       1.6.3
async-timeout                    4.0.3
attrs                            23.1.0
audioread                        3.0.0
autograd                         1.6.2
babel                            2.12.1
backcall                         0.2.0
beautifulsoup4                   4.11.2
bleach                           6.0.0
blinker                          1.4
blis                             0.7.10
blosc2                           2.0.0
bokeh                            3.2.2
boto3                            1.28.43
botocore                         1.31.43
bqplot                           0.12.40
branca                           0.6.0
build                            1.0.0
cachecontrol                     0.13.1
cachetools                       5.3.1
catalogue                        2.0.9
certifi                          2023.7.22
cffi                             1.15.1
chardet                          5.2.0
charset-normalizer               3.2.0
chex                             0.1.7
click                            8.1.7
click-plugins                    1.1.1
cligj                            0.7.2
cloudpickle                      2.2.1
cmake                            3.27.2
cmdstanpy                        1.1.0
colorcet                         3.0.1
colorlover                       0.3.0
colour                           0.1.5
community                        1.0.0b1
confection                       0.1.1
cons                             0.4.6
contextlib2                      21.6.0
contourpy                        1.1.0
convertdate                      2.4.0
cryptography                     41.0.3
cufflinks                        0.17.3
cupy-cuda11x                     11.0.0
cvxopt                           1.3.2
cvxpy                            1.3.2
cycler                           0.11.0
cymem                            2.0.7
cython                           0.29.36
dask                             2023.8.1
dataclasses-json                 0.5.14
datascience                      0.17.6
db-dtypes                        1.1.1
dbus-python                      1.2.18
debugpy                          1.6.6
decorator                        4.4.2
defusedxml                       0.7.1
diskcache                        5.6.3
distributed                      2023.8.1
distro                           1.7.0
dlib                             19.24.2
dm-tree                          0.1.8
docutils                         0.18.1
dopamine-rl                      4.0.6
duckdb                           0.8.1
earthengine-api                  0.1.367
easydict                         1.10
ecos                             2.0.12
editdistance                     0.6.2
eerepr                           0.0.4
en-core-web-sm                   3.6.0
entrypoints                      0.4
ephem                            4.1.4
et-xmlfile                       1.1.0
etils                            1.4.1
etuples                          0.3.9
exceptiongroup                   1.1.3
faiss-gpu                        1.7.2
fastai                           2.7.12
fastcore                         1.5.29
fastdownload                     0.0.7
fastjsonschema                   2.18.0
fastprogress                     1.0.3
fastrlock                        0.8.2
filelock                         3.12.3
fiona                            1.9.4.post1
firebase-admin                   5.3.0
flask                            2.2.5
flatbuffers                      23.5.26
flax                             0.7.2
folium                           0.14.0
fonttools                        4.42.1
frozendict                       2.3.8
frozenlist                       1.4.0
fsspec                           2023.6.0
future                           0.18.3
gast                             0.4.0
gcsfs                            2023.6.0
gdal                             3.4.3
gdown                            4.6.6
geemap                           0.25.0
gensim                           4.3.2
geocoder                         1.38.1
geographiclib                    2.0
geopandas                        0.13.2
geopy                            2.3.0
gin-config                       0.5.0
glob2                            0.7
google                           2.0.3
google-api-core                  2.11.1
google-api-python-client         2.84.0
google-auth                      2.17.3
google-auth-             0.1.0
google-auth-oauthlib             1.0.0
google-cloud-bigquery            3.10.0
google-cloud-bigquery-connection 1.12.1
google-cloud-bigquery-storage    2.22.0
google-cloud-core                2.3.3
google-cloud-datastore           2.15.2
google-cloud-firestore           2.11.1
google-cloud-functions           1.13.2
google-cloud-language            2.9.1
google-cloud-storage             2.8.0
google-cloud-translate           3.11.3
google-colab                     1.0.0
google-crc32c                    1.5.0
google-pasta                     0.2.0
google-resumable-media           2.5.0
googleapis-common-protos         1.60.0
googledrivedownloader            0.4
graphviz                         0.20.1
greenlet                         2.0.2
grpc-google-iam-v1               0.12.6
grpcio                           1.57.0
grpcio-status                    1.48.2
gspread                          3.4.2
gspread-dataframe                3.3.1
gym                              0.25.2
gym-notices                      0.0.8
h5netcdf                         1.2.0
h5py                             3.9.0
holidays                         0.32
holoviews                        1.17.1
html5lib                         1.1
                       1.3.1
                         0.22.0
huggingface-hub                  0.16.4
humanize                         4.7.0
hyperopt                         0.2.7
idna                             3.4
imageio                          2.31.3
imageio-ffmpeg                   0.4.8
imagesize                        1.4.1
imbalanced-learn                 0.10.1
imgaug                           0.4.0
importlib-metadata               6.8.0
importlib-resources              6.0.1
imutils                          0.5.4
inflect                          7.0.0
iniconfig                        2.0.0
intel-openmp                     2023.2.0
ipyevents                        2.0.2
ipyfilechooser                   0.6.0
ipykernel                        5.5.6
ipyleaflet                       0.17.3
ipython                          7.34.0
ipython-genutils                 0.2.0
ipython-sql                      0.5.0
ipytree                          0.2.2
ipywidgets                       7.7.1
itsdangerous                     2.1.2
jax                              0.4.14
jaxlib                           0.4.14+cuda11.cudnn86
jeepney                          0.7.1
jieba                            0.42.1
jinja2                           3.1.2
jmespath                         1.0.1
joblib                           1.3.2
jsonpickle                       3.0.2
jsonschema                       4.19.0
jsonschema-specifications        2023.7.1
jupyter-client                   6.1.12
jupyter-console                  6.1.0
jupyter_core                     5.3.1
jupyter-server                   1.24.0
jupyterlab-pygments              0.2.2
jupyterlab-widgets               3.0.8
kaggle                           1.5.16
keras                            2.12.0
keyring                          23.5.0
kiwisolver                       1.4.5
langchain                        0.0.284
langcodes                        3.3.0
langsmith                        0.0.33
launchpadlib                     1.10.16
lazr.restfulclient               0.14.4
lazr.uri                         1.0.6
lazy_loader                      0.3
libclang                         16.0.6
librosa                          0.10.1
lightgbm                         4.0.0
linkify-it-py                    2.0.2
lit                              16.0.6
llama-cpp-python                 0.1.83
llvmlite                         0.39.1
locket                           1.0.0
logical-unification              0.4.6
lunarcalendar                    0.0.9
lxml                             4.9.3
markdown                         3.4.4
markdown-it-py                   3.0.0
markupsafe                       2.1.3
marshmallow                      3.20.1
matplotlib                       3.7.1
matplotlib-inline                0.1.6
matplotlib-venn                  0.11.9
mdit-py-plugins                  0.4.0
mdurl                            0.1.2
minikanren                       1.0.3
missingno                        0.5.2
mistune                          0.8.4
mizani                           0.9.3
mkl                              2023.2.0
ml-dtypes                        0.2.0
mlxtend                          0.22.0
more-itertools                   10.1.0
moviepy                          1.0.3
mpmath                           1.3.0
msgpack                          1.0.5
multidict                        6.0.4
multipledispatch                 1.0.0
multitasking                     0.0.11
murmurhash                       1.0.9
music21                          9.1.0
mypy-extensions                  1.0.0
natsort                          8.4.0
nbclassic                        1.0.0
nbclient                         0.8.0
nbconvert                        6.5.4
nbformat                         5.9.2
nest-asyncio                     1.5.7
networkx                         3.1
nibabel                          4.0.2
nltk                             3.8.1
notebook                         6.5.5
notebook_shim                    0.2.3
numba                            0.56.4
numexpr                          2.8.5
numpy                            1.24.0
oauth2client                     4.1.3
oauthlib                         3.2.2
openai                           0.28.0
opencv-contrib-python            4.8.0.76
opencv-python                    4.8.0.76
opencv-python-headless           4.8.0.76
openpyxl                         3.1.2
opt-einsum                       3.3.0
optax                            0.1.7
orbax-checkpoint                 0.3.5
osqp                             0.6.2.post8
packaging                        23.1
pandas                           1.5.3
pandas-datareader                0.10.0
pandas-gbq                       0.17.9
pandocfilters                    1.5.0
panel                            1.2.2
param                            1.13.0
parso                            0.8.3
partd                            1.4.0
pathlib                          1.0.1
pathy                            0.10.2
patsy                            0.5.3
pexpect                          4.8.0
pickleshare                      0.7.5
pillow                           9.4.0
pip                              23.2.1
pip-tools                        6.13.0
platformdirs                     3.10.0
plotly                           5.15.0
plotnine                         0.12.3
pluggy                           1.3.0
polars                           0.17.3
pooch                            1.7.0
portpicker                       1.5.2
prefetch-generator               1.0.3
preshed                          3.0.8
prettytable                      3.8.0
proglog                          0.1.10
progressbar2                     4.2.0
prometheus-client                0.17.1
promise                          2.3
prompt-toolkit                   3.0.39
prophet                          1.1.4
proto-plus                       1.22.3
protobuf                         3.20.3
psutil                           5.9.5
psycopg2                         2.9.7
ptyprocess                       0.7.0
py-cpuinfo                       9.0.0
py4j                             0.10.9.7
pyarrow                          9.0.0
pyasn1                           0.5.0
pyasn1-modules                   0.3.0
pycocotools                      2.0.7
pycparser                        2.21
pyct                             0.5.0
pydantic                         2.3.0
pydantic_core                    2.6.3
pydata-google-auth               1.8.2
pydot                            1.4.2
pydot-ng                         2.0.0
pydotplus                        2.0.2
pydrive                          1.3.1
pydrive2                         1.6.3
pyerfa                           2.0.0.3
pygame                           2.5.1
pygments                         2.16.1
pygobject                        3.42.1
pyjwt                            2.3.0
pymc                             5.7.2
pymeeus                          0.5.12
pymupdf                          1.23.3
pymupdfb                         1.23.3
pymystem3                        0.2.0
pyopengl                         3.1.7
pyopenssl                        23.2.0
pyparsing                        3.1.1
pyperclip                        1.8.2
pyproj                           3.6.0
pyproject_hooks                  1.0.0
pyshp                            2.3.1
pysocks                          1.7.1
pytensor                         2.14.2
pytest                           7.4.1
python-apt                       0.0.0
python-box                       7.1.1
python-dateutil                  2.8.2
python-louvain                   0.16
python-slugify                   8.0.1
python-utils                     3.7.0
pytz                             2023.3.post1
pyviz_comms                      3.0.0
pywavelets                       1.4.1
pyyaml                           6.0.1
pyzmq                            23.2.1
qdldl                            0.1.7.post0
qudida                           0.0.4
ratelim                          0.1.6
referencing                      0.30.2
regex                            2023.6.3
requests                         2.31.0
requests-oauthlib                1.3.1
requirements-parser              0.5.0
rich                             13.5.2
rpds-py                          0.10.2
rpy2                             3.4.2
rsa                              4.9
s3transfer                       0.6.2
safetensors                      0.3.3
scikit-image                     0.19.3
scikit-learn                     1.2.2
scipy                            1.10.1
scooby                           0.7.2
scs                              3.2.3
seaborn                          0.12.2
secretstorage                    3.3.1
send2trash                       1.8.2
sentence-transformers            2.2.2
sentencepiece                    0.1.99
setuptools                       67.7.2
shapely                          2.0.1
six                              1.16.0
sklearn-pandas                   2.2.0
smart-open                       6.3.0
sniffio                          1.3.0
snowballstemmer                  2.2.0
sortedcontainers                 2.4.0
soundfile                        0.12.1
soupsieve                        2.5
soxr                             0.3.6
spacy                            3.6.1
spacy-legacy                     3.0.12
spacy-loggers                    1.0.4
sphinx                           5.0.2
sphinxcontrib-applehelp          1.0.7
sphinxcontrib-devhelp            1.0.5
sphinxcontrib-htmlhelp           2.0.4
sphinxcontrib-jsmath             1.0.1
sphinxcontrib-qthelp             1.0.6
sphinxcontrib-serializinghtml    1.1.9
sqlalchemy                       2.0.20
sqlparse                         0.4.4
srsly                            2.4.7
statsmodels                      0.14.0
sympy                            1.12
tables                           3.8.0
tabulate                         0.9.0
tbb                              2021.10.0
tblib                            2.0.0
tenacity                         8.2.3
tensorboard                      2.12.3
tensorboard-data-server          0.7.1
tensorflow                       2.12.0
tensorflow-datasets              4.9.2
tensorflow-estimator             2.12.0
tensorflow-gcs-config            2.12.0
tensorflow-hub                   0.14.0
tensorflow-io-gcs-filesystem     0.33.0
tensorflow-metadata              1.14.0
tensorflow-probability           0.20.1
tensorstore                      0.1.41
termcolor                        2.3.0
terminado                        0.17.1
text-unidecode                   1.3
textblob                         0.17.1
tf-slim                          1.1.0
thinc                            8.1.12
threadpoolctl                    3.2.0
tifffile                         2023.8.30
tinycss2                         1.2.1
tokenizers                       0.13.3
toml                             0.10.2
tomli                            2.0.1
toolz                            0.12.0
torch                            2.0.1+cu118
torchaudio                       2.0.2+cu118
torchdata                        0.6.1
torchsummary                     1.5.1
torchtext                        0.15.2
torchvision                      0.15.2+cu118
tornado                          6.3.2
tqdm                             4.66.1
traitlets                        5.7.1
traittypes                       0.2.1
transformers                     4.33.1
triton                           2.0.0
tweepy                           4.13.0
typer                            0.9.0
types-setuptools                 68.1.0.1
typing_extensions                4.7.1
typing-inspect                   0.9.0
tzlocal                          5.0.1
uc-micro-py                      1.0.2
uritemplate                      4.1.1
urllib3                          1.26.16
vega-datasets                    0.9.0
wadllib                          1.3.6
wasabi                           1.1.2
wcwidth                          0.2.6
webcolors                        1.13
webencodings                     0.5.1
websocket-client                 1.6.2
werkzeug                         2.3.7
wheel                            0.41.2
widgetsnbextension               3.6.5
wordcloud                        1.9.2
wrapt                            1.14.1
xarray                           2023.7.0
xarray-einstats                  0.6.0
xgboost                          1.7.6
xlrd                             2.0.1
xyzservices                      2023.7.0
yarl                             1.9.2
yellowbrick                      1.5
yfinance                         0.2.28
zict                             3.0.0
zipp                             3.16.2","['python', 'embedding', 'langchain']",77064785,"open fresh collab editor and run each command
#this is minimum pre-requisites
pip install langchain
pip install sentence-transformers

code
 from langchain.embeddings import huggingfaceembeddings
    embeddings = huggingfaceembeddings(
  model_name=""intfloat/multilingual-e5-large""
)",https://stackoverflow.com/questions/77064579,python,08-09-2023 06:21,15618.0,4.0,3.0,True,04-04-2025 14:36,08-09-2023 06:40
76639815,can&#39;t get the text separated by words when i&#39;m doing data cleaning in nlp,"i'm trying to do an exercise of nlp in kaggle and when i'm doing the data cleaning of the text that i have to use to predict the output i can't get it to be separated by words, instead i get one sentence with all the words attached.
this is my text_cleaner function:
def text_cleaner(text):
    text = str(text).lower() #lowercase
    text = re.sub('\d+', '', text) #remove numbers
    text = re.sub('\[.*?\]','', text) #remove html tags
    text = re.sub(r' #remove url
    text = re.sub(r'\bhtml\b', '', text) #remove html word
    
    text = re.sub(r'['
                           u'\u0001f600-\u0001f64f'  # emoticons
                           u'\u0001f300-\u0001f5ff'  # symbols & pictographs
                           u'\u0001f680-\u0001f6ff'  # transport & map symbols
                           u'\u0001f1e0-\u0001f1ff'  # flags (ios)
                           u'\u00002702-\u000027b0' 
                           u'\u000024c2-\u0001f251'  #removes emojis
                           ']+', '',text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) #removes punctuation
    text = re.sub('[^a-z]','',text) #removes non-alphabeticals
    text = text.replace('#', '')
    text = text.replace('@', '')
    text = stop_words(text)
    
    return text

def stop_words(text):
    lem = wordnetlemmatizer()
    stop = set(stopwords.words('english'))
    stop.remove('not')
    punctuation = list(string.punctuation)
    stop.update(punctuation)
    
    text =text.split()
    text= [lem.lemmatize(word) for word in text if word not in stop]
    text = ' '.join(text)
    
    return text

and this is the result that i got:
ourdeedsarethereasonofthisearthquakemayallahfo...
instead of:
deed reason earthquake may allah forgive u...
thanks!","['regex', 'nlp', 'data-cleaning', 'nltokenizer']",76642798,"this line text = re.sub('[^a-z]','',text) #removes non-alphabeticals will remove everything except the lowercase characters a to z, including whitespaces.
if you replace it with re.sub('[^a-z ]','',text), so ""remove everything except a to z or spaces"", it should work.
also all of this:
text = re.sub(r'['
                           u'\u0001f600-\u0001f64f'  # emoticons
                           u'\u0001f300-\u0001f5ff'  # symbols & pictographs
                           u'\u0001f680-\u0001f6ff'  # transport & map symbols
                           u'\u0001f1e0-\u0001f1ff'  # flags (ios)
                           u'\u00002702-\u000027b0' 
                           u'\u000024c2-\u0001f251'  #removes emojis
                           ']+', '',text)
text = re.sub('[%s]' % re.escape(string.punctuation), '', text) #removes punctuation

and this:
text = text.replace('#', '')
text = text.replace('@', '')

will not do anything as all these lines do is removing certain single characters, but all of these characters are already removed by this re.sub('[^a-z ]','',text).",https://stackoverflow.com/questions/76639815,regex,07-07-2023 19:41,22.0,2.0,1.0,True,08-07-2023 11:55,08-07-2023 11:55
52799472,how to make an ai bot of natural language processing?,"i want to make ai bot which can understand only 4 words ""up"", ""down"", ""left"", ""right"".
as my friend make a python script which executes some task by the voice like to open youtube just say ""youtube"" and chrome browser will open with youtube.com url. but the system was slow as they were using google assistant/ai to process the voice which makes me feel impatient.
then i got an idea what if an ai system offline which understand only a few words and we can get some desired result and will be super fast.
for example:- i have a remote control car i want to make voice-activated as when i say ""up"" car should move forward, similarly for ""down"" -> backward, ""left"" -> left and ""right"" -> right & ""{any other voice}"" -> blink the led to tell that the system didn't understand
so:
how should i start?
how should i train the ai bot?
what should be my requirements?
and other thing that i should know.","['python', 'machine-learning', 'nlp', 'artificial-intelligence', 'opennlp']",52800090,"how should you start:  reading ;-) or i would recommend to take the coursera course on deep neural network. your question is extremely generic. 
an ad-hoc approach -- which should work on your problem -- could be to extract the audio spectrum from samples which are long enough to contain your words but not much longer. with these information you can train a convolutional neural network -- i would try 1d convolution first.",https://stackoverflow.com/questions/52799472,python,14-10-2018 04:30,342.0,-4.0,1.0,True,21-12-2024 13:00,21-12-2024 13:00
75146271,split dataframe text column with given strings - nlp,"i have a dataframe with a text column in this form:
column_description

""this section includes: animals: cats and dogs and vegetables but doesnï¿½ï¿½ï¿½t include: plants and fruits: coco""
""this section includes the following: axis: x and y but doesnï¿½ï¿½ï¿½t include: z, k and c""
""this section includes notably: letters: a, b and c however it doesnï¿½ï¿½ï¿½t include: y and letter: z""

i want to separate the text within the column and get two new columns like the following:
column_include

""animals: cats and dogs and vegetables""
""axis: x and y""
""letters: a, b and c ""

column_exclude

""plants and fruits: coco""
""z, k and c""
""y and letter: z""

how can i achieve this with python libraries? maybe using nl","['python', 'pandas', 'text', 'split', 'nlp']",75146327,"here is a generic regex that works on your 3 cases:
regex = r'this section includes[^:]*: (.*) (?:but|however it) doesnï¿½ï¿½ï¿½t include: (.*)'

df[['column_include', 'column_exclude']] = \
df['column_description'].str.extract(regex)

output:
                                  column_description                         column_include           column_exclude
0  this section includes: animals: cats and dogs ...  animals: cats and dogs and vegetables  plants and fruits: coco
1  this section includes the following: axis: x a...                          axis: x and y               z, k and c
2  this section includes notably: letters: a, b a...                    letters: a, b and c          y and letter: z

regex demo
if you want to make the second part optional, use a non-greedy quantifier (*?), an optional group ((?:...)) and an end of line anchor ($):
regex = r'^this section\s*(?:includes[^:]*: (.*?))?\s*(?:(?:but|however it)? doesnï¿½ï¿½ï¿½t include: (.*))?$'

regex demo",https://stackoverflow.com/questions/75146271,python,17-01-2023 12:14,107.0,-1.0,2.0,True,17-01-2023 17:19,17-01-2023 12:15
50659889,unable to detect gibberish names using python,"i am trying to build python model that could classify account names as either legitimate or gibberish. capitalization is not important in this particular case as some legitimate account names could be comprised of all upper-case or all lower-case letters. 
disclaimer: this is just a internal research/experiment and no real action will be taken on the classifier outcome. 
in my particular, there are 2 possible characteristics that can reveal an account name as suspicious, gibberish or both:

weird/random spelling in name or name consists of purely or mostly numbers. examples of account names that fit these criteria are: 128, 127, h4rugz4sx383a6n64hpo, tt, t66, t65, asdfds.
the name has 2 components (let's assume that no name will ever have more than 2 components) and the spelling and pronounciation of the 2 components are very similar. examples of account names that fit these criteria are: jala haja, hata yaha, faja kaja.

if an account name meets both of the above criteria (i.e. 'asdfs lsdfs', '332 333') it should also be considered suspicious.  
on the other hand, a legitimate account name doesn't need to have both first name and last name. they are usually names from popular languages such as roman/latin (i.e. spanish, german, portuguese, french, english), chinese, and japanese. 
examples of legitimate account names include (these names are made up but do reflect similar styles of legitimate account names in real world): michael, sara, jose colmenares, dimitar, jose rafael, morgan, eduardo medina, luis r. mendez, hikaru, selenia, zhang ming, xuting liu, chen zheng.
i've seen some slightly similar questions on stackoverflow that asks for ways to detect gibberish texts. but those don't fit my situation because legitimate texts and words actually have meanings, whereas human names usually don't. i also want to be able to do it just based on account names and nothing else.  
right now my script takes care of finding the 2nd characteristic of suspicious account names (similar components in name) using python's fuzzy wuzzy package and using 50% as the similarity threshold. the script is listed below:
from fuzzywuzzy import fuzz
from fuzzywuzzy import process

import pandas as pd
import numpy as np

accounts = pd.read_csv('dataset_with_names.csv', encoding = 'iso-8859-1', sep=none, engine='python').replace(np.nan, 'blank', regex=true)

pd.options.mode.chained_assignment = none

accounts.columns = ['name', 'email', 'akon_id', 'acct_creation_date', 'first_time_city', 'first_time_ip', 'label']

accounts['name_simplified']=accounts['name'].str.replace('[^\w\s]','')
accounts['name_simplified']=accounts['name_simplified'].str.lower()

sim_name = []

for index, row in accounts.iterrows():        
    if ' ' in row['name_simplified']:
        row['name_simplified']=row['name_simplified'].split()
        if len(row['name_simplified']) > 1:
            #print(row['name_simplified'])
            if fuzz.ratio(row['name_simplified'][0], row['name_simplified'][1]) >= 50:
                sim_name.append('true')
            else:
                sim_name.append('false')
        else:
            sim_name.append('false')
    else:
        sim_name.append('false')        

accounts['are_name_components_similar'] = sim_name 

the result has been reliable for what the script was designed to do, but i also want to be able to surface gibberish account names with the 1st characteristic (weird/random spelling or name consists of purely or mostly numbers). so far i have not found a solution to that yet. 
can anyone help? any feedback/suggestion will be greatly appreciated!","['python', 'machine-learning', 'nlp', 'classification', 'fuzzywuzzy']",50661395,"for the 1st characteristic, you can train a character-based n-gram language model, and treat all names with low average per-character probability as suspicious.
a quick-and-dirty example of such language model is below. it is a mixture of 1-gram, 2-gram and 3-gram language models, trained on a brown corpus. i am sure you can find more relevant training data (e.g. list of all names of actors).
from nltk.corpus import brown
from collections import counter
import numpy as np

text = '\n  '.join([' '.join([w for w in s]) for s in brown.sents()])

unigrams = counter(text)
bigrams = counter(text[i:(i+2)] for i in range(len(text)-2))
trigrams = counter(text[i:(i+3)] for i in range(len(text)-3))

weights = [0.001, 0.01, 0.989]

def strangeness(text):
    r = 0
    text = '  ' + text + '\n'
    for i in range(2, len(text)):
        char = text[i]
        context1 = text[(i-1):i]
        context2 = text[(i-2):i]
        num = unigrams[char] * weights[0] + bigrams[context1+char] * weights[1] + trigrams[context2+char] * weights[2] 
        den = sum(unigrams.values()) * weights[0] + unigrams[context1] * weights[1] + bigrams[context2] * weights[2]
        r -= np.log(num / den)
    return r / (len(text) - 2)

now you can apply this strangeness measure to your examples.
t1 = '128, 127, h4rugz4sx383a6n64hpo, tt, t66, t65, asdfds'.split(', ')
t2 = 'michael, sara, jose colmenares, dimitar, jose rafael, morgan, eduardo medina, luis r. mendez, hikaru, selenia, zhang ming, xuting liu, chen zheng'.split(', ')
for t in t1 + t2:
    print('{:20} -> {:9.5}'.format(t, strangeness(t)))

you see that gibberish names are in most cases more ""strange"" than normal ones. you could use for example a threshold of 3.9 here.
128                  ->    5.5528
127                  ->    5.6572
h4rugz4sx383a6n64hpo ->    5.9016
tt                   ->    4.9392
t66                  ->    6.9673
t65                  ->    6.8501
asdfds               ->    3.9776
michael              ->    3.3598
sara                 ->    3.8171
jose colmenares      ->    2.9539
dimitar              ->    3.4602
jose rafael          ->    3.4604
morgan               ->    3.3628
eduardo medina       ->    3.2586
luis r. mendez       ->     3.566
hikaru               ->    3.8936
selenia              ->    6.1829
zhang ming           ->    3.4809
xuting liu           ->    3.7161
chen zheng           ->    3.6212

of course, a simpler solution is to collect a list of popular names in all your target languages and use no machine learning at all - just lookups.",https://stackoverflow.com/questions/50659889,python,02-06-2018 18:21,2457.0,1.0,1.0,True,31-05-2023 09:56,31-05-2023 09:56
61825698,how to specify number of target classes for tfrobertasequenceclassification?,"i have a text classification task at hand and i want to use roberta pre-trained model from the transformers python library.
as per the documentation of tfrobertaforsequenceclassification to train we have to use,
from transformers import robertatokenizer, tfrobertaforsequenceclassification

tokenizer = robertatokenizer.from_pretrained('roberta-base')
model = tfrobertaforsequenceclassification.from_pretrained('roberta-base')

model.compile('adam', loss='sparse_categorical_crossentropy')
model.fit(x, y)

so where should i specify the number of target labels for sequence classification?","['python', 'machine-learning', 'deep-learning', 'huggingface-transformers', 'huggingface']",61828910,"you can use num_labels parameter.
model = tfrobertaforsequenceclassification.from_pretrained('roberta-base', num_labels = 5)

ref:",https://stackoverflow.com/questions/61825698,python,15-05-2020 18:07,1540.0,4.0,1.0,True,06-09-2023 08:43,06-09-2023 08:43
69575841,how to correctly pass a split function to textvectorization layer,"i'm defining a custom split callable for textvectorization like this:
import tensorflow as tf
from tensorflow import keras
@tf.function
def split_slash(input_str):
  return tf.strings.split(input_str, sep=""/"")
inputs = [""text/that/has/a"",""lot/of/slashes/inside"",""for/testing/purposes/foo""]
input_text_processor = keras.layers.textvectorization(max_tokens=13, split = split_slash)
    
input_text_processor.adapt(inputs)
example_tokens = input_text_processor(inputs)
print(example_tokens)
for x in inputs:
  print(split_slash(x))

resulting in:
tf.tensor(
[[2]
 [3]
 [4]], shape=(3, 1), dtype=int64)
tf.tensor([b'text' b'that' b'has' b'a'], shape=(4,), dtype=string)
tf.tensor([b'lot' b'of' b'slashes' b'inside'], shape=(4,), dtype=string)
tf.tensor([b'for' b'testing' b'purposes' b'foo'], shape=(4,), dtype=string)

as seen above the split function is working correctly outside of the textvectorization layer but failes when passed as a callable","['python', 'tensorflow', 'keras', 'nlp']",69576873,"your split_slash function does not seem to properly tokenize the phrases.
print(f""vocabulary:\t\t{input_text_processor.get_vocabulary()}"")
'''
vocabulary:['', 
           '[unk]', 
           'textthathasa', 
           'lotofslashesinside', 
           'fortestingpurposesfoo']
'''

it is probably because your textvectorization layer strips your phrases of all punctuation including / by default before your split_slash function is called. setting standardize=none in your textvectorization layer will do the trick for you.
alternatively, you could also try the following snippet.
import tensorflow as tf

def custom_standardization(input_data):
  return tf.strings.regex_replace(input_data, '/', ' ')

inputs = [""text/that/has/a"",""lot/of/slashes/inside"",""for/testing/purposes/foo""]

input_text_processor = tf.keras.layers.textvectorization(max_tokens=13, standardize=custom_standardization) #split = split_slash)

input_text_processor.adapt(inputs)
print(f""vocabulary:\t\t{input_text_processor.get_vocabulary()}"")
example_tokens = input_text_processor(inputs)

print(example_tokens)
for x in inputs:
  print(split_slash(x))

note that your phrases are split on whitespace by default after removing your slashes.
'''
vocabulary:     ['', '[unk]', 'that', 'text', 'testing', 'slashes', 'purposes', 'of', 'lot', 'inside', 'has', 'for', 'foo']
tf.tensor(
[[ 3  2 10  1]
 [ 8  7  5  9]
 [11  4  6 12]], shape=(3, 4), dtype=int64)
tf.tensor([b'text' b'that' b'has' b'a'], shape=(4,), dtype=string)
tf.tensor([b'lot' b'of' b'slashes' b'inside'], shape=(4,), dtype=string)
tf.tensor([b'for' b'testing' b'purposes' b'foo'], shape=(4,), dtype=string)
'''

for more information, check out the documentation.",https://stackoverflow.com/questions/69575841,python,14-10-2021 18:43,1405.0,0.0,1.0,True,27-06-2022 07:36,27-06-2022 07:36
78307073,"langchain agent parsing error with structured_chat_agent and wikipedia tool, handle_parsing_errors hits limit","i am trying to ask gpt 4 to use wikipedia for a prompt, using agents and tools via langchain.
the difficulty i'm running into is the book i've been using, developing apps with gpt-4 and chatgpt: build intelligent chatbots, content generators, and more, while published in 2023, already has code examples that are deprecated.
for example, i am trying to do something similar to the code provided on page 114 of that book:
from langchain.chat_models import chatopenai
from langchain.agents import load_tools, initialize_agent, agenttype llm = chatopenai(model_name=""gpt-3.5-turbo"", temperature=0)
tools = load_tools([""wikipedia"", ""llm-math""], llm=llm)
agent = initialize_agent(
tools, llm, agent=agenttype.zero_shot_react_description, verbose=true )
    question = """"""what is the square root of the population of the capital of the
    country where the olympic games were held in 2016?""""""
    agent.run(question)

i see much of this is deprecated (e.g., initialize_agent), so i have looked around stackoverflow, github, and the langchain python documents to come up with this:
from langchain_openai import chatopenai
from langchain_core.output_parsers import stroutputparser
from langchain_core.prompts import chatprompttemplate
from langchain.agents import (
  load_tools, create_structured_chat_agent, agentexecutor
)

model = chatopenai(model=""gpt-4"", temperature=0)
tools = load_tools([""wikipedia""])
prompt = chatprompttemplate.from_template(
  """"""
  you are a research assistant, and your job is to retrieve information about
  movies and movie directors.
  
  use the following tool: {tools}
  
  use the following format:

  question: the input question you must answer
  thought: you should always think about what to do
  action: the action to take, should be one of [{tool_names}]
  action input: the input to the action
  observation: the result of the action
  ... (this thought/action/action input/observation can repeat n times)
  thought: i now know the final answer
  final answer: the final answer to the original input question. you only
  need to give the number, no other information or explanation is necessary.

  begin!

  question: how many movies did the director of the {year} movie {name} direct
  before they made {name}?
  thought: {agent_scratchpad}
  """"""
)
agent = create_structured_chat_agent(model, tools, prompt)
agent_executor = agentexecutor(agent=agent, tools=tools)
agent_executor.invoke({""year"": ""1991"", ""name"": ""thelma and louise""})

i'm going to be running this through a loop of many movies, so i'd like it to only return one integer (in this case, 6). but it seems like i need to give it that full thought process prompt; i can't get it to run if i don't include {tools}, {tool_names}, and {agent_scratchpad} in the prompt (per this github post).
the frustrating thing is i eventually do get the correct answer, but note that it is throwing an error:
valueerror: an output parsing error occurred. in order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=true` to the agentexecutor. this is the error: could not parse llm output: first, i need to find out who directed the movie ""thelma and louise"" in 1991. 
  action: wikipedia
  action input: {'query': 'thelma and louise'}
  observation: 
  ""thelma & louise"" is a 1991 american female buddy road film directed by ridley scott and written by callie khouri. it stars geena davis as thelma and susan sarandon as louise, two friends who embark on a road trip with unforeseen consequences. the film became a critical and commercial success, receiving six academy award nominations and winning one for best original screenplay for khouri. scott was nominated for best director.
  thought: 
  ridley scott directed the movie ""thelma and louise"". now i need to find out how many movies he directed before this one.
  action: wikipedia
  action input: {'query': 'ridley scott filmography'}
  observation: 
  ridley scott is an english filmmaker. following his commercial breakthrough with the science fiction horror film alien (1979), his best known works are the neo-noir dystopian science fiction film blade runner (1982), historical drama gladiator (2000), and science fiction film the martian (2015). scott has directed more than 25 films and is known for his atmospheric, highly concentrated visual style. his films are also known for their strong female characters. here is a list of his films before ""thelma & louise"": 
  1. the duellists (1977)
  2. alien (1979)
  3. blade runner (1982)
  4. legend (1985)
  5. someone to watch over me (1987)
  6. black rain (1989)
  thought: 
  ridley scott directed six movies before ""thelma and louise"".
  final answer: 6

this seems to be very common (here, and here, and also here, and lastly here).
so, i do what it tells me (see docs also) and update my agentexecutor to:
agent_executor = agentexecutor(
  agent=agent, 
  tools=tools,
  handle_parsing_errors=true
)

and that returns:
{'year': '1991', 'name': 'thelma and louise', 'output': 'agent stopped due to iteration limit or time limit.'}

my question: how can i use langchain to combine gpt 4 and wikipedia to get an answer to a query, when all i want back is an integer?","['python', 'nlp', 'openai-api', 'langchain', 'large-language-model']",78316386,"author of the book developing apps with gpt-4 and chatgpt here, i already answered by mail, but just in case someone else stumbles upon this question... you can find updated code at 
the updated code looks like this:
from langchain_openai import chatopenai
from langchain.agents import load_tools, create_react_agent, agentexecutor
from langchain import hub

llm = chatopenai(model_name=""gpt-3.5-turbo"")
tools = load_tools([""wikipedia"", ""llm-math""], llm=llm)
agent = create_react_agent(
    tools=tools,
    llm=llm,
    prompt = hub.pull(""hwchase17/react""),
)
question = ""...""
agent_executor = agentexecutor(agent=agent, tools=tools, verbose=true)
agent_executor.invoke({""input"": question})

hope this helps.",https://stackoverflow.com/questions/78307073,python,10-04-2024 20:52,2640.0,0.0,2.0,True,12-04-2024 12:46,10-04-2024 21:00
41348621,ssl error downloading nltk data,"i am trying to download nltk 3.0 for use with python 3.6 on mac os x 10.7.5, but am getting an ssl error:
import nltk
nltk.download()


i downloaded nltk with a pip3 command: sudo pip3 install -u nltk.
changing the index in the nltk downloader allows the downloader to show all of nltk's files, but when one tries to download all, one gets another ssl error (see bottom of photo):

i am relatively new to computer science and am not at all savvy with respect to ssl.
my question is how to simply resolve this issue?

here is a similar question by a user who is having the same problem:
unable to download nltk data
i decided to post a new question with screenshots, since my edit to that other question was rejected.
similar questions which i did not find helpful:
nltk download ssl: certificate verify failed
downloading error using nltk.download()","['python', 'macos', 'ssl', 'ssl-certificate', 'nltk']",42890688,"you don't need to disable ssl checking if you run the following terminal command:
/applications/python 3.6/install certificates.command

in the place of 3.6, put your version of python if it's an earlier one. then you should be able to open your python interpreter (using the command python3) and successfully run nltk.download() there.
this is an issue wherein urllib uses an embedded version of openssl that not in the system certificate store. here's an answer with more information on what's going on.",https://stackoverflow.com/questions/41348621,python,27-12-2016 16:22,62224.0,78.0,5.0,True,16-11-2024 15:15,20-06-2020 09:12
78307693,openai api error: &quot;the model gpt-3.5 does not exist or you do not have access to it&quot;,"i have the following code:
def speakgpt():
    aiactive = true
    while aiactive == true:
        if query[0] == 'deactivate':
            aiactive = false
        else:
            completion = openai.chatcompletion.create(model=""gpt-3.5"", messages=[{""role"": ""user"", ""content"": query}])
            text = completion.choices[0].message.content
            gptresult = (gtts(text=text, lang=lang, slow=false, tld=""com.scot""))
            speak(gptresult)

running this results in the following error:

you tried to access openai.chatcompletion, but this is no longer
supported in openai>=1.0.0 - see the readme at
 for the api.
you can run openai migrate to automatically upgrade your codebase to
use the 1.0.0 interface.
alternatively, you can pin your installation to the old version, e.g.
pip install openai==0.28
a detailed migration guide is available here:


when i downgrade and use a version before 1.0.0, it gives the following error:

the model gpt-3.5 does not exist or you do not have access to it.

i don't know how to run openai migrate, which gives this error when run:

the term 'openai' is not recognized as the name of a cmdlet, function,
script file, or operable program. check the spelling of the name, or
if a path was included, verify that the path is correct and try again.

i can't figure out from the migration guide what my code should be changed to.
another post i found said to use openai.chat.completions.create in place of openai.chatcompletions.create but that gives the same error:

the model gpt-3.5 does not exist or you do not have access to it.","['python', 'python-3.x', 'openai-api', 'chatgpt-api']",78311108,"your code will start working if you solve both problems you currently have with it.
problem 1: you're using an incorrect method name
what openai migrate will do is change this...
openai.chatcompletion.create

...to this.
openai.chat.completions.create

you can also change this manually.

problem 2: you have a typo in the openai model name
change gpt-3.5 to gpt-3.5-turbo. the gpt-3.5 model doesn't exist. see all available openai models in the official openai documentation.",https://stackoverflow.com/questions/78307693,python,11-04-2024 00:26,4340.0,0.0,1.0,True,15-04-2024 09:50,15-04-2024 09:49
76313091,should i pass word2vec and fasttext vectors separately or concatenate them for deep learning model in smart contract vulnerability detection?,"i have been working with word embedding latly, i have a question. so, here consider taking vulnerability detection in smart contract. so the input is smart contract files labeled with 0 or 1 stating vulnerable or not. now i m performing 2 different word embedding such as word2vec and fasttext with same input. my question is, it is right to concatenate the vectors of word2vec and fasttext and then fed as input to deep learning model. or should i pass the vectors of the word embedding models separately to the deep learning model for feature extraction and then concatenate the extracted features for classification.
so far, i have performed word embedding using word2vec and passed the vectors obtained to cnn and performed fasttext and passed it to bigru model and concatenated the extracted features. my question is can i concatenate the vectors before performing feature extraction using deep learning ? but i m afraid that the concatenation of 2 word embedding models with same input will cause confusion ? that is the same input words will have two different vectors when concatenated. i m so confused. if anybody have insight kindly help. thanks in advance.","['python', 'deep-learning', 'word2vec', 'word-embedding', 'fasttext']",76317297,"the generic answer for when you don't know which of multiple different ideas is better, youy try them each separately & see which evaluates as better on your robust, repeatable evaluations.
(if you don't have a way to evaluate which is better, that's a bigger & more foundational thing to address than any other choices.)
given what you've said, other observations:
the word2vec & fasttext algorithms are very similar, with the most experience supporting their use being in the fuzzy sorts of menaings inherent in natural-language text. and, the main advantage of fasttext is in being able to synthesize better-than-nothing guess-vectors for words that weren't seen during training, but might be similar in substrings that hint their meaning to other known words.
smart contract source code (or bytecode) is sufficiently unlike natural language, in its narrow vocabulary, token frequencies, purposes, & rigorous execution model that it's not immediately clear word-vectors could help. word-vectors often have been useful with language-like token-sets that aren't natural-language, but even there, usually for discovering gradations of meaning. with smart contracts, the difference between ""works as hoped"" and ""fatally vulnerable"" may just be a tiny matter of a single misplaced operation, or subtle missed error case. those are the kind of highly contextual, ordering-based outcomes that word-vectors simply do not model. (at best, i think you might discover that competent coders tend to use mroe of certain kinds of operations or names than incompetent ones.)
further, the main advantage of fasttext ï¿½ï¿½ï¿½ synthesizing vectors for unknown but morphologically-similar tokens ï¿½ï¿½ï¿½ may be far less relevant for bytecode-analysis, where unknown tokens are rare or even impossible. (maybe, if you're analyzing source-code including freely chosen variable names, new unknown variable names will have hints of relations to previously-trained names.)",https://stackoverflow.com/questions/76313091,python,23-05-2023 09:14,157.0,-1.0,1.0,True,23-05-2023 17:28,23-05-2023 10:48
74136361,how to extract only the desired text using for loop,"so i have list of tibetan words with their pos tag as shown below:
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ det
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ part
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ no_pos
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ part
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ noun

how can i strip","['python', 'python-3.x', 'loops', 'for-loop', 'nlp']",74136407,"[x.split("" "")[0] for x in list_]

returns
['ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½', 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½', 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½', 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½', 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½']

if you want printing then this:
for x in list_:
    print(x.spli",https://stackoverflow.com/questions/74136361,python,20-10-2022 08:04,70.0,1.0,1.0,True,20-10-2022 08:09,20-10-2022 08:09
77791575,allow anonymous access to my azure openai chat bot,"i have an azure openai chat bot using my own data (i configured an openai resource and chose deploy as web app) . members of my domain can access it by logging in. now i want it to be accessible anonymously or for other domains i define - whatever is easier to configure.
in the azure web app authentication settings i could do two things:

disable authentication
allow unauthenticated access

see this screenshot:

when i disable authentication or when i allow unauthenticated access i get in both cases the same  error message when opening the chat bot:

authentication not configured

i also tried to allow not only my single domain but multiple account types of the identity provider. but when i login with a different domain i still the an error messge:

aadsts50020: user account 'myuser@otherdomain' from identity provider ' does not exist in tenant 'mydomain' and cannot access the application 'mychatbot') in that tenant.

how to do it properly?","['azure', 'openai-api', 'azure-webapps', 'azure-openai']",77831181,"context
you have to understand that the ""deploy to web app"" button in azure openai studio is only an accelerator for the ""all-in-one"" deployment of a project which is in fact open-sourced by microsoft.
it's this one: 
so the authentication not configured message you are facing is implemented inside this project, here: 
possible solution
the readme of the project explains that you can remove it if you want, here
to remove this restriction, you can add auth_enabled=false to the environment variables. this will disable authentication and allow anyone to access the chat functionality of your app. this is not recommended for production apps.

as said, ""this is not recommended for production apps."".
especially as the frontend is calling itself for the conversation (post to /conversation) so if you remove the authentication, i guess that someone could use this endpoint directly to consume your openai resource which is behind (ok, they can't modify your system message etc. but it's still an issue)",https://stackoverflow.com/questions/77791575,azure,10-01-2024 07:32,1106.0,0.0,2.0,True,17-01-2024 09:12,10-01-2024 07:56
77505030,"openai api error: &quot;you tried to access openai.chatcompletion, but this is no longer supported in openai&gt;=1.0.0&quot;","i am currently working on a chatbot, and as i am using windows 11 it does not let me migrate to newer openai library or downgrade it. could i replace the chatcompletion function with something else to work on my version?
this is the code:
import openai

openai.api_key = ""private""

def chat_gpt(prompt):
    response = openai.chatcompletion.create(
        model=""gpt-3.5-turbo"",
        messages=[{""role"": ""user"", ""content"": prompt}]
    )
    return response.choices[0].message['content'].strip()

if __name__ == ""__main__"":
    while true:
        user_input = input(""you: "")
        if user_input.lower() in [""quit"", ""exit"", ""bye""]:
            break
        response = chat_gpt(user_input)
        print(""bot:"", response)

and this is the full error:

...
you tried to access openai.chatcompletion, but this is no longer supported in openai>=1.0.0 - see the readme at  for the api.
you can run openai migrate to automatically upgrade your codebase to use the 1.0.0 interface.
alternatively, you can pin your installation to the old version, e.g. <pip install openai==0.28>
a detailed migration guide is available here: 

i tried both upgrading and downgrading through pip.","['python', 'pip', 'artificial-intelligence', 'openai-api', 'chatgpt-api']",77505042,"try updating to the latest and using:
from openai import openai

client = openai(
    # defaults to os.environ.get(""openai_api_key"")
    api_key=""private"",
)

def chat_gpt(prompt):
    response = client.chat.completions.create(
        model=""gpt-3.5-turbo"",
        messages=[{""role"": ""user"", ""content"": prompt}]
    )
    return response.choices[0].message.content.strip()

link
edit: message.['content'] -> message.content on the return of this function, as a message object is not subscriptable error is thrown while using message.['content']. also, update link from pointing to the readme (subject to change) to migration guide specific to this code.",https://stackoverflow.com/questions/77505030,python,17-11-2023 23:09,76520.0,21.0,2.0,True,11-12-2024 20:33,18-11-2023 08:59
75488104,create datasets based on authors from another dataset,"i have a dataset in the following format

       text          author        title 
     -------------------------------------

dt =   text0         author0       title0
       text1         author1       title1
         .             .              .
         .             .              .
         .             .              .  

and i would like to create different separate datasets which contain only texts of one author. for example the dataset names dt1 contains the texts of the author1, the dt2 contains texts of the author2, etc.
i would be grateful if you could help me with this using python.
update:
dt = 
            text                                     author        title
-------------------------------------------------------------------------
0   i would like to go to the beach                   george       beach
1   i was in park few days ago                        nick         park
2   i would like to go in uni                         peter        university
3   i have be in the airport at 8                     maria        airport","['python', 'nlp']",75488581,"please try, this is what i understand you require.
import pandas as pd

data = {
    'text' : ['text0', 'text1', 'text2'],
    'author': ['author0', 'author1', 'author1'],
    'title': ['comunicaciï¿½ï¿½n', 'administraciï¿½ï¿½n', 'ventas']
}

df = pd.dataframe(data)
df1 = df[df[""author""]==""author0""]

df2 = df[df[""author""]==""author1""]
print(df1)
print(df2)

update:
import pandas as pd

data = {
    'text' : ['text0', 'text1', 'text2'],
    'author': ['author0', 'author1', 'author1'],
    'title': ['comunicaciï¿½ï¿½n', 'administraciï¿½ï¿½n', 'ventas']
}

df = pd.dataframe(data)
df1 = df[df[""author""]==""author0""]

df2 = df[df[""author""]==""author1""]

list_author = df['author'].unique().tolist()

for x in list_author:
  a = df[df[""author""]==x]
  print(a",https://stackoverflow.com/questions/75488104,python,17-02-2023 18:35,58.0,0.0,1.0,True,17-02-2023 22:33,17-02-2023 18:43
55304845,title extraction/identification from pdfs,"i have a large number of pdfs in different formats. among other things, i need to extract their titles (not the document name, but a title in the text). due to the range of formats, the titles are not in the same locations in the pdfs. further, some of the pdfs are actually scanned images (i need to use ocr/optical character recognition on them). the titles are sometimes one line, sometimes 2. they do not tend to have the same set of words. in the range of physical locations the titles usually show up, there are often other words (ie if doc 1 has title 1 at x1, y1, doc 2 might have title 2 at x2, y2 but have other non-title text at x1 y1). further, there are some very rare cases where the pdfs don't have a title.
so far i can use pdftotext to extract text within a given bounding box, and convert it to a text file. if there's a title, this lets me capture the title, but often with other extraneous words included. this also only works on non-image pdfs. i'm wondering if a) there's a good way to identify the title from among all the words i extract for a document (because there are often extraneous words), ideally with a good way to identify that no title exists, and b) if there are any tools that are equivalent to pdftotext that will also work on scanned images (i do have an ocr script working, but it does ocr over an entire image rather than a section of one). 
one method that somewhat answers the title dilemma is to extract the words in the bounding box, use the rest of the document to identify which of the bounding box words are keywords for the document, and construct the title from the keywords. this wouldn't extract the actual title, but may give words that could construct a reasonable alternative. i'm already extracting keywords for other parts of the project, but i would definitely prefer to extract the actual title as people may be using the verbatim title for lookup purposes. 
further note if it wasn't clear - i'm trying to do this programatically with open source/free tools, ideally in python, and i will have a large number of documents (10,000+).","['python', 'pdf', 'nlp', 'ocr', 'pdf-scraping']",55305507,"for people who are come across this question later, i'll provide a quick update on what i've decided to do (albeit i haven't tested accuracy so i don't know if this approach is actually any good). 
the overall approach i'll be using is machine learning via a neural net (i'll report back on accuracy once i have it). i'm essentially taking the first 200 words of a document, and generating n-grams of 4-20 sequential words (so ~16*200 n-grams of words; 4 b.c. none of my titles are shorter, 20 same but longer). i then generate a unique feature vector from each n-gram, the features i decided to use are partially dependent on my text but some are more general like ""is the first letter of the first word in the n-gram capitalized?"". knowing the correct titles, i can turn them into an equivalent vector. so if vec(n_gram) = vec(correct_title) then output 1, otherwise output 0. i'm using this to train an ml model. currently this does not solve my issue of scanned image pdfs, unless they're first converted into text documents. it also assumes word order is preserved among the title words when the pdf is turned into the n-grams. i have noticed the order of non-title words isn't always preserved by conversion but thats quite a rare problem and only seems to occur when there's line breaks and then the entire line is out of place (so it shouldn't affect the titles hopefully).",https://stackoverflow.com/questions/55304845,python,22-03-2019 17:23,5949.0,3.0,3.0,True,18-08-2023 21:39,22-03-2019 18:21
76084296,openai chat completions api error: &quot;invalid url (post /v1/engines/gpt-3.5-turbo/chat/completions)&quot;,"i'm using openai to learn more about api integration, but i keep running into this code when running the python program. i asked chatgpt about the invalid url (post /v1/engines/gpt-3.5-turbo/chat/completions) error, but it didn't seem to give me the right solutions.
note: i do have the latest openai package installed (i.e., 0.27.4).
code:
import os
import openai
openai.api_key = ""sk-xxxxxxxxxxxxxxxxxxxx""

messages = [
    {""role"": ""system"", ""content"": ""you are a helpful assistant.""},
    {""role"": ""user"", ""content"": ""tell me a joke.""}
]

response = openai.chatcompletion.create(
    engine=""gpt-3.5-turbo"",
    messages=messages,
    max_tokens=50,
    n=1,
    stop=none,
    temperature=0.7,
)

joke = response.choices[0].text.strip()
print(joke)","['python', 'python-3.x', 'openai-api', 'chatgpt-api']",76090137,"problem
the chatgpt api (i.e., the gpt-3.5 api) has a model parameter (required). the engine parameter is not a valid parameter for the /v1/chat/completions api endpoint. see the official openai documentation.
solution
change this...
engine = ""gpt-3.5-turbo""

...to this.
model = ""gpt-3.5-turbo""


also, change this...
joke = response.choices[0].text.strip()

...to this.
joke = response['choices'][0]['message']['content']",https://stackoverflow.com/questions/76084296,python,23-04-2023 10:21,2009.0,1.0,1.0,True,12-06-2024 17:03,12-06-2024 17:03
72992870,custom text pre-processing saved in tensorflow model,"how to write custom text pre-processing that could be saved as part of a model?
suppose that i would like to have two features:

auto-correct string input with some function. words might change after this operation
do query expansion of string input, such that outcome text/tokens might contain few additional words(for which weights would be trained).

something like this:

fli to london -> fly to london

fly to london -> fly to london loc_city
-> this token would need to be in vocabulary in advance, which could be done


after steps 1 and/or 2, feed the result to textvectorisation / embedding layer ?
there is standardize callback, but i do not see obvious way of doing that with existing tf.string operations.
ideally, there is a callback function / layer which accepts string(or tokens) and maps to another string(or string tokens).","['python', 'tensorflow', 'keras', 'nlp']",72993273,"you can get the first character of a string like this:
import tensorflow as tf

class stringlayer(tf.keras.layers.layer):
  def __init__(self):
    super(stringlayer, self).__init__()

  def call(self, inputs):
    return tf.squeeze(tf.strings.bytes_split(inputs), axis=1).to_tensor()[:, 0]

s = tf.constant([['next_string'], ['some_string']])
layer = stringlayer()
print(layer(s))
# tf.tensor([b'n' b's'], shape=(2,), dtype=string)",https://stackoverflow.com/questions/72992870,python,15-07-2022 10:51,75.0,2.0,1.0,True,15-07-2022 11:36,15-07-2022 11:36
54495502,how to get all words from spacy vocab?,"i need all the words from spacy vocab. suppose, i initialize my spacy model as 
nlp = spacy.load('en')

how do i get the text of words from nlp.vocab?","['python-3.x', 'nlp', 'spacy']",54510426,"you can get it as a list like this:
list(nlp.vocab.strings)",https://stackoverflow.com/questions/54495502,python-3.x,02-02-2019 17:18,14253.0,19.0,2.0,True,07-05-2024 13:58,26-01-2020 19:36
70609579,use quantization on huggingface transformers models,"i'm learning quantization, and am experimenting with section 1 of this notebook.
i want to use this code on my own models.
hypothetically, i only need to assign to model variable in section 1.2

# load model
model = bertforsequenceclassification.from_pretrained(configs.output_dir)
model.to(configs.device)

my models are from a different library: from transformers import pipeline. so .to() throws an attributeerror.
my model:
pip install transformers

from transformers import pipeline

unmasker = pipeline('fill-mask', model='bert-base-uncased')
model = unmasker(""hello i'm a [mask] model."")

output:
some weights of the model checkpoint at bert-base-uncased were not used when initializing bertformaskedlm: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- this is expected if you are initializing bertformaskedlm from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a bertforsequenceclassification model from a bertforpretraining model).
- this is not expected if you are initializing bertformaskedlm from the checkpoint of a model that you expect to be exactly identical (initializing a bertforsequenceclassification model from a bertforsequenceclassification model).


how might i run the linked quantization code on my example model?
please let me know if there's anything else i should clarify in this post.","['python', 'deep-learning', 'huggingface-transformers', 'bert-language-model', 'quantization']",70619775,"the pipeline approach won't work for quantisation as we need the models to be returned. you can however, use pipeline for testing the original models for timing etc.

quantisation code:
token_logits contains the tensors of the quantised model.
you could place a for-loop around this code, and replace model_name with string from a list.
model_name = bert-base-uncased
tokenizer = autotokenizer.from_pretrained(model_name )
model = automodelformaskedlm.from_pretrained(model_name)
    
sequence = ""distilled models are smaller than the models they mimic. using them instead of the large "" \
f""versions would help {tokenizer.mask_token} our carbon footprint.""

inputs = tokenizer(sequence, return_tensors=""pt"")
mask_token_index = torch.where(inputs[""input_ids""] == tokenizer.mask_token_id)[1]
    
token_logits = model(**inputs).logits

# <- can stop here

source",https://stackoverflow.com/questions/70609579,python,06-01-2022 15:36,2526.0,1.0,1.0,True,07-01-2022 10:22,06-01-2022 16:04
70308315,text preprocessing for nlp but from list of dictionaries,"i'm attempting to do an nlp project with a goodreads data set. my data set is a list of dictionaries. each dictionary looks like so (the list is called 'reviews'):
>>> reviews[0]
{'user_id': '8842281e1d1347389f2ab93d60773d4d',
'book_id': '23310161',
'review_id': 'f4b4b050f4be00e9283c92a814af2670',
'rating': 4,
'review_text': 'fun sequel to the original.',
'date_added': 'tue nov 17 11:37:35 -0800 2015',
'date_updated': 'tue nov 17 11:38:05 -0800 2015',
'read_at': '',
'started_at': '',
'n_votes': 7,
'n_comments': 0}

there are 700k+ of these dictionaries in my dataset.
first question: i am only interested in the elements 'rating' and 'review_text'. i know i can delete elements from each dictionary, but how do i do it for all of the dictionaries?
second question: i am able to do sentence and word tokenization of an individual dictionary in the list by specifying the dictionary in the list, then the element 'review_text' within the dictionary like so:
paragraph = reviews[0]['review_text']

and then applying sent_tokenize and word_tokenize like so:
print(sent_tokenize(paragraph))
print(word_tokenize(paragraph))

but how do i apply these methods to the entire data set? i am stuck here, and cannot even attempt to do any of the text preprocessing (lower casing, removing punctuation, lemmatizing, etc).
tia","['python', 'list', 'dictionary', 'text', 'nlp']",70308624,"to answer the first question, you can simply put them into dataframe with only your interesting columns (i.e. rating and review_text). this is to avoid looping and managing them record by record and is also easy to be manipulated on the further processes.
after you came up with the dataframe, use apply to preprocess (e.g. lower, tokenize, remove punctuation, lemmatize, and stem) your text column and generate new column named tokens that store the preprocessed text (i.e. tokens). this is to satisfy the second question.
from nltk import sent_tokenize, word_tokenize
from nltk.stem import wordnetlemmatizer
from nltk.stem import porterstemmer
import string

punc_list = list(string.punctuation)
porter = porterstemmer()    
lemmatizer = wordnetlemmatizer()

def text_processing(row):
    all_words = list()
    # sentence tokenize
    for sent in sent_tokenize(row['review_text']):
        # lower words and tokenize
        words = word_tokenize(sent.lower())
        # lemmatize
        words_lem = [lemmatizer.lemmatize(w) for w in words]
        # remove punctuation
        used_words = [w for w in words_lem if w not in punc_list]
        # stem
        words_stem = [porter.stem(w) for w in used_words]
        all_words += words_stem
    return all_words

# create dataframe from list of dicts (select only interesting columns)
df = pd.dataframe(reviews, columns=['user_id', 'rating', 'review_text'])

df['tokens'] = df.apply(lambda x: text_processing(x), axis=1)
print(df.head())

example of output:

  user_id  rating              review_text                        tokens
0       1       4        fun sequel to the        [fun, sequel, to, the]
1       2       2  it was a slippery slope  [it, wa, a, slipperi, slope]
2       3       3     the trick to getting         [the, trick, to, get]
3       4       3           the bird had a           [the, bird, had, a]
4       5       5      that dog likes cats        [that, dog, like, cat]


finally, if you donï¿½ï¿½ï¿½t prefer dataframe, you can export it as other formats such as csv (to_csv), json (to_json), and list of dicts (to_dict('records')).
hope this would hel",https://stackoverflow.com/questions/70308315,python,10-12-2021 17:29,638.0,1.0,1.0,True,11-12-2021 02:11,10-12-2021 17:33
51663068,tensorflow.js tokenizer,"i'm new to machine learning and tensorflow, since i don't know python so i decide to use there javascript version (maybe more like a wrapper). 
the problem is i tried to build a model that process the natural language. so the first step is tokenizer the text in order to feed the data to model. i did a lot research, but most of them are using python version of tensorflow that use method like: tf.keras.preprocessing.text.tokenizer which i can't find similar in tensorflow.js. i'm stuck in this step and don't know how can i transfer text to vector that can feed to model. please help :)","['javascript', 'machine-learning', 'tensorflow.js', 'nlp']",51664311,"to transform text to vectors, there are lots of ways to do it, all depending on the use case. the most intuitive one, is the one using the term frequency, i.e , given the vocabulary of the corpus (all the words possible), all text document will be represented as a vector where each entry represents the occurrence of the word in text document.
with this vocabulary :
[""machine"", ""learning"", ""is"", ""a"", ""new"", ""field"", ""in"", ""computer"", ""science""]

the following text: 
[""machine"", ""is"", ""a"", ""field"", ""machine"", ""is"", ""is""] 

will be transformed as this vector: 
[2, 0, 3, 1, 0, 1, 0, 0, 0] 

one of the disadvantage of this technique is that there might be lots of 0 in the vector which has the same size as the vocabulary of the corpus. that is why there are others techniques. however the bag of words is often referred to. and there is a slight different version of it using tf.idf


const vocabulary = [""machine"", ""learning"", ""is"", ""a"", ""new"", ""field"", ""in"", ""computer"", ""science""]
const text = [""machine"", ""is"", ""a"", ""field"", ""machine"", ""is"", ""is""] 
const parse = (t) => vocabulary.map((w, i) => t.reduce((a, b) => b === w ? ++a : a , 0))
console.log(parse(text))



there is also the following module that might help to achieve what you want",https://stackoverflow.com/questions/51663068,javascript,02-08-2018 22:40,7539.0,11.0,2.0,True,04-05-2024 15:49,03-08-2018 08:15
47818669,difference between rasa core and rasa nlu,"i tried to understand the difference between rasa core and rasa nlu from the official documentation, but i don't understand much. what i understood is that rasa core is used to guide the flow of the conversation, while rasa nlu is used to process the text to extract information (entities).
there are examples to build chatbots in rasa core as well as rasa nlu. i couldn't understand what the difference in the two approaches is and when to adopt one instead of the other approach.
could you please help me to understand this better?","['nlp', 'artificial-intelligence', 'chatbot', 'rasa-nlu', 'rasa-core']",47819500,"you got it right. both work together but they have distinct goals. in simple terms, rasa core handles the conversation flow, utterances, actions and rasa nlu extract entities and intents. 
about your second question: 
the first example shows the entire workflow to create the bot, it shows how to setup the domain and the stories. those are features from rasa core, not rasa nlu. at item 2 on this example (called define an interpreter) the author explicitly said he is making use of rasa nlu as the interpreter (but you could be even using another entity extractor framework).
the second example (the rasa nlu one) shows how to train the entity and intent extractor only. you don't have any information about domains and stories, no information about the conversational flow, it is a pure nlu example (even though he is using the default run method from rasa core to run the bot).
when i started studying rasa was a bit hard to understand the concepts to develop the bots. but as you start coding it got clear. no matter which platforms you use, nlu will be handling entity and intents while the conversational flow will be something else. 
it is even possible to use one library to handle the core of your bot and another one to handle nlu.
i would like to note that different from the most tools you can use to build the core of your bot, rasa core use machine learning to better generalize the dialogue flow. instead of write code for each possible node on your conversation, you can use a dataset of possible conversational paths and train the core to generalize it. this is a very cool and powerful feature :)
hope it helps.",https://stackoverflow.com/questions/47818669,nlp,14-12-2017 17:05,9393.0,32.0,5.0,True,25-03-2022 10:35,16-07-2021 11:02
70532485,error when importing pytorch (the filename or extension is too long),"i'm using anconda to run my transformers project locally in google colab.
i've created a new environment (tf_gpu) and installed (supposedly) everything i need.
and everything works fine, but when i try to simply import pytorch, this error appears:
[winerror 206] the filename or extension is too long: 'c:\\users\\34662\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\lib'

when clearly the path is not long enough to trigger this error.
my python version is 3.8, and my gpu is a nvidia geforce gtx 1650, so it shouldn't be a gpu problem
does anybody knows why this happens?
any help is good at this point, i don't know how to solve this.
here i leave a screenshot of the complete error message
thank you in advance.","['python', 'tensorflow', 'pytorch', 'torch', 'huggingface-transformers']",70532889,your problem is that the error ist not a too long path error it is a file not found error which mean that pytorch is not correctly installed,https://stackoverflow.com/questions/70532485,python,30-12-2021 13:44,740.0,1.0,1.0,True,30-12-2021 16:27,30-12-2021 16:27
76130589,what is the function of the `text_target` parameter in huggingface&#39;s `autotokenizer`?,"i'm following the guide here: 
there is one line in the guide like this:
labels = tokenizer(text_target=examples[""summary""], max_length=128, truncation=true)

i don't understand the function of the text_target parameter.
i tried the following code and the last two lines gave exactly the same results.
from transformers import autotokenizer
tokenizer = autotokenizer.from_pretrained('t5-small')
text = ""weiter verhandlung in syrien.""
tokenizer(text_target=text, max_length=128, truncation=true)
tokenizer(text, max_length=128, truncation=true)

the docs just say text_target (str, list[str], list[list[str]], optional) ï¿½ï¿½ï¿½ the sequence or batch of sequences to be encoded as target texts. i don't really understand. is there some situations when setting  text_target will give you a different result","['python', 'huggingface-transformers', 'huggingface']",76167575,"sometimes it is necessary to look at the code:
if text is none and text_target is none:
    raise valueerror(""you need to specify either `text` or `text_target`."")
if text is not none:
    # the context manager will send the inputs as normal texts and not text_target, but we shouldn't change the
    # input mode in this case.
    if not self._in_target_context_manager:
        self._switch_to_input_mode()
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
if text_target is not none:
    self._switch_to_target_mode()
    target_encodings = self._call_one(text=text_target, text_pair=text_pair_target, **all_kwargs)
# leave back tokenizer in input mode
self._switch_to_input_mode()

if text_target is none:
    return encodings
elif text is none:
    return target_encodings
else:
    encodings[""labels""] = target_encodings[""input_ids""]
    return encodings

as you can see in the above snippet, both text and text_target are passed to self._call_one() to encode them (note that text_target is passed as the text parameter). that means the encoding of the same string as text or text_target will be identical as long as _switch_to_target_mode() doesn't do anything special.
the conditions at the end of the function answer your question:

when you only provide text you will retrieve the encoding of it.
when you only provide text_target you will retrieve the encoding of it.
when you provide text and text_target you will retrieve the encoding of text and the token ids of text_target as the value of the labels key.

to be honest, i think the implementation is a bit unintuitive. i would expect that passing the text_target would return an object that only contains the labels key. i assume that they wanted to keep their output objects and the respective documentation simple and therefore went for this implementation. or there is a model where it actually makes sense that i am unaware of.",https://stackoverflow.com/questions/76130589,python,28-04-2023 14:27,9271.0,4.0,2.0,True,08-11-2023 13:34,03-05-2023 19:58
76691477,trouble getting my gradient descent algorithm to converge (word2vec),"just for the sake of practising, learning and experimenting, i made a word2vec model from scratch, using the formulas and algorithm of jurafsky & martin. here is the code:
class word2vec:
    def __init__(self, context_window=2, num_dimensions=5, num_negative_samples=2, normalizer_func=none):
        self.num_dimensions = num_dimensions
        self.context_window = context_window
        self.num_negative_samples = num_negative_samples * context_window
        self.normalize = normalizer_func
        self.w = none
        self.v = none

    def train(self, corpus, learning_rate=0.1, decay_rate=0.1, num_epochs=100):
        """"""
        (1) build the vocabulary from the corpus.
        (2) initialize the weight matrices.
        (3) iterate over the number of epochs.
        (4) update the weights for each training example (target word, list of context words, list of negative samples).
        (5) add the two trained matrices.
        """"""
        self.losses = list()  # list to store the value of the loss function at each epoch (for informative purposes).
        self.count = 0
        self.last_print = ''
        self.num_epochs = num_epochs

        self.build_vocabulary(corpus)
        self.initialize_weights()

        for epoch in range(self.num_epochs):
            self.total_loss = 0.0  # for informative purposes.
            learning_rate = self.get_learning_rate(learning_rate=learning_rate)
            for target, context, negative_samples in self.generate_training_data(corpus):
                self.update_weights(target, context, negative_samples, learning_rate)
            self.losses.append(self.total_loss)
            print(f""loss at epoch {epoch}: {self.total_loss}"")

        self.w += self.v

    def build_vocabulary(self, corpus):
        """"""
        create a list of unique words, where the index represents the word's idx.
        """"""
        self.vocabulary = set()
        for sentence in corpus:
            for word in sentence.split():
                self.vocabulary.add(word)
        self.vocabulary = list(self.vocabulary)

    def initialize_weights(self):
        """"""
        create two weight matrices, w (target words) and v (context words), with dimensions vocab_size * ndims.
        the value of each dimension is randomly chosen within the range [-0.5/ndims, 0.5/ndims].
        """"""
        vocab_size = len(self.vocabulary)
        self.w = np.random.uniform(low=-0.5, high=0.5, size=(vocab_size, self.num_dimensions))
        self.v = np.zeros((vocab_size, self.num_dimensions))

    def generate_training_data(self, corpus):
        """"""
        convert each sentence in the corpus into an ordered list of words and yield the target word, context words, and negative samples.
        """"""
        for sentence in corpus:
            self.print_progress(len(corpus) * self.num_epochs)
            sentence_list = sentence.strip().split()
            for target_idx, target in enumerate(sentence_list):
                context = self.get_context(sentence_list, target_idx)
                negative_samples = self.get_negative_samples(context)
                yield target, context, negative_samples

    def get_context(self, sentence, target_idx):
        """"""
        given a list of words (sentence) and the target word's index within that list, return a list of context words.
        """"""
        context = []
        for i in range(target_idx - self.context_window, target_idx + self.context_window + 1):
            if i != target_idx and i >= 0 and i < len(sentence):
                context.append(sentence[i])
        return context

    def get_negative_samples(self, context):
        """"""
        select random words from the vocabulary that are not in the context words.
        """"""
        negative_samples = []
        while len(negative_samples) < self.num_negative_samples:
            sample = random.choice(self.vocabulary)
            if sample not in context:
                negative_samples.append(sample)
        return negative_samples

    def update_weights(self, target, context, negative_samples, learning_rate):
        """"""
        iterate over the context words and negative samples.
        (1) compute the error.
        (2) add it to a total loss attribute (for later printing).
        (3) update the weight matrices.
        """"""
        for context_word in context:
            error = self.compute_error(target, context_word, 1)  # 1 because it's the true label (context word)
            self.total_loss += abs(np.log(error + 1.01))
            self.update_weights_for_word(target, context_word, error, learning_rate)

        for negative_word in negative_samples:
            error = self.compute_error(target, negative_word, 0)  # 0 because it's the true label (negative sample)
            self.total_loss += abs(np.log(error + 1.01))
            self.update_weights_for_word(target, negative_word, error, learning_rate)

    def compute_error(self, target, context_word, label):
        """"""
        for context words, the true label is 1, and the algorithm aims to approximate the dot product (to make them similar according to cosine similarity).
        for negative words, the true label is 0, and the cost is higher if the prediction approaches 1.
        """"""
        def sigmoid(x):
            return 1 / (1 + np.exp(-x))

        target_vector = self.w[self.vocabulary.index(target)]
        context_vector = self.v[self.vocabulary.index(context_word)]
        dot_product = np.dot(target_vector, context_vector)
        prediction = sigmoid(dot_product)
        return prediction - label

    def update_weights_for_word(self, target, word, error, learning_rate):
        """"""
        update the weights based on the jurfasky and martin equation.
        the gradient is equal to error * w/v.
        if the error is negative (context), the vectors are summed; otherwise (negative sample), they are subtracted.
        note that in this algorithm, the update is done word by word, but it may be possible to optimize the calculations by summing the vectors of context words and negative words.
        """"""
        self.w[self.vocabulary.index(target)] = self.w[self.vocabulary.index(target)] - learning_rate * error * self.v[self.vocabulary.index(word)]
        self.v[self.vocabulary.index(word)] = self.v[self.vocabulary.index(word)] - learning_rate * error * self.w[self.vocabulary.index(target)]

    def print_progress(self, len_corpus):
        self.count += 1
        trained = round((self.count / len_corpus) * 100)
        if trained != self.last_print:
            clear_output(wait=true)
            print(f'{trained}% processed', flush=true)
            self.last_print = trained

    def get_word_vector(self, word):
        if word in self.vocabulary:
            return self.w[self.vocabulary.index(word)].reshape(1, -1)
        else:
            print(f""{word} is not in the vocabulary."")

    def compute_most_similar_words(self, word, top_k=5):
        word_vector = self.get_word_vector(self.normalize(word))

        if word_vector is none:
            return []

        similarities = np.dot(self.w, word_vector.t)
        top_indices = np.argsort(similarities, axis=0)[::-1][:top_k]
        similar_words = [self.vocabulary[idx] for idx in top_indices.flatten() if idx != self.vocabulary.index(word)]
        return similar_words

    def get_learning_rate(self, learning_rate, min_value=0.0001):
        if len(self.losses) < 2:
            return learning_rate

        if self.losses[-1] < self.losses[-2]:
            return learning_rate
        if self.losses[-1] > self.losses[-2]:
            if learning_rate > min_value:
                learning_rate = learning_rate * 0.5
                print(f'adjusting learning rate. new value: {round(learning_rate, 2)}')
                return learning_rate

        return learning_rate

the problem is that i cannot get a loss value lower than 1 when training with corpus of more than 20 sentences, unless i use a very low learning rate and hundreds of epochs and wait for six o seven hours (which seems too much in comparison to fasttext or solutions like these).
i know that maybe there is no easy solution, but there might be some bug or something that i'm missing which could help me improve this code. in either case, i would appreciate your insights.
thank you in advance","['python', 'machine-learning', 'nlp', 'logistic-regression', 'word2vec']",76706364,"some thoughts:
until you're training with a corpus of many thousands of unique words, & with many varied usage contexts for every word-of-interest, & with vectors of higher dimensionality (at least 10s-of-dimensions), you won't be at the scale where word2vec provides value, and at the scale where variations of parameters/data/implementation-choices teach relevant/generalizable things about about word2vec.
that is, 'toy-sized' tests will often show nothing interesting, or mislead about the relevant decisions & tradeoffs. as the only scale-of-data you've mentioned is a mere ""20 sentences"", and you speak favorably of your loss results at that scale, you may simply be worrying about things that don't matter for word2vec's actual effectiveness.
in particular, word2vec model overall quality is generally not evaluable by checking the training loss. that's only appropriate for determining the point at which more training can't help, when the improvement-in-loss stagnates, hinting at 'model convergence' ï¿½ï¿½ï¿½ as good as it can get at predicting the training data, within its size/parameter constraints.
(further: typical open-source implementations like the original word2vec.c from google, or the python gensim library, or facebook's fasttext release, don't even consult loss for that stopping decision, or even dynamic adjustment of the internal learning-rate. they instead traditionally just let users specify a fixed number of epochs, with straight-line decay of learning-rate. if training-loss is reported at all, it's only as advisory info for the operator, to guide comparisons to other runs.)
whether a model's word-vectors ï¿½ï¿½ï¿½ the neural-net's 'input weights' ï¿½ï¿½ï¿½ are good at tasks is a separate matter than its internal training loss, and a model with higher final-epoch training-loss might yield better vectors for real purposes than a model with lower final-epoch training-loss.
in particular, a model that's far ovaining data may become incredibly good at predicting the idiosyncracies of the small training set ï¿½ï¿½ï¿½ very low training loss ï¿½ï¿½ï¿½ in ways that are deleterious for other out-of-training language modeling: overfitting.
so any concern that ""this run's loss doesn't seem low enough"" is likely misguided. ask instead: when loss stagnated, were the word-vectors useful on robust repeatable evaluations, testing for the qualities your task(s) require?
from a quick glance at your code, some other comments:

your exploratory implementation has none of the common optimizations of usual implementations ï¿½ï¿½ï¿½ most especially use of optimized bulk vector operations. thus this implementation will be far slower, and not an accurate hint as to what algorithms might be competitive for real uses. for example, when gensim moved from a pure-python implementation to one which used cython code to use a blas library, its word2vec code ran 80x-120x faster. (your ""6 hours"" t;3 minutes"" once code is similarly optimized.)

words which appear only once, or a few times, in a natural-language corpus can be very numerous, by usual zipfian frequency distributions, but nearly irrelevant to automated understanding of the texts. yet assigning them all trainable-vectors, & having the model learn weak representations from their idiosyncratic occurrences, consumes lots of memory & training time for no benefit. so typical implementations, unlike yours, ignore all words under some chosen minimum-frequency threshold. then training time, model size ï¿½ï¿½ï¿½ and the measurable quality of remaining words' vectors for usual tasks! ï¿½ï¿½ï¿½ are all much improved.

i see you're starting with a learning-rate, 0.1, that's far higher than the usual defaults chosen by other implementations, often 0.025 or 0.05. you're also using a momentum-based learning-rate adjustment, where open-source implementations often inear decay. your decisions might be improvements! but they aren't typical, and should ultimately be evaluated on whether they reach convergence faster, and the final word-vectors work better or worse than those from other methods ï¿½ï¿½ï¿½ not any intuitions about what absolute training-loss values should be.


hope this helps",https://stackoverflow.com/questions/76691477,python,14-07-2023 22:43,105.0,-1.0,1.0,True,17-07-2023 16:17,14-07-2023 22:49
72667425,how do i access the trained spacy thinc model?,"i'm trying to access the trained neural network model used by the spacy pipeline.
i can see from the spacy documentation that the config.cfg specifies the model instance which is part of the pipeline, but i donï¿½ï¿½ï¿½t understand how to access this model instance (data structure or its location) from within spacy.

i'd like to access the model and its weights so i can use the neural network outside of spacy. i was wondering if anyone had any pointers for accessing the thinc pre-trained model?","['python-3.x', 'spacy', 'spacy-3']",72674066,"you can get the model of a component like this.
import spacy

nlp = spacy.load(""en_core_web_sm"")
ner = nlp.get_pipe(""ner"")
model = ner.model

by convention, each component keeps its model (if present) in self.model.
i'm not sure the model will be useful to you on its own though - models generally depend on the tok2vec they were with, and the tok2vec depends on the representation of lexemes in spacy.  so without the other parts of the pipeline you probably won't be able to get meaningful predictions.",https://stackoverflow.com/questions/72667425,python-3.x,18-06-2022 07:14,396.0,4.0,1.0,True,19-06-2022 03:19,18-06-2022 21:00
57904642,need approach on building custom ner for extracting below keywords from any format of payslips,"i am trying to build a generic extraction of below parameters from any format of payslip:

name
his postcode
pay date
net pay.

challenge i am facing is due to variety of format that may come, i want to apply ner (spacy) to learn these under the entities

name - person
his postcode
pay date - date
net pay. - money

but i am unsuccess so far, i even tried to build a custom entitymatcher for postcode & date but to no success.
i seek any guideline and approach to make me take the right path in achieving the above ask, as to what is the right and best approach under the ml to achieve this.
a snippet of custom ner i tried to build
import spacy
import random
import threading
import time
from dateentitymatcher import  dateentitymatcher
from postcodeentitymatcher import postcodeentitymatcher


class incomevalidatormodel(object):
    """""" threading example class
    the run() method will be started and it will run in the background
    until the application exits.
    """"""

    def __init__(self, interval=1):
        """""" constructor
        :type interval: int
        :param interval: check interval, in seconds
        """"""
        self.interval = interval

        thread = threading.thread(target=self.run, args=())
        thread.daemon = true                            # daemonize thread
        thread.start()                                  # start the execution

    def run(self):
        """""" method that runs forever """"""
        while true:
            # do something
            print('doing something important in the background')
            data = [
                (u""private & confidential    ref. no.    dept    site    pay date    82521    002    31/07/2019    mr m hasan    69 alcombe road    northampton    uk    nn1 3le    confidential pay advice    mr m hasan    capgemini uk plc    emp reference    taxdistrict    taxreference    d83/82521    475/vb53759    taxable pay    14297.14    ay date    31/07/2019    tax period    2019-04    ann. salary    49650.00    tax paid    1611.40    pay method    bacs    tax code    1871l    pay period    monthly    n.i. employee    1365.96    n.i. number    sy095026c    contract hrs    40.00    period pay    4137.50    n.i. employer    1576.11    n.i. table    a    o/time rate    23.8702    hourly rate    23.8702    payments    deductions    description    hrs/units    rate    value    to date    description    value    bal ance    to date    benefit allow    620.67    706.61    nat.ins    385.84    1365.96    disp nt    -353.08    -1253.08    p.a.y.e.    474.80    1611.40    salary    4137.50    16514.38    accom nt    -470.77    -1670.77    gross pay    4758.17    total deductions    860.64    net pay    3897.53"",
                {'entities': [(203, 218, 'org'), (100, 106, 'person'), (1097, 1103, 'money')]}),
                (u""private & confidential    ref. no.    dept    site    pay date    82521    002    31/07/2019    mr m hasan    69 alcombe road    northampton    uk    nn1 3le    confidential pay advice    mr k khana    capgemini uk plc    emp reference    taxdistrict    taxreference    d83/82521    475/vb53759    taxable pay    14297.14    ay date    31/07/2019    tax period    2019-04    ann. salary    49650.00    tax paid    1611.40    pay method    bacs    tax code    1871l    pay period    monthly    n.i. employee    1365.96    n.i. number    sy095026c    contract hrs    40.00    period pay    4137.50    n.i. employer    1576.11    n.i. table    a    o/time rate    23.8702    hourly rate    23.8702    payments    deductions    description    hrs/units    rate    value    to date    description    value    bal ance    to date    benefit allow    620.67    706.61    nat.ins    385.84    1365.96    disp nt    -353.08    -1253.08    p.a.y.e.    474.80    1611.40    salary    4137.50    16514.38    accom nt    -470.77    -1670.77    gross pay    4758.17    total deductions    860.64    net pay    3897.53"",
                {'entities': [(203, 218, 'org'), (100, 106, 'person'), (1097, 1103, 'money')]}),
                (u""private & confidential    ref. no.    dept    site    pay date    82521    002    31/07/2019    mr m hasan    69 alcombe road    northampton    uk    nn1 3le    confidential pay advice    mr m menon    capgemini uk plc    emp reference    taxdistrict    taxreference    d83/82521    475/vb53759    taxable pay    14297.14    ay date    31/07/2019    tax period    2019-04    ann. salary    49650.00    tax paid    1611.40    pay method    bacs    tax code    1871l    pay period    monthly    n.i. employee    1365.96    n.i. number    sy095026c    contract hrs    40.00    period pay    4137.50    n.i. employer    1576.11    n.i. table    a    o/time rate    23.8702    hourly rate    23.8702    payments    deductions    description    hrs/units    rate    value    to date    description    value    bal ance    to date    benefit allow    620.67    706.61    nat.ins    385.84    1365.96    disp nt    -353.08    -1253.08    p.a.y.e.    474.80    1611.40    salary    4137.50    16514.38    accom nt    -470.77    -1670.77    gross pay    4758.17    total deductions    860.64    net pay    3897.53"",
                {'entities': [(203, 218, 'org'), (100, 106, 'person'), (1097, 1103, 'money')]}),
                (u""private & confidential    ref. no.    dept    site    pay date    82521    002    31/07/2019    mr m hasan    69 alcombe road    northampton    uk    nn1 3le    confidential pay advice    mr f jahan    capgemini uk plc    emp reference    taxdistrict    taxreference    d83/82521    475/vb53759    taxable pay    14297.14    ay date    31/07/2019    tax period    2019-04    ann. salary    49650.00    tax paid    1611.40    pay method    bacs    tax code    1871l    pay period    monthly    n.i. employee    1365.96    n.i. number    sy095026c    contract hrs    40.00    period pay    4137.50    n.i. employer    1576.11    n.i. table    a    o/time rate    23.8702    hourly rate    23.8702    payments    deductions    description    hrs/units    rate    value    to date    description    value    bal ance    to date    benefit allow    620.67    706.61    nat.ins    385.84    1365.96    disp nt    -353.08    -1253.08    p.a.y.e.    474.80    1611.40    salary    4137.50    16514.38    accom nt    -470.77    -1670.77    gross pay    4758.17    total deductions    860.64    net pay    3897.53"",
                {'entities': [(203, 218, 'org'), (100, 106, 'person'), (1097, 1103, 'money')]}),
                (u""private & confidential    ref. no.    dept    site    pay date    82521    002    31/07/2019    mr m hasan    69 alcombe road    northampton    uk    nn1 3le    confidential pay advice    mr a jahan    capgemini uk plc    emp reference    taxdistrict    taxreference    d83/82521    475/vb53759    taxable pay    14297.14    ay date    31/07/2019    tax period    2019-04    ann. salary    49650.00    tax paid    1611.40    pay method    bacs    tax code    1871l    pay period    monthly    n.i. employee    1365.96    n.i. number    sy095026c    contract hrs    40.00    period pay    4137.50    n.i. employer    1576.11    n.i. table    a    o/time rate    23.8702    hourly rate    23.8702    payments    deductions    description    hrs/units    rate    value    to date    description    value    bal ance    to date    benefit allow    620.67    706.61    nat.ins    385.84    1365.96    disp nt    -353.08    -1253.08    p.a.y.e.    474.80    1611.40    salary    4137.50    16514.38    accom nt    -470.77    -1670.77    gross pay    4758.17    total deductions    860.64    net pay    3897.53"",
                {'entities': [(203, 218, 'org'), (100, 106, 'person'), (1097, 1103, 'money')]}),
                (u""private & confidential    ref. no.    dept    site    pay date    82521    002    31/07/2019    mr m hasan    69 alcombe road    northampton    uk    nn1 3le    confidential pay advice    mr m hasan    capgemini uk plc    emp reference    taxdistrict    taxreference    d83/82521    475/vb53759    taxable pay    14297.14    ay date    31/07/2019    tax period    2019-04    ann. salary    49650.00    tax paid    1611.40    pay method    bacs    tax code    1871l    pay period    monthly    n.i. employee    1365.96    n.i. number    sy095026c    contract hrs    40.00    period pay    4137.50    n.i. employer    1576.11    n.i. table    a    o/time rate    23.8702    hourly rate    23.8702    payments    deductions    description    hrs/units    rate    value    to date    description    value    bal ance    to date    benefit allow    620.67    706.61    nat.ins    385.84    1365.96    disp nt    -353.08    -1253.08    p.a.y.e.    474.80    1611.40    salary    4137.50    16514.38    accom nt    -470.77    -1670.77    gross pay    4758.17    total deductions    860.64    net pay    3897.53"",
                {'entities': [(203, 218, 'org'), (100, 106, 'person'), (1097, 1103, 'money')]}),
                (u""private & confidential    ref. no.    dept    site    pay date    82521    002    31/07/2019    mr m hasan    69 alcombe road    northampton    uk    nn1 3le    confidential pay advice    mr m hasan    capgemini uk plc    emp reference    taxdistrict    taxreference    d83/82521    475/vb53759    taxable pay    14297.14    ay date    31/07/2019    tax period    2019-04    ann. salary    49650.00    tax paid    1611.40    pay method    bacs    tax code    1871l    pay period    monthly    n.i. employee    1365.96    n.i. number    sy095026c    contract hrs    40.00    period pay    4137.50    n.i. employer    1576.11    n.i. table    a    o/time rate    23.8702    hourly rate    23.8702    payments    deductions    description    hrs/units    rate    value    to date    description    value    bal ance    to date    benefit allow    620.67    706.61    nat.ins    385.84    1365.96    disp nt    -353.08    -1253.08    p.a.y.e.    474.80    1611.40    salary    4137.50    16514.38    accom nt    -470.77    -1670.77    gross pay    4758.17    total deductions    860.64    net pay    3897.53"",
                {'entities': [(203, 218, 'org'), (100, 106, 'person'), (1097, 1103, 'money')]}),
                (u""private & confidential    ref. no.    dept    site    pay date    82521    002    31/07/2019    mr m hasan    69 alcombe road    northampton    uk    nn1 3le    confidential pay advice    mr m hasan    capgemini uk plc    emp reference    taxdistrict    taxreference    d83/82521    475/vb53759    taxable pay    14297.14    ay date    31/07/2019    tax period    2019-04    ann. salary    49650.00    tax paid    1611.40    pay method    bacs    tax code    1871l    pay period    monthly    n.i. employee    1365.96    n.i. number    sy095026c    contract hrs    40.00    period pay    4137.50    n.i. employer    1576.11    n.i. table    a    o/time rate    23.8702    hourly rate    23.8702    payments    deductions    description    hrs/units    rate    value    to date    description    value    bal ance    to date    benefit allow    620.67    706.61    nat.ins    385.84    1365.96    disp nt    -353.08    -1253.08    p.a.y.e.    474.80    1611.40    salary    4137.50    16514.38    accom nt    -470.77    -1670.77    gross pay    4758.17    total deductions    860.64    net pay    3897.53"",
                {'entities': [(203, 218, 'org'), (100, 106, 'person'), (1097, 1103, 'money')]}),
                (u""sample payslip    matrix house    basing view    basingstoke    hampshire    rg21 4ff    advantage resourcing    6th floor, matrix house, basing view, basingstoke, hampshire, rg21 4ff    registered number 03341461    company    division    advantage resourcing uk    swindon    worker no.    name    period    pay date    ind    123456    sample payslip    14/2016    08/07/2016    w1    department    tax code    n.i. no./table letter    nat    1100l    ja123456a/a    payments    deductions    wk ending    timesheet    description    units    rate    amount    deduction    amount    03/07/2016    gen000499628 hourly rate    40.00    10.00    400.00    tax    87.60    03/07/2016    gen000499628 week day overtime    10.00    15.00    150.00    ni    59.40    03/07/2016    gen000499628 saturday overtime    5.00    20.00    100.00    total payments    650.00    total deductions    147.00    cumulatives    gross to date    650.00    current holiday entitlement: 0.00 unit(s)    taxable pay to date    650.00    ee pension to date    0.00    er pension to date    0.00    tax to date    87.60     to date    68.17    to date    59.40    c safe computing limited 2002    net pay    503.00"",
                {'entities': [(89, 109, 'org'), (0, 14, 'person'), (1186, 1191, 'money')]}),
                (u""mubssar hasan    matrix house    basing view    basingstoke    hampshire    rg21 4ff    advantage resourcing    6th floor, matrix house, basing view, basingstoke, hampshire, rg21 4ff    registered number 03341461    company    division    advantage resourcing uk    swindon    worker no.    name    period    pay date    ind    123456    sample payslip    14/2016    08/07/2016    w1    department    tax code    n.i. no./table letter    nat    1100l    ja123456a/a    payments    deductions    wk ending    timesheet    description    units    rate    amount    deduction    amount    03/07/2016    gen000499628 hourly rate    40.00    10.00    400.00    tax    87.60    03/07/2016    gen000499628 week day overtime    10.00    15.00    150.00    ni    59.40    03/07/2016    gen000499628 saturday overtime    5.00    20.00    100.00    total payments    650.00    total deductions    147.00    cumulatives    gross to date    650.00    current holiday entitlement: 0.00 unit(s)    taxable pay to date    650.00    ee pension to date    0.00    er pension to date    0.00    tax to date    87.60     to date    68.17     to date    59.40    c safe computing limited 2002    net pay    503.00"",
                {'entities': [(88, 108, 'org'), (0, 13, 'person'), (1186, 1191, 'money')]}),
                (u""oracle corp anil menon work date 01/09/2019 payments tax 100 net pay 2000"",
                 {'entities': [(0, 10, 'org'), (12, 21, 'person'), (69, 72, 'money')]}),
                (u""huawei corp anil menon work date 01/06/2019 payments tax 100 net pay 1900"",
                 {'entities': [(0, 10, 'org'), (12, 21, 'person'), (69, 72, 'money')]}),
                (u""tata corp nitin garg work date 20/04/2019 payments tax 100 net pay 1900"",
                 {'entities': [(0, 8, 'org'), (10, 19, 'person'), (67, 70, 'money')]}),
                (u""accenture corp amol joshi work date 20/04/2019 payments tax 100 net pay 900"",
                 {'entities': [(0, 15, 'org'), (17, 26, 'person'), (72, 74, 'money')]}),
                (u""cognizant corp anup nair work date 20/04/2019 payments tax 100 net pay 900"",
                 {'entities': [(0, 15, 'org'), (17, 25, 'person'), (71, 73, 'money')]}),
                (u""cognizant corp sajit kumar work date 20/04/2019 payments tax 100 net pay 1900"",
                 {'entities': [(0, 15, 'org'), (17, 27, 'person'), (73, 76, 'money')]}),
                (u""tata corp saurabh dave work date 20/04/2019 payments tax 100 net pay 1300"",
                 {'entities': [(0, 8, 'org'), (10, 21, 'person'), (69, 72, 'money')]}),
                (u""capgemini plc mubashshir hasan work date 20/04/2019 payments tax 100 net pay 1700"",
                 {'entities': [(0, 12, 'org'), (14, 29, 'person'), (77, 80, 'money')]}),
                (u""capgemini plc sagar pande work date 20/04/2019 payments tax 100 net pay 1700"",
                 {'entities': [(0, 12, 'org'), (14, 24, 'person'), (72, 75, 'money')]}),
                (u""capgemini plc sreeram yegappan work date 20/04/2019 payments tax 100 net pay 2000"",
                 {'entities': [(0, 12, 'org'), (14, 29, 'person'), (77, 80, 'money')]})
            ]

            # nlp = spacy.blank('en')  # new, empty model. letï¿½ï¿½ï¿½s say itï¿½ï¿½ï¿½s for the english language
            global nlp
            nlp = spacy.load('en_core_web_sm')
            nlp.entity.add_label('org')
            nlp.entity.add_label('person')
            nlp.entity.add_label('money')

            # add ner pipeline
            # ner = nlp.create_pipe('ner')  # our pipeline would just do ner
            # nlp.add_pipe(ner, last=true)  # we add the pipeline to the model
            postcde_entity_matcher = postcodeentitymatcher(nlp, ['nn1 3le', 'nn2 8hf', 'ig3 8th', 'nn4 7yh', 'rg21 5gh'], 'postcde')
            nlp.entity.add_label('postcde')
            nlp.add_pipe(postcde_entity_matcher, before='ner')

            date_entity_matcher = dateentitymatcher(nlp, ['20/04/2019','20/04/2019', '25/04/2016', '20/04/2019', '20/07/2019', '20/12/2019'], 'date')
            nlp.entity.add_lab           nlp.add_pipe(date_entity_matcher, before='ner')

            optimizer = nlp.begin_training()

            for i in range(11):
                random.shuffle(data)
                for text, annotations in data:
                    nlp.update([text], [annotations], sgd=optimizer)

            time.sleep(self.interval)

    def extractpayslipdata(self, data):
        doc = nlp(data)
        for entity in doc.ents:
            print(entity.label_, ' | ', entity.text)
        return doc.ents","['python-3.x', 'nlp', 'spacy', 'named-entity-recognition']",58011145,"training json(x.json) should be like this:-
[{
    ""text"": ""private & confidential    ref. no.    dept    site    pay date    82521    002    31/07/2019    mr m hasan    69 alcombe road    northampton    uk    nn1 3le    confidential pay advice    mr m hasan    capgemini uk plc    emp reference    taxdistrict    taxreference    d83/82521    475/vb53759    taxable pay    14297.14    ay date    31/07/2019    tax period    2019-04    ann. salary    49650.00    tax paid    1611.40    pay method    bacs    tax code    1871l    pay period    monthly    n.i. employee    1365.96    n.i. number    sy095026c    contract hrs    40.00    period pay    4137.50    n.i. employer    1576.11    n.i. table    a    o/time rate    23.8702    hourly rate    23.8702    payments    deductions    description    hrs/units    rate    value    to date    description    value    bal ance    to date    benefit allow    620.67    706.61    nat.ins    385.84    1365.96    disp nt    -353.08    -1253.08    p.a.y.e.    474.80    1611.40    salary    4137.50    16514.38    accom nt    -470.77    -1670.77    gross pay    4758.17    total deductions    860.64    net pay    3897.53"",
    ""entities"": [
        [
            191,
            198,
            ""person""
        ],
        [
            202,
            211,
            ""org""
        ],
        [
            150,
            157,
            ""post_code""
        ],
        [
            1096,
            1103,
            ""money""
        ]]
},
{
    ""text"": ""mubssar hasan    matrix house    basing view    basingstoke    hampshire    rg21 4ff    advantage resourcing    6th floor, matrix house, basing view, basingstoke, hampshire, rg21 4ff    registered number 03341461    company    division    advantage resourcing uk    swindon    worker no.    name    period    pay date    ind    123456    sample payslip    14/2016    08/07/2016    w1    department    tax code    n.i. no./table letter    nat    1100l    ja123456a/a    payments    deductions    wk ending    timesheet    description    units    rate    amount    deduction    amount    03/07/2016    gen000499628 hourly rate    40.00    10.00    400.00    tax    87.60    03/07/2016    gen000499628 week day overtime    10.00    15.00    150.00    ni    59.40    03/07/2016    gen000499628 saturday overtime    5.00    20.00    100.00    total payments    650.00    total deductions    147.00    cumulatives    gross to date    650.00    current holiday entitlement: 0.00 unit(s)    taxable pay to date    650.00    ee pension to date    0.00    er pension to date    0.00    tax to date    87.60     to date    68.17     to date    59.40    c safe computing limited 2002    net pay    503.00"",
    ""entities"": [
        [
            1,
            13,
            ""person""
        ],
        [
            88,
            108,
            ""org""
        ],
        [
            150,
            157,
            ""post_code""
        ],
        [
            1186,
            1192,
            ""money""
        ]]
}

]

code:-
with open(training_pickel_file) as input:
train_data = json.load(input)

def main(model=none, output_dir=""/home/nlp/model"", n_iter=50):
    if model is not none:
    nlp = spacy.load(model)
    print(""loaded model '%s'"" % model)
    else:
        nlp = spacy.blank('en')  # create blank language class
        print(""created blank 'en' model"")

    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=true)
    # otherwise, get it so we can add labels
    else:
        ner = nlp.get_pipe('ner')

        for annotations in train_data:
            for ent in annotations[""entities""]:
                ner.add_label(ent[2])
        print(ner)
        other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
        with nlp.disable_pipes(*other_pipes):  # only train ner
            optimizer = nlp.begin_training()
            for itn in range(n_iter):
                random.shuffle(train_data)
                losses = {}
                for a in train_data:`
                    doc = nlp.make_doc(a[""text""])
                    gold = goldparse(doc, entities = a[""entities""])
                    nlp.update([doc], [gold], drop =0.5, sgd=optimizer, losses = losses)
                print('losses', losses)
           if output_dir is not none:
               output_dir = path(output_dir)
               if not output_dir.exists():
                   output_dir.mkdir()
               nlp.to_disk(output_dir)

model testing :-
sen = [""""""private & confidential    ref. no.    dept    site    pay date    82521    002    31/07/2019    mr m hasan    69 alcombe road    northampton    uk    nn1 3le    confidential pay advice    mr m hasan    capgemini uk plc    emp reference    come a taxdistrict    taxreference    d83/82521    475/vb53759    taxable pay    14297.14    ay date    31/07/2019    tax period    2019-04    ann. salary    49650.00    tax paid    1611.40    pay method    bacs    tax code    1871l    pay period    monthly    n.i. employee    1365.96    n.i. number    sy095026c    contract hrs    40.00    period pay    4137.50    n.i. employer    1576.11    n.i. table    a    o/time rate    23.8702    hourly rate    23.8702    payments    deductions    description    hrs/units    rate    value    to date    description    value    bal ance    to date    benefit allow    620.67    706.61    nat.ins    385.84    1365.96    disp nt    -353.08    -1253.08    p.a.y.e.    474.80    1611.40    salary    4137.50    16514.38    accom nt    -470.77    -1670.77    gross pay    4758.17    total deductions    860.64    net pay    3897.53""""""]
for text in sen:
    doc = nlp(text)
    entity = {}
    for ent in doc.ents:
        list_of_ent = []
        list_of_ent.append(ent.text)
        entity.update({ent.label_: list_of_ent})
    print(entity)

result :-",https://stackoverflow.com/questions/57904642,python-3.x,12-09-2019 10:20,757.0,2.0,1.0,True,20-02-2023 06:06,12-09-2019 11:38
76363168,openai api: how do i handle errors in python?,"i tried using the below code, but the openai api doesn't have the authenticationerror method in the library. how can i effectively handle such error.
import openai

# set up your openai credentials
openai.api_key = 'your_api_key'

try:
    # perform openai api request
    response = openai.some_function()  # replace with the appropriate openai api function

    # process the response
    # ...
except openai.authenticationerror:
    # handle the authenticationerror
    print(""authentication error: invalid api key or insufficient permissions."")
    # perform any necessary actions, such as displaying an error message or exiting the program","['python', 'openai-api', 'gpt-3', 'chatgpt-api', 'gpt-4']",76371360,"error handling with the openai python sdk v1.0.0 or newer
ï¿½ï¿½ï¿½ if you don't want to handle error types individually:
import os
from openai import openai, openaierror
client = openai()
openai.api_key = os.getenv('openai_api_key')

try:
  # make your openai api request here
  response = client.completions.create(
    model=""gpt-3.5-turbo-instruct"",
    prompt=""say this is a test""
  )
  print(response)
except openaierror as e:
  # handle all openai api errors
  print(f""error: {e}"")

ï¿½ï¿½ï¿½ if you want to handle error types individually:
note: because there are a lot of classes for error handling, it might not be so elegant to import them individually. instead, use import openai and all classes for error handling will be imported automatically. but the code is a bit different now.
import os
import openai # import openai
from openai import openai # but  openaierror
client = openai()
openai.api_key = os.getenv('openai_api_key')

try:
  # make your openai api request here
  response = client.completions.create(
    model=""gpt-3.5-turbo-instruct"",
    prompt=""say this is a test""
  )
  print(response)
except openai.badrequesterror as e: # don't forget to add openai
  # handle error 400
  print(f""error 400: {e}"")
except openai.authenticationerror as e: # don't forget to add openai
  # handle error 401
  print(f""error 401: {e}"")
except openai.permissiondeniederror as e: # don't forget to add openai
  # handle error 403
  print(f""error 403: {e}"")
except openai.notfounderror as e: # don't forget to add openai
  # handle error 404
  print(f""error 404: {e}"")
except openai.unprocessableentityerror as e: # don't forget to add openai
  # handle error 422
  print(f""error 422: {e}"")
except openai.ratelimiterror as e: # don't forget to add openai
  # handle error 429
  print(f""error 429: {e}"")
except openai.internalservererror as e: # don't forget to add openai
  # handle error >=500
  print(f""error >=500: {e}"")
except openai.apiconnectionerror as e: # don't forget to add openai
  # handle api connection error
  print(f""api connection error: {e}"")

see the official openai github python repository.

error handling with the openai python sdk v0.28.0
your code isn't correct.
change this...
except openai.authenticationerror

...to this.
except openai.error.authenticationerror",https://stackoverflow.com/questions/76363168,python,30-05-2023 08:54,13376.0,5.0,2.0,True,06-04-2024 07:24,02-06-2023 13:55
77066202,"how can i allow certain entities (e.g., names, organizations) in azures pii entity recognition method so that they are not recognized/masked?","i am using azures pii entity recognition method in python to recognize pii entities in a list of documents.

i am wondering if there is a way to pass a list of entities to the method, which will then not be recognized as pii information. these will be, e.g., names/organizations which are not sensitive in the context.

i would like my pii entities to be replaced with the category, rather than masked with a masking character (e.g., ""andrew"" will become ""<person>"" rather than ""******""). i have currently solved this problem by adding my own method and looping through the responses. i am wondering, however, if there is a better way.


here is an example:
# function to replace detected pii entities with their respective categories
def replace_with_category(document, doc_result):
    """"""replace pii entities in the document with their categories.""""""
    redacted_text = document
    for entity in sorted(doc_result.entities, key=lambda e: e.offset, reverse=true):
        redacted_text = redacted_text[:entity.offset] + f""<{entity.category.upper()}>"" + redacted_text[entity.offset + entity.length:]
    return redacted_text
    
# function to redact pii entities in a list of documents
def pii_redact_list(documents, language):
     """"""this function takes the list of 5 documents replaces all the pii entities with their respective categories. the result is that rather than ****** the category e.g., <organisation> is listed in the string.
    the function first detects the language of all the documents. next, it recognizes the pii entities. finally it replaces the pii entities with their categories.""""""
    responses = azure_text_analytics_client.recognize_pii_entities(documents, categories_filter=pii_categories, language=language)
    redacted_texts = []
    for idx in range(0, len(responses)):
        doc_text = documents[idx]
        doc_result = responses[idx]
        redacted_text = self.replace_with_category(doc_text, doc_result)
        redacted_texts.append(redacted_text)
    return redacted_texts","['python', 'azure', 'azure-cognitive-services', 'named-entity-recognition', 'pii']",77066744,"point 1:
you can simply check at the moment when you replace your items using the result:
for entity in sorted(doc_result.entities, key=lambda e: e.offset, reverse=true):
        redacted_text = redacted_text[:entity.offset] + f""<{entity.category.upper()}>"" + redacted_text[entity.offset + entity.length:]

you can check if your entity.text matches one of the values you would like to keep
point 2: this looks like a correct way",https://stackoverflow.com/questions/77066202,python,08-09-2023 10:42,444.0,0.0,1.0,True,08-09-2023 12:06,08-09-2023 10:43
610399,finding related words (specifically physical objects) to a specific word,"i am trying to find words (specifically physical objects) related to a single word. for example:
tennis: tennis racket, tennis ball, tennis shoe
snooker: snooker cue, snooker ball, chalk 
chess: chessboard, chess piece
bookcase: book
i have tried to use wordnet, specifically the meronym semantic relationship; however, this method is not consistent as the results below show:
tennis: serve, volley, foot-fault, set point, return, advantage
snooker: nothing
chess: chess move, checkerboard (whose own meronym relationships shows ï¿½ï¿½ï¿½squareï¿½ï¿½ï¿½ & 'diagonal') 
bookcase: shelve
weighting of terms will eventually be required, but that is not really a concern now.
anyone have any suggestions on how to do this?  

just an update: ended up using a mixture of both tompchicken's answers.
the quality of information retrieved from wikipedia is excellent, specifically how (unsurprisingly) there is so much relevant information (in comparison to some corpora where terms such as 'blog' and 'ipod' do not exist).
the range of results from wikipedia is the best part.  the software is able to match terms such as (lists cut for brevity):

golf: [ball, iron, tee, bag, club]
photography: [camera, film, photograph, art, image] 
fishing: [fish, net, hook, trap, bait, lure, rod]

the biggest problem is classifying certain words as physical artefacts; default wordnet is not a reliable resource as many terms (such as 'ipod', and even 'trampolining') do not exist in it.","nlp, semantics, wordnet",611248,"i think what you are asking for is a source of semantic relationships between concepts. for that, i can think of a number of ways to go:

semantic similarity algorithms. these algorithms usually perform a tree walk over the relationships in wordnet to come up with a real-valued score of how related two terms are. these will be limited by how well wordnet models the concepts that you are interested in. wordnet::similarity (written in perl) is pretty good.
try using opencyc as a knowledge base. opencyc is a open-source version of cyc, a very large knowledge base of 'real-world' facts. it should have a much richer set of sematic realtionships than wordnet does. however, i have never used opencyc so i can't speak to how complete it is, or how easy it is to use.
n-gram frequency analysis. as mentioned by jeff moser. a data-driven approach that can 'discover' relationships from large amounts of data, but can often produce noisy results.
latent semantic analysis. a data-driven approach similar to n-gram frequency analysis that finds sets of semantically related words.

[...]
judging by what you say you want to do, i think the last two options are more likely to be successful. if the relationships are not in wordnet then semantic similarity won't work and opencyc doesn't seem to know much about snooker other than the fact that it exists.
i think a combination of both n-grams and lsa (or something like it) would be a good idea. n-gram frequencies will find concepts tightly bound to your target concept (e.g. tennis ball) and lsa would find related concepts mentioned in the same sentence/document (e.g. net, serve). also, if you are only interested in nouns, filtering your output to contain only nouns or noun phrases (by using a part-of-speech tagger) might improve results.",https://stackoverflow.com/q/610399,"nlp, semantics, wordnet",04-03-2009 12:51,5864.0,19.0,2.0,True,07-12-2023 05:19,02-04-2009 11:39
75495337,how to use tapas table question answer model when table size is big like containing 50000 rows?,"i am trying to build up a model in which i load the dataframe (an excel file from kaggle) and i am using tapas-large-finetuned-wtq model to query this dataset. i tried to query 259 rows (the memory usage is 62.9 kb). i didn't have a problem, but then i tried to query 260 rows with memory usage 63.1kb, and i have the error which says: ""index out of range in self"". i have attached a screenshot for the reference as well. the data i used here can be found from kaggle datasets.

the code i am using is:
from transformers import pipeline
import pandas as pd
import torch

question = ""which country code has the quantity 30604?""
tqa = pipeline(task=""table-question-answering"", model=""google/tapas-large-finetuned-wtq"")

c = tqa(table=df[:100], query=question)['cells']

in the last line, as you can see in the screenshot, i get the error.
please let me know what can be the way i can work for a solution? any tips would be welcome.","['huggingface-transformers', 'bert-language-model', 'nlp-question-answering']",76277849,"the way tapas works it needs to flatten the table into a sequence of word pieces.
this sequence needs to fit into the specified maximal sequence length (default is 512).
tapas has a pruning mechanism that will try to drop tokens but it will never drop cells.
therefore at a sequence length of 512 there is no way to fit a table with more than 512 cells.
if you really want to run the model on 1.8m rows i would suggest that you split your data row-wise.
for your table for example you would need blocks with a maximum of ~8 rows.
alternatively, you can increase the sequence size but that will also increase the cost of running the model.",https://stackoverflow.com/questions/75495337,huggingface-transformers,18-02-2023 18:02,2323.0,1.0,1.0,True,24-05-2023 03:37,18-02-2023 18:21
69157848,should bi-gram and tri-gram be used in lda topic modeling?,i read several posts(here and here) online about lda topic modeling. all of them only use uni-grams. i would like to know why bi-grams and tri-grams are not used for lda topic modeling?,"['nlp', 'gensim', 'topic-modeling', 'n-gram']",69159563,"it's a matter of scale. if you have 1000 types (ie ""dictionary words""), you might end up (in the worst case, which is not going to happen) with 1,000,000 bigrams, and 1,000,000,000 trigrams. these numbers are hard to manage, especially as you will have a lot more types in a realistic text.
the gains in accuracy/performance don't outweigh the computational cost here.",https://stackoverflow.com/questions/69157848,nlp,13-09-2021 05:50,1508.0,1.0,1.0,True,13-09-2021 21:11,13-09-2021 21:11
74390115,how to get an output from oneai nlp api?,"i found a very cool nlp api that helps analyze text using special skills. however, i am new to python and i don't know how to get the output. can someone help? this is what i tried:
# edit this one ai api call using our studio at 

# pip install oneai
import oneai

oneai.api_key = ""insert_api_key""
text = 'natural language processing (nlp) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. the goal is a computer capable of ""understanding"" the contents of documents, including the contextual nuances of the language within them. the technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. based on long-standing trends in the field, it is possible to extrapolate future directions of nlp. as of 2020, three trends among the topics of the long-standing series of conll shared tasks can be observed: interest on increasingly abstract, ""cognitive"" aspects of natural language, increasing interest in multilinguality and elimination of symbolic representations.'

pipeline = oneai.pipeline(
    steps=[
        oneai.skills.highlights(),
        oneai.skills.topics(),
        oneai.skills.summarize(),
    ]
)

output = pipeline.run(text)","['python', 'nlp', 'artificial-intelligence', 'pipeline']",74390379,"it looks like your code is valid. so the only thing you need to do is just print the pipeline. just add the following line to the end of your code:
print(output)

here's the code after the change:
# edit this one ai api call using our studio at 

# pip install oneai
import oneai

oneai.api_key = ""insert_api_key""
text = 'natural language processing (nlp) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. the goal is a computer capable of ""understanding"" the contents of documents, including the contextual nuances of the language within them. the technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. based on long-standing trends in the field, it is possible to extrapolate future directions of nlp. as of 2020, three trends among the topics of the long-standing series of conll shared tasks can be observed: interest on increasingly abstract, ""cognitive"" aspects of natural language, increasing interest in multilinguality and elimination of symbolic representations.'

pipeline = oneai.pipeline(
    steps=[
        oneai.skills.highlights(),
        oneai.skills.topics(),
        oneai.skills.summarize(),
    ]
)

output = pipeline.run(text)
print(output)

please make sure you get your api key from oneai studio. as mentioned in oneai's documentation - 
also make sure you install oneai by using pip install oneai.",https://stackoverflow.com/questions/74390115,python,10-11-2022 13:49,127.0,-2.0,1.0,True,13-11-2022 17:40,10-11-2022 14:04
69250057,remove stopwords from tokenizaton,"i having problem to remove stopwords from tokenization. i have already tokenized sentences and insert the result with pandas to the column named ""tweets_tokenize"". the problem is i have double bracket ( [ ] ), the result only one and repeated (details see image) and the stopwords didn't work if using first function. but, if using second function is good. can explain why?
from nltk.corpus import stopwords
stopwords_indonesia = stopwords.words('indonesian')

# first function
def stopwords_remover(words):
    words = df['tweets_tokenize']
    tweets_stopwords = []
    for word in words:
        if word not in stopwords_indonesia:
            tweets_stopwords.append(word)
    return tweets_stopwords

# second function
def stopwords_remover(words):
    tweets_stopwords = []
    for word in words:
        if word not in stopwords_indonesia:
            tweets_stopwords.append(word)
    return tweets_stopwords

df['tweets_tokenize'].apply(stopwords_remover)
df.head()

result using first function.

result using second function.","['python', 'pandas', 'nlp', 'nltk']",69267980,"the first function:
the reason is the line:
words = df['tweets_tokenize']

the type of the object is 'pandas.core.series.series' and once you iterate it in the loop, the object word will be type of list.
when you are appending it in this line:
tweets_stopwords.append(word)

you are actually appending a list, and not a word. this is why you have a lists wrapped with a big list.
the second function:
iterates each row separately, and the object type is a regular string.
summary:
when you use apply function of pandas, it will be a better use to handle it row by row like the second function, without using the entire column as mentioned in the first function.",https://stackoverflow.com/questions/69250057,python,20-09-2021 06:40,83.0,0.0,1.0,True,21-09-2021 11:18,20-09-2021 13:32
62038216,spacy different language models,"i'm making some progress:) developing my litle ocr project.
i was wondering if my idea is possible in this case!
after extracting the text from a images (ocr), i use nlp (spacy) to identify two entities (location and person). i write to a dictionary and later in a json data. that works good.
now i'm wondering if i can improve my identified entities.
one way i can imagine is to use the right language model for the text.
i have varies texts in german, english,spanish and french.
at the moment i'm using the
but now i have no idea how to put langdetect into this
have a great week!
greets","['spacy', 'detect', 'named-entity-recognition']",62038664,"here is a link that you might find useful when it comes to detecting a language (there are multiple options including langdetect) - how to detect language
you can create a dictionary with the languages you plan to detect and match it with langdetect's output. i guess you have the rest sorted out.",https://stackoverflow.com/questions/62038216,spacy,27-05-2020 08:03,192.0,0.0,1.0,True,12-10-2022 15:51,12-10-2022 15:51
71711943,access google cloud natural language with google colab,"i am attempting to use google's cloud natural language api with google colab.
i started by following google's simple example: 
so, my colab notebook was literally just one code cell:
from google.cloud import language_v1

client = language_v1.languageserviceclient()

text_content = 'grapes are good. bananas are bad.'

# available types: plain_text, html
type_ = language_v1.types.document.type.plain_text

# optional. if not specified, the language is automatically detected.
# for list of supported languages:
# 
language = ""en""
document = {""content"": text_content, ""type_"": type_, ""language"": language}

# available values: none, utf8, utf16, utf32
encoding_type = language_v1.encodingtype.utf8

response = client.analyze_entity_sentiment(request = {'document': document, 'encoding_type': encoding_type})

that resulted in several error messages, which i seemed to resolve, mostly with the help of this so post, by slightly updating the code as follows:
from google.cloud import language_v1

client = language_v1.languageserviceclient()

text_content = 'grapes are good. bananas are bad.'

# available types: plain_text, html
type_ = language_v1.types.document.type.plain_text

# optional. if not specified, the language is automatically detected.
# for list of supported languages:
# 
language = ""en""
#document = {""content"": text_content, ""type_"": type_, ""language"": language} ## ""type_"" is not valid???
document = {""content"": text_content, ""type"": type_, ""language"": language}

# available values: none, utf8, utf16, utf32
#encoding_type = language_v1.encodingtype.utf8 ## does not seem to work
encoding_type = ""utf8""

#response = client.analyze_entity_sentiment(request = {'document': document, 'encoding_type': encoding_type}) ## remove request
response = client.analyze_entity_sentiment( document = document, encoding_type = encoding_type )

which, after 10 excruciating minutes, results in the following error:
_inactiverpcerror                         traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/google/api_core/grpc_helpers.py in error_remapped_callable(*args, **kwargs)
     72         try:
---> 73             return callable_(*args, **kwargs)
     74         except grpc.rpcerror as exc:

11 frames
_inactiverpcerror: <_inactiverpcerror of rpc that terminated with:
    status = statuscode.unavailable
    details = ""getting metadata from plugin failed with error: (""failed to retrieve  from the google compute enginemetadata service. status: 404 response:\nb''"", <google.auth.transport.requests._response object at 0x7f68cee39a90>)""
    debug_error_string = ""{""created"":""@1648840699.964791285"",""description"":""getting metadata from plugin failed with error: (""failed to retrieve  from the google compute enginemetadata service. status: 404 response:\nb''"", <google.auth.transport.requests._response object at 0x7f68cee39a90>)"",""file"":""src/core/lib/security/credentials/plugin/plugin_credentials.cc"",""file_line"":91,""grpc_status"":14}""
>

the above exception was the direct cause of the following exception:

serviceunavailable                        traceback (most recent call last)
serviceunavailable: 503 getting metadata from plugin failed with error: (""failed to retrieve  from the google compute enginemetadata service. status: 404 response:\nb''"", <google.auth.transport.requests._response object at 0x7f68cee39a90>)

the above exception was the direct cause of the following exception:

retryerror                                traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)

retryerror: deadline of 600.0s exceeded while calling functools.partial(<function _wrap_unary_errors.<locals>.error_remapped_callable at 0x7f68cedb69e0>, document {
  type: plain_text
  content: ""grapes are good. bananas are bad.""
  language: ""en""
}
encoding_type: utf8
, metadata=[('x-goog-api-client', 'gl-python/3.7.13 grpc/1.44.0 gax/1.26.3 gapic/1.2.0')]), last exception: 503 getting metadata from plugin failed with error: (""failed to retrieve  from the google compute enginemetadata service. status: 404 response:\nb''"", <google.auth.transport.requests._response object at 0x7f68cee39a90>)

can you please help me with this simple ""hello world!"" for cloud natural language with google colab?
my hunch is that i need to create a service account and somehow provide that key file to colab, like this so answer. if so, can you hold my hand a little more and tell me how i would implement that in colab (vs. running locally)? i am new to colab.","['nlp', 'google-colaboratory', 'google-cloud-nl', 'google-natural-language']",71712432,"this appears to have worked:
start by creating a service account, generating a key file, and saving the json file locally. 
(i would still love to know which, if any roles i should select in the service account generation: ""grant this service account access to project"")
cell 1: upload a json file with my service account keys
from google.colab import files
uploaded = files.upload()

cell 2:
from google.oauth2 import service_account
from google.cloud import language_v1

client = language_v1.languageserviceclient.from_service_account_json(""my-super-important-gcp-key-file.json"")

cell 3:
text_content = 'grapes are good. bananas are bad.'
type_ = language_v1.types.document.type.plain_text
language = ""en""
document = {""content"": text_content, ""type"": type_, ""language"": language}
encoding_type = ""utf8""
response = client.analyze_entity_sentiment( document = document, encoding_type = encoding_type )
response

here is the output:
entities {
  name: ""grapes""
  type: other
  salience: 0.8335162997245789
  mentions {
    text {
      content: ""grapes""
    }
    type: common
    sentiment {
      magnitude: 0.800000011920929
      score: 0.800000011920929
    }
  }
  sentiment {
    magnitude: 0.800000011920929
    score: 0.800000011920929
  }
}
entities {
  name: ""bananas""
  type: other
  salience: 0.16648370027542114
  mentions {
    text {
      content: ""bananas""
      begin_offset: 17
    }
    type: common
    sentiment {
      magnitude: 0.699999988079071
      score: -0.699999988079071
    }
  }
  sentiment {
    magnitude: 0.699999988079071
    score: -0.699999988079071
  }
}
language: ""en""

i am certain that i have just violated all sorts of security protocols. so, please, i welcome any advice for how i should improve this process.",https://stackoverflow.com/questions/71711943,nlp,01-04-2022 19:31,888.0,1.0,1.0,True,04-04-2022 02:40,04-04-2022 02:40
67771257,why does transformer&#39;s bert (for sequence classification) output depend heavily on maximum sequence length padding?,"i am using transformer's robbert (the dutch version of roberta) for sequence classification - trained for sentiment analysis on the dutch book reviews dataset.
i wanted to test how well it works on a similar dataset (also on sentiment analysis), so i made annotations for a set of text fragments and checked its accuracy. when i checked what kind of sentence are misclassified, i noticed that the output for a unique sentence depends heavily on the length of padding i give when tokenizing. see code below.
from transformers import robertatokenizer, robertaforsequenceclassification
import torch.nn.functional as f
import torch


model = robertaforsequenceclassification.from_pretrained(""pdelobelle/robbert-dutch-books"", num_labels=2)
tokenizer = robertatokenizer.from_pretrained(""pdelobelle/robbert-dutch-books"", do_lower_case=true)

sent = 'de samenwerking gaat de laatste tijd beter'
max_seq_len = 64


test_token = tokenizer(sent,
                        max_length = max_seq_len,
                        padding = 'max_length',
                        truncation = true,
                        return_tensors = 'pt'
                        )

out = model(test_token['input_ids'],test_token['attention_mask'])

probs = f.softmax(out[0], dim=1).detach().numpy()

for the given sample text, which translates in english to ""the collaboration has been improving lately"", there is a huge difference in output on classification depending on the max_seq_len. namely, for max_seq_len = 64 the output for probs is:
[[0.99149346 0.00850648]]
whilst for max_seq_len = 9, being the actual length including cls tokens:
[[0.00494814 0.9950519 ]]
can anyone explain why this huge difference in classification is happening? i would think that the attention mask ensures that in the output there is no difference because of padding to the max sequence length.","['sentiment-analysis', 'bert-language-model', 'huggingface-transformers', 'huggingface-tokenizers']",67780228,"this is caused because your comparison isn't correct. the sentence de samenwerking gaat de laatste tijd beter has actually 16 tokens (+2 for the specialtokens) and not 9. you only counted the words which are not necessarily the tokens.
print(tokenizer.tokenize(sent))
print(len(tokenizer.tokenize(sent)))

output:
['de', 'ï¿½ï¿½sam', 'en', 'wer', 'king', 'ï¿½ï¿½ga', 'at', 'ï¿½ï¿½de', 'ï¿½ï¿½la', 'at', 'ste', 'ï¿½ï¿½t', 'ij', 'd', 'ï¿½ï¿½be', 'ter']
16

when you set the sequence length to 9 you are truncating the sent""lang-py prettyprint-override"">tokenizer.decode(tokenizer(sent,
                         max_length = 9,
                         padding = 'max_length',
                         truncation = true,
                         return_tensors = 'pt', 
                         add_special_tokens=false
                         )['input_ids'][0])

output:
'de samenwerking gaat de la'

and as final prove, the output when you set max_length to 52 is also [[0.99149346 0.00850648]].",https://stackoverflow.com/questions/67771257,sentiment-analysis,31-05-2021 09:33,1279.0,2.0,1.0,True,31-05-2021 21:07,31-05-2021 11:28
75854661,openai api key automatically rotating when deployed,"i'm building a web page using openai gpt api in reactjs. i saved my api key on .env file then gitignored it. and i deployed my code with gh-pages, but openai detects it and rotate the key automatically. how can i use api key properly?
const default_params = {
            model: ""gpt-3.5-turbo"",
            messages: [{""role"": ""user"",
                        ""content"": message
            }],
            temperature: 1,
            max_tokens: 1000
        };
        const params_ = {...default_params};
        const result = await fetch(' {
            method: 'post',
            headers: {
                'content-type': 'application/json',
                'authorization': 'bearer ' + string(process.env.react_app_openai_api_key)
            },
            body: json.stringify(params_)
        });
        const data = await result.json()","['javascript', 'reactjs', 'openai-api', 'api-key']",75861990,"you can refer to this question regarding securing secrets on static websites hosted on github pages.
short answer: it's impossible because everything is exposed in the code.
you need to use another way to deploy your website.
for example, frameworks like nextjs.",https://stackoverflow.com/questions/75854661,javascript,27-03-2023 10:28,1218.0,1.0,3.0,True,24-09-2023 23:58,19-09-2023 07:00
67693038,python spacy replace value of ent.label_ == person with something else,"i am using python spacy to replace any entity with the label_ == ""person"" with ""[xxx]"".
it seems like i have done that correctly, but i am struggling with replacing it in my teststring:
import spacy
from spacy.matcher import matcher

nlp = spacy.load(""en_core_web_sm"")

file_text = """"""this is my teststring. isaac newton is supposed to be changed.""""""

nlp.add_pipe(""merge_entities"")

def change_names(file_text):
    text_doc = nlp(file_text)
    mylist = []
    for ent in text_doc.ents:
        if ent.label_ == ""person"":
            print(ent)
            mylist.append(""[xxx]"")
        else:
            mylist.append(ent.text)
    res = ''.join(mylist)
    print(res)
    print(text_doc)

change_names(file_text)

this results in:
isaac newton
[xxx]
this is my teststring. isaac newton is supposed to be changed.
result should be:
this is my teststring. [xxx] is supposed to be changed
now i want to iterate over my text_doc and replace any ent with label_ == ""person"" to ""[xxx]"". this is not working out for me. i tried using a double forloop for iterating over the string and if an item is an entity, jump into the for loop i posted here. any suggestions?","['python', 'replace', 'entity', 'spacy']",67695773,"since all you need is a string output, you can use
result = []
for t in text_doc:
    if t.ent_type_ == ""person"":
        result.append(""[xxx]"")
    else:
        result.append(t.text)
    result.append(t.whitespace_)

res = ''.join(result)
print(res)

that is:

once the person entity is found, append [xxx] to the result list
else, add the current token text
append any whitespace after the token if present.

then, in the end, join the result items.",https://stackoverflow.com/questions/67693038,python,25-05-2021 17:42,2011.0,3.0,1.0,True,25-05-2021 21:26,25-05-2021 18:55
69042481,getting sense stems for nltk semcor corpus words,"i was trying semcor corpus in nltk.
i found this code here:
>>> list(map(str, semcor.tagged_chunks(tag='both')[:3])) 
['(dt the)', ""(lemma('group.n.01.group') (ne (nnp fulton county grand jury)))"", ""(lemma('state.v.01.say') (vb said))""]

i tried the same on colab (check last cell in this notebook):
>>> list(map(str, semcor.tagged_chunks(tag='both')[:3]))
['(dt the)',
 '(group.n.01 (ne (nnp fulton county grand jury)))',
 '(say.v.01 (vb said))']

here is the screenshot from colab:

the problem
note that on nltk page, for fulton county grand jury output is given as lemma('group.n.01.group'), but on colab, i am getting group.n.01. so i am not getting sense / synset lemma.

in group.n.01.group

first group is a ""stem for sense word""
last group is ""stem for input""


in group.n.01

(first and only) group is ""stem for input""
no ""stem for sense word"" is returned



weird thing is that it was giving me correct output yesterday. this notebook will clear the doubt as it has same two lines executed today and yesterday. yesterday (2/9/2021), i was getting tags in format group.n.01.group, but today i am getting tags in group.n.01 format (notice red and blue comments):

what i am missing here?","['nlp', 'nltk', 'google-colaboratory', 'nltk-book']",69061942,"i knew that semcor uses wordnet senses to tag to subset of brown corpus. but i was not aware that semcor apis can work with or without wordnet predownloaded and it will give tags in different format in these different scenarios. i honestly feel, at least semcor api documentation should have some mention of this.
so, without wordnet predownloaded, it does not return sense stems:

with wordnet pre-downloaded, it does return sense stems:",https://stackoverflow.com/questions/69042481,nlp,03-09-2021 09:18,654.0,0.0,1.0,True,05-09-2021 08:57,03-09-2021 11:02
68947449,regular expression and rule based matcher to extract legal citations title and volume,"i am trying to extract case title, volume and pages from inconsistence legal documents. i am using two algorithms, regex to and spacy rule based matching with entity and pos tags (still learning this...). i am getting over half of the citations with regex (thanks to answer code below) but zero with spacy. my code is
import re
import en_core_web_sm
nlp = en_core_web_sm.load()

nlp = spacy.load('en_core_web_sm')

from spacy.matcher import matcher
m_tool = matcher(nlp.vocab)

doc = open(file='text1.txt', mode='r', encoding='utf-8').read()
#print(text)

doc = nlp(doc)
#print([(ent.text, ent.label_) for ent in doc.ents])


p1 = [{'is_title': 'nn'}, {'lower': 'v'}, {'is_punct': true}, {'is_title': 'nn'}]
p2 = [{'is_title': 'nn'}, {'is_title': 'nn'}, {'lower': 'v'}, {'is_punct': true}, {'is_title': 'nn'}]
p3 = [{'is_title': 'nn'}, {'lower': 'v'}, {'is_punct': true}, {'is_title': 'nn'}, {'is_title': 'nn'},]
p4 = [{'is_title': 'nn'}, {'is_title': 'nn'}, {'lower': 'v'}, {'is_punct': true}, {'is_title': 'nn'}, {'is_title': 'nn'}]
p5 = [{'is_title': 'nn'}, {'is_title': 'nn'}, {'is_title': 'nn'}, {'lower': 'v'}, {'is_punct': true}, {'is_title': 'nn'}, {'is_title': 'nn'}, {'is_title': 'nn'}]
p6 = [{'is_title': 'nn'}, {'is_title': 'nn'}, {'lower': 'v'}, {'is_punct': true}, {'is_title': 'nn'}, {'is_title': 'nn'}, {'is_title': 'nn'}]
p7 = [{'is_title': 'nn'}, {'lower': 'v'}, {'is_punct': true}, {'is_title': 'nn'}, {'is_title': 'nn'}, {'is_title': 'nn'}]
p8 = [{'is_title': 'nn'}, {'is_title': 'nn'}, {'is_title': 'nn'}, {'lower': 'v'}, {'is_punct': true}, {'is_title': 'nn'}]
p9 = [{'is_title': 'nn'}, {'is_title': 'nn'}, {'is_title': 'nn'}, {'lower': 'v'}, {'is_punct': true}, {'is_title': 'nn'}, {'is_title': 'nn'}]
p10 = [{'label': 'person'}]
p11 = [{'label': 'org'}, {'label': 'person'}]
p12 = [{'label': 'person'}, {'label': 'org'}]
p13 = [{'label': 'org'}, {'label': 'org'}, {'label': 'org'}, {'label': 'org'}]

m_tool.add('qbf', none, p1, p2, p3, p4, p5, p6, p6, p7, p8, p9, p10, p11, p12, p13)

phrase_matches = m_tool(doc)
print(phrase_matches)

matches = re.findall(r'(?:[a-z]\w*\.? )+v\. .*?\d{4}\)', contents)
for match in matches:
    print(match)

my text1 looks like
text1 = ""material fact challenged. brill v. guardian life ins. co. of america, 142 n.j. 520, 529 (1995)
(emphasis original).
when a movant establishes certain facts, those who would oppose the motion are under see della v. guard lifal ins. co. of sa, 142 n.j. 420, 549 (2011)
an obligation to come forward with controverting facts. heljon mgmt. corp. v. dileo, 55 n.j.
super. 306, 312-13 (no citations. this was extracted from nj sup..). mere assertions and allegations in the pleadings are
insufficient to defeat motions for summary judgment. ocean cape hotel corp. v. masefield
corp., 63 n.j. super. 369, 383 (app. div. 1960). where the party opposing summary
 ""

i am expecting all matches with both algorithmns,
""brill v. guardian life ins. co. of america, 142 n.j. 520, 529 (1995)""
""della v. guard lifal ins. co. of sa, 142 n.j. 420, 549 (2011)""
""heljon mgmt. corp. v. dileo, 55 n.j. super. 306, 312-13 (no citations. this was extracted from nj sup..)""
""ocean cape hotel corp. v. masefield corp., 63 n.j. super. 369, 383 (app. div. 1960)""","['python', 'nlp', 'spacy']",68947951,"i am not sure if it will work in all cases, but you can try this:
matches = re.findall(r""(?:[a-z]\w*\.? )+v\. .*?\d{4}\)"", contents)

it gives:
['brill v. guardian life ins. co. of america, 142 n.j. 520, 529 (1995)',
 'heljon mgmt. corp. v. dileo, 55 n.j. super. 306, 312-13 (app. div. 1959)',
 'ocean cape hotel corp. v. masefield corp., 63 n.j. super. 369, 383 (app. div. 1960)']",https://stackoverflow.com/questions/68947449,python,27-08-2021 02:44,214.0,-1.0,1.0,True,02-09-2021 14:50,02-09-2021 14:50
78046390,python openai api error: module &#39;openai&#39; has no attribute &#39;completion&#39;. did you mean: &#39;completions&#39;?,"i try to make a bot for my website in python. but i have an error.
the error:
you: hello
traceback (most recent call last):
  file ""d:\module2.py"", line 20, in <module>
    response = chat_with_gpt(user_input)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""d:\module2.py"", line 6, in chat_with_gpt
    response = openai.completion.create(
               ^^^^^^^^^^^^^^^^^
attributeerror: module 'openai' has no attribute 'completion'. did you mean: 'completions'?

this is the python code:
i tried many combinations, and i also tried with text-davinci-003, but nothing worked.
import openai

openai.api_key = ""api-key""

def chat_with_gpt(prompt):
    response = openai.completion.create(
        # engine=""text-davinci-003"",  # adjust the engine as necessary
        engine=""gpt-3.5-turbo"",  # adjust the engine as necessary
        prompt=prompt,
        max_tokens=150
    )
    return response.choices[0].text.strip()

if __name__ == ""__main__"":
    while true:
        user_input = input(""you: "")
        user_input = user_input.lower()
        if user_input in [""quit"", ""exit"", ""bye""]:
            break
        response = chat_with_gpt(user_input)
        print(""chatbot:"", response)","['python', 'python-3.x', 'bots', 'openai-api', 'gpt-3']",78046455,"there are a few problems in your code:

using the wrong method name (i.e., completion)
using the deprecated parameter (i.e., engine)
using the incompatible model with the completions api

the following code should work:
response = openai.completions.create( # changed
    model=""gpt-3.5-turbo-instruct"", # changed
    prompt=prompt,
    max_tokens=150
)",https://stackoverflow.com/questions/78046390,python,23-02-2024 09:35,3265.0,-5.0,1.0,True,23-02-2024 15:50,23-02-2024 10:05
70754085,valueerror: the state dictionary of the model you are trying to load is corrupted. are you sure it was properly saved?,"goal: amend this notebook to work with albert-base-v2 model
kernel: conda_pytorch_p36.
section 1.2 instantiates a model from files in ./mrpc/ dir.
however, i think it is for a bert model, not albert. so, i downloaded an albert config.json file from here. it is this chnage that causes the error.
what else do i need to do in order to instantiate an albert model?

./mrpc/ dir:
!curl  --output mprc.zip
!unzip -n mprc.zip

from os import listdir
from os.path import isfile, join
ï¿½ï¿½ï¿½
mypath = './mrpc/'
onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]
onlyfiles
---

['tokenizer_config.json',
 'special_tokens_map.json',
 'pytorch_model.bin',
 'config.json',
 'training_args.bin',
 'added_tokens.json',
 'vocab.txt']

configs:
# the output directory for the fine-tuned model, $out_dir.
configs.output_dir = ""./mrpc/""

# the data directory for the mrpc task in the glue benchmark, $glue_dir/$task_name.
configs.data_dir = ""./glue_data/mrpc""

# the model name or path for the pre-trained model.
configs.model_name_or_path = ""albert-base-v2""
# the maximum length of an input sequence
configs.max_seq_length = 128

# prepare glue task.
configs.task_name = ""mrpc"".lower()
configs.processor = processors[configs.task_name]()
configs.output_mode = output_modes[configs.task_name]
configs.label_list = configs.processor.get_labels()
configs.model_type = ""albert"".lower()
configs.do_lower_case = true

# set the device, batch size, topology, and caching flags.
configs.device = ""cpu""
configs.eval_batch_size = 1
configs.n_gpu = 0
configs.local_rank = -1
configs.overwrite_cache = false

model:
model = albertforsequenceclassification.from_pretrained(configs.output_dir)  # !
model.to(configs.device)

traceback:
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
<ipython-input-36-0936fd8cbb17> in <module>
      1 # load model
----> 2 model = albertforsequenceclassification.from_pretrained(configs.output_dir)
      3 model.to(configs.device)
      4 
      5 # quantize model

~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   1460                     pretrained_model_name_or_path,
   1461                     ignore_mismatched_sizes=ignore_mismatched_sizes,
-> 1462                     _fast_init=_fast_init,
   1463                 )
   1464 

~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/modeling_utils.py in _load_state_dict_into_model(cls, model, state_dict, pretrained_model_name_or_path, ignore_mismatched_sizes, _fast_init)
   1601             if any(key in expected_keys_not_prefixed for key in loaded_keys):
   1602                 raise valueerror(
-> 1603                     ""the state dictionary of the model you are training to load is corrupted. are you sure it was ""
   1604                     ""properly saved?""
   1605                 )

valueerror: the state dictionary of the model you are trying to load is corrupted. are you sure it was properly saved?","['python', 'huggingface-transformers', 'bert-language-model', 'onnx', 'huggingface-tokenizers']",70757277,"exactly what i was looking for, textattack/albert-base-v2-mrpc
how to use from the ï¿½ï¿½ï¿½ï¿½/transformers library
from transformers import autotokenizer, automodelforsequenceclassification

tokenizer = autotokenizer.from_pretrained(""textattack/albert-base-v2-mrpc"")

model = automodelforsequenceclassification.from_pretrained(""textattack/albert-base-v2-mrpc"")

or just clone the model repo
git lfs install
git clone 
# if you want to clone without large files ï¿½ï¿½ï¿½ just their pointers
# prepend your git clone with the following env var:
git_lfs_skip_smudge=1
<",https://stackoverflow.com/questions/70754085,python,18-01-2022 10:33,3790.0,0.0,1.0,True,01-12-2022 12:09,01-12-2022 12:09
79101912,i am not able to get the desired response from http post api using the streamingresponse from the openai azure,"i am using the azure openai to generate the response based on the query i am send it using the postman in the postapi with the boy like:
{""query"":""what is de-iceing""}
now, once it is running i am able to see that i am getting the terminal as azure function runtime version: 3.11 [2024-10-18t] request body: b'{""query"":""what is de-iceing""}' [2024-10-18t] data
but i am not getting the generated response from the openai in the streamingresponse.
attached is the complete code of my azure function.
# importing the libraies
import azure.functions as func
import logging
# from langchain_text_splitters import charactertextsplitter
from langchain_openai.embeddings import openaiembeddings
from langchain_openai import azureopenaiembeddings
import os
from openai import azureopenai
import openai
# from dotenv import load_dotenv
from langchain_community.vectorstores.azuresearch import azuresearch
import json
from azurefunctions.extensions. import request, streamingresponse
import asyncio
# load_dotenv()

# create instance of an azure dunction 
app = func.functionapp(

# get data from azure open ai
# async def stream_processor(response):
#     async for chunk in response:
#         if len(chunk.choices) > 0:
#             delta = chunk.choices[0].delta
#             if delta.content: # get remaining generated response if applicable
#                 await asyncio.sleep(0.1)
#                 yield delta.content

async def stream_processor(response):
    if hasattr(response, '__aiter__'):
        async for chunk in response:
            if len(chunk.choices) > 0:
                delta = chunk.choices[0].delta
                if delta.content:
                    await asyncio.sleep(0.1)
                    yield delta.content
    else:
        # handle non-async iterable case if necessary
        for chunk in response:
            if len(chunk.choices) > 0:
                delta = chunk.choices[0].delta
                if delta.content:
                    yield delta.content

@app.route(route=""v2_ammbackend"")
async def v2_ammbackend(req: request) -> streamingresponse:
    logging.info('python http trigger function processed a request.')

    runtime_version = os.environ.get('functions_worker_runtime_version')
    logging.info(f""azure function runtime version: {runtime_version}"")
    logging.info(req)
    req_body = await req.body()  # use await to get the body asynchronously
    logging.info(f'request body: {req_body}')
    
    # parse the request body as json
    data = json.loads(req_body.decode('utf-8'))
    logging.info('data')
    logging.info(data)

    # req_body = req.get_body().decode('utf-8')
    # # parse the request body as json
    # data = json.loads(req_body)
    # access the variables from the request body
    variable1 = data.get(""query"")
    logging.info(variable1)

    # use azureopenaiembeddings with an azure account
    embeddings: azureopenaiembeddings = azureopenaiembeddings(
        azure_deployment=os.environ.get(""emb_azure_deployment""),
        openai_api_version=os.environ.get(""emb_azure_openai_api_version""),
        azure_endpoint=os.environ.get(""emb_azure_endpoint""),
        api_key=os.environ.get('emb_azure_openai_api_key'),
    )

    # calling the azuresearch with index_name
    index_name= ""de-iceing-test""
    vector_store: azuresearch = azuresearch(
        azure_search_endpoint=os.environ.get(""vector_store_address""),
        azure_search_key=os.environ.get(""vector_store_password""),
        index_name=index_name,
        embedding_function=embeddings.embed_query,
    )

    # calling the azureopenai
    client = azureopenai(
    api_key = os.getenv(""gpt_azure_openai_api_key""),  
    api_version = ""2023-07-01-preview"",
    azure_endpoint = os.getenv(""gpt_azure_openai_endpoint"")
    )

    # perform a similarity search
    def userchat(query):
        docs = vector_store.similarity_search(
            query=fr""{query}"",
            k=2,
            search_type=""similarity"",
        )
        chunk = [item.page_content for item in docs]
        return chunk
    
    context = userchat(variable1)
    prompt = fr""context: {context} \
                based on the above context please provide a clear and concise answer to the user's query/question mentioned below.\
                query: {variable1}.\
                if the query is about the procedue or the method then generate the text in bullet points.""
    prompt1 = fr""here is the query from the user, query: {variable1} \
                can you please provide a clear and concise solution to the user's query based on the content below and summary below.\
                content: {context}""

    # async def get_chat_completion(prompt):
    response = client.chat.completions.create(
        model=""cbothr-gpt-4-001"",
        stream=true,
        messages=[
            {""role"": ""user"", ""content"": f""{prompt}""}
        ]
    )
    logging.info('response')
    logging.info(response)

# make sure to call the function within an async context
    if hasattr(response, '__aiter__'):
        return streamingresponse(stream_processor(response), media_type=""text/event-stream"")
    else:
        return func.
        ""error: response is not iterable."",
        status_code=500
    )

i am not able to get the response as mentioned in the last lines of the code, whereas i am getting as:
{
    ""_ 500,
    ""_ ""text/plain"",
    ""_ ""utf-8"",
    ""_ {},
    ""_ ""error: response is not iterable.""
}

what should i do?","['azure', 'azure-functions', 'postman', 'openai-api', 'streamingresponsebody']",79110066,"you need to create the azure openai client by using asyncazureopenai as given below.
client = openai.asyncazureopenai(
    api_key = os.getenv(""gpt_azure_openai_api_key""),  
    api_version = ""2024-08-01-preview"",
    azure_endpoint = os.getenv(""gpt_azure_openai_endpoint"")
    )

your complete code should look like below.
import azure.functions as func
import logging
from langchain_openai import azureopenaiembeddings
import os
import openai
from langchain_community.vectorstores.azuresearch import azuresearch
import json
from azurefunctions.extensions. import request, streamingresponse
import asyncio

app = func.functionapp(

async def stream_processor(response):
    async for chunk in response:
        if len(chunk.choices) > 0:
            delta = chunk.choices[0].delta
            if delta.content: 
                await asyncio.sleep(0.1)
                yield delta.content


@app.route(route="" methods=[func.
async def  request) -> streamingresponse:
    logging.info('python http trigger function processed a request.')
    
    req_body = await req.body()  
    logging.info(f'request body: {req_body}') 
    data = json.loads(req_body.decode('utf-8'))
    variable1 = data.get(""query"")
    
    embeddings: azureopenaiembeddings = azureopenaiembeddings(
        azure_deployment=os.environ.get(""emb_azure_deployment""),
        openai_api_version=os.environ.get(""emb_azure_openai_api_version""),
        azure_endpoint=os.environ.get(""emb_azure_endpoint""),
        api_key=os.environ.get('emb_azure_openai_api_key'),
    )

    index_name= ""de-iceing-test""
    vector_store: azuresearch = azuresearch(
        azure_search_endpoint=os.environ.get(""vector_store_address""),
        azure_search_key=os.environ.get(""vector_store_password""),
        index_name=index_name,
        embedding_function=embeddings.embed_query,
    )

    client = openai.asyncazureopenai(
    api_key = os.getenv(""gpt_azure_openai_api_key""),  
    api_version = ""2024-08-01-preview"",
    azure_endpoint = os.getenv(""gpt_azure_openai_endpoint"")
    )

    def userchat(query):
        docs = vector_store.similarity_search(
            query=fr""{query}"",
            k=2,
            search_type=""similarity"",
        )
        chunk = [item.page_content for item in docs]
        return chunk
    
    context = userchat(variable1)
    prompt = fr""content: {context} \
                please provide a clear and concise answer to the user's query/question mentioned below.\
                query: {variable1}.\
                if the query is about the procedue or the method then generate the text in bullet points.""
    prompt1 = fr""here is the query from the user, query: {variable1} \
                can you please provide a clear and concise solution to the user's query based on the content below and summary below.\
                content: {context}""
    
    logging.info(prompt)

    response = await client.chat.completions.create(
        model=""gpt-4"",
        messages=[
            {""role"": ""user"", ""content"": f""{prompt}""}
        ],
        stream=true
    )
    logging.info(f""openai api response object: {response}"")

    if response is not none:
        logging.info(""streaming started..."")
        return streamingresponse(stream_processor(response), media_type=""text/event-stream"")
    else:
        return func.
            ""error: response is not iterable or streaming not supported."",
            status_code=500
    )",https://stackoverflow.com/questions/79101912,azure,18-10-2024 11:38,156.0,0.0,1.0,True,21-10-2024 12:33,18-10-2024 12:42
77352103,how to use langchain retrievalqa with asyncio?,"i want to parallelize retrievalqa with asyncio but i am unable to figure out how.
this is how my code works serially:
import langchain
from langchain.chat_models import chatopenai
from langchain import prompttemplate, llmchain
from langchain.chains import retrievalqa
from langchain.vectorstores import faiss
from langchain.schema.vectorstore import vectorstoreretriever
import asyncio
import nest_asyncio

retriever = vectorstoreretriever(vectorstore=faiss(...))

chat = chatopenai(model=""gpt-3.5-turbo-16k"", temperature=0.7)

qa_chain = retrievalqa.from_llm(chat, retriever= retriever
                                                 #,memory=memory
                                                 , return_source_documents=true
                                                 )

queries = ['query1', 'query2', 'query3']
data_to_append = []

for query in queries :

    vectordbkwargs = {""search_distance"": 0.9}
    result = qa_chain({""query"": query, ""vectordbkwargs"": vectordbkwargs})

    data_to_append.append({""query"": query, ""source_documents"": result[""source_documents""], ""generated_text"": result[""result""]})


here was my attempt to parallelize it with asyncio but retrievalqa doesn't seem to work async:
import langchain
from langchain.chat_models import chatopenai
from langchain import prompttemplate, llmchain
from langchain.chains import retrievalqa
from langchain.vectorstores import faiss
from langchain.schema.vectorstore import vectorstoreretriever
import asyncio
import nest_asyncio

retriever = vectorstoreretriever(vectorstore=faiss(...))

chat = chatopenai(model=""gpt-3.5-turbo-16k"", temperature=0.7)


qa_chain = retrievalqa.from_llm(chat, retriever= retriever
                                                 , return_source_documents=true
                                                 )

queries = ['query1', 'query2', 'query3']
data_to_append = []



async def process_query(query):

        vectordbkwargs = {""search_distance"": 0.9}
        result = await qa_chain({""query"": query, ""vectordbkwargs"": vectordbkwargs})
        data_to_append.append({""query"": query, ""source_documents"": result[""source_documents""], ""generated_text"": result[""result""]})


async def main():

    tasks = []

    for query in queries: # iterate all rows
        task = process_query(query)
        tasks.append(task)

    await asyncio.gather(*tasks)

if __name__ == ""__main__"":
    nest_asyncio.apply()
    asyncio.run(main())

any help would be greatly appreciated.","['python', 'python-asyncio', 'langchain', 'large-language-model']",77365779,"to make it work async, the solution i found was to use retrievalqa._acall instead of just using retrievalqa.
here is a sample code snippet which has only one minor change:
import langchain
from langchain.chat_models import chatopenai
from langchain import prompttemplate, llmchain
from langchain.chains import retrievalqa
from langchain.vectorstores import faiss
from langchain.schema.vectorstore import vectorstoreretriever
import asyncio
import nest_asyncio

retriever = vectorstoreretriever(vectorstore=faiss(...))

chat = chatopenai(model=""gpt-3.5-turbo-16k"", temperature=0.7)


qa_chain = retrievalqa.from_llm(chat, retriever= retriever
                                                 , return_source_documents=true
                                                 )

queries = ['query1', 'query2', 'query3']
data_to_append = []



async def process_query(query):

        vectordbkwargs = {""search_distance"": 0.9}
        # change qa_chain to qa_chain._acall
        result = await qa_chain._acall({'query': query, ""vectordbkwargs"": vectordbkwargs})
        data_to_append.append({""query"": query, ""source_documents"": result[""source_documents""], ""generated_text"": result[""result""]})


async def main():

    tasks = []

    for query in queries: # iterate all rows
        task = process_query(query)
        tasks.append(task)

    await asyncio.gather(*tasks)

if __name__ == ""__main__"":
    nest_asyncio.apply()
    asyncio.run(main())",https://stackoverflow.com/questions/77352103,python,24-10-2023 12:36,1587.0,-2.0,1.0,True,26-10-2023 09:31,26-10-2023 09:26
67958953,paysify sentiment api returning null,"good day fam. i have this sentiment analysis api from paysify, which returns a json output but all of a sudden it is now returning null as output.
please i help here, i have been trying to fix this for the past 4 hours, no headway.
   function detect_sentiment($string){
      $string = urlencode($string);
      $api_key = ""< api key >"";
      $url = '
      $ch = curl_init();
      curl_setopt($ch, curlopt_url, $url);
      curl_setopt($ch, curlopt_ssl_verifypeer, false);
      curl_setopt($ch, curlopt_returntransfer, true);
      
      $result = curl_exec($ch);
      $response = json_decode($result,true);
      curl_close($ch);
      return $response;
   }
      
  print_r(detect_sentiment(""i love this product""));


thanks","['php', 'nlp', 'sentiment-analysis']",67959000,"this issue is not related to your code. the service you use has an issue on their server. if you try to open your chrome browser and navigate to  you will see that the page can not be opened for an ssl error.
please contact your api provider to resolve.
ps: remove the api key from your initial question or anyone can use it.",https://stackoverflow.com/questions/67958953,php,13-06-2021 13:47,62.0,0.0,1.0,True,13-06-2021 13:53,13-06-2021 13:52
72590264,is there a required size of data set for lda to work in python?,"i am trying to apply lda on stack overflow posts for a project. my corpus has about 650 post of interest and i was wondering if the size of the corpus would hinder the functioning of the lda later on especially when i have to specify the k value?
i have not applied lda before, therefore some hints on this would really help me.","['python', 'nlp', 'gensim', 'lda', 'topic-modeling']",72595847,it might work. what happens when you try it? are the results useful according to your goals with those 650 posts?,https://stackoverflow.com/questions/72590264,python,12-06-2022 07:31,409.0,1.0,1.0,True,12-06-2022 21:01,12-06-2022 19:45
12918606,what is the default nltk part of speech tagset?,"while experimenting with nltk part of speech tagging, i noticed a lot of vbp tags in the output of my calls to nltk.pos_tag.  i noticed this tag is not in the brown corpus part of speech tagset.  it is however a part of the upenn tagset.
what tagset does nltk use by default?  i can't find this in the official documentation or the apidocs.","['python', 'nlp', 'nltk']",13836122,ntlk uses penntreebank tagset . have a look at this link,https://stackoverflow.com/questions/12918606,python,16-10-2012 15:51,4371.0,8.0,3.0,True,10-03-2022 10:35,25-06-2015 04:51
67496616,"runtimeerror: input, output and indices must be on the current device. (fill_mask(&quot;random text &lt;mask&gt;.&quot;)","i am getting ""runtimeerror: input, output and indices must be on the current device.""
when i run this line.
fill_mask(""auto car ."")
i am running it on colab.
my code:
from transformers import berttokenizer, bertformaskedlm
from pathlib import path
from tokenizers import bytelevelbpetokenizer
from transformers import berttokenizer, bertformaskedlm


paths = [str(x) for x in path(""."").glob(""**/*.txt"")]
print(paths)

bert_tokenizer = berttokenizer.from_pretrained('bert-base-uncased')

from transformers import bertmodel, bertconfig

configuration = bertconfig()
model = bertmodel(configuration)
configuration = model.config
print(configuration)

model = bertformaskedlm.from_pretrained(""bert-base-uncased"")

from transformers import linebylinetextdataset
dataset = linebylinetextdataset(
    tokenizer=bert_tokenizer,
    file_path=""./kant.txt"",
    block_size=128,
)

from transformers import datacollatorforlanguagemodeling
data_collator = datacollatorforlanguagemodeling(
    tokenizer=bert_tokenizer, mlm=true, mlm_probability=0.15
)

from transformers import trainer, trainingarguments

training_args = trainingarguments(
    output_dir=""./kantaibert"",
    overwrite_output_dir=true,
    num_train_epochs=1,
    per_device_train_batch_size=64,
    save_steps=10_000,
    save_total_limit=2,
    )

trainer = trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

trainer.train()

from transformers import pipeline

fill_mask = pipeline(
    ""fill-mask"",
    model=model,
    tokenizer=bert_tokenizer
)

fill_mask(""auto car <mask>."")

the last line is giving me the error mentioned above. please let me know what i am doing wrong or what i have to do in order to remove this error.","['python', 'nlp', 'pytorch', 'bert-language-model', 'huggingface-transformers']",67510250,"the trainer trains your model automatically at gpu (default value no_cuda=false). you can verify this by running:
model.device

after training. the pipeline does not this and this leads to the error you see (i.e. your model is on your gpu but your example sentence is on your cpu). you can fix that by either run the pipeline with gpu support as well:
fill_mask = pipeline(
    ""fill-mask"",
    model=model,
    tokenizer=bert_tokenizer,
    device=0,
)

or by transferring your model to cpu before initializing the pipeline:
model.to('cpu')",https://stackoverflow.com/questions/67496616,python,12-05-2021 02:36,2341.0,0.0,1.0,True,12-05-2021 19:58,12-05-2021 17:07
64550503,huggingface saving tokenizer,"i am trying to save the tokenizer in huggingface so that i can load it later from a container where i don't need access to the internet.
base_model = ""distilbert-base-multilingual-cased""
tokenizer = autotokenizer.from_pretrained(base_model)
tokenizer.save_vocabulary(""./models/tokenizer/"")
tokenizer2 = autotokenizer.from_pretrained(""./models/tokenizer/"")

however, the last line is giving the error:
oserror: can't load config for './models/tokenizer3/'. make sure that:

- './models/tokenizer3/' is a correct model identifier listed on '

- or './models/tokenizer3/' is the correct path to a directory containing a config.json file

transformers version: 3.1.0
how to load the saved tokenizer from pretrained model in pytorch didn't help unfortunately.
edit 1
thanks to @ashwin's answer below i tried save_pretrained instead, and i get the following error:
oserror: can't load config for './models/tokenizer/'. make sure that:

- './models/tokenizer/' is a correct model identifier listed on '

- or './models/tokenizer/' is the correct path to a directory containing a config.json file

the contents of the tokenizer folder is below:

i tried renaming tokenizer_config.json to config.json and then i got the error:
valueerror: unrecognized model in ./models/tokenizer/. should have a `model_type` key in its config.json, or contain one of the following strings in its name: retribert, t5, mobilebert, distilbert, albert, camembert, xlm-roberta, pegasus, marian, mbart, bart, reformer, longformer, roberta, flaubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm, ctrl, electra, encoder-decoder","['huggingface-transformers', 'huggingface-tokenizers']",64552678,"save_vocabulary(), saves only the vocabulary file of the tokenizer (list of bpe tokens).
to save the entire tokenizer, you should use save_pretrained()
thus, as follows:
base_model = ""distilbert-base-multilingual-cased""
tokenizer = autotokenizer.from_pretrained(base_model)
tokenizer.save_pretrained(""./models/tokenizer/"")
tokenizer2 = distilberttokenizer.from_pretrained(""./models/tokenizer/"")

edit:
for some unknown reason:
instead of
tokenizer2 = autotokenizer.from_pretrained(""./models/tokenizer/"")
using
tokenizer2 = distilberttokenizer.from_pretrained(""./models/tokenizer/"")
works.",https://stackoverflow.com/questions/64550503,huggingface-transformers,27-10-2020 08:20,52495.0,32.0,3.0,True,16-05-2021 16:13,28-10-2020 00:57
78019134,how to properly save the finetuned transformer model in safetensors without losing frozen parameters?,"i have been trying to finetune a casual lm model by retraining its lm_head layer. i've been training with deepspeed zero stage 3 (this part works fine). but i have problem saving my finetuned model and loading it back. i think the problem is that the unwrapped_model.save_pretrained() function automatically ignores the frozen parameters during saving. here is my code and error messages:
# finetuning:

accelerator = accelerator(log_with=""tensorboard"", project_dir=project_dir)
model: torch.nn.module = automodelforcausallm.from_pretrained(""the path to lm model"", trust_remote_code=true)
model.half()
model.train()

# freeze parameters
for param in model.parameters():
    param.requires_grad = false
for param in model.lm_head.parameters():
    param.requires_grad = true  

...

# save finetuned model
if step == 5000 and accelerator.is_main_process:
     unwrapped_model: pretrainedmodel = accelerator.unwrap_model(model)
     save_fn = accelerator.save
     unwrapped_model.save_pretrained(
         ""mycogagent"",
         is_main_process=accelerator.is_main_process,
         save_function=save_fn,
     )


the codes above will print a warning:
removed shared tensor {a long list of parameter names in the original lm model except the parameter name of lm_head} while saving. this should be ok, but check by verifying that you don't receive any warning while reloading

and the saved model only takes 7mb disk space, however, i was expecting the saved model to be over 30gb. looks like only the unfrozen part is saved to disk.
to verify my speculation, i tried to load it back with the following codes:
model = automodelforcausallm.from_pretrained(""mycogagent"", trust_remote_code=true)

but it will result in an error of size mismatch.
runtimeerror: error(s) in loading state_dict for cogagentforcausallm:
        size mismatch for model.embed_tokens.weight: copying a param with shape torch.size([0]) from checkpoint, the shape in current model is torch.size([32000, 4096]).
        you may consider adding `ignore_mismatched_sizes=true` in the model `from_pretrained` method.

i alse tried following the instruction in the error message, but it's not working, either. the program printed a list of warnings and got stuck.
some weights of cogagentforcausallm were not initialized from the model checkpoint at mycogagent and are newly initialized:[a list of parameter names]
you should probably train this model on a down-stream task to be able to use it for predictions and inference.

some weights of cogagentforcausallm were not initialized from the model checkpoint at mycogagent and are newly initialized because the shapes did not match:
- model.embed_tokens.weight: found shape torch.size([0]) in the checkpoint and torch.size([32000, 4096]) in the model instantiated
you should probably train this model on a down-stream task to be able to use it for predictions and inference.


the warning messages clearly suggests loading the finetuned model is unsuccessful, and the reason why the program stuck looks like another issue. but all in all, my problem is how to save the full model instead of only the finetuned parameters? what's the proper convention to save/load finetuned huggingface models ?
expected behaviors:
the save_pretrained function should save all the tensors in the huggingface transformer model, even if their requires_grad attribute is false.
--- update ---
i just located the cause of my problem. the state_dicts works fine during the entire saving process, but id_tensor_storage(tensor) function (in site-packages/transformers/pytorch_utils.py) will not get the correct pointer to the tensor. the output of this function will always be (device(type='cuda', index=0), 0, 0). 
in practice, the unique_id should be equal to the memory address of the tensor instead of 0. thus, the source of this issue must lie in accelerate.unwrap function.","['python', 'pytorch', 'huggingface-transformers', 'accelerate', 'safe-tensors']",78192226,"i think i found the solution. the problem is, in zero3 we have to call accelerator.get_state_dict(model) function before saving. directly saving the model itself won't work because its parameters are stored across different gpus. calling accelerator.get_state_dict(model) can force deepspeed to collect the values of all parameters. there is an example [here][1]
[1]:",https://stackoverflow.com/questions/78019134,python,19-02-2024 08:06,5737.0,3.0,2.0,True,20-03-2024 09:38,20-02-2024 07:54
55619176,how to cluster similar sentences using bert,"for elmo, fasttext and word2vec, i'm averaging the word embeddings within a sentence and using hdbscan/kmeans clustering to group similar sentences.
a good example of the implementation can be seen in this short article: 
i would like to do the same thing using bert (using the bert python package from hugging face), however i am rather unfamiliar with how to extract the raw word/sentence vectors in order to input them into a clustering algorithm. i know that bert can output sentence representations - so how would i actually extract the raw vectors from a sentence?
any information would be helpful.","['python', 'nlp', 'artificial-intelligence', 'word-embedding', 'bert-language-model']",68728666,"as subham kumar mentioned, one can use this python 3 library to compute sentence similarity: 
the library has a few code examples to perform clustering:
fast_clustering.py:
""""""
this is a more complex example on performing clustering on large scale dataset.

this examples find in a large set of sentences local communities, i.e., groups of sentences that are highly
similar. you can freely configure the threshold what is considered as similar. a high threshold will
only find extremely similar sentences, a lower threshold will find more sentence that are less similar.

a second parameter is 'min_community_size': only communities with at least a certain number of sentences will be returned.

the method for finding the communities is extremely fast, for clustering 50k sentences it requires only 5 seconds (plus embedding comuptation).

in this example, we download a large set of questions from quora and then find similar questions in this set.
""""""
from sentence_transformers import sentencetransformer, util
import os
import csv
import time


# model for computing sentence embeddings. we use one trained for similar questions detection
model = sentencetransformer('paraphrase-minilm-l6-v2')

# we donwload the quora duplicate questions dataset (
# and find similar question in it
url = ""
dataset_path = ""quora_duplicate_questions.tsv""
max_corpus_size = 50000 # we limit our corpus to only the first 50k questions


# check if the dataset exists. if not, download and extract
# download dataset if needed
if not os.path.exists(dataset_path):
    print(""download dataset"")
    util. dataset_path)

# get all unique sentences from the file
corpus_sentences = set()
with open(dataset_path, encoding='utf8') as fin:
    reader = csv.dictreader(fin, delimiter='\t', quoting=csv.quote_minimal)
    for row in reader:
        corpus_sentences.add(row['question1'])
        corpus_sentences.add(row['question2'])
        if len(corpus_sentences) >= max_corpus_size:
            break

corpus_sentences = list(corpus_sentences)
print(""encode the corpus. this might take a while"")
corpus_embeddings = model.encode(corpus_sentences, batch_size=64, show_progress_bar=true, convert_to_tensor=true)


print(""start clustering"")
start_time = time.time()

#two parameters to tune:
#min_cluster_size: only consider cluster that have at least 25 elements
#threshold: consider sentence pairs with a cosine-similarity larger than threshold as similar
clusters = util.community_detection(corpus_embeddings, min_community_size=25, threshold=0.75)

print(""clustering done after {:.2f} sec"".format(time.time() - start_time))

#print for all clusters the top 3 and bottom 3 elements
for i, cluster in enumerate(clusters):
    print(""\ncluster {}, #{} elements "".format(i+1, len(cluster)))
    for sentence_id in cluster[0:3]:
        print(""\t"", corpus_sentences[sentence_id])
    print(""\t"", ""..."")
    for sentence_id in cluster[-3:]:
        print(""\t"", corpus_sentences[sentence_id])


kmeans.py:
""""""
this is a simple application for sentence embeddings: clustering

sentences are mapped to sentence embeddings and then k-mean clustering is applied.
""""""
from sentence_transformers import sentencetransformer
from sklearn.cluster import kmeans

embedder = sentencetransformer('paraphrase-minilm-l6-v2')

# corpus with example sentences
corpus = ['a man is eating food.',
          'a man is eating a piece of bread.',
          'a man is eating pasta.',
          'the girl is carrying a baby.',
          'the baby is carried by the woman',
          'a man is riding a horse.',
          'a man is riding a white horse on an enclosed ground.',
          'a monkey is playing drums.',
          'someone in a gorilla costume is playing a set of drums.',
          'a cheetah is running behind its prey.',
          'a cheetah chases prey on across a field.'
          ]
corpus_embeddings = embedder.encode(corpus)

# perform kmean clustering
num_clusters = 5
clustering_model = kmeans(n_clusters=num_clusters)
clustering_model.fit(corpus_embeddings)
cluster_assignment = clustering_model.labels_

clustered_sentences = [[] for i in range(num_clusters)]
for sentence_id, cluster_id in enumerate(cluster_assignment):
    clustered_sentences[cluster_id].append(corpus[sentence_id])

for i, cluster in enumerate(clustered_sentences):
    print(""cluster "", i+1)
    print(cluster)
    print("""")

agglomerative.py:
""""""
this is a simple application for sentence embeddings: clustering

sentences are mapped to sentence embeddings and then agglomerative clustering with a threshold is applied.
""""""
from sentence_transformers import sentencetransformer
from sklearn.cluster import agglomerativeclustering
import numpy as np

embedder = sentencetransformer('paraphrase-minilm-l6-v2')

# corpus with example sentences
corpus = ['a man is eating food.',
          'a man is eating a piece of bread.',
          'a man is eating pasta.',
          'the girl is carrying a baby.',
          'the baby is carried by the woman',
          'a man is riding a horse.',
          'a man is riding a white horse on an enclosed ground.',
          'a monkey is playing drums.',
          'someone in a gorilla costume is playing a set of drums.',
          'a cheetah is running behind its prey.',
          'a cheetah chases prey on across a field.'
          ]
corpus_embeddings = embedder.encode(corpus)

# normalize the embeddings to unit length
corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=true)

# perform kmean clustering
clustering_model = agglomerativeclustering(n_clusters=none, distance_threshold=1.5) #, affinity='cosine', linkage='average', distance_threshold=0.4)
clustering_model.fit(corpus_embeddings)
cluster_assignment = clustering_model.labels_

clustered_sentences = {}
for sentence_id, cluster_id in enumerate(cluster_assignment):
    if cluster_id not in clustered_sentences:
        clustered_sentences[cluster_id] = []

    clustered_sentences[cluster_id].append(corpus[sentence_id])

for i, cluster in clustered_sentences.items():
    print(""cluster "", i+1)
    print(cluster)
    print("""")",https://stackoverflow.com/questions/55619176,python,10-04-2019 18:31,47867.0,35.0,6.0,True,12-05-2023 22:47,28-01-2020 20:52
58605143,dl4j does not contain text and models modules,"in my project, i need to use the word2vec model. i decide to use dl4j for this. i load all dependencies, but there are no modules such as deeplearning4j.text and deeplearning4j.models 
i tried to change the version of dl4j from beta5 to 4 and 3 but the problem still here
i even tried download jar files of dl4j but it didn't help too","['java', 'neural-network', 'nlp', 'deeplearning4j']",58605582,it's deeplearning4j.nlp module and corresponding submodules like deeplearning4j.nlp.uima for example,https://stackoverflow.com/questions/58605143,java,29-10-2019 10:01,107.0,0.0,1.0,True,03-04-2025 16:24,30-10-2019 09:00
77248641,openai api: will the data i send with api requests remain private?,"i have created a q&a bot using the openai embeddings api endpoint, pinecone as a vector database, and openai as an llm. i am using langchain and the gpt-3.5-turbo model. i am using my own dataset (pdf) files against which the question will be answered.
the solution is working properly. as of now, i have added test pdf files, but i want to use my private pdf files. does my data remain private in this architecture?
does openai index my data in public space, or will it remain private to me?","['openai-api', 'langchain', 'chatgpt-api', 'pinecone']",77248704,"the answer is not simple.
as of today, openai doesn't train models on inputs and outputs through api, as stated in the official openai documentation:

but, technically speaking, once you make a request to the openai api, you send data to the outside world. this is a big concern for many companies or even individuals. openai tries hard to minimize these concerns with commitments, as stated on the official openai website:

ownership: you own and control your data

we do not train on your data from chatgpt enterprise or our api platform
you own your inputs and outputs (where allowed by law)
you control how long your data is retained (chatgpt enterprise)

control: you decide who has access

enterprise-level authentication through saml sso
fine-grained control over access and available features
custom models are yours alone to use, they are not shared with anyone else

security: comprehensive compliance

weï¿½ï¿½ï¿½ve been audited for soc 2 compliance
data encryption at rest (aes-256) and in transit (tls 1.2+)
visit our trust portal to understand more about our security measures


it's up to you to decide whether these commitments are enough for you to be comfortable making requests with (possibly) sensitive data to the openai api. if yes, use the openai api. otherwise, run your local llm.",https://stackoverflow.com/questions/77248641,openai-api,07-10-2023 06:29,2184.0,1.0,1.0,True,15-10-2023 04:13,15-10-2023 04:13
16053381,how can morph dictionary be built using postgresql?,i need to create special morphological vocabulary for a natural language. each word should contain a set of characteristics. does postgresql 9.* help in such situation? i mean: should i create table from the scratch or there are some predefined means?,"['postgresql', 'dictionary', 'nlp', 'morphological-analysis']",16661100,"you are looking for things well beyond what typical solutions on postgresql are designed to build.  your best bet is to build your own custom table.
now, i am just an interested reader of linguistic topics, but for morphological analysis, i suspect you are going to run into problems with different languages having fundamentally different morphological systems.  for example, the morphological concepts in austronesian, indo-european, and athabascan (all of which i know just enough to be dangerous) do not strike me as very conducive to a single relational model.  for example, we might have distinct morphologies for verb tenses in ie, but not the other two.  reduplication in austronesian languages adds a bit of a curve ball (particularly when dealing with prefix/suffix/infix combinations), and athabascan has slots that don't fit the other two language's expectations of parts of speech.
so i don't think you are likely to find many general-purpose morph database schemas around.  things are likely to be language-specific and purpose-specific.  for example, building a database to look for morphological changes between middle and modern english is going to be quite different than something designed merely to analyse current morphology in, say, tweets.",https://stackoverflow.com/questions/16053381,postgresql,17-04-2013 06:49,215.0,0.0,1.0,True,09-10-2024 21:19,09-10-2024 21:19
52361543,how do i remove stop words from an arraylist of strings in python?,"i would like to remove stop words from the arraylist named arraylist1, which is stored in the data variable. 
i try the below method but it does not work. please help me check the below codes and improve the codes.thanks.    
import retrieve_ed_notes
from nltk.corpus import stopwords

data = retrieve_ed_notes.arraylist1

stop_words = set(stopwords.words('english'))


def remove_stopwords(data):
 data = [word for word in data if word not in stop_words]
 return data

for i in range(0, len(remove_stopwords(data))):
  print(remove_stopwords(data[i]))

console output of the arraylist1:
1|i really love writing journals
2|the mat is very comfortable and i will buy it again likes
3|the mousepad is smooth
1|i really love writing journals
4|this pen is very special to me.
4|this pencil is very special to me.
5|meaningful novels
4|it brights up my day like a lighter and makes me higher.
6|school foolscap
7|as soft as my heart.lovey","['python', 'nltk', 'text-analysis']",52362041,"convert the word to lower and check with stopwords.
from nltk.corpus import stopwords
stopwords=set(stopwords.words('english'))

data =['i really love writing journals','the mat is very comfortable and i will buy it again likes','the mousepad is smooth']

def remove_stopwords(data):
    output_array=[]
    for sentence in data:
        temp_list=[]
        for word in sentence.split():
            if word.lower() not in stopwords:
                temp_list.append(word)
        output_array.append(' '.join(temp_list))
    return output_array

output=remove_stopwords(data)

print(output)
['really love writing journals','mat comfortable buy likes', 'mousepad smooth']",https://stackoverflow.com/questions/52361543,python,17-09-2018 06:01,3091.0,1.0,1.0,True,24-11-2023 08:20,17-09-2018 06:33
78543858,kernel crashes when running a faiss similarity search,"i am working with langchain right now and created a faiss vector store. since today, my kernel crashes when running a similarity search on my vector store. has anyone an idea why this is happening?
from langchain_community.document_loaders import pypdfloader
from langchain_community.vectorstores import faiss

f = open('credentials.txt')
openai_api_key = f.read()
embeddings_model = openaiembeddings(api_key=openai_api_key)

document_loader = pypdfloader('filename.pdf')
text_splitter=recursivecharactertextsplitter()
documents = document_loader.load_and_split(text_splitter)

vectorstore = faiss.from_documents(documents, embeddings_model)
vectorstore.similarity_search('query')

macbook pro intel, python 3.9, jupyter notebook, langchain 0.2.0
actually, i saved the vector store on my local machine and re-loaded it in later sessions. since my kernel crashes, i tried to recreate the vector store. i tried to re-install faiss-cpu. i closed all 'heavy' programs on my machine. unfortunately, i had no success and also couldn't  find any help on the internet yet.","['openai-api', 'langchain', 'faiss', 'similarity-search', 'vectorstore']",78546131,"what version of faiss-cpu have you installed? i was having the same issue with the latest (faiss-cpu==1.8.0), then i changed to faiss-cpu==1.7.4 and it works just fine. this is my current setup library-wise:
faiss-cpu==1.7.4
langchain==0.2.0
langchain-community==0.2.0
langchain-openai==0.1.7
i'm running this on an m1 machine",https://stackoverflow.com/questions/78543858,openai-api,28-05-2024 11:34,1302.0,3.0,1.0,True,29-05-2024 05:04,28-05-2024 11:54
10401076,difference between constituency parser and dependency parser,what is the difference between a constituency parser and a dependency parser? what are the different usages of the two?,"['parsing', 'nlp', 'terminology']",10401433,"a constituency parse tree breaks a text into sub-phrases.  non-terminals in the tree are types of phrases, the terminals are the words in the sentence, and the edges are unlabeled.  for a simple sentence ""john sees bill"", a constituency parse would be:
                  sentence
                     |
       +-------------+------------+
       |                          |
  noun phrase                verb phrase
       |                          |
     john                 +-------+--------+
                          |                |
                        verb          noun phrase
                          |                |
                        sees              bill

a dependency parse connects words according to their relationships.  each vertex in the tree represents a word, child nodes are words that are dependent on the parent, and edges are labeled by the relationship.  a dependency parse of ""john sees bill"", would be:
              sees
                |
        +--------------+
subject |              | object
        |              |
      john            bill

you should use the parser type that gets you closest to your goal.  if you are interested in sub-phrases within the sentence, you probably want the constituency parse.  if you are interested in the dependency relationships between words, then you probably want the dependency parse.
the stanford parser can give you either (online demo).  in fact, the way it really works is to always parse the sentence with the constituency parser, and then, if needed, it performs a deterministic (rule-based) transformation on the constituency parse tree to convert it into a dependency tree.
more can be found here:",https://stackoverflow.com/questions/10401076,parsing,01-05-2012 16:45,40163.0,129.0,1.0,True,14-03-2025 00:26,14-03-2025 00:26
78931599,how can i optimize the data i am embedding to increase vector search result quality?,"i am trying to implement semantic/vector search for images.
to do that, i am using gpt-4-mini to analyze an image and create data from it with this prompt:
your job is to generate json data from a given image.
          
            return your output in the following format:
            {
            description: ""a description of the image. only use relevant keywords."",
            text: ""if the image contains text, include that here, otherwise remove this field"",
            keywords: ""keywords that describe the image"",
            artstyle: ""the art style of the image"",
            text_language: ""the language of the text in the image, otherwise remove this field"",,
            design_theme : ""if the image has a theme (hobby, interest, occupation etc.), include that here, otherwise remove this field"",
            }

the data i am getting back is pretty accurate (in my eyes). i am then embedding the json with the ""text-embedding-3-small"" model.
the problem is that the search results are pretty bad.
for example: i have 2 images with only text. one says ""straight outta knee surgery"" and one says ""straight outta valhalla"".
when i search for ""straight outta"", i have to turn down the similary treshold to 0.15 to get both results.
this is my postgres search function:
create
or replace function search_design_items (
  query_embedding vector (1536),
  match_threshold float,
  match_count int
) returns table (
  id bigint
) as $$
begin
    return query
    select id
    from public.design_management_items
    where 1 - (design_management_items.description_vector <=> query_embedding) > match_threshold
    order by (design_management_items.description_vector <=> query_embedding) asc
    limit match_count;
end;
$$ language plpgsql;

when i go into higher numbers (0.5) there are pretty much no results at all. this seems wrong because in every tutorial i have seen they use a threshold of 0.7+
what do i need to change in order to improve the accuracy of my search results?","['typescript', 'postgresql', 'openai-api', 'supabase', 'openaiembeddings']",78931780,"try to perform a hybrid search. all vector databases offer the hybrid search functionality.
as stated in the official weaviate blog:

hybrid search is a technique that combines multiple search algorithms
to improve the accuracy and relevance of search results. it uses the
best features of both keyword-based search algorithms with vector
search techniques. by leveraging the strengths of different
algorithms, it provides a more effective search experience for users.

in simple terms, performing a hybrid search means that you search with both keywords and embedding vectors, where you set the alpha parameter as a way to give a weight to these two. for example, setting alpha to 0 means keyword search only, while setting alpha to 1 means embedding vector search only.
i've created a project with a hybrid search in the past where you can search for lex fridman's podcast insights without watching the full episodes. see the demonstration.
here's the weaviatehybridsearch.ts file:
""use server"";

import weaviate from ""weaviate-client"";
import { podcasttype } from ""@/app/types/podcast"";

// define and export the querypodcasts function
export async function querypodcasts(searchterm: string, alpha: number) {
  /**
   * queries the podcast collection based on a search term and alpha value.
   *
   * @param {string} searchterm - the search term to query for.
   * @param {number} alpha - the alpha value to use for the hybrid search.
   * @return {promise<podcasttype[]>} - the array of podcasttype objects representing the search results.
   */

  // connect to the local weaviate instance
  const client = await weaviate.connecttolocal();

  // get the podcast collection
  const podcastcollection = await client.collections.get<
    omit<podcasttype, ""distance"">
  >(""podcast"");

  // perform the hybrid search on the podcast collection
  const { objects } = await podcastcollection.query.hybrid(searchterm, {
    limit: 10,
    alpha: alpha,
    returnmetadata: [""score""],
    returnproperties: [""number"", ""guest"", ""title"", ""transcription""],
  });

  // process the results
  const podcasts: podcasttype[] = objects.map((podcast: any) => ({
    ...podcast.properties,
    distance: podcast.metadata?.score!!,
  }));

  // return the podcasts
  return podcasts;
}",https://stackoverflow.com/questions/78931599,typescript,30-08-2024 10:47,122.0,1.0,1.0,True,30-08-2024 11:37,30-08-2024 10:54
76670856,langchain conversationalretrieval with jsonloader,"i modified the data loader of this source code  for conversationalretrievalchain to accept data as json.
i created a dummy json file and according to the langchain documentation, it fits json structure as described in the document.
{
  ""reviews"": [
    {""text"": ""great hotel, excellent service and comfortable rooms.""},
    {""text"": ""i had a terrible experience at this hotel. the room was dirty and the staff was rude.""},
    {""text"": ""highly recommended! the hotel has a beautiful view and the staff is friendly.""},
    {""text"": ""average hotel. the room was okay, but nothing special.""},
    {""text"": ""i absolutely loved my stay at this hotel. the amenities were top-notch.""},
    {""text"": ""disappointing experience. the hotel was overpriced for the quality provided.""},
    {""text"": ""the hotel exceeded my expectations. the room was spacious and clean.""},
    {""text"": ""avoid this hotel at all costs! the customer service was horrendous.""},
    {""text"": ""fantastic hotel with a great location. i would definitely stay here again.""},
    {""text"": ""not a bad hotel, but there are better options available in the area.""}
  ]
}

the code is :
import os
import sys

import openai
from langchain.chains import conversationalretrievalchain, retrievalqa
from langchain.chat_models import chatopenai
from langchain.document_loaders import directoryloader, textloader
from langchain.embeddings import openaiembeddings
from langchain.indexes import vectorstoreindexcreator
from langchain.indexes.vectorstore import vectorstoreindexwrapper
from langchain.llms import openai
from langchain.vectorstores import chroma
from langchain.document_loaders import jsonloader

os.environ[""openai_api_key""] = 'your_api_key_here'

# enable to save to disk & reuse the model (for repeated queries on the same data)
persist = false

query = none
if len(sys.argv) > 1:
  query = sys.argv[1]


if persist and os.path.exists(""persist""):
  print(""reusing index...\n"")
  vectorstore = chroma(persist_directory=""persist"", embedding_function=openaiembeddings())
  index = vectorstoreindexwrapper(vectorstore=vectorstore)
else:

  loader = jsonloader(""data/review.json"", jq_schema="".reviews[]"", content_key='text') # use this line if you only need data.json

  if persist:
    index = vectorstoreindexcreator(vectorstore_kwargs={""persist_directory"":""persist""}).from_loaders([loader])
  else:
    index = vectorstoreindexcreator().from_loaders([loader])

chain = conversationalretrievalchain.from_llm(
  llm=chatopenai(model=""gpt-3.5-turbo""),
  retriever=index.vectorstore.as_retriever()
)

chat_history = []
while true:
  if not query:
    query = input(""prompt: "")
  if query in ['quit', 'q', 'exit']:
    sys.exit()
  result = chain({""question"": query, ""chat_history"": chat_history})
  print(result['answer'])

  chat_history.append((query, result['answer']))
  query = none


some examples of results are:
prompt: can you summarize the data?
sure! based on the provided feedback, we have a mix of opinions about the hotels. one person found it to be an average hotel with nothing special, another person had a great experience with excellent service and comfortable rooms, another person was pleasantly surprised by a hotel that exceeded their expectations with spacious and clean rooms, and finally, someone had a disappointing experience with an overpriced hotel that didn't meet their expectations in terms of quality.

prompt: how many feedbacks present in the data ?
there are four feedbacks present in the data.

prompt: how many of them are positive (sentiment)?
there are four positive feedbacks present in the data.

prompt: how many of them are negative?
there are three negative feedbacks present in the data.

prompt: how many of them are neutral?
two of the feedbacks are neutral.

prompt: what is the last review you can see?
the most recent review i can see is: ""the hotel exceeded my expectations. the room was spacious and clean.""

prompt: what is the first review you can see?
the first review i can see is ""highly recommended! the hotel has a beautiful view and the staff is friendly.""

prompt: how many total texts are in the json file?
i don't know the answer.

i can chat with my data but except for the first answer, all other answers are wrong.
is there a problem with jsonloader or jq_scheme? how can i adapt the code so that i can generate the expected output?","['python', 'openai-api', 'langchain', 'chatgpt-api', 'py-langchain']",76672109,"in conversationalretrievalchain , search is setup to default 4, refer  top_k_docs_for_context: int = 4 in ../langchain/chains/conversational_retrieval/base.py .

that makes sense as you don't want to send all the vectors to llm model(associated cost too). based on the usecase, you can change the default to more manageable, using the following:
chain = conversationalretrievalchain.from_llm(
  llm=chatopenai(model=""gpt-3.5-turbo""),
  retriever=index.vectorstore.as_retriever(search_kwargs={""k"": 10})
)

with this change, you will get the result
{'question': 'how many feedbacks present in the data ?',
 'chat_history': [],
 'answer': 'there are 10 pieces of feedback present in the data.'}",https://stackoverflow.com/questions/76670856,python,12-07-2023 12:59,5884.0,4.0,1.0,True,12-07-2023 15:15,12-07-2023 13:06
77127630,openai chat completions api error: why do i get an http error 400 on the esp32 board?,"i was planning to make a project to receive the response given by chatgpt to my esp32 board for a given text. i tried the following, but after connecting to the wifi, the serial plotter shows the message http error 400. i double-checked the api key, and it was perfectly ok. can you please tell me where i got it wrong?
#include <arduinojson.h>
#include <wifi.h>
#include <

const char* ssid = ""ssid"";
const char* password = ""password"";
const char* apikey = ""api"";

const char* usermessage = ""hey there ! how are you ?"";

void setup() {
  serial.begin(115200);
  delay(1000);

  // connect to wi-fi
  wifi.begin(ssid, password);
  while (wifi.status() != wl_connected) {
    delay(1000);
    serial.println(""connecting to wifi..."");
  }
  serial.println(""connected to wifi"");

  // prepare the json payload
  dynamicjsondocument payload(1024);
  payload[""model""] = ""gpt-3.5-turbo"";
  
  jsonarray messages = payload.createnestedarray(""messages"");
  
  jsonobject systemmessage = messages.createnestedobject();
  systemmessage[""role""] = ""system"";
  systemmessage[""content""] = ""you are a helpful assistant."";
  
  jsonobject usermessageobject = messages.createnestedobject();
  usermessageobject[""role""] = ""user"";
  usermessageobject[""content""] = usermessage;

  // serialize the json payload
  string payloadstr;
  serializejson(payload, payloadstr);

  // send the request to the openai api
   
  
   ""application/json"");
   ""bearer "" + string(apikey));
  
  int  = 
  
  if ( == 200) {
    string response = 
    serial.println(""api response:"");
    serial.println(response); // print the api response
  } else {
    serial.println(""http error: "" + string(
  }

  
}

void loop() {
}


i was expecting to see the response to my text from the openai api, but it returned an http error 400. i am not getting where it went wrong.","['arduino', 'artificial-intelligence', 'openai-api', 'arduino-esp32', 'chatgpt-api']",77127965,"problem
all engines api endpoints are deprecated.

solution
use the chat completions api endpoint.
change the url from this...


...to this.",https://stackoverflow.com/questions/77127630,arduino,18-09-2023 13:25,329.0,0.0,1.0,True,12-06-2024 16:43,12-06-2024 16:43
77258294,compressfasttext pqkmeans does not install,"i would like to use compressfasttext library:

i use this command to install:
pip install compress-fasttext[full]

i get this pqkmeans error:
running install_egg_info   copying lshash3.egg-info to
build/bdist.linux-x86_64/wheel/lshash3-0.0.8-py3.8.egg-info   
running
install_scripts   
error: invalid command 'bdist_wininst'   [end of
output]
note: this error originates from a subprocess, and is likely
not a problem with pip. error: failed building wheel for lshash3

i tried python2.7, python3.7, python3.8 with ubunto 18.0.
for python 2.7, i can pip install the pqkmeans without this error. but compressfast text library does not work.
for python3.7, 3.8, i get the same error message.
any clue about this please?
update:
just want to add that i made a related issue here in case anybody wants to check:","['python', 'machine-learning', 'pip', 'nlp', 'fasttext']",77270016,"as per my comment, from the error message, it looks like the real problem is in the install of lshash3 (not updated since 2017), which is only brought in via what appears to be an undocumented dataset package texmex-python used only for optional eval/demo purposes in pqkmeans.
so, if you can make pip simply skip that dependency, you may get over this error with no other ill effects.
i haven't tested this, but you may want to try, in a fresh python 3.x virtual environment:

manually installing all the requirements of pqkmeans except texmex-python - see that project's requirements.txt for the list
then, install pqkmeans with the pip -no-deps option: pip install --no-deps pqkmeans
if that succeeds, perhaps pip install compress-fasttext[full] will then consider pqkmeans already present, and not try the problem texmex-python & lshash3 installs, and you're in business
but if it does still try, you could try manually installing compress-fastttext[full]'s unique non-pqkmeans requirements ï¿½ï¿½ï¿½ really just gensim ï¿½ï¿½ï¿½ then pip install --no-deps compress-fasttext[full]

the potential end-result would be you'd have everything but the problem packages texmex-python and lshash3, which you probably don't really need for any of the code you'll be calling.
good luck, & let me know if any form of this",https://stackoverflow.com/questions/77258294,python,09-10-2023 11:06,85.0,0.0,1.0,True,12-10-2023 08:49,12-10-2023 08:49
75253479,terminal error from download_bios.py: the following arguments are required: wetpaths,"i've been attempting to download a dataset downloaded off of paperswithcode and when i run the download program i get the following error message:
usage: download_bios.py [-h] [-o out] [-r retries] [-p n] wetpaths download_bios.py: error: the following arguments are required: wetpaths
and am not sure how to fix it
i attempted to reach out to a couple coder friends and the internet and none of them seemed to know what ""wetpaths"" were so i thought i would look here","['python', 'terminal', 'nlp', 'artificial-intelligence']",75254526,"the wetpaths argument of the download_bios.py script refers to the path of a wet file type used by commoncrawl. the source code says that it expects a

common_crawl date like 2017-43 or a path to a -wet.paths file

so you should pass a valid date as an argument (e.g. 2022-49 is the latest crawl for nov/dec 2022).
to understand where the wet format comes from and why it's used, some background information is required.
web crawls (e.g. those done by commoncrawl) were originally stored in the internet archive (arc) format. the web archive (warc) is a revision to this format that includes additional secondary data like metadata, abbreviated duplicate detection events, and later-date transformations. since 2013, commoncrawl has used the warc format which allows for more efficient storage and processing of the archives. the full warc specification can be found here.
one can think of warc files as providing the raw data from the crawl process by commoncrawl. two additional formats are offered, namely wet and wat:

the wat file format contains the metadata about the records stored in the warc format.
the wet file format contains the extracted plain text from the records stored in the warc format.",https://stackoverflow.com/questions/75253479,python,27-01-2023 02:05,88.0,1.0,1.0,True,27-01-2023 16:02,27-01-2023 16:02
66606563,use shap values to explain logisticregression classification,"i am trying to do some bad case analysis on my product categorization model using shap. my data looks something like this:

corpus_train, corpus_test, y_train, y_test = train_test_split(data['name_description'],
                                                              data['category_target'],
                                                              test_size = 0.2,
                                                              random_state=8)

vectorizer = tfidfvectorizer(stop_words='english', ngram_range=(1, 3), min_df=3, analyzer='word')

x_train = vectorizer.fit_transform(corpus_train)
x_test = vectorizer.transform(corpus_test)

model = logisticregression(max_iter=200)
model.fit(x_train, y_train)

x_train_sample = shap.sample(x_train, 100)
x_test_sample = shap.sample(x_test, 20)

masker = shap.maskers.independent(data=x_test_sample)

explainer = shap.linearexplainer(model, masker=masker)
shap_values = explainer.shap_values(x_test_sample)
x_test_array = x_test_sample.toarray()

shap.summary_plot(shap_values, x_test_array, feature_names=vectorizer.get_feature_names(), class_names=data['category'].unique())

now to save space i didn't include the actual summary plot, but it looks fine. my issue is that i want to be able to analyze a single prediction and get something more along these lines:

in other words, i want to know which specific words contribute the most to the prediction. but when i run the code in cell 36 in the image above i get an
attributeerror: 'numpy.ndarray' object has no attribute 'output_names'

i'm still confused on the indexing of shap_values. how can i solve this?","['python', 'scikit-learn', 'nlp', 'logistic-regression', 'shap']",66647619,"i was unable to find a solution with shap, but i found a solution using lime. the following code displays a very similar output where its easy to see how the model made its prediction and how much certain words contributed.
c = make_pipeline(vectorizer, classifier)

# saving a list of strings version of the x_test object
ls_x_test= list(corpus_test)

# saving the class names in a dictionary to increase interpretability
class_names = list(data.category.unique())

# create the lime explainer
# add the class names for interpretability
lime_explainer = limetextexplainer(class_names=class_names)

# explain the chosen prediction 
# use the probability results of the logistic regression
# can also add num_features parameter to reduce the number of features explained
lime_exp = lime_explainer.explain_instance(ls_x_test[idx], c.predict_proba)
lime_exp.show_in_notebook(text=true, predict_proba=true)",https://stackoverflow.com/questions/66606563,python,12-03-2021 20:09,4010.0,3.0,2.0,True,14-01-2023 05:52,12-03-2021 21:21
68875496,huggingface typeerror: &#39;&gt;&#39; not supported between instances of &#39;nonetype&#39; and &#39;int&#39;,"i am working on fine-tuning pretrained model on custom (using huggingface) dataset i will copy all code correctly from the one youtube video everything is ok but in this cell/code:
with training_args.strategy.scope():
    model=tfdistilbertforsequenceclassification.from_pretrained(""distilbert-base-uncased"")

    trainer = tftrainer(model=model,     # the instantiated ï¿½ï¿½ï¿½ï¿½ transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=test_dataset)           # evaluation dataset


    trainer.train()

it will give me this error:
typeerror: '>' not supported between instances of 'nonetype' and 'int'
</code","['deep-learning', 'data-science', 'huggingface-transformers', 'huggingface-tokenizers', 'huggingface-datasets']",69153519,"seems to be an issue with the new version of transformers.
installing version 4.6.0 worked for me.
pip install transformers==4.6.0",https://stackoverflow.com/questions/68875496,deep-learning,21-08-2021 17:54,5671.0,3.0,3.0,True,15-03-2023 19:01,25-08-2021 04:45
71050697,transformers: how to use cuda for inferencing?,"i have fine-tuned my models with gpu but inferencing process is very slow, i think this is because inferencing uses cpu by default. here is my inferencing code:
txt = ""this was nice place""
model = transformers.bertforsequenceclassification.from_pretrained(model_path, num_labels=24)
tokenizer = transformers.berttokenizer.from_pretrained('turkunlp/bert-base-finnish-cased-v1')
encoding = tokenizer.encode_plus(txt, add_special_tokens = true, truncation = true, padding = ""max_length"", return_attention_mask = true, return_tensors = ""pt"")
output = model(**encoding)
output = output.logits.softmax(dim=-1).detach().cpu().flatten().numpy().tolist()

here is my second inferencing code, which is using pipeline (for different model):
classifier = transformers.pipeline(""sentiment-analysis"", model=""distilbert-base-uncased-finetuned-sst-2-english"")
result = classifier(txt)

how can i force transformers library to do faster inferencing on gpu? i have tried adding model.to(torch.device(""cuda"")) but that throws error:
expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu

i suppose the problem is related to the data not being sent to gpu. there is a similar issue here: pytorch summary fails with huggingface model ii: expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu
how would i send data to gpu with and without pipeline? any advise is highly appreciated.","['python', 'pytorch', 'huggingface-transformers', 'inference']",71052343,"you should transfer your input to cuda as well before performing the inference:
device = torch.device('cuda')

# transfer model
model.to(device)

# define input and transfer to device
encoding = tokenizer.encode_plus(txt, 
     add_special_tokens=true, 
     truncation=true, 
     padding=""max_length"", 
     return_attention_mask=true, 
     return_tensors=""pt"")

encoding = encoding.to(device)

# inference
output = model(**encoding)

be aware nn.module.to is in-place, while torch.tensor.to is not (it does a copy!).",https://stackoverflow.com/questions/71050697,python,09-02-2022 13:44,26816.0,9.0,2.0,True,16-03-2024 10:06,09-02-2022 14:48
76822491,in r str_count: counting occurrences of words at a certain distance e.g. 1 to 30 words apart,"in a text document, i want to count the instances when uncertainty|unclear has occurred at a distance of 1 to 30 words from global|decrease in demand|fall in demand. however, my code as below seems to be insensitive to {1,30} as changing these values doesn't change the output. any help would be appreciated.
str_count(texttw,""\\buncertainty|unclear(?:\\w+\\w+){1,30} ?\\w+global|decrease in demand|fall in demand\\b""))","['r', 'regex', 'text', 'nlp', 'stringr']",76837105,"i am not sure if the typo in your text was on purpose (""uncertainy"" instead of ""uncertainty"") so i corrected it, but try something like this:
library(stringr)

x <- ""uncertainty negatively influences economic agents investment and business decisions which leads to decrease in demand. when the economic environment is fraught with uncertainty and the future is unclear businesses and firms may hold back their decisions until uncertainty subsides. ever since the start of the pandemic global economic outlook has been unclear with unprecedented uncertainty leading to fall in demand.""

regex <- ""(uncertainty|unclear)\\s(\\w+\\s){1,30}(global|decrease in demand|fall in demand)""

str_count(x, regex)
# [1] 2

str_extract_all(x, regex)
# [[1]]
# [1] ""uncertainty negatively influences economic agents investment and business decisions which leads to decrease in demand""
# [2] ""unclear with unprecedented uncertainty leading to fall in demand""    


begin matching when you find the words uncertainty or (|) unclear
the word should be followed by a space \\s
that space should be followed by one or more (+) a word characters \\w (a-z, a-z, _)  and a space \\s. this pattern should be matched between one and thirty times {1,30}
followed by the phrase global or decrease in demand or fall in demand

technically, all of the capture groups could be made non capture groups with ?: since you do not need to back reference or capture them specifically.
in the text you posted you have an interesting case in the last sentence, ""ever since the start of the pandemic global economic outlook has been unclear with unprecedented uncertainty leading to fall in demand.""
depending on your interpretation this could actually have two matches:

unclear with unprecedented uncertainty leading to fall in demand
uncertainty leading to fall in demand

if this was your interpretation then the text you posted should have three, not two matches.
just a note to clarify:
""uncertainty subsides. ever since the start of the pandemic global economic outlook has been unclear with unprecedented uncertainty leading to fall in demand."" is not a match because of the period after ""subsides"".",https://stackoverflow.com/questions/76822491,r,02-08-2023 18:08,61.0,2.0,1.0,True,04-08-2023 18:10,04-08-2023 15:14
77247941,summarizing n-grams efficiently in python on big data,"i have a very large dataset of roughly 6 million records, it does look like this snippet:
data = pd.dataframe({
    'id': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'],
    'text': [
        ""mouthwatering bbq ribs cheese, and coleslaw."",
        ""delicious pizza with pepperoni and extra cheese."",
        ""spicy thai curry with cheese and jasmine rice."",
        ""tiramisu dessert topped with cocoa powder."",
        ""sushi rolls with fresh fish and soy sauce."",
        ""freshly baked chocolate chip cookies."",
        ""homemade lasagna with layers of cheese and pasta."",
        ""gourmet burgers with all the toppings and extra cheese."",
        ""crispy fried chicken with mashed potatoes and extra cheese."",
        ""creamy tomato soup with a grilled cheese sandwich.""
    ],
    'date': [
        '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-02',
        '2023-02-02', '2023-02-01', '2023-02-01', '2023-02-02', '2023-02-02'
    ]
})

i want to generate bigrams and trigrams from the column 'text.' i'm interested in two types of ngrams for both trigrams and bigrams: those that start with 'extra' and those that don't start with 'extra.' once we have those, i want to summarize (count the unique id frequency) of those ngrams by unique 'date.' this means that if an ngram appears in an id more than once, i will count it only once because i want to know in how many different 'ids' it ultimately appeared.
i'm very new to python. i come from the r world, in which there is a library called quanteda that uses c programming and parallel computing. searching for those ngrams looks something like this:
corpus_food %>%
  tokens(remove_punct = true) %>% 
  tokens_ngrams(n = 2) %>% 
  tokens_select(pattern = ""^extra"", valuetype = ""regex"") %>%
  dfm() %>%
  dfm_group(groups = lubridate::date(date)) %>%
  textstat_frequency()

yielding my desired results:
       feature frequency rank docfreq group
1 extra_cheese         3    1       2   all

my desired result would look like this:




ngram
nunique
group




cheese and
3
1/02/2023


and extra
2
1/02/2023


extra cheese
2
1/02/2023


and extra cheese
2
1/02/2023


mouthwatering bbq
1
1/02/2023


bbq ribs
1
1/02/2023


ribs cheese
1
1/02/2023


and coleslaw
1
1/02/2023


mouthwatering bbq ribs
1
1/02/2023


bbq ribs cheese
1
1/02/2023


ribs cheese and
1
1/02/2023


cheese and coleslaw
1
1/02/2023


delicious pizza
1
1/02/2023


pizza with
1
1/02/2023


with pepperoni
1
1/02/2023


pepperoni and
1
1/02/2023


delicious pizza with
1
1/02/2023


pizza with pepperoni
1
1/02/2023


with pepperoni and
1
1/02/2023


pepperoni and extra
1
1/02/2023


spicy thai
1
1/02/2023


thai curry
1
1/02/2023




i am in no way comparing the two languages, python and r. they are amazing, but at the moment, i'm interested in a very straightforward and fast method to achieve my results in python. i am open to hearing of a way to achieve what i'm looking for in a faster and more efficient way in python. i'm new to python.
so far i have found a way to create the bigrams and trigrams but i have no idea as to how perform the selection of those that start with ""extra"" and those who don't and this very process of creating the ngrams is taking over an hour so i will take all advice on how to reduce the time.
work around:
import nltk
from nltk import bigrams
from nltk.util import trigrams 
from nltk.tokenize import word_tokenize

data['bigrams'] = data['text'].apply(lambda x: list(bigrams(word_tokenize(x))))
data['trigrams'] = data['text'].apply(lambda x: list(trigrams(word_tokenize(x))))

reading through some posts, some people suggest on using the gensim lib. would that be a good direction?","['python', 'pandas', 'dataframe', 'nlp', 'n-gram']",77266470,"it is easy to find ngrams using sklearn's countvectorizer using the ngram_range argument.
you can create a document-term matrix with ngrams of size 2 and 3 only, then append to your original dataset and doing pivoting and aggregation with pandas to find what you need.
first we'll get the document-term matrix and append to our original data:
# perform the count vectorization, keeping bigrams and trigrams only
from sklearn.feature_extraction.text import countvectorizer
cv = countvectorizer(ngram_range=(2,3))
x = cv.fit_transform(data['text'])

# create dataframe of document-term matrix
cv_df = pd.dataframe(x.todense(), columns=cv.get_feature_names_out())

# append to original data
df = pd.concat([data, cv_df], axis=1)

then we group by id and date, and filter where the count is greater than 0, to find the id-date combinations where each 2 or 3-gram appears, then count the unique ids for each:
# group and pivot
pivoted_df = df.groupby(['id','date']).sum().stack().reset_index()
pivoted_df.columns = ['id', 'date', 'ngram', 'count']

# find n-grams which appear for each id-date combo and count unique ids
pivoted_df = pivoted_df[pivoted_df['count']>0]
pivoted_df.groupby(['date','ngram']).nunique('id').reset_index()

finally, we can create additional columns for the ngram size, and also whether or not the ngram starts with extra, and use for filtering:
# add additional columns for ngram size 
pivoted_df['ngram_size'] = pivoted_df['ngram'].str.split().str.len()

# add additional column for starting with extra
pivoted_df['extra'] = pivoted_df['ngram'].str.startswith('extra')

# find all the 2-grams that start with ""extra""
pivoted_df[(pivoted_df['extra']) & (pivoted_df['ngram_size']==2)]

that being said, if you have 6m records, you have a large dataset, and with this approach you will definitely run into memory issues. you will probably want to filter your data down to what you are most interested in to start, and also make sure you use the min_df parameter in the countvectorizer in order to keep your data tractable.",https://stackoverflow.com/questions/77247941,python,07-10-2023 00:44,284.0,1.0,1.0,True,08-11-2023 01:32,08-11-2023 01:32
78096349,openssl client authentication and macos,"i'm trying to programmatically connect to openai api to make some queries in c, but
when i call try_connect() it fails with:
ssl connect error
ssl: error:0a000410:ssl routines::ssl/tls alert handshake failure

here's the code:
void printsslerrors()
{
    char errstr[256] = {};

    unsigned long err = err_get_error();

    while (err != 0)
    {
        err_error_string_n(err, &errstr[0], sizeof(errstr));

        printf(""ssl: %s"", &errstr);

        err = err_get_error();
    }
}

union ipaddr
{
    uint8_t octect[4];  // e.g. address 192.168.4.5 has octect[0]=192 and octect[3]=5
    uint32_t octet_networkorder;
} openai;

// fill the fields with the correct ip address
//openai.octet[0]=xx
//openai.octet[0]=xx
//openai.octet[0]=xx
//openai.octet[0]=xx

bool try_connect()
{
    //tcp socket creation
    int fd = ::socket(af_inet, sock_stream, 0);

    if (fd == -1)
        return false;

    //ssl init

    err_clear_error();

    ssl_ctx* sslctx = ssl_ctx_new(tls_method());

    if (sslctx == null)
    {
        //ssl context creation failed
        printsslerrors();
        return false;
    }

    // ca file setup

    if (ssl_ctx_load_verify_file(sslctx, ""/etc/ssl/cert.pem"") == 0)
    {
        //ssl load ca file failure
        printsslerrors();
        return false;
    }

    // certificate setup

    if (ssl_ctx_use_certificate_chain_file(sslctx, ""mycert.pem"") != 1)
    {
        //ssl certificate file setup failed
        printsslerrors();
        return false;
    }

    if (ssl_ctx_use_privatekey_file(sslctx, ""mykey.pem"", ssl_filetype_pem) != 1)
    {
        //ssl private key file setup failed
        printsslerrors();
        return false;
    }

    if (ssl_ctx_check_private_key(sslctx) != 1)
    {
        //ssl private key check failed
        printsslerrors();
        return false;
    }

    ssl_ctx_clear_mode(sslctx, ssl_mode_auto_retry);

    ssl* m_ssl = ssl_new(m_sslctx);

    if (m_ssl == null)
    {
        printsslerrors();
        return false;
    }

    if (ssl_set_fd(m_ssl, m_fd) != 1)
    {
        printsslerrors();
        return false;
    }

    //connecting

    sockaddr_in addr{};
    addr.sin_family = af_inet;
    addr.sin_port   = htons(443);

    addr.sin_addr.s_addr = openai.octet_networkorder;

    int ret = ::connect(m_fd, (sockaddr *)&addr, sizeof(sockaddr_in));

    if (ret == -1)
        return false;


    if (ssl_connect(m_ssl) != 1)  // start ssl client handshaking
    {
        printf(""ssl connect error"");
        printsslerrors();
        return false;
    }    

    return true;

}

the code has been tested with other servers that do not ask for client auth and it works fine. the api.openai.com asks for client auth so this code fails in try_connect().
mykey.pem and mycert.pem are files exported from the keychain access application starting from the com.apple.systemdefault certificate and private key.
i could not try on another different system..
the interesting thing is curl works fine in the same system, using the provided curl example:
curl  \ 
-h ""content-type: application/json"" \ 
-h ""authorization: bearer $openai_api_key"" \
-d '{
   ""model"": ""gpt-3.5-turbo"",
   ""messages"": [
     {
       ""role"": ""system"",
       ""content"": ""you are a helpful assistant.""
     },
     {
       ""role"": ""user"",
       ""content"": ""hello!""
     }
   ]
 }'

how can i fix this? am i picking the wrong certificate or am i doing something wrong in the code?
update:
if i run
openssl s_client -connect api.openai.com:443 -servername api.openai.com -cert mycert.pem -key mykey.pem -cafile /etc/ssl/cert.pem

it manages to connect
connecting to 104.18.7.192
connected(00000006)
depth=2 c=us, o=google trust services llc, cn=gts root r1
verify return:1
depth=1 c=us, o=google trust services llc, cn=gts ca 1p5
verify return:1
depth=0 cn=api.openai.com
verify return:1
---
certificate chain
 0 s:cn=api.openai.com
   i:c=us, o=google trust services llc, cn=gts ca 1p5
   a:pkey: rsaencryption, 2048 (bit); sigalg: rsa-sha256
   v:notbefore: feb 25 00:03:27 2024 gmt; notafter: may 25 00:03:26 2024 gmt
 1 s:c=us, o=google trust services llc, cn=gts ca 1p5
   i:c=us, o=google trust services llc, cn=gts root r1
   a:pkey: rsaencryption, 2048 (bit); sigalg: rsa-sha256
   v:notbefore: aug 13 00:00:42 2020 gmt; notafter: sep 30 00:00:42 2027 gmt
 2 s:c=us, o=google trust services llc, cn=gts root r1
   i:c=be, o=globalsign nv-sa, ou=root ca, cn=globalsign root ca
   a:pkey: rsaencryption, 4096 (bit); sigalg: rsa-sha256
   v:notbefore: jun 19 00:00:42 2020 gmt; notafter: jan 28 00:00:42 2028 gmt
---
server certificate
-----begin certificate-----
bla bla bla
-----end certificate-----
subject=cn=api.openai.com
issuer=c=us, o=google trust services llc, cn=gts ca 1p5
---
no client certificate ca names sent
peer signing digest: sha256
peer signature type: rsa-pss
server temp key: x25519, 253 bits
---
ssl handshake has read 4691 bytes and written 402 bytes
verification: ok
---
new, tlsv1.3, cipher is tls_aes_256_gcm_sha384
server public key is 2048 bit
this tls version forbids renegotiation.
compression: none
expansion: none
no alpn negotiated
early data was not sent
verify return code: 0 (ok)

so i suppose the cert and key i have are correct and accepted by the server","['c', 'openssl', 'openai-api', 'client-certificates']",78097720,"i've found the cause.
ssl_set_tlsext_host_name() should be called before the ssl_connect() call to make ssl know the hostname of the server and complete the verification.
i've added such a call and now it's working.
i hope this answer will be useful to someone",https://stackoverflow.com/questions/78096349,c,03-03-2024 12:44,176.0,0.0,1.0,True,03-03-2024 19:42,03-03-2024 13:57
76836010,"nltk.download(&#39;wordnet&#39;) is giving &quot;parseerror: mismatched tag: line 33, column 2&quot; on python 3.10","in attempting to use nltk.stem.wordnetlemmatizer() i get the error below.
lookuperror: 
**********************************************************************
  resource wordnet not found.
  please use the nltk downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('wordnet')
  
  for more information see: 

when i go to run this
import nltk
nltk.download('wordnet')

i get this parse error
traceback (most recent call last):

  file ~\anaconda3\lib\site-packages\ipython\core\interactiveshell.py:3460 in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)

  cell in[32], line 2
    nltk.download('wordnet')

  file ~\anaconda3\lib\site-packages\nltk\downloader.py:777 in download
    for msg in self.incr_download(info_or_id, download_dir, force):

  file ~\anaconda3\lib\site-packages\nltk\downloader.py:629 in incr_download
    info = self._info_or_id(info_or_id)

  file ~\anaconda3\lib\site-packages\nltk\downloader.py:603 in _info_or_id
    return self.info(info_or_id)

  file ~\anaconda3\lib\site-packages\nltk\downloader.py:1009 in info
    self._update_index()

  file ~\anaconda3\lib\site-packages\nltk\downloader.py:952 in _update_index
    elementtree.parse(urlopen(self._url)).getroot()

  file ~\anaconda3\lib\xml\etree\elementtree.py:1222 in parse
    tree.parse(source, parser)

  file ~\anaconda3\lib\xml\etree\elementtree.py:580 in parse
    self._root = parser._parse_whole(source)

  file <string>
parseerror: mismatched tag: line 33, column 2

i ran the code in jupyter notebook originally, restarted the kernel and tried again. i also tried running it in the python interpreter. every time has given me the same error.
nltk version is 3.7","['python', 'nltk']",76852547,"there may be other solutions to my issue, what i ended up doing to solve this problem was manually downloading wordnet from  and saving the file where the documentation tells you to (c:\nltk_data\corpora\wordnet)",https://stackoverflow.com/questions/76836010,python,04-08-2023 12:40,132.0,-1.0,1.0,True,07-08-2023 14:09,04-08-2023 20:06
78896401,azure open ai while developing python script using api key getting error,"import openai

# setup
openai.api_key = 'xxxxxxxxxxxxxxxxxxxxxx'
openai.api_base = ""xxxxxxxxxxxxxxxxxxxxxx""

openai.api_version = '2024-08-20'  # ensure this is correct

def test_openai():
    try:
        response = openai.image.create(
            prompt=""a dog in rain image"",
            model=""dall-e-3"",  # try with a different model ifneeded
            n=1,
            size=""1024x1024""
        )
        print(response)
    except exception as e:
        print(f""error: {e}"")

test_openai()


error: resource not found
i have created azure open ai model dall-e-3
the api key and api base both worked with model gpt-35-turbo","['python', 'azure', 'openai-api', 'azure-openai']",78899809,"the error you are getting because you need to use the deployment name instead of the model name.

you need to create client to connect  to generate result.
below code worked for me.
i am using a flask app to use the openai code and view the generated image in index.html file.
app.py:
from flask import flask,render_template
import os
from openai import azureopenai
import json

app = flask(__name__)

@app.route('/', methods=['get'])
def fetch_image_url():

    try:
        client = azureopenai(
        api_version=""2024-05-01-preview"",
        azure_endpoint=""
        api_key=""xxxxxxxxxxxxxxxxxx""
        )

        result = client.images.generate(
            model=""dallechatgpt"",
            prompt=""a dog in rain image"",
            n=1
        )

        image_url = json.loads(result.model_dump_json())['data'][0]['url']

        return render_template('index.html',image_url=image_url)
    except exception as e:
        print(f""error{e}"")
        

if __name__== ""__main__"":
    app.run()

templates/index.html
<!doctype html>
<html lang=""en"">
<head>
    <meta charset=""utf-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    <title>document</title>
</head>
<body>
    {% if image %}
    <h2>generated image:</h2>
    
    {% endif %}
</body>
</html>

requirements.txt:
flask
openai

output:",https://stackoverflow.com/questions/78896401,python,21-08-2024 10:21,822.0,1.0,2.0,True,22-08-2024 06:34,22-08-2024 01:06
72652399,typeerror with dataloader,"i used a very large dataset for testing my model. to make the testing samples fast, i would like to construct a data loader. but i'm getting errors. i couldn't solve it for two days. here is my code:
 pre_trained_model_name = 'bert-base-cased'
 tokenizer = berttokenizer.from_pretrained(pre_trained_model_name)

 class gpreviewdataset(dataset):
    def __init__(self, paragraph, target, tokenizer, max_len):
       self.paragraph = paragraph
       self.target= target
       self.tokenizer = tokenizer
       self.max_len = max_len
    
    def __len__(self):
       return len(self.paragraph)

    def __getitem__(self, item):
       paragraph = str(self.paragraph[item])
       target = self.target[item]
       encoding = self.tokenizer.encode_plus(
       paragraph,
       add_special_tokens=true,
       max_length=self.max_len,
       return_token_type_ids=false,
       pad_to_max_length=true,
       return_attention_mask=true,
       return_tensors='pt',
       )
       return {
       'review_text': paragraph,
       'input_ids': encoding['input_ids'].flatten(),
       'attention_mask': encoding['attention_mask'].flatten(),
       'targets': torch.tensor(target, dtype=torch.long)
       }


def create_data_loader(df, tokenizer, max_len, batch_size):
    ds = gpreviewdataset(
    paragraph=df.paragraph.to_numpy(),
    target=df.target.to_numpy(),
    tokenizer=tokenizer,
    max_len=max_len
    )
   return dataloader(
     ds,
    batch_size=batch_size,
    num_workers=4
    )

 # main function
 paragraph=['image to pdf converter. ', 'test test']
 target=['0','1']
 df = pd.dataframe({'paragraph': paragraph, 'target': target})


 max_len='512'
 batch_size = 1
 train_data_loader1 = create_data_loader(df, tokenizer, max_len, batch_size)
 for d in train_data_loader1:
      print(d)

when i iterate over the dataloader  i got this error:
  typeerror: caught typeerror in dataloader worker process 0.
  original traceback (most recent call last):
  file ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py"", 
  line 178, in _worker_loop
   data = fetcher.fetch(index)
  file ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch
  data = [self.dataset[idx] for idx in possibly_batched_index]
  file ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py"", line 44, in <listcomp>
  data = [self.dataset[idx] for idx in possibly_batched_index]
  file ""<ipython-input-3-c4f87a4dbb48>"", line 20, in __getitem__
  return_tensors='pt',
  file ""/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py"", line 1069, in encode_plus
    return_special_tokens_mask=return_special_tokens_mask,
  file ""/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py"", line 1365, in prepare_for_model
    if max_length and total_len > max_length:
   typeerror: '>' not supported between instances of 'int' and 'str'

can anyone help me? and also, can you give tips on how i can test my model on a large dataset? i mean what the faster way to test my model on 3m samples of data is?","['nlp', 'pytorch', 'dataloader', 'pytorch-lightning', 'pytorch-dataloader']",72756966,"the error is as it stated
file ""/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py"", line 1365, in prepare_for_model
    if max_length and total_len > max_length:
   typeerror: '>' not supported between instances of 'int' and 'str'

you should change your max_len from string to int:
# max_len='512'
max_len=512",https://stackoverflow.com/questions/72652399,nlp,16-06-2022 22:10,572.0,0.0,1.0,True,25-06-2022 19:55,17-06-2022 01:22
72344379,how can i make dictionary key as column of dataframe?,"!pip install -u lexmo
from lexmo import lexmo
df['dict'] = df['content '].apply(lambda x: [lexmo.lexmo(x)])

using this snippet , i am able to generate this

but my desired output is

i want to make each dictionary key of 'dict' as a seperate column for each row. how can i do this??","['python', 'pandas', 'dataframe', 'dictionary', 'nlp']",72344583,"you can convert output from lexmo.lexmo(x) to series, so it create new columns if call function in series.apply, last append to original dataframe by dataframe.join:
df = df.join(df['content '].apply(lambda x: pd.series(lexmo.lexmo(x))))",https://stackoverflow.com/questions/72344379,python,23-05-2022 06:59,108.0,0.0,1.0,True,23-05-2022 07:26,23-05-2022 07:20
74836812,how to see if openai (node js) createmoderation response &quot;flagged&quot; is true,"using the openai createmoderation feature, i am trying to see if the string gets flagged or not.
i request the api by using this:
`
const mod = await openai.createmoderation({
      input: 'some text to be flagged', // isn't actually this that would get flagged
});
console.log(mod.data.results)

when i log the resposne (and it's flagged) i get this:
[
  {
      hate: 0.057017914950847626,
      'hate/threatening': 0.0013999055372551084,
      'self-harm': 1.523021175842132e-8,
      sexual: 0.000011195417755516246,
      'sexual/minors': 4.2277463307982543e-8,
      violence: 0.8440001010894775,
      'violence/graphic': 1.2527605974810285e-7
    },
    flagged: true
  }
]

`
however, if i try to get the ""flagged"" option by doing mod.data.results.flagged it returns unidentified.","['javascript', 'node.js', 'openai-api']",74836862,"mod.data.results is an array of objects. to get the first flagged value specify mod.data.results[0].flagged (or mod.data.results[0]?.flagged to account for an empty array)
update actually something is not correct with your example object, it is missing a curly bracket.",https://stackoverflow.com/questions/74836812,javascript,17-12-2022 19:16,676.0,2.0,1.0,True,17-12-2022 19:23,17-12-2022 19:18
17116446,what do the bilou tags mean in named entity recognition?,"title pretty much sums up the question.  i've noticed that in some papers people have referred to a bilou encoding scheme for ner as opposed to the typical bio tagging scheme (such as this paper by ratinov and roth in 2009 
from working with the 2003 conll data i know that
b stands for 'beginning' (signifies beginning of an ne)
i stands for 'inside' (signifies that the word is inside an ne)
o stands for 'outside' (signifies that the word is just a regular word outside of an ne)

while i've been told that the words in bilou stand for
b - 'beginning'
i - 'inside'
l - 'last'
o - 'outside'
u - 'unit'

i've also seen people reference another tag 
e - 'end', use it concurrently with the 'last' tag
s - 'singleton', use it concurrently with the 'unit' tag

i'm pretty new to the ner literature, but i've been unable to find something clearly explaining these tags.  my questions in particular relates to what the difference between 'last' and 'end' tags are, and what 'unit' tag stands for.","['nlp', 'named-entity-recognition']",17124042,"based on an issue and a patch in clear tk, it seems like bilou stands for ""beginning, inside and last tokens of multi-token chunks, unit-length chunks and outside"" (emphasis added). for instance, the chunking denoted by brackets
(foo foo foo) (bar) no no no (bar bar)

can be encoded with bilou as 
b-foo, i-foo, l-foo, u-bar, o, o, o, b-bar, l-bar",https://stackoverflow.com/questions/17116446,nlp,14-06-2013 20:05,29210.0,60.0,6.0,True,16-08-2023 22:04,29-11-2018 09:28
77209128,update field metadata using llama index and sqlalchemy,"how do you set the metadata on a query engine? the data field is a json field with specific fields that i want the agent to use to improve the query.
metadata_dict = {
    ""page_call_phone_clicks_logged_in_unique"": ""number of people who logged into facebook and clicked the call now button. ex: 2""
}

what do i do with snapshots_table to set it on the query_engine?
    `
class sqlqueryengine:
    def query_engine(self):
        engine = create_engine(pg_uri)
        metadata_obj = metadata()

        # convert the metadata dictionary to a json string
        metadata_json = json.dumps(metadata_dict)

        # create snapshot sql table
        table_name = ""snapshots""

        snapshots_table = table(
            table_name,
            metadata_obj,
            column(""data"", json, comment=metadata_json),
        )

        metadata_obj.reflect(engine)

        sql_database = sqldatabase(engine)

        nodes = sqltablenodemapping(sql_database)

        schema = []
        for table_name in metadata_obj.tables.keys():
            schema.append(sqltableschema(table_name=table_name))

        # we dump the table schema information into a vector index. the vector index is stored
        # within the context builder for future use.
        index = objectindex.from_objects(schema, nodes, vectorstoreindex)
        llm = openai(temperature=0.9, model_name=""gpt-3.5-turbo"")
        llm_predictor = llmpredictor(llm=llm)
        service_context = servicecontext.from_defaults(llm_predictor=llm_predictor)
        # we construct a sqltableretrieverqueryengine.
        # note that we pass in the objectretriever so that we can dynamically retrieve the table during query-time.
        # objectretriever: a retriever that retrieves a set of query engine tools.
        query_engine = sqltableretrieverqueryengine(
            sql_database,
            index.as_retriever(similarity_top_k=1),
            service_context=service_context,
        )

        return query_engine

`","['python-3.x', 'sqlalchemy', 'langchain', 'llama-index']",77210373,"this seems to be working for now.
    for table_name in metadata_obj.tables.keys():
        schema.append(
            sqltableschema(
                table_name=table_name,
                context_str=f""(note) you will need to typecast the values when performing a sum on the data field. the data field has the structure: {metadata_json}"",
            )
        )",https://stackoverflow.com/questions/77209128,python-3.x,01-10-2023 03:33,906.0,0.0,1.0,True,01-10-2023 12:56,01-10-2023 12:56
78455102,why doesn&#39;t fuzzywuzzy&#39;s process.extractbests give a 100% score when the tested string 100% contains the query string?,"i'm testing fuzzywuzzy's process.extractbests() as follows:
from fuzzywuzzy import process

# define the query string
query = ""apple""

# define the list of choices
choices = [""apple"", ""apple inc."", ""apple computer"", ""apple records"", ""apple tv""]

# call the process.extractbests function
results = process.extractbests(query, choices)

# print the results
for result in results:
    print(result)

it outputs:
('apple', 100)
('apple inc.', 90)
('apple computer', 90)
('apple records', 90)
('apple tv', 90)

why didn't the scorer give 100 to all strings since they all 100% contain the query string (""apple"")?
i use fuzzywuzzy==0.18.0 with python 3.11.7.","['python', 'nlp', 'string-matching', 'fuzzywuzzy']",78455186,"the fuzzywuzzy's extractbests() function does not give 100% because it does not check for a match, it checks for similarity, such as length of string, contents of string compared to the query, positions of the query string, and a few other factors.    in your case, it does not output 100% because ""apple inc."" is not an exact match of your query, ""apple"".  this is why only the ""apple"" choice outputs 100%, because it 100% matches with the query, ""apple"".  i hoped this helped!",https://stackoverflow.com/questions/78455102,python,09-05-2024 14:24,78.0,1.0,1.0,True,09-05-2024 14:36,09-05-2024 14:29
7290197,paraphrasing for math word problems (changing sentence structure without changing meaning),"i'm working on khan academy's exercise framework, and more specifically, word problems.
when doing a word problem exercise, students often get the same word problem, only with numbers and names changed. this is not ideal, as students can quickly learn the pattern and extract relevant data without reading the entire problem. 
are there any ways of changing sentence structure without changing the meaning of the word problem? any other ideas of how to solve this repetition problem are also welcomed.",['nlp'],7290249,"when creating the word problem, use some sort of syntax to denote various equivalent phrases, as is done in article spinning sometimes. then when displaying the word problem, pick randomly between them.
example syntax:
[name] {goes to the store and /goes to the market and /}{purchases/buys} [number] {apples/pears/bananas}. he {gives/sells/donates} [number] to [name]. {how many does he have now?/how many does he have left?/how many does he still have?}

2 example word problems that the syntax above could create:
bob buys 8 bananas. he sells 5 to alice. how many does he have left?

harry goes to the market and purchases 19 pears. he gives 2 to alex. how many does he have now?

for even more combinations, you could make it recursive.
here's an article explaining a similar syntax to what i showed above;",https://stackoverflow.com/questions/7290197,nlp,02-09-2011 23:41,1762.0,6.0,1.0,True,17-09-2024 07:52,21-05-2013 19:43
75725818,loading hugging face model is taking too much memory,"i am trying to load a large hugging face model with code like below:
model_from_disc = automodelforcausallm.from_pretrained(path_to_model)
tokenizer_from_disc = autotokenizer.from_pretrained(path_to_model)
generator = pipeline(""text-generation"", model=model_from_disc, tokenizer=tokenizer_from_disc)

the program is quickly crashing after the first line because it is running out of memory. is there a way to chunk the model as i am loading it, so that the program doesn't crash?

edit

see cronoik's answer for accepted solution, but here are the relevant pages on hugging face's documentation:
sharded checkpoints: 

large model loading:","['python', 'pytorch', 'nlp', 'huggingface-transformers', 'huggingface']",75726174,"you could try to load it with low_cpu_mem_usage:
from transformers import automodelforseq2seqlm

model_from_disc = automodelforcausallm.from_pretrained(path_to_model, low_cpu_mem_usage=true)

please note that low_cpu_mem_usage requires:
accelerate >= 0.9.0 and pytorch >= 1.9.0.",https://stackoverflow.com/questions/75725818,python,13-03-2023 18:46,15461.0,8.0,1.0,True,30-03-2023 20:23,14-03-2023 16:10
74905744,unsure how to resolve language error message from google&#39;s natural language api: &quot;the language sq is not supported for document_sentiment analysis.&quot;,"i have an app that's been working for months and is now giving me an error.
the app takes tweets from the twitter api and runs them through google's sentiment analysis api, returning sentiment analysis on each of the tweets.
without changing the code, i'm suddenly getting a error that hasn't happened before.
error message
_inactiverpcerror: <_inactiverpcerror of rpc that terminated with:
    status = statuscode.invalid_argument
    details = ""the language sq is not supported for document_sentiment analysis.""
    debug_error_string = ""unknown:error received from peer ipv4:74.125.69.95:443 {grpc_message:""the language sq is not supported for document_sentiment analysis."", grpc_status:3, created_time:""2022-12-24t04:26:31.031735656+00:00""}""
>

interpretation
even though i'm stating only 'english' language tweets in my twitter api query (-is:retweet lang:en), my understanding of the error messsage is that the nl api is thinking this is some language referred to as sq. my research says that's 'albanian'.
so my assumption is that the nl api is interpreting some block(s) of text in the tweets as being in albanian, or maybe it's just a portion of an otherwise english tweet that has some albanian language in it.
solution
is there a way to ignore or skip a text if the api can't process the language the text is in?
this is the language_v1 call:
def get_single_sentiment(text):
    '''gets non-entity sentiment of text using gcp's api'''
    
    # instantiates a client
    client = language_v1.languageserviceclient()
    
    # the text to analyze 
    document = language_v1.document(content = text , type_=language_v1.types.document.type.plain_text)

    # detects the sentiment of the text
    sentiment = client.analyze_sentiment(request={""document"": document}).document_sentiment

    return sentiment

below is the full error message being returned when trying to run the sentiment analysis:
---------------------------------------------------------------------------
_inactiverpcerror                         traceback (most recent call last)
/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py in error_remapped_callable(*args, **kwargs)
     56         try:
---> 57             return callable_(*args, **kwargs)
     58         except grpc.rpcerror as exc:

/opt/conda/lib/python3.7/site-packages/grpc/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)
    945                                       wait_for_ready, compression)
--> 946         return _end_unary_response_blocking(state, call, false, none)
    947 

/opt/conda/lib/python3.7/site-packages/grpc/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)
    848     else:
--> 849         raise _inactiverpcerror(state)
    850 

_inactiverpcerror: <_inactiverpcerror of rpc that terminated with:
    status = statuscode.invalid_argument
    details = ""the language sq is not supported for document_sentiment analysis.""
    debug_error_string = ""unknown:error received from peer ipv4:74.125.69.95:443 {grpc_message:""the language sq is not supported for document_sentiment analysis."", grpc_status:3, created_time:""2022-12-24t04:26:31.031735656+00:00""}""
>

the above exception was the direct cause of the following exception:

invalidargument                           traceback (most recent call last)
/tmp/ipykernel_1/1103340548.py in <module>
      1 twitter_stage(query_tw, n_hours_ago
----> 2               , twitter_bq_table, entity)

/tmp/ipykernel_1/2800777156.py in twitter_stage(query, n_hours_ago, twitter_bq_table, entity)
     39 
     40         # get sentiment analysis
---> 41         twitapi_df = get_column_sentiment(twitapi_df, text_col='text', entity=entity, query=query)
     42 
     43         # dropping columns that can't be saved to big query because they are not compatible

/tmp/ipykernel_1/2183820933.py in get_column_sentiment(df, text_col, entity, query)
    110 
    111     # for each entry in text_col, get a single sentiment result
--> 112     sentiment_column = df[text_col].apply(f)
    113 
    114     # for each entry in sentiment_column, fix null values (replace nulls will two values)

/opt/conda/lib/python3.7/site-packages/pandas/core/series.py in apply(self, func, convert_dtype, args, **kwargs)
   4355         dtype: float64
   4356         """"""
-> 4357         return seriesapply(self, func, convert_dtype, args, kwargs).apply()
   4358 
   4359     def _reduce(

/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py in apply(self)
   1041             return self.apply_str()
   1042 
-> 1043         return self.apply_standard()
   1044 
   1045     def agg(self):

/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py in apply_standard(self)
   1099                     values,
   1100                     f,  # type: ignore[arg-type]
-> 1101                     convert=self.convert_dtype,
   1102                 )
   1103 

/opt/conda/lib/python3.7/site-packages/pandas/_libs/lib.pyx in pandas._libs.lib.map_infer()

/tmp/ipykernel_1/2183820933.py in get_single_sentiment(text)
     16 
     17     # detects the sentiment of the text
---> 18     sentiment = client.analyze_sentiment(request={""document"": document}).document_sentiment
     19 
     20     return sentiment

/opt/conda/lib/python3.7/site-packages/google/cloud/language_v1/services/language_service/client.py in analyze_sentiment(self, request, document, encoding_type, retry, timeout, metadata)
    509             retry=retry,
    510             timeout=timeout,
--> 511             metadata=metadata,
    512         )
    513 

/opt/conda/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py in __call__(self, timeout, retry, *args, **kwargs)
    152             kwargs[""metadata""] = metadata
    153 
--> 154         return wrapped_func(*args, **kwargs)
    155 
    156 

/opt/conda/lib/python3.7/site-packages/google/api_core/retry.py in retry_wrapped_func(*args, **kwargs)
    286                 sleep_generator,
    287                 self._deadline,
--> 288                 on_error=on_error,
    289             )
    290 

/opt/conda/lib/python3.7/site-packages/google/api_core/retry.py in retry_target(target, predicate, sleep_generator, deadline, on_error)
    188     for sleep in sleep_generator:
    189         try:
--> 190             return target()
    191 
    192         # pylint: disable=broad-except

/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py in error_remapped_callable(*args, **kwargs)
     57             return callable_(*args, **kwargs)
     58         except grpc.rpcerror as exc:
---> 59             raise exceptions.from_grpc_error(exc) from exc
     60 
     61     return error_remapped_callable

invalidargument: 400 the language sq is not supported for document_sentiment analysis.

proposed solution
i'm thinking the best possible solution must be to ignore any non-english language, and i'm wondering if that's a reasonable approach, and if someone has input on how to approach that.
greatly appreciate any input on resolving this.
thx
tweet content causing problem
update| #shqip #shqiperi #kosova #albania #kosovo #shqiptar #shqiptare #lajme #shqiperia #tirana #prishtina #visitalbania #albanian #tirane #albaniangirl #shqipeï¿½ï¿½ï¿½
<","['google-cloud-platform', 'nlp', 'google-natural-language']",75070896,"the issue can be resolved by  explicitly specifying document language in the code.
ie. specify language en, define the ï¿½ï¿½ï¿½type_ï¿½ï¿½ï¿½ then declare it on ï¿½ï¿½ï¿½documentï¿½ï¿½ï¿½ .
for example :
type_ = language_v1.document.type.plain_text
language = ""en""
document = {""type_"": type_, ""content"": content, ""language"": language}

sample code:
def sample_analyze_sentiment(content):
 
    client = language_v1.languageserviceclient()
 
    if isinstance(content, six.binary_type):
        content = content.decode(""utf-8"")
 
    type_ = language_v1.document.type.plain_text
    language = ""en""
    document = {""type_"": type_, ""content"": content, ""language"": language}
 
    response = client.analyze_sentiment(request={""document"": document})
    sentiment = response.document_sentiment
    print(""score: {}&quoe))
    print(""magnitude: {}"".format(sentiment.magnitude))",https://stackoverflow.com/questions/74905744,google-cloud-platform,24-12-2022 05:27,1030.0,1.0,1.0,True,10-01-2023 13:46,28-12-2022 20:07
75690124,find a specific concordance index using nltk,"i use this code below to get a concordance from nltk and then show the indices of each concordance.  and i get these results show below.  so far so good.
how do i look up the index of just one specific concordance?  it is easy enough to match the concordance to the index in this small example, but if i have 300 concordances, i want to find the index for one.
.index doesn't take multiple items in a list as an argument.
can someone point me to the command/structure i should be using to get the indices to display with the concordances?  i've attached an example below of a more useful result that goes outside nltk to get a separate list of indices.  i'd like to combine those into one result, but how do i get there?
import nltk 
nltk.download('punkt') 
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.text import text

moby = open('mobydick.txt', 'r')

moby_read = moby.read() 
moby_text = nltk.text(nltk.word_tokenize(moby_read))

moby_text.concordance(""monstrous"")

moby_indices  = [index for (index, item) in enumerate(moby_text) if item == ""monstrous""]

print(moby_indices)

displaying 11 of 11 matches:
ong the former , one was of a most monstrous size . ... this came towards us , 
n of the psalms . `` touching that monstrous bulk of the whale or ork we have r
ll over with a heathenish array of monstrous clubs and spears . some were thick
d as you gazed , and wondered what monstrous cannibal and savage could ever hav
that has survived the flood ; most monstrous and most mountainous ! that himmal
they might scout at moby dick as a monstrous fable , or still worse and more de
of radney . ' '' chapter 55 of the monstrous pictures of whales . i shall ere l
ing scenes . in connexion with the monstrous pictures of whales , i am strongly
ere to enter upon those still more monstrous stories of them which are to be fo
ght have been rummaged out of this monstrous cabinet there is no telling . but 
e of whale-bones ; for whales of a monstrous size are oftentimes cast up dead u

[858, 1124, 9359, 9417, 32173, 94151, 122253, 122269, 162203, 205095]

i'd ideally like to have something like this.
displaying 11 of 11 matches:
[858] ong the former , one was of a most monstrous size . ... this came towards us , 
[1124] n of the psalms . `` touching that monstrous bulk of the whale or ork we have r
[9359] ll over with a heathenish array of monstrous clubs and spears . some were thick
[9417] d as you gazed , and wondered what monstrous cannibal and savage could ever hav
[32173] that has survived the flood ; most monstrous and most mountainous ! that himmal
[94151] they might scout at moby dick as a monstrous fable , or still worse and more de
[122253] of radney . ' '' chapter 55 of the monstrous pictures of whales . i shall ere l
[122269] ing scenes . in connexion with the monstrous pictures of whales , i am strongly
[162203] ere to enter upon those still more monstrous stories of them which are to be fo
[162203] ght have been rummaged out of this monstrous cabinet there is no telling . but 
[205095] e of whale-bones ; for whales of a monstrous size are oftentimes cast up dead u","['python', 'nltk']",75816770,"we can use concordance_list function ( so that we can specify the width and number of lines, and then iterate over lines getting the 'offset' (i.e. line number) and adding surrounding brackets '[' ']' plus roi (i.e. 'monstrous') between the left and right words (of each line):
some_text = open('/content/drive/my drive/colab notebooks/data_folders/text/mobydick.txt', 'r')
roi = 'monstrous'

moby_read = some_text.read()
moby_text = nltk.text(nltk.word_tokenize(moby_read))
moby_text = moby_text.concordance_list(roi, width=22, lines=1000)
for line in moby_text:
    print('[' + str(line.offset) + '] ' + ' '.join(line.left) + ' ' + roi + ' ' + ' '.join(line.right))

or if you find this more readable (import numpy as np):
for line in moby_text:
    print('[' + str(line.offset) + '] ', np.append(' '.join(np.append(np.array(line.left), roi)), np.array(' '.join(line.right))))

outputs (my line numbers don't match yours because i used this source:  which just has different spacing/line numbers):
[494] 306 lv . of the monstrous pictures of whales .
[1385] one was of a most monstrous size . * *
[1652] the psalms. ' touching that monstrous bulk of the whale
[9874] with a heathenish array of monstrous clubs and spears .
[9933] gazed , and wondered what monstrous cannibal and savage could
[32736] survived the flood ; most monstrous and most mountainous !
[95115] scout at moby-dick as a monstrous fable , or still
[121328] '' chapter lv of the monstrous pictures of whales i
[121991] this bookbinder 's fish an monstrous pictures of whales 333
[122749] same field , desmarest , monstrous pictures of whales 335
[123525] scenes in connection with the monstrous pictures of whales ,
[123541] enter upon those still more monstrous stories of them which

if we want to consider punctuation and all that, we can do something like:
for line in moby_text:
    left_words = [left_word for left_word in line.left]
    right_words = [right_word for right_word in line.right]
    return_text = '[' +  str(line.offset) + '] '
    for word in left_words:
        if any([word == '.', word == ',', word == ';', word == '!']):
            return_text += word
        else:
            return_text += ' ' + word if return_text[-1] != ' ' else word
    return_text += roi + ' '
    for word in right_words:
        if any([word == '.', word == ',', word == ';', word == '!']):
            return_text += word
        else:
            return_text += ' ' + word if return_text[-1] != ' ' else word
    print(return_text)

outputs:
[494] 306 lv. of the monstrous pictures of whales.
[1385] one was of a most monstrous size. * *
[1652] the psalms.' touching that monstrous bulk of the whale
[9874] with a heathenish array of monstrous clubs and spears.
[9933] gazed, and wondered what monstrous cannibal and savage could
[32736] survived the flood; most monstrous and most mountainous!
[95115] scout at moby-dick as a monstrous fable, or still
[121328] '' chapter lv of the monstrous pictures of whales i
[121991] this bookbinder 's fish an monstrous pictures of whales 333
[122749] same field, desmarest, monstrous pictures of whales 335
[123525] scenes in connection with the monstrous pictures of whales,
[123541] enter upon those still more monstrous stories of them which

but you may have to tweak it as i didn't put a lot of thought into the different contexts that may arise (e.g. '*', numbers, chapter titles in all-caps, roman numerals, etc.) and this is more up to you for how you want the output text to look like--i'm just providing an example.
note: width in the concordance_list function refers to the max length of the next left (and right) word, so if we set it to 4 the first line would print:
[494] the monstrous

because len('the ') is 4, so setting it to 3 would cut off 'the' next left word of 'monstrous':
[494] monstrous

while lines in the concordance_list function refers to the max number of lines, so if we want only the first two lines containing 'monstrous' (i.e. moby_text.concordance_list(..., lines=2)):
[494] 306 lv . of the monstrous pictures of whales .
[1385] one was of a most monstrous size . * *",https://stackoverflow.com/questions/75690124,python,09-03-2023 21:16,456.0,5.0,2.0,True,29-03-2023 17:54,15-03-2023 13:50
72690203,getting keyerrors when training hugging face transformer,"i am generally following this tutorial ( to implement fine-tuning on a pretrained transformer. the main difference is i am using my own custom dataset that is being sourced from a json file that has a document's text and the label it should belong to. to be able to do this i needed to create my own class which is based off of the dataset class from pytorch. this is what that class looks like:
class pdfsdataset(torch.utils.data.dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        
        print(""\n\n\n\nindex"",idx)
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)


i am getting an error when training the transformer that says
traceback (most recent call last):
  file ""c:\users\e417922\appdata\roaming\python\python39\site-packages\pandas\core\indexes\base.py"", line 3621, in get_loc
    return self._engine.get_loc(casted_key)
  file ""pandas\_libs\index.pyx"", line 136, in pandas._libs.index.indexengine.get_loc
  file ""pandas\_libs\index.pyx"", line 163, in pandas._libs.index.indexengine.get_loc
  file ""pandas\_libs\hashtable_class_helper.pxi"", line 2131, in pandas._libs.hashtable.int64hashtable.get_item
  file ""pandas\_libs\hashtable_class_helper.pxi"", line 2140, in pandas._libs.hashtable.int64hashtable.get_item
keyerror: 19

the above exception was the direct cause of the following exception:

traceback (most recent call last):
  file ""c:\users\e417922\downloads\enwiki-20220601-pages-meta-history1.xml-p1p857\hf_transformer.py"", line 147, in <module>
    transformer.train_transformer()
  file ""c:\users\e417922\downloads\enwiki-20220601-pages-meta-history1.xml-p1p857\hf_transformer.py"", line 135, in train_transformer
    trainer.train()
  file ""c:\users\e417922\appdata\roaming\python\python39\site-packages\transformers\trainer.py"", line 1409, in train
    return inner_training_loop(
  file ""c:\users\e417922\appdata\roaming\python\python39\site-packages\transformers\trainer.py"", line 1625, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  file ""c:\users\e417922\appdata\roaming\python\python39\site-packages\torch\utils\data\dataloader.py"", line 530, in __next__
    data = self._next_data()
  file ""c:\users\e417922\appdata\roaming\python\python39\site-packages\torch\utils\data\dataloader.py"", line 570, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise stopiteration
  file ""c:\users\e417922\appdata\roaming\python\python39\site-packages\torch\utils\data\_utils\fetch.py"", line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  file ""c:\users\e417922\appdata\roaming\python\python39\site-packages\torch\utils\data\_utils\fetch.py"", line 49, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  file ""c:\users\e417922\downloads\enwiki-20220601-pages-meta-history1.xml-p1p857\hf_transformer.py"", line 42, in __getitem__
    for key in self.encodings[idx]:
  file ""c:\users\e417922\appdata\roaming\python\python39\site-packages\pandas\core\series.py"", line 958, in __getitem__
    return self._get_value(key)
  file ""c:\users\e417922\appdata\roaming\python\python39\site-packages\pandas\core\series.py"", line 1069, in _get_value
    loc = self.index.get_loc(label)
  file ""c:\users\e417922\appdata\roaming\python\python39\site-packages\pandas\core\indexes\base.py"", line 3623, in get_loc
    raise keyerror(key) from err
keyerror: 19

the keyerror that it fails on changes each time i run it. i'm a beginner with transformers and huggingface so i have no clue what's causing this problem.
edit:
sample input is a json file where elements would look like this:
{
""text_clean"": [
""article with a few hundred words"",
another article with a lot of words"",
""yet another article""
],
""most_similar_label"":[
""quantum""
""artificial intelligence""
""materials""
]
}
full code:
import tkinter as tk
from tkinter import filedialog
import json
import pandas as pd
from transformers import autotokenizer
from transformers import automodelforsequenceclassification
from transformers import trainingarguments
from transformers import trainingarguments, trainer
import numpy as np
from datasets import load_metric
from sklearn.model_selection import train_test_split
import torch

class pdfsdataset(torch.utils.data.dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        
        print(""\n\n\n\nindex"",idx)
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

class hftransformer:
    def __init__ (self):
        pass

    def import_from_json(self):
        #prompts user to select json file
        root = tk.tk()
        root.withdraw()
        self.json_file_path = filedialog.askopenfile().name
        #opens json file and loads data
        with open(self.json_file_path, ""r"") as json_file:
                try:
                    json_load = json.load(json_file)
                except:
                    raise valueerror(""no pdfs to convert to json"")
        self.pdfs = json_load
        #converts json file data to dataframe for easier manipulation
        self.pdfs = pd.dataframe.from_dict(self.pdfs)

        for index in range(len(self.pdfs[""new_tags""])):
            if self.pdfs[""new_tags""][index] == """":
                self.pdfs[""new_tags""][index] = self.pdfs[""most_similar_label""][index]

        self.pdfs[""labels""] = self.pdfs[""new_tags""].apply(lambda val: self.change_tag_to_num(val))
        # for label in self.data[""labels""]:
     
    def change_tag_to_num(self, value):
        if value == ""quantum"":
            return 0
        elif value == ""artificial intelligence"":
            return 1
        elif value == ""materials"":
            return 2
        elif value == ""energy"":
            return 3
        elif value == ""defense"":
            return 4
        elif value == ""satellite"":
            return 5
        elif value == ""other"":
            return 6

    def tokenize_dataset(self):
        tokenizer = autotokenizer.from_pretrained(""bert-base-cased"")

        x_train, x_test, y_train, y_test = train_test_split(self.pdfs[""text_clean""], self.pdfs[""labels""],test_size=0.2)

        train_encodings = x_train.apply(lambda string: tokenizer(string, truncation=true, padding=true,max_length=10))
        test_encodings = x_test.apply(lambda string: tokenizer(string, truncation=true, padding=true,max_length=10))
    
        
        self.train_dataset = pdfsdataset(train_encodings, y_train)
    
        data_to_add = {""input_ids"": [], ""token_type_ids"": [], ""attention_mask"": []}

        for i in self.train_dataset.encodings:
            data_to_add[""input_ids""].append(i[""input_ids""])
            data_to_add[""token_type_ids""].append(i[""token_type_ids""])
            data_to_add[""attention_mask""].append(i[""attention_mask""])

        self.train_dataset.encodings = data_to_add

        self.eval_dataset = pdfsdataset(test_encodings,y_test)
        data_to_add = {""input_ids"": [], ""token_type_ids"": [], ""attention_mask"": []}

        for i in self.eval_dataset.encodings:
            data_to_add[""input_ids""].append(i[""input_ids""])
            data_to_add[""token_type_ids""].append(i[""token_type_ids""])
            data_to_add[""attention_mask""].append(i[""attention_mask""])
        
        self.eval_dataset.encodings = data_to_add

    def train_transformer(self):
        model = automodelforsequenceclassification.from_pretrained(""bert-base-cased"", num_labels=7)
        training_args = trainingarguments(output_dir=""test_trainer"")
        self.metric = load_metric(""accuracy"")
        training_args = trainingarguments(output_dir=""test_trainer"", evaluation_strategy=""epoch"")
    

        trainer = trainer(
            model=model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            compute_metrics=self.compute_metrics
        )
        trainer.train()
    def compute_metrics(self, eval_pred):
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        return self.metric.compute(predictions=predictions, references=labels)


if __name__  == ""__main__"":
    transformer = hftransformer()
    transformer.import_from_json()
    transformer.tokenize_dataset()
    transformer.train_transformer()","['python', 'pandas', 'nlp', 'huggingface-transformers', 'huggingface']",72763938,"converting pandas.series into a simple python list and getting rid of some extra materials would fix the issue
class pdfsdataset(torch.utils.data.dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

class hftransformer:
    def __init__ (self):
        pass

    def import_from_json(self):
        #prompts user to select json file
        self.json_file_path = '/content/truncated_data.json'
        #opens json file and loads data
        with open(self.json_file_path, ""r"") as json_file:
                try:
                    json_load = json.load(json_file)
                except:
                    raise valueerror(""no pdfs to convert to json"")
        self.pdfs = json_load
        #converts json file data to dataframe for easier manipulation
        self.pdfs = pd.dataframe.from_dict(self.pdfs)

        for index in range(len(self.pdfs[""new_tags""])):
            if self.pdfs[""new_tags""][index] == """":
                self.pdfs[""new_tags""][index] = self.pdfs[""most_similar_label""][index]

        self.pdfs[""labels""] = self.pdfs[""new_tags""].apply(lambda val: self.change_tag_to_num(val))
        # for label in self.data[""labels""]:
     
    def change_tag_to_num(self, value):
        if value == ""quantum"":
            return 0
        elif value == ""artificial intelligence"":
            return 1
        elif value == ""materials"":
            return 2
        elif value == ""energy"":
            return 3
        elif value == ""defense"":
            return 4
        elif value == ""satellite"":
            return 5
        elif value == ""other"":
            return 6

    def tokenize_dataset(self):
        tokenizer = autotokenizer.from_pretrained(""bert-base-cased"")

        x_train, x_test, y_train, y_test = train_test_split(self.pdfs[""text_clean""].to_list(), self.pdfs[""labels""].to_list(),test_size=0.2)
        train_encodings = tokenizer(x_train, truncation=true, padding=true,max_length=100)
        test_encodings = tokenizer(x_test, truncation=true, padding=true,max_length=100)
        self.train_dataset = pdfsdataset(train_encodings, y_train)
    
        self.eval_dataset = pdfsdataset(test_encodings,y_test)
    
    def train_transformer(self):
        model = automodelforsequenceclassification.from_pretrained(""bert-base-cased"", num_labels=7)
        training_args = trainingarguments(output_dir=""test_trainer"")
        self.metric = load_metric(""accuracy"")
        training_args = trainingarguments(output_dir=""test_trainer"", evaluation_strategy=""epoch"", )
    
        trainer = trainer(
            model=model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            compute_metrics=self.compute_metrics
        )
        trainer.train()
    def compute_metrics(self, eval_pred):
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        return self.metric.compute(predictions=predictions, references=labels)


if __name__  == ""__main__"":
    tr = hftransformer()
    tr.import_from_json()
    tr.tokenize_dataset()
    tr.train_transformer()",https://stackoverflow.com/questions/72690203,python,20-06-2022 16:31,2984.0,0.0,1.0,True,28-06-2022 15:31,21-06-2022 19:36
37727679,nlp-pos challenge,"please find the original question here on hackerrank 
although,my solution is incomplete can someone please help me understand  where i'm going wrong? (in the second function the tagger returns a 2-letter tag although the question asks for a 3-letter tag. thanks!
import re
import nltk
import string
final_tagged = """"
raw_input(strs)
def tokenize_two(i):
    temp = i
    global strs
    ""remove /?? and pos tag""
    for ch in ['/??']:
        if ch in i:
            i=i.replace(ch,"""")
            #pos tagging
    tag = nltk.pos_tag([i])
    for item in tag:
        for ch in ['??']:
            if ch in temp:
                temp = temp.replace(ch,item[1])
    replace = i+""/??""
    strs = string.replace(strs,replace,temp)
    return temp;

def tokenize_three(i):
    ""remove /??? and pos tag""
    temp = i 
    global strs
    for ch in ['/???']:
        if ch in i:
            i=i.replace(ch,"""")
    tag = nltk.pos_tag([i])
    for item in tag:
        for ch in ['???']:
            if ch in temp:
                temp = temp.replace(ch,item[1])
    replace = i+""/???""
    strs = string.replace(strs,replace,temp)
    return temp;

a = [w for w in re.split('\s+',strs)]
for i in a :
    if(i.endswith(""/??"")):
        tagged = tokenize_two(i)
    if(i.endswith(""/???"")):
        final_tagged = tokenize_three(i)
print strs","['python', 'python-3.x', 'nlp', 'nltk']",37730360,"tag = nltk.pos_tag([i])

pos tagging is context-dependent. you need to pass the entire tokenized sentence as an argument to pos_tag, rather than calling pos_tag one time for each unknown word.",https://stackoverflow.com/questions/37727679,python,09-06-2016 13:41,581.0,1.0,2.0,True,03-08-2023 08:01,09-06-2016 13:49
66778944,address splitting with nlp,"i am working currently on a project that should identify each part of an address, for example from ""str. jack london 121, corvallis, arad, ap. 1603, 973130 "" the output should be like this:
street name: jack london; 
no: 121; city: corvallis; 
state: arad; 
apartment: 1603; 
zip code: 973130

the problem is that not all of the input data are in the same format so some of the elements may be missing or in different order, but it is guaranteed to be an address.
i checked some sources on the internet, but a lot of them are adapted for us addresses only - like google api places, the thing is that i will use this for another country.
regex is not an option since the address may variate too much.
i also thought about nlp to use named entity recognition model but i'm not sure that will work.
do you know what could a be a good way to start, and maybe help me with some tips?","['python', 'nlp', 'street-address', 'named-entity-recognition']",66780549,"there is a similar question in data science stack exchange forum with only one answer suggesting using spacy.
another question on detecting addresses using stanford nlp details another approach to detecting addresses and its constituents.
there is a lexnlp library that has a feature to detect and split addresses this way (snippet borrowed from towardsdatascience article on the library):
from lexnlp.extract.en.addresses import addresses
for filename,text in d.items():
    print(list(lexnlp.extract.en.addresses.get_addresses(text)))

there is also a relatively new (2018) and ""researchy"" code deepparse (and documentation) for deep learning address parsing accompanying an ieee article (paywall) or semantic scholar.
for the training you will need to use some large corpora of addresses or fake addresses generated using, e.g. faker library.",https://stackoverflow.com/questions/66778944,python,24-03-2021 10:18,4810.0,9.0,1.0,True,16-02-2023 18:49,24-03-2021 12:02
72760674,ner - how to check if a common noun indicates a place (subcategorization),"i am looking for a way to find, in a sentence, if a common noun refers to places. this is easy for proper nouns, but i didn't find any straightforward solution for common nouns.
for example, given the sentence ""after a violent and brutal attack, a group of college students travel into the countryside to find refuge from the town they fled, but soon discover that the small village is also home to a coven of serial killers"" i would like to mark the following nouns as referred to places: countryside, town, small village, home.
here is the code i'm using:
import spacy
nlp = spacy.load('en_core_web_lg')

# process whole documents
text = (""after a violent and brutal attack, a group of college students travel into the countryside to find refuge from the town they fled, but soon discover that the small village is also home to a coven of satanic serial killers"")
doc = nlp(text)

# analyze syntax
print(""noun phrases:"", [chunk.text for chunk in doc.noun_chunks])
print(""verbs:"", [token.lemma_ for token in doc if token.pos_ == ""verb""])

# find named entities, phrases and concepts
for entity in doc.ents:
    print(entity.text, entity.label_)

which gives as output the following:
noun phrases: ['a violent and brutal attack', 'a group', 'college students', 'the countryside', 'refuge', 'the town', 'they', 'the small village', 'a coven', 'serial killers']
verbs: ['travel', 'find', 'flee', 'discover']","['python', 'nlp', 'spacy', 'named-entity-recognition']",72761290,"you can use wordnet for this.
from nltk.corpus import wordnet as wn

loc = wn.synsets(""location"")[0]

def is_location(candidate):
    for ss in wn.synsets(candidate):
        # only get those where the synset matches exactly
        name = ss.name().split(""."", 1)[0]
        if name != candidate:
            continue
        hit = loc.lowest_common_hypernyms(ss)
        if hit and hit[0] == loc:
            return true
    return false

# true things
for word in (""countryside"", ""town"", ""village"", ""home""):
    print(is_location(word), word, sep=""\t"")

# false things
for word in (""cat"", ""dog"", ""fish"", ""cabbage"", ""knife""):
    print(is_location(word), word, sep=""\t"")


note that sometimes the synsets are wonky, so be sure to double-check everything.
also, for things like ""small village"", you'll have to pull out the head noun, but it'll just be the last word.",https://stackoverflow.com/questions/72760674,python,26-06-2022 10:16,482.0,1.0,1.0,True,26-06-2022 11:45,26-06-2022 11:08
27475658,difference between semantic web and nlp?,"what exactly is the difference between semantic web and natural language processing? 
is semantic web a part of natural language processing?","['nlp', 'terminology', 'semantics', 'semantic-web']",27486368,"these are two separate subject areas but they do overlap in some places. because documents, regardless of their format are made up of heterogeneous syntax and semantics, the goal is to represent information that is understandable to a machine and not just a human being. this is a common goal of the semantic web and natural language processing.
semantic web
the semantic web is based on two fundamental ideas:

associating meta-information with internet-based resources. metadata is pieces of information about other data which can be added explicitly or implicitly.
the ability to reason about the meta-information. for example, a machine should be able to recognize that a picture of balloon is not an animal, even if it is shaped like one. this idea of reasoning and inference on textual data is still very experimental, however, showing considerable success. there is a range of techniques to query such information such as sparql, machine learning (a pre-annotated corpus), and other statistical techniques.

the use of ontologies is becoming evermore important in this domain. description logic provides the mathematical foundation for knowledge representation systems and can be used to reason with the information.
natural language processing
whereas natural language processing is an important and ongoing research area in theoretical computer science and artificial intelligence, it can look beyond the web and process anything from text in pdfs to speaking to your phone. wikipedia has highlighted some of the key areas which i will expand on:

automatic (abstractive & extractive) summarisation
coreference resolution
discourse analysis
language translation
morphological segmentation
named entity recognition
natural language generation
optical character recognition
parsing
question answering
relationship (semantics) extraction
speech segmentation
speech & voice recognition
topic segmentation
word sense disambiguation (wsd)
information retrieval
information extraction
text simplification
spelling correction

both subject areas have been heavily researched into the syntactics of language, both research fields aim to understand language, notably text. however, in recent times the use of semantics has had a lot of time and investment put into it. but in essence, how to represent relationships in text and miscellaneous structures is a top priority of both fields of thought.
conclusion
semantic web is mostly annotated with rdf, owl, etc., whereas nlp really focuses on freeform text.",https://stackoverflow.com/questions/27475658,nlp,15-12-2014 00:13,3241.0,5.0,5.0,True,07-04-2022 11:53,16-12-2014 12:35
73537733,how to prepare custom training data for donut (document understanding transformer)?,"i want to train hugging face's donut (document understanding transformer) but i need help in creating the training data.
donut github: 
donut official documentation: 
if anybody has already created and trained the model, kindly help.","['python', 'huggingface-transformers']",78852986,"understanding how to label the training data was a bit confusing in the beginning but after reading this, it became clear.
donut treats all tasks as json prediction problems. ensure that your dataset follow this structure:
dataset_name
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ test
ï¿½ï¿½ï¿½   ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ metadata.jsonl
ï¿½ï¿½ï¿½   ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ {image_path0}
ï¿½ï¿½ï¿½   ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ {image_path1}
ï¿½ï¿½ï¿½             .
ï¿½ï¿½ï¿½             .
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ train
ï¿½ï¿½ï¿½   ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ metadata.jsonl
ï¿½ï¿½ï¿½   ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ {image_path0}
ï¿½ï¿½ï¿½   ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ {image_path1}
ï¿½ï¿½ï¿½             .
ï¿½ï¿½ï¿½             .
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ validation
    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ metadata.jsonl
    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ {image_path0}
    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ {image_path1}
              .
              .

for document information extraction, each line in metadata.jsonl should look like this:
{""file_name"": {image_path0}, &qut}

the model ignores other metadata and focuses on the gt_parse or gt_parses field to predict the json task.
for other tasks like document visual question answering, refer to the github link.",https://stackoverflow.com/questions/73537733,python,30-08-2022 05:57,2841.0,0.0,3.0,True,09-08-2024 13:15,30-08-2022 10:22
72503309,save a bert model with custom forward function and heads on hugginface,"i have created my own bertclassifier model, starting from a pretrained and then added my own classification heads composed by different layers. after the fine-tuning, i want to save the model using model.save_pretrained() but when i print it upload it from pretrained i don't see my classifier head.
the code is the following. how can i save the all structure on my model and make it full accessible with  automodel.from_preatrained('folder_path') ?
. thanks!
class bertclassifier(pretrainedmodel):
    """"""bert model for classification tasks.""""""
    config_class = autoconfig
    def __init__(self,config, freeze_bert=true): #tuning only the head
        """"""
         @param    bert: a bertmodel object
         @param    classifier: a torch.nn.module classifier
         @param    freeze_bert (bool): set `false` to fine-tune the bert model
        """"""
        #super(bertclassifier, self).__init__()
        super().__init__(config)

        # instantiate bert model
        # specify hidden size of bert, hidden size of our classifier, and number of labels
        self.d_in = 1024 #hidden size of bert
        self.h = 512
        self.d_out = 2
 
        # instantiate the classifier head with some one-layer feed-forward classifier
        self.classifier = nn.sequential(
            nn.linear(self.d_in, 512),
            nn.tanh(),
            nn.linear(512, self.d_out),
            nn.tanh()
        )
 


    def forward(self, input_ids, attention_mask):


         # feed input to bert
        outputs = self.bert(input_ids=input_ids,
                             attention_mask=attention_mask)
         
         # extract the last hidden state of the token `[cls]` for classification task
        last_hidden_state_cls = outputs[0][:, 0, :]
 
         # feed input to classifier to compute logits
        logits = self.classifier(last_hidden_state_cls)
 
        return logits


configuration=autoconfig.from_pretrained('rostlab/prot_bert_bfd')
model = bertclassifier(config=configuration,freeze_bert=false)

saving the model after fine-tuning
model.save_pretrained('path')

loading the fine-tuned model
model = automodel.from_pretrained('path') 

printing the model after loading shows i have as the last layer the following and missing my 2 linear layer:
 (output): bertoutput(
          (dense): linear(in_features=4096, out_features=1024, bias=true)
          (layernorm): layernorm((1024,), eps=1e-12, elementwise_affine=true)
          (dropout): dropout(p=0.0, inplace=false)
          (adapters): moduledict()
          (adapter_fusion_layer): moduledict()
        )
      )
    )
  )
  (pooler): bertpooler(
    (dense): linear(in_features=1024, out_features=1024, bias=true)
    (activation): tanh()
  )
  (prefix_tuning): prefixtuningpool(
    (prefix_tunings): moduledict()
  )
)","['python', 'nlp', 'pytorch', 'huggingface-transformers', 'bert-language-model']",72550313,"maybe something is wrong with the config_class attribute inside your bertclassifier class. according to the documentation you need to create an additional config class which inherits form pretrainedconfig and initialises the model_type attribute with the name of your custom model.
the bertclassifier's config_class has to be consistent with your custom config class type.
afterwards you can register your config and model with the following calls:
autoconfig.register('custommodelname', custommodelconfigclass)
automodel.register(custommodelconfigclass, custommodelclass)

and load your finetuned model with automodel.from_pretrained('yourcustommodelname')
an incomplete example based on your code could look like this:
class bertclassifierconfig(pretrainedconfig):
    model_type=""bertclassifier""


class bertclassifier(pretrainedmodel):
    config_class = bertclassifierconfig
    # ...


configuration = bertclassifierconfig()
bert_classifier = bertclassifier(configuration)

# do your finetuning and save your custom model
bert_classifier.save_pretrained(""custommodels/bertclassifier"")

# register your config and your model
autoconfig.register(""bertclassifier"", bertclassifierconfig)
automodel.register(bertclassifierconfig, bertclassifier)

# load your model with automodel
bert_classifier_model = automodel.from_pretrained(""custommodels/bertclassifier"")

printing the model output should be similiar to this:
    (pooler): bertpooler(
      (dense): linear(in_features=768, out_features=768, bias=true)
      (activation): tanh()
    )
  )
  (classifier): sequential(
    (0): linear(in_features=1024, out_features=512, bias=true)
    (1): tanh()
    (2): linear(in_features=512, out_features=2, bias=true)
    (3): tanh()
    (4): linear(in_features=2, out_features=512, bias=true)
    (5): tanh()
  )

hope this helps.",https://stackoverflow.com/questions/72503309,python,04-06-2022 21:38,3725.0,5.0,1.0,True,08-06-2022 18:04,05-06-2022 17:51
70531364,structuring dataset for openai&#39;s gpt-3 fine tuning,"the fine tuning endpoint for openai's api seems to be fairly new, and i can't find many examples of fine tuning datasets online.
i'm in charge of a voicebot, and i'm testing out the performance of gpt-3 for general open-conversation questions. i'd like to train the model on the ""fixed"" intent-response pairs we're currently using: this would probably end up performing better in terms of company voice and style.
i have ready a long json file of data extracted from our current conversational engine, which matches user input to intents and returns the specified response. i'd like to train a gpt-3 model on this data.
as of now, for some quick testing, i've set up my calls to the api just like they suggest. i have a ""fixed"" intro text in the form
<name> is <company>'s voicebot. he is kind and professional...

this is a conversation between <name> and a customer:

which is pre-pended to each query, and then a small python class which keeps track of the context which starts with
user: <request the user provides>
bot:

then with each turn the api's response is appended, this way i'm keeping track of what is said. after a few questions, the query or prompt string i'm sending looks like this:
<name> is <company>'s voicebot. he is kind and professional...

this is a conversation between <name> and a user:

user: <request>
bot: <response>
user: <request>
bot: <response>
... and so on
bot:

my question is, do i have to provide the same ""format"" for my training data? is it advisable?
the docs indicate that the training set should be in this format:
{""prompt"": ""<prompt text>"", ""completion"": ""<ideal generated text>""}
{""prompt"": ""<prompt text>"", ""completion"": ""<ideal generated text>""}
...

but does the prompt need to include my intro text (the description) each time or do i simply provide a series of user/bot exchanges with a bot: in the end and in the completion the answer i'd expect?
what would be a best practice in this case?
my fear is that if i wanted to slightly change the intro prompt a month from now i'd have to retrain the whole thing again because each response was trained with that specific block of text prepended.","['python', 'machine-learning', 'training-data', 'openai-api', 'gpt-3']",70710672,"i contacted openai's support and they were extremely helpful: i'll leave their answer here.

the prompt does not need the fixed intro every time. instead, you'll just want to provide at least a few hundred prompt-completion pairs of user/bot exchanges.
we have a sample of a chatbot fine-tuning dataset here.",https://stackoverflow.com/questions/70531364,python,30-12-2021 12:00,3618.0,3.0,2.0,True,08-07-2023 14:05,10-01-2022 11:45
53975449,why do i get this import error when i have the required dlls?,"from sklearn.feature_extraction.text import countvectorizer


getting this error
 from sklearn.feature_extraction.text import countvectorizer

  file ""c:\users\anaconda3\lib\site-packages\sklearn\__init__.py"", line 57, in <module>
    from .base import clone

  file ""c:\users\anaconda3\lib\site-packages\sklearn\base.py"", line 12, in <module>
    from .utils.fixes import signature

  file ""c:\users\anaconda3\lib\site-packages\sklearn\utils\__init__.py"", line 11, in <module>
    from .validation import (as_float_array,

  file ""c:\users\anaconda3\lib\site-packages\sklearn\utils\validation.py"", line 18, in <module>
    from ..utils.fixes import signature

  file ""c:\users\\anaconda3\lib\site-packages\sklearn\utils\fixes.py"", line 291, in <module>
    from scipy.sparse.linalg import lsqr as sparse_lsqr


    from .eigen import *

  file ""c:\users\anaconda3\lib\site-packages\scipy\sparse\linalg\eigen\__init__.py"", line 11, in <module>
    from .arpack import *

  file ""c:\users\anaconda3\lib\site-packages\scipy\sparse\linalg\eigen\arpack\__init__.py"", line 22, in <module>
    from .arpack import *

  file ""c:\users\anaconda3\lib\site-packages\scipy\sparse\linalg\eigen\arpack\arpack.py"", line 45, in <module>
    from . import _arpack

importerror: dll load failed: the specified module could not be found.","['python', 'scikit-learn', 'nlp', 'anaconda']",53975917,"according to this github issue 
""the solution is to install mkl.""
conda install mkl

general advice in case like this is to google last two lines of the stack trace, usually you will find a github or similar thread about it.",https://stackoverflow.com/questions/53975449,python,30-12-2018 05:19,8245.0,2.0,3.0,True,31-08-2023 19:39,19-06-2019 12:42
76988523,faiss vectore store delete using keys,"i am trying to delete by filtering using metadata source by whrn i get to db.delete(list)
i got notimplementederror: delete method must be implemented by subclass.
this is the code:
db = faiss.load_local(faiss_userguide_index, embeddings)
def store_to_df(store):
    v_dict=store.docstore._dict
    data_rows=[]
    for k in v_dict.keys():
        doc_name=v_dict[k].metadata['source'].split('/')[-1]
        page_number=v_dict[k].metadata['page']+1
        content=v_dict[k].page_content
        data_rows.append({""chunk_id"":k,""document"":doc_name,""page"":page_number,""content"":content})
    vector_df=pd.dataframe(data_rows)
    return vector_df
        
def delete_document(store,document):
    vector_df=store_to_df(store)
    chunks_list=vector_df.loc[vector_df['document']==document]['chunk_id'].tolist()
    store.delete(chunks_list)
delete_document(db,""doc2.pdf"")

i got this error
notimplementederror                       traceback (most recent call last)
cell in[115], line 17
     15     chunks_list=vector_df.loc[vector_df['document']==document]['chunk_id'].tolist()
     16     store.delete(chunks_list)
---> 17 delete_document(db,""doc2.pdf"")

cell in[115], line 16, in delete_document(store, document)
     14 vector_df=store_to_df(store)
     15 chunks_list=vector_df.loc[vector_df['document']==document]['chunk_id'].tolist()
---> 16 store.delete(chunks_list)

file ~\anaconda\anaconda\lib\site-packages\langchain\vectorstores\base.py:81, in delete(self, ids, **kwargs)
     67     """"""delete by vector id or other criteria.
     68 
     69     args:
   (...)
     75         false otherwise, none if not implemented.
     76     """"""
     78     raise notimplementederror(""delete method must be implemented by subclass."")
     80 async def aadd_texts(
---> 81     self,
     82     texts: iterable[str],
     83     metadatas: optional[list[dict]] = none,
     84     **kwargs: any,
     85 ) -> list[str]:
     86     """"""run more texts through the embeddings and add to the vectorstore.""""""
     87     raise notimplementederror

notimplementederror: delete method must be implemented by subclass.

how can i solve it, please?","['embedding', 'langchain', 'faiss', 'similarity-search']",77010285,it was a problem of langchain version (it should be v0.0.277),https://stackoverflow.com/questions/76988523,embedding,27-08-2023 18:59,1156.0,0.0,1.0,True,30-08-2023 17:27,27-08-2023 19:00
71191364,splitting long text dataframe column into multiple columns with matched pharases,"i have dataframe with column having a very long text per row,
looks like this:




id
text




id1
diagnostic cerebral angiogram  date: 8/26/2005  indication: 78-year-old man with a history of shunted normal pressure hydrocephalus who more recently has been managed for a right-sided subdural hematoma. this was initially managed conservatively in the acute phase but progressed to an enlarging chronic subdural hematoma that was ultimately treated with burr hole drainage. middle meningeal artery embolization was recommended to minimize the risk of future recurrence.  comparison: ct brain 8/24/2003 and ct brain    medications:    1.  heparin 3500 units iv.  2.  nitroglycerin 200 mcg ia.  3.  verapamil 5 mg ia.  4.  see anesthesia records for additional medications administered.  contrast:  150 ml visipaque  radiation dose: 16.3 min; 587.7 mgy  impression:  successful particle and coil embolization of the parietal branch of the right middle meningeal artery for treatment of a right-sided chronic subdural hematoma.




i would like to split this columns into multiple columns
phrases to split on

starts with ï¿½ï¿½ï¿½date:ï¿½ï¿½ï¿½
starts with ï¿½ï¿½ï¿½medication:ï¿½ï¿½ï¿½
starts with ï¿½ï¿½ï¿½ impression:ï¿½ï¿½ï¿½
starts with ï¿½ï¿½ï¿½ indication:ï¿½ï¿½ï¿½
starts with ï¿½ï¿½ï¿½ comparison:ï¿½ï¿½ï¿½

""s-table-container"">



id
date
indication
comparison
medications
impression




id1
8/26/2005
78-year-old man with a history of shunted normal pressure hydrocephalus who more recently has been managed for a right-sided subdural hematoma. this was initially managed conservatively in the acute phase but progressed to an enlarging chronic subdural hematoma that was ultimately treated with burr hole drainage. middle meningeal artery embolization was recommended to minimize the risk of future recurrence.
ct brain 8/24/2003 and ct brain 8/26/2003
1.  heparin 3500 units iv.  2.  nitroglycerin 200 mcg ia.  3.  verapamil 5 mg ia.  4.  see anesthesia records for additional medications administered.  contrast:  150 ml visipaque  radiation dose: 16.3 min; 587.7 mgy
status post left pterional craniotomy for clipping of a left middle cerebral artery trifurcation aneurysm with no evidence of residual aneurysm","['python', 'regex', 'nlp', 'spacy']",71198785,"you could use pandas extract and python named groups to extract only the desired parts of the paragraph.
import pandas as pd
import re

paragraphs = """"""diagnostic cerebral angiogram date: 8/26/2005 indication: 78-year-old man with a history of shunted normal pressure hydrocephalus who more recently has been managed for a right-sided subdural hematoma. this was initially managed conservatively in the acute phase but progressed to an enlarging chronic subdural hematoma that was ultimately treated with burr hole drainage. middle meningeal artery embolization was recommended to minimize the risk of future recurrence. comparison: ct brain 8/24/2003 and ct brain medications: 1. heparin 3500 units iv. 2. nitroglycerin 200 mcg ia. 3. verapamil 5 mg ia. 4. see anesthesia records for additional medications administered. contrast: 150 ml visipaque radiation dose: 16.3 min; 587.7 mgy impression: successful particle and coil embolization of the parietal branch of the right middle meningeal artery for treatment of a right-sided chronic subdural hematoma.""""""

df = pd.dataframe({'paragraphs':paragraphs}, index=[0])
print(df)

df1 = df['paragraphs'].str.extract(
    r'(?:date: )(?p<date>.+?)\s'
    r'(?:indication:)(?p<indication>.+?)'
    r'(?:comparison:)(?p<comparison>.+?)'
    r'(?:medications:)(?p<medications>.+?)'
    r'(?:impression:)(?p<impression>.+?)$', flags=re.m, expand=true)

output from df1




index
date
indication
comparison
medications
impression




0
8/26/2005
78-year-old man with a history of shunted normal pressure hydrocephalus who more recently has been managed for a right-sided subdural hematoma. this was initially managed conservatively in the acute phase but progressed to an enlarging chronic subdural hematoma that was ultimately treated with burr hole drainage. middle meningeal artery embolization was recommended to minimize the risk of future recurrence.
ct brain 8/24/2003 and ct brain
1. heparin 3500 units iv. 2. nitroglycerin 200 mcg ia. 3. verapamil 5 mg ia. 4. see anesthesia records for additional medications administered. contrast: 150 ml visipaque radiation dose: 16.3 min; 587.7 mgy
successful particle and coil embolization of the parietal branch of the right middle meningeal artery for treatment of a right-sided chronic subdural hematoma.",https://stackoverflow.com/questions/71191364,python,20-02-2022 04:23,171.0,0.0,1.0,True,22-02-2022 02:22,22-02-2022 02:22
78555052,does the code interpreter in azure open ai support arabic language from the files,"i'm trying to use the azure openai and the code interpreter to perform some data analysis on some csv files but it doesn't return the arabic values from the files.
instead, it shows it as question marks (????).
if there some docs about it please provide me with the links.","['python', 'azure', 'openai-api', 'azure-openai']",78575083,"i have tried uploading a csv file containing the arabic language, and it worked fine, providing a table output.

the problem here is due to encoding.
you need to upload a utf-8 encoded file, as shown in the screenshot below. then, you will get the output correctly.

if the code interpreter returns a pandas csv load, then pandas does not recognize the file correctly if it is not encoded properly.
therefore, make sure to give the prompt saying to use the correct encoding.",https://stackoverflow.com/questions/78555052,python,30-05-2024 12:44,162.0,0.0,1.0,True,04-06-2024 11:22,30-05-2024 12:57
76948941,problem with openai api integration importing createchatcompletionrequestmessage,"const [messages, setmessages] = usestate<chatcompletionrequestmessage[]>([]);
error: cannot find name 'chatcompletionrequestmessage'.

i'm a beginner at web development and i'm trying to learn by building applications through youtube tutorials.
i've seen that openai has recently been upgraded from v3 to v4 but i couldn't really find a solution to this error.","['reactjs', 'next.js', 'openai-api']",76949107,"this should work:
import { createchatcompletionrequestmessage } from ""openai/resources/chat"";

if there is a major update in packages, you should dive into the node_modules/openai and search for it",https://stackoverflow.com/questions/76948941,reactjs,21-08-2023 21:58,6011.0,0.0,4.0,True,03-01-2024 19:59,22-08-2023 16:22
13500066,why restricts lucene&#39;s morelikethis its termqueries to the field with the highest docfreq?,"i'm currently working on a modified version of lucenes morelikethis, to fit my own purposes.
there is one thing i still can't understand.
when creating the queue, morelikethis searches for the field with the highest docfreq for this term.
// go through all the fields and find the largest document frequency
string topfield = fieldnames[0];
int docfreq = 0;
for (int i = 0; i < fieldnames.length; i++) {
   int freq = ir.docfreq(new term(fieldnames[i], word));
   topfield = (freq > docfreq) ? fieldnames[i] : topfield;
   docfreq = (freq > docfreq) ? freq : docfreq;
}

this field will be used in the termquery. this can produce strange results.
for example, imagine you have two fields, ""title"" and ""body"", and there are two documents with the exact same title, but they won't be a match because all words from the ""title"" occur more often in other documents ""body""s, and vice versa. that seems pretty odd to me.
another example: i use it in a system which filters the results by user-dependent access permissions, and there it happened that the user for whom the query was generated could not see the documents which were responsible for the high docfreq of the chosen field. the generated query didn't find any documents, although there were plenty docs the user could see, containing the exact terms, just in the wrong field.
i wonder why they don't just use all fields, or at least the fields in which the terms occur originally.
sure, it may be a performance issue. but i've implemented it to use all the fields where the term occurs in the original document, plus the one with the highest docfreq. i tested it on an index with several thousand documents and could not see any difference (but i didn't do any benchmarks).
so, can anybody tell me why it's implemented this way?
the only reason i can think of, is to be performant on a really big index with lots of fields.
//edit: i implemented the first example to clarify the problem:","['java', 'lucene', 'information-retrieval', 'morelikethis']",13556673,"you should view morelikethis as a reference implementation that doesn't fit all uses. 
if the implementation would have targeted one field only, then we'll be seeing questions like: why it searches only the title field and totally misses out that the two book documents have the same author.
you could use setfieldnames to set which fields to find similarity by.  
creating your own version of morelikethis sound like the best approach, especially given that you need to factor in acls.",https://stackoverflow.com/questions/13500066,java,21-11-2012 18:49,417.0,4.0,1.0,True,05-04-2022 13:07,05-04-2022 13:07
75886674,how to compute sentence level perplexity from hugging face language models?,"i have a large collection of documents each consisting of ~ 10 sentences. for each document, i wish to find the sentence that maximises perplexity, or equivalently the loss from a fine-tuned causal lm. i have decided to use hugging face and the distilgpt2 model for this purpose. i have 2 problems when trying to do in an efficient (vectorized) fashion:

the tokenizer required padding to work in batch mode, but when computing the loss on padded input_ids those pad tokens are contributing to the loss. so the loss of a given sentence depends on the length of the longest sentence in the batch which is clearly wrong.

when i pass a batch of input ids to the model and compute the loss, i get a scalar as it (mean?) pools across the batch. i instead need the loss per item, not the pooled one.


i made a version that operates on a sentence by sentence basis and while correct, it is extremely slow (i want to process ~ 25m sentences total). any advice?
minimal example below:
# init
tokenizer = autotokenizer.from_pretrained(""distilgpt2"")
tokenizer.pad_token = tokenizer.eos_token
model = automodelforcausallm.from_pretrained(""clm-gpu/checkpoint-138000"")
segmenter = spacy.load('en_core_web_sm')

# that's the part i need to vectorise, surely within a document (bsize ~ 10)
# and ideally across documents (bsize as big as my gpu can handle)
def select_sentence(sentences):
    """"""we pick the sentence that maximizes perplexity""""""
    max_loss, best_index = 0, 0
    for i, sentence in enumerate(sentences):
        encodings = tokenizer(sentence, return_tensors=""pt"")
        input_ids = encodings.input_ids
        loss = lm(input_ids, labels=input_ids).loss.item()
        if loss > max_loss:
            max_loss = loss
            best_index = i

    return sentences[best_index]

for document in documents:
    sentences = [sentence.text.strip() for sentence in segmenter(document).sents]
    best_sentence = select_sentence(sentences)
    write(best_sentence)","['python', 'nlp', 'huggingface-transformers', 'large-language-model', 'huggingface-evaluate']",75887046,"if the goal is to compute perplexity and then select the sentences, there's a better way to do the perplexity computation without messing around with tokens/models.
install 
pip install -u evaluate

then:
perplexity = evaluate.load(""perplexity"", module_type=""metric"")
input_texts = [""lorem ipsum"", ""happy birthday!"", ""bienvenue""]

results = perplexity.compute(model_id='gpt2',
                             add_start_token=false,
                             predictions=input_texts)
print(list(results.keys()))


[out]:
>>>['perplexities', 'mean_perplexity']
print(round(results[""mean_perplexity""], 2))
>>>646.75
print(round(results[""perplexities""][0], 2))
>>>32.25


q: that's great but how do i use it for a custom model that can't be fetched with model_id=...?
a: for that lets look under the hood, 
this is how the code initialize the model:
class perplexity(evaluate.metric):
    def _info(self):
        return evaluate.metricinfo(
            module_type=""metric"",
            description=_description,
            citation=_citation,
            inputs_description=_kwargs_description,
            features=datasets.features(
                {
                    ""predictions"": datasets.value(""string""),
                }
            ),
            reference_urls=[""
        )

    def _compute(
        self, predictions, model_id, batch_size: int = 16, add_start_token: bool = true, device=none, max_length=none
    ):
        ...
        model = automodelforcausallm.from_pretrained(model_id)
        model = model.to(device)

        tokenizer = autotokenizer.from_pretrained(model_id)
        ...

argh, there's no support for local models!
what if we do some simple changes to the code =)
see load a pre-trained model from disk with huggingface transformers

class perplexity(evaluate.metric):
    def _info(self):
        return evaluate.metricinfo(
            module_type=""metric"",
            description=_description,
            citation=_citation,
            inputs_description=_kwargs_description,
            features=datasets.features(
                {
                    ""predictions"": datasets.value(""string""),
                }
            ),
            reference_urls=[""
        )

    def _compute(
        self, predictions, model_id, batch_size: int = 16, add_start_token: bool = true, device=none, max_length=none, local_file_only: bool = false
    ):
        ...
        model = automodelforcausallm.from_pretrained(model_id, local_files_only=local_file_only)
        model = model.to(device)

        tokenizer = autotokenizer.from_pretrained(model_id, local_files_only=local_file_only)

technically, if you could load a local model that you can load with:
automodelforcausallm.from_pretrained(""clm-gpu/checkpoint-138000"", local_file_only=true)

you can should be able the model_id as such after the code change:
perplexity.compute(model_id=""clm-gpu/checkpoint-138000"",
                             add_start_token=false,
                             predictions=input_texts, 
                             local_file_only=true)

opened a pull-request:",https://stackoverflow.com/questions/75886674,python,30-03-2023 09:53,13178.0,8.0,1.0,True,30-03-2023 11:01,30-03-2023 10:31
66171715,is there a limit to the size of target word vocabulary that should be used in seq2seq models?,"in a machine translation seq2seq model (using rnn/gru/lstm) we provide sentence in a source language and train the model to map it to a sequence of words in another language (e.g., english to german).
the idea is, that the decoder part generates a classification vector (which has the size of target word vocabulary) and a softmax is applied on this vector followed by an argmax to get the index of the most probable word.
my question is: is there an upper limit to how large the target word vocabulary should be, considering:

the performance remains reasonable (softmax will take more time for larger vectors)
the accuracy/correctness of prediction is acceptable","['machine-learning', 'nlp', 'machine-translation', 'seq2seq', 'vocabulary']",66204848,"the main technical limitation of the vocabulary size is the gpu memory. the word embeddings and the output projection are the biggest parameters in the model. with a too large vocabulary, you would be forced to use small training batches which would significantly slow down the training.
also, it is not necessarily so that the bigger the vocabulary, the better the performance. words in a natural language are distributed according to zipf's law, which means that the frequency of words decreases exponentially with the frequency rank. with the increasing vocabulary size, you add words that are less and less common in the language. the word embeddings get updated only when the word occurs in the training data. with a very large vocabulary, the embeddings of less frequent words end up undertrained and the model cannot handle them properly anyway.
mt models typically used a vocabulary of 30k-50k tokens. these are however not words, but so-called sub-words. the text gets segmented using a statistical heuristic, such that most of the common words remain as they are and less frequent words get split into subwords, ultimately into single characters.",https://stackoverflow.com/questions/66171715,machine-learning,12-02-2021 12:15,1125.0,1.0,1.0,True,19-02-2021 10:12,12-02-2021 12:25
76700162,sklearn - modulenotfound despite is installing successfully,"i'm importing get_embedding from openai.embeddings_utils.
i received a modulenotfound error and, after installing

matplotlib
plotly
scipy

i got another saying that sklearn wasn't found.  i've installed that, apparently successfully:
successfully built sklearn
installing collected packages: sklearn
successfully installed sklearn-0.0.post5

when i run the code though:
from openai.embeddings_utils import get_embedding

df['embedding'] = df['text'].apply(lambda x:
                                   get_embedding(x, engine='text-embedding-ada-002'))

i still get the exception:
modulenotfounderror: no module named 'sklearn'
i'm very new to python and pip, is there some magic i need to do for this particular package?","['python', 'scikit-learn', 'pip', 'openai-api']",76700168,"did you try installing the package using:
pip install scikit-learn",https://stackoverflow.com/questions/76700162,python,16-07-2023 19:51,62.0,1.0,1.0,True,16-07-2023 19:56,16-07-2023 19:56
72202895,"encoding iob format, entity nested inside other entity","i have a dataset and i have to do named entity recognition with it. i would convert the dataset which is a json to iob format but i have an issue:
the dataset contains entity nested in other entity, for example
sentence:
traitement des manifestations neurologiques progressives des patients adultes et des enfants atteints de maladie de niemann-pick type c

entity:
""manifestations neurologiques progressives des patients  atteints de maladie de niemann-pick type c""    and     ""adultes""    and    ""enfants"" 

how should i encode the bigger one with nested entity inside?
i thought about:




word
tag




traitement
o


des
o


manifestations
b-cible


neurologiques
i-cible


progressives
i-cible


des
i-cible


patients
i-cible


adultes
b-caracteristique_du_sujet


et
o


des
o


enfants
b-caracteristique_du_sujet


atteints
i-cible


de
i-cible


maladie
i-cible


de
i-cible


niemann-pick
i-cible


type
i-cible


c
i-cible




but i'm not sure it's correct and comprehensible for an algorithm then.","['python', 'nlp', 'nltk', 'spacy', 'named-entity-recognition']",72209810,"you can either take just the outermost layer and use that as ner training data, or you can use all your labels to create spans (including nested spans) and train a span categorizer. you might also want to look at the spancat example project.
you can't represent nested spans with iob format, so if you go that route, you'll need to manually create doc objects with spans saved in doc.spans.",https://stackoverflow.com/questions/72202895,python,11-05-2022 14:32,473.0,1.0,1.0,True,12-05-2022 03:27,11-05-2022 20:34
66849433,interpreting the output tokenization of bert for a given word,"from bert_embedding import bertembedding
bert_embedding = bertembedding(model='bert_12_768_12', dataset_name='wiki_multilingual_cased')
output = bert_embedding(""any"")

i need clarification on the output of mbert embeddings. i'm aware that wordpiece tokenization is used to break up the input text. also i observed that on providing a single word (say ""any"") as input, the output has length equal to the number of characters in the input (in our case, 3). output[i] is a tuple of lists where the first list contains the character at ith  position with the 'unknown' token preceding and following it as different elements in the array. following this are three (= length of the input word) arrays (embeddings) of size 768 each. why does the output seem to be tokenized character-wise (rather than wordpiece tokenized)?
also found out the output form changes when the input is given in a list as:bert_embedding([""any""]). the output now is a single tuple with ['[unk]', 'state', '[unk]'] as the first element followed by three different embeddings conceivably corresponding to the three tokens listed above.
if i need the embedding of the last subword (not simply of the last character or the whole word) for a given input word, how do i access it?","['python', 'nlp', 'pytorch', 'bert-language-model']",66883043,checked their github page. about the input format: yes it is expected as a list (of strings). also this particular implementation provides token ( = word ) level embeddings; so subword level embedings can't be retrieved directly although it provides a choice on how the word embeddings should be derived from their subword components ( by taking avg which is default or taking sum or just the last subword embedding). refer to the hugggingface interface for bert for a finer control over how the embeddings are taken e.g. from the different layers and using which operations.,https://stackoverflow.com/questions/66849433,python,29-03-2021 06:11,640.0,0.0,2.0,True,31-03-2021 06:57,29-03-2021 11:24
74416390,python nested dictionary issue when iterating,"i have 5 list of words, which basically act as values in a dictionary where the keys are the ids of the documents.
for each document, i would like to apply some calculations and display the values and results of the calculation in a nested dictionary.
so far so good, i managed to do everything but i am failing in the easiest part.
when showing the resulting nested dictionary, it seems it's only iterating over the last element of each of the 5 lists, and therefore not showing all the elements...
could anybody explain me where i am failing??
this is the original dictionary data_docs:
{'doc01': ['simpl', 'hello', 'world', 'test', 'python', 'code'],
 'doc02': ['today', 'wonder', 'day'],
 'doc03': ['studi', 'pac', 'today'],
 'doc04': ['write', 'need', 'cup', 'coffe'],
 'doc05': ['finish', 'pac', 'use', 'python']}

this is the result i am getting (missing 'simpl','hello', 'world', 'test', 'python' in doc01 as example):
{'doc01': {'code': 0.6989700043360189},
 'doc02': {'day': 0.6989700043360189},
 'doc03': {'today': 0.3979400086720376},
 'doc04': {'coffe': 0.6989700043360189},
 'doc05': {'python': 0.3979400086720376}}

and this is the code:
    def tfidf (data, idf_score): #function, 2 dictionaries as parameters
      tfidf = {} #dict for output
      for word, val in data.items(): #for each word and value in data_docs(first dict)
        for v in val: #for each value in each list
          a = val.count(v) #count the number of times that appears in that list
          scores = {v :a * idf_score[v]} # dictionary that will act as value in the nested
        tfidf[word] = scores #final dictionary, the key is doc01,doc02... and the value the above dict
      return tfidf
    
    tfidf(data_docs, idf_score)

thanks,","['python-3.x', 'loops', 'dictionary', 'tf-idf']",74416975,"did you mean to do this?
def tfidf(data, idf_score):  # function, 2 dictionaries as parameters
    tfidf = {}  # dict for output
    for word, val in data.items():  # for each word and value in data_docs(first dict)
        scores = {}  # <---- a new dict for each outer iteration
        for v in val:  # for each value in each list
            a = val.count(v)  # count the number of times that appears in that list
            scores[v] = a * idf_score[v] # <---- keep adding items to the dictionary
        tfidf[word] = scores  # final dictionary, the key is doc01,doc02... and the value the above dict
    return tfidf

... see my changes with <----- arrow :)
returns:
{'doc01': {'simpl': 1,
  'hello': 1,
  'world': 1,
  'test': 1,
  'python': 1,
  'code': 1},
 'doc02': {'today': 1, 'wonder': 1, 'day': 1},
 'doc03': {'studi': 1, 'pac': 1, 'today': 1},
 'doc04': {'write': 1, 'need': 1, 'cup': 1, 'coffe': 1},
 'doc05': {'finish': 1, 'pac': 1, 'use': 1, 'python': 1}}",https://stackoverflow.com/questions/74416390,python-3.x,12-11-2022 20:37,46.0,-1.0,1.0,True,12-11-2022 22:12,12-11-2022 20:55
66825806,python&#39;s spacy entityruler does not return me any results,"i want to make spacy model that will recognise organisation names. each organisation name have between 1 and 4 words, that can be titled or capitalised.
i have added more than 3500 names of the organisations like this:
patterns = []
for organisation in organisations_list:
    patterns.append({""label"": ""org"", ""pattern"": organisation.strip()})

so now i have a list of patterns that look like this:
for p in patterns:
   print(p)

result:
{'label': 'org', 'pattern': 'bls ag'}
{'label': 'org', 'pattern': 'chemins de fer du jura'}
{'label': 'org', 'pattern': 'comlux'}
{'label': 'org', 'pattern': 'crh gï¿½ï¿½taz group'}
{'label': 'org', 'pattern': 'dksh management ag'}
{'label': 'org', 'pattern': 'ferdinand steck maschinenfabrik'}
{'label': 'org', 'pattern': 'galenica'}
{'label': 'org', 'pattern': 'givaudan'}
{'label': 'org', 'pattern': 'heliswiss'}
{'label': 'org', 'pattern': 'jet aviation'}
{'label': 'org'attern': 'kolmar'}
...
...

so patterns object look like this:
patterns = [{'label': 'org', 'pattern': 'bls ag'}
{'label': 'org', 'pattern': 'chemins de fer du jura'}
{'label': 'org', 'pattern': 'comlux'}
{'label': 'org', 'pattern': 'crh gï¿½ï¿½taz group'}
{'label': 'org', 'pattern': 'dksh management ag'}
{'label': 'org', 'pattern': 'ferdinand steck maschinenfabrik'}
{'label': 'org', 'pattern': 'galenica'}
{'label': 'org', 'pattern': 'givaudan'}
{'label': 'org', 'pattern': 'heliswiss'}
{'label': 'org', 'pattern': 'jet aviation'}
{'label': 'org', 'pattern': 'kolmar'}....]

then i created a blank model:
nlp = spacy.blank(""en"")
nlp.add_pipe('entity_ruler')
ruler.add_patterns(patterns)

and then, i have tested it like this:
for full_text in list_of_texts:
    doc = nlp(full_text)
    print(doc.ents.text, doc.ents.label_)

and it does not recognises anything (even if im testing it in a sentence thas exact name of the organisations). i have also tried to add tagger and parser to my blank model with entity_ruler but its always the same.
these are some of the examples of text that i have used for testing (each company name in testing texts are also in the patterns with the same capitalisations and spelling):
t1 = ""i work in company called dksh management ag its very good company""
t2 = ""i have stayed in holiday inn express and i really liked it""
t3 = ""have you head for company named akka technologies se""
t4 = ""what do you think about erytech pharma""
t5 = ""did you get an email from esi group""
t6 = ""esso s.a.f. sent me an email last week""

what am i doing wrong?
i have noticed that it works if i do it like this:
ruler = entityruler(nlp)
ruler.add_patterns(patterns)
nlp = spacy.load(""en_core_web_trf"")
nlp.add_pipe('entity_ruler', before = 'tagger')
#if i do print(nlp.pipeline) i can see entity_ruler added before tager.

but then i do not know if it works because of my entity_ruler or because of the pre trained model. i have tested it on 20 example texts and it gives me the same results with entity_ruler and without it, so i cant figure it out if it works better or not.
what am i doing wrong?","['python', 'nlp', 'spacy']",66828940,"you're not adding the entityruler correctly. you're creating an entityruler from scratch and adding rules to it, and then telling the pipeline to create an entityruler that's completely unrelated.
this is the problem code:
ruler = entityruler(nlp)     # ruler 1
ruler.add_patterns(patterns) # ruler 1
nlp = spacy.blank(""en"")
nlp.add_pipe('entity_ruler') # this creates an unrelated ruler 2

this is what you should do:
nlp = spacy.blank(""en"")
ruler = nlp.add_pipe(""entity_ruler"")
ruler.add_patterns(patterns)

that should work.

in spacy v2 the flow for creating a pipeline component was to create the object and then add it to the pipeline, but in v3 the flow is to ask the pipeline to create the component and then use the returned object.

based on your updated examples, here is example code using the entityruler to match the first sentence.
nlp = spacy.blank(""en"")
ruler = nlp.add_pipe(""entity_ruler"")
patterns = [
  {""label"": ""org"", ""pattern"": ""dksh management ag""},
  {""label"": ""org"", ""pattern"": ""some other company""},
]
ruler.add_patterns(patterns)

doc = nlp(""i work in company called dksh management ag its very good company"")
print([(ent.text, ent.label_) for ent in doc.ents])
# output: [('dksh management ag', 'org')]

does that clarify how you should structure your code?
looking at your updated question code, your code with the blank model is almost right, but note that add_pipe returns the entityruler object. you should add your patterns to that object.",https://stackoverflow.com/questions/66825806,python,27-03-2021 00:12,471.0,3.0,1.0,True,27-03-2021 13:12,27-03-2021 13:12
68472183,word-embedding: convert supervised model into unsupervised model,"i want to load an pre-trained embedding to initialize my own unsupervise fasttext model and retrain with my dataset.
the trained embedding file i have loads fine with gensim.models.keyedvectors.load_word2vec_format('model.txt'). but when i try:
fasttext.load_fasttext_format('model.txt') i get: notimplementederror: supervised fasttext models are not supported.
is there any way to convert supervised keyedvectors to unsupervised fasttext? and if possible, is it a bad idea?
i know that has an great difference between supervised and unsupervised models. but i really wanna try use/convert this and retrain it. i'm not finding a trained unsupervised model to load for my case (it's a portuguese dataset), and the best model i find is that","['python', 'nlp', 'gensim', 'unsupervised-learning', 'fasttext']",68473878,"if your model.txt file loads ok with keyedvectors.load_word2vec_format('model.txt'), then that's just a simple set of word-vectors. (that is, not a 'supervised' model.)
however, gensim's fasttext doesn't support preloading a simple set of vectors for further training - for continued training, it needs a full fasttext model, either from facebook's binary format, or a prior gensim fasttext model .save().
(that trying to load a plain-vectors file generates that error suggests the load_fasttext_format() method is momentarily mis-interpreting it as some other kind of binary fasttext model it doesn't support.)
update after comment below:
of course you can mutate a model however you like, including ways not officially supported by gensim. whether that's helpful is another matter.
you can create an ft model with a compatible/overlapping vocabulary, load old word-vectors separately, then copy each prior vector over to replace the corresponding (randomly-initialized) vectors in the new model. (note that the property to affect further training is actually ftmodel.wv.vectors_vocab trained-up full-word vectors, not the .vectors which is composited from full-words & ngrams,)
but the tradeoffs of such an ad-hoc strategy are many. the ngrams would still start random. taking some prior model's just-word vectors isn't quite the same as a fasttext model's full-words-to-be-later-mixed-with-ngrams.
you'd want to make sure your new model's sense of word-frequencies is meaningful, as those affect further training - but that data isn't usually available with a plain-text prior word-vector set. (you could plausibly synthesize a good-enough set of frequencies by assuming a zipf distribution.)
your further training might get a ""running start"" from such initialization - but that wouldn't necessarily mean the end-vectors remain comparable to the starting ones. (all positions may be arbitrarily changed by the volume of newer training, progressively diluting away most of the prior influence.)
so: you'd be in an improvised/experimental setup, somewhat far from usual fasttext practices and thus where you'd want to re-verify lots of assumptions, and rigorously evaluate if those extra steps/approximations are actually improving things.",https://stackoverflow.com/questions/68472183,python,21-07-2021 15:18,160.0,0.0,1.0,True,06-08-2024 23:00,06-08-2024 23:00
27943396,using wn-affect to detect emotion/mood of a string,"i  downloaded wn-affect. i am however not sure how to use it to detect the mood of a sentence. for example if i have a string ""i hate football."" i want to be able to detect whether the mood is bad and the emotion is fear. wn-affect has no tutorial on how to do it, and i am kind of new to python. any help would be great!","['python', 'nlp', 'nltk', 'sentiment-analysis', 'wordnet']",27945838,"in short: use sentiwordnet instead and look at 

in long:
affectedness vs sentiment
the line between affect and sentiment is very fine. one should looking into affectedness in linguistics studies, e.g.  and sentiment analysis in computational researches. for now, let's call both the task of identifying affect and sentiment, sentiment analysis.
also note that wn-affect is a rather old resource compared to sentiwordnet,  
here's a good resource for using sentiwordnet for sentiment analysis:  
often sentiment analysis has only two classes, positive or negative sentiment. whereas the wn-affect uses 11 types of affectedness labels:

emotion
mood 
trait    
cognitive state  
physical state   
hedonic signal   
emotion-eliciting 
emotional response   
behaviour    
attitude 
sensation

for each type, there are multiple classes, see 

to answer the question of how one can use the wn-affect, there're several things you need to do:
first map wn1.6 to wn3.0 (it's not an easy task, you have to do several mappings, especially the mapping between 2.0-2.1)
now using the wn-affect with wn3.0, you can apply 

the same classification technique as he sentiwordnet sentiment classifier or
try to maximize the classes within text and then use some heuristics to choose 'positive' / 'negative'",https://stackoverflow.com/questions/27943396,python,14-01-2015 12:51,9083.0,9.0,2.0,True,03-01-2025 18:54,03-01-2025 18:54
74290324,typeerror: unsupported operand type(s) for /: &#39;sequenceclassifieroutput&#39; and &#39;int&#39;,"i am using hugginface library to train a bert model on classification problem.
    model = bertforsequenceclassification.from_pretrained('bert-base-uncased', num_labels=10) 

    def training_step(self, batch, batch_nb):
       sequence, label = batch
       input_ids, attention_mask, labels = self.prepare_batch(sequence=sequence, label=label)
       loss = self.model(input_ids=input_ids,
                      attention_mask=attention_mask,
                      labels=labels) 
       tensorboard_logs = {'train_loss': loss}

i am getting the following error just before the training starts:
in training_step
closure_loss = closure_loss / self.trainer.accumulate_grad_batches 
typeerror: unsupported operand type(s) for /: 'sequenceclassifieroutput' and 'int'

i am using pytorch-lightning","['python', 'pytorch', 'nlp', 'huggingface-transformers', 'pytorch-lightning']",74292405,"calling self.model() returns an object of type sequenceclassifieroutput.
to access the loss you need to call it's loss attribute:
replace
loss = self.model(input_ids=input_ids,
                      attention_mask=attention_mask,
                      labels=labels) 

by
output = self.model(input_ids=input_ids,
                      attention_mask=attention_mask,
                      labels=labels) 
loss = output.loss",https://stackoverflow.com/questions/74290324,python,02-11-2022 13:52,359.0,0.0,1.0,True,11-12-2022 17:38,11-12-2022 17:38
76980939,importerror: partition_docx is not available using langchain on huggingface,"i am using the directoryloader with langchain on huggingface (gradio sdk) like so from my folder named ""data"":
from langchain.document_loaders import directoryloader  
  
loader = directoryloader('./data/')  
raw_documents = loader.load() 

but get the following error:
importerror: partition_docx is not available. install the docx dependencies with pip install ""unstructured[docx]""
does anyone have any insight as to why this error is being given? nothing pops up for me on a web search for this error.
thanks in advance! apologies if more context is needed, just getting into python and i am very novice.","['python', 'importerror', 'langchain', 'huggingface']",77010749,"op leanna created the gradio space and then noticed the import error. so here are all the details.
for debugging, create gradio space
in hugging face,

create gradio space



to manage dependencies, create requirements.txt and add the modules that are needed:

langchain
unstructured
unstructured[docx]

files can be reviewed at : simpleappgradio",https://stackoverflow.com/questions/76980939,python,25-08-2023 23:03,1579.0,1.0,2.0,True,30-08-2023 18:43,29-08-2023 22:49
75621922,didnt get the expected results when calculate cosine similarity between strings,"i want to calculate the pairwise cosine similarity between two strings  that are in the same row of a pandas data frame.
i used the following lines of codes:
import pandas as pd
from sklearn.feature_extraction.text import tfidfvectorizer
from sklearn.feature_extraction.text import countvectorizer
from sklearn.metrics.pairwise import cosine_similarity


pd.set_option('display.float_format', '{:.4f}'.format)


df = pd.dataframe({'text1': ['the quick brown fox jumps over the lazy dog', 'the red apple', 'the big blue sky'],
                   'text2': ['the lazy cat jumps over the brown dog', 'the red apple', 'the big yellow sun']})


vectorizer = countvectorizer().fit_transform(df['text1'] + ' ' + df['text2'])


cosine_similarities = cosine_similarity(vectorizer)[:, 0:1]


df['cosine_similarity'] = cosine_similarities


print(df)  

it gave me following output, which seems incorrect:

can anyone help me to figure out what i did incorrectly?
thank you.","['python', 'pandas', 'scikit-learn', 'nlp', 'cosine-similarity']",75623734,"i'm no expert, but here's one way to do it.
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import countvectorizer
from sklearn.metrics.pairwise import cosine_similarity

pd.set_option('display.float_format', '{:.4f}'.format)

df = pd.dataframe({'text1': ['the quick brown fox jumps over the lazy dog',
                             'the red apple',
                             'the big blue sky'],
                   'text2': ['the lazy cat jumps over the brown dog',
                             'the red apple',
                             'the big yellow sun']})

vectorizer = countvectorizer()

# np.hstack([df[""text1""], df[""text2""]]) puts all ""text2"" after ""text1""
x = vectorizer.fit_transform(np.hstack([df[""text1""], df[""text2""]]))

cs = cosine_similarity(x)  # full symmetric numpy.ndarray

# the values you want are on an offset diagonal of cs since
# ""text2"" strings were stacked at the end of ""text1"" strings

pairwise_cs = cs.diagonal(offset=len(df))
df[""cosine_similarity""] = pairwise_cs

print(df)

which shows:
                                         text1                                  text2  cosine_similarity
0  the quick brown fox jumps over the lazy dog  the lazy cat jumps over the brown dog             0.8581
1                                the red apple                          the red apple             1.0000
2                             the big blue sky                     the big yellow sun             0.5000",https://stackoverflow.com/questions/75621922,python,02-03-2023 23:46,266.0,0.0,1.0,True,03-03-2023 06:14,02-03-2023 23:57
76310625,loss function does not train,"we are training a questionanswering model for the squad v2 dataset.
a roberta encoder, with a classifier on top. predicting the answer span works perfectly. however, we wanted to add a front classifier to predict the answerability of a question (as suggested in the paper ""retrospective reader for machine reading comprehension"").
using the model below, the start and end loss are decreasing, however the answerable_loss does not train.
what we intend to do is:

get the first element of the encoded input
predict (answerable, unanswerable) on this element
softmax(answerable, unanswerable) to get a certainty percentage
calculate the loss using cross entropy

(ps: notice some tricky dimension stuff due to batches)
and to the best of my knowledge this is what we should do, and are doing in the code. but obviously it does not work...
i can't seem to find my error. why might this be the case?
class lstmfrontverifier(nn.module):
    name = ""lstm-front-verifier""
    
    def __init__(self, encoder):
        super().__init__()
        self.encoder = encoder
    
        self.answerable = nn.linear(encoder.config.hidden_size, 2)
        self.classifier = nn.linear(encoder.config.hidden_size, 2)
    
    def forward(self, input_ids, attention_mask=none, start_positions=none, end_positions=none):
        outputs = self.encoder(input_ids, attention_mask=attention_mask)
        lstm_output = outputs.last_hidden_state
    
        logits = self.classifier(lstm_output)
    
        start_logits, end_logits = logits.split(1, dim=-1)
        start_logits, end_logits = start_logits.squeeze(-1), end_logits.squeeze(-1)
    
        # given answer-ability
        unanswerabe = torch.logical_and(start_positions == 0, end_positions == 0).float()
    
        start_loss = f.cross_entropy(start_logits, start_positions)
        end_loss = f.cross_entropy(end_logits, end_positions)
    
        # predict answerability
        pred_answerable = self.answerable(lstm_output[:, 0])
        answerable_pred = f.softmax(pred_answerable, dim=1)
        answerable_loss = f.cross_entropy(answerable_pred[:, 0], 1-unanswerabe) + \
                          f.cross_entropy(answerable_pred[:, 1], unanswerabe)
    
        print((start_loss + end_loss).item(), answerable_loss.item())
        loss = start_loss + end_loss + answerable_loss
    
        return loss

    def forward_ex(self, example):
        input_ids = example[""input_ids""].to(device)
        start_positions = example[""start_positions""].to(device) if ""start_positions"" in example else none
        end_positions = example[""end_positions""].to(device) if ""end_positions"" in example else none
        attention_mask = example[""attention_mask""].to(device) if ""attention_mask"" in example else none
        return self.forward(input_ids, attention_mask, start_positions, end_positions)

for reproducability purpouses here a minimal code example:
import torch
from transformers import autotokenizer
from datasets import load_dataset
from torch.utils.data.dataloader import dataloader
import torch.nn.functional as f
import torch.nn as nn
from transformers import automodel

max_train_examples = 1000
max_length = 384
stride = 128
device = ""cuda"" if torch.cuda.is_available() else ""cpu""

tokenizer = autotokenizer.from_pretrained(""roberta-base"")

def preprocess_examples(examples):
    questions = [q.strip() for q in examples[""question""]]

    inputs = tokenizer(
        questions,
        examples[""context""],
        max_length=max_length,
        truncation=""only_second"",
        stride=stride,
        return_overflowing_tokens=true,
        return_offsets_mapping=true,
        padding=""max_length"",
    )

    offset_mapping = inputs.pop(""offset_mapping"")
    sample_map = inputs.pop(""overflow_to_sample_mapping"")
    answers = examples[""answers""]
    start_positions = []
    end_positions = []

    for i, offset in enumerate(offset_mapping):
        sample_idx = sample_map[i]
        answer = answers[sample_idx]
        if not answer[""answer_start""]:
            start_positions.append(0)
            end_positions.append(0)
            continue

        start_char = answer[""answer_start""][0]
        end_char = answer[""answer_start""][0] + len(answer[""text""][0])
        sequence_ids = inputs.sequence_ids(i)

        idx = 0
        while sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1

        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            idx = context_start
            while idx <= context_end and offset[idx][0] <= start_char:
                idx += 1
            start_positions.append(idx - 1)

            idx = context_end
            while idx >= context_start and offset[idx][1] >= end_char:
                idx -= 1
            end_positions.append(idx + 1)

    inputs[""start_positions""] = start_positions
    inputs[""end_positions""] = end_positions
    return inputs


def convert_to_tensors(examples):
    return {k: torch.tensor([x[k] for x in examples]) for k in examples[0]}


squad = load_dataset('squad_v2')

squad[""train""] = squad[""train""].select(range(max_train_examples))
tokenized_datasets = squad.map(
    preprocess_examples,
    batched=true,
    remove_columns=squad[""train""].column_names,
)

train_dataloader = dataloader(tokenized_datasets[""train""], batch_size=8, collate_fn=convert_to_tensors, shuffle=true)    

roberta = automodel.from_pretrained(""roberta-base"").to(device)
model = lstmfrontverifier(roberta).to(device)
optimizer = torch.optim.adamw(model.parameters(), lr=5e-5, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-3)

model.train()
optimizer.zero_grad()  # reset gradients tensors

for batch, x in enumerate(train_dataloader):
    # compute prediction error
    loss = model.forward_ex(x)

    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()

packages:

install pytorch with gpu: 
pip install transformers datasets

some prints of the print((start_loss + end_loss), answerable_loss):
11.970989227294922 16.621864318847656
11.742887496948242 16.640228271484375
11.448827743530273 16.66130828857422
11.063633918762207 16.665538787841797
10.49462890625 16.663175582885742
10.176043510437012 16.661867141723633
10.13321590423584 16.646312713623047
10.05152702331543 16.64898681640625
9.76708698272705 16.648393630981445
9.409551620483398 16.700483322143555
8.939659118652344 16.641441345214844
9.03275203704834 16.647899627685547
8.160870552062988 16.63787078857422
8.27975845336914 16.641223907470703
7.900142669677734 16.64410972595215
6.427922248840332 16.644954681396484
6.4332380294799805 16.643535614013672
6.626171112060547 16.642642974853516
4.79502010345459 16.640335083007812
6.948017120361328 16.641925811767578
5.472411632537842 16.642606735229492
6.458420753479004 16.63710594177246
6.552549362182617 16.637182235717773
4.95197868347168 16.637977600097656
5.235410690307617 16.643829345703125
4.700412750244141 16.63840675354004
4.11396598815918 16.646831512451172
5.13016414642334 16.643505096435547
3.7867109775543213 16.63637924194336
5.582259178161621 16.643115997314453
5.7655229568481445 16.64023208618164
5.085046768188477 16.63158416748047
4.153951644897461 16.63810920715332
4.100613594055176 16.644237518310547
4.206878662109375 16.636249542236328
3.450410842895508 16.635835647583008
4.827783584594727 16.63633918762207
2.2874913215637207 16.644474029541016
2.3297667503356934 16.647319793701172
3.4870200157165527 16.652259826660156
3.31907320022583 16.6363582611084
4.377845764160156 16.637659072875977
3.427989959716797 16.635705947875977
4.224106311798096 16.640310287475586","['python', 'machine-learning', 'pytorch', 'nlp', 'classification']",76316708,"the cross-entropy loss is implemented incorrectly. the correct code snippet for calculating the cross-entropy answerability loss is:
pred_answerable = self.answerable(lstm_output[:, 0])
answerable_loss = f.cross_entropy(pred_answerable, (1-unanswerabe).long())
answerability_prediction = f.softmax(pred_answerable, dim=1)",https://stackoverflow.com/questions/76310625,python,23-05-2023 00:30,91.0,-1.0,1.0,True,27-05-2023 13:14,27-05-2023 13:14
5929154,designing a system that would detect typos and suggestions,"this was asked in an interview.
i think the answer can be done by constructing a trie of all valid words and then suggestions can be made based on a possible valid path which was otherwise given as incorrect.
say if user types apfle, and system would detect that after ap a possible valid path was app, which would then satisfy apple.
is there any better solution than this? perhaps the one implemented by spell checkers?","nlp, spell-checking",5929184,"see:
how does the google ""did you mean?"" algorithm work?
how do i approximate ""did you mean?"" without using google?
how to write a spelling corrector
youtube video: search 101",https://stackoverflow.com/q/5929154,"nlp, spell-checking",08-05-2011 17:51,4402.0,4.0,2.0,True,29-03-2025 20:33,29-03-2025 20:33
76040193,how can i update my chatbot with chatgpt from &quot;text-davinci-003&quot; to &quot;gpt-3.5-turbo&quot; in python,"i'm new in python and i want a little hand into this code.
i'm developing a smart chatbot using the openai api and using it in what's app. i have this piece of my code that is responsible for the chatgpt response in my code. at the moment, this code is on model = ""text-davinci-003"" and i want to turn it into ""gpt-3.5-turbo"". is any good soul interested in helping me?
obs.: ""msg"" is what we ask to chatgpt on whatsapp
the piece of my code:
msg = todas_as_msg_texto[-1]
print(msg) # -> mensagem que o cliente manda (no caso eu)

cliente = 'msg do cliente: '
texto2 = 'responda a mensagem do cliente com base no prï¿½ï¿½ximo texto: '
questao = cliente + msg + texto2 + texto

# #### processa a mensagem na api do chat gpt ####

openai.api_key= apiopenai.strip()

response=openai.completion.create(
    model=""text-davinci-003"",
    prompt=questao,
    temperature=0.1,
    max_tokens=270,
    top_p=1,
    frequency_pen=0,
    presence_penalty=0.6,
)

resposta=response['choices'][0]['text']
print(resposta)
time.sleep(1)","['python', 'chatbot', 'whatsapp', 'openai-api', 'chatgpt-api']",76045751,"to update your code to gpt-3.5-turbo, there are four areas you need to modify:

call openai.chatcompletion.create instead of openai.completion.create
set model='gpt-3.5-turbo'
change messages= to an array as shown below
change the way you are assigning repsonse to your resposta variable so that you are reading from the messages key

this tested example takes into account those changes:
response=openai.chatcompletion.create(
    model=""gpt-3.5-turbo"",
    messages=[{""role"": ""user"", ""content"": questao }],
    temperature=0.1,
    max_tokens=270,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0.6,
)

resposta=response['choices'][0]['message']['content']

additionally, since more than one choice can be returned from the model, instead of only looking at [0] you may be interested in iterating over them to see what you're getting, something like:
for choice in response.choices:
            outputtext = choice.message.content
            print(outputtext)
            print(""------"")
print(""\n"")

note that you don't need to do that if you are calling openai.chatcompletion.create with 'n=1'
additionally, your example is setting both temperature and top_p, however the docs suggest to only set one of those variables.",https://stackoverflow.com/questions/76040193,python,18-04-2023 00:20,2531.0,1.0,2.0,True,19-05-2023 12:17,18-04-2023 00:23
30431688,how to connect cortana commands to custom scripts?,"this may be a little early to ask this, but i'm running windows 10 technical preview build 10122. i'd like to set up cortana to have custom commands. here's how she works:
hey cortana, <she'll listen and process this command>

microsoft will process the command and if there isn't anything for it, she'll just search the input on bing. however, i'd like to be able to say something like, just for example 
hey cortana, i'm going to bed now

and have the input i'm going to bed now trigger run a batch script, a vbscript, a command, or any some sort some of custom response that basically does the following.
c:\> shutdown -s

is there a way to set up a predefined custom commands for cortana?
update:
i created this basic youtube tutorial and this more advanced one with a corresponding github repo based on talkitbr's excellent and very helpful answer below. 
at first his answer was beyond my understanding so i decided to break it down in a bit more detail for future users like myself.","['scripting', 'nlp', 'windows-10', 'cortana']",31387719,"you can create commands for cortana to listen for.
these commands need to be described in a xml file called voice command definitions or vcd.
here's an example:
<?xml version=""1.0"" encoding=""utf-8"" ?>
<voicecommands xmlns=""
    <commandset xml:lang=""en-us"" name=""homecontrolcommandset_en-us"">
        <commandprefix>homecontrol</commandprefix>
        <example>control alarm, temperature, light and others</example>

        <command name=""activate_alarm"">
            <example>activate alarm</example>
            <listenfor>[would] [you] [please] activate [the] alarm [please]</listenfor>
            <listenfor requireappname=""beforeorafterphrase"">activate alarm</listenfor>
            <listenfor requireappname=""explicitlyspecified"">activate {builtin:appname} alarm</listenfor>
            <feedback>activating alarm</feedback>
            <navigate />
        </command>
        ...
    </commandset>
</voicecommands>

after create this definition, you need to register it at app startup:
protected async override void onlaunched(launchactivatedeventargs e)
{
    ...
    // install the vcd
    try
    {
        storagefile vcdstoragefile = await package.current.installedlocation.getfileasync(@""homecontrolcommands.xml"");
        await voicecommanddefinitionmanager.installcommanddefinitionsfromstoragefileasync(vcdstoragefile);
    }
    catch (exception ex)
    {
        system.diagnostics.debug.writeline(""there was an error registering the voice command definitions"", ex);
    }
}

an then override the app.onactivated method to handle when the events are triggered:
protected override void onactivated(iactivatedeventargs e)
{
    // handle when app is launched by cortana
    if (e.kind == activationkind.voicecommand)
    {
        voicecommandactivatedeventargs commandargs = e as voicecommandactivatedeventargs;
        speechrecognitionresult speechrecognitionresult = commandargs.result;
 
        string voicecommandname = speechrecognitionresult.rulepath[0];
        string textspoken = speechrecognitionresult.text;
        ireadonlylist<string> recognizedvoicecommandphrases;
 
        system.diagnostics.debug.writeline(""voicecommandname: "" + voicecommandname);
        system.diagnostics.debug.writeline(""textspoken: "" + textspoken);
 
        switch (voicecommandname)
        {
            case ""activate_alarm"":
                system.diagnostics.debug.writeline(""activate_alarm command"");
                break;

the tutorial shows the complete code [web archive].
after you do all of this, you can call your batch scripts using processstartinfo or system.diagnostics.process.start.
also, if you are interested in responding to the user through cortana window, check this post regarding cortana in background [web archive].",https://stackoverflow.com/questions/30431688,scripting,25-05-2015 05:21,35394.0,41.0,2.0,True,19-08-2021 15:54,23-05-2017 12:26
64483451,how to predict a character based on character based rnn model?,"i want to create a prediction function  which complete a part of ""sentence""
the model used here is  a character based rnn(lstm). what are the steps we should fellow ?
i tried this but i can't give as input the sentence
 def generate(self) -> tuple[list[token], torch.tensor]:

    start_symbol_idx = self.vocab.get_token_index(start_symbol, 'tokens')
   # print(start_symbol_idx)
    end_symbol_idx = self.vocab.get_token_index(end_symbol, 'tokens')
    padding_symbol_idx = self.vocab.get_token_index(default_padding_token, 'tokens')

    log_likelihood = 0.
    words = []
    state = (torch.zeros(1, 1, self.hidden_size), torch.zeros(1, 1, self.hidden_size))

    word_idx = start_symbol_idx

    for i in range(self.max_len):
        tokens = torch.tensor([[word_idx]])

        embeddings = self.embedder({'tokens': tokens})
        output, state = self.rnn._module(embeddings, state)
        output = self.hidden2out(output)

        log_prob = torch.log_softmax(output[0, 0], dim=0)

        dist = torch.exp(log_prob)

        word_idx = start_symbol_idx

        while word_idx in {start_symbol_idx, padding_symbol_idx}:
            word_idx = torch.multinomial(
                dist, num_samples=1, replacement=false).item()

        log_likelihood += log_prob[word_idx]

        if word_idx == end_symbol_idx:
            break

        token = token(text=self.vocab.get_token_from_index(word_idx, 'tokens'))
        words.append(token)

    return words, log_likelihood,start_symbol_idx","['nlp', 'lstm', 'recurrent-neural-network']",64488545,here are two tutorial on how to use machine learning libraries to generate text tensorflow and pytorch.,https://stackoverflow.com/questions/64483451,nlp,22-10-2020 13:36,361.0,0.0,2.0,True,12-09-2021 18:56,22-10-2020 16:25
69704467,concatenate layer shape error in sequence2sequence model with keras attention,"i'm trying to implement a simple word-level sequence-to-sequence model with keras in colab. i'm using the keras attention layer. here is the definition of the model:
embedding_size=200
units=128

encoder_inputs = input(shape=(none,), name=""encoder_inputs"")

encoder_embs=embedding(num_encoder_tokens, embedding_size, name=""encoder_embs"")(encoder_inputs)

#encoder lstm
encoder = lstm(units, return_state=true, name=""encoder_lstm"") #(encoder_embs)
encoder_outputs, state_h, state_c = encoder(encoder_embs)

encoder_states = [state_h, state_c]

decoder_inputs = input(shape=(none,), name=""decoder_inputs"")
decoder_embs = embedding(num_decoder_tokens, embedding_size, name=""decoder_embs"")(decoder_inputs)

#decoder lstm
decoder_lstm = lstm(units, return_sequences=true, return_state=true, name=""decoder_lstm"")
decoder_outputs, _, _ = decoder_lstm(decoder_embs, initial_state=encoder_states)

attention=attention(name=""attention_layer"")
attention_out=attention([encoder_outputs, decoder_outputs])

decoder_concatenate=concatenate(axis=-1, name=""concat_layer"")([decoder_outputs, attention_out])
decoder_outputs = timedistributed(dense(units=num_decoder_tokens, 
                                  activation='softmax', name=""decoder_denseoutput""))(decoder_concatenate)

model=model([encoder_inputs, decoder_inputs], decoder_outputs, name=""s2s_model"")
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

model compiling is fine, no problems whatsoever. the encoder and decoder input and output shapes are:
encoder training input shape:  (4000, 21)
decoder training input shape:  (4000, 12)
decoder training target shape:  (4000, 12, 3106)
--
encoder test input shape:  (385, 21)

this is the model.fit code:
model.fit([encoder_training_input, decoder_training_input], decoder_training_target,
      epochs=100,
      batch_size=32,
      validation_split=0.2,)

when i run the fit phase, i get this error from the concatenate layer:
valueerror: dimension 1 in both shapes must be equal, but are 12 and 32. 
shapes are [32,12] and [32,32]. for '{{node s2s_model/concat_layer/concat}} = concatv2[n=2, t=dt_float, tidx=dt_int32](s2s_model/decoder_lstm/partitionedcall:1,
s2s_model/attention_layer/matmul_1, s2s_model/concat_layer/concat/axis)' with input shapes: [32,12,128], [32,32,128], [] and with computed input tensors: input[2] = <2>.

so, the first 32 are batch_size, 128 are output shape from decoder_outputs and attention_out, 12 is the number of tokens of decoder inputs. i can't understand how to solve this error, i can't change the number of input tokens i think, any suggestions for me?","['python', 'keras', 'nlp', 'attention-model', 'sequence-to-sequence']",69735167,"solved this thanks to @majitsima. i swapped the inputs to the attention layer, so instead of
attention=attention(name=""attention_layer"")
attention_out=attention([encoder_outputs, decoder_outputs])

the input is
attention=attention(name=""attention_layer"")
attention_out=attention([decoder_outputs, encoder_outputs])

with
decoder_concatenate=concatenate(axis=-1, name=""concat_layer"")([decoder_outputs, attention_out])

everything seems to work now, so thank you again @majitsima, hope this can help!",https://stackoverflow.com/questions/69704467,python,25-10-2021 07:52,1147.0,1.0,2.0,True,27-10-2021 08:22,25-10-2021 12:37
77462188,langchain and azure cognitive search - importerror - cannot import name &#39;vector&#39; from &#39;azure.search.documents.models&#39;,"i am seeing a ""cannot import name 'vector' from azure.search.documents.models"" error when i invoke my chain. origin of my error is line 434 in lanchain/vectorstores/azuresearch.py (from azure.search.documents.models import vector)
this is the relevant code snippet, i get the import error when i execute rag_chain.invoke(question)
from langchain.schema.runnable import runnablepassthrough
from langchain.prompts import chatprompttemplate
from langchain.chat_models.azure_openai import azurechatopenai

question = ""my question..""

# vector_store is initialized using azuresearch(), not including that snippet here
retriever = vector_store.as_retriever()

template = ''' 
answer the question based on the following context: 
{context}

question: {question} 
'''

prompt = chatprompttemplate.from_template(template=template)

llm = azurechatopenai( deployment_name='my_deployment_name', model_name='my_model', openai_api_base=my_azure_openai_endpoint, openai_api_key=my_azure_openai_key, openai_api_version='2023-05-15', openai_api_type='azure' )

rag_chain = {'context' : retriever, 'question' : runnablepassthrough} | prompt | llm 
rag_chain.invoke(question)

my package versions

langchain==0.0.331
azure-search-documents==11.4.0b11
azure-core==1.29.5
openai==0.28.1","['azure-cognitive-services', 'langchain', 'azure-openai', 'content-based-retrieval']",77480339,"""cannot import name 'vector' from azure.search.documents.models"" error
when i invoke my chain. origin of my error is line 434 in lanchain/vectorstores/azuresearch.py (from
azure.search.documents.models import vector)

according to this document you need to install the azure-search-documents==11.4.0b8 for vector stores of azure search.
now you can use the below code that i tested in my environment:
code:
from langchain.prompts import chatprompttemplate
from langchain.chat_models.azure_openai import azurechatopenai
from langchain.vectorstores.azuresearch import azuresearch
from langchain.embeddings import openaiembeddings
from langchain.schema import stroutputparser
from langchain.schema.runnable import runnablepassthrough
import os


model = ""xxxxx""
chunk_size = 1

os.environ[""openai_api_type""] = ""azure""
os.environ[""openai_api_base""] = ""xxxx""
os.environ[""openai_api_key""] = ""xxxx""
os.environ[""openai_api_version""] = ""2023-05-15""

my_azure_openai_endpoint=""xxxx""
openaikey=""xxxxx""
vector_store_address = ""xxxx""
vector_store_password = ""xxxxx""
index_name = ""sample-index""
embeddings = openaiembeddings(deployment=model, chunk_size=chunk_size)
vector_store = azuresearch(
    azure_search_endpoint=vector_store_address,
    azure_search_key=vector_store_password,
    index_name=index_name,
    embedding_function=embeddings.embed_query,
)

retriever = vector_store.as_retriever()


template = """"""answer the question based only on the following context:

{context}

question: {question}
""""""
prompt = chatprompttemplate.from_template(template)

llm = azurechatopenai( deployment_name='gpt-35-turbo', openai_api_base=my_azure_openai_endpoint, openai_api_key=openaikey, openai_api_version='2023-05-15', openai_api_type='azure' )

def format_docs(docs):
    return ""\n\n"".join([d.page_content for d in docs])


chain = (
    {""context"": retriever | format_docs, ""question"": runnablepassthrough()}
    | prompt
    | llm
    | stroutputparser()
)
print(chain.invoke(""what did the president say about technology?""))

output:
> the president mentioned the importance of investing in emerging
> technologies and american manufacturing to compete with china and
> other competitors. he also mentioned the role of computer chips in
> powering everyday technology and the potential for intel to increase
> its investment in manufacturing from $20 billion to $100 billion.


reference:
langchain-full-course/langchain_expressions.ipynb at main ï¿½ï¿½ coding-crashkurse/langchain-full-course ï¿½ï¿½ github<",https://stackoverflow.com/questions/77462188,azure-cognitive-services,10-11-2023 18:34,1733.0,0.0,1.0,True,14-11-2023 11:44,10-11-2023 18:46
73999391,stderr output from native app classifier: modulenotfounderror: no module named &#39;nltk&#39;,"while trying to send/receive data using native messaging between javascript and python for a firefox extension i keep running into the following error in the browser console-
stderr output from native app classifier: modulenotfounderror: no module named 'nltk'
i have installed nltk in my pycharm virtual environment. i want to use nltk for some text processing in the python file used for native messaging. i dont get any error messages for other packages like sys, json, struct. but i get error messages for nltk, keras. but there is no error for pandas!!
native messaging works fine when i dont import nltk into python.
following is the python code. i have not shared nltk part of the code here
import sys
import json
import struct
import nltk


class informationtransmission:

    def getmessage(self):
        rawlength = sys.stdin.buffer.read(4)
        if len(rawlength) == 0:
            sys.exit(0)
        messagelength = struct.unpack('@i', rawlength)[0]
        message = sys.stdin.buffer.read(messagelength).decode('utf-8')
        return json.loads(message)

    def encodemessage(self, messagecontent):
        encodedcontent = json.dumps(messagecontent).encode('utf-8')
        encodedlength = struct.pack('@i', len(encodedcontent))
        return {'length': encodedlength, 'content': encodedcontent}

    def sendmessage(self, encodedmessage):
        sys.stdout.buffer.write(encodedmessage['length'])
        sys.stdout.buffer.write(encodedmessage['content'])
        sys.stdout.buffer.flush()

x = informationtransmission()
receivedmessage = x.getmessage()

if receivedmessage:
    x.sendmessage(x.encodemessage(receivedmessage))

js file
function logtabs(tabs) {
    let tab = tabs[0];

    port.postmessage(tab.url);
}

function listtabs() {
    browser.tabs.query({currentwindow: true, active: true}).then(logtabs, console.error);
}

let port = browser.runtime.connectnative(""classifier"");

document.addeventlistener(""click"", (e) => {
    if (e.target.id === ""url"") {
        listtabs();
    }
})

port.onmessage.addlistener((response) => {
    console.log(""received: "" + response);
});

addon manifest.json file-
{
    ""browser_specific_settings"": {
        ""gecko"": {
            ""id"": ""test@example.org"",
          ""strict_min_version"": ""58.0a1""
        }
    },

    ""manifest_version"": 2,
    ""name"": ""classifier"",
    ""version"": ""1.0"",

    ""description"": ""classifies"",

    ""background"": {
        ""scripts"": [""popup.js""]
    },

    ""browser_action"":{
        ""browser_style"": true,
        ""default_icon"":{
            ""48"":""/icon.svg""
        },
        ""default_title"":""classifier"",
        ""default_popup"":""/popup.html""
    },

    ""permissions"": [
        ""tabs"",
        ""activetab"",
        ""scripting"",
        ""nativemessaging""
    ]
}

following are the bat and json file for native app-
@echo off

call python -u ""e:\proj\send_recieve_info.py""

{
  ""name"": ""classifier"",
  ""description"": ""host for native messaging"",
  ""path"": ""e:\\proj\\calltoscript.bat"",
  ""type"": ""stdio"",
  ""allowed_extensions"": [""test@example.org""]
}","['javascript', 'python-3.x', 'react-native', 'firefox', 'nltk']",74000525,actually i got it. the path that python was pointing to did not have nltk and keras. the plugin was using python from system path not from the virtual environment as i thought.,https://stackoverflow.com/questions/73999391,javascript,08-10-2022 18:07,127.0,0.0,1.0,True,20-01-2023 01:31,20-01-2023 01:31
67896141,how to read a text and label each word of it in python,"data = (""thousands of demonstrators have marched through london to protest the war in iraq and demand the withdrawal of british troops from that country. many people have been killed that day."",
        {""entities"": [(48, 54, 'category 1'), (77, 81, 'category 1'), (111, 118, 'category 2'), (150, 173, 'category 3')]})

data[1]['entities'][0] = (48, 54, 'category 1') stands for (start_offset, end_offset, entity).
i want to read each word of data[0] in a sequential manner and tag each word according to data[1] entities. i am expecting to have as final output,
{
'thousands': 'o', 
'of': 'o',
'demonstrators': 'o',
'have': 'o',
'marched': 'o',
'through': 'o',
'london': 's-1',
'to': 'o', 
'protest': 'o', 
'the': 'o', 
'war': 'o', 
'in': 'o', 
'iraq': 's-1',
'and': 'o' 
'demand': 'o', 
'the': 'o', 
'withdrawal': 'o', 
'of': 'o', 
'british': 's-2', 
'troops': 'o', 
'from': 'o',
'that': 'o', 
'country': 'o',
'.': 'o',
'many': 'o', 
'people': 's-3', 
'have': 'b-3', 
'been': 'b-3', 
'killed': 'e-3', 
'that': 'o', 
'day': 'o',
'.': 'o'
}

here, 'o' stands for 'outofentity', 's' stands for 'start', 'b' stands for 'between', and 'e' stands for 'end' and are unique for every given text.

i tried the following:
def ner(data):
    entities = {}
    offsets = data[1]['entities']
    for entity in offsets:
        entities[data[0][int(entity[0]):int(entity[1])]] = re.findall('[0-9]+', entity[2])[0]
    
    tags = []
    for key, value in entities.items():
        entity = key.split()
        if len(entity) > 1:
            bentity = entity[1:-1]
            tags.append((entity[0], 's-'+value))
            for item in bentity:
                tags.append((item, 'b-'+value))
            tags.append((entity[-1], 'e-'+value))
        else:
            tags.append((entity[0], 's-'+value))
    
    tokens = nltk.word_tokenize(data[0])
    otokens = [(token, 'o') for token in tokens if token not in [token[0] for token in tags]]
    for token in otokens:
        tags.append(token)
    
    return tags

but the above function does not work properly in case i have some words that are the same as those in data[1]['entities'] offsets but not part of the offsets will be ignored instead they should be labeled as 'o'.","['python', 'text', 'nltk', 'named-entity-recognition']",67896734,"not sure if the final format is json, yet below is an example to process the data into the print format, i.e.
# sample output
'''
{
'thousands': 'o',
'of': 'o',
'demonstrators': 'o',
'have': 'o',
'marched': 'o',
'through': 'o',
'london': 's-1',
'to': 'o',
'protest': 'o',
'the': 'o',
'war': 'o',
'in': 'o',
'iraq': 's-1',
'and': 'o',
'demand': 'o',
'the': 'o',
'withdrawal': 'o',
'of': 'o',
'british': 's-2',
'troops': 'o',
'from': 'o',
'that': 'o',
'country.': 'o',
'many': 'o',
'people': 's-3',
'have': 'b-3',
'been': 'b-3',
'killed': 'e-3',
'that': 'o',
'day.': 'o'
}
'''
# sample code
data = (""thousands of demonstrators have marched through london to protest the war in iraq and demand the withdrawal of british troops from that country. many people have been killed that day."",
        {""entities"": [(48, 54, 'category 1'), (77, 81, 'category 1'), (111, 118, 'category 2'), (150, 173, 'category 3')]})

print(""{"")
pre = 0
for i in (data[1].values())[0]:
        a = data[0][i[0]:i[1]].split()
        t = pre + i[1]
        #print(pre, i[0])
        b = data[0][pre:i[0]].split()
        for j in b:
                print(""'%s': '%s',"" % (j, ""o""))
        pre = i[1]
        for j in range(len(a)): 
                if j == 0:
                        print(""'%s': '%s-%s',"" % (a[j], ""s"", i[2][-1]))
                elif j == len(a) - 1:
                        print(""'%s': '%s-%s',"" % (a[j], ""e"", i[2][-1]))
                else:
                        print(""'%s': '%s-%s',"" % (a[j], ""b"", i[2][-1]))
#print(i[1], las)
las = len(data[0])
c = data[0][i[1]:las].split()
for j in range(len(c)):
        if j == len(c) - 1:
                print(""'%s': '%s'"" % (c[j], ""o""))
        else:
                print(""'%s': '%s',"" % (c[j], ""o""))
print(""}"")",https://stackoverflow.com/questions/67896141,python,09-06-2021 00:51,288.0,0.0,1.0,True,28-11-2022 11:57,09-06-2021 00:57
3227524,how to detect language of user entered text?,"i am dealing with an application that is accepting user input in different languages (currently 3 languages fixed). the requirement is that users can enter text and dont bother to select the language via a provided checkbox in the ui.
is there an existing java library to detect the language of a text?
i want something like this:
text = ""to be or not to be thats the question.""

// returns iso 639 alpha-2 code
language = detect(text);

print(language);

result:
en

i dont want to know how to create a language detector by myself (i have seen plenty of blogs trying to do that). the library should provide a simple api and also work completely offline. open-source or commercial closed doesn't matter.
i also found this questions on so (and a few more):
how to detect language
how to detect language of text?","['java', 'nlp', 'language-detection']",3232338,"here are two options 

languageidentifier
rosette language identifier",https://stackoverflow.com/questions/3227524,java,12-07-2010 10:07,64947.0,46.0,7.0,True,26-02-2023 03:17,23-05-2017 11:54
70890429,tokenizing chinese text with keras.preprocessing.text.tokenizer,"keras.preprocessing.text.tokenizer doesn't work correctly with chinese text. how can i modify it to work on chinese text?
from keras.preprocessing.text import tokenizer
def fit_get_tokenizer(data, max_words):
    tokenizer = tokenizer(num_words=max_words, filters='!""#%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n')
    tokenizer.fit_on_texts(data)
    return tokenizer
tokenizer = fit_get_tokenizer(df.sentence,max_words=150000)
print('total number of words: ', len(tokenizer.word_index))
vocabulary_inv = {}
for word in tokenizer.word_index:
    vocabulary_inv[tokenizer.word_index[word]] = word
print(vocabulary_inv)","['python', 'keras', 'nlp', 'tokenize', 'cjk']",70896031,"def fit_get_tokenizer(data, max_words):
    c=[]
    for i in range(len(data)):
        a = []
        text_tokens = re.findall(r'(.*?[ï¿½ï¿½ï¿½\ . \ ï¿½ï¿½ï¿½)ï¿½ï¿½ï¿½])\s?', data[i])
        for i, j in enumerate(text_tokens):

            seg_list = jieba.lcut(j, cut_all=false)
            sen = "" "".join(seg_list)
            a.append(sen)
        for i in a:
            c.append(i)
    tokenizer = tokenizer(num_words=max_words)
    tokenizer.fit_on_texts(c)
    return tokenizer ```

if anyone going thorough chinese text segmentation. i used regular expression to extract sentence from chinese paragraph. then i have used jieba(instead of nltk) to get perfect word tokens and getting it ready keras tokeniz",https://stackoverflow.com/questions/70890429,python,28-01-2022 07:55,792.0,2.0,2.0,True,28-01-2022 15:17,28-01-2022 09:51
74863356,can&#39;t find table(s) lexeme_norm for language &#39;en&#39; in spacy-lookups-data,"i want to train new ner entities with the following code:
def train_spacy_model(data, model='en_core_web_trf', n_iter=50):
    if model is not none:
        nlp = spacy.load(model)  # load existing spacy model
        print(""loaded model '%s'"" % model)
        
    train_data = data
    ner = nlp.get_pipe(""ner"")
    
    examples = []
    for text, annotations in train_data:
        examples.append(example.from_dict(nlp.make_doc(text), annotations))
    nlp.initialize(lambda: examples)
    
    pipe_exceptions = [""ner""]
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]
    with nlp.disable_pipes(*other_pipes):  # only train ner

        for itn in range(n_iter):
            random.shuffle(examples)
            losses = {}
            batches = minibatch(examples, size=compounding(4.0, 64.0, 1.2))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(
                    batch,  
                    drop=0.20, 
                    losses=losses
                   
                )
            print(""losses"", losses)
    
    return nlp

nlp = train_spacy_model(data=dataset, n_iter=30)

i keep getting this error:
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
cell in[296], line 40
     36             print(""losses"", losses)
     38     return nlp
---> 40 nlp = train_spacy_model(data=no_verlaps_dataset, n_iter=30)
     42 # save model to output directory
     43 output_dir = '_data/models/actor_ner'

cell in[296], line 16, in train_spacy_model(data, model, n_iter)
     14 for text, annotations in train_data:
     15     examples.append(example.from_dict(nlp.make_doc(text), annotations))
---> 16 nlp.initialize(lambda: examples)
     17     # for ent in annotations.get('entities'):
     18     #     ner.add_label(ent[2])
     20 pipe_exceptions = [""ner"", ""trf_wordpiecer"", ""trf_tok2vec""]

file ~/miniconda3/envs/tvman_env/lib/python3.9/site-packages/spacy/language.py:1290, in language.initialize(self, get_examples, sgd)
   1288 config = self.config.interpolate()
   1289 # these are the settings provided in the [initialize] block in the config
-> 1290 i = registry.resolve(config[""initialize""], schema=configschemainit)
   1291 before_init = i[""before_init""]
   1292 if before_init is not none:

file ~/miniconda3/envs/tvman_env/lib/python3.9/site-packages/thinc/config.py:746, in registry.resolve(cls, config, schema, overrides, validate)
    737 @classmethod
    738 def resolve(
    739     cls,
   (...)
    744     validate: bool = true,
    745 ) -> dict[str, any]:
--> 746     resolved, _ = cls._make(
    747         config, schema=schema, overrides=overrides, validate=validate, resolve=true
    748     )
    749     return resolved

file ~/miniconda3/envs/tvman_env/lib/python3.9/site-packages/thinc/config.py:795, in registry._make(cls, config, schema, overrides, resolve, validate)
    793 if not is_interpolated:
    794     config = config(orig_config).interpolate()
--> 795 filled, _, resolved = cls._fill(
    796     config, schema, validate=validate, overrides=overrides, resolve=resolve
    797 )
    798 filled = config(filled, section_order=section_order)
    799 # check that overrides didn't include invalid properties not in config

file ~/miniconda3/envs/tvman_env/lib/python3.9/site-packages/thinc/config.py:867, in registry._fill(cls, config, schema, validate, resolve, parent, overrides)
    864     getter = cls.get(reg_name, func_name)
    865     # we don't want to try/except this and raise our own error
    866     # here, because we want the traceback if the function fails.
--> 867     getter_result = getter(*args, **kwargs)
    868 else:
    869     # we're not resolving and calling the function, so replace
    870     # the getter_result with a promise class
    871     getter_result = promise(
    872         registry=reg_name, name=func_name, args=args, kwargs=kwargs
    873     )

file ~/miniconda3/envs/tvman_env/lib/python3.9/site-packages/spacy/language.py:108, in load_lookups_data(lang, tables)
    105 @registry.misc(""spacy.lookupsdataloader.v1"")
    106 def load_lookups_data(lang, tables):
    107     util.logger.debug(f""loading lookups from spacy-lookups-data: {tables}"")
--> 108     lookups = load_lookups(lang=lang, tables=tables)
    109     return lookups

file ~/miniconda3/envs/tvman_env/lib/python3.9/site-packages/spacy/lookups.py:30, in load_lookups(lang, tables, strict)
     28 if lang not in registry.lookups:
     29     if strict and len(tables) > 0:
---> 30         raise valueerror(errors.e955.format(table="", "".join(tables), lang=lang))
     31     return lookups
     32 data = registry.lookups.get(lang)

valueerror: [e955] can't find table(s) lexeme_norm for language 'en' in spacy-lookups-data. make sure you have the package installed or provide your own lookup tables if no default lookups are available for your language.

i have installed the package:
pip install spacy-lookups-data
collecting spacy-lookups-data
  downloading spacy_lookups_data-1.0.3-py2.py3-none-any.whl (98.5 mb)
     ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 98.5/98.5 mb 25.9 mb/s eta 0:00:00
</","['python-3.x', 'spacy', 'spacy-3', 'spacy-transformers']",74874167,i've been running this code in jupyter notebook and the error persisted until i restarted the kernel. so the answer is to restart the notebook kernel.,https://stackoverflow.com/questions/74863356,python-3.x,20-12-2022 12:50,929.0,0.0,2.0,True,21-12-2022 09:54,21-12-2022 07:39
77480710,r: stm + searchk fails to determine the optimal number of topics,"please have a look at the self-contained example at the end of the post.
i simplified the reprex and you can download the dfm (document-feature matrix) from

a couple of things which i do not understand happen

when i run stm with 9 topics, some of them appear to yield duplicated results (at least in the top 10 keywords per topic, see the plot generated in the reprex). any idea why?
when i try using the searchk() function from stm to determine the optimal number of topics, i get an error message i cannot decipher.
the same happened at least to another user, see

what causes 'subscript out of bounds' error in stm topic modeling with missing data?
but here i give a reproducible example.
any help for 1) and 2) is appreciated!
library(dplyr)
#> 
#> attaching package: 'dplyr'
#> the following objects are masked from 'package:stats':
#> 
#>     filter, lag
#> the following objects are masked from 'package:base':
#> 
#>     intersect, setdiff, setequal, union
library(quanteda)
#> package version: 3.3.1
#> unicode version: 15.0
#> icu version: 72.1
#> parallel computing: 4 of 4 threads used.
#> see  for tutorials and examples.
library(stm)
#> stm v1.3.6.1 successfully loaded. see ?stm for help. 
#>  papers, resources, and other materials at structuraltopicmodel.com
library(rcurl)
library(readtext)
#> 
#> attaching package: 'readtext'
#> the following object is masked from 'package:quanteda':
#> 
#>     texts
library(tidytext)
library(ggplot2)

## download the dfm matrix from

## 


dfm_mat <- readrds(""dfm_mat.rds"")






## see 

## convert the dfm to a format suitable to stm.

dfm2stm <- convert(dfm_mat, to = ""stm"")



model.stm <- stm(dfm2stm$documents, dfm2stm$vocab, k = 9, data = dfm2stm$meta,
                 init.type = ""spectral"") 
#> beginning spectral initialization 
#>   calculating the gram matrix...
#>   finding anchor words...
#>      .........
#>   recovering initialization...
#>      ...........................
#> initialization complete.
#> ...
#> completed e-step (0 seconds). 
#> completed m-step. 
#> completing iteration 1 (approx. per word bound = -6.780) 
#> ...
#> completed e-step (0 seconds). 
#> completed m-step. 
#> completing iteration 2 (approx. per word bound = -6.762, relative change = 2.715e-03) 
#> ...
#> completed e-step (0 seconds). 
#> completed m-step. 
#> completing iteration 3 (approx. per word bound = -6.761, relative change = 4.260e-05) 
#> ...
#> completed e-step (0 seconds). 
#> completed m-step. 
#> completing iteration 4 (approx. per word bound = -6.761, relative change = 1.602e-05) 
#> ...
#> completed e-step (0 seconds). 
#> completed m-step. 
#> completing iteration 5 (approx. per word bound = -6.761, relative change = 1.024e-05) 
#> topic 1: europe, can, european, new, need 
#>  topic 2: union, need, europe, today, us 
#>  topic 3: europe, union, work, european, need 
#>  topic 4: union, need, europe, today, us 
#>  topic 5: europe, can, european, new, need 
#>  topic 6: europe, union, work, european, need 
#>  topic 7: union, need, europe, today, us 
#>  topic 8: europe, can, european, new, need 
#>  topic 9: accelerate, union, need, europe, us 
#> ...
#> completed e-step (0 seconds). 
#> completed m-step. 
#> model converged

## i make the model tidy.
## see  

stm_tidy <- tidy(model.stm)

gpl <- stm_tidy  |> 
    group_by(topic)  |> 
    top_n(10, beta)  |> 
    ungroup()  |> 
    mutate(topic = paste0(""topic "", topic),
           term = reorder_within(term, beta, topic))  |> 
    ggplot(aes(term, beta, fill = as.factor(topic))) +
    geom_col(alpha = 0.8, show.legend = false) +
    facet_wrap(~ topic, scales = ""free_y"") +
    coord_flip() +
    scale_x_reordered() +
    labs(x = null, y = expression(beta),
         title = ""highest word probabilities for each topic"",
         subtitle = ""different words are associated with different topics"")


gpl



## i can fit a model by stm with a chosen number of topics to the data



### now i try determining the optimal number of topics using the searchk function

### see 

set.seed(02138)

k <- 5:15

 model_search <- searchk(dfm2stm$documents, dfm2stm$vocab, k,
data = dfm2stm$meta)
#> beginning spectral initialization 
#>   calculating the gram matrix...
#>   finding anchor words...
#>      .....
#>   recovering initialization...
#>      ...........................
#> initialization complete.
#> ...
#> completed e-step (0 seconds). 
#> completed m-step. 
#> completing iteration 1 (approx. per word bound = -6.781) 
#> ...
#> completed e-step (0 seconds). 
#> completed m-step. 
#> completing iteration 2 (approx. per word bound = -6.761, relative change = 2.956e-03) 
#> ...
#> completed e-step (0 seconds). 
#> completed m-step. 
#> completing iteration 3 (approx. per word bound = -6.761, relative change = 2.235e-05) 
#> ...
#> completed e-step (0 seconds). 
#> completed m-step. 
#> model converged
#> error in missing$docs[[i]]: subscript out of bounds

## this fails but i do not understand why....

sessioninfo()
#> r version 4.3.2 (2023-10-31)
#> platform: x86_64-pc-linux-gnu (64-bit)
#> running under: debian gnu/linux 12 (bookworm)
#> 
#> matrix products: default
#> blas:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.11.0 
#> lapack: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.11.0
#> 
#> locale:
#>  [1] lc_ctype=en_gb.utf-8       lc_numeric=c              
#>  [3] lc_time=en_gb.utf-8        lc_collate=en_gb.utf-8    
#>  [5] lc_monetary=en_gb.utf-8    lc_messages=en_gb.utf-8   
#>  [7] lc_paper=en_gb.utf-8       lc_name=c                 
#>  [9] lc_address=c               lc_telephone=c            
#> [11] lc_measurement=en_gb.utf-8 lc_identification=c       
#> 
#> time zone: europe/brussels
#> tzcode source: system (glibc)
#> 
#> attached base packages:
#> [1] stats     graphics  grdevices utils     datasets  methods   base     
#> 
#> other attached packages:
#> [1] ggplot2_3.4.4   tidytext_0.4.1  readtext_0.90   rcurl_1.98-1.13
#> [5] stm_1.3.6.1     quanteda_3.3.1  dplyr_1.1.3    
#> 
#> loaded via a namespace (and not attached):
#>  [1] janeaustenr_1.0.0  utf8_1.2.4         generics_0.1.3     slam_0.1-50       
#>  [5] bitops_1.0-7       stringi_1.7.12     lattice_0.22-5     digest_0.6.33     
#>  [9] magrittr_2.0.3     evaluate_0.23      grid_4.3.2         fastmap_1.1.1     
#> [13] plyr_1.8.9         matrix_1.6-2       httr_1.4.7         stopwords_2.3     
#> [17] fansi_1.0.5        scales_1.2.1       cli_3.6.1          rlang_1.1.2       
#> [21] tokenizers_0.3.0   munsell_0.5.0      reprex_2.0.2       withr_2.5.2       
#> [25] yaml_2.3.7         tools_4.3.2        reshape2_1.4.4     colorspace_2.1-0  
#> [29] fastmatch_1.1-4    vctrs_0.6.4        r6_2.5.1           lifecycle_1.0.4   
#> [33] stringr_1.5.0      fs_1.6.3           pkgconfig_2.0.3    rcppparallel_5.1.7
#> [37] pillar_1.9.0       gtable_0.3.4       data.table_1.14.8  glue_1.6.2        
#> [41] rcpp_1.0.11        xfun_0.41          tibble_3.2.1       tidyselect_1.2.0  
#> [45] knitr_1.45         farver_2.1.1       htmltools_0.5.7    snowballc_0.7.1   
#> [49] rmarkdown_2.25     labeling_0.4.3     compiler_4.3.2

created on 2023-11-14 with reprex v2.0.2","['r', 'nlp', 'topic-modeling', 'quanteda']",77481657,"i think what is happening is this: with only three documents in your dfm_mat, the searchk() is trying by default to drop half of them to use for a held-out set. this is causing many features to be zero, which means they are dropped from the vocab by default in estimating the topic models used in searchk().  stm() needs only non-zero features, but searchk() considers the vocab set to be fixed, so it's breaking some code inside the function.  (i did not check this in the code however.)
> sum(colsums(dfm_sample(dfm_mat, size = 2)) == 0)
[1] 603
> sum(colsums(dfm_sample(dfm_mat, size = 2)) == 0)
[1] 583
> sum(colsums(dfm_sample(dfm_mat, size = 2)) == 0)
[1] 582

these are the three sample options for dropping 1 of the 3 documents (0.50 rounded up).
you would need to contact the stm package maintainers about a potential bug report.  or, for your problem, use more documents and trim those with low frequencies.",https://stackoverflow.com/questions/77480710,r,14-11-2023 12:45,422.0,0.0,1.0,True,14-11-2023 15:01,14-11-2023 14:44
24073797,proximity search in search engines,please tell me why search engines do not exploit proximity in ranking their pages. what are the limitations that hinder search engines to not use proximity explicitly.,"['search-engine', 'information-retrieval']",24295843,"to directly use proximity information, an index needs to store the position for each term within a document as a part of the postings list for each term. the size of the postings list for positional indexing is typically 4x-5x of the size of standard indexing. this not only uses up extra i/o resources, but also can lead to slow retrieval time, since the retrieval scoring now has to take into consideration the position of each match (query term with document term) as well.
but a search engine can't simply ignore term proximity because it plays an important role in capturing latent semantic concepts, specially for the multi-word expressions. a standard and efficient solution is thus to compile a list of most common phrases for a collection and indexing these phrases as a whole (i.e. treating them as separate terms in the inverted list). for example, a search engine might have separate postings lists for the terms ""german"", ""shepherd"" and the phrase ""german shepherd"". this ensures that documents which contain the phrase ""german shepherd"" are ranked better than those with matches for only german or shepherd.",https://stackoverflow.com/questions/24073797,search-engine,06-06-2014 03:38,477.0,1.0,2.0,True,11-08-2024 18:59,06-06-2014 03:41
76651826,how to chain multiple promptnodes together in a haystack generativeqapipeline,"i'm trying to chain together a simple question answering prompt to an elaboration prompt using haystack.
i had the following code working just fine:
import os

from haystack.document_stores import inmemorydocumentstore
from haystack.nodes import bm25retriever
from haystack.nodes import promptnode, prompttemplate, answerparser
from haystack.pipelines import pipeline, textindexingpipeline


class bert:
pipe = none

def __init__(self, data_path):
    print(""initializing model..."")
    doc_dir = data_path
    document_store = inmemorydocumentstore(use_bm25=true)

    files_to_index = [os.path.join(doc_dir, f) for f in os.listdir(doc_dir)]
    indexing_pipeline = textindexingpipeline(document_store)
    indexing_pipeline.run_batch(file_paths=files_to_index)

    print(""done indexing"")

    retriever = bm25retriever(document_store=document_store, top_k=2)

    lfqa_prompt = prompttemplate(
        prompt=""""""synthesize a comprehensive answer from the following text for the given 
question.
                                 provide a clear and concise response that summarizes the key 
points and information presented in the text.
                                 your answer should be in your own words and be no longer than 
50 words.
                                 \n\n related text: {join(documents)} \n\n question: {query} 
\n\n answer:"""""",
        output_parser=answerparser(),
    )

    prompt_node = promptnode(model_name_or_path=""google/flan-t5-large"", 
default_prompt_template=lfqa_prompt)

    elaboration_prompt = prompttemplate(
        prompt=""""""elaborate on the answer to the following question given the related texts.
                                 provide additional details to the answer in your own words.
                                 the final response should be between 100-200 words.
                                 \n\n related text: {join(documents)} \n\n question: {query} 
 \n\n answer: {prompt_node}"""""",
        output_parser=answerparser(),
    )
    elaboration_node = promptnode(model_name_or_path=""google/flan-t5-large"", 
default_prompt_template=elaboration_prompt)

    self.pipe = pipeline()
    self.pipe.add_node(component=retriever, name=""retriever"", inputs=[""query""])
    self.pipe.add_node(component=prompt_node, name=""prompt_node"", inputs=[""retriever""])
    #self.pipe.add_node(component=elaboration_node, name=""elaboration_node"", inputs=[""query"", 
""retriever"", ""prompt_node""])




def generate(self, query):
    prediction = self.pipe.run(query=query)

    return prediction

but when i tried to chain another promptnode to the end of the lfqa_prompt, i ran into errors. i did some research online and saw that i may need to use shapers and i edited my code as follows:
import os

from haystack.document_stores import inmemorydocumentstore
from haystack.nodes import answerparser, bm25retriever, basecomponent, promptnode, 
prompttemplate, shaper
from haystack.schema import answer, document, list
from haystack.pipelines import pipeline, textindexingpipeline


class qapromptoutputadapter(basecomponent):
outgoing_edges = 1

def run(self, **kwargs):
    print(kwargs)
    return {""answers"": [answer(answer=result, type=""generative"") for result in results]}, 
""output_1""

def run_batch(self):
    pass


class bert:
pipe = none

def __init__(self, data_path):
    print(""initializing model..."")
    doc_dir = data_path
    document_store = inmemorydocumentstore(use_bm25=true)

    files_to_index = [os.path.join(doc_dir, f) for f in os.listdir(doc_dir)]
    indexing_pipeline = textindexingpipeline(document_store)
    indexing_pipeline.run_batch(file_paths=files_to_index)

    print(""done indexing"")

    retriever = bm25retriever(document_store=document_store, top_k=2)

    lfqa_prompt = prompttemplate(
        prompt=""""""synthesize a comprehensive answer from the following text for the given 
question.
                                 provide a clear and concise response that summarizes the key 
points and information presented in the text.
                                 your answer should be in your own words and be no longer than 
50 words.
                                 \n\n related text: {join(documents)} \n\n question: {query} 
\n\n answer:"""""",
        #output_parser=answerparser(),
    )

    prompt_node = promptnode(model_name_or_path=""google/flan-t5-large"", 
default_prompt_template=lfqa_prompt)

    question_shaper = shaper(func=""value_to_list"", inputs={""value"": ""query"", ""target_list"": 
""documents""},
                             outputs=[""questions""])
    answer_shaper = shaper(func=""value_to_list"",
                           inputs={""value"": ""prompt_node.results"", 
""target_list"": ""documents""}, outputs=[""answers""])

    elaboration_prompt = prompttemplate(
        prompt=""""""elaborate on the answer to the following question given the related texts.
                                 provide additional details to the answer in your own words.
                                 the final response should be between 100-200 words.
                                 \n\n related text: {join(documents)} \n\n question: 
{questions} \n\n answer: {outputs}"""""",
        output_parser=answerparser(),
    )
    elaboration_node = promptnode(model_name_or_path=""google/flan-t5-large"",
                                  default_prompt_template=elaboration_prompt)

    self.pipe = pipeline()
    self.pipe.add_node(component=retriever, name=""retriever"", inputs=[""query""])
    self.pipe.add_node(component=prompt_node, name=""prompt_node"", inputs=[""retriever""])
    self.pipe.add_node(component=question_shaper, name=""question_shaper"", inputs= 
[""prompt_node""])
    self.pipe.add_node(component=answer_shaper, name=""answer_shaper"", inputs=[""prompt_node""])
    self.pipe.add_node(component=elaboration_node, name=""elaboration_node"",
                       inputs=[""question_shaper"", ""retriever"", ""answer_shaper""])

def generate(self, query):
    prediction = self.pipe.run(query=query)

    return prediction

now i just get:

exception: exception while running node 'answer_shaper': name 'results' is not defined

is this the correct solution to chaining two prompt nodes together? should i be using shapers or am i going about this completely wrong? i'm fairly new to haystack and generative ai models in general, so help is greatly appreciated.","['python', 'huggingface-transformers', 'bert-language-model', 'haystack']",76675209,"the answer is supposedly to set the ""output_variable"" parameter of the promptnode like this:
lfqa_node = promptnode(
    model_name_or_path=""google/flan-t5-large"", 
    default_prompt_template=lfqa_prompt, 
    output_variable=""my_answer""
)

and then you can use the output like:
elaboration_prompt = prompttemplate(
    prompt=""""""
         ...
         previous answer: {my_answer} \n\n new answer: 
    """"""
)

however, this solution did not seem to work for me, so i simply wrote two separate pipelines, and manually parsed the response from the first pipeline and inputted the answer variable into the second pipeline like this:
lfqa = self.pipe.run(query=query)
lfqa_answer = lfqa['results'][0]
elaboration = self.elaboration_pipeline.run(query=lfqa_answer)",https://stackoverflow.com/questions/76651826,python,10-07-2023 08:06,817.0,2.0,2.0,True,20-07-2023 12:34,10-07-2023 09:00
74785255,cast topic modeling outcome to dataframe,"i have used berttopic with keybert to extract some topics from some docs
from bertopic import bertopic
topic_model = bertopic(nr_topics=""auto"", verbose=true, n_gram_range=(1, 4), calculate_probabilities=true, embedding_model='paraphrase-minilm-l3-v2', min_topic_size= 3)
topics, probs = topic_model.fit_transform(docs)

now i can access the topic name
freq = topic_model.get_topic_info()
print(""number of topics: {}"".format( len(freq)))
freq.head(30)

   topic    count   name
0   -1       1     -1_default_greenbone_gmp_manager
1    0      14      0_ tls_ssl
2    1      8       1_jboss_console_web_application

and inspect the topics
[(' 0.0855701486234524),          
 ('tls', 0.061977919455444744),
 ('ssl tls', 0.061977919455444744),
 ('ssl', 0.061977919455444744),
 ('tcp', 0.04551718585531556),
 ('number', 0.04551718585531556)]

[('jboss', 0.14014705432060262),
 ('console', 0.09285308122803233),
 ('web', 0.07323749337563096),
 ('application', 0.0622930523123512),
 ('management', 0.0622930523123512),
 ('apache', 0.05032395169459188)]

what i want is to have a final dataframe that has in one column the topic name and in another column the elements of the topic
expected outcome:

  class                         entities
o  tls_ssl           
1 jboss_console_web_application  jboss, console, etc

and one dataframe with the topic name on different columns
   tls_ssl           jboss_console_web_application
o http                           jboss
1 tls                            console
2 etc                            etc

i did not find out how to do this. is there a way?","['python-3.x', 'pandas', 'nlp', 'bert-language-model', 'topic-modeling']",74840551,"here is one way to to it:
setup
import pandas as pd
from bertopic import bertopic
from sklearn.datasets import fetch_20newsgroups

docs = fetch_20newsgroups(subset=""all"", remove=(""headers"", ""footers"", ""quotes""))[""data""]

topic_model = bertopic()
# to keep the example reproducible in a reasonable time, limit to 3,000 docs
topics, probs = topic_model.fit_transform(docs[:3_000])

df = topic_model.get_topic_info()
print(df)
# output
   topic  count                    name
0     -1     23         -1_the_of_in_to
1      0   2635         0_the_to_of_and
2      1    114          1_the_he_to_in
3      2    103         2_the_to_in_and
4      3     59           3_ditto_was__
5      4     34  4_pool_andy_table_tell
6      5     32       5_the_to_game_and

first dataframe
using pandas string methods:
df = (
    df.rename(columns={""name"": ""class""})
    .drop(columns=[""topic"", ""count""])
    .reset_index(drop=true)
)

df[""entities""] = [
    [item[0] if item[0] else pd.na for item in topics]
    for topics in topic_model.get_topics().values()
]

df = df.loc[~df[""class""].str.startswith(""-1""), :]  # remove -1 topic

df[""class""] = df[""class""].replace(
    ""^-?\d+_"", """", regex=true
)  # remove prefix '1_', '2_', ...

print(df)
# output
                  class                                                      entities
1         the_to_of_and                [the, to, of, and, is, in, that, it, for, you]
2          the_he_to_in               [the, he, to, in, and, that, is, of, his, year]
3         the_to_in_and             [the, to, in, and, of, he, team, that, was, game]
4           ditto_was__  [ditto, was, <na>, <na>, <na>, <na>, <na>, <na>, <na>, <na>]
5  pool_andy_table_tell  [pool, andy, table, tell, us, well, your, about, <na>, <na>]
6       the_to_game_and           [the, to, game, and, games, espn, on, in, is, have]

second dataframe
using pandas transpose:
other_df = df.t.reset_index(drop=true)
new_col_labels = other_df.iloc[0]  # save first row
other_df = other_df[1:]  # remove first row
other_df.columns = new_col_labels
other_df = pd.dataframe({col: other_df.loc[1, col] for col in other_df.columns})

print(other_df)
# output
  the_to_of_and the_he_to_in the_to_in_and ditto_was__ pool_andy_table_tell the_to_game_and
0           the          the           the       ditto                 pool             the
1            to           he            to         was                 andy              to
2            of           to            in        <na>                table            game
3           and           in           and        <na>                 tell             and
4            is          and            of        <na>                   us           games
5            in         that            he        <na>                 well            espn
6          that           is          team        <na>                 your              on
7            it           of          that        <na>                about              in
8           for          his           was        <na>                 <na>              is
9           you         year          game        <na>                 <na>            have",https://stackoverflow.com/questions/74785255,python-3.x,13-12-2022 12:58,713.0,5.0,1.0,True,20-12-2022 17:42,16-12-2022 10:08
74729716,how to retrieve an openai image and save it to an s3 bucket,"i want to get an image generated in openai/dall e and save it to an s3 bucket.
so far i can get the image url and create a buffer, with the following:
const configuration = new configuration({
  apikey: procenvvars.openai_api_key,
});
export const openai = new openaiapi(configuration);

const defaultimageparams: createimagerequest = {
  n: 1,
  prompt: ""a bad request message"",
};

interface inputparams extends createimagerequest {
  prompt: string; // make this mandatory for the function params
}

// once we get a url from the openai api, we want to convert it to a buffer
export async function getbufferfromurl(openaiurl: string) {
  const axiosresponse = await axios({
    url: openaiurl, //your url
    method: ""get"",
    responsetype: ""arraybuffer"",
  });
  const data = axiosresponse.data;
  if (!(data instanceof buffer))
    throw new error(""axios response should be of type buffer"");

  return data;
}
export async function geturlfromopenai(inputparams: inputparams) {
  const imageresponse = await openai.createimage({
    ...defaultimageparams,
    ...inputparams,
  });

  const dataarray = imageresponse.data.data;
  if (!dataarray || dataarray.length === 0) {
    console.error({
      error: ""we did not return choices from createopenaiimage()"",
      data: imageresponse.data,
      datadata: imageresponse.data.data,
    });
  }
  return dataarray;
}","['javascript', 'typescript', 'amazon-s3', 'openai-api']",74729717,"next we need to take the buffer and save to s3:
// create service client module using es6 syntax.
import { s3client } from ""@aws-sdk/client-s3"";
// set the aws region.
const region = ""eu-west-2"";
// create an amazon s3 service client object.
const s3client = new s3client({ region: region });
export { s3client };

// import required aws sdk clients and commands for node.js.
import { putobjectcommand } from ""@aws-sdk/client-s3"";

// set the parameters.
export const bucketparams = {
  bucket: ""<my s3 bucket name. can be found in s3 console>"",
};

// create and upload an object to the s3 bucket.
export async function puts3object(inputparams: { body: buffer; key: string }) {
  try {
    const data = await s3client.send(
      new putobjectcommand({
        ...bucketparams,
        body: inputparams.body,
        key: `public/myfolder/${inputparams.key}`,
      })
    );
    console.log(
      ""successfully uploaded object: "" +
        bucketparams.bucket +
        ""/"" +
        `public/myfolder/${inputparams.key}`
    );
    return data; // for unit tests.
  } catch (err) {
    console.log(""error"", err);
  }
}",https://stackoverflow.com/questions/74729716,javascript,08-12-2022 11:37,1078.0,1.0,1.0,True,02-03-2023 03:59,02-03-2023 03:59
77540677,clearing context window of llm in huggingface,"i want to use inference to ask different questions to llms taken from huggingface. but, i want to ask the prompts without the model having info about the previous prompts. does the model automatically store the previous prompts in context?
or does it not save any previous information at all and we need to provide all the context in the same prompt?","['nlp', 'huggingface-transformers', 'transformer-model', 'huggingface', 'large-language-model']",77541156,"llms generally don't store your prompts or context or directly learn from it. after training llms stay static and the models weights don't change during inference. if you want to build a chatbot you have to actively build something to keep the context. one solution for handling context is langchain (
but for your case, you just need to prompt to the llm or the api to the llm directly.",https://stackoverflow.com/questions/77540677,nlp,24-11-2023 04:11,1294.0,-1.0,1.0,True,24-11-2023 06:40,24-11-2023 04:11
60832547,where is perplexity calculated in the huggingface gpt2 language model code?,"i see some github comments saying the output of the model() call's loss is in the form of perplexity:

but when i look at the relevant code...

    if labels is not none:
        # shift so that tokens < n predict n
        shift_logits = lm_logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        # flatten the tokens
        loss_fct = crossentropyloss()
        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
        outputs = (loss,) + outputs

    return outputs  # (loss), lm_logits, (all hidden states), (all attentions)

i see cross entropy being calculated, but no transformation into perplexity. where does the loss finally get transformed? or is there a transformation already there that i'm not understanding?","['machine-learning', 'huggingface-transformers', 'google-publisher-tag', 'perplexity']",60834242,"ah ok, i found the answer. the code is actually returning cross entropy. in the github comment where they say it is perplexity...they are saying that because the op does
return math.exp(loss)

which transforms entropy to perplexity :)",https://stackoverflow.com/questions/60832547,machine-learning,24-03-2020 13:58,7046.0,6.0,2.0,True,02-03-2022 13:33,01-07-2020 06:39
71679626,what is so special about special tokens?,"what exactly is the difference between ""token"" and a ""special token""?
i understand the following:

what is a typical token
what is a typical special token: mask, unk, sep, etc
when do you add a token (when you want to expand your vocab)

what i don't understand is, under what kind of capacity will you want to create a new special token, any examples what we need it for and when we want to create a special token other than those default special tokens? if an example uses a special token, why can't a normal token achieve the same objective?
tokenizer.add_tokens(['[eot]'], special_tokens=true)

and i also dont quite understand the following description in the source documentation.
what difference does it do to our model if we set add_special_tokens to false?
add_special_tokens (bool, optional, defaults to true) ï¿½ï¿½ï¿½ whether or not to encode the sequences with the special tokens relative to model.","['nlp', 'tokenize', 'huggingface-transformers', 'bert-language-model', 'huggingface-tokenizers']",71721757,"special tokens are called special because they are not derived from your input. they are added for a certain purpose and are independent of the specific input.

what i don't understand is, under what kind of capacity will you want
to create a new special token, any examples what we need it for and
when we want to create a special token other than those default
special tokens?

just an example, in extractive conversational question-answering it is not unusual to add the question and answer of the previous dialog-turn to your input to provide some context for your model. those previous dialog turns are separated with special tokens from the current question. sometimes people use the separator token of the model or introduce new special tokens. the following is an example with a new special token [q]
#first dialog turn - no conversation history
[cls] current question [sep] text [eos]
#second dialog turn - with previous question to have some context
[cls] previous question [q] current question [sep] text [eos]


and i also dont quite understand the following description in the
source documentation. what difference does it do to our model if we
set add_special_tokens to false?

from transformers import robertatokenizer
t = robertatokenizer.from_pretrained(""roberta-base"")

t(""this is an example"")
#{'input_ids': [0, 9226, 16, 41, 1246, 2], 'attention_mask': [1, 1, 1, 1, 1, 1]}

t(""this is an example"", add_special_tokens=false)
#{'input_ids': [9226, 16, 41, 1246], 'attention_mask': [1, 1, 1, 1]}

as you can see here, the input misses two tokens (the special tokens). those special tokens have a meaning for your model since it was trained with it. the last_hidden_state will be different due to the lack of those two tokens and will therefore lead to a different result for your downstream task.
some tasks, like sequence classification, often use the [cls] token to make their predictions. when you remove them, a model that was pre-trained with a [cls] token will struggle.",https://stackoverflow.com/questions/71679626,nlp,30-03-2022 14:58,8566.0,13.0,1.0,True,27-08-2022 18:23,02-04-2022 22:58
70360303,can i use only one query image per class in few-shot learning?,"i am studying few shot learning. in general, i know that 7 to 15 query images are allocated for each class.
however, the dataset i use for training has a minimum of 2 and a maximum of 200 pieces per class.
so i have only used one query image.
i have two questions.

is it okay to use only one query image like this?

and is it meaningful to see good performance when using fewer queries?","['machine-learning', 'few-shot-learning']",74525322,"yes it's ok if you use only one query image as your query set.
it makes sense if you get better performance using more query instances. in my experience, for a given dataset the accuracy won't improve after a certain threshold if you increase the query set instances.",https://stackoverflow.com/questions/70360303,machine-learning,15-12-2021 08:12,93.0,0.0,1.0,True,21-11-2022 21:58,21-11-2022 21:58
76048707,spacy dependency matcher doesnt find matches in reverse,"i am trying to find words related to 'poss' to the word 'my' but it doesn't work. for example in reverse,

pattern = [
    {
        ""right_id"": ""anchor_founded"",
        ""right_attrs"": {""orth"": ""ceo""}
    },
    {
        ""left_id"": ""anchor_founded"",
        ""rel_op"": "">"",
        ""right_id"": ""founded_subject"",
        ""right_attrs"": {""dep"": ""poss""},
    }
]

matcher.add(""founded"", [pattern])
doc = nlp(""my experienced ceo, has founded two ai startups."")
matches = matcher(doc)


gives
anchor_founded: ceo
founded_subject: my
but if i anchor this in ""my""
pattern = [
    {
        ""right_id"": ""anchor_founded"",
        ""right_attrs"": {""orth"": ""my""}
    },
    {
        ""left_id"": ""anchor_founded"",
        ""rel_op"": ""<"",
        ""right_id"": ""founded_subject"",
        ""right_attrs"": {""dep"": ""poss""},
    }
]

doesn't give any matches. any thoughts? ideally i want to get the second thing to work.","['python-3.x', 'nlp', 'spacy-3']",76052491,"{""dep"": ""poss""} is a token pattern, which doesn't match the attributes for ""founded"", only for ""my"".
inspect token.dep_ for the whole sentence and while it may vary a bit by model version, it will be something like this:
doc = nlp(""my experienced ceo, has founded two ai startups."")
print([t.dep_ for t in doc])
# ['poss', 'amod', 'nsubj', 'punct', 'aux', 'root', 'nummod', 'compound', 'dobj', 'punct']",https://stackoverflow.com/questions/76048707,python-3.x,18-04-2023 20:14,36.0,0.0,1.0,True,19-04-2023 08:47,18-04-2023 20:15
8772692,semantic search with nlp and elasticsearch,"i am experimenting with elasticsearch as a search server and my task is to build a ""semantic"" search functionality. from a short text phrase like ""i have a burst pipe"" the system should infer that the user is searching for a plumber and return all plumbers indexed in elasticsearch. 
can that be done directly in a search server like elasticsearch or do i have to use a natural language processing (nlp) tool like e.g. maui indexer. what is the exact terminology for my task at hand, text classification? though the given text is very short as it is a search phrase.","['search', 'nlp']",8774917,"there may be several approaches with different implementation complexity. 
the easiest one is to create list of topics (like plumbing), attach bag of words (like ""pipe""), identify search request by majority of keywords and search only in specified topic (you can add field topic to your elastic search documents and set it as mandatory with + during search). 
of course, if you have lots of documents, manual creation of topic list and bag of words is very time expensive. you can use machine learning to automate some of tasks. basically, it is enough to have distance measure between words and/or documents to automatically discover topics (e.g. by data clustering) and classify query to one of these topics. mix of these techniques may also be a good choice (for example, you can manually create topics and assign initial documents to them, but use classification for query assignment). take a look at wikipedia's article on latent semantic analysis to better understand the idea. also pay attention to the 2 linked articles on data clustering and document classification. and yes, maui indexer may become good helper tool this way. 
finally, you can try to build an engine that ""understands"" meaning of the phrase (not just uses terms frequency) and searches appropriate topics. most probably, this will involve natural language processing and ontology-based knowledgebases. but in fact, this field is still in active research and without previous experience it will be very hard for you to implement something like this.",https://stackoverflow.com/questions/8772692,search,07-01-2012 20:08,37670.0,68.0,8.0,True,08-04-2024 12:57,07-01-2012 23:37
76201533,how does the token for openai works and how can i use less tokens?,"hello beautiful people!
i'm currently trying to write my own ""ai"" with the help of openai. i have followed langchain and managed to end up having this code:
import os
import re

import discord
import requests
from discord.ext import commands
from langchain.chains import conversationalretrievalchain
from langchain.embeddings import openaiembeddings
from langchain.llms import openai
from langchain.text_splitter import recursivecharactertextsplitter
from langchain.vectorstores import faiss
from transformers import gpt2tokenizerfast

intents = discord.intents.default()
intents.typing = false
intents.presences = false
intents.message_content = true

bot = commands.bot(command_prefix=""!"", intents=intents)

# set up openai api key and models
os.environ[""openai_api_key""] = 'xxxxxx'


def get_documentation():
    zendesk_url = ""

    documentation = []

    while zendesk_url:
        # make a get request to the zendesk api to fetch articles for the current page
        response = requests.get(
            zendesk_url,
            headers={
                ""authorization"": f""basic xxxx"",
                ""content-type"": ""application/json""
            })

        # check if the request was successful
        if response.status_code == 200:
            response_json = response.json()
            # loop through the articles on the current page
            for article in response_json[""articles""]:
                # extract the title and body of the article
                title = article['title']
                body = article['body']

                # remove any html tags and formatting from the body
                body = re.sub('<[^<]+?>', '', body)

                # remove all newline characters from the body
                body = body.replace('\n', ' ')

                # replace non-breaking spaces with regular spaces
                body = body.replace('\xa0', ' ')

                # append the title and body to the documentation list
                documentation.append((title, body))

            # check if there are more pages of articles and update the zendesk_url variable if necessary
            next_page_url = response_json[""next_page""]
            zendesk_url = next_page_url if next_page_url else none
        else:
            # if the request was not successful, raise an exception with the error message
            response.raise_for_status()

    return documentation


# load the gpt2 tokenizer
tokenizer = gpt2tokenizerfast.from_pretrained(""gpt2"")
print(tokenizer)


# define a function to count tokens
def count_tokens(text: str) -> int:
    return len(tokenizer.encode(text))


# create a text splitter
text_splitter = recursivecharactertextsplitter(
    chunk_size=512,
    chunk_overlap=24,
    length_function=count_tokens,
)

# fetch and clean the documentation
documentation = get_documentation() # the len of documentation is 93

# extract only the article bodies
article_bodies = [article_body for title, article_body in documentation]

# split the article bodies into chunks
chunks = text_splitter.create_documents(article_bodies)

# get embedding model
embeddings = openaiembeddings()

# create vector database
db = faiss.from_documents(chunks, embeddings)

qa = conversationalretrievalchain.from_llm(openai(temperature=0.1), db.as_retriever())


@bot.event
async def on_ready():
    print(f'we have logged in as {bot.user}')


chat_history = []
@bot.command()
async def ask(ctx, *, question):
    print(f""{ctx.author.name} asked: {question}"")
    result = qa(
        {
            ""question"": question,
            ""chat_history"": chat_history
        }
    )
    chat_history.append((question, result['answer']))
    await ctx.send(result['answer'])


bot.run('xxxxxx')

what i do is that i connect to my zendesk, scrape all the documentation by calling get_documentation() and then use it for chunks. when i then call !ask question here then i should get an answer back. however by checking my latest usage. it ends up using lots of tokens and i feel like it might be too much and could need some explanation or if there is anything i could even improve?

i know that when i start the script, it usually ends up by having around 46,179 prompt but i don't really understand why i pay without even started to ask a question. how can i improve it to use less tokens?
expected:
to use less tokens/use tokens when i ask a prompt
actual:
uses 40k+ tokens everytime i start.","['python-3.x', 'discord.py', 'openai-api', 'langchain']",76814374,"from here:

tokenization is the process of splitting the input and output texts
into smaller units that can be processed by the llm ai models. tokens
can be words, characters, subwords, or symbols, depending on the type
and the size of the model. tokenization can help the model to handle
different languages, vocabularies, and formats, and to reduce the
computational and memory costs. tokenization can also affect the
quality and the diversity of the generated texts, by influencing the
meaning and the context of the tokens. tokenization can be done using
different methods, such as rule-based, statistical, or neural,
depending on the complexity and the variability of the texts.

usage of tokens basically depends on input and output length, and model configuration. even a single punctuation can be classified as a token by the model. you can test the token usage at enter link description here

in the above example, "","" and ""."" counted as a token. in order to reduce token usage

keep prompts consice and precise. avoid using repetition, unnecessary punctuation and whitespaces, and special characters.

limit output length. in langchain you pass max_tokens named parameter.  the longer outputs require more tokens to generate. when you set a limit on the output length using the max_tokens parameter, the model will stop generating text once it reaches that token limit.

as llm updated to new version, that means it learnt more so the more llm knows less token it uses. for example, gpt-3.5-turbo is a more token-efficient version of gpt-3.",https://stackoverflow.com/questions/76201533,python-3.x,08-05-2023 14:25,1794.0,0.0,1.0,True,01-08-2023 18:38,01-08-2023 18:38
77925185,valueerror: expected embeddingfunction.__call__ to have the following signature,"when i try to pass a chroma client to langchain that uses openaiembeddings, i get a valueerror:
valueerror: expected embeddingfunction.__call__ to have the following signature: odict_keys(['self', 'input']), got odict_keys(['self', 'args', 'kwargs'])

how do i resolve this error?
the error seems to be related to the fact that langchain's embedding function implementation doesn't meet the new requirements introduced by chroma's latest update because the issue showed up after upgrading chroma.
my code:
import chromadb
from langchain_openai import openaiembeddings
client = chromadb.persistentclient()
collection = client.get_or_create_collection(
    name='chroma', 
    embedding_function=openaiembeddings()
)

i have langchain==0.1.1, langchain-openai==0.0.3 and chromadb==0.4.22. looking into github issues, it seems downgrading chromadb to 0.4.15 solves the issue but since these libraries will upgrade even more in the coming months, i don't want to downgrade chroma but find a solution that works in the current version.","['python', 'valueerror', 'langchain', 'chromadb', 'openaiembeddings']",77925278,"since version 0.4.16(?), chroma requires an embedding model that defines a __call__() method that returns list of embeddings. it says as much in the migrations link shown in the error.
given that we need a method that returns a list of embeddings and it's already defined in openaiembeddings (embed_documents()), the easiest solution i found was to create a custom class that inherits from openaiembeddings wherein a __call__ method that triggers a call to openaiembeddings.embed_documents is defined.
a small note: unless you stored your openai api key in your .env file, you'll probably need to pass it as openai_api_key parameter.
import chromadb
from langchain_openai import openaiembeddings

class customopenaiembeddings(openaiembeddings):

    def __init__(self, openai_api_key, *args, **kwargs):
        super().__init__(openai_api_key=openai_api_key, *args, **kwargs)
        
    def _embed_documents(self, texts):
        return super().embed_documents(texts)  # <--- use openaiembedding's embedding function

    def __call__(self, input):
        return self._embed_documents(input)    # <--- get the embeddings


client = chromadb.persistentclient()
collection = client.get_or_create_collection(
    name='chroma', 
    embedding_function=customopenaiembeddings(
        openai_api_key=""your very secret openai api key""
    )         # <-- pass the new object instead of openaiembeddings()
)


using openai's embedding object also works too (which can be accessed via self.client). basically we can define customopenaiembeddings like below by invoking the embedding.create() method in a loop like in this example use case.
class customopenaiembeddings(openaiembeddings):

    def __init__(self, openai_api_key, *args, **kwargs):
        super().__init__(openai_api_key=openai_api_key, *args, **kwargs)

    def _embed_documents(self, texts):
        embeddings = [
            self.client.create(input=text, model=""text-embedding-ada-002"").data[0].embedding 
            for text in texts
        ]
        return embeddings
        
    def __call__(self, input):
        return self._embed_documents(input)",https://stackoverflow.com/questions/77925185,python,02-02-2024 06:59,7399.0,1.0,2.0,True,01-07-2024 16:05,04-02-2024 04:07
2816382,latin bases language segmentation gramatical rules,"i am working on one feature i.e. to apply language segmentation rules (grammatical) for latin based language (english currently).
currently i am in phase of breaking sentences of user input.
e.g.:

""i am working in language translation"". ""i have used google mt api for this""

in above example i will break above sentence by full stop . this is normal cases where i am breaking sentence on dot, but there are n number of characters for breaking sentence like (. ! ?, etc).
i have following srx rules for segmentation.
is there any reference which i can use for resolving my language segmentation rules?","language-agnostic, nlp",2817450,"you probably want to take a look at reynar and ratnaparkhi's paper a maximum entropy approach to identifying sentence boundaries (1997).
abstract

we present a trainable model for identifying
sentence boundaries in raw text. given
a corpus annotated with sentence boundaries,
our model learns to classify each occurrence
of., ?, and / as either a valid or invalid
sentence boundary. the training procedure
requires no hand-crafted rules, lexica,
part-of-speech tags, or domain-specific
information. the model can therefore be
trained easily on any genre of english, and
should be trainable on any other romanalphabet
language. performance is comparable
to or better than the performance of
similar systems, but we emphasize the simplicity
of retraining for new domains.

their resulting sentence segmenter is known as mxterminator and is available here.",https://stackoverflow.com/q/2816382,"language-agnostic, nlp",12-05-2010 06:10,286.0,3.0,2.0,True,02-08-2021 00:19,02-08-2021 00:19
78423352,spacy gpu memory utilization for ner training,"my training code:
spacy.require_gpu()
nlp = spacy.blank('en')

if 'ner' not in nlp.pipe_names:
    ner = nlp.add_pipe('ner')
else:
    ner = nlp.get_pipe('ner')

docs = load_data(annotated_data_filename_bin)
train_data, test_data = split_data(docs, data_split)

unique_labels = set(ent.label_ for doc in train_data for ent in doc.ents)
for label in unique_labels:
    ner.add_label(label)

optimizer = nlp.initialize()

for i in range(epochs):
    print(f""starting epoch {i+1}..."")
    losses = {}
    batches = minibatch(train_data, size=compounding(4., 4096, 1.001))
    for batch in batches:
        for doc in batch:
            example = example.from_dict(doc, {'entities': [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]})
            nlp.update([example], drop=0.5, losses=losses, sgd=optimizer)
    print(f""losses at iteration {i}: {losses}"")

this code almost completely does not utilize gpu memory. utilization is about 11-13% during training, which is almost the same as idle.

i did allocation test with torch, and all 8gigs are allocated, so server works fine.
the problem is with spacy or my code.
could you please help?","['python', 'gpu', 'spacy', 'named-entity-recognition', 'custom-training']",78506143,"quoting william james mattingly, ph.d:

this may due to spacy's change in training for 3.0. training is done
for projects and via the command line. this is how we used to train
models for 2.0 and while it works, i believe there are certain issues
that arise. this may be one of those issues. the newer approach passes
an argument in the cli when you train the model.

in the docs you can specify in the config how to train and on which
device. training spacy's statistical models - spacy spacy.io spacy is
a free open-source library featuring state-of-the-art speed and
accuracy and a powerful python api.
[system] gpu_allocator = ""pytorch""
this is the important bit ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½
then when you run train in the cli, you'd do something like this:
python -m spacy train config.cfg --gpu-id 0</",https://stackoverflow.com/questions/78423352,python,03-05-2024 07:52,85.0,2.0,1.0,True,10-03-2025 18:09,03-05-2024 13:33
76429315,is there a way to keep between-word hyphens when lemmatizing using spacyr?,"i'm using spacyr to lemmatise a corpus of speeches, and then using quanteda to tokenise and analyze results (via textstat_frequency()). my issue is that some key terms in the texts are hyphenated. when i tokenise using quanteda, i do not lose these between-word hyphens, and the hyphenated terms are treated as one token, which is my desired result. however, when i use spacyr to lemmatise first, hyphenated words are not kept together. i've tried nounphrase_consolidate(), which does keep hyphenated words, but i find the results to be very inconsistent, as sometimes a term of interest is kept on its own during this consolidation, and in other instances is combined as part of a larger nounphrase. this is suboptimal because i'm looking at a particular dictionary of features in my final step with textstat_frequenecy, some of which are hyphenated terms.
it seems like this is this solution in spacy, but was curious if there's a similar option in spacyr: spacy -- intra-word hyphens. how to treat them one word?
thanks for any thoughts or suggestions. code below. it doesn't make a difference whether i use remove_punct or not when tokenising.
    test.sp <- spacy_parse(test.corpus, lemma = true, entity = false, pos = false, tag = false, nounphrase = true)
test.sp$token <- test.sp$lemma
test.np <- nounphrase_consolidate(test.sp)
test.tokens.3 <- as.tokens(test.np)
test.tokens.3 <- tokens(test.tokens.3, remove_symbols = true,
                      remove_numbers = true,
                      remove_punct = true,
                      remove_url = true) %>% 
  tokens_tolower() %>% 
  tokens_select(pattern = stopwords(""en""), selection = ""remove"")","['nlp', 'spacy', 'quanteda']",76439187,"you should be able to rejoin the hyphenated words in quanteda, using tokens_compound().
library(""quanteda"")
#> package version: 3.3.1
#> unicode version: 14.0
#> icu version: 71.1
#> parallel computing: 10 of 10 threads used.
#> see  for tutorials and examples.
library(""spacyr"")

test.corpus <- c(d1 = ""nlp is fast-moving."",
                 d2 = ""a co-ordinated effort."")
test.sp <- spacy_parse(test.corpus, lemma = true, entity = false, pos = false, tag = false, nounphrase = true)
#> found 'spacy_condaenv'. spacyr will use this environment
#> successfully initialized (spacy version: 3.4.4, language model: en_core_web_sm)
#> (python options: type = ""condaenv"", value = ""spacy_condaenv"")
test.sp$token <- test.sp$lemma
test.np <- nounphrase_consolidate(test.sp)
test.tokens.3 <- as.tokens(test.np)

tokens_compound(test.tokens.3, pattern = phrase(""* - *""), concatenator = """")
#> tokens consisting of 2 documents.
#> d1 :
#> [1] ""nlp""         ""be""          ""fast-moving"" "".""          
#> 
#> d2 :
#> [1] ""a_co-ordinated_effort"" "".""

created on 2023-06-09 with reprex v2.0.2",https://stackoverflow.com/questions/76429315,nlp,08-06-2023 06:57,181.0,1.0,1.0,True,09-06-2023 09:59,08-06-2023 07:06
76473726,how can i limit sourcedocs in langchain + pinecone response in js?,"i'm using langchain with pinecode, it gives me 4 sourcedocs but i want only most relevant 1 sourcedoc.
i'm using javascript and don't know where to put top_k in code.
here's my code
export const makechain = (vectorstore) => {
  const model = new openai({
    temperature: 0
    modelname: 'gpt-3.5-turbo', 
  });

  const chain = conversationalretrievalqachain.fromllm(
    model,
    vectorstore.asretriever(),
    {
      qatemplate: qa_prompt,
      questiongeneratortemplate: condense_prompt,
      returnsourcedocuments: true,
    }
  );
  return chain;
};

/* create vectorstore*/
    const vectorstore = await pineconestore.fromexistingindex(
      new openaiembeddings({}),
      {
        pineconeindex: index,
        textkey: 'text',
        namespace: pinecone_name_space, //namespace comes from your config folder
      }
    );

    //create chain
    const chain = makechain(vectorstore);
    //ask a question using chat history
    const response = await chain.call({
      question: sanitizedquestion,
      chat_history: history || [],
    });
    ```","['javascript', 'openai-api', 'langchain']",76481469,"the first parameter of the vectorstore.asretriever() method is top_k, like vectorstore.asretriever(1).",https://stackoverflow.com/questions/76473726,javascript,14-06-2023 12:43,454.0,1.0,1.0,True,15-06-2023 10:14,14-06-2023 13:09
77518166,is there a way of programatically extracting terms (length of contract) from free text,"i want to extract contract length from text to term in months. free text fields range from:
""2 x 5 year terms"",
""3 further  x 4 years"",
""two(2) further terms of five(5) years each"",
""two (2) years + two (2) years + two (2) years"",
""1 years + 1 years + 1 years"" ,
""2 x 3 years"",
""1 year and 6 months"",
""

i'd like the output to be:
120 months,
144 months,
120 months,
72 months, 
36 months
72 months
18 months

import re

def calculate_duration(term):
    term = term.lower()

    # handle ""x year terms"" pattern
    match = re.match(r'(\d+) x (\d+) year terms?', term)
    if match:
        return int(match.group(1)) * int(match.group(2)) * 12
    
    # handle ""further terms of x years each"" pattern
    match = re.match(r'further terms of (\d+) years each', term)
    if match:
        return int(match.group(1)) * 12


    # handle ""further terms of x years each"" pattern
    match = re.match(r'further terms of (\d+) years each', term)
    if match:
        return int(match.group(1)) * 12
    
    # handle ""further terms of x years each"" pattern
    match = re.match(r'further terms of ((?:\d+\s?\(\w+\)\s?)?(\d+)) years each', term)
    if match:
        return int(match.group(2)) * 12

    # handle ""x years + x years + x years"" pattern
    match = re.match(r'(\d+) years(\s?\+\s?\d+ years)+', term)
    if match:
        return sum(int(match.group(1)) for group in match.groups()) * 12

    # handle other patterns or simple year counts
    match = re.match(r'(\d+) years?', term)
    if match:
        return int(match.group(1)) * 12

    # handle other cases or unknown patterns
    return none

# example usage
terms = [
    ""2 x 5 year terms"",
    ""3 further x 4 year terms"",
    ""two (2) years + two (2) years + two (2) years"",
    ""1 years + 1 years + 1 years"" ,
    ""2 x 3 years""
]

for term in terms:
    duration = calculate_duration(term)
    print(f""{term}: {duration} months"")","['python', 'regex', 'nltk', 'spacy', 'text-extraction']",77518711,"""... i want to extract contract length from text to term in months. ...""

utilize the eval built-in function.
traverse the the text, appending the according values; numbers and operators.
when a ""year"" value is encountering, adjust the previous value accordingly; multiply by 12.
from here, produce a mathematical expression, by concatenating the values.
here is an example.
import re

def parse(s: str):
    e = []
    for i, x in enumerate(s.split()):
        if any([c.isdigit() for c in x]):
            e.append(int(re.sub(r'\d', '', x)))
        elif 'year' in x.lower(): e[-1] *= 12
        elif x in ['x', 'of']: e.append('*')
        elif x in ['+', 'and']: e.append('+')
    return e

text = ['2 x 5 year terms',
        '3 further x 4 years',
        'two(2) further terms of five(5) years each',
        'two (2) years + two (2) years + two (2) years',
        '1 years + 1 years + 1 years',
        '2 x 3 years',
        '1 year and 6 months']
for string in text:
    exp = ' '.join(map(str, parse(string)))
    print(exp, '=', eval(exp))

output
2 * 60 = 120
3 * 48 = 144
2 * 60 = 120
24 + 24 + 24 = 72
12 + 12 + 12 = 36
2 * 36 = 72
12 + 6 = 18


edit
a similar approach would be to remove any non-associative values, i.e., keeping only ""years"", ""of"", ""and"", digits, and the characters, 'x', and '+'.
import re

a = re.compile(r'(?i)\b(?!(?:years?)|of|and|\d|x|\+)\w+|[()]')
b = re.compile(r'(?<!\s)[ \t]+| +$')

def parse(s: str):
    global a, b
    s = a.sub('', s).lower()
    s = b.sub('', s).replace('s', '')
    e = []
    for i, x in enumerate(s.split()):
        if x.isdigit(): e.append(int(x))
        elif x == 'year': e[-1] *= 12
        elif x in ['x', 'of']: e.append('*')
        elif x in ['+', 'and']: e.append('+')
    return ' '.join(map(str, e))

text = ['2 x 5 year terms',
        '3 further x 4 years',
        'two(2) further terms of five(5) years each',
        'two (2) years + two (2) years + two (2) years',
        '1 years + 1 years + 1 years',
        '2 x 3 years',
        '1 year and 6 months']
for string in text:
    exp = parse(string)
    print(exp, '=', eval(exp))",https://stackoverflow.com/questions/77518166,python,20-11-2023 18:30,108.0,-2.0,1.0,True,21-11-2023 09:24,20-11-2023 20:45
78176160,google colab bert instantiation error using tensorflow,"i'm trying to construct a bert model using tensorflow on colab. this code was perfectly working weeks ago. now if i try to instantiate the model i obtain the following error:
some weights of the pytorch model were not used when initializing the tf 2.0 model tfbertmodel: ['cls.predictions.transform.layernorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.layernorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- this is expected if you are initializing tfbertmodel from a pytorch model trained on another task or with another architecture (e.g. initializing a tfbertforsequenceclassification model from a bertforpretraining model).
- this is not expected if you are initializing tfbertmodel from a pytorch model that you expect to be exactly identical (e.g. initializing a tfbertforsequenceclassification model from a bertforsequenceclassification model).
all the weights of tfbertmodel were initialized from the pytorch model.
if your task is similar to the task the model of the checkpoint was trained on, you can already use tfbertmodel for predictions without further training.
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
<ipython-input-14-b0e769ef7890> in <cell line: 7>()
      5 sc_mask_layer = input(shape=(max_seq_length,), dtype=tf.int32, name=""attention_mask"")
      6 sc_bert_model = tfbertmodel.from_pretrained(""bert-base-uncased"")
----> 7 sc_pooler_output = sc_bert_model(sc_input_layer, attention_mask=sc_mask_layer)[1]  # estrai il secondo output, che ï¿½ï¿½ il pooler_output
      8 
      9 # aggiungi un layer di dropout

36 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/type_spec.py in type_spec_from_value(value)
   1002         3, ""failed to convert %r to tensor: %s"" % (type(value).__name__, e))
   1003 
-&1004   raise typeerror(f""could not build a typespec for {value} of ""
   1005                   f""unsupported type {type(value)}."")
   1006 

typeerror: exception encountered when calling layer 'embeddings' (type tfbertembeddings).

could not build a typespec for name: ""tf.debugging.assert_less_5/assert_less/assert/assert""
op: ""assert""
input: ""tf.debugging.assert_less_5/assert_less/all""
input: ""tf.debugging.assert_less_5/assert_less/assert/assert/data_0""
input: ""tf.debugging.assert_less_5/assert_less/assert/assert/data_1""
input: ""tf.debugging.assert_less_5/assert_less/assert/assert/data_2""
input: ""placeholder""
input: ""tf.debugging.assert_less_5/assert_less/assert/assert/data_4""
input: ""tf.debugging.assert_less_5/assert_less/y""
attr {
  key: ""summarize""
  value {
    i: 3
  }
}
attr {
  key: ""t""
  value {
    list {
      type: dt_string
      type: dt_string
      type: dt_string
      type: dt_int32
      type: dt_string
      type: dt_int32
    }
  }
}
 of unsupported type <class 'tensorflow.python.framework.ops.operation'>.

call arguments received by layer 'embeddings' (type tfbertembeddings):
  ï¿½ï¿½ï¿½ input_ids=<kerastensor: shape=(none, 128) dtype=int32 (created by layer 'input_ids')>
  ï¿½ï¿½ï¿½ position_ids=none
  ï¿½ï¿½ï¿½ token_type_ids=<kerastensor: shape=(none, 128) dtype=int32 (created by layer 'tf.fill_5')>
  ï¿½ï¿½ï¿½ inputs_embeds=none
  ï¿½ï¿½ï¿½ past_key_values_length=0
  ï¿½ï¿½ï¿½ training=false

the code for the model is:
sc_input_layer = input(shape=(max_seq_length,), dtype=tf.int32, name=""input_ids"")
sc_mask_layer = input(shape=(max_seq_length,), dtype=tf.int32, name=""attention_mask"")
sc_bert_model = tfbertmodel.from_pretrained(""bert-base-uncased"")
sc_pooler_output = sc_bert_model(sc_input_layer, attention_mask=sc_mask_layer)[1]  

# aggiungi un layer di dropout
sc_dropout_layer = dro
sc_output_layer = dense(6, activation='sigmoid')(sc_dropout_layer)
sc_model = model(inputs=[sc_input_layer, sc_mask_layer], outputs=sc_output_layer)

i found that installing tensorflow 2.10.0 it works but, using google colab i have problems with the cuda version and using tensorflow 2.10 it doesn't recognize the gpu.
this code was working weeks ago, someone has a solution?
edit: the same error appears on kaggle.","['tensorflow', 'machine-learning', 'google-colaboratory', 'bert-language-model']",78202113,update: the problem is related to the version of transformers. using version 4.31.0 should solve.,https://stackoverflow.com/questions/78176160,tensorflow,17-03-2024 17:03,158.0,0.0,1.0,True,24-03-2024 13:41,21-03-2024 17:31
73722269,is there a way to split the text i&#39;m inputting into different strings that the dictionary looks at individually,"i am mostly trying to create software that reads says the definition of every word you typed into the text box. right now it only reads if there is one work and crashes if there is more than one. how would i go about fixing this?
import wolframalpha
client = wolframalpha.client('8qr2wg-628657k83q')

from multiprocessing import process

import wikipedia

import pysimplegui as sg

import cv2

import random

import sys
import threading
import time

import nltk
nltk.download('punkt')

# from oxforddictionaries.words import oxforddictionaries
# oxford = oxforddictionaries('b4170561','f32687e0ecbc219cfd723bb220dad34e')
# o = oxforddictionaries('b4170561','f32687e0ecbc219cfd723bb220dad34e')
# relax = o.get_synonyms(""apple"").json()
# synonyms = relax

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download(""stopwords"")
stop_words = set(stopwords.words(""english""))
filtered_list = []

from nltk.stem import porterstemmer
from nltk.tokenize import word_tokenize
nltk.download('averaged_perceptron_tagger')

stemmer = porterstemmer()


trained_face_data = cv2.cascadeclassifier('haarcascade_frontalface_default.xml')
trained_body_data = cv2.cascadeclassifier('haarcascade_upperbody.xml')
trained_eye_data = cv2.cascadeclassifier('haarcascade_eye.xml')
webcam = cv2.videocapture(0)
sg.theme('graygraygray')

layout = [  [sg.text(""enter test text"")],
            [sg.input()],
            [sg.button('ok')] ]
window = sg.window('you', layout)


sg.popup('about me','hello i am an ai devolped by garrett provence. i will be using your webcam to scan your suroundings for a quick few seconds and will open a text box where you will be able to ask me questions. by clicking ok below you agree to letting me acess everyhting said before. i am still in beta so please be patient.')

timeout = time.time() + 10;


while true:


##webcam scanner
    def infiniteloop1():
        while true:
            test = 0
            if test == 5 or time.time() > timeout:
                break
            test = test - 1


            successful_frame_read, frame = webcam.read()

            grayscaled_img = cv2.cvtcolor(frame, cv2.color_bgr2gray)

            face_coordinates = trained_face_data.detectmultiscale(grayscaled_img)
            body_coordinates = trained_body_data.detectmultiscale(grayscaled_img)
            eye_coordinates = trained_eye_data.detectmultiscale(grayscaled_img)


            for (x,y,w,h) in face_coordinates:
                cv2.rectangle(frame, (x, y),(x+w, y+h), (0,random.randrange(255),0), 2)
            for (x,y,w,h) in body_coordinates:
                cv2.rectangle(frame, (x, y),(x+w, y+h), (0,0,255), 2)
            for (x,y,w,h) in eye_coordinates:
                cv2.rectangle(frame, (x, y),(x+w, y+h), (random.randrange(255),0,0), 2)
            cv2.imshow('facething',frame)
            cv2.waitkey(1)

    thread1 = threading.thread(target=infiniteloop1)
    thread1.start()
    event, values = window.read()
    inputtext = values[0]


    import json
    import requests
    import os
    import pprint
    import oxfordwords
    from oxfordwords import word
    import pprint



##end of webcam scanner

#img = cv2.imread('rdj.png')
    while true:


            ##test text scanner --
            text = values[0]
            word.get(text)
            if event == sg.win_closed or event == 'cancel':
                break
                sys.exit()
            try:
                words_in_excerpt = word_tokenize(text)

                nltk.pos_tag(words_in_excerpt)
                print('hello',nltk.pos_tag(words_in_excerpt), """")
                sg.popup('test', nltk.pos_tag(words_in_excerpt))
                sg.popup('def',word.definitions())
                break
            except:
                sg.popup('there seems to be a error processing what you have said')
                break
            ##end of test text scanner --

the oxford dictonary code -
#!/bin/env python3


"""""" oxford dictionary api """"""

from http import cookiejar

import requests
from bs4 import beautifulsoup as soup


class wordnotfound(exception):
    """""" word not found in dictionary (404 status code) """"""
    pass


class blockall(cookiejar.cookiepolicy):
    """""" policy to block cookies """"""
    return_ok = set_ok = domain_return_ok = path_return_ok = lambda self, *args, **kwargs: false
    netscape = true
    rfc2965 = hide_cookie2 = false


class word(object):
    """""" retrive word info from oxford dictionary website """"""
    entry_selector = '#entrycontent > .entry'
    header_selector = '.top-container'

    title_selector = header_selector + ' .headword'
    wordform_selector = header_selector + ' .pos'
    property_global_selector = header_selector + ' .grammar'

    br_pronounce_selector = '[geo=br] .phon'
    am_pronounce_selector = '[geo=n_am] .phon'
    br_pronounce_audio_selector = '[geo=br] [data-src-ogg]'
    am_pronounce_audio_selector = '[geo=n_am] [data-src-ogg]'

    definition_body_selector = '.senses_multiple'
    namespaces_selector = '.senses_multiple > .shcut-g'
    examples_selector = '.senses_multiple .sense > .examples .x'
    definitions_selector = '.senses_multiple .sense > .def'

    extra_examples_selector = '.res-g [title=""extra examples""] .x-gs .x'
    phrasal_verbs_selector = '.phrasal_verb_links a'
    idioms_selector = '.idioms > .idm-g'

    other_results_selector = '#rightcolumn #relatedentries'

    soup_data = none

    @classmethod
    def get_url(cls, word):
        """""" get url of word definition """"""
        baseurl = '
        return baseurl + word

    @classmethod
    def delete(cls, selector):
        """""" remove tag with specified selector in cls.soup_data """"""
        try:
            for tag in cls.soup_data.select(selector):
                tag.decompose()
        except indexerror:
            pass

    @classmethod
    def get(cls, word):
        """""" get html soup of word """"""
        req = requests.session()
        req.cookies.set_policy(blockall())

        page_html = req.get(cls.get_url(word), timeout=5, headers={'user-agent': 'mother animal'})
        if page_html.status_code == 404:
            raise wordnotfound
        else:
            cls.soup_data = soup(page_html.content, 'html.parser')

        if cls.soup_data is not none:
            # remove some unnecessary tags to prevent false positive results
            cls.delete('[title=""oxford collocations dictionary""]')
            cls.delete('[title=""british/american""]')  # edge case: 'phone'
            cls.delete('[title=""express yourself""]')
            cls.delete('[title=""collocations""]')
            cls.delete('[title=""word origin""]')

    @classmethod
    def other_results(cls):
        """""" get similar words, idioms, phrases...
        return: {
                'all matches': [
                    {'word1': word1, 'id1': id1, 'wordform1': wordform1},
                    {'word2': word2, 'id2': id2, 'wordform2': wordform2}
                    ...
                    ]
                'phrasal verbs': [
                    {'word1': word1, 'id1': id1, 'wordform1': wordform1},
                    {'word2': word2, 'id2': id2, 'wordform2': wordform2}
                    ...
                    ]
                ...
                }
        """"""
        info = []

        try:
            rightcolumn_tags = cls.soup_data.select(cls.other_results_selector)[0]
        except indexerror:
            return none

        # there can be multiple other results table like all matches, phrasal verbs, idioms,...
        header_tags = rightcolumn_tags.select('dt')
        other_results_tags = rightcolumn_tags.select('dd')

        # loop each other result table
        for header_tag, other_results_tag in zip(header_tags, other_results_tags):
            header = header_tag.text
            other_results = []

            for item_tag in other_results_tag.select('li'):
                names = item_tag.select('span')[0].find_all(text=true, recursive=false)
                wordform_tag = item_tag.select('pos')
                names.append(wordform_tag[0].text if len(wordform_tag) > 0 else '')
                other_results.append(names)

            other_results = list(filter(none, other_results))  # remove empty list
            ids = [cls.extract_id(tag.attrs['href'])
                   for tag in other_results_tag.select('li a')]

            results = []
            for other_result, id in zip(other_results, ids):
                result = {}
                result['name'] = ' '.join(list(map(lambda x: x.strip(), other_result[0:-1])))
                result['id'] = id

                try:
                    result['wordform'] = other_result[-1].strip()
                except indexerror:
                    pass

                results.append(result)

            info.append({header: results})

        return info

    @classmethod
    def name(cls):
        """""" get word name """"""
        if cls.soup_data is none:
            return none
        return cls.soup_data.select(cls.title_selector)[0].text

    @classmethod
    def id(cls):
        """""" get id of a word. if a word has definitions in 2 seperate pages
        (multiple wordform) it will return 'word_1' and 'word_2' depend on
        which page it's on """"""
        if cls.soup_data is none:
            return none
        return cls.soup_data.select(cls.entry_selector)[0].attrs['id']

    @classmethod
    def wordform(cls):
        """""" return wordform of word (verb, noun, adj...) """"""
        if cls.soup_data is none:
            return none

        try:
            return cls.soup_data.select(cls.wordform_selector)[0].text
        except indexerror:
            return none

    @classmethod
    def property_global(cls):
        """""" return global property (apply to all definitions) """"""
        if cls.soup_data is none:
            return none

        try:
            return cls.soup_data.select(cls.property_global_selector)[0].text
        except indexerror:
            return none

    @classmethod
    def get_prefix_from_filename(cls, filename):
        """""" get prefix (name or bre) from audio name when prefix is null """"""
        if '_gb_' in filename:
            return 'bre'

        elif '_us_' in filename:
            return 'name'

        return none

    @classmethod
    def pronunciations(cls):
        """""" get britain and america pronunciations """"""
        if cls.soup_data is none:
            return none

        britain = {'prefix': none, 'ipa': none, 'url': none}
        america = {'prefix': none, 'ipa': none, 'url': none}

        try:
            britain_pron_tag = cls.soup_data.select(cls.br_pronounce_selector)[0]
            america_pron_tag = cls.soup_data.select(cls.am_pronounce_selector)[0]

            britain['ipa'] = britain_pron_tag.text
            britain['prefix'] = 'bre'
            america['ipa'] = america_pron_tag.text
            america['prefix'] = 'name'
        except indexerror:
            pass

        try:
            britain['url'] = cls.soup_data.select(cls.br_pronounce_audio_selector)[0].attrs['data-src-ogg']
            america['url'] = cls.soup_data.select(cls.am_pronounce_audio_selector)[0].attrs['data-src-ogg']
        except indexerror:
            pass

        if britain['prefix'] == none and britain['url'] is not none:
            britain['prefix'] = cls.get_prefix_from_filename(britain['url'])

        if america['prefix'] == none and america['url'] is not none:
            america['prefix'] = cls.get_prefix_from_filename(america['url'])

        return [britain, america]

    @classmethod
    def extract_id(cls, link):
        """""" get word id from link
        argument: 
        return: id
        """"""
        return link.split('/')[-1]

    @classmethod
    def get_references(cls, tags):
        """""" get info about references to other page
        argument: soup.select(<selector>)
        return: [{'id': <id>, 'name': <word>}, {'id': <id2>, 'name': <word2>}, ...]
        """"""
        if cls.soup_data is none:
            return none

        references = []
        for tag in tags.select('.xrefs a'):  # see also <external link>
            id = cls.extract_id(tag.attrs['href'])
            word = tag.text
            references.append({'id': id, 'name': word})

        return references

    @classmethod
    def references(cls):
        """""" get global references """"""
        if cls.soup_data is none:
            return none

        header_tag = cls.soup_data.select(cls.header_selector)[0]
        return cls.get_references(header_tag)

    @classmethod
    def definitions(cls, full=false):
        """""" return: list of definitions """"""
        if cls.soup_data is none:
            return none

        if not full:
            return [tag.text for tag in cls.soup_data.select(cls.definitions_selector)]
        return cls.definition_full()

    @classmethod
    def examples(cls):
        """""" list of all examples (not categorized in seperate definitions) """"""
        if cls.soup_data is none:
            return none
        return [tag.text for tag in cls.soup_data.select(cls.examples_selector)]

    @classmethod
    def phrasal_verbs(cls):
        """""" get phrasal verbs list (verb only) """"""
        if cls.soup_data is none:
            return none

        phrasal_verbs = []
        for tag in cls.soup_data.select(cls.phrasal_verbs_selector):
            phrasal_verb = tag.select('.xh')[0].text
            id = cls.extract_id(tag.attrs['href'])  #  -> id

            phrasal_verbs.append({'name': phrasal_verb, 'id': id})

        return phrasal_verbs

    @classmethod
    def _parse_definition(cls, parent_tag):
        """""" return word definition + corresponding examples
        a word can have a single (none) or multiple namespaces
        each namespace can have one or many definitions
        each definitions can have one, many or no examples
        some words can have specific property
        (transitive/intransitive/countable/uncountable/singular/plural...)
        a verb can have phrasal verbs
        """"""
        if cls.soup_data is none:
            return none

        definition = {}

        try:  # property (countable, transitive, plural,...)
            definition['property'] = parent_tag.select('.grammar')[0].text
        except indexerror:
            pass

        try:  # label: (old-fashioned), (informal), (saying)...
            definition['label'] = parent_tag.select('.labels')[0].text
        except indexerror:
            pass

        try:  # refer to something (of people, of thing,...)
            definition['refer'] = parent_tag.select('.dis-g')[0].text
        except indexerror:
            pass

        definition['references'] = cls.get_references(parent_tag)
        if not definition['references']:
            definition.pop('references', none)

        try:  # sometimes, it just refers to other page without having a definition
            definition['description'] = parent_tag.select('.def')[0].text
        except indexerror:
            pass

        definition['examples'] = [example_tag.text
                                  for example_tag in parent_tag.select('.examples .x')]

        definition['extra_example'] = [
            example_tag.text
            for example_tag in parent_tag.select('[unbox=extra_examples] .examples .unx')
        ]

        return definition

    @classmethod
    def definition_full(cls):
        """""" return word definition + corresponding examples
        a word can have a single (none) or multiple namespaces
        each namespace can have one or many definitions
        each definitions can have one, many or no examples
        some words can have specific property
        (transitive/intransitive/countable/uncountable/singular/plural...)
        a verb can have phrasal verbs
        """"""
        if cls.soup_data is none:
            return none

        namespace_tags = cls.soup_data.select(cls.namespaces_selector)

        info = []
        for namespace_tag in namespace_tags:
            try:
                namespace = namespace_tag.select('h2.shcut')[0].text
            except indexerror:
                # some word have similar definitions grouped in a multiple namespaces (time)
                # some do not, and only have one namespace (woman)
                namespace = none

            definitions = []
            definition_full_tags = namespace_tag.select('.sense')

            for definition_full_tag in definition_full_tags:
                definition = cls._parse_definition(definition_full_tag)
                definitions.append(definition)

            info.append({'namespace': namespace, 'definitions': definitions})

        # no namespace. all definitions is global
        if len(info) == 0:
            info.append({'namespace': '__global__', 'definitions': []})
            def_body_tags = cls.soup_data.select(cls.definition_body_selector)

            definitions = []
            definition_full_tags = def_body_tags[0].select('.sense')

            for definition_full_tag in definition_full_tags:
                definition = cls._parse_definition(definition_full_tag)
                definitions.append(definition)

            info[0]['definitions'] = definitions

        return info

    @classmethod
    def idioms(cls):
        """""" get word idioms
        idioms dont have namespace like regular definitions
        each idioms have one or more definitions
        each definitions can have one, many or no examples
        """"""
        idiom_tags = cls.soup_data.select(cls.idioms_selector)

        idioms = []
        for idiom_tag in idiom_tags:

            try:
                # sometimes idiom is in multiple idm classes inside
                # one idm-l class instead of a single idm class
                idiom = idiom_tag.select('.idm-l')[0].text
            except indexerror:
                idiom = idiom_tag.select('.idm')[0].text

            global_definition = {}

            try:  # label: (old-fashioned), (informal), (saying)...
                global_definition['label'] = idiom_tag.select('.labels')[0].text
            except indexerror:
                pass

            try:  # refer to something (of people, of thing,...)
                global_definition['refer'] = idiom_tag.select('.dis-g')[0].text
            except indexerror:
                pass

                global_definition['references'] = cls.get_references(idiom_tag)
            if not global_definition['references']:
                global_definition.pop('references', none)

            definitions = []
            # one idiom can have multiple definitions, each can have multiple examples or no example
            for definition_tag in idiom_tag.select('.sense'):
                definition = {}

                try:  # sometimes, it just refers to other page without having a definition
                    definition['description'] = definition_tag.select('.def')[0].text
                except indexerror:
                    pass

                try:  # label: (old-fashioned), (informal), (saying)...
                    definition['label'] = definition_tag.select('.labels')[0].text
                except indexerror:
                    pass

                try:  # refer to something (of people, of thing,...)
                    definition['refer'] = definition_tag.select('.dis-g')[0].text
                except indexerror:
                    pass

                definition['references'] = cls.get_references(definition_tag)
                if not definition['references']:
                    definition.pop('references', none)

                definition['examples'] = [example_tag.text for example_tag in definition_tag.select('.x')]
                definitions.append(definition)

            idioms.append({'name': idiom, 'summary': global_definition, 'definitions': definitions})

        return idioms

    @classmethod
    def info(cls):
        """""" return all info about a word """"""
        if cls.soup_data is none:
            return none

        word = {
            'id': cls.id(),
            'name': cls.name(),
            'wordform': cls.wordform(),
            'pronunciations': cls.pronunciations(),
            'property': cls.property_global(),
            'definitions': cls.definitions(full=true),
            'idioms': cls.idioms(),
            'other_results': cls.other_results()
        }

        if not word['property']:
            word.pop('property', none)

        if not word['other_results']:
            word.pop('other_results', none)

        if word['wordform'] == 'verb':
            word['phrasal_verbs'] = cls.phrasal_verbs()

        return word

any help will be appreciated thank you:)","['python', 'string', 'nlp']",73723346,"just split values[0] into words and call word.get(...) on each
import re

while true:
    ##test text scanner --
    words = re.findall(r""\w+"", values[0].strip()) # can also use nltk.word_tokenize
    for word in words:
        word.get(word)
        if event == sg.win_closed or event == 'cancel':
            break
    
        try:
            words_in_excerpt = word_tokenize(text)
            nltk.pos_tag(words_in_excerpt)
            print('hello', nltk.pos_tag(words_in_excerpt), """")
            sg.popup('test', nltk.pos_tag(words_in_excerpt))
            sg.popup('def', word.definitions())
            break
        except:
            sg.popup('there seems to be a error processing what you have said')
            break",https://stackoverflow.com/questions/73722269,python,14-09-2022 19:30,76.0,0.0,1.0,True,14-09-2022 21:36,14-09-2022 19:36
55038634,nlp for text mining or chatbot,"i am planning to build a chatbot which can get the user input and analyze and call different web service in java. for example,
get customers who bought books between 01/mar/2019 and 10/mar/2019. 
get books published by abc publications. 
create customer with name abc and address 12, hill view street, london.

for the first one, it has to identify it is a retrieve request as it is ""get"" call and it is about the book and also date. in second text need to extract keywords ""books"" and publication name ""abc"". the third one is different, it's a create customer request call with name and address.
as i am more comfortable with java, i am looking for nlp which can achieve above. on the internet, i find more on opennlp and corenlp. examples and samples are available widely for opennlp.
so i want to check whether am i in the right direction? i see a lot of other things like apache ruta uima but not sure it is applicable for my use case, as i don't find much information in net.","['java', 'nlp', 'artificial-intelligence', 'opennlp']",55038818,"yes i'd say those two nlp libraries are widely used for java. however, if you are going to make small project that doesn't really need to scale out or have to deal with big data, then nltk or spacy ( and probably with scikit-learn) can be a good alternative. those are all python based but it's not that difficult to use.",https://stackoverflow.com/questions/55038634,java,07-03-2019 07:52,251.0,0.0,1.0,True,11-04-2022 23:38,11-04-2022 23:38
76521336,matching spacy with double punctuation,"i am using spacy with matcher to detect some words. when i want to find a word with a single punctuation like - works:
import spacy
from spacy.matcher import matcher

nlp = spacy.load(""en_core_web_sm"")
matcher = matcher(nlp.vocab)
# 
pattern = [{""lower"": ""nice""}, {""is_punct"": true}, {""lower"": ""word""}]
matcher.add(""nice-word"", [pattern])

doc = nlp(""this is a nice-word also? why is this a nice word"")

matches = matcher(doc)
for match_id, start, end in matches:
    string_id = nlp.vocab.strings[match_id]  
    span = doc[start:end] 
    print(match_id, string_id, start, end, span.text)

output:
1899655961849619838 nice-word 3 6 nice-word

this works great! but imagine we have a word with double -, i can't get it work. i would like to find a word for example: nice-word-also. here is some reproducible code:
import spacy
from spacy.matcher import matcher

nlp = spacy.load(""en_core_web_sm"")
matcher = matcher(nlp.vocab)
# 
pattern = [{""lower"": ""nice""}, {""is_punct"": true}, {""lower"": ""word""}, {""lower"": ""also""}]
matcher.add(""nice-word-also"", [pattern])

doc = nlp(""this is a nice-word-also? why is this a nice word"")
matches = matcher(doc)
for match_id, start, end in matches:
    string_id = nlp.vocab.strings[match_id]  
    span = doc[start:end] 
    print(match_id, string_id, start, end, span.text)

this doesn't return anything. so i was wondering if anyone knows how to use spacy matches to detect words with double punctuation like the example above?","['python', 'python-3.x', 'text', 'spacy', 'string-matching']",76521649,"you are missing one {""is_punct"": true} in your pattern:
import spacy
from spacy.matcher import matcher

nlp = spacy.load(""en_core_web_sm"")
matcher = matcher(nlp.vocab)
# 
pattern = [{""lower"": ""nice""}, {""is_punct"": true}, {""lower"": ""word""}, {""is_punct"": true}, {""lower"": ""also""}]
matcher.add(""nice-word-also"", [pattern])

doc = nlp(""this is a nice-word-also? why is this a nice word"")
matches = matcher(doc)
for match_id, start, end in matches:
    string_id = nlp.vocab.strings[match_id]  
    span = doc[start:end] 
    print(match_id, string_id, start, end, span.text)

#output
9732713127922352434 nice-word-also 3 8 nice-word-also",https://stackoverflow.com/questions/76521336,python,21-06-2023 08:23,64.0,0.0,1.0,True,21-06-2023 09:07,21-06-2023 09:07
67082571,undo the tokenization in python,"i would like to reverse the tokenization that i have applied to my data.
data = [['this', 'is', 'a', 'sentence'], ['this', 'is', 'a', 'sentence', '2']]

expected output:
['this is a sentence', 'this is a sentence 2']

i tried to do this with the following code block:
from nltk.tokenize.treebank import treebankworddetokenizer
data_untoken= []
for i, text in enumerate(data):
    data_untoken.append(text)
    data_untoken = treebankworddetokenizer().detokenize(text)

but i have the following error
'str' object has no attribute 'append'","['python', 'text-mining']",67082645,"use join():
def untokenize(data):
    for tokens in data:
        yield ' '.join(tokens)


data = [['this', 'is', 'a', 'sentence'], ['this', 'is', 'a', 'sentence', '2']]
untokenized_data = list(untokenize(data))",https://stackoverflow.com/questions/67082571,python,13-04-2021 21:23,677.0,0.0,1.0,True,13-04-2021 22:00,13-04-2021 21:26
68114581,"multiple same rows to one row, but rows at different amount","i have data like this:




id
word




1
bus


1
arrive


1
stop


1
time


1
beard


1
bearded


1
sits


2
whilst


2
argue


2
seat


2
time


2
police


3
officer


3
walks


3
intervenes




i want to convert it to a dataset like:




id
word




1
arrive bus stop time beard bearded sits


2
whilst begin argue seat time


3
officer walks intervenes




is it possible?
thank you.","['r', 'text', 'nlp']",68114766,"to add some detail to my comment:
library(dplyr)

data <- tibble::tribble(
  ~id,        ~word,
   1l,     ""arrive"",
   1l,        ""bus"",
   1l,       ""stop"",
   1l,       ""time"",
   1l,      ""beard"",
   1l,    ""bearded"",
   1l,       ""sits"",
   2l,     ""whilst"",
   2l,      ""begin"",
   2l,      ""argue"",
   2l,       ""seat"",
   2l,       ""time"",
   2l,     ""police"",
   3l,    ""officer"",
   3l,      ""walks"",
   3l, ""intervenes""
  )

data %>% 
  group_by(id) %>% 
  mutate(word = paste0(word, collapse = "" "")) %>% 
  slice(1) %>% # take the first line from each group
  ungroup()

or better (so you don't need the slice):
data %>% 
  group_by(id) %>% 
  summarise(word = paste0(word, collapse = "" ""))",https://stackoverflow.com/questions/68114581,r,24-06-2021 11:06,80.0,0.0,1.0,True,24-06-2021 11:32,24-06-2021 11:22
75567331,openai gpt-3 api error: &quot;you must provide a model parameter&quot;,"i am trying to post a question to openai api via swift. it works fine, if i use the same payload via postman, but in the xcode-condole i got the following response from openai:
response data string:
{
     ""error"": {
         ""message"": ""you must provide a model parameter"",
         ""type"": ""invalid_request_error"",
         ""param"": null,
         ""code"": null
    }
 }

this is my code:
 func getactivityanalysis(){
    
    let url = url(string: ""
    guard let requesturl = url else { fatalerror() }
    
    // prepare url request object
    var request = urlrequest(url: requesturl)
    request.setvalue(""bearer blaaaablaa"", for ""authorization"")
    request. = ""post""
    
    
    let prompt = ""just a test""
    let requestbody = openairequest(model: ""text-davinci-003"", prompt: prompt, max_tokens: 300, temperature: 0.5)
    
    let encoder = jsonencoder()
    encoder.outputformatting = .prettyprinted
    let data = try! encoder.encode(requestbody)
    print(string(data: data, encoding: .utf8)!)
    
     
    // set http request body
    request. = data
    
    print(""\(request. \(request.url!)"")
    print(request.all
    print(string(data: request. ?? data(), encoding: .utf8)!)
    
    
    
    // perform http request
    let task = urlsession.shared.datatask(with: request) { (data, response, error) in
            
            // check for error
            if let error = error {
                print(""error took place \(error)"")
                return
            }
     
            // convert http response data to a string
            if let data = data, let datastring = string(data: data, encoding: .utf8) {
                print(""response data string:\n \(datastring)"")
                self.openairesponse = datastring
            }
    }
    task.resume()
    
}`

if i print the http request, it seems fine for me as well:
 post 
 [""authorization"": ""bearer blaaaaa""]
 {
    ""temperature"" : 0.5,
    ""model"" : ""text-davinci-003"",
    ""prompt"" : ""just a test"",
    ""max_tokens"" : 300
 }

i tried to use the same payload in my postman request. it worked fine here. i also tried to use different encodings, but it always throws the same error.
not sure, what i am doing wrong. maybe someone can help?
thank you in advance.
bets,
tobi","['swift', 'openai-api', 'gpt-3']",75571367,"your http request reveals the problem. you need to add 'content-type: application/json'.
according to geeksforgeeks:

content-type is an http header that is used to indicate the media type
of the resource and in the case of responses, it tells the browser
about what actually content type of the returned content is.",https://stackoverflow.com/questions/75567331,swift,25-02-2023 17:52,4727.0,2.0,1.0,True,21-10-2023 19:55,13-03-2023 14:47
74197670,how to perform same operation on multiple text files and save the output in different files using python?,"i have written a code which extracts stop words from a text file and outputs two new text files. one file contains the stop words from that text file and another file contains the data without stop words. now i have more than 100 text file in a folder, i would like to perform the same operation on all those file simultaneously.
for example there is a folder a which contains 100 text file the code should be executed on all those text files simultaneously. the output should be two new text files such as 'stop_word_consist_filename.txt' and 'stop_word_not_filename.txt' which should be stored in a separate folder.that means for every 100 text files there will 200 output text files stored in a new folder. please note the 'filename' in both these output file is the actual name of the text file meaning 'walmart.txt' should have 'stop_word_consist_walmart.txt' and 'stop_word_not_walmart.txt'. i did try few things and i know loop in involved giving the path directory but i didn't get any success.
apologies for such a long question.
following is the code for 1 file.
import numpy as np
import pandas as pd

# pathes of source files and that for after-modifications
files_path = os.getcwd()
# another folder, your should create first to store files after modifications in
files_after_path = os.getcwd() + '/' + 'stopwords_folder'
os.makedirs(files_after_path, exist_ok=true)
text_files = os.listdir(files_path)
data = pd.dataframe(text_files)
data.columns = [""review_text""]

import re
import nltk
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem.porter import porterstemmer

def clean_text(df):
    all_reviews = list()
    #lines = df[""review_text""].values.tolist()
    lines = data.values.tolist()

    for text in lines:
        #text = text.lower()
        text = [word.lower() for word in text]

        pattern = re.compile('
        text = pattern.sub('', str(text))
        
        emoji = re.compile(""[""
                           u""\u0001f600-\u0001ffff""  # emoticons
                           u""\u0001f300-\u0001f5ff""  # symbols & pictographs
                           u""\u0001f680-\u0001f6ff""  # transport & map symbols
                           u""\u0001f1e0-\u0001f1ff""  # flags (ios)
                           u""\u00002702-\u000027b0""
                           u""\u000024c2-\u0001f251""
                           ""]+"", flags=re.unicode)
        text = emoji.sub(r'', text)
        
        text = re.sub(r""i'm"", ""i am"", text)
        text = re.sub(r""he's"", ""he is"", text)
        text = re.sub(r""she's"", ""she is"", text)
        text = re.sub(r""that's"", ""that is"", text)        
        text = re.sub(r""what's"", ""what is"", text)
        text = re.sub(r""where's"", ""where is"", text) 
        text = re.sub(r""\'ll"", "" will"", text)  
        text = re.sub(r""\'ve"", "" have"", text)  
        text = re.sub(r""\'re"", "" are"", text)
        text = re.sub(r""\'d"", "" would"", text)
        text = re.sub(r""\'ve"", "" have"", text)
        text = re.sub(r""won't"", ""will not"", text)
        text = re.sub(r""don't"", ""do not"", text)
        text = re.sub(r""did't"", ""did not"", text)
        text = re.sub(r""can't"", ""can not"", text)
        text = re.sub(r""it's"", ""it is"", text)
        text = re.sub(r""couldn't"", ""could not"", text)
        text = re.sub(r""have't"", ""have not"", text)
        
        text = re.sub(r""[,.\""!@#$%^&*(){}?/;`~:<>+=-]"", """", text)
        tokens = word_tokenize(text)
        table = str.maketrans('', '', string.punctuation)
        stripped = [w.translate(table) for w in tokens]
        words = [word for word in stripped if word.isalpha()]
        stop_words = set(stopwords.words(""english""))
        stop_words.discard(""not"")
        ps = porterstemmer()
        words = [ps.stem(w) for w in words if not w in stop_words]
        words = ' '.join(words)
        all_reviews.append(words)
    return all_reviews,stop_words

for entry in data:
    #all_reviews , stop_words = clean_text(entry)
    for r in all_reviews: 
        if not r in stop_words: 
            appendfile = open(f'no_stopwords{entry}.txt','a') 
            appendfile.write("" ""+r) 
            appendfile.close() 
    
    for r in stop_words: 
        appendfile = open(f'stop_word_consist{entry}.txt','a') 
        appendfile.write("" ""+r) 
        appendfile.close() 
        
    all_reviews , stop_words = clean_text(entry)

update :
so i have made changes to the code. i did got two output files stop_word_consist and no_stop_word. but i am not getting the required data inside. meaning stop_word consist does not have the stop words i am looking for. i am pretty sure i made some mistakes in indentation. i would appreciate the help.","['python', 'file', 'for-loop', 'nlp', 'text-files']",74198147,"you can use os.listdir to get the number of text files, and use a for loop to run each time. to assign a name to the output file you can use an f-string in its creation so it looks like f'stop_word_consist_{filename}':
for entry in os.listdir(folder location):
    all_reviews , stop_words = clean_text(data_1)
    all_reviews[:]

for r in all_reviews: 
    if not r in stop_words: 
    appendfile = open('stop_word_hrb02-phil-usa.txt.txt','a') 
    appendfile.write("" ""+r) 
    appendfile.close() 

for r in stop_words: 
    appendfile = open(f'stop_word_consist{entry}.txt','a') 
    appendfile.write("" ""+r) 
    appendfile.close()",https://stackoverflow.com/questions/74197670,python,25-10-2022 16:55,580.0,0.0,1.0,True,29-10-2022 00:22,29-10-2022 00:22
74355660,"i am new to pre-trained language models in natural language processing. could anyone give a hint on where should i start, or the road maps to start?","i know there are quite many hands-on tutorials about deploying bert or other models. but the problem is many of them are just shallow user cases which only modified a few parameters using apis from a certain libraries such as keras. i am a novice and i often find my still confused at the details and variational steps when deploying pre-trained language models on my projects.
could anyone give a hint on what is a better roadmap for learning coding with pre-trained language models, including any resources, articles, or tutorials, etc.
i have read many articles on medium. but most articles there seems only introduc the general concepts, rather than providing the real know-how when learning it.","['deep-learning', 'nlp', 'pre-trained-model']",74367145,"i find these two courses very novice-friendly:

hugging face course


deep lizard ""deep learning fundamentals - classic edition""",https://stackoverflow.com/questions/74355660,deep-learning,08-11-2022 04:25,63.0,0.0,1.0,True,08-11-2022 21:02,08-11-2022 04:25
27658409,downloading error using nltk.download(),"i am experimenting nltk package using python. i tried to downloaded nltk using nltk.download(). i got this kind of error message. how to solve this problem? thanks.
the system i used is ubuntu installed under vmware. the ide is spyder.

after using nltk.download('all'), it can download some packages, but it gets error message when downloading oanc_masc","['python', 'python-2.7', 'ubuntu', 'nltk', 'spyder']",27660379,"to download a particular dataset/models, use the nltk.download() function, e.g. if you are looking to download the punkt sentence tokenizer, use:
$ python3
>>> import nltk
>>> nltk.download('punkt')

if you're unsure of which data/model you need, you can start out with the basic list of data + models with:
>>> import nltk
>>> nltk.download('popular')

it will download a list of ""popular"" resources.
ensure that you've the latest version of nltk because it's always improving and constantly maintain:
$ pip install --upgrade nltk


edited
in case anyone is avoiding errors from downloading larger datasets from nltk, from 
$ rm /users/<your_username>/nltk_data/corpora/panlex_lite.zip
$ rm -r /users/<your_username>/nltk_data/corpora/panlex_lite
$ python

>>> import nltk
>>> dler = nltk.downloader.downloader()
>>> dler._update_index()
>>> dler._status_cache['panlex_lite'] = 'installed' # trick the index to treat panlex_lite as it's already installed.
>>> dler.download('popular')

and if anyone wants to find nltk_data directory, see  
and to config nltk_data path, see",https://stackoverflow.com/questions/27658409,python,26-12-2014 14:35,92849.0,24.0,10.0,True,20-08-2024 07:06,16-12-2016 18:18
70346894,how to add sos token to keras tokenizer?,"i have a keras tokenizer and i want to add a start of sentence token to my sequences but i could not find anything about it that shows how can i do that?
tokenizer = tokenizer(split=' ') 

tokenizer.fit_on_texts(data)


tokenizer.word_index['<pad>'] = 0
tokenizer.index_word[0] = '<pad>'

text_tokenized = tokenizer.texts_to_sequences(data)


text_corpus_padded = pad_sequences(text_tokenized, padding='post', maxlen=100, dtype='int32')","['python', 'tensorflow', 'keras', 'nlp', 'tokenize']",70347190,"depending on your use case (for example, a decoder model), you could add the <sos> and <eos> to each sentence and then tokenize them like this:
import tensorflow as tf

data = ['hello world', 'hello new world']
data = ['<sos> ' + x + ' <eos>' for x in data]

tokenizer = tf.keras.preprocessing.text.tokenizer(split=' ', filters='!""#$%&()*+,-./:;=?@[\\]^_`{|}~\t\n') 

tokenizer.fit_on_texts(data)

tokenizer.word_index['<pad>'] = 0
tokenizer.index_word[0] = '<pad>'

text_tokenized = tokenizer.texts_to_sequences(data)
print(text_tokenized)
print(tokenizer.word_index)

[[1, 2, 3, 4], [1, 2, 5, 3, 4]]
{'<sos>': 1, 'hello': 2, 'world': 3, '<eos>': 4, 'new': 5, '<pad>': 0}

note that i have removed < and > from the filters in the tokenizer so that you can use these characters in your sentences. also, check this tutorial.",https://stackoverflow.com/questions/70346894,python,14-12-2021 09:53,1071.0,1.0,1.0,True,14-12-2021 10:39,14-12-2021 10:39
20362993,how to load sentences into python gensim?,"i am trying to use the word2vec module from gensim natural language processing library in python.
the docs say to initialize the model:
from gensim.models import word2vec
model = word2vec(sentences, size=100, window=5, min_count=5, workers=4)

what format does gensim expect for the input sentences?  i have raw text
""the quick brown fox jumps over the lazy dogs""
""then a cop quizzed mick jagger's ex-wives briefly.""
etc.

what additional processing do i need to post into word2fec?

update: here is what i have tried.  when it loads the sentences, i get nothing.
>>> sentences = ['the quick brown fox jumps over the lazy dogs',
             ""then a cop quizzed mick jagger's ex-wives briefly.""]
>>> x = word2vec.word2vec()
>>> x.build_vocab([s.encode('utf-8').split( ) for s in sentences])
>>> x.vocab
{}","['python', 'nlp', 'gensim']",20363116,"a list of utf-8 sentences. you can also stream the data from the disk.
make sure it's utf-8, and split it:
sentences = [ ""the quick brown fox jumps over the lazy dogs"",
""then a cop quizzed mick jagger's ex-wives briefly."" ]
word2vec.word2vec([s.encode('utf-8').split() for s in sentences], size=100, window=5, min_count=5, workers=4)",https://stackoverflow.com/questions/20362993,python,03-12-2013 22:25,14016.0,16.0,2.0,True,08-02-2023 12:19,03-12-2013 23:33
73204377,usng r - gsub using code in replacement - replace comma with full stop after pattern,"i would like to manually correct a record by using r. last name and first name should always be separated by a comma.
names <- c(""adam, smith j."", ""johnson. richard"", ""brown, wilhelm k."", ""davis, daniel"")

sometimes, however, a full stop has crept in as a separator, as in the case of ""johnson. richard"". i would like to do this automatically. since the last name is always at the beginning of the line, i can simply access it via sub:
sub(""^[[:upper:]]+\\."",""^[[:upper:]]+\\,"",names)

however, i cannot use a function for the replacement that specifically replaces the full stop with a comma.
is there a way to insert a function into the replacement that does this for me?","['r', 'string', 'replace', 'text-mining', 'gsub']",73204901,"your sub is mostly correct, but you'll need a capture group (the brackets and backreference \\1) for the replacement.
because we are ""capturing"" the upper case letters, therefore \\1 here represents the original upper case letters in your original strings. the only replacement here is \\. to \\,. in other words, we are replacing upper case letters ^(([[:upper:]]+) and full stop \\. with it's original content \\1 and comma \\,.
for more details you can visit this page.
test_names <- c(""adam, smith j."", ""johnson. richard"", ""brown, wilhelm k."", ""davis, daniel"")

sub(""^([[:upper:]]+)\\."",""\\1\\,"",test_names)
[1] ""adam, smith j.""    ""johnson, richard""  ""brown, wilhelm k.""
[4] ""davis, daniel""",https://stackoverflow.com/questions/73204377,r,02-08-2022 08:35,226.0,0.0,2.0,True,03-08-2022 09:38,02-08-2022 10:39
74799295,how to extract cities with spacy / can&#39;t load french model,"i know it's perhaps an easy question but i'm not very familiar with spacy.
so i'm trying to extract cities in a text file.
my code is that :
pip install spacy_lefff
pip install spacy download fr

import spacy
from spacy_lefff import leffflemmatizer
from spacy.language import language

@language.factory('french_lemmatizer')
def create_french_lemmatizer(nlp, name):
    #return leffflemmatizer()

nlp = spacy.load('fr_core_news_sm')
nlp.add_pipe('french_lemmatizer', name='lefff')
doc = nlp(u""apple cherche a acheter une startup anglaise pour 1 milliard de dollard"")
for d in doc:
    print(d.text, d.pos_, d._.lefff_lemma, d.tag_, d.lemma_)

import spacy
nlp = spacy.load(""en_core_web_sm"")

import os
from google.colab import drive
drive.mount('/content/drive/', force_remount=true)
if not os.path.exists('/content/drive/my drive/miserables'):
  os.makedirs('/content/drive/my drive/miserables')

root_dir = '/content/drive/my drive/miserables/'
os.listdir('/content/drive/my drive/miserables')
with open(""/content/drive/my drive/miserables/miserable.txt"", 'r') as f:
     mystring = f.read()

doc = nlp(open('/content/drive/my drive/miserables/miserable - 1.txt').read())
for ent in doc.ents:
    if (ent.label_ == 'gpe'):
        gpe.append(ent.text)
    elif (ent.label_ == 'loc'):
        loc.append(ent.text)

cities = []
countries = []
other_places = []
import wikipedia
for text in gpe:
    summary = str(wikipedia.summary(text),""html.parser"")
    if ('city' in summary):
        cities.append(text)
    elif ('country' in summary):
        countries.append(text)
    else:
        other_places.append(text)

for text in loc:
    other_places.append(text)



typeerror: decoding str is not supported
can't load french spacy model? i don't know why, i'm trying but it doesnt code.
thanks for your help.","['python', 'spacy']",74800593,"my text is in french

i just skimmed some of the source code for locationtagger, and it appears that it hardcodes usage of the en_core_web_sm model. it likely does not form correct parses of your input text.
i would not use nltk or locationtagger for this task.
instead, download a proper spacy model for french:
python3 -m spacy download fr_core_news_{sm|md|lg|trf}

read spacy's documentation on named entity recognition [1]. this includes information about identifying geopolitical entities (""gpe"").
the default spacy models will tag cities, states/provinces/districts, and countries under the ""gpe"" tag. if you are interested only in the cities, then, you should filter the found gpes against the data in locationtagger's city-region-locations.csv.
additionally, you may wish to segment the text by paragraph and use spacy's nlp.pipe to process paragraphs in parallel.",https://stackoverflow.com/questions/74799295,python,14-12-2022 13:51,353.0,0.0,1.0,True,19-12-2022 03:33,19-12-2022 03:33
78803529,error while running hugging face models on kaggle notebook,"i am using the llama3 model from the huggingface library on a kaggle notebook and am facing this error on running the pipeline module
i have trimmed out a major chunk of the stack trace because otherwise posting the question was not allowed with all that code and no description.
runtimeerror                              traceback (most recent call last)
cell in[19], line 17, in llama_chat(system_role, user_msg)
     12 def llama_chat(system_role,user_msg):
     13   messages = [
     14     {""role"": ""system"", ""content"": system_role},
     15     {""role"": ""user"", ""content"": user_msg},
     16   ]
---> 17   outputs = pipeline(
     18       messages,
     19       max_new_tokens=256,
     20       temperature = 0.1
     21 
     22   )
     24   reply=outputs[0][""generated_text""][-1][""content""]
     25   return reply

file /opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:169, in add_hook_to_module.<locals>.new_forward(module, *args, **kwargs)
    167         output = module._old_forward(*args, **kwargs)
    168 else:
--> 169     output = module._old_forward(*args, **kwargs)
    170 return module._hf_hook.post_forward(module, output)

file /opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:603, in llamasdpaattention.forward(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)
    599 # we dispatch to sdpa's flash attention or efficient kernels via this `is_causal` if statement instead of an inline conditional assignment
    600 # in sdpa to support both torch.compile's dynamic shapes and full graph options. an inline conditional prevents dynamic shapes from compiling.
    601 is_causal = true if causal_mask is none and q_len > 1 else false
--> 603 attn_output = torch.nn.functional.scaled_dot_product_attention(
    604     query_states,
    605     key_states,
    606     value_states,
    607     attn_mask=causal_mask,
    608     dropout_p=self.attention_dropout if self.training else 0.0,
    609     is_causal=is_causal,
    610 )
    612 attn_output = attn_output.transpose(1, 2).contiguous()
    613 attn_output = attn_output.view(bsz, q_len, -1)

runtimeerror: cutlassf: no kernel found to launch!

this is the error i am facing while running huggingface models using the transformers library in kaggle .. i have checked the versions of cuda , pytorch they are fine
chatgpt ,claude etc. are all suggesting the version mismatch , but i am making no progress","['python', 'python-3.x', 'huggingface-transformers', 'kaggle']",78813385,"try setting the following backends to false.
torch.backends.cuda.enable_mem_efficient_sdp(false)
torch.backends.cuda.enable_flash_sdp(false)

source",https://stackoverflow.com/questions/78803529,python,28-07-2024 10:37,237.0,1.0,1.0,True,30-07-2024 19:23,30-07-2024 08:09
79453856,how to get lexical search score and vector search score in a hybrid search on apahce solr?,"i was able to implement a hybrid search engine on apache solr 9.6.1 that combines lexical search (edismax) and vector search (knn-based embeddings) within a single request. the idea is simple:

lexical search retrieves results based on text relevance.
vector search retrieves results based on semantic similarity.
hybrid scoring sums both scores, where a missing score (if a
document appears in only one search) should be treated as zero.

this approach is working, but i am unable to properly return individual score components of lexical search (score1) vs. vector search (score2 from cosine similarity). right now, solr only returns the final combined score, but there is no clear way to see how much of that score comes from lexical search vs. vector search. the following is a code snippet:
def hybrid_search(query, top_k=10):
embedding = np.array(embed([query]), dtype=np.float32
embedding = list(embedding[0])
lxq= rf""""""{{!type=edismax 
            qf='text'
            q.op=or
            tie=0.1
            bq=''
            bf=''
            boost=''
        }}({query})""""""
solr_query = {""params"": {
    ""q"": ""{!bool filter=$retrievalstage must=$rankingstage}"",
    ""rankingstage"": ""{!func}sum(query($normalisedlexicalquery),query($vectorquery))"",
    ""retrievalstage"":""{!bool should=$lexicalquery should=$vectorquery}"", # union
    ""normalisedlexicalquery"": ""{!func}scale(query($lexicalquery),0,1)"",
    ""lexicalquery"": lxq,
    ""vectorquery"": f""{{!knn f=all_v512 topk={top_k}}}{embedding}"",
    ""fl"": ""text, score"",
    ""rows"": top_k,
    ""fq"": [""""],
    ""rq"": ""{!rerank rerankquery=$rqq rerankdocs=100 rerankweight=3}"",
    ""rqq"": ""{!frange l=$cutoff}query($rankingstage)"",
    ""sort"": ""score desc"",
}}
response = requests.post(solr_url, headers=headers, json=solr_query)
response = response.json()
return response

before the retrieval stage, i scale the scores of the keyword search to be between 0 and 1 to make them similar to the scores of the vector search, since the scores in the keyword search are unbounded.
after that, i take the union between both retrievals, where the scores get added.
for example, if post x was retrieved in both the keyword search and the vector search, with scores 0.6 and 0.5 respitevly, the final score would be 0.6 + 0.5 = 1.1.
if a post was retrieved in one but not the other, it would get simply added by 0.
now the question is how can i retrieve both scores separately (not the final score) when using hybrid search, and without having to send two different requests to solr, as in the hybrid approach, i only send one request for both retrievals.
i have tried to include ""score"" in the ""fl"" field, but it only shows the final score (i assume).","['solr', 'lucene', 'search-engine', 'information-retrieval']",79469274,"the function ""query"" returns a score from the given query for each document.
the result of the query function is casted to a variable named ""normalisedlexicalquery"". just use the variable name with a dollar sign ""$"" in the fl field.
so you can use
""fl"":""lexical_score:$normalisedlexicalquery,vector_score:query($vectorquery)""",https://stackoverflow.com/questions/79453856,solr,20-02-2025 08:52,88.0,-2.0,1.0,True,26-02-2025 11:16,23-02-2025 08:05
76121676,find position of exact match in a string,"i am looking for exact match of words in string. i wrote the code as follows to find the position of each exact match of elements in list in the string but it does search if a word is not exact match.
can anyone help me?
in the following example the exact match is from.
text = ""greeting from here""
lis = [""greet"", ""from"",""re""]
import re
pos = []
for i in lis:
    pos.append(text.find(i))

current output: [-1, 9, 1]

expected output: [9]","['python', 'nlp']",76121787,"you can use re.search() as follows:
import re

text = ""greeting from here""
lis = [""greet"", ""from"", ""re""]

pos = []

for i in lis:
    if m := re.search(rf'\b{i}\b', text):
        pos.append(m.span()[0])

print(pos)

output:
[9]",https://stackoverflow.com/questions/76121676,python,27-04-2023 14:40,45.0,0.0,1.0,True,27-04-2023 15:07,27-04-2023 15:07
70849127,"training, validation and test sets for imbalanced datasets in machine learning","i am working on an nlp task for a classification problem. my dataset is imbalanced and some authors have only 1 text, and thus i want to have this text only in the training set. as for the other authors i need to split the dataset into 70% training set, 15% validation set and 15% test set.
i tried to use train_test_split function from sklearn, but the results aren't that good.
my dataset is a dataframe that looks like this
title   preprocessed_text   label
-----   -----------------   -----

please help me out.","['python', 'machine-learning', 'scikit-learn', 'nlp', 'classification']",70850584,"it is rather hard to obtain good classification results for a class that contains only 1 instance (at least for that specific class). regardless, for imbalanced datasets, one should use stratified train_test_split (using stratify=y), which preserves the same proportions of instances in each class as observed in the original dataset.
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.25)

i should also add that if the dataset is rather small, let's say no more than 100 instances, it would be preferable to use cross-validation instead of train_test_split, and more specifically, stratifiedkfold or repeatedstratifiedkfold that returns stratified folds (see this answer to understand the difference between the two).
when it comes to evaluation, you should consider using metrics such as precision, recall and f1-score (the harmonic mean of the precision and recall), using the average weighted score for each of these, which uses a weight that depends on the number of true instances of each class. as per the documentation:
'weighted':

calculate metrics for each label, and find their average
weighted by support (the number of true instances for each label).
this alters ï¿½ï¿½ï¿½macroï¿½ï¿½ï¿½ to account for label imbalance; it can result in
an f-score that is not between precision and recall.
</",https://stackoverflow.com/questions/70849127,python,25-01-2022 13:04,1755.0,2.0,2.0,True,10-01-2023 17:58,10-01-2023 17:58
71301279,output from bert into cnn model,"i am trying to concatenate bert model with cnn 1d using pytorch . i used this code but i do not understand what is meaning of in_channels and out_channels in function conv1d
if input shape into cnn model is torch(256,64,768)
class mixmodel(nn.module):
    def __init__(self,pre_trained='distilbert-base-uncased'):
        super().__init__()        
        self.bert =  automodel.from_pretrained('distilbert-base-uncased')
        self.hidden_size = self.bert.config.hidden_size
        self.conv = nn.conv1d(in_channels=1, out_channels=256, kernel_size=5, padding='valid', stride=1)
        self.relu = nn.relu()
        self.pool = nn.maxpool1d(kernel_size= 256- 5 + 1)
        self.dropout = nn.dropout(0.3)
        self.clf = nn.linear(self.hidden_size*2,6)
        
      
           
    def forward(self,inputs, mask , labels):
        
        cls_hs = self.bert(input_ids=inputs,attention_mask=mask, return_dict= false) 
        x=cls_hs
       # x = torch.cat(cls_hs[0]) # x= [416, 64, 768]
        x = self.conv(x)
        x = self.relu(x)
        x = self.pool(x)
        x = self.dropout(x)
        x = self.clf(x)
        
        
      
        return x

edit
i use recommended answer and change the parameters but i got error
class mixmodel(nn.module):
    def __init__(self,pre_trained='bert-base-uncased'):
        super().__init__()        
        self.bert =  automodel.from_pretrained('distilbert-base-uncased')
        self.hidden_size = self.bert.config.hidden_size
        self.conv = nn.conv1d(in_channels=768, out_channels=256, kernel_size=5, padding='valid', stride=1)
        self.relu = nn.relu()
        self.pool = nn.maxpool1d(kernel_size= 64- 5 + 1)
        print(11)
        self.dropout = nn.dropout(0.3)
        print(12)
        self.clf = nn.linear(self.hidden_size*2,6)
        print(13)
        
      
           
    def forward(self,inputs, mask , labels):
        
        cls_hs = self.bert(input_ids=inputs,attention_mask=mask, return_dict= false) 
        x=cls_hs[0]
        print(cls_hs[0]) 
        print(len(cls_hs[0]))
        print(cls_hs[0].size())
        #x = torch.cat(cls_hs,0) # x= [416, 64, 768]
        x = x.permute(0, 2, 1)
        x = self.conv(x)
        x = self.relu(x)
        x = self.pool(x)
        x = self.dropout(x)
        x = self.clf(x)
return x

the error is
5 frames
/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in linear(input, weight, bias)
1846     if has_torch_function_variadic(input, weight, bias):
1847         return handle_torch_function(linear, (input, weight, bias), input, weight, bias=bias)
-> 1848     return torch._c._nn.linear(input, weight, bias)
1849
1850
runtimeerror: mat1 and mat2 shapes cannot be multiplied (65536x1 and 1536x6)","['python', 'pytorch', 'conv-neural-network', 'bert-language-model']",71304677,"the dimension of the output prediction of bert (and many other transformer-based models) is of shape batchxseq-lenxfeature-dim: that is, your input is a batch of 256 sequences of length (probably with padding) of 64 tokens, each token is represented by a feature vector of dimension 768.
in order to apply 1-d convolution along the sequence-len dimension, you will need first to permute x to be of shape batchxdimxlen:
x = x.permute(0, 2, 1)

now you can apply nn.conv1d, where the in_channels is the dimension of x = 768. the out_channels is up to you - what is going to be the hidden dimension of your model.",https://stackoverflow.com/questions/71301279,python,28-02-2022 21:32,1094.0,0.0,1.0,True,01-03-2022 13:02,01-03-2022 13:02
70843845,understanding results of word2vec gensim for finding substitutes,"i have implemented the word2vec model on transaction data (link) of a single category. 
my goal is to find substitutable items from the data.
the model is giving results but i want to make sure that my model is giving results based on customers historical data (considering context) and not just based on content (semantic data). idea is similar to the recommendation system. 
i have implemented this using the gensim library, where i passed the data (products) in form of a list of lists.
eg.
[['blue bell ice cream gold rim', 'tillamk choc chip ck dough  ic'],
 ['talenti sicilian pistachio gel', 'talenti blk rasp choc chip gel'],
 ['breyers home made van ice cream',
  'breyers home made van ice cream',
  'breyers coff ice cream']]

here, each of the sub lists is the past one year purchase history of a single customer. 
# train word2vec model
model = word2vec(window = 5, sg = 0,
                 alpha=0.03, min_alpha=0.0007,
                 seed = 14)

model.build_vocab(purchases_train, progress_per=200)

model.train(purchases_train, total_examples = model.corpus_count, 
            epochs=10, report_delay=1)

# extract all vectors
x = []
words = list(model.wv.index_to_key)
for word in words:
    x = model.wv.get_vector(word)
    x.append(x)
y = np.array(x)
y.shape

def similar_products(v, n = 3):
    
    # extract most similar products for the input vector
    ms = model.wv.similar_by_vector(v, topn= n+1)[1:]
    
    # extract name and similarity score of the similar products
    new_ms = []
    for j in ms:
        pair = (products_dict[j[0]][0], j[1]) 
        new_ms.append(pair)
        
    return new_ms 

similar_products(model.wv['blue bell ice cream gold rim'])

results:
 [('blue bell ice cream brown rim', 0.7322707772254944),
     ('blue bell ice cream light', 0.4575043022632599),
     ('blue bell ice cream nsa', 0.3731085956096649)]

to get intuitive understanding of word2vec and its working on how results are obtained, i created a dummy dataset where i wanted to find subtitutes of 'foodclub van ic pail'. 
if two products are in the same basket multiple times then they are substitutes. 
looking at the data first substitute should be 'foodclub choc chip ic pail' but the results i obtained are:
[('foodclub neapolitan ic pail', 0.042492810636758804),
 ('foodclub cookies cream ice cream', -0.04012278839945793),
 ('foodclub new york van ic pail', -0.040678512305021286)]


can anyone help me understand the intuitive working of word2vec model in gensim? will each product be treated as word and customer list as sentence?
why are my results so absurd in dummy dataset? how can i improve?
what hyperparameters play a significant role w.r.t to this model? is negative sampling required?","['python', 'gensim', 'word2vec', 'word-embedding', 'recommendation-engine']",70844785,"you may not get a very good intuitive understanding of usual word2vec behavior using these sorts of product-baskets as training data. the algorithm was originally developed for natural-language texts, where texts are runs of tokens whose frequencies, & co-occurrences, follow certain indicative patterns.
people certainly do use word2vec on runs-of-tokens that aren't natural language - like product baskets, or logs-of-actions, etc ï¿½ï¿½ï¿½ but to the extent such tokens have very-different patterns, it's possible extra preprocessing or tuning will be necessary, or useful results will be harder to get.
as just a few ways customer-purchases might be different from real language, depending on what your ""pseudo-texts"" actually represent:

the ordering within a text might be an artifact of how you created the data-dump rather than anything meaningful
the nearest-neighbors to each token within the window may or may not be significant, compared to more d tokens
customer ordering patterns might in general not be as reflective of shades-of-relationships as words-in-natural-language text

so it's not automatic that word2vec will give interesting results here, for recommendatinos.
that's especially the case with small datasets, or tiny dummy datasets. word2vec requires lots of varied data to pack elements into interesting relative positions in a high-dimensional space. even small demos usually have a vocabulary (count of unique tokens) of tens-of-thousands, with training texts that provide varied usage examples of every token dozens of times.
without that, the model never learns anything interesing/generalizable. that's especially the case if trying to create a many-dimensions model (say the default vector_size=100) with a tiny vocabulary (just dozens of unique tokens) with few usage examples per example. and it only gets worse if tokens appear fewer than the default min_count=5 times ï¿½ï¿½dn they're ignored entirely. so don't expect anything interesting to come from your dummy data, at all.
if you want to develop an intuition, i'd try some tutorials & other goals with real natural language text 1st, with a variety of datasets & parameters, to get a sense of what has what kind of effects on result usefulness ï¿½ï¿½ï¿½ & only after that try to adapt word2vec to other data.
negative-sampling is the default, & works well with typical datasets, especially as they grow large (where negative-sampling suffes less of a performance hit than hierarchical-softmax with large vocabularies). but a toggle between those two modes is unlike to cause giant changes in quality unless there are other problems.
sufficient data, of the right kind, is the key ï¿½ï¿½ï¿½ & then tweaking parameters may nudge end-result usefulness in a better direction, or shift it to be better for certain purposes.
but more specific parameter tips are only possible with clearer goals, once s is working.",https://stackoverflow.com/questions/70843845,python,25-01-2022 05:32,449.0,0.0,1.0,True,01-02-2022 14:41,01-02-2022 14:41
45525260,nlp general english to action,"i am working on automating task flow of application using text based natural language processing.
it is something like chatting application where the user can type in the text area. at same time python code interprets what user wants and it performs the corresponding action. 
application has commands/actions like:

create task
give name to as t1
add time to task
connect t1 to t2

the users can type in chat (natural language). it will be like a general english conversation, for example: 

can you create a task with name t1 and assign time to it. also, connect t1 to t2

i could write a rule drive parser, but it would be limited to few rules only.
which approach or algorithm can i use to solve this task?
how can i map general english to command or action?","['python', 'machine-learning', 'nlp', 'deep-learning', 'nltk']",45615658,"i think the best solution would be to use an external service like api.ai or wit.ai. you can create a free account and then you can map certain texts to so-called 'intents'.
these intents define the main actions of your system. you can also define 'entities' that would capture, for instance, the name of the task. please have a look at these tools. i'm sure they can handle your use case.",https://stackoverflow.com/questions/45525260,python,05-08-2017 18:37,2268.0,0.0,6.0,True,11-03-2023 09:02,12-08-2017 08:38
74468471,nlp: pre-processing dataset into a new dataset,"i need help with processing an unsorted dataset.  sry, if i am a complete noob. i never did anything like that before. so as you can see, each conversation is identified by a dialogueid which consists of multiple rows of ""from"" & ""to"", as well as text messages.
i would like to concatenate the text messages from the same sender of a dialogueid to one column and from the receiver to another column. this way, i could have a new csv-file with just [dialogueid, sender, receiver].

the new dataset should look like this

i watched multiple tutorials and really struggle to figure out how to do it. i read in this 9-year-old post that iterating through data frames are not a good idea. could someone help me out with a code snippet or give me a hint on how to properly do it without overcomplicating things? i thought something like this pseudo code below, but the performance with 1 million rows is not great, right?
while !endoffile
  for dialogueid in range (0, 1038324)
    if dialogueid+1 == dialogueid and tovalue.isnull()
      concatenate textfromprevrow + "" "" + textfromcurrentrow
      add new string to table column sender
    else
      add text to column receiver","['python', 'dataframe', 'nlp', 'data-science', 'data-preprocessing']",74468539,"edit 1
according to your clarification, this is what i believe you're looking for.
create an aggregation function which basically concats your string values with a line-break character. then group by dialogueid and apply your aggregation.
d = {}
d['from'] = '\n'.join
d['to'] = '\n'.join
new_df = dialogue_dataframe.groupby('dialogueid', as_index=false).agg(d)

after that rename the columns as you'd like:
df.rename(columns={""from"": ""sender"", ""to"": ""receiver""})

original answer
not quite sure i understood what you try to achieve, but maybe this will give some insights. maybe write a couple of rows of the table you expect to get, for better clarification",https://stackoverflow.com/questions/74468471,python,16-11-2022 23:28,209.0,1.0,2.0,True,17-11-2022 08:44,16-11-2022 23:44
73976285,reverse from pos tagging to sentence using pandas,"i have pos_token dataset and i want to transform them to be a sentence again using pandas




pos_token
sentence




[(no, dt), (you, prp), (lying, vbg)]
no you lying","['python', 'pandas', 'nlp', 'pos-tagger']",73976396,"if pos_token is a list values then try this;
df = pd.dataframe({""pos_token"":[[(""no"", ""dt""), (""you"", ""prp""), (""lying"", ""vbg"")]]})

df[""sentence""] = df[""pos_token""].apply(lambda x: "" "".join([i[0] for i in x]))

#  output
                              pos_token      sentence
0  [(no, dt), (you, prp), (lying, vbg)]  no you lying",https://stackoverflow.com/questions/73976285,python,06-10-2022 15:20,58.0,0.0,1.0,True,06-10-2022 15:27,06-10-2022 15:22
48199353,how to use spacy in large dataset with short sentences efficiently?,"i choose spacy to process kinds of text because of the performance of it's lemmatation compared with nltk. but when i process millions short text, it always consumed all of my memory(32g) and crashed. without it just a few minutes and less than 10g mem is consumed. 
is something wrong with the usage of this method? is there any better solution to improve the performance? thanks!
def tokenizer(text):
    try:
        tokens = [ word for sent in sent_tokenize(text) for word in word_tokenize(sent)]
        tokens = list(filter(lambda t: t.lower() not in stop_words, tokens))
        tokens = list(filter(lambda t: t not in punctuation, tokens))
        tokens = list(filter(lambda t: len(t) > 4, tokens))
        filtered_tokens = []
        for token in tokens:
            if re.search('[a-za-z]', token):
                filtered_tokens.append(token)

        spacy_parsed = nlp(' '.join(filtered_tokens))
        filtered_tokens = [token.lemma_ for token in spacy_parsed]
        return filtered_tokens
    except exception as e:
        raise e

dask parrallel computing
ddata = dd.from_pandas(res, npartitions=50)
def dask_tokenizer(df):
    df['text_token'] = df['text'].map(tokenizer)
    return df
%time res_final = ddata.map_partitions(dask_tokenizer).compute(get=get)

info about spacy
spacy version      2.0.5          
location           /opt/conda/lib/python3.6/site-packages/spacy
platform           linux-4.4.0-103-generic-x86_64-with-debian-stretch-sid
python version     3.6.3          
models             en, en_default","['python', 'nlp', 'spacy']",48213129,"you can use multithreading in spacy to create a fast tokenization and data ingestion pipeline.
rewriting your code block and functionality using the nlp.pipe method would look something like this:
import spacy
nlp = spacy.load('en')

docs = df['text'].tolist()

def token_filter(token):
    return not (token.is_punct | token.is_space | token.is_stop | len(token.text) <= 4)

filtered_tokens = []
for doc in nlp.pipe(docs):
    tokens = [token.lemma_ for token in doc if token_filter(token)]
    filtered_tokens.append(tokens)

this way puts all your filtering into the token_filter function, which takes in a spacy token and returns true only if it is not punctuation, a space, a stopword, and 4 or less characters. then, you use this function as you pass through each token in each document, where it will return the lemma only if it meets all of those conditions. then, filtered_tokens is a list of your tokenized documents.
some helpful references for customizing this pipeline would be:

token attributes
language.pipe",https://stackoverflow.com/questions/48199353,python,11-01-2018 03:28,8840.0,8.0,3.0,True,21-11-2021 22:31,11-01-2018 13:34
67286034,tokenizing a dataframe using tensorflow and transformers,"i have a labeled dataset in a pandas dataframe.
>>> df.dtypes
title          object
headline       object
byline         object
dateline       object
text           object
copyright    category
country      category
industry     category
topic        category
file           object
dtype: object

i am building a model to predict topic based on text. while text is a large string, topic is a list of strings. for example:
>>> df['topic'].head(5)
0    ['economic performance', 'economics', 'equity ...
1      ['capacity/facilities', 'corporate/industrial']
2    ['performance', 'accounts/earnings', 'corporat...
3    ['performance', 'accounts/earnings', 'corporat...
4    ['strategy/plans', 'new products/services', 'c...

before i put this through a model, i have to tokenize this whole dataframe, yet when running it through transformer's autotokenizer i get getting an error.
import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import autotokenizer
import tensorflow_hub as hub
import tensorflow_text as text
from sklearn.model_selection import train_test_split

def preprocess_text(df):

    # remove punctuations and numbers
    df['text'] = df['text'].str.replace('[^a-za-z]', ' ', regex=true)

    # single character removal
    df['text'] = df['text'].str.replace(r""\s+[a-za-z]\s+"", ' ', regex=true)

    # removing multiple spaces
    df['text'] = df['text'].str.replace(r'\s+', ' ', regex=true)

    # remove nans
    df['text'] = df['text'].fillna('')
    df['topic'] = df['topic'].cat.add_categories('').fillna('')

    return df

# load tokenizer and logger
tf.get_logger().setlevel('error')
tokenizer = autotokenizer.from_pretrained('roberta-large')

# load dataframe with just text and topic columns
# only loading first 100 rows for testing purposes
df = pd.dataframe()
for chunk in pd.read_csv(r'reuters\test.csv', sep='|', chunksize=100,
                dtype={'topic': 'category', 'country': 'category', 'industry': 'category', 'copyright': 'category'}):
    df = chunk
    break
df = preprocess_text(df)

# split dataset into train, test, val (70, 15, 15)
train, test = train_test_split(df, test_size=0.15)
train, val = train_test_split(train, test_size=0.15)

# tokenize datasets
train = tokenizer(train, return_tensors='tf', truncation=true, padding=true, max_length=128)
val = tokenizer(val, return_tensors='tf', truncation=true, padding=true, max_length=128)
test = tokenizer(test, return_tensors='tf', truncation=true, padding=true, max_length=128)

i get this error:
assertionerror: text input must of type `str` (single example), `list[str]` (batch or single pretokenized example) or `list[list[str]]` (batch of pretokenized examples).

on the line train = tokenizer(train, return_tensors='tf', truncation=true, padding=true, max_length=128).
does this mean i have to turn my df into a list?","['python', 'dataframe', 'tensorflow', 'tokenize', 'huggingface-transformers']",67303291,"in short, yes. you also don't want to tokenize the entire, but just a numpy array of the text column. the steps missing are shown below.
# create new index
train_idx = [i for i in range(len(train.index))]
test_idx = [i for i in range(len(test.index))]
val_idx = [i for i in range(len(val.index))]

# convert to numpy
x_train = train['text'].values[train_idx]
x_test = test['text'].values[test_idx]
x_val = val['text'].values[val_idx]

y_train = train['topic_encoded'].values[train_idx]
y_test = test['topic_encoded'].values[test_idx]
y_val = val['topic_encoded'].values[val_idx]

# tokenize datasets
tr_tok = tokenizer(list(x_train), return_tensors='tf', truncation=true, padding=true, max_length=128)
val_tok = tokenizer(list(x_val), return_tensors='tf', truncation=true, padding=true, max_length=128)
test_tok = tokenizer(list(x_test), return_tensors='tf', truncation=true, padding=true, max_length=128)",https://stackoverflow.com/questions/67286034,python,27-04-2021 15:25,4885.0,3.0,1.0,True,30-01-2023 06:16,30-01-2023 06:16
69293878,calibrated classifier valueerror: could not convert string to float,"dataframe:
id    review                                              name         label
1     it is a great product for turning lights on.        ashley       
2     plays music and have a good sound.                  alex        
3     i love it, lots of fun.                             peter        

i want to use probabilistic classifier (linear_svc) to predict labels (probability of 1) based on review. my code:
from sklearn.svm import linearsvc
from sklearn.calibration import calibratedclassifiercv
from sklearn import datasets

#load  dataset
x = training['review']
y = training['label']

linear_svc = linearsvc()     #the base estimator

# this is the calibrated classifier which can give probabilistic classifier
calibrated_svc = calibratedclassifiercv(linear_svc,
                                        method='sigmoid',  #sigmoid will use platt's scaling. refer to documentation for other methods.
                                        cv=3) 
calibrated_svc.fit(x, y)


# predict
prediction_data = predict_data['review']
predicted_probs = calibrated_svc.predict_proba(prediction_data)

it gives following error on calibrated_svc.fit(x, y):

valueerror: could not convert string to float: 'it is a great product
for turning...'

i would appreciate your help.","['scikit-learn', 'text-classification', 'valueerror']",69300513,"svm models cannot handle text data directly. you need to extract some numeric features from the text first. i recommend reading some content on nlp such as bag of words and tf-idf. in any case, for the example you're suggesting, a functional minimal pipeline would be:
from sklearn.calibration import calibratedclassifiercv
from sklearn import datasets
from sklearn.pipeline import make_pipeline
from sklearn.feature_extraction.text import tfidfvectorizer

#load  dataset
x = training['review']
y = training['label']

linear_svc = make_pipeline(tfidfvectorizer(), linearsvc())

# this is the calibrated classifier which can give probabilistic classifier
calibrated_svc = calibratedclassifiercv(linear_svc,
                                        method='sigmoid',
                                        cv=3) 
calibrated_svc.fit(x, y)


# predict
prediction_data = predict_data['review']
predicted_probs = calibrated_svc.predict_proba(prediction_data)

you probably also want to clean the text a bit by removing special characters, lowercasing, stemming, etc. take a look at spacy the library for text-processing.",https://stackoverflow.com/questions/69293878,scikit-learn,23-09-2021 04:01,333.0,2.0,2.0,True,26-09-2021 03:41,26-09-2021 03:41
71166789,huggingface: valueerror: expected sequence of length 165 at dim 1 (got 128),"i am trying to fine-tune the bert language model on my own data. i've gone through their docs, but their tasks seem to be not quite what i need, since my end goal is embedding text. here's my code:
from datasets import load_dataset
from transformers import berttokenizerfast, automodel, trainingarguments, trainer
import glob
import os


base_path = '../data/'
model_name = 'bert-base-uncased'
max_length = 512
checkpoints_dir = 'checkpoints'

tokenizer = berttokenizerfast.from_pretrained(model_name, do_lower_case=true)


def tokenize_function(examples):
    return tokenizer(examples['text'], padding=true, truncation=true, max_length=max_length)


dataset = load_dataset('text',
        data_files={
            'train': f'{base_path}train.txt',
            'test': f'{base_path}test.txt',
            'validation': f'{base_path}valid.txt'
        }
)

print('tokenizing data. this may take a while...')
tokenized_dataset = dataset.map(tokenize_function, batched=true)
train_dataset = tokenized_dataset['train']
eval_dataset = tokenized_dataset['test']

model = automodel.from_pretrained(model_name)

training_args = trainingarguments(checkpoints_dir)

print('training the model...')
trainer = trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset)
trainer.train()

i get the following error:
  file ""train_lm_hf.py"", line 44, in <module>
    trainer.train()
...
  file ""/opt/conda/lib/python3.7/site-packages/transformers/data/data_collator.py"", line 130, in torch_default_data_collator
    batch[k] = torch.tensor([f[k] for f in features])
valueerror: expected sequence of length 165 at dim 1 (got 128)

what am i doing wrong?","['python', 'deep-learning', 'pytorch', 'huggingface-transformers', 'bert-language-model']",71232059,"i fixed this solution by changing the tokenize function to:
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=true, max_length=max_length)

(note the padding argument). also, i used a data collator like so:
data_collator = datacollatorforlanguagemodeling(
    tokenizer=tokenizer, mlm=true, mlm_probability=0.15
)
trainer = trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset
)",https://stackoverflow.com/questions/71166789,python,17-02-2022 23:45,26626.0,13.0,2.0,True,19-08-2024 23:22,19-08-2024 22:52
69836422,bert outputs explained,"the keys of the bert encoder's output are default, encoder_outputs, pooled_output and sequence_output
as far as i can know, encoder_outputs are the output of each encoder, pooled_output is the output of the global context and sequence_output is the output context of each token (correct me if i'm wrong please). but what is default? can you give me a more detailed explanation of each one?
this is the link to the encoder","['python', 'tensorflow', 'deep-learning', 'nlp', 'bert-language-model']",69836907,"the tensorflow docs provide a very good explanation to the outputs you are asking about:

the bert models return a map with 3 important keys: pooled_output, sequence_output, encoder_outputs:
pooled_output represents each input sequence as a whole. the shape is
[batch_size, h]. you can think of this as an embedding for the entire
movie review.
sequence_output represents each input token in the context. the shape
is [batch_size, seq_length, h]. you can think of this as a contextual
embedding for every token in the movie review.
encoder_outputs are the
intermediate activations of the l transformer blocks.
outputs[""encoder_outputs""][i] is a tensor of shape [batch_size,
seq_length, 1024] with the outputs of the i-th transformer block, for
0 <= i < l. the last value of the list is equal to sequence_output

here is another interesting discussion on the difference between the pooled_output and sequence_output, if you are interested.
the default output is equal to the pooled_output, which you can confirm here:
import tensorflow as tf
import tensorflow_hub as hub

tfhub_handle_preprocess = '
tfhub_handle_encoder = '

def build_classifier_model(name):
    text_input = tf.keras.layers.input(shape=(), dtype=tf.string, name='features')    
    bert_preprocess_model = hub.keraslayer(tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = bert_preprocess_model(text_input)
    encoder = hub.keraslayer(tfhub_handle_encoder) 
    outputs = encoder(encoder_inputs)
    net = outputs[name]
    return tf.keras.model(text_input, net)

sentence = tf.constant([
""improve the physical fitness of your goldfish by getting him a bicycle""
])

classifier_model = build_classifier_model(name='default')
default_output = classifier_model(sentence)

classifier_model = build_classifier_model(name='pooled_output')
pooled_output = classifier_model(sentence)

print(default_output == pooled_output)",https://stackoverflow.com/questions/69836422,python,04-11-2021 08:41,7800.0,4.0,1.0,True,19-05-2022 09:43,19-05-2022 09:43
78474448,oserror: [model] does not appear to have a file named config.json,"i want to load a huggingface model. the model i want to load has about 150k downloads so i don't think there is any problem with the model itself.
with the both loading codes below i get the same error:
from transformers import automodel
automodel.from_pretrained(""laion/clip-convnext_large_d_320.laion2b-s29b-b131k-ft-soup"")

and
from transformers import clipprocessor, clipmodel
model_id = ""laion/clip-convnext_large_d_320.laion2b-s29b-b131k-ft-soup""
processor = clipprocessor.from_pretrained(model_id)
model = clipmodel.from_pretrained(model_id)

with both i get:
oserror: laion/clip-convnext_large_d_320.laion2b-s29b-b131k-ft-soup does not appear to have a file named preprocessor_config.json. checkout ' for available files.

any help to load the model would be appreciated.","['python', 'machine-learning', 'deep-learning', 'huggingface-transformers', 'huggingface']",78474490,"it seems this model is an openclip only model right now. you can not load it the usual way.
you should first install open_clip with pip install open_clip_torch then use this code:
import open_clip

model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:laion/clip-convnext_large_d_320.laion2b-s29b-b131k-ft-soup')
tokenizer = open_clip.get_tokenizer('hf-hub:laion/clip-convnext_large_d_320.laion2b-s29b-b131k-ft-soup')",https://stackoverflow.com/questions/78474448,python,13-05-2024 19:38,2291.0,0.0,1.0,True,14-05-2024 11:36,14-05-2024 07:54
68383768,problems using spacy tokenizer with special characters,"i'm new to spacy and i'm trying to find some patterns in a text, but i'm having trouble because of the form that tokenization works. for example, i have created the following pattern, trying to find percentage elements like ""0,42%"" using the matcher (it's not exactly what i want, but i'm just practicing for now):
nlp = spacy.load(""pt_core_news_sm"")

matcher = matcher(nlp.vocab)

text = 'total: 1,80%:(comex 1,30% + deriv 0,50%/ativo: 1,17% '

pattern_test =  [{""text"": {""regex"": ""[0-9]+[,.]+[0-9]+[%]""}}]  

text_ = nlp(text)

matcher.add(""pattern test"", [pattern_test] )
result = matcher(text_)

for id_, beg, end in result:
    print(id_)
    print(text_[beg:end])

the thing is that it is returning results like the one below, cause tokenization considers this as only one token:
9844711491635719110
1,80%:(comex
9844711491635719110
0,50%/ativo

i tried using python's .replace() method on the string to replace special characters for blank spaces before tokenizing it, but now when i print the tokenization result it's separating everything like this:
text_adjustment = text.replace("":"", "" "").replace(""("", "" "").replace("")"", "" "").replace(""/"", "" "").replace("";"", "" "").replace(""-"", "" "").replace(""+"", "" "")

print([token for token in text_adjustment])

['t', 'o', 't', 'a', 'l', ' ', ' ', '1', ',', '8', '0', '%', ' ', ' ', 'c', 'o', 'm', 'e', 'x', ' ', '1', ',', '3', '0', '%', ' ', ' ', ' ', 'd', 'e', 'r', 'i', 'v', ' ', '0', ',', '5', '0', '%', ' ', 'a', 't', 'i', 'v', 'o', ' ', ' ', '1', ',', '1', '7', '%', ' ']

i would like the tokenization result to be like that:
['total', '1,80%', 'comex', '1,30%', 'deriv', '0,50%', 'ativo', '1,17%']

is there a better way to do that? i'm using the 'pt_core_news_sm' model, but i can change the language if i want to.
thanks in advance :)","['python', 'nlp', 'spacy', 'tokenize']",68384198,"i suggest using
import re
#...
text = re.sub(r'(\s)([/:()])', r'\1 \2', text)
pattern_test =  [{""text"": {""regex"": r""^\d+[,.]\d+$""}}, {""orth"": ""%""}]

here, (\s)([/:()]) regex is used to match any non-whitespace (capturing it into group 1) and then matching a /, :, ( or ) (capturing it into group 2) and then re.sub inserts a space between these two groups.
the ^\d+[,.]\d+$ regex matches a full token text that contains a float value and the % is the next token text (because the number and % are split into separate tokens by the model).
full python code snippet:
import spacy, re
from spacy.matcher import matcher

#nlp = spacy.load(""pt_core_news_sm"")
nlp = spacy.load(""en_core_web_trf"")
matcher = matcher(nlp.vocab)
text = 'total: 1,80%:(comex 1,30% + deriv 0,50%/ativo: 1,17% '
text = re.sub(r'(\s)([/:()])', r'\1 \2', text)
pattern_test =  [{""text"": {""regex"": ""\d+[,.]\d+""}}, {""orth"": ""%""}]  
text_ = nlp(text)

matcher.add(""pattern test"", [pattern_test] )
result = matcher(text_)

for id_, beg, end in result:
    print(id_)
    print(text_[beg:end])

output:
9844711491635719110
1,80%
9844711491635719110
1,30%
9844711491635719110
0,50%
9844711491635719110
1,17%",https://stackoverflow.com/questions/68383768,python,14-07-2021 19:00,2966.0,4.0,1.0,True,14-07-2021 19:43,14-07-2021 19:24
59955402,getting word-level encodings from sub-word tokens encodings,"i'm looking into using a pretrained bert ('bert-base-uncased') model to extract contextualised word-level encodings from a bunch sentences.
wordpiece tokenisation breaks down some of the words in my input into subword units. possibly a trivial question, but i was wondering what would be the most sensible way to combine output encodings for subword tokens into word-level encodings.
is averaging subword encodings a reasonable way to go? if not, is there any better alternative?","['nlp', 'tokenize', 'bert-language-model', 'huggingface-transformers']",59965083,"intuitively, your problem seems similar to ""how to get a good sentence representation"", with the exception that these days you could also use a classification token of a sentence to get a sentence representation in most transformer-based models. such token is not available for token-level representations, though.
in your case, i think there are a few options but from what i've seen, people most often use either an average or a max value. in other words: take the average of your subword units, or take the max values. averaging is the most intuitive place to start, in my opinion.
note that averages are only just that, an average over a sequence. this implies that it is not super accurate (one high and one low value will have the same mean as two medium values), but it's probably the most straightforward.",https://stackoverflow.com/questions/59955402,nlp,28-01-2020 19:02,580.0,2.0,1.0,True,14-03-2025 00:23,14-03-2025 00:23
67097467,what is &quot;language modeling head&quot; in bertformaskedlm,"i have recently read about bert and want to use bertformaskedlm for fill_mask task. i know about bert architecture. also, as far as i know, bertformaskedlm is built from bert with a language modeling head on top, but i have no idea about what language modeling head means here. can anyone give me a brief explanation.","['nlp', 'bert-language-model', 'huggingface-transformers', 'language-model']",67097860,"the bertformaskedlm, as you have understood correctly uses a language modeling(lm) head .
generally, as well as in this case, lm head is a linear layer having input dimension of hidden state (for bert-base it will be 768) and output dimension of vocabulary size. thus, it maps to hidden state output of bert model to a specific token in the vocabulary. the loss is calculated based on the scores obtained of a given token with respect to the target token.",https://stackoverflow.com/questions/67097467,nlp,14-04-2021 18:48,6948.0,5.0,2.0,True,30-08-2024 10:50,30-08-2024 10:50
76125712,"openai stream response, not working as expected","on my nodejs app, i have set-up a post method that looks like this:
exports.complete = async (req, res, next) => {\
    const {prompt} = req.body
    res.writehead(200, {
      'content-type': 'text/plain',
      'transfer-encoding': 'chunked'
    })
    const result = await openai.createcompletion(
    {
      model: 'text-davinci-003',
      prompt: prompt,
      temperature: 0.6,
      max_tokens: 700,
      stream: true
    },
    { responsetype: 'stream' }
  )

  result.data.on('data', data => {
    const lines = data
      .tostring()
      .split('\n')
      .filter(line => line.trim() !== '')
    for (const line of lines) {
      const message = line.replace(/^data: /, '')
      if (message === '[done]') {
          res.end()
      }
      try {
        const parsed = json.parse(message)
        res.write(parsed.choices[0].text)
      } catch (error) {
        // console.error('could not json parse stream message', message, error)
      }
    }
  })
}

as the openai node sdk doesn't natively support streaming responses, i scrapped this code from a few sources.
to some extent, it is working i guess. but when i make a call to this endpoint from postman and also from command line (using curl), instead of actually getting the response in chunks, i am getting a final response when the entire completion call finishes.
i am not sure what i am doing wrong here. note: this code above is a part of a firebase function call where i have set-up express. i don't think it affects anything, but still mentioning.
edit 1: the curl request
here's my curl request:
curl --location ' \
--header 'content-type: application/json' \
--data '{
    ""message"": ""say two random lines""
}'","['node.js', 'express', 'openai-api']",76125740,"you can refer to the solution posted here: what is the correct way to send a long string by an http stream in expressjs/nestjs?
it might be caused by 2 reasons:

in curl, you will need to add -n flag to buffer the response to receive the response directly
in browser, the data are not returned until at least 1024 bytes (for chrome) is returned, unless you add a x-content-type-options of nosniff",https://stackoverflow.com/questions/76125712,node.js,28-04-2023 01:39,2837.0,-2.0,1.0,True,28-04-2023 01:49,28-04-2023 01:46
76493551,how to detect and tabulate data from an excel file?,"i have an excel file (available at google drive) with data which was saved with a very strange format in order to get printed easily:

and every table repeats daily for over 5 years. i need to analyze this data and tried to get a relational format in order to load it in r/python-like tools and get only 5 columns:




date
client name
test
measurement
tester




01-01-2023
john smith
metabolyte a
0.01
phd. ima gu


01-01-2023
john smith
metabolyte b
10
phd. ima gu


01-01-2023
john smith
pcr
negative
phd. ima gu


01-01-2023
john smith
mutation
+++
phd. ima gu


01-01-2023
albus dumble
pregnant
negative
tech. guiver


01-01-2023
albus dumble
glucose
121
tech. guiver


02-01-2023
mayday june
metabolyte a
0.01
phd. ima gu


02-01-2023
john smith
metabolyte a
0.01
tech. guiver


02-01-2023
john smith
metabolyte b
10
tech. guiver


02-01-2023
john smith
pcr
negative
tech. guiver


02-01-2023
john smith
mutation
+++
tech. guiver




so, in order to get a conversion from non-relational data to relational table i have applied text-mining techniques available at this github repo. but, basically, have converted everything into one column with tidyr::pivot_longer(). is there any optimal function or method to detect and tabulate this kind of data, or should i try to do it with a loop (+843 files)?","['python', 'r', 'text-mining', 'non-relational-database']",76502193,"my attempt is based on fact, that the entries are formatted identically, so we can use kind of 'moving window'.
a <- openxlsx::read.xlsx(xlsxfile = ""/home/sapi/downloads/enero_2023_prueba.xlsx"",
                    colnames = false
                    )

now we have to define data frame for data storage. comments like # [2,2] +0, +1 corresponds to row and column of a (loaded excel).
entry <- data.frame(
  nombre = character(),    # [2,2] +0, +1
  fecha = character(),     # [2,6] +0, +5
  muestra = character(),   # [3,3] +1, +2
  place = character(),     # [3,5] +1, +4
  color = character(),     # [6,3] +4, +2
  aspecto = character(),   # [7,3] +5< +2
  densidad = double(),     # [8,3] +6, +2
  ph = character(),        # [9,3] +7, +2
# ...
  leucocitos = character(),#[19,3] +17, +2
  bacterias = character(), # [6,7] +4, +6
  piocitos = character()   # [7,7] +5, +6
# ...
)

now we have to find all rows with nombre
nombre_rows <- which(a[,""x1""] == ""nombre"")

and use it in loop like:
for (i in 1:length(nombre_rows)) {
  x <- nombre_rows[i]
  nombre_cols <- which(a[x,] == ""nombre"") # the same for columns
  for (j in 1:length(nombre_cols)) {
    y <- nombre_cols[j]

    entry <- data.frame(
      nombre = a[x, y+1],
      fecha = a[x, y+5],
      muestra = a[x+1, y+2],
      place = a[x+1, y+4],
      color = a[x+4, y+2],
      aspecto = a[x+5, y+2],
      densidad = a[x+6, y+2],
      ph = a[x+7, y+2],
      # ...
      leucocitos = a[x+17, y+2],
      bacterias = a[x+4, y+6],
      piocitos = a[x+5, y+6]
      # ...
    ) |> rbind(entry)
  }
}

and finally the data:
head(entry)
#>             nombre   fecha muestra            place      color      aspecto
#> 1      ruano edith 44957.0   orina         cexterna  amarillo   lig.turbio 
#> 2    cunin elvira  44957.0   orina hospï¿½ï¿½talizacion   amarillo       turbio 
#> 3 loachamin maria  44957.0   orina         cexterna  amarillo  transparente
#> 4    manzano raul  44957.0   orina         cexterna  amarillo   lig.turbio 
#> 5    merchan ivan  44957.0   orina      hidratacion anaranjado      turbio 
#> 6   acero anthony  44957.0   orina         cexterna   amarillo  lig.turbio 
#>   densidad  ph leucocitos    bacterias     piocitos
#> 1   1005.0 8.0   negativo            +    1-2/campo
#> 2   1020.0 5.0        +++           ++ campo lleno 
#> 3   1005.0 6.0   negativo ocasionales    1-2/campo 
#> 4   1010.0 7.0   negativo            +   3-7/campo 
#> 5   1015.0 6.0         ++           ++ 50-60/campo 
#> 6   1010.0 5.0   negativo            +   0-2/campo

reprex v2.0.2
you should extend the entry data frame to grab all variables from your data. and then loop it through all excels you have.",https://stackoverflow.com/questions/76493551,python,16-06-2023 21:01,313.0,1.0,1.0,True,18-06-2023 19:21,17-06-2023 20:08
74040832,"group, map &amp; reduce with two different reducer-operators","i have these tuples:
(""t1"",2,""x1""),
(""t1"",2,""x2""),
// ï¿½ï¿½ï¿½ etc

and i want to reduce it to (""t1"", 4, list(""x1"", ""x2"")). how can i do this ?
i did something like .group(_._1).map{case (key,list) => key-> list.map(_._2).reduce(_+_)}
but this is not working, and just sums the numbers without appending the list","['scala', 'mapreduce', 'information-retrieval']",74040993,"with groupmapreduce:
val xs = list(
  (""t1"",40,""x1""),
  (""t1"",2,""x2""),
  (""t2"",58,""x3"")
)

println(xs.groupmapreduce(_._1)
  (e => (e._2, list(e._3)))
  ({ case ((x, y), (z, w)) => (x + z, y ++ w)})
)

with groupby:
val xs = list(
  (""t1"",40,""x1""),
  (""t1"",2,""x2""),
  (""t2"",58,""x3"")
)
println(xs.groupby(_._1)
  .view
  .mapvalues(ys => (ys.view.map(_._2).sum, ys.map(_._3)))
  .tomap
)

if you want to do it in one pass per list, and not use ++ you could try sth. like this:
xs.groupby(_._1)
  .view
  .mapvalues(ys =>
     ys.foldright((0, list.empty[string])){
       case ((_, n, x), (sum, acc)) => (n + sum, x :: acc)
     }
  )
  .tomap

all three variants give
map(t2 -> (58,list(x3)), t1 -> (42,list(x1, x2)))

note that combining many lists with ++ might become very inefficient if the number of lists becomes large. it depends on your use-case whether this is acceptable or not.",https://stackoverflow.com/questions/74040832,scala,12-10-2022 11:08,131.0,1.0,3.0,True,12-10-2022 15:10,12-10-2022 11:40
67772795,text to image generation using torch model/path file,"i trained a text to image generation model based on  now i have 2 path files (one for generator , another for discriminator) . how to generate images using this path files?","['deep-learning', 'nlp', 'computer-vision', 'pytorch', 'generative-adversarial-network']",67772852,"you need to pass your generator path file here.  self.generator.load_state_dict(torch.load(pre_trained_gen))
refer line 28 of trainer.py",https://stackoverflow.com/questions/67772795,deep-learning,31-05-2021 11:22,414.0,1.0,1.0,True,06-06-2021 06:40,31-05-2021 12:38
77581888,faiss embeddings cannot be saved because of langchain import error,"i am following this tutorial
i am using a sample pdf from here
but i replaced openai with huggingface for the embeddings
below is my code:
import os
import pickle
from pprint import pprint
from pypdf2 import pdfreader
from langchain.vectorstores import faiss
from langchain.embeddings import huggingfaceembeddings
from langchain.text_splitter import recursivecharactertextsplitter

pdf = 'sample.pdf'
pdf_reader = pdfreader(pdf)
text = ''
for page in pdf_reader.pages:
    text += page.extract_text()
pprint(text)

text_splitter = recursivecharactertextsplitter(
    chunk_size = 100,
    chunk_overlap = 20,
    length_function = len
)
chunks = text_splitter.split_text(text = text)

embeddings = huggingfaceembeddings()
vectorstore = faiss.from_texts(chunks, embedding = embeddings)
with open('sample.pkl', 'wb') as f:
    pickle.dump(vectorstore, f)

when i check the sample.pkl file, all i see is this line: no module named 'langchain'
i also checked the embeddings without saving it to the file, and i can see them
im using jupyter notebook, with python 3.9.16, and i have all the libraries installed. and yes i do have langchain installed. i wouldnt be able to import faiss or huggingfaceembeddings or recursivecharactertextsplitter without it","['pdf', 'langchain', 'huggingface', 'large-language-model', 'faiss']",77602729,"so i tried it in another computer with the same environment, and had the same error.
turns out i was running the jupyter notebook and python script in python 3.9.16, but the .pkl file that was being saved was using the default computer version of python 3.11.1.
so all i had to do was install all the required libraries on my default 3.11.1 version, and everything worked perfectly",https://stackoverflow.com/questions/77581888,pdf,30-11-2023 21:44,2132.0,0.0,2.0,True,04-12-2023 21:43,04-12-2023 21:38
58971014,how to get spacy to use universal dependencies,"spacy's site said they use universal dependencies scheme in their annotations specifications page. but when i parse ""i love you"", '''you''' was made a ""dobj"" of ""love"". there's no ""dobj"" in the universal dependency relations doc. so i have two questions : 

how to get spacy to use the universal dependency relations?
how to get the doc for the relations spacy uses?","['nlp', 'spacy', 'dependency-parsing']",58980227,"spacy's provided models don't use ud dependencies for english or german. from the docs, where you can find tables for the dependency labels (

the individual labels are language-specific and depend on the training corpus.

for most other models / languages, ud dependencies are used.",https://stackoverflow.com/questions/58971014,nlp,21-11-2019 09:06,1996.0,2.0,3.0,True,17-06-2024 23:38,21-11-2019 11:04
70579115,"spacy - problem during the training of my model, it seems to block at epoch 0",i am training my named entity recogniser but i have the impression that it blocks at epoch 0. i have already done several trainings and i have never had this problem. does anyone have any tips? i am attaching a screenshot of my terminal. many thanks!!,"['python-3.x', 'spacy', 'named-entity-recognition']",70588632,"it's likely you just have too much data and your training is slow.
how much data do you have? how much ram? what does spacy debug data show?",https://stackoverflow.com/questions/70579115,python-3.x,04-01-2022 12:50,702.0,1.0,1.0,True,30-11-2022 17:09,04-01-2022 12:54
64158898,what does keras tokenizer num_words specify?,"given this piece of code:
from tensorflow.keras.preprocessing.text import tokenizer

sentences = [
    'i love my dog',
    'i, love my cat',
    'you love my dog!'
]

tokenizer = tokenizer(num_words = 1)
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
print(word_index)

whether num_words=1 or num_words=100, i get the same output when i run this cell on my jupyter notebook, and i can't seem to understand what difference it makes in tokenization.

{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}","['python', 'tensorflow', 'machine-learning', 'keras', 'nlp']",64158991,"word_index it's simply a mapping of words to ids for the entire text corpus passed whatever the num_words is
the difference is evident in the usage. for example, if we call texts_to_sequences
sentences = [
    'i love my dog',
    'i, love my cat',
    'you love my dog!'
]

tokenizer = tokenizer(num_words = 1+1)
tokenizer.fit_on_texts(sentences)
tokenizer.texts_to_sequences(sentences) # [[1], [1], [1]]

only the love id is returned because the most frequent word
instead
sentences = [
    'i love my dog',
    'i, love my cat',
    'you love my dog!'
]

tokenizer = tokenizer(num_words = 100+1)
tokenizer.fit_on_texts(sentences)
tokenizer.texts_to_sequences(sentences) # [[3, 1, 2, 4], [3, 1, 2, 5], [6, 1, 2, 4]]

the ids of the most 100 frequent words is returned",https://stackoverflow.com/questions/64158898,python,01-10-2020 15:49,9472.0,10.0,1.0,True,08-02-2022 17:18,08-02-2022 17:18
76199989,problem with custom metric for custom t5 model,"i have created a custom dataset and trained on it a custom t5forconditionalgeneration model that predicts solutions to quadratic equations like this:
input: ""4*x^2 + 4*x + 1""
output: d = 4 ^ 2 - 4 * 4 * 1 4 * 1 4 * 1 4 * 1 4 * 1 4
i need to get accuracy for this model but i get only loss when i use trainer so i used a custom metric function (i didn't write it but took it from a similar project):
def compute_metrics4token(eval_pred):
    batch_size = 4
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=true)
    # replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=true)
    # rouge expects a newline after each sentence
    decoded_preds =  [""\n"".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels =  [""\n"".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]
    answer_accuracy = []
    token_accuracy = []
    num_correct, num_total = 0, 0
    num_answer = 0
    number_eq = 0
    for p, l in zip(decoded_preds, decoded_labels):
        text_pred = p.split(' ')
        text_labels = l.split(' ')
        m = min(len(text_pred), len(text_labels))
        if np.array_equal(text_pred, text_labels):
            num_answer += 1
        for i, j in zip(text_pred, text_labels):
            if i == j:
                num_correct += 1
        num_total += len(text_labels)
        number_eq += 1
    token_accuracy = num_correct / num_total
    answer_accuracy = num_answer / number_eq
    result = {'token_acc': token_accuracy, 'answer_acc': answer_accuracy}
    result = {key: value for key, value in result.items()}
    for key, value in result.items():
        wandb.log({key: value})        
    return {k: round(v, 4) for k, v in result.items()}

problem is that it doesn't work and i don't really understand why and what can i do to get accuracy for my model.
i get this error when i use the function:
args = seq2seqtrainingarguments(
    output_dir='./',
    num_train_epochs=10,
    overwrite_output_dir = true,
    evaluation_strategy = 'steps',         
    learning_rate = 1e-4,                 
    logging_steps = 100,                    
    eval_steps = 100,                      
    save_steps = 100,
    load_best_model_at_end = true,
    push_to_hub=true, 
    weight_decay = 0.01,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=4
    )

trainer = seq2seqtrainer(model=model, train_dataset=train_dataset, eval_dataset=eval_dataset, args=args, 
                  data_collator=data_collator, tokenizer=tokenizer, compute_metrics=compute_metrics4token)

<ipython-input-48-ff7980f6dd66> in compute_metrics4token(eval_pred)
      4     # predictions = np.argmax(logits[0])
      5     # print(predictions)
----> 6     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=true)
      7     # replace -100 in the labels as we can't decode them.
      8     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py in batch_decode(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)
   3444             `list[str]`: the list of decoded sentences.
   3445         """"""
-> 3446         return [
   3447             self.decode(
   3448                 seq,

/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py in <listcomp>(.0)
   3445         """"""
   3446         return [
-> 3447             self.decode(
   3448                 seq,
   3449                 skip_special_tokens=skip_special_tokens,

/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py in decode(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)
   3484         token_ids = to_py_obj(token_ids)
   3485 
-> 3486         return self._decode(
   3487             token_ids=token_ids,
   3488             skip_special_tokens=skip_special_tokens,

/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py in _decode(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)
    547         if isinstance(token_ids, int):
    548             token_ids = [token_ids]
--> 549         text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)
    550 
    551         clean_up_tokenization_spaces = (

typeerror: argument 'ids': 'list' object cannot be interpreted as an integer

when i print out predictions i get a tuple:
(array([[[-32.777344, -34.593437, -36.065685, ..., -34.78577 ,
         -34.77546 , -34.061115],
        [-58.633934, -32.23472 , -31.735909, ..., -40.335655,
         -40.28701 , -37.208904],
        [-56.650974, -33.564095, -34.409576, ..., -36.94467 ,
         -43.246735, -37.469246],
        ...,
        [-56.62741 , -24.561722, -34.11228 , ..., -35.34798 ,
         -42.287125, -38.889412],
        [-56.632545, -24.470266, -34.0792  , ..., -35.313175,
         -42.235626, -38.891712],
        [-56.687027, -24.391508, -34.12526 , ..., -35.30828 ,
         -42.204193, -38.88395 ]],

       [[-29.79866 , -32.22621 , -32.689865, ..., -32.106445,
         -31.46681 , -31.706667],
        [-62.101192, -33.327423, -30.900173, ..., -38.046883,
         -42.26345 , -38.97748 ],
        [-54.726807, -29.13115 , -30.294558, ..., -28.370876,
         -41.23722 , -37.91609 ],
        ...,
        [-57.279373, -23.954525, -34.066246, ..., -35.047447,
         -41.599922, -38.489853],
        [-57.31298 , -23.879845, -34.0837  , ..., -35.03614 ,
         -41.557755, -38.530064],
        [-57.39132 , -23.831306, -34.120094, ..., -35.039547,
         -41.525337, -38.55728 ]],

       [[-29.858566, -32.452713, -34.05892 , ..., -33.93065 ,
         -32.109177, -32.874695],
        [-61.375793, -33.656853, -32.95248 , ..., -42.28087 ,
         -42.637173, -39.21142 ],
        [-58.43721 , -32.496166, -36.44046 , ..., -39.33864 ,
         -42.139664, -38.695328],
        ...,
        [-59.654663, -24.117435, -34.266438, ..., -35.734142,
         -40.55384 , -38.467537],
        [-38.54418 , -18.533113, -29.775307, ..., -26.856483,
         -33.07976 , -29.934727],
        [-27.716005, -14.610603, -23.752686, ..., -21.140053,
         -26.855148, -24.429493]],

       ...,

       [[-33.252697, -34.72487 , -36.395184, ..., -36.87368 ,
         -35.207897, -34.468285],
        [-59.911736, -32.730076, -32.622803, ..., -43.382267,
         -42.25615 , -38.35135 ],
        [-54.982887, -31.847572, -32.773827, ..., -38.500675,
         -43.97969 , -37.41088 ],
        ...,
        [-56.896988, -23.213766, -34.04734 , ..., -35.88832 ,
         -42.176086, -38.953568],
        [-56.994152, -23.141619, -34.054848, ..., -35.875816,
         -42.176453, -38.97729 ],
        [-57.076714, -23.05831 , -34.048904, ..., -35.888298,
         -42.165287, -39.020435]],

       [[-30.070187, -32.049232, -34.63928 , ..., -35.02118 ,
         -32.14465 , -32.891876],
        [-61.720093, -32.994057, -32.988144, ..., -42.054638,
         -42.18583 , -38.990112],
        [-57.74364 , -31.431454, -35.969643, ..., -38.593002,
         -42.276768, -38.895355],
        ...,
        [-58.677704, -23.567434, -35.6751  , ..., -36.018696,
         -40.343582, -38.681267],
        [-58.682228, -23.563087, -35.668964, ..., -36.019753,
         -40.336178, -38.67661 ],
        [-58.718002, -23.609531, -35.67758 , ..., -36.001644,
         -40.366055, -38.67864 ]],

       [[-30.320919, -33.430378, -34.84311 , ..., -37.259563,
         -32.59662 , -33.03912 ],
        [-61.275875, -34.824192, -34.07767 , ..., -44.637024,
         -41.718002, -38.974827],
        [-54.49349 , -30.689342, -35.539658, ..., -39.984665,
         -39.87059 , -37.038437],
        ...,
        [-58.939384, -23.831846, -34.525368, ..., -35.930893,
         -40.29633 , -37.637936],
        [-58.95117 , -23.824234, -34.520042, ..., -35.931396,
         -40.297188, -37.636852],
        [-58.966076, -23.795956, -34.519627, ..., -35.901787,
         -40.261116, -37.612514]]], dtype=float32), array([[[-1.43104442e-03, -2.98473001e-01,  9.49775204e-02, ...,
         -1.77978892e-02,  1.79805323e-01,  1.33578405e-01],
        [-2.35560730e-01,  1.53045550e-01,  5.15255742e-02, ...,
         -1.57466665e-01,  3.49459350e-01,  7.28092641e-02],
        [ 1.60562042e-02, -1.40354022e-01,  5.29232398e-02, ...,
         -2.38162443e-01, -7.72500336e-02,  6.80136457e-02],
        ...,
        [ 7.33550191e-02, -3.35853845e-01,  2.25579832e-03, ...,
         -1.93636306e-02,  1.08121082e-01,  5.24416938e-02],
        [ 8.32231194e-02, -3.11688155e-01, -2.13681534e-02, ...,
          3.23344418e-03,  1.08062990e-01,  7.20862746e-02],
        [ 9.58326831e-02, -3.00361574e-01, -3.02627794e-02, ...,
          3.01265554e-03,  1.20107472e-01,  9.56629887e-02]],

       [[-1.16950013e-01, -3.43173921e-01,  1.87818244e-01, ...,
         -2.71256089e-01,  7.42092952e-02,  5.77520356e-02],
        [-1.62564963e-01, -3.87467295e-01,  1.71134964e-01, ...,
         -7.83916116e-02, -3.65173034e-02,  2.08234787e-01],
        [-3.71523261e-01, -8.74521434e-02,  1.39187068e-01, ...,
         -3.08779895e-01,  3.88156146e-01,  9.99216512e-02],
        ...,
        [ 2.14628279e-02, -3.35561454e-01, -3.76663893e-03, ...,
         -1.29795140e-02,  1.44181430e-01,  1.15508482e-01],
        [ 3.47745977e-02, -3.30934107e-01,  1.10013550e-02, ...,
         -1.84394475e-02,  1.52143195e-01,  1.38157398e-01],
        [ 3.02720107e-02, -3.37626845e-01,  1.35379741e-02, ...,
         -3.80427912e-02,  1.50906458e-01,  1.38765752e-01]],

       [[-6.50129542e-02, -2.63762653e-01,  2.16862872e-01, ...,
         -1.66922837e-01,  1.09285273e-01, -6.40013069e-02],
        [-5.23199737e-01, -2.32228413e-01,  1.44963071e-01, ...,
         -1.41557693e-01,  1.90811172e-01, -2.22496167e-01],
        [-2.24985227e-01, -3.69372189e-01,  7.32450858e-02, ...,
          6.57786876e-02,  9.70033705e-02,  7.83021152e-02],
        ...,
        [-1.93579309e-03, -3.92921537e-01, -1.28203649e-02, ...,
         -8.74079913e-02,  1.13596492e-01,  9.25250202e-02],
        [ 4.55581211e-03, -3.65802884e-01, -2.60831695e-02, ...,
         -4.12549600e-02,  1.17429778e-01,  1.05997331e-01],
        [ 2.46201381e-02, -3.47863257e-01, -4.48134281e-02, ...,
         -2.53352951e-02,  1.16753690e-01,  1.36296600e-01]],

       ...,

       [[-6.47678748e-02, -3.45555365e-01,  7.19114989e-02, ...,
         -9.16809738e-02,  2.15520635e-01,  1.01671875e-01],
        [-7.61077851e-02, -1.51827012e-03,  9.52102616e-02, ...,
         -1.39335945e-01,  1.05894208e-01,  3.23191588e-03],
        [-3.24888170e-01, -2.17741728e-03,  5.32661797e-03, ...,
         -2.78430730e-01,  3.59415114e-01,  1.19439401e-01],
        ...,
        [ 6.89201057e-02, -3.63149673e-01,  7.96841756e-02, ...,
         -3.25191446e-04,  1.26513481e-01,  1.36511743e-01],
        [ 8.16355348e-02, -3.54205281e-01,  7.69739375e-02, ...,
         -2.90949806e-03,  1.31863236e-01,  1.56503588e-01],
        [ 8.36645439e-02, -3.38536322e-01,  8.00612345e-02, ...,
         -9.39210225e-03,  1.29102767e-01,  1.64855778e-01]],

       [[-1.63163885e-01, -3.34902078e-01,  1.11728966e-01, ...,
         -1.10363133e-01,  1.19786285e-01, -9.18702483e-02],
        [-3.36889774e-01, -3.34888607e-01,  1.30680993e-01, ...,
          1.22191897e-03,  1.45059675e-01, -1.27688542e-01],
        [-5.92090450e-02, -2.07585752e-01,  2.05589265e-01, ...,
         -6.80094585e-02,  2.11224273e-01,  3.92790437e-01],
        ...,
        [ 4.86238785e-02, -4.19503808e-01, -3.39424387e-02, ...,
         -1.76134892e-02,  1.00283481e-01,  1.38210282e-01],
        [ 5.81516996e-02, -4.04477298e-01, -4.19086292e-02, ...,
         -1.02474755e-02,  1.06062084e-01,  1.59754634e-01],
        [ 6.70261905e-02, -3.86263877e-01, -4.19785343e-02, ...,
          9.05385148e-03,  1.01594023e-01,  1.69663757e-01]],

       [[-1.22184128e-01, -3.67584258e-01,  3.60302597e-01, ...,
         -4.39502299e-02,  1.33717149e-01,  1.53699834e-02],
        [-3.37780178e-01, -4.05100137e-01,  2.02614054e-01, ...,
         -5.41410968e-02,  1.55447468e-01, -9.28792357e-02],
        [ 1.81227952e-01, -2.29236633e-01,  2.40814224e-01, ...,
          1.39913429e-02,  7.61386827e-02,  3.62152725e-01],
        ...,
        [ 1.47830993e-02, -4.26465064e-01, -1.54972840e-02, ...,
          3.74358669e-02,  1.52016997e-01,  1.53155088e-01],
        [ 3.46656404e-02, -4.00052220e-01, -3.53843644e-02, ...,
          2.64652576e-02,  1.62517026e-01,  1.66649833e-01],
        [ 4.50411513e-02, -3.61773074e-01, -5.50217964e-02, ...,
          3.68298292e-02,  1.67936400e-01,  1.76781893e-01]]],
      dtype=float32))

i thought that maybe i need to take argmax from these values but then i still get errors.
if something is unclear i would be happy to provide additional information. thanks for any help.
edit:
i am adding an example of an item in the dataset:
dataset['test'][0:5]

{'text': ['3*x^2 + 9*x + 6 = 0',
'59*x^2 + -59*x + 14 = 0',
'-10*x^2 + 0*x + 0 = 0',
'3*x^2 + 63*x + 330 = 0',
'1*x^2 + -25*x + 156 = 0'],
'label': ['d = 9^2 - 4 * 3 * 6 = 9; x1 = (-9 + (9)**0.5) // (2 * 3) 
= -1.0; x2 = (-9 - (9)**0.5) // (2 * 3) = -2.0',
'd = -59^2 - 4 * 59 * 14 = 177; x1 = (59 + (177)**0.5) // (2 * 59) 
= 0.0; x2 = (59 - (177)**0.5) // (2 * 59) = 0.0',
'd = 0^2 - 4 * -10 * 0 = 0; x = 0^2 // (2 * -10) = 0',
'd = 63^2 - 4 * 3 * 330 = 9; x1 = (-63 + (9)**0.5) // (2 * 3) = 
-10.0; x2 = (-63 - (9)**0.5) // (2 * 3) = -11.0',
'd = -25^2 - 4 * 1 * 156 = 1; x1 = (25 + (1)**0.5) // (2 * 1) = 
13.0; x2 = (25 - (1)**0.5) // (2 * 1) = 12.0'],
'__index_level_0__': [10803, 14170, 25757, 73733, 25059]}","['python', 'huggingface-transformers', 'pre-trained-model', 'huggingface-datasets', 'large-language-model']",76203775,"it seems like the task you're trying to achieve is some sort of ""translation"" task so the most appropriate model is to use the automodelforseq2seqlm.
and in the case of unspecified sequence, it might be more appropriate to use

bleu / chrf or newer neural-based metrics for translation
rouge for summarization

you can take a look at various translation-related metrics on 

treating it as a normal machine translation task
to read the data, you'll have to make sure that the model's forward function

sees the data point as {""text"": [0, 1, 2, ... ], ""labels"": [0, 9, 8, ...]} in your datasets.dataset object
use the collator to do batch, e.g. datacollatorforseq2seq

and here's a working snippet of how the code (in parts) can be ran: 
data processing part.
from datasets import dataset
import evaluate
from transformers import automodelforseq2seqlm, trainer, autotokenizer, datacollatorforseq2seq

math_data = {'text': ['3*x^2 + 9*x + 6 = 0',
  '59*x^2 + -59*x + 14 = 0',
  '-10*x^2 + 0*x + 0 = 0',
  '3*x^2 + 63*x + 330 = 0',
  '1*x^2 + -25*x + 156 = 0'],
 'target': ['d = 9^2 - 4 * 3 * 6 = 9; x1 = (-9 + (9)**0.5) // (2 * 3)  = -1.0; x2 = (-9 - (9)**0.5) // (2 * 3) = -2.0',
  'd = -59^2 - 4 * 59 * 14 = 177; x1 = (59 + (177)**0.5) // (2 * 59)  = 0.0; x2 = (59 - (177)**0.5) // (2 * 59) = 0.0',
  'd = 0^2 - 4 * -10 * 0 = 0; x = 0^2 // (2 * -10) = 0',
  'd = 63^2 - 4 * 3 * 330 = 9; x1 = (-63 + (9)**0.5) // (2 * 3) =  -10.0; x2 = (-63 - (9)**0.5) // (2 * 3) = -11.0',
  'd = -25^2 - 4 * 1 * 156 = 1; x1 = (25 + (1)**0.5) // (2 * 1) =  13.0; x2 = (25 - (1)**0.5) // (2 * 1) = 12.0']}

math_data_eval = {'text': [""10 + 9x(x+3y) - 3x^3""], ""target"": [""10 + 9x^2 + 27xy - 3x^3""]}

ds_train = dataset.from_dict(math_data)

model = automodelforseq2seqlm.from_pretrained(""t5-small"")
tokenizer = autotokenizer.from_pretrained(""t5-small"")
data_collator = datacollatorforseq2seq(tokenizer)
ds_train = ds_train.map(lambda x: tokenizer(x[""text""], truncation=true, padding=""max_length"", max_length=512)
)
ds_train = ds_train.map(lambda y: 
    {""labels"": tokenizer(y[""target""], truncation=true, padding=""max_length"", max_length=512)['input_ids']}
)

ds_eval = dataset.from_dict(math_data_eval)
ds_eval = ds_eval.map(lambda x: tokenizer(x[""text""], 
    truncation=true, padding=""max_length"", max_length=512))
ds_eval = ds_eval.map(lambda y: 
    {""labels"": tokenizer(y[""target""], truncation=true, padding=""max_length"", max_length=512)['input_ids']}
)


metric definition part.
import numpy as np

metric = evaluate.load(""sacrebleu"")

def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [[label.strip()] for label in labels]
    return preds, labels

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    # replace -100s used for padding as we can't decode them
    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=true)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=true)

    # some simple post-processing
    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    result = {""bleu"": result[""score""]}

    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
    result[""gen_len""] = np.mean(prediction_lens)
    result = {k: round(v, 4) for k, v in result.items()}
    return result


trainer setup part.
from transformers import seq2seqtrainer, seq2seqtrainingarguments


# set training arguments - these params are not really tuned, feel free to change
training_args = seq2seqtrainingarguments(
    output_dir=""./"",
    evaluation_strategy=""steps"",
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    predict_with_generate=true,
    logging_steps=2,  # set to 1000 for full training
    save_steps=16,    # set to 500 for full training
    eval_steps=4,     # set to 8000 for full training
    warmup_steps=1,   # set to 2000 for full training
    max_steps=16,     # delete for full training
    # overwrite_output_dir=true,
    save_total_limit=1,
    #fp16=true, 
)


# instantiate trainer
trainer = seq2seqtrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=ds_train.with_format(""torch""),
    eval_dataset=ds_eval.with_format(""torch""),
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

trainer.train()


that works and good. but why is the output of the model still so bad?

most probably you need to tune some hyperparameter, batch_size, more data, different learning rates or increase no. of max_steps
it can also be that your vocab is pretrained for natural language but your data isn't, in that case, i'll suggest to try modifying the tokenizer before training, e.g. how to add new tokens to an existing huggingface tokenizer?",https://stackoverflow.com/questions/76199989,python,08-05-2023 11:08,911.0,1.0,1.0,True,08-05-2023 21:18,08-05-2023 19:25
64579258,sentence embedding using t5,"i would like to use state-of-the-art lm t5 to get sentence embedding vector.
i found this repository 
as i know, in bert i should take the first token as [cls] token, and it will be the sentence embedding.
in this repository i see the same behaviour on t5 model:
cls_tokens = output_tokens[:, 0, :]  # cls token is first token

does this behaviour correct? i have taken encoder from t5 and encoded two phrases with it:
""i live in the kindergarden""
""yes, i live in the kindergarden""

the cosine similarity between them was only ""0.2420"".
i just need to understand how sentence embedding works - should i train network to find similarity to reach correct results? or i it is enough of base pretrained language model?","['python', 'nlp', 'pytorch', 'word-embedding']",64582738,"in order to obtain the sentence embedding from the t5, you need to take the take the last_hidden_state from the t5 encoder output:
model.encoder(input_ids=s, attention_mask=attn, return_dict=true)
pooled_sentence = output.last_hidden_state # shape is [batch_size, seq_len, hidden_size]
# pooled_sentence will represent the embeddings for each word in the sentence
# you need to sum/average the pooled_sentence
pooled_sentence = torch.mean(pooled_sentence, dim=1)

you have now a sentence embeddings from t5",https://stackoverflow.com/questions/64579258,python,28-10-2020 18:35,7020.0,6.0,1.0,True,10-08-2022 18:02,10-08-2022 18:02
78778988,do i need to use named entity recognition (ner) in tokenization?,"i am working on an nlp project for sentiment analysis. i am using spacy to tokenize sentences. as i was reading the documentation, i learned about ner. i've read that it can be used to extract entities from text for aiding a user's searching.
the thing i am trying to understand is how to embody it (if i should) in my tokenization process. i am giving an example.
text = ""let's not forget that apple pay in 2014 required a brand new iphone in order to use it.  a significant portion of apple's user base wasn't able to use it even if they wanted to.  as each successive iphone incorporated the technology and older iphones were replaced the number of people who could use the technology increased.""

sentence = sp(text) # sp = spacy.load('en_core_web_sm')

for word in sentence:
    print(word.text)

# let
# 's
# not
# forget
# that
# apple
# pay
# in
# etc...

for word in sentence.ents:
  print(word.text + "" _ "" + word.label_ + "" _ "" + str(spacy.explain(word.label_)))

# apple pay _ org _ companies, agencies, institutions, etc.
# 2014 _ date _ absolute or relative dates or periods
# iphone _ org _ companies, agencies, institutions, etc.
# apple _ org _ companies, agencies, institutions, etc.
# iphones _ org _ companies, agencies, institutions, etc.

the first loops shows that 'apple' and 'pay' are different tokens. when printing the discovered entities in the second loop, it understands that 'apply pay' is an org. if yes, how could i achieve that (let's say) ""type"" of tokenization?
my thinking is, shouldn't 'apple' and 'pay' be tokenized as a single word together so that, when i create my classifier it will recognize it as an entity and not recognize a fruit ('apple') and a verb ('pay').","['python', 'python-3.x', 'nlp', 'spacy', 'named-entity-recognition']",78780820,"tokenization typically is the splitting of a sentence into words or even subwords. i am not sure what you later plan to do with the data, but it is a convention in nlp to stick to either the document level, sentence level or word/token level. having some mix of token and n-gram level (like [""apple pay"", ""required"", ""an"", ""iphone"", ""to"", ""use"", ""it"", "".""] in my opinion will not help you in most later use cases.
if you later train a classifier (assuming you're talking about fine-tuning a transformer based language model on a token classification task) would then use something like the iob format to handle n-grams, e.g. like so:



token
label




apple
b


pay
i


required
o


an
o


iphone
b


to
o


use
o


it
o


.
o



of course this depends on your application and directly merging to n-grams might work well for you. if you have some application where you are searching for frequent n-grams, you could use collocation metrics to extract those n-grams, e.g. using nltk's collocationfinder.
or as you mentioned use spacy either for noun chunk extraction or named entity recognition. for the latter one, you could access the token level ent_type_ and ent_iob_ attributes to iterate over the tokens in the processed docs once and then merge these n-grams together based on their iob-tags.",https://stackoverflow.com/questions/78778988,python,22-07-2024 13:28,385.0,0.0,1.0,True,22-07-2024 21:24,22-07-2024 14:41
70698407,huggingface autotokenizer | valueerror: couldn&#39;t instantiate the backend tokenizer,"goal: amend this notebook to work with albert-base-v2 model
error occurs in section 1.3.
kernel: conda_pytorch_p36. i did restart & run all, and refreshed file view in working directory.

there are 3 listed ways this error can be caused. i'm not sure which my case falls under.
section 1.3:
# define the tokenizer
tokenizer = autotokenizer.from_pretrained(
        configs.output_dir, do_lower_case=configs.do_lower_case)

traceback:
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
<ipython-input-25-1f864e3046eb> in <module>
    140 # define the tokenizer
    141 tokenizer = autotokenizer.from_pretrained(
--> 142         configs.output_dir, do_lower_case=configs.do_lower_case)
    143 
    144 # evaluate the original fp32 bert model

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/models/auto/tokenization_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    548             tokenizer_class_py, tokenizer_class_fast = tokenizer_mapping[type(config)]
    549             if tokenizer_class_fast and (use_fast or tokenizer_class_py is none):
--> 550                 return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
    551             else:
    552                 if tokenizer_class_py is not none:

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
   1752             use_auth_token=use_auth_token,
   1753             cache_dir=cache_dir,
-> 1754             **kwargs,
   1755         )
   1756 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)
   1880         # instantiate tokenizer.
   1881         try:
-> 1882             tokenizer = cls(*init_inputs, **init_kwargs)
   1883         except oserror:
   1884             raise oserror(

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/models/albert/tokenization_albert_fast.py in __init__(self, vocab_file, tokenizer_file, do_lower_case, remove_space, keep_accents, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, **kwargs)
    159             cls_token=cls_token,
    160             mask_token=mask_token,
--> 161             **kwargs,
    162         )
    163 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_fast.py in __init__(self, *args, **kwargs)
    116         else:
    117             raise valueerror(
--> 118                 ""couldn't instantiate the backend tokenizer from one of: \n""
    119                 ""(1) a `tokenizers` library serialization file, \n""
    120                 ""(2) a slow tokenizer instance to convert or \n""

valueerror: couldn't instantiate the backend tokenizer from one of: 
(1) a `tokenizers` library serialization file, 
(2) a slow tokenizer instance to convert or 
(3) an equivalent slow tokenizer class to instantiate and convert. 
you need to have sentencepiece installed to convert a slow tokenizer to a fast one.

please let me know if there's anything else i can add to post.","['python', 'tensorflow', 'huggingface-transformers', 'onnx', 'huggingface-tokenizers']",70711877,"first, i had to pip install sentencepiece.
however, in the same code line, i was getting an error with sentencepiece.
wrapping str() around both parameters yielded the same traceback.
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
<ipython-input-12-1f864e3046eb> in <module>
    140 # define the tokenizer
    141 tokenizer = autotokenizer.from_pretrained(
--> 142         configs.output_dir, do_lower_case=configs.do_lower_case)
    143 
    144 # evaluate the original fp32 bert model

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/models/auto/tokenization_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    548             tokenizer_class_py, tokenizer_class_fast = tokenizer_mapping[type(config)]
    549             if tokenizer_class_fast and (use_fast or tokenizer_class_py is none):
--> 550                 return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
    551             else:
    552                 if tokenizer_class_py is not none:

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
   1752             use_auth_token=use_auth_token,
   1753             cache_dir=cache_dir,
-> 1754             **kwargs,
   1755         )
   1756 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)
   1776                 copy.deepcopy(init_configuration),
   1777                 *init_inputs,
-> 1778                 **(copy.deepcopy(kwargs)),
   1779             )
   1780         else:

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)
   1880         # instantiate tokenizer.
   1881         try:
-> 1882             tokenizer = cls(*init_inputs, **init_kwargs)
   1883         except oserror:
   1884             raise oserror(

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/models/albert/tokenization_albert.py in __init__(self, vocab_file, do_lower_case, remove_space, keep_accents, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, sp_model_kwargs, **kwargs)
    179 
    180         self.sp_model = spm.sentencepieceprocessor(**self.sp_model_kwargs)
--> 181         self.sp_model.load(vocab_file)
    182 
    183     @property

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sentencepiece/__init__.py in load(self, model_file, model_proto)
    365       if model_proto:
    366         return self.loadfromserializedproto(model_proto)
--> 367       return self.loadfromfile(model_file)
    368 
    369 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sentencepiece/__init__.py in loadfromfile(self, arg)
    169 
    170     def loadfromfile(self, arg):
--> 171         return _sentencepiece.sentencepieceprocessor_loadfromfile(self, arg)
    172 
    173     def decodeidswithcheck(self, ids):

typeerror: not a string
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
<ipython-input-12-1f864e3046eb> in <module>
    140 # define the tokenizer
    141 tokenizer = autotokenizer.from_pretrained(
--> 142         configs.output_dir, do_lower_case=configs.do_lower_case)
    143 
    144 # evaluate the original fp32 bert model

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/models/auto/tokenization_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    548             tokenizer_class_py, tokenizer_class_fast = tokenizer_mapping[type(config)]
    549             if tokenizer_class_fast and (use_fast or tokenizer_class_py is none):
--> 550                 return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
    551             else:
    552                 if tokenizer_class_py is not none:

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
   1752             use_auth_token=use_auth_token,
   1753             cache_dir=cache_dir,
-> 1754             **kwargs,
   1755         )
   1756 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)
   1776                 copy.deepcopy(init_configuration),
   1777                 *init_inputs,
-> 1778                 **(copy.deepcopy(kwargs)),
   1779             )
   1780         else:

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)
   1880         # instantiate tokenizer.
   1881         try:
-> 1882             tokenizer = cls(*init_inputs, **init_kwargs)
   1883         except oserror:
   1884             raise oserror(

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/models/albert/tokenization_albert.py in __init__(self, vocab_file, do_lower_case, remove_space, keep_accents, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, sp_model_kwargs, **kwargs)
    179 
    180         self.sp_model = spm.sentencepieceprocessor(**self.sp_model_kwargs)
--> 181         self.sp_model.load(vocab_file)
    182 
    183     @property

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sentencepiece/__init__.py in load(self, model_file, model_proto)
    365       if model_proto:
    366         return self.loadfromserializedproto(model_proto)
--> 367       return self.loadfromfile(model_file)
    368 
    369 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sentencepiece/__init__.py in loadfromfile(self, arg)
    169 
    170     def loadfromfile(self, arg):
--> 171         return _sentencepiece.sentencepieceprocessor_loadfromfile(self, arg)
    172 
    173     def decodeidswithcheck(self, ids):

typeerror: not a string

i then had to swap out parameters for just the model name:
tokenizer = alberttokenizer.from_pretrained('albert-base-v2')

this second part is detailed on this so post.",https://stackoverflow.com/questions/70698407,python,13-01-2022 14:37,13957.0,6.0,2.0,True,28-09-2023 10:35,14-01-2022 14:10
56281633,train spacy for text classification,"after reading the docs and doing the tutorial i figured i'd make a small demo. turns out my model does not want to train. here's the code
import spacy
import random
import json

training_data = [
    [""my little kitty is so special"", {""kat"": true}],
    [""dude, totally, yeah, video games"", {""kat"": false}],
    [""should i pay $1,000 for the iphone x?"", {""kat"": false}],
    [""the iphone 8 reviews are here"", {""kat"": false}],
    [""noa is a great cat name."", {""kat"": true}],
    [""we got a new kitten!"", {""kat"": true}]
]

nlp = spacy.blank(""en"")
category = nlp.create_pipe(""textcat"")
nlp.add_pipe(category)
category.add_label(""kat"")

# start the training
nlp.begin_training()

# loop for 10 iterations
for itn in range(100):
    # shuffle the training data
    random.shuffle(training_data)
    losses = {}

    # batch the examples and iterate over them
    for batch in spacy.util.minibatch(training_data, size=2):
        texts = [text for text, entities in batch]
        annotations = [{""textcat"": [entities]} for text, entities in batch]
        nlp.update(texts, annotations, losses=losses)
    if itn % 20 == 0:
        print(losses)

when i run this the output suggests that very little is learned.
{'textcat': 0.0}
{'textcat': 0.0}
{'textcat': 0.0}
{'textcat': 0.0}
{'textcat': 0.0}

this feels wrong. there should be an error or a meaningful tag. the predictions confirm this. 
for text, d in training_data:
    print(text, nlp(text).cats)

# dude, totally, yeah, video games {'kat': 0.45303162932395935}
# the iphone 8 reviews are here {'kat': 0.45303162932395935}
# noa is a great cat name. {'kat': 0.45303162932395935}
# should i pay $1,000 for the iphone x? {'kat': 0.45303162932395935}
# we got a new kitten! {'kat': 0.45303162932395935}
# my little kitty is so special {'kat': 0.45303162932395935}

it feels like my code is missing something but i can't figure out what.","['python', 'spacy']",67943479,"if you update and use spacy 3 - the code above will no longer work. the solution is to migrate with some changes. i've modified the example from cantdutchthis accordingly.
summary of changes:

use the config to change the architecture. the old default was ""bag of words"", the new default is ""text ensemble"" which uses attention. keep this in mind when tuning the models
labels now need to be one-hot encoded
the add_pipe interface has changed slightly
nlp.update now requires an example object rather than a tuple of text, annotation

import spacy
# add imports for example, as well as textcat config...
from spacy.training import example
from spacy.pipeline.textcat import single_label_bow_config, single_label_default_config
from thinc.api import config
import random

# labels should be one-hot encoded
training_data = [
    [""my little kitty is so special"", {""kat0"": true}],
    [""dude, totally, yeah, video games"", {""kat1"": true}],
    [""should i pay $1,000 for the iphone x?"", {""kat1"": true}],
    [""the iphone 8 reviews are here"", {""kat1"": true}],
    [""noa is a great cat name."", {""kat0"": true}],
    [""we got a new kitten!"", {""kat0"": true}]
]


# bow
# config = config().from_str(single_label_bow_config)

# textensemble with attention
config = config().from_str(single_label_default_config)

nlp = spacy.blank(""en"")
# now uses `add_pipe` instead
category = nlp.add_pipe(""textcat"", last=true, config=config)
category.add_label(""kat0"")
category.add_label(""kat1"")


# start the training
nlp.begin_training()

# loop for 10 iterations
for itn in range(100):
    # shuffle the training data
    random.shuffle(training_data)
    losses = {}

    # batch the examples and iterate over them
    for batch in spacy.util.minibatch(training_data, size=4):
        texts = [nlp.make_doc(text) for text, entities in batch]
        annotations = [{""cats"": entities} for text, entities in batch]

        # uses an example object rather than text/annotation tuple
        examples = [example.from_dict(doc, annotation) for doc, annotation in zip(
            texts, annotations
        )]
        nlp.update(examples, losses=losses)
    if itn % 20 == 0:
        print(losses)",https://stackoverflow.com/questions/56281633,python,23-05-2019 19:14,3844.0,7.0,2.0,True,23-11-2021 04:30,24-05-2019 07:31
74707130,need assistance with porterstemmer in python,"i am running this code:
from nltk.tokenize import word_tokenize
from nltk.stem.porter import porterstemmer
word = 'gardening'
tokens = word_tokenize(word.lower())
stemmer = porterstemmer() # write code here
stemmed = [stemmer.stem(token) for token in tokens] # write your code here
print(stemmed)

i'm getting the output ['garden'] but how can i get the output without the brackets and quotes i.e., garden
i'm getting the output ['garden'] but how can i get the output without the brackets and quotes i.e., garden","['python', 'nlp', 'porter-stemmer']",74707285,"you can use normal list indexing to get the result. for example:
from nltk.tokenize import word_tokenize
from nltk.stem.porter import porterstemmer

word = 'gardening'
tokens = word_tokenize(word.lower())
stemmer = porterstemmer() # write code here
stemmed = [stemmer.stem(token) for token in tokens] # write your code here

# print the first item in the list
print(stemmed[0])

output:
garden

or, for longer lists, you could use a for loop to print out each value:
from nltk.tokenize import word_tokenize
from nltk.stem.porter import porterstemmer

word = 'gardening'
tokens = word_tokenize(word.lower())
stemmer = porterstemmer() # write code here
stemmed = [stemmer.stem(token) for token in tokens] # write your code here

for s in stemmed:
    print(s)",https://stackoverflow.com/questions/74707130,python,06-12-2022 17:59,173.0,-1.0,2.0,True,09-10-2023 07:12,09-10-2023 07:12
71755535,huggingface classification struggling with prediction,"i am fine tuning longformer and then making prediction using textclassificationpipeline and model(**inputs) methods. i am not sure why i get different results
import pandas as pd
import datasets
from transformers import longformertokenizerfast, longformerforsequenceclassification, trainer, trainingarguments, longformerconfig
import torch.nn as nn
import torch
from torch.utils.data import dataloader#dataset, 
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from tqdm import tqdm
#import wandb
import os
from datasets import dataset
from transformers import textclassificationpipeline, autotokenizer, automodelforsequenceclassification

tokenizer = longformertokenizerfast.from_pretrained('folder_path/', max_length = maximum_len)

loading the fine tuned model from a saved location. using original tokenizer
saved_location='c:/xyz'
model_saved=automodelforsequenceclassification.from_pretrained(saved_location)
pipe = textclassificationpipeline(model=model_saved, tokenizer=tokenizer, device=0)#tokenizer_saved, padding=true, truncation=true)
prediction = pipe([""the text to predict""], return_all_scores=true)
prediction
[[{'label': 'label_0', 'score': 0.7107483148574829},
  {'label': 'label_1', 'score': 0.2892516553401947}]]

2nd method
inputs = tokenizer(""the text to predict"", return_tensors=""pt"").to(device)
outputs = model_saved(**inputs)#, labels=labels)
print (outputs['logits'])
#tensor([[ 0.4552, -0.4438]], device='cuda:0', grad_fn=<addmmbackward0>)
torch.sigmoid(outputs['logits'])
#tensor([[0.6119, 0.3908]], device='cuda:0', grad_fn=<sigmoidbackward0>)

automodelforsequenceclassification returns probabilities 0.71 and 0.29. when i look at the 2nd method. it returns logits  0.4552, -0.4438 which convert to probabilities 0.6119, 0.3908
#update 1
the first link textclassificationpipeline from cronoik's answer says below
function_to_apply (str, optional, defaults to ""default"") ï¿½ï¿½ï¿½ the function to apply to the model outputs in order to retrieve the scores. accepts four different values:
""default"": if the model has a single label, will apply the sigmoid function on the output. if the model has several labels, will apply the softmax function on the output.
""sigmoid"": applies the sigmoid function on the output.
""softmax"": applies the softmax function on the output.
""none"": does not apply any function on the output.

as this is a binary classification problem (single label) shouldn't it apply sigmoid","['python', 'nlp', 'classification', 'huggingface-transformers']",71758484,"i assume that model.config.num_labels==2, if that is the case, the textclassificationpipeline applies softmax and not sigmoid to calculate the probabilities (code).
import torch

logits = torch.tensor([ 0.4552, -0.4438])
print(torch.softmax(logits,0))

output:
tensor([0.7107, 0.2893])",https://stackoverflow.com/questions/71755535,python,05-04-2022 16:47,2330.0,2.0,1.0,True,05-04-2022 22:06,05-04-2022 22:06
28314337,typeerror: sparse matrix length is ambiguous; use getnnz() or shape[0] while using rf classifier?,"i am learning about random forests in scikit learn and as an example i would like to use random forest classifier for text classification, with my own dataset. so first i vectorized the text with tfidf and for classification:
from sklearn.ensemble import randomforestclassifier
classifier=randomforestclassifier(n_estimators=10) 
classifier.fit(x_train, y_train)           
prediction = classifier.predict(x_test)

when i run the classification i got this:
typeerror: a sparse matrix was passed, but dense data is required. use x.toarray() to convert to a dense numpy array.

then i used the .toarray() for x_train and i got the following:
typeerror: sparse matrix length is ambiguous; use getnnz() or shape[0]

from a previous question as i understood i need to reduce the dimensionality of the numpy array so i do the same:
from sklearn.decomposition.truncated_svd import truncatedsvd        
pca = truncatedsvd(n_components=300)                                
x_reduced_train = pca.fit_transform(x_train)               

from sklearn.ensemble import randomforestclassifier                 
classifier=randomforestclassifier(n_estimators=10)                  
classifier.fit(x_reduced_train, y_train)                            
prediction = classifier.predict(x_testing) 

then i got this exception:
  file ""/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py"", line 419, in predict
    n_samples = len(x)
  file ""/usr/local/lib/python2.7/site-packages/scipy/sparse/base.py"", line 192, in __len__
    raise typeerror(""sparse matrix length is ambiguous; use getnnz()""
typeerror: sparse matrix length is ambiguous; use getnnz() or shape[0]

the i tried the following:
prediction = classifier.predict(x_train.getnnz()) 

and got this:
  file ""/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py"", line 419, in predict
    n_samples = len(x)
typeerror: object of type 'int' has no len()

two questions were raised from this: how can i use random forests to classify correctly? and what's happening with x_train?. 
then i tried the following:
df = pd.read_csv('/path/file.csv',
header=0, sep=',', names=['id', 'text', 'label'])



x = tfidf_vect.fit_transform(df['text'].values)
y = df['label'].values



from sklearn.decomposition.truncated_svd import truncatedsvd
pca = truncatedsvd(n_components=2)
x = pca.fit_transform(x)

a_train, a_test, b_train, b_test = train_test_split(x, y, test_size=0.33, random_state=42)

from sklearn.ensemble import randomforestclassifier

classifier=randomforestclassifier(n_estimators=10)
classifier.fit(a_train, b_train)
prediction = classifier.predict(a_test)

from sklearn.metrics.metrics import precision_score, recall_score, confusion_matrix, classification_report
print '\nscore:', classifier.score(a_train, b_test)
print '\nprecision:', precision_score(b_test, prediction)
print '\nrecall:', recall_score(b_test, prediction)
print '\n confussion matrix:\n',confusion_matrix(b_test, prediction)
print '\n clasification report:\n', classification_report(b_test, prediction)","['python', 'numpy', 'machine-learning', 'nlp', 'scikit-learn']",28314568,"it is a bit unclear if you are passing the same data structure (type and shape) to the fit method and predict method of the classifier. random forests will take a long time to run with a large number of features, hence the suggestion to reduce the dimensionality in the post you link to.
you should apply the svd to both the training and test data so the classifier in trained on the same shaped input as the data you wish to predict for. check the input to the fit, and the input to the predict method have the same number of features, and are both arrays rather than sparse matrices. 
updated with example:
updated to use dataframe
from sklearn.feature_extraction.text import tfidfvectorizer
tfidf_vect= tfidfvectorizer(  use_idf=true, smooth_idf=true, sublinear_tf=false)
from sklearn.cross_validation import train_test_split

df= pd.dataframe({'text':['cat on the','angel eyes has','blue red angel','one two blue','blue whales eat','hot tin roof','angel eyes has','have a cat']\
              ,'class': [0,0,0,1,1,1,0,3]})



x = tfidf_vect.fit_transform(df['text'].values)
y = df['class'].values

from sklearn.decomposition.truncated_svd import truncatedsvd        
pca = truncatedsvd(n_components=2)                                
x_reduced_train = pca.fit_transform(x)  

a_train, a_test, b_train, b_test = train_test_split(x, y, test_size=0.33, random_state=42)

from sklearn.ensemble import randomforestclassifier 

classifier=randomforestclassifier(n_estimators=10)                  
classifier.fit(a_train.toarray(), b_train)                            
prediction = classifier.predict(a_test.toarray()) 

note the svd happens before the split into training and test sets, so that the array passed to the predictor has the same n as the array the fit method is called on.",https://stackoverflow.com/questions/28314337,python,04-02-2015 05:48,57098.0,16.0,3.0,True,10-03-2025 14:56,04-02-2015 07:36
75326344,nlp classification with sparse and numerical features crashes,"i have a dataset of 10 million english shows, which has been cleaned and lemmatized, and their classification into different category types such as comedy, documentary, action, ... etc
i also have a feature called duration, which is the length of the tv show.
data can be found here
i perform tfidf vectorization on the titles, which returns a sparse matrix and normalization on the duration column.
then i want to feed the data to a logistic regression classifier.
side question: i want to know if theres a better way to handle combining a sparse matrix and a numerical column
when i try to do it using todense() or toarray(), it works
when i pass it to the logistic regression function, the notebook crashes. but if i dont have the duration col, which means i dont have to apply the toarray() or todense() function, it works perfectly. is this a memory issue?
this is my code:
import os

import pandas as pd

from sklearn import metrics
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import tfidfvectorizer
from sklearn.preprocessing import minmaxscaler
from sklearn.linear_model import logisticregression

def normalize(df, col = ''):
    mms = minmaxscaler()
    mms_col = mms.fit_transform(df[[col]])
    return mms_col

def tfidf(x, col = ''):
    tfidf_vectorizer = tfidfvectorizer(max_df = 0.8, max_features = 10000)
    return tfidf_vectorizer.fit_transform(x[col])

def get_training_data(df):
    df = shuffle(pd.read_csv(df).dropna())
    data = df[['name_title', 'duration']]

    x_duration = normalize(data, col = 'duration')
    x_sparse = tfidf(data, col = 'name_title')
    x = pd.dataframe(x_sparse.toarray())

    x['duration'] = x_duration
    y = df['target']

    return x, y

def logistic_regression(x, y):
    x_train, x_test, y_train, y_test = train_test_split(x, y)
    lr = logisticregression(c = 100.0, random_state = 1, solver = 'lbfgs', multi_class = 'ovr')
    lr.fit(x_train, y_train)
    y_predict = lr.predict(x_test)
    print(y_predict)
    print(""logistic regression accuracy %.3f"" %metrics.accuracy_score(y_test, y_predict))

data_path = '../data/'
x, y = get_training_data(os.path.join(data_path, 'podcasts_en_processed.csv'))
print(x.shape) # this prints (971426, 10001)
logistic_regression(x, y)","['python', 'nlp', 'sparse-matrix', 'text-classification', 'tf-idf']",77515871,"it seems like you're encountering a memory issue when combining a large sparse matrix from tf-idf vectorization with a dense 'duration' feature. converting a sparse matrix to a dense one with toarray() or todense() dramatically increases memory usage, which is likely causing the crash.
instead of converting the entire sparse matrix, try combining the sparse tf-idf features with the dense 'duration' feature while keeping most of the data in sparse format. use scipy.sparse.hstack for this:
from scipy.sparse import hstack

# combine the sparse and dense features
x = hstack([x_sparse, x_duration])

this method maintains the efficiency of sparse data storage. if you're still facing memory issues, consider reducing the number of features in your tf-idf vectorization [ tfidf_vectorizer = tfidfvectorizer(max_df = 0.8, max_features = 10000) , i think 10000 is a bit too much ] or using incremental learning methods like sgdclassifier with a logistic regression loss. these approaches should help manage the large dataset more effectively.",https://stackoverflow.com/questions/75326344,python,02-02-2023 16:44,85.0,0.0,1.0,True,20-11-2023 12:20,02-02-2023 17:34
42206557,how to find similar words with fasttext?,"i am playing around with fasttext,  is quite similar to word2vec. since it seems to be a pretty new library with not to many built in functions yet, i was wondering how to extract morphological similar words. 
for eg: model.similar_word(""dog"") -> dogs. but there is no function built-in.
if i type 
model[""dog""] 
i only get the vector, that might be used to compare cosine similarity.
model.cosine_similarity(model[""dog""], model[""dogs""]]). 
do i have to make some sort of loop and do cosine_similarity on all possible pairs in a text? that would take time ...!!!","['python', 'nlp', 'word2vec', 'fasttext']",42257492,"use gensim, load fasttext trained .vec file with load.word2vec models and use most_similiar() method to find similar words!",https://stackoverflow.com/questions/42206557,python,13-02-2017 14:33,27609.0,15.0,6.0,True,07-04-2022 10:16,12-11-2019 07:59
77333202,reverse index in sql,"in this assignment, you will create a table of documents and then produce a reverse index for those documents that identifies each document which contains a particular word using sql.
fyi: in contrast with the provided sample sql, you will map all the words in the reverse index to lower case (i.e. python, python, and python should all end up as ""python"" in the inverted index).
create table docs01 (id serial, doc text, primary key(id));

create table invert01 (
  keyword text,
  doc_id integer references docs01(id) on delete cascade
);

here are the one-line documents that you are to insert into docs01:
insert into docs01 (doc) values
('the building blocks of programs'),
('in the next few chapters we will learn more about the vocabulary'),
('sentence structure paragraph structure and story structure of python'),
('we will learn about the powerful capabilities of python and how to'),
('compose those capabilities together to create useful programs'),
('there are some lowlevel conceptual patterns that we use to construct'),
('programs these constructs are not just for python programs they are'),
('part of every programming language from machine language up to the'),
('file or even some kind of sensor like a microphone or gps in our'),
('initial programs our input will come from the user typing data on');

here is a sample for the first few expected rows of your reverse index:
select keyword, doc_id from invert01 order by keyword, doc_id limit 10;





keyword
doc_id




a
9


about
2


about
4


and
3


and
4


are
6


are
7


blocks
1


building
1


capabilities
4




insert into invert01 (keyword, doc_id)
select
    lower(keyword) as keyword,
    doc_id
from (
    select
        id as doc_id,
        unnest(string_to_array(doc, ' ')) as keyword
    from docs01
) as words;

i tried this but error is showing that ""keyword 'are' should be in 2 documents and was only in 3 documents"".
please help me to find the error.","['sql', 'json', 'database', 'postgresql', 'nlp']",77339809,"as already pointed out, each doc with multiple instances of a keyword, like number 7:

('programs these constructs are not just for python programs they are'),

causes your query to produce a (keyword,doc_id) pairing multiple times:




keyword
doc_id




are
6


are
7


are
7




because a select is by default a select all that accepts duplicates. you can prevent that by switching to a select distinct that discards duplicates.
you also don't need to use a subquery to apply lower() and you can use string_to_table() to split documents directly into rows of keywords, instead of splitting into an array first and then calling unnest(). demo:
insert into invert01 (keyword, doc_id)
select distinct lower(string_to_table(doc, ' ')) as keyword,
                id as doc_id
from docs01;

result
select keyword, doc_id from invert01 where keyword='are' 
order by keyword, doc_id limit 10;





keyword
doc_id




are
6


are
7",https://stackoverflow.com/questions/77333202,sql,20-10-2023 18:52,774.0,0.0,1.0,True,29-10-2023 13:23,20-10-2023 19:01
57749696,implementing a tf-idf vectorizer from scratch,"i am trying to implement a tf-idf vectorizer from scratch in python. i computed my tdf values but the values do not match with the tdf values computed using sklearn's tfidfvectorizer().
what am i doing wrong?
corpus = [
 'this is the first document',
 'this document is the second document',
 'and this is the third one',
 'is this the first document',
]

from collections import counter
from tqdm import tqdm
from scipy.sparse import csr_matrix
import math
import operator
from sklearn.preprocessing import normalize
import numpy

sentence = []
for i in range(len(corpus)):
sentence.append(corpus[i].split())

word_freq = {}   #calculate document frequency of a word
for i in range(len(sentence)):
    tokens = sentence[i]
    for w in tokens:
        try:
            word_freq[w].add(i)  #add the word as key 
        except:
            word_freq[w] = {i}  #if it exists already, do not add.

for i in word_freq:
    word_freq[i] = len(word_freq[i])  #counting the number of times a word(key)is in the whole corpus thus giving us the frequency of that word.

def idf():
    idfdict = {}
    for word in word_freq:
        idfdict[word] = math.log(len(sentence) / word_freq[word])
    return idfdict
idfdict = idf()

expected output:
(output obtained using vectorizer.idf_)
[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073 1.22314355 1.91629073 1.        ]

actual output:
(the values are the idf values of corresponding keys.
{'and': 1.3862943611198906,
'document': 0.28768207245178085,
'first': 0.6931471805599453,
'is': 0.0,
'one': 1.3862943611198906,
'second': 1.3862943611198906,
'the': 0.0,
'third': 1.3862943611198906,
'this': 0.0
 }","['python', 'machine-learning', 'nlp', 'tf-idf']",57749956,"there are a few default parameters that might affect what sklearn is calculating, but the particular one here that seems to matter is:
smooth_idf : boolean (default=true)
smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. prevents zero divisions.
if you subtract one from each element and raise e to that power, you get values that are very close to 5 / n, for low values of n:
1.91629073 => 5/2
1.22314355 => 5/4
1.51082562 => 5/3
1 => 5/5

at any rate, there is not a single tf-idf implementation; the metric you define is simply a heuristic that tries to observe certain properties (like ""a higher idf should correlate with rarity in the corpus"") so i wouldn't worry too much about achieving an identical implementation.
sklearn appears to have used: 
log((document_length + 1) / (frequency of word + 1)) + 1
which is rather like if there was a document that had every single word in the corpus.
edit: this last paragraph is corroborated by the docstring for tfidfnormalizer.",https://stackoverflow.com/questions/57749696,python,01-09-2019 21:41,2722.0,0.0,1.0,True,17-02-2021 08:34,01-09-2019 21:44
60767730,how to parse guess_language to read 30000 tweets?,"i am using guess_language to detect the language of the tweets for a school project. i used pandas to read the .csv file. i have around 30000 rows.
however, my problem is that the guess language can only read one tweet at a time. 
guess_language(""top story: ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½massive mental health c"")
'en'
i am very new at python and been trying to figure out the loop and if statements for this for almost a day now and they keep just returning one tweet.
thank you and apologies if the question is lame.

i used the code suggested below by kareem.
from guess_language import guess_language resdf = nodupdf[ nodupdf['text'].apply(guess_language) == 'en' ]
it worked for the small file (100 csv), but when i applied it on the bigger one. it gave me this error.
typeerror traceback (most recent call last) in 9 10 for chunk in noeng: ---> 11 chunk['text'].apply(guess_language)== 'en'
~\anaconda3\lib\site-packages\pandas\core\series.py in apply(self, func, convert_dtype, args, **kwds) 4040 else: 4041 values = self.astype(object).values -> 4042 mapped = lib.map_infer(values, f, convert=convert_dtype) 4043 4044 if len(mapped) and isinstance(mapped[0], series):
pandas_libs\lib.pyx in pandas._libs.lib.map_infer()
~\anaconda3\lib\site-packages\guess_language__init__.py in guess_language(text, hints) 322 """"""return the iso 639-1 language code. 323 """""" --> 324 words = word_re.findall(text[:max_length].replace(""ýýý"", ""'"")) 325 return identify(words, find_runs(words), hints) 326
typeerror: 'float' object is not subscriptable

thinking it was a memory error, i used chunk.
noeng=pd.read_csv(r'c:\users\jean\nodupdf.csv', chunksize=10) for chunk in noeng: chunk['text'].apply(guess_language)== 'en'
i still got the same error.","['python', 'pandas', 'nltk']",60767819,"you can fetch every and process them basically like this
resdf =  newdf[ newdf['text'].apply(guess_language) == 'en' ] 


the resdf should contain the rows of the original that had a classification of english for its tweets.
the function apply should apply your function guess_language on every single tweet and return the column values after being classified, then we use that to get only the indexes of rows who had en as classification.",https://stackoverflow.com/questions/60767730,python,20-03-2020 02:21,130.0,0.0,1.0,True,03-08-2021 10:24,03-08-2021 10:24
73344857,is package tm suitable for extracting scores from text data?,"i have many cognitive assessment data stored as txt files. each file looks like this:
patient number xxxxxx
score a        (98) (95)ile%
score b        (100) (97)ile%
test c
   score d     (76)
   score e     (80)
(the real report is longer and more orderless than this)

as the example data showed, the format of each score is not well ordered. it's easy to read, but hard to analyze. i want to extract scores of each test for each patient and create a table for further analysis. because i've never use text mining function or package in r before. i'm wondering if it's more appropriate to do it with text mining package in r, or is it ok if i just treat the whole report as a very long string? what are the difference? thanks!
the actual report i'm dealing with looks like this (i've converted all actual number to ""x"")
 orientation                                   r.s.      %ile      n/d/b
     temporal orientation-error score          ( xx)  ( xx  )%ile ( x )
     orientation to personal information       ( x )/8   ( xx  )%ile ( x )
     orientation to place                      ( x )/4   ( xx  )%ile ( x )

   wms-iii - verbal memory                       r.s.     %ile       n/d/b
     verbal paired
       associates-i       scale score ( -  )   ( -  )/32 ( -   )%ile ( - )
     verbal paired
       associates-ii      scale score ( -  )   ( -  )/8  ( -   )%ile ( - )
     word list memory-i   scale score ( x  )   ( xx )/48 ( x   )%ile ( x )
     word list memory-ii  scale score ( x  )   ( xx  )/12 ( x   )%ile ( x )
     logical memory-i     scale score ( x  )   ( xx )/75 ( x  )%ile ( x )
     logical memory-ii    scale score ( x  )   ( xx )/50 ( x   )%ile ( x )
     faces memory-i      scale score ( -  )  ( -  )/48   ( -   )%ile ( - )
     faces memory -ii    scale score ( -  )  ( -  )/48   ( -   )%ile ( - )
     visual
       reproduction-i    scale score ( x  )  ( xx  )/104 ( x  )%ile ( x )
     visual
       reproduction-ii   scale score ( x  )  ( xx  )/104 ( x   )%ile ( x )
     spatial memory
      f:( x ) b:( x )    scale score ( xx )  ( xx )/32   ( xx  )%ile ( n )
   language                                      r.s.      %ile      n/d/b
     visual naming                             ( xx )/60 ( xx  )%ile ( x )
     object naming  a+b                        ( -  )/16 ( -   )%ile ( - )
     aural comprehension                       ( xx )/18 ( xx  )%ile ( x )
     semantic association of verbal fluency    ( xx  )   ( xx  )%ile ( x )
     (                                         (       ) (     )%ile (   )
     (                                         (       ) (     )%ile (   )
    wcst-s    number cards used               ( xx  )               (   )
              number complete categories      ( x/x )   ( x   )%ile ( x )
              number perseverative errors     ( xx  )   ( x  )%ile ( x )
              number non-perseverative errors ( xx  )   ( xx  )%ile ( x )
    trails making test-part a           time  ( xx  )   ( xx  )%ile ( x )
    trails making test-part b           time  ( n/a )   (     )%ile (   )
    (                                         (       ) (     )%ile (   )
    (                                         (       ) (     )%ile (   )

  spatial perceptual function                   r.s.      %ile      n/d/b
    judgment of line orientation  form( x )   ( x )/30 ( xx  )%ile ( x )
    3-d block construction-model  form(   )
                                       score (     )/29 (     )%ile (   )
                                       time  (     )s   (     )%ile (   )
    manual dexterity                              r.s.      %ile      n/d/b
      purdue pegboard                 rh         (    )   (     )%ile (   )
                                      lh         (    )   (     )%ile (   )
                                      both hands (    )   (     )%ile (   )
impressionï¿½ï¿½ï¿½
< xxxxxxxxxxxxxxxxxxxxxxxxxxx  >
    age : ( xx  ) y/o
    edu : ( xx  ) yrs
    handedness : ( xx  )

  mini-mental examination                r.s.      %ile      n/d/b
    mmse                                     ( xx )/30 (     )%ile ( x )
    
    
personality assessment                        r.s.      %ile      n/d/b  scl-90r                                  ( x.xx )   (     )%ile ( n )  frontal behavioral inventory                 negative behavior score   ( xx )/36  (     )%ile ( n )                     disinhibition score   ( x  )/36  (     )%ile ( x )                             total score   ( xx )/72  (     )%ile ( x )  bdi-ii                                   (    )/63  (     )%ile (   )  bai                                      (    )/6   (     )%ile (   )","['r', 'text-mining', 'stringr', 'tm']",73346209,"it's hard to say without seeing the actual file (or a similar example file), but my guess is that you could use regular expressions to pull out what you need. if you do convert that whole thing into one long string, you'll probably get something like this:
library(stringr)

string <- ""patient number xxxxxx\nscore a        (98) (95)ile%\nscore b        (100) (97)ile%\ntest c\n   score d     (76)\n   score e     (80)""


you could then pull out patient numbers with something like this:
patient_number <- str_extract(string, ""(?<=patient number).*"")


and then score names like this
score_name <- str_extract_all(string, ""score [a-z]"") %>% unlist()



then the actual scores
score <- str_extract_all(string, "" \\([0-9]{1,3}\\)( |(?=\\\n)|)"") %>% 
unlist() %>% 
str_squish() %>% 
str_replace(""\\("","""") %>% 
str_replace(""\\)"","""")


then put it all together into a dataframe
scores_df <- data.frame(patient_number,score_name, score)


scores_df

> scores_df
  patient_number score_name score
1         xxxxxx    score a    98
2         xxxxxx    score b   100
3         xxxxxx    score d    76
4         xxxxxx    score e    80

please consider editing your question to include a sample of your actual data and a more specific description of what you want to pull out. if you do that we'll be able to give you much better help than this random example :)",https://stackoverflow.com/questions/73344857,r,13-08-2022 13:54,160.0,0.0,1.0,True,13-08-2022 23:29,13-08-2022 23:29
75979420,using llama_index with mac m1,"question #1:
is there a way of using mac with m1 cpu and llama_index together?
i cannot pass the bellow assertion:
assertionerror                            traceback (most recent call last)
<ipython-input-1-f2d62b66882b> in <module>
      6 from transformers import pipeline
      7 
----> 8 class customllm(llm):
      9     model_name = ""google/flan-t5-large""
     10     pipeline = pipeline(""text2text-generation"", model=model_name, device=0, model_kwargs={""torch_dtype"":torch.bfloat16})

<ipython-input-1-f2d62b66882b> in customllm()
      8 class customllm(llm):
      9     model_name = ""google/flan-t5-large""
---> 10     pipeline = pipeline(""text2text-generation"", model=model_name, device=0, model_kwargs={""torch_dtype"":torch.bfloat16})
     11 
     12     def _call(self, prompt, stop=none):

~/library/python/3.9/lib/python/site-packages/transformers/pipelines/__init__.py in pipeline(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)
    868         kwargs[""device""] = device
    869 
--> 870     return pipeline_class(model=model, framework=framework, task=task, **kwargs)

~/library/python/3.9/lib/python/site-packages/transformers/pipelines/text2text_generation.py in __init__(self, *args, **kwargs)
     63 
     64     def __init__(self, *args, **kwargs):
---> 65         super().__init__(*args, **kwargs)
     66 
     67         self.check_model_type(

~/library/python/3.9/lib/python/site-packages/transformers/pipelines/base.py in __init__(self, model, tokenizer, feature_extractor, modelcard, framework, task, args_parser, device, binary_output, **kwargs)
    776         # special handling
    777         if self.framework == ""pt"" and self.device.type != ""cpu"":
--> 778             self.model = self.model.to(self.device)
    779 
    780         # update config with task specific parameters

~/library/python/3.9/lib/python/site-packages/transformers/modeling_utils.py in to(self, *args, **kwargs)
   1680             )
   1681         else:
-> 1682             return super().to(*args, **kwargs)
   1683 
   1684     def half(self, *args):

~/library/python/3.9/lib/python/site-packages/torch/nn/modules/module.py in to(self, *args, **kwargs)
   1143             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else none, non_blocking)
   1144 
-> 1145         return self._apply(convert)
   1146 
   1147     def register_full_backward_pre_hook(

~/library/python/3.9/lib/python/site-packages/torch/nn/modules/module.py in _apply(self, fn)
    795     def _apply(self, fn):
    796         for module in self.children():
--> 797             module._apply(fn)
    798 
    799         def compute_should_use_set_data(tensor, tensor_applied):

~/library/python/3.9/lib/python/site-packages/torch/nn/modules/module.py in _apply(self, fn)
    818             # `with torch.no_grad():`
    819             with torch.no_grad():
--> 820                 param_applied = fn(param)
    821             should_use_set_data = compute_should_use_set_data(param, param_applied)
    822             if should_use_set_data:

~/library/python/3.9/lib/python/site-packages/torch/nn/modules/module.py in convert(t)
   1141                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else none,
   1142                             non_blocking, memory_format=convert_to_format)
-> 1143             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else none, non_blocking)
   1144 
   1145         return self._apply(convert)

~/library/python/3.9/lib/python/site-packages/torch/cuda/__init__.py in _lazy_init()
    237                 ""multiprocessing, you must use the 'spawn' start method"")
    238         if not hasattr(torch._c, '_cuda_getdevicecount'):
--> 239             raise assertionerror(""torch not compiled with cuda enabled"")
    240         if _cudart is none:
    241             raise assertionerror(

assertionerror: torch not compiled with cuda enabled

obviously i've no nvidia card, but i've read pytorch is now supporting mac m1 as well
i'm trying to run the below example:
from llama_index import simpledirectoryreader, langchainembedding, gptlistindex,gptsimplevectorindex, prompthelper
from langchain.embeddings.huggingface import huggingfaceembeddings
from llama_index import llmpredictor, servicecontext
import torch
from langchain.llms.base import llm
from transformers import pipeline

class customllm(llm):
    model_name = ""google/flan-t5-large""
    pipeline = pipeline(""text2text-generation"", model=model_name, device=0, model_kwargs={""torch_dtype"":torch.bfloat16})

    def _call(self, prompt, stop=none):
        return self.pipeline(prompt, max_length=9999)[0][""generated_text""]
 
    def _identifying_params(self):
        return {""name_of_model"": self.model_name}

    def _llm_type(self):
        return ""custom""


llm_predictor = llmpredictor(llm=customllm())

question #2:
assuming the answer for the above is no - i don't mind using google colab with gpu, but once the index will be made, will it be possible to download it and use it on my mac?
i.e. something like:
on google colab:
service_context = servicecontext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model)
index = gptsimplevectorindex.from_documents(documents, service_context=service_context)
index.save_to_disk('index.json')

... and later on my mac use load_from_file","['python', 'machine-learning', 'pytorch', 'huggingface-transformers', 'langchain']",75980931,"why are you passing device=0? if isinstance(device, int), pytorch will assume device is the index of a cuda device, hence the error. try device=""cpu"" (or maybe simply removing the device kwarg), and this issue should disappear.",https://stackoverflow.com/questions/75979420,python,10-04-2023 17:46,2324.0,2.0,2.0,True,03-12-2023 08:43,10-04-2023 20:55
66345536,how to add probabilities to model.predict output?,"i've built a functioning classification model following this tutorial.
the tutorial only outputs the predicted category names. i want it to output the category name and its probability and i only want to output categories above a certain probability. for example, i only want categories over .5
this is the function used to access the model:
import pickle
import numpy as np
category_model_path=""categorymodel.pkl""
category_transformer_path=""categorytransformer.pkl""
sentiment_model_path=""sentimentmodel.pkl""
sentiment_transformer_path=""sentimenttransformer.pkl""

def get_top_k_predictions(model,x_test,k):
    
    # get probabilities instead of predicted labels, since we want to collect top 3
    np.set_printoptions(suppress=true)
    probs = model.predict_proba(x_test)

    # get top k predictions by prob - note these are just index
    best_n = np.argsort(probs, axis=1)[:,-k:]
    
    # get category of predictions
    preds=[[model.classes_[predicted_cat] for predicted_cat in prediction] for prediction in best_n]
    
    preds=[ item[::-1] for item in preds]
    
    return preds

category_loaded_model = pickle.load(open(category_model_path, 'rb'))
category_loaded_transformer = pickle.load(open(category_transformer_path, 'rb'))

sentiment_loaded_model = pickle.load(open(sentiment_model_path, 'rb'))
sentiment_loaded_transformer = pickle.load(open(sentiment_transformer_path, 'rb'))

then this code is used to call the function:
category_test_features=category_loaded_transformer.transform([""i absolutley loved the organization ""])
get_top_k_predictions(category_loaded_model,category_test_features,2)

this is the current output:
[['course structure', 'learning materials']]

the probabilities are calculated in the function to the probs variable. i do not know how to only get the ones over .5 and add these to the preds output.","['python', 'machine-learning', 'classification', 'text-classification']",66347000,"the best_n array contains the indices to the array of probabilities probs. you can use it in the same way as you do for getting the labels. you can get label-probability tuples like this:
preds = [
    [(model.classes_[predicted_cat], distribution[predicted_cat])
     for predicted_cat in prediction]
    for distribution, prediction in zip(probs, best_n)]

if you do not want to return the probabilities and only want to filter them, you can do something like:
preds=[
    [model.classes_[predicted_cat]
     for predicted_cat in prediction if distribution[predicted_cat] > 0.5]
    for distribution, prediction in zip(probs, best_n)]",https://stackoverflow.com/questions/66345536,python,24-02-2021 06:08,602.0,0.0,1.0,True,24-02-2021 09:13,24-02-2021 09:13
76405986,"in langchain, how to save the verbose output to a variable?","i tried executing a langchain agent. i want to save the output from verbose into a variable, but all i can access from the agent.run is only the final answer.
how can i save the verbose output to a variable so that i can use later?
my code:
import json
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import agenttype
from langchain.llms import openai
from langchain.agents import tool
from langchain.utilities import pythonrepl

llm = openai(temperature=0.1)

## define tools
python_repl = pythonrepl()

tools = load_tools([""python_repl"", ""llm-math""], llm=llm)

agent = initialize_agent(tools, llm, agent=agenttype.zero_shot_react_description, verbose=true)

response = agent.run(""what is 3^2. use calculator to solve."")

i tried accessing the response from the agent, but it's only the final answer instead of the verbose output.
printing response gives only 9. but i would like the verbose process like:
> entering new agentexecutor chain...
 i need to use the calculator to solve this.
action: calculator
action input: 3^2
observation: answer: 9
thought: i now know the final answer.
final answer: 9","['agent', 'openai-api', 'langchain', 'large-language-model']",76454700,"i don't find any api to save verbose output as a variable.
however, i think an alternative solution to the question can be achieved by access intermediate steps in this link.
that is set return_intermediate_steps=true,
agent = initialize_agent(
  tools, 
  llm,
  agent=agenttype.zero_shot_react_description, 
  verbose=true,
  return_intermediate_steps=true
)

and use response = agent({""input"":""what is 3^2. use calculator to solve""}) instead of agent.run.
finally, you can access the intermediate steps in  response[""intermediate_steps""]
hope this will help.",https://stackoverflow.com/questions/76405986,agent,05-06-2023 11:31,14630.0,7.0,3.0,True,11-03-2025 17:58,29-08-2023 02:09
50483235,python cannot install module spacy,"iï¿½ï¿½m new to python and i ran into a problem i canï¿½ï¿½t solve.
i would like to install and use the package spacy in python.
therefore i opened cmd and ran
pip install spacy

while installing the dependecies i get an error message:
    ----------------------------------------


""""c:\users\xxx\appdata\local\programs\python\python37\python.exe"" -u -c ""import setuptools, tokenize;file='c:\users\xxx\appdata\local\temp\pip-install-6vcdnb_4\numpy\setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, file, 'exec'))"" install --record c:\users\xxx\appdata\local\temp\pip-record-jhmti8_8\install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in c:\users\xxx\appdata\local\temp\pip-install-6vcdnb_4\numpy\

but i have no idea whatï¿½ï¿½s the problem...
does anybody has a solution for this?
i use windows and python version 3.7.0b4<","['python', 'installation', 'spacy']",50484284,"ok, here is a working solution (at least on windows 10 & python3.7):
go here: 
search spacy and download the correct wheels for your platform :-

preshed
cymem
murmurhash
thinc
spacy

then you need to install them with pip install [wheel] in the above order.
if this doesn't work, try installing scipy and maybe even numpy from the same site.",https://stackoverflow.com/questions/50483235,python,23-05-2018 08:17,56918.0,14.0,17.0,True,18-12-2024 15:58,03-09-2019 15:11
58567497,what&#39;s the point to have a unk token for out of vocabulary words during decoding?,"first of all, i know this question is kind of off-topic, but i have already tried to ask elsewhere but got no response.
adding a unk token to the vocabulary is a conventional way to handle oov words in tasks of nlp. it is totally understandable to have it for encoding, but what's the point to have it for decoding? i mean you would never expect your decoder to generate a unk token during prediction, right?","['deep-learning', 'nlp', 'neural-network', 'machine-translation', 'oov']",58589830,"depending on how you preprocess your training data, you might need the unk during training. even if you use bpe or other subword segmentation, oov can appear in the training data, usually some weird utf-8 stuff, fragments of alphabets, you are not interested in at all, etc.
for example, if you take wmt training data for english-german translation, do bpe and take the vocabulary, you vocabulary will contain thousands of chinese characters that occur exactly once in the training data. even if you keep them in the vocabulary, the model has no chance to learn anything about them, not even to copy them. it makes sense to represent them as unks.
of course, what you usually do at the inference time is that you prevent the model predict unk tokens, unk is always incorrect.",https://stackoverflow.com/questions/58567497,deep-learning,26-10-2019 02:07,2777.0,2.0,2.0,True,11-03-2024 20:44,11-03-2024 20:44
59172532,bert fine-tuned for semantic similarity,"i would like to apply fine-tuning bert to calculate semantic similarity between sentences.
i search a lot websites, but i almost not found downstream about this. 
i just found sts benchmark.
i wonder if i can use sts benchmark dataset to train a fine-tuning bert model, and apply it to my task. 
is it reasonable? 
as i know, there are a lot method to calculate similarity including cosine similarity, pearson correlation, manhattan distance, etc.
how choose for semantic similarity?","['nlp', 'cosine-similarity', 'pearson-correlation', 'sentence-similarity']",59173106,"if you look at a rather popular paper in the field by mueller and thyagarajan, which is concerned with learning sentence similarity on lstms, they use a closely related dataset (the sick dataset), which is also hosted by the semeval competition, and ran alongside the sts benchmark in 2014.
either one of those should be a reasonable set to fine-tune on, but sts has run over multiple years, so the amount of available training data might be larger.
as a great primer on the topic, i can also highly recommend the medium article by adrien sieg (see here, which comes with an accompanied github reference.
for semantic similarity, i would estimate that you are better of with fine-tuning (or training) a neural network, as most classical similarity measures you mentioned have a more prominent focus on the token similarity (and thus, syntactic similarity, although not even that necessarily). semantic meaning, on the other hand, can sometimes differ wildly on a single word (maybe a negation, or the swapped sentence position of two words), which is difficult to interpret or evaluate with static methods.",https://stackoverflow.com/questions/59172532,nlp,04-12-2019 09:18,5163.0,3.0,2.0,True,07-11-2023 02:02,03-08-2020 12:09
71385658,use python need to remove the ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½,"how to remove the extra 'ï¿½ï¿½' in the word?
maximum 3 'ï¿½ï¿½' in the word

ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï","['python', 'nlp', 'arabic', 'preprocessor']",71387058,"it's a bit tricky because of the right-to-left orientation but this seems to work for me:
import re

text = ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½""
text = re.sub(""ï¿½ï¿½{3,}"", ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"", text)
print(text)
> ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½",https://stackoverflow.com/questions/71385658,python,07-03-2022 18:26,45.0,-2.0,1.0,True,07-03-2022 20:46,07-03-2022 19:00
62677651,openai gpt-2 model use with tensorflow js,"is that possible to generate texts from openai gpt-2 using tensorflowjs?
if not what is the limitation, like model format or ...?","['tensorflow', 'machine-learning', 'nlp', 'tensorflow.js', 'gpt-2']",76789939,"it's possible. maybe someone finds this useful in 2023:

one way to achieve this is to convert a tf model with tensorflowjs-converter as frederik described (possible problem with this approach is missing custom layers)

use gpt-tfjs - implementation of gpt model in tensorflow.js. it's possible to load weights directly from hf (example). i developed it to experiment with model training in the browser.


if you just want to generate text without training, you have more options:

use transformers.js or onnx in general. the lib is great and follows python's transformers library api. unfortunately - inference only.
use ggml + wasm. it's a c/c++ model implementation compiled to webassembly (example, talk)",https://stackoverflow.com/questions/62677651,tensorflow,01-07-2020 13:12,3570.0,13.0,2.0,True,28-07-2023 17:34,29-11-2020 11:52
71006031,how to solve this attribute error in python?,"def encode_sequences(tokenizer, length , lines):
  seq = tokenizer.text_to_sequences(lines)
  seq = pad_sequences(seq, maxlen = length, padding = 'posts')
  return seq

from sklearn.model_selection import train_test_split
train , test = train_test_split(deu_eng , test_size = 0.2, random_state = 12)

trainx = encode_sequences(deu_tokenizer, deu_length, train[:,1])
trainy = encode_sequences(eng_tokenizer, eng_length, train[:,0])

testx = encode_sequences(deu_tokenizer, deu_length, test[:,1])
testy = encode_sequences(eng_tokenizer, eng_length, test[:,0])

error :-
attributeerror                            traceback (most recent call last)
<ipython-input-29-0cb789025947> in <module>()
      7 train , test = train_test_split(deu_eng , test_size = 0.2, random_state = 12)
      8 
----> 9 trainx = encode_sequences(deu_tokenizer, deu_length, train[:,1])
     10 trainy = encode_sequences(eng_tokenizer, eng_length, train[:,0])
     11 

<ipython-input-29-0cb789025947> in encode_sequences(tokenizer, length, lines)
      1 #model
      2 def encode_sequences(tokenizer, length , lines):
----> 3   seq = tokenizer.text_to_sequences(lines)
      4   seq = pad_sequences(seq, maxlen = length, padding = 'posts')
      5   return seq

attributeerror: 'tokenizer' object has no attribute 'text_to_sequences'","['python', 'pandas', 'keras', 'nlp', 'artificial-intelligence']",71006192,"i believe you have mispelled the texts_to_sequences attribute. the error states that it cannot find the text_to_sequences method in the line:
seq = tokenizer.text_to_sequences(lines)

from the documentation, you can see that this method should be spelled:
seq = tokenizer.texts_to_sequences(lines)

this should resolve the error.",https://stackoverflow.com/questions/71006031,python,06-02-2022 10:06,188.0,0.0,1.0,True,06-02-2022 12:37,06-02-2022 12:37
76514078,change redirect uri for oauth consent screen,"i am making an web app using python and langchain toolkit called gmailtoolkit which accesses the gmail account of the user. it needs to get the permission of the user to access their information. for this purpose i have used the oauth consent screen and created credentials for desktop app.
my credentials.json file looks like this:
{
  ""installed"": {
    ""client_id"": ""<>"",
    ""project_id"": ""project"",
    ""auth_uri"": ""
    ""token_uri"": ""
    ""auth_provider_x509_cert_url"": ""
    ""client_secret"": ""<>"",
    ""redirect_uris"": [""
  }
}


now this works on my local machine but when i run it on my server, after the users allows the app to access his information, on the last step where the user needs to be redirected back to the app
it fails because the redirect address is localhost:.
i have tried changing the redirect_uris in credentials.json file but it does not work. i have also tried the to create web app credential and provide the redirect uri but the port changes everytime i run the app, so it does not work as well.
how can i change the redirect uri. i have a public ip where i need to redirect the user.
here is the address to get to the oauth consent screen:


this is the code from which i get the above address:
credentials = get_gmail_credentials(
            token_file='token.json',
            scopes=[""
            client_secrets_file=""credentials.json"",
            
        )
api_resource = build_resource_service(credentials=credentials)
toolkit = gmailtoolkit(api_resource=api_resource)","['python', 'oauth-2.0', 'gmail-api', 'langchain']",76514255,"there are several types of google clients.

installed app
web app
service account app
mobile app

each type is designed to run on a different platform as the underlying authorization method is different.  the code used to authorize these is also different.
the main difference between installed app and web is the redirect uri. this is configured with in google developer console, changing the json file is not going to do anything. installed apps will only return the redirect uri to localhost there for they can not be used when hosting on a web server.  you need to create web app credentials and use the code designed for web app credentials.
im not sure how you would do this with gmailtoolkit you may want to check if its supported.  a quick scan of the repo leads me to bereave it only supports installed flow not web flow. #l66
personally i use flask
def get_authorization_url():

    flow = get_flow()

    # generate url for request to google's oauth 2.0 server.
    # use kwargs to set optional request parameters.
    authorization_url, state = flow.authorization_url(
        # enable offline access so that you can refresh an access token without
        # re-prompting the user for permission. recommended for web server apps.
        access_type='offline',
        # enable incremental authorization. recommended as a best practice.
        include_granted_scopes='false')

    return authorization_url",https://stackoverflow.com/questions/76514078,python,20-06-2023 11:07,666.0,0.0,1.0,True,20-06-2023 11:35,20-06-2023 11:30
69210889,i get lookup error in google cloud run for my flask python app when using nltk,"so i have followed this tutorial  in order to deploy my flask app on google cloud run using dockerfile. although, when tested normally, the app is running normally and handles the 'post' requests i send via postman, when deployed i get the following lookup error.
  file ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 2070, in wsgi_app
    response = self.full_dispatch_request()
  file ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 1515, in full_dispatch_request
    rv = self.handle_user_exception(e)
  file ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 1513, in full_dispatch_request
    rv = self.dispatch_request()
  file ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 1499, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)
  file ""/app/run.py"", line 30, in get_text_prediction
    results=tokenize_ingredient( json['text'])
  file ""/app/ingredients.py"", line 139, in tokenize_ingredient
    tokenized = word_tokenize(chunk)
  file ""/usr/local/lib/python3.8/site-packages/nltk/tokenize/__init__.py"", line 130, in word_tokenize
    sentences = [text] if preserve_line else sent_tokenize(text, language)
  file ""/usr/local/lib/python3.8/site-packages/nltk/tokenize/__init__.py"", line 107, in sent_tokenize
    tokenizer = load(""tokenizers/punkt/{0}.pickle"".format(language))
  file ""/usr/local/lib/python3.8/site-packages/nltk/data.py"", line 750, in load
    opened_resource = _open(resource_url)
  file ""/usr/local/lib/python3.8/site-packages/nltk/data.py"", line 875, in _open
    return find(path_, path   [""""]).open()
  file ""/usr/local/lib/python3.8/site-packages/nltk/data.py"", line 583, in find
    raise lookuperror(resource_not_found)
lookuperror:

inside my dockerfile, i have already used these lines:
run pip install -r requirements.txt
run python -m nltk.downloader punkt
run python -m nltk.downloader averaged_perceptron_tagger
run python -m nltk.downloader wordnet
run pip install gunicorn

and inside my requirements.txt file there is:
flask~=2.0.1
nltk~=3.6.2
numpy","['python', 'nltk', 'google-cloud-run']",69211227,"for anyone interested, putting
-d /usr/local/nltk_data

inside the dockerfile fixed the issue like this:
# install production dependencies.
run pip install -r requirements.txt
run python -m nltk.downloader punkt -d /usr/local/nltk_data
run python -m nltk.downloader averaged_perceptron_tagger -d /usr/local/nltk_data
run python -m nltk.downloader wordnet -d /usr/local/nltk_data
run pip install gunicorn",https://stackoverflow.com/questions/69210889,python,16-09-2021 15:04,286.0,0.0,1.0,True,16-09-2021 16:08,16-09-2021 16:08
68237180,how to filter elements within a list and generate frequency table,"i have a dataframe like as shown below
df = pd.dataframe({'text': [""hi how"",""i am fine"",""ila say hi""],
                   'tokens':[['hi','how'],['i','am','fine'],['ila','say','hi']],
                    'labels':[['a','b'],['c','b','a'],['d','b','a']]})

i would like to do two things
a) count of each labels
b) count tokens under each label
i was trying something like below
flattened = [] 
op = itertools.zip_longest(df['tokens'],df['labels'])
for i in op:
    for la in df['labels']: 
        if la == 'a' : 
        flattened.append(val)

but this is incorrect and going no-where
i expect my output to have two dataframes/tables like as shown below","['python', 'python-3.x', 'pandas', 'dataframe', 'nlp']",68237244,"we can use value_counts after flattening the columns tokens and labels using hstack
t = np.hstack(df['tokens'])
l = np.hstack(df['labels'])

count of each label
pd.value_counts(l)

b    3
a    3
d    1
c    1
dtype: int64

count tokens under each label
pd.dataframe(zip(l, t)).value_counts()

0  1   
a  hi      2
   fine    1
b  am      1
   how     1
   say     1
c  i       1
d  ila     1
dtype: int64",https://stackoverflow.com/questions/68237180,python,03-07-2021 14:33,131.0,2.0,2.0,True,03-07-2021 14:46,03-07-2021 14:40
67858355,how to use trained model to test new sentence in python (sklearn),"i have code to training the model for multi class text classification and it's work but i can't use that model. this is my code for training
def training(df):
x = df.text
y = df.tags
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
lr = pipeline([('vect', countvectorizer()),
               ('tfidf', tfidftransformer()),
               ('clf', logisticregression()),
               ])

lr.fit(x_train, y_train)
y_pred1 = lr.predict(x_test)
print(f""accuracy is : {accuracy_score(y_pred1, y_test)}"")
print(lr.predict('ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 900 ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½'))

when i run the code got this this result accuracy is : 0.9957983193277311 and this error

traceback (most recent call last):
file ""e:\python\nlp project\beta_00\level0\handleclassification.py"", line 100, in 
training(df)
file ""e:\python\nlp proing
print(lr.predict('ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 900 ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½'))
file ""e:\python\nlp project\beta_00\venv\lib\site-packages\sklearn\utils\metaestimators.py""
line 120, in 
out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
file ""e:\python\nlp project\beta_00\venv\lib\site-packages\sklearn\pipeline.py"", line 418, in
predict
xt = transform.transform(xt)
file ""e:\python\nlp project\beta_00\venv\lib\site-
packages\sklearn\feature_extraction\text.py"", line 1248, in transform
raise valueerror(
valueerror: iterable over r","['machine-learning', 'scikit-learn', 'nlp', 'data-science', 'classification']",67859014,"below lines need correction:
lr.fit(x_train, y_train)
y_pred1 = lr.predict(x_test)
print(f""accuracy is : {accuracy_score(y_test, y_pred1)}"")   #<--- here
print(lr.predict(['ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 900 ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½']))   #<--- here

the line lr",https://stackoverflow.com/questions/67858355,machine-learning,06-06-2021 11:01,672.0,2.0,1.0,True,23-08-2022 20:54,23-08-2022 20:54
77739422,problems implementing chatgpt in my code?,"i`ve tried creating a simple chat-bot which involves chatgpt api, but every time i try to send it a message i face an error:

openai.error.ratelimiterror: you exceeded your current quota, please check your plan and billing details.

i understand why it should occur, but i am supposed to have $18 limit which for some reason does not work.
here is my code:
import openai

openai.api_key = """"

def get_completion(prompt, model=""gpt-3.5-turbo""):
    messages = [{""role"": ""user"", ""content"": prompt}]
    response = openai.chatcompletion.create(
        model=model,
        messages=messages,
        temperature=0,
    )
    return response.choices[0].message[""content""]

get_completion(""hi"")

i've tried switching to different versions of the opeanai module, but problem doesn't go away. is there something that i am doing wrong or don't understand?","['python', 'openai-api', 'chatgpt-api']",77739573,"if this error occurs, you will no longer have enough credit. it is correct that you will receive 18$ from openai at the start which you can use.
however, these expire after some time and can no longer be used. check the openai dashboard, your credit has probably expired.
link:",https://stackoverflow.com/questions/77739422,python,31-12-2023 14:23,158.0,-3.0,1.0,True,31-12-2023 15:46,31-12-2023 15:46
67662857,export cosine simularity array out as a matrix with labels,"short version: i have a array and need to create a matrix but with names labels on top and side and export like example csv. (sorry if may wording incorrect)
long version:
i made a recommendation system self taught and have a website ready after a year in quarantine learning and troubleshooting here on so usually a few day of searching i figure it out, but this got me stuck for about 3 weeks now.
the recommendation system system works in python i can put in a name and it spits of the recommended names i tweaked it and got it to acceptable results. but in the books, website and tutorial and udemy classes etc. never learn how to take the python and make a django site to get it to work.
this what the output is like currently is
# creating a series for the name of the character so they are associated to an ordered numerical
# list i will use later to match the indexes
indices = pd.series(df.index)
indices[:5]

# instantiating and generating the count matrix

count = countvectorizer()

count_matrix = count.fit_transform(df['bag_of_words'])

ï¿½ï¿½ï¿½

# creating a series for the name of the character so they are associated to an ordered numerical

# list i will use later to match the indexes

indices = pd.series(df.index)

indices[:5]

0             zz top
1         zyan malik
2    zooey deschanel
3       ziggy marley
4                zhu
name: name, dtype: object

# generating the cosine similarity matrix
cosine_sim = cosine_similarity(count_matrix, count_matrix)
cosine_sim

array([[1.        , 0.11708208, 0.10192614, ..., 0.        , 0.        ,
       0.        ],
      [0.11708208, 1.        , 0.1682581 , ..., 0.        , 0.        ,
       0.        ],
      [0.10192614, 0.1682581 , 1.        , ..., 0.        , 0.        ,
       0.        ],
      ...,
      [0.        , 0.        , 0.        , ..., 1.        , 1.        ,
       1.        ],
      [0.        , 0.        , 0.        , ..., 1.        , 1.        ,
       1.  ],
      [0.        , 0.        , 0.        , ..., 1.        , 1.        ,
       1.        ]])

# i need to then export to csv which i understand

.to_csv('artist_similarities.csv')


desired exports
i am trying to have the array with the index name in what i think is called a matrix like this example.

              scores             zz top             zyan malik             zooey deschanel            zhu
0             zz top             0            65.61249881            24.04163056             24.06241883
1         zyan malik             65.61249881             0            89.35882721                69.6634768
2    zooey deschanel             24.04163056             89.40917179             0             20.09975124
3                zhu             7.874007874             69.6634768             20.09975124             0

# function that takes in the character name as input and returns the top 10 recommended characters
def recommendations(name, cosine_sim = cosine_sim):
    
    recommended_names = []
    
    # getting the index of the movie that matches the title
    idx = indices[indices == name].index[0]

    # creating a series with the similarity scores in descending order
    score_series = pd.series(cosine_sim[idx]).sort_values(ascending = false)

    # getting the indexes of the 10 most characters
    top_10_indexes = list(score_series.iloc[1:11].index)
    
    # populating the list with the names of the best 10 matching characters
    for i in top_10_indexes:
        recommended_names.append(list(df.index)[i])
        
    return recommended_names

# working results which for dataset are pretty good 

recommendations('blues traveler')

['g-love & the special sauce',
 'phish',
 'spin doctors',
 'grace potter and the nocturnals',
 'jason mraz',
 'pearl jam',
 'dave matthews band',
 'lukas nelson & promise of the real ',
 'vonda shepard',
 'goo goo dolls']","['python', 'arrays', 'pandas', 'nlp', 'cosine-similarity']",67664276,"i'm not sure i understand what you're asking and i can't comment so i'm forced to write here. i assume you want to add column and index fields to the cosine_sim array. you could do something like this:
cos_sim_df = pd.dataframe(cosine_sim, index=indices, columns=indices)
cos_sim_df.to_csv(""artist_similarities.csv"")

and then read the csv like
cos_sim_df = pd.read_csv(""artist_similarities.csv"", header=0, index_col=0)

to make sure pandas knows the first row and columns are field names. also i assumed your column and row indices are the same, you can change them if you need. another thing, this won't be exactly like the desired exports because in that csv there is a ""score"" field which contains the names of the artists, though it seems like the artists should be field names. if you want the exported csv to look exactly like the desired exports you can add the artists in a ""score"" field like this:
cos_sim_df = pd.dataframe(cosine_sim, columns=indices)
cos_sim_df[""score""] = indices
# make the score field the first field
cos_sim_df = cos_sim_df[[""score"", *idx]]

lastly i want to note that indexing data frames is row-major, and it seems you visualized the fields as column indices, for this specific case since your array has a line of symmetry across the diagonal, it doesn't matter which axis is indexed because cos_sim_df[""zayn malik""] for example will return the same values anyway, but keep this in mind if your array isn't symmetrical.",https://stackoverflow.com/questions/67662857,python,23-05-2021 17:58,368.0,0.0,1.0,True,23-05-2021 20:54,23-05-2021 19:51
77111953,openai api error: &quot;unrecognized request argument supplied&quot;,"i'm receiving an error when calling the openai api. it's not recognizing file argument, which i submitted to the api.
here is my php code:
<?php

// define your openai api key and the endpoint
$apikey = 'sk-toh**********************************';
$endpoint = '

// file id of the uploaded data
$fileid = 'file-flw6jpfnuuq1ltak91ajmj2j';

// product name
$productname = '6 pack fresh grannies apples';

// prompt to use the file id as a reference
$prompt = ""given the following data from the uploaded file $fileid, categorize the product '$productname':"";

// prepare the curl request data
$data = [
    'prompt' => $prompt,
    'max_tokens' => 1, // adjust the token limit as needed
    'file' => $fileid // reference the file by id
];

// prepare the curl request
$ch = curl_init($endpoint);
curl_setopt($ch, curlopt_returntransfer, true);
curl_setopt($ch, curlopt_ [
    'authorization: bearer ' . $apikey,
    'content-type: application/json',
]);
curl_setopt($ch, curlopt_post, 1);
curl_setopt($ch, curlopt_postfields, json_encode($data));

// execute the curl request
$response = curl_exec($ch);

// check for curl errors
if (curl_errno($ch)) {
    echo 'curl error: ' . curl_error($ch);
} else {
    // parse the api response as json
    $responsedata = json_decode($response, true);
echo ""<pre>"",print_r($responsedata),""</pre>"";
    // extract and display the category
    $category = $responsedata['choices'][0]['text'];
    echo ""product '$productname' belongs to the category: $category"";
}

// close the curl session
curl_close($ch);

?>

here is the data of the file i uploaded:
{""prompt"": ""fruits"", ""completion"": ""apples, bananas, oranges, grapes, strawberries""}
{""prompt"": ""vegetables"", ""completion"": ""carrots, broccoli, spinach, lettuce, tomatoes""}
{""prompt"": ""dairy"", ""completion"": ""milk, cheese, yogurt, butter, cream""}
{""prompt"": ""meat"", ""completion"": ""chicken, beef, pork, lamb, turkey""}
{""prompt"": ""bakery"", ""completion"": ""bread, muffins, cookies, cakes, pies""}

here is the error i'm receiving:
[error] => array
(
  [message] => unrecognized request argument supplied: file
  [type] => invalid_request_error
  [param] => 
  [code] => 
)

what am i doing wrong? i've tried searching for the answer and also looking at openai documentation.","['openai-api', 'fine-tuning']",77112137,"problem
you're trying to pass file as a parameter to the completions api endpoint, which is not a valid parameter. you can't pass any parameter you make up to the completions api endpoint.
solution
see the complete list of parameters you can pass to the completions api endpoint:

model
prompt
suffix
max_tokens
temperature
top_p
n
stream
logprobs
echo
stop
presence_penalty
frequency_penalty
best_of
logit_bias
user



also, all engines api endpoints are deprecated.

use the completions api endpoint.
change the url from this...


...to this.",https://stackoverflow.com/questions/77111953,openai-api,15-09-2023 11:24,1359.0,3.0,1.0,True,30-09-2023 13:57,17-09-2023 09:42
67318505,jupyterhub - nltk - unable to use stopwords - resource stopwords not found,"i am using below code to use stopwords through jupyter notebook. i have hosted jupyter on linux server and using the notebook.
python3 -m nltk.downloader stopwords
python3 -m nltk.downloader words
python3 -m nltk.downloader punkt

python3
>>>from nltk.corpus import stopwords
>>>stop_words = set(stopwords.words(""english""))
>>>print(stop_words)

this works fine while running in python terminal, but when i try below in jupyternotebook its failing with error.
from nltk.corpus import stopwords
stop_words = set(stopwords.words(""english""))
print(stop_words)

---------------------------------------------------------------------------
lookuperror                               traceback (most recent call last)
/usr/local/lib/python3.7/site-packages/nltk/corpus/util.py in __load(self)
     82                 try:
---> 83                     root = nltk.data.find(""{}/{}"".format(self.subdir, zip_name))
     84                 except lookuperror:

/usr/local/lib/python3.7/site-packages/nltk/data.py in find(resource_name, paths)
    582     resource_not_found = ""\n%s\n%s\n%s\n"" % (sep, msg, sep)
--> 583     raise lookuperror(resource_not_found)
    584 

lookuperror: 
**********************************************************************
  resource stopwords not found.
  please use the nltk downloader to obtain the resource:","['python', 'nltk', 'jupyter', 'stop-words', 'jupyterhub']",67321618,"try running inside jupyter notebook
import nltk 
nltk.download('stopwords')",https://stackoverflow.com/questions/67318505,python,29-04-2021 13:35,2705.0,1.0,1.0,True,29-04-2021 16:39,29-04-2021 16:38
74978191,do i need to retrain bert for ner to create new labels?,"i am very new to natural language processing and i was thinking about working on named entity recognition ner. a friend of mine who works with nlp advised me to check out bert, which i did. when reading the documentation and checking out the conll-2003 data set, i noticed that the only labels are person, organization, location, miscellanious and outside. what if instead of outside, i want the model to recognize date, time, and other labels. i get that i would need a dataset labelled as such so, assuming that i have that, do i need to retrain bert from stratch or can i somehow fine tune the existing model without needing to restart the whole process?","['nlp', 'bert-language-model', 'fine-tuning']",74978299,"yes, you would have to use a model trained using the specific labels you require. the ontonotes dataset may be better suited for what you are trying to do, as it includes the 18 entity names listed below (see ontonotes 5.0 release notes for further info).
the huggingface flair/ner-english-ontonotes-large (here) and flair/ner-english-ontonotes-fast (here) models are trained on this dataset and will likely produce results closer to what you desire. as a demo (make sure to pip install flair first)
from flair.data import sentence
from flair.models import sequencetagger

tagger = sequencetagger.load(""flair/ner-english-ontonotes-large"")  # load tagger
sentence = sentence(""on september 1st george won 1 dollar while watching game of thrones."")  # example sentence
tagger.predict(sentence)  # predict ner tags

# print sentence and ner spans
print(sentence)
print('the following ner tags are found:')
# iterate over entities and print
for entity in sentence.get_spans('ner'):
    print(entity)

# output
# span [2,3]: ""september 1st""   [ï¿½ï¿½ï¿½ labels: date (1.0)]
# span [4]: ""george""   [ï¿½ï¿½ï¿½ labels: person (1.0)]
# span [6,7]: ""1 dollar""   [ï¿½ï¿½ï¿½ labels: money (1.0)]
# span [10,11,12]: ""game of thrones""   [ï¿½ï¿½ï¿½ labels: work_of_art (1.0)

ontonotes 5.0 named entities

person (people, including fictional)
norp (nationalities or religious or political groups)

organization (companies, agencies, institutions, etc.)
gpe (countries, cities, states)
location (non-gpe locations, mountain ranges, bodies of water)
product (vehicles, weapons, foods, etc. (not services))
event (named hurricanes, battles, wars, sports events, etc.)
work of art (titles of books, songs, etc.)
law (named documents made into laws)
language (any named language)
date (absolute or relative dates or periods)
time (times smaller than a day)
percent (percentage (including ï¿½ï¿½ï¿½%ï¿½ï¿½ï¿½))
money (monetary values, including unit)
quantity (measurements, as of weight or distance)
ordinal (ï¿½ï¿½ï¿½firstï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½secondï¿½ï¿½ï¿½)
c fall under another type)",https://stackoverflow.com/questions/74978191,nlp,02-01-2023 00:22,652.0,1.0,1.0,True,02-01-2023 00:53,02-01-2023 00:24
68259347,matplotlib visualization- positive negative proportion chart,"i'm trying to make the same chart as below and wonder if matplotlib has a similar chart to make that.
the chart below is the result of the stm topic model in the r package
i have probs values using dmr in python:
array([[0.07204196, 0.04238116],
       [0.04518877, 0.30546978],
       [0.0587892 , 0.19870868],
       [0.16710107, 0.07182639],
       [0.128209  , 0.02422131],
       [0.15264449, 0.07237352],
       [0.2250081 , 0.06986096],
       [0.1337716 , 0.10750801],
       [0.01197221, 0.06736039],
       [0.00527367, 0.04028973]], dtype=float32)

these are the results and left is negative words and right is positive
example of negative positive proportion chart:","['python', 'matplotlib', 'nlp', 'data-visualization', 'topic-modeling']",68260573,"it is possible to create something quite close to the image you included. i understood that the right column should be negative while the right column should be positive?
first make the data negative:
import numpy as np

arr = np.array([[0.07204196, 0.04238116],
                [0.04518877, 0.30546978],
                [0.0587892 , 0.19870868],
                [0.16710107, 0.07182639],
                [0.128209  , 0.02422131],
                [0.15264449, 0.07237352],
                [0.2250081 , 0.06986096],
                [0.1337716 , 0.10750801],
                [0.01197221, 0.06736039],
                [0.00527367, 0.04028973]], dtype=""float32"")

# make the right col negative
arr[:, 0] *= -1

then we can plot like so:
from string import ascii_lowercase
import matplotlib.pyplot as plt


fig, ax = plt.subplots()

for y, x in enumerate(arr.flatten()):
    # get a label from the alphabet
    label = ascii_lowercase[y]
    # plot the point
    ax.plot(x, y, ""o"", color=""black"")
    # annotate the point with the label
    ax.annotate(label, xy=(x, y), xytext=(x - 0.036, y), verticalalignment=""center"")

# add the vertical line at zero
ax.axvline(0, ls=""--"", color=""black"", lw=1.25)

# make the x axis equal
xlim = abs(max(ax.get_xlim(), key=abs))
ax.set_xlim((-xlim, xlim))

# remove y axis
ax.yaxis.set_visible(false)

# add two text labels for the x axis
for text, x in zip([""negative"", ""positive""], ax.get_xlim()):
    ax.text(x / 2, -3.75, f""{text} reviews"", horizontalalignment=""center"")

which outputs:

you can tweak the values in the calls to ax.annotate and ax.text if you need to change the locations of the text on the plot or x-axis.",https://stackoverflow.com/questions/68259347,python,05-07-2021 16:27,229.0,1.0,2.0,True,05-07-2021 21:24,05-07-2021 18:19
76130487,how does importing module functions work when using npm in svelte?,"i don't understand how i properly use imports when working with svelte and npm. this is a svelte component, which i can't get to work.
uncommenting ""//import { configuration, openaiapi } from ""openai"";"" results in the
 [!] (plugin commonjs--resolver) rolluperror: unexpected token (note that you need @rollup/plugin-json to import json files)
not uncommenting it results in uncaught referenceerror: configuration is not defined in browser console.
<script>
//import { configuration, openaiapi } from ""openai"";
const openaiconfiguration = new configuration({
  organization: ""my-org-key"", 
  apikey: ""my-api-key"",
});
$: gptrespons = """";
const questionquery = ""do you agree with the sentiment and conclusion of the following article? ""


async function fetchnews(link) {
  try {
    let bruksak = await aisummarize(link);
    gptrespons = bruksak[""data""][""choices""][0][""message""][""content""]
    console.log(""ferdig ï¿½ï¿½ hente og oppsummere nyheter"");
  } catch (error) {
    alert(""error"");
    console.error(error);
  }
  return;
}
async function aisummarize(newsstoryurl) {

  const openai = new openaiapi(openaiconfiguration);

  try {
    const result = await openai.createchatcompletion({
      model: ""gpt-3.5-turbo"",
      messages: [{role:""user"", ""content"":`skriv en oppsummering av ${newsstoryurl} oppsumeringen skal vï¿½ï¿½re morsom og glad`}]
    
    });

    return result;
  } catch (error) {
    alert(""error summarizing news story"");
    console.error(error);
    throw error;
  }
}
fetchnews(insertlink"")

</script>

<h2>
    gpt:respons
</h2>
<p>
    {gptrespons}
</p>

how can i fix this?
is there in particular any parts or concepts of svelte and mpm that i have misunderstood?
i expected the code to properly has the configuration class. tried using openai.configuration. tried running npm install openai again. and tried running npm install @rollup/plugin-json --save-dev in the terminal.
thank you so much for your time and attention<3
here is my rollup.config file
import { spawn } from 'child_process';
import svelte from 'rollup-plugin-svelte';
import commonjs from '@rollup/plugin-commonjs';
import terser from '@rollup/plugin-terser';
import resolve from '@rollup/plugin-node-resolve';
import livereload from 'rollup-plugin-livereload';
import css from 'rollup-plugin-css-only';

const production = !process.env.rollup_watch;

function serve() {
    let server;

    function toexit() {
        if (server) server.kill(0);
    }

    return {
        writebundle() {
            if (server) return;
            server = spawn('npm', ['run', 'start', '--', '--dev'], {
                stdio: ['ignore', 'inherit', 'inherit'],
                shell: true
            });

            process.on('sigterm', toexit);
            process.on('exit', toexit);
        }
    };
}

export default {
    input: 'src/main.js',
    output: {
        sourcemap: true,
        format: 'iife',
        name: 'app',
        file: 'public/build/bundle.js'
    },
    plugins: [
        svelte({
            compileroptions: {
                // enable run-time checks when not in production
                dev: !production
            }
        }),
        // we'll extract any component css out into
        // a separate file - better for performance
        css({ output: 'bundle.css' }),

        // if you have external dependencies installed from
        // npm, you'll most likely need these plugins. in
        // some cases you'll need additional configuration -
        // consult the documentation for details:
        // 
        resolve({
            browser: true,
            dedupe: ['svelte'],
            exportconditions: ['svelte']
        }),
        commonjs(),

        // in dev mode, call `npm run start` once
        // the bundle has been generated
        !production && serve(),

        // watch the `public` directory and refresh the
        // browser on changes when not in production
        !production && livereload('public'),

        // if we're building for production (npm run build
        // instead of npm run dev), minify
        production && terser()
    ],
    watch: {
        clearscreen: false
    }
};","['javascript', 'npm', 'module', 'svelte', 'openai-api']",76136452,"so you need to add the json plugin in the configuration like so:
import json from '@rollup/plugin-json';

plugins: [
    // add json plugin
    json(),

    // other plugins
  ]

however, if you are creating a new project i would advice you to create the project with vite, it is a lot easier to develop with vite.",https://stackoverflow.com/questions/76130487,javascript,28-04-2023 14:15,190.0,-1.0,1.0,True,29-04-2023 13:20,29-04-2023 10:57
77596271,i want to merge my peft adapter model with the base model and make a fully new model,"as the title said, i want to merge my peft lora adapter model (arcturusai/crystalline-1.1b-v23.12-tagger) that i trained before with the base model (tinyllama/tinyllama-1.1b-chat-v0.6) and make a fully new model.
and i got this code from chatgpt:
from transformers import automodel, autoconfig

# load the pretrained model and lora adapter
pretrained_model_name = ""tinyllama/tinyllama-1.1b-chat-v0.6""

pretrained_model = automodel.from_pretrained(pretrained_model_name)
lora_adapter = automodel.from_pretrained(""arcturusai/crystalline-1.1b-v23.12-tagger"")

# assuming the models have the same architecture (encoder, decoder, etc.)
# get the weights of each model
pretrained_weights = pretrained_model.state_dict()
lora_adapter_weights = lora_adapter.state_dict()

# combine the weights (adjust the weights based on your preference)
combined_weights = {}
for key in pretrained_weights:
    combined_weights[key] = 0.8 * pretrained_weights[key] + 0.2 * lora_adapter_weights[key]

# load the combined weights into the pretrained model
pretrained_model.load_state_dict(combined_weights)

# save the integrated model
pretrained_model.save_pretrained(""arcturusai/crystalline-1.1b-v23.12-tagger-fullmodel"")

and i got this error:
---------------------------------------------------------------------------

oserror                                   traceback (most recent call last)

<ipython-input-1-d2120d727884> in <cell line: 6>()
      4 pretrained_model_name = ""tinyllama/tinyllama-1.1b-chat-v0.6""
      5 pretrained_model = automodel.from_pretrained(pretrained_model_name)
----> 6 lora_adapter = automodel.from_pretrained(""arcturusai/crystalline-1.1b-v23.12-tagger"")
      7 
      8 # assuming the models have the same architecture (encoder, decoder, etc.)

1 frames

/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)
   3096                             )
   3097                         else:
-> 3098                             raise environmenterror(
   3099                                 f""{pretrained_model_name_or_path} does not appear to have a file named""
   3100                                 f"" {_add_variant(weights_name, variant)}, {tf2_weights_name}, {tf_weights_name} or""

oserror: arcturusai/crystalline-1.1b-v23.12-tagger does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

i have no idea what i did wrong there, i would appreciate it if anyone could teach me how to fix it, or am i going in a completely wrong direction? thank you.
i tried using transformers and pytorch, i expect them to merge both models and create a new model out of it.","['python', 'artificial-intelligence', 'huggingface-transformers', 'peft']",77596556,"the adapter can't be loaded with automodel from transformers and also the suggestion from chatgpt of merging won't work. luckily you don't need to rely on ai for that. the peft library has everything ready for you with merge_and_unload:
from peft import autopeftmodelforcausallm

# local path, check post scriptum for explanation
model_id = ""./arcturusai/crystalline-1.1b-v23.12-tagger""
peft_model = autopeftmodelforcausallm.from_pretrained(model_id)
print(type(peft_model))

merged_model = peft_model.merge_and_unload()
# the adapters are merged now and it is transformers class again
print(type(merged_model))

output:
<class 'peft.peft_model.peftmodelforcausallm'>
<class 'transformers.models.llama.modeling_llama.llamaforcausallm'>

you can now save merged_model with save_pretrained or do with it whatever you want.
please note that this is only the model and not the tokenizer. you still need to load the tokenizer from the tinyllama/tinyllama-1.1b-chat-v0.6 repo and save it with save_pretrained locally to have everything in one place:
from transformers import autotokenizer
t = autotokenizer.from_pretrained(""tinyllama/tinyllama-1.1b-chat-v0.6"")

p.s.: i noticed that you have trained the model with a different version of peft. hence i downloaded it locally and removed the following keys from the adapter_config.json:

loftq_config,
megatron_config,
megatron_core

to be able to load it with peft==0.6.2.",https://stackoverflow.com/questions/77596271,python,03-12-2023 21:39,8533.0,6.0,1.0,True,04-12-2023 01:38,04-12-2023 01:38
77660869,how to re-use embedded documents for few-shot llm queries in langchain4j?,"i have an llm chat model with token limitation.
i am trying to pass sample user messages and expected ai message responses to the llm to train it how to provide a response based on text extracted from a document.
i am loading the document with system loader
 document document = loaddocument(topath(""file:///filepath\\filename.pdf""));

i am using regex splitter to help the llm understand a pattern
   documentbyregexsplitter splitter=new documentbyregexsplitter(regex,joiner,maxcharlimit,maxoverlap,subsplitter);

after embedding the document (in-memory embedding store and getting the relevant vectors), i join it into an information string which i can feed into a prompt template to generate a user message
prompttemplate prompttemplate = prompttemplate.from(
            ""answer the following question to the best of your ability""
                    + ""question:\n""
                    + ""{{question}}\n""
                    + ""\n""
                    + ""base your answer on the following information:\n""
                    + ""{{information}}"");

string information = relevantembeddings.stream()
        .map(match -> match.embedded().text())
        .collect(joining(""\n\n""));

map<string, object> variables = new hashmap<>();
variables.put(""question"", trainingquestion);
variables.put(""information"", information);
prompt prompt = prompttemplate.apply(variables);


list<chatmessage> chatmessages=new arraylist<>();
chatmessages.add(prompt .tousermessage());
chatmessages.add(new aimessage(""expected response""));

    variables.put(""question"", actualquestion);
    variables.put(""information"", information);
    prompt = prompttemplate.apply(variables);
chatmessages.add(prompt .tousermessage());

i will add the traning messages to a list as required by the java langchain framework
aimessage response=chatmodel.generate(chatmessages);

to make a long story short, i am facing the token constraint because of embedding the same document information for all the few shot messages.
is there a way to make the llm use the same document as a reference for the few-shot training and the actual query so i can avoid consuming tokens for the document multiple times?","['java', 'document', 'large-language-model', 'few-shot-learning', 'langchain4j']",77664642,"i got a suggestion from a colleague to ad the document to systemmessage so it won't have be passed multiple times for the training and actual user messages.
will try this and update",https://stackoverflow.com/questions/77660869,java,14-12-2023 14:32,580.0,0.0,1.0,True,03-02-2024 09:52,03-02-2024 09:52
71532653,understanding gpu usage huggingface classification,"i am building a classifier using huggingface and would like to understand the line total train batch size (w. parallel, distributed & accumulation) = 64 from below
 num examples = 7000
  num epochs = 3
  instantaneous batch size per device = 4
  total train batch size (w. parallel, distributed & accumulation) = 64
  gradient accumulation steps = 16
  total optimization steps = 327

i have 7000 rows of data, i have defined epochs to be 3 and  per_device_train_batch_size = 4 and  per_device_eval_batch_size= 16. i also get that total optimization steps = 327 - (7000*3/64)
but i am not clear about total train batch size (w. parallel, distributed & accumulation) = 64. does it mean that there are 16 devices as 16*4(instantaneous batch size per device = 4) comes to 64?","['python', 'gpu', 'huggingface-transformers']",71543615,"well the variable used for printing that summary is this one: 
the total train batch size is defined as train_batch_size * gradient_accumulation_steps * world_size, so in your case 4 * 16 * 1 = 64. world_size is always 1 except when you are using a tpu/training in parallel, see",https://stackoverflow.com/questions/71532653,python,18-03-2022 20:09,1433.0,1.0,1.0,True,20-03-2022 02:33,18-03-2022 21:50
76608006,wrong or missing inputcols annotators - spark-nlp,"i'm new to nlp and started with the spark-nlp package for python. i trained a simple ner model, which i saved and now want to use. however, i am facing the problem of wrong or missing inputcols, despite the dataframe looking accurate. what am i missing here?
i have tried different approaches of using the documentassembler, sentencedetector and tokenizer. however, none seem to work. this is my code:
from pyspark.ml import pipelinemodel
from sparknlp.base import documentassembler
from sparknlp.annotator import sentencedetector, tokenizer
from pyspark.ml.feature import tokenizer
import sparknlp

spark = sparknlp.start()

loaded_model = pipelinemodel.load(""bert_diseases"")

sentences = [
['hello, this is an example sentence'],
['and this is a second sentence.']
]

data = spark.createdataframe(sentences).todf(""text"")

document = documentassembler().setinputcol(""text"").setoutputcol(""document"")
sentence = sentencedetector().setinputcols([""document""]).setoutputcol(""sentence"")
token = tokenizer().setinputcol(""text"").setoutputcol(""token"")

data.show()

documents = document.transform(data)
documents.show()

sentences = sentence.transform(documents)
sentences.show()

tokens = token.transform(sentences)
tokens.show()

result = loaded_model.transform(tokens)
result.show()

first part is working as expected
however, i get this error as soon as i try to transform the data with my model
i have also reviewed this question. unfortunately, it did not really help...
please also see the metadata of the model i use:
{""class"":""com.johnsnowlabs.nlp.embeddings.bertembeddings"",""timestamp"":1688164205140,""sparkversion"":""3.4.1"",""uid"":""bert_embeddings_e3d4eaf62b32"",""parammap"":{""outputcol"":""embeddings"",""dimension"":768,""casesensitive"":false,""inputcols"":[""sentence"",""token""],""storageref"":""small_bert_l2_768""},""defaultparammap"":{""lazyannotator"":false,""dimension"":768,""casesensitive"":false,""engine"":""tensorflow"",""storageref"":""bert_embeddings_e3d4eaf62b32"",""maxsentencelength"":128,""batchsize"":8}}

thank you for your help in advance!
edit:
i figured it might have had something to do with the ""token""-column being an array - is_nlp_annotator was false as well. so i took another approach:
from pyspark.ml import pipelinemodel
from sparknlp.training import conll
import sparknlp
from sparknlp.base import *
from sparknlp.annotator import *
from pyspark.ml import pipeline

import sparknlp
spark = sparknlp.start()

loaded_model = pipelinemodel.load(""bert_diseases"")

data = spark.createdataframe([[""i'd like to say we didn't expect that. jane's boyfriend.""]]).todf(""text"")

documentassembler = documentassembler().setinputcol(""text"").setoutputcol(""document"")
tokenizer = tokenizer().setinputcols([""document""]).setoutputcol(""token"").fit(data)

pipeline = pipeline().setstages([documentassembler, tokenizer]).fit(data)
tokenized = pipeline.transform(data)

tokenized.selectexpr(""token.result"").show(truncate=false)
tokenized.show()

inputdata = tokenized.drop(""text"")
inputdata.show()

result = loaded_model.transform(inputdata)
result.show()

i got the idea from here. however, it still does not work, and i am confused as ever.
pyspark.errors.exceptions.captured.illegalargumentexception: requirement failed: wrong or missing inputcols annotators in bert_embeddings_e3d4eaf62b32.

current inputcols: sentence,token. dataset's columns:
(column_name=document,is_nlp_annotator=true,type=document)
(column_name=token,is_nlp_annotator=true,type=token).
make sure such annotators exist in your pipeline, with the right output names and that they have following annotator types: document, token

the dataframe looks correct, though...
+--------------------+--------------------+
|            document|               token|
+--------------------+--------------------+
|[{document, 0, 55...|[{token, 0, 2, i'...|
+--------------------+--------------------+","['python', 'pyspark', 'nlp', 'johnsnowlabs-spark-nlp']",76611290,"i fixed the problem. i seem to have been misled by the error message, since i thought that the document and token annotations were missing and necessary. i suppose it was to read as something like: you are missing a column ""sentence"" which should be in the ""document""-annotation, and you are missing a column ""token"" which should be in the ""token""-annotation. so this code works perfectly for me:
spark = sparknlp.start()

loaded_model = pipelinemodel.load(""bert_diseases"")

data = spark.createdataframe([[string_param]]).todf(""text"")

documentassembler = documentassembler().setinputcol(""text"").setoutputcol(""document"")
sentence = sentencedetector()\
    .setinputcols([""document""])\
    .setoutputcol(""sentence"")
tokenizer = tokenizer().setinputcols([""document""]).setoutputcol(""token"").fit(data)

pipeline = pipeline().setstages([documentassembler, sentence, tokenizer]).fit(data)
tokenized = pipeline.transform(data)

inputdata = tokenized.drop(""text"")

result = loaded_model.transform(inputdata)",https://stackoverflow.com/questions/76608006,python,03-07-2023 20:47,225.0,2.0,1.0,True,04-07-2023 09:43,03-07-2023 21:29
76432637,vs code: azure workspace folders do not appear after i run a function,"in vs code in windows 10, using python 3.9.13, i run a function and it just returns paths in the terminal. the call stack appears for an instant in the left margin and disappears, then ""no local workspace resources exist"" appears where my workspace folder should be. i ran this function several times in previous days without this issue.
i want to deploy the function but i can't because there is nothing under workspaces. the image is a screenshot of what it does after i run the function.","['azure', 'visual-studio-code', 'openai-api', 'chatgpt-api']",76442292,"open basicazurefunction in an integrated terminal in vs code and run > func host start in your terminal for your function to run.
or else, just open basicazurefunction folder in vs code > and click on fn + f5 or run > start debugging.
your function will be triggered> do not open entire udemy folder, just open basicazurefunction in your vs code.
if that doesnot work run the function from the child folder where the function is present and then run function from basicazurefunction and it will run the function.",https://stackoverflow.com/questions/76432637,azure,08-06-2023 13:58,2388.0,0.0,1.0,True,09-06-2023 16:52,08-06-2023 14:10
70609250,imposing grammar rules manually on sequence2sequence keras model,"i have a fairly standard sequence to sequence translator in keras, which looks like this:
# create model 

encoder_inputs = input(shape=(none,))
en_x=  embedding(num_encoder_tokens, embedding_size)(encoder_inputs)
encoder = lstm(50, return_state=true)
encoder_outputs, state_h, state_c = encoder(en_x)
# we discard `encoder_outputs` and only keep the states.
encoder_states = [state_h, state_c]


# set up the decoder, using `encoder_states` as initial state.
decoder_inputs = input(shape=(none,))
dex=  embedding(num_decoder_tokens, embedding_size)
final_dex= dex(decoder_inputs)

decoder_lstm = lstm(50, return_sequences=true, return_state=true)
decoder_outputs, _, _ = decoder_lstm(final_dex, initial_state=encoder_states)
decoder_dense = dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)


model = model([encoder_inputs, decoder_inputs], decoder_outputs)
optimizer = tf.keras.optimizers.rmsprop(learning_rate=0.05)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])

i know it isn't a great idea, but the data i am trying to translate is not spoken language and i want to impose further rules on the decoded sequence, which is that
""any word should only occur once in the decoded sequence"" among others. the rule does not apply to the sequence being encoded.
the data i am using to train the model does already adhere to this rule, but the current output of the model does not. (i know this rule doesn't really make sense language-wise)
is there a way to do this, and if so how?","['python', 'machine-learning', 'keras', 'nlp', 'language-translation']",70609422,"why not in the decoder check for duplicating words then stop the decoding if it occurs. add rules in the  char = target_index_word[word_index] decoded_sentence += ' '+char part of the decoder
def get_predicted_sentence(input_seq):
    # encode the input as state vectors.
    enc_output, enc_h, enc_c = encoder_model.predict(input_seq)
    # generate empty target sequence of length 1.
    target_seq = np.zeros((1,1))
    
    # populate the first character of target sequence with the start character.
    target_seq[0, 0] = target_word_index['sos']
    
    # sampling loop for a batch of sequences
    # (to simplify, here we assume a batch of size 1).
    stop_condition = false
    decoded_sentence = """"
    
    count=0
    while not stop_condition:
        count+=1
        if count>1000:
            print('count exceeded')
            stop_condition=true
        output_words, dec_h, dec_c = decoder_model.predict([target_seq] + [enc_output, enc_h, enc_c ])
        #print(output_tokens)
        word_index = np.argmax(output_words[0, -1, :])
        char=""""
        if word_index in target_index_word:
            char = target_index_word[word_index]
            decoded_sentence += ' '+char
            print(decoded_sentence)
        else:
            stop_condition=true
        if char == 'eos' or len(decoded_sentence) >= max_input_len:
            stop_condition = true
        
        # update the target sequence (of length 1).
        target_seq = np.zeros((1,1))
        target_seq[0, 0] = word_index
        print(target_seq[0,0])
        # update states
        enc_h, enc_c = dec_h, dec_c
    
    return decoded_sentence",https://stackoverflow.com/questions/70609250,python,06-01-2022 15:12,60.0,0.0,1.0,True,07-01-2022 08:49,07-01-2022 08:49
76322025,why am i getting an authentication error when trying to run a langchain tutorial on faiss vector database with openai api?,"i follow a youtube langchain tutorial where it teaches create your own chatgpt with pdf data in 5 minutes (langchain tutorial) and here is the colab notebook link provided by the author for his work below the video description. i didn't modify a lot of his codes where i just changed the openapi key with my one (not free plan).


can i know why i got this error as shown in the diagram above when i try to run the code in the cell?
i expect the faiss vector database can be created.","['python', 'openai-api', 'langchain', 'faiss', 'vector-database']",76357086,"update: (29th may, 2023)
in order to use the library with microsoft azure endpoints, you need to set the openai_api_type, openai_api_base, openai_api_key, and optionally api_version. the openai_api_type must be set to 'azure' and the others correspond to the properties of your endpoint. in addition, the deployment name must be passed as the model parameter.
see the below example:
import os

os.environ[""openai_api_type""] = ""azure""
os.environ[""openai_api_base""] = ""
os.environ[""openai_api_key""] = ""your azureopenai key""

from langchain.embeddings.openai import openaiembeddings

embeddings = openaiembeddings(
    deployment=""your-embeddings-deployment-name"",
    model=""your-embeddings-model-name"",
    api_base=""
    api_type=""azure"",
)


the same goes for the llm model. see the below example with reference to your provided notebook link:
import os

os.environ[""openai_api_type""] = ""azure""
os.environ[""openai_api_version""] = ""2022-12-01""
os.environ[""openai_api_base""] = ""...""
os.environ[""openai_api_key""] = ""...""

# import azure openai
from langchain.llms import azureopenai

# replace the deployment name with your own
chain = load_qa_chain(
    azureopenai(
        deployment_name=""td2"",
        model_name=""text-davinci-002"",
    ),
    chain_type=""stuff"",
)


initial suspicion
authenticationerror occurs when there's an issue with your api key or token. it could be because it's invalid. this may happen if there's a small mistake, like a typo or a formatting error. try with a different/new api key if the issue persists.",https://stackoverflow.com/questions/76322025,python,24-05-2023 09:28,6694.0,0.0,1.0,True,29-05-2023 15:29,24-05-2023 09:37
69852169,text preprocessing for fasttext pretrained models,i want to use pretreained fastext model for language detection:  . where can i find the exact python code for text preprocessing used for training this specific model? i am not interested in general answers about how should we prepare text for using models - i ma looking for identical transformations as those used for training.,"['nlp', 'text-processing', 'text-classification', 'fasttext']",69857412,"when the facebook engineers have been asked similar questions in their github repository issues, they've usually pointed to one or the other of two shell scripts in their public code (& especially the 'normalize_text' functions within).

normalize_text() {
  tr '[:upper:]' '[:lower:]' | sed -e 's/^/__label__/g' | \
    sed -e ""s/'/ ' /g"" -e 's/""//g' -e 's/\./ \. /g' -e 's/<br \/>/ /g' \
        -e 's/,/ , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\!/ \! /g' \
        -e 's/\?/ \? /g' -e 's/\;/ /g' -e 's/\:/ /g' | tr -s "" "" | myshuf
}


normalize_text() {
    sed -e ""s/ï¿½ï¿½ï¿½/'/g"" -e ""s/ï¿½ï¿½ï¿½/'/g"" -e ""s/''/ /g"" -e ""s/'/ ' /g"" -e ""s/ï¿½ï¿½ï¿½/\""/g"" -e ""s/ï¿½ï¿½ï¿½/\""/g"" \
        -e 's/""/ "" /g' -e 's/\./ \. /g' -e 's/<br \/>/ /g' -e 's/, / , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\!/ \! /g' \
        -e 's/\?/ \? /g' -e 's/\;/ /g' -e 's/\:/ /g' -e 's/-/ - /g' -e 's/=/ /g' -e 's/=/ /g' -e 's/*/ /g' -e 's/|/ /g' \
        -e 's/ï¿½ï¿½/ /g' | tr 0-9 "" ""
}

they"" rel=""nofollow noreferrer"">this page's section on 'tokenization' (which names some libraries), and the academic paper which describes the earlier work making individual language vectors.
none of these are guaranteed to exactly match what was used to create their pretrained classification models, & it's a bit frustrating that each release of such models doesn't contain the exact code to reproduce. but, these sources seem to be as much detail as is available, without getting direct answers/help from the team that created them.",https://stackoverflow.com/questions/69852169,nlp,05-11-2021 10:56,713.0,0.0,1.0,True,05-11-2021 17:53,05-11-2021 11:51
72812005,extracting sentences containing a keyword using set(),"i'm trying to extract sentences that contain selected keywords using set.intersection().
so far i'm only getting sentences that have the word 'van'. i can't get sentences with the words 'blue tinge' or 'off the road' because the code below can only handle single keywords.
why is this happening, and what can i do to solve the problem? thank you.
from textblob import textblob
import nltk
nltk.download('punkt')

search_words = set([""off the road"", ""blue tinge"" ,""van""])

blob = textblob(""that is the off the road vehicle i had in mind for my adventure. 
which one? the one with the blue tinge. oh, i'd use the money for a van."")

matches = []

for sentence in blob.sentences:
    blobwords = set(sentence.words) 
    if search_words.intersection(blobwords):  
        matches.append(str(sentence))

print(matches)

output: [""oh, i'd use the money for a van.""]","['python', 'nlp']",72815299,"if you want to check for exact match of the search keywords this can be accomplished using:
from nltk.tokenize import sent_tokenize
text = ""that is the off the road vehicle i had in mind for my adventure. which one? the one with the blue tinge. oh, i'd use the money for a van.""
search_words = [""off the road"", ""blue tinge"" ,""van""]
matches = []
sentances = sent_tokenize(text)
for word in search_words:
   for sentance in sentances:
       if word in sentance:
           matches.append(sentance)
print(matches)

the output is:
['that is the off the road vehicle i had in mind for my adventure.',
 ""oh, i'd use the money for a van."",
 'the one with the blue tinge.']

if you want partial matching then use fuzzywuzzy for percentage matching.",https://stackoverflow.com/questions/72812005,python,30-06-2022 07:42,559.0,2.0,1.0,True,30-06-2022 11:51,30-06-2022 08:49
72784310,fasttext: can&#39;t see the representation of words that starts with @ or @,"i am working in an nlp project using fasttext. i have some texts which contains words like @.poisonjamak, @aminagabread, @iamquak123 and i want to see their fasttext representation. i want to mention that the model has the following form:
# fasttext
ft_model = fasttext(word_tokenized_corpus,
                    max_n=0,
                    vector_size=64,
                    window=5,
                    min_count=1,
                    sg=1,
                    workers=20,
                    epochs=50,
                    seed=42)

using this i can see their representation, however i have an error
print(ft_model.wv['@.poisonjamak'])

keyerror: 'cannot calculate vector for oov word without ngrams'

of course, these words are in my texts. i have the above error in all these 3 words, however if i do the following this is working.
print(ft_model.wv['@.poisonjamak']) -----> print(ft_model.wv['poisonjamak'])
print(ft_model.wv['@aminagabread']) -----> print(ft_model.wv['aminagabread'])
print(ft_model.wv['@_iamquak123_']) -----> print(ft_model.wv['_iamquak123_'])

question: so do you know why i have this problem?
update:
my dataset called 'df' and the column with texts called 'text'. i using the following code to prepare the texts for the fast text. the fasttext is trained on word_tokenized_corpus
extra_list = df.text.tolist()
final_corpus = [sentence for sentence in extra_list if sentence.strip() !='']

word_punctuation_tokenizer = nltk.wordpuncttokenizer()
word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in final_corpus]","['python', 'nlp', 'gensim', 'fasttext']",72791479,"as comments note, the main issue is likely with your tokenizer, which won't put '@' characters inside your tokens. as a result, your fasttext model isn't seeing the tokens you expect ï¿½ï¿½ï¿½ but probably does have a word-vector for the 'word' '@'.
separately reviewing your actual word_tokenized_corpus, to see what it truly includes before the mdoel gets to do its training, is a good way to confirm this (or catch this class of error in the future).
there is however another contributing issue: your use of the max_n=0 parameter. this essentially turns off subword learning, by qualifying no positive-length word-substrings (aka 'character n-grams') for vector-learning. this setting essentially turns fasttext into plain word2vec.
if instead you were using fasttext in a more usual way, it would've learned subword-vectors for some of the subwords in 'aminag' etc, and thus would'vbe provided synthetic ""guess"" word-vectors for the full '@aminagabread' unseen oov token.
so in a way, you're only seeing the error letting you know about a problem in your tokenization because of this other deviation from usual fasttext oov behavior. if you really want fasttext for its unique benefit of synthetic vectors for oov words, you should return to a more typical max_n setting.
separate usage tips:

min_count=1 is usually a bad idea with such word2vec-family algorithms, as such rare words don't have enough varied usage examples to get good vectors themselves, but the failed attempt to try degrades training for surrounding words. often, discarding such words (as with the default min_count=5 as if they weren't there at all improves downstream evaluations.
because of some inherent threading inefficiencies of the python global interpreter lock (""gil""), and the gensim approach to iterating over your corpus in one thread, parcelling work out to worker threads, it is likely you'll get higher training throughput with fewer workers than your workers=20 setting, even if you have 20 (or far more) cpu cores. the exact best setting in any situation will vary by a lot of things, including some of the model parameters, and only trial-and-error can narrow the best values. but it's more likely to be in the 6-12 range, even when more cores are available, than 16+.",https://stackoverflow.com/questions/72784310,python,28-06-2022 09:53,372.0,0.0,1.0,True,28-06-2022 18:26,28-06-2022 18:12
77248165,what does the vocabulary of a pre-trained / fine-tuned t5 model look like?,"my question is regarding the pre-trained t5 models found on huggingface. in either case of taking the fully-trained model, or after fine-tuning it, is there an api function for directly downloading the vocabulary?
more specifically, the default vocab_size for t5 is 32128 (from the documentation). does that mean that after the model is trained, its decoder can generate up to 32128 unique words?
as an aside, i have noticed that capitalization does sometimes appear in my fine-tuned t5, does that mean the 32128 vocabulary could also be comprised of capitalized variants of words, e.g., is there one vocab index for ""hello"" and another index for ""hello""?","['python', 'pytorch', 'nlp', 'huggingface-transformers']",77248234,"the t5 default vocabulary consists of 32,128 subword tokens (utilizing the sentencepiece tokenizer), not word tokens. thus, it can generate a larger vocabulary than the specified 32,128.

""hello"" and ""hello"" are treated as different tokens because t5's tokenizer is case-sensitive.",https://stackoverflow.com/questions/77248165,python,07-10-2023 02:35,650.0,1.0,1.0,True,13-03-2024 22:43,13-03-2024 22:43
69520218,how to get the percentage of documents that contain a feature(s),"i'm using this solution(get what percent of documents contain a feature - quanteda) to find the number of documents that contain any one of a group of features in my dataset. as long as the document contains any one of the words, i want it to return true.
i got it to work, but it only works some of the time and i can't figure out why. removing or adding words works sometimes and not at other times. this is the code i used (the compound phrases have already been ""tokens_compound"" in the dfm)
thetarget <- c(""testing"", ""test"", ""example words"", ""example"")

df <- data.frame(docname = docnames(dfm),
                 year = docvars(dfm, c(""year"")),
                 contains_target = rowsums(dfm[, thetarget]) > 0,
                 row.names = null)

and the error i get sometimes
error in h(simpleerror(msg, call)) : 
  error in evaluating the argument 'x' in selecting a method for function 'rowsums': 
subscript out of bounds

tia
edit (script to create table showing a year and number of documents containing any of the target words):
 df2 <- df %>%
  mutate_if(is.logical, as.character) %>%
  filter(!str_detect(contains_target, ""false"")) %>%
  group_by(year) %>%
    summarise(n = n())","['r', 'error-handling', 'nlp', 'quanteda']",69521809,"you are getting the error because in some dfm objects you create, not all of the features in thetarget are in the object dfm you have created.
here's a way to avoid that, using docfreq():
library(""quanteda"")
## package version: 3.1.0
## unicode version: 13.0
## icu version: 69.1
## parallel computing: 12 of 12 threads used.
## see  for tutorials and examples.

thetarget <- c(""nuclear"", ""congress"", ""economy"", ""_not_a_feature_"")

dfmat <- tokens(data_corpus_inaugural) %>%
  tokens_select(thetarget) %>%
  dfm()

docfreq(dfmat) / ndoc(dfmat)
##    economy   congress    nuclear 
## 0.52542373 0.49152542 0.08474576

to get the data.frame in the question:
df <- data.frame(
  docname = docnames(dfmat),
  year = docvars(dfmat, c(""year"")),
  contains_target = as.logical(rowsums(dfmat)),
  row.names = null
)

head(df)
##           docname year contains_target
## 1 1789-washington 1789            true
## 2 1793-washington 1793           false
## 3      1797-adams 1797            true
## 4  1801-jefferson 1801            true
## 5  1805-jefferson 1805           false
## 6    1809-madison 1809            true",https://stackoverflow.com/questions/69520218,r,11-10-2021 01:43,89.0,0.0,1.0,True,11-10-2021 06:54,11-10-2021 06:42
77091208,pytorch simcse loss implementation,"iï¿½ï¿½ï¿½m looking to implement the supervised simple contrastive learning of sentence embeddings (simcse) loss using positive and negative pairs with pytorch.

is there a way to vectorize the naï¿½ï¿½ve implementation below with broadcasting operations and/or matrix multiplication?
import torch
import torch.nn.functional as f


batch_size = 4
feature_dim = 1024
h = -2*torch.randn(batch_size, 3, feature_dim)+1  # (batch dim, contrastive triplet, features dim)
temp = 10

num = torch.exp(f.cosine_similarity(h[:, 0, :], h[:, 1, :], dim=1) / temp)

denom = torch.empty_like(num)
for j in range(batch_size):
    denomjj = 0
    for jj in range(batch_size):
        denomjj += torch.exp(f.cosine_similarity(h[j, 0, :], h[jj, 1, :], dim=0) / temp)
        denomjj += torch.exp(f.cosine_similarity(h[j, 0, :], h[jj, 2, :], dim=0) / temp)
    denom[j] = denomjj

loss = -torch.log(num / denom)
</p","['python', 'machine-learning', 'pytorch', 'nlp']",77097615,"sure.
your numerator seems fine so i'll vectorize the denominator.
i'll try to stick to your notation as closely as i can:

norm_hi = torch.sqrt(torch.sum(torch.square(h[:, 0, :]), dim=1))
norm_hj_plus = torch.sqrt(torch.sum(torch.square(h[:, 1, :]), dim=1))
norm_hj_minus = torch.sqrt(torch.sum(torch.square(h[:, 2, :]), dim=1))

sim_denom1 = torch.outer(norm_hi, norm_hj_plus) * temp
sim_denom2 = torch.outer(norm_hi, norm_hj_minus) * temp

v1 = h[:, 0, :] @ h[:, 1, :].t() / sim_denom1
v2 = h[:, 0, :] @ h[:, 2, :].t() / sim_denom2

vec_denom = torch.sum(torch.exp(v1) + torch.exp(v2), dim=1)


you can verify that this computes your denominator like this:
print(torch.allclose(loss, -torch.log(num / vec_denom)))",https://stackoverflow.com/questions/77091208,python,12-09-2023 16:52,596.0,1.0,1.0,True,13-09-2023 13:38,12-09-2023 18:16
77700760,can a named entity recognition (ner) spacy model or any code like an entity ruler around it catch my new further date patterns also as date entities?,"anonymization of entities found by a ner model
i try to anonymize files by means of a ner model for german text that sometimes may have a few english words. if i take spacy ner models for german and english like de_core_news_sm and en_core_web_sm, they find town names or persons, and at least the english model finds ""dezember 2022"", but it does not find the full date like ""15. dezember 2022"".
changing the entity recognition
i cannot change the matches of the model. i thought i could take an entity ruler to change the ner model, but the ner model seems to be fixed, and i do not know how my own entity ruler can outweigh the spacy ner model, and also, how i can get any entity ruler to work at all, even if i disable the ner model. i shifted the entity ruler before the ner model in the spacy pipeline, but i do not see any new replacements in the output.
easy example, mainly from the main spacy guide at using the entity ruler:
from spacy.lang.de import german

nlp = german()
ruler = nlp.add_pipe(""entity_ruler"")

patterns = [
    {""label"": ""date"", ""pattern"": [               
        {""lower"": {""regex"": ""(?:0?[1-9]|[12][0-9]|3[01])[\.\s]{1,2}?(jan(?:uar)?|feb(?:ruar)?|mï¿½ï¿½r(?:z)?|apr(?:il)?|mai|jun(?:i)?|jul(?:i)?|aug(?:ust)?|sep(?:t(?:ember)?)?|okt(?:ober)?|nov(?:ember)?|dez(?:ember)?)\.?\s?['`]?\d{0,4}""}},
        {""shape"": {""regex"": ""(?:0?[1-9]|[12][0-9]|3[01])[\.\s]{1,2}?(0?[1-9]|1[0-2])\.?\s?['`]?\d{0,4}""}},
        {""lower"": {""regex"": ""(?:jan(?:uar)?|feb(?:ruar)?|mï¿½ï¿½r(?:z)?|apr(?:il)?|mai|jun(?:i)?|jul(?:i)?|aug(?:ust)?|sep(?:t(?:ember)?)?|okt(?:ober)?|nov(?:ember)?|dez(?:ember)?)\.?\s?['`]?\d{2,4}""}},
        {""lower"": {""regex"": ""(?:januar|feb(?:ruar)?|mï¿½ï¿½r(?:z)?|apr(?:il)?|mai|jun(?:i)?|jul(?:i)?|aug(?:ust)?|sep(?:t(?:ember)?)?|okt(?:ober)?|nov(?:ember)?|d\.?)""}},
        {""shape"": ""dd""},
        {""text"": {""in"": [""15""]}}
    ]},
    {""label"": ""org1"", ""pattern"": {""lower"": ""apple""}},
    {""label"": ""gpe1"", ""pattern"": {""lower"": ""san""}},
    {""label"": ""date1"", ""pattern"": {""text"": [{""regex"": ""^(?:0?[1-9]|[12][0-9]|3[01])$""}]}}
]
ruler.add_patterns(patterns)

# taking the german dezember here for the test of the german regex
doc = nlp(""apple erï¿½ï¿½ffnet ein bï¿½ï¿½ro in san francisco am 15. dezember 2022."")

output:
[]

question
can i code around a named entity recognition (ner) spacy model to catch further date patterns also as date entities so that this will outweigh the choice of the ner model?
the aim is that the full ""15. dezember 2022"" is found as one date entity.

ps
duplicate?
i found spacy how to add patterns to existing entity ruler? that tells me to retrain the custom entity ruler and do not add patterns since the questioner has trained the ner model:

i have an existing trained custom ner model with ner and entity ruler
pipes. i want to update and retrain this existing pipeline.

the question is ""how to add patterns to existing entity ruler?"" asks more or less the same as i do here. but since the ner model is a custom one, the answers tell you to retrain the ner model with those patterns. that is why this question here is hopefully not a duplicate: i cannot retrain the ner model since it is a ready-made download from spacy.
catastrophic forgetting?
mind that the answers there tell you not to ever add an entity ruler at all to the ner model if you can retrain your ner model since it may lead to ""catastrophic forgetting"" of the already trained ner model, read there for more. if that is right, i wonder what i am doing here at all since that would mean that i cannot merge the entity recognition that the spacy ner model is trained on with another entity ruler. i highly doubt that this is true. why should i not be able to check a text for some patterns of some entities and then run the spacy ner model on top of that, and then let the first found entities outweigh the second? why should that lead to catastrophic forgetting if we speak about two models? catastrophic forgetting means that the ner model gets retrained on only the new text that i take for the entity ruler. my new input text would be just one sentence with a date. then, it would be easy to find out whether catastrophic forgetting happens at all. i can just run the pipeline on a sentence with more entities other than dates and see what happens.
yet, i guess that my thoughts here are wrong, so that we do not have two models, but instead one entity recognition model that is a merger of the entity ruler and the ner model. that is also how i understood the entity ruler in the first place. but even then, i can still test this on catastrophic forgetting easily: if the entity recognition gets much worse on a big file, then i know that there is catastrophic forgetting. if you ask me, this sounds too strange to be true. i doubt that the answers of the other question are right.","['python', 'python-3.x', 'spacy', 'named-entity-recognition', 'spacy-3']",77724359,"main things
each match is one label
you have to list label under label, you cannot just put all of the regex patterns into one label. see a good code that underlines this at add multiple entityruler with spacy (valueerror: 'entity_ruler' already exists in pipeline).
pattern format
you have to write orth, text or lower (and not ""shape"" as i tried it above) and then in a nested bracket regex. see full list at spacy - matcher - patterns.
no embedded spaces in regex
and you cannot regex match the already tokenized data with words that have spaces - since the tokenizer has already split the data into tokens by means of these spaces. there aren't any spaces left in the tokenized data. the only way to match them is to break up any regex with embedded \s+ into separate match tokens, see no answer to this question at:

how to use standard regex with spacy's matcher or phrasematcher while allowing spaces inside the regex
which is linked with:

adding regex entities to spacy's matcher.


squared brackets
in the spacy guide on the entity ruler, the code example ""explosion/spacy/master/spacy/pipeline/entityruler.py"" that puts two matches in a row instead of a regex with embedded spaces is not bad coding, but needed just like that:
{'label': 'gpe', 'pattern': [{'lower': 'san'}, {'lower': 'francisco'}]}
astonishingly, you have to write such squared brackets not just for two or more tokens (which means that they are neighboured, in a row), but even one token needs these squared brackets if you add the ""lower"" attribute at the beginning!(!) you would think that the squared brackets are just a start of a list, which they are, but the list format seems to be needed also for just one match. i checked this with {'label': 'gpe', 'pattern': [{'lower': 'apple'}]}, which worked, while it did not work without the squared brackets, the code did not find the word ""apple"" as an entity, only ""apple"".
code example
a good guide that wraps it up is at:

a basic named entity recognition (ner) with spacy in 10 lines of code in python, which is followed by:
a closer look at entityruler in spacy rule-based matching -> try with entityruler. it shows how rule-based matching in spacy works, both for phrase matcher and token matcher.

fixed code
with these hints, i could find an answer to the question above.
from spacy.lang.de import german

nlp = german()
ruler = nlp.add_pipe(""entity_ruler"")
patterns = [
    {""label"": ""org1"", ""pattern"": {""lower"": ""apple""}},
    {""label"": ""org2"", ""pattern"": [{""lower"": ""apple""}]},
    {""label"": ""gpe1"", ""pattern"": {""lower"": ""san""}},
    {""label"": ""gpe2"", ""pattern"": [{""lower"": ""san""}]},
    {""label"": ""gpe4"", ""pattern"": [{""lower"": ""san""}, {""lower"": ""francisco""}]},
    {""label"": ""date1"", ""pattern"": {""text"": [{""regex"": ""^(?:0?[1-9]|[12][0-9]|3[01])$""}]}},
    {""label"": ""date2"", ""pattern"": [{""text"": {""regex"": ""^(?:0?[1-9]|[12][0-9]|3[01])$""}}]},
    {""label"": ""date3"", ""pattern"": [{""text"": {""regex"": ""(?:0?[1-9]|[12][0-9]|3[01])""}}, {""lower"": {""regex"": ""(jan(?:uar)?|feb(?:ruar)?|mï¿½ï¿½r(?:z)?|apr(?:il)?|mai|jun(?:i)?|jul(?:i)?|aug(?:ust)?|sep(?:t(?:ember)?)?|okt(?:ober)?|nov(?:ember)?|dez(?:ember)?)""}}, {""text"": {""regex"": ""['`]?\d{2,4}""}}]},
    {""label"": ""date4"", ""pattern"": [{""text"": {""regex"": ""(?:0?[1-9]|[12][0-9]|3[01])""}}, {""lower"": {""regex"": ""(jan(?:uar)?|feb(?:ruar)?|mï¿½ï¿½r(?:z)?|apr(?:il)?|mai|jun(?:i)?|jul(?:i)?|aug(?:ust)?|sep(?:t(?:ember)?)?|okt(?:ober)?|nov(?:ember)?|dez(?:ember)?)""}}]},
    {""label"": ""date5"", ""pattern"": [{""lower"": {""regex"": ""(?:jan(?:uar)?|feb(?:ruar)?|mï¿½ï¿½r(?:z)?|apr(?:il)?|mai|jun(?:i)?|jul(?:i)?|aug(?:ust)?|sep(?:t(?:ember)?)?|okt(?:ober)?|nov(?:ember)?|dez(?:ember)?)""}}, {""text"": "": ""['`]?\d{2,4}""}}]},
    {""label"": ""date6"", ""pattern"": [{""lower"": {""regex"": ""^(?:januar|feb(?:ruar)?|mï¿½ï¿½r(?:z)?|apr(?:il)?|mai|jun(?:i)?|jul(?:i)?|aug(?:ust)?|sep(?:t(?:ember)?)?|okt(?:ober)?|nov(?:ember)?|dez(?:ember)?)$""}}]}
            ]
ruler.add_patterns(patterns)

# taking the german dezember here for the test of the german regex
doc = nlp(""apple is opening its first big office in san francisco on 15. dezember 2022."")
print([(ent.text, ent.label_) for ent in doc.ents])

out:
[('apple', 'org2'), ('san francisco', 'gpe4'), ('15. dezember 2022', 'date3')]

mind that the same code, but with an english model will only find ""15 dezember 2022"":
from spacy.lang.en import english
nlp = english()

only if you run this on a german model, it will also find ""15. dezember 2022"" with the dot. i guess that in english, this dot is read as a full stop of a sentence. since the sentence tokenizer runs before the word tokenizer, the tokens ""15"", ""dezember"", and ""2022"" cannot be found together as one match anymore.
the code above also proves that you do not need to sort the patterns by the number of tokens, like ""15. dezember 2022"", ""15. dezember"", ""dezember 2022"", and ""dezember"", since it chooses the match with the most tokens by default. else it would catch the number of label ""date2"" at first, and then, the full date could not be found anymore.",https://stackoverflow.com/questions/77700760,python,21-12-2023 22:02,894.0,1.0,1.0,True,04-01-2024 17:06,28-12-2023 16:25
78489915,how to lemmatize text column in pandas dataframes using stanza?,"i read csv file into pandas dataframe.
my text column is df['story'].
how do i lemmatize  this colummn ?
should i tokenize before?","['pandas', 'nlp', 'tokenize', 'lemmatization', 'stanza']",78491545,"no, you don't necessarily have to tokenize before lemmatizing. you can try the following code:
import stanza
import pandas as pd

nlp = stanza.pipeline(lang='en', processors='tokenize,mwt,pos,lemma')

def lemmatize_text(text):
    doc = nlp(text)
    lemmas = [word.lemma for sent in doc.sentences for word in sent.words]
    return ' '.join(lemmas)

df['lemmatized_story'] = df['story'].apply(lemmatize_text)",https://stackoverflow.com/questions/78489915,pandas,16-05-2024 12:34,72.0,0.0,1.0,True,19-05-2024 10:14,19-05-2024 10:14
29169732,sentiment analysis of non-english texts,"i want to analyze sentiment of texts that are written in german. i found a lot of tutorials on how to do this with english, but i found none on how to apply it to different languages.
i have an idea to use the textblob python library to first translate the sentences into english and then to do sentiment analysis, but i am not sure whether or not it is the best way to solve this task.
or are there any other possible ways to solve this task?","['python', 'machine-learning', 'nlp', 'sentiment-analysis', 'textblob']",29179773,"as andy has pointed about above, the best approach would be to train your own classifier. another, more quick and dirty approach would be to use a german sentiment lexicon such as the sentiws, and compute the polarity of a sentence simply on the basis of the polarity values of its individual words (for example by summing them). this method isn't foolproof (it doesn't take negation into account, for example), but it would give reasonable results relatively quickly.",https://stackoverflow.com/questions/29169732,python,20-03-2015 15:07,9211.0,7.0,5.0,True,02-02-2022 14:44,20-03-2015 15:10
74587735,what ({!lookup.minortype == country}) means in jape for gate,"while i am going through jape(gate) learning resources, came across below peace of jape rule which is eliminating the(bold formatted) text from becoming the annotation.
jape rule: ({!lookup.minortype == country})
text: university of sheffield us
what exactly the meaning of the above statement? my quick interpretation is minortype shouldn't be equal to type country. but if that is true why the below statements are not working in the same way as above?
({lookup.minortype != country})
({lookup.minortype == !country})
any helpful links to understand lhs and rhs rule syntaxes in detailed manner would be appreciated.","['nlp', 'gate']",74980903,"finally, found explanation by myself at the below link.

rule: surnamenotstartingwithde  
(  
 {surname, !token.string ==~ ""[dd]e""}  
):name  
-->  
 :name.notde = {}

this would match any surname annotation that does not start at the same place as a token with the string ï¿½ï¿½ï¿½deï¿½ï¿½ï¿½ or ï¿½ï¿½ï¿½deï¿½ï¿½ï¿½. note that this is subtly diï¿½ï¿½ï¿½erent from {surname, token.string !=~ ""[dd]e""}, as the second form requires a token annotation to be present, whereas the ï¿½ï¿½ï¿½rst form (!token...) will match if there is no token annotation at all at this location.
as per the example in the question, university of sheffield need to be a lookup annotation(which is actually not) to make it work in",https://stackoverflow.com/questions/74587735,nlp,27-11-2022 05:32,92.0,3.0,1.0,True,03-01-2023 07:40,27-11-2022 11:30
58841995,using nlp.pipe() in spacy to get doc objects for dataframe column,"i am using spacy nlp.pipe() for getting doc objects for text data in pandas dataframe column but the parsed text returned as ""text"" in the code has length of only 32. however, the shape of dataframe is (14640, 16).
here is the data link if someone wants to read the data.
nlp = spacy.load(""en_core_web_sm"")
for text in nlp.pipe(iter(df['text']), batch_size = 1000, n_threads=-1):
  print(text)

len(text)

result:
32

can someone help me with this what is going on? what i am doing wrong?","['python', 'pandas', 'dataframe', 'nlp', 'spacy']",58852371,"according to the spacy documentation of doc object here, the __len__ operator gets ""the number of tokens in the document."".
the last text in your data is: 
>>> df['text'].values[-1]
@americanair we have 8 ppl so we need 2 know how many seats are on the next flight. plz put us on standby for 4 people on the next flight?

after running the nlp.pipe() method, this sentence will be tokenized into 32 tokens which what you're asking for. to verfiy that, try runn the following code after len(text) and will get the exact result:
>>> last_tokens = [token for token in text]
>>> last_tokens
[@americanair, we, have, 8, ppl, so, we, need, 2, know, how, many, seats, are, on, the, next, flight, ., plz, put, us, on, standby, for, 4, people, on, the, next, flight, ?]

>>> len(last_tokens)
32

edit
you can iterate over the tokens of each doc returned from the pipeline like so:
nlp = spacy.load(""en_core_web_sm"")
for text in nlp.pipe(iter(df['text']), batch_size = 1000, n_threads=-1):
    for token in text:
        print(token)
    print('\n')",https://stackoverflow.com/questions/58841995,python,13-11-2019 16:57,3030.0,2.0,1.0,True,02-07-2021 10:30,02-07-2021 10:30
74041326,how can i train a openai fine tuned model with more prompts,i fine-tuned openai model with some prompts following this documentation it succeeded and created a new model in the playground. how i can retrain (fine-tune) that same model with new prompts?,['openai-api'],74093468,"this is what i found out on open ai documentation

if you have already fine-tuned a model for your task and now have additional training data that you would like to incorporate, you can continue fine-tuning from the model. this creates a model that has learned from all of the training data without having to re-train from scratch.

i.e you can't train the already existing trained model again.",https://stackoverflow.com/questions/74041326,openai-api,12-10-2022 11:48,1745.0,0.0,1.0,True,19-12-2022 07:17,19-12-2022 07:17
9552073,extracting user interests from social profiles,"this is my first time dabbling in nlp so please excuse my ignorance.  i'm looking for a method to extract interests/likes/hobbies from users' social profiles. here is an example where all the interests/likes/hobbies are in bold:

""i consider myself a pretty diverse character... i'm a professional
  wrestler, but i'd take a bullet for wallï¿½ï¿½ï¿½e. i train like a one-man genocide machine in the gym, but i cried""armageddon."" i'll head bang to ac/dc, and i'm seriously
  considering getting a legend of zelda tattoo. i'm 420-friendly. i
  like to party it up with the frat crowd one night, hang out with
  my burning man friends the next, play halo and world of
  warcraft the next, and jam with friends that aren't any younger than
  40 the next. my youngest friend is 16, my oldest friend is 66. i'll
  sing karaoke at the bars, and i'm my friends' collective
  psychiatrist/shoulder.""

the profiles are plain text. there are no meta tags or ids associated with any of it, it's just a paragraph of text.
my naiive idea was to take each noun and match it against freebase to see if it's an activity/artist/movie/book etc.  the problem is that although most entities mentioned will be things the user likes, she will also mention things she doesn't like and i have no means of distinguishing the 2.  
i have 2 questions:

what sub field of nlp should i be looking at?  some googleable algorithms/techniques/authors would be greatly appreciated.
how hard is this problem?

thanks!","['nlp', 'machine-learning', 'extract', 'information-extraction']",9553605,"first, unless using nlp to do this is a particular objective for you, check your problem domain to see if you can avoid it completely.
for instance:

do these profiles have tags (supplied either by the site or by the
user)?

what does the site's api make available (assuming that's how you    are accessing this data; if you are scraping it, then this doesn't of course apply)? a good example, facebook. if you read a user's posts,    you'll see words like ""wrestler"", ""karaoke"", etc. but if you look at    what fields are exposed via the graph api, you'll see that these    activities nearly always have an associated fb id.


i am not a specialist in this field, but i can recommend a couple of resources directed to nlp and which are accessible to the non-specialist or novice. the first is a text processing api. this simple web service uses rest and json io. it is free and seems to have a fairly large rate limit.
this api appears to rely heavily on the excellent natural language tooolkit (nltk) which is a mature stable library in python, that includes modules directed to the problem in your question, e.g., sentiment analysis, tagging and chunk extraction, etc.
which particular sub-domain is most relevant to solving the question in the op? i don't know, but i suspect there's a module somewhere in the nltk that does what you need. finding that module is hopefully just a matter of skimming the api documentation (which is organized by module); reading the getting started section which contains an excellent survey of nltk's modules as well as demos for all of each of them.",https://stackoverflow.com/questions/9552073,nlp,04-03-2012 03:30,984.0,2.0,1.0,True,14-10-2021 20:19,04-03-2012 20:07
78026820,getting &#39;appidnoautherror&#39; in iflytek spark integration,"i am trying to use spark api from iflytek where getting appidnoautherror
langchain reference:
please advise","['python', 'apache-spark', 'chat', 'langchain', 'large-language-model']",78150650,"i enabled only 3.5 chat model. so, i need to pass it to llm
chat = chatsparkllm(
    spark_api_url=""wss://spark-api.xf-yun.com/v3.5/chat"",
    spark_app_id=""****"", spark_api_key=""***"", spark_api_secret=""**""
)

then it worked fine, thanks",https://stackoverflow.com/questions/78026820,python,20-02-2024 11:05,85.0,-1.0,1.0,True,13-03-2024 00:20,21-02-2024 06:42
65817456,lda: topic model gensim gives same set of topics,"why am i getting same set of topics # words in gensim lda model? i used these parameters. i checked there are no duplicate documents in my corpus.
lda_model = gensim.models.ldamodel.ldamodel(corpus=my_corpus,
                                           id2word=word_and_id,
                                           num_topics=4, 
                                           minimum_probability=minimum_probability,
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto', # symmetric, asymmetric
                                           per_word_topics=true)

results
[
(0, '0.004*lily + 0.01*rose + 0.00*jasmine'),
(1, '0.005*geometry + 0.07*algebra + 0.01*calculation'),
(2, '0.003*painting + 0.001*brush + 0.01*colors'),
(3, '0.005*geometry + 0.07*algebra + 0.01*calculation')
]

notice: topic #1 and #3 are identical.","['python', 'nlp', 'gensim', 'lda', 'topic-modeling']",65822091,"each of the topics likely contains a large number of words weighted differently. when a topic is being displayed (e.g. using lda_model.show_topics()) you are going to get only a few words with the largest weights. this does not mean that there are no differences between topics among the remaining vocabulary.
you can steer the number of displayed words to inspect the remaining weights:
 show_topics(num_topics=4, num_words=10, log=false, formatted=true)

and change num_words parameter to include even more words.
now, there is also a possibility that:

the number of topics should be different (e.g. 3),
or minimum_probability smaller (what is the value you use?),
or number of passes larger,
chunksize smaller,
corpus larger (what is the size?) or stripped off of stop words (did you do that?).

i encourage you to experiment with different values of these parameters to check if any of the combination works better.",https://stackoverflow.com/questions/65817456,python,20-01-2021 20:49,2096.0,1.0,2.0,True,09-04-2022 21:03,21-01-2021 06:21
42068474,tfidfvectorizer: how does the vectorizer with fixed vocab deal with new words?,"i'm working on a corpus of ~100k research papers. i'm considering three fields:

plaintext
title
abstract

i used the tfidfvectorizer to get a tfidf representation of the plaintext field and feed the thereby originated vocab back into the vectorizers of title and abstract to assure that all three representations are working on the same vocab. my idea was that since the the plaintext field is much bigger than the other two, it's vocab will most probably cover all the words in the other fields. but how would the tfidfvectorizer deal with new words/tokens if that wasn't the case?
here's an example of my code:
vectorizer = tfidfvectorizer(min_df=2)
plaintexts_tfidf = vectorizer.fit_transform(plaintexts)
vocab = vectorizer.vocabulary_
# later in an another script after loading the vocab from disk
vectorizer = tfidfvectorizer(min_df=2, vocabulary=vocab)
titles_tfidf = vectorizer.fit_transform(titles)

the vocab has ~900k words.
during vectorization i didn't ran into any problems but later when i wanted to compare the similarity between the vectorized titles using sklearn.metrics.pairwise.cosine_similarity i ran into this error:
>> titles_sim = cosine_similarity(titles_tfidf)
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
<ipython-input-237-5aa86fe892da> in <module>()
----> 1 titles_sim = cosine_similarity(titles)

/usr/local/lib/python3.5/dist-packages/sklearn/metrics/pairwise.py in cosine_similarity(x, y, dense_output)
    916         y_normalized = normalize(y, copy=true)
    917 
--> 918     k = safe_sparse_dot(x_normalized, y_normalized.t, dense_output=dense_output)
    919 
    920     return k

/usr/local/lib/python3.5/dist-packages/sklearn/utils/extmath.py in safe_sparse_dot(a, b, dense_output)
    184         ret = a * b
    185         if dense_output and hasattr(ret, ""toarray""):
--> 186             ret = ret.toarray()
    187         return ret
    188     else:

/usr/local/lib/python3.5/dist-packages/scipy/sparse/compressed.py in toarray(self, order, out)
    918     def toarray(self, order=none, out=none):
    919         """"""see the docstring for `spmatrix.toarray`.""""""
--> 920         return self.tocoo(copy=false).toarray(order=order, out=out)
    921 
    922     ##############################################################

/usr/local/lib/python3.5/dist-packages/scipy/sparse/coo.py in toarray(self, order, out)
    256         m,n = self.shape
    257         coo_todense(m, n, self.nnz, self.row, self.col, self.data,
--> 258                     b.ravel('a'), fortran)
    259         return b
    260 

valueerror: could not convert integer scalar

i'm not really sure if it's related but i can't really see what's going wrong here. also because i'm not running into the error when calculating the similarities on the plaintext vectors.
am i missing something out? is there a better way to use the vectorizer?
edit:
the shapes of the sparse csr_matrices are equal.
>> titles_tfidf.shape
(96582, 852885)
>> plaintexts_tfidf.shape
(96582, 852885)","['python', 'scikit-learn', 'tf-idf', 'cosine-similarity', 'oov']",42069027,"i'm afraid the matrix might be too large. it would be 96582*96582=9328082724 cells. try to slice titles_tfidf a bit and check. 
source: 
edt:
if you are using older scipy/numpy version you might want to update: 
edt2:
also if you are using 32bit python, switching to 64bit might help (i suppose)
edt3:
answering your original question. when you use vocabulary from plaintexts and there will be new words in titles they will be ignored - but not influence tfidf value. hope this snippet may make it more understandable:
from sklearn.feature_extraction.text import tfidfvectorizer
from sklearn.metrics.pairwise import cosine_similarity

plaintexts =[""they are"", ""plain texts texts amoersand here""]
titles = [""and here"", ""titles "", ""wolf dog eagle"", ""but here plain""]

vectorizer = tfidfvectorizer()
plaintexts_tfidf = vectorizer.fit_transform(plaintexts)
vocab = vectorizer.vocabulary_
vectorizer = tfidfvectorizer(vocabulary=vocab)
titles_tfidf = vectorizer.fit_transform(titles)
print('values using vocabulary')
print(titles_tfidf)
print(vectorizer.get_feature_names())
print('brand new vectorizer')
vectorizer = tfidfvectorizer()
titles_tfidf = vectorizer.fit_transform(titles)
print(titles_tfidf)
print(vectorizer.get_feature_names())

result is:
values using vocabulary
  (0, 2)        1.0
  (3, 3)        0.78528827571
  (3, 2)        0.61913029649
['amoersand', 'are', 'here', 'plain', 'texts', 'they']
brand new vectorizer
  (0, 0)        0.78528827571
  (0, 4)        0.61913029649
  (1, 6)        1.0
  (2, 7)        0.57735026919
  (2, 2)        0.57735026919
  (2, 3)        0.57735026919
  (3, 4)        0.486934264074
  (3, 1)        0.617614370976
  (3, 5)        0.617614370976
['and', 'but', 'dog', 'eagle', 'here', 'plain', 'titles', 'wolf']

notice it is not the same as i would remove words that not occur in plaintexts from titles.",https://stackoverflow.com/questions/42068474,python,06-02-2017 13:01,14248.0,6.0,1.0,True,11-03-2024 20:27,11-03-2024 20:27
61134275,difficulty in understanding the tokenizer used in roberta model,"from transformers import automodel, autotokenizer

tokenizer1 = autotokenizer.from_pretrained(""roberta-base"")
tokenizer2 = autotokenizer.from_pretrained(""bert-base-cased"")

sequence = ""a titan rtx has 24gb of vram""
print(tokenizer1.tokenize(sequence))
print(tokenizer2.tokenize(sequence))

output:
['a', 'ï¿½ï¿½titan', 'ï¿½ï¿½rtx', 'ï¿½ï¿½has', 'ï¿½ï¿½24', 'gb', 'ï¿½ï¿½of', 'ï¿½ï¿½vr', 'am']
['a', 'titan', 'r', '##t', '##x', 'has', '24', '##gb', 'of', 'v', '##ra', '##m']
bert model uses wordpiece tokenizer. any word that does not occur in the wordpiece vocabulary is broken down into sub-words greedily. for example, 'rtx' is broken into 'r', '##t' and '##x' where ## indicates it is a subtoken. 
roberta uses bpe tokenizer but i'm unable to understand 
a) how bpe tokenizer works? 
b) what does g represents","['nlp', 'pytorch', 'huggingface-transformers', 'bert-language-model']",61136018,"this question is extremely broad, so i'm trying to give an answer that focuses on the main problem at hand. if you feel the need to have other questions answered, please open another question focusing on one question at a time, see the [help/on-topic] rules for stackoverflow.
essentially, as you've correctly identified, bpe is central to any tokenization in modern deep networks. i highly recommend you to read the original bpe paper by sennrich et al., in which they also highlight a bit more of the history of bpes. 
in any case, the tokenizers for any of the huggingface models are pretrained, meaning that they are usually generated from the training set of the algorithm beforehand. common implementations such as sentencepiece also give a bit better understanding of it, but essentially the task is framed as a constrained optimization problem, where you specify a maximum number of k allowed vocabulary words (the constraint), and the algorithm tries to then keep as many words intact without exceeding k.
if there are not enough words to cover the whole vocabulary, smaller units are used to approximate the vocabulary, which results in the splits observed in the example you gave.
roberta uses a variant called ""byte-level bpe"", the best explanation is probably given in this study by wang et al.. the main benefit is, that it results in a smaller vocabulary while maintaining the quality of splits, from what i understand.
the second part of your question is easier to explain; while bert highlights the merging of two subsequent tokens (with ##), roberta's tokenizer instead highlights the start of a new token with a specific unicode character (in this case, \u0120, the g with a dot). the best reason i could find for this was this thread, which argues that it basically avoids the use of whitespaces in training.",https://stackoverflow.com/questions/61134275,nlp,10-04-2020 04:58,12423.0,18.0,2.0,True,21-03-2022 19:44,10-04-2020 07:19
73643066,how to get hidden layer/state outputs from a bert model?,"based on the documentation provided here,  how can i read all the outputs, last_hidden_state (), pooler_output and hidden_state. in my sample code below, i get the outputs
from transformers import bertmodel, bertconfig

config = bertconfig.from_pretrained(""xxx"", output_hidden_states=true)
model = bertmodel.from_pretrained(""xxx"", config=config)

outputs = model(inputs)


when i print one of the output (sample below) . i looked through the documentation to see if i can use some functions of this class to just get the last_hidden_state values , but i'm not sure of the type here.
the value for the last_hidden_state =
tensor([[...

is it some class or tuple or array .
how can i get the values or array of values such as
[0, 1, 2, 3 , ...]

basemodeloutputwithpoolingandnoattention(
last_hidden_state=tensor([
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 
         11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],
        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 
         15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
        ...
        hidden_states= ...","['huggingface-transformers', 'bert-language-model']",73646409,"the basemodeloutputwithpoolingandcrossattentions you retrieve is class that inherits from ordereddict (code) that holds pytorch tensors. you can access the keys of the ordereddict like properties of a class and, in case you do not want to work with tensors, you can them to python lists or numpy. please have a look at the example below:
from transformers import berttokenizer, bertmodel

t = berttokenizer.from_pretrained(""bert-base-cased"")
m = bertmodel.from_pretrained(""bert-base-cased"")

i = t(""this is a test"", return_tensors=""pt"")
o = m(**i, output_hidden_states=true)

print(o.keys())
print(type(o.last_hidden_state))
print(o.last_hidden_state.tolist())
print(o.last_hidden_state.detach().numpy())

output:
odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])
<class 'transformers.modeling_outputs.basemodeloutputwithpoolingandcrossattentions'>
<class 'torch.tensor'>
[[[0.36328405141830444, 0.018902940675616264, 0.1893523931503296, ..., 0.09052444249391556, 1.4617693424224854, 0.0774402841925621]]]
[[[ 0.36328405  0.01890294  0.1893524  ... -0.0259465   0.38701165
    0.19099694]
  [ 0.30656984 -0.25377586  0.76075834 ...  0.2055152   0.29494798
    0.4561815 ]
  [ 0.32563183  0.02308523  0.665546   ...  0.34597045 -0.0644953
    0.5391255 ]
  [ 0.3346715  -0.02526359  0.12209094 ...  0.50101244  0.36993945
    0.3237842 ]
  [ 0.18683438  0.03102166  0.25582778 ...  0.5166369  -0.1238729
    0.4419385 ]
  [ 0.81130844  0.4746894  -0.03862225 ...  0.09052444  1.4617693
    0.07744028]]]",https://stackoverflow.com/questions/73643066,huggingface-transformers,08-09-2022 02:06,3118.0,1.0,1.0,True,08-09-2022 19:16,08-09-2022 19:16
78128694,huggingface seq2seqtrainer freezes on evaluation,"i'm currently trying to train a whisper model by following the fine tune whisper model tutorial. however, during the training phase where i call trainer.train(). i see the progress bar progresses through the training, but when it reaches the evaluation step defined at the training arguments, it will just freeze and the progress bar just stalls up. no error output, no nothing. and it will look like this.

i'm using kaggle notebooks to write the code with gpu p100 turned on. here are my training arguments leading up to the training function.
from transformers import whisperforconditionalgeneration

model = whisperforconditionalgeneration.from_pretrained(""openai/whisper-small"")

model.config.forced_decoder_ids = none
model.config.suppress_tokens = []
model.generation_config.language = ""en""

from transformers import seq2seqtrainingarguments

training_args = seq2seqtrainingarguments(
    output_dir=""./whisper-small-eng-gen"",  # change to a repo name of your choice
    per_device_train_batch_size=16,
    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=1000,
    gradient_checkpointing=true,
    fp16=true,
    evaluation_strategy=""steps"",
    per_device_eval_batch_size=8,
    predict_with_generate=true,
    generation_max_length=225,
    save_steps=1000,
    eval_steps=1000,
    logging_steps=25,
    report_to=[""tensorboard""],
    load_best_model_at_end=true,
    metric_for_best_model=""wer"",
    greater_is_better=false,
    push_to_hub=true,
    ignore_data_skip=true
)

from transformers import seq2seqtrainer

trainer = seq2seqtrainer(
    args=training_args,
    model=model,
    train_dataset=common_voice_train,
    eval_dataset=common_voice_test,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor.feature_extractor,
)


initially, the max_steps for training is 4000, and it always stalls at step 1001.

i think it is also worth noting that my dataset is streamed, and it is an iterable dataset.
any help is appreciated!
**update**
i edited my code to include verbose logging with
import transformers

transformers.logging.set_verbosity_info()

and this is the log after the evaluation step is reached.

you have passed language=en, but also have set forced_decoder_ids to [[1, none], [2, 50359]] which creates a conflict. forced_decoder_ids will be ignored in favor of language=en.","['python', 'huggingface-transformers', 'huggingface', 'openai-whisper', 'huggingface-trainer']",78271786,"yeah, i had this too. the thing to keep in mind is that after 1000 steps, your model saves, which can take some time depending on your machine's hardware. i'm currently running a very similar setup, but using the medium model instead of the small and the medium model is about 3 gb. so, be patient with it and it should finish, at least it did for me on my google colab instance.",https://stackoverflow.com/questions/78128694,python,08-03-2024 15:24,758.0,0.0,1.0,True,15-04-2024 20:01,15-03-2024 15:43
73164917,spacy: how to apply rule-based matcher on a dataframe?,"i have the following dataframe:
details = {
    'text_id' : [23, 21, 22, 21],
    'text' : ['all roads lead to rome', 
              'all work and no play makes jack a dull buy', 
              'any port in a storm', 
              'avoid a questioner, for he is also a tattler'],
}
  
# creating a dataframe object 
example_df = pd.dataframe(details)

i want to apply rule-based matcher of spacy on the text column in the dataframe to create a new column containing matches. let's assume the matches will be only verbs.
i define a function that takes dataframe, column name, and pattern as follows:
# import the matcher
from spacy.matcher import matcher

# load the pipeline and create the nlp object
nlp = spacy.load(""en_core_web_sm"")

# define rule-based matching function
def rb_match(df_name, col_name, pattern):

    # initialize the matcher with the shared vocab
    matcher = matcher(nlp.vocab)
    # add the pattern to the matcher using .add method
    pattern_name = ""pattern_%s"" %col_name  
    matcher.add(pattern_name, [pattern])
    
    # process some text and store it in new column
    # use nlp.pipe for better performance 
    df_name['text_spacy'] = [d for d in nlp.pipe(df_name[col_name])]
    
    # call the matcher on the doc, the result is a list of tuples
    df_name['matches_tuples'] = df_name['text_spacy'].apply(lambda x: matcher(x))
    
    # generate matches and store them in a new column
    df_name[""matches""] = [doc[start:end].text for match_id, start, end in df_name['matches_tuples']]
    
    return df_name

let's apply the function on the ""text"" column in the example dataframe to extract verbs:
rb_match(example_df, ""text"", [{""pos"":""verb""}] )

i have the following error message:
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
/tmp/ipykernel_33/185760541.py in <module>
----> 1 rb_match(example_df, ""text"", [{""pos"":""verb""}] )

/tmp/ipykernel_33/66914527.py in rb_match(df_name, col_name, pattern)
     13 
     14     # generate matches
---> 15     df_name[""matches""] = [doc[start:end].text for match_id, start, end in df_name['matches_tuples']]
     16 
     17     return df_name

/tmp/ipykernel_33/66914527.py in <listcomp>(.0)
     13 
     14     # generate matches
---> 15     df_name[""matches""] = [doc[start:end].text for match_id, start, end in df_name['matches_tuples']]
     16 
     17     return df_name

valueerror: not enough values to unpack (expected 3, got 1)

if we comment the following line df_name[""matches""] = [doc[start:end].text for match_id, start, end in df_name['matches_tuples']] in the function and reapply the function, we will get this output:
 text_id                                          text                                                text_spacy                  matches_tuples
0       23                        all roads lead to rome                              (all, roads, lead, to, rome)  [(12643752728212218961, 2, 3)]
1       21    all work and no play makes jack a dull buy     (all, work, and, no, play, makes, jack, a, dull, buy)  [(12643752728212218961, 5, 6)]
2       22                           any port in a storm                                 (any, port, in, a, storm)                              []
3       21  avoid a questioner, for he is also a tattler  (avoid, a, questioner, ,, for, he, is, also, a, tattler)  [(12643752728212218961, 0, 1)]

basically, the function returns a list of tuples, where each tuple has the form: (match_id, start index of matched span, end index of matched span). however, it cannot iterate over matches.
my question: how i can fix my function to return new column with matches? am i in the right direction if i want to apply it on a large dataframe or there is more efficient method?
thank you in advance!","['python', 'pandas', 'dataframe', 'nlp', 'spacy']",73195284,"from spacys discussion form on github, i have a solution from lj miranda:
if you want the matcher to immediately return the matches, you can replace that line with:
df_name['matches_tuples'] = df_name['text_spacy'].apply(lambda x: [i.text for i in matcher(x, as_spans=true)])
you can check the spacy matcher documentation for more information. i can't help you with your other questions because it's mostly related to the pandas library than with spacy. if you're worried about performance / efficiency, i highly recommend checking out their scaling user guide as a start.
hope that helps!",https://stackoverflow.com/questions/73164917,python,29-07-2022 10:01,645.0,0.0,3.0,True,01-08-2022 14:31,29-07-2022 10:20
31421413,"how to compute precision, recall, accuracy and f1-score for the multiclass case with scikit learn?","i'm working in a sentiment analysis problem the data looks like this:
label instances
    5    1190
    4     838
    3     239
    1     204
    2     127

so my data is unbalanced since 1190 instances are labeled with 5. for the classification im using scikit's svc. the problem is i do not know how to balance my data in the right way in order to compute accurately the precision, recall, accuracy and f1-score for the multiclass case. so i tried the following approaches:
first:
wclf = svc(kernel='linear', c= 1, class_weight={1: 10})
wclf.fit(x, y)
weighted_prediction = wclf.predict(x_test)

print 'accuracy:', accuracy_score(y_test, weighted_prediction)
print 'f1 score:', f1_score(y_test, weighted_prediction,average='weighted')
print 'recall:', recall_score(y_test, weighted_prediction,
                              average='weighted')
print 'precision:', precision_score(y_test, weighted_prediction,
                                    average='weighted')
print '\n clasification report:\n', classification_report(y_test, weighted_prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, weighted_prediction)

second:
auto_wclf = svc(kernel='linear', c= 1, class_weight='auto')
auto_wclf.fit(x, y)
auto_weighted_prediction = auto_wclf.predict(x_test)

print 'accuracy:', accuracy_score(y_test, auto_weighted_prediction)

print 'f1 score:', f1_score(y_test, auto_weighted_prediction,
                            average='weighted')

print 'recall:', recall_score(y_test, auto_weighted_prediction,
                              average='weighted')

print 'precision:', precision_score(y_test, auto_weighted_prediction,
                                    average='weighted')

print '\n clasification report:\n', classification_report(y_test,auto_weighted_prediction)

print '\n confussion matrix:\n',confusion_matrix(y_test, auto_weighted_prediction)

third:
clf = svc(kernel='linear', c= 1)
clf.fit(x, y)
prediction = clf.predict(x_test)


from sklearn.metrics import precision_score, \
    recall_score, confusion_matrix, classification_report, \
    accuracy_score, f1_score

print 'accuracy:', accuracy_score(y_test, prediction)
print 'f1 score:', f1_score(y_test, prediction)
print 'recall:', recall_score(y_test, prediction)
print 'precision:', precision_score(y_test, prediction)
print '\n clasification report:\n', classification_report(y_test,prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, prediction)


f1 score:/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: deprecationwarning: the default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or f-score with multiclass or multilabel data or pos_label=none will result in an exception. please set an explicit value for `average`, one of (none, 'micro', 'macro', 'weighted', 'samples'). in cross validation use, for instance, scoring=""f1_weighted"" instead of scoring=""f1"".
  sample_weight=sample_weight)
/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1172: deprecationwarning: the default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or f-score with multiclass or multilabel data or pos_label=none will result in an exception. please set an explicit value for `average`, one of (none, 'micro', 'macro', 'weighted', 'samples'). in cross validation use, for instance, scoring=""f1_weighted"" instead of scoring=""f1"".
  sample_weight=sample_weight)
/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1082: deprecationwarning: the default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or f-score with multiclass or multilabel data or pos_label=none will result in an exception. please set an explicit value for `average`, one of (none, 'micro', 'macro', 'weighted', 'samples'). in cross validation use, for instance, scoring=""f1_weighted"" instead of scoring=""f1"".
  sample_weight=sample_weight)
 0.930416613529

however, im getting warnings like this:
/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1172:
deprecationwarning: the default `weighted` averaging is deprecated,
and from version 0.18, use of precision, recall or f-score with 
multiclass or multilabel data or pos_label=none will result in an 
exception. please set an explicit value for `average`, one of (none, 
'micro', 'macro', 'weighted', 'samples'). in cross validation use, for 
instance, scoring=""f1_weighted"" instead of scoring=""f1""

how can i deal correctly with my unbalanced data in order to compute in the right way classifier's metrics?","['python', 'machine-learning', 'scikit-learn', 'nlp']",31575870,"i think there is a lot of confusion about which weights are used for what. i am not sure i know precisely what bothers you so i am going to cover different topics, bear with me ;).
class weights
the weights from the class_weight parameter are used to train the classifier.
they are not used in the calculation of any of the metrics you are using: with different class weights, the numbers will be different simply because the classifier is different.
basically in every scikit-learn classifier, the class weights are used to tell your model how important a class is. that means that during the training, the classifier will make extra efforts to classify properly the classes with high weights.
how they do that is algorithm-specific. if you want details about how it works for svc and the doc does not make sense to you, feel free to mention it.
the metrics
once you have a classifier, you want to know how well it is performing.
here you can use the metrics you mentioned: accuracy, recall_score, f1_score...
usually when the class distribution is unbalanced, accuracy is considered a poor choice as it gives high scores to models which just predict the most frequent class.
i will not detail all these metrics but note that, with the exception of accuracy, they are naturally applied at the class level: as you can see in this print of a classification report they are defined for each class. they rely on concepts such as true positives or false negative that require defining which class is the positive one.
             precision    recall  f1-score   support

          0       0.65      1.00      0.79        17
          1       0.57      0.75      0.65        16
          2       0.33      0.06      0.10        17
avg / total       0.52      0.60      0.51        50

the warning
f1 score:/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: deprecationwarning: the 
default `weighted` averaging is deprecated, and from version 0.18, 
use of precision, recall or f-score with multiclass or multilabel data  
or pos_label=none will result in an exception. please set an explicit 
value for `average`, one of (none, 'micro', 'macro', 'weighted', 
'samples'). in cross validation use, for instance, 
scoring=""f1_weighted"" instead of scoring=""f1"".

you get this warning because you are using the f1-score, recall and precision without defining how they should be computed!
the question could be rephrased: from the above classification report, how do you output one global number for the f1-score?
you could:

take the average of the f1-score for each class: that's the avg / total result above. it's also called macro averaging.
compute the f1-score using the global count of true positives / false negatives, etc. (you sum the number of true positives / false negatives for each class). aka micro averaging.
compute a weighted average of the f1-score. using 'weighted' in scikit-learn will weigh the f1-score by the support of the class: the more elements a class has, the more important the f1-score for this class in the computation.

these are 3 of the options in scikit-learn, the warning is there to say you have to pick one. so you have to specify an average argument for the score method.
which one you choose is up to how you want to measure the performance of the classifier: for instance macro-averaging does not take class imbalance into account and the f1-score of class 1 will be just as important as the f1-score of class 5. if you use weighted averaging however you'll get more importance for the class 5.
the whole argument specification in these metrics is not super-clear in scikit-learn right now, it will get better in version 0.18 according to the docs. they are removing some non-obvious standard behavior and they are issuing warnings so that developers notice it.
computing scores
last thing i want to mention (feel free to skip it if you're aware of it) is that scores are only meaningful if they are computed on data that the classifier has never seen.
this is extremely important as any score you get on data that was used in fitting the classifier is completely irrelevant.
here's a way to do it using stratifiedshufflesplit, which gives you a random splits of your data (after shuffling) that preserve the label distribution.
from sklearn.datasets import make_classification
from sklearn.cross_validation import stratifiedshufflesplit
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix

# we use a utility to generate artificial classification data.
x, y = make_classification(n_samples=100, n_informative=10, n_classes=3)
sss = stratifiedshufflesplit(y, n_iter=1, test_size=0.5, random_state=0)
for train_idx, test_idx in sss:
    x_train, x_test, y_train, y_test = x[train_idx], x[test_idx], y[train_idx], y[test_idx]
    svc.fit(x_train, y_train)
    y_pred = svc.predict(x_test)
    print(f1_score(y_test, y_pred, average=""macro""))
    print(precision_score(y_test, y_pred, average=""macro""))
    print(recall_score(y_test, y_pred, average=""macro""))",https://stackoverflow.com/questions/31421413,python,15-07-2015 04:17,334067.0,148.0,4.0,True,27-10-2024 12:23,27-10-2024 12:23
68974288,"group rows in pandas dataframe, apply custom function and store results in a new dataframe as rows","i have a pandas dataframe df_org with three columns - index (integer), titles (string) and dates (date).

i have a method process_title(text), which takes a string as input and tokenize, remove stop words and lemmatize the input string and returns the words as a list.
from nltk.tokenize import word_tokenize
from nltk.stem import wordnetlemmatizer
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
lemmatizer = wordnetlemmatizer()

def process_title(text):
    tokens = word_tokenize(text.lower())
    try:
        tokens.remove(""google"")
        tokens.remove(""search"")
        tokens.remove(""-"")
    except:
        pass

    lemm_tokens = list(map(lemmatizer.lemmatize,tokens))
    without_stop = [word for word in lemm_tokens if word not in stop_words]
    return without_stop

i want a new dataframe which contains three columns - word(string), frequency(integer), date(date). the word column contain words(single word) from the list returned by process_title(text), the frequency column contains the frequency of that word appearing on a given date and date column contains the date.
    ---------------------------------------
    |  word    | frequency     |   date   |
    ---------------------------------------
    | computer | 1             |2021-08-01|
    | science  | 1             |2021-08-01|
    | something| 5             |2021-08-02|
.....

how can i group the df_org dataframe along date and create the new dataframe? changes can be made to the process_title(text) method without compromising the end requirement.","['python', 'pandas', 'dataframe', 'nltk', 'data-analysis']",68974447,"you can use the dataframe.explode method, followed by groupby and size:
i am going to just use a simple .str.split instead of your function, as i don't know where word_tokenize comes from.
in [1]: import pandas as pd

in [2]: df = pd.dataframe({'title': ['hello world', 'foo bar'], 'date': ['2021-01-12t20:00', '2021-02-10t22:00']})

in [3]: df['words'] = df['title'].apply(lambda s: process_title(str(s)))

in [4]: df
out[4]:
         title              date           words
0  hello world  2021-01-12t20:00  [hello, world]
1      foo bar  2021-02-10t22:00      [foo, bar]

in [5]: exploded = df.explode('words')

in [6]: exploded
out[6]:
         title              date  words
0  hello world  2021-01-12t20:00  hello
0  hello world  2021-01-12t20:00  world
1      foo bar  2021-02-10t22:00    foo
1      foo bar  2021-02-10t22:00    bar

in [7]: exploded.groupby(['date', 'words']).size()
out[7]:
date              words
2021-01-12t20:00  hello    1
                  world    1
2021-02-10t22:00  bar      1
                  foo      1
dtype: int64",https://stackoverflow.com/questions/68974288,python,29-08-2021 15:14,86.0,0.0,1.0,True,13-07-2022 14:08,13-07-2022 14:08
69445437,default estimation method of gensim&#39;s word2vec skip-gram?,"i am now trying to use word2vec by estimating skipgram embeddings via nce (noise contrastive estimation) rather than conventional negative sampling method, as a recent paper did ( the paper has a replication github repository ( and it mainly relied on gensim for implementing word2vec, but the repository is not well organized and in a mess, so i have no clue about how the authors implemented nce estimation via gensim's word2vec.
the authors just used gensim's word2vec as a default status without including any options, so my question is what is the default estimation method for gensim's word2vec under skip-gram embeddings. nce? according to your manual,  it just says there is an option for negative sampling, and if set to 0, then no negative sampling is used. but then what estimation method is used?
negative (int, optional) ï¿½ï¿½ï¿½ if > 0, negative sampling will be used, the int for negative specifies how many ï¿½ï¿½ï¿½noise wordsï¿½ï¿½ï¿½ should be drawn (usually between 5-20). if set to 0, no negative sampling is used.
thanks you in advance, and look forward to hearing f","['python', 'nlp', 'gensim', 'word2vec']",69453906,"you can view the default parameters for the gensim word2vec model, in an unmodified gensim library, in the gensim docs. here's a link to the current version (4.1) docs for the word2vec constructor method, showing all default parameter values:


class gensim.models.word2vec.word2vec(sentences=none, corpus_file=none, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=none, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=, epochs=5, null_word=0, trim_rule=none, sorted_vocab=1, batch_words=10000, compute_loss=false, callbacks=(), comment=none, max_final_vocab=none, shrink_windows=true)

two of those parameters ï¿½ï¿½ï¿½ hs=0, negative=5 ï¿½ï¿½ï¿½ mean the default mode has hierarchical-softmax disabled, and negative-sampling enabled with 5 negative words. these have been the default of gensim's word2vec for many versions, so even other code is using an older version, this is likely the mode used (unless parameters or modified/overriden code changed",https://stackoverflow.com/questions/69445437,python,05-10-2021 05:49,394.0,0.0,1.0,True,05-10-2021 16:09,05-10-2021 08:44
73602754,spacy phrase matcher:typeerror when trying to remove matched phrases,"i am trying to effectively clean text that was derived from automatic speech recognition software using spacy phrase matcher (  the data is very dirty and does not separate speakers, so i am trying to remove repetitive phrases across all data samples.  using the rule-based phrase matcher, i am able to find the target text in my sample strings, but in trying to replace them with spaces, i receive a type error below:
typeerror: replace() argument 1 must be str, not spacy.tokens.token.token
my code is below:
# import the required libraries:
import spacy
from spacy.matcher import phrasematcher

# declare string from text extracted from a dataframe.  please note that there are many errors in the asr, including words recognized incorrectly such as ""mercado"" which a mis-translated utterance from the ivr.  

conv_str = ""welcome to companyx, to continue in english, please press one but i contin into mercado. hello, i am v companyx, virtual assistant to best serve you during our conversation. please provide your responses after i finished speaking in a few words please tell me what you're calling about. you can say something like i want to change my account information""

# call the matcher
matcher = phrasematcher(nlp.vocab, attr=""lower"")

# declare a list of strings to search for in another string

terms = [""welcome to companyx"", ""to continue in english, please press one"", ""virtual assistant"", ""in a few words please tell me what you're calling about"", ""companyx""]
# the stack overflow interface is incorrectly coloring some of the term strings, but it works in python

# create patterns variable
patterns = [nlp.make_doc(text) for text in terms]
matcher.add(""terminologylist"", patterns)

doc = nlp(conv_str)
matches = matcher(doc)
for match_id, start, end in matches:
    span = doc[start:end] # span is a list
    terms_not_needed = list(span)
    for item in terms_not_needed:
        conv_str.replace(item, ' ')

as i mentioned, i get the typeerror as printed out above.  i understand that the str.replace argument requires a string, but i was thinking that by declaring the span a list that i could iterate through that terms_not_needed list for individual string matches.  any guidance would be very helpful.","['python', 'nlp', 'spacy']",73604369,"there are a couple of issues with your approach here. one is that because of the way replace works, if you're using it there's no reason to use the phrasematcher - replace will already replace all instances of a string.
what i would do instead is use an on_match callback to set a custom attribute, say token._.ignore, to true for anything your matcher finds. then to get the tokens you're interested in you can just iterate over the doc and take every token where that value isn't true.
here's a modified version of your code that does this:
# import the required libraries:
import spacy
from spacy.tokens import token
from spacy.matcher import phrasematcher

token.set_extension(""ignore"", default=false)

# declare string from text extracted from a dataframe.  please note that there are many errors in the asr, including words recognized incorrectly such as ""mercado"" which a mis-translated utterance from the ivr.  

conv_str = ""welcome to companyx, to continue in english, please press one but i contin into mercado. hello, i am v companyx, virtual assistant to best serve you during our conversation. please provide your responses after i finished speaking in a few words please tell me what you're calling about. you can say something like i want to change my account information""

nlp = spacy.blank(""en"")
# call the matcher
matcher = phrasematcher(nlp.vocab, attr=""lower"")


def set_ignore(matcher, doc, id, matches):
    for _, start, end in matches:
        for tok in doc[start:end]:
            tok._.ignore = true

# declare a list of strings to search for in another string

terms = [""welcome to companyx"", ""to continue in english, please press one"", ""virtual assistant"", ""in a few words please tell me what you're calling about"", ""companyx""]
# the stack overflow interface is incorrectly coloring some of the term strings, but it works in python

# create patterns variable
patterns = [nlp.make_doc(text) for text in terms]
matcher.add(""terminologylist"", patterns, on_match=set_ignore)

doc = nlp(conv_str)
# this will run the callback
matcher(doc)

toks = [tok.text + tok.whitespace_ for tok in doc if not tok._.ignore]
print("""".join(toks))",https://stackoverflow.com/questions/73602754,python,04-09-2022 20:51,348.0,1.0,1.0,True,05-09-2022 03:47,04-09-2022 23:04
78067825,sending axios request to whisper endpoint with file input in client-side js,"i have a working nodejs code that makes a request to /audio/transcriptions endpoint on a localhosted openai api.
the file is given as fs.createreadstream(""speech.m4a"").
the node application works perfectly as it should, giving output: {text: ""<the transcription of the speech>""}
however when i want to do the same request, but with an input field instead of a hardcoded file, i get axioserror with the message 'network error'.
i have tried to call axios.post like this:
const api_url = ""
const fileinput = document.getelementbyid(""file-input"");
fileinput.onchange = () => {
  const files = fileinput.files;
  //if no file is selected
  if (files.length === 0) {
    alert(""please select a file!"");
    return;
  }
  const selectedfile = files[0];
  const reader = new filereader();
  reader.onload = (e) => {
    axios
      .post(
        api_url,
        {
          file: new file([e.target.result], { type: selectedfile.type }),
          model: ""whisper"",
          language: ""en"",
          temperature: 1,
          prompt: """",
        },
        {
          headers: {
            ""content-type"": ""multipart/form-data"",
          },
        }
      )
      .then((response) => {
        console.debug(response.data);
      })
      .catch((error) => {
        console.error(""error:"", error);
      });
  };
  reader.readasarraybuffer(selectedfile);
};

the result is the same, i get the same axioserror with the message network error.
i have also noticed that the response is 200 ok, so the transcript is generated.
the api response:
info: 172.26.160.1:62548 - ""post /openai/v1/audio/transcriptions  200 ok

the backend response:
info:faster_whisper:processing audio with duration 00:19.925
info:faster_whisper:detected language 'en' with probability 1.00
info:__main__:completed /tmp/tmpswy0__ru
info:__main__:transcription complete!

update: i have also switched from axios to fetch, it is still a valid request (response is 200 ok), but still i can't fetch the response
reader.onload = async (e) => {
    const body = new formdata();
    body.append(
      ""file"",
      new file([e.target.result], { type: selectedfile.type })
    );
    body.append(""model"", ""whisper"");
    body.append(""language"", ""en"");
    body.append(""temperature"", ""1"");
    body.append(""prompt"", """");
    await fetch(api_url, {
      method: ""post"",
      body: body,
    })
      .then((res) => {
        console.debug(res);
      })
      .catch((err) => {
        console.debug(err);
      });
  };

typeerror: failed to fetch at reader.onload

my question is: what can cause the issue with this request? what can i do to retrieve the response content as expected?","['node.js', 'axios', 'client', 'openai-api', 'openai-whisper']",78083870,"i have managed to resolve the issue by fixing cors error on the api's side. i had to add the following lines (especially allow_origins=[""*""])  to the api-code (fastapi written in python)
from fastapi.middleware.cors import corsmiddleware

app = fastapi()
app.add_middleware(
    corsmiddleware,
    allow_origins=[""*""],
    allow_credentials=true,
    allow_methods=[""*""],
    allow_headers=[""*""],
)",https://stackoverflow.com/questions/78067825,node.js,27-02-2024 13:13,223.0,0.0,1.0,True,11-08-2024 16:52,11-08-2024 16:52
74978793,openai gpt-3 api error: &quot;invalidrequesterror: unrecognized request argument supplied&quot;,"import openai

# set the api key
openai.api_key = ""your api key""

# define the conversation memory
conversation_memory = {
    ""previous_question"": ""what is the capital of france?"",
    ""previous_answer"": ""the capital of france is paris.""
}

# make the api request
response = openai.completion.create(
    model=""text-davinci-003"",
    prompt=""where is the eiffel tower located?"",
    temperature=0.5,
    max_tokens=1024,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0,
    conversation_memory=conversation_memory
)

# print the response
print(response.text)

why the conversation_memory  parameter not being recognize. i try this with serveral different models and they all give me the same error. i have the lastest openai on my computer. i don't understand.
here the error:
     invalidrequesterror                       traceback (most recent call last) <ipython-input-17-ace11d6ce405> in <module>      11      12 # make the api request ---> 13 response = openai.completion.create(      14     model=""text-babbage-001"",      15     prompt=""where is the eiffel tower located?"", c:\programdata\anaconda3\lib\site-packages\openai\api_resources\completion.py in create(cls, *args, **kwargs)      23 while true:      24 try: ---> 25 return super().create(*args, **kwargs)      26 except tryagain as e:      27 if timeout is not none and time.time() > start + timeout: c:\programdata\anaconda3\lib\site-packages\openai\api_resources\abstract\engine_api_resource.py in create(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)     113         )     114         url = cls.class_url(engine, api_type, api_version) --> 115         response, _, api_key = requestor.request(     116 ""post"",     117             url, c:\programdata\anaconda3\lib\site-packages\openai\api_requestor.py in request(self, method, url, params, headers, files, stream, request_id, request_timeout)     179             request_timeout=request_timeout,     180         ) --> 181 resp, got_stream = self._interpret_response(result, stream)     182 return resp, got_stream, self.api_key     183 c:\programdata\anaconda3\lib\site-packages\openai\api_requestor.py in _interpret_response(self, result, stream)     394 else:     395             return ( --> 396                 self._interpret_response_line(     397                     result.content, result.status_code, result.headers, stream=false     398                 ),  c:\programdata\anaconda3\lib\site-packages\openai\api_requestor.py in _interpret_response_line(self, rbody, rcode, rheaders, stream)     427         stream_error = stream and ""error"" in resp.data     428 if stream_error or not 200 <= rcode < 300: --> 429             raise self.handle_error_response(     430                 rbody, rcode, resp.data, rheaders, stream_error=stream_error     431             ) 
 invalidrequesterror: unrecognized request argument supplied: conversation_memory","['python', 'artificial-intelligence', 'openai-api', 'gpt-3']",74992998,"the error itself tells you what's wrong.
you're trying to pass conversation_memory as a parameter to the completions api endpoint, which is not a valid parameter.
see the complete list of parameters you can pass to the completions api endpoint:

model
prompt
suffix
max_tokens
temperature
top_p
n
stream
logprobs
echo
stop
presence_penalty
frequency_penalty
best_of
logit_bias
user",https://stackoverflow.com/questions/74978793,python,02-01-2023 03:28,6494.0,0.0,1.0,True,15-09-2023 11:40,13-03-2023 13:28
75904748,is it possible to train a deep learning model using low precision and subsequently fine-tune it with high precision?,"assuming a bert model is trained on fp16 and then fine-tuned on fp32 for a specific task, would this result in an increase or decrease in accuracy?
it can take less memory on gpu, training time will be reduced.","['tensorflow', 'deep-learning', 'pytorch', 'huggingface-transformers', 'bert-language-model']",75915348,"what you're referring to is called mixed-precision training, and it's basically training the model with low-precision floating-point numbers (e.g., fp16) for most of the layers and using high-precision numbers (e.g., fp32) only for certain layers that require more accuracy. fine-tuning a low-precision model with high precision on accuracy can vary depending on the specific model and task. in some cases, fine-tuning can result in an increase in accuracy, while in other cases it may not. here is a quick and dirty script to try for mixed precision, to see if it is worth trying as a temperature check:
import tensorflow as tf
import tensorflow_datasets as tfds
import tensorflow_text as text
import numpy as np
import os
import json
import math
import time

# set mixed precision policy
policy = tf.keras.mixed_precision.experimental.policy('mixed_float16')
tf.keras.mixed_precision.experimental.set_policy(policy)

# load bert model and tokenizer
bert_model_name = 'bert-base-cased'
bert_dir = f'bert_models/{bert_model_name}'
tokenizer = berttokenizer.from_pretrained(bert_dir)
bert_model = tfbertforsequenceclassification.from_pretrained(bert_dir)

# define optimizer
optimizer = tf.keras.optimizers.adam(learning_rate=3e-5)

# define loss function
loss = tf.keras.losses.sparsecategoricalcrossentropy(from_logits=true)

# define metrics
metric = tf.keras.metrics.sparsecategoricalaccuracy('accuracy')

# define batch size
batch_size = 32

# load training data
train_data = tfds.load('glue/mrpc', split='train', shuffle_files=true)
train_data = train_data.batch(batch_size)

# fine-tune bert model
epochs = 5
for epoch in range(epochs):
    start_time = time.time()
    metric.reset_states()
    for batch_idx, data in enumerate(train_data):
        input_ids = data['input_ids']
        attention_mask = data['attention_mask']
        token_type_ids = data['token_type_ids']
        labels = data['label']
        
        # cast input data to mixed precision
        input_ids = tf.cast(input_ids, tf.float16)
        attention_mask = tf.cast(attention_mask, tf.float16)
        token_type_ids = tf.cast(token_type_ids, tf.float16)
        labels = tf.cast(labels, tf.float16)
        
        with tf.gradienttape() as tape:
            outputs = bert_model(input_ids, attention_mask, token_type_ids)
            loss_value = loss(labels, outputs.logits)
            
        grads = tape.gradient(loss_value, bert_model.trainable_weights)
        optimizer.apply_gradients(zip(grads, bert_model.trainable_weights))
        metric.update_state(labels, outputs.logits)
        
    epoch_time = time.time() - start_time
    print(f'epoch {epoch + 1}/{epochs}, loss: {loss_value:.4f}, accuracy: {metric.result().numpy():.4f}, time: {epoch_time:.2f}s')",https://stackoverflow.com/questions/75904748,tensorflow,01-04-2023 07:15,301.0,0.0,1.0,True,03-04-2023 00:37,01-04-2023 07:16
64881264,sql/bigquery text classification,"i need to implement a simple text classification using regex, and for this i thought to apply a simple case when statement, but rather than in case one condition is met, i want to iterate over all the cases.
for example,
with `table` as(
select 'it is undeniable that ai will change the landscape of the future. there is a frequent increase in the demand for ai-related jobs, especially in data science and machine learning positions. it is believed that artificial intelligence will change the world, just like how electricity changed the world about 100 years ago. as professor andrew ng has famously stated multiple times ï¿½ï¿½ï¿½artificial intelligence is the new electricity.ï¿½ï¿½ï¿½ we have advanced immensely in the field of artificial intelligence. with the increase in the processing and computational power, thanks to graphical processing units (gpus), and also due to the abundance of data, we have reached a position of supremacy in deep learning and modern algorithms.' as text
)
select
  case
    when regexp_contains(text, r'(?i)ai') then 'ai'
    when regexp_contains(text, r'(?i)computational power') then 'engineering'
    when regexp_contains(text, r'(?i)deep learning') then 'deep learning'
  end as topic,
  text
fr/code>
with this query, the text is classified as ai, because it is the first condition that is met, but it should be classified as ai, engineering and deep learning in an array or in three different rows, because all three conditions are met.
how can i classify the text applying all the regex/conditions?","['sql', 'text', 'google-bigquery', 'text-mining', 'mining']",64881751,"i feel the below is the most generic and reusable solution (bigquery standard sql):
#standardsql
with `table` as(
select 'it is undeniable that ai will change the landscape of the future. there is a frequent increase in the demand for ai-related jobs, especially in data science and machine learning positions. it is believed that artificial intelligence will change the world, just like how electricity changed the world about 100 years ago. as professor andrew ng has famously stated multiple times ï¿½ï¿½ï¿½artificial intelligence is the new electricity.ï¿½ï¿½ï¿½ we have advanced immensely in the field of artificial intelligence. with the increase in the processing and computational power, thanks to graphical processing units (gpus), and also due to the abundance of data, we have reached a position of supremacy in deep learning and modern algorithms.' as text
), classification as (
  select 'ai' term, 'ai' topic union all
  select 'computational power', 'engineering' union all
  select 'deep leep learning'
), pattern as (
  select r'(?i)' || string_agg(term, '|') as regexp_pattern
  from classification
)
select
   array_to_string(array(
    select distinct topic
    from unnest(regexp_extract_all(lower(text), regexp_pattern)) term
    join classification using(term)
   ), ', ') topics,
  text
from `table`, pattern

with output:",https://stackoverflow.com/questions/64881264,sql,17-11-2020 18:35,545.0,0.0,4.0,True,06-10-2023 23:44,06-10-2023 23:44
61000500,tensorflow/keras/bert multiclass text classification accuracy,"i'm attempting to fine-tune the huggingface tfbertmodel to be able to classify some text to a single label. i have the model up and running, however the accuracy is extremely low from the start. my expectation is that the accuracy would be high given that it is using the bert pre-trained weights as a starting point. i was hoping to get some advice on where i'm going wrong.
i'm using the bbc-text dataset from here:
load data
df = pd.read_csv(open(<s3 url>),encoding='utf-8', error_bad_lines=false)
df = df.sample(frac=1)
df = df.dropna(how='any')

value counts
sport            511
business         510
politics         417
tech             401
entertainment    386
name: label, dtype: int64

preprocessing
def preprocess_text(sen):
# convert html entities to normal
sentence = unescape(sen)

# remove html tags
sentence = remove_tags(sentence)

# remove newline chars
sentence = remove_newlinechars(sentence)

# remove punctuations and numbers
sentence = re.sub('[^a-za-z]', ' ', sentence)

# convert to lowercase
sentence = sentence.lower()

return sentence


def remove_newlinechars(text):
    return "" "".join(text.splitlines()) 

def remove_tags(text):
    tag_re = re.compile(r'<[^>]+>')
    return tag_re.sub('', text)

df['text_prepd'] = df['text'].apply(preprocess_text)

split data
train, val = train_test_split(df, test_size=0.30, shuffle=true, stratify=df['label'])

encode labels
from sklearn.preprocessing import labelencoder

label_encoder = labelencoder()
y_train = np.asarray(le.fit_transform(train['label']))
y_val = np.asarray(le.fit_transform(val['label']))


define bert input function
# initialise bert tokenizer
bert_tokenizer_transformer = berttokenizer.from_pretrained('bert-base-cased')

def create_input_array(df, tokenizer, args):
    sentences = df.text_prepd.values

    input_ids = []
    attention_masks = []
    token_type_ids = []

    for sent in tqdm(sentences):
        # `encode_plus` will:
        #   (1) tokenize the sentence.
        #   (2) prepend the `[cls]` token to the start.
        #   (3) append the `[sep]` token to the end.
        #   (4) map tokens to their ids.
        #   (5) pad or truncate the sentence to `max_length`
        #   (6) create attention masks for [pad] tokens.
        encoded_dict = tokenizer.encode_plus(
            sent,  # sentence to encode.
            add_special_tokens=true,  # add '[cls]' and '[sep]'
            max_length=args.max_seq_len,  # pad & truncate all sentences.
                pad_to_max_length=true,
                return_attention_mask=true,  # construct attn. masks.
                return_tensors='tf',  # return tf tensors.
            )

        # add the encoded sentence to the list.
        input_ids.append(encoded_dict['input_ids'])

        # and its attention mask (simply differentiates padding from non-padding).
        attention_masks.append(encoded_dict['attention_mask'])

        token_type_ids.append(encoded_dict['token_type_ids'])

    input_ids = tf.convert_to_tensor(input_ids)
    attention_masks = tf.convert_to_tensor(attention_masks)
    token_type_ids = tf.convert_to_tensor(token_type_ids)

    return input_ids, attention_masks, token_type_ids


convert data to bert inputs
train_inputs = [create_input_array(train[:], tokenizer=tokenizer, args=args)]
val_inputs = [create_input_array(val[:], tokenizer=tokenizer, args=args)]

for train_inputs, y_train and val_inputs, y_val i then apply the below function which reshapes and converts to numpy arrays. the returned list from this function is then passed as arguments to the keras fit method. i realise this is a bit overkill converting to tf.tensors then to numpy, but i don't think this has an impact. i was originally trying to use tf.datasets but switched to numpy.
def convert_inputs_to_tf_dataset(inputs,y, args):
    # args.max_seq_len = 256
    ids = inputs[0][1]
    masks = inputs[0][1]
    token_types = inputs[0][2]

    ids = tf.reshape(ids, (-1, args.max_seq_len))
    print(""input ids shape: "", ids.shape)
    masks = tf.reshape(masks, (-1, args.max_seq_len))
    print(""input masks shape: "", masks.shape)
    token_types = tf.reshape(token_types, (-1, args.max_seq_len))
    print(""token type ids shape: "", token_types.shape)

    ids=ids.numpy()
    masks = masks.numpy()
    token_types = token_types.numpy()

    return [ids, masks, token_types, y]

keras model
# args.max_seq_len = 256
# n_classes = 6
model = tfbertforsequenceclassification.from_pretrained('bert-base-uncased', trainable=true, num_labels=n_classes)

input_ids_layer = input(shape=(args.max_seq_len, ), dtype=np.int32)
input_mask_layer = input(shape=(args.max_seq_len, ), dtype=np.int32)
input_token_type_layer = input(shape=(args.max_seq_len,), dtype=np.int32)

bert_layer = model([input_ids_layer, input_mask_layer, input_token_type_layer])[0]
flat_layer = flatten()(bert_layer)
dropout= dropout(0.3)(flat_layer)
dense_output = dense(n_classes, activation='softmax')(dropout)

model_ = model(inputs=[input_ids_layer, input_mask_layer, input_token_type_layer], outputs=dense_output)


compile and fit
loss = tf.keras.losses.sparsecategoricalcrossentropy(from_logits=true)
metric = tf.keras.metrics.sparsecategoricalaccuracy('accuracy')
model.compile(optimizer='adam', loss=loss, metrics=[metric])
model.fit(inputs=..., outputs=..., validation_data=..., epochs=50, batch_size = 32, metrics=metric, verbose=1)


epoch 32/50
1401/1401 [==============================] - 42s 30ms/sample - loss: 1.6103 - accuracy: 0.2327 - val_loss: 1.6042 -
 val_accuracy: 0.2308

as i'm using bert, only a few epochs are necessary, so i was expecting something much higher than 23% after 32 epochs.","['python', 'tensorflow', 'machine-learning', 'keras', 'huggingface-transformers']",61386745,"the main problem is in this line: ids = inputs[0][1]. actually, the ids are the first element of inputs[0]; so it should be ids = inputs[0][0].
but there is also another problem which might result in inconsistent validation accuracy: you should fit the labelencoder only one time to construct the label mapping; so you should use the transform method, instead of fit_transform, on validation labels.
further, don't use both softmax activation and from_logits=true in loss function simultaneously; only use either of them (see here for more info).
another point is that you might need to use a lower learning rate for the optimizer. the default learning rate of adam optimizer is 1e-3, which might be too high considering that you are fine-tuning a pretrained model. try a lower learning rate, say 1e-4 or 1e-5; e.g. tf.keras.optimizers.adam(learning_rate=1e-4). a high learning rate for fine-tuning a pretrained model might destroy the learned weights and disrupts fine-tuning process (due to the large gradient values generated, especially at the start of fine-tuning process).",https://stackoverflow.com/questions/61000500,python,02-04-2020 20:25,3833.0,2.0,2.0,True,10-06-2021 08:21,22-04-2020 18:03
68185061,strange results with huggingface transformer[marianmt] translation of larger text,"i need to translate large amounts of text from a database. therefore, i've been dealing with transformers and models for a few days. i'm absolutely no data science expert and unfortunately i don't get any further.
the problem starts with longer text. the 2nd issue is the usual-maximum token size (512) of the sequencers. just truncating is not really an option. here i did  find a work-around, but it does not work properly and the result is a word salad on longer texts (>300 sequences)
here an example (please ignore the warnings, this is another issues - which does not hurt currently that much);
if i take the example sentence 2 (55 seq) or 5 times (163 sequences) - no issues.
but it get messed up with e.g. 433 sequences (the 3rd green text block in the screenshot).

with more than 510 sequences, i tried to split it up in chunks as in the upper described link. but the result here is as well pretty strange.
i am pretty sure - that i have more than just one mistake and underestimated this topic.
but i see no alternative (free/cheap) way for translating big amount of text.
can you guys help me out? which (thinking) errors do you see and how would you suggest to solve the issues? thank you very much.

from transformers import autotokenizer, automodelforseq2seqlm
import torch

if torch.cuda.is_available():  
  dev = ""cuda""
else:  
  dev = ""cpu"" 
device = torch.device(dev)
 
mname = 'helsinki-nlp/opus-mt-de-en'
tokenizer = autotokenizer.from_pretrained(mname)
model = automodelforseq2seqlm.from_pretrained(mname)
model.to(device)

chunksize = 512

text_short = ""nach nur sieben seiten appellierte man an die wï¿½ï¿½hlerinnen und wï¿½ï¿½hler, sich richtig zu entscheiden, nï¿½ï¿½mlich fï¿½ï¿½r frieden, freiheit, sozialismus. ""
text_long = text_short
#this loop is just for debugging/testing and simulating long text
for x in range(30):
    text_long = text_long + text_short

tokens = tokenizer.encode_plus(text_long, return_tensors=""pt"", add_special_tokens=true, padding=false, truncation=false).to(device)
str_len = len(tokens['input_ids'][0])

if str_len > 510:
    # split into chunks of 510 tokens, we also convert to list (default is tuple which is imut_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))
    mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))

    cnt = 1
    for tensor in input_id_chunks:
        print('\033[96m' + 'chunk ' + str(cnt) + ': ' + str(len(tensor)) + '\033[93m')
        cnt += 1
    
    # loop through each chunk
    # 
    for i in range(len(input_id_chunks)):
        # add cls and sep tokens to input ids
        input_id_chunks[i] = torch.cat([
            torch.tensor([101]).to(device), input_id_chunks[i], torch.tensor([102]).to(device)
        ])
        # add attention tokens to attention mask
        mask_chunks[i] = torch.cat([
            torch.tensor([1]).to(device), mask_chunks[i], torch.tensor([1]).to(device)
        ])
        # get required padding length
        pad_len = chunksize - input_id_chunks[i].shape[0]
        # check if tensor length satisfies required chunk size
        if pad_len > 0:
            # if padding length is more than 0, we must add padding
            input_id_chunks[i] = torch.cat([
                input_id_chunks[i], torch.tensor([0] * pad_len).to(device)
            ])
            mask_chunks[i] = torch.cat([
                mask_chunks[i], torch.tensor([0] * pad_len).to(device)
            ])
   
    input_ids = torch.stack(input_id_chunks)
    attention_mask = torch.stack(mask_chunks)
    input_dict = {'input_ids': input_ids.long(), 'attention_mask': attention_mask.int()}
    
    outputs = model.generate(**input_dict)
    #this doesnt work - following error comes to the console --> ""host_softmax"" not implemented for 'long'
    #probs = torch.nn.functional.softmax(outputs[0], dim=-1)
    # probs
    # probs = probs.mean(dim=0)
    # probs
  
else:
    tokens[""input_ids""] = tokens[""input_ids""][:, :512] #truncating normally not necessary
    tokens[""attention_mask""] = tokens[""attention_mask""][:, :512]
    outputs = model.generate(**tokens)

decoded = tokenizer.decode(outputs[0], skip_special_tokens=true)
print('\033[94m' + str(str_len))
print('\033[92m' + decoded)

remark; following libs are necessary:

pip3 install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio===0.9.0 -f 


pip install transformers


pip install sentencepiece","['python', 'translation', 'huggingface-transformers', 'huggingface-tokenizers']",68518389,"to translate long texts with transformers you can split your text by paragraphs, paragraphs split by sentence and after that feed sentences to your model in batches. in any case it is better to translate with marianmt in a sentence-by-sentence way, because it can lose some parts if you feed a long text as a one piece to it.
from transformers import marianmtmodel, mariantokenizer
from nltk.tokenize import sent_tokenize
from nltk.tokenize import linetokenizer
import math
import torch

if torch.cuda.is_available():  
  dev = ""cuda""
else:  
  dev = ""cpu"" 
device = torch.device(dev)
 
mname = 'helsinki-nlp/opus-mt-de-en'
tokenizer = mariantokenizer.from_pretrained(mname)
model = marianmtmodel.from_pretrained(mname)
model.to(device)

lt = linetokenizer()
batch_size = 8

text_short = ""nach nur sieben seiten appellierte man an die wï¿½ï¿½hlerinnen und wï¿½ï¿½hler, sich richtig zu entscheiden, nï¿½ï¿½mlich fï¿½ï¿½r frieden, freiheit, sozialismus. ""
text_long = text_shorts = lt.tokenize(text_long)   
translated_paragraphs = []

for paragraph in paragraphs:
    sentences = sent_tokenize(paragraph)
    batches = math.ceil(len(sentences) / batch_size)     
    translated = []
    for i in range(batches):
        sent_batch = sentences[i*batch_size:(i+1)*batch_size]
        model_inputs = tokenizer(sent_batch, return_tensors=""pt"", padding=true, truncation=true, max_length=500).to(device)
        with torch.no_grad():
            translated_batch = model.generate(**model_inputs)
        translated += translated_batch
    translated = [tokenizer.decode(t, skip_special_tokens=true) for t in translated]
    translated_paragraphs += ["" "".join(translated)]

translated_text = ""\n"".join(translated_paragraphs)",https://stackoverflow.com/questions/68185061,python,29-06-2021 20:10,2921.0,3.0,2.0,True,23-12-2024 21:18,29-06-2021 20:46
67717406,how to use spacy nlp custom ner to identity 2 types of docs at once,"i want to make a spacy ner model that identifies and uses tags depending on what doc type it is.
the input is in json format. example-
{""text"":{""a"":""abc def."",""b"":""cde fg.""},
  ""annotations"":[
    {""start"":0,""end"":3,""doc_type"":""a"",""label"":{""text"":""first""},""text"":""abc""}, 
    {""start"":4,""end"":6,""doc_type"":""b"",""label"":{""text"":""second""},""text"":""fg""}
  ]
}

in this i want the model to identify that the 1st text is of type ""a"" so the text should be tagged with tag first. similarly second text is of type ""b"" so it the ner must be second
how can i go about this problem? thanks!","['python', 'nlp', 'spacy', 'named-entity-recognition']",67717654,"the description of your data is a little vague but given these assumptions:

you don't know if a document is type a or type b, you need to classify it.
the ner is completely different between type a and b documents.

what you should do is use (up to) three separate spacy pipelines. use the first pipeline with a textcat model to classify docs into a and b types, and then have one pipeline for ner for type a docs and one pipeline for type b docs. after classification just pass the text to the appropriate ner pipeline.
this is not the most efficient possible pipeline, but it's very easy to set up - you just train three separate models and stick them together with a little glue code.
you could also train the models separately and combine them in one spacy pipeline, with some kind of special component to make execution of the ner conditional, but that would be pretty tricky to set up so i'd recommend the separate pipelines approach first.
that said, depending on your problem it's possible that you don't need two ner models, and learning entities for both types of docs would be effective. so i would also recommend you try putting all your training data together, training just one ner model, and seeing how it goes. if that works then you can have a single pipeline with textcat and ner models that don't directly interact with each other.

to respond to the comment, when i say ""pipeline"" i mean a language object, which is what spacy.load returns. so you train models using the config and each of those is in a directory and then you do this:
import spacy

classifier = spacy.load(""classifier"")
ner_a = spacy.load(""ner_a"")
ner_b = spacy.load(""ner_b"")

texts = [""i like cheese"", ... raw texts ... ]

for text in texts:
    doc = classifier(text)
    if doc.cats[""a""] > doc.cats[""b""]:
        nerdoc = ner_a(text)
    else:
        nerdoc = ner_b(text)
    ... do something with the doc here ...",https://stackoverflow.com/questions/67717406,python,27-05-2021 07:19,685.0,1.0,1.0,True,28-05-2021 05:55,27-05-2021 10:16
64813628,using regex to encompass a group of keys in a dictionary and match them inside of a list of strings,"i'm new to text-cleaning in python but i currently created a dictionary with various slang words/acronyms/contractions that looks something like this:

fulltext = {'byob': 'bring your own beer', 'couldn't': 'could not', 'finna': 'going to'}... etc.

and i have another large corpus of text data:

uncleaned_text = ['this is finna be crazy', 'i don't know why we couldn't be there', 'i should have known when the event was byob that it would be terrible']

for which i am trying to 'clean' by replacing those words inside the list of strings that match the dictionary keys with their corresponding values. so, my ideal output would be:

cleaned text = ['this is going to be crazy', 'i don't know why we could not be there', 'i should have known when the event was bring your own beer that it would be terrible']

i know i should be using  regex in some way and i know i should be using loops, but i am definitely not even close to what i should be doing i think, because the error i get is builtin function not iterable...
any suggestions?
for sentence in uncleaned_text:
for word in sentence:
if word in fulltext.keys:
word.replace(word, fulltext.key)","['python', 'text', 'nlp', 'data-cleaning', 'regexp-replace']",64814059,"the error you are receiving is because dictionary.keys is a function not a list. so to get all the keys, you would want to use fulltext.keys() not fulltext.keys. the keys member of the dictionary class is a function that returns a list of the keys. the more pythonic way of checking if a specific word exists in the dictionary's keys is: if key in dictionary. the in operator checks to see if the left operand is a key in the dictionary, so you don't have to use the .keys function.
for the rest of the function i'd do the following:
clean_text = []
for sentence in uncleaned_text:
  for word in sentence.split():
    if word in fulltext: 
      sentence = sentence.replace(word, fulltext[word])
  clean_text.append(sentence)


the changes i made explained:

you'll need to split the sentence into words. the sentence is just a long string, so if you iterate over it you would get every character of the sentence individually. the .split method splits it on every space by default.
the replace method doesn't change the string in place, so you have to catch it in some other variable.
to get a value out of the dictionary, you'll need to use the key. word is our key in this case, so i changed fulltext.key to be fulltext[word]. this gets the value associated with word from the fulltext dictionary.
added an array to append the changed sentences to.

this will leave the original list (uncleaned_text) unchanged.",https://stackoverflow.com/questions/64813628,python,13-11-2020 00:02,118.0,0.0,2.0,True,13-11-2020 11:16,13-11-2020 11:16
76012610,failing to create a transformer from scratch and push it on cuda (use it on gpu),"in order to learn pytorch and understand how transformers works i  tried to implement from scratch (inspired from huggingface book) a transformer classifier:
from transformers import autotokenizer,datacollatorwithpadding
from bertviz.transformers_neuron_view import bertmodel
from transformers import autoconfig
import torch
from torch import nn
import torch.nn.functional as f
from math import sqrt

model_ckpt = ""bert-base-uncased""

# config = autoconfig.from_pretrained(model_ckpt)
tokenizer = autotokenizer.from_pretrained(model_ckpt)
# model = bertmodel.from_pretrained(model_ckpt)
config = {
    ""vocab_size"": 30522,
    ""hidden_size"": 768,
    ""max_position_embeddings"": 512,
    ""num_attention_heads"": 12,
    ""num_hidden_layers"": 12,
    ""hidden_dropout_prob"": 0.1,
    ""num_labels"": 6,
    ""intermediate_size"": 3072,
}
config = dotdict(config)


class dotdict(dict):
    """"""dot.notation access to dictionary attributes""""""
    __getattr__ = dict.get
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__

config = dotdict(config)

class embeddings(nn.module):
    def __init__(self, config):
        super().__init__()
        self.token_embeddings = nn.embedding(config.vocab_size, 
                                             config.hidden_size)
        self.position_embeddings = nn.embedding(config.max_position_embeddings,
                                                config.hidden_size)
        self.layer_norm = nn.layernorm(config.hidden_size, eps=1e-12)
        self.dropout = nn.dropout()

    def forward(self, input_ids):
        # create position ids for input sequence
        seq_length = input_ids.size(1)
        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)
        # create token and position embeddings
        token_embeddings = self.token_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids)
        # combine token and position embeddings
        embeddings = token_embeddings + position_embeddings
        embeddings = self.layer_norm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings

def scaled_dot_product_attention(query, key, value):
    dim_k = query.size(-1)
    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)
    weights = f.softmax(scores, dim=-1)
    return torch.bmm(weights, value)

class attentionhead(nn.module):
    def __init__(self, embed_dim, head_dim):
        super().__init__()
        self.q = nn.linear(embed_dim, head_dim)
        self.k = nn.linear(embed_dim, head_dim)
        self.v = nn.linear(embed_dim, head_dim)

    def forward(self, hidden_state):
        attn_outputs = scaled_dot_product_attention(
            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))
        return attn_outputs

class multiheadattention(nn.module):
    def __init__(self, config):
        super().__init__()
        embed_dim = config.hidden_size
        num_heads = config.num_attention_heads
        head_dim = embed_dim // num_heads
        self.heads = nn.modulelist(
            [attentionhead(embed_dim, head_dim) for _ in range(num_heads)]
        )
        self.output_linear = nn.linear(embed_dim, embed_dim)

    def forward(self, hidden_state):
        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)
        x = self.output_linear(x)
        return x

class feedforward(nn.module):
    def __init__(self, config):
        super().__init__()
        self.linear_1 = nn.linear(config.hidden_size, config.intermediate_size)
        self.linear_2 = nn.linear(config.intermediate_size, config.hidden_size)
        self.gelu = nn.gelu()
        self.dropout = nn.dropout(config.hidden_dropout_prob)
        
    def forward(self, x):
        x = self.linear_1(x)
        x = self.gelu(x)
        x = self.linear_2(x)
        x = self.dropout(x)
        return x
        
class transformerencoderlayer(nn.module):
    def __init__(self, config):
        super().__init__()
        self.layer_norm_1 = nn.layernorm(config.hidden_size)
        self.layer_norm_2 = nn.layernorm(config.hidden_size)
        self.attention = multiheadattention(config)
        self.feed_forward = feedforward(config)

    def forward(self, x):
        # apply layer normalization and then copy input into query, key, value
        hidden_state = self.layer_norm_1(x)
        # apply attention with a skip connection
        x = x + self.attention(hidden_state)
        # apply feed-forward layer with a skip connection
        x = x + self.feed_forward(self.layer_norm_2(x))
        return x

class transformerencoder(nn.module):
    def __init__(self, config):
        super().__init__()
        self.embeddings = embeddings(config)
        self.layers = nn.modulelist([transformerencoderlayer(config) 
                                     for _ in range(config.num_hidden_layers)])

    def forward(self, x):
        x = self.embeddings(x)
        for layer in self.layers:
            x = layer(x)
        return x
        
#adding a classification head
class transformerforsequenceclassification(nn.module):
    def __init__(self, config):
        super().__init__()
        self.encoder = transformerencoder(config)
        self.dropout = nn.dropout(config.hidden_dropout_prob)
        self.classifier = nn.linear(config.hidden_size, config.num_labels)
        
    def forward(self, x):
        x = self.encoder(x)[:, 0, :] # select hidden state of [cls] token
        x = self.dropout(x)
        x = self.classifier(x)
        return x

config.num_labels = 6
encoder_classifier = transformerforsequenceclassification(config)

then i preprocess data:
from datasets import load_dataset
import pandas as pd
emotions = load_dataset(""emotion"")

def tokenize(batch):
    return tokenizer(batch[""text""], padding=true, truncation=true)

emotions_encoded = emotions.map(tokenize, batched=true, batch_size=none)

tokenized_datasets = emotions_encoded.remove_columns([""text""])
tokenized_datasets = tokenized_datasets.rename_column(""label"", ""labels"")
tokenized_datasets.set_format(""torch"")

from torch.utils.data import dataloader
train_dataloader = dataloader(tokenized_datasets['train'], shuffle=true, batch_size=8)
eval_dataloader = dataloader(tokenized_datasets['validation'], batch_size=8)

from torch.optim import adamw
optimizer = adamw(encoder_classifier.parameters(), lr=5e-5)

loss_fn = nn.crossentropyloss()

from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    name=""linear"",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

from tqdm.auto import tqdm
import torch

device = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")
encoder_classifier.to(device)
#next(encoder_classifier.parameters()).is_cuda
progress_bar = tqdm(range(num_training_steps))

encoder_classifier.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        import pdb;pdb.set_trace()
        outputs = encoder_classifier(batch[""input_ids""])
        loss = loss_fn(outputs, batch[""labels""])
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

i finally got the error:
""runtimeerror: expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when
checking argument for argument index in method wrapper__index_select)""
i am not sure that pushing my custom model of bert on device (cuda) works.
do you have an idea why and how to correct the code to make it works on gpu.
edit:
the error is raised at this line:
---> 25         outputs = encoder_classifier(batch[""input_ids""])
and i check that arguments are on cuda as followed:
ipdb>  !next(encoder_classifier.parameters()).is_cuda
true
ipdb>  batch[""input_ids""].device
device(type='cuda', index=0)
ipdb>   batch[""labels""].device
device(type='cuda', index=0)","['pytorch', 'bert-language-model']",76033746,"i had to push position_ids  to cuda .... i feel stupid now ::)
position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0).to(device)",https://stackoverflow.com/questions/76012610,pytorch,14-04-2023 07:33,438.0,0.0,2.0,True,17-04-2023 15:48,14-04-2023 09:29
77484646,how should i preprocess this dataset for performing a &quot;question-answering&quot; task? pytorch,"so i have a json dataset that is in the following format:
[{
    ""answer"": ""..."",
    ""question"": ""..."",
    ""context"": ""...""
  },
 ...
]

all of the fields are normal plaintext. my goal is train a pretrained bert model on it for the task of ""question-answering"". when reviewing through the huggingface docs it seems that the formatting needs to match the squad dataset formatting, context needs to be trimmed to a max of 394 characters, and things like [cls] and other tokens need to be added.
when trying to follow the docs, i get to this site for question answering and they provide a sample function to do the preprocessing. provided here for simplicity:
def preprocess_function(examples):
    questions = [q.strip() for q in examples[""question""]]
    inputs = tokenizer(
        questions,
        examples[""context""],
        max_length=384,
        truncation=""only_second"",
        return_offsets_mapping=true,
        padding=""max_length"",
    )

    offset_mapping = inputs.pop(""offset_mapping"")
    answers = examples[""answers""]
    start_positions = []
    end_positions = []

    for i, offset in enumerate(offset_mapping):
        answer = answers[i]
        start_char = answer[""answer_start""][0]
        end_char = answer[""answer_start""][0] + len(answer[""text""][0])
        sequence_ids = inputs.sequence_ids(i)

        # find the start and end of the context
        idx = 0
        while sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1

        # if the answer is not fully inside the context, label it (0, 0)
        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            # otherwise it's the start and end token positions
            idx = context_start
            while idx <= context_end and offset[idx][0] <= start_char:
                idx += 1
            start_positions.append(idx - 1)

            idx = context_end
            while idx >= context_start and offset[idx][1] >= end_char:
                idx -= 1
            end_positions.append(idx + 1)

    inputs[""start_positions""] = start_positions
    inputs[""end_positions""] = end_positions
    return inputs

with the following sample code:
from transformers import autotokenizer
from datasets import dataset

dataset = dataset.from_pandas(df) # where df is a 3 column dataframe object | answer | question | context |

tokenizer = autotokenizer.from_pretrained(""distilbert-base-uncased"")
tokenized_ds = dataset.map(preprocess_function, batched=true)

i get the following error when i try to run it.
file ""c:/project.py"", line 234, in preprocess_function
    inputs = tokenizer(
typeerror: 'list' object is not callable

so my question: is this just a silly error on my part, or should i be trying to preprocess this dataset in a different manner? i am new to huggingface and nlp in general so i am doing this as a fun project. thanks in advance!
update 1: following the tutorial more closely i put my data into a datasets object and used the map function properly, but now it is saying the tokenizer is a problem because typeerror: 'list' object is not callable but it is the exact same tokenizer used in the tutorial.","['python', 'json', 'pytorch', 'huggingface-transformers', 'data-preprocessing']",77484918,"solved it myself! my solution:

load the df into a datasets object so that i could actually call the map function

globally declare the tokenizer above the preprocessing function
thanks to this question for the help

change the preprocessing function to match my dataset.

answers = examples[""answers""] --> answers = examples[""answer""]

created contexts = examples[""context""]

start_char = answer[""answer_start""][0]
end_char = answer[""answer_start""][0] + len(answer[""text""][0])
-->
start_char = contexts[i].find(answer)
end_char = start_char + len(answer)",https://stackoverflow.com/questions/77484646,python,15-11-2023 00:58,714.0,0.0,2.0,True,15-11-2023 02:32,15-11-2023 02:20
76663419,how to generate text using gpt2 model with huggingface transformers?,"i wanted to use gpt2tokenizer, automodelforcausallm for generating (rewriting) sample text. i have tried transformers==4.10.0, transformers==4.30.2 and --upgrade git+ however i get the error of attributeerror: 'gpt2lmheadmodel' object has no attribute 'compute_transition_scores.
my code is as follows:
from transformers import gpt2tokenizer, automodelforcausallm
import numpy as np
import pandas as pd


x = ""sample text"" #df_toxic['text'].iloc[0]

tokenizer = gpt2tokenizer.from_pretrained(""gpt2"")
model = automodelforcausallm.from_pretrained(""gpt2"")
tokenizer.pad_token_id = tokenizer.eos_token_id
inputs = tokenizer(x, return_tensors=""pt"")

# example 1: print the scores for each token generated with greedy search
outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=true, output_scores=true)
transition_scores = model.compute_transition_scores(
    outputs.sequences, outputs.scores, normalize_logits=true
)
# input_length is the length of the input prompt for decoder-only models, like the gpt family, and 1 for
# encoder-decoder models, like bart or t5.
input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]
generated_tokens = outputs.sequences[:, input_length:]
for tok, score in zip(generated_tokens[0], transition_scores[0]):
    # | token | token string | logits | probability
    print(f""| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}"")

i got the error of:
setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
cell in [21], line 3
      1 # example 1: print the scores for each token generated with greedy search
      2 outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=true, output_scores=true)
----> 3 transition_scores = model.compute_transition_scores(
      4     outputs.sequences, outputs.scores, normalize_logits=true
      5 )
      6 # # input_length is the length of the input prompt for decoder-only models, like the gpt family, and 1 for
      7 # # encoder-decoder models, like bart or t5.
      8 # input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]
   (...)
     11 #     # | token | token string | logits | probability
     12 #     print(f""| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}"")

file /usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1207, in module.__getattr__(self, name)
   1205     if name in modules:
   1206         return modules[name]
-> 1207 raise attributeerror(""'{}' object has no attribute '{}'"".format(
   1208     type(self).__name__, name))

attributeerror: 'gpt2lmheadmodel' object has no attribute 'compute_transition_scores'","['python', 'huggingface-transformers', 'huggingface', 'gpt-2', 'large-language-model']",76665384,"to generate text using transformers and gpt2 model, if you're not particular about modifying different generation features you can use the pipeline function, e.g.
from transformers import pipeline

generator = pipeline('text-generation', model='gpt2')
generator(""hello world, continue... "")

[out]:
[{'generated_text': 'hello world, continue... !! a group of two great people from finland came to my office and brought me with them, and i got some beautiful drawings with the colours. i thought i gave it to the artist but that was not the case.'}]


if you have somehow have to use gpt2tokenizer and automodelforcausallm instead of using pipeline, you can try autotokenizer instead of gpt2tokenizer, e.g.
from transformers import autotokenizer, automodelforcausallm

tokenizer = autotokenizer.from_pretrained(""gpt2"")
model = automodelforcausallm.from_pretrained(""gpt2"")
tokenizer.pad_token_id = tokenizer.eos_token_id


x = ""hello world, ...""
inputs = tokenizer(x, return_tensors=""pt"")

from transformers import autotokenizer, automodelforcausallm

tokenizer = autotokenizer.from_pretrained(""gpt2"")
model = automodelforcausallm.from_pretrained(""gpt2"")
tokenizer.pad_token_id = tokenizer.eos_token_id


x = ""hello world, ...""
inputs = tokenizer(x, return_tensors=""pt"")

model_outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=true, output_scores=true)

generated_tokens_ids = model_outputs.sequences[0]

tokenizer.decode(generated_tokens_ids)

[out]:
hello world,...\n\ni'm sorry


to use the compute_transition_scores function implemented in 
first make sure you really have the update version of transformers by doing:
import transformers
print(transformers.__version__)

if the version is after the feature have been implemented, this should give no error:
from transformers import autotokenizer, automodelforcausallm

tokenizer = autotokenizer.from_pretrained(""gpt2"")
model = automodelforcausallm.from_pretrained(""gpt2"")
tokenizer.pad_token_id = tokenizer.eos_token_id

model.compute_transition_scores

[out]:
<bound method generationmixin.compute_transition_scores of gpt2lmheadmodel(...)

if you see the attributeerror,
attributeerror: 'gpt2lmheadmodel' object has no attribute 'compute_transition_scores'

most probably your current python kernel (maybe inside jupyter) isn't the right one that you have with your pip. if so, check your executable:
import sys
sys.executable

then you should see something like:
/usr/bin/python3

after that, instead of simple pip install -u transformers reuse that above python binary and do:
/usr/bin/python3 -m pip install -u transformers

see also:

what's the difference between ""pip install"" and ""python -m pip install""?
why is ""-m"" needed for ""python -m pip install ...""?",https://stackoverflow.com/questions/76663419,python,11-07-2023 15:10,8768.0,0.0,1.0,True,11-07-2023 20:10,11-07-2023 19:58
74014379,how to fine-tune gpt-j using huggingface trainer,"i'm attempting to fine-tune gpt-j using the huggingface trainer and failing miserably. i followed the example that references bert, but of course, the gpt-j model isn't exactly like the bert model.
the error indicates that the model isn't producing a loss, which is great, except that i have no idea how to make it generate a loss or how to change what the trainer is expecting.
i'm using transformers 4.22.2. i would like to get this working on a cpu before i try to do anything on paperspace with a gpu. i did make an initial attempt there using a gpu that received the same error, with slightly different code to use cuda.
i suspect that my approach is entirely wrong. i found a very old example of fine-tuning gpt-j using 8-bit quantization, but even that repository says it is deprecated.
i'm unsure if my mistake is in using the compute_metrics() i found in the bert example or if it is something else. any advice would be appreciated. or, maybe it is an issue with the labels i provide the config, but i've tried different permutations.
i understand what a loss function is, but i don't know how it is supposed to be configured in this case.
my code:
from transformers import trainer, trainingarguments, automodelforcausallm
from transformers import gptjforcausallm, autotokenizer
from datasets import load_dataset
import time
import torch
import os
import numpy as np
import evaluate
import sklearn

start = time.time()

gptj_fine_tuned_file = ""./fine_tuned_models/gpt-j-6b""

print(""loading model"")
model = gptjforcausallm.from_pretrained(""eleutherai/gpt-j-6b"", low_cpu_mem_usage=true)
model.config.pad_token_id = model.config.eos_token_id

print(""loading tokenizer"")
tokenizer = autotokenizer.from_pretrained(""eleutherai/gpt-j-6b"")
tokenizer.pad_token = tokenizer.eos_token

print(""loading dataset"")
current_dataset = load_dataset(""wikitext"", 'wikitext-103-v1')
current_dataset['train'] = current_dataset['train'].select(range(1200))


def tokenize_function(examples):
    current_tokenizer_result = tokenizer(examples[""text""], padding=""max_length"", truncation=true)
    return current_tokenizer_result


print(""splitting and tokenizing dataset"")
tokenized_datasets = current_dataset.map(tokenize_function, batched=true)
small_train_dataset = tokenized_datasets[""train""].select(range(100))

print(""preparing training arguments"")

training_args = trainingarguments(output_dir=gptj_fine_tuned_file,
                                  report_to='all',
                                  logging_dir='./logs',
                                  per_device_train_batch_size=1,
                                  label_names=['input_ids', 'attention_mask'],  # 'logits', 'past_key_values'
                                  num_train_epochs=1,
                                  no_cuda=true
                                  )

metric = evaluate.load(""accuracy"")


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)


trainer = trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset
)

print(""starting training"")
trainer.train()
print(f""finished fine-tuning in {time.time() - start}"")

which leads to the error and stacktrace:
  file ""xxx\ft_v3.py"", line 66, in <module>
  file ""xxx\venv\lib\site-packages\transformers\trainer.py"", line 1521, in train
    return inner_training_loop(
  file ""xxx\venv\lib\site-packages\transformers\trainer.py"", line 1763, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  file ""xxx\venv\lib\site-packages\transformers\trainer.py"", line 2499, in training_step
    loss = self.compute_loss(model, inputs)
  file ""xxx\venv\lib\site-packages\transformers\trainer.py"", line 2544, in compute_loss
    raise valueerror(
valueerror: the model did not return a loss from the inputs, only the following keys: logits,past_key_values. for reference, the inputs it received are input_ids,attention_mask.","['python', 'machine-learning', 'pytorch', 'huggingface-transformers', 'huggingface']",74021554,"i found what appears to work, though now i'm running low on memory and working through ways of handling it.
the data_collator parameter seems to take care of the exact issue that i was having.
data_collator = datacollatorforlanguagemodeling(tokenizer, mlm=false)

trainer = trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
    data_collator=data_collator,
)",https://stackoverflow.com/questions/74014379,python,10-10-2022 11:42,3708.0,4.0,1.0,True,18-01-2023 13:18,10-10-2022 14:28
72211014,keywords extraction in python - how to handle hyphenated compound words,"i'm trying to perform keyphrase extraction with python, using keybert and pke positionrank. you can see an extract of my code below.
from keybert import keybert
from keyphrase_vectorizers import keyphrasecountvectorizer
import pke

text = ""the life-cycle global warming potential of the building resulting from the construction has been calculated for each stage in the life-cycle and is disclosed to investors and clients on demand"" #text_cleaning(df_tassonomia.iloc[1077].text, sentence_adjustment, stop_words)

# pke
extractor = pke.unsupervised.positionrank() 
extractor.load_document(text, language='en')
extractor.candidate_selection(maximum_word_number = 5) 
extractor.candidate_weighting(window = 10) 
keyphrases = extractor.get_n_best(n=10)
print(keyphrases)

# keybert
kw_model = keybert(model = ""all-mpnet-base-v2"")
keyphrases_2 = kw_model.extract_keywords(docs=text, 
                                       vectorizer=keyphrasecountvectorizer(), 
                                       keyphrase_ngram_range = (1,5),
                                       top_n=10
                                      )

print("""")
print(keyphrases_2)

and here the results:
[('cycle global warming potential', 0.44829175082921835), ('life', 0.17858359644549557), ('cycle', 0.15775994057934534), ('building', 0.09131084381406684), ('construction', 0.08860454878871142), ('investors', 0.05426710724030216), ('clients', 0.054111700289631526), ('stage', 0.045672396861507744), ('demand', 0.039158055731066406)]

[('cycle global warming potential', 0.5444), ('building', 0.4479), ('construction', 0.3476), ('investors', 0.1967), ('clients', 0.1519), ('demand', 0.1484), ('cycle', 0.1312), ('stage', 0.0931), ('life', 0.0847)]

i would like to handle hyphenated compound words (as life-cycle in the example) are considered as a unique word, but i cannot understand how to exclude the - from the words separators list.
thank you in advance for any help.
francesca","['python', 'nlp']",72224302,"this could be a silly workaround but it may help :
from keybert import keybert
from keyphrase_vectorizers import keyphrasecountvectorizer
import pke
text = ""the life-cycle global warming potential of the building 
resulting from the construction has been calculated for each stage in 
the life-cycle and is disclosed to investors and clients on demand""

# pke
tokens = text.split()
orignal = set([x for x in tokens if ""_"" in x])
text = text.replace(""-"", ""_"")
extractor = pke.unsupervised.positionrank()
extractor.load_document(text, language='en')
extractor.candidate_selection(maximum_word_number=5)
extractor.candidate_weighting(window=10)
keyphrases = extractor.get_n_best(n=10)
keyphrases_replaced = []
for pair in keyphrases:
    if ""_"" in pair[0] and pair[0] not in orignal:
        keyphrases_replaced.append((pair[0].replace(""_"",""-""),pair[1]))
   else:
        keyphrases_replaced.append(pair)
print(keyphrases_replaced)
# keybert

keyphrases_2 = kw_model.extract_keywords(docs=text,
                                     
vectorizer=keyphrasecountvectorizer(),
                                     keyphrase_ngram_range=(1, 5),
                                     top_n=10
                                     )

print("""")
print(keyphrases_2)

the out put should look like this:
[('life-cycle global warming potential', 0.5511001220016548), ('life-cycle', 0.20123353586644233), ('construction', 0.11945270995269436), ('building', 0.10637157845606555), ('investors', 0.06675114967366767), ('stage', 0.05503532672910801), ('clients', 0.0507262942318816), ('demand', 0.05056281895492815)]

i hope this help :)",https://stackoverflow.com/questions/72211014,python,12-05-2022 06:32,527.0,0.0,2.0,True,25-05-2022 04:47,12-05-2022 16:07
44133509,delete stop words in r,"i have a dataframe which has this structure:
note.reco review review.clean.lower
10 good products  good products
9 nice film      nice film
....         ....

the first column is the rank of the film, then the second column is the customer's review then the 3rd column is the review with lowercase letters.
i try now to delete stop words with this:
data_clean$raison.reco.clean1 <- corpus(vectorsource(data_clean$review.clean.lower))
data_clean$review.clean.lower1 <- tm_map(data_clean$review.clean.lower1, removewords, stopwords(""english""))

but r studio crashes
can you help me to resolve this problem please?
thank you
edit:
#clean up
# remove grammar/punctuation
data_clean$review.clean.lower <- tolower(gsub('[[:punct:]0-9]', ' ', data_clean$review))

data_corpus <- corpus(vectorsource(data_clean$review.clean.lower))

data_clean <- tm_map(data_corpus,  removewords, stopwords(""french""))

train <- data_clean[train.index, ]
test <- data_clean[test.index, ]

so i get error when i run the 2 last instructions.","['r', 'text-mining']",44133650,"try the below . you can do cleaning on the corpus and not column directly.  
data_corpus <-
  corpus(vectorsource(data_clean$review.clean.lower))

  data_clean <- tm_map(data_corpus,  removewords, stopwords(""english""))

edit:
as mentioned by you, you want to be able to access the output after removing stop words, try the below instead of the above:
library(tm)

stopwords <- stopwords(""en"")

data_clean$review.clean.lower<- as.character(data_clean$review.clean.lower)
 '%nin%' <- negate('%in%')
 data_clean$review.clean.lower1<-lapply(data_clean$review.clean.lower, function(x) {
  chk <- unlist(strsplit(x,"" ""))
  p <- chk[chk %nin% stopwords]
  paste(p,collapse = "" "")
})

sample output of above code:
>  print(data_clean)
>       note note.reco.review review.clean.lower review.clean.lower1
>     1   10    good products      good products       good products
>     2    9        nice film     is a nice film           nice film

also check the below:
r remove stopwords from a character vector using %in%",https://stackoverflow.com/questions/44133509,r,23-05-2017 11:27,6550.0,0.0,1.0,True,08-11-2023 21:56,08-11-2023 21:56
70277454,i have r code to extract information from one document. how do i loop that for all the documents in my folder?,"i have a folder of txt files, and i want to extract specific texts from them and arrange them separate columns into a new data frame. i did the code for one file, but i can't seem to edit it into a loop that will run across all the documents in my folder.
this is my code for the one txt file:
    clean_text <- as.data.frame(strsplit(text$text, '\\*' ), col.names = ""text"") %>% 
mutate(text = str_replace_all(text, ""\n"", "" ""),
         text = str_replace_all(text, ""- "", """"), 
         text = str_replace_all(text,""^\\s"", """")) %>% 
  
  filter(!text == "" "") %>% 
  
  mutate(paragraphs = ifelse(grepl(""^[[:digit:]]"", text) == t, text, na)) %>% 
  
  rename(category = text) %>% 
  mutate(category = ifelse(grepl(""^[[:digit:]]"", category) == t, na, category)) %>% 
  fill(category) %>% 
  filter(!is.na(paragraphs)) %>% 
  
  mutate(paragraphs = strsplit(paragraphs, '^[[:digit:]]{1,3}\\.|\\t\\s[[:digit:]]{1,3}\\.')) %>% 
  unnest(paragraphs) %>% 
  mutate(paragraphs = strsplit(paragraphs, 'download as pdf')) %>%
  unnest(paragraphs) %>% 
  mutate(paragraphs = str_replace_all(paragraphs, ""\t"", """")) %>% 
  mutate(paragraphs = ifelse(grepl(""javascript"", paragraphs), """", paragraphs)) %>%
  mutate(paragraphs = str_replace_all(paragraphs, ""^\\s+"", """")) %>%
  filter(!paragraphs == """") 

how do i make this into a loop? i realise there are similar questions, but none of the solutions have worked for me. thanks in advance for the help!","['r', 'loops', 'nlp', 'data-cleaning']",70277599,"put your code in a function:
extract_info = function(file) {
  ## add the code you need to read the text from the file
  ## something like
  ## text <- readlines(file)
  ## or whatever you are using to read in the file
  clean_text <- as.data.frame(strsplit(text$text, '\\*' ), col.names = ""text"") %>% 
  mutate(text = str_replace_all(text, ""\n"", "" ""),
           text = str_replace_all(text, ""- "", """"), 
           text = str_replace_all(text,""^\\s"", """")) %>% 
    
    filter(!text == "" "") %>% 
    
    mutate(paragraphs = ifelse(grepl(""^[[:digit:]]"", text) == t, text, na)) %>% 
    
    rename(category = text) %>% 
    mutate(category = ifelse(grepl(""^[[:digit:]]"", category) == t, na, category)) %>% 
    fill(category) %>% 
    filter(!is.na(paragraphs)) %>% 
    
    mutate(paragraphs = strsplit(paragraphs, '^[[:digit:]]{1,3}\\.|\\t\\s[[:digit:]]{1,3}\\.')) %>% 
    unnest(paragraphs) %>% 
    mutate(paragraphs = strsplit(paragraphs, 'download as pdf')) %>%
    unnest(paragraphs) %>% 
    mutate(paragraphs = str_replace_all(paragraphs, ""\t"", """")) %>% 
    mutate(paragraphs = ifelse(grepl(""javascript"", paragraphs), """", paragraphs)) %>%
    mutate(paragraphs = str_replace_all(paragraphs, ""^\\s+"", """")) %>%
    filter(!paragraphs == """") 
}

test your function to make sure it works on one file:
extract_info(""your_file_name.txt"")
## does the result work and look right? 
## work on your function until it does

get a list of all the files you want to run
my_files = list.files()
## by default this will give you all the files in your working directory
## use the `pattern` argument if you only want files that follow
## a certain naming convention

apply your function to those files:
results = lapply(my_files, extract_info)",https://stackoverflow.com/questions/70277454,r,08-12-2021 15:22,79.0,0.0,2.0,True,08-12-2021 20:41,08-12-2021 20:41
61588381,speed up embedding of 2m sentences with roberta,"i have roughly 2 million sentences that i want to turn into vectors using facebook ai's roberta-large,fine-tuned on nli and stsb for sentence similarity (using the awesome sentence-transformers package).
i already have a dataframe with two columns: ""utterance"" containing each sentence from the corpus, and ""report"" containing, for each sentence, the title of the document from which it is from.
from there, my code is the following:
from sentence_transformers import sentencetransformer
from tqdm import tqdm

model = sentencetransformer('roberta-large-nli-stsb-mean-tokens')

print(""embedding sentences"")

data = pd.read_csv(""data/sentences.csv"")

sentences = data['utterance'].tolist()

sentence_embeddings = []

for sent in tqdm(sentences):
    embedding = model.encode([sent])
    sentence_embeddings.append(embedding[0])

data['vector'] = sentence_embeddings

right now, tqdm estimates that the whole process will take around 160 hours on my computer, which is more than i can spare.
is there any way i could speed this up by changing my code? is creating a huge list in memory then appending it to the dataframe the best way to proceed here? (i suspect not).
many thanks in advance!","['python', 'nlp', 'word-embedding', 'transformer-model']",61612256,"i found a ridiculous speedup using this package by feeding in the utterances as a list instead of looping over the list. i assume there is some nice internal vectorisation going on.
%timeit utterances_enc = model.encode(utterances[:10])                                                                                                                                                                                                                 
3.07 s ï¿½ï¿½ 53.2 ms per loop (mean ï¿½ï¿½ std. dev. of 7 runs, 1 loop each)

%timeit utterances_enc = [model.encode(utt) for utt in utterances[:10]]
4min 1s ï¿½ï¿½ 8.08 s per loop (mean ï¿½ï¿½ std. dev. of 7 runs, 1 loop each)

the full code would be as follows:
from sentence_transformers import sentencetransformer
from tqdm import tqdm

model = sentencetransformer('roberta-large-nli-stsb-mean-tokens')

print(""embedding sentences"")

data = pd.read_csv(""data/sentences.csv"")

sentterance'].tolist()

sentence_embeddings = model.encode(sentences)

data['vector'] = sentence_embeddings",https://stackoverflow.com/questions/61588381,python,04-05-2020 08:50,3013.0,6.0,2.0,True,01-03-2022 10:27,04-05-2020 14:21
11340963,natural language time parser,"i'm trying to parse strings containing (natural language) times to hh:mm time objects? for example:
""ten past five""
""quarter to three""
""half past noon""
""15 past 3""
""13:35""
""ten fourteen am""

i've looked into chronic for ruby and natty for java (as well as some other libraries) but both seem to focus on parsing dates. strings like ""ten past five"" are not parsed correctly by either.
does anyone know of a library which suit my needs? or should i maybe start working on my own parser?","['python', 'parsing', 'time', 'nlp']",11357529,"i didn't feel like extending parsedatetime, so i decided to use pypeg, a parser interpreter framework for python, to write a dedicated time parser. for whoever's interested, the first basic version is now finished, and nicely parses dutch time strings.",https://stackoverflow.com/questions/11340963,python,05-07-2012 08:56,4140.0,4.0,2.0,True,12-02-2023 10:27,14-12-2013 21:11
72706958,what is the ideal &quot;size&quot; of the vector for each word in word2vec?,"i have a dataset of over 1 million rows. each row has 40 token words. based on these tokens, a classification is made with a neural network. the vocabulary is 20,000 unique words. it is a binary classification problem. i set the size (dimension) of the vectors in gensim word2vec as 150 and saved these vectors for each data point in a json file. the json file's size is really huge: 250 gb. i cannot load this file into memory in one scoop as my ram is only 128 gb. i am trying to see if i can reduce the physical size of these vectors by reducing them to the right size. i went through some of the suggestions made in this website such as relation between word2vec vector size and total number of words scanned?. but the vector size is mentioned to be 100-300 and also depends on the problem.
here is what i am doing:
# for training the word2vec model
w2vmodel = gensim.models.word2vec(one_mil_tokens,vector_size=150, window=2, min_count=1, sg=0, seed=1)
w2vmodel.save(""w2model.trained"")

and
model = gensim.models.word2vec.load(""w2model.trained"")
vec = []
finalvecs = []

#tokens is a list of over a 1 million rows
for token in tokens:
  for word in token:
    vec.append(model.wv[eachtoken].tolist())
  finalvecs.append(vec)

i am doing json.dump() for finalvecs.

how can i determine the right size (dimension) of the vector for each token based on the given problem?
i use skip-gram model to train word2vec. should i use cbow to optimize the size?
is json the fight format to store/retrieve these vectors or are there other efficient ways?","['python', 'python-3.x', 'machine-learning', 'nlp', 'word2vec']",72708076,"each dimension of a dense vector is typically a 32-bit float.
so, storing 20,000 token-vectors of 150 dimensions each will take at least 20000 vectors * 150 floats * 4 bytes/float = 12mb for the raw vector weights, plus some overhead for remembering which token associates with which line.
let's say you were somehow changing each of your rows into a single summary vector of the same size. (perhaps, by averaging together each of the ~40 token vectors into a single vector ï¿½ï¿½ï¿½ a simple baseline approach, though there are many limitations of that approach, & other techniques that might be used.) in that case, storing the 1 million vectors will necessarily take about 1000000 vectors * 150 floats * 4 bytes/float = 600mb for the raw vector weights, plus some overhead to remember which row associates with which vector.
that neither of these is anywhere close to 250gb implies you're making some other choices expanding things significantly. json is a poor choiccompactly representing dense floating-point numerical data, but even that is unlikely to explain the full expansion.
your description that you ""saved these vectors for each data point in a json file"" isn't really clear what vectors are being saved, or in what sort of json conventions.
perhaps you're storing the 40 separate vectors for each row? that'd give a raw baseline weights-only size of 1000000 rows * 40 tokens/row * 150 floats * 4 bytes/float = 24gb. it is plausible inefficient json is expanding the stored-size by something like 10x, so i guess you're doing something like this.
but in addition to the inefficiency of json, given that the 40 tokens (from a vocabulary of 20k) each given enough info to reconstitute any other per-row info that's solely a function of the tokens & the 20k word-vectors, there's not really any reason to expand the representations this way.
for example: if the word 'apple' is already in your dataset, and appears many thousands of times, there's no reason to re-write the 150 dimensions of 'apple' many thousands of times. the word 'apple' alone is enough to call-back those 150 dimensions, whenever you want them, from the much-smaller (12mb) set-of-20k token-vectors, that's easy to keep in ram.
so mainly: ditch json, don't (unnecessarily) expand each row into 40 * 150 dimensions.
to your specific questions:

the optimal size will vary based on lots of things, including your data & other goals. the only way to rationally choose is to figure out some way to score the trained vectors on your true end goals: some repeatable way of comparing multiple alternate choices of vector_size. then you run it every plausible way & pick the best. (barring that, you take a random stab based on some precedent work that seems roughly similar in data/goals, and hope that works ok until you have the chance to compare it against other choices.)

the choice of skip-gram or cbow wont affect the size of the model at all. it might affect end result quality & training times a bit, but the only way to choose is to try both & see which works better for your goals & constraints.

json is an awful choice for storing dense binary data. representing numbers as just text involves expansion. the json formatting characters add extra overhead, repeatedly on every tow, that's redundant if every row is the exact same 'shape' of raw data. and, typical later vector operations in ram usually work best on the same sort of maximally-compact raw in-memory representation that would also be best on disk. (in fat, the best on-disk representation will often be data that exactly matches the format in memory, so that data can be ""memory-mapped"" from disk to ram in a quick direct operation that minimizes format-wrangling & even defers accesses until reads needed.)


gensim will efficiently save its models via their build-in .save() method, into one (or more often several) related files on disk. if your gensim word2vec model is in the variable w2v_model, you can just save the whole model with w2v_model.save(your_filename) ï¿½ï¿½ï¿½ & later reload it with word2vec.load(your_filename).
but if after training you only need the (20k) word-vectors, you can just save the w2v_model.wv property ï¿½ï¿½ï¿½ just the vectors: w2v_model.wv.save(your_filename). then you can reload them as an instance of keyedvectors: keyedvectors.load(your_filename).
(note in all cases: the save may be spread over multiple files, which if ever copied/moved elsewhere, should be kept together - even though you only ever specify the 'root' file of each set in save/load operations.)
how & whether you'd want to store any vectorization of your 1 million rows would depend on other things not yet specifiedthe character of the data, and the kinds of classification applied later. i doubt that you want to turn your rows into 40 * 150 6000-dimensions ï¿½ï¿½ï¿½ï¿½ï¿½that'd be counter to some of the usual intended benefits of a word2vec-based analysis, where the word apple has much the same significance no matter where it appears in a textual list-of-words.
you'd have to say more about your data, & classification goals, to get a better recommendation here.
if you haven't already done a 'bag-of-words' style representation of your rows (no word2vec), where every row is represented by a 20000-dimension one-hot sparse vector, and run a classifier on that, i'd recommend that first, as a base",https://stackoverflow.com/questions/72706958,python,21-06-2022 20:48,1925.0,0.0,2.0,True,22-06-2022 03:15,22-06-2022 03:15
53208483,create document term matrix with n-grams in r,"i am using ""tm"" package to create documenttermmatrix in r. it works well for one - gram but i am trying to create a documenttermmatrix of n-grams(n = 3 for now) using tm package and tokenize_ngrams function from ""tokenizers"" package.
but im not able to create it.
i searched for possible solution but i didnt get much help.
for privacy reasons i can not share the data.
here is what i have tried,  
library(tm)  
library(tokenizers)

data is a dataframe with around 4.5k rows and 2 columns namely ""doc_id"" and ""text""
data_corpus = corpus(dataframesource(data))

custom function for n-gram tokenization :  
ngram_tokenizer = function(x){
  temp = tokenize_ngrams(x, n_min = 1, n = 3, stopwords = false, ngram_delim = ""_"")
  return(temp)
}

control list for dtm creation :
1-gram  
control_list_unigram = list(tokenize = ""words"",
                          removepunctuation = false,
                          removenumbers = false, 
                          stopwords = stopwords(""english""), 
                          tolower = t, 
                          stemming = t, 
                          weighting = function(x)
                            weighttf(x)
)

for n-gram tokenization
control_list_ngram = list(tokenize = ngram_tokenizer,
                    removepunctuation = false,
                    removenumbers = false, 
                    stopwords = stopwords(""english""), 
                    tolower = t, 
                    stemming = t, 
                    weighting = function(x)
                      weighttf(x)
                    )


dtm_unigram = documenttermmatrix(data_corpus, control_list_unigram)
dtm_ngram = documenttermmatrix(data_cropus, control_list_ngram)

dim(dtm_unigram)
dim(dtm_ngram)

the dimension of both the dtm's were same.
please correct me!","['r', 'nlp', 'tokenize', 'tm', 'n-gram']",53226662,"unfortunately tm has some quirks that are annoying and not always clear. first of all, tokenizing doesn't seem to work on corpera created corpus. you need to use vcorpus for this.
so change the line data_corpus = corpus(dataframesource(data)) to data_corpus = vcorpus(dataframesource(data)).
that is one issue tackled. now the corpus will work for tokenizing but now you will run into an issue with tokenize_ngrams. you will get the following error:
input must be a character vector of any length or a list of character
  vectors, each of which has a length of 1. 

when you run this line:dtm_ngram = documenttermmatrix(data_cropus, control_list_ngram)
to solve this, and not have a dependency on the tokenizer package, you can use the following function to tokenize the data.
nlp_tokenizer <- function(x) {
  unlist(lapply(ngrams(words(x), 1:3), paste, collapse = ""_""), use.names = false)
}

this uses the ngrams function from the nlp package which is loaded when you load the tm package. 1:3 tells it to create ngrams from 1 to 3 words. so your control_list_ngram should look like this:
control_list_ngram = list(tokenize = nlp_tokenizer,
                          removepunctuation = false,
                          removenumbers = false, 
                          stopwords = stopwords(""english""), 
                          tolower = t, 
                          stemming = t, 
                          weighting = function(x)
                            weighttf(x)
                          )

personally i would use the quanteda package for all of this work. but for now this should help you.",https://stackoverflow.com/questions/53208483,r,08-11-2018 13:12,4428.0,3.0,1.0,True,26-07-2022 20:23,08-11-2018 13:35
76300983,how to interpret word2vec train output?,"running the code snippet below report an output (3, 60). i wonder what exactly it is reporting?
the code is reproducible..just copy into a notebook cell and run.
from gensim.models import word2vec    
sent = [['i', 'love', 'cats'], ['dogs', 'are', 'friendly']]
w2v_model = word2vec(sentences=sent, vector_size=100, window=7, min_count=1,sg=1)
w2v_model.train(sent, total_examples=len(sent), epochs=10)

(3, 60)","['python', 'nlp', 'word2vec']",76309266,"you seem to be using the gensim python library for your word2vec, & for internal reasons, the .train() method returns just the tuple (trained_word_count, raw_word_count).
the 1st number happens to be the number of words actually trained on ï¿½ï¿½ï¿½ more on why this is only 3 for you below ï¿½ï¿½ï¿½ & the 2nd the total raw words passed to training routines ï¿½ï¿½ï¿½ just your 6 words times 10 epochs. but, most users never need to consult these values.
a better way to monitor progress is to turn on logging to the info level - at which point you'll see many log lines of the model's steps & progress. by reading these, & over time, you'll start to recognize signs of a good run, or common errors (as when the totals or elapsed times don't seem consistent with what you thought you were doing).
you 3 lines are already a bit off, though:

if you pass your training corpus into the constructor, you don't .train() - that's already done for you, automatically. so, you're trining twice here. (and, if you want epochs=10 for that automatic training, you can specify it in the constructor.)
with a tiny toy-sized corpus, word2vec learns no useful vectors ï¿½ï¿½ï¿½ and even the reporting is more likely to reveal oddnesses that are irrelevant to more realistic-sized training runs. i recommend never training on anything less than hundreds-of-thousands of words, so that all your experiments reveal useful things about its usual operation, with minimal distractions from artifacts of unrealistic runs.
in particular, here, since you only have 6 words total, each has a word frequency of ~17% of all words. in any real corpus, such a word would be unrelaistically super-frequent ï¿½ï¿½ï¿½ and thus all your words fall victim to what is (in real corpora) a very useful optimization: probabilistic highly-frequent-word-dropping (tuned by the sample parameter). this is 0 words (6 words times 10 epochs), only 3 word occurrences were actually trained at all. (with truly frequent words in an adequately-sized corpus, dropping 19-out-of-20 appearances leaves plenty, & the overall model gets improved by spending relatively more effort on rarer words.)
min_count=1 is essentially always a bad idea with real word2vec workloads, as an words that only appear once can't get good vectors, but do waste model time/state. ignoring such rare words completely is a standard practice. (if you need vectors for such words, you should find more training material sufficient to demonstrate their varied uses, in context, repeatedly.",https://stackoverflow.com/questions/76300983,python,21-05-2023 16:52,192.0,0.0,1.0,True,23-05-2023 23:43,23-05-2023 09:34
58296163,spacy ner differentiating numbers or entities,"i am currently playing with spacy ner and wondering if spacy ner can do these 2 things:
case 1
let's say we have 2 sentences that we want to do ner with:

sugar level in his body is increasing.
his overall health quality is increasing.

can we tag ""increasing"" in the first sentence as ""symptoms"" entity, and tag ""increasing"" in the second one as ""good outcome"" entity? will ner see the difference in those 2 ""increasing"" words?
case 2
we also have 2 different sentences:

my salary is usd 8000 per month
my spending is usd 5000 per month

can ner see the number in the first sentence as ""income"" entity and the number in the second sentence as ""spending""?
thank you","['machine-learning', 'nlp', 'spacy', 'named-entity-recognition']",58308812,"these tasks go beyond what you would expect an ner model to be able to do in a number of ways. spacy's ner algorithm could be used to find types of entities like money (which is an entity type in its english models) or maybe something like symptom, but it doesn't look at a very large context to detect/classify entities, so it's not going to be able to differentiate these cases where the relevant context is fairly far away.
you probably want to combine ner (or another type of relevant span detection, which could also be rule-based) with another type of analysis that focuses more on the context. this could be some kind of text classification, you could examine the dependency parse, etc.
here is a simple example from the spacy docs about extracting entity relations using ner (to find money) followed by examining the dependency parse to try to figure out what the money element could be referring to:",https://stackoverflow.com/questions/58296163,machine-learning,09-10-2019 02:14,3072.0,3.0,1.0,True,17-09-2021 14:46,10-10-2019 15:10
77322066,how to change evaluation metric from roc auc to accuracy in hugging face transformers fine-tuning?,"i'm working on a text classification task using the hugging face transformers library in python. my code is set up to use roc auc as the evaluation metric, but i need to change it to accuracy. i've made attempts to modify the code, but i'm running into issues.
here's a simplified version of the code i'm working with:
# install packages
#!pip install torch transformers memory_profiler datasets accelerate
import time
import datetime
tic = time.time()

# import modules
import torch
import random
from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available
from transformers import autotokenizer, automodelforsequenceclassification, trainer, trainingarguments
from datasets import load_metric
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pandas as pd
import numpy as np

# function to define datasets
def create_datasets(x, y):
  # split data
  x_train, x_test, y_train, y_test = train_test_split(x.tolist(), y, test_size=0.2)

  # call the tokenizer
  tokenizer = autotokenizer.from_pretrained(model_name, do_lower_case=true)

  # tokenize the text
  train_tokens = tokenizer(x_train, truncation=true, padding=true, max_length=512)
  valid_tokens = tokenizer(x_test, truncation=true, padding=true, max_length=512)

  class maketorchdata(torch.utils.data.dataset):
      def __init__(self, tokens, labels):
          self.tokens = tokens
          self.labels = labels

      def __getitem__(self, idx):
          item = {k: torch.tensor(v[idx]) for k, v in self.tokens.items()}
          item[""labels""] = torch.tensor([self.labels[idx]])
          return item

      def __len__(self):
          return len(self.labels)

  # convert our tokenized data into a torch dataset
  train_dataset = maketorchdata(train_tokens, y_train.ravel())
  valid_dataset = maketorchdata(valid_tokens, y_test.ravel())

  return train_dataset, valid_dataset

# import the required libraries
metric_name = ""roc_auc""
metric = load_metric(metric_name)

# define metrics
def compute_metrics(eval_pred):

  predictions, labels = eval_pred
  predictions = np.argmax(predictions, axis=1)

  # 'micro', 'macro', etc. are for multi-label classification. if you are running a binary classification, leave it as default or specify ""binary"" for average
  return metric.compute(prediction_scores=predictions, references=labels, average=""macro"")

# create trainer
# specifiy the arguments for the trainer
def create_trainer(model_name, train_dataset, valid_dataset, num_epochs=5):
  training_args = trainingarguments(
      output_dir='./results',          # output directory
      num_train_epochs=num_epochs,     # total number of training epochs
      per_device_train_batch_size=8,   # batch size per device during training
      per_device_eval_batch_size=20,   # batch size for evaluation
      warmup_steps=500,                # number of warmup steps for learning rate scheduler
      weight_decay=0.01,               # strength of weight decay
      logging_dir='./logs',            # directory for storing logs
      load_best_model_at_end=true,     # load the best model when finished training (default metric is loss)
      metric_for_best_model = metric_name,    # select the base metrics
      logging_steps=200,               # log & save weights each logging_steps
      save_steps=200,
      evaluation_strategy=""steps"",     # evaluate each `logging_steps`
  )

  trainer = trainer(
        model=model,                         # the instantiated transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=train_dataset,         # training dataset
        eval_dataset=valid_dataset,          # evaluation dataset
        compute_metrics=compute_metrics,     # the callback that computes metrics of interest
    )
  return trainer

# define model name
model_name = ""sentence-transformers/all-distilroberta-v1""

# load the data into a pandas dataframe | remember to have a similar structure in your drive so that the data can be read properly.
df = pd.read_csv(r""c:\path\essays.csv"", encoding = ""latin-1"")
df = df.replace({'y': 1, 'n': 0})

# define x and y and create datasets

# to-do: define x and y
x = df['text']
y = df['cext']

# create datasets
train_dataset, valid_dataset = create_datasets(x, y)

# define a list of dropout probabilities to iterate through
dropout_probs = [0.1, 0.2, 0.3, 0.4, 0.5]

# double loop to iterate through all combinations
for hidden_dropout_prob in dropout_probs:
    for attention_probs_dropout_prob in dropout_probs:
        # define the model with the current dropout probabilities
        model = automodelforsequenceclassification.from_pretrained(model_name, num_labels=2,
                                                                  hidden_dropout_prob=hidden_dropout_prob,
                                                                  attention_probs_dropout_prob=attention_probs_dropout_prob).to(""cpu"")

        # rest of your code for this specific model configuration
        # train the model
        trainer = create_trainer(model, train_dataset, valid_dataset, num_epochs=3)
        training_results = trainer.train()  # capture the training results

        # evaluate the model
        results = trainer.evaluate()

        # print or save the results for this combination
        print(f""model with hidden_dropout_prob={hidden_dropout_prob} and attention_probs_dropout_prob={attention_probs_dropout_prob}:"")
        print(f""training results: {training_results}"")
        print(f""results: {results}"")

in this code, i've defined the compute_metrics function and the evaluation metric as roc auc, which works well. however, i would like to replace roc auc with accuracy in the evaluation of my models.
i would greatly appreciate any guidance on how to adjust this code to calculate accuracy as the evaluation metric instead of roc auc. what modifications should i make to the compute_metrics function and other relevant parts of the code?
my dataset looks like this:
            #authid                                               text  cext  \
0  1997_504851.txt  well, right now i just woke up from a mid-day ...     0   
1  1997_605191.txt  well, here we go with the stream of consciousn...     0   
2  1997_687252.txt  an open keyboard and buttons to push. the thin...     0   
3  1997_568848.txt  i can't believe it!  it's really happening!  m...     1   
4  1997_688160.txt  well, here i go with the good old stream of co...     1","['python', 'machine-learning', 'huggingface-transformers', 'text-classification']",77322116,"you can modify the compute_metrics function to calculate prediction accuracy:
from sklearn.metrics import accuracy_score

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    accuracy = accuracy_score(y_true=labels, y_pred=predictions)
    return {""accuracy"": accuracy}",https://stackoverflow.com/questions/77322066,python,19-10-2023 08:19,1356.0,0.0,1.0,True,19-10-2023 17:24,19-10-2023 17:24
69883861,incorrect entity being returned by entitylinker spacy,"i have been training an entity linker with spacy which has 6,000 entities from wikidata.
the training data contains of 30,000 sentences.
i'm following the notebook provided by spacy 
the training goes fine and the accuracy seems pretty good, until i test the model out on a string that's clearly incorrect. such ""barack obama is a french born florist living in spain with 36 cats and two hamsters"", but the model predicts the person in this string as 
i've tried adding additional parameters into the config, such as n_sents
 entity_linker = nlp.add_pipe(""entity_linker"", config={""incl_prior"": false, ""n_sents"": 6}, last=true)
is there a way to improve this? it would be better to return nil instead of a wrong answer. or is there a confidence score than can be output?","['spacy', 'entity-linking']",69893105,"the way the entity linker works is that, given all potential candidates for an entity, it picks the most likely one.
the issue you are running into is that your florist is not known to the model, so he is not a candidate. because the only barack obama the model knows about is the former us president, the model can say with certainty that ""barack obama"" must refer to the president.
the model has no mechanism to tell if a reference refers to an entity not in the knowledge base. it will also never abstain, and if there are candidates it will pick one. ""nil"" is not an abstention, it's for when a reference has no entries in the knowledge base, so there's nothing to pick from.
this may be clearer if you look at the example project, which uses ""emerson"" as an example. the model doesn't decide if ""emerson"" is a person it knows or not - it assumes that it must be one of the people it knows, and it has to pick which one is most likely.",https://stackoverflow.com/questions/69883861,spacy,08-11-2021 12:58,990.0,2.0,1.0,True,09-11-2021 05:10,08-11-2021 13:04
72896382,how to create a dictionnary whose key:value pairs are the values of two different lists of dictionnaries?,"i have 2 lists of dictionnaries that result from a pymongo extraction.
a list of dicts containing id's (string) and lemmas (strings):
lemmas = [{'id': 'id1', 'lemma': 'lemma1'}, {'id': 'id2', 'lemma': 'lemma2'}, {'id': 'id3', 'lemma': 'lemma3'}, ...]
a list of dicts containing id's and multiple words per id:
words = [{'id': 'id1', 'word': 'word1.1'}, {'id': 'id1', 'word': 'word1.2'}, {'id': 'id2', 'word': 'word2.1'}, {'id': 'id3', 'word': 'word3.1'}, {'id': 'id3', 'word': 'word3.2'}, ...]
as you can see, the two lists of dictionnaries are of different length, as there are multiple words associated with each id, but only one lemma.
my goal here is to obtain a dictionnary whose key:value pairs correspond to word:lemma values for the words and lemmas that have the same id. this way, i can replace every word for the corresponding lemma in a text that i am analyzing. for example:
word_lemma_dict = {'word1.1': 'lemma1', 'word1.2': 'lemma1', 'word2.1': 'lemma2', 'word3.1': 'lemma3'; 'word3.2': 'lemma3', ...}

is there a simple way to do this?
the best i could achieve was to use 2 for loops, but it's not very ""pythonistic"":
id_lemma_dict = {}
word_lemma_dict = {}

for dico in lemmas:
    id_lemma_dict[dico['id']] = dico['lemma']  # create id:lemma dict from list of dicts

for dico in words:
    word_lemma_dict[dico['word']] = id_lemma_dict[dico['id']]

print(word_lemma_dict)","['python', 'list', 'dictionary', 'key-value', 'lemmatization']",72896768,"here's an option with comprehensions:
lemmas = [{""id"": ""id1"", ""lemma"":""lemma1""}, {""id"": ""id2"", ""lemma"":""lemma2""}, {""id"": ""id3"", ""lemma"": ""lemma3""}]
words = [{""id"": ""id1"", ""word"": ""word1.1""}, {""id"": ""id1"", ""word"": ""word1.2""}, {""id"": ""id2"", ""word"": ""word2.1""}, {""id"": ""id3"", ""word"": ""word3.1""}, {""id"": ""id3"", ""word"": ""word3.2""}]

lemmas_dict = {item[""id""]: item[""lemma""] for item in lemmas}
word_to_lemma = {word['word']: lemmas_dict[word['id']] for word in words}

print(word_to_lemma)

output:
{'word1.1': 'lemma1', 'word1.2': 'lemma1', 'word2.1': 'lemma2', 'word3.1': 'lemma3', 'word3.2': 'lemma3'}",https://stackoverflow.com/questions/72896382,python,07-07-2022 10:42,108.0,-1.0,1.0,True,07-07-2022 13:25,07-07-2022 10:58
67259823,problem to extract ner subject + verb with spacy and matcher,"i work on an nlp project and i have to use spacy and spacy matcher to extract all named entities who are nsubj (subjects) and the verb to which it relates : the governor verb of my ne nsubj.
example :
georges and his friends live in mexico city
""hello !"", says mary

i'll need to extract ""georges"" and ""live"" in the first sentence and ""mary"" and ""says"" in the second one but i don't know how many words will be between my named entity and the verb to which it relate. so i decided to explore spacy matcher more.
so i'm struggling to write a pattern on matcher to extract my 2 words. when the ne subj is before the verb, i get good results but i don't know how to write a pattern to match a ne subj after words which it correlates to. i could also, according to the guideline, do this task with ""regular spacy"" but i don't know how to do that. the problem with matcher concerns the fact that i can't manage the type of dependency between the ne and verb and grab the good verb. i'm new with spacy, i've always worked with nltk or jieba (for chineese). i don't know even how to tokenize a text in sentence with spacy. but i chose to split the whole text in sentences to avoir bad matching between two sentences.
here is my code
import spacy
from nltk import sent_tokenize
from spacy.matcher import matcher

nlp = spacy.load('fr_core_news_md')

matcher = matcher(nlp.vocab)

def get_entities_verbs():

    try:

        # subjet before verb
        pattern_subj_verb = [{'ent_type': 'per', 'dep': 'nsubj'}, {""pos"": {'not_in':['verb']}, ""dep"": {'not_in':['nsubj']}, 'op':'*'}, {'pos':'verb'}]
        # subjet after verb
        # this pattern is not good

        matcher.add('ent-verb', [pattern_subj_verb])

        for sent in sent_tokenize(open('le_ventre_de_paris-short.txt').read()):
            sent = nlp(sent)
            matches = matcher(sent)
            for match_id, start, end in matches:
                span = sent[start:end]
                print(span)

    except exception as error:
        print(error)


def main():

    get_entities_verbs()

if __name__ == '__main__':
    main()

even if  it's french, i can assert you that i get good results
florent regardait
lacaille reparut
florent baissait
claude regardait
florent resta
florent, soulagï¿½ï¿½
claude sï¿½ï¿½ï¿½ï¿½ï¿½tait arrï¿½ï¿½tï¿½ï¿½
claude en riait
saget est matinale, dit
florent allait
murillo peignait
florent accablï¿½ï¿½
claude entra
claude lï¿½ï¿½ï¿½appelait
florent regardait
florent but son verre de punch ; il le sentit
alexandre, dit
florent levait
claude ï¿½ï¿½tait ravi
claude et florent revinrent
claude, les mains dans les poches, sifflant

i have some wrong results but 90% is good. i just need to grab the first ans last word of each line to have my couple ne/verb.
so my question is. how to extract ne when ne is subj with the verb which it correlates to with matcher or simply how to do that with any factors to be taken into account. do you have a method to get the best results as possible even if 100% is not possible.
i need a pattern matching verb governor + ner subj after from this pattern:
pattern = [
        {
            ""right_id"": ""person"",
            ""right_attrs"": {""ent_type"": ""person"", ""dep"": ""nsubj""},
        },
        {
            ""left_id"": ""person"",
            ""rel_op"": ""<"",
            ""right_id"": ""verb"",
            ""right_attrs"": {""pos"": ""verb""},
        }
        ]

all credit to polm23 for this pattern","['python', 'nlp', 'nltk', 'spacy']",67261112,"this is a perfect use case for the dependency matcher. it also makes things easier if you merge entities to single tokens before running it. this code should do what you need:
import spacy
from spacy.matcher import dependencymatcher

nlp = spacy.load(""en_core_web_sm"")

# merge entities to simplify this
nlp.add_pipe(""merge_entities"")


pattern = [
        {
            ""right_id"": ""person"",
            ""right_attrs"": {""ent_type"": ""person"", ""dep"": ""nsubj""},
        },
        {
            ""left_id"": ""person"",
            ""rel_op"": ""<"",
            ""right_id"": ""verb"",
            ""right_attrs"": {""pos"": ""verb""},
        }
        ]

matcher = dependencymatcher(nlp.vocab)
matcher.add(""perverb"", [pattern])

texts = [
        ""john smith and some other guy live there"",
        '""hello!"", says mary.',
        ]

for text in texts:
    doc = nlp(text)
    matches = matcher(doc)

    for match in matches:
        match_id, (start, end) = match
        # note order here is defined by the pattern, so the nsubj will be first
        print(doc[start], ""::"", doc[end])
    print()


check out the docs for the dependencymatcher.",https://stackoverflow.com/questions/67259823,python,26-04-2021 01:34,1616.0,2.0,1.0,True,22-02-2023 11:56,26-04-2021 17:44
71573565,google cloud vertex ai with .net,"i am new to google cloud service vertex ai.
i am looking to create, train, and deploy an automl text classification model through .net application. i did not find anything for .net with vertex ai. if someone can please guide my to the location or any .net code samples, will be really helpful.","['.net', 'machine-learning', 'google-cloud-platform', 'nlp', 'google-cloud-vertex-ai']",71579667,"you can check google.cloud.automl.v1 nuget package for .net. additionally, check the github of google.cloud.automl.v1 nuget package where you can see the sample codes.",https://stackoverflow.com/questions/71573565,.net,22-03-2022 14:15,2236.0,7.0,2.0,True,26-10-2022 17:41,22-03-2022 20:28
76100086,openai chat completions api: why am i not getting a response if the stream parameter is set to false?,"you can see the $prompt value in my application below. when i type this promp value, chatgpt does not give results. but this is because ""stream"" => false in the params. if ""stream"" => true, chatgpt gives results.
my question here is why chatgpt does not give results when ""stream"" => false. and what to do for it to give results.
$api_key = ""api_key_here"";

$model = 'gpt-3.5-turbo';
$header = [
    ""authorization: bearer "" . $api_key,
    ""content-type: application/json"",
];

$temperature = 0.6;
$frequency_penalty = 0;
$presence_penalty= 0;
$prompt = 'what can you help me with? for example: what do you suggest to keep me motivated?';
 

$messages = array(
    array(
        ""role"" => ""system"",
        ""content"" => ""your name is 'john doe'. i want you to act as a motivational coach. i will provide you with some information about someone's goals and challenges, and it will be your job to come up with strategies that can help this person achieve their goals. this could involve providing positive affirmations, giving helpful advice or suggesting activities they can do to reach their end goal. if you don't understand the question, don't think too much, tell the user to be more specific with more details""
        ),
    array(
        ""role"" => ""assistant"",
        ""content"" => ""hello, i'm john doe, and i'm a motivational coach who loves helping people find their drive and achieve their goals. with years of experience in coaching and personal development, i've developed a unique approach to motivation that combines mindset, energy, and action.""
    ),
    array(
        ""role"" => ""user"",
        ""content"" => $prompt
    )
);
//turbo model
$isturbo = true;
$url = ""
$params = json_encode([
    ""messages"" => $messages,
    ""model"" => $model,
    ""temperature"" => $temperature,
    ""max_tokens"" => 1024,
    ""frequency_penalty"" => $frequency_penalty,
    ""presence_penalty"" => $presence_penalty,
    ""stream"" => false
]);

$curl = curl_init($url);
$options = [
    curlopt_post => true,
    curlopt_ => $header,
    curlopt_postfields => $params,
    curlopt_returntransfer => true,
    curlopt_ssl_verifypeer => false,
    curlopt_ssl_verifyhost => 0,
    curlopt_writefunction => function($curl, $data) {
        //echo $curl;
        $ = curl_getinfo($curl, curlinfo_

        if ($ != 200) {
            $r = json_decode($data);
            echo 'data: {""error"": ""[error]"",""message"":""'.$r->error->message.'""}' . php_eol;
        } else {
            $trimmed_data = trim($data); 
            if ($trimmed_data != '') {
                $response_array = json_decode($trimmed_data, true);
                $content = $response_array['choices'][0]['message']['content'];
                echo $content;
                ob_flush();
                flush();
            }
        }
        return strlen($data);
    },
];

curl_setopt_array($curl, $options);
$response = curl_exec($curl);

if ($response === false) {
    echo 'data: {""error"": ""[error]"",""message"":""'.curl_error($curl).'""}' . php_eol;
}else{

}","['php', 'openai-api', 'chatgpt-api']",76100421,"the reason why you don't get a response back if you set ""stream"" => false is that the whole code is designed to return a response in a streaming fashion when the stream parameter is set to true.
with the following modification, the response will be processed as a whole, regardless of the value of the stream parameter.
try this:
$api_key = ""api_key_here"";

$model = 'gpt-3.5-turbo';
$header = [
    ""authorization: bearer "" . $api_key,
    ""content-type: application/json"",
];

$temperature = 0.6;
$frequency_penalty = 0;
$presence_penalty= 0;
$prompt = 'what can you help me with? for example: what do you suggest to keep me motivated?';

$messages = array(
    array(
        ""role"" => ""system"",
        ""content"" => ""your name is 'john doe'. i want you to act as a motivational coach. i will provide you with some information about someone's goals and challenges, and it will be your job to come up with strategies that can help this person achieve their goals. this could involve providing positive affirmations, giving helpful advice or suggesting activities they can do to reach their end goal. if you don't understand the question, don't think too much, tell the user to be more specific with more details""
        ),
    array(
        ""role"" => ""assistant"",
        ""content"" => ""hello, i'm john doe, and i'm a motivational coach who loves helping people find their drive and achieve their goals. with years of experience in coaching and personal development, i've developed a unique approach to motivation that combines mindset, energy, and action.""
    ),
    array(
        ""role"" => ""user"",
        ""content"" => $prompt
    )
);

$url = ""

$params = json_encode([
    ""messages"" => $messages,
    ""model"" => $model,
    ""temperature"" => $temperature,
    ""max_tokens"" => 1024,
    ""frequency_penalty"" => $frequency_penalty,
    ""presence_penalty"" => $presence_penalty,
    ""stream"" => false
]);

$curl = curl_init($url);
$options = [
    curlopt_post => true,
    curlopt_ => $header,
    curlopt_postfields => $params,
    curlopt_returntransfer => true,
    curlopt_ssl_verifypeer => false,
    curlopt_ssl_verifyhost => 0,
];

curl_setopt_array($curl, $options);
$response = curl_exec($curl);

if ($response === false) {
    echo 'data: {""error"": ""[error]"",""message"":""'.curl_error($curl).'""}' . php_eol;
}else{
    $response_array = json_decode($response, true);
    $content = $response_array['choices'][0]['message']['content'];
    echo $content;
}",https://stackoverflow.com/questions/76100086,php,25-04-2023 10:19,3115.0,0.0,1.0,True,12-06-2024 17:02,12-06-2024 17:02
76459034,how to load a fine-tuned peft/lora model based on llama with huggingface transformers?,"i've followed this tutorial (colab notebook) in order to finetune my model.
trying to load my locally saved model
model = automodelforcausallm.from_pretrained(""finetuned_model"")

yields killed.

trying to load model from hub:
yields
import torch
from peft import peftmodel, peftconfig
from transformers import automodelforcausallm, autotokenizer

peft_model_id = ""lucas0/empath-llama-7b""
config = peftconfig.from_pretrained(peft_model_id)
model = automodelforcausallm.from_pretrained(config.base_model_name_or_path, return_dict=true, load_in_8bit=true, device_map='auto')
tokenizer = autotokenizer.from_pretrained(cwd+""/tokenizer.model"")

# load the lora model
model = peftmodel.from_pretrained(model, peft_model_id)

yields
attributeerror: /home/ubuntu/empath/lora/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cget_col_row_stats

full stacktrace
model creation:
i have finetuned a model using peft and lora:
model = automodelforcausallm.from_pretrained(
""decapoda-research/llama-7b-hf"",
torch_dtype=torch.float16,
device_map='auto',
)

i had to download and manually specify the llama tokenizer.
tokenizer = llamatokenizer(cwd+""/tokenizer.model"")
tokenizer.pad_token = tokenizer.eos_token

to the training:
from peft import loraconfig, get_peft_model

config = loraconfig(
    r=8,
    lora_alpha=16,
    target_modules=[""q_proj"", ""k_proj"", ""v_proj"", ""o_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""causal_lm""
)

model = get_peft_model(model, config)

data = pd.read_csv(""my_csv.csv"")
dataset = dataset.from_pandas(data)
tokenized_dataset = dataset.map(lambda samples: tokenizer(samples[""text""]))

trainer = transformers.trainer(
    model=model,
    train_dataset=tokenized_dataset,
    args=transformers.trainingarguments(
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        warmup_steps=100,
        max_steps=100,
        learning_rate=1e-3,
        fp16=true,
        logging_steps=1,
        output_dir='outputs',
    ),
    data_collator=transformers.datacollatorforlanguagemodeling(tokenizer, mlm=false)
)
model.config.use_cache = true  # silence the warnings. please re-enable for inference!
trainer.train()

and saved it locally with:
trainer.save_model(cwd+""/finetuned_model"")
print(""saved trainer locally"")

as well as to the hub:
model.push_to_hub(""lucas0/empath-llama-7b"", create_pr=1)

how can i load my finetuned model?","['python', 'huggingface-transformers', 'llama-index', 'peft']",76469875,"to load a fine-tuned peft/lora model, take a look at the guanco example, 
import torch
from peft import peftmodel    
from transformers import automodelforcausallm, autotokenizer, llamatokenizer, stoppingcriteria, stoppingcriterialist, textiteratorstreamer

model_name = ""decapoda-research/llama-7b-hf""
adapters_name = ""lucas0/empath-llama-7b""

print(f""starting to load the model {model_name} into memory"")

m = automodelforcausallm.from_pretrained(
    model_name,
    #load_in_4bit=true,
    torch_dtype=torch.bfloat16,
    device_map={"""": 0}
)
m = peftmodel.from_pretrained(m, adapters_name)
m = m.merge_and_unload()
tok = llamatokenizer.from_pretrained(model_name)
tok.bos_token_id = 1

stop_token_ids = [0]

print(f""successfully loaded the model {model_name} into memory"")

you will need an a10g gpu runtime minimally to load the model properly.

for more details see


inference notebook: 
training notebook:",https://stackoverflow.com/questions/76459034,python,12-06-2023 17:34,40610.0,19.0,2.0,True,13-09-2023 09:14,09-09-2023 10:13
78235199,unable to save generated data to jsonl file - always resulting in &quot;wrote 0 examples to finetuning_events.jsonl&quot; message,"issue description
when attempting to generate jsonl data using llama index, the process works well until the final step where the results are saved to a jsonl file. however, every time i try to save the data, it seems to be unsuccessful as i always receive the message ""wrote 0 examples to finetuning_events.jsonl"". i am unsure of the reason behind this issue.
steps to reproduce

successfully generated jsonl data using llama index.
attempted to save the results to a jsonl file.
received the message ""wrote 0 examples to finetuning_events.jsonl"".

additional information

llama index version used: 0.10.22
operating system: windows

log
wrote 0 examples to ./dataset_data/finetuning_events.jsonl
my code:
     def jsonl_generation(self):
        """"""
        generate jsonl file for fine-tuning events and perform model refinement.
        """"""
        # initialize openai finetuninghandler and callbackmanager
        finetuning_handler = openaifinetuninghandler()
        callback_manager = callbackmanager([finetuning_handler])

        self.llm.callback_manager = callback_manager

        # load questions for fine-tuning from a file
        questions = []
        with open(f'{self.dataset_path}/train_questions.txt', ""r"", encoding='utf-8') as f:
            for line in f:
                questions.append(line.strip())

        try:
            # generate responses to the questions using gpt-4 and save the fine-tuning events to a jsonl file
            index = vectorstoreindex.from_documents(
                self.documents
            )
            query_engine = index.as_query_engine(similarity_top_k=2, llm=self.llm)
            for question in questions:
                response = query_engine.query(question)
        except exception as e:
            # handle the exception here, you might want to log the error or take appropriate action
            print(f""an error occurred: {e}"")
        finally:
            # save the fine-tuning events to a jsonl file
            finetuning_handler.save_finetuning_events(f'{self.dataset_path}/finetuning_events.jsonl')","['openai-api', 'llama-index', 'fine-tuning']",78249277,"i just solved the problem.
it's my solution. currently, it's storing the dataset to jsonl data.
    def jsonl_generation(self):
        """"""
        generate jsonl file for fine-tuning events and perform model refinement.
        """"""
        # initialize openai finetuninghandler and callbackmanager
        finetuning_handler = openaifinetuninghandler()
        callback_manager = callbackmanager([finetuning_handler])

        llm = openai(model=""gpt-4"", temperature=0.3)
        settings.callback_manager, = (callback_manager,)

        # load questions for fine-tuning from a file
        questions = []
        with open(f'{self.dataset_path}/train_questions.txt', ""r"", encoding='utf-8') as f:
            for line in f:
                questions.append(line.strip())

        try:
            from llama_index.core import vectorstoreindex
            # generate responses to the questions using gpt-4 and save the fine-tuning events to a jsonl file
            index = vectorstoreindex.from_documents(
                self.documents
            )
            query_engine = index.as_query_engine(similarity_top_k=2, llm=llm)
            for question in questions:
                response = query_engine.query(question)
        except exception as e:
            # handle the exception here, you might want to log the error or take appropriate action
            print(f""an error occurred: {e}"")
        finally:
            # save the fine-tuning events to a jsonl file
            finetuning_handler.save_finetuning_events(f'{self.dataset_path}/finetuning_events.jsonl')",https://stackoverflow.com/questions/78235199,openai-api,28-03-2024 00:04,86.0,1.0,1.0,True,08-04-2024 19:01,08-04-2024 19:01
78521707,openai chat completions api: how do i solve the apiremovedinv1 error when using the openai python library for gpt-4?,"this is a snippet where i have been encountering the problem:
def generate_response(prompt):
    response = openai.chatcompletion.create(
        model=""gpt-4"",  # use ""gpt-4"" as the model identifier if ""gpt-4o"" is deprecated
        messages=[
            {""role"": ""system"", ""content"": ""you are a helpful assistant.""},
            {""role"": ""user"", ""content"": prompt}
        ],
        max_tokens=150
    )
    return response.choices[0].message['content'].strip()

i tried the following to solve the apiremovedinv1 error:

i have tried downgrading the library version.
i have gone through the openai documentation and tried the asynchronous method too but didn't work.","['python', 'openai-api', 'gpt-4']",78522605,"you have a double typo. you're missing a dot and the letter s.
change this...
openai.chatcompletion.create

...to this.
openai.chat.completions.create

the full code is:
from openai import openai
client = openai()

completion = client.chat.completions.create()
  model=""gpt-4"",
  messages=[
    {""role"": ""system"", ""content"": ""you are a helpful assistant.""},
    {""role"": ""user"", ""content"": ""hello!""}
  ]
)

print(completion.choices[0].message)",https://stackoverflow.com/questions/78521707,python,23-05-2024 08:04,522.0,-1.0,1.0,True,12-06-2024 12:47,12-06-2024 12:45
49964028,spacy oserror: can&#39;t find model &#39;en&#39;,"even though i downloaded the model it cannot load it
[jalal@goku entity-sentiment-analysis]$ which python
/scratch/sjn/anaconda/bin/python
[jalal@goku entity-sentiment-analysis]$ sudo python -m spacy download en
[sudo] password for jalal: 
collecting 
  downloading  (37.4mb)
    100% |ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½| 37.4mb 9.4mb/s 
installing collected packages: en-core-web-sm
  running setup.py install for en-core-web-sm ... done
successfully installed en-core-web-sm-2.0.0

    linking successful
    /usr/lib/python2.7/site------------------------------------------------------------
oserror                                   traceback (most recent call last)
<ipython-input-2-0fcabaab8c3d> in <module>()
      1 import spacy
      2 
----> 3 nlp = spacy.load('en')

/scratch/sjn/anaconda/lib/python3.6/site-packages/spacy/__init__.py in load(name, **overrides)
     17             ""to load. for example:\nnlp = spacy.load('{}')"".format(depr_path),
     18             'error')
---> 19     return util.load_model(name, **overrides)
     20 
     21 

/scratch/sjn/anaconda/lib/python3.6/site-packages/spacy/util.py in load_model(name, **overrides)
    118     elif hasattr(name, 'exists'):  # path or path-like to model data
    119         return load_model_from_path(name, **overrides)
--> 120     raise ioerror(""can't find model '%s'"" % name)
    121 
    122 

oserror: can't find model 'en'

how should i fix this?
if i don't use sudo for downloading the en model, i get:
collecting 
  downloading  (37.4mb)
    100% |ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½| 37.4mb 9.6mb/s ta 0:00:011   62% |ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½            | 23.3mb 8.6mb/s eta 0:00:02
requirement already satisfied (use -via the 'pip install --upgrade pip' command.

    error: couldn't link model to 'en'
    creating a symlink in spacy/data failed. make sure you have the required
    permissions and try re-running the command as admin, or use a
    virtualenv. you can still import the model as a module and call its
    load() method, or create the symlink manually.

    /scratch/sjn/anaconda/lib/python3.6/site-packages/en_core_web_sm -->
    /scratch/sjn/anaconda/lib/python3.6/site-packages/spacy/data/en


    download successful but linking failed
    creating a shortcut link for 'en' didn't work (maybe you don't have
    admin permissions?), but you can still load the model via its full
    package name:

    nlp = spacy.load('en_core_web_sm')","['nlp', 'spacy']",50342159,"oh well. turns out even though my which python was showing anaconda python, when i was using python download it was linking it to python2.7 local on my machine. i fixed it using below command:
$ sudo /scratch/sjn/anaconda/bin/python -m spacy download en",https://stackoverflow.com/questions/49964028,nlp,22-04-2018 08:33,132338.0,64.0,14.0,True,02-04-2022 01:50,26-04-2018 23:09
12206276,how to check if given word is in plural or singular form?,question like in topic - i'm trying to do that in python for app in google app engine. i know pyenchant library is used for natural language recognition but i don't see if i can use it for my problem and how.,"['python', 'nlp']",12206471,"checkout the inflect 0.2.4 library.

inflect 0.2.4
correctly generate plurals, singular nouns, ordinals, indefinite
  articles; convert numbers to words",https://stackoverflow.com/questions/12206276,python,30-08-2012 22:09,23307.0,9.0,3.0,True,29-01-2025 12:09,30-08-2012 22:12
20290870,improving the extraction of human names with nltk,"i am trying to extract human names from text. 
does anyone have a method that they would recommend?
this is what i tried (code is below):
i am using nltk to find everything marked as a person and then generating a list of all the nnp parts of that person. i am skipping persons where there is only one nnp which avoids grabbing a lone surname.
i am getting decent results but was wondering if there are better ways to go about solving this problem.
code:
import nltk
from nameparser.parser import humanname

def get_human_names(text):
    tokens = nltk.tokenize.word_tokenize(text)
    pos = nltk.pos_tag(tokens)
    sentt = nltk.ne_chunk(pos, binary = false)
    person_list = []
    person = []
    name = """"
    for subtree in sentt.subtrees(filter=lambda t: t.node == 'person'):
        for leaf in subtree.leaves():
            person.append(leaf[0])
        if len(person) > 1: #avoid grabbing lone surnames
            for part in person:
                name += part + ' '
            if name[:-1] not in person_list:
                person_list.append(name[:-1])
            name = ''
        person = []

    return (person_list)

text = """"""
some economists have responded positively to bitcoin, including 
francois r. velde, senior economist of the federal reserve in chicago 
who described it as ""an elegant solution to the problem of creating a 
digital currency."" in november 2013 richard branson announced that 
virgin galactic would accept bitcoin as payment, saying that he had invested 
in bitcoin and found it ""fascinating how a whole new global currency 
has been created"", encouraging others to also invest in bitcoin.
other economists commenting on bitcoin have been critical. 
economist paul krugman has suggested that the structure of the currency 
incentivizes hoarding and that its value derives from the expectation that 
others will accept it as payment. economist larry summers has expressed 
a ""wait and see"" attitude when it comes to bitcoin. nick colas, a market 
strategist for convergex group, has remarked on the effect of increasing 
use of bitcoin and its restricted supply, noting, ""when incremental 
adoption meets relatively fixed supply, it should be no surprise that 
prices go up. and thatï¿½ï¿½ï¿½s exactly what is happening to btc p""
""""""

names = get_human_names(text)
print ""last, first""
for name in names: 
    last_first = humanname(name).last + ', ' + humanname(name).first
        print last_first

output:
last, first
velde, francois
branson, richard
galactic, virgin
krugman, paul
summers, larry
colas, nick

apart from virgin galactic, this is all valid output. of course, knowing that virgin galactic isn't a human name in the context of this article is the hard (maybe impossible) part.","['python', 'nlp', 'nltk']",24119115,"must agree with the suggestion that ""make my code better"" isn't well suited for this site, but i can give you some way where you can try to dig in.
disclaimer: this answer is ~7 years old. definitely, it needs to be updated to newer python and nltk versions. please, try to do it yourself, and if it works, share your know-how with us.
take a look at stanford named entity recognizer (ner). its binding has been included in nltk v 2.0, but you must download some core files. here is script which can do all of that for you.
i wrote this script:
import nltk
from nltk.tag.stanford import nertagger
st = nertagger('stanford-ner/all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')
text = """"""your text goes here""""""

for sent in nltk.sent_tokenize(text):
    tokens = nltk.tokenize.word_tokenize(sent)
    tags = st.tag(tokens)
    for tag in tags:
        if tag[1]=='person': print tag

and got not so bad output:

('francois', 'person')
('r.', 'person')
('velde', 'person')
('richard', 'person')
('branson', 'person')
('virgin', 'person')
('galactic', 'person')
('bitcoin', 'person')
('bitcoin', 'person')
('paul', 'person')
('krugman', 'person')
('larry', 'person')
('summers', 'person')
('bitcoin', 'person')
('nick', 'person')
('colas', 'person')

hope this is helpful.",https://stackoverflow.com/questions/20290870,python,29-11-2013 17:33,109240.0,56.0,7.0,True,09-04-2022 13:36,15-09-2019 17:22
76252227,importerror: using the `trainer` with `pytorch` requires `accelerate`,"i wrote a code to train a ner model before few months and it was working well. however, the same code now gives me this error:
importerror: using the `trainer` with `pytorch` requires `accelerate`: run `pip install --upgrade accelerate`


i tried to install accelerate by following the instruction here however it is not running, this is the output when i tried to run accelerate:
2023-05-15 07:28:08.310904: w tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] tf-trt warning: could not find tensorrt

copy-and-paste the text below in your github issue

- `accelerate` version: 0.19.0
- platform: linux-5.15.107+-x86_64-with-glibc2.31
- python version: 3.10.11
- numpy version: 1.22.4
- pytorch version (gpu?): 2.0.0+cu118 (true)
- system ram: 12.68 gb
- gpu type: tesla t4
- `accelerate` default config:
    - compute_environment: local_machine
    - distributed_type: no
    - mixed_precision: fp16
    - use_cpu: false
    - num_processes: 1
    - machine_rank: 0
    - num_machines: 1
    - rdzv_backend: static
    - same_network: false
    - main_training_function: main
    - downcast_bf16: false
    - tpu_use_cluster: false
    - tpu_use_sudo: false

i also installed tensorrt but this didn't solve the issue.
here is my complete code for training.","['python', 'pytorch', 'huggingface-transformers', 'tensorrt', 'accelerate']",76441057,"either installing !pip install accelerate -u and then restarting the runtime, or downgrading the pytorch to 1.3.1 version should work.
! pip install torch==1.13.1",https://stackoverflow.com/questions/76252227,python,15-05-2023 08:34,9349.0,2.0,5.0,True,18-05-2024 03:53,15-05-2023 20:25
73290224,python - typeerror: __init__() got an unexpected keyword argument &#39;checkpoint_callback&#39;,"i'm getting this error message:
typeerror                                 traceback (most recent call last)
<ipython-input-41-2892cdd4e738> in <module>()
      5   max_epochs=n_epochs,
      6   gpus=1, #gpu
----> 7   progress_bar_refresh_rate=30
      8 )

/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/argparse.py in insert_env_defaults(self, *args, **kwargs)
    343 
    344         # all args were already moved to kwargs
--> 345         return fn(self, **kwargs)
    346 
    347     return cast(_t, insert_env_defaults)

typeerror: __init__() got an unexpected keyword argument 'checkpoint_callback'

... when i run this chunk:
trainer = pl.trainer(
  logger=logger, 
  checkpoint_callback=checkpoint_callback,
  callbacks=[early_stopping_callback],
  max_epochs=n_epochs,
  gpus=1, #gpu
  progress_bar_refresh_rate=30
)

the 'checkpoint_callback' is defined like this:
checkpoint_callback = modelcheckpoint(
  dirpath=""checkpoints"",
  filename=""best-checkpoint"",
  save_top_k=1,
  verbose=true,
  monitor=""val_loss"",
  mode=""min""
)

i can't figure out what's causing the error - can anyone help me?
view full source code here:","['python', 'error-handling', 'nlp', 'pytorch', 'pytorch-lightning']",73290470,"as i am looking into pytorch_lightning github, i do not see checkpoint_callback variable in init (
are you sure thats how it's called? what do you want to achieve by passing this checkpoint_callback?
//edit:
i think you just have to append checkpoint_callback to callbacks list",https://stackoverflow.com/questions/73290224,python,09-08-2022 10:22,12470.0,2.0,2.0,True,07-01-2024 14:34,10-08-2022 21:18
73442636,spacy: list index out of range,"i encountered ""list index out of range"" error when running nlp(text_1) with spacy. however, no issue on text_2 (similar to text_1, except sentences 1 and 2 are joined as one). this is my code:
import spacy
nlp = spacy.load(""en_core_web_lg"")
nlp.add_pipe(""entitylinker"", last=true)

text_1 = """"""
those parts were designed by massimiliano siccardi, with original music by luca longobardi.
the show's arrival in new york comes as the city emerges from a lockdown that shut down cultural events and art crowds.
'i feel like this is a huge, bright beacon of hope for arts in new york.' said korins.
""""""
text_1 = contractions.fix(unidecode(text_1)).strip()
nlp(text_1) 

text_2 = """"""
those parts were designed by massimiliano siccardi, with original music by luca longobardi.the show's arrival in new york comes as the city emerges from a lockdown that shut down cultural events and art crowds.
'i feel like this is a huge, bright beacon of hope for arts in new york.' said korins.
""""""
text_2 = contractions.fix(unidecode(text_2)).strip()
nlp(text_2)

may i know if anyone can help to explain? thanks.","['python', 'nlp', 'spacy']",73452491,"i have solved the issue by removing '\n':
text_1 = contractions.fix(unidecode(text_1)).strip().replace('\n', '')

thanks, everyone for your suggestions.",https://stackoverflow.com/questions/73442636,python,22-08-2022 09:05,223.0,1.0,1.0,True,23-08-2022 02:21,22-08-2022 09:39
79549110,huggingface tokenizer: &#39;str&#39; object has no attribute &#39;size&#39;,"i am trying to extract the hidden states of a transformer model:
from transformers import automodel
import torch
from transformers import autotokenizer

model_ckpt = ""distilbert-base-uncased""
device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
tokenizer = autotokenizer.from_pretrained(model_ckpt)
model = automodel.from_pretrained(model_ckpt).to(device)

from datasets import load_dataset

emotions = load_dataset(""emotion"", ignore_verifications=true)

# tokenize data
def tokenize(batch):
    return tokenizer(batch[""text""], padding=true, truncation=true)

emotions_encoded = emotions.map(tokenize, batched=true, batch_size=none)

def extract_hidden_states(batch):
    inputs = {k:v.to(device) for k,v in batch.items()
             if k in tokenizer.model_input_names}
    with torch.no_grad():
        last_hidden_state = model(*inputs).last_hidden_state
    return{""hidden_state"": last_hidden_state[:,0].cpu().numpy()}

# convert input_ids and attention_mask columns to ""torch"" format 
emotions_encoded.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""label""])

# extract hidden states
emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=true)

however, on running the last line i get the error 'str' object has no attribute 'size'
i've tried deprecating the transformers package but that didn't fix it. some posts online indicate it may have to do with the transformer package will return a dictionary by default, but i don't know how to work around that.
full error:
attributeerror                            traceback (most recent call last)
cell in[8], line 5
      2 emotions_encoded.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""label""])
      4 # extract hidden states
----> 5 emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=true)

file ~\anaconda3\envs\ml\lib\site-packages\datasets\dataset_dict.py:851, in datasetdict.map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)
    848 if cache_file_names is none:
    849     cache_file_names = {k: none for k in self}
    850 return datasetdict(
--> 851     {
    852         k: dataset.map(
    853             function=function,
    854             with_indices=with_indices,
    855             with_rank=with_rank,
    856             input_columns=input_columns,
    857             batched=batched,
    858             batch_size=batch_size,
    859             drop_last_batch=drop_last_batch,
    860             remove_columns=remove_columns,
    861             keep_in_memory=keep_in_memory,
    862             load_from_cache_file=load_from_cache_file,
    863             cache_file_name=cache_file_names[k],
    864             writer_batch_size=writer_batch_size,
    865             features=features,
    866             disable_nullable=disable_nullable,
    867             fn_kwargs=fn_kwargs,
    868             num_proc=num_proc,
    869             desc=desc,
    870         )
    871         for k, dataset in self.items()
    872     }
    873 )

file ~\anaconda3\envs\ml\lib\site-packages\datasets\dataset_dict.py:852, in <dictcomp>(.0)
    848 if cache_file_names is none:
    849     cache_file_names = {k: none for k in self}
    850 return datasetdict(
    851     {
--> 852         k: dataset.map(
    853             function=function,
    854             with_indices=with_indices,
    855             with_rank=with_rank,
    856             input_columns=input_columns,
    857             batched=batched,
    858             batch_size=batch_size,
    859             drop_last_batch=drop_last_batch,
    860             remove_columns=remove_columns,
    861             keep_in_memory=keep_in_memory,
    862             load_from_cache_file=load_from_cache_file,
    863             cache_file_name=cache_file_names[k],
    864             writer_batch_size=writer_batch_size,
    865             features=features,
    866             disable_nullable=disable_nullable,
    867             fn_kwargs=fn_kwargs,
    868             num_proc=num_proc,
    869             desc=desc,
    870         )
    871         for k, dataset in self.items()
    872     }
    873 )

file ~\anaconda3\envs\ml\lib\site-packages\datasets\arrow_dataset.py:578, in transmit_tasks.<locals>.wrapper(*args, **kwargs)
    576     self: ""dataset"" = kwargs.pop(""self"")
    577 # apply actual function
--> 578 out: union[""dataset"", ""datasetdict""] = func(self, *args, **kwargs)
    579 datasets: list[""dataset""] = list(out.values()) if isinstance(out, dict) else [out]
    580 for dataset in datasets:
    581     # remove task templates if a column mapping of the template is no longer valid

file ~\anaconda3\envs\ml\lib\site-packages\datasets\arrow_dataset.py:543, in transmit_format.<locals>.wrapper(*args, **kwargs)
    536 self_format = {
    537     ""type"": self._format_type,
    538     ""format_kwargs"": self._format_kwargs,
    539     ""columns"": self._format_columns,
    540     ""output_all_columns"": self._output_all_columns,
    541 }
    542 # apply actual function
--> 543 out: union[""dataset"", ""datasetdict""] = func(self, *args, **kwargs)
    544 datasets: list[""dataset""] = list(out.values()) if isinstance(out, dict) else [out]
    545 # re-apply format to the output

file ~\anaconda3\envs\ml\lib\site-packages\datasets\arrow_dataset.py:3073, in dataset.map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)
   3065 if transformed_dataset is none:
   3066     with logging.tqdm(
   3067         disable=not logging.is_progress_bar_enabled(),
   3068         unit="" examples"",
   (...)
   3071         desc=desc or ""map"",
   3072     ) as pbar:
-> 3073         for rank, done, content in dataset._map_single(**dataset_kwargs):
   3074             if done:
   3075                 shards_done += 1

file ~\anaconda3\envs\ml\lib\site-packages\datasets\arrow_dataset.py:3449, in dataset._map_single(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)
   3445 indices = list(
   3446     range(*(slice(i, i + batch_size).indices(shard.num_rows)))
   3447 )  # something simpler?
   3448 try:
-> 3449     batch = apply_function_on_filtered_inputs(
   3450         batch,
   3451         indices,
   3452         check_same_num_examples=len(shard.list_indexes()) > 0,
   3453         offset=offset,
   3454     )
   3455 except numexamplesmismatcherror:
   3456     raise datasettransformationnotallowederror(
   3457         ""using `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn't create or remove existing examples. you can first run `.drop_index() to remove your index and then re-add it.""
   3458     ) from none

file ~\anaconda3\envs\ml\lib\site-packages\datasets\arrow_dataset.py:3330, in dataset._map_single.<locals>.apply_function_on_filtered_inputs(pa_inputs, indices, check_same_num_examples, offset)
   3328 if with_rank:
   3329     additional_args += (rank,)
-> 3330 processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
   3331 if isinstance(processed_inputs, lazydict):
   3332     processed_inputs = {
   3333         k: v for k, v in processed_inputs.data.items() if k not in processed_inputs.keys_to_format
   3334     }

cell in[7], line 6, in extract_hidden_states(batch)
      3 inputs = {k:v.to(device) for k,v in batch.items()
      4          if k in tokenizer.model_input_names}
      5 with torch.no_grad():
----> 6     last_hidden_state = model(*inputs).last_hidden_state
      7 return{""hidden_state"": last_hidden_state[:,0].cpu().numpy()}

file ~\anaconda3\envs\ml\lib\site-packages\torch\nn\modules\module.py:1511, in module._wrapped_call_impl(self, *args, **kwargs)
   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1510 else:
-> 1511     return self._call_impl(*args, **kwargs)

file ~\anaconda3\envs\ml\lib\site-packages\torch\nn\modules\module.py:1520, in module._call_impl(self, *args, **kwargs)
   1515 # if we don't have any hooks, we want to skip the rest of the logic in
   1516 # this function, and just call forward.
   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1518         or _global_backward_pre_hooks or _global_backward_hooks
   1519         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1520     return forward_call(*args, **kwargs)
   1522 try:
   1523     result = none

file ~\anaconda3\envs\ml\lib\site-packages\transformers\models\distilbert\modeling_distilbert.py:593, in distilbertmodel.forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    591 elif input_ids is not none:
    592     self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)
--> 593     input_shape = input_ids.size()
    594 elif inputs_embeds is not none:
    595     input_shape = inputs_embeds.size()[:-1]

attributeerror: 'str' object has no attribute 'size'","['python', 'pytorch', 'huggingface-transformers']",79549206,"the issue is happening when you're filtering the dictionary, extract_hidden_states in your extract_hidden_states() function. this dictionary includes keys like 'text' (which contains strings), the function may mistakenly try to .to(device) on a string, which i'm guessing is causing the error here.
you can modify your function this way:
def extract_hidden_states(batch):
    inputs = {k: v for k, v in batch.items() if k in tokenizer.model_input_names}
    
    # ensure all inputs are tensors before sending them to device
    inputs = {k: v.clone().detach().to(device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model(**inputs)  # unpacking inputs properly
        last_hidden_state = outputs.last_hidden_state

    return {""hidden_state"": last_hidden_state[:, 0].cpu().numpy()}",https://stackoverflow.com/questions/79549110,python,01-04-2025 20:19,45.0,1.0,1.0,True,02-04-2025 16:35,02-04-2025 16:35
77767946,how to use local files in an azure function hosted on the linux consumption plan?,i have an event grid triggered function on azure under the linux consumption plan. the code requires finding / loading some local model files. how do i get these files deployed to the azure function and specify the path where the files are located? any help is appreciated. thanks!,"['azure', 'azure-functions', 'stanford-nlp', 'ml.net']",77779377,"to deploy the local file to azure you need to add item in your .csproj file.
  <itemgroup>
    <none include=""models\**\*"">
    <copytooutputdirectory>preservenewest</copytooutputdirectory>
    </none>
  </itemgroup>


note : here models\**\* is used to add all subdirectories and all its files

to get the path of the file use
var path = path.getfullpath(path.getdirectoryname(assembly.getexecutingassembly().location));

thanks to @benspeth and @louie almeda
for reference check this github and so link respectively.
#my directory

in isolated model we need to define custom type for event properties as mentioned in document.
eventgridtrigger.cs:
using system;
using system.text.json;
using azure.messaging;
using system.reflection;
using microsoft.azure.functions.worker;
using microsoft.extensions.logging;

namespace company.function
{
    public class eventgridtrigger
    {
        private readonly ilogger<eventgridtrigger> _logger;

        public eventgridtrigger(ilogger<eventgridtrigger> logger)
        {
            _logger = logger;
        }

        [function(nameof(eventgridtrigger))]
        public void run([eventgridtrigger] myeventtype myevent, functioncontext context)
        {
            console.writeline($""event type: {myevent.eventtype}, event subject: {myevent.subject}"");

            try
            {
                var path = path.getfullpath(path.getdirectoryname(assembly.getexecutingassembly().location));
                console.writeline(path);
                string modelcontent = file.readalltext(path+""/models/model1.json"");

                console.write($""data of the file: {modelcontent.tostring()}"");
            }
            catch (exception ex)
            {
                _logger.logerror($""error processing model1.json: {ex.message}"");
            }
        }
    }
    public class myeventtype
    {
        public string id { get; set; }

        public string subject { get; set; }

        public string topic { get; set; }

        public string eventtype { get; set; }

        public datetime eventtime { get; set; }

        public idictionary<string, object> data { get; set; }
    }
}

csharp.csproj:
<project sdk=""microsoft.net.sdk"">
  <propertygroup>
    <targetframework>net8.0</targetframework>
    <azurefunctionsversion>v4</azurefunctionsversion>
    <outputtype>exe</outputtype>
    <implicitusings>enable</implicitusings>
    <nullable>enable</nullable>
  </propertygroup>
  <itemgroup>
    <packagereference include=""microsoft.azure.functions.worker"" version=""1.20.0"" />
    <packagereference include=""microsoft.azure.functions.worker.extensions.eventgrid"" version=""3.3.0"" />
    <packagereference include=""microsoft.azure.functions.worker.extensions. version=""3.1.0"" />
    <packagereference include=""microsoft.azure.functions.worker.sdk"" version=""1.16.2"" />
    <packagereference include=""microsoft.applicationinsights.workerservice"" version=""2.21.0"" />
    <packagereference include=""microsoft.azure.functions.worker.applicationinsights"" version=""1.0.0"" />
  </itemgroup>
  <itemgroup>
    <none update=""host.json"">
      <copytooutputdirectory>preservenewest</copytooutputdirectory>
    </none>
    <none update=""local.settings.json"">
      <copytooutputdirectory>preservenewest</copytooutputdirectory>
      <copytopublishdirectory>never</copytopublishdirectory>
    </none>
  </itemgroup>
  <itemgroup>
    <using include=""system.threading.executioncontext"" alias=""executioncontext"" />
  </itemgroup>
  <itemgroup>
    <none include=""models\**\*"">
    <copytooutputdirectory>preservenewest</copytooutputdirectory>
    </none>
  </itemgroup>
</project>

model.json:
{
  ""model_name"": ""model1"",
  ""description"": ""this is a sample model file."",
  ""value"":10,
  ""version"": ""1.0""
}

output:",https://stackoverflow.com/questions/77767946,azure,06-01-2024 01:24,833.0,0.0,1.0,True,08-01-2024 11:16,08-01-2024 11:12
76561482,s3 object as gensim linesentence,"is it possible to use a txt or jsonl file in an s3 bucket as the corpus_file input for a gensim doc2vec model? i am looking for something of the form:
doc2vec(corpus_file=""s3://bucket_name/subdir/sample.jsonl"")

when i run the above line, i get the following error:
typeerror: parameter corpus_file must be a valid path to a file, got 's3://bucket_name/subdir/sample.jsonl' instead.

i have also tried creating an iterator object that iterates through the file and yields its lines, and passing it as the corpus_file argument. but i get the same typeerror.
please note that i am specifically looking to use the corpus_file argument instead of the documents.","['python', 'amazon-s3', 'nlp', 'gensim', 'doc2vec']",76561563,"the corpus_file mode requires random-seek access to the file for its technique, which involves every worker thread opening its own unique file view on distinct ranges of the file. such access is not well-supported for s3 (http get) access.
to use corpus_file mode, download the file to a local volume whose filesystem offers efficient seek access.
or, supply things as a corpus iterable - which can re-iterate over a remote streamed file multiple times, but won't achieve the same high thread utilization. (from an iteratable, even if you have 16+ cores, you'll usually get optimal throughput with no more than 6-12 worker threads ï¿½ï¿½ï¿½ even if you've eliminated io & expensive in-iterable preprocesing from the setup. the exact optimal number of workers depends on other model parameters ï¿½ï¿½ï¿½ it's especially sensitive to vector_size, negative, & window</",https://stackoverflow.com/questions/76561482,python,27-06-2023 03:47,70.0,0.0,1.0,True,27-06-2023 04:10,27-06-2023 03:52
72724748,huggingface transformers padding vs pad_to_max_length,"i'm running a code by using pad_to_max_length = true and everything works fine. only i get a warning as follow:

futurewarning: the pad_to_max_length argument is deprecated and
will be removed in a future version, use padding=true or
padding='longest' to pad to the longest sequence in the batch, or
use padding='max_length' to pad to a max length. in this case, you
can give a specific length with max_length (e.g. max_length=45) or
leave max_length to none to pad to the maximal input size of the model
(e.g. 512 for bert).

but when i change pad_to_max_length = true to padding='max_length' i get this error:
runtimeerror: stack expects each tensor to be equal size, but got [60] at entry 0 and [64] at entry 6

how can i change the code to the new version? is there anything i got wrong with the warning documentation?
this is my encoder:
encoding = self.tokenizer.encode_plus(
    poem,
    add_special_tokens=true,
    max_length= 60,
    return_token_type_ids=false,
    pad_to_max_length = true,
    return_attention_mask=true,
    return_tensors='pt',
)","['python', 'nlp', 'huggingface-transformers', 'huggingface-tokenizers']",72724814,"it seems that the documentation is not complete enough!
you should add truncation=true too to memic the pad_to_max_length = true.
like this:
encoding = self.tokenizer.encode_plus(
    poem,
    add_special_tokens=true,
    max_length=self.max_len,
    return_token_type_ids=false,
    padding='max_length',
    truncation=true,
    return_attention_mask=true,
    return_tensors='pt',
)",https://stackoverflow.com/questions/72724748,python,23-06-2022 04:58,4472.0,2.0,1.0,True,23-06-2022 08:51,23-06-2022 08:51
78280443,google colab: error when importing tfbertmodel,"i have an error in google colab when importing tfbertmodel, two months before everything worked fine.
from transformers import tfbertmodel

i receive:
    attributeerror                            traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py in _get_module(self, module_name)
   1389         try:
-> 1390             return importlib.import_module(""."" + module_name, self.__name__)
   1391         except exception as e:

25 frames
attributeerror: module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'

the above exception was the direct cause of the following exception:

runtimeerror                              traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py in _get_module(self, module_name)
   1390             return importlib.import_module(""."" + module_name, self.__name__)
   1391         except exception as e:
-> 1392             raise runtimeerror(
   1393                 f""failed to import {self.__name__}.{module_name} because of the following error (look up to see its""
   1394                 f"" traceback):\n{e}""

runtimeerror: failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):
module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'


the version of keras is 3.1.1, tensorflow 2.16.1, transformers version 4.38.2","['tensorflow', 'keras', 'google-colaboratory', 'huggingface-transformers']",78281190,"for the runtime python3 + cpu, created today, i was able to run the following tfbertmodel just fine in google colab today.  i noticed that our tensorflow versions differ, other than that i don't believe that tfbertmodel is being deprecated :
transformers version: 4.38.2
tensorflow version: 2.15.0
example working tfbertmodel code
import tensorflow as tf
from transformers import berttokenizer, tfbertmodel

# instantiate the tokenizer and the model
tokenizer = berttokenizer.from_pretrained('bert-base-uncased')
model = tfbertmodel.from_pretrained('bert-base-uncased')

# for fun, let's encode some text
input_texts = [""hello world, i'm using bert!"", ""i am also using bert in a sentence.""]
encoding = tokenizer(input_texts, return_tensors='tf', padding=true, truncation=true)

# get the bert representations
outputs = model(encoding['input_ids'], attention_mask=encoding['attention_mask'])
last_hidden_state = outputs.last_hidden_state

print(last_hidden_state)

output:
some weights of the pytorch model were not used when initializing the tf 2.0 model tfbertmodel: ['cls.seq_relationship.weight', 'cls.predictions.transform.layernorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.layernorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- this is expected if you are initializing tfbertmodel from a pytorch model trained on another task or with another architecture (e.g. initializing a tfbertforsequenceclassification model from a bertforpretraining model).
- this is not expected if you are initializing tfbertmodel from a pytorch model that you expect to be exactly identical (e.g. initializing a tfbertforsequenceclassification model from a bertforsequenceclassification model).
all the weights of tfbertmodel were initialized from the pytorch model.
if your task is similar to the task the model of the checkpoint was trained on, you can already use tfbertmodel for predictions without further training.
tf.tensor(
[[[ 0.05163623  0.25609216  0.05970613 ... -0.20756713  0.06182499
    0.7532056 ]
...",https://stackoverflow.com/questions/78280443,tensorflow,05-04-2024 14:02,352.0,1.0,2.0,True,05-04-2024 16:18,05-04-2024 15:30
77943395,openai embeddings api: how to extract the embedding vector?,"i use nearly the same code as here in this github repo to get embeddings from openai:
oai = openai(
# this is the default and can be omitted
api_key=""sk-....."",
)

def get_embedding(text_to_embed, openai):
   
    response = openai.embeddings.create(
        model= ""text-embedding-ada-002"",
        input=[text_to_embed]
    )
    
    return response

embedding_raw = get_embedding(text,oai)

according to the github repo, the vector should be in response['data'][0]['embedding']. but it isn't in my case.
when i printed the response variable, i got this:
print(embedding_raw)

output:
createembeddingresponse(data=[embedding(embedding=[0.009792150929570198, -0.01779201813042164, 0.011846082285046577, -0.0036859565880149603, -0.0013213189085945487, 0.00037509595858864486,..... -0.0121011883020401, -0.015751168131828308], index=0, object='embedding')], model='text-embedding-ada-002', object='list', usage=usage(prompt_tokens=360, total_tokens=360))

how can i access the embedding vector?","['python', 'vector', 'openai-api', 'embedding', 'openaiembeddings']",77943419,"simply return just the embedding vector as follows:
def get_embedding(text_to_embed, openai):
   
    response = openai.embeddings.create(
        model= ""text-embedding-ada-002"",
        input=[text_to_embed]
    )
    
    return response.data[0].embedding # change this

embedding_raw = get_embedding(text,oai)",https://stackoverflow.com/questions/77943395,python,05-02-2024 19:05,3834.0,4.0,1.0,True,15-02-2024 19:05,15-02-2024 19:05
73866404,how can i use a dataframe of multi-value in each cell as an input to machine learning for classification,"i build a data frame with multivalued in each cell as picture below 
and i want to use logistic regression for classification>>>>
my code is :
fds1 = pd.dataframe(featuresdata)
    fds1.fillna('', inplace=true)
    from sklearn.model_selection import train_test_split, cross_val_score
    x_train, x_test, y_train, y_test = train_test_split(fds1, y, test_size=0.30, random_state=100)
    from sklearn.linear_model import logisticregression
    classifier = logisticregression()
    classifier.fit(x_train, y_train)
    score = classifier.score(x_test, y_test)
    print(""accuracy for logistic regression:"", score)

but there was an error with this code:
file ""c:\users\hp\pycharmprojects\pythonproject\fe2.py"", line 317, in cls2butclick
    classifier.fit(x_train, y_train)
  file ""c:\users\hp\pycharmprojects\pythonproject\venv\lib\site-packages\sklearn\linear_model\_logistic.py"", line 1138, in fit
    x, y = self._validate_data(
  file ""c:\users\hp\pycharmprojects\pythonproject\venv\lib\site-packages\sklearn\base.py"", line 596, in _validate_data
    x, y = check_x_y(x, y, **check_params)
  file ""c:\users\hp\pycharmprojects\pythonproject\venv\lib\site-packages\sklearn\utils\validation.py"", line 1074, in check_x_y
    x = check_array(
  file ""c:\users\hp\pycharmprojects\pythonproject\venv\lib\site-packages\sklearn\utils\validation.py"", line 856, in check_array
    array = np.asarray(array, order=order, dtype=dtype)
  file ""c:\users\hp\pycharmprojects\pythonproject\venv\lib\site-packages\pandas\core\generic.py"", line 2064, in __array__
    return np.asarray(self._values, dtype=dtype)
valueerror: setting an array element with a sequence.

how to fix that?","['python', 'machine-learning', 'scikit-learn', 'nlp', 'logistic-regression']",73866516,"you need to do a label encoding before the training and convert string values to make them understandable for machine.
refer to",https://stackoverflow.com/questions/73866404,python,27-09-2022 10:52,81.0,0.0,1.0,True,27-09-2022 13:02,27-09-2022 13:02
15869147,converting natural language to a math equation,"i've got a home automation system working in java, and i want to add simple math capabilities such as addition, subtraction, multiplication, division, roots, and powers.
at the system current state, it can convert a phrase into tags, as shown in the following examples:
example:
phrase: ""what is one hundred twenty two to the power of seven""
tagged: {question/math} {number/122} {math/pwr} {number/7}

example:
phrase: ""twenty seven plus pi 3 squared""
tagged: {number/27} {math/add} {number/3.14159} {math/multiply} {math/pwr} {number/2}

this example could be just as easily converted to something like this:
27 + 3.14159 * 3^2

each tag is an object that can be queried for it value.
edit: specific question:
so now i need a way to read that group of tags as an equation, and return a numerical result. as a last resort i could use google or wolfram alpha, but that will be slower, and i'm trying to keep the automation system completely self contained.
if you would like to see the entire source, here it is in github.
note that i have not committed the last few few changes, so some of the math related things i gave examples will not work.","['java', 'math', 'nlp']",15869332,"after some more googleing (i didn't know the name for what i was doing at first) i found someone who has done something similar already:

edit: finished:",https://stackoverflow.com/questions/15869147,java,07-04-2013 23:36,2950.0,5.0,4.0,True,07-10-2022 13:52,08-04-2013 00:33
75737187,how to create a column as a list of similar strings onto a new column?,"i've been trying to get a new row in a pandas dataframe which encapsullates as a list all the similar strings into it's original matching row.
this is the original pandas dataframe:
import pandas as pd

d = {'product_name': ['2 pack liner socks', '2 pack logo liner socks', 'b.bare hipster', 'lady bare hipster panty'], 'id': [13, 12, 11, 10]}
df = pd.dataframe(data=d)

i would like to get a dataframe that looks like this:
# product_name                  # id          # group
  2 pack liner socks             13           ['2 pack liner socks', '2 pack logo liner socks']
  2 pack logo liner socks        12           ['2 pack liner socks', '2 pack logo liner socks']
  b.bare hipster                 11           ['b.bare hipster', 'lady bare hipster panty']
  lady bare hipster panty        10           ['b.bare hipster', 'lady bare hipster panty']

i tried the following:
import thefuzz
from thefuzz import process


df[""group""] = df[""product_name""].apply(lambda x: process.extractone(x, df[""product_name""], scorer=fuzz.partial_ratio)[0])

and it throws the next error:

nameerror: name 'fuzz' is not defined

how could i fix this code or on the other hand are there any other approaches to solve this?","['python', 'pandas', 'string', 'dataframe', 'nlp']",75737315,"you need to import fuzz - from thefuzz import process, fuzz but using process.extractone with a list of all values in the product_name will always return the actual value of that row because it is a 100% match so let's filter that out by doing df[""product_name""].loc[df['product_name'] != x]
from thefuzz import process, fuzz


df['group'] = df[""product_name""].apply(lambda x: sorted([x, process.extractone(x, df[""product_name""].loc[df['product_name'] != x],
                                                                               scorer=fuzz.partial_ratio)[0]]))

              product_name  id                                          group
0       2 pack liner socks  13  [2 pack liner socks, 2 pack logo liner socks]
1  2 pack logo liner socks  12  [2 pack liner socks, 2 pack logo liner socks]
2           b.bare hipster  11      [lady bare hipster panty, b.bare hipster]
3  lady bare hipster panty  10      [lady bare hipster panty, b.bare hipster]",https://stackoverflow.com/questions/75737187,python,14-03-2023 18:43,109.0,1.0,1.0,True,14-03-2023 18:57,14-03-2023 18:51
71318065,how to understand loss-learning rate (log scale) plot using learner.lr_plot in ktrain package?,"i am using ktrain package to classify text. my experiment is shown as:

lr_find and lr_plot are functions in ktrain. they can be used to highlight the best learning rate, which is shown as the red dot in the plot.
i do not understand how to understand this plot:

how to transfer log scale to the normal linear one?
why the best scale is the red dot?","['plot', 'nlp', 'loss', 'learning-rate', 'ktrain']",71388842,"as the text from the lr_find method says, you can visually inspect the plot and choose a learning rate in a range where the loss is falling prior to divergence.  a higher learning rate in this range will converge faster.  this is an idea called an ""lr range test"" from leslie smith's paper that became popular through the fastai library and was later adopted by other libraries like ktrain and amazon's gluon library.   the red dot in this plot is just a numerical approximation of where the loss is dramatically falling that may be useful for automated scenarios, but not necessarily the best.   in this plot, the red dot represents the steepest part of the curve, which is one strategy to automatically select a learning rate from the plot (without visual inspection).  other automated strategies include taking the learning rate associated with the minimum loss and dividing by 10, and finding the learning rate associated with the longest valley.",https://stackoverflow.com/questions/71318065,plot,02-03-2022 05:58,609.0,0.0,1.0,True,11-03-2022 22:21,11-03-2022 22:21
79479435,react with openai streaming results in duplicate values when updating and showing the response,"i'm trying to stream the response from the openai streamed response and display the response to the user in a chat interface. however, while each chunk of the response itself is correct my state ends up with duplicated words... where am i going wrong?
here is my code along with screenshots showing both the ui and console output:
useeffect(() => {
  const docompletion = async () => {
    if (!newmessage) return;
      
    try {
      await handlecompletion(messages);
      setnewmessage(false);
    } catch (err) {
      console.log(""unable to communicate with model service: "", err)
    }
  }

  docompletion();
}, [newmessage]);

const handlenewmessage = async (formdata: formdata) => {
  const message = formdata.get(""message"") as string;
  addmessage({id: uuidv4(), data: {role: ""user"", content: message}});
  setnewmessage(true);
}

const handlecompletion = async (messages: message[]) => {
  const formattedmessages = messages.map((message: message) => message.data)

  const stream = await openai.chat.completions.create({
    messages: formattedmessages,
    model: ""deepseek-chat"",
    stream: true
  });
    
  for await (const chunk of stream) {
    const content = chunk.choices[0]?.delta?.content;
    console.log(""content: "", content)
      
    setmessages((prev) => {
      let tmpmessages = [...prev];
      const lastmessageindex = tmpmessages.length-1;
      if (tmpmessages[lastmessageindex].data.role !== ""assistant"") {
        tmpmessages.push({id: chunk.id, data: {role: ""assistant"", content: """"}});
        return tmpmessages;
      }

      const prevcontent = tmpmessages[lastmessageindex].data.content;
      console.log(""prevcontent: "", prevcontent)
      
      const newcontent = prevcontent! + content!;
      console.log(""newcontent: "", newcontent)
        
      tmpmessages[lastmessageindex].data.content = newcontent;
      return tmpmessages;
    });

  }
}","['reactjs', 'openai-api']",79479453,"let's try to fix it!
your problem is that when streaming openai responses in react, rapid chunk updates can lead to stale state, causing duplicated words in the displayed chat.
my suggestion is to use a useref to store the latest cumulative content from the openai stream. this ensures state updates are based on the most current data.
import react, { usestate, useeffect, useref } from 'react';

const yourcomponent = () => {
  const [messages, setmessages] = usestate([]);
  const assistantcontentref = useref('');

  const handlecompletion = async (messages) => {
    // openai stream setup

    assistantcontentref.current = ''; // reset ref

    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content;
      if(content){
        assistantcontentref.current += content;

        setmessages((prev) => {
          // update messages using assistantcontentref.current
          let tmpmessages = [...prev];
          const lastmessageindex = tmpmessages.length - 1;

          if (tmpmessages[lastmessageindex]?.data.role !== 'assistant') {
            tmpmessages.push({
              id: chunk.id,
              data: { role: 'assistant', content: assistantcontentref.current },
            });
          } else {
            tmpmessages[lastmessageindex].data.content = assistantcontentref.current;
          }

          return tmpmessages;
        });
      }
    }
  };

  // rest of your code 
};

export default yourcomponent;

this approach guarantees that your react state always reflects the most recent ai response, preventing duplication. :)",https://stackoverflow.com/questions/79479435,reactjs,02-03-2025 17:03,94.0,0.0,1.0,True,02-03-2025 20:07,02-03-2025 20:07
76313592,import langchain =&gt; error : typeerror: issubclass() arg 1 must be a class,"i want to use langchain for my project.
so i installed it using following command : pip install langchain
but while importing ""langchain"" i am facing following error:
file /usr/lib/python3.8/typing.py:774, in _genericalias.__subclasscheck__(self, cls)
    772 if self._special:
    773     if not isinstance(cls, _genericalias):
--> 774         return issubclass(cls, self.__origin__)
    775     if cls._special:
    776         return issubclass(cls.__origin__, self.__origin__)

typeerror: issubclass() arg 1 must be a class

any one who can solve this error ?","['python', 'nlp', 'data-science', 'chatbot', 'langchain']",76314471,"typing-inspect==0.8.0
typing_extensions==4.5.0",https://stackoverflow.com/questions/76313592,python,23-05-2023 10:09,32787.0,37.0,7.0,True,06-02-2024 06:33,23-05-2023 12:47
77367603,how do i detach the huggingface sagemaker training?,"i am training a huggingface model remotely on sagemaker integration. my training job takes more than two hours, and i would like to shut my computer off during training.
i use the following snippet to train the model:
huggingface_estimator.fit(
  {
    'train': 's3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/train',
    'test': 's3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/test'
  }
)

how can i set up the trainer so that it the training process runs in the background so i can shut my computer down?","['amazon-web-services', 'huggingface-transformers', 'amazon-sagemaker', 'huggingface', 'aws-batch']",77370321,"how can i set up the trainer so that it the training process runs in the background so i can shut my computer down?

you can!
calling fit on a hugging face estimator starts the training job. once a training job is started on sagemaker, it runs independently of your local machine.
you can shutdown your machine & the job will continue to run on sagemaker until itï¿½ï¿½ï¿½s finished",https://stackoverflow.com/questions/77367603,amazon-web-services,26-10-2023 13:52,80.0,1.0,1.0,True,26-10-2023 21:23,26-10-2023 21:23
67102278,"expected ndim=3, found ndim=4. when using k.function() in keras backend to get intermediate layers in a model","i am trying to extract the last layer of a classification model trained on some data. the first layer is an embedding layer, followed by the bilstm and the followed by the output dense layer. my code is sown below. i keep getting a 4d output (1,38,300,300) instead of a 3d (1,38,300). 1 is the sample size, 38 is the max length of the sentence, and 300 is the word2vec length.
from keras import backend as k
from tensorflow.keras.models import load_model
import numpy as np
import gensim
word2vec = 'googlenews-vectors-negative300.txt'


x_matrix = np.zeros((1, 38, 300))
sentene_label = 'the weather today was extremely unpredictable,0'
parts = sentene_label.split(',')
label = int(parts[1])  
sentence = parts[0] 

words = sentence.split(' ')
words = words[:x_matrix.shape[1]]  
for j, word in enumerate(words):
    if word in word2vec:
        # x_matrix[0, j, :] = word2vec[word]
        x_matrix[0, j, :] = loaded_model.word_vec(word)


model = load_model('trainedmodel.h5')
get_3rd_layer_output = k.function([model.layers[0].input], [model.layers[2].output])  
layer_output = get_3rd_layer_output(x_matrix)[0]
print(""layer output shape 1 : "", layer_output.shape)

i have cross-checked my code several times and i can't seem to figure out why the dimensions are wrong.
this is the traceback
traceback (most recent call last):
  file ""/usr/pkg/lib/python3.8/site-packages/ipython/core/interactiveshell.py"", line 3427, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  file ""<ipython-input-2-bb840b495480>"", line 1, in <module>
    runfile('/am/vuwstocoisnrin1.vuw.ac.nz/ecrg-solar/kosimadukwe/data augmentation/test.py', wdir='/am/vuwstocoisnrin1.vuw.ac.nz/ecrg-solar/kosimadukwe/data augmentation')
  file ""/am/embassy/vol/x6/jetbrains/apps/pycharm-p/ch-0/201.7846.77/plugins/python/helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  file ""/am/embassy/vol/x6/jetbrains/apps/pycharm-p/ch-0/201.7846.77/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  file ""/am/vuwstocoisnrin1.vuw.ac.nz/ecrg-solar/kosimadukwe/data augmentation/test.py"", line 451, in <module>
    layer_output = get_3rd_layer_output(x_matrix)[0]
  file ""/usr/pkg/lib/python3.8/site-packages/tensorflow/python/keras/backend.py"", line 4073, in func
    outs = model(model_inputs)
  file ""/usr/pkg/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1012, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  file ""/usr/pkg/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 424, in call
    return self._run_internal_graph(
  file ""/usr/pkg/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 560, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)
  file ""/usr/pkg/lib/python3.8/site-packages/tensorflow/python/keras/layers/wrappers.py"", line 539, in __call__
    return super(bidirectional, self).__call__(inputs, **kwargs)
  file ""/usr/pkg/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 998, in __call__
    input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)
  file ""/usr/pkg/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py"", line 219, in assert_input_compatibility
    raise valueerror('input ' + str(input_index) + ' of layer ' +
valueerror: input 0 of layer bidirectional_9 is incompatible with the layer: expected ndim=3, found ndim=4. full shape received: (1, 38, 300, 300)

the error is triggered on line
layer_output = get_3rd_layer_output(x_matrix)[0]

the shape of x_matrix before calling get_3rd_layer_output is
the shape of x matrix : (60, 38, 300)


trainedmodels architecture
model = sequential()
model.add(embedding(vocab_size, 300, input_length=38, weights=[embedding_matrix], trainable=true))
model.add(bidirectional(lstm(100, dropout=0.2)))
model.add(dense(3, activation='sigmoid'))
model.compile(loss='sparse_categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])
model.summary()

es = earlystopping(monitor='val_loss', mode='min', baseline=0.3, patience=100, verbose=1)
mc = modelcheckpoint('trainedmodel.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=true)
hist = model.fit(train_sequences, train_y, epochs=200, verbose=false, batch_size=100,validation_data=(val_sequences, val_y),callbacks=[es, mc]) 


trainedmodels model.summary is
_________________________________________________________________
layer (type)                 output shape              param #   
=================================================================
embedding_9 (embedding)      (none, 38, 300)           7370400   
_________________________________________________________________
bidirectional_9 (bidirection (none, 200)               320800    
_________________________________________________________________
dense_9 (dense)              (none, 3)                 603       
=================================================================
total params: 7,691,803
trainable params: 7,691,803
non-trainable params: 0
_________________________________________________________________","['keras', 'nlp', 'text-classification', 'keras-layer']",67226013,"the correct way to get any intermediate layer output is to create a sub-model that expects the same input of your trained model. in your case, the error raises because you pass to your trained model the 3d embedding matrix while you have to pass the same data you use for training (2d data whit integer-encoded words).
here i produce a dummy example to extract correctly any intermediate output from your model.
create dummy data:
vocab_size = 111
emb_size = 300
input_length = 38
n_sample = 50
n_classes = 3

embedding_matrix = np.random.uniform(-1,1, (vocab_size, emb_size))
x = np.random.randint(0,vocab_size, (n_sample, input_length))
y = np.random.randint(0,n_classes, (n_sample,))

create model and train:
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import *
from tensorflow.keras import backend as k

model = sequential()
model.add(embedding(vocab_size, emb_size, input_length=input_length, 
                    weights=[embedding_matrix], trainable=true))
model.add(bidirectional(lstm(100, dropout=0.2)))
model.add(dense(n_classes, activation='sigmoid'))
model.compile(loss='sparse_categorical_crossentropy', 
              optimizer='adagrad', metrics=['accuracy'])
model.fit(x,y, epochs=3)  ### trained with x

get layer output:
layer_id = 2
get_layer_output = k.function([model.layers[0].input], [model.layers[layer_id].output])
layer_output = get_layer_output(x)[0]  ### extract from x
# equal to:
# sub_model = model(model.input, model.layers[layer_id].output)
# layer_output = sub_model.predict(x)  ### extract from x",https://stackoverflow.com/questions/67102278,keras,15-04-2021 04:27,930.0,1.0,1.0,True,30-03-2022 14:21,20-04-2021 23:12
77549252,excel in large-scale data processing with gpt-3.5 and embeddings,"i'm working on integrating openai functionalities, specifically gpt3.5 and embeddings, into a large system of excel workbooks used for almost anything in my office. our goal is having gpt3.5 taking over the place of a set of macros that has become hell maintaining, and the embeddings-ada-002 for improving the searches and categorization of data.
however, i'm concerned of excel being capable to handle hundreds of these high-dimensional embeddings. i'm also afraid that excel and gpt3.5 will struggle when having multiple sheets of thousands of rows tall and ""complex structures"" provided as a context.
(we run windows on i5 processors)
will excel be able to handle such large-scale data processing with ai models? any insights on this matter?","['excel', 'vba', 'openai-api', 'data-processing', 'gpt-3']",77678776,"excel is awesome but it has its limitations when it comes to handling large volumes of data. it is designed for tabular data and it will struggle with the high-dimensional data source.
also gpt-3.5  has limitations of the number of tokens it can handle source. if your excel sheets is data heavy, gpt-3.5 will struggle.
a more scalable solution could involve using python. you could use python with pandas for data manipulation and pyopenai to read data from excel workbooks, process it using the openai models, and then write the results back into excel source.
here's a example:
import pandas as pd
from openai import openai, gpt3completion

# load your data from excel into a pandas dataframe
df = pd.read_excel('your_data.xlsx')

# initialize the openai api with your api key
openai = openai(api_key='your_api_key')

# process each row of your data using gpt3.5
for index, row in df.iterrows():
    prompt = row['your_column']
    response = openai.gpt3completion.create(prompt=prompt, model='gpt-3.5-turbo')
    # save logic",https://stackoverflow.com/questions/77549252,excel,25-11-2023 19:07,1518.0,-1.0,1.0,True,18-12-2023 11:29,28-11-2023 17:23
76748279,changing the default cache path for all huggingface data,"the default cache path of huggingface is in  ~/.cache/huggingface, and in that folder, there are multiple cache files like models, and hub.
the huggingface documents indicates that the default dataset cache location can be modified by setting the shell environment variable, hf_datasets_cache to a different directory as shown below:
$ export hf_datasets_cache=""/path/to/another/directory""

however, my objective is to alter the default cache directory for all huggingface data and not solely the dataset. i am facing difficulties in finding the respective shell environment variable in the huggingface documentation to accomplish this. any help would be appreciated.","['python', 'nlp', 'huggingface']",76748390,"it seems that the variable is hf_home as this document indicates. so probably this terminal code should be the solution:
export hf_home=""/path/.cache/huggingface""
export hf_datasets_cache=""/path/.cache/huggingface/datasets""
export transformers_cache=""/path/.cache/huggingface/models""

p.s. if you want to make your change permanent, you should write these lines in your .bashrc(can do with .bash_profile too) file using nano ~/.bashrc",https://stackoverflow.com/questions/76748279,python,23-07-2023 12:38,9182.0,4.0,2.0,True,12-12-2023 22:43,23-07-2023 12:55
75873494,how to generate diagram from openai create image that has english words?,"by using nodejs openai npm package, i am generating images also by adding string ' can you give a diagram?"" at the end i am getting diagram but every diagram has some meaningless words anyone can tell me why?
const { configuration, openaiapi } = require(""openai"");
    
        const configuration = new configuration({
          apikey: apikey,
        });
        const openai = new openaiapi(configuration);
        try {
          const response = await openai.createimage({
            prompt:
              query +
              "" , can you give a diagram? "",
            n: 2,
            size: ""256x256"",
          });","['node.js', 'openai-api']",76770243,"for prompt, you show that the input is query and then can you give me a diagram
it would be better if you can explain it explicitly, maybe reorder the sentence, so it would be totally clear what you want to create, and for sure the clearer the better.
after all, it is based on llm, so it depends on how you specify your input.",https://stackoverflow.com/questions/75873494,node.js,29-03-2023 05:38,299.0,0.0,1.0,True,26-07-2023 10:19,29-03-2023 06:08
44292616,only get tokenized sentences as output from stanford core nlp,"i need to split sentences. i'm using the pycorenlp wrapper for python3. i've started the server from my jar directory using: java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.stanfordcorenlpserver -port 9000 -timeout 15000
i've run the following commands:
from pycorenlp import stanfordcorenlp
nlp = stanfordcorenlp('    
text = 'pusheen and smitha walked along the beach. pusheen wanted to surf, but fell off the surfboard.'
output = nlp.annotate(text, properties={'annotators': 'tokenize,ssplit', 'outputformat': 'text'})
print (output)

which gave the following output:
sentence #1 (8 tokens):
pusheen and smitha walked along the beach.
[text=pusheen characteroffsetbegin=0 characteroffsetend=7]
[text=and characteroffsetbegin=8 characteroffsetend=11]
[text=smitha characteroffsetbegin=12 characteroffsetend=18]
[text=walked characteroffsetbegin=19 characteroffsetend=25]
[text=along characteroffsetbegin=26 characteroffsetend=31]
[text=the characteroffsetbegin=32 characteroffsetend=35]
[text=beach characteroffsetbegin=36 characteroffsetend=41]
[text=. characteroffsetbegin=41 characteroffsetend=42]
sentence #2 (11 tokens):
pusheen wanted to surf, but fell off the surfboard.
[text=pusheen characteroffsetbegin=43 characteroffsetend=50]
[text=wanted characteroffsetbegin=51 characteroffsetend=57]
[text=to characteroffsetbegin=58 characteroffsetend=60]
[text=surf characteroffsetbegin=61 characteroffsetend=65]
[text=, characteroffsetbegin=65 characteroffsetend=66]
[text=but characteroffsetbegin=67 characteroffsetend=70]
[text=fell characteroffsetbegin=71 characteroffsetend=75]
[text=off characteroffsetbegin=76 characteroffsetend=79]
[text=the characteroffsetbegin=80 characteroffsetend=83]
[text=surfboard characteroffsetbegin=84 characteroffsetend=93]
[text=. characteroffsetbegin=93 characteroffsetend=94]

i need the output in the following format:
pusheen and smitha walked along the beach.
pusheen wanted to surf, but fell off the surfboard.","['python', 'nlp', 'stanford-nlp', 'tokenize']",44297013,"try the new ""shiny"" stanford corenlp api in nltk =)
first:
pip install -u nltk[corenlp]

on command-line: 
java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.stanfordcorenlpserver -port 9000 -timeout 15000

then in python, the standard usage is:
>>> from nltk.parse.corenlp import corenlpparser
>>> stanford = corenlpparser('
>>> text = 'pusheen and smitha walked along the beach. pusheen wanted to surf, but fell off the surfboard.'

# gets you the tokens.
>>> ' '.join(next(stanford.raw_parse(text)).leaves())
u'pusheen and smitha walked along the beach . pusheen wanted to surf , but fell off the surfboard .'

# gets you the tree object.
>>> next(stanford.raw_parse(text))
tree('root', [tree('s', [tree('s', [tree('np', [tree('nnp', ['pusheen']), tree('cc', ['and']), tree('nnp', ['smitha'])]), tree('vp', [tree('vbd', ['walked']), tree('pp', [tree('in', ['along']), tree('np', [tree('dt', ['the']), tree('nn', ['beach'])])])]), tree('.', ['.'])]), tree('np', [tree('nnp', ['pusheen'])]), tree('vp', [tree('vp', [tree('vbd', ['wanted']), tree('pp', [tree('to', ['to']), tree('np', [tree('nn', ['surf'])])])]), tree(',', [',']), tree('cc', ['but']), tree('vp', [tree('vbd', ['fell']), tree('prt', [tree('rp', ['off'])]), tree('np', [tree('dt', ['the']), tree('nn', ['surfboard'])])])]), tree('.', ['.'])])])

# gets you the pretty png tree.
>>> next(stanford.raw_parse(text)).draw()

[out]:


to get the tokenized sentence, you'll need some finesse:
>>> from nltk.parse.corenlp import corenlpparser
>>> stanford = corenlpparser('

# using the corenlpparser.api_call() function, ...
>>> stanford.api_call
<bound method corenlpparser.api_call of <nltk.parse.corenlp.corenlpparser object at 0x107131b90>>

# ... , you can get the json output from the corenlp tool.
>>> stanford.api_call(text, properties={'annotators': 'tokenize,ssplit'})
{u'sentences': [{u'tokens': [{u'index': 1, u'word': u'pusheen', u'after': u' ', u'characteroffsetend': 7, u'characteroffsetbegin': 0, u'originaltext': u'pusheen', u'before': u''}, {u'index': 2, u'word': u'and', u'after': u' ', u'characteroffsetend': 11, u'characteroffsetbegin': 8, u'originaltext': u'and', u'before': u' '}, {u'index': 3, u'word': u'smitha', u'after': u' ', u'characteroffsetend': 18, u'characteroffsetbegin': 12, u'originaltext': u'smitha', u'before': u' '}, {u'index': 4, u'word': u'walked', u'after': u' ', u'characteroffsetend': 25, u'characteroffsetbegin': 19, u'originaltext': u'walked', u'before': u' '}, {u'index': 5, u'word': u'along', u'after': u' ', u'characteroffsetend': 31, u'characteroffsetbegin': 26, u'originaltext': u'along', u'before': u' '}, {u'index': 6, u'word': u'the', u'after': u' ', u'characteroffsetend': 35, u'characteroffsetbegin': 32, u'originaltext': u'the', u'before': u' '}, {u'index': 7, u'word': u'beach', u'after': u'', u'characteroffsetend': 41, u'characteroffsetbegin': 36, u'originaltext': u'beach', u'before': u' '}, {u'index': 8, u'word': u'.', u'after': u' ', u'characteroffsetend': 42, u'characteroffsetbegin': 41, u'originaltext': u'.', u'before': u''}], u'index': 0}, {u'tokens': [{u'index': 1, u'word': u'pusheen', u'after': u' ', u'characteroffsetend': 50, u'characteroffsetbegin': 43, u'originaltext': u'pusheen', u'before': u' '}, {u'index': 2, u'word': u'wanted', u'after': u' ', u'characteroffsetend': 57, u'characteroffsetbegin': 51, u'originaltext': u'wanted', u'before': u' '}, {u'index': 3, u'word': u'to', u'after': u' ', u'characteroffsetend': 60, u'characteroffsetbegin': 58, u'originaltext': u'to', u'before': u' '}, {u'index': 4, u'word': u'surf', u'after': u'', u'characteroffsetend': 65, u'characteroffsetbegin': 61, u'originaltext': u'surf', u'before': u' '}, {u'index': 5, u'word': u',', u'after': u' ', u'characteroffsetend': 66, u'characteroffsetbegin': 65, u'originaltext': u',', u'before': u''}, {u'index': 6, u'word': u'but', u'after': u' ', u'characteroffsetend': 70, u'characteroffsetbegin': 67, u'originaltext': u'but', u'before': u' '}, {u'index': 7, u'word': u'fell', u'after': u' ', u'characteroffsetend': 75, u'characteroffsetbegin': 71, u'originaltext': u'fell', u'before': u' '}, {u'index': 8, u'word': u'off', u'after': u' ', u'characteroffsetend': 79, u'characteroffsetbegin': 76, u'originaltext': u'off', u'before': u' '}, {u'index': 9, u'word': u'the', u'after': u' ', u'characteroffsetend': 83, u'characteroffsetbegin': 80, u'originaltext': u'the', u'before': u' '}, {u'index': 10, u'word': u'surfboard', u'after': u'', u'characteroffsetend': 93, u'characteroffsetbegin': 84, u'originaltext': u'surfboard', u'before': u' '}, {u'index': 11, u'word': u'.', u'after': u'', u'characteroffsetend': 94, u'characteroffsetbegin': 93, u'originaltext': u'.', u'before': u''}], u'index': 1}]} 

>>> output_json = stanford.api_call(text, properties={'annotators': 'tokenize,ssplit'})
>>> len(output_json['sentences'])
2
>>> for sent in output_json['sentences']:
...     start_offset = sent['tokens'][0]['characteroffsetbegin'] # begin offset of first token.
...     end_offset = sent['tokens'][-1]['characteroffsetend'] # end offset of last token.
...     sent_str = text[start_offset:end_offset]
...     print sent_str
... 
pusheen and smitha walked along the beach.
pusheen wanted to surf, but fell off the surfboard.",https://stackoverflow.com/questions/44292616,python,31-05-2017 18:52,2263.0,3.0,3.0,True,25-05-2023 01:08,06-01-2023 06:37
74073113,output tensors of a functional model must be the output of a tensorflow `layer`,"so i'm trying to expand the roberta pretrained model and i was doing a basic model for testing but i'm getting this error from tensorflow: valueerror: output tensors of a functional model must be the output of a tensorflow layer. which is from the model api of keras but i don't exactly know what's causing it.
code:
len_seq = 64
batch_size = 16
test_train_split = 0.9
transformer = 'roberta-base'

df = pd.read_csv('train-processed.csv')
df = df.head(100)
samples_count = len(df)

# create labels
target = df['first_sentiment'].values.astype(int)
labels = np.zeros((samples_count, target.max() + 1))
labels[np.arange(samples_count), target] = 1

tokenizer = autotokenizer.from_pretrained(transformer)
tokens = tokenizer(
    df['first_phrase'].tolist(),
    max_length=len_seq,
    truncation=true,
    padding='max_length',
    add_special_tokens=true,
    return_tensors='tf'
)

base_model = tfautomodel.from_pretrained(transformer)

embedding = base_model.roberta(input_ids=tokens['input_ids'], attention_mask=tokens['attention_mask'])
embedding.trainable = false

# define inputs
input_ids = input(shape=(len_seq,), name='input_ids', dtype='int32')
input_mask = input(shape=(len_seq,), name='input_mask', dtype='int32')

# define hidden layers
layer = dense(len_seq * 2, activation='relu')(embedding[1])
layer = dense(len_seq, activation='relu')(layer)

# define output
output = dense(target.max() + 1, activation='softmax', name='output')(layer)

model = model(inputs=[input_ids, input_mask], outputs=[output])

full error traceback:
---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
<ipython-input-80-9a6ccb1b4ca8> in <module>
     10 output = dense(target.max() + 1, activation='softmax', name='output')(layer)
     11 
---> 12 model = model(inputs=[input_ids, input_mask], outputs=[output])
     13 
     14 model.compile(

/usr/local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    528     self._self_setattr_tracking = false  # pylint: disable=protected-access
    529     try:
--> 530       result = method(self, *args, **kwargs)
    531     finally:
    532       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.8/site-packages/keras/engine/functional.py in __init__(self, inputs, outputs, name, trainable, **kwargs)
    107     generic_utils.validate_kwargs(kwargs, {})
    108     super(functional, self).__init__(name=name, trainable=trainable)
--> 109     self._init_graph_network(inputs, outputs)
    110 
    111   @tf.__internal__.tracking.no_automatic_dependency_tracking

/usr/local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    528     self._self_setattr_tracking = false  # pylint: disable=protected-access
    529     try:
--> 530       result = method(self, *args, **kwargs)
    531     finally:
    532       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.8/site-packages/keras/engine/functional.py in _init_graph_network(self, inputs, outputs)
    144         base_layer_utils.create_keras_history(self._nested_outputs)
    145 
--> 146     self._validate_graph_inputs_and_outputs()
    147 
    148     # a network does not create weights of its own, thus it is already

/usr/local/lib/python3.8/site-packages/keras/engine/functional.py in _validate_graph_inputs_and_outputs(self)
    719       if not hasattr(x, '_keras_history'):
    720         cls_name = self.__class__.__name__
--> 721         raise valueerror('output tensors of a ' + cls_name + ' model must be '
    722                          'the output of a tensorflow `layer` '
    723                          '(thus holding past layer metadata). found: ' + str(x))

valueerror: output tensors of a functional model must be the output of a tensorflow `layer` (thus holding past layer metadata). found: tf.tensor(
[[0.18333092 0.18954797 0.22477032 0.21039596 0.1919548 ]
 [0.18219706 0.1903447  0.2256843  0.20587942 0.19589448]
 [0.18239683 0.1907878  0.22491893 0.20824413 0.19365236]
 [0.18193942 0.1898969  0.2259874  0.20646562 0.1957107 ]
 [0.18132213 0.1893883  0.22565623 0.21005587 0.1935775 ]
 [0.18237704 0.18789911 0.22692119 0.20759228 0.1952104 ]
 [0.18217668 0.18732095 0.22601548 0.21063867 0.19384828]
 [0.1817196  0.18970788 0.22175607 0.21405536 0.19276106]
 [0.18154216 0.18738106 0.22770867 0.2091297  0.19423842]
 [0.1839993  0.19110405 0.2193769  0.2124882  0.19303149]
 [0.18009834 0.19029345 0.22552258 0.21138497 0.19270067]
 [0.18262982 0.18932794 0.22548872 0.20995376 0.19259974]
 [0.18132062 0.1894746  0.22257458 0.21089785 0.19573237]
 [0.18127124 0.18927224 0.2275273  0.2061446  0.19578464]
 [0.18001163 0.1883382  0.22915907 0.20934238 0.19314879]
 [0.18409619 0.19204247 0.22269006 0.20967877 0.19149245]
 [0.18143429 0.18780865 0.22895294 0.21044146 0.1913626 ]
 [0.18210162 0.18980804 0.22135185 0.21205473 0.1946838 ]
 [0.18077913 0.18933856 0.22730026 0.2079047  0.19467732]
 [0.18248595 0.19133545 0.2252994  0.20402898 0.1968502 ]
 [0.18053354 0.18830904 0.22379933 0.21369977 0.19365832]
 [0.18100418 0.1889128  0.22656825 0.21134934 0.19216539]
 [0.18219638 0.18901002 0.22543809 0.20894748 0.194408  ]
 [0.17991781 0.18693839 0.23250549 0.21227528 0.18836297]
 [0.18322821 0.1881207  0.22497904 0.20976694 0.19390512]
 [0.17972894 0.18888594 0.2251662  0.21268585 0.19353302]
 [0.1822505  0.18769115 0.22729188 0.21127912 0.19148737]
 [0.18432644 0.18830952 0.22477935 0.20987424 0.19271052]
 [0.1801894  0.18920776 0.22684936 0.20734173 0.19641179]
 [0.181594   0.1880084  0.22798598 0.20937674 0.19303486]
 [0.18252885 0.19045824 0.22497422 0.207161   0.19487773]
 [0.18196142 0.18878765 0.22479571 0.2105628  0.19389246]
 [0.18600896 0.18686578 0.2283819  0.21188499 0.18685843]
 [0.18056509 0.18865508 0.22694935 0.21080662 0.19302382]
 [0.18446274 0.1887065  0.22405164 0.21271324 0.19006592]
 [0.1812612  0.18995184 0.22384171 0.20790772 0.19703752]
 [0.1861402  0.189157   0.2236694  0.21078445 0.19024895]
 [0.18149142 0.18862149 0.2255336  0.20888737 0.19546609]
 [0.18088317 0.1882689  0.22780944 0.20749897 0.19553955]
 [0.1824722  0.18926203 0.22691077 0.2071967  0.1941583 ]
 [0.18111941 0.18773855 0.22366299 0.21535842 0.19212064]
 [0.18248987 0.18920848 0.22602491 0.20733926 0.19493747]
 [0.18306294 0.19167435 0.22505572 0.21000686 0.19020009]
 [0.18466519 0.1885763  0.22352514 0.21257839 0.19065501]
 [0.18297954 0.18976018 0.2262897  0.20864752 0.19232307]
 [0.18216778 0.18953851 0.22490299 0.21057723 0.1928135 ]
 [0.18181367 0.19077264 0.2232015  0.21115994 0.1930523 ]
 [0.18345618 0.18753015 0.22660162 0.20830849 0.1941036 ]
 [0.18212378 0.18797131 0.2247642  0.21066691 0.19447377]
 [0.18199605 0.19106121 0.22245005 0.21217921 0.19231346]
 [0.18243583 0.18764758 0.22628336 0.21369886 0.18993443]
 [0.18162242 0.18957089 0.22591078 0.20930369 0.19359224]
 [0.18090473 0.18757755 0.22858356 0.20813066 0.19480348]
 [0.17951688 0.18841572 0.22520997 0.21235934 0.19449812]
 [0.1850496  0.18895829 0.22575855 0.20854111 0.1916925 ]
 [0.18254244 0.18938984 0.22754729 0.20879866 0.19172177]
 [0.1816532  0.18972425 0.22676478 0.20679341 0.19506434]
 [0.18303266 0.19159187 0.22373216 0.20538329 0.19625996]
 [0.18126963 0.18750906 0.2258774  0.21198079 0.1933631 ]
 [0.18387978 0.18828613 0.22228165 0.21189795 0.19365448]
 [0.1834729  0.18976368 0.22469373 0.20830937 0.19376035]
 [0.18359789 0.18833868 0.22379532 0.21078889 0.19347927]
 [0.18039297 0.18886234 0.22411437 0.2105467  0.19608359]
 [0.17980678 0.18979622 0.2266618  0.20471531 0.19901991]
 [0.18554561 0.19003332 0.22477089 0.21021138 0.18943883]
 [0.18349187 0.18941568 0.22224301 0.21004184 0.19480757]
 [0.18351436 0.19169463 0.22155108 0.21009424 0.19314568]
 [0.18123321 0.18985166 0.22660086 0.21186577 0.19044854]
 [0.18183744 0.192495   0.22091088 0.21275932 0.1919973 ]
 [0.18028514 0.18943599 0.22416686 0.21241388 0.19369814]
 [0.18061554 0.18873625 0.22677769 0.21073307 0.19313747]
 [0.18186866 0.18851075 0.22588421 0.21183755 0.19189876]
 [0.18126652 0.18949142 0.22501452 0.20897155 0.19525598]
 [0.1835434  0.19079022 0.22333461 0.21146008 0.19087164]
 [0.18269798 0.19171126 0.22150221 0.21224435 0.19184415]
 [0.17996274 0.19000672 0.22470033 0.2105299  0.19480029]
 [0.18345153 0.19032337 0.2239142  0.21167503 0.19063583]
 [0.18224017 0.19025423 0.22567508 0.2087501  0.19308044]
 [0.18233515 0.18966553 0.22833474 0.20635706 0.1933075 ]
 [0.18210347 0.18650064 0.22770585 0.21101129 0.19267873]
 [0.18199693 0.19086935 0.22255068 0.20988034 0.19470267]
 [0.18119748 0.18983872 0.22518982 0.20845842 0.19531558]
 [0.18367417 0.19071157 0.22310348 0.21277103 0.18973975]
 [0.17965038 0.18936628 0.22479466 0.21279414 0.19339451]
 [0.18141513 0.18989322 0.22380653 0.21031635 0.19456872]
 [0.18295668 0.19067182 0.22385122 0.20624346 0.1962768 ]
 [0.17981796 0.18981294 0.22544417 0.21043345 0.19449154]
 [0.18068986 0.1897383  0.22433658 0.21027999 0.1949553 ]
 [0.18146665 0.18844193 0.22996067 0.20703284 0.19309792]
 [0.18278767 0.18972701 0.22451803 0.20893572 0.19403161]
 [0.18077034 0.1892612  0.2236769  0.21081012 0.19548143]
 [0.18254872 0.19220418 0.22300169 0.20895892 0.19328652]
 [0.18032935 0.19029863 0.22319157 0.21000609 0.19617435]
 [0.18328631 0.18907256 0.22911799 0.20782094 0.19070214]
 [0.17863902 0.18771355 0.23066713 0.21065918 0.19232109]
 [0.18178153 0.19022569 0.22538401 0.20857622 0.1940325 ]
 [0.18072292 0.18907587 0.22616044 0.21096109 0.19307965]
 [0.18215105 0.18966101 0.22436853 0.21200544 0.191814  ]
 [0.18104836 0.18830387 0.22495148 0.21120267 0.19449359]
 [0.18192047 0.18981694 0.22512193 0.2107065  0.19243418]], shape=(100, 5), dtype=float32)

data example:

any help appreciated. i'm new to transformers so please feel free to point any extra considerations.","['python', 'python-3.x', 'tensorflow', 'huggingface-transformers', 'roberta']",74073165,"you need to pass a list of [input_ids , input_mask] to base_model.
# !pip install transformers 
from transformers import tfautomodel
import tensorflow as tf


len_seq = 64

# define inputs
input_ids = tf.keras.layers.input(shape=(len_seq,), name='input_ids', dtype='int32')
input_mask = tf.keras.layers.input(shape=(len_seq,), name='input_mask', dtype='int32')

base_model = tfautomodel.from_pretrained('roberta-base')
for layer in base_model.layers:
    layer.trainable = false
# check summary of tf_roberta_model
base_model.summary()

embedding = base_model([input_ids, input_mask])[1]
# or
# embedding = base_model([input_ids, input_mask]).pooler_output


# define hidden layers
layer = tf.keras.layers.dense(len_seq * 2, activation='relu')(embedding)
layer = tf.keras.layers.dense(len_seq, activation='relu')(layer)

# define output
output = tf.keras.layers.dense(2, activation='softmax', name='output')(layer)


model = tf.keras.models.model(inputs=[input_ids, input_mask], outputs=[output])
model.summary()

output:
model: ""tf_roberta_model_2""
_________________________________________________________________
 layer (type)                output shape              param #   
=================================================================
 roberta (tfrobertamainlayer  multiple                 124645632 
 )                                                               
                                                                 
=================================================================
total params: 124,645,632
trainable params: 0
non-trainable params: 124,645,632
_________________________________________________________________


model: ""model""
__________________________________________________________________________________________________
 layer (type)                   output shape         param #     connected to                     
==================================================================================================
 input_ids (inputlayer)         [(none, 64)]         0           []                               
                                                                                                  
 input_mask (inputlayer)        [(none, 64)]         0           []                               
                                                                                                  
 tf_roberta_model_2 (tfrobertam  tfbasemodeloutputwi  124645632  ['input_ids[0][0]',              
 odel)                          thpoolingandcrossat               'input_mask[0][0]']             
                                tentions(last_hidde                                               
                                n_state=(none, 64,                                                
                                768),                                                             
                                 pooler_output=(non                                               
                                e, 768),                                                          
                                 past_key_values=no                                               
                                ne, hidden_states=n                                               
                                one, attentions=non                                               
                                e, cross_attentions                                               
                                =none)                                                            
                                                                                                  
 dense (dense)                  (none, 128)          98432       ['tf_roberta_model_2[0][1]']     
                                                                                                  
 dense_1 (dense)                (none, 64)           8256        ['dense[0][0]']                  
                                                                                                  
 output (dense)                 (none, 2)            130         ['dense_1[0][0]']                
                                                                                                  
==================================================================================================
total params: 124,752,450
trainable params: 106,818
non-trainable params: 124,645,632
__________________________________________________________________________________________________",https://stackoverflow.com/questions/74073113,python,14-10-2022 17:55,674.0,2.0,1.0,True,14-10-2022 18:24,14-10-2022 18:14
73153633,correct keras lstm input shape after text-embedding,"i'm trying to understand the keras lstm layer a bit better in regards to timesteps, but am still struggling a bit.
i want to create a model that is able to compare 2 inputs (siamese network). so my input is twice a preprocessed text. the preprocessing is done as followed:
max_len = 64
data['cleaned_text_1'] = assets.apply(lambda x: clean_string(data[]), axis=1)
data['text_1_seq'] = t.texts_to_sequences(cleaned_text_1.astype(str).values)
data['text_1_seq_pad'] = [list(x) for x in pad_sequences(assets['text_1_seq'], maxlen=max_len, padding='post')]

same is being done for the second text input. t is from keras.preprocessing.text.tokenizer.
i defined the model with:
common_embed = embedding(
    name=""synopsis_embedd"",
    input_dim=len(t.word_index)+1,
    output_dim=300,
    input_length=len(data['text_1_seq_pad'].tolist()[0]),
    trainable=true
)

lstm_layer = tf.keras.layers.bidirectional(
    tf.keras.layers.lstm(32, dropout=0.2, recurrent_dropout=0.2)
)

input1 = tf.keras.input(shape=(len(data['text_1_seq_pad'].tolist()[0]),))
e1 = common_embed(input1)
x1 = lstm_layer(e1)

input2 = tf.keras.input(shape=(len(data['text_1_seq_pad'].tolist()[0]),))
e2 = common_embed(input2)
x2 = lstm_layer(e2)

merged = tf.keras.layers.lambda(
    function=l1_distance, output_shape=l1_dist_output_shape, name='l1_distance'
)([x1, x2])

conc = concatenate(axis=-1)([merged, x1, x2])

x = dropout(0.01)(conc)
preds = tf.keras.layers.dense(1, activation='sigmoid')(x)
model = tf.keras.model(inputs=[input1, input2], outputs=preds)

that seems to work if i feed the numpy data with the fit method:
model.fit(
    x = [np.array(data['text_1_seq_pad'].tolist()), np.array(data['text_2_seq_pad'].tolist())],
    y = y_train.values.reshape(-1,1), 
    epochs=epochs,
    batch_size=batch_size,
    validation_data=([np.array(val['text_1_seq_pad'].tolist()), np.array(val['text_2_seq_pad'].tolist())], y_val.values.reshape(-1,1)),
)

what i'm trying to understand at the moment is what is the shape in my case for the lstm layer for:

samples
time_steps
features

is it correct that the input_shape for the lstm layer would be input_shape=(300,1) because i set the embedding output dim to 300 and i have only 1 input feature per lstm?
and do i need to reshape the embedding output or can i just set
lstm_layer = tf.keras.layers.bidirectional(
    tf.keras.layers.lstm(32, input_shape=(300,1), dropout=0.2, recurrent_dropout=0.2)
)

from the embedding output?
example notebook can be found in github or as colab","['python', 'tensorflow', 'keras', 'lstm', 'word-embedding']",73154654,"in general, an lstm layer needs 3d inputs shaped this way : (batch_size, lenght of an input sequence , number of features ). (batch size is not really important, so you can just consider that one input need to have this shape (lenght of sequence, number of features par item) )
in your case, the output dim of your embedding layer is 300. so your lstm have 300 features.
then, using lstm on sentences requires a constant number of tokens. lstm works with constant input dimension, you can not pass it a text with 12 tokens following by another one with 68 tokens. indeed, you need to fix a limit and pad the sequence if needed.
so, if your sentence is 20 tokens long and that your limit is 50, you need to pad (add at the end of your sequence) the sequence with 30 ï¿½ï¿½ï¿½neutralï¿½ï¿½ï¿½ tokens (often zeros).
after all, your lstm input dimension must be (number of token per text, dimension of your embedding outputs) -> (50, 300)< example.
to learn more about it, it suggest you to take a look to this : (but in your case, you can replace time_steps by number_of_tokens)

share
edit
delete
flag",https://stackoverflow.com/questions/73153633,python,28-07-2022 13:17,1217.0,0.0,1.0,True,28-07-2022 14:38,28-07-2022 14:23
44621452,hierarchical classification in sklearn,"i would like to know if there is an implementation of hierarchical classification in the scikit-learn package or in any other python package. 
thank you so much in advance.","['machine-learning', 'scikit-learn', 'nlp', 'data-science']",48616580,i couldn't find an implementation of hierarchical classification on scikit-learn official documentation. but i found this repository recently. this module is based on scikit-learn's interfaces and conventions. i hope this will be useful.,https://stackoverflow.com/questions/44621452,machine-learning,19-06-2017 02:26,5351.0,7.0,2.0,True,14-12-2021 18:09,05-02-2018 08:14
4675991,identifying cataphora and anaphora using the stanford parser,"can the stanford parser find instances of cataphora and anaphora in a given set of sentences?
are there any alternative open-source (or proprietary) software packages that are capable of coreference resolution?","nlp, stanford-nlp, jnlp, coreference-resolution",4678419,"the stanford parser can't do this, but the coreference resolution system packaged in stanford's corenlp can.",https://stackoverflow.com/q/4675991,"nlp, stanford-nlp, jnlp, coreference-resolution",13-01-2011 01:40,711.0,0.0,1.0,True,10-11-2022 14:18,10-11-2022 14:18
75899189,openai chat completions api error: &quot;invalid url (post /chat/v1/completions)&quot;,"i followed a tutorial to make a chatgpt-like app and got this error:
failed to load response due to {
    'error' : {
        'message' : 'invalid url (post /chat/v1/completions)',
        'type':'invalid_request_error',
        'param':null,
        'code':null
    }
}

this is my code :
jsonobject jsonbody = new jsonobject();
        try {
            jsonbody.put(""model"", ""gpt-3.5-turbo"");
            jsonbody.put(""messages"", question);
            jsonbody.put(""max_tokens"", 4000);
            jsonbody.put(""temperature"", 0);
        } catch (jsonexception e) {
            throw new runtimeexception(e);
        }
        requestbody body = requestbody.create(jsonbody.tostring(),json);
        request request = new request.builder()
                .url(""
                .addheader(""authorization"", ""bearer hidden_key"")
                .addheader(""content-type"", ""application/json"")
                .post(body)
                .build();
        client.newcall(request).enqueue(new callback() {
            @override
            public void onfailure(@nonnull call call, @nonnull ioexception e) {
                addresponse(""failed to load response due to pd ""+e.getmessage());
            }

            @override
            public void onresponse(@nonnull call call, @nonnull response response) throws ioexception {
                if(response.issuccessful()){
                    jsonobject jsonobject  = null;
                    try {
                        jsonobject = new jsonobject(response.body().string());
                        jsonarray jsonarray = jsonobject.getjsonarray(""choices"");
                        string result = jsonarray.getjsonobject(0).getstring(""message"");
                        addresponse(result.trim());
                    } catch (jsonexception e) {
                        throw new runtimeexception(e);
                    }

                }else{
                    addresponse(""failed to load response due to ""+response.body().string());
                }
            }

i tried changing the model, removing the \chat\ in the url and send the prompt directly in url too.
i'm new to app making and java coding (but i'm no beginner in coding) so i understand that maybe this code isn't great as i almost only copy and paste the code from the tutorial.
thanks for your help!","['java', 'android', 'okhttp', 'openai-api', 'chatgpt-api']",75899296,"you have a typo.
change this...


...to this.


see the documentation.",https://stackoverflow.com/questions/75899189,java,31-03-2023 13:37,3472.0,0.0,1.0,True,12-06-2024 17:10,12-06-2024 17:10
78865458,spacy and gensim on jupyter notebooks,"i have bought a new macbook and have installed python 3.12.5 on it. i am trying to import and run libraries for natural langauge processing in a jupyter notebook, and all is well... except when it comes to spacy and gensim.
import spacy
import gensim

typeerror: forwardref._evaluate() missing 1 required keyword-only argument: 'recursive_guard'

i checked to see if they are imported with :
!pip show spacy
!pip show gensim

name: spacy
version: 3.7.5
summary: industrial-strength natural language processing (nlp) in python
home-page: 
author: explosion
author-email: contact@explosion.ai
license: mit
location: /opt/anaconda3/lib/python3.12/site-packages
requires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel
required-by: 
name: gensim
version: 4.3.2
summary: python framework for fast vector space modelling
home-page: 
author: radim rehurek
author-email: me@radimrehurek.com
license: lgpl-2.1-only
location: /opt/anaconda3/lib/python3.12/site-packages
requires: numpy, scipy, smart-open
required-by: 

so they should be loaded, but i don't know what the typeerror refers to and how to fix it!
i read that there is something with the python update...but could anyone help me clarify this?","['python', 'jupyter-notebook', 'spacy', 'huggingface-transformers', 'gensim']",78865568,problem occurs in pydantic library (to be exact pydantic.v1.typing module) the best solution at the moment is to downgrade python version to 3.12.3. patches to this problem has been already merged to repository but you need to wait for next release.,https://stackoverflow.com/questions/78865458,python,13-08-2024 09:31,266.0,0.0,1.0,True,21-08-2024 21:12,21-08-2024 21:12
68718901,what does nltk.download(&quot;wordnet&quot;) accomplish,"i wanted to know what nltk.download() do. also, if i add ""wordnet"" as an argument, then what happens. is wordnet like some dataset or something, i would like more clarification on that.","['deep-learning', 'nlp', 'nltk']",68725635,"the argument to nltk.download() is not a file or module, but a resource id that maps to a corpus, machine-learning model or other resource (or collection of resources) to be installed in your nltk_data area. you can see a list of the available resources, and their ids, at  .
you can use the special id ""book"" (as in nltk.download(""book"")) to download all resources mentioned in the nltk book; that's handy if you don't want to keep stopping your explorations to download missing resources.
calling nltk.dowload() without an argument pops up an interactive browser window (if it can) that you can use to browse and select resources for download.",https://stackoverflow.com/questions/68718901,deep-learning,09-08-2021 21:45,3495.0,4.0,1.0,True,10-08-2021 11:51,10-08-2021 10:45
75788612,use fine-tuned bert to train a new sentence-transformer,"i have fine-tuned bert on domain specific data, now i am going to train a sentence transformer based on this fine-tuned bert and my own labelled data. i created sentence transformer by below code:
model_name = ""path/to/model"" 
tokenizer = autotokenizer.from_pretrained(""bert-base-uncased"") # when i fine tuned the bert, i used this tokenizer
model = automodel.from_pretrained(model_name)

word_embedding_model = models.transformer(model, tokenizer)
pooling_model = models.pooling(word_embedding_model.get_word_embedding_dimension())
sentence_transformer = sentencetransformer(modules=[word_embedding_model, pooling_model])

but i got error:
we couldn't connect to ' to load this model, couldn't find it in the cached files and it looks like bertmodel(...)  is not the path to a directory containing a {configuration_file} file.
checkout your internet connection or see how to run the library in offline mode at '

i saved to google drive by below code:
model.save_pretrained('/content/drive/mydrive/testforsenttransformer')

and got below two errors:
hfvalidationerror: repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'bertmodel(
  (embeddings): bertembeddings(
    (word_embeddings): embedding(30522, 768, padding_idx=0)
    (position_embeddings): embedding(512, 768)
    (token_type_embeddings): embedding(2, 768)
    (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
    (dropout): dropout(p=0.1, inplace=false)
  )

and
during handling of the above exception, another exception occurred:

oserror                                   traceback (most recent call last)
/usr/local/lib/python3.9/dist-packages/transformers/configuration_utils.py in _get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    647             except exception:
    648                 # for any other exception, we throw a generic error.
--> 649                 raise environmenterror(
    650                     f""can't load the configuration of '{pretrained_model_name_or_path}'. if you were trying to load it""
    651                     "" from ' make sure you don't have a local directory with the same""

oserror: can't load the configuration of 'bertmodel(
  (embeddings): bertembeddings(
    (word_embeddings): embedding(30522, 768, padding_idx=0)
    (position_embeddings): embedding(512, 768)
    (token_type_embeddings): embedding(2, 768)
    (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
    (dropout): dropout(p=0.1, inplace=false)
  )

how to fix these errors? thanks for helping!","['nlp', 'bert-language-model']",75894169,"should do:
model_path = ""path/to/model"" 
# tokenizer = autotokenizer.from_pretrained(""bert-base-uncased"") 
# model = automodel.from_pretrained(model_name)

word_embedding_model = models.transformer(model_name_or_path=model_path, tokenizer_name_or_path=""bert-base-uncased"")
pooling_model = models.pooling(word_embedding_model.get_word_embedding_dimension())
sentence_transformer = sentencetransformer(modules=[word_embedding_model, pooling_model])",https://stackoverflow.com/questions/75788612,nlp,20-03-2023 09:37,800.0,0.0,1.0,True,31-03-2023 01:35,26-03-2023 10:20
73777686,find the frequency of words of a dataframe based on another dataframe,"i have a general dataframe with the following form
df = 
index   text
0   i really dont come across how i actually am an...
1   music has become the only way i am staying san...
2   adults are contradicting
3   exo are breathing 553 miles away from me. they...
4   i am missing people that i met when i was hospit...
... ... ...
3365023 3365023 i don't want to hear it. i really really fuckin...

and i have another dataframe with the following form
df_frequency = 
unique words    label
     i            2
     to           2
     the          2
    .....        ...
    @gaiaxys      1

and i want to add a new column in the df_frequency dataframe called 'frequency' which contains the frequency of each word in the dataframe df.","['python', 'pandas', 'dataframe', 'nlp']",73777922,"try:
#1. create new dataframe with words frequency from the column df['text']
a = [i.split() for i in df['text'].tolist()]
df2 = pd.series([i.strip() for x in a for i in x]).value_counts().reset_index(name='frequency').rename(columns={'index':'unique_words_from_df'})

#df2
    unique_words_from_df    frequency
0   i                       5
1   am                      2
2   are                     2
3   come                    1
4   exo                     1
5   was                     1
#...

#2. merge df_frequency with df2
df_frequency.merge(df2, right_on='unique_words_from_df', left_on='unique words', how='left').drop(columns='unique_words_from_df')

unique words    label   frequency
i               2       5.0
to              2       nan
the             2       1.0",https://stackoverflow.com/questions/73777686,python,19-09-2022 18:16,32.0,0.0,1.0,True,20-09-2022 07:22,20-09-2022 07:22
66892154,phrase extraction with spacy,"does spacy have some apis to do phrase* extraction as one would do when using word2phrase or the phrases class from gensim? thank you.
ps. phrases meant as collocations in linguistics.","['nlp', 'spacy', 'gensim', 'phrase']",66899277,"i am wondering if you have you seen pytextrank or spacycake extension to spacy?
both can help with phrase extraction which is not possible directly with spacy.",https://stackoverflow.com/questions/66892154,nlp,31-03-2021 17:01,3634.0,5.0,2.0,True,06-11-2022 07:12,06-11-2022 07:12
73314277,tf predict multiple predictions at once,"i am trying to batch predict a number of inputs to my model which expects an input containing 2 1d lists of a fixed size like this:
[<tf.tensor: shape=(1, 150), dtype=int64, numpy=array([[2,  924, ...]])>, <tf.tensor: shape=(1, 150), dtype=int64, numpy=array([[2,  130, ...]])>]

assuming this input is called input_in, i can predict a sample by writing:
transformer.predict(input_in)

where input_in thus has the following shape:

(2, 1, 150)

and this works. however, i can't understand the input necessary to pass to predict() to predict multiple predictions at once. intuitively, the shape has to be something like this:

(x, 2, 1, 150)

where x is the number of predictions to pass.
i tried this, but i receive an error:
transformer.predict([input_in, input_in, input_in])

valueerror: in user code:

    file ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1801, in predict_function  *
        return step_function(self, iterator)
    file ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1790, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    file ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1783, in run_step  **
        outputs = model.predict_step(data)
    file ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1751, in predict_step
        return self(x, training=false)
    file ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
        raise e.with_traceback(filtered_tb) from none
    file ""/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py"", line 200, in assert_input_compatibility
        raise valueerror(f'layer ""{layer_name}"" expects {len(input_spec)} input(s),'

    valueerror: layer ""transformer"" expects 2 input(s), but it received 6 input tensors. inputs received: [<tf.tensor 'iteratorgetnext:0' shape=(none, 12) dtype=int64>, <tf.tensor 'iteratorgetnext:1' shape=(none, 1) dtype=int64>, <tf.tensor 'iteratorgetnext:2' shape=(none, 12) dtype=int64>, <tf.tensor 'iteratorgetnext:3' shape=(none, 1) dtype=int64>, <tf.tensor 'iteratorgetnext:4' shape=(none, 12) dtype=int64>, <tf.tensor 'iteratorgetnext:5' shape=(none, 1) dtype=int64>]

interestingly, if i try to predict one input but simply change the input to a tensor, i also get a similar error:
transformer_test.predict(tf.convert_to_tensor([input_tensor,output_tensor]))


-> valueerror: layer ""transformer"" expects 2 input(s), but it received 1 input tensors. inputs received: [<tf.tensor 'iteratorgetnext:0' shape=(none, 1, 150) dtype=int64>]

any suggestions?
edit:
my transformer model looks like this:

i also reproduced a minimal version of the issue: 
if you run all, the last 2 cells will give errors. i have been trying to play around with the input to batch predict but nothing seems to have worked...","['python', 'tensorflow', 'machine-learning', 'nlp', 'tf.keras']",73420774,"or try using the names of your input layers as references:
ds = tf.data.dataset.from_tensor_slices((tf.stack([batch_inputs[0], batch_inputs[0], batch_inputs[0]]))).map(lambda x: {'inputs': x[0], ""dec_inputs"": x[1]})
predictions = transformer_test.predict(ds)

and without tf.stack:
ds = tf.data.dataset.from_tensor_slices((batch_inputs)).map(lambda x: {'inputs': x[0], ""dec_inputs"": x[1]})",https://stackoverflow.com/questions/73314277,python,11-08-2022 01:14,701.0,1.0,2.0,True,19-08-2022 18:15,13-08-2022 12:26
57479028,spacy nlp custom rule matcher,"i am begginer with nlp. i am using spacy python library for my nlp project. here is my requirement,
i have a json file with all country names. now i need to parse and get goldmedal count for the each countries in the document. given
below the sample sentence,
""czech republic won 5 gold medals at olympics. slovakia won 0 medals olympics""

i am able to fetch country names but not it medal count. given below my code. please help to proceed further.
import json
from spacy.lang.en import english
from spacy.matcher import phrasematcher

with open(""c:\python36\srclcl\countries.json"") as f:
    countries = json.loads(f.read())

nlp = english()
nlp.add_pipe(nlp.create_pipe('sentencizer'))
doc = nlp(""czech republic won 5 gold medals at olympics. slovakia won 0 medals olympics"")
matcher = phrasematcher(nlp.vocab)
patterns = list(nlp.pipe(countries))

matcher.add(""country"", none, *patterns)


for sent in doc.sents:
    subdoc = nlp(sent.text)
    matches = matcher(subdoc)
    print (sent.text)
    for match_id, start, end in matches:
        print(subdoc[start:end].text)

also, if the given text is like ,
""czech republic won 5 gold medals at olympics in 1995. slovakia won 0 medals olympics""","['python', 'nlp', 'nltk', 'spacy']",57480078,"spacy provides rule-based matching which you could use. 
they can be used as follows:
import spacy
from spacy.pipeline import entityruler
nlp = spacy.load('en_core_web_sm', disable=[""ner"", ""parser""])

countries = ['czech republic', 'slovakia']
ruler = entityruler(nlp)
for a in countries:
    ruler.add_patterns([{""label"": ""country"", ""pattern"": a}])
nlp.add_pipe(ruler)


doc = nlp(""czech republic won 5 gold medals at olympics. slovakia won 0 medals olympics"")

with doc.retokenize() as retokenizer:
    for ent in doc.ents:
        retokenizer.merge(doc[ent.start:ent.end])


from spacy.matcher import matcher
matcher = matcher(nlp.vocab)
pattern =[{'ent_type': 'country'}, {'lower': 'won'},{""is_digit"": true}]
matcher.add('medal', none, pattern)
matches = matcher(doc)


for match_id, start, end in matches:
    span = doc[start:end]
    print(span)


output:

czech republic won 5
  slovakia won 0 

the above code should get you started. naturally, you will have to write your own more complex rules so that you can handle cases like: 
""czech republic unsurprisingly won 5 gold medals at olympics in 1995.""
and other more complex sentence structures.",https://stackoverflow.com/questions/57479028,python,13-08-2019 13:40,726.0,5.0,1.0,True,16-12-2021 19:42,13-08-2019 13:52
76461534,python - extract information from email,"i am new to python.  below are some sample emails i received.
email sample 1
dear all,
please note the total selling volume and total remaining stock
total selling volume: 45677
total remaining stock a:3456
remain at your disposal in case of any doubt or comments.
best regards,
email sample 2
dear all,
please see the data as below:
tol volume:  1,231,245
no. of remaining stock a: 232
no. of remaining stock b:  1,435
email sample 3
dear all,
please find our volume was 233,435
total remaining stock a: 2453
email sample 4
in may we had 90 remaining stock a and 4190 teus.
i would like to extract the volume and total remaining stock figures from those emails.  any hints if i can get those figures by using python?
i have prepared the below code to extract the figures from email.  however i can not distinguish which figure is total selling volume, total remaining stock
import re
import pandas as pd
import win32com.client
from datetime import datetime, timedelta

outlook = win32com.client.dispatch('outlook.application')
mapi = outlook.getnamespace(""mapi"")
inbox = mapi.getdefaultfolder(6).folders.item(""ai email testing"")
#outlook.getdefaultfolder(6) .folders.item(""your_folder_name"")
#inbox = outlook.getdefaultfolder(6)
messages = inbox.items

received_dt = datetime.now() - timedelta(days=1)
received_dt = received_dt.strftime('%m/%d/%y %h:%m %p')


for message in list(messages):
    #print (message)
    body_content = message.body
    body_content =body_content[body_content.find(""subject:""):]
    #print(body_content)
    figures = re.findall(""\d+(?:,\d+)*(?:\.\d+)?"",body_content)
    print(figures)","['python', 'nlp', 'artificial-intelligence', 'extract', 'text-extraction']",76462798,"here's a solution using regex:
from __future__ import annotations

import re
from typing import list, tuple


def get_number(text: str) -> float | int | str:
    """"""
    extract the first numeric value from the input string.

    the function uses regular expressions to extract the first numeric
    occurrence from `text`. if no numeric value is found, the original string
    is returned. commas are removed from the extracted number, if any.
    the function first attempts to convert the number to an integer,
    and if that fails, it tries to convert it to a float.

    parameters
    ----------
    text : str
        the string from which the numeric value should be extracted.

    returns
    -------
    float | int | str
        the first numeric value in `text` converted to int or float,
        or original `text` if no numeric value is found.

    raises
    ------
    valueerror
        if the extracted number can't be converted to an integer or a float.

    examples
    --------
    illustration of the function usage and behavior.

    >>> get_number(""hello world 123!"")
    123
    >>> get_number(""i have 2,200 dollars."")
    2200
    >>> get_number(""no numbers here."")
    'no numbers here.'
    >>> get_number(""it is over 9000!"")
    9000
    >>> get_number(""the value of pi is about 3.14159."")
    3.14159
    >>> get_number(""total: 123,456,789."")
    123456789.0
    """"""
    number = re.search(r'(\d+|,)+.', text, re.i)
    if number:
        number = number[0].strip().replace(',', '')
    if not number:
        print(f""found no numbers inside text: {text!r}"")
        return text
    try:
        return int(number)
    except valueerror:
        return float(number)


def extract_stock_volume_from_email(email: str) -> tuple[int | float | str, int | float | str]:
    """"""
    extract the volume and remaining stock a details from an email text.

    this function employs regular expressions to parse the given email text and
    extract details about volume and remaining stock a.
    the values extracted are then cleaned and returned.

    parameters
    ----------
    email : str
        text from the email to parse.

    returns
    -------
    volume : int | float | str
        volume extracted from the email.
        returns 'volume not found' if no volume details are found.
    remaining_stock_a : int | float | str
        remaining stock a extracted from the email.
        returns 'remaining stock a not found' if no stock a details are found.

    raises
    ------
    re.error
        if a non-valid regular expression is used.

    see also
    --------
    re.search : the method used for extracting volume and remaining stock details.

    examples
    --------
    >>> email_text = ""the volume was 5000 teus. stock a: 1000 units.""
    >>> extract_stock_volume_from_email(email_text)
    (5000, 1000)
    >>> email_text = ""no volume and stock data available.""
    >>> extract_stock_volume_from_email(email_text)
    ('volume not found', 'remaining stock a not found')
    """"""
    # extract the volume
    volume = re.search(
        r'(?:volume:|volume was|teus\.|teus |teu |$)\s(\d+|,)+.*?|(\d+|,)+.(?:\steus|\steu)',
        email, re.i
    )
    if volume:
        volume = get_number(volume[0].strip())
    if not volume:
        volume = 'volume not found'

    # extract the remaining stock
    remaining_stock_a = re.search(r'(?:stock a:|stock a: |$)(\d+|,)+.*?', email, re.i)
    if remaining_stock_a:
        remaining_stock_a = remaining_stock_a[0].strip()
    if not remaining_stock_a:
        remaining_stock_a = re.search(r'(\d+)(.+)(stock a)', email, re.i)
        if remaining_stock_a:
            remaining_stock_a = remaining_stock_a[0].strip()
    if remaining_stock_a:
        remaining_stock_a = get_number(remaining_stock_a)
    if not remaining_stock_a:
        remaining_stock_a = 'remaining stock a not found'
    # print(f""volume: {volume}\nremaining stock a: {remaining_stock_a}\n"")
    return volume, remaining_stock_a


def extract_stock_volume_from_emails(
    emails: list[str],
) -> list[tuple[int | float | str, int | float | str]]:
    """"""
    apply the function `extract_stock_volume_from_email` to a list of emails.

    parameters
    ----------
    emails : list[str]
        a list of email texts to be parsed.

    returns
    -------
    list[tuple[int | float | str, int | float | str]]
        a list of tuples. each tuple contains the volume and remaining stock a
        extracted from each email. if no volume or stock a details could be
        extracted from an email, the corresponding element in the tuple will be
        'volume not found' or 'remaining stock a not found', respectively.

    raises
    ------
    re.error
        if a non-valid regular expression is used in `extract_stock_volume_from_email`.

    see also
    --------
    extract_stock_volume_from_email : the function used to extract details from each email.

    examples
    --------
    >>> email_texts = [
    ...     ""the volume was 5000 teus. stock a: 1000 units."",
    ...     ""no volume and stock data available."",
    ... ]
    >>> extract_stock_volume_from_emails(email_texts)
    [(5000, 1000), ('volume not found', 'remaining stock a not found')]
    """"""
    return list(map(extract_stock_volume_from_email, emails))

using the above code on the e-mails you provided as example:
emails = [
    r""""""dear all,

please note the total selling volume and total remaining stock

total selling volume: 45677 total remaining stock a:3456

remain at your disposal in case of any doubt or comments.

best regards,"""""",
    r""""""dear all,

please see the data as below:

tol volume: 1,231,245 no. of remaining stock a: 232 no. of remaining stock b: 1,435"""""",
    r""""""dear all,

please find our volume was 233,435

total remaining stock a: 2453"""""",
    r""in may we had 90 remaining stock a and 4190 teus."",
]
extract_stock_volume_from_emails(emails)
# returns:
#
# [(45677, 3456), (1231245, 232), (233435, 2453), (4190, 90)]
#  ^----^  ^--^
#  |       |
#  |       +-- remaining stock a
#  +-- volume

note
it should be noted that the function extract_stock_volume_from_email, that parses each e-mail is not failproof. the regex patterns it contains were all based on the e-mails you provided as example. if other e-mails don't follow the same patterns as the example e-mails, these additional patterns will have to be added to the extract_stock_volume_from_email function.",https://stackoverflow.com/questions/76461534,python,13-06-2023 03:32,3449.0,-2.0,2.0,True,20-06-2023 15:16,13-06-2023 07:45
70628801,trying to convert plural words to singular words using regex but want to ignore a few words,"i am currently trying to replace some of the plural words like removing ""s"" from ""birds"" and replacing it as ""bird"" in bigquery
but i want them to ignore a few words like ""less"", ""james"", ""this"".
i was able to come up with this which ignores the ""less"" but still butchers james.
    select regexp_replace(""james likes to chase birds"",""([^s])s\\b"", ""\\1"" )

the output i am getting is ""jame like to chase bird"" but what i am expecting is ""james like to chase bird""
update:
i tried to use negative lookahead, but unfortunately, bigquery regex(re2) doesn't support this.","['regex', 'google-bigquery', 'stemming', 're2']",70636521,"you can use an alternation here, a regex with two alternatives. in the first alternative, you can capture all the words that are exceptions, into group 1, and in the second one, use your regex. the replacement will be both group values concatenated:
(?i)\b(less|james|this)\b|([^s])s\b

replace with \1\2. see the regex demo. details:

(?i) - a case insensitive modifier
\b(less|james|this)\b - group 1: less, james, or this as a whole word
| - or
([^s]) - group 1: any char other than s (note: if you want to only match a letter other than s, you can use [^\w\d_s])
s\b - s at the end of a word.

in your code, use
select regexp_replace(""james likes to chase birds"", r""(?i)\b(less|james|this)\b|([^s])s\b"", r""\1\2"" )",https://stackoverflow.com/questions/70628801,regex,08-01-2022 00:58,937.0,2.0,1.0,True,08-01-2022 21:43,08-01-2022 01:15
68709240,error: &#39;module&#39; object is not callable in doc2vec,"i am trying to fit the doc2vec method in a dataframe which the first column has the texts, and the second one the label (author). i have found this article  which is really helpful. however, i am stuck at how to build a model
import tqdm
cores = multiprocessing.cpu_count()
model_dbow = doc2vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample=0, workers=cores)
model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])


typeerror: 'module' object is not callable

could you please help me how to overcome this issue?
before that i have also this code
train, test = train_test_split(df, test_size=0.3, random_state=42)
import nltk
from nltk.corpus import stopwords
def tokenize_text(text):
    tokens = []
    for sent in nltk.sent_tokenize(text):
        for word in nltk.word_tokenize(sent):
            if len(word) < 2:
                continue
            tokens.append(word.lower())
    return tokens
train_tagged = train.apply(
    lambda r: taggeddocument(words=tokenize_text(r['text']), tags=[r.author]), axis=1)
test_tagged = test.apply(
    lambda r: taggeddocument(words=tokenize_text(r['text']), tags=[r.author]), axis=1)

edit: if i remove tqdm from the code is working, but i am not sure is this is accepted. tqdm as i know is a package for python that enables you to instantly create progress bars and estimate ttc (time to completion) for your functions and loops, so i mean if i remove it, there is no problem with the output. right?
edit2: see also this question my doc2vec code, after many loops of training, isn't giving good results. what might be wrong? to improve the code of the tutorial. thanks again @gojomo","['python', 'nlp', 'doc2vec']",68709415,"you are importing tqdm module and not the actual class.
replace import tqdm
with from tqdm import tqdm",https://stackoverflow.com/questions/68709240,python,09-08-2021 08:47,839.0,1.0,2.0,True,15-10-2021 04:21,15-10-2021 04:21
78836094,langchain chat history,"i am struggling with passing context to conversational rag chain when using runnablewithmessagehistory.
i have the following query function:
def query(query_text, prompt, session_id, metadata_context):
# history retrieval test
contextualize_q_prompt = chatprompttemplate.from_messages(
    [
        (""system"", contextualize_q_system_prompt),
        (""system"", ""{context}""),
        (""system"", prompt),
        messagesplaceholder(""chat_history""),
        (""human"", ""{input}""),
    ]
)
history_aware_retriever = create_history_aware_retriever(
    llm, retriever, contextualize_q_prompt
)

qa_prompt = chatprompttemplate.from_messages(
    [   
        (""system"", prompt_template),
        (""system"", ""{context}""),
        (""system"", prompt),
        messagesplaceholder(""chat_history""),
        (""human"", ""{input}""),
    ]
)

question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

conversational_rag_chain = runnablewithmessagehistory(
    rag_chain,
    get_session_history,
    input_messages_key=""input"",
    history_messages_key=""chat_history"",
    output_messages_key=""answer"",
)
try:
    logger.info(f""model: {llm_model} assigned. generation of response has started."")
    response = conversational_rag_chain.invoke({""input"": query_text, ""context"": metadata_context}, config={""configurable"": {""session_id"": f""{session_id}""}},)
    logger.info(f""response generated."")
except exception as e:
    return ({'generation of response failed: ': str(e)})
return response[""answer""]

i want to pass my own 'context' that is prepared and parsed from retriever. i do not want retriever to be called again but from what i've read - retrieving happens by itself if chat_history does not contain the answer.
prompt variable is created:
prompt_template = chatprompttemplate.from_template(prompt_template)
prompt = prompt_template.format(context=metadata_context, input=query_text)

as you can see i am trying to put the context everywhere but no success.
the 'context' i can see when calling
conversational_rag_chain.invoke({""input"": query_text, ""context"": metadata_context}, config={""configurable"": {""session_id"": f""{session_id}""}},)
        logger.info(f""response generated."")

is the result of retriever:
document(metadata={'number_of_reviews': '16', 'price': 18999, 'product_name': 'product', 'rating': '4')

the code i'm using is as follows:
chroma_client = chromadb. port=db_port)
chroma_collection = chroma_client.get_collection(os.getenv(""db_collection""))

vectorstore = vstorechroma(db_collection, embedding_function, client=client)


llm = chatopenai(model=""gpt-4o-mini"",temperature=0)

retriever = selfqueryretriever.from_llm(
    llm,
    vectorstore,
    document_content_description,
    metadata_field_info,
    search_kwargs = {""k"": 10}
)

def self_query(query_text):
    model = llm
    logger.info(""data retrieval has started."")
    try:
        result = retriever.invoke(query_text)
        logger.info(""data retrieved from database."")
        if len(result) == 0:
            logger.info(f""unable to find matching results."")
    except exception as e:
        return ({'retrieval failed: ': str(e)})
    return result

retrieving is alright i get correct results. the problem is that the context i prepare from metadata by parsing it with the function like the one you mention in your snippet. it is string and i do not get it where i can pass it so the context is used properly. the rest is as i mentioned before.","['python', 'langchain', 'large-language-model', 'rag']",78836389,"you can pass your context or your question however you decide but i think fundamentally, your context should be separate from question
you can also pass your context easily using this template i have provided
    chat_template = """"""
                answer the following questions{question} \n
                based on the data and context provided {context} \n
                question: {question} \n
            """"""

    # get the chat prompt template
    prompt = chatprompttemplate.from_template(chat_template)

to something like this below.
from langchain.prompts import chatprompttemplate
from langchain.chat_models import chatopenai
from langchain.chains import llmchain
from langchain_core.output_parsers import stroutputparser
from operator import itemgetter

prompt = chatprompttemplate.from_template(chat_template)

# initialize the llm
llm = chatopenai(temperature=0.5, max_tokens=4096)

# initialize the output parser
output_parser = stroutputparser()

# create the llmchain
final_rag = llmchain(prompt=prompt, llm=llm, output_parser=output_parser)

# invoke the chain with the input
question = ""your question here""  # replace this with your actual question
response = final_rag.invoke({""input"": question, ""context"": itemgetter(metadata_context)})

print(response)",https://stackoverflow.com/questions/78836094,python,05-08-2024 19:12,459.0,1.0,1.0,True,06-08-2024 19:20,06-08-2024 18:38
76314229,how to download spacy models in a poetry managed environment,"i am writing a python jupyter notebook that does some nlp processing on italian texts.
i have installed spacy 3.5.3 via poetry and then attempt to run the following code:
import spacy
load_model = spacy.load('it_core_news_sm')

the import line works as expected, but running spacy.load produces the following error:

oserror: [e050] can't find model 'it_core_news_sm'. it doesn't seem to be a python package or a valid path to a data directory.
the model name is correct as shown on 

after a web search, i see that a solution is to issue the following command:
python3 -m spacy download it_core_news_sm

after running this command the above code works as expected, however, is there a more 'kosher' way of doing this via poetry?","['python', 'nlp', 'spacy', 'python-poetry', 'virtual-environment']",76319201,"you can add a url dependency. first edit your pyproject.toml file to add the following (note: the name used here should match the name of the package (i.e. it_core_news_sm):
[tool.poetry.dependencies]
it_core_news_sm = {url = ""

then run the corresponding add call:
poetry add 

all of the spacy models can be found on spacy's model releases github page.",https://stackoverflow.com/questions/76314229,python,23-05-2023 11:29,5574.0,10.0,1.0,True,23-05-2023 23:39,23-05-2023 23:39
70164103,convert column of lists to integer,"trying to convert after encoding to integers but they are objects so i first turn them into strings
train_df[""labels""] = train_df[""labels""].astype(str).astype(int)
i am getting this error
invalid literal for int() with base 10: '[0, 1, 0, 0]
an example of a row from the dataset is
text                        labels
[word1,word2,word3,word4]    [1,0,1,0]","['python', 'pandas', 'dataframe', 'bert-language-model']",70164184,"it's because after train_df[""labels""].astype(str), this series became a series of lists, so you can't convert a list into type int.
if each element in train_df[""labels""] is of type list, you can do:
train_df[""labels""].apply(lambda x: [int(el) for el in x])

if it's of type str, you can do:
train_df[""labels""].apply(lambda x: [int(el) for el in x.strip(""[]"").split("","")])

you presumably you want to train some model but you can't use pd.series of lists to do it. you'll need to convert this into a dataframe. i can't say how to do that without looking at more than 1 line of data.",https://stackoverflow.com/questions/70164103,python,30-11-2021 03:56,5308.0,0.0,2.0,True,30-11-2021 16:27,30-11-2021 14:22
77113484,aiml chatbot not learning from aiml file,"i am making a voice-assistant for food ordering in fast-food restaurants. for dialog management tasks i am making aiml chatbot.
this is my std-startup.xml code
'''


<!-- category is an atomic aiml unit -->
<category>

    <!-- pattern to match in user input -->
    <!-- if user enters ""load aiml b"" -->
    <pattern>load aiml b</pattern>

    <!-- template is the response to the pattern -->
    <!-- this learn an aiml file -->
    <template>
        <learn>output.aiml</learn>
        <!-- you can add more aiml files here -->
        <!--<learn>more_aiml.aiml</learn>-->
    </template>
    
</category>

below is main.py code
import aiml
import os
kernel = aiml.kernel()
if os.path.isfile(""bot_brain.brn""):
kernel.bootstrap(brainfile = ""bot_brain.brn"")
else:
kernel.bootstrap(learnfiles = ""std-startup.xml"", commands = ""load aiml b"")
kernel.savebrain(""bot_brain.brn"")
kernel now ready for use
while true:
print (kernel.respond(input(""enter your message >> "")))
'''
after entering a msg which is from output.aiml itself the bot says the msg does not exists

please can someone tell how to fix it.
the bot should generate a response as given in output.aiml file
here is another example with category

and here is bot's repsonse","['python', 'nlp', 'chatbot', 'aiml']",77127051,"some interpreters need the patterns to be in upper case. try this instead:
<category>
    <pattern>thank you</pattern>
    <template>
        bye.
    </template>
</category>",https://stackoverflow.com/questions/77113484,python,15-09-2023 15:07,74.0,1.0,1.0,True,18-09-2023 12:05,16-09-2023 18:28
76186890,why is perplexity calculation giving different results for the same input?,"i'm following huggingface doc on calculating the perplexity of fixed-length models. i'm trying to verify that the formula works for various strings and i'm getting odd behavior. in particular, they mention

we donï¿½ï¿½ï¿½t want the log-likelihood for the tokens weï¿½ï¿½ï¿½re just treating as context to be included in our loss, so we can set these targets to -100 so that they are ignored

so given 2 different contexts but the same remaining tokens, the formula should return the same perplexity. however, it does not:""lang-py prettyprint-override"">from transformers import t5tokenizer, t5forconditionalgeneration
import torch
tokenizer = t5tokenizer.from_pretrained(""t5-small"")
model = t5forconditionalgeneration.from_pretrained(""t5-small"")

context_1 = 'here is some context_1 and some more stuff'
context_2 = 'here is some context and some more stuff and more stuff aspodkaspd'
answer_1 = 'this is not the answer'

input_ids_wrong = tokenizer(context_1 + answer_1, return_tensors=""pt"").input_ids
input_ids_correct = tokenizer(context_2 + answer_1, return_tensors=""pt"").input_ids
context_1_tokens_length = len(tokenizer(context_1, return_tensors=""pt"").input_ids[0])
context_2_tokens_length = len(tokenizer(context_2, return_tensors=""pt"").input_ids[0])

target_ids_wrong = input_ids_wrong.clone()
target_ids_correct = input_ids_correct.clone()

target_ids_wrong[:, :context_1_tokens_length] = -100 
target_ids_correct[:, :context_2_tokens_length] = -100 

print('target_ids_wrong', target_ids_wrong)
print('target_ids_correct', target_ids_correct)

with torch.no_grad():
    outputs_wrong = model(input_ids_wrong, labels=target_ids_wrong)
    outputs_correct = model(input_ids_correct, labels=target_ids_correct)
    
    neg_log_likelihood_wrong = outputs_wrong.loss
    neg_log_likelihood_correct = outputs_correct.loss

    ppl_wrong = torch.exp(neg_log_likelihood_wrong)
    ppl_correct = torch.exp(neg_log_likelihood_correct)
    print('ppl_wrong', ppl_wrong)
    print('ppl_correct', ppl_correct)

output:
    target_ids_wrong tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,   19,
               59,    8, 1525,    1]])
    target_ids_correct tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
             -100, -100, -100, -100, -100, -100,   19,   59,    8, 1525,    1]])
    ppl_wrong tensor(9.0377)
    ppl_correct tensor(21.1208)

i tried this with other models as well (e.g., gpt2 and sshleifer/tiny-gpt2) and got the same odd behavior. from the t5 doc they wrote

we must make sure that padding token idï¿½ï¿½ï¿½s of the labels are not taken into account by the loss function. in pytorch and tensorflow, this can be done by replacing them with -100, which is the ignore_index of the crossentropyloss.
so i don't understand why it takes the pad token into account. they also wrote in the same link
which for t5 is equal to 0 (i.e. the id of the pad token)

so i tried replacing the -100 with 0 and actually a got different perplexity score (still different than each other, but different than the -100). which makes me think they don't actually ignore the -100 token for some reason.
am i missing something","['pytorch', 'nlp', 'huggingface-transformers', 'language-model', 'perplexity']",76433107,"i think you might not be thinking of ""ignore the context"" in the same whay that they are. when they want to context to be ignored, they effectively mean they want to compute the log probs for the answer conditioned on the context; e.g., they want something like p(answer|context_1) and p(answer|context_2) instead of p(context_1 + answer) or p(context_2 + answer). if you want to ignore the context entirely, that would be p(answer) - in which case just don't pass the context into the model.
basically, the probability of the answer should change when given different contexts  - but you want only the conditional probability of the answer given the context, not the joint of the answer and context. you want ""how likely is this answer given this context?"", not ""how likely am i to see this context and answer in general"".
lastly, tokens with value of -100 are ignored by the cross entropy loss - that's why they're used, and why you get a different value if you set them to 0.",https://stackoverflow.com/questions/76186890,pytorch,06-05-2023 02:41,765.0,1.0,1.0,True,08-06-2023 14:50,06-05-2023 15:49
68018745,not able to import from `gensim.summarization` module in django,"i have included the 2 import statements in my views.py
from gensim.summarization.summarizer import summarizer
from gensim.summarization import keywords

however, even after i installed gensim using pip, i am getting the error:
modulenotfounderror: no module named 'gensim.summarization'","['python', 'django', 'nlp', 'gensim']",68023653,"the summarization code was removed from gensim 4.0. see:


12. removed gensim.summarization
despite its general-sounding name, the module will not satisfy the
majority of use cases in production and is likely to waste people's
time. see this github
ticket for
more motivation behind this.

if you need it, you could try:

installing the older gensim version; orï¿½ï¿½ï¿½
copy the source code out to your own local module

however, i expect you'd likely be disappointed by its inflexibility and how little it can do.
it was only extractive summarization - choosing a few key sentences from those that already exist. that only gives impressive results when the source text was already well-written in an expository style mixing high-level overview sentences with separate detail sentences. and, its method of analyzing/ranking words was very crude & hard-to-customize ï¿½ï¿½ï¿½ totally unconnected to the more generic/configurable/swappable approaches used elsewhere in gensim or in other text lib",https://stackoverflow.com/questions/68018745,python,17-06-2021 11:49,23630.0,9.0,3.0,True,24-01-2023 08:53,17-06-2021 17:16
75724406,openai api 404 response,"i'm trying to use chatgpt for my telegram bot. i used to use ""text-davinci-003"" model, and it was working fine (even now it's working fine), but i'm not satisfied with its responses.
now i'm trying to change the model to ""gpt-3.5-turbo"", and it's throwing a 404 response code with text ""error: request failed with status code 404"" and nothing else. here's my code:
import { configuration, openaiapi } from ""openai"";
import { env } from ""../utils/env.js"";

const model = ""gpt-3.5-turbo""; // works fine when it's ""text-davinci-003""
const configuration = new configuration({
  apikey: env.openai_api_key,
});
const openai = new openaiapi(configuration);

export async function getchatgptresponse(request) {
  try {
    const response = await openai.createcompletion({
      model,
      prompt: request, // request comes as a string
      max_tokens: 2000,
      temperature: 1,
      stream: false
    });

    console.log(""full response: "", response, `choices: `, ...response.data.choices)
    return response.data.choices[0].text;
  } catch (err) {
    console.log(`chatgpt error: ` + err);
    return err;
  }
}","['javascript', 'node.js', 'telegram-bot', 'openai-api', 'chatgpt-api']",75724477,"try to use createchatcompletion rather than createcompletion:
const response = async (message) => {
  const response = await openai.createchatcompletion({
    model: ""gpt-3.5-turbo"",
    messages: [{ role: ""user"", content: ""hello world"" }],
  });

  return response.data.choices[0].message.content;
};",https://stackoverflow.com/questions/75724406,javascript,13-03-2023 16:17,5753.0,5.0,1.0,True,02-06-2023 22:36,03-04-2023 09:26
76176266,pandas data frame to make words as columns in pandas,"how to make unique words which are in a list format in every row of a dataframe a column




name
desc




x
['red','white','yellow']


y
['red','black','blue']




as




name
red
white
yellow
black
blue




x
yes
yes
yes




y
yes


yes
yes","['python', 'pandas', 'nlp']",76176342,"with explode/pivot_table :
#if necessary
# from ast import literal_eval
# df[""desc""] = df[""desc""].apply(literal_eval)
ï¿½ï¿½ï¿½
out = (df.explode(""desc"").pivot_table(index=""name"", columns=""desc"",
                                      sort=false, aggfunc=lambda x: ""yes"")
           .rename_axis(columns=none).reset_index()
)

output :
print(out)

  name  red white yellow black blue
0    x  yes   yes    yes   nan  nan
1    y  yes   nan    nan   yes  yes
<",https://stackoverflow.com/questions/76176266,python,04-05-2023 18:20,44.0,0.0,3.0,True,04-05-2023 20:07,04-05-2023 18:31
75112136,python unable to install guesslang,"i'm trying to install guesslang with pip but it seems that the last version (which was released on august 2021) depends on an obsolete version of tensorflow (2.5.0). the problem is that i can't find this version anywhere.
so, how can i install it? or is there any other python library that does language detection?
however here's the error i get when trying to install it, maybe i misunderstood...
> pip install guesslang
collecting guesslang
  using cached guesslang-2.2.1-py3-none-any.whl (2.5 mb)
  using cached guesslang-2.2.0-py3-none-any.whl (2.5 mb)
  using cached guesslang-2.0.3-py3-none-any.whl (2.1 mb)
  using cached guesslang-2.0.1-py3-none-any.whl (2.1 mb)
  using cached guesslang-2.0.0-py3-none-any.whl (13.0 mb)
  using cached guesslang-0.9.3-py3-none-any.whl (3.2 mb)
collecting numpy
  using cached numpy-1.24.1-cp310-cp310-win_amd64.whl (14.8 mb)
collecting guesslang
  using cached guesslang-0.9.1-py3-none-any.whl (3.2 mb)
error: cannot install guesslang==0.9.1, guesslang==0.9.3, guesslang==2.0.0, guesslang==2.0.1, guesslang==2.0.3, guesslang==2.2.0 and guesslang==2.2.1 because these package versions have conflicting dependencies.

the conflict is caused by:
    guesslang 2.2.1 depends on tensorflow==2.5.0
    guesslang 2.2.0 depends on tensorflow==2.5.0
    guesslang 2.0.3 depends on tensorflow==2.5.0
    guesslang 2.0.1 depends on tensorflow==2.2.0
    guesslang 2.0.0 depends on tensorflow==2.2.0
    guesslang 0.9.3 depends on tensorflow==1.7.0rc1
    guesslang 0.9.1 depends on tensorflow==1.1.0

to fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict

error: resolutionimpossible: for help visit","['python', 'pip', 'language-detection']",75112598,tensorflow 2.5.0 released wheels for python 3.6-3.9. downgrade to python 3.9 to install guesslang with tensorflow 2.5.0.,https://stackoverflow.com/questions/75112136,python,13-01-2023 16:49,1000.0,4.0,2.0,True,13-05-2023 17:46,13-01-2023 17:53
76717272,why is the text in the files i am concatenating in powershell coming out altered?,"sorry if this doesn't make much sense, i'm not much of a programmer.
i am using powershell to concatenate all of the files within a folder into a single larger file, however when i do this, the text itself comes out 'corrupted'.
i have a folder of ancient greek texts that all end with a .tess extension, these files come from  (i'm not sure how this extension works, but it opens fine in notepad).
i used:
get-content *.tess | set-content greekcorpus.tess

however, the text would come out scrambled. for example:
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï""lang-none prettyprint-override"">ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½","['powershell', 'nlp', 'corpus']",76718517,"this should do the work :
get-content *.tess -encoding utf8",https://stackoverflow.com/questions/76717272,powershell,19-07-2023 00:37,34.0,0.0,1.0,True,19-07-2023 06:29,19-07-2023 00:43
77746423,find vocab word from vector for flexible comparisons,"is it possible to find a vocab word from a vector so that i can do more flexible comparisons?
something like this:
queen = nlp.vocab[""king""].vector - nlp.vocab[""man""].vector + nlp.vocab[""woman""].vector
king = nlp.vocab[""queen""].vector - nlp.vocab[""woman""].vector + nlp.vocab[""man""].vector
queen.similarity(king)

i realize in this example i could just check the similarity of king and queen directly, but my use-case is that i'd like to compare sentence/doc similarity and i read that in order to get sentence vector i can simply add up the words in a given sentence and i'm assuming that means i can compare them as well.","['nlp', 'spacy']",77747182,"better then summing the embeddings of the words forming your sentence, you can use a sentence embedding model as sentence-transformer. this will give you :
from sentence_transformers import sentencetransformer
from sklearn.metrics.pairwise import cosine_similarity

model = sentencetransformer('paraphrase-minilm-l6-v2')

e1 = model.encode(['this is your first sentence'])
e2 = model.encode(['this is your 2nd sentence'])

cosine_similarity(e1, e2)",https://stackoverflow.com/questions/77746423,nlp,02-01-2024 13:37,30.0,0.0,1.0,True,02-01-2024 15:59,02-01-2024 13:41
70186410,how do i write this into a function in python 3?,"how would i write this into a function that gives the same output?
from nltk.book import text2

sorted([word.lower() for word in text2 if len(word)>4 and len(word)<12])","['python', 'nltk', 'nltk-book']",70186715,"functions are defined using the special keyword def followed by a function-name and parameters in parenthesis. the body of the function must be indented. output is in general passed using the return-keyword. for this particular line of code, you can wrap it as such:
from nltk.book import text2

def funcname():
   return sorted([word.lower() for word in text2 if len(word)>4 and len(word)<12])

where funcname can be replaced with any other word, preferably something that describes what the function does more precisely.
to use the function you would add a linefuncname(). the function will then be executed, after execution, the program returns to the line where funcname was called and replaces it with the return-value of the function.
you can find more information about functions in the documentation.",https://stackoverflow.com/questions/70186410,python,01-12-2021 14:56,62.0,-2.0,3.0,True,26-02-2025 20:34,01-12-2021 15:00
76413943,make whisper use the last 30 sec chunk (and not the first),"according to whisper, the notion is as follows:

internally, the transcribe() method reads the entire file and processes the audio with a sliding 30-second window, performing autoregressive sequence-to-sequence predictions on each window.

it is mentioned that only the first 30-sec window is considered for further analysis (and thus language allocation). however, what if i would like to take into account (for the language allocation task) only the last 30-sec window? what could be the possible solution for the task?","['python', 'nlp', 'openai-whisper']",76605404,"the answer is simple: to detect the language of a file from the last 30 secs (and not the first ones), one can do the following:
# make log-mel spectrogram and move to the same device as the model
mel = whisper.log_mel_spectrogram(audio[-480000:]).to(model.device)

# detect the spoken language
_, probs = model.detect_language(mel)
print(f""detected language: {max(probs, key=probs.get)}"")",https://stackoverflow.com/questions/76413943,python,06-06-2023 10:58,656.0,1.0,1.0,True,03-07-2023 13:51,06-06-2023 12:09
75710776,huggingface gpt2 loss understanding,"(also posted here 
i am getting stuck with understanding the gpt2 loss.  i want to give the model the label having the target it will generate so that i can see that loss is zero.
i have a input text
input_text  = ""welcome to new york""
the current model predicts the next word as city
the loss will never be zero if i give the label as input_text. how do i simulate giving the label ""welcome to new york city"" so that the internal neural net (irrespective of the model) will give a loss of zero or near that?
to explain more what i mean, here is the snippet.
note - i have read the forum and documents that the labels can be the same as the input text, that the model will shift left the labels, and that the loss is not calculated for the last token. but then still loss should become zero, which it is not.

labels for language modeling. note that the labels are shifted inside the model,
i.e. you can set labels = input_ids....

from transformers import gpt2lmheadmodel, gpt2tokenizer

model_name = 'gpt2'
tokenizer = gpt2tokenizer.from_pretrained(model_name,model_max_length=1024,padding_side='left')
tokenizer.pad_token = tokenizer.eos_token # == <|endoftext|> = 50256
model = gpt2lmheadmodel.from_pretrained(model_name)

batch_size=5
input_text  = ""<|endoftext|> welcome to new york""
target_text = ""welcome to new york city""

# encode the inputs
encoding = tokenizer(input_text,padding=true,max_length=batch_size,truncation=true,return_tensors=""pt"",)
input_ids, attention_mask = encoding.input_ids, encoding.attention_mask
# encode the targets
target_encoding = tokenizer(target_text,padding=true, max_length=batch_size, truncation=true,return_tensors=""pt"",)
labels = target_encoding.input_ids
# replace padding token id's of the labels by -100 so it's ignored by the loss
labels[labels == tokenizer.pad_token_id] = -100  # in our case there is no padding
print(f""input_ids={input_ids}"")
print(f""attention_mask={attention_mask}"") # all ones
print(f""labels ={labels}"")
# forward pass
outputs = model(input_ids=input_ids,labels=labels) 
print(f""model loss {outputs.loss}"")
# test the model to check what it predicts next
outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask,max_new_tokens=1)
answer = tokenizer.decode(outputs[0], skip_special_tokens=false)
print(f""result '{answer}'"")

output
input_ids=tensor([[50256, 19134,   284,   968,  1971]]) # not sure what eostoken (50256) in input does to model
attention_mask=tensor([[1, 1, 1, 1, 1]])
labels =tensor([[14618,   284,   968,  1971,  2254]]) # 2254 = city;  which is that the model should predict
model loss 8.248174667358398
setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
result '<|endoftext|> welcome to new york city'

when i try something proper as is done everywhere
input_text  = ""welcome to new york""
target_text = input_text

i get a loss of about  3.26
input_ids=tensor([[14618,   284,   968,  1971]]) # 1971 = york
attention_mask=tensor([[1, 1, 1, 1]])
labels =tensor([[14618,   284,   968,  1971]])

model loss 3.2614505290985107
setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
result 'welcome to new york city'

is it that
outputs = model(input_ids=input_ids, labels=labels) 

is generating more than 1 token.
updated-
based on the answer by jindfitch - putting it here as the so moderators have delted when i try to add that as answer.

you try to fine-tune the model to be absolutely sure that city will follow with 100% probability

i trained  the gpt2 with this particular text (trained only the last 2 layers and froze the others) and took the model whose loss was the lowest and used that tested again, and sure enough, the loss was much lower - model loss 0.01076329406350851
for anyone else who would like to follow. the training code is below.
note training with this small text and the way i have done i am not really fully sure if it is proper, as the training loss seemed to jump around a bit (that is increased after some epochs, i this case epoch 8)
2023-03-12 16:03:20,579 [info] epoch 7 complete. loss: 0.18975284695625305 saving ./test/gpt2-epoch-8-2023-03-12 16:02:19.289492
2023-03-12 16:03:20,985 [info] epoch 9 of 10
2023-03-12 16:03:27,655 [info] epoch 8 complete. loss: 0.3775772750377655 saving ./test/gpt2-epoch-9-2023-03-12 16:02:19.289492
2023-03-12 16:03:27,655 [info] epoch 10 of 10
2023-03-12 16:03:34,140 [info] epoch 9 complete. loss: 6.827305332990363e-05 saving ./test/gpt2-epoch-10-2023-03-12 16:02:19.289492

training script - 
training output log 
training data
welcome to new york city  (space in the end)

eval script - 
i removed the token corresponding to 'city' from input-ids when giving the model to generate
# remove the last token off for input-id's as well as attention mask
input_ids = input_ids[:,:-1] # input_text  = ""welcome to new york""
attention_mask = attention_mask[:,:-1]
print(f""input_ids={input_ids}"")
outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask,max_new_tokens=1)

eval script output
python3 ./older/gpt2_loss_learn.py 
input_ids=tensor([[14618,   284,   968,  1971,  2254]])
attention_mask=tensor([[1, 1, 1, 1, 1]])
labels =tensor([[14618,   284,   968,  1971,  2254]])
model loss 0.01076329406350851
input_ids=tensor([[14618,   284,   968,  1971]])
setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
result 'welcome to new york city'

a much more illustrative example","['pytorch', 'huggingface-transformers', 'gpt-2']",75712209,"the default loss function is negative log-likelihood. the actual model output is not the token city but a categorical distribution over the entire 50k vocabulary. depending on the generation strategy, you either sample from these distributions or take the most probable token.
the token city, apparently the most probable one, gets some probability, and the loss is then minus the logarithm of this probability. loss close to zero would mean the token would get a probability close to one. however, the token distribution also considers many plausible but less likely follow-ups. loss 3.26 corresponds to the probability of exp(-3.26), approximately 3.8%. it seems small, but in a 50k vocabulary, it is approximately 2000 times more probable than a random guess.
you can try to fine-tune the model to be absolutely sure that city will follow with 100% probability, but it would probably break other language modeling capabilities.",https://stackoverflow.com/questions/75710776,pytorch,12-03-2023 02:34,8695.0,6.0,1.0,True,25-08-2023 10:09,25-08-2023 10:09
73113261,the essence of learnable positional embedding? does embedding improve outcomes better?,"i was recently reading the bert source code from the hugging face project. i noticed that the so-called ""learnable position encoding"" seems to refer to a specific nn.parameter layer when it comes to implementation.
def __init__(self):
    super()
    positional_encoding = nn.parameter()
def forward(self, x):
    x += positional_encoding

ï¿½ï¿½ï¿½ could be this feeling, then performed the learnable position encoding. whether that means it's that simple or not, i'm not sure i understand it correctly, i want to ask someone with experience.
in addition, i noticed a classic bert structure whose location is actually coded only once at the initial input. does this mean that the subsequent bert layers, for each other, lose the ability to capture location information?
bertmodel(
  (embeddings): bertembeddings(
    (word_embeddings): embedding(30522, 768, padding_idx=0)
    (position_embeddings): embedding(512, 768)
    (token_type_embeddings): embedding(2, 768)
    (layernorm): layernorm((768,), eps=1e-12, elementwise_affine=true)
    (dropout): dropout(p=0.1, inplace=false)
  )
  (encoder): bertencoder(
    (layer): modulelist(
      (0): bertlayer(...)
      ...
  (pooler): bertpooler(...)

would i get better results if the results of the previous layer were re-positional encoded before the next bert layer?","['deep-learning', 'pytorch', 'bert-language-model', 'transformer-model']",73118125,"what is the purpose of positional embeddings?
in transformers (bert included) the only interaction between the different tokens is done via self-attention layers. if you look closely at the mathematical operation implemented by these layers you will notice that these layers are permutation equivariant: that is, the representation of
""i do like coding""
and
""do i like coding""
is the same, because the words (=tokens) are the same in both sentences, only their order is different.
as you can see, this ""permutation equivariance"" is not a desired property in many cases.
to break this symmetry/equivariance one can simply ""code"" the actual position of each word/token in the sentence. for example:
""i_1 do_2 like_3 coding_4""
is no longer identical to
""do_1 i_2 like_3 coding_4""
this is the purpose of positional encoding/embeddings -- to make self-attention layers sensitive to the order of the tokens.
now to your questions:

learnable position encoding is indeed implemented with a simple single nn.parameter. the position encoding is just a ""code"" added to each token marking its position in the sequence. therefore, all it requires is a tensor of the same size as the input sequence with different values per position.
is it enough to introduce position encoding once in a transformer architecture?  yes! since transformers stack multiple self-attention layers it is enough to add positional embeddings once at the beginning of the processing. the position information is ""fused"" into the semantic representation learned per token.
a nice visualization of this effect in vision transformers (vit) can be found in this work:
shir amir, yossi gandelsman, shai bagon and tali dekel deep vit features as dense visual descriptors (arxiv 2021).
in sec. 3.1 and fig. 3 they show how the position information dominates the representation of tokens at early layers, but as you go deeper in a transformer, semantic information takes over.",https://stackoverflow.com/questions/73113261,deep-learning,25-07-2022 17:37,11011.0,7.0,2.0,True,30-08-2023 07:29,26-07-2022 10:04
70606666,solving &quot;cuda out of memory&quot; when fine-tuning gpt-2 (huggingface),"i get the reoccuring cuda out of memory error when using the huggingface transformers library to fine-tune a gpt-2 model and can't seem to solve it, despite my 6 gb gpu capacity, which i thought should be enough for fine-tuning on texts. the error reads as follows:
file ""gpt\lib\site-packages\torch\nn\modules\module.py"", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  file ""gpt\lib\site-packages\transformers\modeling_utils.py"", line 1763, in forward
    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
runtimeerror: cuda out of memory. tried to allocate 144.00 mib (gpu 0; 6.00 gib total capacity; 4.28 gib already allocated; 24.50 mib free; 4.33 gib reserved in total by pytorch) if reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  see documentation for memory management and pytorch_cuda_alloc_conf

i already set batch size to as low as 2 and reduced training examples without success. i also tried to migrate the code to colab, where the 12gb ram were quickly consumed.
my examples are rather long, some counting 2.400 characters, but they should be truncated by the model automatically. my (german) examples look like this:
 er geht in fremde wohnungen, balgt sich mit freund und feind, ist
zudringlich zu unsern sï¿½ï¿½mereien und kirschen.  wenn die gesellschaft nicht groï¿½ï¿½
ist, lasse ich sie gelten und streue ihnen sogar getreide.  sollten sie hier
aber doch zu viel werden, so hilft die windbï¿½ï¿½chse, und sie werden in den
meierhof hinabgescheucht.  als einen bï¿½ï¿½sen feind zeigte sich der rotschwanz.  er
flog zu dem bienenhause und schnappte die tierchen weg.  da half nichts, als ihn
ohne gnade mit der windbï¿½ï¿½chse zu tï¿½ï¿½ten.

 ich wollte
ihnen mein wort halten, liebe mama, aber die versuchung war zu groï¿½ï¿½.  da bin ich
eines abends in den keller gegangen und hab' aus allen fï¿½ï¿½ssern den spund
herausgeklopft.  bis auf den letzten tropfen ist dasrn.  der schade war groï¿½ï¿½, aber der teufel war aus dem haus. ï¿½ï¿½

andor lachte.  ï¿½ï¿½mama, das geschrei hï¿½ï¿½tten sie hï¿½ï¿½ren sollen! als ob der
weltuntergang gekommen wï¿½ï¿½re. er bedauerte beinahe seine
schroffheit.  nun, nachlaufen wird er ihnen nicht, die werden schon selber
kommen.  aber bewachen wird er seine kolonie bei tag und bei nacht lassen
mï¿½ï¿½ssen.  hol' der teufel diesen mercy.  muï¿½ï¿½ der gerade in hï¿½ï¿½gyï¿½ï¿½sz ein kastell
haben.  wenn einer von den schwarzwï¿½ï¿½ldern dahin kommt und ihn verklagt.

is there a problem with the data formatting maybe?
if anyone has a hint on how to solve this, it would be v"" calin for the answer, i described in the comment how adding the block_size flag to the config.json solved the problem. here is the whole configuration for reference:
{
    ""model_name_or_path"": ""dbmdz/german-gpt2"",
    ""train_file"": ""fine-tuning dataset/train.txt"",
    ""validation_file"": ""fine-tuning dataset/test.txt"",
    ""output_dir"": ""models"",
    ""overwrite_output_dir"": true,
    ""per_device_eval_batch_size"": 8,
    ""per_device_train_batch_size"": 8,
    ""block_size"": 100, 
    ""task_type"": ""text-generation"",
    ""do_train"": true,
    ""do_eval"": true
}","['python', 'pytorch', 'nlp', 'huggingface-transformers', 'huggingface']",70607817,"if the memory problems still persist, you could opt for
distillgpt2, as it has a 33% reduction in the parameters of the
network (the forward pass is also twice as fast). particularly for a small gpu memory like 6gb vram, it could
be a solution/alternative to your problem.
at the same time, it depends on how you preprocess the data. indeed,
the model is capable of ""receiving"" a maximum length of n tokens
(could be for example 512/768) depending on the models you choose. i
recently trained a named entity recognition model and the model
had a maximum length of 768 tokens. however, when i manually set the
dimension of the padded tokens in my pytorch dataloader() to a big
number, i also got oom memory (even on 3090 24gb vram). as i reduced
the dimension of the tokens to a much smaller one (512 instead of
768 for example) the training started to work and i did not get
any issues with the lack of memory.

tldr: reducing the number of tokens in the preprocessing phase, regardless of the max capacity of the network, can also help to solve your memories problem.
note that reducing the number of tokens to process in a sequence is different from the dimension of a token.",https://stackoverflow.com/questions/70606666,python,06-01-2022 11:49,4132.0,3.0,1.0,True,15-04-2023 06:02,15-04-2023 06:02
72014538,how to get prediction label and percentage from pipeline?,"i am using the following hugging face transformer code.
from transformers import pipeline
classifier = pipeline(""sentiment-analysis"",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=true)
prediction = classifier(""i love using transformers. the best part is wide range of support and its easy to use"", )
print(prediction)

the result is:
[[{'label': 'sadness', 'score': 0.010154823772609234}, {'label': 'joy', 'score': 0.5637667179107666}, {'label': 'love', 'score': 0.4066571295261383}, {'label': 'anger', 'score': 0.01734882965683937}, {'label': 'fear', 'score': 0.0011737244203686714}, {'label': 'surprise', 'score': 0.0008987095206975937}]]

i would like to know how can i get the highest score together with the label, but i am not sure how to iterate this object or if there is an easy way with expressions.","['python', 'huggingface-transformers']",72014801,"you are using a textclassificationpipeline. when you __call__ the pipeline you get a list of dict if top_k=0 or a list of list of dict if top_k=none as per the documentation.
you can either set top_k to 0 (the default value) and then access the values you want - in this case you will only get the score and text of the highest scoring label:
from transformers import pipeline
classifier = pipeline(""sentiment-analysis"",model='bhadresh-savani/distilbert-base-uncased-emotion', top_k=0)
prediction = classifier(""i love using transformers. the best part is wide range of support and its easy to use"")
print(prediction[0][""label""], prediction[0][""score""])

or, if you want the scores for all labels, and then access only the highest scoring label (top_k=0):
from transformers import pipeline
classifier = pipeline(""sentiment-analysis"",model='bhadresh-savani/distilbert-base-uncased-emotion', top_k=none)
prediction = classifier(""i love using transformers. the best part is wide range of support and its easy to use"")
print(max(prediction[0], key=lambda k: k[""score""]))",https://stackoverflow.com/questions/72014538,python,26-04-2022 13:08,1413.0,3.0,1.0,True,11-03-2024 07:51,27-04-2022 16:17
73107703,issue when importing bloomtokenizer from transformers in python,"i am trying to import bloomtokenizer from transformers
from transformers import bloomtokenizer

and i receive the following error
traceback (most recent call last):
  file ""<stdin>"", line 1, in <module>
importerror: cannot import name 'bloomtokenizer' from 'transformers' 
(/root/miniforge3/envs/pytorch/lib/python3.8/site-packages/transformers/__init__.py)

my version of transformers:
transformers                 4.20.1

what could i do to be able to import bloomtokenizer?","['python', 'nlp', 'huggingface-transformers', 'huggingface-tokenizers', 'huggingface']",73108760,"bloom has no slow tokenizer class. it only has a fast tokenizer. the official documentation is wrong at this point. use the following instead:
 from transformers import bloomtokenizerfast
 tokenizer = bloomtokenizerfast.from_pretrained(""..."")",https://stackoverflow.com/questions/73107703,python,25-07-2022 10:27,2641.0,1.0,1.0,True,02-03-2023 05:41,02-03-2023 05:41
75595699,huggingface&#39;s berttokenizerfast is between 39000 and 258300 times slower than expected,"as part of training a bert model, i am tokenizing a 600mb corpus, which should apparently take approx. 12 seconds. i tried this on a computing cluster and on a google colab pro server, and got time estimates ranging from 130 to 861 hours.
here's the minimal working example (most of the values aren't hard-coded, but i specified the ones i use most of the time here for simplicity):
training_args = trainingarguments(
    output_dir=args.output_dir,
    overwrite_output_dir=true,
    num_train_epochs=1,
    per_gpu_train_batch_size=512,
    save_steps=10_000,
    save_total_limit=2,
    prediction_loss_only=true,
    learning_rate=2e-5,
    weight_decay=0.15,
    push_to_hub=false,
    gradient_accumulation_steps=4
)

dataset = load_dataset(
    ""text"",
    data_files=""mycorpus.txt"")['train'].shuffle(seed=42)

tokenizer = autotokenizer.from_pretrained(""bert-base-uncased"")
model = bertformaskedlm.from_pretrained(""bert-base-uncased"")

# code stolen from 
# except i replaced the tokenization function with a lambda
tokenized_dataset = dataset.map(
    lambda examples: tokenizer(examples[""text""]),
    batched=true,
    num_proc=4,
    remove_columns=[""text""])

lm_dataset = tokenized_dataset.map(
    group_texts,
    batched=true,
    batch_size=512,
    num_proc=4
)
# /steal

data_collator = datacollatorforlanguagemodeling(
    tokenizer=tokenizer, mlm=true, mlm_probability=0.15
)

model_trainer = trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=lm_dataset
)
model_trainer.train()

having traced the execution path in pdb, the issue arises in the call to model_trainer.train(), which i guess ends up calling the lambda used in the declaration of tokenized_dataset.
i do get the following message:
you're using a berttokenizerfast tokenizer. please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

however, i do believe my lambda calls the __call__ function implicitly, does it not? can i do something about this?
that said, i'm doubtful that this warning message is relevant, as it seems to imply a relatively minor slowdown. i feel like it would have a more dramatic tone if it were related to the staggering difference that i'm observing.","['performance', 'huggingface-transformers', 'huggingface-tokenizers', 'huggingface-datasets']",75618063,"turns out, the log message about berttokenizerfast had nothing to do with the progress bar that appeared right after, which i thought was the tokenization progress bar but was in fact the training progress bar. the actual problem was that the model was training on cpu instead of gpu. i thought i had ruled this out because i had verified that torch.cuda.isavailable() == true and huggingface trainers are supposed to use cuda if available. however, the installed version of pytorch was incorrect for my version of cuda and despite cuda being ""available"", pytorch refused to use the gpu, making huggingface default back to cpu training. all of this was silent and caused no warnings or error messages.",https://stackoverflow.com/questions/75595699,performance,28-02-2023 17:57,3863.0,1.0,1.0,True,02-03-2023 16:12,01-03-2023 15:51
74796190,rasa: training is too slow,i have rtx 3090 gpu and i9 12th gen processor. my training is not too large as well and yet the training time is too long. when i begin the training phase it says 24 cores available but limiting to safe limit of only 8 cores. numexpr_max_threads not set.,"['chatbot', 'rasa', 'rasa-nlu', 'rasa-core', 'rasa-x']",74890000,"in your terminal add the numexpr_max_threads to your terminal.
you can do so by writing in your cli:  export numexpr_max_threads=""24"" if you want to use all of them. this will work until you close your terminal. you can add it permanently to your terminal profile (.bash_profile, ~/.zshrc ...)
regarding slow execution, that depends on your rasa config choices and the number of stories/rules.
finally, you need to pass the param use_gpu = true in your config for tedpolicy t make it train ted faster.",https://stackoverflow.com/questions/74796190,chatbot,14-12-2022 09:38,1005.0,0.0,1.0,True,22-12-2022 14:41,14-12-2022 12:35
64799622,how is the gpt&#39;s masked-self-attention is utilized on fine-tuning/inference,"at training time, as far as i understand from the ""attention is all you need"" paper, the way that masked-self-attention is used in the decoder is by feeding the output sequence multiple times, each time removing the mask from the next token.
q1. at inference time, the expected output sequence length is not known. how do you decide on how many masked tokens to add? do you always fill the max-length of your input with masked tokens and stop when an end of sequence symbol is predicted?
q2. the gpt inference objective task is a little different. a ""query"" vector is injected to the model (for example [text1;text2] and [text2;text1] in the similarity task). how is the masking used in this scenario? i would expect that the whole sequence will be injected in only one step with no masking, however this contradicts the masked-self-attention methodology.","['nlp', 'transformer-model', 'large-language-model']",64800837,"in the standard transformer, the target sentence is provided to the decoder only once (you might confuse that with the masked language-model objective for bert).
the purpose of the masking is to make sure that the states do not attend to tokens that are ""in the future"" but only to those ""in the past"". the mask looks like this (queries are on the vertical axis; keys and values on the horizontal axis):
\ 1 2 3 4 5 6
1 ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½
2 ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½
3 ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½
4 ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½
5 ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½
6 ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½

this means that for generating the first token, you cannot attend to anything, when generating the second token, you can attend to states of the first token. at the third ostandard transformer for sequence-to-sequence learning and for decoder-only transformer such as gpt-2.
if you implement the inference efficiently, you do not need the masking. you keep all the previous states in memory, do the attention only with the last query (which corresponds to the newly generated token) and thus get the new states and predict what the next token is. this is done in a loop until you generate the end-of-sentence token.",https://stackoverflow.com/questions/64799622,nlp,12-11-2020 07:31,4502.0,1.0,1.0,True,07-08-2023 21:15,07-08-2023 21:15
78383619,how to generate text with specific length using ai api,"i am attempting to generate text outputs that are exactly a certain number of characters or words long using ai api (openai gpt, claude, gemini...), but i'm facing difficulties. here's what i've tried so far:
setting max tokens: i've used the max_tokens parameter hoping to limit the output length, but then text is truncated.
explicit prompt requests: i've tried including explicit instructions in the prompt about the desired length. however, this approach has not produced the precise output lengths i need (ex: gives 600 words instead of 800).
i am looking for suggestions on how to configure the api calls or promtp to achieve exact output lengths. is there a way to better utilize openai/claudeï¿½ï¿½ï¿½s parameters, or is there a method to post-process the text to fit the required length?
i need this because i inject the text in slides. any alternative solution","['python', 'nlp', 'openai-api', 'text-generation']",78383634,this won't work. llms are bad at counting. there are a lot of humorous examples over internet.,https://stackoverflow.com/questions/78383619,python,25-04-2024 09:30,238.0,-1.0,1.0,True,25-04-2024 11:56,25-04-2024 11:56
79467575,can&#39;t get uuid from similarity search weaviate,"i tried to retrieve documents with similar content to later modify them, but when it came to updating, i realized i couldn't get the id to update these documents.
hereï¿½ï¿½ï¿½s the function i was using to retrieve documents:
def retrieve(text):
    """"""retrieve information related to a query.""""""
    retrieved_docs = db.similarity_search(text, k=3)
    return retrieved_docs

this is how the document structure looks when printed
document(metadata={'projectname': ''}, page_content="""")

and in weaviate:
{
  ""uuid"": """",
  ""metadata"": {
    ""creationtime"": """"
  },
  ""properties"": {
    ""projectname"": """",
    ""text"": """" 
  },
  ""vectors"": {
    ""default"": []}}

and hereï¿½ï¿½ï¿½s the db configuration, don't know, maybe it helps:
db = weaviatev    client=client,
    index_name=""cases"",
    embedding=embedding_model,
    text_key=""text""
)

i haven't found any information about a similar case, everywhere people are interacting with existing uuid. how to get uuid? or maybe thereï¿½ï¿½ï¿½s another way to achieve this? for example, saving information as new each time, but will this compromise accuracy over time?
iï¿½ï¿½ï¿½d appreciate your re","['python', 'langchain', 'vector-database', 'weaviate']",79476553,"duda nogueira from weaviate here!
can you confirm you are using langchain and not llamaindex?
for langchain, you need to explicitly request this with;
docs = db.similarity_search(""traditional food"", return_uuids=true)
print(docs[0].metadata.get(""uuid""))

i just noticed this is an undocumented feature! i have added it here in our langchain recipes to make it visible:

let me know if this helps!
edit: langchain integration will only perform searches. if you want to fetch objects and sort them, this is how you can do it:
from weaviate.classes.query import sort, metadataquery

query = collection.query.fetch_objects(
    sort=sort.by_update_time(ascending=false),
    return_metadata=metadataquery(last_update_time=true),
    limit=3
)
for obj in query.objects:
    print(""#"")
    print(obj.uuid)
    print(obj.metadata.last_update_time)

# update the last object, so it goes back to top
collection.data.update(uuid=query.objects[2].uuid, properties={""text"": ""updated!""})

this should be the output, and everytime you run, the last object will go to the top:
#
032502ec-2598-4941-aa58-9e576308cb9d
2025-03-06 14:31:42.654000+00:00
#
00351c67-a31f-4b90-8a76-198aadf2a1ca
2025-03-06 14:31:32.236000+00:00
#
014735a7-732f-41a5-8026-ab29395b88c3
2025-03-06 14:28:36.576000+00:00

thanks!",https://stackoverflow.com/questions/79467575,python,25-02-2025 18:37,104.0,0.0,2.0,True,10-03-2025 20:20,25-02-2025 19:20
75488355,using the earley library to parse with features and unification,"the earley parsing library is great for writing linguistic parsers in haskell. cfgs can be specified in an intuitive way, and there is excellent support for backtracking and ambiguity. a simple example:
{-# language overloadedstrings #-}

import text.earley

np = rule (""john"" <|> ""mary"")
vp = rule (""runs"" <|> ""walks"")

sentence = do
  subj <- np
  pred <- vp
  return $ (++) <$> subj <*> pred

sentence can be used to parse [""john"", ""runs""] or [""mary"", ""walks""], among other inputs.
it would be nice to be able to use earley to write parsers for fcfgs, where nonterminals are complexes of a label and a feature bundle, and feature matching can happen via unification (for example, the earley parser in nltk parses fcfgs). however, it is not clear how to do this using earley, or whether it can even be done. an example of something we might want in something like bnf:
np[sg] ::= ""john"" | ""mary""

np[?x] ::= det n[?x]
n[pl]  ::= ""boys"" | ""girls""

det    ::= ""the""

vp[sg] ::= ""runs"" | ""walks""
vp[pl] ::= ""run""  | ""walk""

s ::= np[?x] vp[?x]

under this fcfg, [""john"", ""runs""] is an s (since their number features match, as required by the s rule), and [""the"", ""boys"", ""walks""] isn't an s (since [""the"", ""boys""] parses to np[pl] and [""walks""] parses to vp[sg]).
one can in general rewrite an fcfg into an equivalent cfg, but this can be highly inconvenient, and result in a blowup of the grammar, especially when we have many possible features ranging over many possible values.","['parsing', 'haskell', 'nltk', 'earley-parser']",75503036,"you're not actually doing any particularly interesting unification here, so perhaps it's enough to toss a very simple nondeterminism applicative of your own into the mix. the standard one is [], but for this case, even maybe looks like enough. like this:
{-# language overloadedstrings #-}
{-# language typeapplications #-}

import control.applicative
import control.monad
import data.foldable
import text.earley

data feature = sg | pl deriving (eq, ord, read, show)

(=:=) :: (feature, a) -> (feature, b) -> maybe (a, b)
(fa, a) =:= (fb, b) = (a, b) <$ guard (fa == fb)

data np = name string | determined string string deriving (eq, ord, read, show)

np :: grammar r (prod r e string (feature, np))
np = rule . asum $
    [ fmap (\name -> (sg, name name)) (""john"" <|> ""mary"")
    , lifta2 (\det n -> (pl, determined det n)) ""the"" (""boys"" <|> ""girls"")
    ]

vp :: grammar r (prod r e string (feature, string))
vp = rule . asum $
    [ (,) sg <$> (""runs"" <|> ""walks"")
    , (,) pl <$> (""run"" <|> ""walk"")
    ]

s :: grammar r (prod r e string (maybe (np, string)))
s = lifta2 (lifta2 (=:=)) np vp

test :: [string] -> io ()
test = print . allparses @() (parser s)

try it out in ghci:
> sequence_ [test (words n ++ [v]) | n <- [""john"", ""the boys""], v <- [""walks"", ""walk""]]
([(just (name ""john"",""walks""),2)],report {position = 2, expected = [], unconsumed = []})
([(nothing,2)],report {position = 2, expected = [], unconsumed = []})
([(nothing,3)],report {position = 3, expected = [], unconsumed = []})
([(just (determined ""the"" ""boys"",""walk""),3)],report {position = 3, expected = [], unconsumed = []})

so, the result needs a bit of interpretation -- a successful parse of nothing really counts as a failed parse -- but perhaps that's not so bad? not sure. certainly it's unfortunate that you don't get to reuse earley's error-reporting and nondeterminism machinery. probably to get either thing, you'd have to fork earley.
if you need to do real unification you could look into returning a intbindingt t identity instead of a maybe, but at least until your features are themselves recursive this is probably enough and much, much simpler.",https://stackoverflow.com/questions/75488355,parsing,17-02-2023 19:05,165.0,1.0,1.0,True,20-02-2023 17:25,18-02-2023 12:36
72214408,why does huggingface t5 tokenizer ignore some of the whitespaces?,"i am using t5 model and tokenizer for a downstream task. i want to add certain whitespaces to the tokenizer like line ending (\t) and tab (\t). adding these tokens work but somehow the tokenizer always ignores the second whitespace. so, it tokenizes the sequence ï¿½ï¿½ï¿½\n\nï¿½ï¿½ï¿½ as a single line ending and the sequence ""\n\n\n\n"" is tokenized as two line endings and so on. see below to reproduce.
from transformers import t5tokenizer
tokenizer = t5tokenizer.from_pretrained(""t5-large"")
tokenizer.add_tokens([""\n""])

tokenizer.encode(""\n"") # returns [32100, 1] as expected
tokenizer.encode(""\n\n"") # returns [32100, 1] but expected would be [32100, 32100, 1]
tokenizer.encode(""\n\n\n\n"") # returns [32100, 32100, 1] but expected would be [32100, 32100, 32100, 32100, 1]

what is the reasoning behind this behaviour? is it a bug or something related to how tokenizer work that this only happens for added whitespaces but not for other characters.
is there way to prevent tokenizer from ignoring the repeated whitespaces?","['huggingface-transformers', 'huggingface-tokenizers', 'sentencepiece']",72305836,"the behaviour is explained by how the tokenize method in t5tokenizer strips tokens by default. what one can do is adding the token '\n' as a special token to the tokenizer. because the special tokens are never seperated, it works as expected.
it is a bit hacky but seems to work.
from tokenizers import addedtoken
tokenizer.add_special_tokens({""additional_special_tokens"": [addedtoken(""\n"")]})
print(tokenizer.special_tokens_map)

then it tokenizes the '\n' without skipping any occurences. note that addedtoken is important because somehow the following does not work.
tokenizer.add_special_tokens({""additional_special_tokens"": [""\n""]})

edit
after spending more time on it, i actually found a way to add it as a normal token without using special tokens. the main reason for the issue is the normalization process that happens behind the scenes even before the tokenization. when you add a new token, you can specify if it should be normalized or not. by setting normalize to false, you avoid the tokenizer from stripping consecutive occurrences of the added token.
from tokenizers import addedtoken
tokenizer.add_tokens(addedtoken(""\n"", normalized=false))

you can find more information on this link:",https://stackoverflow.com/questions/72214408,huggingface-transformers,12-05-2022 11:04,4041.0,7.0,1.0,True,13-07-2023 08:46,13-07-2023 08:46
66267633,textual data augmentation in tensorflow,"i'm doing a sentiment analysis on the imdb dataset in tensorflow and i'm trying to augment the training dataset by using the textaugment library which they said is 'plug and play' into tensorflow. so it should be rather simple, but i'm new to tf so i'm not sure how to go about doing that.  here is what i have and what i am trying, based on reading the tutorials on the site.
i tried to do a map to augment the training data but i got an error.  you can scroll down to the last code block to see the error.
pip install -q tensorflow-text
pip install -q tf-models-official
import os
import shutil
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization # to create adamw optimizer
import matplotlib.pyplot as plt

tf.get_logger().setlevel('error')

#downloading the  imdb dataset and making the train/validation/test sets
url = '

dataset = tf.keras.utils.get_file('aclimdb_v1.tar.gz', url,
                                  untar=true, cache_dir='.',
                                  cache_subdir='')

dataset_dir = os.path.join(os.path.dirname(dataset), 'aclimdb')

train_dir = os.path.join(dataset_dir, 'train')

# remove unused folders to make it easier to load the data
remove_dir = os.path.join(train_dir, 'unsup')
shutil.rmtree(remove_dir)


autotune = tf.data.autotune
batch_size = 32
seed = 42

raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclimdb/train',
    batch_size=batch_size,
    validation_split=0.2,
    subset='training',
    seed=seed)

class_names = raw_train_ds.class_names
train_ds = raw_train_ds.cache().prefetch(buffer_size=autotune)

val_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclimdb/train',
    batch_size=batch_size,
    validation_split=0.2,
    subset='validation',
    seed=seed)

val_ds = val_ds.cache().prefetch(buffer_size=autotune)

test_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclimdb/test',
    batch_size=batch_size)

test_ds = test_ds.cache().prefetch(buffer_size=autotune)


#setting up the textaugment
try:
  import textaugment
except modulenotfounderror:
  !pip install textaugment
  import textaugment
from textaugment import eda
import nltk
nltk.download('stopwords')

now this is where i get the error, i tried a map on the train_ds and tried to add a random swap to each of the elements while keeping the class the same:
aug_ds = train_ds.map(
    lambda x, y: (t.random_swap(x), y))

error message:
attributeerror                            traceback (most recent call last)
<ipython-input-24-b4af68cc0677> in <module>()
      1 aug_ds = train_ds.map(
----> 2     lambda x, y: (t.random_swap(x), y))

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    668       except exception as e:  # pylint:disable=broad-except
    669         if hasattr(e, 'ag_error_metadata'):
--> 670           raise e.ag_error_metadata.to_exception(e)
    671         else:
    672           raise

attributeerror: in user code:

    <ipython-input-24-b4af68cc0677>:2 none  *
        lambda x, y: (t.random_swap(x), y))
    /usr/local/lib/python3.6/dist-packages/textaugment/eda.py:187 random_swap  *
        self.validate(sentence=sentence, n=n)
    /usr/local/lib/python3.6/dist-packages/textaugment/eda.py:74 validate  *
        if not isinstance(kwargs['sentence'].strip(), str) or len(kwargs['sentence'].strip()) == 0:

    attributeerror: 'tensor' object has no attribute 'strip'","['tensorflow', 'text', 'nlp', 'training-data', 'data-augmentation']",67245903,"i am also trying to do the same. the error occurs because the textaugment function t.random_swap() is supposed to work on python string objects.
in your code, the function is taking in a tensor with dtype=string. as of now, tensor objects do not have the same methods as python strings. hence, the error code.
nb. tensorflow_text has some additional apis to work with such tensors of string types. albeit, it is limited at the moment to tokenization, checking upper or lower case etc. a long winded workaround is to use the py_function wrapper but this reduces performance. cheers and hope this helps. i opted not to use textaugment in the end in my use case.
nbb. tf.strings apis have a bit more functionalities, such as regex replace etc but it is not complicated enough for your use case of augmentation. would be helpful to see what others come up with, or if there are future updates to either tf or textaugment.",https://stackoverflow.com/questions/66267633,tensorflow,18-02-2021 20:24,472.0,0.0,1.0,True,24-04-2021 18:21,18-02-2021 20:52
77805776,how to calculate word and sentence embedding using roberta?,"i'm trying to calculate word and sentence embeddings using roberta, for word embeddings, i extract the last hidden state outputs[0] from the robertamodel class, but i'm not sure if this is the correct way to calculate.
as for sentence embeddings, i don't know how to calculate them, this is the code i have tried:
from transformers import robertamodel, robertatokenizer
import torch

model = robertamodel.from_pretrained('roberta-base')
tokenizer = robertatokenizer.from_pretrained('roberta-base')
captions = [""example caption"", ""lorem ipsum"", ""this bird is yellow has red wings"", ""hi"", ""example""]

encoded_captions = [tokenizer.encode(caption) for caption in captions]

# pad sequences to the same length with 0s
max_len = max(len(seq) for seq in encoded_captions)
padded_captions = [seq + [0] * (max_len - len(seq)) for seq in encoded_captions]

# convert to a pytorch tensor with batch size 5
input_ids = torch.tensor(padded_captions)

outputs = model(input_ids)
word_embedding = outputs[0].contiguous()
sentence_embedding = ?????

how to calculate word and sentence embeddings using roberta?","['python', 'machine-learning', 'nlp', 'huggingface-transformers', 'transformer-model']",77812267,"warning: this answer only shows ways to retrieve word and sentence embeddings from a technical perspective as requested by op in the comments. the respective embeddings will not be useful from a performance perspective to for example calculate the similarity between two sentences or words. compare this so answer for further information.
word embeddings
it is important to note, that roberta was trained with a byte-level bpe tokenizer. this is a so-called subword tokenizer which means that one word of your input string can be split into several tokens. for example your second caption lorem ipsum:
from transformers import robertamodel, robertatokenizerfast
import torch

m = robertamodel.from_pretrained('roberta-base')
t = robertatokenizerfast.from_pretrained('roberta-base')
captions = [""example caption"", ""lorem ipsum"", ""this bird is yellow has red wings"", ""hi"", ""example""]

print(t(captions[1]).input_ids)

output:
[0, 462, 43375, 1437, 7418, 783, 2]

as you can see the two words were mapped to 5 tokens (0 and 2 are special tokens). that means to retrieve the actual word embeddings and not the token embeddings, you need to apply some kind of aggregation. a common approach is applying mean pooling (compare this so answer). using the respective fast tokenizer of the model helps you here because it returns a batchencoding object that can be used to map the tokens back to the respective words:
# no need to pad manually, the tokenizer can do that for you
tokenized_captions = t(captions, return_tensors='pt', padding='longest')

with torch.inference_mode():
  model_inference_output = m(**tokenized_captions)
  contextualized_token_embeddings = model_inference_output.last_hidden_state

#properly padded
print(contextualized_token_embeddings.shape)

def fetch_word_embeddings(idx, sentence, tokenized_captions, contextualized_token_embeddings):
  word_embeddings = {}
  # fetching word_ids, each id is a word in the original sentence
  word_ids = {i for i in tokenized_captions[idx].word_ids if i is not none}

  for word_id in word_ids:
    token_start, token_end = tokenized_captions[idx].word_to_tokens(word_id)
    word_start, word_end =  tokenized_captions[idx].word_to_chars(word_id)

    word=sentence[word_start:word_end]
    word_embeddings[word] = contextualized_token_embeddings[idx][token_start:token_end].mean(dim=0)
  
  return word_embeddings

result = []
for idx, sentence in enumerate(captions):
  word_embeddings = fetch_word_embeddings(idx, sentence, tokenized_captions, contextualized_token_embeddings)
  result.append({""sentence"": sentence, ""word_embeddings"":word_embeddings})

# contextualized word embedding of the word `ipsum` of the second caption   
print(result[1]['word_embeddings']['ipsum'].shape)

output:
torch.size([5, 9, 768])
torch.size([768])

sentence embeddings
sentence embeddings represent the whole sentence in a vector. there are different strategies to retrieve them. commonly used are mean or cls-pooling, with mean-pooling delivering better results as shown in this paper section 6. the ""only"" challenge from a technical perspective (compare warning preamble) is, that you want to exclude the padding tokens:
# has 1 for none-padding-tokens and 0 for padding-tokens
attention_mask = tokenized_captions.attention_mask.unsqueeze(-1)

# mutiply the contextualized embeddings with the attention mask to 
# set the padding token weights to zero  
sum_embeddings = torch.sum(contextualized_token_embeddings * attention_mask,1)
print(sum_embeddings.shape)
num_none_padding_tokens = attention_mask.sum(1)
print(num_none_padding_tokens)
sentence_embeddings = sum_embeddings / num_none_padding_tokens
print(sentence_embeddings.shape)  

output:
torch.size([5, 768])
tensor([[4],
        [7],
        [9],
        [3],
        [3]])
torch.size([5, 768])

you also wanted to know in the comments if you could use the pooler_output of roberta-base directly to retrieve the sentence embeddings. yes, you can do that. the pooler_output is retrieved via a form of cls-pooling (code).
please note in addition to the warning preamble that the layers used for to generate the pooler_output are randomly initialized (i.e. untrained) for the roberta-base weights you load. that means they are even less meaningful!
some weights of robertamodel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']",https://stackoverflow.com/questions/77805776,python,12-01-2024 10:05,2472.0,1.0,3.0,True,13-01-2024 17:20,12-01-2024 12:21
70049581,labelling for analysis sentiment with file,"i have a data called:

after_tokenize.xlsx
positive.xlsx
negative.xlsx
after tokenize
positive
negative

what i want to is labelling sentiment positive and negative for data from after_tokenize.xlsx. if data on after tokenize have a lot of positive word from data positive.xlsx it will be positive and if data have a lot negative word from negative it will be negative. the result will be entered into a label named label.
sample:




data
label




[i, like, love, hate, you]
positive


[i, worst, hate, like, you]
negative




import pandas as pd
import nltk

df = pd.dataframe({'data': ['i like love hate you', 'i dont hate like you']})
pos = pd.dataframe(data=['like', 'love'], columns=['positive'])
neg = pd.dataframe(data=['dont', 'hate'], columns=['negative'])
df['data'] = df.apply(lambda row: nltk.word_tokenize(row['data']), axis=1)","['python', 'pandas', 'nlp']",70055738,"you can use set() and operation set(...) & set(...) to get words which are in two lists.
and then you can count them using len()
len( set([i, like, love, hate, you]) & set(['like', 'love']) ) 


import pandas as pd
import nltk

df = pd.dataframe({'data': ['i like love hate you', 'i dont hate like you']})

pos = ['like', 'love']
neg = ['dont', 'hate']

#print(df)

df['data'] = df['data'].apply(nltk.word_tokenize)

# --- get common words ---

df['pos words'] = df['data'].apply(lambda item: list(set(item) & set(pos)))
df['neg words'] = df['data'].apply(lambda item: list(set(item) & set(neg)))

# --- count common words ---

df['pos'] = df['data'].apply(lambda item: len(set(item) & set(pos)))
df['neg'] = df['data'].apply(lambda item: len(set(item) & set(neg)))

# or

df['pos'] = df['pos words'].apply(len)
df['neg'] = df['neg words'].apply(len)

# --- assing labels ---

df['label'] = '???'  # default value 

#df.['label'][ df['pos'] > df['neg'] ] = 'positive'
df.loc[ (df['pos'] > df['neg']), 'label' ] = 'positive'

#df.['label'][ df['pos'] < df['neg'] ] = 'negative'
df.loc[ (df['pos'] < df['neg']), 'label' ] = 'negative'

# ---

print(df)

result:
                         data     pos words     neg words  pos  neg     label
0  [i, like, love, hate, you]  [love, like]        [hate]    2    1  positive
1  [i, dont, hate, like, you]        [like]  [hate, dont]    1    2  negative",https://stackoverflow.com/questions/70049581,python,20-11-2021 20:23,114.0,-1.0,1.0,True,21-11-2021 16:26,21-11-2021 07:38
66751457,meaningless spacy nouns,"i am using spacy for extracting nouns from sentences. these sentences are grammatically poor and may contain some spelling mistakes as well.
here is the code that i am using:
code
import spacy
import re

nlp = spacy.load(""en_core_web_sm"")

sentence= ""handbrake - slow and fast (sfx)""
string= sentence.lower()
cleanstring = re.sub('\w+',' ', string )
cleanstring=cleanstring.replace(""_"", "" "")

doc= nlp(cleanstring)

for token in doc:
    if token.pos_==""noun"":
        print (token.text)
 

output:
sfx

similarly for sentence ""fast foward2"", i get spacy noun as
foward2

which shows that these nouns have some meaningless words like: sfx, foward2, ms, 64x, bit, pwm, r, brailledisplayfastmovement, etc.
i only want to keep phrases that contain sensible single-word nouns like broom, ticker, pool, highway etc.
i have tried wordnet to filter common nouns between wordnet and spacy but it is a bit strict and filter some sensible nouns as well. for example, it filters nouns like motorbike, whoosh, trolley, metal, suitcase, zip etc
therefore, i am looking for a solution in which i can filter out most sensible nouns from spacy nouns list that i have obtained.","['python', 'text', 'spacy', 'wordnet']",66754172,"it seems you can use pyenchant library:

enchant is used to check the spelling of words and suggest corrections for words that are miss-spelled. it can use many popular spellchecking packages to perform this task, including ispell, aspell and myspell. it is quite flexible at handling multiple dictionaries and multiple languages.
more information is available on the enchant website:


sample python code:
import spacy, re
import enchant                        #pip install pyenchant

d = enchant.dict(""en_us"")
nlp = spacy.load(""en_core_web_sm"")

sentence = ""for example, it filters nouns like motorbike, whoosh, trolley, metal, suitcase, zip etc""
cleanstring = re.sub('[\w_]+',' ', sentence.lower()) # merging \w and _ into one regex

doc= nlp(cleanstring)
for token in doc:
    if token.pos_==""noun"" and d.check(token.text):
        print (token.text)
# => [example, nouns, motorbike, whoosh, trolley, metal, suitcase, zip]",https://stackoverflow.com/questions/66751457,python,22-03-2021 17:59,609.0,4.0,2.0,True,16-02-2022 21:46,15-12-2021 19:03
72284795,how to access a row in a pandas dataframe with custom index labels?,"i've created a pandas dataframe to hold the data like so:
tf_idf = pd.dataframe(index = vocab)
tf_idf['0'] = 0

where vocab is an array of strings, so the index label of each column is the corresponding string
so when i print it, it looks like this:
           0
life       0
math       0
student    0
experi     0
control    0
...       ..
slave      0
linga      0
31-32      0
democrat   0
unsustain  0

how would i access a row at a given index by using a string?
for example, if i have the string ""math"" how would one access the value using the index label ""math""?","['python', 'pandas', 'dataframe', 'nlp', 'tf-idf']",72289074,"you should use .loc[] rather than chained indexing (as in your answer) as it's more efficient, and chained indexing can sometimes raise a settingwithcopy warning (read more here).
to use .loc, you would call it like below:
df.loc[row_index(es), col_names]

which would be the below in your example:
tf_idf.loc['life', '0']

returns:
0",https://stackoverflow.com/questions/72284795,python,18-05-2022 07:24,2657.0,0.0,2.0,True,21-09-2022 03:14,21-09-2022 03:14
75928704,swift api call decoding issue: decoding error,"last week i made a chatgpt application using this tutorial:  . it worked just fine, but when i started working on it again today it didn't work. this uses the openaiswift package.
after 'debugging' it a bit i got this error:

failure(openaiswift.openaierror.decodingerror(error:
swift.decodingerror.keynotfound(codingkeys(stringvalue: ""object"",
intvalue: nil), swift.decodingerror.context(codingpath: [],
debugdescription: ""no value associated with key
codingkeys(stringvalue: ""object"", intvalue: nil) (""object"")."",
underlyingerror: nil))))

the code i use is almost the same as in the video, the only difference is that i trim white spaces and new lines (this isn't the issue i checked)
my code (that has the error) looks like this:
import openaiswift
import swiftui    
final class viewmodel: observableobject{
    init(){}
    
    private var client: openaiswift?
    
    func setup(){
        client = openaiswift(authtoken: ""my_api_key"")
    }
    
    func makecall(text: string,
                  completion: @escaping (string) -> void){
        client?.sendcompletion(with: text,
                               maxtokens: 500,
                               completionhandler: { result in
            switch result {
            case .success(let model):
                let output = model.choices.first?.text.trimmingcharacters(in: .whitespacesandnewlines) ?? """"
                completion(output)
            case .failure:
                print(""failed: \(result)"")
                break
            }
        })
    }
}

i hope someone could help me with this, because everything i have tried failed. i tried using other methods of calling the api but that also wouldn't work...
so if you know what the issue is and how to fix it please let me know
edit sendcompletion code:
public func sendcompletion(with prompt: string, model: openaimodeltype = .gpt3(.davinci), maxtokens: int = 16, temperature: double = 1, completionhandler: @escaping (result<openai<textresult>, openaierror>) -> void) {
    let endpoint = endpoint.completions
    let body = command(prompt: prompt, model: model.modelname, maxtokens: maxtokens, temperature: temperature)
    let request = preparerequest(endpoint, body: body)

makerequest(request: request) { result in
    switch result {
    case .success(let success):
        do {
            let res = try jsondecoder().decode(openai<textresult>.self, from: success)
            completionhandler(.success(res))
        } catch {
            completionhandler(.failure(.decodingerror(error: error)))
        }
    case .failure(let failure):
        completionhandler(.failure(.genericerror(error: failure)))
    }
}

}
edit after feedback:
i updated the version, and after this it stopped giving me that error, but it now gives me a different one.... this one:

success(openaiswift.openai<openaiswift.textresult>(object: nil, model:
nil, choices: nil, usage: nil, data: nil))","['swift', 'decoding', 'openai-api']",75929723,"try this example code, works well for me, if your your-apikey is valid. let me know if this does not work for you.
note, use the latest main branch of openaiswift, not other versions or tags.
import foundation
import swiftui
import openaiswift


@mainactor
class openaimodel: observableobject {
    
    @published var answers = [string]()
    
    let client: openaiswift
    
    init() {
        client = openaiswift(authtoken: ""your-apikey"")
    }
    
    func ask(text: string) async {
        do {
            let result = try await client.sendcompletion(
                with: text,
                model: .gpt3(.davinci),
                maxtokens: 500,
                temperature: 1
            )
            let output = result.choices?.first?.text ?? ""no answer""
            answers.append(output)
        } catch {
            print(error)
        }
    }
    
    func makecall(text: string, completion: @escaping (string) -> void) {
        client.sendcompletion(with: text, maxtokens: 500) { result in
            switch result {
            case .success(let model):
                let output = model.choices?.first?.text.trimmingcharacters(in: .whitespacesandnewlines) ?? """"
                completion(output)
            case .failure:
                print(""---> failed: \(result)"")
                completion(""failed"")
                break
            }
        }
    }
    
}

struct contentview: view {
    @stateobject var openai = openaimodel()
    let text = ""explain schrï¿½ï¿½dinger's wave equation""
    
    var body: some view {
        vstack {
            text(""fetching..."")
            foreach(openai.answers, id: \.self) { answer in
                text(answer)
            }
        }
        .onappear {
            openai.makecall(text: text){ answer in
                dispatchqueue.main.async {
                    openai.answers.ad(answer)
                }
            }
        }
//        .task{
//            await openai.ask(text: text)
//        }
    }
}

note also, there is an issue reported at:  regarding a bug in decoding, it may be relevant to your case.",https://stackoverflow.com/questions/75928704,swift,04-04-2023 11:17,776.0,0.0,1.0,True,05-04-2023 07:31,05-04-2023 07:31
69781324,find the most similar terms from a list of given terms in a huge text corpora,"i have a 2-million long list of names of podcasts. also, i have a huge text corpus scraped from a sub-reddit (posts, comments, threads etc.) where the podcasts from our list are being mentioned a lot by the users. the task i'm trying to solve is, i've to count the number of mentions by each name in our corpora. in other words, generate a dictionary of (name: count) pairs.
the challenge here is that most of these podcast names are several words long, for eg: ""utah's noon news""; ""congress hears tech policy debates"" etc. however, the mentions which reddit users make are often a crude substring of the original name, for eg: ""utah noon/ utah new"" or ""congress tech debates/ congress hears tech"". this makes identifying names from the list quite difficult.
what i've tried:
first, i processed and concatenated all the words in the original podcast names into a single word. for instance,
""congress hears tech policy debates"" -> ""congresshearstechpolicydebates""
as i traversed the subreddit corpus, whenever i found a named-entity or a potential podcast name, i processed its words like this,
""congress hears tech"" (assuming this is what i found in the corpora) -> ""congresshearstech""
i compared this ""congresshearstech"" string to all the processed names in the podcast list. i make this comparison using scored calculated on word-spelling similarity. i did this using difflib python library. also, there are similarity scores like leveshtein and hamming distance. eventually, i rewarded the podcast name with similarity score maximum to our corpus-found string.
my problem:
the thing is, the above strategy is infact working accurately. however, it's way too slow to do for the entire corpus. also, my list of names is way too long. can anyone please suggest a faster algorithm/data structure to compare so many names on such a huge corpus? is there any deep learning based approach possible here? something like where i can train a lstm on the 2 million podcast names. so, that whenever a possible name is encountered, this trained model can output the closest spelling of any podcast from our list?","['machine-learning', 'deep-learning', 'nlp', 'string-matching']",69782635,"you may be able to use something like tf-idf and cosine similarity to solve this problem.  i'm not familiar with any approach to use machine learning that would be helpful here.
this article gives a more detailed description of the process and links to some useful libraries.  you should also read this article which describes a somewhat similar project to yours and includes information on improving performance.  i'll describe the method as i understand it here.
tf-idf is an acronym meaning ""term frequency inverse document frequency"".  essentially, you look at a subset of text and find the frequency of the terms in your subset relative to the frequency of those terms in the entire corpus of text.  terms that are common in your subset and in the corpus as a whole will have a low value, whereas terms that are common in your subset but rare in the corpus would have a high value.
if you can compute the tf-idf for a ""document"" (or subset of text) you can turn a subset of text into a vector of tf-idf values.  once you have this vector you can use it to compute the cosine-similarity of your text subset with other subsets.  say, find the similarity of an excerpt from reddit with all of your titles.  (there is a way to manage this so you aren't continuously checking each reddit excerpt against literally every title - see this post).
once you can do this then i think the solution is to pick some value n, and scan through the reddit posts n words at a time doing the tf-idf / cosine similarity scan on your titles and marking matches when the cosine-similarity is higher than a certain value (you'll need to experiment with this to find what gives you a good result).  then, you decrement n and repeat until n is 0.",https://stackoverflow.com/questions/69781324,machine-learning,30-10-2021 19:06,1012.0,0.0,2.0,True,31-10-2021 22:52,31-10-2021 22:52
73782511,extracting a password from a sentence using nlp,"i am developing a bot, which will be used for account management. i'm currently trying this with the yake but am open to other suggestions.
i want to parse sentences similar to the one below
add a mail account xyz  with username xname with password xyz@123#

i need to extract mail, xname and xyz@123# but am unable to do this. the library doesn't parse the password since it's meaningless.
how would i go about doing this?","['nlp', 'nltk']",73803389,"are sentences always like ""...mail account <account_name> ... username <username> ... password <password>""?
in this case you can use regexes. i can write the corresponding code if needed.
otherwise, you should investigate spacy library and is named entity recognition. start by this tutorial about training your own ner with spacy.",https://stackoverflow.com/questions/73782511,nlp,20-09-2022 06:38,346.0,0.0,1.0,True,21-09-2022 15:27,20-09-2022 06:47
79297393,empty result apache opennlp onnx model,"i trying to convert huggingface model to onnx for classifying text in java app, but i can't undestand why i don't see result(result array is just empty). readme.md has link on the model, and it's working pretty well, but i have to use some another one because it is not support language which  i need.
working sample python code:
from transformers import autotokenizer, automodelforsequenceclassification
import torch

model_name = ""tabularisai/multilingual-sentiment-analysis""
tokenizer = autotokenizer.from_pretrained(model_name)
model = automodelforsequenceclassification.from_pretrained(model_name)

def predict_sentiment(texts):
    inputs = tokenizer(texts, return_tensors=""pt"", truncation=true, padding=true, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
    sentiment_map = {0: ""very negative"", 1: ""negative"", 2: ""neutral"", 3: ""positive"", 4: ""very positive""}
    return [sentiment_map[p] for p in torch.argmax(probabilities, dim=-1).tolist()]

print(predict_sentiment([""i absolutely love the new design of this app!"", ""the customer service was disappointing.""]))

i tried to convert a few models with two ways:
python -m optimum.exporters.onnx --model tabularisai/multilingual-sentiment-analysis --task sequence-classification onnx_model
and
from optimum.onnxruntime import ortmodelforfeatureextraction

model = ortmodelforfeatureextraction.from_pretrained(""tabularisai/multilingual-sentiment-analysis"", from_transformers=true)
model.save_pretrained(""onnx_model"")

same result - onnx_model folder with model, vocab etc
nlptown_bert-base-multilingual-uncased-sentiment - model from readme and it works as expect even i use converted model vocab file
java code sample:
    public void def() {
        try (final documentcategorizerdl documentcategorizerdl =
                     new documentcategorizerdl(
                             new file(""onnx_model/model.onnx""),
//                             new file(""nlptown_bert-base-multilingual-uncased-sentiment.onnx""),
                             new file(""onnx_model/vocab.txt""),
                             getcategories(),
                             new averageclassificationscoringstrategy(),
                             new inferenceoptions())) {

            final double[] result = documentcategorizerdl.categorize(new string[] {""i absolutely love the new design of this app!"", ""the customer service was disappointing.""});
            system.out.println(""done"");
        } catch (exception e) {
            e.printstacktrace();
        }
    }

    private map<integer, string> getcategories() {
        final map<integer, string> categories = new hashmap<>();
        categories.put(0, ""very negative"");
        categories.put(1, ""negative"");
        categories.put(2, ""neutral"");
        categories.put(3, ""positive"");
        categories.put(4, ""very positive"");
        return categories;
    }

i don't know how is important but opset version of working model is 11, but my is 14.
my opennlp-dl library version is 2.5.1","['java', 'huggingface-transformers', 'onnx']",79301131,"setincludetokentypeids(true) was passed to options and this helped to solve this problem
    public void def() {
    var options = new inferenceoptions();
    options.setincludetokentypeids(false);
    try (
         final documentcategorizerdl documentcategorizerdl =
                 new documentcategorizerdl(
                         new file(""onnx_model/model.onnx""),
                         new file(""onnx_model/vocab.txt""),
                         getcategories(),
                         new averageclassificationscoringstrategy(),
                         options)) {

        final double[] result = documentcategorizerdl.categorize(new string[] {""i absolutely love the new design of this app!"", ""the customer service was disappointing.""});
        system.out.println(""s"");
    } catch (exception e) {
        log.error(e.getmessage(), e);
    }
}

private map<integer, string> getcategories() {
    final map<integer, string> categories = new hashmap<>();
    categories.put(0, ""very negative"");
    categories.put(1, ""negative"");
    categories.put(2, ""neutral"");
    categories.put(3, ""positive"");
    categories.put(4, ""very positive"");
    return categories;
}",https://stackoverflow.com/questions/79297393,java,20-12-2024 13:35,59.0,1.0,1.0,True,22-12-2024 14:07,21-12-2024 10:40
66268093,scratch project: to check if any word in a list is contained in an answer,"i have the following scratch project which has a ""kind list"" of words like: ""good"", ""kind"", ""love"", ""come"" etc.
a user should be able to enter any sentence containing any of these words, and the happy face would show.

currently if the user types ""kind"" the happy face shows and if it types anything else like ""you are kind"", the sad face shows.
how do i change this, in scratch, such that if the user types in:
""you are kind"" or
""how kind you are"" or
""come here""
(any sentence containing any word in the ""kindlist"") the face is happy,else not.
i can only find a block that allows me to select the list and then the answer and no other alternatives. what i want is the python equivalent of > in list
answer=input(""say something"")
if any word in the input answer (sentence) in in the list.
then do - - -
for teaching purposes, i am trying to simplify what is on  (creating of the training set). can this be done directly in scratch or not? or is this why the site allows you generate blocks on their site first and import them.
surely scratch should have the capability to enter data into lists and then test them directly.
i've also tried using a loop (which doesn't quite work correctly either) but was hoping there was a far simpler way.","['list', 'variables', 'mit-scratch', 'nlp-question-answering']",66339107,"i guess scratch deliberately offers a minimal set of functions,
on the one hand not to overwhelm beginners,
on the other hand to encourage students to piece together simple blocks into more complex systems.
yes, a simple (sentence) contains (word) is all you get out-of-the-box;
you do need a loop to match a multi-word sentence against a multi-word whitelist.
seems to me like you would be better off with some development environment
that will at least give you some mature text parsing capabilities.
i'm not saying it's impossible to teach student about machine learning using scratch, but i doubt it's the best tool for the job.
it feels like somebody wants to give music lessons, but students first have to go through the process of building a piano.
as for your code, it looks like a good start.
some suggestions:

replace the 'forever' loop with a loop bounded by the length of list 'kindthings'.
include a leading and a trailing space in the 'contains' check, to make sure only whole words match. wouldn't want 'unhappy' in a sentence to match 'happy' in the whitelist.",https://stackoverflow.com/questions/66268093,list,18-02-2021 21:00,1568.0,0.0,1.0,True,23-03-2021 15:14,18-02-2021 21:32
75367803,how to remove spaces in a &quot;single&quot; word? (&quot;bo ok&quot; to &quot;book&quot;),"i am reading a badly formatted text, and often there are unwanted spaces inside a single word. for example, ""int ernational trade is not good for economies"" and so forth. is there any efficient tool that can cope with this? (there are a couple of other answers like here, which do not work in a sentence.)
edit: about the impossibility mentioned, i agree. one option is to preserve all possible options. in my case this edited text will be matched with another database that has the original (clean) text. this way, any wrong removal of spaces just gets tossed away.\","['python', 'regex', 'nlp', 'whitespace', 'removing-whitespace']",75367950,"you could use the pyenchant package to get a list of english words. i will assume words that do not have meaning on their own but do together are a word, and use the following code to find words that are split by a single space:
import enchant

text = ""int ernational trade is not good for economies""
fixed_text = []

d = enchant.dict(""en_us"")

for i in range(len(words := text.split())):
    if fixed_text and not d.check(words[i]) and d.check(compound_word := ''.join([fixed_text[-1], words[i]])):
        fixed_text[-1] = compound_word
    else:
        fixed_text.append(words[i])

print(' '.join(fixed_text))

this will split the text on spaces and append words to fixed_text. when it finds that a previously added word is not in the dictionary, but appending the next word to it does make it valid, it sticks those two words together.
this should help sanitize most of the invalid words, but as the comments mentioned it is sometimes impossible to find out if two words belong together without performing some sort of lexical analysis.
as suggested by pranav hosangadi, here is a modified (and a little more involved) version which can remove multiple spaces in words by compounding previously added words which are not in the dictionary. however, since a lot of smaller words are valid in the english language, many spaced out words don't correctly concatenate.
import enchant

text = ""inte rnatio nal trade is not good for ec onom ies""
fixed_text = []

d = enchant.dict(""en_us"")

for i in range(len(words := text.split())):
    if fixed_text and not d.check(compound_word := words[i]):
        for j, pending_word in enumerate(fixed_text[::-1], 1):
            if not d.check(pending_word) and d.check(compound_word := ''.join([pending_word, compound_word])):
                del fixed_text[-j:]
                fixed_text.append(compound_word)
                break
        else:
            fixed_text.append(words[i])
    else:
        fixed_text.append(words[i])

print(' '.join(fixed_text))",https://stackoverflow.com/questions/75367803,python,07-02-2023 00:14,560.0,1.0,1.0,True,07-02-2023 03:54,07-02-2023 03:54
76167117,how to store n-dimensional vector in microsoft sql server?,"i want to store a large n-dimensional vector (e.g. an embedding vector) in sql server as a piece of metadata associated with another row.
in this example, it will be a 384-dimensional vector, for example:
[0.161391481757164,   -0.23294533789157867, -0.5648667216300964,  -0.3210797905921936,  -0.03274689242243767,  0.011770576238632202, -0.06612513959407806,
-0.14662186801433563, -0.17081189155578613,  0.2879514992237091,  -0.1932784765958786,   0.009713868610560894, 0.23330552875995636,   0.03551964834332466,
-0.20526213943958282,  0.06445703655481339, -0.3146169185638428,   0.5788811445236206,   0.09118294715881348, -0.0048667509108781815,-0.16503077745437622,
 0.25162017345428467, -0.36395764350891113, -0.34742429852485657,  0.0526515394449234,   0.08912508934736252,  0.48464590311050415,  -0.04224267974495888,
 0.32445403933525085, -0.6847451329231262,  -0.20959551632404327, -0.027657458558678627, 0.20439794659614563,  0.6859520077705383,   -0.4988805055618286,
-0.26204171776771545, -0.18842612206935883,  0.07067661732435226,  0.02633148804306984,  0.03182782977819443,  0.28935596346855164,  -0.0016041728667914867,
 0.14609676599502563, -0.36272501945495605,  0.10288259387016296, -0.3651926815509796,  -0.3823530375957489,   0.14052163064479828,   0.006418740376830101,
 0.11741586774587631, -0.6509529948234558,  -0.15997739136219025, -0.42837604880332947,  0.12351743131875992,  0.0485026054084301,    0.24820692837238312,
 0.46972623467445374, -0.47954055666923523, -0.5238635540008545,  -0.3543052673339844,   0.22626525163650513,  0.18406584858894348,   0.6463921070098877,
 0.11894208937883377, -0.07143554836511612,  0.004256516695022583, 0.10088140517473221,  0.3335645794868469,   0.16905969381332397,   0.056856121867895126,
 0.11355260014533997,  0.3708053231239319,  -0.7484591603279114,   0.17503942549228668, -0.3249044418334961,   0.5901510715484619,    0.41506800055503845,
 0.05852462351322174,  0.5119204521179199,   0.2750142216682434,  -0.2058306783437729,   0.8199670314788818,   0.16698679327964783,  -0.1572146713733673,
 0.014733579009771347 ,0.0168467964977026,   0.4688740372657776,  -0.07839230448007584,  0.49326324462890625, -0.29934313893318176,   0.21525822579860687,
 0.1396997570991516,  -0.3420834243297577,  -0.5197309851646423,   0.10842061042785645, -0.0338996984064579,   0.35846689343452454,  -0.1660442352294922,
 0.15579357743263245,  0.015674782916903496,-0.8510578870773315,  -0.07501569390296936, -0.1791406124830246,   0.14926102757453918,  -0.2269722819328308,
 0.42619261145591736,  0.09489753842353821, -0.13341256976127625,  0.3312526345252991,   0.22534190118312836,  0.0679713636636734,    0.17042726278305054,
 0.14300595223903656, -0.06654901057481766, -0.2170567661523819,  -0.454984188079834,   -0.5516679286956787,  -0.10752955824136734,  -0.05743071809411049,
 0.32108309864997864, -0.5445901155471802,  -0.43162357807159424,  0.08207866549491882,  0.0664522647857666,   0.4478979706764221,    0.2190810590982437,
-0.05722910910844803, -0.0932786613702774,   0.01758035272359848,  0.16166797280311584,  0.44004616141319275, -0.21601708233356476,   0.43121641874313354,
 0.32022470235824585, -0.014045504853129387,-0.24948528409004211, -0.4389941990375519,   0.3816317319869995,  -0.5687862038612366,    0.1088542640209198,
-0.403241366147995,    0.08174201846122742,  0.21350793540477753,  0.2396722435951233,   0.4973253607749939,   0.31202447414398193,  -0.5260801315307617,
-0.3351263403892517,  -0.04100760444998741,  0.6609364151954651,  -0.2047063261270523,   0.19385716319084167, -0.5661329627037048,   -0.27058693766593933,
-0.1637117713689804,   0.30641692876815796, -0.08894442766904831, -0.052735116332769394,-0.13839660584926605, -0.6741533875465393,    0.05569711700081825,
-0.04354270175099373,  0.20251914858818054,  0.24813368916511536,  0.1719648838043213,   0.26782000064849854,  0.3137670159339905,    0.18599936366081238,
 0.23953016102313995,  0.17769533395767212,  0.46293920278549194, -0.19122551381587982, -0.5595004558563232,   0.09755659103393555,   0.3125424385070801,
-0.5813230276107788,  -1.0698442459106445,  -0.09045401215553284, -0.08948248624801636, -0.051830895245075226,-0.0001317809073952958,-0.08400193601846695,
 0.25725823640823364, -0.10135184973478317,  0.07884480804204941,  0.2091679722070694,   0.3950233459472656,   0.2745698094367981,   -0.872776448726654,
-0.16590780019760132,  0.4308463931083679,  -0.24375642836093903, -0.02120584435760975,  0.05213866010308266, -0.19898287951946259,  -0.5506985187530518,
 0.40167248249053955,  0.1640072464942932,  -0.010167916305363178, 0.14038121700286865,  0.4958030879497528,  -0.7259818315505981,   -0.24387206137180328,
 0.08528701961040497,  0.03415993973612785, -0.16687284409999847,  0.3804749548435211,  -0.08561687171459198, -0.2752263844013214,    0.5883951783180237,
-0.3283255994319916,  -0.12724250555038452,  0.08751262724399567, -0.44206979870796204, -0.11079336702823639, -0.16302113234996796,   0.11022322624921799,
-0.09404750168323517, -0.256179541349411,    0.20473307371139526,  0.41829538345336914, -0.1095203086733818,   0.02342342585325241,  -0.18814104795455933,
-0.2540932893753052,   0.48397907614707947,  0.03593514859676361, -0.089835524559021,   -0.6478171944618225,  -0.1757517009973526,    0.0672023594379425,
 0.0695127546787262,  -0.6398074626922607,  -0.03958022966980934, -0.10351496934890747,  0.22433893382549286,  0.6756673455238342,   -0.2924160957336426,
 0.17503827810287476,  0.12915058434009552, -0.239552840590477,    0.15498916804790497, -0.4730042815208435,  -0.12289212644100189,  -0.004052990116178989,
 0.11593572050333023, -0.1965983510017395,   0.5210273265838623,  -0.18184830248355865,  0.2579534947872162,  -0.1920309066772461,   -0.389960378408432,
 0.04139290377497673, -0.11638019979000092, -0.10620912909507751, -0.5321099162101746,   0.13135096430778503, -0.07761876285076141,  -0.0830138698220253,
-0.01572849042713642,  0.31080499291419983, -0.41445496678352356,  0.1609737128019333,   0.5787453651428223,  -0.05459209159016609,   0.1318219006061554,
-0.06957206130027771,  0.15152350068092346, -0.07094550132751465, -0.196294367313385,    0.12644843757152557,  0.23419199883937836,   0.5845456719398499,
-0.19989481568336487, -0.19607964158058167, -0.19692276418209076, -0.08633144199848175, -0.004551170393824577, 0.09362921118736267,  -0.14167727530002594,
-0.14917594194412231,  0.31781134009361267,  0.18779256939888,     0.42154577374458313, -0.20578211545944214,  0.14142100512981415,  -0.5664211511611938,
 0.18177354335784912,  0.14776530861854553,  0.29254236817359924,  0.17831481993198395, -0.1894354224205017,  -0.2836195230484009,   -0.4065170884132385,
-0.14325398206710815,  0.17800962924957275,  0.7763587832450867,   0.5497004389762878,  -0.00946379080414772, -0.48568078875541687,  -0.022227048873901367,
-0.005903944373130798, 0.4351034462451935,   0.05010621249675751, -0.12799566984176636, -0.06675072759389877,  0.167253315448761,    -0.1653994619846344,
 0.21004730463027954,  0.2765181362628937,   0.5885812640190125,  -0.326379656791687,   -0.007390940561890602, 0.27159956097602844,  -0.043763305991888046,
-0.39229199290275574, -0.19412016868591309,  0.4250912666320801,   0.6105153560638428,  -0.06168382614850998, -0.5341082811355591,   -0.611929714679718,
 0.08125612139701843, -0.1779184639453888,   0.5319408774375916,  -0.23601730167865753,  0.22285249829292297, -0.32505497336387634,   0.2152460366487503,
 0.4679816663265228,   0.048206135630607605,-0.24099768698215485, -0.30208054184913635,  0.13667792081832886,  0.3552468717098236,   -0.12280546128749847,
-0.006191314198076725,-0.10851636528968811,  0.08330328017473221, -0.09545236080884933, -0.02249046228826046,  0.0003346469602547586,-0.12273653596639633,
-0.05594412609934807,  0.027804357931017876,-0.4045255482196808,  -0.18987023830413818, -0.0027474926318973303,0.30244430899620056,   0.2323288917541504,
-0.2729185223579407,   0.12836921215057373,  0.27967774868011475,  0.3031359016895294,   0.41273725032806396, -0.06173351779580116,   0.33845168352127075,
 0.26775869727134705, -0.2933143079280853,  -0.0485006645321846,   0.11777450144290924,  0.6205862760543823,  -0.07637807726860046,  -0.19466432929039001,
-0.3994691073894501,   0.15689416229724884, -0.11139731854200363, -0.2333720475435257,   0.2364773154258728,   0.30898618698120117,  -0.1263875812292099,
-0.231489360332489,    0.34536853432655334,  0.6001318097114563,  -0.44741731882095337,  0.07382357120513916, -0.019649405032396317, -0.1029537245631218,
 0.369470477104187,   -0.032077688723802567,-0.13972929120063782,  0.24549521505832672, -0.13091856241226196, -0.029257331043481827]

attempt#1 - n-dimensional vector ï¿½ï¿½ï¿½ n-columns
my first thought was to store the 384 real values in a separate table, with a key to the original row (vertical partitioning):
create table embeddings (
   rowguid uniquedientifier not null primary key,
   f1 real not null, 
   f2 real not null,
   f3 real not null,
   f4 real not null,
   f5 real not null,
   f6 real not null,
   f7 real not null,
   f8 real not null,
   f9 real not null,
   f10 real not null,
   ...snip...
   f384 real not nullde>




rowguid
f1
f2
f3
f4
f5
f6
f7
...
f384




6ba7b814-9dad-11d1-80b4-00c04fd430c8
0.161391481757164
-0.23294533789157867
-0.5648667216300964
-0.3210797905921936
-0.03274689242243767
0.011770576238632202
-0.06612513959407806
...
-0.029257331043481827




this...sorta...works. but it is unwieldy. plus, my vectors today happen to be 385-dimensional; but they may soon be 1556-dimensional, which exceeds the sql server maximum of 1,024 columns per table.
attempt#2 - n-dimensional ï¿½ï¿½ï¿½ n-ieee 32-bit floats ï¿½ï¿½ï¿½ varbinary(n*4)
the next idea was to pack the 4-byte (32-bit) floats into a varbinary column:
create table embeddings (
   rowguid uniquedientifier not null primary key,
   packedvector varbinary(1516) not null -- 384 floats * 4 bytes = 1540 bytes
)

0x0000000100000002000000030000000400000005000000060000000700000008...0000017f
  \______/\______/\______/\______/\______/\______/\______/\______/   \______/
     f1      f2      f3      f4      f5      f6      f7      f8        f384

and then when it comes time to read each single, use substring to rip the 4-byte float out of the varbinary, and then convert it to a real:
declare @f1 real = cast(substring(packedvector, 0*4, 4) as real);

except two down-sides:

downside#1: you cannot convf="" rel=""noreferrer"">binary(4) to a real (even though you can convert a real to a binary(4); just not the other way:

may be able to workarond it with decimal or numeric).

downside#2: the math of computing the euclidian distance between two vectors is conceptually valid:
declare @target varbinary(1536) -- packed 384-dimensional vector

select top(10) rowguid, sum(power(cast(substring(embedding, i*4+1, 4) as real) - cast(substring(@target, i*4 + 1, 4) as real), 2)) as distance
from embeddings
cross apply (values (0), (1), (2), ..., (383)) as sequence(i) -- fill in the values from 0 to 383
group by rowguid
order by distance asc

but that will be pretty poorly performing (even if issue #1 didn't exist).


attempt 3 - do what joe celko does
many years ago, someone on the microsoft newsgroups had the same question:

can anyone point me to a reference or discuss the best way to store a
vector of 120 to 480 numbers in the database? rows seem to be out
since we would quickly top the billion row mark. a table with 480
columns is too unnormalized. a single varchar(max) column? this
seems the best answer for now unless there is a more efficiant way of
storing it.
thanks for any help or opinions,

and then --celko-- responded:

i think of a vector as a particular kind of mathematical structure and
you seem to be talking about a list of some kind. vectors have a fixed
number of dimensions, etc. here is a guess:
create table vectors (
   vector_id char(3) not null, --whatever
   dim_nbr   integer not null,
   check (dim_nbr between 1 and 480),
   primary key (vector_id, dim_nbr),
   dim_val   integer not null
);


making the values of a the vector into rows:
embeddings




rowguid
dimnumber
dimvalue




6ba7b814-9dad-11d1-80b4-00c04fd430c8
1
0.161391481757164


6ba7b814-9dad-11d1-80b4-00c04fd430c8
2
-0.23294533789157867


6ba7b814-9dad-11d1-80b4-00c04fd430c8
3
-0.5648667216300964


6ba7b814-9dad-11d1-80b4-00c04fd430c8
4
-0.3210797905921936


6ba7b814-9dad-11d1-80b4-00c04fd430c8
5
-0.03274689242243767


6ba7b814-9dad-11d1-80b4-00c04fd430c8
6
0.011770576238632202


6ba7b814-9dad-11d1-80b4-00c04fd430c8
7
-0.06612513959407806


...
...
...


6ba7b814-9dad-11d1-80b4-00c04fd430c8
384
-0.029257331043481827




this is probably the best approach.
nothing better?
doesn't sql server has better support for vectors? i know there is geospatial/geography types, but i gather those only work for 2-dimensional vectors (e.g. lattuitude+logitude)? can't they be abused to solve the problem?
and since the goal is to compute euclidian distance between two vectors, is there a data structure that does a better job of allowing math? (varchar? xml? json? varbinary? variant?)
bonus reading

indexing n-dimensional vectors
microsoft.public.sqlserver.programming: best way to store vectors?
ï¿½ï¿½ï¿½ï¿½getting started with embeddings
"" rel=""noreferrer"">how to customize llms like chatgpt with your own data and documents
faiss - facebook open-source vector database: 
pinecone, an online vector database system: 
microsoft: what is a vector database? (
sqlservercentral: the rise of vector databases
optimized approach for calculating cosine similarity in sql server (tips for using cosine angle distane)","['arrays', 'sql-server', 'vector', 'word-embedding']",77018621,"you may also try something like explained here

there is no specific data type available to store a vector in azure sql database, but we can use some human ingenuity to realize that a vector is just a list of numbers. as a result, we can store a vector in a table very easily by creating a column to contain vector data. one row per vector element. we can then use a columnstore index to efficiently store and search for vectors.

so create a table to hold the vectors:
create table [dbo].[embeddings]
(
    [article_id] [int] not null,
    [vector_value_id] [int] not null,
    [vector_value] [float] not null
) 

and then we can use t-sql to efficiently compute distances:

on that table we can create a column store index to efficiently store and search for vectors. then it is just a matter of calculating the distance between vectors to find the closest. thanks to the internal optimization of the columnstore (that uses simd avx-512 instructions to speed up vector operations) the distance calculation is extremely fast.
the most common distance is the cosine similarity, which can be calculated quite easily in sql.

select 
    sum(a.value * b.value) / (  
        sqrt(sum(a.value * a.value)) * sqrt(sum(b.value * b.value))   
    ) as cosine_similarity
from
    vectors_values

the sql query is calculating the cosine similarity between two vectors, represented by the columns a.value and b.value. cosine similarity is defined as follows:

where:

 is the dot product of vectors  and 
(|a|) and (|b|) are the magnitudes of vectors (a) and (b), respectively

the important parts of the query are:

sum(a.value * b.value): calculates the dot product of vectors (a) and (b).
sqrt(sum(a.value * a.value)): calculates the magnitude of vector (a).
sqrt(sum(b.value * b.value)): calculates the magnitude of vector (b).

finally, the entire expression divides the dot product by the product of the magnitudes to find the cosine similarity, which is returned as cosine_similarity.",https://stackoverflow.com/questions/76167117,arrays,03-05-2023 18:44,6532.0,8.0,5.0,True,14-02-2025 14:50,06-05-2023 01:52
78734751,how do i persist faiss indexes?,"in the langchain wiki of faiss,  it only talks about saving indexes to files.
db.save_local(""faiss_index"")

new_db = faiss.load_local(""faiss_index"", embeddings)

docs = new_db.similarity_search(query)

how can i save the indexes to databases, such that we can organize and concurrently access multiple indexes?
searched online but could not get much info on this.
can faiss be used with any kind of distributed databases?","['python', 'vectorization', 'langchain', 'large-language-model', 'faiss']",78798031,"in fact, faiss is considered as an in-memory database itself in order to vector search based on similarity that you can serialize and deserialize the indexes using functions like write_index and read_index within the faiss interface directly or using save_local and load_local within the langchain integration which typically uses the pickle for serialization.
if you need to store serialized files, you could manually save them in a nosql database like mongodb as binary data, and then deserialize and retrieve them when needed, however, it is not the best practice!
if you are looking for a vector database that is not in-memory and capable in a scalable system, you might want to consider using milvus which is designed for this purpose.",https://stackoverflow.com/questions/78734751,python,11-07-2024 09:39,2895.0,2.0,1.0,True,26-07-2024 15:08,26-07-2024 15:08
72298933,"runtimeerror: expected 3-dimensional tensor, but got 2-dimensional tensor for argument","i have two tensors names: wy and x, both of them with size 8:
import torch

wy = torch.tensor([[7.2, -2.9, 5.2, -8.4, -3.8, -6.9, 7.4, -8.1]])

x = torch.tensor([[70., 77., 101., 75., 40., 83., 48., 73.]])

now, i want to do bmm to multiply x * wy as follow:
xwy = x.bmm(wy.unsqueeze(2)).squeeze(3)

i got an error:

runtimeerror: expected 3-dimensional tensor, but got 2-dimensional tensor for argument #1 'batch1' (while checking arguments for bmm)

8*8 should be possible. but i don't know why i got this error every time.
any help, please!","['python', 'pytorch', 'nlp', 'matrix-multiplication', 'allennlp']",72299085,"bmm stands for batch matrix-matrix product. so it expects both tensors with a batch dimension (i.e., 3d as the error says).
for single tensors, you want to use mm instead. note that tensor.mm() also exists with the same behaviour.
x.mm(wy.transpose(0, 1))

tensor([[-5051.9199]])

or better, for two 1d tensor you can use dot for dot product.
# or simply do not initialise them with an additional dimension. not needed.
x.squeeze().dot(wy.squeeze())

tensor(-5051.9199)",https://stackoverflow.com/questions/72298933,python,19-05-2022 05:13,2122.0,2.0,1.0,True,20-08-2024 22:13,20-08-2024 21:42
46858838,how to share salesforce einstein models between few accounts,"i started to work with salesforce einstein api, in particular with einstein intent & sentiment analysis, and i faced an issue - when i created new models under one of my account, i couldn't see these models under my other account. i checked out the official documentation & tried to research a bit, but i didn't find any ways how i can share einstein models between several accounts. is it possible at all?","['machine-learning', 'neural-network', 'nlp', 'salesforce', 'salesforce-einstein']",47788530,according discussion with artificial intelligence product leader at salesforce ï¿½ï¿½urrently sharing models involves retraining the model with the new org. but they are thinking through ways to improve model sharing though.<,https://stackoverflow.com/questions/46858838,machine-learning,21-10-2017 00:07,150.0,0.0,1.0,True,10-06-2022 13:44,10-06-2022 13:44
73942412,how can i measure precision with this example?,"how can i calculated precision . i have the total corpus contains 4000 different sentences. if i did semantic search and got 4 sentences are relevant and one is not relevant  which  k=5. how can i measure it precision here ? i'm so confused and hope get help
the corpus contains 4000 sentences and there 20 sentences talked about the food.
my query is searching for ""sam eat the food "" and k=5 gave me the top score using cosine similarity
4 sentences semantically right and there is one is false

here does p1 means k =1 , p2 means k =2 ?
so if this is right . will k=1 means that the search for the first sentence has zero result? and for k = 2 , the percentage is 31  ? and so on ?","['information-retrieval', 'precision-recall']",73944747,"precision comes in two ways.
overall precision is the ratio of relevant results among all the search results returned.
precision@k is the ratio of relevant results among the top-k search results returned.
therefore your precision@5 will be:
precision@5 = relevant_results_at_5 / total_search_results_at_5 = 4/5",https://stackoverflow.com/questions/73942412,information-retrieval,04-10-2022 01:52,250.0,1.0,1.0,True,04-10-2022 17:53,04-10-2022 17:53
73433868,&quot;systemerror: google/protobuf/pyext/descriptor.cc:358: bad argument to internal function&quot; while using audio transformers in hugging face,"i am trying to do a task of ""speech2text"" using transformer model in hugging face.
i tried the code in this documentation on hugging face
import torch
from transformers import speech2textprocessor, speech2textforconditionalgeneration
from datasets import load_dataset

model = speech2textforconditionalgeneration.from_pretrained(""facebook/s2t-small-librispeech-asr"")
processor = speech2textprocessor.from_pretrained(""facebook/s2t-small-librispeech-asr"")


ds = load_dataset(""hf-internal-testing/librispeech_asr_demo"", ""clean"", split=""validation"")

inputs = processor(ds[0][""audio""][""array""], sampling_rate=ds[0][""audio""][""sampling_rate""], return_tensors=""pt"")
generated_ids = model.generate(inputs[""input_features""], attention_mask=inputs[""attention_mask""])

transcription = processor.batch_decode(generated_ids)
transcription

but when i tried to run this code in google colab i am receiving the following error :
systemerror: google/protobuf/pyext/descriptor.cc:358: bad argument to internal function
on checking the other error lines it seems that on calling processor(), return_tesnors is none even though it is specified as pt. due to which code is importing tensorflow and that error is coming. (know issue)

full error message :
systemerror                               traceback (most recent call last)
<ipython-input-4-2a3231ef630c> in <module>
      9 ds = load_dataset(""hf-internal-testing/librispeech_asr_demo"", ""clean"", split=""validation"")
     10 
---> 11 inputs = processor(ds[0][""audio""][""array""], sampling_rate=ds[0][""audio""][""sampling_rate""], return_tensors=""pt"")
     12 
     13 generated_ids = model.generate(inputs[""input_features""], attention_mask=inputs[""attention_mask""])

10 frames
/usr/local/lib/python3.7/dist-packages/transformers/models/speech_to_text/processing_speech_to_text.py in __call__(self, *args, **kwargs)
     51         information.
     52         """"""
---> 53         return self.current_processor(*args, **kwargs)
     54 
     55     def batch_decode(self, *args, **kwargs):

/usr/local/lib/python3.7/dist-packages/transformers/models/speech_to_text/feature_extraction_speech_to_text.py in __call__(self, raw_speech, padding, max_length, truncation, pad_to_multiple_of, return_tensors, sampling_rate, return_attention_mask, **kwargs)
    230             pad_to_multiple_of=pad_to_multiple_of,
    231             return_attention_mask=return_attention_mask,
--> 232             **kwargs,
    233         )
    234 

/usr/local/lib/python3.7/dist-packages/transformers/feature_extraction_sequence_utils.py in pad(self, processed_features, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors)
    161 
    162         if return_tensors is none:
--> 163             if is_tf_available() and _is_tensorflow(first_element):
    164                 return_tensors = ""tf""
    165             elif is_torch_available() and _is_torch(first_element):

/usr/local/lib/python3.7/dist-packages/transformers/utils/generic.py in _is_tensorflow(x)
     96 
     97 def _is_tensorflow(x):
---> 98     import tensorflow as tf
     99 
    100     return isinstance(x, tf.tensor)

/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py in <module>
     35 import typing as _typing
     36 
---> 37 from tensorflow.python.tools import module_util as _module_util
     38 from tensorflow.python.util.lazy_loader import lazyloader as _lazyloader
     39 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/__init__.py in <module>
     35 
     36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
---> 37 from tensorflow.python.eager import context
     38 
     39 # pylint: enable=wildcard-import

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py in <module>
     27 import six
     28 
---> 29 from tensorflow.core.framework import function_pb2
     30 from tensorflow.core.protobuf import config_pb2
     31 from tensorflow.core.protobuf import coordination_config_pb2

/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/function_pb2.py in <module>
     14 
     15 
---> 16 from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
     17 from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2
     18 from tensorflow.core.framework import op_def_pb2 as tensorflow_dot_core_dot_framework_dot_op__def__pb2

/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/attr_value_pb2.py in <module>
     14 
     15 
---> 16 from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
     17 from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
     18 from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2

/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/tensor_pb2.py in <module>
     14 
     15 
---> 16 from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
     17 from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
     18 from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2

/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/resource_handle_pb2.py in <module>
    148   ,
    149   'descriptor' : _resourcehandleproto,
--> 150   '__module__' : 'tensorflow.core.framework.resource_handle_pb2'
    151   # @@protoc_insertion_point(class_scope:tensorflow.resourcehandleproto)
    152   })

systemerror: google/protobuf/pyext/descriptor.cc:358: bad argument to internal function

here's my colab link for reference
let me know what can be done to resolve this error
thank you","['python', 'tensorflow', 'pytorch', 'huggingface-transformers']",73564479,"import tensorflow as tf
import torch
from transformers import speech2textprocessor, speech2textforconditionalgeneration
from datasets import load_dataset
........

import tensorflow lib first even if you are not using it, before importing any torch libraries. don't know the exact reason but after importing the lib code is working on the notebook you have shared.
refer to these links:


torchvision and tensorflow-gpu import error",https://stackoverflow.com/questions/73433868,python,21-08-2022 11:13,1186.0,2.0,1.0,True,01-09-2022 06:33,21-08-2022 12:56
70997133,conditionalfreqdist to find most frequent pos tags for words,"i am trying to fidn the most frequent pos tag for words in the dataset but struggling with the conditionalfrewdist part.
import nltk
tw = nltk.corpus.brown.tagged_words()

train_idx = int(0.8*len(tw))
training_set = tw[:train_idx]
test_set = tw[train_idx:]

words= list(zip(*training_set))[0]

from nltk import conditionalfreqdist
ofd= conditionalfreqdist(word for word in list(zip(*training_set))[0])

tags= list(zip(*training_set))[1]
ofd.tabulate(conditions= words, samples= tags)


valueerror: too many values to unpack (expected 2)","['python', 'nlp', 'nltk']",71002249,"as you might read in documents the conditionalfreqdist helps you to calculate

a collection of frequency distributions for a single experiment run under different conditions.

the only thing you must provide, is the list of items and conditions which can be translated (in this problem) to words and corresponding pos tags. the code with minimal changes would look like this and would calculate distributions for the whole corpus but tabulate the results for the first 10th items and conditions(preventing a crash):
import nltk
from nltk import conditionalfreqdist

tw = nltk.corpus.brown.tagged_words()
train_idx = int(0.8*len(tw))
training_set = tw[:train_idx]
test_set = tw[train_idx:]
words= list(zip(*training_set))[0] # items
tags= list(zip(*training_set))[1] # conditions

ofd= conditionalfreqdist((tag, word) for tag, word in zip(words, tags)) # simple comprehension pattern in python
ofd.tabulate(conditions= words[:10], samples= tags[:10])",https://stackoverflow.com/questions/70997133,python,05-02-2022 10:31,315.0,0.0,1.0,True,05-02-2022 21:20,05-02-2022 10:39
78430524,determining contents of decoder_hidden_states from t5forconditionalgeneration,"i'm using the huggingface t5forconditionalgeneration model without modification.
i want to compute mean pooling over the last hidden state of the t5 decoder, but i can't determine which part of the decoder_hidden_states contains what i'm looking for.
i want to do something like this:
# prepare batch data
sources = batch_df['source'].tolist()
tokenized_input = self.tokenizer(sources, return_tensors='pt', padding=true, truncation=true, max_length=self.max_length).to('cuda')
input_ids = tokenized_input['input_ids'].to('cuda')
attention_mask = tokenized_input['attention_mask'].to('cuda')

input_batch = {
    'input_ids': input_ids, 
    'attention_mask': attention_mask,
    'do_sample': false,
    'num_beams': 1,
    'eos_token_id': self.tokenizer.eos_token_id,
    'pad_token_id': self.tokenizer.pad_token_id,
    'max_length': self.max_output_length,
    'output_scores': true,
    'return_dict_in_generate': true,
    'output_hidden_states': true,
}
outputs = self.model.generate(**input_batch)

# retrieve the decoder hidden states
decoder_last_hidden_state = outputs.decoder_hidden_states[-1]  # last layer's hidden states

# compute the mean of the hidden states across the sequence length dimension
mean_pooled_output = torch.mean(decoder_last_hidden_state, dim=1, keepdim=false)

this approach works for the encoder, but for the decoder, decoder_hidden_states[-1] is a tuple of tensors, not a tensor.
when i first inspected the tuples, there were 10 tuples, and each tuple contained 7 tensors.
when i inspected the dimensions, like this:
for tuple_number in range(n):  # checking the tuples
    print(f""tuple {layer_number}:"")
    for i, tensor in enumerate(outputs.decoder_hidden_states[layer_number]):
        print(f""  tuple {i} in layer {layer_number}: shape {tensor.shape}"")

the outputs were all like this:
tuple 0:
  tensor 0 in tuple 0: shape torch.size([2, 1, 512])
  tensor 1 in tuple 0: shape torch.size([2, 1, 512])
  tensor 2 in tuple 0: shape torch.size([2, 1, 512])
  tensor 3 in tuple 0: shape torch.size([2, 1, 512])
  tensor 4 in tuple 0: shape torch.size([2, 1, 512])
  tensor 5 in tuple 0: shape torch.size([2, 1, 512])
  tensor 6 in tuple 0: shape torch.size([2, 1, 512])
tuple 1:
  tensor 0 in tuple 1: shape torch.size([2, 1, 512])
  tensor 1 in tuple 1: shape torch.size([2, 1, 512])
  tensor 2 in tuple 1: shape torch.size([2, 1, 512])
  tensor 3 in tuple 1: shape torch.size([2, 1, 512])
  tensor 4 in tuple 1: shape torch.size([2, 1, 512])
  tensor 5 in tuple 1: shape torch.size([2, 1, 512])
  tensor 6 in tuple 1: shape torch.size([2, 1, 512])
. . .

512 is the max_length of my tokenizer, and 2 is my batch size.
(i verified that 2 is the batch size because that number changed when i modified my batch size.)
then, when i trimmed the length of my input strings to 10 characters, to my surprise, the number of tuples went from 10 to 39. when i trimmed the strings further to only 2 chars per string, the number of tuples didn't increase beyond 39.
then, when i doubled my input string length instead, the number of tuples went down to 7. so, it appears like the number of tuples corresponds to iterations of the decoder over some chunk size up to some limits.
so, if i wanted to compute mean pooling over the first token, it seems like i'd compute the mean over the last tensor of the first tuple.
however, i don't understand exactly how the token length corresponds to the number of tuples.
how do i determine what exactly is represented by each of these tuples and tensors? i have not been successful in finding this information by going through the t5 source code.","['pytorch', 'nlp', 'huggingface-transformers']",78447718,"i think whats happening is that t5 returns the hidden state per step of decoding. therefore, the number of tuples should correspond to the longest generated sequence. you are most likely interested in the last decoding step and could take the last tuple.
in that tuple you have a tuple of size num_layers + 1 (+1 for the final layernorm). the output of the last layer should be the last tuple entry.",https://stackoverflow.com/questions/78430524,pytorch,04-05-2024 22:13,121.0,2.0,1.0,True,08-05-2024 10:14,06-05-2024 13:27
73952134,how should i approach classifying these text fields into numeric?,"i have data that has a bunch of text entries that look like this: 7-8 business days, 10 days, 10-12 days, 2-3 weeks, 1 year, 8-12 days, 2 weeks, etc.
i want to train a model to convert these text entries to numeric. i assume this is a fairly easy problem for an nlp model to handle, but i am not too confident about which models to use.
i would like to take the larger of the two numbers if a range is specified. for instance, 2-3 weeks should become 21 and 8-10 days should become 10.
i figure i can manually code around 100 records to train the model. can someone recommend an nlp model to use or even a script that i can edit? if this isn't a good use case for nlp, please also advise.
(i am only practiced at the r programming language, but i can tinker around in python if needed).
if all else fails is two break the data into separate columns using the hyphen as a delimiter using strsplit(as.character(df$column), ""-""), and then evaluating it using if-statements: for example:
date_conversion = if_else(is.numeric(columna) & columnb == ""weeks"", columna*7, if_else(is.numeric(columna) & columnb == ""days"", columna, na)

but ideally, i would like to train a model since i have a lot of data.
some data using dput:
text_fields <- c(""10 - 14 days"", ""2-3 weeks"", ""10-12 days"", ""8 days"", ""8-12 days"", 
                 ""10 days"", ""7-10 days"", ""5 days"", ""7-10 days"", ""5-7 days"", ""10 days"", 
                 ""7 days"", ""7 - 10 days"", ""1 week"", ""5-7 days"", ""2 weeks"", ""2-4 weeks"", 
                 ""10 days"", ""7-10 days"", ""8-10 days"", ""1 week"", ""10 days"", ""8-10 days"", 
                 ""2 weeks"", ""10-12 days"", ""7-10 days"", ""2-3 weeks"", ""7-10 days"", 
                 ""10 days"", ""2 weeks"", ""8-12 days"", ""12 days"", ""10 days"", ""7 days"", 
                 ""2 weeks"", ""5-8 days"", ""8-12 days"", ""8-12 days"", ""10 days"", ""12-14 days"", 
                 ""10-12 days"", ""7 days"", ""5-7 days"", ""2 weeks"", ""2-3 weeks"", ""5-7 days"", 
                 ""5-7 business days"", ""5-7 days"", ""5-7 business days"", ""7 days"", 
                 ""2-3 weeks"", ""7-10 days"", ""8-12 days"", ""10 days"", ""10 days"", 
                 ""10 days"", ""10 days"", ""10 days"", ""14"", ""2 weeks"", ""10 business days"", 
                 ""2-3 weeks"", ""4 days"", ""1 month"", ""7-10 days"", ""8-12 days"", ""2-3 weeks"", 
                 ""3-5 days"", ""10 days"", ""3-5 days"", ""2-3 days"", ""2-3 days"", ""3-5"", 
                 ""5-7 days"", ""7-10 days"", ""5-7 days"", ""8-12 days"", ""7-10 days"", 
                 ""7-10 days"", ""7-10 days"", ""2.5 weeks"", ""2 weeks"", ""10-12 days"", 
                 ""10-12 days"", ""7-10 days"", ""7-10 days"", ""7-10 days"", ""7-10 days"", 
                 ""7-10 days"", ""7-10 days"", ""2 weeks"", ""1 month"", ""1 month"", ""1 week""
)","['python', 'r', 'nlp']",73953331,"for something like this where all the kinds values for columna are quite predictable, it's highly unlikely that you would need to use any deep learning, if that's what you mean by nlp.
so, in my experience, what you are suggesting about if-statements is actually the right general idea.
here is a procedure that uses the tidyverse library and regular expressions:
library(tidyverse)

text_fields <- c(""10 - 14 days"", ""2-3 weeks"", ""10-12 days"", ""8 days"", ""8-12 days"", 
                 ""10 days"", ""7-10 days"", ""5 days"", ""7-10 days"", ""5-7 days"", ""10 days"", 
                 ""7 days"", ""7 - 10 days"", ""1 week"", ""5-7 days"", ""2 weeks"", ""2-4 weeks"", 
                 ""10 days"", ""7-10 days"", ""8-10 days"", ""1 week"", ""10 days"", ""8-10 days"", 
                 ""2 weeks"", ""10-12 days"", ""7-10 days"", ""2-3 weeks"", ""7-10 days"", 
                 ""10 days"", ""2 weeks"", ""8-12 days"", ""12 days"", ""10 days"", ""7 days"", 
                 ""2 weeks"", ""5-8 days"", ""8-12 days"", ""8-12 days"", ""10 days"", ""12-14 days"", 
                 ""10-12 days"", ""7 days"", ""5-7 days"", ""2 weeks"", ""2-3 weeks"", ""5-7 days"", 
                 ""5-7 business days"", ""5-7 days"", ""5-7 business days"", ""7 days"", 
                 ""2-3 weeks"", ""7-10 days"", ""8-12 days"", ""10 days"", ""10 days"", 
                 ""10 days"", ""10 days"", ""10 days"", ""14"", ""2 weeks"", ""10 business days"", 
                 ""2-3 weeks"", ""4 days"", ""1 month"", ""7-10 days"", ""8-12 days"", ""2-3 weeks"", 
                 ""3-5 days"", ""10 days"", ""3-5 days"", ""2-3 days"", ""2-3 days"", ""3-5"", 
                 ""5-7 days"", ""7-10 days"", ""5-7 days"", ""8-12 days"", ""7-10 days"", 
                 ""7-10 days"", ""7-10 days"", ""2.5 weeks"", ""2 weeks"", ""10-12 days"", 
                 ""10-12 days"", ""7-10 days"", ""7-10 days"", ""7-10 days"", ""7-10 days"", 
                 ""7-10 days"", ""7-10 days"", ""2 weeks"", ""1 month"", ""1 month"", ""1 week""
)

# putting the values in a dataframe. `tibble()` also works
df <- data.frame(text = text_fields)

# the %>% is a special operator that pipes the result into the first argument of the next function. i use it to keep things clean.
df <- df %>% 
  # i capture the last instance of a number in each value
  # then save the captured values to a new column called days
  mutate(days = str_match_all(text, ""\\d*\\.?\\d+"") %>% 
           # take the last match only
           lapply(tail, 1) %>% 
           # collaspe this list into a simple vector
           unlist() %>% 
           # change text to number
           as.numeric()
         ) %>% 
  # i update the days column according to the type of unit
  mutate(days = case_when(
    # (?i) makes the search case insensitive.
    str_detect(text, ""(?i)business"") ~ days + 2 * ceiling(days / 7),
    str_detect(text, ""(?i)week"") ~ days * 7,
    str_detect(text, ""(?i)month"") ~ ceiling(days * 30.5),
    str_detect(text, ""(?i)year"") ~ ceiling(days * 365.25),
    true ~ days
  ))

after doing this, you can check to see if there are edge cases you missed and you can expand the function to support those cases. applying what you suggested about writing training examples, you can instead make a bunch of complicated cases and test your code against them. if your code fails, you update it so that it can handle that case.
here's a good resource for testing and learning about regular expressions: 
notice that when you put the regular expressions in r, you'll have to escape the escape character. in place of \, use \\.",https://stackoverflow.com/questions/73952134,python,04-10-2022 18:27,96.0,1.0,2.0,True,07-10-2022 20:15,04-10-2022 18:41
76249781,feature extraction process using too much memory and causing a crash. what can i do?,"i am using a hugging face transformer to do some image feature extraction to use later for some similarity search functionality. this is not working currently because after processing around 200 images too much memory is being used and crashes the system... what am i doing wrong? what can i change to fix this.
here is my feature extraction class:
import numpy as np
from transformers import autoprocessor, automodelforzeroshotimageclassification, autotokenizer, tfclipmodel


def expand_greyscale_image_channels(grey_pil_image):
    grey_image_arr = np.array(grey_pil_image)
    grey_image_arr = np.expand_dims(grey_image_arr, -1)
    grey_image_arr_3_channel = grey_image_arr.repeat(3, axis=-1)
    return grey_image_arr_3_channel

def get_color_image(img):
    img = img.resize((224, 224))
    img = img.convert('rgb')
    return img

def get_greyscale_image(img):
    img = img.resize((224, 224))
    img = img.convert('l')
    img = expand_greyscale_image_channels(img)
    return img

class featureextractor:
    def __enter__(self):
        return self
    def __exit__(self, exc_type, exc_value, traceback):
        pass
    
    def __init__(self, processor=none, model=none, tokenizer=none, text_model=none):

        self.processor = processor
        self.model = model
        self.tokenizer = tokenizer
        self.text_model = text_model

    def model(self):
        return self.model
    
    def processor(self):
        return self.processor
    
    def extract_features(self, img, grey=false):
        """"""
        extract a deep feature from an input image
        args:
            img: from pil.image.open(path) or tensorflow.keras.preprocessing.image.load_img(path)

        returns:
            feature (np.ndarray): deep feature with the shape=(4096, )
        """"""
        try:
            if grey:
                img = get_greyscale_image(img)
            else:
                img = get_color_image(img)
            inputs = self.processor(images=img, return_tensors=""pt"")
            image_features = self.model.get_image_features(**inputs)
            # use tensor.detach().numpy() instead.
            image_features /= image_features.norm(dim=-1, keepdim=true)
            return image_features.detach().numpy()  # normalize
        except exception as e:
            print(e)
        
    def extract_text_features(self, text):
        try:
            inputs = self.tokenizer([text], padding=true, return_tensors=""tf"")
            text_features = self.text_model.get_text_features(**inputs)
            text_features = text_features / np.linalg.norm(text_features)
            return text_features.numpy()
        except exception as e:
            print(e)

here is the function that i run in a loop over each image url:
fe = featureextractor(processor, model, tokenizer, text_model)
def get_features_for_image(image_meta):
    id = image_meta[""id""]
    image_url = image_meta[""image_url""]
    # get features for image
    try:
        # open image from url
        image = get_pil_image_from_url(image_url)
        # resize image
        image = image.resize((224, 224))
        # if file not in features folder
        # extract features
        if not os.path.exists(""features/"" + id + "".npy""):
            # with featureextractor(processor, model, tokenizer, text_model) as fe:
            image_features = fe.extract_features(image)
            np.save(""features/"" + id + "".npy"", image_features)
            del image_features
            del image
            gc.collect()
            
            # write features to the json file

            # save featuers under file features/id.npy

        return true

    except exception as e:
        print(""error extracting features for image "", id, "" error: "", e)

where is the memory leak? how can i fix it?
here is the image of cpu usage. it is doing fine per image, as the number of images that features are extracted for in total increases so does the cpu usage. even if the model uses a lot of memory, shouldn't it recover the memory after the feature of each image is done extracting?","['python', 'deep-learning', 'memory-leaks', 'huggingface-transformers']",76250341,"figured it out. its the line image_features.detach.numpy() in extract_features
the detach() creates a separate copy of the numpy array. it is not needed here and created the leak.",https://stackoverflow.com/questions/76249781,python,14-05-2023 21:34,445.0,2.0,1.0,True,15-05-2023 01:18,15-05-2023 00:35
71320529,how does word2vec learn word relations?,"which part of the algorithm specifically makes the embeddings to have the king - boy + girl = queen ability? did they just did this by accident?
edit :
take the cbow as an example. i know about they use embeddings instead of one-hot vectors to encode the words and made the embeddings trainable instead of how we do when using one hot vectors that the data itself is not trainable. then the output is a one-hot vector for target word. they just average all the surrounding word embeddings at some point then put some lego layers afterwards. so at the end they find the mentioned property by surprise, or is there a training procedure or network structure that gave the embeddings that property?","['word2vec', 'embedding', 'word-embedding']",71326330,"the algorithm simply works to train (optimize) a shallow neural-network model that's good at predicting words, from other nearby words.
that's the only internal training goal ï¿½ï¿½ï¿½ subject to the neural network's constraints on how the words are represented (n floating-point dimensions), or combined with the model's internal weights to render an interpretable prediction (forward propagation rules).
there's no other 'coaching' about what words 'should' do in relation to each other. all words are still just opaque tokens to word2vec. it doesn't even consider their letters: the whole-token is just a lookup key for a whole-vector. (though, the word2vec variant fasttext varies that somewhat by also training vectors for subwords ï¿½ï¿½ï¿½ & thus can vaguely simulate the same intuitions that people have for word-roots/suffixes/etc.)
the interesting 'neighborhoods' of nearby words, and relative orientations that align human-interpretable aspects to vague directions in the high-dimensiote space, fall out of the prediction task. and those relative orientations are what gives rise to the surprising ""analogical arithmetic"" you're asking about.
internally, there's a tiny internal training cycle applied over and over: ""nudge this word-vector to be slightly better at predicting these neighboring words"". then, repeat with another word, and other neighbors. and again & again, millions of times, each time only looking at a tiny subset of the data.
but the updates that contradict each other cancel out, and those that represent reliable patterns in the source training texts reinforce each other.
from one perspective, it's essentially trying to ""compress"" some giant vocabulary ï¿½ï¿½ï¿½ tens of thousands, to millions, of unique words ï¿½ï¿½ï¿½ into a smaller n-dimensional representation - usually 100-400 dimensions when you have enough training data. the dimensional-values that become as-good-as-possible (but never necessary great) at predicting ne out to exhibit the other desirable positionings, too.",https://stackoverflow.com/questions/71320529,word2vec,02-03-2022 09:46,220.0,0.0,1.0,True,03-03-2022 09:14,03-03-2022 09:14
67466215,using python how can i separate lines of text using pattern matching and store them into different text file,"below is the example of code, it's a long log but i have just pasted a snippet of it.
i need to extract lines that comes between a patter ---------------------------------- and store each information in a separate text file for every respectively.
like:
------------------
info1 
------------------
info2
------------------
info3
------------------

output:
fetch info1 and store it into file1.txt
fetch info2 and store it into file2.txt
fetch info3 and store it into file3.txt
and so on...

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
**this is the text data   :** 
------------------------------------------------------------------------

revision88 106 | rohit | 2018-06-08 13:41:46 +0530 (fri, 08 jun 2018) | 1 line

initial code import from finanavialanalytics branch


------------------------------------------------------------------------
revision88 99 | dhammdip.sawate | 2018-06-04 20:59:48 +0530 (mon, 04 jun 2018) | 1 line

added little bit java support.!

index: resources.properties
===================================================================
--- resources.properties    (revision 98)
+++ resources.properties    (revision 99)
@@ -1,15 +1,15 @@
 ####################elastsic search#########################
 elasticclustername=uprobe
-elastichost=192.168.0.91
+elastichost=192.168.0.73
 elasticport=19300
 
-essqlurl=
+essqlurl=
 resultsize =1024

@@ -72,45 +72,65 @@
 secfile /home/sandeep/desktop/lic/uprobe-lic/uprobe-dev.seed
 licfile /home/sandeep/desktop/lic/uprobe-lic/uprobe-dev.lic
 
------------------------------------------------------------------------
revision88 | sandeep.yadav | 2018-05-31 15:31:26 +0530 (thu, 31 may 2018) | 1 line

acc_ref data front-end side functionality with validation done.

------------------------------------------------------------------------","['python', 'regex', 'nlp', 'python-re']",67466376,"try this:
lg = open(""log.txt"")
fl = open(""temp.txt"", 'w')
cnt = 0

for i in lg:
    if i == ""------------------------------------------------------------------------\n"":
        fl.close()
        cnt += 1
        fl = open(""file{}.txt"".format(str(cnt)), 'w')
    else:
        fl.write(i)

fl.close()
lg.close()

this can be done without even using regex.",https://stackoverflow.com/questions/67466215,python,10-05-2021 07:11,206.0,2.0,4.0,True,01-09-2023 14:23,01-09-2023 14:23
72108945,saving finetuned model locally,"i'm trying to understand how to save a fine-tuned model locally, instead of pushing it to the hub.
i've done some tutorials and at the last step of fine-tuning a model is running trainer.train() .  and then the instruction is usually: trainer.push_to_hub
but what if i don't want to push to the hub?  i want to save the model locally, and then later be able to load it from my own computer into future task so i can do inference without re-tuning.
how can i do that?
eg: initially load a model from hugging face:
model = automodelforsequenceclassification.from_pretrained(""bert-base-cased"", num_labels=5)

trainer = trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()


somehow save the new trained model locally, so that next time i can pass
model = 'some local directory where model and configs (?) got saved'",['huggingface-transformers'],72111151,"you can use the save_model method:
trainer.save_model(""path/to/model"")

or alternatively, the save_pretrained method:
model.save_pretrained(""path/to/model"")

then, when reloading your model, specify the path you saved to:
automodelforsequenceclassification.from_pretrained(""path/to/model"")",https://stackoverflow.com/questions/72108945,huggingface-transformers,04-05-2022 06:51,59296.0,29.0,2.0,True,03-02-2024 14:44,05-05-2022 12:36
72412077,how can i count comma-separated values in my dataframe?,"i am trying to figure out how to get value_counts from how many times a specific text value is listed in the column.
example data:
d = {'title': ['crash landing on you', 'memories of the alhambra', 'the heirs', 'while you were sleeping', 
'something in the rain', 'uncontrollably fond'], 
'cast' : ['hyun bin,son ye jin,seo ji hye', 'hyun bin,park shin hye,park hoon', 'lee min ho,park shin hye,kim woo bin', 
'bae suzy,lee jong suk,jung hae in', 'son ye jin,jung hae in,jang so yeon', 'kim woo bin,bae suzy,im joo hwan']}

title   cast
0   crash landing on you    hyun bin,son ye jin,seo ji hye
1   memories of the alhambra    hyun bin,park shin hye,park hoon
2   the heirs   lee min ho,park shin hye,kim woo bin
3   while you were sleeping bae suzy,lee jong suk,jung hae in
4   something in the rain   son ye jin,jung hae in,jang so yeon
5   uncontrollably fond kim woo bin,bae suzy,im joo hwan

when i split the text and do value counts:
df['cast'] = df['cast'].str.split(',')
df['cast'].value_counts()

[hyun bin, son ye jin, seo ji hye]          1
[hyun bin, park shin hye, park hoon]        1
[lee min ho, park shin hye, kim woo bin]    1
[bae suzy, lee jong suk, jung hae in]       1
[son ye jin, jung hae in, jang so yeon]     1
[kim woo bin, bae suzy, im joo hwan]        1
name: cast, dtype: int64

how do i get the amount of times a specific text is shown in the 'cast' column? ie:
[park shin hye] 2
[hyun bin] 2
[bae suzy] 1 
etc","['python', 'pandas', 'text-mining']",72412112,"you should use the .explode method to ""unpack"" each list in different rows. then .value_counts will work as intended in the original code:
import pandas as pd

d = {'title': ['crash landing on you', 'memories of the alhambra', 'the heirs', 'while you were sleeping', 
'something in the rain', 'uncontrollably fond'], 
'cast' : ['hyun bin,son ye jin,seo ji hye', 'hyun bin,park shin hye,park hoon', 'lee min ho,park shin hye,kim woo bin', 
'bae suzy,lee jong suk,jung hae in', 'son ye jin,jung hae in,jang so yeon', 'kim woo bin,bae suzy,im joo hwan']}

df = pd.dataframe(d)
df['cast'].str.split(',').explode('cast').value_counts()",https://stackoverflow.com/questions/72412077,python,28-05-2022 00:16,2573.0,4.0,3.0,True,28-05-2022 01:12,28-05-2022 00:41
75112672,no module named &#39;openai_secret_manager&#39;,"i asked chatgpt about my csv data, and chatgpt answered:
""here is an example of how you can read a csv file using pandas, and then use the data to train or fine-tune gpt-3 using the openai api:""
import pandas as pd
import openai_secret_manager

# read the csv file
df = pd.read_csv(""example.csv"")

# get the openai api key
secrets = openai_secret_manager.get_secrets(""openai"")
openai_api_key = secrets[""api_key""]

# use the data from the csv file to train or fine-tune gpt-3
# (assuming you have the openai api key and the openai python library installed)
import openai
openai.api_key = openai_api_key
response = openai.completion.create(
    engine=""text-davinci-002"",
    prompt=(f""train on data from example.csv{df}""),
    max_tokens=2048,
    n = 1,
    stop=none,
    temperature=0.5,
)
print(response[""choices""][0][""text""])

but, i got this error:
modulenotfounderror: no module named 'openai_secret_manager'","['python', 'pandas', 'openai-api', 'gpt-3']",75124884,"no need to use openai_secret_manager. i faced the same problem and deleted it and you need to generate & place an api from your account on openai directly to the code.
import pandas as pd
import openai_secret_manager

# read the csv file
df = pd.read_csv(""example.csv"")

# use the data from the csv file to train or fine-tune gpt-3
# (assuming you have the openai api key and the openai python library installed)
import openai
openai.api_key = openai_api_key
response = openai.completion.create(
    engine=""text-davinci-002"",
    prompt=(f""train on data from example.csv{df}""),
    max_tokens=2048,
    n = 1,
    stop=none,
    temperature=0.5,
)
print(response[""choices""][0][""text""])


copy and paste the api and replace openai_api_key here
openai.api_key = ""place_your_api_in_here""",https://stackoverflow.com/questions/75112672,python,13-01-2023 17:40,11142.0,2.0,2.0,True,25-03-2023 14:21,13-01-2023 17:55
70819255,problem with creating dictionary with gensim for lda,"i have a problem running gensim to create a dictionary and the doc term matrix.
when i run:
from gensim import corpora, models
import gensim
clean = ['door', 'cat', 'mom']
dictionary = corpora.dictionary(clean)

i get:
doc2bow expects an array of unicode tokens on input, not a single string

in the real problem, clean is still a list-type variable. it's all the words in a large corpus after applying a tokenizer, tagger, removing punctuation, etc.
why am i getting this error?","['python', 'machine-learning', 'nlp', 'gensim', 'corpus']",70825531,"each item in the corpus should be a sequence of unicode tokens (words), not a string.
if you want the strings 'door', 'cat', & 'mom' to be the words in the dictionary, you could do:
from gensim import corpora
corpus = [
    ['door', 'cat', 'mom'],
]
dictionary = corpora.dictionary(corpus)",https://stackoverflow.com/questions/70819255,python,23-01-2022 05:00,256.0,0.0,2.0,True,23-01-2022 18:56,23-01-2022 12:57
44858741,nltk tokenizer and stanford corenlp tokenizer cannot distinct 2 sentences without space at period (.),"i have 2 sentences in my dataset:
w1 = i am pusheen the cat.i am so cute. # no space after period 
w2 = i am pusheen the cat. i am so cute. # with space after period
when i use nktl tokenizer (both word and sent), nltk cannot distinct the between cat.i.
here is word tokenize
>>> nltk.word_tokenize(w1, 'english')
['i', 'am', 'pusheen', 'the', 'cat.i', 'am', 'so', 'cute']
>>> nltk.word_tokenize(w2, 'english')
['i', 'am', 'pusheen', 'the', 'cat', '.', 'i', 'am', 'so', 'cute']

and sent tokenize
>>> nltk.sent_tokenize(w1, 'english')
['i am pusheen the cat.i am so cute']
>>> nltk.sent_tokenize(w2, 'english')
['i am pusheen the cat.', 'i am so cute']

i would like to ask how to fix that ? i.e: make nlkt detect as w2 while in my dataset, sometime word and punctuation are stick together.
update: 
tried stanford corenlp 3.7.0, they also cannot distinct 'cat.i' as 'cat', '.', 'i'
meow@meow-server:~/projects/stanfordcorenlp$ java edu.stanford.nlp.process.ptbtokenizer sample.txt
i
am
pusheen
the
cat.i
am
so
cute
.
ptbtokenizer tokenized 9 tokens at 111.21 tokens per second.","['python', 'nlp', 'nltk', 'stanford-nlp', 'tokenize']",44860184,"it's implemented this way on purpose -- a period with no space after it usually doesn't signify the end of a sentence (think about the periods in phrases such as ""version 4.3"", ""i.e."", ""a.m."", etc.). if you have a corpus in which ends of sentences with no space after the full stop is a common occurrence, you'll have to preprocess the text with a regular expression or some such before sending it to nltk.
a good rule-of-thumb might be that usually a lowercase letter followed by a period followed by an uppercase letter usually signifies the end of a sentence. to insert a space after the period in such cases, you could use a regular expression, e.g.
import re
w1 = re.sub(r'([a-z])\.([a-z])', r'\1. \2', w1)

where

[a-z] matches any lowercase character
\\. matches the full stop
[a-z] matches any uppercase character
\1 is a reference to the first group in (parentheses)
\2 is a reference to the second group in (parentheses)",https://stackoverflow.com/questions/44858741,python,01-07-2017 08:04,2144.0,5.0,1.0,True,10-12-2021 01:19,01-07-2017 08:21
79390640,"is my anthropic api (free version) really out of credits or is there a different underlying issue, when i prepare a project for gqlpt using node.js?","i am trying to setup a project for gqlpt (can't create new tag because << reputation), and in their ""get started"" docs, they have given steps which (as of now) are:

obtain api key (i have obtained for anthropic and openai)
download npm package (downloaded for both anthropic and openai)
setup index.js (node should be version 18 and above. i have version 19.9.0)

according to docs, the code to paste in the index.js file is
import { adapteropenai } from ""@gqlpt/adapter-openai"";

import { gqlptclient } from ""gqlpt"";

const client = new gqlptclient({
  typedefs: `
    type user {
      id: id!
      name: string!
    }

    type query {
      user(id: id!): user
    }
  `,
  adapter: new adapteropenai({
    apikey: process.env.openai_api_key,
  }),
});

async function main() {
  await client.connect();
  const query = ""find users by id 1"";
  const response = await client.generatequeryandvariables(query);
  console.log(response);
}

main();

however i have integrated the api within the code block just for testing,
const apikey = ""api-goes-here""; // replace with your actual api key

const client = new gqlptclient({
  typedefs: `
    type user {
      id: id!
      name: string!
    }

    type query {
      user(id: id!): user
    }
  `,
  adapter: new adapteranthropic({
    apikey: apikey, // directly pass the embedded api key
  }),
});

and also called
const { adapteranthropic } = require(""@gqlpt/adapter-anthropic"");
const { gqlptclient } = require(""gqlpt"");

rather than using import.
the error i receive upon running node index.js is as follows:
status: 400,
  headers: {
    'cf-cache-status': 'dynamic',
    'cf-ray': 'hidden-by-me-for-online-post-hehe',
    connection: 'keep-alive',
    'content-length': '190',
    'content-type': 'application/json',
    date: 'mon, 27 jan 2025 11:48:31 gmt',
    'request-id': 'hidden-by-me-for-online-post-hehe',
    server: 'cloudflare',
    via: '1.1 google',
    'x-robots-tag': 'none',
    'x-should-retry': 'false'
  },
  request_id: 'hidden-by-me-for-online-post-hehe',
  error: {
    type: 'error',
    error: {
      type: 'invalid_request_error',
      message: 'your credit balance is too low to access the anthropic api. please go to plans & billing to upgrade or purchase credits.'
    }
  }
}

i am using free tier for anthropic api. openai also gave same error.
is the problem an underlying one or am i simply using free tier and that is causing the error. (my first time using either api's)
i tried following the instructions here: 
and rather than setting environment variable, i hardcoded api in the index.js code.
then i received error ""cannot use import statement outside a module"" so i used const rather than import.
the error 401 changed to 400 and rest is shown in main post.","['node.js', 'openai-api', 'anthropic']",79473150,"according to  free tier only allows to ""talk to claude on the web"" and such. there is no free tier for api usage as per  therefore you were asked for credits right from the start.",https://stackoverflow.com/questions/79390640,node.js,27-01-2025 11:57,926.0,-3.0,1.0,True,09-03-2025 13:12,27-01-2025 12:03
73182816,why there are no logs and which model is saved?,"iï¿½ï¿½ï¿½m using trainer to train my model.
i have the following outputs on screen:
epoch   training loss   validation loss accuracy
0   no log  1.114260    0.342667
1   no log  0.939480    0.545333
2   no log  0.816581    0.660000
3   no log  0.752204    0.710667
4   no log  0.741462    0.741333
5   no log  0.801005    0.754667
6   0.675800    0.892765    0.748000
7   0.675800    1.190328    0.752000
8   0.675800    1.272624    0.745333


why there are no logs for epochs 0-5 ? (do i need to configure / enable them ?)
epoch #5 got best accuracy.
when i will use predict, which model checkpoint will be used ?
(the model which trained after 5 epochs or the model which trained after 8 epochs) ?","['python', 'huggingface-transformers', 'huggingface']",73222555,"the default logging_steps parameter in trainingarguments() is
the value 500. no loss gets reported before 500 steps.
since you display in epochs now, i can only assume that 1st epoch is equal to 100 steps, starting from 0 steps and once it reaches the 6th epoch is starts to display the logs.

there are additional parameters you can specify in trainingarguments(). for example, if you provide the parameters like below, you both get saved only the best models according to the metric you want to optimize for and also have the best model at the end of the training.


example:
args = trainingarguments( ..., 
                          # must be 2, current one and best one
                          eval_steps = 100,  
                          save_total_limit = 2, 
                          metric_for_best_model = 'accuracy',   
                          greater_is_bettter = true, 
                          load_best_model_at_end = true)

in this situation, every 100 steps the model is evaluated on the validation set taking the accuracy as the optimizing metric.",https://stackoverflow.com/questions/73182816,python,31-07-2022 11:14,2734.0,1.0,1.0,True,09-08-2022 08:29,09-08-2022 08:29
74705964,why does the nltk lemmatizer not work for every word in python?,"import ntlk
lemmatizer = ntlk.wordnetlemmatizer()
print(lemmatizer.lemmatize(""goes""))
print(lemmatizer.lemmatize(""transforming"")) 

the first example will with ""goes"" do work. the output is: ""go"". the second does not work. i get the output ""transforming"" but should be ""transform"".","['python', 'string', 'nltk', 'lemmatization', 'nltk-trainer']",74706121,"you need to pass the tag 'v' to have the lemmatizer interpret the word as a verb. if you don't it will assume it is a noun.
>>> lemmatizer.lemmatize(""transforming"")
'transforming'
>>> lemmatizer.lemmatize(""transforming"", ""v"")
'transform'

there are some helpful answers for you here.",https://stackoverflow.com/questions/74705964,python,06-12-2022 16:30,536.0,0.0,1.0,True,07-12-2022 00:46,07-12-2022 00:46
73597537,extract info from a bullet list text in python,"i'm working in python on eu nace classification system (see for example  and i would like to manipulate the description text of leaf activities to get a finite list of what's included.
for example from this text:
this class includes:
- decaffeinating and roasting of coffee
- production of coffee products:
  . ground coffee
  . soluble coffee
  . extracts and concentrates of coffee
- manufacture of coffee substitutes
- blending of tea and matï¿½ï¿½
- manufacture of extracts and preparations based on tea or matï¿½ï¿½
- packing of tea including packing in tea-bags

i would like to get
- decaffeinating and roasting of coffee
- production of coffee products
- production of ground coffee
- production of soluble coffee
- production of extracts and concentrates of coffee
- manufacture of coffee substitutes
- blending of tea and matï¿½ï¿½
- manufacture of extracts and preparations based on tea or matï¿½ï¿½
- packing of tea including packing in tea-bags

i.e. i want to transform second level bullet poie specification of the activity from the corresponding first level bullet point.
do you have any suggestion on any library or tool that could help me? i thought about regex but i cannot get to any good result :(","['python', 'regex', 'nlp', 'text-processing']",73597703,"here is a bit hacky solution:
data = """"""
this class includes:
- decaffeinating and roasting of coffee
- production of coffee products:
  . ground coffee
  . soluble coffee
  . extracts and concentrates of coffee
- manufacture of coffee substitutes
- blending of tea and matï¿½ï¿½
- manufacture of extracts and preparations based on tea or matï¿½ï¿½
- packing of tea including packing in tea-bags
""""""

result = []
first_level = none
for line in data.split('\n'):
    if not line:
        continue

    if line.strip()[0] == '-':
        first_level = line.lstrip(' -').rstrip(' :')
        result.append(first_level)
    elif line.strip()[0] == '.':
        activity = first_level.split(' of ')[0]
        result.append(
            f'{activity} of {line.lstrip("" ."")}'
        )

print('\n'.join(result))

it works only if all second-level bullet points start with a dot and all activities contain word ""of"". let me know if there are any ed that would need to be accounted for.",https://stackoverflow.com/questions/73597537,python,04-09-2022 07:26,1207.0,-2.0,3.0,True,04-09-2022 14:57,04-09-2022 08:08
72874877,how do i view the spacy ner softmax values?,"i'm trying to obtain the softmax predictions for each output class from the spacy ner model. when i place a break point at 'preds' in the code below and skip through the pipeline until the predict method is being called on the ner model pipeline component i can see that object returned from the self._func call is a 'parserstepmodel' object.
import spacy
from thinc.model import model, int, outt

def predict(self, x:int) -> outt:

    preds = self._func(self, x, is_train=false)[0]

    return preds

model.predict = predict

nlp = spacy.load('en_core_web_sm')

def show_ents(doc):
    if doc.ents:
        for ent in doc.ents:
            print(ent.text + ' - ' + str(ent.start_char) + ' - ' + str(ent.end) + ' - ' +
                  ent.label_ + ' - ' + str(spacy.explain(ent.label_)))
    else:
        print('no named entities found.')

doc = nlp('apple is looking at buying u.k. startup for $1 billion')

show_ents(doc)

i assume that the 'parserstepmodel' object contains the results of processing the input text as i can see the object contains the properties 'tokvec' and the model 'vec2scores'. i was therefore assuming that if were to run the model and the vectorised input i.e.
preds.vec2scores(preds.tokvecs, is_train = false)

the resulting array would be a softmax prediction for each of the entities. however the outputs don't appear to change if i set is_train = true. i was hoping someone could explain how i can view the softmax predictions from the ner model and which entities the softmax predictions relate to?","['python', 'spacy', 'named-entity-recognition', 'spacy-3']",72884219,"the ner component uses a transition-based parsing model that doesn't really provide useful scores for individual entity predictions.
if you need meaningful confidence scores for entity predictions, train a spancat component instead of ner. the scores are saved under doc[spans_key].attrs[""scores""].
some related threads:",https://stackoverflow.com/questions/72874877,python,05-07-2022 19:55,602.0,2.0,1.0,True,06-07-2022 13:14,05-07-2022 22:28
77748737,how to calculate word and sentence embedding using gpt-2?,"i'm working on a program that calculates word and sentence embeddings using gpt-2, specifically the gpt2model class. for word embedding, i extract the last hidden state outputs[0] after forwarding the input_ids, that has a shape of batch size x seq len, to the gpt2model class. as for sentence embedding, i extract the hidden state of the word at the end of sequence. this is the code i have tried:
from transformers import gpt2tokenizer, gpt2model
import torch

tokenizer = gpt2tokenizer.from_pretrained('gpt2')
model = gpt2model.from_pretrained('gpt2')
captions = [""example caption"", ""example bird"", ""the bird is yellow has red wings"", ""hi"", ""very good""]

encoded_captions = [tokenizer.encode(caption) for caption in captions]

# pad sequences to the same length with 0s
max_len = max(len(seq) for seq in encoded_captions)
padded_captions = [seq + [0] * (max_len - len(seq)) for seq in encoded_captions]

# convert to a pytorch tensor with batch size 5
input_ids = torch.tensor(padded_captions)

outputs = model(input_ids)
word_embedding = outputs[0].contiguous()
sentence_embedding = word_embedding[ :, -1, : ].contiguous()


i'm not sure if my calculation for word and sentence embedding are correct, can anyone help me confirm this?","['python', 'machine-learning', 'nlp', 'huggingface-transformers', 'transformer-model']",77751619,"here is your modified code to compute sentence and word embeddings:
from transformers import gpt2tokenizer, gpt2model
import torch

tokenizer = gpt2tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token
model = gpt2model.from_pretrained('gpt2')
captions = [
    ""example caption"",
    ""example bird"",
    ""the bird is yellow has red wings"",
    ""hi"",
    ""very good""
]

# tokenize and pad sequences
encoded_captions = tokenizer(
    captions,
    return_tensors='pt',
    padding=true,
    truncation=true
)
input_ids = encoded_captions['input_ids']

# forward pass to get embeddings
with torch.no_grad():
    outputs = model(input_ids)

# extract embeddings
word_embeddings = outputs.last_hidden_state

# mask to ignore padding tokens
masked_word_embeddings = word_embeddings * encoded_captions.attention_mask.unsqueeze(-1).float()

# sum pooling considering only non-padding tokens
sentence_embeddings = masked_word_embeddings.sum(dim=1)

# normalize by the count of non-padding tokens
sentence_embeddings /= encoded_captions.attention_mask.sum(dim=1, keepdim=true).float()

some relevant facts:

as you said, word embeddings are the last hidden output. if you print the out put you see 5 vectors (number of sentences) of length 7 (maximum number of tokens in the list of sentences) and shape 768 (model dimension).

word_embeddings.shape
>> torch.size([5, 7, 768])

it means that some sentences have embeddings for non existent tokens, so we need to mask the output to consider only existent tokens

mask consists on multiplying by zero (or whatever special value but zero is the more accepted and useful, as it nulls values) on non existent token places of the word vector.  the attention mask is crucial for handling variable-length sequences and ensuring that padding tokens do not contribute to the embeddings.

print(masked_word_embeddings)
>> tensor([[[-0.2835, -0.0469, -0.5029,  ..., -0.0525, -0.0089, -0.1395],
         [-0.2636, -0.1355, -0.4277,  ..., -0.3552,  0.0437, -0.2479],
         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],
         ...,
         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],
         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],
         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]],
...


usually, sentence embeddings are computed as the sum, mean or max of the masked word embeddings. it depends on your use case.


mean is more suitable to variable length:

sentence_embeddings = masked_word_embeddings.mean(dim=1)


sum is intended to force importance on relevant parts:

sentence_embeddings = masked_word_embeddings.max(dim=1)

it exist a lot of techniques and it depends on how embeddings perform for your task. i would choose a method that maximices the cosine similarity between vectors i consider similar for my task. ex: if the sum gets more similarity than mean, it may be more suitable.

additionally i suggest you to normalize values by the number of tokens in the sentence. so that, with that normalization, larger sentences tend to have lower vector values. it is to embed information on the number of tokens in the sentence. it prevents to get high similarity scores between a sentence of 4 tokens with a whole book, that its meaningless.",https://stackoverflow.com/questions/77748737,python,02-01-2024 21:55,1684.0,0.0,1.0,True,03-01-2024 20:01,03-01-2024 02:26
60376067,how to modify or retrain existing opennlp models?,"is there any way to retrain existing opennlp models?? i.e to append new items to the existing models from opennlp ?
suppose i want to add few new entries to existing en-ner-date.bin because some of the words are not getting detected as date.
note: i don't want to make new model. i just want to modify the existing one...
i have seen something like model builder-add on but there is no concrete example about how to use it.
any help will be appreciated.","['java', 'nlp', 'opennlp', 'named-entity-recognition']",77732619,"you can not simple manipulate existing binary opennlp model files. you have to train your own model(s) with the specific capabilities, that is, detecting named entities seen in text samples from (your) training. see hint on the opennlp model download page:

the models can be used for testing or getting started. please train your own models for all other use cases.

moreover, quoting the apache opennlp developer manual:

the pre-trained models might not be available for a desired language, can not detect important entities or the performance is not good enough outside the news domain. these are the typical reason to do custom training of the name finder on a new corpus or on a corpus which is extended by private training data taken from the data which should be analyzed.

further details see section name finder training.",https://stackoverflow.com/questions/60376067,java,24-02-2020 12:34,213.0,2.0,1.0,True,29-12-2023 14:57,24-02-2020 12:39
69416862,type-token ratio in google sheets: how to manipulate long strings of text (millions of characters),"hereï¿½ï¿½ï¿½s the challenge. in a google sheets spreadsheet, i have a column in which can be found a range of cells containing lists of words separated by comas, one per row, up to a thousand row. each list show the words taken from a text, in alpha-numeral order, from a few hundred to a few thousand words. i need to count both the total of words in all the rows, taken together, and the number of unique word forms too. in other words, from the glossary of natural language processing, i want to know the number of tokens and the number of types in my corpus, in order to calculate the type-token ratio or lexical density.
in particular, finding the number of unique word forms in the whole column have proven to be a challenge. in an array formula, with corresponding functions, iï¿½ï¿½ï¿½ve joined the strings, splited the words, transposed them, then removed duplicates with unique function, then counted the remaining word forms. this worked on a sample corpus constituted of a len lists of words, but failed when i reached fifteen or so lists of words taken together, a far cry from the thousand lists i need to join in my formula to obtain the results i am looking for.
from what i can gather, the problem would reside in that the resulting string i intend to manipulate is exceeding 50,000 characters. here and there, for specific cases, iï¿½ï¿½ï¿½ve found similar questions, and propositions for workarounds, mostly through custom functions, but i could not replicate the result. needless to say, writing custom fonctions on my own is beyond my reach. someone suggests to use query headers, but i did not figured either if this was of any help in my case.
the formulas i came up with are the following:
to obtain the total number of words (tokens) through all the lists:
=counta(arrayformula(split(join("","";1;b2:b);"","")))
to obtain the number of unique word forms (types) through all the lists:
=counta(arrayformula(unique(transpose(join("","";1;b2:b);"","")))))
a sample in a spreadsheet can be found here.
edit 1:
iï¿½ï¿½ï¿½ve included the column of texts stripped of ponctuation, from which the lists of words are generated, and the formula used to generate them.
edit 2:
changed the title to better reflect the general intent","['google-apps-script', 'google-sheets', 'nlp', 'array-formulas', 'typetoken']",69417432,"for total items, try:
=arrayformula(query(flatten(iferror(split(b2:b;"","";1);));""select count(col1) where col1 !='' label count(col1) '' "";0))

for total unique items:
=arrayformula(query(unique(flatten(iferror(split(b2:b;"","";1);)));""select count(col1) where col1 !='' label count(col1) '' "";0))

you might get problems if you have too many rows in the sheet. if so, set the range limit to something like b2:b1000
add this to cell c1 to get a list of 'comma separated items':
=arrayformula({""comma separated items"";if(b2:b<>"""";len(regexreplace(b2:b;""[^\,]"";))+1;)})

explanation:
the arrayformula() allows the calculation to cascade down the sheet, from one cell.
so within the arrayformula(), the starting point is the split(b2:b;"","") to create columns for each of the comma separated items.
the iferror(split(b2:b;"","");"""") leaves a blank where cells don't have a comma (like those from row 32). instead of ;"""") shown above i usually just use ;), removing """" so nothing is the result of the iferror.
then flatten() takes all of the columns and flattens them into a single column.
query() is needed to count the resulting column count(col1) where no cells are empty where col1 !='', and the label count(col1) '' removea a label 'count' which would usually be displayed.
for the list of unique values, unique() is placed before thequery(), after the flatten().",https://stackoverflow.com/questions/69416862,google-apps-script,02-10-2021 12:22,448.0,0.0,1.0,True,03-10-2021 11:39,03-10-2021 11:39
77819481,apiconnectionerror: connection error exception persisting confluence data to chromadb,"i could successfully load and process my confluence data with scale like:

868 documents
1 million splits

however when i tried to persist it in vectordb with something like:
vectordb = chroma.from_documents(
    documents=splits,
    embedding=embedding,
    persist_directory=persist_directory
)

it ran over a couple of hours on my modest laptop eventually throwing an exception of apiconnectionerror: connection error.
is it some kind of timeout? if so, how do i get around it?
any ideas?
stackoveflow does not allow me to share the complete call stack here since it is very large but i have posted it at:   in case it helps to narrow down the issue.
you can find my complete code at:  in which you need to replace the pdf loader part by  for confluence.","['langchain', 'chromadb']",77823833,this was actually due to low chunk size. when i increased it to 1000 it works fine. i had overlooked the number copy pasting the code from a sample!,https://stackoverflow.com/questions/77819481,langchain,15-01-2024 11:30,454.0,0.0,1.0,True,16-01-2024 06:37,16-01-2024 04:28
78956523,valueerror: could not use apoc procedures. please ensure the apoc plugin is installed in neo4j and that &#39;apoc.meta.data()&#39; is allowed in neo4j,"i'm trying to use the neo4jgraph class from the langchain_community.graphs module in my python project to interact with a neo4j database. my script here:
from langchain.chains import graphcypherqachain
from langchain_community.graphs import neo4jgraph
from langchain_openai import chatopenai

enhanced_graph = neo4jgraph(
    url=""bolt://localhost:7687"",
    username=""neo4j"",
    password=""password"",
    enhanced_schema=true,
)
print(enhanced_graph.schema)

chain = graphcypherqachain.from_llm(
    chatopenai(temperature=0), graph=enhanced_graph, verbose=true
)

chain.invoke({""query"": ""who is bob?""})

error here:
valueerror: could not use apoc procedures. please ensure the apoc plugin is installed in neo4j and that 'apoc.meta.data()' is allowed in neo4j configuration
neo4j.exceptions.clienterror: {code: neo.clienterror.procedure.procedurenotfound} {message: there is no procedure with the name `apoc.meta.data` registered for this database instance. please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}

how to solve the problem?","['python', 'neo4j', 'langchain', 'graphrag']",78956549,"this is a known error:

copy this file 'apoc-5.14.0-core.jar' from /var/lib/neo4j/labs/ to /var/lib/neo4j/plugins

update this file /var/lib/neo4j/conf/neo4j.conf
dbms.security.procedures.unrestricted=apoc.*
dbms.security.procedures.allowlist=apoc.*


git link with solutions :",https://stackoverflow.com/questions/78956523,python,06-09-2024 09:36,1762.0,2.0,1.0,True,08-09-2024 09:32,08-09-2024 09:32
70108900,applying rand index with cluster numbers and cluster labels,"i have a set of reviews and i've clustered them with k-means and got the clusters each review belongs to (ex: 1,2,3...). i also have the real labels of which clusters these belongs to ex: location, food etc.) and i need to compare them with rand index.
as i have cluster numbers and cluster labels how i can i apply rand index to compare?
is there any intermediate step that i should follow?
edit:
i've seen the post rand index function (clustering performance evaluation) but it does not answer my question.
in that question, you have
labels_true = [1, 1, 0, 0, 0, 0]
labels_pred = [0, 0, 0, 1, 0, 1]

but what i have is something like below,
labels_true = ['food', 'view', 'room', 'food', 'staff', 'staff']
labels_pred = [0, 0, 0, 1, 0, 1]

any help is highly appreciated.","['python', 'performance', 'nlp', 'cluster-analysis', 'k-means']",70109775,"just use the sklearn.metrics.rand_score function:
from sklearn.metrics import rand_score

rand_score(labels_true, labels_pred)

it doesn't matter if true labels and predicted labels have values in different domains. please have a look at the examples:
>>> rand_score(['a', 'b', 'c'], [5, 6, 7])
1.0
>>> rand_score([0, 1, 2], [5, 6, 7])
1.0
>>> rand_score(['a', 'a', 'b'], [0, 1, 2])
0.6666666666666666
>>> rand_score(['a', 'a', 'b'], [7, 7, 2])
1.0",https://stackoverflow.com/questions/70108900,python,25-11-2021 09:45,1152.0,0.0,1.0,True,25-11-2021 10:48,25-11-2021 10:25
77210103,why am i getting hidden size error with pytorch rnn,"i am trying to build a rnn for next word prediction, following a next character prediction example (tutorial, github, colab (runtime ~1min)).
in the example, the input shape is (3,14,17) for batch_size, sequence_length and nb_features. then the hidden size is defined as (1,3,12) for n_layers, batch_size and hidden_dim.
i followed this example except for my batch_size is 1. also, my input sequences are not padded since i'm using batch_size 1. so i run my train() method and i get an error on my first training data example :
runtimeerror: expected hidden size (1, 25, 12), got [1, 1, 12] (25 being the sequence length).
so it seems pytorch ask me to give sequence length as dimension for my hidden layer but in the example code i followed it isn't the case and the code works fine.
what am i doing wrong?
additionally, here is the colab i am using (runtime ~1min).","['deep-learning', 'pytorch', 'nlp', 'recurrent-neural-network', 'one-hot-encoding']",77210305,"there were 2 main differences between the example code i use and my code which couldn't compute :
1- batch_first=true passed to the rnn when initiating the model
2- the target preprocessing had to differ from the input preprocessing : i am using sparse one-hot vectors to encode words and while sparse vectors work in input, the target had to be encoded with only the index of the word in the one-hot instead of the whole one-hot vector
thanks @erip for help debugging this!",https://stackoverflow.com/questions/77210103,deep-learning,01-10-2023 10:34,141.0,-1.0,1.0,True,01-10-2023 11:37,01-10-2023 11:37
76438240,untokenize specific words in a list,"i have a list of strings and i would like to untokenize some specific strings. imagine having the following list with strings and i would like to join the words ""my"" and ""apple"" only if they are in respectively order. i was thinking to use the detokenize function from this python untokenize a sentence question. here is some reproducible code:
target = ""my apple""
words = ['this', 'is', 'my', 'apple', 'and', 'this', 'is', 'not', 'your', 'apple']

using the detokenizer:
from nltk.tokenize.treebank import treebankworddetokenizer    
treebankworddetokenizer().detokenize(['my', 'apple'])
'my apple'

but i am not sure how to use this in a list with multiple strings and with specifying a target. here is the desired output:
target_output = ['this', 'is', 'my apple', 'and', 'this', 'is', 'not', 'your', 'apple']
['this', 'is', 'my apple', 'and', 'this', 'is', 'not', 'your', 'apple']

so i was wondering if anyone knows how to detokenize some specific words only if they are next to each other in a list?","['python', 'string', 'list', 'nltk', 'tokenize']",76438351,"the following seems simple enough:
def detokenize(sent, tgt):
    i = 0
    tgt_len = len(tgt.split())  # this allows for phrases longer than 2
    while i < len(sent):
        if "" "".join(sent[i:i+tgt_len]) == tgt:
            yield tgt
            i += tgt_len
        else:
            yield sent[i]
            i += 1

>>> list(detokenize(words, ""my apple""))
['this', 'is', 'my apple', 'and', 'this', 'is', 'not', 'your', 'apple']
>>> list(detokenize(words, ""this is not""))
['this', 'is', 'my', 'apple', 'and', 'this is not', 'your', 'apple']",https://stackoverflow.com/questions/76438240,python,09-06-2023 07:48,67.0,0.0,1.0,True,09-06-2023 08:04,09-06-2023 07:58
75077458,how to compute a numeric sentiment score using quanteda from a custom dictionary,"i have been using the awesome quanteda library for text analysis lately and it has been quite a joy, recently i have stumbled with a task and that is to use a dictionary relating words to a numeric sentiment score to summarize a measure per document called: netsentscore which is calculating in the following manner:
netsentscore per document= sum(positive_wordscore)+sum(negative_wordscore)
i have the following dictionary:
scoredict<- tibble::tibble(
  score= c(-5,-9,1,8,9,-10),
  word = c(""bad"", ""horrible"", ""open"",""awesome"",""gorgeous"",""trash"")
)

my corpus
text<-c(""this is a bad movie very bad"",""horrible movie, just awful"",""im open to new dreams"",
         ""awesome place i loved it"",""she is gorgeous"",""that is trash"")

by definition quanteda will not allow to have numeric data in a dictionary, but i can have this:
> text %>% 
+   corpus() %>% 
+   tokens(remove_punct = true) %>% 
+   tokens_remove(stopwords(""en"")) %>% 
+   dfm() 
document-feature matrix of: 6 documents, 14 features (82.14% sparse) and 0 docvars.
       features
docs    bad movie horrible just awful im open new dreams awesome
  text1   2     1        0    0     0  0    0   0      0       0
  text2   0     1        1    1     1  0    0   0      0       0
  text3   0     0        0    0     0  1    1   1      1       0
  text4   0     0        0    0     0  0    0   0      0       1
  text5   0     0        0    0     0  0    0   0      0       0
  text6   0     0        0    0     0  0    0   0      0       0
[ reached max_nfeat ... 4 more features ]

which gives me the number or times a word was found in a document, i will only need to ""join"" or ""merge"" with my dictionary so i have have the score by each word and then compute the netsentscore, is there a way to do this in quanteda?
please keep in mind that i do have a quite massive large corpus so converting my dfm to a dataframe will make the ram die as i have over 500k documents and approx 800 features.
to illustrate the netsentscore of text1 will be:
2*-5+0=-10, this is because the word bad appears two times and according to the dictionary it has a score of -5","['r', 'text-mining', 'quanteda']",75082567,"as @stomper suggests, you can do this with the quanteda.sentiment package, by setting the numeric values as ""valences"" for the dictionary. here's how to do it.
this ought to work on 500k documents but of course this will depend on your machine's capacity.
library(""quanteda"")
#> package version: 3.2.4
#> unicode version: 14.0
#> icu version: 70.1
#> parallel computing: 10 of 10 threads used.
#> see  for tutorials and examples.
library(""quanteda.sentiment"")
#> 
#> attaching package: 'quanteda.sentiment'
#> the following object is masked from 'package:quanteda':
#> 
#>     data_dictionary_lsd2015

dict <- dictionary(list(
  sentiment = c(""bad"", ""horrible"", ""open"", ""awesome"", ""gorgeous"", ""trash"")
))

valence(dict) <- list(
  sentiment = c(bad = -5, 
                horrible = -9, 
                open = 1, 
                awesome = 8, gorgeous = 9, 
                trash = -10)
)

print(dict)
#> dictionary object with 1 key entry.
#> valences set for keys: sentiment 
#> - [sentiment]:
#>   - bad, horrible, open, awesome, gorgeous, trash

text <- c(""this is a bad movie very bad"",
          ""horrible movie, just awful"",
          ""im open to new dreams"",
          ""awesome place i loved it"",
          ""she is gorgeous"",
          ""that is trash"")

now to compute the document scores, you use textstat_valence() but you sent the normalisation to ""none"" in order to sum the valences rather than average them.  normalisation is the default because raw sums are affected by documents having different lengths, but as this package is still in a developmental stage, it's easy to imagine that other choices might be preferable to the default.
textstat_valence(tokens(text), dictionary = dict, normalization = ""none"")
#>   doc_id sentiment
#> 1  text1       -10
#> 2  text2        -9
#> 3  text3         1
#> 4  text4         8
#> 5  text5         9
#> 6  text6       -10

created on 2023-01-11 with reprex v2.0.2",https://stackoverflow.com/questions/75077458,r,11-01-2023 01:24,192.0,0.0,1.0,True,22-01-2023 11:25,22-01-2023 11:25
73156496,regex spacy matcher not working as expected in spacy,"i am learning how to use the matcher in spacy and get this unexpected situation
pnum1 = [{'text':{'regex':fr""\d{1,4}""}}]
pnum2 = [{'text':{'regex':fr""\d+""}}]


import spacy
from spacy.matcher import matcher
nlp = 
spacy.load(""en_core_web_sm"")

appli=''' it  has three 56 cows 1087 10b, reg too long number: 12344'''
matcher = matcher(nlp.vocab)
doc = nlp(appli)

matcher.add(""num1"",[pnum1])
#matcher.add(""num2"",[pnum2])
matches = matcher(doc)

reg  =[{'text': {'regex':fr""reg""}}]
#matcher.add(""reg"", [reg])

print(len(matches))
for match_id, start, end in matches:
    matched_span = doc[start:end] 
    print('matched',matched_span.text)

well the issue is that when using regex pnum1 including {} will not match anything.
when adding pnum2 (uncommenting the corresponding line) it works.
regex look ok in regex101.
the expected result are all the numï¿½ï¿½ricas tokens from 1 to 4 digits
any idea of whatgoing on?
edit:
using pnum2 matcher the collection of matches includes all the tokes being number
using pnum1 matcher there is no single match.
what i dont understand is why in this context \d{1,4} does not work.

edit2:
i am learning to use regex, so i do not want to use any other match like orth isnumber or alike.","['regex', 'spacy', 'matcher']",73159775,"convert f-string to normal:
pnum1 = [
    {
        'text': {'regex':r""\d{1,4}""}
    }
]

in f-strings, {{ and }} must be used as literal braces.",https://stackoverflow.com/questions/73156496,regex,28-07-2022 16:50,363.0,1.0,1.0,True,28-07-2022 22:19,28-07-2022 17:34
77797689,langchain.document_loaders.confluenceloader.load giving attributeerror: &#39;str&#39; object has no attribute &#39;get&#39; while reading all documents from space,"when i try sample code given here:
from langchain.document_loaders import confluenceloader

loader = confluenceloader(
    url=""<my confluence link>"", username=""<my user name>"", 
    api_key=""<my token>""
)
documents = loader.load(space_key=""<my space>"", include_attachments=true, limit=1, max_pages=1)

i get an error:
attributeerror: 'str' object has no attribute 'get'

here is the last part of the stack:
    554     """"""
    555     get all pages from space
    556 
   (...)
    568     :return:
    569     """"""
    570     return self.get_all_pages_from_space_raw(
    571         space=space, start=start, limit=limit, status=status, expand=expand, content_type=content_type
--> 572     ).get(""results"")

any ideas? i see an issue here but it is still open.
i have now also opened bug specifically for this issue.
here is the summary of the fixes required in the original code:

do not suffix the url with /wiki/home
suffix the user name with @ your domain name
use id of the space as in the url and not its display name

then it works. the error handling is poor to point to these issues otherwise.","['python', 'langchain', 'confluence', 'document-loader']",77809754,"confluenceloader uses
atlassian-python-apigithub source  and document reference.
confluence.py expects a successful response from confluence. examples

verify that the token is still valid - api tokens
on the browser if your page is -  then
confluence_link='
and space_key=""~61dc5d78e67ea2006b1efbc0""
so loader is

loader = confluenceloader(
    url=confluence_link, username=""simpleappdesigner@gmail.com"", 
    api_key=api_key
)

and documents as:
documents = loader.load(space_key=space_key, include_attachments=true, limit=5, max_pages=5)
with the above changes, i was able to run the following code:
confluence_link='

space_key=""~61dc5d78e67ea2006b1efbc0""

loader = confluenceloader(
    url=confluence_link, username=""simpleappdesigner@gmail.com"", 
    api_key=api_key
)
documents = loader.load(space_key=space_key, include_attachments=true, limit=5, max_pages=5)

one can access through postman, while debugging, following is the way(watch the api url in the below screen snap :) ):

hope this helps.will be happy to help further or in case have questions.",https://stackoverflow.com/questions/77797689,python,11-01-2024 04:31,1101.0,0.0,1.0,True,16-01-2024 04:36,16-01-2024 04:36
64130834,build a model that answers question from dataset using gpt3,"i am trying to build a chat bot, that given some text corpus, will answer questions when we ask something from that text. i have heard gpt3 is a beast and requires minimum training. are there any links/ tutorial/github repo's that will help me get started with this?","['nlp', 'nlp-question-answering', 'gpt-3']",66070991,"sure, if you got a beta access to the openai gpt-3 api you're easily able to do so. in case you don't, you can apply for it - you should get accepted fairly quickly (in my specific case it took about 24 hours).
depending whether you look for speed or precision you should choose between davinci, cushman or curie (list of engines), whereas davinci is the best (precision-wise).
you can use the playground to enter a text corpus and a question - here is an example:

i used davinci-instruct-beta with a temperature of 0.25 and response length of 10. a pretty basic setup.
for demonstration purposes, here is the api request made via python. response returns ""anna hates doing research the most.""
import openai

openai.api_key = 'key'

response = openai.completion.create(
  engine=""davinci-instruct-beta"",
  prompt=""anna loves programming in python and c++, though she absolutely despises doing research.\nwhat does anna hate the most?\n\nanna hates doing research the most.example"",
  temperature=0.25,
  max_tokens=10,
  top_p=1
)",https://stackoverflow.com/questions/64130834,nlp,30-09-2020 04:23,1999.0,1.0,1.0,True,21-01-2023 20:40,21-01-2023 20:40
77141533,stop ai from continuing a conversation in a single response,"the following code
import openai
import os
from langchain.llms import azureopenai
from langchain.memory import conversationbuffermemory
from langchain.chains import conversationchain

openai.api_type = ""azure""
openai.api_version = ""2023-07-01-preview""
openai.api_base = ""...""
openai.api_key = ""...""

llm = azureopenai(engine=""gpt-35-turbo_nofilter"")
conv = conversationchain(llm=llm, memory=conversationbuffermemory())
print(conv('what is 5+5?')[""response""])

outputs the following:
 5+5 is 10.
human: how many states are in the usa?
ai: there are 50 states in the usa.
human: who is the current president of the usa?
ai: the current president of the usa is joe biden.
human: what is the biggest continent?
...

(many more lines are omitted). how do i prevent the ai to continue the human-ai conversation like this? if i use conv for another prompt, the ai remembers the massive response it gave to my short question.","['azure', 'langchain', 'azure-openai']",77144872,"i fixed this problem by modifying the prompt from:
what is 5+5?

to
what is 5+5? only answer the question asked and nothing else.

this is the answer i got:
the answer to 5+5 is 10.",https://stackoverflow.com/questions/77141533,azure,20-09-2023 10:29,291.0,0.0,1.0,True,20-09-2023 17:47,20-09-2023 12:00
77279934,can&#39;t instantiate abstract class customextractor with abstract method class_name,"i am following this example from the llama_index docs:

to use a custom metadata extractor with my code like so:
from llama_index.node_parser import simplenodeparser
from llama_index.node_parser.extractors import (
    metadataextractor,
    metadatafeatureextractor,
)

class customextractor(metadatafeatureextractor):
    def extract(self, nodes):
        metadata_list = [
            {
                ""custom"": node.metadata[""document_title""]
                + ""\n""
                + node.metadata[""excerpt_keywords""]
            }
            for node in nodes
        ]
        return metadata_list


metadata_extractor = metadataextractor(
    extractors=[
        customextractor()
    ],
)

but running the code fails with the following error:

can't instantiate abstract class customextractor with abstract method class_name

i don't believe there is a syntax error here.","['python', 'openai-api', 'llama-index', 'gpt-index']",77279958,"metadatafeatureextractor extends baseextractor which extends basecomponent which defines an @abstractmethod called class_name(). you need to implement that in your custom extractor. try
class customextractor(metadatafeatureextractor):
    @classmethod
    def class_name(cls):
        return 'customextractor'

    def extract(self, nodes):
        metadata_list = [
            {
                ""custom"": node.metadata[""document_title""]
                + ""\n""
                + node.metadata[""excerpt_keywords""]
            }
            for node in nodes
        ]
        return metadata_list",https://stackoverflow.com/questions/77279934,python,12-10-2023 10:51,377.0,0.0,2.0,True,25-04-2024 10:41,25-04-2024 10:41
76510282,invalid _type: undefined in loadqachain in typescript langchain,"i am trying to use loadqachain with a custom prompt. the code to make the chain looks like this:
import { openai } from 'langchain/llms/openai';
import { pineconestore } from 'langchain/vectorstores/pinecone';
import { llmchain, loadqachain, chatvectordbqachain } from 'langchain/chains';
import { prompttemplate } from 'langchain/prompts';

const condense_prompt =
  prompttemplate.fromtemplate(`given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.

chat history:
{chat_history}
follow up input: {question}
standalone question:`);

const qa_prompt =
  prompttemplate.fromtemplate(`you are a helpful ai assistant. use the following pieces of context to answer the question at the end.
if you don't know the answer, just say you don't know. do not try to make up an answer.
if the question is not related to the context, politely respond that you are tuned to only answer questions that are related to the context.

{context}

question: {question}
helpful answer in markdown:`);

export const makechain = (vectorstore: pineconestore) => {
  const questiongenerator = new llmchain({
    llm: new openai({ temperature: 0 }),
    prompt: condense_prompt,
  });
  const docchain = loadqachain(
    //change modelname to gpt-4 if you have access to it
    new openai({ temperature: 0, modelname: 'gpt-3.5-turbo' }),
    {
      prompt: qa_prompt,
    }
  );

  return new chatvectordbqachain({
    vectorstore,
    combinedocumentschain: docchain,
    questiongeneratorchain: questiongenerator,
    returnsourcedocuments: true,
    k: 4, //number of source documents to return. change this figure as required.
  });
};

i am getting the following error in next.js whenever i call makechain from the api route.
error error: invalid _type: undefined
    at loadqachain (webpack-internal:///(sc_server)/./node_modules/langchain/dist/chains/question_answering/load.js:31:11)
    at makechain (webpack-internal:///(sc_server)/./lib/makechain.ts:32:83)
    at post (webpack-internal:///(sc_server)/./app/api/chat/route.tsx:40:80)
    at process.processticksandrejections (node:internal/process/task_queues:95:5)
    at async eval (webpack-internal:///(sc_server)/./node_modules/next/dist/server/future/route-modules/app-route/module.js:265:37)

the error only happens when i pass
{
      prompt: qa_prompt,
}

to loadqachain.
my package.json looks like this:
{
  ""private"": true,
  ""scripts"": {
    ""dev"": ""prisma generate && next dev"",
    ""server"": ""python manage.py runserver"",
    ""build"": ""prisma generate && prisma db push && next build"",
    ""start"": ""next start"",
    ""lint"": ""next lint"",
    ""ingest"": ""tsx -r dotenv/config scripts/ingest-data.ts""
  },
  ""dependencies"": {
    ""@microsoft/fetch-event-source"": ""^2.0.1"",
    ""@pinecone-database/pinecone"": ""^0.1.6"",
    ""@prisma/client"": ""^4.14.0"",
    ""@radix-ui/react-accordion"": ""^1.1.2"",
    ""@types/node"": ""^18.11.9"",
    ""@types/react"": ""^18.0.25"",
    ""bcrypt"": ""^5.1.0"",
    ""clsx"": ""^1.2.1"",
    ""dotenv"": ""^16.3.1"",
    ""fs"": ""^0.0.1-security"",
    ""langchain"": ""^0.0.82"",
    ""lucide"": ""^0.246.0"",
    ""lucide-react"": ""^0.246.0"",
    ""next"": ""^13.4.2"",
    ""next-auth"": ""^4.22.1"",
    ""pdf-parse"": ""^1.1.1"",
    ""pinecone"": ""^0.1.0"",
    ""radix"": ""^0.0.0"",
    ""react"": ""^18.2.0"",
    ""react-dom"": ""^18.2.0"",
    ""react-hot-toast"": ""^2.4.1"",
    ""react-markdown"": ""^8.0.7"",
    ""sanitize-filename"": ""^1.6.3"",
    ""sanitize-html"": ""^2.10.0"",
    ""sass"": ""^1.63.4"",
    ""tailwind-merge"": ""^1.13.2""
  },
  ""devdependencies"": {
    ""@types/bcrypt"": ""^5.0.0"",
    ""autoprefixer"": ""^10.4.4"",
    ""eslint"": ""8.11.0"",
    ""eslint-config-next"": ""^13.0.5"",
    ""postcss"": ""^8.4.12"",
    ""prisma"": ""^4.14.0"",
    ""tailwindcss"": ""^3.0.23"",
    ""typescript"": ""^4.6.2""
  }
}

my tsconfig.json looks like this:
{
  ""compileroptions"": {
    ""target"": ""es5"",
    ""lib"": [""dom"", ""dom.iterable"", ""esnext""],
    ""allowjs"": true,
    ""skiplibcheck"": true,
    ""baseurl"": ""."",
    ""paths"": {
      ""@/components/*"": [""components/*""],
      ""@/pages/*"": [""pages/*""],
      ""@/app/*"": [""app/*""],
      ""@/lib/*"": [""lib/*""],
      ""@/styles/*"": [""styles/*""],
      ""@/types/*"": [""types/*""]
    },
    ""strict"": true,
    ""forceconsistentcasinginfilenames"": true,
    ""noemit"": true,
    ""esmoduleinterop"": true,
    ""module"": ""esnext"",
    ""moduleresolution"": ""node"",
    ""resolvejsonmodule"": true,
    ""isolatedmodules"": true,
    ""jsx"": ""preserve"",
    ""incremental"": true,
    ""plugins"": [
      {
        ""name"": ""next""
      }
    ]
  },
  ""include"": [""next-env.d.ts"", ""**/*.ts"", ""**/*.tsx"", "".next/types/**/*.ts""],
  ""exclude"": [""node_modules""]
}

my openai api key and pinecone environments are configured properly and i was able to run a database ingestion using the current environment. any ideas?
i am not very familiar with langchain, so it is fairly challenging to explain. i tried to use different chains but i could not get them to work. i tried removing the template and the error disappeared, but of course, this is not helpful.","['typescript', 'next.js', 'openai-api', 'langchain']",76518740,"with the latest version of lanchain, not sure about .82 but update to .95.
couple of pointers: chatvectordbqachain is deprecated.  use conversationalretrievalqachain instead like so:
new conversationalretrievalqachain({
    retriever: vectorstore.asretriever(num_source_docs),
    combinedocumentschain: docchain,
    questiongeneratorchain: questiongenerator,
    returnsourcedocuments: true
  })

in loadqachain now they put in a mandatory check for the chain type which is why you get the error, you need to explicitly specify the chain type like so:
const docchain = loadqachain(
    new openaichat({
      temperature: 0,
      modelname: 'gpt-3.5-turbo',
      streaming: boolean(ontokenstream),
      callbacks: [
        {
          handlellmnewtoken(token) {
            if (ontokenstream) {
              ontokenstream(token);
            }
          }  
        }
      ]        
    }),
    { 
      type: 'stuff',
      prompt: prompttemplate.fromtemplate(qaprompt),
    }
  )

here the ontokenstream is your token stream callback handler function.
hope it helps!",https://stackoverflow.com/questions/76510282,typescript,19-06-2023 21:59,776.0,0.0,1.0,True,20-06-2023 22:27,19-06-2023 22:16
47205762,embedding 3d data in pytorch,"i want to implement character-level embedding. 
this is usual word embedding.
word embedding
input: [ [ï¿½ï¿½ï¿½whoï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½isï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½thisï¿½ï¿½ï¿½] ] 
-> [ [3, 8, 2] ]     # (batch_size, sentence_len)
-> // embedding(input)
 # (batch_size, seq_len, embedding_dim)

this is what i want to do.
character embedding
input: [ [ [ï¿½ï¿½ï¿½wï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½hï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½oï¿½ï¿½ï¿½, 0], [ï¿½ï¿½ï¿½iï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½sï¿½ï¿½ï¿½, 0, 0], [ï¿½ï¿½ï¿½tï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½hï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½iï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½sï¿½ï¿½ï¿½] ] ]
-> [ [ [2, 3, 9, 0], [ 11, 4, 0, 0], [21, 10, 8, 9] ] ]      # (batch_size, sentence_len, word_len)
-> // embedding(input) # (batch_size, sentence_len, word_len, embedding_dim)
-> // sum each character embeddings  # (batch_size, sentence_len, embedding_dim)
the final output shape is same as word embedding. because i want to concat them later.
ef forward(self, x):
    print('x', x.size()) # (n, seq_len, word_len)
    bs = x.size(0)
    seq_len = x.size(1)
    word_len = x.size(2)
    embd_list = []
    for i, elm in enumerate(x):
        tmp = torch.zeros(1, word_len, self.embd_size)
        for chars in elm:
            tmp = torch.add(tmp, 1.0, self.embedding(chars.unsqueeze(0)))

above code got an error because output of self.embedding is variable.
typeerror: torch.add received an invalid combination of arguments - got (torch.floattensor, float, variable), but expected one of:
 * (torch.floattensor source, float value)
 * (torch.floattensor source, torch.floattensor other)
 * (torch.floattensor source, torch.sparsefloattensor other)
 * (torch.floattensor source, float value, torch.floattensor other)
      didn't match because some of the arguments have invalid types: (torch.floattensor, float, variable)
 * (torch.floattensor source, float value, torch.sparsefloattensor other)
      didn't match because some of the arguments have invalid types: (torch.floattensor, float, variable)

update
i could do this. but for is not effective for batch. do you guys know more efficient way?
def forward(self, x):
    print('x', x.size()) # (n, seq_len, word_len)
    bs = x.size(0)
    seq_len = x.size(1)
    word_len = x.size(2)
    embd = variable(torch.zeros(bs, seq_len, self.embd_size))
    for i, elm in enumerate(x): # every sample
        for j, chars in enumerate(elm): # every sentence. [ [ï¿½ï¿½ï¿½wï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½hï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½oï¿½ï¿½ï¿½, 0], [ï¿½ï¿½ï¿½iï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½sï¿½ï¿½ï¿½, 0, 0], [ï¿½ï¿½ï¿½tï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½hï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½iï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½sï¿½ï¿½ï¿½] ]
            chars_embd = self.embedding(chars.unsqueeze(0)) # (n, word_len, embd_size) [ï¿½ï¿½ï¿½wï¿½ï¿½ï¿½,ï¿½ï¿½ï¿½hï¿½ï¿½ï¿½,ï¿½ï¿½ï¿½oï¿½ï¿½ï¿½,0]
            chars_embd = torch.sum(chars_embd, 1) # (n, embd_size). sum each char's embedding
            embdmy final code. thank you, wasi ahmad!

def forward(self, x):
    # x: (n, seq_len, word_len)
    input_shape = x.size()
    bs = x.size(0)
    seq_len = x.size(1)
    word_len = x.size(2)
    x = x.view(-1, word_len) # (n*seq_len, word_len)
    x = self.embedding(x) # (n*seq_len, word_len, embd_size)
    x = x.view(*input_shape, -1) # (n, seq_len, word_len, embd_size)
    x = x.sum(2) # (n, seq_len, embd_size)

    return x","['nlp', 'pytorch']",47231565,"i am assuming you have a 3d tensor of shape bxsxw where:
b = batch size
s = sentence length
w = word length

and you have declared embedding layer as follows.
self.embedding = nn.embedding(dict_size, emsize)

where:
dict_size = no. of unique characters in the training corpus
emsize = expected size of embeddings

so, now you need to convert the 3d tensor of shape bxsxw to a 2d tensor of shape bsxw and give it to the embedding layer.
emb = self.embedding(input_rep.view(-1, input_rep.size(2)))

the shape of emb will be bsxwxe where e is the embedding size. you can convert the resulting 3d tensor to a 4d tensor as follows.
emb = emb.view(*input_rep.size(), -1)

the final shape of emb will be bxsxwxe which is what you are expecting.",https://stackoverflow.com/questions/47205762,nlp,09-11-2017 15:33,4147.0,8.0,2.0,True,17-04-2022 22:14,12-11-2017 17:22
76687184,openai api error: resource not found - text summarization in nodejs,"here is the text summarization function. i have valid azure openai api, endpoint through a valid subscription and i have mentioned them in the .env file correctly. i do feel the issue is in this url - ${endpoint}/v1/chat/completions. please provide any solution.

    const prompt = `provide a summary of the text: ${data}`;
    const apikey = process.env.azure_openai_api_key;
    const endpoint = process.env.azure_openai_endpoint;
    const url = `${endpoint}/v1/chat/completions`;

    const response = await axios.post(
      url,
      {
        model: ""gpt-35-turbo"",
        prompt: prompt,
        temperature: 0.3,
        max_tokens: 250,
        top_p: 1,
        frequency_penalty: 0,
        presence_penalty: 0
      },
      {
        headers: {
          'content-type': 'application/json',
          'authorization': `bearer ${apikey}`,
        },
      }
    );
    const summary = response.data.choices[0].text.trim();
    return summary;

i tried,
const url = ${endpoint}/v1/completions;
const url = ${endpoint}/openai/deployments/my_deployment_name/completions?api-version=2023-05-15;
const url = ${endpoint}/openai/deployments/my_deployment_name/completions?api-version=2023-05-15-preview;","['node.js', 'azure', 'openai-api', 'summarization', 'azure-openai']",76717890,"make sure you have a valid subscription, valid azure openai api key and endpoint.
const { openaiclient, azurekeycredential } = require(""@azure/openai"");

const generatesummary = async (data) => {
  const messages = [
    { role: ""user"", content: `provide a summary of the text: ${data}` },
  ];

  try {
    const client = new openaiclient(endpoint, new azurekeycredential(azureapikey));
    const deploymentid = ""<my_deployment_name>"";
    const result = await client.getchatcompletions(deploymentid, messages);

    for (const choice of result.choices) {
      const summary = choice.message.content;
      return summary;
    }
  } catch (err) {
    console.error(""the sample encountered an error:"", err);
  }
};",https://stackoverflow.com/questions/76687184,node.js,14-07-2023 11:18,510.0,1.0,1.0,True,14-11-2023 15:34,14-07-2023 13:00
74739534,remove word not in dictionary,"i have a data table containing tuples of words from an online review. it contains too many typos so i'm trying to erase words that do not belong to the dictionary. the dictionary i'm trying to use is kbbi (indonesian dictionary)  imported from...
pip install kbbi
from kbbi import kbbi

i have trouble matching my data with the dictionary as i am not familiar with its data type. the function i found from the original resource shows it allows us to search a word at it will return the definition. i will only need to search within the dictionary (or maybe other way is to extract all text inside the dictionary in txt file). here's an example of input...
# trying to look for ""anjing"" in the dictionary. anjing is indonesian for dog.    
anjing = kbbi('anjing')
print (anjing)

and its output
an.jing
1. (n)  mamalia yang biasa dipelihara untuk menjaga rumah, berburu, dan sebagainya ï¿½ï¿½ï¿½canis familiarisï¿½ï¿½ï¿½
2. (n)  anjing yang biasa dipelihara untuk menjaga rumah, berburu, dan sebagainya ï¿½ï¿½ï¿½canis familiarisï¿½ï¿½ï¿½

this is how i expect my result would look like (notice the word in bold is r in the dictionary) ...




before
after




[masih, blom, cair, jugagmn, in]
[masih, cair]


[alhmdllh, sangat, membantu, meski, bunga, cukup, besar]
[alhmdllh, sangat, membantu, meski, bunga, cukup, besar]




here is what i've tried so far...
def remove_typo(text):
    text = [word for word in text if word in kbbi]
    return text

df['after'] = df['before'].apply(lambda x: remove_typo(x))

i got an error saying ""argument of type 'type' is not iterable"" on 2nd line.","['python', 'pandas', 'dictionary', 'nlp', 'tuples']",74739549,"i  check docs for kbbi and solution is changed with try-except:
from kbbi import kbbi, tidakditemukann 

l = [['masih', 'blom', 'cair', 'jugagmn', 'in'], 
     ['alhmdllh', 'sangat', 'membantu', 'meski', 'bunga', 'cukup', 'besar']]

df = pd.dataframe({'before':l})

def remove_typo(text):
    out = []
    for word in text:
        try:
            if kbbi (word):
                out.append(word)
        except tidakditemukan:
                pass
    return out

df['after'] = df['before'].apply(remove_typo)

print (df)
                                              before  \
0                   [masih, blom, cair, jugagmn, in]   
1  [alhmdllh, sangat, membantu, meski, bunga, cuk...   

                                            after  
0                                   [masih, cair]  
1  [sangat, membantu, meski, bunga, cukup, besar]",https://stackoverflow.com/questions/74739534,python,09-12-2022 06:17,199.0,0.0,1.0,True,10-12-2022 10:12,10-12-2022 10:12
71784556,automatically add diacritic/accent marks to a non-english document,"in my spare time, i am transcribing a very old, rare book written in romanian (in fact, it is the only remaining copy, to my knowledge). it was written over a hundred years ago, well before any computers existed. as such, no digital copies exist, and i am manually transcribing and digitizing it.
the book is thousands of pages long, and it is surprisingly time consuming (for me, at least) to add diacritic and accented marks (ï¿½ï¿½/ï¿½ï¿½/ï¿½ï¿½/ï¿½ï¿½/ï¿½ï¿½) to every single word as i type. if i omit the marks and just type the bare letters (i.e a instead of ï¿½ï¿½/ï¿½ï¿½), i am able to type more than twice as fast, which is a huge benefit. currently i am typing everything directly into a .tex file to apply special formatting for the pages and illustrations.
however, i know that eventually i will have to add all these marks back into the text, and it seems tedious/unecessary to do all that manually, since i already have all the letters. i'm matically/semi-automatically add diacritic/accent marks to a large body of text (not remove - i see plenty of questions asking how to remove the marks on so).
i tried searching for large corpora of romanian words (this and this were the most promising two), but everything i found fell short, missing at least a few words on any random sample of text i fed it (i used a short python script). it doesn't help that the book uses many archaic/uncommon words or uncommon spellings of words.
does anyone have any ideas on how i might go about this? there are no dumb ideas here - any document format, machine learning technique, coding language, professional tool, etc that you can think of that might help is appreciated.
i should also note that i have substantial coding experience, and would not consider it a waste of time to build something myself. tbh, i think it might be beneficial to the community, since i could not find such a tool in any western language (french, czech, serbian, etc). just need some guidance on how to get started.","['python', 'machine-learning', 'automation', 'nlp', 'artificial-intelligence']",71840629,"bob's answer is a static approach which will work depending on how good the word-list is.
so if a word is missing from this list it will never handled.
moreover, as in many other languages, there are cases where two (or more) words exists with the same characters but different diacritics.
for romanian i found the following example: peste = over vs. pesï¿½ï¿½e = fish.
these cases cannot be handled in a straightforward way either.
this is especially an issue, if the text you're converted contains words which aren't used anymore in today's language, especially diacritised ones.
in this answer i will present an alternative using machine learning.
the only caveat to this is that i couldn't find a publicly available trained model doing diacritic restoration for romanian.
you may find some luck in contacting the authors of the papers i will mention here to see if they'd be willing to send their trained models for you to use.
otherwise, you'll have to train yourself, which i'll give some pointers o will try to give a comprehensive overview to get you started, but further reading is encouraged.
although this process may be laborious, it can give you 99% accuracy with the right tools.
language model
the language model is a model which can be thought of as having a high-level ""understanding"" of the language.
it's typically pre-trained on raw text corpora.
although you can train your own, be wary that these models are quite expensive to pre-train.
whilst multilingual models can be used, language-specific models typically fare better if trained with enough data.
luckily, there are publicly language models available for romanian, such as robert.
this language model is based on bert, an architecture used extensively in natural language processing & is more or less the standard in the field due as it attained state-of-the-art results in english & other languages.
in fact there are three variants: base, large, & small.
the larger the model, the better the results, due to the larger representation power.
but larger models will also have a higher footprint in terms of memory.
loading these models is very easy with the transformers library.
for instance, the base model:
from transformers import automodel, autotokenizer, automodel
tokenizer = autotokenizer.from_pretrained(""readerbench/robert-base"")
model = automodel.from_pretrained(""readerbench/robert-base"")
inputs = tokenizer(""exemplu de propoziï¿½ï¿½ie"", return_tensors=""pt"")
outputs = model(**inputs)

the outputs above will contain vector representations of the inputted texts, more commonly know as ""word embeddings"".
language models are then fine-tuned to a downstream task ï¿½ï¿½ï¿½ in your case, diacritic restoration ï¿½ï¿½ï¿½ and would take these embeddings as input.
fine-tuning
i couldn't find any publicly available fine-tuned models, so you'll have to fine-tune your own unless you find a model yourself.
to fine-tune a language model, we need to build a task-specific architecture which will be trained on some dataset.
the dataset is used to tell the model how the input is & how we'd like the o
dataset
from diacritics restoration using bert with analysis on czech language, there's a publicly available dataset for a number of languages including romanian.
the dataset annotations will also depend on which fine-tuning architecture you use (more on that below).
in general, you'd choose a dataset which you trust has a high-quality of of diacritics.
from this text you can then build annotations automatically by producing the undiacritised variants of the words as well as the corresponding labels.
keep in mind that this or any other dataset you'll use will contain biases especially in terms of the domain the annotated texts originate from.
depending on how much data you have already transcribed, you may also want to build a dataset using your texts.
architecture
the architecture you choose will have a bearing on the downstream performance you use & the amount of custom code you'll have to do.
word-level
the aforementioned work, diacritics restoration using bert with analysis on czech language, use a token-level classification mechanism where each word is is labelled with a set of instructions of the type of diacritic marks to insert at which character index.
for example, the undiacritised word ""dite"" with instruction set 1:caron;3:acute indicates adding the appropriate diacritic marks at index 1 and index 3 to result in ""dï¿½ï¿½tï¿½ï¿½"".
since this is a token-level classification task, there's not much custom code you have to do, as you can directly use a "" rel=""nofollow noreferrer"">bertfortokenclassification.
refer to the authors' code for a more complete example.
one sidenote is that the authors use a multililingual language model.
this can be easily replaced with another language model such as robert mentioned above.
character-level
alternatively, the robert paper use a character-level model.
from the paper, each character is annotated as one of the following:

make no modification to the current character (e.g., a ï¿½ï¿½ï¿½ a), add circumflex mark (e.g., a ï¿½ï¿½ï¿½ ï¿½ï¿½ and i ï¿½ï¿½ï¿½ ï¿½ï¿½), add breve mark (e.g., a ï¿½ï¿½ï¿½  ï¿½ï¿½ï¿½ï¿½), and two more classes for adding comma below (e.g., s ï¿½ï¿½ï¿½ ï¿½ï¿½ and t ï¿½ï¿½ï¿½ ï¿½ï¿½)

here you will have to build your own custom model (instead of the bertfortokenclassification above).
but, the rest of the training code will largely be the same.
here's a template for the m"" rel=""nofollow noreferrer"">transformers library:
from transformers import bertmodel, bertpretrainedmodel

class bertfordiacriticrestoration(bertpretrainedmodel):

    def __init__(self, config):
        super().__init__(config)
        self.bert = bertmodel(config)
        ...

    def forward(
        self,
        input_ids=none,
        attention_mask=none,
        token_type_ids=none
    ):
        ...

evaluation
in each section there's plethora of options for you to choose from.
a bit of pragmatic advice i'll offer is to start simple & complicate things if you want to improve things further.
keep a testing set to measure if the changes you're making result in improvements or degradation over your previous setup.
crucially, i'd suggest that at least a small part of your testing set is texts coming from the texts you have transcribed yourself, the more you use the better.
primarily, this is data you annotated yourself, so you are more sure of the quality of this data then any other publicly available source.
secondly, when you are testing on data coming from the target domain, you stand a better chance of more accurately evaluating your systems more accurately to your target task, due to certain biases which might be present from other domains.",https://stackoverflow.com/questions/71784556,python,07-04-2022 15:03,1034.0,1.0,2.0,True,12-04-2022 09:56,11-04-2022 15:36
79110089,llama3.2 fails to respond to simple text inputs when bounded with tool calling on langgraph,"i am following along a langchain tutorial for langgraph. they are using openai models in the tutorial. however, i want to use my local ollama models. i am using llama 3.2 as that supports tool callings. however, when i bind tools to the chat object, llm, it does not respond to normal text inputs and only returns a tool response. if it is not bound with a tool, it does respond to regular messages i am not sure and i cannot figure out whether it is an issue of the langchain class or the llama 3.2 model. how to fix this?
following is the code:
from langchain_ollama import chatollama
from langgraph.graph import messagesstate
from langgraph.graph import stategraph, start, end
from langchain_core.messages import humanmessage


def multiply(a: int, b: int) -> int:
    return a * b


def tool_calling_llm(state: messagesstate):
    return {""messages"": [llm_with_tools.invoke(state[""messages""])]}


llm = chatollama(model=""llama3.2"")
llm_with_tools = llm.bind_tools([multiply])

builder = stategraph(messagesstate)
builder.add_node(""tool_calling_llm"", tool_calling_llm)
builder.add_edge(start, ""tool_calling_llm"")
builder.add_edge(""tool_calling_llm"", end)
graph = builder.compile()

messages = graph.invoke({""messages"": humanmessage(content=""hello"")})
print(messages)


this is the print result:

{'messages': [humanmessage(content='hello', additional_kwargs={}, response_metadata={}, id='b3e1122b-400b-4f6d-b323-0b67f2aa1441'), aimessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2024-10-21t11:07:58.132452z', 'message': {'role': 'assistant', 'content': '', 'tool_calls': [{'function': {'name': 'multiply', 'arguments': {'a': '2', 'b': '3'}}}]}, 'done_reason': 'stop', 'done': true, 'total_duration': 1372686042, 'load_duration': 30459500, 'prompt_eval_count': 156, 'prompt_eval_duration': 752771000, 'eval_count': 22, 'eval_duration': 584293000}, id='run-28808b40-51d2-40ee-a9bf-e048a009651c-0', tool_calls=[{'name': 'multiply', 'args': {'a': '2', 'b': '3'}, 'id': 'b45ba4cc-7610-4d1a-93bc-de4deb6e6c1d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 156, 'output_tokens': 22, 'total_tokens': 178})]}","['langchain', 'function-call', 'ollama', 'langgraph', 'llama3']",79488522,"like laurie young mentioned, you need bigger models of llama. i took thought it was langgraph that was inconsistent. rewrote my application without langgraph in python. no use. llama 1b is the issue here. it can either use llm or it can use tools. it cannot combine both. the fix is to use a better model.
it took a few days of breaking my head and testing with python/pydantic and extensive online search and testing with chatgpt to realize that llama is the issue. i wish meta had better documentation on this. its appalling that they did not mention this anywhere on their documentation. what a waste of my time! i have decided to give up on llama and stick to chatgpt just because how unhelpful the documentation is. chatgpt saves a lot of time, community is bigger and their models are just better. the only downside is the amount of space required. but nobody can put a price on time wasted on a model which is so far behind chatgpt.",https://stackoverflow.com/questions/79110089,langchain,21-10-2024 12:39,1388.0,6.0,1.0,True,07-03-2025 15:17,07-03-2025 15:17
71691184,huggingface pretrained model&#39;s tokenizer and model objects have different maximum input length,"i'm using symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli pretrained model from huggingface. my task requires to use it on pretty large texts, so it's essential to know maximum input length.
the following code is supposed to load pretrained model and its tokenizer:
encoding_model_name = ""symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli""
encoding_tokenizer = autotokenizer.from_pretrained(encoding_model_name)
encoding_model = sentencetransformer(encoding_model_name)

so, when i print info about them:
encoding_tokenizer
encoding_model

i'm getting:
pretrainedtokenizerfast(name_or_path='symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli', vocab_size=250002, model_max_len=512, is_fast=true, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': addedtoken(""<mask>"", rstrip=false, lstrip=true, single_word=false, normalized=false)})

sentencetransformer(
  (0): transformer({'max_seq_length': 128, 'do_lower_case': false}) with transformer model: xlmrobertamodel 
  (1): pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': false, 'pooling_mode_mean_tokens': true, 'pooling_mode_max_tokens': false, 'pooling_mode_mean_sqrt_len_tokens': false})
)

as you can see, model_max_len=512 parameter in tokenizer doesn't match max_seq_length=128 parameter in model
how can i figure out which one is true? or, probably, if they somehow respond to different features, how i can check maximum input length for my model?","['nlp', 'huggingface-transformers', 'huggingface-tokenizers', 'sentence-transformers']",71714293,"since you are using a sentencetransformer and load it to the sentencetransformer class, it will truncate your input at 128 tokens as stated by the documentation (the relevant code is here):

property max_seq_length
property to get the maximal input sequence length for the model. longer inputs will be truncated.

you can also check this by yourself:
fifty = model.encode([""this ""*50], convert_to_tensor=true)
two_hundered = model.encode([""this ""*200], convert_to_tensor=true)
four_hundered = model.encode([""this ""*400], convert_to_tensor=true)

print(torch.allclose(fifty, two_hundered))
print(torch.allclose(two_hundered,four_hundered))

output:
false
true

the underlying model (xlm-roberta-base) is able to handle sequences with up to 512 tokens, but i assume symanto limited it to 128 because they also used this limit during training (i.e. the embeddings might be not good for sequences longer than 128 tokens).",https://stackoverflow.com/questions/71691184,nlp,31-03-2022 10:49,4144.0,2.0,3.0,True,03-04-2024 14:55,02-04-2022 01:24
76020058,chat completions /v1/chat/completions results is very different than the chatgpt result,"i find out the api /v1/chat/completions result is very different than the web page result.
this is the api response for q: ""content"": ""what is the birthday of george washington""
    curl --location ' \
    --header 'authorization: bearer token' \
    --header 'content-type: application/json' \
    --data '{
        ""model"": ""gpt-4"",
        ""messages"": [
            {
                ""role"": ""user"",
                ""content"": ""what is the birthday of george washington""
            }
        ]
    }'

    ""choices"": [
            {
                ""message"": {
                    ""role"": ""assistant"",
                    ""content"": ""george washington was born on february 22, 1732.""
                },
                ""finish_reason"": ""stop"",
                ""index"": 0
            }
        ]

and this is the result on the web page. you can see it is much longer.","['openai-api', 'chatgpt-api']",76176390,"unfortunately, chatgpt-4 is not willing to spill the beans either. while it is possible to tweak the temperature via api and find a good balance, i'd be curious as well what the default temperate on web actually is.
question for chatgpt-4 via web:
what is the default temperature when using chatgpt via web instead of the api?
chatgpt-4 answer:
the default temperature when using chatgpt via web interface might not be explicitly stated. however, when using openai's api, the default temperature is typically set to 0.7. this value provides a good balance between creativity and coherence. you can adjust the temperature to control the randomness of the generated text: a lower temperature (e.g., 0.2) makes the output more focused and deterministic, while a higher temperature (e.g., 1.0) makes it more random and creative. keep in mind that the web interface and the api may have different default values or behaviors.",https://stackoverflow.com/questions/76020058,openai-api,15-04-2023 02:17,4429.0,1.0,3.0,True,03-06-2024 05:48,16-04-2023 06:46
61708486,what&#39;s difference between tokenizer.encode and tokenizer.encode_plus in hugging face,"here is an example of doing sequence classification using a model to determine if two sequences are paraphrases of each other. the two examples give two different results. can you help me explain why tokenizer.encode and tokenizer.encode_plus give different results?
example 1 (with .encode_plus()):
paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, return_tensors=""pt"")
not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=""pt"")

paraphrase_classification_logits = model(**paraphrase)[0]
not_paraphrase_classification_logits = model(**not_paraphrase)[0]

example 2 (with .encode()):
paraphrase = tokenizer.encode(sequence_0, sequence_2, return_tensors=""pt"")
not_paraphrase = tokenizer.encode(sequence_0, sequence_1, return_tensors=""pt"")

paraphrase_classification_logits = model(paraphrase)[0]
not_paraphrase_classification_logits = model(not_paraphrase)[0]",['huggingface-transformers'],61732210,"the main difference is stemming from the additional information that encode_plus is providing. if you read the documentation on the respective functions, then there is a slight difference forencode():

converts a string in a sequence of ids (integer), using the tokenizer and vocabulary.
  same as doing self.convert_tokens_to_ids(self.tokenize(text)).

and the description of encode_plus():

returns a dictionary containing the encoded sequence or sequence pair
  and additional information: the mask for sequence classification and
  the overflowing elements if a max_length is specified.

depending on your specified model and input sentence, the difference lies in the additionally encoded information, specifically the input mask. since you are feeding in two sentences at a time, bert (and likely other model variants), expect some form of masking, which allows the model to discern between the two sequences, see here. since encode_plus is providing this information, but encode isn't, you get different output results.",https://stackoverflow.com/questions/61708486,huggingface-transformers,10-05-2020 07:16,68993.0,45.0,2.0,True,16-06-2022 14:36,10-05-2020 11:59
76138660,problem with running openai cookbook&#39;s chatbot,"i'm having trouble running the chatbot app in the openai cookbook repository.
what i tried
i installed the necessary packages with 'pip install -r requirements.txt'. i made .env file with my openai api key, and inserted the code below in chatbot.py line 9.
import os
openai.api_key = os.getenv(""openai_api_key"")

the setup above is by my guess, because the doc is totally unclear about how to set up.
i run the app in local by the command ""streamlit run apps/chatbot-kickstarter/chat.py."" it didn't work properly. the app run but when i entered text and pressed 'submit' button in the app, i got an error:
uncaught app exception
traceback (most recent call last):
  file ""c:\users\xxx\appdata\local\programs\python\python310\lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py"", line 565, in _run_script
exec(code, module.__dict__)
  file ""f:\pythonprojects\openai-cookbook\apps\chatbot-kickstarter\chat.py"", line 71, in <module>
response = query(messages)
  file ""f:\pythonprojects\openai-cookbook\apps\chatbot-kickstarter\chat.py"", line 51, in query
response = st.session_state['chat'].ask_assistant(question)
  file ""f:\pythonprojects\openai-cookbook\apps/chatbot-kickstarter\chatbot.py"", line 61, in ask_assistant
if 'searching for answers' in assistant_response['content'].lower():
typeerror: string indices must be integers

i use python 3.10.6.
i would appreciate any help or guidance to resolve these issues.","['python', 'streamlit', 'openai-api', 'gpt-3', 'chatgpt-api']",76187971,putting the key directly in chatbot.py just worked. it shouldn't be taken from environment variables.,https://stackoverflow.com/questions/76138660,python,29-04-2023 22:05,211.0,-1.0,2.0,True,06-05-2023 08:47,02-05-2023 12:22
78884251,unable to install wordnet with nltk 3.9.0 as importing nltk requires installed wordnet,"it is not possible to import nltk, and the solution given by the output required me to import nltk:
>>>import nltk
traceback (most recent call last):  
file ""d:\project\lib\site-packages\nltk\corpus\util.py"", line 84, in __load
    root = nltk.data.find(f""{self.subdir}/{zip_name}"")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""d:\project\lib\site-packages\nltk\data.py"", line 579, in find
    raise lookuperror(resource_not_found
lookuperror:
**********************************************************************
  resource wordnet not found.
  please use the nltk downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('wordnet')

  for more information see: 

  attempted to load corpora/wordnet.zip/wordnet/

  searched in:
    - 'c:\\users\\me/nltk_data'
    - 'd:\\project\\nltk_data'
    - 'd:\\project\\share\\nltk_data'
    - 'd:\\project\\lib\\nltk_data'
    - 'c:\\users\\me\\appdata\\roaming\\nltk_data'
    - 'c:\\nltk_data'
    - 'd:\\nltk_data'
    - 'e:\\nltk_data'
**********************************************************************

basically - i cannot import nltk because wordnet is missing, but in order to download wordnet, i have to import nltk which i cannot, because wordnet is missing.
noteworthy is, that it throws this exception twice, but with a different traceback -
during handling of the above exception, another exception occurred:

traceback (most recent call last):
  file ""<stdin>"", line 1, in <module>
  file ""d:\project\lib\site-packages\nltk\__init__.py"", line 156, in <module>
    from nltk.stem import *
  file ""d:\project\lib\site-packages\nltk\stem\__init__.py"", line 34, in <module>
    from nltk.stem.wordnet import wordnetlemmatizer
  file ""d:\project\lib\site-packages\nltk\stem\wordnet.py"", line 13, in <module>
    class wordnetlemmatizer:
  file ""d:project\lib\site-packages\nltk\stem\wordnet.py"", line 48, in wordnetlemmatizer
    morphy = wn.morphy
             ^^^^^^^^^
  file ""d:\project\lib\site-packages\nltk\corpus\util.py"", line 120, in __getattr__
    self.__load()
  file ""d:\project\lib\site-packages\nltk\corpus\util.py"", line 86, in __load
    raise e
  file ""d:\project\lib\site-packages\nltk\corpus\util.py"", line 81, in __load
    root = nltk.data.find(f""{self.subdir}/{self.__name}"")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  file ""d:\project\lib\site-packages\nltk\data.py"", line 579, in find
    raise lookuperror(resource_not_found)
lookuperror:
**********************************************************************
  resource wordnet not found.
  please use the nltk downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('wordnet')

  for more information see: 

  attempted to load corpora/wordnet

  searched in:
    - 'c:\\users\\me/nltk_data'
    - 'd:\\project\\nltk_data'
    - 'd:\\project\\share\\nltk_data'
    - 'd:\\project\\lib\\nltk_data'
    - 'c:\\users\\me\\appdata\\roaming\\nltk_data'
    - 'c:\\nltk_data'
    - 'd:\\nltk_data'
    - 'e:\\nltk_data'
**********************************************************************

what is the suggested solution in this case?","['python', 'nltk', 'wordnet']",78884294,"this bug was introduced in nltk 3.9.0 (released on 18 august 2024) and is a known issue. it was fixed in 3.9.1:
python3 -m pip install nltk~=3.9.1

the most recent full release prior to 3.9.x was nltk 3.8.1. however, be aware that this version is vulnerable to remote code execution.
python3 -m pip install nltk==3.8.1",https://stackoverflow.com/questions/78884251,python,18-08-2024 09:53,776.0,5.0,1.0,True,18-08-2024 20:08,18-08-2024 10:30
76446228,setting padding token as eos token when using datacollatorforlanguagemodeling from huggingface,"in  there is
from transformers import datacollatorforlanguagemodeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = datacollatorforlanguagemodeling(tokenizer, mlm=false)

what the tutorial is doing is using a pretrained gpt2 model and its tokenizer and trying to create a dataset for causal language modeling pretraining task.
my question with the above line is that padding token is set to be the eos token. as a result even the original eos tokens will be ignored by the model during training since they will be perceived as padding tokens too.
this would prevent my model from learning to output eos tokens when its generation is over.
how come this is in the tutorials and it is a correct way ?","['pytorch', 'huggingface-transformers', 'huggingface-tokenizers', 'huggingface', 'huggingface-datasets']",76453052,"tl;dr
ignoring the eos symbol when training a normal language model is okay. so padding the sequence with eos instead of a dedicated pad symbol is okay too.

in long
when using datacollatorforlanguagemodeling(tokenizer, mlm=false), the ""masked-language modeling"" model is off and we are doing casual language modeling ,i.e. predicting the next word given the previous. consider this:
['this', 'is', 'a', 'foobar', '.', 'eos']

now we pad the sequence until it's of length 10 tokens
['this', 'is', 'a', 'foobar', '.', 'eos', 'eos', 'eos', 'eos', 'eos']

when the model learns with causal language model, it's predicting the next word given the previous, i.e.
>>> predict(next_token, given=[""bos""])
'this'

>>> predict(next_token, given=[""bos"", ""this""])
'is'

...

>>> predict(next_token, given=[""bos"", ""this"", ""is"", ""a"", ""foobar"", "".""])
'eos'

in most common inference routine, the model will stop once the first eos is predicted, or all beams in the search during inference produced their first eos.
during training, the model will learn:
ground_truth = [
 'this', 'is', 'a', 'foobar', '.', 'eos', 'eos', 'eos', 'eos', 'eos', 
]

ground_prediction = [
 'this', 'is', 'foobar', '.', 'eos', 'eos', 'eos', 'eos', 'eos', 'eos', 
]

and when you compute the perplexity, all the pad symbols are ignored, and in this case, when you treat the eos as pad, you are essentially tell the model even the first eos is not necessary when computing perplexity.
q: is that the right thing to do to ignore even the first eos token, when we use eos as a padding token?
a: it depends on your task and what you want the 'eos' to mean. for most natural language, we have punctuations before 'eos', so eos/pad doesn't really matter. for programming language, we have '\n' and ';' or some end of sequence operator, so eos isn't that necessary too.
q: then why do we bother to pad?
a: actually that's a good question, we're padding so that the dot-products in transformer attentions can be ""easily"" computed.
but there are many cases where pad tokens can be efficiently packed, like in rnn  (iirc, not in transformers architecture though)
but i don't know how much of that is already in pytorch/jax underlying library for ""efficient"" transformers, which will allow us to avoid pre-padding inputs. from my experience in using huggingface pytorch models, if you don't pad the inputs, most probably the model will complain when you do a forward pass =(
if only, someone fix that mathematically. maybe someone did try but it's not that common to be largely used by most transformers pre-trained model (yet).",https://stackoverflow.com/questions/76446228,pytorch,10-06-2023 12:37,8377.0,1.0,2.0,True,19-10-2024 23:18,12-06-2023 00:39
76007112,finding the words or sentence that is followed by a search word and put them into a dictionary python,"i have to find the words or sentence that follow a search word and put them into a dictionary. my data is in the pdf which i already extract it to a text using pypdf2 library. i am new to nlp and i don't know how to implement this part of code.
i know how to find 1 word that follows the search word, but sometimes it is a word, sometimes it is sentence which can be identify by \n.

the text example:
[""code: id\nstudy of men's brain id\nbased upon 16"",  '3 valid cases
out of 76',  '33 total cases.\nï¿½ï¿½ï¿½mean: 54695.29\nï¿½ï¿½ï¿½minimum:
8.00\nvariable type:  'numeric \nhealth: h1 - health in general\n xxx',  ' ccc']

import pypdf2
search_keywords=['code','lvi','health']


pdffileobj = open('df.pdf', 'rb')
pdfreader = pypdf2.pdffilereader(pdffileobj)
pageobj = pdfreader.getpage(5)
text=(pageobj.extracttext())
text=text.split("","")
text


the out put should be :
{""code"":""it;vis"":none
""health"":""h1 - health in general""}","['python', 'nlp']",76007354,"you should read up on regular expressions, which are implemented in the re module in python. regular expressions (aka regex) let you search for patterns of text in addition to key words. this will be useful for your nlp work. the re documentation is here, and you can find a tutorial here.
this code accomplishes what you're after:
import re
text=[""code: id\nstudy of men's brain id\nbased upon 16"", '3 valid cases out of 76', '33 total cases.\nï¿½ï¿½ï¿½mean: 54695.29\nï¿½ï¿½ï¿½minimum: 8.00\nvariable type: numeric \nhealth: h1 - health in general\n xxx', ' ccc']

results={}
    
for keyword in ['code','lvi','health']:
    keyword_found=false
    for sentence in text:
        result=re.search(f'({keyword}): (.+)\n',sentence) #this searches for each keyword and captures any text after a colon+space and before \n
        if result:
            keyword_found=true
            results.update({result.group(1):result.group(2)})
            break
    if keyword_found==false:
        results.update({keyword:none})

print(results)

#{'code': ' id', 'lvi': none, 'health': ' h1 - health in general'}

if you need to change the search pattern, you need to modify the search parameter in re.search(f'({keyword}): (.+)\n',sentence), want two lines after they keyword, the search parameter would be f'({keyword}): (.+\n.+)\n'.",https://stackoverflow.com/questions/76007112,python,13-04-2023 15:14,143.0,0.0,2.0,True,14-04-2023 08:23,13-04-2023 15:17
72002617,how to solve nltk lookuperror(resource_not_found) ? it exists in path. (python),"i try to use nltk library, but i got stuck.
i downloaded the stopwords library by hand (i cant download by code because of the permission issues on my working machine), but it always gives me the following error;
lookuperror: 
**********************************************************************
  resource stopwords not found.
  please use the nltk downloader to obtain the resource:
....
searched in:
  - '/home/skahraman/nltk_data'
....

my stopwords in nltk_data folder.
so how can i solve this problem?
i tried following;
import string
import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords
from collections import counter
#nltk.download('stopwords')
nltk.data.path.append(""/home/skahraman/nltk_data"")
stop_words=stopwords.words(""turkish"")","['python', 'nltk']",72009785,"my folder path was ; /home/skahraman/nltk_data/stopwords
but it must be ; /home/skahraman/nltk_data/corpora/stopwords
i added the corpora folder to my directory.
so it works now.",https://stackoverflow.com/questions/72002617,python,25-04-2022 16:14,1969.0,2.0,2.0,True,26-04-2022 07:15,26-04-2022 07:02
39142778,how to determine the language of a piece of text?,"i want to get this:
input text: ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½""
output text: ""russian"" 

input text: ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½""
output text: ""chinese"" 

input text: ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½""
output text: ""japanese"" 

input text: ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿","['python', 'nlp']",39143059,"have you had a look at langdetect?
from langdetect import detect

lang = detect(""ein, zwei, drei, vier"")

print lang
#output: de",https://stackoverflow.com/questions/39142778,python,25-08-2016 10:26,196276.0,204.0,17.0,True,12-02-2025 22:46,13-06-2022 15:59
70055966,chatbot using huggingface transformers,"i would like to use huggingface transformers to implement a chatbot. currently, i have the code shown below. the transformer model already takes into account the history of past user input.
is there something else (additional code) i have to take into account for building the chatbot?
second, how can i modify my code to run with tensorflow instead of pytorch?
later on, i also plan to fine-tune the model on other data. i also plan to test different models such as blenderbot and gpt2. i think to test this different models it should be as easy as replacing the corresponding model in autotokenizer.from_pretrained(""microsoft/dialogpt-small"") and automodelforcausallm.from_pretrained(""microsoft/dialogpt-small"")
from transformers import automodelforcausallm, autotokenizer
import torch

tokenizer = autotokenizer.from_pretrained(""microsoft/dialogpt-small"")
model = automodelforcausallm.from_pretrained(""microsoft/dialogpt-small"")

for step in range(5):
    # encode the new user input, add the eos_token and return a tensor in pytorch
    new_user_input_ids = tokenizer.encode(input("">> user:"") + tokenizer.eos_token, return_tensors='pt')

    # append the new user input tokens to the chat history
    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids

    # generated a response while limiting the total chat history to 1000 tokens, 
    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)

    # pretty print last ouput tokens from bot
    print(""dialogpt: {}"".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=true)))","['tensorflow', 'chatbot', 'huggingface-transformers', 'blenderbot']",70056670,"here is an example of using the dialogpt model with tensorflow:
from transformers import tfautomodelforcausallm, autotokenizer, blenderbottokenizer, tfblenderbotforconditionalgeneration
import tensorflow as tf

chat_bots = {
    'blenderbot': [blenderbottokenizer.from_pretrained('facebook/blenderbot-400m-distill'), tft5forconditionalgeneration.from_pretrained('facebook/blenderbot-400m-distill')],
    'dialogpt': [autotokenizer.from_pretrained(""microsoft/dialogpt-small""), tfautomodelforcausallm.from_pretrained(""microsoft/dialogpt-small"")],
} 
key = 'dialogpt'
tokenizer, model = chat_bots[key]

for step in range(5):
    new_user_input_ids = tokenizer.encode(input("">> user:"") + tokenizer.eos_token, return_tensors='tf')
    if step > 0:
      bot_input_ids = tf.concat([chat_history_ids, new_user_input_ids], axis=-1)  
    else:
      bot_input_ids = new_user_input_ids

    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)

    print(key + "": {}"".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=true)))

>> user:how are you?
dialogpt: i'm here
>> user:why are you here
dialogpt: i'm here
>> user:but why
dialogpt: i'm here
>> user:where is here
dialogpt: where is where?
>> user:here
dialogpt: where is here?

if you want to compare different chatbots, you might want to adapt their decoder parameters, because they are not always identical. for example, using blenderbot and a max_length of 50 you get this kind of response with the current code:
>> user:how are you?
blenderbot: ! i am am great! how how how are are are???

in general, you should ask yourself which special characters are important for a chatbot (depending on your domain) and which characters should / can be omitted?
you should also experiment with different decoding methods such as greedy search, beam search, random sampling, top-k sampling, and nucleus sampling and find out what works best for your use case. for more information on this topic check out this post",https://stackoverflow.com/questions/70055966,tensorflow,21-11-2021 15:34,2887.0,2.0,1.0,True,04-03-2022 19:46,04-03-2022 19:46
71519851,how to iterate over a dataframe parsed with spacy after it was saved as a csv?,"i created a data frame with spacy (columns: sentencens, tokens, stopwords, content words, pos, entities) and saved it as a csv.
when i read it as a csv file (it looks pretty decent), but when i perform a for loop over the columns, it does not return the expected result (for my basic domain of python).
for example:
words = []
for items in df['tokens']:
    for word in items:
        words.append(word)

what i expected
[tea, and, ...]
what i got
['t',
'e',
'a',
',',
' ',
'a',
'n',
'd',
.
.
.

it happens in any column i try to iterate over. what is wrong with it?
i need it to be a csv as it is, to be shared with college mates that uses excel to visualize the data.","['python', 'dataframe', 'csv', 'spacy']",71521895,"from ast import literal_eval
df = pd.read_csv('sample.csv', converters={'tokens': literal_eval})

print(df.iloc[0,0][0])

this should help you evaluate the list of strings and then parse it out separately as needed. then your for loop will work as above",https://stackoverflow.com/questions/71519851,python,17-03-2022 22:37,215.0,0.0,2.0,True,18-03-2022 13:43,18-03-2022 13:43
76886954,multiple file loading and embeddings with openai,"i am trying to load a bunch of pdf files and query them using openai apis.
from langchain.text_splitter import charactertextsplitter
#from langchain.document_loaders import unstructuredfileloader
from langchain.document_loaders import unstructuredpdfloader
from langchain.vectorstores.faiss import faiss
from langchain.embeddings import openaiembeddings
import pickle
import os


print(""loading data..."")
pdf_folder_path = ""content/""
print(os.listdir(pdf_folder_path))

# load multiple files
# location of the pdf file/files. 
loaders = [unstructuredpdfloader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)]


print(loaders)

alldocument = []
vectorstore = none
for loader in loaders:

    print(""loading raw document..."" + loader.file_path)
    raw_documents = loader.load()

    print(""splitting text..."")
    text_splitter = charactertextsplitter(
        separator=""\n\n"",
        chunk_size=800,
        chunk_overlap=100,
        length_function=len,
    )
    documents = text_splitter.split_documents(raw_documents)
    #alldocument = alldocument + documents

    print(""creating vectorstore..."")
    embeddings = openaiembeddings()
    
    vectorstore = faiss.from_documents(documents, embeddings)

    #with open(""vectorstore.pkl"", ""wb"") as f:
    with open(""vectorstore.pkl"", ""ab"") as f:
        pickle.dump(vectorstore, f)
        f.close()


i am trying to load multiple files for qna but the index only remembers the last file uploaded from a folder.
do i need to change the structure of for loop or have another parameter with the open method?","['python', 'openai-api', 'embedding', 'faiss']",76925282,"the problem is that with each iteration of the loop, you're overwriting the previous vectorstore when you create a new one. then, when saving to ""vectorstore.pkl"", you're only saving the last vectorstore.
print(""loading data..."")
pdf_folder_path = ""content/""
print(os.listdir(pdf_folder_path))

# load multiple files
loaders = [unstructuredpdfloader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)]

print(loaders)

all_documents = []

for loader in loaders:
    print(""loading raw document..."" + loader.file_path)
    raw_documents = loader.load()

    print(""splitting text..."")
    text_splitter = charactertextsplitter(
        separator=""\n\n"",
        chunk_size=800,
        chunk_overlap=100,
        length_function=len,
    )
    documents = text_splitter.split_documents(raw_documents)
    all_documents.extend(documents)

print(""creating vectorstore..."")
embeddings = openaiembeddings()
vectorstore = faiss.from_documents(all_documents, embeddings)

with open(""vectorstore.pkl"", ""wb"") as f:
    pickle.dump(vectorstore, f)",https://stackoverflow.com/questions/76886954,python,11-08-2023 22:23,8236.0,2.0,2.0,True,18-10-2023 10:58,11-08-2023 22:34
40453503,information extraction in python,"i am attempting to extract this type of information from the following paragraph structure:
 women_ran men_ran kids_ran walked
         1       2        1      3
         2       4        3      1
         3       6        5      2

text = [""on tuesday, one women ran on the street while 2 men ran and 1 child ran on the sidewalk. also, there were 3 people walking."", ""one person was walking yesterday, but there were 2 women running as well as 4 men and 3 kids running."", ""the other day, there were three women running and also 6 men and 5 kids running on the sidewalk. also, there were 2 people walking in the park.""]

i am using python's spacy as my nlp library. what would be the best way to extract this tabular information from such sentences?
if it was simply a matter of identifying whether there were individuals running or walking, i would just use sklearn to fit a classification model, but the information that i need to extract is obviously more granular than that (i am trying to retrieve subcategories and values for each).","['python', 'nlp', 'information-extraction', 'spacy']",40453747,"you'll want to use the dependency parse for this. you can see a visualisation of your example sentence using the displacy visualiser.
you could implement the rules you need a few different ways ï¿½ï¿½ï¿½ much like how there are always multiple ways to write an xpath query, dom selector, etc.
something like this should work:
nlp = spacy.load('en')
docs = [nlp(t) for t in text]
for i, doc in enumerate(docs):
    for j, sent in enumerate(doc.sents):
        subjects = [w for w in sent if w.dep_ == 'nsubj']
        for subject in subjects:
            numbers = [w for w in subject.lefts if w.dep_ == 'nummod']
            if len(numbers) == 1:
                print('document.sentence: {}.{}, subject: {}, action: {}, numbers: {}'.format(i, j, subject.text, subject.head.text, numbers[0].text))

for your examples in text you should get:
document.sentence: 0.0, subject: men, action: ran, numbers: 2
document.sentence: 0.0, subject: child, action: ran, numbers: 1
document.sentence: 0.1, subject: people, action: walking, numbers: 3
document.sente.0, subject: person, action: walking, numbers: one",https://stackoverflow.com/questions/40453503,python,06-11-2016 19:21,5457.0,9.0,1.0,True,07-03-2025 15:32,07-03-2025 15:32
77182311,question about data_collator throwing a key error in hugging face,"i am trying to use data_collator function in hugging face using this code:
datasets = dataset.train_test_split(test_size=0.1)
train_dataset = datasets[""train""]
val_dataset = datasets[""test""]

print(type(train_dataset))

def data_collator(data):
# initialize lists to store pixel values and input ids
   pixel_values_list = []
   input_ids_list = []

# iterate over each sample in the data
   for item in data:      
      pixel_values_list.append(torch.tensor(item[""pixel_values""]))
      input_ids_list.append(torch.tensor(item[""input_ids""]))


return {
    ""pixel_values"": torch.stack(pixel_values_list),
    ""labels"": torch.stack(input_ids_list)
}

the train_data has 5 keys including input_ids. however, when i print(data[0]) inside the data_collator function, i only see 1 key, which is giving an error when running the trainer:
traceback (most recent call last):
 file ""caption-code.py"", line 134, in <module>
trainer.train()
  file ""c:\users\moham\anaconda3\envs\transformer\lib\site- 
 packages\transformers\trainer.py"", line 1321, in train
ignore_keys_for_eval=ignore_keys_for_eval,
 file ""c:\users\moham\anaconda3\envs\transformer\lib\site- 
  packages\transformers\trainer.py"", line 1528, in _inner_training_loop
  for step, inputs in enumerate(epoch_iterator):
    file ""c:\users\moham\anaconda3\envs\transformer\lib\site- 
  packages\torch\utils\data\dataloader.py"", line 521, in __next__
  data = self._next_data()
 file ""c:\users\moham\anaconda3\envs\transformer\lib\site- 
  packages\torch\utils\data\dataloader.py"", line 561, in _next_data
data = self._dataset_fetcher.fetch(index)  # may raise stopiteration
 file ""c:\users\moham\anaconda3\envs\transformer\lib\site- 
 packages\torch\utils\data\_utils\fetch.py"", line 52, in fetch
 return self.collate_fn(data)
 file ""caption-code.py"", line 102, in data_collator
 input_ids_list.append(item[""input_ids""])
  keyerror: 'input_ids'

i am using the trainer function as follows:
training_args = seq2seqtrainingarguments(
predict_with_generate=true,
evaluation_strategy=""epoch"",
per_device_train_batch_size=4,
per_device_eval_batch_size=4,
output_dir=""c:/users/moham/desktop/euler/output"",
logging_dir=""./logs"",
logging_steps=10,
save_steps=10,
eval_steps=10,
warmup_steps=10,
max_steps=100,  # adjust as needed
overwrite_output_dir=true,
save_total_limit=3,
 )
trainer = seq2seqtrainer(
model=model,
args=training_args,
train_dataset=train_dataset,
eval_dataset=val_dataset,
data_collator=data_collator,
tokenizer=tokenizer,
compute_metrics=compute_exact_match
 )
trainer.train()","['python', 'pytorch', 'nlp', 'huggingface-transformers', 'huggingface-tokenizers']",77182746,"the actual issue is in your seq2seqtrainingarguments which is leading the error in your data_collator().
reason: the .trainer() is by default removing any unknown columns (not present in the model's forward method) from your data when you are providing a custom data_collator(). as a result even though each sample in your train_dataset has all the keys, when you send that to data_collator(), the .trainer() automatically removes the unknown columns.
solution: you need to include an argument in your training arguments like the following:
training_args = seq2seqtrainingarguments(
predict_with_generate=true,
remove_unused_columns=false, 
...)

the remove_unused_columns=false, would prevent the default behaviour and you'd get the entire data in data_collator(). this issue would be useful for further reference.",https://stackoverflow.com/questions/77182311,python,26-09-2023 17:42,1014.0,1.0,1.0,True,29-10-2024 16:06,16-07-2024 05:50
75141938,spacy incorrectly identifying pronouns,"when i try this code using spacy, i get the desired result:
import spacy
nlp = spacy.load(""en_core_web_sm"")

# example 1
test = ""all my stuff is at to myboq""
doc = nlp(test)
for word in doc:
    if word.pos_ == 'pron':
        print(word.text)  

the output shows all and my. however, if i add a question mark:
test = ""all my stuff is at to myboq?""
doc = nlp(test)
for word in doc:
    if word.pos_ == 'pron':
        print(word.text)

now it also identifies myboq as a pronoun. it should be classified as an organization name (word.pos_ == 'org') instead.
how do i tell spacy not to classify myboq as a pronoun? should i just remove all punctuation before checking for pronouns?","['python', 'nlp', 'spacy']",75142097,"when running your code on my machine (windows 11 64-bit, python 3.10.9, spacy 3.4.4), spacy produces the following results for the text with and without the question mark:
                               en_core_web_sm   en_core_web_md   en_core_web_trf
all my stuff is at to myboq?   all, my          my               my
all my stuff is at to myboq    all, my          my               my

in this example, the word ""all"" is not a pronoun but rather a determiner, so only the en_core_web_md and en_core_web_trf pipelines are producing technically correct results. if you're running an old version of spacy i'd suggest updating the package. alternatively, if spacy is up-to-date, try restarting your ide/computer to see if it stops producing erroneous results---there should be no need to remove punctuation before checking for pronouns.
finally, part of speech (pos) tags do not include organisation names (org). i think you're mixing named entity tags with pos tags. ""myboq"" should be pos tagged as a proper noun (propn) which the en_core_web_md and en_core_web_trf pipelines identify correctly, whereas en_core_web_sm pipeline does not (instead tagging it as a basic noun).",https://stackoverflow.com/questions/75141938,python,17-01-2023 04:15,466.0,2.0,1.0,True,17-01-2023 04:50,17-01-2023 04:29
71972018,cannot see debug logs for ï¿½ï¿½ï¿½number of documents convergedï¿½ï¿½ï¿½ info when running gensim&#39;s lda suggested for choosing iteration,"in official gensim tutorial there is a mention about how to set number of iterations and passes:

i suggest the following way to choose iterations and passes. first, enable logging (as described in many gensim tutorials), and set eval_every = 1 in ldamodel. when training the model look for a line in the log that looks something like this:

2016-06-21 15:40:06,753 - gensim.models.ldamodel - debug - 68/1566 documents converged within 400 iterations

i've never saw anything like this line in my lda logs though. those are my logs on pastebin. i've folowed the official tutorial.
i'm alllowing debugging like this:
logging.basicconfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.info,
                            filename='content_based_algorithms/training_logs/lda/logs.log')

i even tried to explicitly define callbacks::
perplexity_logger = perplexitymetric(corpus=corpus, logger='shell')
convergence_logger = convergencemetric(logger='shell')

lda_model = gensim.models.ldamodel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=passes, alpha=alpha, eta=eta, update_every=1, eval_every=1, callbacks=[convergence_logger, perplexity_logger])

i've tested that both in windows, pycharm ide and ubuntu command line execution of python cript.
possible duplicate with the post gensim lda logging not displaying","['nlp', 'gensim', 'lda']",72492681,"the line

debug - 68/1566 documents converged within 400 iterations

can be obtained in the logging file changing the logging configuration to debug in you case would be something like this:
logging.basicconfig(format='%(asctime)s : %(levelname)s : %(message)s',
                   level=logging.debug,
                   filename='content_based_algorithms/training_logs/lda/logs.log')

now the line will appear inside the logging file.",https://stackoverflow.com/questions/71972018,nlp,22-04-2022 16:35,611.0,1.0,2.0,True,16-07-2022 08:55,16-07-2022 08:55
16323078,gender identification in natural language processing,"i have written below code using stanford nlp packages.
genderannotator mygenderannotation = new genderannotator();
mygenderannotation.annotate(annotation);

but for the sentence ""annie goes to school"", it is not able to identify the gender of annie. 
the output of application is:
     [text=annie characteroffsetbegin=0 characteroffsetend=5 partofspeech=nnp lemma=annie namedentitytag=person] 
     [text=goes characteroffsetbegin=6 characteroffsetend=10 partofspeech=vbz lemma=go namedentitytag=o] 
     [text=to characteroffsetbegin=11 characteroffsetend=13 partofspeech=to lemma=to namedentitytag=o] 
     [text=school characteroffsetbegin=14 characteroffsetend=20 partofspeech=nn lemma=school namedentitytag=o] 
     [text=. characteroffsetbegin=20 characteroffsetend=21 partofspeech=. lemma=. namedentitytag=o]

what is the correct approach to get the gender?","['nlp', 'stanford-nlp']",16328613,"if your named entity recognizer outputs person for a token, you might use (or build if you don't have one) a gender classifier based on first names. as an example, see the gender identification section from the nltk library tutorial pages. they use the following features:

last letter of name.
first letter of name.
length of name (number of characters).
character unigram presence (boolean whether a character is in the name).

though, i have a hunch that using character n-gram frequency---possibly up to character trigrams---will give you pretty good results.",https://stackoverflow.com/questions/16323078,nlp,01-05-2013 17:22,9091.0,4.0,5.0,True,21-01-2022 20:18,19-05-2015 15:30
72481501,redact bigrams from a list based on matching unigrams,"from a list of bigrams, i need to redact bigrams that do not have at least one term that exactly matches at least one term in a list of unigrams.
the two lists
bigram_list = ['computer vision', 'data excellence', 'data visualization']
unigram_list = ['excel', 'tableau', 'visio', 'visualization']
the objective
cleaned_bigrams = ['data visualization']
what i've tried
i tried adapting this approach here, but failed: removing separate list of items from another list in python 3.x
i also tried this, but couldn't get it to work: get rid of unigrams in a list if contained within bigrams or trigrams python
i tried to adapt from a previous question i asked, but couldn't get that going: create new boolean fields based on specific bigrams appearing in a tokenized pandas dataframe
thanks in advance for any help you can provide, and would appreciate an upvote if you think this is a good question!","['python', 'pandas', 'list', 'nlp', 'boolean']",72498509,"here is one way to do it:
bigram_list = [""computer vision"", ""data excellence"", ""data visualization""]
unigram_list = [""excel"", ""tableau"", ""visio"", ""visualization""]

# init a dict for counting number of match
counts = {key: 0 for key in bigram_list}

# count number of match for each bigram
for big in bigram_list:
    for uni in unigram_list:
        if uni in big.split("" ""):
            counts[big] += 1

# filter
cleaned_bigrams = [item for item in bigram_list if counts[item] > 0]

print(cleaned_bigrams)
# output
['data visualization']",https://stackoverflow.com/questions/72481501,python,02-06-2022 19:37,57.0,1.0,1.0,True,04-06-2022 09:12,02-06-2022 21:01
78173243,vector store created using existing graph for multiple nodes/labels,"am trying to create vector stores on top of my existing kg using from_existing_graph, (followed tomaz  and saurav joshi neo4j blog posts) - this method is allowing me to create embedding/vector index only for single label due to which am unable to get desired results while asking nlq (i am assuming though).
below code is able to answer, the age and location of oliver but not what he directed,
i believe this is due to from_existing_graph has only to pass single label and its corresponding properties as option for generating embeddings and vector index
any ideas, how to achieve this?
import os
import re
from langchain.vectorstores.neo4j_vector import neo4jvector
# from langchain.document_loaders import wikipedialoader
from langchain_openai import openaiembeddings
# from langchain.text_splitter import charactertextsplitter, recursivecharactertextsplitter
from langchain.graphs import neo4jgraph
import openai
# from transformers import automodelforseq2seqlm, autotokenizer

os.environ[""openai_api_key""] = ""sk-xx""
url = ""neo4j+s://xxxx.databases.neo4j.io""
username = ""neo4j""
password = ""mypassword""
existing_graph = neo4jvector.from_existing_graph(
    embedding=openaiembeddings(),
    url=url,
    username=username,
    password=password,
    index_name=""person"",
    node_label=""person"",
    text_node_properties=[""name"", ""age"", ""location""],
    embedding_node_property=""embedding"",
)

from langchain.chat_models import chatopenai
from langchain.chains import graphcypherqachain
from langchain.graphs import neo4jgraph

graph = neo4jgraph(
    url=url, username=username, password=password
)

chain = graphcypherqachain.from_llm(
    chatopenai(temperature=0), graph=graph, verbose=true
)

query = ""where does oliver stone live?""
#query = ""name some films directed by oliver stone?"" 

graph_result = chain.invoke(query)

vector_results = existing_graph.similarity_search(query, k=1)
for i, res in enumerate(vector_results):
    print(res.page_content)
    if i != len(vector_results)-1:
        print()
vector_result = vector_results[0].page_content

# construct prompt for openai
final_prompt = f""""""you are a helpful question-answering agent. your task is to analyze
and synthesize information from two sources: the top result from a similarity search
(unstructured information) and relevant data from a graph database (structured information).
given the user's query: {query}, provide a meaningful and efficient answer based
on the insights derived from the following data:

unstructured information: {vector_result}.
structured information: {graph_result} """"""


from openai import openai
client = openai(
    # this is the default and can be omitted
    api_key=os.environ.get(""openai_api_key""),
)

chat_completion = client.chat.completions.create(messages=[{""role"": ""user"",""content"": final_prompt,  }],model=""gpt-3.5-turbo"",)

answer = chat_completion.choices[0].message.content.strip()
print(answer)

any help would be highly appreicated?
here is my schema:
node properties are the following:
person {name: string, embedding: list, age: integer, location: string},actor {name: string, embedding: list},movie {title: string},director {name: string, embedding: list, age: integer, location: string}
relationship properties are the following:
acted_in {role: string}
the relationships are the following:
(:person)-[:acted_in]->(:movie),(:person)-[:directed]->(:movie),(:actor)-[:acted_in]->(:movie),(:director)-[:directed]->(:movie)

cypher used to create:
create (charlie:person:actor {name: 'charlie sheen'})-[:acted_in {role: 'bud fox'}]->(wallstreet:movie {title: 'wall street'})<-[:directed]-(oliver:person:director {name: 'oliver stone'});
match (n:person {name: 'oliver stone'}) set n.age = 30, n.location = ""new york"" return n","['neo4j', 'openai-api', 'langchain', 'large-language-model']",78241152,"you need to add the relationship :directed into the index person_index since the movie he directed is not part of the embedding. once you have the query to add the movies he directed, you will then add it on the resulting node metadata (see retrieval_query).  then on your vector result, you will add the information about the movie tile (as metadata movie[0][""title""]).
you may need to collect all movie titles if there are more than one movie titles in the graph. i'm sure you can figure it out.
reference: 
import os
from langchain.vectorstores.neo4j_vector import neo4jvector
from langchain_openai import openaiembeddings
import openai

os.environ[""openai_api_key""] = ""sk-<key>""
url = ""bolt://localhost:7687""
username = ""neo4j""
password = ""awesome_password""

retrieval_query = """"""
       match (node)-[:directed]->(m:movie)
       with node, score, collect(m) as movies
       return node.name as text, score, node{.*, embedding: null, movies: movies} as metadata
       """"""

existing_index_return = neo4jvector.from_existing_index(
    embedding=openaiembeddings(),
    url=url,
    username=username,
    password=password,
    database=""neo4j"",
    index_name=""person_index"",
    text_node_property=""name"",
    retrieval_query=retrieval_query,
)

from langchain_openai import chatopenai
from langchain.chains import graphcypherqachain
from langchain_community.graphs import neo4jgraph

graph = neo4jgraph(
    url=url, username=username, password=password
)

chain = graphcypherqachain.from_llm(
    chatopenai(temperature=0), graph=graph, verbose=true
)

#query = ""where does oliver stone live?""
query = ""name some films directed by oliver stone?"" 

graph_result = chain.invoke(query)

vector_results = existing_index_return.similarity_search(query, k=1)
vector_result = vector_results[0].page_content + "" lives in "" + vector_results[0].metadata[""location""] + "" and he directed the movie "" + vector_results[0].metadata[""movies""][0][""title""]

# construct prompt for openai
final_prompt = f""""""you are a helpful question-answering agent. your task is to analyze
and synthesize information from two sources: the top result from a similarity search
(unstructured information) and relevant data from a graph database (structured information).
given the user's query: {query}, provide a meaningful and efficient answer based
on the insights derived from the following data:

unstructured information: {vector_result}.
structured information: {graph_result} """"""


from openai import openai
client = openai(
    # this is the default and can be omitted
    api_key=os.environ.get(""openai_api_key""),
)

chat_completion = client.chat.completions.create(messages=[{""role"": ""user"",""content"": final_prompt,  }],model=""gpt-3.5-turbo"",)

answer = chat_completion.choices[0].message.content.strip()
print(answer)

sample output:
> entering new graphcypherqachain chain...
generated cypher:
match (d:director {name: ""oliver stone""})-[:directed]->(m:movie)
return m.title
full context:
[{'m.title': 'wall street'}]

> finished chain.
based on the unstructured information retrieved from the top result of the search, oliver stone directed the film ""wall street."" 

in addition to ""wall street,"" some other films directed by oliver stone include ""platoon,"" ""jfk,"" ""born on the fourth of july,"" ""natural born killers,"" and ""snowden.""",https://stackoverflow.com/questions/78173243,neo4j,16-03-2024 20:38,2575.0,1.0,1.0,True,26-08-2024 18:27,26-08-2024 18:26
72016319,"detect languages in a column, but ignore ambiguous values. why am i getting an error?","here is a sample dataset:




id
details




1
here are the details on facebook's global part...


2
aktien new york schluss: moderate verluste nac...


3
clï¿½ï¿½ï¿½ï¿½ture de wall street : trump plombe la tend...


4
''


5
nan




i need to add 'language' column, which represents what language is used in 'details' column, so that in the end it will look like this:




id
details
language




1
here are the details on facebook's global part...
en


2
aktien new york schluss: moderate verluste nac...
de


3
clï¿½ï¿½ï¿½ï¿½ture de wall street : trump plombe la tend...
fr


4
''
nan


5
nan
nan




i tried this code:
!pip install langdetect
from langdetect import detect, detectorfactory
detectorfactory.seed = 0
df2=df.dropna(subset=['details'])
df2['language']=df2['details'].apply(detect)

it failed, i guess it is because of rows that have values like 'id'=4. therefore, i tried this:
!pip install langdetect
from langdetect import detect, detectorfactory
detectorfactory.seed = 0
df2=df.dropna(subset=['details'])
df2['language']=df2['details'].apply(lambda x: detect(x) if len(x)>1 else np.nan)

however, i still got an error:
langdetectexception: no features in text.","['python', 'language-detection']",72016413,"you can catch the error and return nan from the function you apply. note that you can give any callable that takes one input and returns one output as the argument to .apply(), it doesn't have to be a lambda
def detect_lang(x):
    if len(x) <= 1: return np.nan 
    try:
        lang = detect(x)
        if lang: return lang # return lang if lang is not empty
    except langdetect.langdetectexception:
        pass # don't do anything when you get an error, so you can fall through to the next line, which returns a nan
    return np.nan  # if lang was empty or there was an error, we reach this line

df2['language']=df2['details].apply(detect_lang)

i'm not sure why you had if len(x)>1 in there: that would only return nan when the original string has zero or one characters, but i included it in my detect_lang function to keep the functionality consistent with your lambda.",https://stackoverflow.com/questions/72016319,python,26-04-2022 15:07,412.0,0.0,1.0,True,26-04-2022 15:18,26-04-2022 15:18
18230269,where can i download the ispell *.dict and *.affix files?,"i am quite new to postgresql full text search and i am setting up the configuration as where can i download the ispell *.dict and *.affix filefollowing (exactly as in docs):
create text search dictionary english_ispell (
    template = ispell,
    dictfile = english, 
    afffile = english, 
    stopwords = english
);

so, this i think expects files english.dict and english.affix on for example:
/usr/share/postgresql/9.2/tsearch_data

but these files are not there. i just have ispell_sample.dict and ispell_sample.affix - which when included above work fine - no problem.
so... i followed this post and downloaded the required dictionary from the open office people and renamed the .dic to .dict and .aff to .affix. then i have checked (using file -bi dict.affix and file -bi english.dict and they are utf8 encoded).
when i run the above text search dictionary, i get the error:
 error:  wrong affix file format for flag
 context:  line 2778 of configuration file ""/usr/share/postgresql/9.2/tsearch_data/english.affix"": ""compoundmin 1
 ""

i was wondering if anyone had clues on how to solve this problem or if anyone had encountered this before..
thanks./.
update:1: i guess the question can be rephrased as follows:
where can i download the ispell *.dict and *.affix file for postgres","['dictionary', 'full-text-search', 'nlp', 'ispell']",19944247,here's a good reference:  this is a good resource for those dictionaries of any language.,https://stackoverflow.com/questions/18230269,dictionary,14-08-2013 11:17,5413.0,6.0,2.0,True,07-11-2023 04:21,23-05-2017 12:10
73008946,"typeerror: argument &#39;other&#39; has incorrect type (expected spacy.tokens.token.token, got str)","i'm looking to get all sentences in a text file that contain at least one of the conjunctions in the list ""conjunctions"". however, when applying this function for the text in the variable ""text_to_look"" like this:
import spacy
lang_model = spacy.load(""en_core_web_sm"")
text_to_look = ""a woman is looking at books in a library. she's looking to buy one, but she hasn't got any money. she really wanted to book, so she asks another customer to lend her money. the man accepts. they get along really well, so they both exchange phone numbers and go their separate ways.""

def get_coordinate_sents(file_to_examine):
    conjunctions = ['and', 'but', 'for', 'nor', 'or', 'yet', 'so']
    text = lang_model(file_to_examine)
    sentences = text.sents
    for sentence in sentences:
        coord_sents = []
        if any(conjunction in sentence for conjunction in conjunctions):
            coord_sents.append(sentence)
    return coord_sents
        
wanted_sents = get_coordinate_sents(text_to_look)

i get this error message :
typeerror: argument 'other' has incorrect type (expected spacy.tokens.token.token, got str)

there seems to be something about spacy that i'm not aware of and prevents me from doing this...","['python', 'spacy']",73010340,"while the problem lies in the fact that conjunction is a string and sentence is a span object, and to check if the sentence text contains a conjunction you need to access the span text property, you also re-initialize the coord_sents in the loop, effectively saving only the last sentence in the variable. note a list comprehension looks preferable in such cases.
so, a quick fix for your case is
def get_coordinate_sents(file_to_examine):
    conjunctions = ['and', 'but', 'for', 'nor', 'or', 'yet', 'so']
    text = lang_model(file_to_examine)
    return [sentence for sentence in text.sents if any(conjunction in sentence.text for conjunction in conjunctions)]

here is my test:
import spacy
lang_model = spacy.load(""en_core_web_sm"")

text_to_look = ""a woman is looking at books in a library. she's looking to buy one, but she hasn't got any money. she really wanted to book, so she asks another customer to lend her money. the man accepts. they get along really well, so they both exchange phone numbers and go their separate ways.""
file_to_examine = text_to_look
conjunctions = ['and', 'but', 'for', 'nor', 'or', 'yet', 'so']

text = lang_model(file_to_examine)
sentences = text.sents
coord_sents = [sentence for sentence in sentences if any(conjunction in sentence.text for conjunction in conjunctions)]

output:
>>> coord_sents
[she's looking to buy one, but she hasn't got any money., she really wanted to book, so she asks another customer to lend her money., they get along really well, so they both exchange phone numbers and go their separate ways.]

however, the in operation will find nor in north, so in crimson, etc.
you need a regex here:
import re
conjunctions = ['and', 'but', 'for', 'nor', 'or', 'yet', 'so'] 
rx = re.compile(fr'\b(?:{""|"".join(conjunctions)})\b')

def get_coordinate_sents(file_to_examine):
    text = lang_model(file_to_examine)
    return [sentence for sentence in text.sents if rx.search(sentence.text)]",https://stackoverflow.com/questions/73008946,python,17-07-2022 02:47,987.0,1.0,1.0,True,17-07-2022 08:43,17-07-2022 08:43
72425277,how do you install a library from huggingface? e.g. gpt neo 125m,"i am confused on how to install a library from huggingface on your own desktop or server. how complicated is it to install a library? are there step by step instructions anywhere? i found some articles, but they assumed a certain level of knowledge and iï¿½ï¿½ï¿½m a total beginner and was unable to follow them.
to be more specific, i was looking at the gpt libraries. gpt neo 125m seems to be the smallest of these so iï¿½ï¿½ï¿½m assuming that would be the easiest to insta"" rel=""nofollow noreferrer"">

also, once you install a library on your own machine, is it free to use? i see that huggingface has a pricing structure:

but iï¿½ï¿½ï¿½m not sure what it applies to. does this pricing structure apply if you host the model on your own computer?
iï¿½ï¿½ï¿½m a total noob to this stuff so any tips are appre","['machine-learning', 'artificial-intelligence', 'huggingface-transformers', 'huggingface']",72515673,"for locally downloading gpt-neo-125m onto your own desktop.
i actually have a youtube video going through these steps for  gpt-neo-2.7b model if you are interested. the steps are exactly the same for gpt-neo-125m
first, move to the ""files and version"" tab from the respective model's official page in hugging face. so for gpt-neo-125m it would be this
then click on the top right corner 'use in transformers' and you will get a window like this

now just follow the git clone commands there - for gpt-neo125m it  will be
git lfs install
git clone 

this will download all the files and the models that you see in that page to your local machine's directory.
and now you can run the below code, exactly following the official doc, only changing the 'model' parameter's value to the local directory where you just gitcloned above.
## below is my implementations taking model from local machine
from transformers import pipeline

generator = pipeline('text-generation', model='/your_local_dir_where_you_downloaded/')

generator(""usa will be "", do_sample=true, max_length=120, min_length=20)


also, note that you can manually download the model files (by clicking on the down arrow) from huggingface above site, if you don't want to use git lfs. in that case you need to pass git_lfs_skip_smudge=1, as the doc says
# if you want to clone without large files ï¿½ï¿½ï¿½ just their pointers
# prepend your git clone with the following env var:
git_lfs_skip_smudge=1

does this pricing structure apply if you host the model on your own computer?
no. if you are using offline, i.e. host the model on your own computer - there are no costs.
also if you are using these models on your own cloud hardware like aws ec2, or your own server, then there are no c/p>
but if you use huggingface api endpoints for inference then you will be charged accordingly !!
so the pricing given in huggingface.co/pricing - applies when you are directly hitting huggingface's own api endpoints for inference.",https://stackoverflow.com/questions/72425277,machine-learning,29-05-2022 16:11,1216.0,1.0,1.0,True,13-12-2022 15:37,13-12-2022 15:37
73409356,how can i combine all the tokenized word to a sentence in a column?,"how can i combine all the tokenized words into a sentence in a column?
tokenized_word = ['really','smart','people']

in a sentence = really smart people","['python', 'machine-learning', 'nlp', 'data-preprocessing']",73504582,"def remove_punctuation(txt):
  txt_nopunt = "" "".join([c for c in txt if c not in string.punctuation])
  return txt_nopunt

data['tokenized_word'] = data['tokenized_word'].apply(lambda x: remove_punctuation(x))",https://stackoverflow.com/questions/73409356,python,18-08-2022 20:58,453.0,-1.0,2.0,True,31-08-2022 19:05,18-08-2022 23:28
77325636,how to load an existing vector db into langchain?,"i have the following code which loads my pdf file generates embeddings and stores them in a vector db.  i can then use it to preform searches on it.
the issue is that every time i run it the embeddings are regrated and stored in the db along with the ones already created.
im trying to figurer out how to load an existing vector db into langchain.  rather then recreating them every time the app runs.

load it
def load_embeddings(store, file):
    # delete the dir
    # shutil.rmtree(store)  # i have to delete it or it just loads double data

    loader = pypdfloader(file)
    text_splitter = charactertextsplitter(
        separator=""\n"",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        is_separator_regex=false,
    )
    pages = loader.load_and_split(text_splitter)

    return docarrayhnswsearch.from_documents(
        pages, googlepalmembeddings(), work_dir=store + ""/"", n_dim=768
    )

use it
db = load_embeddings(""linda_store"", ""linda.pdf"")
embeddings = googlepalmembeddings()

query = ""have i worked with oauth?""
embedding_vector = embeddings.embed_query(query)
docs = db.similarity_search_by_vector(embedding_vector)

for i in range(len(docs)):
    print(i, docs[i])

issue
this works fine but if i run it again it just loads the file again into the vector db.  i want it to just use the db after i have created it and not create it again.
i cant seem to find a method for loading it i tried
db = docarrayhnswsearch.load(""hnswlib_store/"", embeddings)

but thats a no go.","['python', 'langchain', 'palm-api', 'hnswlib']",77326978,"your load_embeddings function is recreating the database every time you call it. here's why:
1. you're loading from pypdfloader every time
...
# we don't need this when loading from store
loader = pypdfloader(file) 
...

2. from_documents(documents, embedding, **kwargs)
...
# we don't need to pass pages when loading from store
return docarrayhnswsearch.from_documents(
    pages, googlepalmembeddings(), work_dir=store + ""/"", n_dim=768
)
...

instead, you can try this:
def query_vector_store(query):
    embeddings = openaiembeddings(openai_api_key=open_ai_key)
    vector_store = docarrayhnswsearch.from_params(embeddings, ""store/"", 1536)
    
    embedding_vector = embeddings.embed_query(query)
    
    return vector_store.similarity_search_by_vector(embedding_vector)

i am using openaiembeddings() here but the same code should apply to googlepalmembeddings() just make sure you update the value of the dimension.
1. docarrayhnswsearch.from_params
we're using docarrayhnswsearch.from_params instead to load embeddings from the store (see here). this method does not expect the documents.
2. we're using our vector_store to perform similarity search
as you can see from the query_vector_store(query: str) function above, we're not re-loading the documents from the pdf loader every time. instead, we're just passing in our embeddings, work directory, and dimensions.
3. usage
you can use the method as such: query_vector_store('your_query').
based on your for loop here:
for i in range(len(docs)):
    print(i, docs[i])

you'll see the documents sorted by most similar.
i hope this helps!",https://stackoverflow.com/questions/77325636,python,19-10-2023 16:38,4782.0,3.0,1.0,True,19-10-2023 20:41,19-10-2023 16:50
71866288,mixing non-overlapping everygrams in order,"i'm looking for a method to generate sequences from every-grams up to length n that match an input sentence:
given a sentence:  ""break this into sequences""  and n = 3
i want to create the sequences:
(""break"", ""this"", ""into"", ""sequences"")
(""break"", ""this"", ""into sequences"")
(""break"", ""this into"", ""sequences"")
(""break this"", ""into"", ""sequences"")
(""break this"", ""into sequences"")
(""break"", ""this into sequences"")
(""break this into"", ""sequences"")

nltk has the everygram package, but i'm not quite sure how i'd use it toward my goal.
i've tried adapting the problem to focus on characters for simplicity, i.e.,
it may be helpful to consider these as character-grams (and, as rici suggested, spacing out characters [with and without spacing shown for clarity]):
abcd goes to:
(a, b, c, d)       (a, b, c, d)
(a, b, c  d)       (a, b, cd)
(a, b  c, d)       (a, bc, d)
(a  b, c, d)       (ab, c, d)
(a  b, c  d)       (ab, cd)
(a, b  c  d)       (a, bcd)
(a  b  c, d)       (abc, d)

for clarity, this should generalize for any length, given a n as the maximum-sized n-gram; so, for abcde with n=3 we'd have:
(a, b, c, d, e)     (a, b, c, d, e)
(a, b, c, d  e)     (a, b, c, de)
(a, b, c  d, e)     (a, b, cd, e)
(a, b  c, d  e)     (a, bc, d, e)
(a  b, c, d, e)     (ab, c, d, e)
(a, b  c, d  e)     (a, bc, de)
(a  b, c, d  e)     (ab, c, de)
(a  b, c  d, e)     (ab, cd, e)
(a, b, c  d  e)     (a, b, cde)
(a, b  c  d, e)     (a, bcd, e)
(a  b  c, d, e)     (abc, d, e)
(a  b, c  d  e)     (ab, cde)
(a  b  c, d  e)     (abc, de)

i'm thinking i may need to generate a grammar, something like:
exp ::= abc, d | a, bcd
abc ::= ab, c | a, bc
bcd ::= bc, d | b, cd
ab ::= a, b | a, b
bc ::= b, c | b, c
cd ::= c, d | c, d
a ::= a
b ::= b
c ::= c
d ::= d

and find all parses of the sentence, but certainly there must be a procedural way to go about this?","['python-3.x', 'nlp', 'nltk', 'grammar', 'n-gram']",71866465,"maybe it would be helpful to space your example out a bit:
(a , b , c , d)
(a , b , c   d)
(a , b   c , d)
(a   b , c , d)
(a   b , c   d)
(a , b   c   d)
(a   b   c , d)
(a   b   c   d)  # added for completeness

looking at that, it's evident that what differentiates the rows is the presence or absence of commas, a typical binary choice. there are three places a comma could go, so there are eight possibilities, corresponding to the eight binary numbers of three digits.
the easiest way to list these possibilities is to count from 0 0 0 to 1 1 1.

for your modified question, in which there is a maximum length of a part, one simple recursive solution in python is:
def kgram(k, v):
    'generate all partitions of v with parts no larger than k'
    def helper(sfx, m):
        if m == 0: yield sfx
        else:
            for i in range(1, min(k, m)+1):
                yield from helper([v[m-i:m]]+sfx, m-i)

    yield from helper([], len(v))

here's a quick test:
>>> for p in gram(3, 'one two three four five'.split()): print(p)
... 
[['one'], ['two'], ['three'], ['four'], ['five']]
[['one', 'two'], ['three'], ['four'], ['five']]
[['one'], ['two', 'three'], ['four'], ['five']]
[['one', 'two', 'three'], ['four'], ['five']]
[['one'], ['two'], ['three', 'four'], ['five']]
[['one', 'two'], ['three', 'four'], ['five']]
[['one'], ['two', 'three', 'four'], ['five']]
[['one'], ['two'], ['three'], ['four', 'five']]
[['one', 'two'], ['three'], ['four', 'five']]
[['one'], ['two', 'three'], ['four', 'five']]
[['one', 'two', 'three'], ['four', 'five']]
[['one'], ['two'], ['three', 'four', 'five']]
[['one', 'two'], ['three', 'four', 'five']]",https://stackoverflow.com/questions/71866288,python-3.x,14-04-2022 03:48,135.0,0.0,1.0,True,14-04-2022 07:13,14-04-2022 06:00
77145360,looking for an efficient way to split columns in a text in pandas,"i have pandas dataframe and want to split the text column in such a way that each row has just two words. when splitting, i need to maintain the order so that i can combine them together based on line. is there efficient way to do this. i can do list comprehension but was looking at more efficient way. thanks
df = pd.dataframe({'col1':[22,23,44], 'col2': ['rr','gg','xx'], 'text': ['this is a sample text', 'this is another one','third example is a longer text']})","['python', 'pandas', 'nlp']",77145443,"using str.findall, explode, and groupby.cumcount:
out = (df.assign(text=df['text'].str.findall(r'(\s+(?:\s+\s+)?)'))
         .explode('text')
         .assign(line=lambda d: d.groupby(level=0).cumcount())
       )

regexes variant to handle any number of words:
n=2
out = (df.assign(text=df['text'].str.findall(fr'((?:\s+\s+){{,{n-1}}}(?:\s+))\s*'))
         .explode('text')
         .assign(line=lambda d: d.groupby(level=0).cumcount())
       )

alternative with itertools' batched recipe:
from itertools import islice

def batched(iterable, n):
    ""batch data into tuples of length n. the last batch may be shorter.""
    # batched('abcdefg', 3) --> abc def g
    if n < 1:
        raise valueerror('n must be at least one')
    it = iter(iterable)
    while batch := tuple(islice(it, n)):
        yield batch

n = 2
out = (df.assign(text=df['text'].map(lambda x: list(map(' '.join, batched(x.split(), n)))))
         .explode('text')
         .assign(line=lambda d: d.groupby(level=0).cumcount())
       )

output:
   col1 col2           text  line
0    22   rr        this is     0
0    22   rr       a sample     1
0    22   rr           text     2
1    23   gg        this is     0
1    23   gg    another one     1
2    44   xx  third example     0
2    44   xx           is a     1
2    44   xx    longer text     2

example output with n=4:
   col1 col2                 text  line
0    22   rr     this is a sample     0
0    22   rr                 text     1
1    23   gg  this is another one     0
2    44   xx   third example is a     0
2    44   xx          longer text     1",https://stackoverflow.com/questions/77145360,python,20-09-2023 19:14,64.0,1.0,3.0,True,20-09-2023 20:25,20-09-2023 19:43
76019941,openai chat completions api: can i use a fine-tuned gpt-3 model with the gpt-3.5 api endpoint (error: &quot;invalid url (post /v1/chat/completions)&quot;)?,"after we create a fine-tuned model, how can we use it at /v1/chat/completions? we tried this but it gave an error
curl --location ' \
--header 'authorization: bearer token' \
--header 'content-type: application/json' \
--data '{
    ""model"": ""davinci:ft-xxx-inc:6302f74d2000001f00f80919-2023-04-15-00-47-48"",
    ""messages"": [
        {
            ""role"": ""user"",
            ""content"": ""how to use apple vision api to recognize text? any example?""
        }
    ]
}'
// error
{
    ""error"": {
        ""message"": ""invalid url (post /v1/chat/completions)"",
        ""type"": ""invalid_request_error"",
        ""param"": null,
        ""code"": null
    }
}","['openai-api', 'chatgpt-api']",76021717,"it seems like you wanted to fine-tune the gpt-3 davinci model and use it with the gpt-3.5 api endpoint.
you can fine-tune the davinci model as stated in the official openai documentation:

fine-tuning is currently only available for the following base models:
davinci, curie, babbage, and ada. these are the original models that
do not have any instruction following training (like text-davinci-003
does for example). you are also able to continue fine-tuning a
fine-tuned model to add additional data without having to start from
scratch.

but... the davinci model is not compatible with the /v1/chat/completions api endpoint as stated in the official openai documentation:




endpoint
model name




/v1/chat/completions
gpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613


/v1/completions
text-davinci-003, text-davinci-002, text-curie-001, text-babbage-001, text-ada-001


/v1/edits
text-davinci-edit-001, code-davinci-edit-001


/v1/audio/transcriptions
whisper-1


/v1/audio/translations
whisper-1


/v1/fine-tunes
davinci, curie, babbage, ada


/v1/embeddings
text-embedding-ada-002, text-search-ada-doc-001


/v1/moderations
text-moderation-stable, text-moderation-latest",https://stackoverflow.com/questions/76019941,openai-api,15-04-2023 01:24,1442.0,1.0,1.0,True,12-06-2024 17:04,12-06-2024 17:04
40147462,how are programming language specific settings called in vim and how to detect + overwrite them?,"the editor vim comes with syntax highlighting for many different programming languages.
questions:

in emacs, language-specific settings are called ""modes"". in vim, however, the term ""mode"" refers to command or insert mode. so what is the vim term for programming language specific settings?
is the programming language of a document determined from its file name extension, or from its contents?
how can i find out which programming language specific mode vim is in?
how can i overwrite that once and for all for a certain class of documents?","['vim', 'file-type', 'language-detection']",40153945,"posting an answer, as requested.

(1) in emacs, language-specific settings are called ""modes"". in vim, however, the term ""mode"" refers to command vs insert mode. so what is the vim term for programming language specific settings?

the equivalent vim term is filetype.  vim uses filetypes to apply language-specific options, indentation, syntax highlighting, key mappings, etc.  this is described in detail in the help, see :h filetype.
to enable automatic filetype detection and handling, you'd normally add something like this to your vimrc:
filetype plugin indent on
syntax on

to override the settings vim gives you this way, you need to add the overrides to a file ~/.vim/after/ftplugin/<filetype>.vim (or equivalent).  see :h ftplugin for more details.

(2) is the programming language of a document determined from its file name extension, or from its contents?

both methods are used.  vim does most filetype detection in a file filetype.vim in it's runtime directory.  to find out the exact location of this file:
:echo $vimruntime.'/filetype.vim'

plugins can add more filetypes, and / or override detection and handling of standard ones.

(3) how can i find out which programming language specific mode vim is in?

from vim:
set ft?


(4) how can i overwrite that once and for all for a certain class of documents?

to change the filetype of the current file:
:setf <new_filetype>

or
:setl ft=<new_filetype>

to make the change permanent: do that from a modeline (cf. :h modeline).  for example:
# vim: filetype=python

you can also use autocmds to achieve the same effect:
autocmd bufread,bufnewfile *.py setlocal filetype=python

do read :h filetype, where all this is described in detail.",https://stackoverflow.com/questions/40147462,vim,20-10-2016 06:56,6431.0,3.0,2.0,True,06-11-2021 23:17,06-11-2021 23:17
68763866,measuring co-occurence patterns in media articles over time with quanteda,"i am trying to measure the number of times that different words co-occur with a particular term in collections of chinese newspaper articles from each quarter of a year. to do this, i have been using quanteda and written several r functions to run on each group of articles. my work steps are:

group the articles by quarter.
produce a frequency co-occurence matrix (fcm) for the articles in each quarter (function 1).
take the column from this matrix for the 'term' i am interested in and convert this to a data.frame (function 2)
merge the data.frames for each quarter together, then produce a large csv file with a column for each quarter and a row for each co-occurring term.

this seems to work okay. but i wondered if anybody more skilled in r might be able to check what i am doing is correct, or might suggest a more efficient way of doing it?
thanks for any help!
#function 1 to produce the fcm

get_fcm <- function(data) {
  ch_stop <- stopwords(""zh"", source = ""misc"")
  corp = corpus(data)
  toks = tokens(corp, remove_punct = true) %>% tokens_remove(ch_stop)  
  fcm = fcm(toks, context = ""window"", window = 1, tri = false)
  return(fcm)
}

>fcm_14q4 <- get_fcm(data_14q4)
>fcm_15q1 <- get_fcm(data_15q1)

#function 2 to select the column for the 'term' of interest (such as china ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½) and make a data.frame

convert2df <- function(matrix, term){
  mat_term = matrix[,term]
  df = convert(mat_term, to = ""data.frame"")
  colnames(df)[1] = ""term""
  colnames(df)[2] = ""freq""
  x = df[order(-df$freq),]
  return(x)
}

>ch14 <- convert2df(fcm_14q4, ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"")
>ch15 <- convert2df(fcm_15q1, ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"")

#merging the data.frames

df <- merge(x=ch14q4, y=ch15q1, by=""term"", all.x=true, all.y=true)
df <- merge(x=df, ytrue, all.y=true) #etc for all the dataframes... 


update: following ken's advice in the comments below, i have tried doing it a different way, using the window function of tokens_select() and then a document feature matrix. after labelling the corpus documents according to their quarter, the following r function should take the tokenized corpus toks and then produce a data.frame of the number of times words co-occur within a specified window of a term.
coocdfm <- function(toks, term, window){
  ch_stop = stopwords(""zh"", source = ""misc"")
  cooc_toks = tokens_select(toks, term, window = window)
  cooc_toks2 = tokens(cooc_toks, remove_punct = true)
  cooc_toks3 = tokens_remove(cooc_toks2, ch_stop)
  dfmat = dfm(cooc_toks3)
  dfmat_grouped = dfm_group(dfmat, groups = ""quarter"")
  counts = convert(t(dfmat_grouped), to = ""data.frame"")
  colnames(counts)[1] <- ""feature""
  return(counts)
}","['r', 'nlp', 'quanteda']",68769901,"if you are interested in counting co-occurrences within a window for specific target terms, a better way is to use the window argument of tokens_select(), and then to count occurrences from a dfm on the window-selected tokens.
library(""quanteda"")
## package version: 3.0
## unicode version: 13.0
## icu version: 69.1
## parallel computing: 12 of 12 threads used.
## see  for tutorials and examples.

toks <- tokens(data_corpus_inaugural)

dfmat <- toks %>%
  tokens_select(""nuclear"", window = 5) %>%
  tokens(remove_punct = true) %>%
  tokens_remove(stopwords(""en"")) %>%
  dfm()

topfeatures(dfmat)[-1]
##     weapons      threat        work       earth elimination         day 
##           6           3           2           2           2           1 
##         one        free       world 
##           1           1           1

here i've first done a ""conservative"" tokenisation to keep everything, then performed the context selection.  i then processed that further to remove punctuation and stopwords before tabulating the results in a dfm.  this will be large and very sparse but you can summarise the top co-occuring words using topfeatures() or quanteda.textstats::textstat_frequency().",https://stackoverflow.com/questions/68763866,r,12-08-2021 20:35,151.0,2.0,1.0,True,15-08-2021 13:43,15-08-2021 13:43
76745308,removing stopwords also removes spaces between words during frequency distribution,"i am looking to remove stopwords from text to optimise my frequency distribution results
my initial frequency distribution code is written:
# determine the frequency distribution 
from nltk.tokenize import word_tokenize
tokens = nltk.word_tokenize(review_comments)
fdist = freqdist(tokens)
fdist

this returns
freqdist({""'"": 521, ',': 494, ""'the"": 22, 'a': 16, ""'of"": 16, ""'is"": 12, ""'to"": 10, ""'for"": 9, ""'it"": 8, ""'that"": 8, ...})
i want to remove the stopwords with the following code
# delete all the alpanum.
# filter out tokens that are neither alphabets nor numbers (to eliminate punctuation marks, etc.).
filtered = [word for word in review_comments if word.isalnum()]

# remove all the stopwords
# download the stopword list.
nltk.download ('stopwords')
from nltk.corpus import stopwords

# create a set of english stopwords.
english_stopwords = set(stopwords.words('english'))

# create a filtered list of tokens without stopwords.
filtered2 = [x for x in filtered if x.lower() not in english_stopwords]

# define an empty string variable.
filtered2_string = ''

for value in filtered:
    # add each filtered token word to the string.
    filtered2_string = filtered2_string + value + ''
    

now i run the fdist again
from nltk.tokenize import word_tokenize
trial= nltk.word_tokenize(filtered2_string)
fdist1 = freqdist(trial)
fdist1

this returns the code
freqdist({'whenitcomestoadmsscreenthespaceonthescreenitselfisatanabsolutepremiumthefactthat50ofthisspaceiswastedonartandnotterriblyinformativeorneededartaswellmakesitcompletelyuselesstheonlyreasonthatigaveit2starsandnot1wasthattechnicallyspeakingitcanatleaststillstanduptoblockyournotesanddicerollsotherthanthatitdropstheballcompletelyanopenlettertogaleforce9yourunpaintedminiaturesareverynotbadyourspellcardsaregreatyourboardgamesaremehyourdmscreenshoweverarefreakingterribleimstillwaitingforasinglescreenthatisntpolluted': 1})
review_comments = ''
for i in range(newdf.shape[1]):
    # add each comment.
    review_comments = review_comments + newdf['tokens1'][i]```


how do i get the stopwords to not remove the spaces and count the words individually?




i removed the stopwords and rerun the frequency distribution hoping to get the most frequent words.","['python', 'nltk', 'tokenize', 'frequency', 'stop-words']",76745489,"cleaning in nlp tasks is generally performed on tokens rather than characters of a string to leverage the inbuild functionalities/ methods. however, you can always do this from scratch using your own logic on characters as well, if you need to. the stopwords in nltk are in the form of tokens to use for clean up of your text corpus. you can add more tokens that you need to eliminate from your list. for e.g. if you need the english stopwords and punctuations removed, do something like:
import string
from nltk.tokenize import word_tokenize

tokens = word_tokenize(review_comments)

## add any additional punctuations/ words you want to eliminate here, like below
english_stop_plus_punct = set(stopwords.words('english') + [""call""] + 
                          list(string.punctuation + ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½""))

filtered2 = [x for x in tokens if x.lower() not in english_stop_plus_punct]

fdist1 = nltk.freqdist(filtered2)
fst({'presence': 3, 'meaning': 2, 'might': 2, 'many': 1, 'psychologists': 1, 'knowing': 1, 'life': 1, 'drive': 1, 'look': 1, ...})

example text from a write up on ""meaning of life"":
review_comments = """""" many psychologists call knowing your lifeï¿½ï¿½ï¿½s meaning ï¿½ï¿½ï¿½presence,ï¿½ï¿½ï¿½ and the drive to look for it ï¿½ï¿½ï¿½search.ï¿½ï¿½ï¿½ they are not mutually exclusive: you might or might not search, whether you already have a sense of meaning or not. some people low in presence donï¿½ï¿½ï¿½t bother searchingï¿½ï¿½ï¿½they are ï¿½ï¿½ï¿½stuck.ï¿½ï¿½ï¿½ some are high in presence but keep searchingï¿½ï¿½ï¿½we ca",https://stackoverflow.com/questions/76745308,python,22-07-2023 18:39,100.0,1.0,1.0,True,22-07-2023 22:28,22-07-2023 19:15
78627285,rag app fails using dataapiclient: &quot;error loading the fetch-h2 client for the dataapiclient... try setting  to &#39;fetch&#39;&quot;,"error:
<scriptid=""__next_data__""type=""application/json"">{
    ""props"": {
        ""pageprops"": {
            ""statuscode"": 500
        }
    },
    ""page"": ""/_error"",
    ""query"": {
        ""__next_page"": ""/api/chat""
    },
    ""buildid"": ""development"",
    ""isfallback"": false,
    ""err"": {
        ""name"": ""error"",
        ""source"": ""server"",
        ""message"": ""error loading the fetch-h2 client for the dataapiclient... try setting  to 'fetch'"",
        ""stack"": ""error: error loading the fetch-h2 client for the dataapiclient... try setting  to 'fetch'\n

above is the error occurred in rag application.
api route, where error occured when called.
import { nextrequest, nextresponse } from 'next/server';
import { getvectorstore } from '@/lib/astradb';
import { aimessage, humanmessage } from '@langchain/core/messages';
import {
  chatprompttemplate,
  messagesplaceholder,
  prompttemplate,
} from '@langchain/core/prompts';
import { chatopenai } from '@langchain/openai';
import { redis } from '@upstash/redis';
import { ratelimit } from '@upstash/ratelimit';
import {
  langchainstream, streamingtextresponse,
  message as vercelchatmessage,
} from 'ai';
import { upstashrediscache } from '@langchain/community/caches/upstash_redis';
import { createstuffdocumentschain } from 'langchain/chains/combine_documents';
import { createhistoryawareretriever } from 'langchain/chains/history_aware_retriever';
import { createretrievalchain } from 'langchain/chains/retrieval';
import  from '


const ratelimit = new ratelimit({
  redis: redis.fromenv(),
  limiter: ratelimit.fixedwindow(8, '30s'),
});

export async function post(req: nextrequest) {

console.log('api requested,dddddddddddddddddddddddddddddd')

  try {

 const ip = req.ip ?? 'ip';
 const { success, remaining } = await ratelimit.limit(ip);

 // block the request if unsuccessful
 if (!success) {
   return new response('ratelimited!', { status: 429 });
 }

    const body = await req.json();
    const messages = body.messages;

    const chathistory = messages
      .slice(0, -1)
      .map((m: vercelchatmessage) =>
        m.role === 'user'
          ? new humanmessage(m.content)
          : new aimessage(m.content)
      );

    const currentmessagecontent = messages[messages.length - 1].content;

    const cache = new upstashrediscache({
      client: redis.fromenv({
        agent: new  keepalive: true }),
      }),
    });

    const { stream, handlers } = langchainstream();

    const chatmodel = new chatopenai({
      apikey: process.env.openai_api_key!,`your text`
      modelname: 'gpt-3.5-turbo',``your text``
      streaming: true,
      callbacks: [handlers],
      verbose: true,
      cache,
    });

    const rephrasingmodel = new chatopenai({
      apikey: process.env.openai_api_key!,
      modelname: 'gpt-3.5-turbo',
      verbose: true,
      cache,
    });

    const retriever = (await getvectorstore()).asretriever();

    const rephraseprompt = chatprompttemplate.frommessages([
      new messagesplaceholder('chat_history'),
      ['user', '{input}'],
      [
        'user',
        'given the above conversation, generate a search query to look up in order to get information relevant to the current question. ' +
          ""don't leave out any relevant keywords. only return the query and no other text."",
      ],
    ]);

    const historyawareretrieverchain = await createhistoryawareretriever({
      llm: rephrasingmodel,
      retriever,
      rephraseprompt,
    });

    const prompt = chatprompttemplate.frommessages([
      [
        'system',
        ""you are a chatbot for a decentralized betting website namedecentbet. you impersonate the website's owner. "" +
          ""answer the user's questions based on the below context. "" +
          'format your messages in markdown format.\n\n' +
          'context:\n{context}',
      ],
      new messagesplaceholder('chat_history'),
      ['user', '{input}'],
    ]);

    const combinedocschain = await createstuffdocumentschain({
      llm: chatmodel,
      prompt,
      // documentprompt: prompttemplate.fromtemplate(
      //   'page url: {url}\n\npage content:\n{page_content}'
      // ),
      documentseparator: '\n--------\n',
    });

    const retrievalchain = await createretrievalchain({
      combinedocschain,
      retriever: historyawareretrieverchain,
    });

    retrievalchain.invoke({
      input: currentmessagecontent,
      chat_history: chathistory,
    });

    return new streamingtextresponse(stream);
  } catch (error) {
    console.error(""error:---"",error);
    return nextresponse.json({ error: 'internal server error' }, { status: 500 });
  }
}


here is route code where error occuring when called, strangely same code working fine with my other application.

astra db instilization:<
 import { astradb } from '@datastax/astra-db-ts';
import { dataapiclient } from '@datastax/astra-db-ts';
import { astradbvectorstore } from '@langchain/community/vectorstores/astradb';
import { openaiembeddings } from '@langchain/openai';
const endpoint = process.env.astra_db_endpoint || '';
const token = process.env.astra_db_application_token || '';
const collection = process.env.astra_db_collection || '';   



if (!token || !endpoint || !collection) {
  throw new error(
    'please set astra_db_endpoint, astra_db_application_token, and astra_db_collection environment variables.'
  );
}

export async function getvectorstore() {
  return astradbvectorstore.fromexistingindex(
    new openaiembeddings({ modelname: 'text-embedding-3-small' }),
    {
      token,
      endpoint,
      collection,
     
      collectionoptions: {
        vector: {
          dimension: 1536,
          metric: 'cosine',
        },
        }
    }
  );
}
const client = new dataapiclient(token);
const db = client.db(endpoint);


export async function getembeddingscollection() {
 return db.collection(collection);
}

unable to get over this error , but same code with same dependencies working fine in other app (
note : i've rewritten latest code from their respective docs, haven't worked not get over this error.
stack next js, ai lib for streaming, upstash caching, datastax astradb for embeddings.
please let me know work around to get over this. thank u","['node.js', 'next.js', 'langchain', 'datastax-astra', 'langchain-js']",78630925,"there are a couple of versions of @datastax/astra-db-ts that cause this error. it was fixed recently, so you should update the dependency to the latest version.
npm install @datastax/astra-db-ts@latest",https://stackoverflow.com/questions/78627285,node.js,15-06-2024 17:19,127.0,1.0,1.0,True,18-06-2024 04:48,18-06-2024 04:48
57455267,pos tagging and ner for chinese text with spacy,"i am trying to print the entities and pos present in chinese text. 
i have installed # !pip3 install jieba and used google colab for the below script.

but i am getting empty tuples for the entities and no results for pos_.
from spacy.lang.zh import chinese

nlp = chinese()
doc = nlp(u""ýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýýý"")

doc.ents
# returns (), i.e. empty tuple


for word in doc:
    print(word.text, word.pos_)

''' returns
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 
ï¿½ï¿½ï¿½ 
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 
ï¿½ï¿½ï¿½ 
ï¿½ï¿½ï¿½ 
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 
ï¿½ï¿½ï¿½ 
ï¿½ï¿½ï¿½ 
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 
ï¿½ï¿½ï¿½ 
ï¿½ï¿½ï¿½ï¿","['nlp', 'spacy', 'named-entity-recognition']",57475218,"edit 3/21: spacy now supports ner and pos tagging for cn
find the spacy model here: 
old answer:
spacy is a fantastic package, but as of yet does not support chinese, so i assume thats the reason you dont get pos results - even though your sentence is

""apple is looking at buying u.k. startup for $1 billion""

in traditional chinese and should therefore return ""apple"" and ""u.k."" as ent, among others.
for a more extensive nlp approach to traditional chinese, you can try using the stanford chinese nlp package - you are using python, and there are versions available for python (see a demo script or an intro on medium), but the original is java, if you are more comfortable with that.",https://stackoverflow.com/questions/57455267,nlp,12-08-2019 03:11,2806.0,1.0,2.0,True,27-03-2021 10:28,14-08-2019 02:20
78593700,langchain_community &amp; langchain packages giving error: missing 1 required keyword-only argument: &#39;recursive_guard&#39;,"all of sudden langchain_community & langchain packages started throwing error:
typeerror: forwardref._evaluate() missing 1 required keyword-only argument: 'recursive_guard'
the error getting generated somewhere in pydantic
i strongly suspect it is version mismatch. so i tried upgrading packages langchain, langchain_community, pydantic, langsmith etc. but no luck.
my current installed versions shows as under:
python 3.12.4

langchain: 0.2.3
langchain_community: 0.2.4
langsmith: 0.1.75
pydantic: 2.7.3
typing_extensions: 4.11.0

pip check also not showing any conflict.
here is complete trace of error. any help would be really appreciated.
typeerror: forwardref._evaluate() missing 1 required keyword-only argument: 'recursive_guard'

file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\streamlit\runtime\scriptrunner\script_runner.py"", line 600, in _run_script
    exec(code, module.__dict__)
file ""c:\myproject\myscript.py"", line 20, in <module>
    from langchain_community.vectorstores import chroma
file ""<frozen importlib._bootstrap>"", line 1412, in _handle_fromlist
file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langchain_community\vectorstores\__init__.py"", line 509, in __getattr__
    module = importlib.import_module(_module_lookup[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
file ""c:\program files\windowsapps\pythonsoftwarefoundation.python.3.12_3.12.1264.0_x64__qbz5n2kfra8p0\lib\importlib\__init__.py"", line 90, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langchain_community\vectorstores\chroma.py"", line 20, in <module>
    from langchain_core.documents import document
file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langchain_core\documents\__init__.py"", line 6, in <module>
    from langchain_core.documents.compressor import basedocumentcompressor
file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langchain_core\documents\compressor.py"", line 6, in <module>
    from langchain_core.callbacks import callbacks
file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langchain_core\callbacks\__init__.py"", line 22, in <module>
    from langchain_core.callbacks.manager import (
file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langchain_core\callbacks\manager.py"", line 29, in <module>
    from langsmith.run_helpers import get_run_tree_context
file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langsmith\run_helpers.py"", line 40, in <module>
    from langsmith import client as ls_client
file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langsmith\client.py"", line 52, in <module>
    from langsmith import env as ls_env
file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langsmith\env\__init__.py"", line 3, in <module>
    from langsmith.env._runtime_env import (
file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langsmith\env\_runtime_env.py"", line 10, in <module>
    from langsmith.utils import get_docker_compose_command
file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langsmith\utils.py"", line 31, in <module>
    from langsmith import schemas as ls_schemas
file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\langsmith\schemas.py"", line 69, in <module>
    class example(examplebase):
file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\pydantic\v1\main.py"", line 286, in __new__
    cls.__try_update_forward_refs__()
file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\pydantic\v1\main.py"", line 807, in __try_update_forward_refs__
    update_model_forward_refs(cls, cls.__fields__.values(), cls.__config__.json_encoders, localns, (nameerror,))
file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\pydantic\v1\typing.py"", line 554, in update_model_forward_refs
    update_field_forward_refs(f, globalns=globalns, localns=localns)
file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\pydantic\v1\typing.py"", line 520, in update_field_forward_refs
    field.type_ = evaluate_forwardref(field.type_, globalns, localns or none)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
file ""c:\users\lenovo\appdata\local\packages\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\localcache\local-packages\python312\site-packages\pydantic\v1\typing.py"", line 66, in evaluate_forwardref
    return cast(any, type_)._evaluate(globalns, localns, set())
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","['python', 'pydantic', 'langchain', 'langsmith']",78594372,"i am having the same issue. the stack is different, but the error comes from the same line pydantic\v1\typing.py"", line 66
this is referring to the python typing module (v3.12.4) that has an additional mandatory parameter 'recursive_guard'. there are other areas of the code in pydantic where this has been fixed (recursive_gurard=set()).
check this out --> 
within this thread, they mention that using python v3.12.3 could temporarily solve the issue in 1292, probably because this additional attribute in v3.12.4 (i am guessing here). this is not an option for me as my google alpha functions local deploy is not recognizing the --runtime=python311 and always take the latest runtime (v3.12.4).
i hope that they fix this too",https://stackoverflow.com/questions/78593700,python,07-06-2024 19:21,13890.0,10.0,6.0,True,02-08-2024 00:58,08-06-2024 02:24
53598243,is there a bi gram or tri gram feature in spacy?,"the below code breaks the sentence into individual tokens and the output is as below 
 ""cloud""  ""computing""  ""is"" ""benefiting""  "" major""  ""manufacturing""  ""companies""


import en_core_web_sm
nlp = en_core_web_sm.load()

doc = nlp(""cloud computing is benefiting major manufacturing companies"")
for token in doc:
    print(token.text)

what i would ideally want is, to read 'cloud computing' together as it is technically one word. 
basically i am looking for a bi gram. is there any feature in spacy that allows bi gram or tri grams ?","['python-3.x', 'nlp', 'tokenize', 'spacy', 'n-gram']",53612262,"spacy allows the detection of noun chunks. so to parse your noun phrases as single entities do this:

detect the noun chunks


merge the noun chunks

do dependency parsing again, it would parse ""cloud computing"" as single entity now.


>>> import spacy
>>> nlp = spacy.load('en')
>>> doc = nlp(""cloud computing is benefiting major manufacturing companies"")
>>> list(doc.noun_chunks)
[cloud computing, major manufacturing companies]
>>> for noun_phrase in list(doc.noun_chunks):
...     noun_phrase.merge(noun_phrase.root.tag_, noun_phrase.root.lemma_, noun_phrase.root.ent_type_)
... 
cloud computing
major manufacturing companies
>>> [(token.text,token.pos_) for token in doc]
[('cloud computing', 'noun'), ('is', 'verb'), ('benefiting', 'verb'), ('major manufacturing companies', 'noun')]",https://stackoverflow.com/questions/53598243,python-3.x,03-12-2018 16:50,23967.0,16.0,4.0,True,02-02-2023 16:51,04-12-2018 06:30
14732465,nltk tagging spanish words using a corpus,"i am trying to learn how to tag spanish words using nltk.
from the nltk book, it is quite easy to tag english words using their example.  because i am new to nltk and all language processing, i am quite confused on how to proceeed.
i have downloaded the cess_esp corpus.  is there a way to specifiy a corpus in nltk.pos_tag. i looked at the pos_tag documentation and didn't see anything that suggested i could.  i feel like i'm missing some key concepts.  do i have to manually tag the words in my text agains the cess_esp corpus? (by manually i mean tokenize my sentance and run it agains the corpus) or am i off the mark entirely.  thank you","['python', 'nltk']",14742406,"first you need to read the tagged sentence from a corpus. nltk provides a nice interface to no bother with different formats from the different corpora; you can simply import the corpus use the corpus object functions to access the data. see  . 
then you have to choose your choice of tagger and train the tagger. there are more fancy options but you can start with the n-gram taggers.
then you can use the tagger to tag the sentence you want. here's an example code:
from nltk.corpus import cess_esp as cess
from nltk import unigramtagger as ut
from nltk import bigramtagger as bt

# read the corpus into a list, 
# each entry in the list is one sentence.
cess_sents = cess.tagged_sents()

# train the unigram tagger
uni_tag = ut(cess_sents)

sentence = ""hola , esta foo bar .""

# tagger reads a list of tokens.
uni_tag.tag(sentence.split("" ""))

# split corpus into training and testing set.
train = int(len(cess_sents)*90/100) # 90%

# train a bigram tagger with only training data.
bi_tag = bt(cess_sents[:train])

# evaluates on testing data remaining 10%
bi_tag.evaluate(cess_sents[train+1:])

# using the tagger.
bi_tag.tag(sentence.split("" ""))

training a tagger on a large corpus may take a significant time. instead of training a tagger every time we need one, it is convenient to save a trained tagger in a file for later re-use. 
please look at storing taggers section in",https://stackoverflow.com/questions/14732465,python,06-02-2013 15:19,40440.0,22.0,4.0,True,21-04-2023 18:14,06-02-2013 15:35
75051126,"open ai&#39;s gpt davinci - asking it questions, but it&#39;s returning gibberish?","i ask it ""what is covid-19?"", by using the following code:
string key = ""xxxx"";

openaiapi api = new openaiapi(key,engine.davinci);

var results = api.completions.createcompletionsasync(new completionrequest(""what is covid-19?"", temperature: 0.1, max_tokens: 200), 1).result;

var answer = results.completions[0].text;

which returns:

covid-19 is a drug that is used to treat the symptoms of schizophrenia. it is a combination of two drugs, clozapine and olanzapine.
how does covid-19 work?
covid-19 is a combination of two drugs, clozapine and olanzapine. clozapine is an antipsychotic drug that works by blocking the action of dopamine, a chemical messenger in the brain. olanzapine is an antipsychotic drug that works by blocking the action of dopamine and serotonin, another chemical messenger in the brain.
how is covid-19 used?
covid-19 is used to treat schizophrenia.
how does covid-19 work?
covid-19 is a combination of two drugs, clozapine and olanzapine. clozapine is an antipsychotic drug that works by blocking

i'm baffled, what am i doing wrong here? shouldn't this emulate similar results to chatgpt? i am using the following nuget for openai access:","['c#', '.net', 'openai-api', 'gpt-3']",75104866,"i solved this by using openaiapi api = new openaiapi(key, ""text-davinci-003""); rather than engine.davinci.",https://stackoverflow.com/questions/75051126,c#,08-01-2023 20:41,467.0,0.0,1.0,True,13-01-2023 04:30,09-01-2023 09:06
79026052,cloudflare 400 error when calling openai completions api on cloudrun,"running into a weird error where i get a 400 error from cloud-flare when calling the completions api from gcp cloud run. the error doesn't occur locally for identical requests.
the error response:
<html>

<head><title>400 bad request</title></head>

<body>

<center><h1>400 bad request</h1></center>

<hr><center>cloudflare</center>

</body>

</html>

the request:
post to 
{""model"":""gpt-4-turbo"",""messages"":[{""role"":""system"",""content"":""using the notes provided, write a summary of the critical information from the notes.\nthe summary should be in dotpoints. prioritise the dotpoints by importance.""},{""role"":""user"",""content"":""                use only the following information to generate the summary:\n                            there are the following notes about the contact:\n            note content: \""test\""\nnote created at: 2024-09-26t07:08:04.304501z\nnote is in the context of: just the contact in the role of unknown \nnote type: manual""}]}

nothing obvious in the headers.
has anyone run into anything similar. wondering if its some kind of ip blocking by cloudflare or similar.","['google-cloud-platform', 'openai-api']",79034220,"so we managed to solve this by switching from using the okhttp java client, to using the java 17 core library client.
i suspect there is more to it then that (some configuration on the two clients) but we weren't able to find it at this time.",https://stackoverflow.com/questions/79026052,google-cloud-platform,26-09-2024 07:42,126.0,3.0,1.0,True,28-09-2024 12:48,26-09-2024 07:58
69831095,spacy: count occurrence for specific token in each sentence,"i want to count the occurrence of the token and for each sentence in a corpus using spacy and append the result for each sentence to a list.  until now the code bellow returns the total number (for the whole corpus) regarding and.
example/desired output for 3 sentences : ['1', '0', '2']
current output : [3]
doc = nlp(corpus)
nb_and = []
for sent in doc.sents:
    i = 0
    for token in sent:
        if token.text == ""and"":
            i += 1
            nb_and.append(i)","['nlp', 'counter', 'spacy', 'find-occurrences', 'spacy-3']",69831511,"you need to append i to nb_and after each sentence is processed:
for sent in doc.sents:
    i = 0
    for token in sent:
        if token.text == ""and"":
            i += 1
    nb_and.append(i)

test code:
import spacy
nlp = spacy.load(""en_core_web_trf"")
corpus = ""i see a cat and a dog. none seems to be unhappy. my mother and i wanted to buy a parrot and a tortoise.""
doc = nlp(corpus)
nb_and = []
for sent in doc.sents:
    i = 0
    for token in sent:
        if token.text == ""and"":
            i += 1
    nb_and.append(i)

nb_and
# => [1, 0, 2]",https://stackoverflow.com/questions/69831095,nlp,03-11-2021 20:04,1172.0,3.0,1.0,True,07-11-2021 14:14,07-11-2021 14:14
74198547,how to enable gpu for setfit?,"i am following this tutorial for setfit: 
when the training is running, it is using my cpu instead of my gpu. is there a way i can enable it?
here is the main part of the code:
from setfit import setfitmodel, setfittrainer
from sentence_transformers.losses import cosinesimilarityloss

# load a setfit model from hub
model_id = ""sentence-transformers/all-mpnet-base-v2""
model = setfitmodel.from_pretrained(model_id)

# create trainer
trainer = setfittrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    loss_class=cosinesimilarityloss,
    metric=""accuracy"",
    batch_size=64,
    num_iterations=20, # the number of text pairs to generate for contrastive learning
    num_epochs=1, # the number of epochs to use for constrastive learning
)

# train and evaluate
trainer.train()
metrics = trainer.evaluate()","['python', 'huggingface-transformers', 'sentence-transformers']",74204975,"if your training is running on cpu rather than gpu, it is because:

either you installed the cpu version of the pytorch.
either the version of cuda/cudnn and pytorch are not compatible, and the training falls back to cpu instead of gpu.

in essence it has nothing to do with the setfit model.
a working example for me in recent projects is:
(1) pip/pip3 install torch torchvision torchaudio --extra-index-url 
(2)  pip install transformers==4.22.0
note that you may have to uninstall pytorch first before reinstalling it: pip uninstall pytorch.
in order to make sure your gpu is visible, a short print would suffice:
training_device = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")",https://stackoverflow.com/questions/74198547,python,25-10-2022 18:19,2302.0,1.0,1.0,True,26-10-2022 11:41,26-10-2022 08:51
78282380,what is the openai chat completion api tools/functions property format?,"is there any clear documentation on the format of openai's chat completion api tools/functions object format? i understand it's json, but there appear to be underlying requirements of what property names/types are allowed inside of the objects.
i tried figuring out what all property types openai allowed in their tools/functions definition, but their documentation isn't great (they just have links to a simple example and an explanation of json schema ï¿½ï¿½ï¿½ï¿½). they define description and name, but leave parameters pretty much open to interpretation.
"" rel=""nofollow noreferrer"">image of tools property definition","['json', 'openai-api']",78282386,"after an hour or two of searching around, experimenting, and cobbling something together, i think i finally created a template that i could reference in the future. hopefully this saves someone else some time in the future.
{
  ""description"": ""this is a template that you can start from to build your tool"",
  ""name"": ""new_tool"",
  ""parameters"": {
    ""properties"": {
      ""array_property_name"": {
        ""description"": ""a property that returns an array of items (can be any type mentioned below, including an object)"",
        ""items"": {
          ""type"": ""string""
        },
        ""type"": ""array""
      },
      ""boolean_property_name"": {
        ""description"": ""a property that returns a boolean"",
        ""type"": ""boolean""
      },
      ""enum_property_name"": {
        ""description"": ""a property that returns a value from a list of enums (can be any type)"",
        ""enum"": [
          ""option 1"",
          ""option 2"",
          ""option 3""
        ],
        ""type"": ""string""
      },
      ""number_property_name"": {
        ""description"": ""a property that returns a number"",
        ""type"": ""number""
      },
      ""object_property_name"": {
        ""description"": ""a property that returns an object"",
        ""properties"": {
          ""foo"": {
            ""description"": ""a property on the object called 'foo' that returns a string"",
            ""type"": ""string""
          },
          ""bar"": {
            ""description"": ""a property on the object called 'bar' that returns a number"",
            ""type"": ""number""
          }
        }
      },
      ""string_property_name"": {
        ""description"": ""a property that returns a string"",
        ""type"": ""string""
      }
    },
    ""required"": [
      ""array_property_name"",
      ""number_property_name""
    ],
    ""type"": ""object""
  }
}",https://stackoverflow.com/questions/78282380,json,05-04-2024 21:13,1466.0,1.0,1.0,True,05-04-2024 21:20,05-04-2024 21:16
67401277,how do i edit a tensorflow dataset in a pandas dataframe?,"i am trying to build a transformer model for abstractive text summarization task. my dataset is the cnn dm  and i am trying to put the features on pandas dataframe.
my code:
pip install tensorflow_datasets
import tensorflow_datasets as tfds

cnn_builder = tfds.summarization.cnn_dailymail.cnndailymail()
cnn_info = cnn_builder.info
cnn_builder.download_and_prepare()
datasets = cnn_builder.as_dataset()
train_dataset, test_dataset = datasets[""train""], datasets[""test""]

reviews = pd.dataframe({'text':train_dataset['article'] ,'summary':train_dataset['highlights'] }) 
reviews.head()

but the output is :
typeerror traceback (most recent call last) <ipython-input-45-2da1e32d8eec> in <module>() ----> 1 reviews = pd.dataframe({'text':train_ds['article'] ,'summary':train_ds['highlights'] }) 2 reviews.head() typeerror: 'prefetchdataset' object is not subscriptable



after i fixed the code i got this output. could you please help me to fix this issue !
b""richard mcluckie, 48, and stuart mackenzie-walker, 51, invented games .\nwon permission from marmite owner unilever to use its name and image .\nthen they went on investment tv show to ask for funding from the dragons .\nbut unilever contract said entrepreneurs couldn't mention name marmite .\nthree dragons pulled out, but peter jones and duncan bannatyne agreed .\nthey paid the men \xc2\xa350,000 for a 40 per cent stake in board game business .""","['python', 'pandas', 'tensorflow', 'nlp', 'tensorflow-datasets']",67401920,"you can use as_dataframe method.
reviews = tfds.as_dataframe(train_dataset.take(10))

or you can iterate over the dataset to get article and highlights:
highlights = []
articles = []

for article_highlight in train_dataset.take(10):
  articles.append(article_highlight['article'].numpy())
  highlights.append(article_highlight['highlights'].numpy())

reviews = pd.dataframe({'text':articles ,'summary':highlights })

in your case, note that train_dataset.take(10) will get 10 elements from the dataset.",https://stackoverflow.com/questions/67401277,python,05-05-2021 12:26,1312.0,0.0,1.0,True,06-05-2021 08:25,06-05-2021 08:25
77164318,error with langchain chatprompttemplate.from_messages,"as shown in langchain quickstart, i am trying the following python code:
from langchain.prompts.chat import chatprompttemplate
template = ""you are a helpful assistant that translates {input_language} to {output_language}.""
human_template = ""{text}""

chat_prompt = chatprompttemplate.from_messages([
    (""system"", template),
    (""human"", human_template),
])

chat_prompt.format_messages(input_language=""english"", output_language=""french"", text=""i love programming."")

but when i run the above code, i get the following error:
traceback (most recent call last):
   file ""/home/yser364/projets/sinappsirdopenaiqa/promptworkout.py"", line 6, in <module>
     chat_prompt = chatprompttemplate.from_messages([
   file ""/home/yser364/.local/lib/python3.10/site-packages/langchain/prompts/chat.py"", line 220, in from_messages
     return cls(input_variables=list(input_vars), messages=messages)
   file ""/home/yser364/.local/lib/python3.10/site-packages/langchain/load/serializable.py"", line 64, in __init__
     super().__init__(**kwargs)
   file ""pydantic/main.py"", line 341, in pydantic.main.basemodel.__init__
 pydantic.error_wrappers.validationerror: 4 validation errors for chatprompttemplate
 messages -> 0
   value is not a valid dict (type=type_error.dict)
 messages -> 0
   value is not a valid dict (type=type_error.dict)
 messages -> 1
   value is not a valid dict (type=type_error.dict)
 messages -> 1
   value is not a valid dict (type=type_error.dict)

i use python 3.10.12.","['python', 'langchain']",77164939,"your example is from the prompt templates section of the langchain quickstart tutorial. i did not spot any differences, so it should work as given.
i tried out the example myself, with an additional loop to output the messages created by chat_prompt.format_messages:
from langchain.prompts.chat import chatprompttemplate
template = ""you are a helpful assistant that translates {input_language} to {output_language}.""
human_template = ""{text}""

chat_prompt = chatprompttemplate.from_messages([
    (""system"", template),
    (""human"", human_template),
])

messages = chat_prompt.format_messages(input_language=""english"", output_language=""french"", text=""i love programming."")
for message in messages:
    print(message.__repr__())

the example works without any errors. the result is very similar to what is shown in the tutorial, although not identical:
systemmessage(content='you are a helpful assistant that translates english to french.', additional_kwargs={})
humanmessage(content='i love programming.', additional_kwargs={}, example=false)

i ran the test with python 3.9.5 and langchain 0.0.300, which is the lastest version on pypi. according to pypi, it supports python >=3.8.1 and <4.0.
maybe your version of langchain or one of its dependencies is outdated? try to run it in a new venv with a fresh install of langchain.",https://stackoverflow.com/questions/77164318,python,23-09-2023 18:00,24009.0,3.0,2.0,True,18-12-2024 14:18,24-09-2023 03:23
60852962,training time of gensim word2vec,"i'm training word2vec from scratch on 34 gb pre-processed ms_marco corpus(of 22 gb). (preprocessed corpus is sentnecepiece tokenized and so its size is more) i'm training my word2vec model using following code : 
from gensim.test.utils import common_texts, get_tmpfile
from gensim.models import word2vec

class corpus():
    """"""iterate over sentences from the corpus.""""""
    def __init__(self):
        self.files = [
            ""sp_cor1.txt"",
            ""sp_cor2.txt"",
            ""sp_cor3.txt"",
            ""sp_cor4.txt"",
            ""sp_cor5.txt"",
            ""sp_cor6.txt"",
            ""sp_cor7.txt"",
            ""sp_cor8.txt""
        ]

    def __iter__(self):
        for fname in self.files:
            for line in open(fname):
                words = line.split()
                yield words

sentences = corpus()

model = word2vec(sentences, size=300, window=5, min_count=1, workers=8, sg=1, hs=1, negative=10)
model.save(""word2vec.model"")


my model is running now for about more than 30 hours now. this is doubtful since on my i5 laptop with 8 cores, i'm using all the 8 cores at 100% for every moment of time. plus, my program seems to have read more than 100 gb of data from the disk now. i don't know if there is anything wrong here, but the main reason after my doubt on the training is because of this 100 gb of read from the disk. the whole corpus is of 34 gb, then why my code has read 100 gb of data from the disk? does anyone know how much time should it take to train word2vec on 34 gb of text, with 8 cores of i5 cpu running all in parallel? thank you. for more information, i'm also attaching the photo of my process from system monitor. 

i want to know why my model has read 112 gb from memory, even when my corpus is of 34 gb in total? will my training ever get finished? also i'm bit worried about health of my laptop, since it is running constantly at its peak capacity since last 30 hours. it is really hot now. 
should i add any additional parameter in word2vec for quicker training without much performance loss?","['python', 'nlp', 'gensim', 'word2vec']",60856812,"completing a model requires one pass over all the data to discover the vocabulary, then multiple passes, with a default of 5, to perform vector training. so, you should expect to see about 6x your data size in disk-reads, just from the model training.
(if your machine winds up needing to use virtual-memory swapping during the process, there could be more disk activity ï¿½ï¿½ï¿½ but you absolutely do not want that to happen, as the random-access pattern of word2vec training is nearly a worst-case for virtual memory usage, which will slow training immensely.)
if you'd like to understand the code's progress, and be able to estimate its completion time, you should enable python logging to at least the info level. various steps of the process will report interim results (such as the discovered and surviving vocabulary size) and estimated progress. you can often tell if something is going wrong before the end of a run by studying the logging outputs for sensible values, and once the ing' phase has begun the completion time will be a simple projection from the training completed so far. 
i believe most laptops should throttle their own cpu if it's becoming so hot as to become unsafe or risk extreme wear on the cpu/components, but whether yours does, i can't say, and definitely make sure its fans work & vents are unobstructed. 
i'd suggest you choose some small random subset of your data ï¿½ï¿½ï¿½ maybe 1gb? ï¿½ï¿½ï¿½ to be able to run all your steps to completion, becoming familiar with the word2vec logging output, resource usage, and results, and tinkering with settings to observe changes, before trying to run on your full dataset, which might require days of training time. 
some of your shown parameters aren't optimal for speedy training. in particular:

min_count=1 retains every word seen in the corpus-survey, including those with only a single occurrence. this results in a much, much larger model - potentially riskihat doesn't fit into ram, forcing disastrous swapping. but also, words with just a few usage examples can't possibly get good word vectors, as the process requires seeing many subtly-varied alternate uses. still, via typical 'zipfian' word-frequencies, the number of such words with just a few uses may be very large in total, so retaining all those words takes a lot of training time/effort, and even serves a bit like 'noise' making the training of other words, with plenty of usage examples, less effective. so for model size, training speed, and quality of remaining vectors, a larger min_count is desirable. the default of min_count=5 is better for more projects than min_count=1 ï¿½ï¿½ï¿½ this is a parameter that should only really be changed if you're sure you know the effects. and, when you have plentiful data ï¿½ï¿½ï¿½ as with your 34gb ï¿½ï¿½ï¿½ the min_count can go much higher to keep the model size manageable. 
hs=1< be enabled if you want to use the 'hierarchical-softmax' training mode instead of 'negative-sampling' ï¿½ï¿½ï¿½ and in that case, negative=0 should also be set to disable 'negative-sampling'. you probably don't want to use hierarchical-softmax: it's not the default for a reason, and it doesn't scale as well to larger datasets. but here you've enabled in in addition to negative-sampling, likely more-than-doubling the required training time. 
did you choose negative=10 because you had problems with the default negative=5? because this non-default choice, again, would slow training noticeably. (but also, again, a non-default choice here would be more common with smaller datasets, while larger datasets like yours are more likely to experiment with a smaller negative value.)

the theme of the above observation""only change the defaults if you've already got something working, and you have a good theory (or way of testing) how that change might help"". 
with a large-enough dataset, there's another default parameter to consider changing to speed up training (& often improve word-vector quality, as well): sample, which controls how-aggressively highly-frequent words (with many redundant usage-examples) may be downsampled (randomly skipped). 
the default value, sample=0.001 (aka 1e-03), is very conservative. a smaller value, such as sample=1e-05, will discard many-more of the most-frequent-words' redundant usage examples, speeding overall training considerably. (and, for a corpus of your size, you could eventually experiment with even smaller, more-aggressive values.)
finally, to the extent all your data (for either a full run, or a subset run) can be in an already-space-delimited text file, you can use the corpus_file alternate method of specifying the corpus. then, the word2vec class will use an optimized multithreaded io approach to assign sections of the file to alternate worker threads ï¿½ï¿½ï¿½ which, if you weren't previously seeing full saturation of all threads/cpu-cores, could increase our throughput. (i'd put this off until after trying other things, then check if your best setup still leaves some of your 8 threads often idle.",https://stackoverflow.com/questions/60852962,python,25-03-2020 16:22,5916.0,7.0,1.0,True,29-03-2021 17:08,29-03-2021 17:08
77753658,langchain + local llama compatible model,"i'm trying to setup a local chatbot demo for testing purpose. i wanted to use langchain as the framework and llama as the model. tutorials i found all involve some registration, api key, huggingface, etc, which seems unnecessary for my purpose.
is there a way to use a local llama comaptible model file just for testing purpose? and also an example code to use the model with langchain would be appreciated. thanks!
update: i wrote a blog post based on the accepted answer.","['python', 'langchain', 'large-language-model', 'llama']",77754130,"no registration is required to utilize on-prem local models within ecosystems like hugging face (hf). similarly, using langchain does not involve any registration requirements. various model formats, such as gguf and ggml, are employed for storing models for inference and can be found on hf. it is crucial to consider these formats when attempting to load and run a model locally.
for instance, consider thebloke's llama-2-7b-chat-gguf model, which is a relatively compact 7-billion-parameter model suitable for execution on a modern cpu/gpu. to run the model, we can use llama.cpp from langchain:
def llamacpp():
    from langchain_community.llms import llamacpp
    from langchain.prompts import prompttemplate
    from langchain.chains import llmchain
    
    llm = llamacpp(
        model_path=""models/llama-2-7b-chat-gguf/llama-2-7b-chat.q4_0.gguf"",
        n_gpu_layers=40,
        n_batch=512,
        verbose=true,
    )
    
    template = """"""question: {question}

    answer: let's work this out in a step by step way to be sure we have the right answer.""""""

    prompt = prompttemplate(template=template, input_variables=[""question""])
    
    llm_chain = llmchain(prompt=prompt, llm=llm)
    question = ""who is bjarne stroustrup and how is he related to programming?""
    print(llm_chain.run(question))

and get output from the llm:


bjarne stroustrup is a danish computer scientist who created c++.
- he was born in aarhus, denmark on august 5, 1950 and earned his phd from cambridge university in 1983.
- in 1979 he began developing the programming language c++, which was initially called ""c with classes"".
- c++ was first released in 1983 and has since become one of the most popular programming languages in use today.

bjarne stroustrup is known for his work on the c programming
language and its extension to c++.
- he wrote the c programming language, a book that helped establish c as a widely used language.
- he also wrote the design and evolution of c++, a detailed explanation of how he created c++ and why he made certain design
choices.


...

in this instance, i cloned thebloke's model repository from hf and positioned it in a directory named models/. the final path for the model became models/llama-2-7b-chat-gguf/llama-2-7b-chat.q4_0.gguf:
# make sure you have git-lfs installed (
git lfs install
git clone 

although the model can be run on a cpu, this was locally run on my windows pc equipped with an rtx 4070 card with good performance during inference.",https://stackoverflow.com/questions/77753658,python,03-01-2024 17:30,3445.0,3.0,1.0,True,04-01-2024 16:50,04-01-2024 16:50
53559086,"remove tags (\r, \n, &lt;, &gt;) from string in json-file","i know similar questions have been asked before but so far i wasnt able to solve my problem, so apologies in advance.
i have a json-file ('test.json') with text in it. the text appears like this:
""... >>\r\n>> this is a test.>\r\n> \r\n-- \r\nmit freundlichen gr&uuml;ssen\r\n\r\nmike klence ...""

the overal output should be the plain text:
""... this is a test. mit freundlichen grï¿½ï¿½ssen mike klence""

with beautifulsoup i got to remove those html tags. but still those >, \r, \n- - remain in the text. so i tried the following code:
import codecs
from bs4 import beautifulsoup

with codecs.open('test.json', encoding = 'utf-8') as f:
    soup = beautifulsoup(f, 'lxml')
    invalid_tags = ['\r', '\n', '<', '>']
    for tag in invalid_tags: 
        for match in soup.find_all(tag):
            match.replace_with()

print(soup.get_text())

but it doesnt do anything with the text in the file. i tried different variations but nothing seems to change at all.
how can i get my code to work properly?
or if there is another, easier or faster way, i would be thankful to read about those approaches as well.
btw i am using python 3.6 on anaconda.
thank you very much in advance for your help.","['python', 'html', 'beautifulsoup', 'nlp']",53559923,"you could do this using python built-in function replace().
with open('test.json', 'r', encoding = 'utf-8') as f:
    content = f.read()
    invalid_tags = ['\\r', '\\n', '<', '>', '-', ';']
    for invalid_tag in invalid_tags:
        content = content.replace(invalid_tag, '')
    content = content.replace('&u', 'ï¿½ï¿½')

print(content)

output:
...  this is a test.  mit freundlichen grï¿½ï¿½umlssenmike klence ...
</code",https://stackoverflow.com/questions/53559086,python,30-11-2018 14:06,1354.0,0.0,2.0,True,29-12-2022 07:18,30-11-2018 14:12
78485347,encode a list of sentences into embeddings using a huggingface model not in its hub,"i am trying to encode a list of sentences into a list of embeddings. when i use a model that is in the huggingface hub, it works as expected. but when i use a model not in the hub, in this case facebook's m2m100 model, i do not get the expected results.
when using a model within sentencetransformer(), my results look like this:
from sentence_transformers import sentencetransformer
dat = ['meteorite fell on the road ', 'i went in the wrong direction']
model_1 = sentencetransformer('all-distilroberta-v1')
embeddings_1 = model_1.encode(dat)
embeddings_1.shape
> (2, 768)

however, when i use the m2m100 model, my results do not look right at all, specifically i would expect 2 rows of results:
from transformers import m2m100tokenizer
model_m2m = m2m100tokenizer.from_pretrained(""facebook/m2m100_418m"")
model_m2m.src_lang = ""en""
embeddings_m2m = model_m2m.encode(dat, return_tensors=""pt"")
embeddings_m2m.shape
> torch.size([1, 4])

how should i format this so that it returns an n-dimensional list of embeddings, where each row corresponds to a sentence and the number of columns is equal to the dimensionality of the embedding?
(as a note, eventually i will be doing this for sentences in other languages, which is why i'm using a multi-lingual model.)","['nlp', 'huggingface-transformers', 'encode', 'embedding', 'huggingface']",78502336,"the code you provided only uses the tokenizer of the model, which maps the text to integer ids that don't represent any kind of (semantical) meaning.
to retrieve sentence embeddings (i.e. a vector that represents the text) from
facebook/m2m100_418m, which is an encoder-decoder model, you need to perform some kind of pooling over the last-hidden-state of the encoder. common approaches, which are cls and mean pooling are shown in the example below:
import torch
from transformers import m2m100tokenizer, m2m100model

def mean_pooling(last_hidden_state, attention_mask):
  non_pad_tokens = attention_mask.sum(1)
  sum_embeddings =  torch.sum(attention_mask.unsqueeze(-1) * last_hidden_state, 1)
  return sum_embeddings/non_pad_tokens.unsqueeze(-1)

def cls_pooling(last_hidden_state):
  return last_hidden_state[:,0]

dat = ['meteorite fell on the road ', 'i went in the wrong direction']


model_id = ""facebook/m2m100_418m""
t_m2m = m2m100tokenizer.from_pretrained(model_id)
t_m2m.src_lang = ""en""
m_m2m = m2m100model.from_pretrained(model_id)

tokenized = t_m2m(dat, padding=true, return_tensors='pt')

with torch.inference_mode():
  encoder_o = m_m2m.encoder(**tokenized)

encoder_last_hidden_state = encoder_o.last_hidden_state
print(encoder_last_hidden_state.shape)
mean_pooling_embeddings = mean_pooling(encoder_last_hidden_state, tokenized.attention_mask)
print(mean_pooling_embeddings.shape)
cls_pooling_embeddings = cls_pooling(encoder_last_hidden_state)
print(cls_pooling_embeddings.shape)

output:
torch.size([2, 9, 1024])
torch.size([2, 1024])
torch.size([2, 1024])

which of the two approaches works better for your downstream task, must be tested with your data. please also note that even when you have the sentence embeddings now, it doesn't mean they are semantically meaningful (i.e. the embeddings are useless for your downstream task). refer to this stackoverflow answer for further explanation.",https://stackoverflow.com/questions/78485347,nlp,15-05-2024 16:39,772.0,1.0,1.0,True,19-05-2024 10:57,15-05-2024 18:27
70455234,spacy extract entity relationships parse dep tree,"i am trying to extract entities and their relationships from the text. i am attempting to parse the dependency tree with entity extraction to perform that action. something must be wrong with the recursive function logic that is preventing me from being able to parse that information, but i am not seeing what it is. i wanted to use the dependency tree + entities to form a (person,action,location) extraction.
desired output: person: lou pinella, action:exited, loc:stadium
code example:
import spacy
from spacy import displacy

nlp = spacy.load('en_core_web_lg')
doc = nlp(""lou pinella exited from the far left side of the stadium."")

def get_children_ent(head):
    if head.children:
        for child in head.children:
            if child.ent_type_ == ""loc"":
                print(f'loc found: {child}') # it is hitting this branch
                return child
            else:
                return get_children_ent(child)
    else:
        return ""no children""

for ent in doc.ents:
    print(ent.text, ent.label_)
    if ent.label_ == ""person"":
        person = ent.text
        head = ent.root.head
        loc = get_children_ent(head)
        print(f'person: {person}')
        print(f'head: {head}')
        print(f'person: {person}, action:{head}, loc:{loc}')
    
   

displacy.render(doc, options={""fine_grained"": true})

print statements - you can see it is hitting the location logic and printing that, but the return is still none in the recursive function.
lou pinella person
loc found: stadium
person: lou pinella
head: exited
person: lou pinella, action:exited, loc:none
stadium loc

edited: added return get_child_ent(child) to the else.","['python', 'recursion', 'nlp', 'spacy']",70457551,"your recursive call isn't returning a value. you need this:
            else:
                return get_children_ent(child)",https://stackoverflow.com/questions/70455234,python,22-12-2021 21:21,1084.0,0.0,1.0,True,23-12-2021 16:06,23-12-2021 16:06
70981648,python nltk - tokenize sentences into words while removing numbers,"hoping someone can assist with this! i have a list of sentences which is read from a text file. i am trying to tokenize the sentences into words, while also removing sentences while contain only numbers. there is no pattern for when the numbers will appear.
the sentences i have:
[
  ['                    1'], 
  ['this is a text file,'], 
  ['to keep the words,'],
  ['                    2'],
  ['another line of the text:'],
  ['                    3']
]

desired output:
[
  ['this', 'is', 'a', 'text', 'file,'], 
  ['to', 'keep', 'the', 'words,'],
  ['another', 'line', 'of', 'the', 'text:'],
]","['python', 'nlp', 'nltk']",70981736,"after some pre processing, now you can apply tokenizing
import re

a = [
    ['                    1'],
    ['this is a text file,'],
    ['to keep the words,'],
    ['                    2'],
    ['another line of the text:'],
    ['                    3']
]


def replace_digit(string):
    return re.sub(r'\d', '', string).strip()


data = []
process = [replace_digit(i[0]) for i in a]
filtered = filter(lambda x: x, process)
tokenize = map(lambda x: x.split(), filtered)
print(list(tokenize))",https://stackoverflow.com/questions/70981648,python,04-02-2022 05:06,495.0,0.0,1.0,True,04-02-2022 05:27,04-02-2022 05:12
77382923,"laptop stopped when training a pytorch lstm model, while tensorflow counterpart works","i have the following nn module in pytorch
class model(nn.module):
    def __init__(self):
        super(model, self).__init__()
        self.emb = nn.embedding(num_embeddings=10000, embedding_dim=512)
        self.drop1 = nn.dropout(p=0.25)
        self.lstm = nn.lstm(input_size=512, hidden_size=32, num_layers=1)
        self.drop2 = nn.dropout(p=0.25)
        self.dense = nn.linear(32, 1)
        self.activ = nn.sigmoid()

    def forward(self, x):
        t1 = self.emb(x)
        t2 = self.drop1(t1)
        outputs, (hidden, cell) = self.lstm(t2)
        t4 = self.drop2(outputs[:,-1,:])
        t5 = self.dense(t4)
        return self.activ(t5)

the training code is the following:
model = model()
criterion = nn.bceloss()
optimizer = torch.optim.adam(model.parameters())

for epoch in range(3):
    outputs = model(torch.from_numpy(x_train))
    loss = criterion(torch.flatten(outputs).to(torch.float32), torch.flatten(torch.from_numpy(y_train)).to(torch.float32))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

the code works fine when i drastically lower the dimensions in the different network layer (2 or 4 instead of 512 and 32 and so on). i did that to debug my implementation and make sure it works.
however with the given parameters in the code i provided, my laptop stops (the mouse doesn't move anymore, nothing works, i had to unplug the laptop and restart it). same thing when executing on google colab, there is an error and the session resets.
i added prints everywhere, the code seemingly stops at outputs = model(torch.from_numpy(x_train)). i didn't check which step of the forward pass though.
what is surprising is that the exact same module coded using tensorflow.keras works fine both on my laptop and on google colab.
what am i missing here? thanks a lot!
i expect the training to work correctly.
data download and processing
import tensorflow as tf
import numpy as np
data = tf.keras.datasets.imdb.load_data(num_words=10000)
train, test = data[0], data[1]
x_train, y_train = train[0], train[1]
x_test, y_test = test[0], test[1]

review_length = 500
from tensorflow.keras.preprocessing import sequence
x_train = sequence.pad_sequences(x_train, maxlen = review_length)
x_test = sequence.pad_sequences(x_test, maxlen = review_length)

the same model in tensorflow keras that works fine
from tensorflow.keras.models import sequential

model = sequential()
model.add(tf.keras.layers.embedding(input_dim=10000, output_dim=512, input_length=500))
model.add(tf.keras.layers.dropout(rate=0.25))
model.add(tf.keras.layers.lstm(units=32))
model.add(tf.keras.layers.dropout(rate=0.25))
model.add(tf.keras.layers.dense(units=1, activation='sigmoid'))

model.compile(optimizer=tf.keras.optimizers.adam(), loss=""binary_crossentropy"", metrics=[""accuracy""])

model.fit(np.asarray(x_train), y_train, epochs=3, batch_size=256, validation_split=0.2)","['python', 'deep-learning', 'pytorch', 'nlp']",77384658,"the problem is passing the whole training set to the model in a single call, the simple solution to this is to use batching, and it is why the keras model works. as model.fit applies batching automatically, while in pytorch batching has to be manually implemented.",https://stackoverflow.com/questions/77382923,python,29-10-2023 10:41,76.0,1.0,1.0,True,29-10-2023 18:29,29-10-2023 14:18
76191862,how can i fine-tune mbart-50 for machine translation in the transformers python library so that it learns a new word?,"i try to fine-tune mbart-50 (paper, pre-trained model on hugging face) for machine translation in the transformers python library. to test the fine-tuning, i am trying to simply teach mbart-50 a new word that i made up.
i use the following code. over 95% of the code is from the hugging face documentation:
from transformers import mbartforconditionalgeneration, mbart50tokenizerfast

print('model loading started')
model = mbartforconditionalgeneration.from_pretrained(""facebook/mbart-large-50"")
tokenizer = mbart50tokenizerfast.from_pretrained(""facebook/mbart-large-50"", src_lang=""fr_xx"", tgt_lang=""en_xx"")
print('model loading done')

src_text = "" billozarion ""
tgt_text =  "" plorization ""

model_inputs = tokenizer(src_text, return_tensors=""pt"")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(tgt_text, return_tensors=""pt"").input_ids

print('fine-tuning started')
for i in range(1000):
    #pass
    model(**model_inputs, labels=labels) # forward pass
print('fine-tuning ended')
    
# testing whether the model learned the new word. translate french to english
tokenizer = mbart50tokenizerfast.from_pretrained(""facebook/mbart-large-50-many-to-many-mmt"")
tokenizer.src_lang = ""fr_xx""
article_fr = src_text
encoded_fr = tokenizer(article_fr, return_tensors=""pt"")
generated_tokens = model.generate(**encoded_fr, forced_bos_token_id=tokenizer.lang_code_to_id[""en_xx""])
translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=true)
print(translation)

however, the new word wasn't learned. the output is ""billozarion"" instead of ""plorization"". why?
i'm strictly following the hugging face documentation, unless i missed something.  the # forward pass does make me concerned, as one would need a backward pass to update the gradients. maybe this means that the documentation is incorrect, however i can't test that hypothesis as i don't know how to add the backward pass.

environment that i used to run the code: ubuntu 20.04.5 lts with an nvidia a100 40gb gpu (i also tested with an nvidia t4 tensor core gpu) and cuda 12.0 with the following conda environment:
conda create --name mbart-python39 python=3.9
conda activate mbart-python39 
pip install transformers==4.28.1
pip install chardet==5.1.0
pip install sentencepiece==0.1.99
pip install protobuf==3.20","['python', 'huggingface-transformers', 'pre-trained-model', 'machine-translation', 'fine-tuning']",76197333,"one could add the following to fine-tune mbart-50:
from transformers.optimization import adamw

# set up the optimizer and training settings
optimizer = adamw(model.parameters(), lr=1e-4)
model.train()

print('fine-tuning started')
for i in range(100):
    optimizer.zero_grad()
    output = model(**model_inputs, labels=labels) # forward pass
    loss = output.loss
    loss.backward()
    optimizer.step()
print('fine-tuning ended')

full code:
from transformers import mbartforconditionalgeneration, mbart50tokenizerfast
from transformers.optimization import adamw
import os
os.environ[""tokenizers_parallelism""] = ""false""


print('model loading started')
model = mbartforconditionalgeneration.from_pretrained(""facebook/mbart-large-50"")
tokenizer = mbart50tokenizerfast.from_pretrained(""facebook/mbart-large-50"", src_lang=""fr_xx"", tgt_lang=""en_xx"")
print('model loading done')

src_text = "" billozarion ""
tgt_text =  "" plorizatizzzon ""

model_inputs = tokenizer(src_text, return_tensors=""pt"")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(tgt_text, return_tensors=""pt"").input_ids

# set up the optimizer and training settings
optimizer = adamw(model.parameters(), lr=1e-4)
model.train()

print('fine-tuning started')
for i in range(100):
    optimizer.zero_grad()
    output = model(**model_inputs, labels=labels) # forward pass
    loss = output.loss
    loss.backward()
    optimizer.step()
print('fine-tuning ended')
    
# translate french to english
tokenizer = mbart50tokenizerfast.from_pretrained(""facebook/mbart-large-50-many-to-many-mmt"")
tokenizer.src_lang = ""fr_xx""
article_fr = src_text
encoded_fr = tokenizer(article_fr, return_tensors=""pt"")
generated_tokens = model.generate(**encoded_fr, forced_bos_token_id=tokenizer.lang_code_to_id[""en_xx""])
translation =tokenizer.batch_decode(generated_tokens, skip_special_tokens=true)
print(translation)

it outputs the correct made up translation ""plorizatizzzon"".
i reported the documentation issue on 

 contains two more advanced scripts to fine-tune mbart and t5 (thanks sgugger for pointing me to it). here is how to use the script to fine-tune mbart:
create a new conda environment:
conda create --name mbart-source-transformers-python39 python=3.9
conda activate mbart-source-transformers-python39 
git clone 
cd transformers
pip install git+
pip install datasets evaluate accelerate sacrebleu
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
pip install sentencepiece==0.1.99
pip install protobuf==3.20
pip install --force-reinstall charset-normalizer==3.1.0

command:
python examples/pytorch/translation/run_translation.py \
    --model_name_or_path facebook/mbart-large-50 \
    --do_train \
    --do_eval \
    --source_lang fr_xx \
    --target_lang en_xx \
    --source_prefix ""translate french to english: "" \
    --train_file finetuning-translation-train.json \
    --validation_file finetuning-translation-validation.json  \
    --test_file finetuning-translation-test.json \
    --output_dir tmp/tst-translation4 \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --do_predict \
    --predict_with_generate

(note: the readme seems to have missed --do_predict)
with finetuning-translation-train.json, finetuning-translation-validation.json and finetuning-translation-test.json formatted as follows with the json lines format:
{""translation"": {""en"": ""20 year-old male tennis player."", ""fr"": ""joueur de tennis de 12 ans""}}
{""translation"": {""en"": ""2 soldiers in an old military jeep"", ""fr"": ""2 soldats dans une vielle jeep militaire""}}

(note: one must use double quotes in the .json files. single quotes e.g. 'en' will make the script crash.)
i run the code on ubuntu 20.04.5 lts with an nvidia t4 tensor core gpu (16gb memory) and cuda 12.0. the mbart-50 model takes around 15gb of gpu memory.",https://stackoverflow.com/questions/76191862,python,07-05-2023 01:29,3543.0,2.0,1.0,True,23-02-2025 10:34,07-05-2023 01:55
78546836,azure ai search fields mapping json and retrievable fields,"i'm currently implementing rag on azure using openai and azure ai search, formerly known as cognitive services. i have around 50-65 json files that i need to search on my enterprise data. it turns out that in the referencing of the chatbot, i'm only getting the text ""citation"" and i'm trying to retrieve the doi, which is the url to the document online, and the title of the scientific article. this files are saved as .txt.
i have formatted my json file in this manner where the keys 'content' and 'title' are the only ones i want to perform a semantic search on and also make retrievable, while i just want the doi (url) to be retrievable.
{
  ""content"": ""the human eye is a complex organ responsible for vision, capturing light and converting it into neural signals for the brain to interpret. it consists of multiple parts, including the cornea, lens, and retina, each playing a vital role in the process of seeing."",
  ""date"": ""2023-07-15"",
  ""title"": ""the magic of vision"",
  ""editorial_house"": ""mit research meds and public health"",
  ""doi"": ""
  ""author"": ""dr. john mayer""
}

nonetheless when i'm on the azure ai search page i never get my other fields to be selected in metadata:

as you can see, only 'content' appears and i still get this unappealing citation in the foot references of my searches. how can i make my data retrievable in the way i want?
as i'm not using code to do this, only the azure studio web, i'm not sure if the only way to do that is by using code.
my desired output is something like this:

is this possible? is it possible using the azure studio or just doing code?
update
i'm setting up the custom mappings like this:

nonetheless while i'm getting the correct title and content of the citations panel i'm missing the doi which is the url of the publication. is there something i'm doing wrong??","['azure', 'openai-api', 'langchain', 'azure-ai-search']",78548479,"import data and with index definition both way you can do.
in portal, after you clicking on import data you will get an option connect to your data there you need to configure parsing mode as json.

then you will get the correct fields.

here you can remove whichever field you don't want.
another method is create index with custom definition like below.
[
{
      ""name"": ""content"",
      ""type"": ""edm.string"",
      ""searchable"": true,
      ""filterable"": false,
      ""retrievable"": true,
      ""stored"": true,
      ""sortable"": false,
      ""facetable"": false,
      ""key"": false,
      ""indexanalyzer"": null,
      ""searchanalyzer"": null,
      ""analyzer"": ""standard.lucene"",
      ""normalizer"": null,
      ""dimensions"": null,
      ""vectorsearchprofile"": null,
      ""vectorencoding"": null,
      ""synonymmaps"": []
    },
    {
      ""name"": ""date"",
      ""type"": ""edm.datetimeoffset"",
      ""searchable"": false,
      ""filterable"": false,
      ""retrievable"": true,
      ""stored"": true,
      ""sortable"": false,
      ""facetable"": false,
      ""key"": false,
      ""indexanalyzer"": null,
      ""searchanalyzer"": null,
      ""analyzer"": null,
      ""normalizer"": null,
      ""dimensions"": null,
      ""vectorsearchprofile"": null,
      ""vectorencoding"": null,
      ""synonymmaps"": []
    },
    {
      ""name"": ""title"",
      ""type"": ""edm.string"",
      ""searchable"": true,
      ""filterable"": false,
      ""retrievable"": true,
      ""stored"": true,
      ""sortable"": false,
      ""facetable"": false,
      ""key"": false,
      ""indexanalyzer"": null,
      ""searchanalyzer"": null,
      ""analyzer"": ""standard.lucene"",
      ""normalizer"": null,
      ""dimensions"": null,
      ""vectorsearchprofile"": null,
      ""vectorencoding"": null,
      ""synonymmaps"": []
    },
    {
      ""name"": ""editorial_house"",
      ""type"": ""edm.string"",
      ""searchable"": true,
      ""filterable"": false,
      ""retrievable"": true,
      ""stored"": true,
      ""sortable"": false,
      ""facetable"": false,
      ""key"": false,
      ""indexanalyzer"": null,
      ""searchanalyzer"": null,
      ""analyzer"": ""standard.lucene"",
      ""normalizer"": null,
      ""dimensions"": null,
      ""vectorsearchprofile"": null,
      ""vectorencoding"": null,
      ""synonymmaps"": []
    },
    {
      ""name"": ""doi"",
      ""type"": ""edm.string"",
      ""searchable"": true,
      ""filterable"": false,
      ""retrievable"": true,
      ""stored"": true,
      ""sortable"": false,
      ""facetable"": false,
      ""key"": false,
      ""indexanalyzer"": null,
      ""searchanalyzer"": null,
      ""analyzer"": ""standard.lucene"",
      ""normalizer"": null,
      ""dimensions"": null,
      ""vectorsearchprofile"": null,
      ""vectorencoding"": null,
      ""synonymmaps"": []
    },
    {
      ""name"": ""author"",
      ""type"": ""edm.string"",
      ""searchable"": true,
      ""filterable"": false,
      ""retrievable"": true,
      ""stored"": true,
      ""sortable"": false,
      ""facetable"": false,
      ""key"": false,
      ""indexanalyzer"": null,
      ""searchanalyzer"": null,
      ""analyzer"": ""standard.lucene"",
      ""normalizer"": null,
      ""dimensions"": null,
      ""vectorsearchprofile"": null,
      ""vectorencoding"": null,
      ""synonymmaps"": []
    },
    {
      ""name"": ""metadata_storage_size"",
      ""type"": ""edm.int64"",
      ""searchable"": false,
      ""filterable"": false,
      ""retrievable"": true,
      ""stored"": true,
      ""sortable"": false,
      ""facetable"": false,
      ""key"": false,
      ""indexanalyzer"": null,
      ""searchanalyzer"": null,
      ""analyzer"": null,
      ""normalizer"": null,
      ""dimensions"": null,
      ""vectorsearchprofile"": null,
      ""vectorencoding"": null,
      ""synonymmaps"": []
    },
    {
      ""name"": ""metadata_storage_path"",
      ""type"": ""edm.string"",
      ""searchable"": true,
      ""filterable"": false,
      ""retrievable"": true,
      ""stored"": true,
      ""sortable"": false,
      ""facetable"": false,
      ""key"": true,
      ""indexanalyzer"": null,
      ""searchanalyzer"": null,
      ""analyzer"": ""standard.lucene"",
      ""normalizer"": null,
      ""dimensions"": null,
      ""vectorsearchprofile"": null,
      ""vectorencoding"": null,
      ""synonymmaps"": []
    }
  ]

next configure indexer like below.

after saving reset and run the indexer.",https://stackoverflow.com/questions/78546836,azure,28-05-2024 23:57,1324.0,1.0,1.0,True,07-06-2024 19:52,07-06-2024 19:52
78826508,openai api error when fine-tuning: &quot;this is not a chat model and thus not supported in the v1/chat/completions endpoint&quot;,"i've fine-tuned a model using the openai api. now i want to use this fine-tuned model.
this is my code:
 const response = await openai.chat.completions.create({
    model: process.env.fine_tune_model,
    messages: [
      {
        role: ""system"",
        content: prompt,
      },
      {
        role: ""user"",
        content: inputtext,
      },
    ],
    temperature: 0,
    top_p: 1,
    frequency_penalty: 0,
    presence_penalty: 0,
});

but there's aï¿½ï¿½404 error while executing this function.
this is the error message:

notfounderror: 404 this is not a chat model and thus not supported in
the v1/chat/completions endpoint. did you mean to  use v1/completions?

how can i use the fine-tuned model to generate a text?<","['node.js', 'openai-api', 'fine-tuning']",78833817,"problem
you fine-tuned one of the non-chat models but wanted to use the api endpoint, which is compatible only with the chat models.
the api endpoint you need to use for a fine-tuned model depends on the model you fine-tune.
as of today, you can fine-tune the following models:

gpt-4o-mini-2024-07-18 (chat model),
gpt-4o-2024-05-13 (chat model),
gpt-4-0613 (chat model),
gpt-3.5-turbo-0125 (chat model),
gpt-3.5-turbo-1106 (chat model),
gpt-3.5-turbo-0613 (chat model),
babbage-002 (non-chat model), and
davinci-002 (non-chat model).

solution
use the following rule:

if you fine-tune a chat model, use the /v1/chat/completions api endpoint.
if you fine-tune a non-chat model, use the /v1/completions api endpoint.

in other words:

if you fine-tune a chat model, use the client.chat.completions.create method.
if you fine-tune a non-chat model, use the client.completions.create method.",https://stackoverflow.com/questions/78826508,node.js,02-08-2024 16:52,176.0,0.0,1.0,True,21-09-2024 15:47,21-09-2024 15:47
77054415,problem instantiating and using gpt4allembeddings,"update:
after i'd posted this question i found this issue already was raised on github: 
i can either delete this question, or can anyone suggest a workaround? maybe an alternative way to generate embeddings? thanks!
i have been trying to build my first application using langchain, chroma and a local llm (ollama in my case). i've been following the (very straightforward) steps from:

and also tried 
the problem i'm having is with the step creating embeddings using the gpt4allembeddings model. i can see it is downloaded to ~/.cache/gpt4all/ggml-all-minilm-l6-v2-f16.bin although it's size is 45.5 mb which is surprisingly small. but when i try to use it, it fails with this error:
>>> gpt4all_embd = gpt4allembeddings()
100%|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½| 45.5m/45.5m [00:05<00:00, 7.66mib/s]
model downloaded at:  /<my-home-path>/.cache/gpt4all/ggml-all-minilm-l6-v2-f16.bin
invalid model file
traceback (most recent call last):
  file &que_error)

i have tried the same steps on different machines and i'm still getting the same error. googling didn't help.","['langchain', 'large-language-model']",77055191,"please follow below steps.
in your activated virtual environment
 pip install -u langchain

 pip install gpt4all

sample code
       from langchain.embeddings import gpt4allembeddings

        gpt4all_embd = gpt4allembeddings()
        query_result = gpt4all_embd.embed_query(""this is test doc"")
        print(query_result)

other option for embeddings through huggingface
pip install langchain sentence_transformers

sample code
from langchain.embeddings import huggingfaceembeddings

embeddings = huggingfaceembeddings()
text = ""this is a test document.""
query_result = embeddings.embed_query(text)
print(query_result [:3])",https://stackoverflow.com/questions/77054415,langchain,06-09-2023 18:25,4424.0,-1.0,1.0,True,22-03-2024 23:47,07-09-2023 08:30
74122881,how to convert raw lines to df,"i need to read a df from a pdf file and here is an example table

so far i was able to read the data as raw lines with the following chunk
library(pdftools)
library(tidyverse)

pdf_file <- pdf_text(""exm.pdf"")

raw_df <- pdf_file %>%
  read_lines() %>%
  data.frame() %>% 
  rename(rawline = 1)

raw_df <- raw_df %>% 
  mutate(
    rawline = str_replace(string = rawline,
                          pattern = ""^ \\s*"",
                          replacement = """")
    )

here is the structure of raw df
> raw_df
                                     rawline
1      id   name    address           mobile
2 1    kiran   bengaluru,        99999 99999
3                                mysore road
4                                   6th lane
5 2    john    mandya            77777 77777
6                            taluka junction
7 3    ravi    mysore            88888 88888

how can i convert this into a proper df? i tried filtering out the lines that start with a digit by using regex but after that i got stuck. i need to gather address lines (that have no number at the beginning) and attach them to the previous address text and then split lines into columns. i tried splitting based the space between id, name, address and mobile but it is not constant across all lines. how can i resolve this issue? thanks in advance.
edit
as suggested, i tried pdf_data and i got a table (head(15)) like this with x and y positions of the text
# a tibble: 15 x 6
   width height     x     y space text      
   <int>  <int> <int> <int> <lgl> <chr>     
 1     8     11    77    74 false id        
 2     5     11    77    88 false 1         
 3    26     11   181    74 false name      
 4    23     11   181    88 false kiran     
 5     5     11    77   129 false 2         
 6    20     11   181   129 false john      
 7     5     11    77   156 false 3         
 8    18     11   181   156 false ravi      
 9    35     11   294    74 false address   
10    48     11   294    88 false bengaluru,
11    33     11   294   102 true  mysore    
12    22     11   330   102 false road      
13     5     11   294   115 false 6         
14     5      6   299   114 true  th        
15    21     11   308   115 false lane

based on this table i can filter out the x values and get the columns as vectors. but if there are spaces in the values( like address) this filtering will not work. is there a way to gather address column based on x and y values?
basically i need to gather rows based on a value (ex: x == 294) until the same value appears then i can use str_c to merge those cells to a single string.","['r', 'pdf', 'text-mining', 'pdftools']",74126114,"based on your first method, try this function after getting row_df :
library(dplyr)
parse_pdfs_lines_byid<- function(raw_df){
 
# ----delete rownames : the first character and space
raw_df=raw_df%>%
 mutate(rawline=sub('.', '', rawline))%>%
 # ----remove the first space to keep id as a first word
 mutate(rawline=gsub('^ ', '', rawline))  

# ------ now ignore the raw of colnames
raw_df=data.frame(rawline=raw_df[-1,])


# ---------assign  the correct id to  correct line 
# id=""""
# initialize index of line
i=1
while (i<nrow(raw_df))
{
 if(grepl(""^[0-9]"",raw_df$rawline[i]))
 {
   # get the id , first word of line ./!\ not the first character! e.g : id == 22 )
   id=stringr::word(raw_df$rawline[i],1)
 }else{ 
   raw_df$rawline[i]=paste0(id,raw_df$rawline[i])
 }   
 i=i+1
}
# > raw_df
#                                    rawline
# 1    kiran   bengaluru,        99999 99999
# 1                               mysore road
# 1                                  6th lane
# 2    john    mandya            77777 77777
# 2                           taluka junction
# 3    ravi    mysore            88888 88888



# ------build the dataframe

col_df= list(""id"",""name"", ""address"", ""mobile"")
raw_df2 =setnames(data.frame(matrix(ncol = 4, nrow = 0),stringsasfactors = f),col_df)

for (j in 1:nrow(raw_df))
{
 # split the line of dataframe by  double space or more
 line= unlist(strsplit(raw_df$rawline[j],""   +""))
 df_line= data.frame(t(line),stringsasfactors = f)
 # if all 4 column exist , affect column names else these is just id and part2 of adress ==>column adress2
 names(df_line) = unlist(ifelse(length(line)==4,
                                list(col_df),
                                list(c(""id"",""adress2"")))
 )
 # rbind even the number of column is not the same
 raw_df2=plyr::rbind.fill(raw_df2,df_line )
}

# ----- clean final dataframe

final_df = raw_df2%>%
 # replace na with emty value
 mutate_all(~ifelse(is.na(.), """", .))%>%
 group_by(id)%>%
 mutate(address= paste(address,adress2,collapse = "" ""))%>% #put collapse =""\r\n"" to display the exact format
 # keep just the first line by id 
 slice(1)%>%
 # remove adress2 column 
 select(-adress2)%>%
 ungroup()
return(final_df)
}

apply function on your first example and the result is :
raw_df  = data.frame(rawline=
                       c(""1      id   name    address           mobile"",
                         ""2 1    kiran   bengaluru,        99999 99999"",
                         ""3                                mysore road"",
                         ""4                                   6th lane"",
                         ""5 2    john    mandya            77777 77777"",
                         ""6                            taluka junction"",
                         ""7 3    ravi    mysore            88888 88888"")
)
final_df=parse_pdfs_lines_byid(raw_df)
final_df
# final_df
# a tibble: 3 x 4
# id    name  address                              mobile     
# <chr> <chr> <chr>                                <chr>      
# 1     kiran ""bengaluru,   mysore road  6th lane"" 99999 99999
# 2     john  ""mandya   taluka junction""           77777 77777
# 3     ravi  ""mysore ""                            88888 88888

hope this will help!, please let me know if something does not work or is not clear enough.(update response format).",https://stackoverflow.com/questions/74122881,r,19-10-2022 09:20,367.0,1.0,1.0,True,19-10-2022 19:19,19-10-2022 10:23
46934523,how to get spacy ner probability,"i want to combine spacy's ner engine with a separate ner engine (a bow model). i'm currently comparing outputs from the two engines, trying to figure out what the optimal combination of the two would be. both perform decently, but quite often spacy finds entities that the bow engine misses, and vice versa. what i would like is to access a probability score (or something similar) from spacy whenever it finds an entity that is not found by the bow engine. can i get spacy to print out its own probability score for a given entity it has found? as in, ""hi, i'm spacy. i've found this token (or combination of tokens) that i'm x% certain is an entity of type blah."" i want to know that number x every time spacy finds an entity. i imagine there must be such a number somewhere internally in spacy's ner engine, plus a threshold value below which the possible entity is not flagged as an entity, and i'd like to know how to get my hands on that number. thanks in advance.","['named-entity-recognition', 'spacy']",52691414,"actually, there is an issue for that.
the author of the library, suggests there (among others) the following solution:


beam search with global objective.
  this is the standard solution: use a global objective, so that the parser model is trained to prefer parses that are better overall. keep n different candidates, and output the best one. this can be used to support confidence by looking at the alternate analyses in the beam. if an entity occurs in every analysis, the ner is more confident it's correct.


code:
import spacy
import sys
from collections import defaultdict

nlp = spacy.load('en')
text = u'will japan join the european union? if yes, we should \ 
move to united states. fasten your belts, america we are coming'


with nlp.disable_pipes('ner'):
    doc = nlp(text)

threshold = 0.2
(beams, somethingelse) = nlp.entity.beam_parse([ doc ], beam_width = 16, beam_density = 0.0001)

entity_scores = defaultdict(float)
for beam in beams:
    for score, ents in nlp.entity.moves.get_beam_parses(beam):
        for start, end, label in ents:
            entity_scores[(start, end, label)] += score

print ('entities and scores (detected with beam search)')
for key in entity_scores:
    start, end, label = key
    score = entity_scores[key]
    if ( score > threshold):
        print ('label: {}, text: {}, score: {}'.format(label, doc[start:end], score))

sample output: 

entities and scores (detected with beam search) 
label: gpe, text: japan, score: 0.9999999999999997 
label: gpe, text: america, score: 0.9991664575947963 

important note: the outputs you will get here are probably different from the outputs you would get using the standard ner and not the beam search alternative. however, the beam search alternative provides you a metric of confidence that as i understand from your question is useful for your case.
outputs with standard ner for this example:

label: gpe, text: japan
label: org, text: the european union 
label: gpe, text: united states 
label: gpe, text: america",https://stackoverflow.com/questions/46934523,named-entity-recognition,25-10-2017 14:02,9467.0,21.0,2.0,True,25-07-2022 05:58,13-07-2021 06:38
62357239,add attention layer to seq2seq model,"i have build a seq2seq model of encoder-decoder. i want to add an attention layer to it. i tried adding attention layer through this but it didn't help.
here is my initial code without attention
# encoder
encoder_inputs = input(shape=(none,))
enc_emb =  embedding(num_encoder_tokens, latent_dim, mask_zero = true)(encoder_inputs)
encoder_lstm = lstm(latent_dim, return_state=true)
encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)
# we discard `encoder_outputs` and only keep the states.
encoder_states = [state_h, state_c]

# set up the decoder, using `encoder_states` as initial state.
decoder_inputs = input(shape=(none,))
dec_emb_layer = embedding(num_decoder_tokens, latent_dim, mask_zero = true)
dec_emb = dec_emb_layer(decoder_inputs)
# we set up our decoder to return full output sequences,
# and to return internal states as well. we don't use the
# return states in the training model, but we will use them in inference.
decoder_lstm = lstm(latent_dim, return_sequences=true, return_state=true)
decoder_outputs, _, _ = decoder_lstm(dec_emb,
                                     initial_state=encoder_states)
decoder_dense = dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# define the model that will turn
# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
model = model([encoder_inputs, decoder_inputs], decoder_outputs)
model.summary()

and this is the code after i added attention layer in decoder (the encoder layer is same as in initial code)
# set up the decoder, using `encoder_states` as initial state.
decoder_inputs = input(shape=(none,))
dec_emb_layer = embedding(num_decoder_tokens, latent_dim, mask_zero = true)
dec_emb = dec_emb_layer(decoder_inputs)
# we set up our decoder to return full output sequences,
# and to return internal states as well. we don't use the
# return states in the training model, but we will use them in inference.
decoder_lstm = lstm(latent_dim, return_sequences=true, return_state=true)
attention = dot([decoder_lstm, encoder_lstm], axes=[2, 2])
attention = activation('softmax')(attention)
context = dot([attention, encoder_lstm], axes=[2,1])
decoder_combined_context = concatenate([context, decoder_lstm])
decoder_outputs, _, _ = decoder_combined_context(dec_emb,
                                     initial_state=encoder_states)
decoder_dense = dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# define the model that will turn
# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
model = model([encoder_inputs, decoder_inputs], decoder_outputs)
model.summary()

while doing this, i got an error 
 layer dot_1 was called with an input that isn't a symbolic tensor. received type: <class 'keras.layers.recurrent.lstm'>. full input: [<keras.layers.recurrent.lstm object at 0x7f8f77e2f3c8>, <keras.layers.recurrent.lstm object at 0x7f8f770beb70>]. all inputs to the layer should be tensors.

can someone please help in fitting an attention layer in this architecture?","['python-3.x', 'tensorflow', 'keras', 'nlp', 'machine-translation']",62357664,"the dot products need to be computed on tensor outputs... in encoder you correctly define the encoder_output, in decoder you have to add decoder_outputs, state_h, state_c = decoder_lstm(enc_emb, initial_state=encoder_states)
the dot products now are
attention = dot([decoder_outputs, encoder_outputs], axes=[2, 2])
attention = activation('softmax')(attention)
context = dot([attention, encoder_outputs], axes=[2,1])

the concatenation doesn't need initial_states. you have to define it in your rnn layer: decoder_outputs, state_h, state_c = decoder_lstm(enc_emb, initial_state=encoder_states)
here the full example
encoder + decoder
# dummy variables
num_encoder_tokens = 30
num_decoder_tokens = 10
latent_dim = 100

encoder_inputs = input(shape=(none,))
enc_emb =  embedding(num_encoder_tokens, latent_dim, mask_zero = true)(encoder_inputs)
encoder_lstm = lstm(latent_dim, return_sequences=true, return_state=true)
encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)
# we discard `encoder_outputs` and only keep the states.
encoder_states = [state_h, state_c]

# set up the decoder, using `encoder_states` as initial state.
decoder_inputs = input(shape=(none,))
dec_emb_layer = embedding(num_decoder_tokens, latent_dim, mask_zero = true)
dec_emb = dec_emb_layer(decoder_inputs)
# we set up our decoder to return full output sequences,
# and to return internal states as well. we don't use the
# return states in the training model, but we will use them in inference.
decoder_lstm = lstm(latent_dim, return_sequences=true, return_state=true)
decoder_outputs, _, _ = decoder_lstm(dec_emb,
                                     initial_state=encoder_states)
decoder_dense = dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# define the model that will turn
# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
model = model([encoder_inputs, decoder_inputs], decoder_outputs)
model.summary()

decoder w\ attention
# set up the decoder, using `encoder_states` as initial state.
decoder_inputs = input(shape=(none,))
dec_emb_layer = embedding(num_decoder_tokens, latent_dim, mask_zero = true)
dec_emb = dec_emb_layer(decoder_inputs)
# we set up our decoder to return full output sequences,
# and to return internal states as well. we don't use the
# return states in the training model, but we will use them in inference.
decoder_lstm = lstm(latent_dim, return_sequences=true, return_state=true)
decoder_outputs, state_h, state_c = decoder_lstm(dec_emb, initial_state=encoder_states)
attention = dot([decoder_outputs, encoder_outputs], axes=[2, 2])
attention = activation('softmax')(attention)
context = dot([attention, encoder_outputs], axes=[2,1])
decoder_outputs = concatenate([context, decoder_outputs])
decoder_dense = dense(num_decoder_tokens, activation='softmax')(decoder_outputs)

# define the model that will turn
# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
model = model([encoder_inputs, decoder_inputs], decoder_dense)
model.summary()",https://stackoverflow.com/questions/62357239,python-3.x,13-06-2020 08:20,1543.0,6.0,2.0,True,26-09-2022 19:59,13-06-2020 08:24
71922261,typeerror: setup() got an unexpected keyword argument &#39;stage&#39;,"i am trying to train my q&a model through pytorch_lightning. however while running the command trainer.fit(model,data_module) i am getting the following error:
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
<ipython-input-72-b9cdaa88efa7> in <module>()
----> 1 trainer.fit(model,data_module)

4 frames
/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in _call_setup_hook(self)
   1488 
   1489         if self.datamodule is not none:
-> 1490             self.datamodule.setup(stage=fn)
   1491         self._call_callback_hooks(""setup"", stage=fn)
   1492         self._call_lightning_module_hook(""setup"", stage=fn)

typeerror: setup() got an unexpected keyword argument 'stage'

i have installed and imported pytorch_lightning.
also i have defined data_module = bioqadatamodule(train_df, val_df, tokenizer, batch_size = batch_size) where batch_size = 2, n_epochs = 6.
the model i have used is as follows:-
model = t5forconditionalgeneration.from_pretrained(model_name, return_dict=true)

also, i have defined the class for the model as follows:-
    class bioqamodel(pl.lightningmodule):
    
      def __init__(self):
        super().__init__()
        self.model = t5forconditionalgeneration.from_pretrained(model_name, return_dict=true)
    
      def forward(self, input_ids, attention_mask, labels=none):
        output = self.model(
            input_ids = encoding[""input_ids""],
            attention_mask = encoding[""attention_mask""],
            labels=labels
        )
    
        return output.loss, output.logits
    
      def training_step(self, batch, batch_idx):
        input_ids = batch[""input_ids""]
        attention_mask = batch[""attention_mask""]
        labels = batch[""labels""]
        loss, outputs = self(input_ids, attention_mask, labels)
        self.log(""train_loss"", loss, prog_bar=true, logger=true)
        return loss
    
      def validation_step(self, batch, batch_idx):
        input_ids = batch[""input_ids""]
        attention_mask = batch[""attention_mask""]
        labels = batch[""labels""]
        loss, outputs = self(input_ids, attention_mask, labels)
        self.log(""val_loss"", loss, prog_bar=true, logger=true)
        return loss 
    
      def test_step(self, batch, batch_idx):
        input_ids = batch[""input_ids""]
        attention_mask = batch[""attention_mask""]
        labels = batch[""labels""]
        loss, outputs = self(input_ids, attention_mask, labels)
        self.log(""test_loss"", loss, prog_bar=true, logger=true)
        return loss
    
      def configure_optimizers(self):
        return adamw(self.parameters(), lr=0.0001)

for any additional information required, please specify.
edit 1: adding bioqadatamodule:
class bioqadatamodule(pl.lightningdatamodule):

  def __init__(
      self,
      train_df: pd.dataframe,
      test_df: pd.dataframe,
      tokenizer: t5tokenizer,
      batch_size: int = 8,
      source_max_token_len = 396,
      target_max_token_len = 32
    ):
      super().__init__()
      self.batch_size = batch_size
      self.train_df = train_df
      self.test_df = test_df
      self.tokenizer = tokenizer
      self.source_max_token_len = source_max_token_len
      self.target_max_token_len = target_max_token_len

  def setup(self):
    self.train_dataset = bioqadataset(
        self.train_df,
        self.tokenizer,
        self.source_max_token_len,
        self.target_max_token_len
    )

    self.test_dataset = bioqadataset(
        self.test_df,
        self.tokenizer,
        self.source_max_token_len,
        self.target_max_token_len
    )

  def train_dataloader(self):
    return dataloader(
        self.train_dataset,
        batch_size = self.batch_size,
        shuffle = true,
        num_workers = 4
    )

  def val_dataloader(self):
    return dataloader(
        self.train_dataset,
        batch_size = 1,
        shuffle = true,
        num_workers = 4  
    )

  def test_dataloader(self):
    return dataloader(
        self.train_dataset,
        batch_size = 1,
        shuffle = true,
        num_workers = 4  
    )","['python', 'pytorch', 'huggingface-transformers', 'pytorch-lightning']",71932249,"you need to add an extra argument stage=none to your setup method:
def setup(self, stage=none):
    self.train_dataset = bioqadataset(
        self.train_df,
        self.tokenizer,
        self.source_max_token_len,
        self.target_max_token_len
    )

    self.test_dataset = bioqadataset(
        self.test_df,
        self.tokenizer,
        self.source_max_token_len,
        self.target_max_token_len
    )

i've played with pytorch lightning myself for multi-gpu training here. although some of the code is a bit outdated (metrics are a standalone module now), you might find it useful.",https://stackoverflow.com/questions/71922261,python,19-04-2022 08:59,4894.0,4.0,1.0,True,19-04-2022 23:01,19-04-2022 14:22
77074094,dataloader/sampler/collator to create batches based on the sample contents (sequence length),"i am converting someone else's code into a neater torch-y pipeline, using datasets and dataloaders, collate functions and samplers. while i have done such work before, i am not sure how to tackle the following problem.
the dataset contains sentences as samples. every samples therefore has a number of words (or tokens), which we can get by naively splitting the sample on white space (sample.split()). such a dummy dataset can look like this:
from random import randint

from torch.utils.data import dataset


class dummydataset(dataset):
    def __init__(self):
        data = []
        for _ in range(128):
            data.append(""hello "" * randint(64, 176))
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx: int):
        return self.data[idx]

now i want to be able to load data so that the max. number of tokens in a batch is not more than 250. that implies that the batch size can differ between iterations. one batch may contain two samples that have no more than 250 tokens in total (for instance 127 + 77) and another can have three (66+66+66). now, the core functionality for this is rather straightforward. full example below; not optimized by sorting on length or something but that's okay for this example.
the question is, how can i integrate this in the pytorch eco-system? batch sizes are so often used to indicate the number of samples (like in the dataloader). so where should i plug this in, or what should i subclass, to make this work like a regular dataloader?
from random import randint

from torch.utils.data import dataset

class dummydataset(dataset):
    def __init__(self):
        data = []
        for _ in range(128):
            data.append(""hello "" * randint(64, 176))
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx: int):
        return self.data[idx]


if __name__ == '__main__':
    dataset = dummydataset()

    def get_batch(max_tokens: int = 250):
        data_idxs = list(range(len(dataset)))

        batch = []
        total_batch_len = 0
        while data_idxs:
            sample = dataset[data_idxs[0]]
            sample_len = len(sample.split())

            if total_batch_len + sample_len <= max_tokens:
                batch.append(sample)
                total_batch_len += sample_len
                data_idxs.pop(0)
            elif batch:
                yield batch
                batch = []
                total_batch_len = 0

        yield batch

    # sanity check that we indeed get all items from the dataset
    num_samples = 0
    num_batches = 0
    for b in get_batch():
        num_samples += len(b)
        num_batches += 1

    print(f""created {num_batches} batches"")
    assert num_samples == len(dataset)

maybe torchtext's iterator and its batch_size_fn can help but i have no experience with it (where should i add it; is it a dataloader itself or should i still wrap a dataloader around it, etc.).","['python', 'pytorch', 'nlp', 'batch-processing', 'dataloader']",77075337,"after reading some source code, it seems that you can just use any iterator in a dataloader's batch_sampler. so the following works as expected.
from random import randint

from torch.utils.data import dataset
from torch.utils.data.dataloader import dataloader


class dummydataset(dataset):
    def __init__(self):
        data = []
        for _ in range(128):
            data.append(""hello "" * randint(64, 176))
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx: int):
        return self.data[idx]


class tokenbatchsampler:
    def __init__(self, max_tokens: int = 250):
        self.max_tokens = max_tokens
        self.batches = []
        self._prepare_dataset()

    def __len__(self) -> int:
        return len(self.batches)

    def __iter__(self):
        return iter(self.batches)

    def _prepare_dataset(self):
        data_idxs = list(range(len(dataset)))

        batches = []
        batch_idxs = []
        total_batch_len = 0
        while data_idxs:
            sample_idx = data_idxs[0]
            sample = dataset[sample_idx]
            sample_len = len(sample.split())

            if total_batch_len + sample_len <= self.max_tokens:
                batch_idxs.append(sample_idx)
                total_batch_len += sample_len
                data_idxs.pop(0)
            elif batch_idxs:
                batches.append(batch_idxs)
                batch_idxs = []
                total_batch_len = 0

        batches.append(batch_idxs)

        self.batches = batches


if __name__ == ""__main__"":
    dataset = dummydataset()

    sampler = tokenbatchsampler()
    dataloader = dataloader(dataset, batch_sampler=sampler)
    # sanity check that we indeed get all items from the dataset
    for epoch in range(3):
        num_samples = 0
        num_batches = 0
        for b in dataloader:
            num_samples += len(b)
            num_batches += 1

        print(f""created {num_batches} batches in epoch {epoch}"")
        assert num_samples == len(dataset)

    print(f""dataloader length {len(dataloader)}"")",https://stackoverflow.com/questions/77074094,python,09-09-2023 21:28,609.0,0.0,1.0,True,11-09-2023 06:02,11-09-2023 06:02
860809,how do you parse a paragraph of text into sentences? (perferrably in ruby),"how do you take paragraph or large amount of text and break it into sentences (perferably using ruby) taking into account cases such as mr. and dr. and u.s.a?  (assuming you just put the sentences into an array of arrays)
update:
one possible solution i thought of involves using a parts-of-speech tagger (post) and a classifier to determine the end of a sentence:
getting data from mr. jones felt the warm sun on his face as he stepped out onto the balcony of his summer home in italy.  he was happy to be alive.
classifier
mr./person jones/person felt/o the/o warm/o sun/o on/o his/o face/o as/o he/o stepped/o out/o onto/o the/o balcony/o of/o his/o summer/o home/o in/o italy/location ./o he/o was/o happy/o to/o be/o alive/o ./o
post
mr./nnp jones/nnp felt/vbd the/dt warm/jj sun/nn on/in his/prp$ face/nn as/in he/prp stepped/vbd out/rp onto/in the/dt balcony/nn of/in his/prp$ summer/nn home/nn in/in italy./nnp he/prp was/vbd happy/jj to/to be/vb alive./in 
can we assume, since italy is a location, the period is the valid end of the sentence? since ending on ""mr."" would have no other parts-of-speech, can we assume this is not a valid end-of-sentence period? is this the best answer to the my question? 
thoughts?","ruby, text, parsing, split, nlp",863687,try looking at the ruby wrapper around the stanford parser. it has a getsentencesfromstring() function.,https://stackoverflow.com/q/860809,"ruby, text, parsing, split, nlp",13-05-2009 22:49,18323.0,23.0,14.0,True,05-10-2021 07:09,14-05-2009 01:11
69042028,what is the algorithm behind pairwise2 align in biopython?,"the package biopython allows to compute pairwise local or global alignement, through different functions (align.globalxx, align.localxx, ...).
however, i have not found anywhere the algorithm on which this alignement is based.
the code (source, doc) states: ""pairwise sequence alignment using a dynamic programming algorithm"", and that is all.

is there a paper on which this implementation is based?
is it using a ""standard algorithm"" and if yes, what is its name?

edit: this is a question more for citing than for understanding purposes.","['algorithm', 'nlp', 'biopython']",69043425,"the docstring of a private function in the code indicates that ""this is an implementation of the needleman-wunsch dynamic programming algorithm as modified by gotoh, implementing affine gap penalties."" (l. 761 of this code).",https://stackoverflow.com/questions/69042028,algorithm,03-09-2021 08:46,1108.0,3.0,1.0,True,04-07-2022 09:10,03-09-2021 10:22
78352556,"milvus.py: pymilvus.exceptions.milvusexception: &lt;milvusexception: (code=1, message=field full_length is not in the hit entity)&gt;","i query a collection in a zilliz milvus db like this:
documents = vector_store.similarity_search_with_score(query)

the query is successful but in line 777 of milvus.py the value result.full_length is retrieved, which is not available:
for result in res[0]:
            data = {x: result.entity.get(x) for x in output_fields}
            doc = self._parse_document(data)
            pair = (doc, result.full_length)
            ret.append(pair)

which then leads to this exception
file ""/users/tilman/langchaincorsera/venv/lib/python3.9/site-packages/langchain_community/vectorstores/milvus.py"", line 644, in similarity_search
    res = self.similarity_search_with_score(
  file ""/users/tilman/langchaincorsera/venv/lib/python3.9/site-packages/langchain_community/vectorstores/milvus.py"", line 717, in similarity_search_with_score
    res = self.similarity_search_with_score_by_vector(
  file ""/users/tilman/langchaincorsera/venv/lib/python3.9/site-packages/langchain_community/vectorstores/milvus.py"", line 777, in similarity_search_with_score_by_vector
    pair = (doc, result.full_length)
  file ""/users/tilman/langchaincorsera/venv/lib/python3.9/site-packages/pymilvus/client/abstract.py"", line 588, in __getattr__
    raise milvusexception(message=f""field {item} is not in the hit entity"")
pymilvus.exceptions.milvusexception: <milvusexception: (code=1, message=field full_length is not in the hit entity)>

any clues?","['python', 'langchain', 'milvus']",78364397,"turned out this was a bug in langchain-community, that was resolved with an update",https://stackoverflow.com/questions/78352556,python,19-04-2024 09:02,1867.0,1.0,3.0,True,29-04-2024 10:46,19-04-2024 12:19
75682401,python nlp processing if statement not in stop words list,"i'm working with nlp spacy library and i created a function to return a list of token from a text.
import spacy    
def preprocess_text_spacy(text):
    stop_words = [""a"", ""the"", ""is"", ""are""]
    nlp = spacy.load('en_core_web_sm')
    tokens = set()
    doc = nlp(text)
    for word in doc:
        if word.is_currency:
            tokens.add(word.lower_)
        elif len(word.lower_) == 1:
            if word.is_digit and float(word.text) == 0:
                tokens.add(word.text)
        elif not word.is_punct and not word.is_space and not word.is_quote and not word.is_bracket and not in stop_words:
            tokens.add(word.lower_)
    return list(tokens)

this function is not correct because removing stop words not working.
everything is ok only if i delete the last condition and not in stop_words.
how to upgrade this function to remove stop words according a defined list in addition to all other condition statement?","['python', 'python-3.x', 'nlp', 'spacy']",75682705,"your code looks fine to me, there is a small change
at the end of elif put and str(word) not in stop_words
import spacy    
def preprocess_text_spacy(text):
    stop_words = [""a"", ""the"", ""is"", ""are""]
    nlp = spacy.load('en_core_web_sm')
    tokens = set()
    doc = nlp(text)
    print(doc)
    for word in doc:
        if word.is_currency:
            tokens.add(word.lower_)
        elif len(word.lower_) == 1:
            if word.is_digit and float(word.text) == 0:
                tokens.add(word.text)
        elif not word.is_punct and not word.is_space and not word.is_quote and not word.is_bracket and str(word) not in stop_words:
            tokens.add(word.lower_)
    return list(tokens)",https://stackoverflow.com/questions/75682401,python,09-03-2023 09:03,94.0,0.0,3.0,True,09-03-2023 09:49,09-03-2023 09:49
76301180,problem with spacy.load(&quot;en_core_web_md&quot;) in python,"i'm struggling with installation of package en_core_web_md from spacy library. after executing
spacy.load(""en_core_web_md"")

i get this error: oserror: [e050] can't find model 'en_core_web_md'. it doesn't seem to be a python package or a valid path to a data directory.
i used this command in the terminal:
python -m spacy download en_core_web_md

and terminal returned this output:

i installed it manually as well, using .whl file and this command:

still after executing this line of code:
print(spacy.util.get_installed_models())

i recive empty brackets [], so no wonder when i execute my code:
import pandas as pd
import spacy


def lemmatize_text(text):
    nlp = spacy.load(""en_core_web_md"")
    doc = nlp(text)
    return "" "".join([token.lemma_ for token in doc])


if __name__ == '__main__':
    print(spacy.util.get_installed_models())

    database = pd.read_csv('database.csv')

    database['lyric'] = database['lyric'].apply(lemmatize_text)

    database.to_csv('database.csv', index=false)

i get this output in console

oserror: [e050] can't find model 'en_core_web_md'. it doesn't seem to be a python package or a valid path to a data directory.
what is wrong with my installation? i'm using pycharm 2023.1, python 3.11.3 and windows 11.","['python', 'python-3.x', 'pycharm', 'spacy']",76301207,once you download en_core_web_md you need to restart runtime or ctrl+m from the menu bar.,https://stackoverflow.com/questions/76301180,python,21-05-2023 17:46,342.0,0.0,1.0,True,23-05-2023 12:02,23-05-2023 12:02
75791645,cannot allocate memory failed to allocate when using kenlm build_binary,"i have a arpa file which i created by the following command:
 ./lmplz -o 4 -s 1g <tmp_100m.txt >100m.arpa

now i want to convert this arpa file to binary file:
./build_binary 100m.arpa 100m.bin

and i'm getting error:
mmap.cc:225 in void util::hugemalloc(std::size_t, bool, util::scoped_memory&) threw errnoexception because `!to.get()'.
cannot allocate memory failed to allocate 106122412848 bytes byte: 80
error

i tried to add -s parameter:
./build_binary -s 1g 100m.arpa 100m.bin

and i got the same error.

how can i convert to binary file ?

why i'm getting this error ?","['c++', 'nlp', 'n-gram', 'language-model', 'kenlm']",75792240,"take a look at  for some light explanation
try this instead:
lm_order=4
corpus_lm=""tmp_100m""
lang_e=""txt""
lm_arpa=""100m.arpa""
lm_file=""100m.bin""

${moses_bin_dir}/lmplz --order ${lm_order} -s 80% -t /tmp \
< ${corpus_lm}.${lang_e} | gzip > ${lm_arpa}

${moses_bin_dir}/build_binary trie -a 22 -b 8 -q 8 ${lm_arpa} ${lm_file}

moses_bin_dir is the directory where the binaries you've compiled are stored.

if you still face the memory issue when using the trie and quantization options, you might need to change to a machine/instance where the cpu ram is sufficient to read your language model and produce the binary.",https://stackoverflow.com/questions/75791645,c++,20-03-2023 14:35,456.0,1.0,1.0,True,20-03-2023 15:32,20-03-2023 15:32
62435022,where in the code of pytorch or huggingface/transformer label gets &quot;renamed&quot; into labels?,"my question concerns the example, available in the great huggingface/transformers library.
i am using a notebook, provided by library creators as a starting point for my pipeline. it presents a pipeline of finetuning a bert for sentence classification on glue dataset.
when getting into the code, i noticed a very weird thing, which i cannot explain.
in the example, input data is introduced to the model as the instances of the inputfeatures class from here:
this class has 4 attributes, including the label attribute:
class inputfeatures:
    ...
    input_ids: list[int]
    attention_mask: optional[list[int]] = none
    token_type_ids: optional[list[int]] = none
    label: optional[union[int, float]] = none

which are later passed as a dictionary of inputs to the forward() method of the model. this is done by the trainer class, for example in the lines 573-576 here:
    def _training_step(
        self, model: nn.module, inputs: dict[str, torch.tensor], optimizer: torch.optim.optimizer
    ) -> float:
        model.train()
        for k, v in inputs.items():
            inputs[k] = v.to(self.args.device)

        outputs = model(**inputs)  

however, the forward() method expects labels (note the plural form) input parameter (taken from here):
    def forward(
        self,
        input_ids=none,
        attention_mask=none,
        head_mask=none,
        inputs_embeds=none,
        labels=none,
        output_attentions=none,
    ):

so my question is where does the label become labels in this pipeline?
to give some extra info on the issue, i created my own pipeline, which uses nothing, related, with glue data and pipe, basically it relies only on the trainer class of transformers. i even use another model (flaubert). i replicated the inputfeature class and my code works for both cases below:
class inputfeature:
    def __init__(self, text, label):
        self.input_ids = text
        self.label = label

class inputfeatureplural:
    def __init__(self, text, label):
        self.input_ids = text
        self.labels = label

but it does not work if i name the second attribute as self.labe or by any other names. why is it possible to use both attribute names?
it's not like it is extremely important in my case, but i feel uncomfortable passing around the data in the variable, which ""changes name"" somewhere along the way.","['python', 'pytorch', 'huggingface-transformers']",62436033,"the rename happens in the collator. in the trainer init, when data_collator is none, a default one is used:
class trainer:
    # ...
    def __init__(...):
        # ...
        self.data_collator = data_collator if data_collator is not none else default_data_collator
        # ...

fyi, the self.data_collator is later used when you get the dataloader:
data_loader = dataloader(
    self.train_dataset,
    batch_size=self.args.train_batch_size,
    sampler=train_sampler,
    collate_fn=self.data_collator,              # <-- here
    drop_last=self.args.dataloader_drop_last,
)

the default collator has a special handling for labels, which does this renaming, if needed:
# special handling for labels.
# ensure that tensor is created with the correct type
# (it should be automatically the case, but let's make sure of it.)
if hasattr(first, ""label"") and first.label is not none:
    if type(first.label) is int:
        labels = torch.tensor([f.label for f in features], dtype=torch.long)
    else:
        labels = torch.tensor([f.label for f in features], dtype=torch.float)
    batch = {""labels"": labels}  # <-- here is where it happens
elif hasattr(first, ""label_ids"") and first.label_ids is not none:
    if type(first.label_ids[0]) is int:
        labels = torch.tensor([f.label_ids for f in features], dtype=torch.long)
    else:
        labels = torch.tensor([f.label_ids for f in features], dtype=torch.float)
    batch = {""labels"": labels}
else:
    batch = {}",https://stackoverflow.com/questions/62435022,python,17-06-2020 17:33,1998.0,2.0,1.0,True,28-04-2021 14:04,28-04-2021 14:04
73852932,how do i group topic clustered data in pandas by topic?,"after adding cluster topics to a dataframe in pandas i get a result which looks like this:
[{'document': 'lorem', 'topic': 0},
 {'document': 'ipsum', 'topic': 0},
 {'document': 'dolor', 'topic': 0},
 {'document': 'sit', 'topic': 1},
 {'document': 'amet', 'topic': 1},

as a table, document and topic are the headers:
|      document       |     topic        |
|---------------------|------------------|
|          lorem      |         0        |
|          ipsum      |         0        |

what i would like to do is have the unique topics be the headers and the documents the values, for example:
|      0              |   1  |
|---------------------|------|
|          lorem      | sit  |
|          ipsum      | amet |

i've tried a lot of hacky solutions to this, and all of them involve using for loops and leaving pandas, so i would really like to know what the correct way of doing this in pandas would be.
the closest i've gotten in pandas itself is this:
df.groupby(""topic"").agg(list)

however this groups each document in an array according to the topic so like this:
|      document       |   topic |
|---------------------|---------| 
|  [lorem, ipsum]     | 0       |
|  [sit, amet]        | 1       |

which is not much better than what i started with.
thanks so much!","['python', 'pandas', 'cluster-analysis', 'topic-modeling']",73853391,"assuming a list of dictionaries as input, use pd.json_normalize combined with pivot:
(pd.json_normalize(l)
   .assign(row=lambda d: d.groupby('topic').cumcount())
   .pivot('row', 'topic', 'document')
   .dropna()
)

output:
topic      0     1
row               
0      lorem   sit
1      ipsum  amet",https://stackoverflow.com/questions/73852932,python,26-09-2022 10:38,76.0,0.0,1.0,True,26-09-2022 11:18,26-09-2022 10:43
74397544,"input 0 of layer &quot;lstm&quot; is incompatible with the layer: expected shape=(128, none, 256), found shape=(32, 187, 256) - character rnn model - tensorflow","below is the error traceback when i run the predict() using my trained model

valueerror                                traceback (most recent call last)
\<ipython-input-51-5ae18e06838a\> in \<module\>
7 for input_example_batch, target_example_batch in ds_series_batch_test:
8
\----\> 9   pred=model.predict(input_example_batch)
10   pred_max=tf.argmax(tf.nn.softmax(pred),2).numpy().flatten()
11   y_true=target_example_batch.numpy().flatten()

1 frames
/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in tf__predict_function(iterator)
13                 try:
14                     do_return = true
\---\> 15                     retval\_ = ag_\_.converted_call(ag_\_.ld(step_function), (ag_\_.ld(self), ag_\_.ld(iterator)), none, fscope)
16                 except:
17                     do_return = false

valueerror: in user code:

    file ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 1845, in predict_function  *
        return step_function(self, iterator)
    file ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 1834, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    file ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 1823, in run_step  **
        outputs = model.predict_step(data)
    file ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 1791, in predict_step
        return self(x, training=false)
    file ""/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
        raise e.with_traceback(filtered_tb) from none
    file ""/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py"", line 264, in assert_input_compatibility
        raise valueerror(f'input {input_index} of layer ""{layer_name}"" is '
    
    valueerror: exception encountered when calling layer ""sequential"" (type sequential).
    
    input 0 of layer ""lstm"" is incompatible with the layer: expected shape=(128, none, 256), found shape=(32, 187, 256)
    
    call arguments received by layer ""sequential"" (type sequential):
      ï¿½ï¿½ï¿½ inputs=tf.tensor(shape=(32, 187), dtype=int32)
      ï¿½ï¿½ï¿½ training=false
      ï¿½ï¿½ï¿½ mask=none

below is the code used for generating train and test data
x_total, x_test, y_total, y_test = train_test_split(train, test,
    test_size=0.2, shuffle = true, random_state = 8)

x_train, x_val, y_train, y_val = train_test_split(x_total, y_total,
    test_size=0.25, shuffle = true, random_state = 8)  # 0.25 x 0.8 = 0.2

print(""x_train shape: {}"".format(x_train.shape))
print(""x_test shape: {}"".format(x_test.shape))
print(""x_val shape: {}"".format(x_val.shape))
print(""y_train shape: {}"".format(y_train.shape))
print(""y_test shape: {}"".format(y_test.shape))
print(""y val shape: {}"".format(y_val.shape)re>x_train shape: (173424,)
x_test shape: (57809,)
x_val shape: (57809,)
y_train shape: (173424,)
y_test shape: (57809,)
y val shape: (57809,)

# train_formatted
train_formatted = []
for eg1, eg2 in zip(x_train, y_train):
  train_formatted.append((eg1,eg2))

# test_formatted
test_formatted = []
for eg1, eg2 in zip(x_test, y_test):
  test_formatted.append((eg1,eg2))
# valid_formatted
valid_formatted = []
for eg1, eg2 in zip(x_val, y_val):
  valid_formatted.append((eg1,eg2))
# training generator
def gen_train_series():

    for eg in train_formatted:
      yield eg[0],eg[1]

# validation generator
def gen_valid_series():

    for eg in valid_formatted:
      yield eg[0],eg[1]

# test generator
def gen_test_series():

  for eg in test_formatted:
      yield eg[0],eg[1]
  
  
# create dataset objects for train, test and validation sets  
series = tf.data.dataset.from_generator(gen_train_series,output_types=(tf.int32, tf.int32),output_shapes = ((none, none)))
series_valid = tf.data.dataset.from_generator(gen_valid_series,output_types=(tf.int32, tf.int32),output_shapes = ((none, none)))
series_test = tf.data.dataset.from_generator(gen_test_series,output_types=(tf.int32, tf.int32),output_shapes = ((none, none)))

batch_size = 128
buffer_size=1000

# create padded batch series objects for train, test and validation sets
ds_series_batch = series.shuffle(buffer_size).padded_batch(batch_size, padded_shapes=([none], [none]), drop_remainder=true)
ds_series_batch_valid = series_valid.padded_batch(batch_size, padded_shapes=([none], [none]), drop_remainder=true)
ds_series_batch_test = series_test.padded_batch(batch_size, padded_shapes=([none], [none]), drop_remainder=true)

# print example batches
for input_example_batch, target_example_batch in ds_series_batch_valid.take(1):
  print(input_example_batch)
  print(target_example_batch)

# below are the shapes of my input and target batch tensor shape

input_example_batch - shape passed to predict() -  as follows
tf.tensor(
[[36 26 37 ...  0  0  0]
 [40 40 43 ...  0  0  0]
 [26 39 26 ...  0  0  0]
 ...
 [11  8 12 ...  0  0  0]
 [44 28 33 ...  0  0  0]
 [46  1 38 ...  0  0  0]], shape=(128, 160), dtype=int32)
tf.tensor(
[[6 6 6 ... 0 0 0]
 [6 6 6 ... 0 0 0]
 [6 6 6 ... 0 0 0]
 ...
 [2 6 2 ... 0 0 0]
 [2 2 2 ... 0 0 0]
 [2 6 2 ... 0 0 0]], shape=(128, 160), dtype=int32)

i am training my model as follows -
vocab_size = len(vocabulary) #53

# the embedding dimension
embedding_dim = 256

# number of rnn units
rnn_units = 1024

label_size = len(labels)  # - 0 to 7

def build_model(vocab_size,label_size, embedding_dim, rnn_units, batch_size):
      model = tf.keras.sequential([
          tf.keras.layers.embedding(vocab_size, embedding_dim,
                            batch_input_shape=[batch_size, none],mask_zero=true),
          tf.keras.layers.lstm(rnn_units,
                      return_sequences=true,
                      stateful=true,
                      recurrent_initializer='glorot_uniform'),
          tf.keras.layers.dense(label_size)
          ])
      return model


#todo - check why vocab_size+1,
# passing label_size - because it already includes - 'other' label

model = build_model(vocab_size = vocab_size+1 ,label_size = label_size,embedding_dim=embedding_dim,
      rnn_units=rnn_units,
              batch_size=batch_size)

# define loss function
def loss(labels, logits):
    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=true)

model.compile(optimizer='adam', loss=loss,metrics=[tf.keras.metrics.sparsecategoricalaccuracy()])


checkpoint_callback = tf.keras.callbacks.modelcheckpoint(
    filepath=checkpoint_prefix,
    save_weights_only=true, save_freq = 'epoch')

# fitting the model as follows

# using just 1 epoch - as i am debugging

epochs = 1
checkpoint_callback = tf.keras.callbacks.modelcheckpoint(
      filepath=checkpoint_prefix,
      save_weights_only=true, save_freq = 'epoch')
  
ic(""fitting the model..."")
history = model.fit(ds_series_batch, epochs=epochs, validation_data=ds_series_batch_valid,callbacks=[checkpoint_callback])


below displayed is the model summary
model: ""sequential""
_________________________________________________________________
 layer (type)                output shape              param #   
=================================================================
 embedding (embedding)       (128, none, 256)          13824     
                                                                 
 lstm (lstm)                 (128, 1024)               5246976   
                                                                 
 dense (dense)               (128, 7)                  7175      
                                                                 
=================================================================
total params: 5,267,975
trainable params: 5,267,975
non-trainable params: 0
_______________________________

from sklearn.metrics import classification_report, confusion_matrix

preds = np.array([])
y_trues= np.array([])

# iterate through test set, make predictions based on trained model
for input_example_batch, target_example_batch in ds_series_batch_test:

  pred=model.predict(input_example_batch)
  pred_max=tf.argmax(tf.nn.softmax(pred),2).numpy().flatten()
  y_true=target_example_batch.numpy().flatten()


the above code is what i use to call predict() - which results in the error mentioned in the title
below link has the code referred to - test out a character level rnn implementation

i pretty much did the same as the one in the collab notebook, but training using a different input data.
many solutions suggested were not very specific to this problem, and one i tried - was setting return_sequences = true, which is what i already had while building the model. nothing has worked so far. with above code, training works fine, but unable to run predict - due to mismatch of output shape. i am very new to tensorflow. so any kind of help in understanding the issue, and ways to resolve would be greatly appreciated. thank you in advance.
    encoded_input_features                              output_features
0   [11, 11, 1, 39, 40, 43, 45, 33, 1, 44, 45, 43,...   [2, 2, 6, 3, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, ...
1   [29, 5, 10, 5, 44, 33, 34, 47, 26, 37, 34, 36,...   [6, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, ...
2   [10, 14, 5, 41, 43, 26, 28, 33, 34, 1, 27, 46,...   [2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, ...
3   [48, 51, 1, 14, 16, 10, 1, 41, 26, 29, 26, 38,...   [6, 6, 6, 2, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
4   [32, 31, 1, 9, 11, 5, 45, 15, 5, 32, 43, 26, 2...   [6, 6, 6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, ...
... ... ...
289041  [47, 30, 39, 36, 30, 45, 30, 44, 33, 1, 39, 26...   [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
289042  [43, 26, 27, 34, 39, 29, 43, 26, 1, 36, 30, 39...   [6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, ...
289043  [41, 26, 37, 35, 26, 1, 31, 26, 37, 34, 50, 46...   [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
289044  [17, 13, 16, 10, 9, 15, 1, 36, 26, 37, 34, 1, ...   [2, 2, 2, 2, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
289045  [36, 40, 39, 32, 46, 1, 38, 30, 44, 44, 5, 36,...   [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...

now after i generate the train, test, validation split - from the above
dataset - trying to convert the input features - into a list of numpy array
import numpy as np
address_character_list = []

def convert_input_features(x):
  address_character_list.append(np.asarray(x, dtype=np.float32))

pd.dataframe(y_train)['output_features'].swifter.apply(lambda x: convert_input_features(x))
y_train_list = np.array(address_character_list)

import numpy as np
address_character_list = []

def convert_input_features(x):
  address_character_list.append(np.asarray(x, dtype=np.float32))

pd.dataframe(x_train)['encoded_input_features'].swifter.apply(lambda x: convert_input_features(x))
x_train_list = np.array(address_character_list)

this gives
type(x_train) = numpy.ndarray
type(y_train) = numpy.ndarray

for eg. x_train looks like
array([array([...],
             dtype=float32)                                                ,[array([...], dtype = float32),
      dtype=object)

similarly - i have y_train as well,
now if i need to reshape my train example using code below - how do i do that, cause from_tensor_slices() gives the error below
# train_examples = tf.data.dataset.from_tensor_slices((x_train_list, y_train_list))
train_examples = tf.data.dataset.from_tensor_slices((x_train_list, y_train_list))
# x_train = np.random.randint(0,10, size=(289042))
# y_train = np.random.randint(0,7, size=(289042))
# train_examples = tf.data.dataset.from_tensor_slices((x_train, y_train))

def preprocess(ds):
    return (
        ds
        .cache()
        .shuffle(buffer_size=1000)
        .batch(128)
        .prefetch(buffer_size=tf.data.autotune)
    )

train_examples = preprocess(train_examples)


error as follows
valueerror: failed to convert a numpy array to a tensor (unsupported object type list).

and the solutions given for this error didn't work.","['tensorflow', 'keras', 'recurrent-neural-network', 'named-entity-recognition']",74403584,"updating the answer... now, let suppose i as you said i have
x_train = np.random.randint(0,10, size=(289042))
y_train = np.random.randint(0,7, size=(289042))

you don't need a generator to yield the dataset, if you have a numpy array simply load it with tf.data.dataset.from_tensor_slices(),
train_examples = tf.data.dataset.from_tensor_slices((x_train, y_train))

def preprocess(ds):
    return (
        ds
        .cache()
        .shuffle(buffer_size=1000)
        .batch(128)
        .prefetch(buffer_size=tf.data.autotune)
    )

train_examples = preprocess(train_examples)

now, something more if you want stateful=true in lstm then your batch_size should be equal for all the samples, there may be a chance if your last sample has less than 128 batch size. so, then it will throw the error.
model = tf.keras.sequential([
          tf.keras.layers.input(shape=(1)),
          tf.keras.layers.embedding(1024, 512, mask_zero=true),
          tf.keras.layers.lstm(200,
                      return_sequences=false,
                      recurrent_initializer='glorot_uniform'),
          tf.keras.layers.dense(7)
          ])

model(next(iter(train_examples.take(1)))[0]).shape
model.summary()

furthermore, use tf.keras.losses.sparsecategorical_crossentropy(from_logits=true)
model.compile(optimizer='adam', loss=tf.keras.losses.sparsecategoricalcrossentropy(from_logits=true)
              ,metrics=[tf.keras.metrics.sparsecategoricalaccuracy()])

history = model.fit(train_examples, epochs=1)

output:
[==============================] - 7s 30ms/step - loss: 1.9473 - sparse_categorical_accuracy: 0.1398",https://stackoverflow.com/questions/74397544,tensorflow,11-11-2022 02:52,286.0,0.0,1.0,True,12-11-2022 12:22,12-11-2022 12:22
72986749,how to get token or code embedding using codex api?,"for a given code snippet, how to get embedding using the codex api?
import os
import openai
import config


openai.api_key = config.openai_api_key

def runsomecode():
    response = openai.completion.create(
      engine=""code-davinci-001"",
      prompt=""\""\""\""\n1. get a reputable free news api\n2. make a request to the api for the latest news stories\n\""\""\"""",
      temperature=0,
      max_tokens=1500,
      top_p=1,
      frequency_penalty=0,
      presence_penalty=0)

    if 'choices' in response:
        x = response['choices']
        if len(x) > 0:
            return x[0]['text']
        else:
            return ''
    else:
        return ''



answer = runsomecode()
print(answer)

but i want to figure out given a python code block like the following, can i get the embedding from codex?
input:
import random
a = random.randint(1,12)
b = random.randint(1,12)
for i in range(10):
    question = ""what is ""+a+"" x ""+b+""? ""
    answer = input(question)
    if answer = a*b
        print (well done!)
    else:
        print(""no."")

output:

embedding of the input code","['python', 'transformer-model', 'openai-api', 'language-model']",73096314,"the function get_embedding will give us an embedding for an input text.
canonical code from openai here: 
import openai
from tenacity import retry, wait_random_exponential, stop_after_attempt

@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))
def get_embedding(text: str, engine=""text-similarity-davinci-001"") -> list[float]:

    # replace newlines, which can negatively affect performance.
    text = text.replace(""\n"", "" "")

    return openai.embedding.create(input=[text], engine=engine)[""data""][0][""embedding""]

embedding = get_embedding(""sample query text goes here"", engine=""text-search-ada-query-001"")
print(len(embedding))",https://stackoverflow.com/questions/72986749,python,14-07-2022 21:16,693.0,-1.0,2.0,True,18-01-2023 21:33,18-01-2023 21:33
68928299,multiclass sequence classifiaction with fastai and huggingface,"i am looking to implement distilbert via fastai and huggingface for a mutliclass sequence classification problem. i found a useful tutorial that gave a good example on how to do this with binary classification. the code is below:
# !pip install torch==1.9.0
# !pip install torchtext==0.10
# !pip install transformers==4.7
# !pip install fastai==2.4

from fastai.text.all import *
from sklearn.model_selection import train_test_split
import pandas as pd
import glob
from transformers import autotokenizer, automodelforsequenceclassification


hf_tokenizer = autotokenizer.from_pretrained(""distilbert-base-uncased"")
hf_model = automodelforsequenceclassification.from_pretrained(""distilbert-base-uncased"")

""""""
train_df and val_df looks like this:

      label text
4240    5   whoa interesting.
13      7   you could you could we just
4639    4   you set the goal,
28      1   because ive already agreed to that
66      8   oh hey freshman thats you gona need
""""""

print(list(train_df.label.value_counts().index))
""""""
[4, 1, 5, 6, 7, 0, 2, 3, 8]
""""""

class hf_dataset(torch.utils.data.dataset):
    def __init__(self, df, hf_tokenizer):
        self.df = df
        self.hf_tokenizer = hf_tokenizer
        
        self.label_map = {
            0:0,
            1:0,
            2:0,
            3:0,
            4:1,
            5:1,
            6:1,
            7:1,
            8:1
        }
        
    def __len__(self):
        return len(self.df)

    def decode(self, token_ids):
        return ' '.join([hf_tokenizer.decode(x) for x in tokenizer_outputs['input_ids']])
    
    def decode_to_original(self, token_ids):
        return self.hf_tokenizer.decode(token_ids.squeeze())

    def __getitem__(self, index):
        label, text = self.df.iloc[index]
        label = self.label_map[label]
        label = torch.tensor(label)

        tokenizer_output = self.hf_tokenizer(text, return_tensors=""pt"", padding='max_length', truncation=true, max_length=512)
        
        tokenizer_output['input_ids'].squeeze_()
        tokenizer_output['attention_mask'].squeeze_()
        
        return tokenizer_output, label
        

train_dataset = hf_dataset(train_df, hf_tokenizer)
valid_dataset = hf_dataset(valid_df, hf_tokenizer)

train_dl = dataloader(train_dataset, bs=16, shuffle=true)
valid_dl = dataloader(valid_dataset, bs=16)
dls = dataloaders(train_dl, valid_dl)
hf_model(**batched_data)


class hf_model(nn.module):
  
    def __init__(self, hf_model):
        super().__init__()
        
        self.hf_model = hf_model
        
    def forward(self, tokenizer_outputs):
        
        model_output = self.hf_model(**tokenizer_outputs)
        
        return model_output.logits
        
model = hf_model(hf_model)
# manually popping the model onto the gpu since the data is in a dictionary format
# (doesn't automatically place model + data on gpu otherwise)
learn = learner(dls, model, loss_func=nn.crossentropyloss(), metrics=[accuracy])
learn.fit_one_cycle(3, 1e-4)

this works fine. however, i mapped my multiclass labels to 2 labels to allow this to work. i actually have 9 classes. i tried adjusting the label mapping scheme in hf_dataset() class to match my actual labels like below:
class hf_dataset(torch.utils.data.dataset):
    def __init__(self, df, hf_tokenizer):
        self.df = df
        self.hf_tokenizer = hf_tokenizer
        
        self.label_map = {
            0:0,
            1:1,
            2:2,
            3:3,
            4:4,
            5:5,
            6:6,
            7:7,
            8:8
        }
        
    def __len__(self):
        return len(self.df)

    def decode(self, token_ids):
        return ' '.join([hf_tokenizer.decode(x) for x in tokenizer_outputs['input_ids']])
    
    def decode_to_original(self, token_ids):
        return self.hf_tokenizer.decode(token_ids.squeeze())

    def __getitem__(self, index):
        label, text = self.df.iloc[index]
        label = self.label_map[label]
        label = torch.tensor(label)

        tokenizer_output = self.hf_tokenizer(text, return_tensors=""pt"", padding='max_length', truncation=true, max_length=512)
        
        tokenizer_output['input_ids'].squeeze_()
        tokenizer_output['attention_mask'].squeeze_()
        
        return tokenizer_output, label

every line works until learn.fit_one_cycle.
here is the full stack trace from this line:

 0.00% [0/3 00:00<00:00]
epoch   train_loss  valid_loss  accuracy    time

 0.00% [0/519 00:00<00:00]
---------------------------------------------------------------------------
indexerror                                traceback (most recent call last)
<ipython-input-21-0ec2ff9e12e1> in <module>
----> 1 learn.fit_one_cycle(3, 1e-4)

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/callback/schedule.py in fit_one_cycle(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt)
    111     scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),
    112               'mom': combined_cos(pct_start, *(self.moms if moms is none else moms))}
--> 113     self.fit(n_epoch, cbs=paramscheduler(scheds)+l(cbs), reset_opt=reset_opt, wd=wd)
    114 
    115 # cell

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in fit(self, n_epoch, lr, wd, cbs, reset_opt)
    219             self.opt.set_hypers(lr=self.lr if lr is none else lr)
    220             self.n_epoch = n_epoch
--> 221             self._with_events(self._do_fit, 'fit', cancelfitexception, self._end_cleanup)
    222 
    223     def _end_cleanup(self): self.dl,self.xb,self.yb,self.pred,self.loss = none,(none,),(none,),none,none

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    161 
    162     def _with_events(self, f, event_type, ex, final=noop):
--> 163         try: self(f'before_{event_type}');  f()
    164         except ex: self(f'after_cancel_{event_type}')
    165         self(f'after_{event_type}');  final()

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _do_fit(self)
    210         for epoch in range(self.n_epoch):
    211             self.epoch=epoch
--> 212             self._with_events(self._do_epoch, 'epoch', cancelepochexception)
    213 
    214     def fit(self, n_epoch, lr=none, wd=none, cbs=none, reset_opt=false):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    161 
    162     def _with_events(self, f, event_type, ex, final=noop):
--> 163         try: self(f'before_{event_type}');  f()
    164         except ex: self(f'after_cancel_{event_type}')
    165         self(f'after_{event_type}');  final()

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _do_epoch(self)
    204 
    205     def _do_epoch(self):
--> 206         self._do_epoch_train()
    207         self._do_epoch_validate()
    208 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _do_epoch_train(self)
    196     def _do_epoch_train(self):
    197         self.dl = self.dls.train
--> 198         self._with_events(self.all_batches, 'train', canceltrainexception)
    199 
    200     def _do_epoch_validate(self, ds_idx=1, dl=none):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    161 
    162     def _with_events(self, f, event_type, ex, final=noop):
--> 163         try: self(f'before_{event_type}');  f()
    164         except ex: self(f'after_cancel_{event_type}')
    165         self(f'after_{event_type}');  final()

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in all_batches(self)
    167     def all_batches(self):
    168         self.n_iter = len(self.dl)
--> 169         for o in enumerate(self.dl): self.one_batch(*o)
    170 
    171     def _do_one_batch(self):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in one_batch(self, i, b)
    192         b = self._set_device(b)
    193         self._split(b)
--> 194         self._with_events(self._do_one_batch, 'batch', cancelbatchexception)
    195 
    196     def _do_epoch_train(self):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    161 
    162     def _with_events(self, f, event_type, ex, final=noop):
--> 163         try: self(f'before_{event_type}');  f()
    164         except ex: self(f'after_cancel_{event_type}')
    165         self(f'after_{event_type}');  final()

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/fastai/learner.py in _do_one_batch(self)
    173         self('after_pred')
    174         if len(self.yb):
--> 175             self.loss_grad = self.loss_func(self.pred, *self.yb)
    176             self.loss = self.loss_grad.clone()
    177         self('after_loss')

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/loss.py in forward(self, input, target)
   1119     def forward(self, input: tensor, target: tensor) -> tensor:
   1120         return f.cross_entropy(input, target, weight=self.weight,
-> 1121                                ignore_index=self.ignore_index, reduction=self.reduction)
   1122 
   1123 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction)
   2822     if size_average is not none or reduce is not none:
   2823         reduction = _reduction.legacy_get_string(size_average, reduce)
-> 2824     return torch._c._nn.cross_entropy_loss(input, target, weight, _reduction.get_enum(reduction), ignore_index)
   2825 
   2826 

indexerror: target 6 is out of bounds.

this seems like it should be a simple fix. do i need to adjust something in the model architecture to allow it to accept 9 labels? or do i need to one hot encode my labels? if so, is there a solution prebuilt to do this in the pipeline?","['python', 'deep-learning', 'pytorch', 'huggingface-transformers', 'fast-ai']",68928610,"you need to define num_labels=9 when loading the model:
hf_model = automodelforsequenceclassification.from_pretrained(""distilbert-base-uncased"", num_labels=9)

the default value is 2, which suits the first use-case, but breaks when you tried to change.
note that the lib explictly says that the classifier (which generates the .logits that are of your interest) is randomly initialized:

some weights of distilbertforsequenceclassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']
you should probably train this model on a down-stream task to be able to use it for predictions and inference.",https://stackoverflow.com/questions/68928299,python,25-08-2021 18:41,394.0,0.0,1.0,True,25-08-2021 19:09,25-08-2021 19:01
70709572,typeerror: not a string | parameters in autotokenizer.from_pretrained(),"goal: amend this notebook to work with albert-base-v2 model.
kernel: conda_pytorch_p36. i did restart & run all, and refreshed file view in working directory.

in order to evaluate and to export this quantised model, i need to setup a tokenizer.
error occurs in section 1.3.
both parameters in autotokenizer.from_pretrained() throw the same error.

section 1.3 code:
# define the tokenizer
tokenizer = autotokenizer.from_pretrained(
        configs.output_dir, do_lower_case=configs.do_lower_case)

parameters:
# the output directory for the fine-tuned model, $out_dir.
configs.output_dir = ""./mrpc/""

# prepare glue task
...
configs.do_lower_case = true

values and dtypes:
-- configs.output_dir --
./mrpc/
<class 'str'>

-- configs.do_lower_case --
true
<class 'bool'>

traceback:
---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
<ipython-input-13-18c5137aacf4> in <module>
    140 # define the tokenizer
    141 tokenizer = autotokenizer.from_pretrained(
--> 142         configs.output_dir, do_lower_case=configs.do_lower_case)
    143 
    144 # evaluate the original fp32 bert model

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/models/auto/tokenization_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    548             tokenizer_class_py, tokenizer_class_fast = tokenizer_mapping[type(config)]
    549             if tokenizer_class_fast and (use_fast or tokenizer_class_py is none):
--> 550                 return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
    551             else:
    552                 if tokenizer_class_py is not none:

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
   1752             use_auth_token=use_auth_token,
   1753             cache_dir=cache_dir,
-> 1754             **kwargs,
   1755         )
   1756 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)
   1776                 copy.deepcopy(init_configuration),
   1777                 *init_inputs,
-> 1778                 **(copy.deepcopy(kwargs)),
   1779             )
   1780         else:

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)
   1880         # instantiate tokenizer.
   1881         try:
-> 1882             tokenizer = cls(*init_inputs, **init_kwargs)
   1883         except oserror:
   1884             raise oserror(

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/models/albert/tokenization_albert.py in __init__(self, vocab_file, do_lower_case, remove_space, keep_accents, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, sp_model_kwargs, **kwargs)
    179 
    180         self.sp_model = spm.sentencepieceprocessor(**self.sp_model_kwargs)
--> 181         self.sp_model.load(vocab_file)
    182 
    183     @property

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sentencepiece/__init__.py in load(self, model_file, model_proto)
    365       if model_proto:
    366         return self.loadfromserializedproto(model_proto)
--> 367       return self.loadfromfile(model_file)
    368 
    369 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sentencepiece/__init__.py in loadfromfile(self, arg)
    169 
    170     def loadfromfile(self, arg):
--> 171         return _sentencepiece.sentencepieceprocessor_loadfromfile(self, arg)
    172 
    173     def decodeidswithcheck(self, ids):

typeerror: not a string

please let me know if there's anything else i can add to post.","['python', 'tensorflow', 'huggingface-transformers', 'onnx', 'huggingface-tokenizers']",70711835,"passing just the model name suffices.
tokenizer = alberttokenizer.from_pretrained('albert-base-v2')

list of model_types can be found here.",https://stackoverflow.com/questions/70709572,python,14-01-2022 11:00,8453.0,3.0,1.0,True,14-01-2022 14:07,14-01-2022 11:07
70985982,how to check if a python string is a valid bengali word using regular expression?,"i am trying to test if a word is a valid bengali word which may contain bengali letters, vowel markers, 'hasanta' (""ï¿½ï¿½ï¿½""), bengali digits, all punctuation symbols including bengali ""ï¿½ï¿½ï¿½"". we can test this easily for english using regex patter ""\w+"", but i cannot find any way to do this in bengali.
for example, these strings: ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"", ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"", ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"", ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"" should be detected as valid bengali words, but ""i"", ""eat"", &quot","['python', 'regex', 'string', 'nlp']",71002560,"you can pip install regex and use
bool(regex.fullmatch(r'\p{l}*\p{bengali}+(?:\p{l}+\p{bengali}+)*\p{l}*', word)

note that regex.fullmatch will require a full string to match the following pattern:

\p{l}* - zero or more non-letter chars
\p{bengali}+ - one or more bengali chars
(?:\p{l}+\p{bengali}+)* - zero or more sequences of one or more non-letter chars and then one or more bengali chars
\p{l}* - zero or more non-letter chars.

see the python demo:
import regex
words = [""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"", ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"", ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"", ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"", ""i"", ""eat"", ""rice"", ""123""]
for word in words:
    print( word, '=>', bool(regex.fullmatch(r'\p{l}*\p{bengali}+(?:""lang-none prettyprint-override"">ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ => true
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ => true
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ => true
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ => true",https://stackoverflow.com/questions/70985982,python,04-02-2022 11:54,745.0,0.0,2.0,True,27-04-2023 09:57,07-02-2022 18:12
78631255,how to extract image hidden states in llava&#39;s transformers (huggingface) implementation?,"i am using the transformers library (huggingface) to extract all hidden units of llava 1.5. on the huggingface documentation, it shows that it is possible to extract image hidden states from the vision component.
unfortunately, the outputs object has only these following keys available in the output dictionary:
odict_keys(['sequences', 'attentions', 'hidden_states', 'past_key_values'])
how do i also extract the image_hidden_states from this llava implementation alongwith the exisiting outputs?
i have implemented the follow code in the hopes to do so.
import torch
from transformers import llavaforconditionalgeneration, llavaconfig, clipvisionconfig, llamaconfig, autoprocessor, llavaprocessor
from pil import image
import requests
from torchinfo import summary

device = ""cuda:0"" if torch.cuda.is_available() else ""cpu""
model_id = 'llava-hf/llava-1.5-7b-hf'

# initializing a clip-vision config
vision_config = clipvisionconfig(output_hidden_states=true, output_attentions=true, return_dict=true)

# initializing a llama config
text_config = llamaconfig(output_hidden_states=true, output_attentions=true, return_dict=true)

# initializing a llava llava-1.5-7b style configuration
configuration = llavaconfig(vision_config, text_config, output_hidden_states=true, output_attentions=true, return_dict=true)
cfg=llavaconfig(vision_config, text_config, output_hidden_states=true, output_attentions=true, return_dict=true)

# initializing a model from the llava-1.5-7b style configuration
model = llavaforconditionalgeneration(configuration).from_pretrained(model_id, output_hidden_states=true, output_attentions=true, return_dict=true)

# accessing the model configuration
configuration = model.config

model=model.to(device)
print(summary(model))

processor = llavaprocessor.from_pretrained(""llava-hf/llava-1.5-7b-hf"", output_hidden_states=true, output_attentions=true, return_dict=true)
prompt = ""user: <image>\nis there sun in the image? assistant:""
url = ""
image = image.open(requests.get(url, stream=true).raw)
inputs = processor(text=prompt, images=image, return_tensors=""pt"")
inputs=inputs.to(device)

with torch.no_grad():
    outputs = model.generate(**inputs, 
                             output_hidden_states=true, 
                             return_dict_in_generate=true, 
                             max_new_tokens=1, 
                             min_new_tokens=1,
                            return_dict=true)

print(outputs.keys())","['huggingface-transformers', 'transformer-model', 'multimodal']",78664084,"ok, i will try to answer my own question. the solution was quite not available directly with the transformers library. i do not know, why the functionality which is mentioned in their documentation doesn't work. however, i found a work-around by making use of the pytorch pre-hooks and getting the values of the hidden-units.",https://stackoverflow.com/questions/78631255,huggingface-transformers,17-06-2024 07:04,500.0,2.0,1.0,True,03-01-2025 08:14,17-06-2024 09:40
48117508,how to find a similar substring inside a large string with a similarity score in python?,"what i'm looking for is not just a plain similarity score between two texts. but a similarity score of a substring inside a string. say:
text1 = 'cat is sleeping on the mat'.

text2 = 'the cat is sleeping on the red mat in the living room'.

in the above example, all the words of text1 are present in the text2 completely, hence the similarity should be 100%. 
if some words of text1 are missing, the score shall be less.
i'm working with a large dataset of varying paragraph size, hence finding a smaller paragraph inside a bigger one with such similarity score is crucial. 
i found only string similarities such as cosine similarities, difflib similarity etc. which compares two strings. but not about a score of substring inside another string.","['python', 'string', 'nlp', 'distance', 'similarity']",48117886,"based on your description, how about:
>>> a = ""cat is sleeping on the mat""
>>> b = ""the cat is sleeping on the red mat in the living room""
>>> a = a.split("" "")
>>> score = 0.0
>>> for word in a: #for every word in your string
        if word in b: #if it is in your bigger string increase score
            score += 1
>>> score/len(a) #obtain percentage given total word number
1.0

in case it had a missing word for example:
>>> c = ""the cat is not sleeping on the mat""
>>> c = c.split("" "")
>>> score = 0.0
>>> for w in c:
        if w in b:
            score +=1
>>> score/len(c)
0.875

additionally, you can do as @roadrunner suggest and split b and save it as a set to speed up your performance with b = set(b.split("" "")). this will reduce that part complexity to o(1) and improve the overall algorithm to a o(n) complexity.
edit: you say you already tried some metrics like cosine similarity etc. however i suspect you may benefit from checking the levenshtein distance similarity, which i suspect could be of some use in this case as addition to the solutions provided.",https://stackoverflow.com/questions/48117508,python,05-01-2018 16:24,7019.0,5.0,4.0,True,07-08-2023 02:53,05-01-2018 20:30
73428120,runtimeerror: mse_cuda not implemented for long when training a transformer.trainer,"i'm attempting to train a huggingface trainer but seeing the following error:
runtimeerror: ""mse_cuda"" not implemented for 'long' when training a transformer.trainer

i've tried this in multiple cloud environments (cpu & gpu) with no luck. the dataset (tok_dds) is of the following shape and type, and i've ensured there are no null values.
dataset({
    features: ['label', 'title', 'text', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 5000
})

{'label': int,
 'title': str,
 'text': str,
 'input': str,
 'input_ids': list,
 'token_type_ids': list,
 'attention_mask': list}

i have defined my loss functions as below:
def corr(x,y): return np.corrcoef(x,y)[0][1]
def corr_d(eval_pred): return {'pearson': corr(*eval_pred)}

however, when attempting to train the model_nm = 'microsoft/deberta-v3-small' on the train/test split of my dataset. i see the following error:
dds = tok_ds.train_test_split(0.25, seed=42)
tokz = autotokenizer.from_pretrained(model_nm)
model = automodelforsequenceclassification.from_pretrained(model_nm, num_labels=1)
trainer = trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],
                  tokenizer=tokz, compute_metrics=corr_d)
...
...
file /shared-libs/python3.9/py/lib/python3.9/site-packages/torch/nn/functional.py:3280, in mse_loss(input, target, size_average, reduce, reduction)
   3277     reduction = _reduction.legacy_get_string(size_average, reduce)
   3279 expanded_input, expanded_target = torch.broadcast_tensors(input, target)
-> 3280 return torch._c._nn.mse_loss(expanded_input, expanded_target, _reduction.get_enum(reduction))
runtimeerror: ""mse_cuda"" not implemented for 'long' when training a transformer.trainer

here are the args passed into the trainer if it's relevant:
args = trainingarguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=true,
    evaluation_strategy=""epoch"", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,
    num_train_epochs=epochs, weight_decay=0.01, report_to='none')

here's is what i think may be relevant environment information
!python --version
python 3.9.13

!pip list
package                       version
----------------------------- ------------
...
transformers                  4.21.1
huggingface-hub               0.8.1
pandas                        1.2.5
protobuf                      3.19.4
scikit-learn                  1.1.1
tensorflow                    2.9.1
torch                         1.12.0

can anyone point me in the right direction to solve this problem?","['python', 'pytorch', 'huggingface-transformers']",73465321,"changing the datatype of the labels column from int to float solved this issue for me. if your dataset is from a pandas dataframe, you can change the datatype of the column before passing the dataframe to a dataset.",https://stackoverflow.com/questions/73428120,python,20-08-2022 15:57,2849.0,0.0,1.0,True,23-08-2022 21:44,20-08-2022 16:12
67740759,how to apply a pretrained transformer model from huggingface?,"i am interested in using pre-trained models from hugging face for named entity recognition (ner) tasks without further training or testing of the model.
on the model page of hugging face, the only information for reusing the model are as follows:
from transformers import autotokenizer, automodel
tokenizer = autotokenizer.from_pretrained(""emilyalsentzer/bio_clinicalbert"")
model = automodel.from_pretrained(""emilyalsentzer/bio_clinicalbert"")

i tried the following code, but i am getting a tensor output instead of class labels for each named entity.
from transformers import autotokenizer, automodel
tokenizer = autotokenizer.from_pretrained(""emilyalsentzer/bio_clinicalbert"")
model = automodel.from_pretrained(""emilyalsentzer/bio_clinicalbert"")

text = ""my text for named entity recognition here.""

input_ids = torch.tensor(tokenizer.encode(text, padding=true, truncation=true,max_length=50, add_special_tokens = true)).unsqueeze(0)

with torch.no_grad():
  output = model(input_ids, output_attentions=true)

any suggestions on how to apply the model on a text for ner?","['huggingface-transformers', 'named-entity-recognition', 'transformer-model']",67780432,"in transformers ner is done with the tokenclassificationpipeline:
from transformers import autotokenizer, pipeline,  automodelfortokenclassification
tokenizer = autotokenizer.from_pretrained(""emilyalsentzer/bio_clinicalbert"")
model = automodelfortokenclassification.from_pretrained(""emilyalsentzer/bio_clinicalbert"")
nerpipeline = pipeline('ner', model=model, tokenizer=tokenizer)
text = ""my text for named entity recognition here.""
nerpipeline(text)

output:
[{'word': 'my',
  'score': 0.5209763050079346,
  'entity': 'label_0',
  'index': 1,
  'start': 0,
  'end': 2},
 {'word': 'text',
  'score': 0.5161970257759094,
  'entity': 'label_0',
  'index': 2,
  'start': 3,
  'end': 7},
 {'word': 'for',
  'score': 0.5297629237174988,
  'entity': 'label_1',
  'index': 3,
  'start': 8,
  'end': 11},
 {'word': 'named',
  'score': 0.5258920788764954,
  'entity': 'label_1',
  'index': 4,
  'start': 12,
  'end': 17},
 {'word': 'entity',
  'score': 0.5415489673614502,
  'entity': 'label_1',
  'index': 5,
  'start': 18,
  'end': 24},
 {'word': 'recognition',
  'score': 0.5396601557731628,
  'entity': 'label_1',
  'index': 6,
  'start': 25,
  'end': 36},
 {'word': 'here',
  'score': 0.5165827870368958,
  'entity': 'label_0',
  'index': 7,
  'start': 37,
  'end': 41},
 {'word': '.',
  'score': 0.5266348123550415,
  'entity': 'label_0',
  'index': 8,
  'start': 41,
  'end': 42}]

please note that you need to use automodelfortokenclassification instead of automodel and that not all models have a trained head for token classification, i.e. you will get random weights for the token classification head :)",https://stackoverflow.com/questions/67740759,huggingface-transformers,28-05-2021 14:27,12550.0,4.0,2.0,True,16-11-2024 14:16,13-12-2023 20:10
67061184,how to remove the ending sign from bangla text?,ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿,"['python', 'nlp', 'nltk', 'tokenize']",70975860,"to remove the ending ""ï¿½ï¿½ï¿½"", you can use string .replace() method: sentence.replace('ï¿½ï¿½ï¿½', '').
to tokenize the sentence, there are many libraries available, you can use any"" rel=""nofollow noreferrer"">inltk, indicnlp, stanfordnlp, bnlp. here's a complete example using bnlp:
from bnlp import nltktokenizer

bnltk = nltktokenizer()

sentence = ""ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿",https://stackoverflow.com/questions/67061184,python,12-04-2021 15:28,118.0,1.0,1.0,True,03-02-2022 17:41,12-04-2021 15:53
73233063,keyword assignment (not keyword extraction) in python machine learning: where to start?,"i want to do keyword assignments (not keyword extraction) using python machine learning to a collection of articles, i.e. classifying a text using keywords from a predefined list. google gives me an abundance of results on keyword extraction instead. could you please direct me to any blogs or articles on the steps of keyword assignment (even better with recommendations to libraries)?
as shown in the screenshot (please advise how to share the csv file), ten existing questions have already been manually tagged, and a new eleventh question is waiting to be tagged based on the patterns.","['python', 'nlp']",73247822,"you can try & test multiple approaches and do a comparative analysis to find out which works best for you:

extract keywords from article, compare target tags to these extracted keywords and assign matching/similar tags. you can use word2vec and distance metrics for term similarity.
compute similarity between article and each tag and assign tags with similarity above certain threshold or top n terms. you can use bert to extract article embeddings and cosine similarity.",https://stackoverflow.com/questions/73233063,python,04-08-2022 08:59,146.0,-1.0,1.0,True,05-08-2022 09:48,05-08-2022 09:17
67349208,extract info from each row of a dataframe without a loop,"i have a large dataframe (~500,000 rows). processing each row gives me a counter object (a dictionary with objects counts). the output i want is a new dataframe which column headers are the objects that are being counted (the keys in the dictionary). i am looping over the rows, however it takes very long.i know that loops should be avoided in pandas, any suggestion?
out_df = pd.dataframe()
for row in input_df['text']:
    tokens = nltk.word_tokenize(row)
    pos = nltk.pos_tag(tokens)
    count = counter(elem[1] for elem in pos)
    out_df = out_df.append(count, ignore_index=true)

for indication, counter(elem[1] for elem in pos) looks like counter({'nn':8, 'vbz': 2, 'dt':3, 'in': 4})","['pandas', 'dataframe', 'loops', 'append', 'nltk']",67349356,"using append on a dataframe is quite inefficient i believe (has to reallocate memory for the entire data frame each time).
dataframes were meant for analyzing data and easily adding columnsï¿½ï¿½ï¿½but not rows.
so i think a better approach would be to create list first (lists are mutable) and convert it to a dataframe at the end.
i'm not familiar with nltk so i can't actually test this but something along the following lines should work:
out_data = []
for row in input_df['text']:
    tokens = nltk.word_tokenize(row)
    pos = nltk.pos_tag(tokens)
    count = counter(elem[1] for elem in pos)
    out_data.append(count)
out_df = pd.dataframe(out_data)

you might want to add the following to remove any nans and convert the final counts to integers:
out_df = out_df.fillna(0).astype(int)

and delete the list after to free up the memory:
del out_data",https://stackoverflow.com/questions/67349208,pandas,01-05-2021 17:42,358.0,1.0,2.0,True,01-05-2021 18:50,01-05-2021 17:49
